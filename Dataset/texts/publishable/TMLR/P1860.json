{
  "Abstract": "Encoding policies that solve sequential decision-making problems as programs offers advan-tages over neural representations, such as interpretability and modifiability of the policies.On the downside, programmatic policies are elusive because their generation requires oneto search in spaces of programs that are often discontinuous. In this paper, we leveragethe ability of large language models (LLMs) to write computer programs to speed up thesynthesis of programmatic policies. We use an LLM to provide initial candidates for thepolicy, which are then improved by local search. Empirical results in three problems thatare challenging for programmatic representations show that LLMs can speed up local searchand facilitate the synthesis of policies. We conjecture that LLMs are effective in this set-ting because we give them access to the outcomes of the policies rollouts. That way, LLMscan try policies encoding different behaviors, once they observe what a previous policy hasaccomplished. This process forces the search to explore different parts of the space throughexploratory initial programs. Experiments also show that much of the knowledge LLMsleverage comes from the domain-specific language that defines the search space - the overallperformance of the system drops sharply if we change the name of the functions used in thelanguage to meaningless names. Since our system only queries the LLM in the first step ofthe search, it offers an economical method for using LLMs to guide the synthesis of policies.",
  "Introduction": "There is a growing interest in using programmatic representations to encode solutions to sequential decision-making problems (Bonet et al., 2010; Aguas et al., 2018; Bastani et al., 2018; Trivedi et al., 2021; Moraeset al., 2023; Liang et al., 2023). Depending on the language used, programs that encode programmaticsolutions can generalize to unseen scenarios better than neural representations (Inala et al., 2020; Trivediet al., 2021). Such solutions can also be interpretable (Verma et al., 2018a; Medeiros et al., 2022; Aleixo &Lelis, 2023), which could allow users to manually modify computer-generated solutions. The challenge of using programmatic representations is that one needs to search in large and discontinuousspaces of programs, typically defined by a domain-specific language. In the context of supervised learning,a common approach to finding programmatic solutions is to learn a function that guides the search in theprogrammatic space (Balog et al., 2017; Odena et al., 2021; Odena & Sutton, 2020; Fijalkow et al., 2022;Ameen & Lelis, 2023). This function is learned in a self-supervised manner by exploiting the structure of",
  "Published in Transactions on Machine Learning Research (11/2024)": "{ (W(17)(0, (0,3), 1, 1),mv(u))(W(19)(0, (1,0), 1, 0),mv(r))(W(21)(0, (2,1), 1,1),ret(l))(B(4)(0, (1,1), 10, 0),wt(10)) },{ (W(15)(0, (1,2), 1, 0),mv(d))(W(19)(0, (3,0), 1, 1),mv(l))(W(21)(0, (3,1), 1,1),mv(l))(B(4)(0, (1,1), 10, 0),wt(10))(W(17)(0, (0,1), 1, 0),wt(10))(Br(23)(0, (2,2),4, 0),wt(10)) },{ (W(18)(1, (7,7), 1, 0),mv(l))(W(22)(1, (5,6), 1, 1),mv(r))(Rg(26)(1, (5,4), 1,0),mv(u))(B(6)(1, (7,6), 10, 0),wt(10))(W(16)(1, (6,5), 1, 0),wt(10))(W(20)(1, (7,5),1, 0),wt(10))(Br(24)(1, (5,5), 4, 0),wt(10)) },{ (Rg(25)(0, (3,2), 1, 0),wt(10)) },{ (Rg(28)(0, (2,5), 1, 0),att_loc(5,5)) },{ (B(6)(1, (7,6), 10, 0),prod(l,W))(W(33)(1, (7,5), 1, 0),mv(u))(W(37)(1, (7,7), 1,0),mv(l))(Br(36)(1, (6,5), 4, 0),wt(10)) },{ (Rg(25)(0, (5,4), 1, 0),att_loc(7,6))(Rg(31)(0, (6,4), 1, 0),att_loc(7,6))(Rg(28)(0,(3,6), 1, 0),wt(10)) },{ (B(6)(1, (7,6), 2, 0),wt(10)) }The strategy-2 failed to defeat strategy-1.",
  "Problem Definition": "Let (S, A, p, r, , ) be a Markov decision process (MDP). Here, S is the set of states, and A is the set ofactions. The function p(st+1|st, at) encodes the transition function, which returns the probability of reachingst+1 given that the agent is st and chooses at. The agent observes a reward value Rt+1, which is given bya function r, when moving from st to st+1. is the distribution of initial states of the MDP and in is the discount factor. A policy is a function that receives a state s and an action a and returns theprobability in which a is chosen in s. A solution to an MDP is a policy that maximizes the expected sumof discounted rewards for starting at s0 : R= E,p,[k=0 kRk+1]. We consider policies encoded in programs written in a domain-specific language (DSL). The set of programsthat a DSL accepts defines the set of policies that the agent can consider. This programmatic policy spaceis defined as a context-free grammar (M, , W, I), where M, , W, and S are the sets of non-terminals,terminals, the production rules of the grammar, and the grammars initial symbol, respectively. shows an example of a DSL, where M = {I, C, B}, = {c1, c2, b1, b2, if, then}, W are the production rules(e.g., C CC), and I is the initial symbol. We represent programs as abstract syntax trees (AST), where each node n and its children represent aproduction rule if n represents a non-terminal symbol. For example, the root of the tree in , whichrepresents the non-terminal I, and its children correspond to the production rule I if(B) then C. Leafnodes are terminals. shows an example of an AST for if b1 then c1 c2. A DSL D defines thepossibly infinite space of programs D; in our case, each p in D represents a policy. Given such a space,",
  "Programmatic Policy Learning as Local Search": "Previous work showed that a simple implementation of stochastic hill-climbing (SHC) achieves state-of-the-art results in challenging domains (Carvalho et al., 2024; Moraes & Lelis, 2024), so this is what we use inour experiments. SHC starts with an arbitrary policy as a candidate that maximizes the agents return. The initial candidate is generated starting with the initial symbol I and applying a production rule to replaceI; the rule is chosen uniformly at random or according to a probability distribution, if one is available (Trivediet al., 2021). We replace the leftmost non-terminal X of the resulting string with the symbols on the right-hand side of a production rule for X; the rule is randomly chosen among those for X. This process continuesuntil a string with only terminal symbols is generated, which represents the searchs initial candidate. SHC searches in the space defined by both the DSL and a neighborhood function Nk(). This functionreceives a candidate and returns a set of k policiesthe neighbors of in the search space. SHC evaluatesall k neighbors in terms of an estimate of the return R of each neighbor; SHC selects the neighbor of withthe largest estimated value R. The value of R is estimated by rolling out each of the neighboring policies.This process is repeated with the newly selected candidate policy. SHC stops if none of the neighbors hasan estimated R-value that is larger than the current candidate; the search reaches a local optimum. Weimplement SHC with a restarting strategy: once SHC reaches a local optimum, we restart the search from arandomly chosen initial candidate. SHC with restarts returns the best solution encountered in all searches. SHC is stochastic because the generation of the initial candidate and the neighborhood function are stochas-tic.In non-deterministic environments, we can only approximate the values of R when deciding whichneighbor to select. The neighbors Nk returns are generated as follows. Each of the k neighbors is generatedby randomly choosing a node n in the AST of the program that represents a non-terminal symbol (e.g.,the nodes I, B or C in ) and replacing the sub-tree rooted at n with a randomly generated sub-tree.This new sub-tree is generated by following the same process described to generate the initial candidate.The difference is that we start by replacing the non-terminal of node n, instead of the initial symbol I.",
  "Local Search with LLM (LS-LLM)": "Local Search with LLM (LS-LLM) is described in Algorithm 1, and a schematic view of it is shown in. It receives the MDP P, a large language model M, a budget B that specifies the number ofmodel queries that are allowed, a domain-specific language D, and a local search algorithm, SHC in ourimplementation.LS-LLM returns an approximate solution to P. shows which code lines inAlgorithm 1 interact with the MDP (first column), the LLM (second column), and SHC (third column). LS-LLM leverages the ability of the LLM to encode common knowledge from its training data, includingthe ability to generate computer programs, to speed up the search for programmatic policies. LS-LLM firstrequests from the LLM a set of policies that solve the MDP, for which a textual description is provided",
  ": return the result of SHC while using as initial candidate": "(lines 1 and 4). The policies are requested sequentially from the model. In this way, LS-LLM can providefeedback to the LLM by rolling out the policy in the environment (e.g., by informing the model of the actionsthe agent has taken in the environment); the feedback is denoted as F in Algorithm 1. Note that LS-LLMdoes not receive any feedback when requesting the first policy from M. In this case, the prompt simplyrequests a strong policy for the MDP (see .5 and the supplementary materials for details). The best policy, in terms of the average return R, that the LLM generates is given as input to SHC, whichattempts to further improve the policy with search (line 9). Even if the LLM is unable to directly generatea strong policy, its attempt at generating one might allow the search to start in a more promising part ofthe programmatic space. Such an initialization might allow for quick improvements with SHC.",
  "LS-LLM in Multi-Agent Settings": "In our experiments, we consider MDPs given by two-player zero-sum games. In this section, we explainAlgorithm 1 for this setting. In this setting, instead of approximating a solution to one MDP, we need toapproximate a solution to multiple MDPs. Each MDP is defined by fixing the opponent, and searching for apolicy that maximizes the agents return for that MDP; the opponent can be seen as part of the environment.In game theory terms, by maximizing the agents return, we approximate a best response to the opponent.We use self-play algorithms to define the MDPs to be solved so that the agent can learn to play a game. Iterated Best Response (IBR) is the simplest of such self-play algorithms. Let i and i be the two players.IBR starts with an arbitrary policy 0 in D for one of the players, for example, i, and approximates abest response 1 to 0 for i. Then, it approximates a best response to 1 for i. This process is repeated ntimes, which is determined by a computational budget. The last policies, n and n1, are returned as anapproximate solution to the game. In IBR, we use Algorithm 1 to compute each of these best responses. The self-play process IBR follows generates a sequence of policies for i and i, but IBR only considers thelatest policy while computing a best response, which can hinder the algorithms ability to learn how to playthe game. Other algorithms, such as Fictitious Play (FP) (Brown, 1951), are more informative in guiding thelearning process. This is because FP computes best responses to a policy that mixes all the best responsescomputed in previous iterations. Intuitively, FP forces the agent to learn to play against all of its previousversions. In practical terms, when evaluating the return R of a policy in Algorithm 1, we evaluate thepolicys return on multiple MDPs, one for each best response computed in the FP process. If FP computedthe policies 1, 2, , t1 so far, then the average return of policy t in Algorithm 1 is t1j=11",
  "t1Rj. Here,Rj is the return of t in the MDP in which j is fixed as part of the environment": "Other self-play algorithms, such as Double Oracle (DO) (McMahan et al., 2003), can be selective with whichopponents to fix and fold into the environment. In particular, Local Learner (2L) (Moraes et al., 2023)includes only the policies that were shown to provide search guidance in the program space. As a result, theevaluation of R for both DO and 2L can be computationally cheaper than the evaluation of R if FP is used.This is because the evaluation can be performed in fewer MDPs for DO and 2L. Previous work showed that",
  "Empirical Methodology": "We evaluate in three games our hypothesis that LLMs can be used to speed up the synthesis of programmaticpolicies by seeding the SHC search: Poachers & Rangers (PR) and Climbing Monkey (CM) (Moraes et al.,2023), and MicroRTS (Ontan et al., 2018). PR and CM are challenging for programmatic policies buteasy for humans; MicroRTS is a challenging real-time strategy game with an annual competition.2",
  "Poachers & Rangers": "PR is an unconstrained security game in which one of the players tries to defend a national park (rangers)and the other player attempts to attack the park (poachers). Given a number of gates N, rangers win thegame if they defend all attacked gates; poachers win the game if they attack an unprotected gate. The gameis unconstrained because rangers can choose to defend all N gates and poachers can choose to attack allN gates. Programs encoding strategies for rangers are of the form defend(1), defend(20), while encodingstrategies for poachers are of the form attack(1), attack(2), attack(10). Although the optimal strategyfor rangers is to trivially defend all gates, the synthesis of this strategy through self-play was shown to bedifficult (Moraes et al., 2023). This is because the program can be arbitrarily long, depending on N, and",
  "Climbing Monkey": "CM is a game in which two monkeys compete in a climbing contest: the monkey that can reach the highestbranch of a tree is the winner; the game ends in a draw if both monkeys climb to the same branch. In thisgame, a monkey must climb to branch i + 1 from i; no branch can be skipped. For example, the strategyclimb(1), climb(2), climb(20) allows the monkey to reach the branch 2 of the tree; instruction climb(20)is ignored. Similarly to PR, the optimal strategy is trivial, but hard to achieve with current systems. Incontrast to PR, the instructions in the optimal program must be ordered from branch 1 to N, which presentsdifferent challenges to the algorithms. Similarly to PR, the DSL for CM also does not allow for loops. For both PR and CM we provide as feedback F a textual description of the end-game state of the matchplayed between t and the last policy generated by the self-play algorithm, t1. For PR, the feedbackdescribes which gates were protected and which undefended gates the poachers attacked.For CM, thefeedback reports how many branches the monkeys climbed and if they won or lost the match. The promptswith the feedback are given in Appendix C.",
  "MicroRTS": "MicroRTS is a challenging real-time strategy game in which the agent controls many units in real time. Thegame is played on a gridded map that can vary in structure and size; different maps often require differentstrategies. In MicroRTS, the player starts with a Base unit and, depending on the map, with other units.The Base allows the player to train Worker units and store resources; Workers can build structures (Base orBarracks), collect resources, and attack opponent units. Barracks can train combat units, including Light,Ranged, and Heavy units. These units differ in their resource cost, the amount of damage they can cause toother units, the amount of damage they can suffer before being removed from the game, and in attack range.A player wins the game if they eliminate all units and structures of the other player. We use the followingmaps from the MicroRTS repository,3 with the map size in brackets: NoWhereToRun (9 8), DoubleGame(24 24), and BWDistantResources (32 32). The images of the maps are provided in Appendix A. The MicroRTS policies are written in Microlanguage, a DSL of the game (Mario et al., 2021). The Mi-crolanguage includes functions such as harvest and moveToUnit, which allow one to encode strong policieseven in short programs. The Microlanguage uses for-loops to prioritize actions. That is, functions calledearlier in a loop have a higher priority over those called later. The Microlanguage is described in Appendix C. MicroRTS is not a symmetric game depending on the starting location of the players. If policy 1 defeats2 when the former starts at location 1 and the latter at location 2, it does not mean that 1 will defeat 2if we swap their locations. To ensure a fair evaluation, each pair of policies plays two matches on each map,one in each initial location. We used GPT 3.5, which was trained with data up to September 2021. To the best of our knowledge, noprogrammatic policy for PR, CM, and MicroRTS had been published online before the models trainingcut-off date.The repository of Mario et al. (2021) contains programs written in an earlier version ofMicrolanguage.This earlier version of the language differs significantly from the version we use in ourexperiments in terms of the domain-specific functions available and the number of parameters requiredin the functions.To illustrate, none of the programs available in Mario et al.s repository can be runin the interpreter of our version of the language. Moreover, Mario et al. did not experiment with theNoWhereToRun and BWDistantResources maps that we use. Considering the differences in the languageand in the maps used, it is unlikely that Mario et al.s programs have influenced our results.",
  "Baseline Systems": "We evaluate LS-LLM with 2L as the learning algorithm; that is, we use 2L to determine the sequence ofMDPs LS-LLM is invoked to synthesize policies for. In the results plots, we call this combination 2L(LS-LLM). We use 2L because it was shown to outperform IBR, FP, and DO in the three games that we use inour experiments (Moraes et al., 2023). Furthermore, 2L placed second in the 2023 MicroRTS competition,only behind a programmatic strategy written by human programmers (MicroRTS AI Competition, 2023).Therefore, 2L(LS) (using local search, but without LLM) is our main baseline. We also use FP(LS) andIBR(LS) as baselines to offer a larger pool of policies. All algorithms used the same SHC implementation.Note that in self-play algorithms, we need to compute a sequence of best-responsesi.e., solve a sequence ofMDPs. While 2L(LS-LLM) uses an LLM to initialize each of these searches, the baselines 2L(LS), FP(LS),and IBR(LS) use the latest best response computed to initialize the search in the programmatic space. We did not consider deep reinforcement learning baselines because we focus on testing our hypothesis thatLLMs can be used to speed up the synthesis of programmatic policies.Moreover, the 2023 MicroRTScompetition showed the general trend that one should expect from such a comparison. On smaller maps, aneural policy outperforms programmatic ones. This is because the DSL used in the competition and also inour experiments does not allow for fine-grained control of the game units, which neural policies can quicklylearn on smaller maps. However, as the size of the maps increases, programmatic policies outperform neuralones. This is because the inductive bias the DSL provides allows for the synthesis of policies with stronghigh-level strategies (e.g., how to train stronger units and reach the opponent in larger spaces). See theMicroRTS AI Competition (2023) website for more information. We also did not consider other programmatic representations such as decision trees (Bastani et al., 2018)and finite state machines (Koul et al., 2019) in our experiments because they either require one to first traina neural model that is then distilled into a programmatic representation or to directly map the neural modelonto a program (Orfanos & Lelis, 2023). Due to this dependency on using a neural policy to guide thesynthesis process, it is unclear how to leverage LLMs to seed up learning with these representations. Onecould use decision trees and finite state machines, without neural guidance, as the underlying representationof the policies. However, we would lose the inductive bias the domain-specific language provides while stillpaying the cost of having to search in discrete and discontinuous spaces of programs.",
  "Language Model and Prompts": "The self-play algorithms require an initial policy to start the iterative process of computing best responses.For 2L(LS-LLM), we request an initial strategy from the LLM. For all baselines, we randomly generate thisinitial policy, as described in . 2L(LS-LLM) uses three different prompts for each domain: one forthe initial policy of the first iteration of 2L (called initial), one for the models first attempt to generatea best response to t (called first-attempt), and one for the remaining B 1 attempts (called feedback-attempt). We provide a description of the domain-specific language in all three prompts. The descriptionis given as a context-free grammar with explanations of the functions used in the language. In first-attemptand feedback-attempt we also provide the last policy generated in self-play, t, and explain that this is thepolicy that needs to be defeated. In the feedback-attempt we provide a sample of 10 actions of the modelsprevious attempt at a best response to t issued in a match against t. We do not provide all actions issuedin the match due to the models limited context length. We provide all prompts in Appendix C.",
  "Experiments Performed": "We performed three experiments.In the first experiment, we compare 2L(LS-LLM) on PR, CM, andthe three MicroRTS maps to our baselines 2L(LS), FP(LS), and IBR(LS). These baselines use the samelocal search algorithm as LS-LLM, SHC, but without LLMs. In this experiment, we compare the differentapproaches in terms of gates protected (PR), branches climbed (CM), and winning rate (MicroRTS). Thewinning rate of a policy is computed for a set of opponent policies and is computed while using as opponentsthe policies the other evaluated systems generate. Specifically, we sum the number of victories and half thenumber of draws and divide this sum by the total number of matches played (Ontan, 2017). For example,",
  "the winning rate of a strategy that wins 5, loses 2, and draws 3 matches is 5+1.5": "10= 0.65. The performancemetric of each domain is evaluated in terms of the number of games each system needs to play to achievea level of performance. The results are presented in plots where the y-axis shows the performance and thex-axis the number of games played ( shows two examples). This experiment evaluates our hypothesisthat LS-LLM can speed up the process of learning policies, in terms of the number of games played. The second and third experiments are intended to improve our understanding of LS-LLM. In the second, weevaluate LS-LLM while removing from feedback-attempt the set of actions sampled from the match playedbetween the LLM-generated policy and the last policy generated by the self-play algorithm. This experimentmeasures the impact of the feedback on the generation of an initial candidate policy for the SHC search. In the third experiment, we encrypt the names of the functions and nonterminal symbols in the Mi-crolanguage. We still explain in the prompt what each function does, but we use meaningless names forthem. For example, in the prompts used in the first and second experiments, we explain that the functionhasNumberOfUnits(T, N) checks if the ally player has N units of type T. In the third experiment, thisfunction is called b1, with the same explanation provided. Our goal is to verify how important the names offunctions and nonterminals are to the LLM.",
  "Other Specifications": "All experiments were run on computers with 2.6 GHz CPUs and 12 GB of RAM. We used B = 5 for LS-LLM. In PR, we set the number of gates to 60 and leave the number of branches for CM unbounded. Weuse the value of k = 1, 000 in Nk. SHC is run with a time limit of 2, 000 seconds. Once SHC reaches a localoptimum, if there is still time allowed to search, it restarts the search from the initial candidate the LLMhas suggested. After reaching the time limit, the search returns the best program encountered in all runs.This is SHCs approximation to a solution to the MDP.",
  "Winning Rate": "Map: BWDistantResources (32x32) IBR (LS)FP (LS)2L (LS)2L (LS-LLM) : Average winning rate by the number of games played for 2L(LS-LLM) (ours), 2L(LS), FP(LS), and IBR(LS)in MicroRTS. The winning rate is computed by having the strategy a system synthesized, at a given number of gamesplayed, play against the strategies each of the other systems synthesized after the maximum number of games wasplayed (50,000). The average and the 95% confidence interval are over 30 independent runs of the systems.",
  "Synthesizing Programmatic Policies": "presents the results for PR and CM. 2L(LS-LLM) represents our contribution, which uses 2L asthe learning algorithm, and LS-LLM to approximate the best responses. IBR(LS), FP(LS), and 2L(LS) areour baselines. We refer to the algorithms in the abbreviated forms LS-LLM, IBR, FP, and 2L. LS-LLM outperforms all baselines by a large margin. In PR, LS-LLM learns to defend all 60 gates withless than 20 games played. Often, the initial policy the LLM suggests already covers most of the gates inthe game. By contrast, progress for systems that rely only on search is slow. The LLM not only generatesbest responses, but the best responses it generates cover more gates than is required to best respond tothe poachers policy. For example, if the current poachers policy p is attack(1), then the simplest bestresponse to it is defend(1), which is the response an algorithm searching in the space of programs will mostlikely find, due to the minimal size of the program. Instead, the LLM often returns a best response such asdefend(1), defend(2), defend(3), which allows quicker progress toward the policy covering all 60 gates. We observe a similar behavior for the CM domain, where LS-LLM quickly generates programs that climbto much higher branches than the programs the baselines generate. Most of the progress that we observefor LS-LLM comes from the best responses the LLM suggests. As programs become longer, it becomes lesslikely that a neighbor program will represent an improvement with respect to the current program. shows the results for MicroRTS, where the winning rate is computed by having the policy a systemgenerated, for a given number of games played, play against the policy each of the other systems generatedafter the maximum number of games used in the experiment (50,000). LS-LLM outperforms all baselinesby a large margin in the three maps. Especially for the larger maps, 2424 and 3232, LS-LLM generatesstronger policies more quickly than the baselines. For example, 2L needs to play approximately 10 timesmore games than LS-LLM on the 3232 map to reach the winning rate around 60%. Although the results on MicroRTS are similar to those in PR and CM, the explanation why LS-LLMperforms better than the baselines is different in MicroRTS. In PR and CM, the LLM is often able togenerate a program that already solves the MDP. In MicroRTS this happens only in the early iterations oflearning, when the self-play policies are weak. Later, as the system generates stronger policies, the LLM failsto directly generate an optimal policy. LS-LLM is more sample-efficient in MicroRTS due to its initializationof the search in the neighborhood of policies that can solve the MDP. We conjecture that the LLM helpsthe search explore different types of policy. For example, the best policies in the 98 map require the use ofRanged units. The LLM quickly generates programs that can train various types of unit, including Rangedunits. The process of discovering such policies with search alone takes more iterations. As representativeexamples, we show in Appendix B two examples of policies the LLM generated, as its attempt to provide abest response to a target policy. The two policies encode different types of strategies for playing MicroRTS.",
  "Ablation Experiments": "shows the results of the second (left) and third (right) experiments. 2L(LS-LLM-Enc) is theversion of LS-LLM where we encrypt the name of the functions of the DSL. 2L(LS-LLM-Enc) performssimilarly to IBR and is worse than FP and 2L. These results suggest that much of the general knowledgethat the LLM leverages to provide initial candidates for search comes from the function names. 2L(LS-LLM-No-FB) is the version of LS-LLM that does not provide feedback to the LLM. 2L(LS-LLM-No-FB) performs comparable to the baselines, suggesting that the feedback encodes valuable information.We conjecture that the feedback allows the LLM to verify whether the program it generates achieves thegoals the LLM sets to it. We make such a conjecture because the LLM often explains why it generated apolicy, as it outputs the policy. For example, it could state that its policy will train Ranged units to protectthe Base. The feedback could show that no Ranged units are being trained, allowing the LLM to modify itspolicy to achieve its goal in its next attempt at generating a programmatic best response.",
  "Related Work": "Programmatic reinforcement learning is an active area of research (Verma et al., 2018b; 2019; Inala et al.,2020; Trivedi et al., 2021; Qiu & Zhu, 2022; Carvalho et al., 2024). All of these previous works employ aform of search in programmatic spaces to solve MDPs. An SMT solver has been used to synthesize policiesfor single-player puzzles (Butler et al., 2017) and a SAT solver has been used for logic games (Farzan &Kincaid, 2018). We differ from all these previous works in that we use an LLM to speed up the search forprogrammatic policies. Others have considered multi-agent settings similar to the ones used in our experiments, but have mainlyaimed at extracting interpretable policies from neural models (Milani et al., 2022; Liu et al., 2023c). Thiscontrasts with our approach of searching for policies in DSL-defined spaces. Still in the multi-agent settingand closer to our work, various approaches have been used to find policies for two-player zero-sum games,from genetic programming (Mario & Toledo, 2022), to Monte Carlo Tree Search (Medeiros et al., 2022) andlocal search (Mario et al., 2021; Moraes et al., 2023). Previous work has focused on searching for generatingpolicies, but has not been enhanced by LLMs. Previous work has used LLMs in games, but not for generating programmatic policies. Much of this workhas focused on online planning in open-world games such as Minecraft or Sims-like social simulations, whereLLMs have been used to perceive, plan, and act (Park et al., 2023), often decomposing long-horizon goals into",
  "Discussion and Conclusions": "The results in PR, CM, and MicroRTS showed that LLMs can be used to enhance stochastic local searchalgorithms in the context of programmatic policies, even if the model is used only sparingly, to seed the searchin the programmatic space. The LLM model we used in our experiments was not trained with data about PRand CM, since these games were published after the cutoff date to collect training data to train the model.Although the model had some knowledge of MicroRTS, it did not have knowledge of the Microlanguage andthe programmatic policies one can write with it. However, the general knowledge encoded in the modelallowed it to attempt reasonable programs that likely forced the search to better explore the policy space,thus improving the sample efficiency of the overall system. Programmatic representations of policies can offer advantages over alternative representations, such as in-terpretability. The drawback of programmatic representations is that one needs to solve hard combinatorialsearch problems to generate programs encoding policies. Such search problems are particularly challengingbecause the optimization landscape is full of discontinuities and thus is not suitable for gradient descentoptimization. In this paper, we presented LS-LLM, a system that uses an LLM to guide the search forprogrammatic policies. The LLM is used only to initialize the search in the programmatic space, makingLS-LLM a viable method given the often high monetary costs of using LLMs. We hypothesized that theuse of an LLM would speed up the process of computing policies in terms of the number of samples requiredto learn such policies. We tested our hypothesis in three games that are challenging for current systems,including a challenging real-time strategy game. Our results supported our hypothesis, as the programmatic",
  "policies LS-LLM generated were stronger than those generated by current state-of-the-art systems for thesame number of played games": "This research was supported by Canadas NSERC and the CIFAR AI Chairs program, and was enabled inpart by support provided by the Digital Research Alliance of Canada. This research has also received fundingfrom the project ALIGN4Energy (NWA.1389.20.251) of the research programme NWA ORC 2020 which is(partly) financed by the Dutch Research Council (NWO), and from the European Unions Horizon EuropeResearch and Innovation Programme, under Grant Agreement number 101120406. The paper reflects onlythe authors view and the EC is not responsible for any use that may be made of the information it contains.",
  "GW Brown. Iterative solution of games by fictitious play. Activity Analysis of Production and Allocation,pp. 374376, 1951": "Eric Butler, Emina Torlak, and Zoran Popovi. Synthesizing interpretable strategies for solving puzzle games.In Proceedings of the 12th International Conference on the Foundations of Digital Games, pp. 110, 2017. Tales Henrique Carvalho, Kenneth Tjhia, and Levi H. S. Lelis. Reclaiming the source of programmaticpolicies: Programmatic versus latent spaces. In The Twelfth International Conference on Learning Rep-resentations, 2024.",
  "Joel Lehman, Jonathan Gordon, Shawn Jain, Kamal Ndousse, Cathy Yeh, and Kenneth O. Stanley. Evolutionthrough large models. CoRR, abs/2206.08896, 2022. doi: 10.48550/ARXIV.2206.08896. URL": "Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence, and AndyZeng. Code as policies: Language model programs for embodied control. In IEEE International Conferenceon Robotics and Automation, ICRA 2023, London, UK, May 29 - June 2, 2023, pp. 94939500. IEEE, 2023.doi: 10.1109/ICRA48891.2023.10160591. URL Fei Liu, Xi Lin, Zhenkun Wang, Shunyu Yao, Xialiang Tong, Mingxuan Yuan, and Qingfu Zhang. Largelanguage model for multi-objective evolutionary optimization. CoRR, abs/2310.12541, 2023a. doi: 10.48550/ARXIV.2310.12541. URL",
  "Julian R. H. Mario and Claudio Toledo. Evolving interpretable strategies for zero-sum games. Applied SoftComputing, pp. 108860, 2022. ISSN 1568-4946. doi: URL": "Julian R. H. Mario, Rubens O. Moraes, Tassiana C. Oliveira, Claudio Toledo, and Levi H. S. Lelis. Pro-grammatic strategies for real-time strategy games. In Proceedings of the AAAI Conference on ArtificialIntelligence, volume 35, pp. 381389, 2021. H Brendan McMahan, Geoffrey J Gordon, and Avrim Blum. Planning in the presence of cost functionscontrolled by an adversary. In Proceedings of the 20th International Conference on Machine Learning(ICML-03), pp. 536543, 2003.",
  "MicroRTS AI Competition.2023 microrts ai competition results. 2023. Accessed: Au-gust 31, 2024": "Stephanie Milani, Zhicheng Zhang, Nicholay Topin, Zheyuan Ryan Shi, Charles A. Kamhoua, Evangelos E.Papalexakis, and Fei Fang. MAVIPER: learning decision tree policies for interpretable multi-agent rein-forcement learning. In Machine Learning and Knowledge Discovery in Databases - European Conference,volume 13716 of Lecture Notes in Computer Science, pp. 251266. Springer, 2022. Rubens O. Moraes and Levi H. S. Lelis. Searching for programmatic policies in semantic spaces. In Proceed-ings of the International Joint Conference on Artificial Intelligence, pp. 59905998. International JointConferences on Artificial Intelligence Organization, 2024. Rubens O. Moraes, David S. Aleixo, Lucas N. Ferreira, and Levi H. S. Lelis. Choosing well your opponents:How to guide the synthesis of programmatic strategies. In Proceedings of the International Joint Conferenceon Artificial Intelligence, pp. 48474854, 2023. Muhammad Umair Nasir, Sam Earle, Julian Togelius, Steven James, and Christopher W. Cleghorn. Ll-matic: Neural architecture search via large language models and quality-diversity optimization. CoRR,abs/2306.01102, 2023.doi: 10.48550/ARXIV.2306.01102.URL",
  "Augustus Odena and Charles Sutton. Learning to represent programs with property signatures. In Interna-tional Conference on Learning Representations. OpenReview.net, 2020": "Augustus Odena, Kensen Shi, David Bieber, Rishabh Singh, Charles Sutton, and Hanjun Dai. BUSTLE:bottom-up program synthesis through learning-guided exploration. In International Conference on Learn-ing Representations, 2021. S. Ontan. The combinatorial multi-armed bandit problem and its application to real-time strategy games.In Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment,pp. 5864. AAAI, 2013.",
  "Spyros Orfanos and Levi H. S. Lelis. Synthesizing programmatic policies with actor-critic algorithms andrelu networks, 2023. URL": "Joon Sung Park, Joseph C. OBrien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and Michael S.Bernstein. Generative agents: Interactive simulacra of human behavior. In Proceedings of the AnnualACM Symposium on User Interface Software and Technology, pp. 2:12:22. ACM, 2023. Wenjie Qiu and He Zhu. Programmatic reinforcement learning without oracles. In The Tenth InternationalConference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net,2022. URL",
  "Abhinav Verma, Hoang Le, Yisong Yue, and Swarat Chaudhuri. Imitation-projected programmatic rein-forcement learning. Advances in Neural Information Processing Systems, 32, 2019": "Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, andAnima Anandkumar.Voyager: An open-ended embodied agent with large language models.CoRR,abs/2305.16291, 2023a.doi: 10.48550/ARXIV.2305.16291.URL Zihao Wang, Shaofei Cai, Anji Liu, Xiaojian Ma, and Yitao Liang. Describe, explain, plan and select: Inter-active planning with large language models enables open-world multi-task agents. CoRR, abs/2302.01560,2023b. doi: 10.48550/ARXIV.2302.01560. URL",
  "Best Response Attempt 1Best Response Attempt 2": "1for ( Unit u)2i f ( OpponentHasNumberOfUnits ( Ranged , 5 ) )3u . t r a i n ( Heavy ,Up, 1 0 )4i f ( OpponentHasNumberOfUnits ( Light , 1 0 ) )5u . t r a i n ( Ranged ,Up, 5 )6u . t r a i n ( Heavy ,Up, 5 )7i f ( OpponentHasNumberOfUnits ( Worker , 5 ) )8u . attack ( Weakest )9u . t r a i n ( Light ,Up, 2 0 )10u . build ( Barracks ,Up, 1 )11for ( Unit u)12u . attack ( Closest )",
  "BExamples of LLM-Generated Programs": "We conjectured in .1 that LS-LLM is effective because the LLM allows it to quickly explore differenttypes of strategies. shows two examples of policies that the LLM generated as attempts to provide abest response to the same target policy provided by the 2L self-play algorithm for the BWDistantResourcesmap. The two policies encode different strategies for playing MicroRTS. The policy on the left decides onthe types of unit the player trains based on the units the opponent controls. For example, it will train Heavyunits if the opponent controls Ranged units (lines 2 and 3). The policy on the right focuses on trainingRanged units (line 3). Later in the game, after the player controls 10 Ranged units, if resources are stillavailable, it will train Light units (line 5).These two policies do not include instructions for collectingresources and training Worker units. Therefore, while they might not be effective in practice, they offerdifferent template functions that the local search algorithm can further improve into effective policies."
}