{
  "Abstract": "In decentralized multi-agent reinforcement learning, agents learning in isolation can lead torelative over-generalization (RO), where optimal joint actions are undervalued in favor ofsuboptimal ones. This hinders effective coordination in cooperative tasks, as agents tendto choose actions that are individually rational but collectively suboptimal. To addressthis issue, we introduce MaxMax Q-Learning (MMQ), which employs an iterative processof sampling and evaluating potential next states, selecting those with maximal Q-valuesfor learning. This approach refines approximations of ideal state transitions, aligning moreclosely with the optimal joint policy of collaborating agents. We provide theoretical analysissupporting MMQs potential and present empirical evaluations across various environmentssusceptible to RO. Our results demonstrate that MMQ frequently outperforms existingbaselines, exhibiting enhanced convergence and sample efficiency.",
  "Introduction": "Cooperative multi-agent reinforcement learning (MARL) has become increasingly important for address-ing complex real-world challenges that require coordinated behaviors among multiple agents. Successfulapplications included playing card games (Brown & Sandholm, 2018), autonomous driving (Shalev-Shwartzet al., 2016; Zhou et al., 2021), unmanned aerial vehicles (Wang et al., 2020), wireless sensor networks (Xuet al., 2020; Sahraoui et al., 2021) and traffic light control (Bazzan, 2009; Zhou et al., 2023). A dominantframework in MARL is centralized training with decentralized execution (CTDE) (Lowe et al., 2017; Rashidet al., 2018; Son et al., 2019), which often relies on a centralized coordinator to aggregate information, suchas local observation or individual actions from other agents during training. While this approach has beenwidely adopted due to its effectiveness in leveraging global information, it may still face scalability challenges,particularly in environments with large number of agents or complex interactions. Additionally, in scenarioswhere privacy is a critical concern, CTDE methods that require access to individual agent data could beless desirable. While many CTDE methods do not require exhaustive local information from all agents,the potential for scalability and privacy issues in certain implementations warrants consideration. Whileinter-agent communication can partially mitigate some of these challenges (Foerster et al., 2016; Zhu et al.,2022), it introduces additional overhead, which can be prohibitive in environments where communication iscostly or unreliable.",
  "Published in Transactions on Machine Learning Research (11/2024)": "Yue Wu, Shuangfei Zhai, Nitish Srivastava, Joshua Susskind, Jian Zhang, Ruslan Salakhutdinov, and HanlinGoh. Uncertainty weighted actor-critic for offline reinforcement learning. arXiv preprint arXiv:2105.08140,2021. Jing Xu, Fangwei Zhong, and Yizhou Wang. Learning multi-agent coordination for enhancing target coveragein directional sensor networks. Advances in Neural Information Processing Systems, 33:1005310064, 2020. Chao Yu, Akash Velu, Eugene Vinitsky, Jiaxuan Gao, Yu Wang, Alexandre Bayen, and Yi Wu. The surprisingeffectiveness of ppo in cooperative multi-agent games. Advances in Neural Information Processing Systems,35:2461124624, 2022. M. Zhou, X. Ma, and Y. Li. A novel multi-objective routing scheme based on cooperative multi-agentreinforcement learning for metaverse services in fixed 6g. In WOCN, 2023. URL DOI: 10.1109/WOCC52294.2023.00029. Ming Zhou, Jun Luo, Julian Villella, Yaodong Yang, David Rusu, Jiayu Miao, Weinan Zhang, MontgomeryAlban, Iman Fadakar, Zheng Chen, Chongxi Huang, Ying Wen, Kimia Hassanzadeh, Daniel Graves,Zhengbang Zhu, Yihan Ni, Nhat Nguyen, Mohamed Elsayed, Haitham Ammar, Alexander Cowen-Rivers,Sanjeevan Ahilan, Zheng Tian, Daniel Palenicek, Kasra Rezaee, Peyman Yadmellat, Kun Shao, dongchen, Baokuan Zhang, Hongbo Zhang, Jianye Hao, Wulong Liu, and Jun Wang. Smarts: An open-sourcescalable multi-agent rl training school for autonomous driving. In Jens Kober, Fabio Ramos, and ClaireTomlin (eds.), Proceedings of the 2020 Conference on Robot Learning, volume 155 of Proceedings of MachineLearning Research, pp. 264285. PMLR, 1618 Nov 2021.",
  "Related work": "Centralised learning methods. Within the centralized training paradigm, RO has been discussed mostlyfor value factorization methods like QMIX (Rashid et al., 2018). The monotonic factorization in QMIX cannotrepresent the dependency of one agents value on others policies, making it prone to RO. Proposed solutionsinclude weighting schemes during learning (Rashid et al., 2020), curriculum transfer from simpler tasks(Gupta et al., 2021; Shi & Peng, 2022), and sequential execution policy (Liu et al., 2024). Soft Q-learningextensions to multi-agent actor-critics (Wei et al., 2018; Lowe et al., 2017) utilise energy policies for globalsearch to mitigate RO. However, unlike our decentralized approach, these methods require a centralized criticwith joint action access during training. Fully decentralized learning. Decentralized approaches in MARL aim to overcome the scalability andprivacy issues associated with centralized methods. However, they face unique challenges, particularlyin addressing non-stationarity and RO. Existing approaches can be categorized based on their strategiesfor tackling these issues. Basic independent learning methods like Independent Q-learning (IQL) (Tan,1993) and independent PPO (Yu et al., 2022) form the foundation of decentralized MARL. However, theirsimultaneous updates can lead to non-stationarity, potentially compromising convergence. Recent work bySu & Lu (2023) on DPO addresses this by providing monotonic improvement and convergence guarantees.To mitigate negative impacts of uncoordinated learning, several methods promote optimism toward otheragents behaviors. Distributed Q-learning (Lauer & Riedmiller, 2000) selectively updates Q-values basedonly on positive TD errors. Hysteretic Q-learning (Matignon et al., 2007) uses asymmetric learning rates,while Lenient Q-learning (Wei & Luke, 2016) selectively ignores negative TD errors. These techniques aim toovercome convergence to suboptimal joint actions by dampening unhelpful Q-value changes. Taking a differentapproach, the recently introduced Ideal Independent Q-learning (I2Q) (Jiang & Lu, 2022) explicitly modelsideal cooperative transitions. However, it requires learning an additional utility function over state pairs.Our proposed method, MMQ, builds upon these approaches by encoding uncertainty about decentralizedMARL dynamics. We model other agents as sources of heteroscedastic uncertainty with an epistemic flavor,providing a more flexible way to represent optimistic policies. By sampling from possible next states, MMQavoids the need for heuristic corrections or separate Q-functions, offering a novel solution to the challenges ofdecentralized MARL. Uncertainty quantification. Quantifying different sources of uncertainty is crucial in reinforcementlearning, particularly in multi-agent settings. Prior work distinguishes between aleatoric uncertainty, arisingfrom environment stochasticity, and epistemic uncertainty, due to insufficient experiences (Osband et al.,2016; Depeweg et al., 2016). Various methods, including variance networks (Kendall & Gal, 2017; Wu et al.,2021) and ensembles (Lakshminarayanan et al., 2017), have been proposed to model these uncertainties,with applications in single-agent RL (Chua et al., 2018; Sekar et al., 2020). MARL introduces additionalcomplexity due to the dynamic nature of agent interactions, leading to non-stationarity. This non-stationaritylimits an agents ability to reduce epistemic uncertainty through repeated state visits (Hernandez-Leal et al.,2017) and can be viewed as another form of epistemic uncertainty. Our proposed MMQ algorithm addressesthese challenges by using quantile networks to effectively manage two key sources of epistemic uncertainty inmulti-agent settings: limited experiential data and evolving strategies of other agents. This approach allowsMMQ to better handle the unique uncertainties present in decentralized MARL environments.",
  "Multi-agent Markov Decision Process": "Consider a multi-agent Markov Decision Process (MDP) represented by M = (S, A, R, Penv, ). Within thistuple, S denotes the state space, A is the joint action space, Penv(s|s, a) and R(s, s) are respectively theenvironment dynamics and reward function for states s, s S and action a A, and is the discount factor.Given N agents, the action space is of the form A = A1 AN with any action a A taking the form",
  "C-600": "a = (a1, . . . , aN). At each time step t, an agent indexed by i {1, . . . , N} selects an individual action, ai.When the N actions are executed, the environment transitions from state s to state s, and every agentreceives a global reward, rt. The objective is to maximize the expected return, i.e., E[Tt=0 trt], where T isthe time horizon. The individual environment dynamics is defined as",
  "Relative over-generalization through an example": "Consider a two-agent game with the reward structure shown in . In this game, there are three possibleactions: A, B, and C. Agents would receive a joint reward of +3 if they take A together. However, if onlyone agent takes A, that agent incurs a penalty of 6. Agents end up choosing less optimal actions (B orC) if they perceive the reward for choosing A to be lower, based on their expectations of the other agentsactions. For agent 1, the utility function Q1() is related to the probability that the other agent choosesA, i.e. 2(A). In this case, Q1(A) would be smaller than Q1(B) or Q1(C) if 2(A) < 2",
  "Independent Q-Learning": "In independent Q-learning (Tan, 1993), each agent i learns a policy independently, treating other agents aspart of the environment. The individual Q-function is Qi(s, ai) = E [t=0 trt | s0 = s, ai,0 = ai] for agenti. Each agent updates its Q-function by minimizing the loss EPi(s|s,ai)(yi Qi(s, ai))2, where yi is thetarget value defined as R(s, s) + maxai Qi(s, ai). The RO problem arises in this setup as each agent seeksto maximize its own expected return based on experiences where other agents policies evolve and containrandom explorations.",
  "Ideal transition probabilities": "To address the RO problem in this context, some approaches introduce implicit coordination mechanismscentered on the concept of an ideal transition model (Lauer & Riedmiller, 2000; Matignon et al., 2007;Wei & Luke, 2016; Palmer et al., 2018; Jiang & Lu, 2022). These methods guide each agents learningwith hypothetical transitions that assume optimal joint behavior, aligning independent learners towardscoordination. Let i denote the joint policy of other agents, and Q the optimal joint Q-function. Theoptimal joint policy of other agents can be expressed as i(s, ai) = arg maxai Q(s, ai, ai). The concept of ideal transitions refers to hypothetical state transitions that assume other agents are followingoptimal joint policies. These ideal transition probabilities represent the dynamics that would occur if allagents achieved perfect coordination, and are defined as P i (s|s, ai) = Penv(s|s, ai, i(s, ai)). Based on",
  "where Qi (s, ai) is the optimal Q-function for agent i": "An important theoretical result from Jiang & Lu (2022) establishes that when all agents perform Q-learning based on these ideal transition probabilities, the individual and joint optimality align, that is,maxai Qi (s, ai) = Q(s, (s)), where Q(s, a) is the optimal joint Q-function for any action a A, and is the optimal joint policy. However, achieving true ideal transitions is intractable in practice due to theevolving, uncontrolled nature of learning agents. This motivates developing techniques to approximate idealtransitions.",
  ",(2)": "where fenv(s, ai, ai) is the deterministic transition function that maps the current state s and the jointactions of all agents (ai, ai) to the next state s, Aj is the action space for each agent j and the Cartesianproduct j=i Aj represents all possible combinations of actions by the other agents. It is noted that we onlyuse fenv to define the set Ss,ai here, but do not need to learn this global transition function directly in ouralgorithm. Encoding the deterministic transitions by a delta function, fenv(s,a)(s), Eq. (1) is rewritten as",
  "where Ss,ai is any subset of Ss,ai including s = fenv(s, ai, i(s, ai))": "By reformulating the Q-value optimization over the set Ss,ai, our approach allows for targeting the optimalvalue Q(s, ai) under the true coordinated joint behavior, without directly approximating s. When thereis no information about s, the set Ss,ai could be set in principle to Ss,ai, if it were known, to ensure theinclusion of s. However, this will also make the maximisation over Ss,ai in Eq. (3b) more computationallychallenging, implying a trade off between reducing the size of Ss,ai and ensuring the inclusion of s. Wepropose a learning procedure that enables each agent to progressively shrink their set of next states Ss,ai, asall agents explore and accumulate experiences. In practice, at the algorithmic step t, we work with a subset Ss,ai,t which approximates one of the possiblesubsets Ss,ai. Since neither Ss,ai nor s are known in practice due to the incomplete information about otheragents policies and the environment dynamics, we cannot guarantee that s Ss,ai,t Ss,ai holds, but wewill show in our performance assessment that s Ss,ai,t holds with high probability. Furthermore, directmaximization over Ss,ai,t is challenging as this set is infinite in general. To address this, we resort to Monte Carlo optimization, as in e.g. Robert et al. (1999), by introducing a finiteset SMs,ai,t of M points randomly sampled from Ss,ai,t. Assuming no approximation error in the predicted",
  "With the considered approach, there are two natural phases when running the associated algorithms:": "1. With little information to rely on, the agents explore the state space at random and collect diversetrajectories, which improve their understanding of the range of possible next state Ss,ai. In thisphase, the estimated sets Ss,ai,t will be close to Ss,ai. 2. As agents refine their estimates of the set Ss,ai of possible next states and accumulate sufficientreward information, the maximisation in Eq. (4b) yields increasingly stable values, facilitating policyconvergence. This, in turns, means that the new trajectories will be more similar and optimised,progressively outnumbering the initial diverse trajectories. This will cause the sets Ss,ai,t to zero inon s, hence facilitating the Monte Carlo optimisation in Eq. (4b). An illustration of our sampling and selection process is shown in : given a set of possible next statesamples, our algorithm selects the state with the highest estimated Q-value for updating, which implicitlyindicating the optimal action of other agents. As agents explore more possible actions, the estimated setSs,ai,t increasingly approximates the true set Ss,ai. Crucially, if the optimal next state is contained withinthe estimated set, the equality",
  "R(s, s) + maxaiQi (s, ai)": "holds. This property is fundamental to our method, as it implies that through iterative learning and effectivesampling, each agent can learn Q-values that closely align with those derived from ideal transition probabilities.In the following section, we analyse the convergence properties of this approach under ideal conditions. Thecomplete algorithm, including implementation details, will be presented in .3.",
  "The proof can be found in Appendix B": "This result demonstrates that the Monte Carlo optimization error d(s, s) diminishes as the number ofsamples M increases. Incidentally, the Monte Carlo optimisation error could exhibit exponential dependenceas the dimensionality of the state space increases. However, in multi-agent scenarios, partially-observed MDPstypically limit effective dimensionality growth, as agents rely on a restricted view of the overall state space. Seemingly, there is an inherent trade-off involved in expanding the state set Ss,ai, i.e., expanding u in theabove one-dimension case, to cover more possibilities while managing the resultant error. A broader Ss,aireduces the gap between s and the true best state s, as it increases the likelihood of encompassing s.This action effectively shrinks the error term d(s, s). Yet, increasing the size of Ss,ai, i.e., increasing u, alsotypically increases variability, leading to a larger error d(s, s) and necessitating more samples to maintaina given level of precision.",
  "end for": "To capture the range of possible next states, our implementation utilises two non-parametrised quantilemodels, gli and gui , which employ neural networks to predict the l = 0.05 and u = 0.95 quantiles for eachdimension of the next state. The neural network parameters, denoted by li and ui , are learnt by minimisingthe quantile loss according to their respective values over samples from individual replay buffer Di:",
  "L(i) = Es,aiDi[L(gi (s, ai; i) s)],(5)": "where L(u) = I(u > 0)u + I(u < 0)(1 )u. For each (s, ai) pair, the two quantile models predict bounds[gli (s, ai), gui (s, ai)]. We then construct the potential next state set S by including the true s and M samplesdrawn from the quantile bounds. We also explored a parametrised multivariate Gaussian model as anothermethod to estimate the possible next states, detailed in Appendix C.3.",
  "Ri(s, s; i) + maxaiQi(s, ai; i),(6)": "where Ri(s, s; i) is an estimate from a learned reward model parameterised by i. Here we apply astop-gradient operator to the target value to prevent gradient flow back to the next state estimation process.This operation is crucial for maintaining stability in the learning dynamics by separating the optimization ofthe Q-network from the updates to the next state estimation. The loss function for optimising the Q-networkparameter i is then given by",
  "L(i) = Es,aiDi [Qi(s, ai; i) Yi(s, ai; i)]2 .(7)": "This loss aims to align the Q-networks predictions with the maximum expected return, considering boththe immediate reward and the discounted future Q-values of the potential next states sampled from theestimated quantile bound. Additionally, the learned reward function Ri(s, s; i), parameterised by i, istrained to approximate the rewards for state transitions. The loss for this reward model is the mean squarederror between the predicted and actual rewards,",
  "L(i) = EsDi[Qi(s, i(s; i); i)].(9)": "The training process is summarised in Algorithm 1,and the full source code is available at The full algorithm interleaves the optimisation of vari-ous constituent models, allowing agents to adaptively learn and improve their policies based on their ownexperiences and the evolving environmental dynamics. Our implementation incorporates two key strategies.First, a delayed update approach for the actor network relative to the critic network, where the critic isupdated 10 times more frequently to maintain stability (Fujimoto et al., 2018). Second, negative rewardshifting (Sun et al., 2022), which enhances our double-max-style updates (see also Appendix C.1).",
  "We evaluated the MMQ algorithm in three types of cooperative MARL environments characterized by theneed for complex coordination among agents; see for an overview": "Differential GamesWe adapted this environment from Jiang & Lu (2022), where N agents move withinthe range . At each time step, an agent indexed by i selects an action ai . The state of thisagent then transitions to clip{xi + 0.1 ai, 1, 1}, where xi is the previous state and the clip(y, ymin, ymax)function restricts y within [ymin, ymax]. The global state is the position vector (x1, x2). The reward function,detailed in Appendix A, assigns rewards following each action. A narrow optimal reward region is centred,surrounded by a wide zero-reward area and suboptimal rewards at the edges (see DG in ). This setupcan lead to RO problems as agents might prefer staying in larger suboptimal areas. Multiple Particle EnvironmentWe designed six variants of cooperative navigation tasks with ROrewards as shown in . The common goal is for two disk-shaped agents, D1 and D2, to simultaneouslyreach a disk-shaped target. To encourage coordination, we introduce a penalty for scenarios where only oneagent is within a certain distance from the target. Specifically, we define a disk D centered on the target with",
  "(c) MPE scenarios": ": Task visualization. (a) Differential Game(DG): agents need to cross a wide zero-reward area tomove to the center to gain the optimal reward. (b) Half-Cheetah 2x3: the Half-Cheetah 2x3 scenario inMAmujoco domain; (c) MPE scenarios; Cooperative navigation(CN): two agents need to enter the grey areaof the target together to gain the reward, the solo entry would induce a penalty. CN + More penalty: Sametask as CN but with more penalty for solo entry; CN + HT: Agents could choose to approach one of the twoTargets with different reward settings; CN+HA: same task as CN but two agents have different sizes andvelocity; Predator-Prey(PP): two agents need to enter the grey area of a pre-trained prey. Sequential Task:two agents need to first go through the grey area of Target B and then enter the grey of Target A with thesame RO reward design as CN. radius rD and penalize agents if only one is within D. This setup is designed to illustrate the RO problem,where agents might prefer staying outside D rather than risk being the only one inside it. The task difficultyincreases as the radius rD decreases. The reward function, designed to reflect the RO problem, is defined as:",
  "Rout potherwise": "The rewards for the three cases should satisfy Rout p < Rout < Rin. Detailed descriptions of Rout andRin for different settings are provided in Appendix A. In task CN, the penalty for solo entry into the circleis p = 0.2; in task CN+More Penalty, the penalty increases to p = 0.5 for entering the circle alone; intask CN+Heterogeneous Agents (HA), two agents performing the CN task are heterogeneous, havingdifferent sizes and velocities; in task CN+Heterogeneous Targets (HT), there are two targets, whereentering the circle of target A follows the previous RO design, and entering the circle of target B incurs noRO penalty but offers a reward lower than Rin; in the sub-optimal scenario, agents might only enter thecircle of target B; in task Sequential Task, agents must coordinate over a longer periodthey could eitherdirectly reach target A with the same RO reward as before or first reach target B to pick up cargo, thenreceive a bonus each step (a higher Rin) when they later enter target A together; in task Predator-Prey(PP), two predators (which we control) and one prey, who interact in an environment with two obstacles.The prey, trained using MADDPG (Lowe et al., 2017), is adept at escaping faster than the predators. Thepredators need to enter the preys disk together to receive the reward Rin. Multi-agent MuJoCo EnvironmentWe employ the Half-Cheetah 2x3 scenario from the Multi-agentMuJoCo framework (de Witt et al., 2020). This environment features two agents, each controlling threejoints of the Half-Cheetah robot via torque application. It presents a partial observability setting, with eachagent accessing only its local observations. We implement an RO reward structure designed to necessitate",
  ": Performance comparison for two-agents setting in DG, MPE scenarios and Half-Cheetah": "high coordination between agents. The reward function rv is defined as: 7 if v < vl, 9 if vl v vu, and2, otherwise; where vl = 0.035/dt, vu = 0.04/dt, and dt = 0.05. This structure rewards agents for movingforward at speeds exceeding vu, penalizes speeds between vl and vu, and provides a moderate penalty forvery low speeds. Agents failing to overcome the RO problem may settle for maintaining low speeds to avoidthe harshest penalty. : Settings with N = 2 agents. Mean Returns and 95% Confidence Interval (over eight seeds) for allalgorithms at the end of training. Values are bolded if their confidence intervals overlap with the maximumvalue.",
  ": (a) Ablation study for different sample number M in three tasks; (b) Percentage of each dim oftrue next states fall within the predicted quantile bound for three tasks": "entiate between two versions of I2Q: the original implementation by Jiang & Lu (2022), which we refer toas I2Q*, involves multiple updates of all network components after every 50 interaction steps. In contrast,our implementation, denoted as I2Q, updates only the critic network multiple times every 50 steps. Thisdistinction allows us to more accurately assess the performance impact of these differing update strategies.",
  "Experiment Results": "To compare the performance among different baselines, we consider the mean episode return over 8 seeds,where the episode return is defined as the accumulated reward Tt=0 rt for the whole episode, with T beingthe episode length. Differential GamesOur evaluations, depicted in and , show that MMQ outperforms otheralgorithms with 15 samples drawn from the quantile bounds predicted by two quantile models. HyDDPGand I2Q also perform well. Interestingly, the performance of HyDDPG and IDDPG surpasses that reportedfor I2Q in Jiang & Lu (2022), possibly due to our implementations emphasis on updating the critic networkmore frequently than the actor network to stabilize training. However, I2Q learns much slower compared toI2Q, which updates all modules rather than just the critic multiple times. These update strategies havedifferent effects on various algorithms. With more agents, the performance of other algorithms degradesslightly, showing higher variability, as shown in . MMQ consistently identifies the optimal region inall test cases, demonstrating its higher sample efficiency and scalability. We also tested MMQ in a stochasticversion of this game (Appendix D.1), confirming that MMQ still outperforms the baselines under stochasticstate transitions and reward dynamics. Multiple Particle EnvironmentAs shown in and , our algorithm, using 15 samplesfrom the predicted quantile bounds, successfully overcomes the Relative Over-generalisation (RO) problemin all settings. HyDDPG performs particularly well, especially in the lower penalty scenarios, such as CN,CN+HT, and CN+HA. In CN and CN+HT, I2Q initially demonstrated some ability to solve thetask but its performance deteriorated over time. We observed that the learned Q-values of some seedsincreased rapidly and incorrectly simultaneously, despite setting the weight of the Qss(s, s) value to a minimalparameter during the update process. This might be due to the challenges in computing effective Qss(s, s)values in I2Q, leading to less accurate predictions of optimal states and resulting in cumulative estimationerrors over time. With a higher solo-entry penalty in CN+More Penalty, all baselines performancesignificantly declines, remaining stuck in suboptimal areas except for HyDDPG, which could learn to alimited extent. Our sampling-based approach, however, demonstrates robustness even with the increased ROproblem.",
  "nD 100%": "The percentage is already high initially for the DG and CN environments but is a bit lower for Half-Cheetah,possibly due to its more complex dynamics compared to the other two. These results indicate that thequantile model can effectively capture most state changes as expected, suggesting that Ss,ai closely reflectsthe true set Ss,ai, as analyzed in .1.",
  "Broader Impact Statement": "This paper presents work whose goal is to advance the field of decentralized learning of multi-agent systems.There are many potential societal consequences of our work, none of which we feel must be specificallyhighlighted here. David Baldazo, Juan Parras, and Santiago Zazo. Decentralized multi-agent deep reinforcement learning inswarms of drones for flood monitoring. In 2019 27th European Signal Processing Conference (EUSIPCO),pp. 15. IEEE, 2019. Sumeet Batra, Zhehui Huang, Aleksei Petrenko, Tushar Kumar, Artem Molchanov, and Gaurav S Sukhatme.Decentralized control of quadrotor swarms with end-to-end deep reinforcement learning. In Conference onRobot Learning, pp. 576586. PMLR, 2022.",
  "Scott Fujimoto, Herke Hoof, and David Meger. Addressing function approximation error in actor-criticmethods. In International conference on machine learning, pp. 15871596. PMLR, 2018": "Tarun Gupta, Anuj Mahajan, Bei Peng, Wendelin Bhmer, and Shimon Whiteson. Uneven: Universal valueexploration for multi-agent reinforcement learning. In International Conference on Machine Learning, pp.39303941. PMLR, 2021. Pablo Hernandez-Leal, Michael Kaisers, Tim Baarslag, and Enrique Munoz de Cote. A survey of learning inmultiagent environments: Dealing with non-stationarity. In 2017 16th Conference on Autonomous Agentsand MultiAgent Systems (AAMAS), pp. 11641170. IFAAMAS, 2017.",
  "Alex Kendall and Yarin Gal. What uncertainties do we need in bayesian deep learning for computer vision?Advances in neural information processing systems, 30, 2017": "Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive uncertaintyestimation using deep ensembles. Advances in neural information processing systems, 30, 2017. Martin Lauer and Martin A Riedmiller. An algorithm for distributed reinforcement learning in cooperativemulti-agent systems. In Proceedings of the Seventeenth International Conference on Machine Learning, pp.535542, San Francisco, CA, USA, 2000. Morgan Kaufmann Publishers Inc. Shanqi Liu, Dong Xing, Pengjie Gu, Xinrun Wang, Bo An, and Yong Liu. Solving homogeneous andheterogeneous cooperative tasks with greedy sequential execution. In The Twelfth International Conferenceon Learning Representations, 2024. URL Ryan Lowe, Yi I Wu, Aviv Tamar, Jean Harb, OpenAI Pieter Abbeel, and Igor Mordatch. Multi-agentactor-critic for mixed cooperative-competitive environments. Advances in neural information processingsystems, 30, 2017. Latitia Matignon, Guillaume J Laurent, and Nadine Le Fort-Piat. Hysteretic q-learning: an algorithm fordecentralized reinforcement learning in cooperative multi-agent teams. In 2007 IEEE/RSJ InternationalConference on Intelligent Robots and Systems, pp. 6469. IEEE, 2007. Laetitia Matignon, Guillaume J Laurent, and Nadine Le Fort-Piat. Independent reinforcement learners incooperative markov games: a survey regarding coordination problems. The Knowledge Engineering Review,27(1):131, 2012.",
  "Liviu Panait, Sean Luke, and R Paul Wiegand. Biasing coevolutionary search for optimal multiagent behaviors.IEEE Transactions on Evolutionary Computation, 10(6):629645, 2006": "Tabish Rashid, Mikayel Samvelyan, Christian Schroeder, Gregory Farquhar, Jakob Foerster, and ShimonWhiteson. QMIX: Monotonic value function factorisation for deep multi-agent reinforcement learning. InJennifer Dy and Andreas Krause (eds.), Proceedings of the 35th International Conference on MachineLearning, volume 80 of Proceedings of Machine Learning Research, pp. 42954304. PMLR, 1015 Jul 2018.URL Tabish Rashid, Gregory Farquhar, Bei Peng, and Shimon Whiteson. Weighted qmix: Expanding monotonicvalue function factorisation for deep multi-agent reinforcement learning. Advances in neural informationprocessing systems, 33:1019910210, 2020.",
  "Christian P Robert, George Casella, and George Casella. Monte Carlo statistical methods, volume 2. Springer,1999": "A. Sahraoui, M. Boulmalf, and A. Tahri. Schedule-based cooperative multi-agent reinforcement learning formulti-channel communication in wireless sensor networks. Wireless Personal Communications, 120(1):429447, 2021. URL DOI: 10.1007/s11277-021-08362-6. Ramanan Sekar, Oleh Rybkin, Kostas Daniilidis, Pieter Abbeel, Danijar Hafner, and Deepak Pathak.Planning to explore via self-supervised world models. In International Conference on Machine Learning,pp. 85838592. PMLR, 2020."
}