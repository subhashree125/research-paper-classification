{
  "Abstract": "We investigate a specific security risk in FL: a group of malicious clients has impacted themodel during training by disguising their identities and acting as benign clients but laterswitching to an adversarial role. They use their data, which was part of the training set, totrain a substitute model and conduct transferable adversarial attacks against the federatedmodel. This type of attack is subtle and hard to detect because these clients initially appearto be benign. The key question we address is: How robust is the FL system to such covert attacks, especiallycompared to traditional centralized learning systems? We empirically show that the proposedattack imposes a high security risk to current FL systems. By using only 3% of the clientsdata, we achieve the highest attack rate of over 80%. To further offer a full understandingof the challenges the FL system faces in transferable attacks, we provide a comprehensiveanalysis over the transfer robustness of FL across a spectrum of configurations. Surprisingly,FL systems show a higher level of robustness than their centralized counterparts, especiallywhen both systems are equally good at handling regular, non-malicious data. We attribute this increased robustness to two main factors: 1) Decentralized Data Training:Each client trains the model on its own data, reducing the overall impact of any singlemalicious client. 2) Model Update Averaging: The updates from each client are averagedtogether, further diluting any malicious alterations. Both practical experiments and theoreti-cal analysis support our conclusions. This research not only sheds light on the resilience ofFL systems against hidden attacks but also raises important considerations for their futureapplication and development.",
  "Introduction": "Although Federated Learning (FL) provides a promising solution to collaboratively train models withoutexchanging data, especially in privacy-concerned areas Li et al. (2022), it is still susceptible to attacks such asdata poisoning Huang et al. (2011), model poisoning Bhagoji et al. (2019); Bagdasaryan et al. (2020); Huanget al. (2023), free-riders attack Lin et al. (2019), and reconstruction attacks Geiping et al. (2020); Zhu et al.(2019). It is also vulnerable to adversarial attacks during inference Biggio et al. (2013); Szegedy et al. (2013),including adversarial examples designed to deceive the model Zizzo et al. (2020). Research on robust FLmethods against adversarial examples has primarily focused on a white-box setting where attackers have fullmodel access Zhou et al. (2020); Reisizadeh et al. (2020a); Hong et al. (2021); Qiao et al. (2024). However,real-world FL applications, like Gboard Hard et al. (2018), usually restrict such access.",
  "Published in Transactions on Machine Learning Research (11/2024)": "Xiang Li, Kaixuan Huang, Wenhao Yang, Shusen Wang, and Zhihua Zhang.On the convergence offedavg on non-iid data. In 8th International Conference on Learning Representations, ICLR 2020, AddisAbaba, Ethiopia, April 26-30, 2020. OpenReview.net, 2020b. URL Yijiang Li, Wentian Cai, Ying Gao, Chengming Li, and Xiping Hu. More than encoder: Introducing transformerdecoder to upsample. In 2022 IEEE International Conference on Bioinformatics and Biomedicine (BIBM),pp. 15971602. IEEE, 2022.",
  "Adversarial Robustness": "The adversarial robustness of a model is usually defined as the models ability to predict consistently in thepresence of small changes in the input. Intuitively, if the changes to the image are so tiny that they areimperceptible to humans, these perturbations will not alter the prediction. Formally, given a well trainedclassifier f and image-label pairs (x, y) on which the model correctly classifies f(x) = y, f is defined to be-robust with respect to distance metric d(,) if",
  "maxl(f(x + ; ), y) s.t. d(x + , x) < (3)": "where l(, ) denotes the loss function (e.g., cross-entropy loss) for training the model f parameterized by .While these attack methods are powerful, they usually require some degrees of knowledge about the targetmodel f (e.g., the gradient). Arguably, for many real-world settings, such knowledge is not available, andwe can only expect less availability of such knowledge on FL applications trained and deployed by serviceproviders. On the other hand, the hostile attacker having access to some but limited amount of users data isa much more realistic scenario. Thus, we propose the following assumption for practical attack in FL: giventhe data of n malicious users Dm = ni=1 Di where Di = {(xk, yk)|k = 1, , mi}(i) contains mi data points,we aim to acquire a transferable perturbation by maximizing the same objective as in Equation 3 but witha surrogate model f parameterized by trained by Dm:",
  "The Security and Robustness of Federated Learning": "Poisoning and Backdoor Attack.Poisoning attacks Biggio et al. (2012); Fang et al. (2020) aim todisrupt the global model by injecting malicious data or manipulating local model parameters on compromiseddevices. In contrast, backdoor attacks Bagdasaryan et al. (2020); Sun et al. (2019); Wang et al. (2023)infuse a malicious task into the existing model without impacting its primary task accuracy Zhang et al.(2023); Huang et al. (2024), including model replacement Chen et al. (2017), label-flipping Fung et al. (2018),fixed-trigger backdoor Dai & Li (2023); Liu et al. (2024) and trigger-optimization Huang (2020); Li et al.(2023a); Nguyen et al. (2024). Defenses against these attacks often involve anomaly detection methods, suchas Byzantine-tolerant aggregation Shejwalkar & Houmansadr (2021) (e.g., Krum, MultiKrum Blanchardet al. (2017), Bulyan Guerraoui et al. (2018), Trimmed-mean and Median Yin et al. (2018)), focusing on thegeometric distance between hostile and benign gradients. More advanced defenses take detection, aggregation,detection, and differential privacy into consideration Huang et al. (2023); Nguyen et al. (2022). The robustnessand attack performance of backdoor attacks is significantly influenced by FL data heterogeneity Zawad et al.(2021). Transfer Attack.Transfer-based adversarial attacks employ the full training set of the target model to traina surrogate model Zhou et al. (2018), a challenging condition to meet in practice, especially in FL where dataprivacy is paramount. Another line of inquiry delves into the mechanisms of black-box attacks, which exploitthe high transferability of adversarial examples even between different model architectures Szegedy et al.(2013); Goodfellow et al. (2014). This transferability is partially attributed to the similarity between sourceand target models Goodfellow et al. (2014); Liu et al. (2016); Li et al. (2023b), as adversarial perturbationsalign closely with a models weight vectors, and different models learn similar decision boundaries for the sametask. Tramr et al. (2017) found that adversarial examples span a large, contiguous subspace, facilitatingtransferability. Meanwhile, Ilyas et al. (2019) posits that adversarial perturbations are non-robust featurescaptured by the model, and Waseda et al. (2023) utilizes this theory to explain differing mistakes in transferattacks. Additionally, Demontis et al. (2019); Zhang et al. (2024) reveals that similar gradients in source andtarget models and lower variance in loss landscapes increase transfer attack probability. Despite transferattacks being more realistic and practical, not much attention has been focused on this aspect of the safety androbustness of FL. We take the initiative to investigate this area and underscore our settings distinctivenessand importance compared to others. Key Difference 1:Different from query-based or transfer-based black-box attack, we assume the maliciousclients possess the data themselves, impacting the target model during training and attack during inference.",
  "Investigation Setup and Research Goals": "GOAL 1: We aim to investigate the possibility of a transfer attack with limited data and validate whether itis possible and practical for the attacker to lay benign during the training process and leverage the obtaineddata to perform the adversarial attack. GOAL 2: We aim to explore how different degrees of decentralization, the heterogeneity of data and theaggregation, i.e. average affect the transferability of the adversarial examples against the FL model in apractical configuration.",
  "Experiment Setup": "Threat Model. Following (Zizzo et al., 2020), we use PGD (Madry et al., 2017) with 10 iterations, norandom restart, and an epsilon of 8 / 255 over L norm on CIFAR10. For experiments on ImageNet200, weuse PGD (Madry et al., 2017) of the same setup but with an epsilon of 2 / 255. Settings. We first build up the basic FL setting. We split the datatset into 100 partitions of equal size inan iid fashion. We adopt two models for the experiments: CNN from (McMahan et al., 2017) since it iscommonly used in the FL literature and the widely used ResNet50 (He et al., 2016) which represents a morerealistic evaluation. We conduct training in three paradigms: the centralized model, the federated modeland the source model with a limited number of clients data. For the federated model, we use SGD withoutmomentum, weight decay of 1e-3, learning rate of 0.1, and local batch size of 50 following (Acar et al., 2021).We follow the cross-device setting and use a subset of clients in each round of training. We use a 10% asdefault where not specified. We train locally 5 epochs on ResNet50 and 1 epoch on CNN. For centralized andsource model training, we leverage SGD with a momentum of 0.9, weight decay of 1e-3, a learning rate of0.01 and batch size of 64. For adversarial training, we use the same setting as centralized and leverage PGDto perform the adversarial training. We refer to (Zizzo et al., 2020) for the details of adversarial training. Allexperiments are conducted with one RTX 2080Ti GPU. Metrics. We report Accuracy (Acc) and adversarial accuracy (Adv.Acc) for the performance and therobustness of white-box attack, and for adversarial transferability, we report transfer accuracy (T.Acc) andtransfer success rate (T.Rate) as detailed in 3.2.",
  "Adversarial Transferability in Federated Learning": "To define the transferability of adversarial examples, we first introduce the definition of the source model,target model and adversarial example. The source model is the surrogate model used to generate adversarialexamples while the target model is the target aimed to attack. Given the validation set x = {(xi, yi)}, sourcemodel f , target model f and adversarial perturbation function adv(, ) (e.g., PGD), we first define thefollowing sets:",
  "s4 = {xi|f(adv(xi, f )) = yi}": "Adversarial examples are defined as those samples that are originally correctly classified by model f but aremisclassified when the adversarial perturbation is added, i.e., s1 s2. Adversarial transferability against thetarget model refers to the ability of adversarial examples generated from the source model to attack the targetmodel (become an adversarial example of the target model). We define transfer rate (T.Rate) and transferaccuracy (T.Acc) to measure the adversarial transferability: T.Rate = ||s1s2s3s4||",
  "||s1s2s3|| , T.Acc = 1 ||s4||": "||x|| where|| || denotes the cardinality of a set. We are the first one to propose and use the Transfer Rate metric tomeasure the transferability of adversarial examples which measures the transferability of the surrogate modelby measuring the portion of transferable examples. This serves as a complimentary to accuracy as plainaccuracy fails to accurately measure robustness.",
  "federatedR5055.0313.4260.5915.68R50 (adv)50.1838.3154.9241.13": "In order to provide a preliminary understanding about therobustness of the FL model, we train the centralized model for200 epochs and the federated model for 400 rounds resultingin a decent accuracy of over 90% (see the regular column ofTab. 1). For the CNN model, we train 200 epochs for thecentralized model and 600 rounds for the federated model toachieve an accuracy of over 75%. We can observe that the federated models clean and ad-versarial accuracy is lower than its centralized counterpart,aligned as the result in (Zizzo et al., 2020). However, weconjecture that such an increase in adversarial accuracy isnot attributed to the intrinsic robustness of the centralizedmodel but largely due to its high clean accuracy. To validatethis hypothesis and facilitate a fair comparison between thetwo paradigms, we early-stop both models when their cleanaccuracy reaches 90% (75% for CNN) and report the resultsin the same-acc column of Tab. 1. We early stop at 80%for adversarial training (72% for CNN). We can see thatwhen both models reach a comparable clean accuracy, the FLmodel shows greater robustness against white-box attackscompared with the centralized model. To further validate our hypothesis, we perform the experimenton a much larger and more realistic dataset, i.e. ImageNet200. We early stopped both models at 55% accuracy.Results can be seen in the bottom two rows of Tab. 1. We can see that FL models demonstrate superiorrobustness against white-box attacks compared with the centralized model on both same-acc and regularsettings.",
  "R50federated2.29 / 95.606.52 / 86.74centralized 22.72 / 54.008.27 / 83.13": "We turn to the black-box attack which is more practical and realisticin real-world applications.We explore the examples generatedby two different training paradigms and their transferability todifferent models. Since the similarity of decision boundary andclean accuracy influences and reflects the transferability betweenmodels (Goodfellow et al., 2014; Liu et al., 2016; Demontis et al.,2019), we early stop both federated and centralized models. ForCIFAR10, we early stop both models at 90% of accuracy (75% forCNN). For ImageNet200, we follow .1 and early stop at55%. We follow this training setting for the rest of this paper. Tab. 2 shows that the adversarial examples generated by the fed-erated model are highly transferable to both the federated andcentralized model while adversarial examples generated by the cen-tralized model exhibit less transferability. The T.Rate of federated-to-centralized attack is even larger than centralized-to-centralizedattack. Secondly, T.Rate of adversarial examples between modelstrained under same paradigms is larger than models trained underdifferent paradigms, which can be attributed to the difference ofthe two training paradigms, e.g., the discrepancy in the decisionboundary (Goodfellow et al., 2014; Liu et al., 2016) or different sub-space (Tramr et al., 2017).",
  "Transfer Attack with limited data": "In this section, we comprehensively evaluate the practicality and plausibility of our proposed attack setting,i.e. transfer attack with limited data. We present the overview of our attack setup in Algorithm 1. Tosimulate this scenario, we fix the generated partition used in the federated training and randomly select aspecified number of users as malicious clients whose data is available for performing the attack. To performthe transfer attack, we train a surrogate model in a centralized manner with the collected data. Trainingdetails are specified in .1. Algorithm 1: Our Attack as Benign SettingInput: A set of N clients {C1, C2, . . . , CN} where M clients {C1, C2, . . . , CM} are corrupted by attacker,datasets Xi for each client Ci, sampling rate K, total communication round T, local iteration number Eand surrogate model training iteration number Ts.",
  "i=1, ,M Xi)end AdvPerturb(f , x);": "One of the key differences between the proposed setting and the conventional transfer-based attack is theamount of available data to train the substitute (source) model, which is a key factor for a successful attacksince one would reason that more data will lead to a higher success rate. This is measured by the number of",
  "Observation 1. T.Rate increases as the number of users increases, which is consistently observed in bothcentralized and federated models": "Observation 2. With only 20% of clients the source model achieves an T.Rate of 90% and 50% withResNet50 and CNN respectively. We notice that with ResNet50, the T.Rate of 20% clients is even largerthan a transfer attack with full training data (71.94% T.Rate). With CNN, the source model trained with20% clients can achieve 50% T.Rate which only lags behind the transfer attack by 6% (56.59%). From observation 2, we can see that the proposed attack can achieve comparable or even better T.Rate orlower T.Acc. Consequently, we can conclude that the proposed attack setting with limited datais likely to cause significant security breaches in the current and future FL applications. Tofurther explain observation 1 and an intriguing phenomenon that the T.Rate of ResNet50 model rises to thepeak and then decreases, we provide the following hypothesis: When the number of clients used to train thesource model is small, the clean accuracy of the source model is also low, leading to a large discrepancy in thedecision boundary. Increasing the number of users used in the source model minimizes such discrepancy untilthe amount of data is sufficient to train a source model with similar accuracy. At this point, the differencebetween the federated and centralized (Caldarola et al., 2022) becomes the dominant factor affecting thetransferability since the source model is trained in the centralized paradigm.",
  "transfer rate": "1235710 20 30 50 70 90 100 25%~75% (different partition) 25%~75% (same partition) Range within 1.5IQR Median Line Mean Outliers : Difference between same partition and different partition; Left: transfer accuracy v.s. number ofusers data leveraged in source model training; Right: transfer rate v.s. number of users data leveraged insource model training distinct random seeds and then perform the source model training and the target model training on thetwo different partitions. Then transfer attack is performed with the source model against the target modelfollowing the above configuration. We repeat the experiment 4 times with different random seeds and reportthe averaged results in . We observe no significant difference between the same partition and thedifferent partition settings. To further validate this observation, we perform a Hypothesis Test on the obtainedresults with the Paired Sample T-Test and achieve a p-value of .393 meaning that there is no significantdifference. This further demonstrates the possibility of attacking a federated learning system through ourproposed attacks and illustrates a higher security risk.",
  "(c)": ": (a) Comparison with query-based black-box attack. (b) Attack with the surrogate model trained withthe same distribution (no participation in the FL training) (c) Attack with the surrogate model trained with similarknowledge, i.e. CIFAR100. Practical evaluation. To practically evaluate our attack setting, we consider two cases: 1. the maliciousclients can only participate in some periods during the FL training and 2. the attacker has no knowledgeof the model deployed (i.e. unknown architecture). We simulate these situations by attacking in differenttraining stages and with different architectures, as shown in (d) of . Is Data Distribution or Similar Knowledge Sufficient? We emphasize the significance of the processthat malicious clients stay benign during training and affect the training with their own data. We demonstratein this section that this obtained training data is crucial to a successful attack. We first show that using a similar distribution is not as good as the actual training data to train a surrogatemodel, as shown in (b) of . We note that the inherent heterogeneity of FL training will further imposechallenges. The distinct distribution of each clients data makes it nearly impossible to approximate an overalltraining distribution due to the scale and variability of the data.",
  "Two intrinsic properties contributing to transfer robustness": "To fully understand how adversarial examples transfer between centralized and federated models, we studytwo intrinsic properties of FL and its relation with transfer robustness. To protect the privacy of clients andleverage the massive data from user-end, FL utilizes distributed data to train a global model through localupdates and aggregation at the server (McMahan et al., 2017). As a consequence, the heterogeneity of thedistributed data and the aggregation operation is the core component of an FL method. In this section, westudy how these two properties affect the transfer robustness of the FL model. : T.Rate vs. data of different heterogeneity and dispersion degree. (a): top 3 are results of ResNet50; (b):bottom 3 are results of CNN; Left: T.Rate as a function of the number of users in federated training; Middle: T.Rateas a function of dirichlet alpha; Right:T.Rate as a function of unbalanced sgm.",
  "This section aims to explore the relationship between the degree of heterogeneity and transfer robustness": "Control the degree of dispersion and heterogeneity. To explore the impact of distributed data onadversarial transferability, we control decentralization and heterogeneity through four indexes. By varyingthe number of clients in the partition, we alter the degree of dispersion of distributed data. We provide twoapproaches to control the heterogeneity of the distributed data. 1. Change the number of maximum classesper client (McMahan et al., 2017). 2. Change the alpha value of Dirichlet distributions () (Wang et al., 2020;Yurochkin et al., 2019) (smaller means a more non-iid partition) and the log-normal variance (sgm) of theLog-Normal distribution (larger variance denotes more unbalanced partitions) used in unbalanced partition(Zeng et al., 2021). We leverage the FedLab framework to generate the different partitions (Zeng et al., 2021).For simplicity, we leverage the centralized trained model as the source model for the rest of the experiments.",
  "(b)CNN on CIFAR10": ": Transfer rate v.s. maximum number of classesper client; (a): Results of ResNet50 on CIFAR10 dataset;(b): Results of CNN on CIFAR10 dataset; It is notewor-thy to observe that the transfer rate in the 10-user settingpersistently surpasses that in the 100-user setting. This fur-ther substantiates the proposition that more decentralizedtraining leads to lower adversarial transferability for thefederated model. Data heterogeneity affects transfer attack. Asdiscussed in Sec. 5.1, we provide two approaches forcontrolling the heterogeneity of the data. First, wealter the alpha values of the Dirichlet distributionsto generate heterogeneous data of different degrees.As per the middle plot of (a) and (b) in , wecan see that T.Rate increases as increases. We also alter the maximum number of classes perclient to generate heterogeneous data of different de-grees. Results are reported in . We can observefrom a clear increase trend in both the 10-user partition and the 100-user partition setting withResNet50 and CNN. As the degree of heterogeneitydecreases, the transferability increases. Experimentsin both settings can illustrate our findings. Thisproves that our observations hold to wider circum-stances and scenarios. We further explore whether unbalanced data (i.e. different clients possess different numbers of samples) leadsto a decrease in transferability in (right of (a) and (b)). We can observe that a larger variance leads toa lower transfer rate, meaning that unbalanced data also contribute to higher robustness. To validate the above observation, we provide statistical testing for the correlation coefficient in Appendix C.With the Spearman correlation coefficient, we report a significant negative correlation on ResNet50 betweenlog-normal variance and T.Rate under a significance level of 0.1 (p-value=0.05). We report a significantcorrelation on all results with a level of 0.05 except the CNN experiments with different Dirichlet andunbalanced sgm. Visualization of linear fitting is in Appendix H. To provide a more practical evaluation,we follow the setting in practical evaluation of Sec. 4.3 and evaluate the above findings in different trainingstages and with different architectures. As per (c) and (d) of , we can observe a similar correlation andtrends between Dirichlet alpha and transfer rate. This further illustrates that our findings hold in varioussettings and configurations.",
  "Averaging Leads to Robustness": "We explore the other core property, averaging operation, of FL and its correlation with transfer robustness.To change the degree of averaging in FL, we alter the number of clients selected to average at each round. Tocomprehensively illustrate this finding, we use two source models to attack. The first one is a centralizedmodel trained with all the data. The Second surrogate model is trained with 30% of all the clients data,simulating the attack in Sec. 4.3. We plot the relation of T.Rate and #averaged users in (a) to (d) of .Both CNN and ResNet50 exhibit a decreasing trend as more users participate in the averaging operation.This demonstrates that the averaging operation contributes to the robustness of the FL model and moreclients to average per round leads to higher transfer robustness. We provide statistical testing to validate",
  "the correlation in Appendix C. With the Spearman correlation coefficient, we report a significant negativecorrelation in all four experiments (all p-values are less than .001)": "To provide a more practical evaluation, we follow the setting in practical evaluation of Sec. 4.3 and evaluatethe above findings in different training stages and with different architectures. As shown in (e), (f) of ,we can observe a similar trend as mentioned above. This demonstrates that this phenomena hold to morepractical settings and wider configurations. We further illustrate that this observation generalizes to differentaggregation methods in Appendix B.",
  "source traind with fed centralized federated": ": Additional experiments on SVHN, CIFAR100 and ImageNet200. (a) Our attack with ResNet18 on SVHN(first row), ResNet50 on CIFAR100 (second row) and ResNet50 on ImageNet200 (third row). (b) How decentralizationrelates to the transfer robustness of FL model on SVHN (first row), CIFAR100 (second row) and ImageNet200 (thirdrow) datasets. (c) How heterogeneity relates to the transfer robustness of FL model on SVHN (first row), CIFAR100(second row) and ImageNet200 (third row) datasets. (d) How averaging operation relates to the transfer robustness ofFL model on SVHN dataset (first row), CIFAR100 (second row) and ImageNet200 (third row) datasets.",
  "We summarize the above investigations as the below take-home messages and provide implications forunderstanding adversarial transferability in FL and secure FL applications:": "The heterogeneous data and a large degree of decentralization both result in lower transferability ofadversarial examples from the surrogate model. The attacker can benefit from closing the discrepancybetween the surrogate model and the target model (e.g., train the surrogate model in a federated manner). With more clients to average at each round, the federated model becomes increasingly robust to black-boxattacks. Defenders can benefit from increasing the number of clients selected at each round to average.",
  "ResNet50-ResNet50 ResNet50->VGG16 VGG16->ResNet50": ": Transfer rate v.s. different number of clients selected to average in each round; (a) ResNet50 results withsource model trained in centralized manner with full data; (b) ResNet50 results with source model trained with 30users; (c) CNN results with source model trained in centralized manner with full data; (d) CNN results with sourcemodel trained trained with 30 users;6Supporting Theoretical Evidence Notation.We use (X, Y ) to denote a dataset, where X Rnp and Y Rn. We use (x, y) to denote asample following . We consider the problem of federated learning optimization model: min{l() Nk=1 pklk()},wherelk() =1nknkj=1 l(f(x; ), y), N is the total number devices and nk is the numberof samples possessed by device k. pk is the weight of device k with the constraint that pk = 1. FollowingLi et al. (2020b). we assume the algorithm performs a total of T stochastic gradient descent (SGD) iterationsand each round of local training possesses E iterations.TE is thus the total communication times. At eachcommunication, a maximal number of K devices will participate in the process. We quantify the degree ofnon-iid using = l Nk=1 pklk where l and lk is the minimum value of l() and lk() respectively. Weuse the relative increase of adversarial loss Demontis et al. (2018) to measure the transferability for easierderivation, which is defined as the loss of the target model of an adversarial example, simplified through alinear approximation of the loss function: T = l(f(x + ; ), y) l(f(x; ), y) + T xl(f(x; ), y), where issome perturbation generated by the surrogate model, which corresponds to the maximization of an innerproduct over an -sized ball under the above linear approximation: arg maxp<l(f (x + ; ), y),maxp< T xl(f (x; ), y) = xl(f (x; ), y)q",
  "Remark 6.4 aligns with our empirical findings in .1, which will provide more insights to futureresearch on federated adversarial robustness": "Theorem 6.5. Let T denote the train set T = (X, Y ) sampled from some data distribution and x denotethe adversarial example following some adversarial distribution. Let f(, T ) be the model trained with datasetT and f() = ET [f(; T )] is the expectation of the model trained on dataset T . We denote the an average ofn models as fn = 1",
  "attack with limited data setting. Our evaluation shows that limited data can yield a comparable transfer rateto a full-dataset attack": "To fully understand how adversarial examples transfer between centralized and federated models, we furtherstudy two intrinsic properties of FL and its relation with transfer robustness. We discover that decentralizedtraining, heterogeneous data, and averaging operations enhance transfer robustness and reduce the transfer-ability of adversarial examples. We provide evidence from both the perspective of empirical experiments andtheoretical analysis.",
  "Our findings have implications for understanding the robustness of federated learning systems and poses apractical question for federated learning applications": "Durmus Alp Emre Acar, Yue Zhao, Ramon Matas Navarro, Matthew Mattina, Paul N Whatmough, andVenkatesh Saligrama. Federated learning based on dynamic regularization. arXiv preprint arXiv:2111.04263,2021. Eugene Bagdasaryan, Andreas Veit, Yiqing Hua, Deborah Estrin, and Vitaly Shmatikov. How to backdoorfederated learning. In International Conference on Artificial Intelligence and Statistics, pp. 29382948.PMLR, 2020. Arjun Nitin Bhagoji, Supriyo Chakraborty, Prateek Mittal, and Seraphin Calo. Analyzing federated learningthrough an adversarial lens. In International Conference on Machine Learning, pp. 634643. PMLR, 2019.",
  "Battista Biggio, Blaine Nelson, and Pavel Laskov. Poisoning attacks against support vector machines. arXivpreprint arXiv:1206.6389, 2012": "Battista Biggio, Igino Corona, Davide Maiorca, Blaine Nelson, Nedim rndi, Pavel Laskov, Giorgio Giacinto,and Fabio Roli. Evasion attacks against machine learning at test time. In Joint European conference onmachine learning and knowledge discovery in databases, pp. 387402. Springer, 2013. Peva Blanchard, El Mahdi El Mhamdi, Rachid Guerraoui, and Julien Stainer. Machine learning withadversaries: Byzantine tolerant gradient descent. Advances in Neural Information Processing Systems, 30,2017.",
  "Yanbo Dai and Songze Li. Chameleon: Adapting to peer images for planting durable backdoors in federatedlearning. In International Conference on Machine Learning, pp. 67126725. PMLR, 2023": "Ambra Demontis, Marco Melis, Maura Pintor, Matthew Jagielski, B. Biggio, Alina Oprea, C. Nita-Rotaru,and F. Roli. Why do adversarial attacks transfer? explaining transferability of evasion and poisoningattacks. Usenix Security Symposium, 2018. Ambra Demontis, Marco Melis, Maura Pintor, Matthew Jagielski, Battista Biggio, Alina Oprea, CristinaNita-Rotaru, and Fabio Roli. Why do adversarial attacks transfer? explaining transferability of evasionand poisoning attacks. In 28th USENIX security symposium (USENIX security 19), pp. 321338, 2019.",
  "Anbu Huang. Dynamic backdoor attacks against federated learning. arXiv preprint arXiv:2011.07429, 2020": "Ling Huang, Anthony D Joseph, Blaine Nelson, Benjamin IP Rubinstein, and J Doug Tygar. Adversarialmachine learning. In Proceedings of the 4th ACM workshop on Security and artificial intelligence, pp.4358, 2011. Siquan Huang, Yijiang Li, Chong Chen, Leyu Shi, and Ying Gao. Multi-metrics adaptively identifies backdoorsin federated learning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp.46524662, 2023. Wenke Huang, Mang Ye, Zekun Shi, Guancheng Wan, He Li, Bo Du, and Qiang Yang. Federated learning forgeneralization, robustness, fairness: A survey and benchmark. IEEE Transactions on Pattern Analysis andMachine Intelligence, 2024. Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Logan Engstrom, Brandon Tran, and Aleksander Madry.Adversarial examples are not bugs, they are features. Advances in neural information processing systems,32, 2019.",
  "Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towardsdeep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083, 2017": "Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. Communication-efficient learning of deep networks from decentralized data. In Artificial intelligence and statistics, pp.12731282. PMLR, 2017. Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. Deepfool: a simple and accuratemethod to fool deep neural networks. In Proceedings of the IEEE conference on computer vision andpattern recognition, pp. 25742582, 2016. Thien Duc Nguyen, Phillip Rieger, Roberta De Viti, Huili Chen, Bjrn B Brandenburg, Hossein Yalame, HelenMllering, Hossein Fereidooni, Samuel Marchal, Markus Miettinen, et al. {FLAME}: Taming backdoors infederated learning. In 31st USENIX Security Symposium (USENIX Security 22), pp. 14151432, 2022. Thuy Dung Nguyen, Tuan A Nguyen, Anh Tran, Khoa D Doan, and Kok-Seng Wong. Iba: Towards irreversiblebackdoor attacks in federated learning. Advances in Neural Information Processing Systems, 36, 2024. Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z Berkay Celik, and Ananthram Swami.Practical black-box attacks against machine learning. In Proceedings of the 2017 ACM on Asia conferenceon computer and communications security, pp. 506519, 2017.",
  "Yu Qiao, Apurba Adhikary, Chaoning Zhang, and Choong Seon Hong. Towards robust federated learning vialogits calibration on non-iid data. arXiv preprint arXiv:2403.02803, 2024": "Amirhossein Reisizadeh, Farzan Farnia, Ramtin Pedarsani, and Ali Jadbabaie. Robust federated learning:The case of affine distribution shifts. Advances in Neural Information Processing Systems, 33:2155421565,2020a. Amirhossein Reisizadeh, Aryan Mokhtari, Hamed Hassani, Ali Jadbabaie, and Ramtin Pedarsani. Fed-paq: A communication-efficient federated learning method with periodic averaging and quantization. InInternational conference on artificial intelligence and statistics, pp. 20212031. PMLR, 2020b.",
  "Hongyi Wang, Mikhail Yurochkin, Yuekai Sun, Dimitris Papailiopoulos, and Yasaman Khazaeni. Federatedlearning with matched averaging. arXiv preprint arXiv:2002.06440, 2020": "Xinjiang Wang, Xingyi Yang, Shilong Zhang, Yijiang Li, Litong Feng, Shijie Fang, Chengqi Lyu, Kai Chen,and Wayne Zhang. Consistent-teacher: Towards reducing inconsistent pseudo-targets in semi-supervisedobject detection. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,pp. 32403249, 2023. Futa Waseda, Sosuke Nishikawa, Trung-Nghia Le, Huy H Nguyen, and Isao Echizen. Closer look at thetransferability of adversarial examples: How they fool different models differently. In Proceedings of theIEEE/CVF Winter Conference on Applications of Computer Vision, pp. 13601368, 2023. Dongxian Wu, Yisen Wang, Shu-Tao Xia, James Bailey, and Xingjun Ma. Skip connections matter: On thetransferability of adversarial examples generated with resnets. arXiv preprint arXiv:2002.05990, 2020a. Weibin Wu, Yuxin Su, Xixian Chen, Shenglin Zhao, Irwin King, Michael R Lyu, and Yu-Wing Tai. Boostingthe transferability of adversarial samples via attention. In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition, pp. 11611170, 2020b. Dong Yin, Yudong Chen, Ramchandran Kannan, and Peter Bartlett. Byzantine-robust distributed learning:Towards optimal statistical rates. In International Conference on Machine Learning, pp. 56505659. PMLR,2018. Mikhail Yurochkin, Mayank Agarwal, Soumya Ghosh, Kristjan Greenewald, Nghia Hoang, and YasamanKhazaeni. Bayesian nonparametric federated learning of neural networks. In International Conference onMachine Learning, pp. 72527261. PMLR, 2019. Syed Zawad, Ahsan Ali, Pin-Yu Chen, Ali Anwar, Yi Zhou, Nathalie Baracaldo, Yuan Tian, and Feng Yan.Curse or redemption? how data heterogeneity affects the robustness of federated learning. In Proceedingsof the AAAI Conference on Artificial Intelligence, volume 35, pp. 1080710814, 2021.",
  "Dun Zeng, Siqi Liang, Xiangjing Hu, and Zenglin Xu. Fedlab: A flexible federated learning framework. arXivpreprint arXiv:2107.11621, 2021": "Yechao Zhang, Shengshan Hu, Leo Yu Zhang, Junyu Shi, Minghui Li, Xiaogeng Liu, and Hai Jin. Why doeslittle robustness help? a further step towards understanding adversarial transferability. In Proceedings ofthe 45th IEEE Symposium on Security and Privacy (S&P24), volume 2, 2024. Zaixi Zhang, Qi Liu, Zhicai Wang, Zepu Lu, and Qingyong Hu.Backdoor defense via deconfoundedrepresentation learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition, pp. 1222812238, 2023. Wen Zhou, Xin Hou, Yongjun Chen, Mengyun Tang, Xiangqi Huang, Xiang Gan, and Yong Yang. Transferableadversarial perturbations. In Proceedings of the European Conference on Computer Vision (ECCV), pp.452467, 2018.",
  "AAttack with Same or Different Partition": "We show in .3 that training surrogate models in a federated manner can lead to even highertransferability of adversarial examples. However, in the experiment in .3, we partition the collecteddata from malicious clients as the federated model and train the source model in a federated manner. In thissection, we further evaluate whether this partition information is crucial to the higher transferability. That is,whether different partitions used by the source model affect the transferability of its adversarial examplesagainst the target model. To simulate this setting, we first randomly generate two different partitions with 1235710 20 30 50 70 90 100 number of users",
  "BAveraging Leads to Adversarial Robustness with Different Aggregation Methods": "We show in .2 that the averaging operation contributes to the robustness of the FL model and moreclients to average per round leads to higher transfer robustness. We further validate that this observationgeneralizes to different aggregation methods, i.e. Krum Blanchard et al. (2017), Geometric Mean Yin et al.(2018) and Trimmed Mean Yin et al. (2018). As per , we can see that with all three aggregationmethods, there are decreasing trends as the number of averaged users per round increases.",
  "CStatistical Hypothesis Testing on Spearman correlation coefficient": "In .1, we elucidate the correlation between the degree of decentralization and the heterogeneityof training data with the transferability of adversarial examples. The findings denote that, with moredecentralized, heterogeneous data, the federated model is more robust to transfer attack. Furthermore,.2 portrays a discernible inverse relationship between the number of clients and the average transfersuccess rate, as depicted through box plots, which illustrate the averaging operation leads to better robustnessagainst adversarial examples. To statistically validate these correlations, we perform two-tailed Hypothesis Testing on Spearman correlationcoefficient. To conduct Hypothesis Testing for Spearman correlation coefficient on a specified correlation, wefirst calculate the Spearman correlation coefficient on the two sets of points (e.g., T.Rate and Dirichlet ):",
  "We choose the significance level to be 0.1, which means we reject H0 is p-value is smaller than 0.1. We reportthe p-value and Spearman correlation coefficient in": "We can see, as reported in .1 and 5.2, all experiments except the CNN experiments on dirichlet and unbalance sgm can deomenstrates significant correlation under a significance level of 0.1. More to thepoint, we report numerous correlations with p-value less than .001 (significant under level of .001). ThisHypothesis Testing validated the findings of our investigation.",
  "1(7)": "With Lemma D.1, can be directly measured by the cosine similarity of the parameters and . Furthermore,as we need to offer a discussion regarding multiple aspects of the model, such as the loss, the parameters, andthe data, we follow the previous convention Li et al. (2020b) to focus on a narrower scope the model family:",
  "kSt kt . kt denotes the parameters of device k at iteration t": "We use Theorem 2 from Li et al. (2020b) as our lemma to prove our Theorem 6.1.Lemma D.2. With assumption 4 to 8 and L, , k and G be defined therein. Let L denote the minimumloss obtained by optimal estimation from the centralized model and l() denotes the loss of the federatedmodel. Then",
  "EProof of Theorem 6.5": "Proof. We adopt the Bias-Variance decomposition Geman et al. (1992) to provide theoretical justification forthe observation in .2. Let T denote the train set T = (X, Y ) sampled from some data distributionand x denote the adversarial example following some adversarial distribution. First, we give the Bias-Variancedecomposition as follows:",
  "GComparison between transfer attack and attack using traces of training": "In this section, we explore using the traces of training (checkpoints during the training of the federated model)to attack and compare with the transfer attack presented in the paper. We must emphasize that the formeris a different threat model with the capability of keeping traces of the model at different stages of training.That is to say, the attack is a purely white-box one. We present the attack result using checkpoints of different stages of the training as shown in Tab 4. We earlystopped the model when the accuracy reaches 90% which is around 75 epochs. Then we use the checkpointsfrom 20, 40 and 60 epochs to perform the attack."
}