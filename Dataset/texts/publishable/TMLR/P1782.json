{
  "Abstract": "We describe a fast computation method for leave-one-out cross-validation (LOOCV) fork-nearest neighbours (k-NN) regression. We show that, under a tie-breaking condition fornearest neighbours, the LOOCV estimate of the mean square error for k-NN regression isidentical to the mean square error of (k + 1)-NN regression evaluated on the training data,multiplied by the scaling factor (k + 1)2/k2. Therefore, to compute the LOOCV score, oneonly needs to fit (k + 1)-NN regression only once, and does not need to repeat training-validation of k-NN regression for the number of training data.Numerical experimentsconfirm the validity of the fast computation method.",
  "Introduction": "k-Nearest Neighbours (k-NN) regression (Stone, 1977) is a classic nonparametric regression method thatoften performs surprisingly well in practice despite its simplicity (Chen et al., 2018) and thus has beenactively studied both theoretically and methdologically (e.g, Gyrfi et al., 2002; Kpotufe, 2011; Jiang, 2019;Azadkia, 2019; Madrid Padilla et al., 2020; Kpotufe and Martinet, 2021; Lalande and Doya, 2023; Ignatiadiset al., 2023; Matabuena et al., 2024). It has a wide range of applications such as missing-value imputation(Troyanskaya et al., 2001), conditional independence testing (Runge, 2018), outlier detection (Breunig et al.,2000), approximate Bayesian computation (Biau et al., 2015), and function-valued regression (Lian, 2011),to just name a few. For any test input, k-NN regression obtains its k-nearest training inputs and averages the corresponding ktraining outputs to predict the test output. Thus, the number k of nearest neighbours (and the distancefunction on the input space) is a key hyperparameter of k-NN regression and must be selected carefully.Indeed, theoretically, it is known that k should increase as the training data size n increases for k-NNregression to converge to the true function (e.g., Gyrfi et al., 2002, Theorem 6.2). Hence, one should notuse a prespecified value for k (e.g., k = 5) and must select k depending on the training data. A standard way for selecting the hyperparameters of a learning method is cross-validation (e.g., Hastieet al., 2009, .10). However, cross-validation can be costly when the training data size is large. Inparticular, leave-one-out cross-validation (LOOCV) (Stone, 1974) can be computationally intensive since, ifnaively applied, it requires training the learning method on a training dataset of size n 1 and repeating itn times. This issue also applies to the use of LOOCV for k-NN regression. On the other hand, theoretically,LOOCV is known to be better than cross-validation of a smaller number of split-folds as an estimator ofthe generalization error (e.g., Arlot and Celisse, 2010, .2). Azadkia (2019) theoretically analysesLOOCV for k-NN regression in selecting k and discusses its optimality. This paper describes a fast method for computing LOOCV for k-NN regression. Specifically, we show that,under a tie-breaking assumption for nearest neighbours, the LOOCV estimate of the mean-squared error isidentical to the mean square error of (k + 1)-NN regression evaluated on the training data, multiplied by thescaling factor (k + 1)2/k2 (Corollary 1 in ). Therefore, to perform LOOCV, one only needs to fit",
  "Dn := {(x1, y1), . . . , (xn, yn)} X Y,(1)": "where X and Y are the input and output spaces. Specifically, X is a metric space with a distance metricdX : X X [0, ) (e.g., X = RD is a D-dimensional space and dX (x, x) = x x is the Euclideandistance).The output space Y can be discrete (e.g., Y = {0, 1} in the case of binary classification) orcontinuous (e.g., Y = R for the case of real-valued regression, or Y = RM for vector-valued regression withM outputs). Below, we use the following notation for the set of training inputs:",
  "yi = f(xi) + i,i = 1, . . . , n,": "where 1, . . . , n Y are independent zero-mean noise random variables, the task is to estimate the functionf based on the training dataset Dn. k-NN regression is a simple, nonparametric method for this purpose,which often performs surprisingly well in practice and has solid theoretical foundations (e.g, Gyrfi et al.,2002; Kpotufe, 2011; Chen et al., 2018).",
  ".1k-NN Regression": "Let k N be fixed. For an arbitrary test input x X, k-NN regression predicts its output by first searchingfor the k-nearest neighbours of x from Xn and then computing the average of the corresponding k trainingoutputs. To be more precise, define the set of indices for k nearest neighbours as",
  "fk,Dn\\{(x,y)}(x) y2.(4)": "That is, for each = 1, . . . , n, the held-out pair (x, y) is used as validation data for the k-NN regressionfitted on the training data Dn\\{(x, y)} of size n 1.Calculating the LOOCV score for a large n iscomputationally expensive since one needs to fit k-NN regression n times if naively implemented. Next, wewill show how this computation can be done efficiently by fitting (k + 1)-NN regression only once.",
  "NN(x, k, Xn\\{x}) {x} = NN(x, k + 1, Xn).(6)": "See for an illustration of the case k = 3. The reasoning is as follows. The first nearest neighbour ofx in Xn is, of course, x Xn. The second nearest neighbour of x in Xn is the first nearest neighbour ofx in Xn\\{x}. Generally, for 1 m k, the (m + 1)-th nearest neighbour of x in X is the m-th nearestneighbour of x in Xn\\{x}. Therefore, the identity (6) holds.",
  "Published in Transactions on Machine Learning Research (12/2024)": ":LOOCV scores for the Diabetes dataset (left) and the Wine dataset (right), each of which onlyuses one input feature. The used input feature has many duplicates and thus does not satisfy the tie-breakingcondition in Assumption 1. Left: The best k with the lowest LOOCV score is 17 for both LOOCV-Bruteand LOOCV-Efficient. Right: The best k with the lowest LOOCV score is 21 for LOOCV-Brute and 17for LOOCV-Efficient.",
  "kfk+1,Dn(x) 1": "ky, which does not involve the hold-out operation of removing (x, y) fromDn; it just requires fitting the (k + 1)-NN regression fk+1,Dn on Dn, evaluate it on x, scale it by (k + 1)/kand subtract it by y/k. Thus, to compute the LOOCV score (4), one needs to fit the (k + 1)-NN regression only once on the datasetDn. The resulting computationally efficient formula for the LOOCV score is given below.Corollary 1. Under Assumption 1, for the LOOCV score in (4), we have",
  "Discussion on the Tie-breaking Condition": "Lastly, as a caveat, we consider the case where the tie-break condition in Assumption 1 is not satisfied.We use the same Diabetes and Wine datasets as in , but only employ one input feature in eachdataset: BMI for the Diabetes and malic-acid for the Wine. Since each feature has many duplicates,there are many i = j with xi = xj, thus violating the tie-breaking condition. shows the resultscorresponding to the left figures of Figures 2 and 3. LOOCV-Efficient is no longer exact and overestimatesthe LOOCV-Brute when k is small. This phenomenon can be understood by considering the case where k = 1. Suppose there is {1, . . . , n}such that x = xi = xj for some i = j = . In this case, the first nearest neighbour of x in Xn = {x1, . . . , xn}is x, xi and xj, since dX (x, x) = dX (x, xi) = dX (x, xj) = 0. Therefore, depending on the tie-breakingrule of the nearest neighbour search algorithm, xi and xj may be selected as the first k + 1 = 2 nearestneighbours of x in Xn; in this case the identity in (6) does not hold.",
  "Conclusions": "We showed that LOOCV for k-NN can be computed quickly by fitting (k + 1)-NN regression only once,evaluating the mean-square error on the training data and multiplying it by the scaling factor (k + 1)2/k2.By applying this technique, many applications of k-NN regression can be accelerated. It also opens up thepossibility of using LOOCV for optimising not only k but also the distance function dX ; this would be oneimportant future direction.",
  "Madrid Padilla, O. H., Sharpnack, J., Chen, Y., and Witten, D. M. (2020). Adaptive nonparametric regres-sion with the k-nearest neighbour fused Lasso. Biometrika, 107(2):293310": "Matabuena, M., Vidal, J. C., Padilla, O. H. M., and Onnela, J.-P. (2024). kNN algorithm for conditionalmean and variance estimation with automated uncertainty quantification and variable selection. arXivpreprint arXiv:2402.01635. Runge, J. (2018). Conditional independence testing based on a nearest-neighbor estimator of conditionalmutual information. In International Conference on Artificial Intelligence and Statistics, pages 938947.PMLR."
}