{
  "Abstract": "Memory complexity and data scarcity have so far prohibited learning solution operators ofpartial differential equations (PDE) at high resolutions. We address these limitations byintroducing a new data efficient and highly parallelizable operator learning approach withreduced memory requirement and better generalization, called multi-grid tensorized neuraloperator (MG-TFNO). MG-TFNO scales to large resolutions by leveraging local and globalstructures of full-scale, real-world phenomena, through a decomposition of both the inputdomain and the operators parameter space. Our contributions are threefold: i) we enableparallelization over input samples with a novel multi-grid-based domain decomposition, ii)we represent the parameters of the model in a high-order latent subspace of the Fourier do-main, through a global tensor factorization, resulting in an extreme reduction in the numberof parameters and improved generalization, and iii) we propose architectural improvementsto the backbone FNO. Our approach can be used in any operator learning setting. Wedemonstrate superior performance on the turbulent 2D Navier-Stokes equations where weachieve less than half the error with over 150 compression compared to the FNO baseline.The tensorization combined with the domain decomposition, yields over 150 reduction inthe number of parameters and 7 reduction in the domain size without losses in accuracy.",
  "Introduction": "Real-world scientific computing problems often time require repeatedly solving large-scale and high-resolutionpartial differential equations (PDEs). For instance, in weather forecasts, large systems of differential equa-tions are solved to forecast the future state of the weather. Due to internal inherent and aleatoric uncertain-ties, multiple repeated runs are carried out by meteorologists every day to quantify prediction uncertainties.Conventional PDE solvers constitute the mainstream approach used to tackle such computational problems.Such methods provide convergence guarantess and explicit error bounds to the true solution and have longserved as the backbone for tackling many engireening problems. However, these methods are known to beslow and memory-intensive. They require an immense amount of computing power, are unable to learnand adapt based on observed data, and oftentimes require sophisticated tuning (Slingo & Palmer, 2011;Leutbecher & Palmer, 2008; Blanusa et al., 2022).",
  "Published in Transactions on Machine Learning Research (08/2024)": "Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew Stuart,and Anima Anandkumar. Neural operator: Graph kernel network for partial differential equations. arXivpreprint arXiv:2003.03485, 2020a. Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Andrew Stuart, Kaushik Bhattacharya,and Anima Anandkumar. Multipole graph neural operator for parametric partial differential equations.Advances in Neural Information Processing Systems, 33:67556766, 2020b. Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew Stu-art, and Anima Anandkumar.Markov neural operators for learning chaotic systems.arXiv preprintarXiv:2106.06898, 2021a. Zongyi Li, Nikola Borislavov Kovachki, Kamyar Azizzadenesheli, Burigede liu, Kaushik Bhattacharya, An-drew Stuart, and Anima Anandkumar. Fourier neural operator for parametric partial differential equations.In International Conference on Learning Representations, 2021b. Zongyi Li, Hongkai Zheng, Nikola Kovachki, David Jin, Haoxuan Chen, Burigede Liu, Kamyar Azizzade-nesheli, and Anima Anandkumar. Physics-informed neural operator for learning partial differential equa-tions. arXiv preprint arXiv:2111.03794, 2021c. Burigede Liu, Nikola Kovachki, Zongyi Li, Kamyar Azizzadenesheli, Anima Anandkumar, Andrew M Stuart,and Kaushik Bhattacharya. A learning-based multiscale method and its application to inelastic impactproblems. Journal of the Mechanics and Physics of Solids, 158:104668, 2022. Lu Lu, Pengzhan Jin, and George Em Karniadakis. Deeponet: Learning nonlinear operators for identi-fying differential equations based on the universal approximation theorem of operators. arXiv preprintarXiv:1910.03193, 2019.",
  "Here, we review related works and introduce the background necessary to explain our approach": "Many physical phenomena are governed by PDEs and a wide range of scientific and engineering computationproblems are based on solving these equations. In recent years, a new perspective to PDEs dictates toformulate these problems as machine learning problems where solutions to PDEs are learned. Prior worksmainly focused on using neural networks to train for the solution map of PDEs (Guo et al., 2016; Zhu &Zabaras, 2018; Adler & Oktem, 2017; Bhatnagar et al., 2019; Gupta et al., 2021). The use of neural networksin the prior works limits them to a fixed grid and narrows their applicability to PDEs where maps betweenfunction spaces are desirable. Multiple attempts have been made to address this limitation. For examplemesh free methods are proposed that locally output mesh-free solution (Lu et al., 2019; Esmaeilzadeh et al.,2020), but they are still limited to fixed input gird. A new deep learning paradigm, neural operators, are proposed as maps between function spaces (Li et al.,2020a; Kovachki et al., 2021b). They are discretization invariants maps. The input functions to neuraloperators can be presented in any discretization, mesh, resolution, or basis. The output functions can beevaluated at any point in the domain, either on a regular grid (e.g. FFT based architectures) or for any ar-bitrary geometry (e.g. graph based architectures). Variants of neural operators deploy a variety of Nystrmapproximation to develop new neural operator architecture. Among these, multi-pole neural operators (Liet al., 2020b) utilize the multi-pole approach to develop computationally efficient neural operator archi-tecture. Inspired by the spectral method, Fourier-based neural operators show significant applicability inpractical applications (Li et al., 2021b; Yang et al., 2021; Wen et al., 2022; Rahman et al., 2022a), and thearchitectures have been used in neural networks for vision and text tasks (Guibas et al., 2021; Dao et al.,2022). Principle component analysis and u-shaped methods are also considered (Bhattacharya et al., 2020;Liu et al., 2022; Rahman et al., 2022b; Yang et al., 2022). It is also shown that neural operators can beadditionally trained using PDEs, resulting in physics-informed neural operators, opening new venues forhybrid data and equation methods (Li et al., 2021c) to tackle problems in scientific computing. Decomposing the domain in smaller subdomains is at the core of many methods in computational sci-ences(Dryja & Widlund, 1990; Chan & Mathew, 1994) and extensively developed in deep learning (Dosovit-skiy et al., 2020). Prior deep learning methods on neural networks propose to decompose the input finitedimension vector to multiple patches, accomplish local operations, and aggregate the result of such process inthe global sense (Dosovitskiy et al., 2020; Guibas et al., 2021). Such methods do not decompose the outputdomain and directly predict the entire output vector. In contrast, MG-TFNO works on function spaces,and not only decomposes the input domain, but also decomposes the domain of the output functions, andseparately predicts the output at each subdomain. For the purposes of this paper, tensors are multi-dimensional arrays and generalize the concept of matrices tomore than 2 modes (dimensions). For instance, RGB images are encoded as third-order (three-dimensional)tensors, videos are 4th order tensors and so on and so forth. Tensor methods generalize linear algebraicmethods to these higher-order structures. They have been very successful in various applications in computer",
  "vision, signal processing, data mining and machine learning (Panagakis et al., 2021b; Janzamin et al., 2019b;Sidiropoulos et al., 2017; Papalexakis et al., 2016)": "Using tensor decomposition Kolda & Bader (2009); Janzamin et al. (2019a), previous works have been ableto compress and improve deep networks for vision tasks Panagakis et al. (2021a). Either a weight matrixis tensorized and factorized Novikov et al. (2015), or tensor decomposition is directly to the convolutionalkernels before fine-tuning to recover-for lost accuracy, which also allows for an efficient reparametrization ofthe network (Lebedev et al., 2015; Kim et al., 2016; Gusak et al., 2019). There is a tight link between efficientconvolutional blocks and tensor factorization and factorized higher-order structures (Kossaifi et al., 2020).Similar strategies have been applied to multi-task learning (Bulat et al., 2020a) and NLP (Papadopouloset al., 2022; Cordonnier et al., 2020). Of all these prior works, none has been applied to neural operator.In this work, we propose the first application of tensor compression to learning operators and propose aTensorized Fourier Neural Operator (TFNO).",
  "Let A := {a : DA RdA} and U := {u : DU RdU } denote two input and output function spacesrespectively. Each function a, in the input function space A, is a map from a bounded, open set DA Rd": "to the dA-dimensional Euclidean space.Any function in the output function space U is a map from abounded open set DU Rd to the dU-dimensional Euclidean space.In this work we consider the caseD = DA = DU Rd. We aim to learn an operator G : A U which is a mapping between the two function spaces. In particular,given a dataset of N samples (e.g. PDE solves) {(aj, uj)}Nj=1, where the pair (aj, uj) are functions satisfyingG(aj) = uj, we build an approximation of the operator G . As a backbone operator learning model, we useneural operators as they are consistent and universal learners in function spaces. We refer the reader toKovachki et al. (2021b) for an overview of theory and implementation. We specifically use the FNO andgive details in the forthcoming section (Li et al., 2021b).",
  "G(v) = F1F() F(v), v L2(Td; Rm)(1)": "where L2(Td; Rnm) is a function constituting the layer parameters and F, F1 are the Fourier transformand its inverse respectively. The Fourier transform of the function is parameterized directly by some fixednumber of Fourier nodes denoted N. To implement equation 1, F, F1 are replaced by the discrete fast Fourier transformsF, F1.Letv Rs1sdm denote the evaluation of the function v on a uniform grid discretizing Td withsj N points in each direction.We replace F() with a weight tensor T Covs1sdnm",
  "WWeight tensor parameterizing the entire operatorCovnn2d1L": "AInput function spaceInfiniteUoutput function spaceInfiniteaInput functionInfiniteuOutput functionInfiniteDADomain of function adDUDomain of function uddADimension of the co-domain of the input functions1dUDimension of the co-domain of the output functions1FFourier transformInfiniteF1Fourier transformInfiniteLNumber of integral operation layersIn NlLayer indexBetween 1 and LPoint-wise activation operationInfinitebBias vectorvFunction at each layerInfiniteNumber of kept frequencies in Fourier spaceBetween 1 and 1",
  "Kept modes": ":Visualization of the hyper-corners containing the low-frequency in the real-FFT. On the left, we show the low-frequencymodes used by our algorithm.Equivalently, we can first apply anFFT-shift (right) and keep the central coefficients. In particular, we parameterize halfthe corners of the d-dimensionalhyperrectangle with 2d1hyper-cubes with length size .Thatis,T is made up of the free-parameter tensors T1, , T2d1 Covnm situated in half ofthe corners of T. Each corner diag-onally opposite of a tensor Tj is as-signed the conjugate transpose val-ues of Tj.All other values of Tare set to zero. This is illustratedin the middle-top part of .3for the case d = 2 with T1 and T2.Note that, equivalently, we can ap-ply an FFT-shift to the output ofthe real FFT so the low-frequencycoefficients are located in one blockin the middle. In that case, we thenapply another FFT-shift to the output of the contraction before applying the inverse real FFT. We willuse the notation T(k, ) = Tk for any k [2d1]. The discrete version of equation 1 then becomes themapping G : Rs1sdm Rs1sdn defined as",
  "i=1T(l1, . . . , ld, j, i) F(v)(l1, . . . , ld, i).(3)": "From equation 2, a full FNO layer is build by adding a point-wise linear action to v, a bias term, and applyinga non-linear activation. In particular, from an input v Rs1sdm, the output q Rs1sdn is givenasq(l1, , ld, :) = Qv(l1, , ld, :) + G(v) + b with : R R a fixed, non-linear activation, and b Rn, Q Rnm, T1, , T2d1 Covnm arethe learnable parameters of the layer. The full FNO model consists of L N such layers each with weighttensors T1, , TL that have learnable parameters T(l)k= Tl(k, ) for any l [L] and k [2d1]. In thecase n = m for all layers, we introduce the joint parameter tensor W Covnm2d1L so that",
  "W. . . , 2d1(l 1) + k + 1= T(l)k .(4)": "A perusal of the above discussion reveals that there are (2dd + 1)mn + n total parameters in each FNOlayer. Note that, since m and n constitute the respective input and output channels of the layer, the numberof parameters can quickly explode due to the exponential scaling factor 2dd if many wavenumbers arekept. Preserving a large number of modes could be crucial for applications where the spectral decay of theinput or output functions is slow such as in image processing or the modeling of multi-scale physics. In thefollowing section, we describe a tensorization method that is able to mitigate this growth without sacrificingapproximation power.",
  "Architectural improvements": "Our proposed approach uses FNO as a backbone. To improve its performance, we first study various aspectsof the Fourier Neural Architecture and perform thorough ablation to validate each aspect. In particular, wepropose improvements to the base architecture that improve performance. Normalization in neural operatorsWhile normalization techniques, such as Batch-Normalization Ioffe& Szegedy (2015), have proven very successful in training neural networks, additional consideration mustbe given when applying those to neural operators in order to preserve its properties, notably discretizationinvariance. Specifically, it cannot depend on the spatial variables and therefore has to be either a global ora function-wise normalization. We investigate several configurations using instance normalization Ulyanovet al. (2016) and layer-normalization Ba et al. (2016), in conjunction with the use-of preactivation He et al.(2016). Channel mixingFNO relies on a global convolution realized in the spectral domain. Inspired by previousworks, e.g. Guibas et al. (2021), we propose adding an MLP in the original space, after each Spectralconvolution. In practice, we found that two-layer bottleneck MLP works well, e.g. we decrease the co-dimension by half in the first linear layer before restoring it in the second one. Boundary conditions: domain padding and skip connectionsFourier neural operators circumventthe limitation of traditional Fourier methods to inputs with periodic boundaries only.This is achievedthrough a local linear transformation added to the spectral convolution.This can be seen as a linearskip connection.We investigate replacing these with an identity skip-connection and a soft-gated skip-connection Bulat et al. (2020b). We represent in Figure. 4 the original FNO architecture (Li et al., 2021b), subfigure 4(a), the improvedversion with double (sequential) skip connections (subfigure 4(b)) and our best architecture, both with andwithout preactivation (subfigures 4(c) and 4(d), respectively).",
  "(d) Improved backbone": ": Original FNO and Improved Backbone Architecture. The original FNO architecture (Liet al., 2021b) is composed of simply a Spectral Convolution, with a (linear, soft-gating, or identity) skipconnection to recover high-frequency information and handle non-periodic inputs (4(a)). We improve thearchitecture as detailed in section 3.4.In particular, we have a version with a double (sequential) skipconnection (4(b)), while our best architecture uses nested skip connections, and can be made both with pre-or post-activation (subfigures 4(c) and 4(d), respectively). The latter, subfigure 4(d), is our best architecture. We also investigate the impact of domain-padding, found by Li et al. (2021b) to improve results, especiallyfor non-periodic inputs, and padding for the multi-grid decomposition.In fact, the multi-grid domaindecomposition inherently breaks any periodic boundary conditions (if the function itself is periodic, itspatches typically likely are not). The effect of this can be mitigated by padding the inputs, which increasescomputational cost, resulting in a tradeoff between padding and computational efficiency.",
  "Tensor Fourier Neural Operators": "In the previous section, we introduced a unified formulation of FNO where the whole operator is parametrizedby a single parameter tensor W. This enables us to introduce the tensor operator, which parameterizes ef-ficiently W with a low-rank, tensor factorization. We introduce the method for the case of a Tucker decom-position Tucker (1966c), for its flexibility. Other decompositions, such as Canonical Polyadic (CP) Carroll& Chang (1970); Harshman (1970), can be readily integrated. This joint parametrization has several ad-vantages: i) it applies a low-rank constraint on the entire tensor W, thus regularizing the model. Theseadvantages translate into i) a huge reduction in the number of parameters, ii) better generalization and anoperator less prone to overfitting. We show superior performance for low-compression ratios (up to 200) andvery little performance degradation when largely compressing (> 450) the model, iii) better performancein a low-data regime. In practice, we express W in a low-rank factorized form, e.g.Tucker or CP. In the case of a Tuckerfactorization with rank (R1, , Rd, RL, RI, RO), where RL controls the rank across layers, RI = RO controlthe rank across the input and output co-dimension, respectively, and R1, , Rd control the rank across thedimensions of the operator:",
  "Here, G is the core of size RL RI RO R1 Rd and U(L), U(I), U(O), U(1), , U(d) are factormatrices of size (RL L), (RI I), (RO O), (R1 ), , (Rd ), respectively": "Note that the mode (dimension) corresponding to the co-dimension can be left uncompressed by settingRL = L and U(L) = Id. This leads to layerwise compression. Also note that having a rank of 1 along any ofthe modes would mean that the slices along that mode differ only by a (multiplicative) scaling parameter.Also note that during the forward pass, we can pass T directly in factorized form to each layer by selectingthe corresponding rows in U(L). While the contraction in equation 3 can be done using the reconstructedtensor, it can also be done directly by contracting F(v) with the factors of the decomposition. For small,adequately chosen ranks, this can result in computational speedups.",
  "k=1ixkik)": "This joint factorization along the entire operator allows us to leverage redundancies both locally and acrossthe entire operator. This leads to a large reduction in the memory footprint, with only a fraction of the pa-rameter. It also acts as a low-rank regularizer on the operator, facilitating training. Finally, through globalparametrization, we introduce skip connections that allow gradients to flow through the latent parametriza-tion to all the layers jointly, leading to better optimization. Importantly, this formulation is general and works with any tensor factorization.For instance, we alsoexplore a Canonical-Polyadic decomposition (CP) which can be seen as a special case of Tucker with asuper-diagonal core. In that case, we set a single rank R and express the weights as a weighted sum of Rrank-1 tensors. Concretely:",
  "U(I)(:, r) U(O)(:, r) U(L)(:, r)": "where U(L), U(I), U(O), U(1), , U(d) are factor matrices of size (RL), (RI), (RO), (R), , (R),respectively and RR. Note that the CP, contrarily to the Tucker, has a single rank parameter, sharedbetween all the dimensions. This means that to maintain the number of parameters the same, R needs tobe very high, which leads to memory issues. This makes CP more suitable for large compression ratios,and indeed, we found it leads to better performance at high-compression / very low-rank. In this paper,",
  "In the experimental section 4.3, we show results of TFNO trained with a Tucker, TT and CP factorization": "Separable Fourier ConvolutionThe proposed tensorization approach introduces a factorization of theweights in the spectral domain. When a CP Kolda & Bader (2009) is used, this induces separability overthe learned kernel.In the case where the number of input channels is equal to the number of outputchannels (n = m in eq. 1), we propose to make this separability explicit by not performing any channelmixing in the spectral domain and relying on the MLP introduced above to do so. Specifically, we can nowrewrite the parameters in Eq. 4 as W Covn2d1L (notice this has one less dimension that for thenon-separable case). The operation in the spectral domain simplifies from a tensor contraction to just anelement-wise matrix multiplication:",
  "T F(v)(l1, . . . , ld, i) = T(l1, . . . , ld, i) F(v)(l1, . . . , ld, i)": "The separable Spectral convolution can be considered a depthwise convolution performed in the Fourierdomain, e.g. without any channel mixing, and with separability along each dimension. The mixing betweenchannels is instead done in the spatial domain. This results in a significant reduction in the number ofparameters while having minimal impact on performance (we found it necessary to increase the depth of thenetwork, however, to ensure the network retained enough capacity).",
  "Having introduced our decomposition in the operators parameter space, we now introduce our novel multi-grid approach to decompose the problem domain": "Domain decomposition is a method commonly used to parallelize classical solvers for time-dependentPDEs that is based on the principle that the solution for a fixed local region in space depends mostly onthe input at the same local region (Chan & Mathew, 1994). In particular, since the time-step h > 0 ofthe numerical integrator is small, the solution u(x, t + h), for any point x D and t R+, depends moststrongly on the points u(y, t) for all y Bx, r(h)) where Bx, r(h)denotes the ball centered at x withradius r(h). This phenomenon is easily seen for the case of the heat equation where, in one dimension, the",
  "u(y, t) dy": "with the approximation holding since 99.9937% of the kernels mass is contained within B(x, 4h). Whilesome results exist, there is no general convergence theory for this approach, however, its empirical successhas made it popular for various numerical methods (Albin & Bruno, 2011). Domain deomposition methodsare also widely used for boundary value, for example, in solving elliptic PDEs (Dryja & Widlund, 1990).Differences in methods vary based on how boundary data is handled between adjecent subdomains. In ourproposed appraoch we supplemant global data to each subdomain by using overlapping patches and stackingchannels with an increasing receptive between at coarser scales. To exploit this localization , the domain D is split in q N pairwise-disjoint regions D1, , Dq so thatD = qj=1Dj. Each region Dj is then embedded into a larger one Zj Dj so that points away from thecenter of Dj have enough information to be well approximated. A model can then be trained so that theapproximation G(a|Zj)|Dj u|Dj holds for all j [q]. This idea is illustrated in (a) where D = 2",
  "and all Dj, Zj are differently sized squares. This allows the model to be ran fully in parallel hence its timeand memory complexities are reduced linearly in q": "Multi-Grid.Domain decomposition works well in classical solvers when the time step h > 0 is smallbecause the mapping u(, t) u(, t + h) is close to the identity. However, the major advancement made bymachine learning-based operator methods for PDEs is that a model can approximate the solution, in oneshot, for very large times i.e. h > 1. But, for larger h, the size of Zj relative to Dj must increase to obtainthe same approximation accuracy, independently of model capacity. This causes any computational savingsmade by the decomposition approach to be lost. To mitigate this, we propose a multi-grid based domain decomposition approach where global informationis added hierarchically at different resolutions. While our approach is inspired by the classical multi-gridmethod, it is not based on the V-cycle algorithm (McCormick, 1985). For ease of presentation, we describethis concept when a domain D = T2 is uniformly discretized by 2s 2s points, for some s N, but notethat generalizations can readily be made. Given a final level L N, we first sub-divide the domain into 22L",
  "The process is illustrated for the case L = 2 in (b). Since we work with the torus, the region of theprevious level is always at the center of the current level": "The intuition behind this method is that since the dependence of points inside a local region diminishes thefurther we are from that region, it is enough to have coarser information, as we go farther. We combinethis multi-grid method with the standard domain decomposition approach by building appropriately paddedsquares Z(l)jof size 2sL + 2p 2sL + 2p around each D(l)jwhere p N is the amount of padding to beadded in each direction. We then take the evaluations of the input function a at each level and concatenatethem as channels. In particular, we train a model so that G(a|Z(0)j , , a|Z(L)j)|D(0)j u|D(0)j . Since the",
  "T2 = 0,x T2, t (0, T](7)": "with initial condition (0, ) = 0 where T2 = [0, 2)2 is the torus, f L2(T2; R) is a forcing function, andRe > 0 is the Reynolds number. Then (t, ) Hs(T2; R) for any t (0, T] and s > 0, is the uniqueweak solution to equation 7 (Temam, 1988). We consider the non-linear operator mapping f (T, ) withT = 5 and fix the Reynolds number Re = 500. We define the Gaussian measure = N(0, C) on the forcingfunctions where we take the covariance C = 27( + 9I)4, following the setting in (De Hoop et al., 2022).Input data is obtained by generating i.i.d. samples from by a KL-expansion onto the eigenfunctions ofC (Powell et al., 2014). Solutions to equation 7 are then obtained by a pseudo-spectral scheme (Chandler& Kerswell, 2013). Generating a single instance of the data at the 128 128 resilution on a single GPUtakes approximately one minute. All machine learning methods compared in this work have an executiontime of less than one second, providing, at least, 100 speed-up. The cost of generating the high resolutiondata scales quadrtically due to the CFL condition and the speed-up due to the machine learning methodsbecomes much larger.",
  "u|t=0 = u0,x T(8)": "for initial condition u0 L2(T; R) and viscosity > 0. Then u(t, ) Hs(T; R), for any t R+ and s > 0,is the unique weak solution to 8 (Evans, 2010). We consider the non-linear operator u0 u(T, ) withT = 0.5 or 1 and fix = 0.01. We define the Gaussian measure = N(0, C) where we take the covarianceC = 35/2( d2 dx2 + 9I)3. Input data is obtained by generating i.i.d. samples from by a KL-expansion ontothe eigenfunctions of C. Solutions to equation 8 are then obtained by a pseudo-spectral solver using Heunsmethod. We use 8K samples for training and 2K for testing.",
  "Implementation details": "ImplementationWe use PyTorch Paszke et al. (2017) for implementing all the models.The tensoroperations are implemented using TensorLy Kossaifi et al. (2019) and TensorLy-Torch Kossaifi (2021). Ourcode will be released under the permissive MIT license, as a Python package that is well-tested and comeswith extensive documentation, to encourage and facilitate downstream scientific applications. It will bemade available along withe the final version of the manuscript. Hyper-parametersWe train all models via gradient backpropagation using a mini-batch size of 16, theAdam optimizer, with a learning rate of 1e3, weight decay of 1e4, for 500 epochs, decreasing the learningrate every 100 epochs by a factors of 1",
  "MG-TFNO (CP)0.49 %447K401.9MG-TFNO (Tucker)0.42 %447K191.9": "number of input channels to that width. The projection layer projects from the width to 256 and a predictionlinear layer outputs the predictions. 10000 samples were used for training, as well as a separate set of 2000samples for testing. All experiments are done on a NVIDIA Tesla V100 GPU. The resolution is 128 128unless specified otherwise. To disentangle the effect of each of our components, the comparisons between the original FNO, the MG-FNO, TFNO, and the MG-TFNO were conducted in the same setting, with a mini-batch size of 32, modesof 42 and 21 for the height and width, respectively, and an operator width of 64. For the comparison between our best models, we use all the modes (64 and 32) and a mini-batch size of16, which leads to improved performance for all models but longer training times. For each comparison, thesame setting and hyper-parameters were used for all models. Training the operator.Since MG-TFNO predicts local regions which are then stitched together to forma global function without any communication, aliasing effects can occur where one output prediction doesnot flow smoothly into the next. To prevent this, we train our model using the H1 Sobolev norm (Czarneckiet al., 2017; Li et al., 2021a). By matching derivatives, training with this loss prevents any discontinuitiesfrom occurring and the output prediction is smooth. We note that this approach is only applicable if thePDE solution is globally H1 which may not always be the case, for example, in problems which containsshocks. A new approach for smoothly stiching the patch-wise solution will be needed in such cases and weleave this to future work. We note that all problems considered in our experimental section satisfy the H1",
  "In this section, we empirically verify the performance of our method, by comparing it with previous works,and through thorough ablation studies": "Comparison with previous worksIn this section, we compare our approach with both the regularFNO Li et al. (2021b), the Factorized-FNO (FFNO) Tran et al. (2023) and the U-Shaped Neural Opera-tor Rahman et al. (2022b). FFNO separately applied FFT along each mode before combining the results. We used the same settingsas described in the original paper Tran et al. (2023). U-NO uses a U-shaped architecture and reduces thespacial resolution while increasing the co-domain size.",
  "CP TFNO0.3%0.87%0.3%0.93%0.3%0.93%0.3%0.93%CP MG-TFNO0.49%1.2%0.49%1.3%0.49%1.5%0.49%1.6%": "of 0.87% in relative L2 error. We found the ordering of normalization, activation, and weights (includingpreactivation), did not have a significant impact on performance. Finally, when not using multi-grid domaindecomposition, the inputs are periodic and padding is not necessary. In that case, not padding the input im-proves performance. We use all these improvements for the backbone of the best version of our MG-TFNO,Fig 1 where we show that our improved backbone significantly outperforms the original FNO, while ourapproach significantly outperforms both, with a small fraction of the parameters, opening the door to theapplication of MG-TFNO to high-resolution problems.",
  "Resolution invariance": "TFNO is resolution invariant, meaning that it can be trained on one resolution and tested on a differentone. To illustrate this, we show zero-shot super-resolution results: we trained our best model (.4.4)on images of resolution 128 128 and tested it on unseen samples at higher resolutions (256 256 and512 512), . As can be seen, our method does as well on unseen, higher-resolution unseen testingsamples as it does on the training resolution, confirming the resolution invariance property of our neuraloperator.",
  "Training on higher-resolution with Multi-grid": "One important advantage of our multi-grid domain decomposition is that it enables training much largermodels on large inputs by distributing over patches. We demonstrate this, by training on larger resolution(512x512 discretization) and using the largest FNO and TFNO that fits in memory, on a V100 GPU. Forthe original FNO, this corresponds to a width of 12, first row in table 5. We then compare its performance",
  "MG-FNO8816401.8Tucker MG-TFNO8016461.3": "with the multigrid approach with a neural operator as large as fits into the same V100 GPUs i.e. each widthin the table has been optimized to be as large as memory allows. As we can see, our approach allows to fita larger model and reaches a much lower relative L2 error.",
  "Overfitting and Low-Rank Constraint": "Here, we show that lower ranks (higher compressions) lead to reduced overfitting. In , we show thetraining and testing H1 errors for our TOP with Tucker decomposition at varying compression ratios (2x,49x and 172x). We can see how, while the test error does not vary much, the gap between training andtest errors reduces as we decrease the rank. As we can see, while being the most flexible, Tucker does notperform as well at higher compression ratios. In those extreme cases, CP and Tensor-Train lead to lowererrors.",
  "Tensor-Train and TOP": "Our approach is independent of the choice of tensor decomposition. We already showed how Tucker is mostflexible and works well across all ranks. We also showed that while memory demanding for high rank, a CPdecomposition leads to better performance and low rank. Our method can also be used in conjunction withother decompositions, such as tensor-train. To illustrate this, we show the convergence behavior of TNOwith a Tensor-Train decomposition for a compression ratio of 178, figure 10(b).",
  "TFNO [Tucker]0.37%28 M2.3TFNO [CP]0.46%808 K83TFNO [TT]1.18%117 K574": ": Ablation comparing the performance on the relative L2 test error of our MG-TFNO ap-proach, compared with its parts TFNO and MG-FNO and the regular FNO, on Navier-Stokes.CR stands for compression ratio. Tensorization and multi-grid domain decomposition both individually im-prove performance while enabling space savings. The two techniques combined lead to further improvements,enabling large compression for both input and parameter, while outperforming regular FNO.",
  "Decomposing domain and weights: MG-TFNO": "Tensorization and multi-grid domain decomposition not only improve performance individually but theiradvantages compound and lead to a strictly better algorithm that scales well to higher-resolution data bydecreasing the number of parameters in the model as well as the size of the inputs thereby improvingperformance as well as memory and computational footprint. compares FNO with Tensorizationalone, multi-grid domain decomposition alone, and our joint approach combining the two, MG-TFNO. Inall cases, for , we keep 40 Fourier coefficients for height and 24 for the width and use an operator width of64. Our results imply that, under full parallelization, the memory footprint of the models inference can bereduced by 7 and the size of its weights by 10 while also improving performance. Consistently with our other experiments, we find that the tensor compression in the weights acts as aregularizer and improves performance across the board. Our results imply that, under full parallelization,the memory footprint of the models inference can be reduced by 7 and its weight size by 10 while alsoimproving performance.",
  "Burgers Equation": "We test the efficacy of the standard domain decomposition approach by training on two separate Burgersproblems: one with a final time T = 0.5 and one with T = 1. As described in .6, we expect thatfor T = 1, each region requires more global information thus significantly more padding need to be used inorder to reach the same error. The results of indeed confirm this. The domain compression ratiosneeded for the approach to reach the performance of the full-field model are higher, indicating the need forincorporating global information. These results motivate our multi-grid domain decomposition approach.",
  "Model# ParamsGPU RAM (nvml)Max GPU RAM (PyTorch)Runtime": "FNO277 M6.362 Gb5.435 Gb0.038 sFFNO5 M9.805 Gb8.698 Gb0.082 sUNO96 M5.054 Gb3.562 Gb0.026 sTUNO [CP-fac]381 K4.966 Gb3.504 Gb0.023 sTUNO [TUCKER-fac]5 M5.095 Gb3.600 Gb0.022 sTUNO [TUCKER-fac]1 M4.950 Gb3.480 Gb0.022 sTFNO [CP-fac]314 K6.033 Gb4.678 Gb0.029 sTFNO [CP-fac]496 K6.485 Gb4.939 Gb0.032 sTFNO [CP-fac]858 K6.755 Gb5.477 Gb0.038 sTFNO [CP-rec]858 K11.852 Gb10.721 Gb0.059 sTFNO [TT-fac]2 M6.513 Gb4.994 Gb0.031 sTFNO [TT-rec]2 M9.476 Gb8.503 Gb0.044 sTFNO [TUCKER-fac]14 M6.889 Gb5.480 Gb0.031 sTFNO [TUCKER-fac]3 M6.157 Gb4.858 Gb0.030 s",
  "Computational efficiency": "In this section, we perform ablation studies to explore the impact of our proposed tensorization on memoryconsumption and training and inference times. The tensorized formulation enables an efficient implementa-tion obtained by directly contracting the activations with the factors of the decomposition, allowing us tofurther scale the size of the models we can fit in the memory. However, this speedup depends heavily on thehardware and parameters. Going forward, we plan to further improve computational efficiency and memorysavings.",
  "In this section, we show some examples of input forcing functions, the corresponding ground-truth targetfunction and the function predicted by our proposed MG-TFNO": "In particular, we visualize the predictions from our proposed method for two different settings: in both caseswe train the model on a fixed size of 128 128 input functions but we either test with i) the same resolutionor ii) with a new, much higher resolution, unseen during training (zero-shot super-evaluation). Training and evaluating on the same resolution:In this setting, we train our proposed method onthe full training set of functions discretized at resolution 128 128 and test on new unseen samples at thesame resolution, Figure. 4.6.",
  "Zero-shot super-evaluation:In this setting, we take the same model as above, trained on the full trainingset of functions discretized at resolution 128 128": "However, this time, we give our model as input new unseen samples at new, unseen resolution of 10241024,much higher than the training one, Figure. 4.6. The model is able to use high-frequency information in thenew resolution and produce results as good as in the training resolution. A natural question is whether there is an advantage compared to just downscaling the input, feeding it atthe training resolution to the model and reupsampling it. In other words, is the model able to leveragehigh-frequency information it has not seen during training. To answer this question, we generated 1000 samples of the Navier-Stokes equation, in the same setting aspreviously introduced, but with resolution 1024 1024. We trained our proposed TFNO on the full trainingset of samples at resolution 128 128. We then evaluated the model on the high resolution, unseen, testsamples of resolution 1024 1024 in two settings: 1. [upscaling] In that case, we downscale the input to the training resolution (128 128), predictthe solution which we then re-upsample to the target high-resolution (1024 1024) for evaluation.In that case, we get a relative l2 error of 4.5%. As expected, the model is not able to leveragehigh-frequency information since that is lost when downsampling.",
  "Conclusion": "In this work, we introduced i) a novel tensor operator (TFNO) as well as a multi-grid domain decompositionapproach which together form MG-TFNO, ii) an operator model that outperforms the FNO with a fraction ofthe parameters and memory complexity requirements, and iii) architectural improvements to the FNO. Ourmethod scales better, generalizes better, and requires fewer training samples to reach the same performance;while the multi-grid domain decomposition enables parallelism over large input sizes. This is a promisingdirection for applications on high-resolution data.",
  "Mackenzie L Blanusa, Carla J Lpez-Zurita, and Stephan Rasp. The role of internal variability in globalclimate projections of extreme events. arXiv preprint arXiv:2208.08275, 2022": "Adrian Bulat, Jean Kossaifi, Georgios Tzimiropoulos, and Maja Pantic. Incremental multi-domain learningwith network latent tensor factorization. In Proceedings of the AAAI Conference on Artificial Intelligence,volume 34, pp. 1047010477, 2020a. Adrian Bulat, Jean Kossaifi, Georgios Tzimiropoulos, and Maja Pantic. Toward fast and accurate humanpose estimation via soft-gated skip connections. In 2020 15th IEEE International Conference on AutomaticFace & Gesture Recognition, 2020b.",
  "Maarten De Hoop, Daniel Zhengyu Huang, Elizabeth Qian, and Andrew M Stuart.The cost-accuracytrade-off in operator learning with neural networks. arXiv preprint arXiv:2203.13181, 2022": "Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Un-terthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. Maksymilian Dryja and Olof B. Widlund. Chapter 16 - some domain decomposition algorithms for ellipticproblems. In David R. Kincaid and Linda J. Hayes (eds.), Iterative Methods for Large Linear Systems,pp. 273291. Academic Press, 1990. Soheil Esmaeilzadeh, Kamyar Azizzadenesheli, Karthik Kashinath, Mustafa Mustafa, Hamdi A Tchelepi,Philip Marcus, Mr Prabhat, Anima Anandkumar, et al. Meshfreeflownet: A physics-constrained deepcontinuous space-time super-resolution framework. In SC20: International Conference for High Perfor-mance Computing, Networking, Storage and Analysis, pp. 115. IEEE, 2020.",
  "R. A. Harshman. Foundations of the PARAFAC procedure: Models and conditions for an \"explanatory\"multi-modal factor analysis. UCLA Working Papers in Phonetics, 16:184, 1970": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual networks. InComputer VisionECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 1114,2016, Proceedings, Part IV 14, pp. 630645. Springer, 2016. Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducinginternal covariate shift. In International conference on machine learning, pp. 448456. pmlr, 2015. Majid Janzamin, Rong Ge, Jean Kossaifi, and Anima Anandkumar.Spectral learning on matrices andtensors. Foundations and Trends in Machine Learning, 12(56):393536, 2019a. ISSN 1935-8245. doi:10.1561/2200000057. URL",
  "I. V. Oseledets. Tensor-train decomposition. SIAM J. Sci. Comput., 33(5):22952317, September 2011": "Yannis Panagakis, Jean Kossaifi, Grigorios G. Chrysos, James Oldfield, Mihalis A. Nicolaou, Anima Anand-kumar, and Stefanos Zafeiriou. Tensor methods in computer vision and deep learning. Proceedings of theIEEE, 109:863890, 2021a. URL Yannis Panagakis, Jean Kossaifi, Grigorios G. Chrysos, James Oldfield, Mihalis A. Nicolaou, Anima Anand-kumar, and Stefanos Zafeiriou. Tensor methods in computer vision and deep learning. Proceedings of theIEEE, 109(5):863890, 2021b. doi: 10.1109/JPROC.2021.3074329. Christos Papadopoulos, Yannis Panagakis, Manolis Koubarakis, and Mihalis Nicolaou. Efficient learning ofmultiple nlp tasks via collective weight factorization on bert. In Findings of the Association for Compu-tational Linguistics: NAACL 2022, pp. 882890, 2022. Evangelos E Papalexakis, Christos Faloutsos, and Nicholas D Sidiropoulos. Tensors for data mining and datafusion: Models, applications, and scalable algorithms. ACM Trans. Intell. Syst. and Technol. (TIST), 8(2):144, 2016.",
  "Md Ashiqur Rahman, Zachary E Ross, and Kamyar Azizzadenesheli. U-no: U-shaped neural operators.arXiv preprint arXiv:2204.11127, 2022b": "Nicholas D Sidiropoulos, Lieven De Lathauwer, Xiao Fu, Kejun Huang, Evangelos E Papalexakis, andChristos Faloutsos. Tensor decomposition for signal processing and machine learning. Transactions SignalProcessing, 65(13):35513582, 2017. Julia Slingo and Tim Palmer. Uncertainty in weather and climate prediction. Philosophical Transactions ofthe Royal Society A: Mathematical, Physical and Engineering Sciences, 369(1956):47514767, 2011.",
  "Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Instance normalization: The missing ingredientfor fast stylization. arXiv preprint arXiv:1607.08022, 2016": "Gege Wen, Zongyi Li, Kamyar Azizzadenesheli, Anima Anandkumar, and Sally M Benson.U-fnoanenhanced fourier neural operator-based deep-learning model for multiphase flow.Advances in WaterResources, 163:104180, 2022. Yan Yang, Angela F Gao, Jorge C Castellanos, Zachary E Ross, Kamyar Azizzadenesheli, and Robert WClayton. Seismic wave propagation and inversion with neural operators. The Seismic Record, 1(3):126134,2021. Yan Yang, Angela F Gao, Jorge C Castellanos, Zachary E Ross, Kamyar Azizzadenesheli, and Robert WClayton. Accelerated full seismic waveform modeling and inversion with u-shaped neural operators. arXivpreprint arXiv:2209.11955, 2022."
}