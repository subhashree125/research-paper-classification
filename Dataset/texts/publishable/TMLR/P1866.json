{
  "Abstract": "With the recent advance of geometric deep learning, neural networks have been extensivelyused for data in non-Euclidean domains. In particular, hyperbolic neural networks haveproved successful in processing hierarchical information of data. However, many hyperbolicneural networks are numerically unstable during training, which precludes using complexarchitectures. This crucial problem makes it difficult to build hyperbolic generative mod-els for real and complex data. In this work, we propose a hyperbolic generative networkin which we design novel architecture and layers to improve stability in training. Our pro-posed network contains a hyperbolic autoencoder (AE) that produces hyperbolic embeddingfor input data and a hyperbolic generative adversarial network (GAN) for generating thehyperbolic latent embedding of the AE from simple noise. Our generator inherits the de-coder from the AE and the generator from the GAN. Our architecture fosters expressiveand numerically stable representation in the hyperbolic space. Theoretically, we validatethe training of GAN in the hyperbolic space, and prove stability of our hyperbolic layersused in the AE. Experiments show that our model is capable of generating tree-like graphsas well as complex molecular data with comparable structure-related performance.",
  "Introduction": "High-dimensional data often show an underlying geometric structure, which cannot be easily captured byneural networks designed for Euclidean spaces. Recently, there is intense interest in learning good representa-tion for hierarchical data, for which the most natural underlying geometry is hyperbolic. A hyperbolic spaceis a Riemannian manifold with a constant negative curvature (Anderson, 2006). The exponential growth ofthe radius of the hyperbolic space provides high capacity, which makes it particularly suitable for modelingtree-like hierarchical structures. Hyperbolic representation has been successfully applied to, for instance,social network data in product recommendation (Wang et al., 2019), molecular data in drug discovery (Yuet al., 2020; Wu et al., 2021), and skeletal data in action recognition (Peng et al., 2020). Many recent works (Ganea et al., 2018; Shimizu et al., 2021; Chen et al., 2021) have successfully designedhyperbolic neural operations. These operations have been used for generating samples in the hyperbolicspace. For instance, several recent works (Nagano et al., 2019; Mathieu et al., 2019; Dai et al., 2021b) havebuilt hyperbolic variational autoencoders (VAE) (Kingma & Welling, 2014). On the other hand, Lazcanoet al. (2021) have generalized generative adversarial networks (GAN) (Goodfellow et al., 2014; Arjovskyet al., 2017) to the hyperbolic space. However, the above hyperbolic generative models are known to suffer",
  "Published in Transactions on Machine Learning Research (09/2024)": "Graph DecoderThe graph decoder assembles a molecular graph given a junction tree T = ( V , E) andgraph embedding zG. Let Gi be the set of possible candidate subgraphs around tree node i, i.e. the differentways of attaching neighboring clusters to cluster i. We want to design a scoring function for each candidatesubgraph G(i)j Gi. To this end, we first use the hyperbolic GCN and hyperbolic centroid to acquire the",
  "Hyperbolic Geometry": "Hyperbolic geometry is a special kind of Riemannian geometry with a constant negative curvature (Cannonet al., 1997; Anderson, 2006). To extract hyperbolic representations, it is necessary to choose a model,or coordinate system, for the hyperbolic space. Popular choices include the Poincar ball model and theLorentz model, where the latter is found to be more numerically stable (Nickel & Kiela, 2018). We workwith the Lorentz model LnK = (L, g) with a constant negative curvature K, which is an n-dimensionalmanifold L embedded in the (n + 1)-dimensional Minkowski space, together with the Riemannian metrictensor g = diag([1, 1n ]), where 1n denotes the n-dimensional vector whose entries are all 1s. Every pointin LnK is represented by x = [xt, xs ], xt > 0, xs Rn, and satisfies x, xL = 1/K, where , L is theLorentz inner product induced by gK:x, yL := xgy = xtyt + xs ys, x, y LnK. In the rest of thepaper, we will refer to xt as the time component and xs as the spatial component. Extensive details areprovided in Appendix A.1.",
  "NotationWe use dL(x, y) to denote the length of a geodesic (distance along the manifold) connectingx, y LnK. For each point x LnK, the tangent space at x is denoted by TxLnK. The norm L =": ", L.For x, y LnK and v TxLnK, we use expKx (v) to denote the exponential map of v at x; on the other hand,we use logKx : LnK TxLnK to denote the logarithmic map such that logKx (expKx (v)) = v. For two pointsx, y LnK, we use PTKxy to denote the parallel transport map which transports a vector from TxLnK toTyLnK along the geodesic from x to y.",
  "Hyperbolic Neural Operations": "In this section, we introduce the fundamental linear layers designs in Hyperbolic space. Additional intro-ductions of Graph Neural Network layers and Hyperbolic-Euclidean conversion layers can be found in theAppendix A.2. Fully connected linear layers are the fundamental building block of Euclidean deep learning. In Euclideanspace, each linear layer can be represented by a linear transformation add bias. Specifically, suppose theinput is x Rn, an Euclidean linear layer is",
  "where y Rm is the output, W Rmn is learnable weight matrix, b Rm is the learnable bias": "To define a linear layer in hyperbolic space, the crucial part is to define a hyperbolic linear transformation.However, if we have an x LnK and use an arbitrary matrix W R(n+1)(m+1) as weight matrix, the outputy is not guaranteed in the LmK. To constraint the output in the hyperbolic space, there are two lines of works,the tangent linear layers (Ganea et al., 2018; Shimizu et al., 2021; Chami et al., 2019) and fully hyperboliclinear layers (Chen et al., 2021). Tangent Linear LayersThe idea of this kind of linear layers is to perform linear transformation in thetangent space. Since the tangent space of any Riemannian manifold is an Euclidean subspace, it is possible toperform linear transformation in such space. Chami et al. (2019) defined the linear transformation of x LnKto y LmK by first logmap x to the tangent space of the origin, perform Euclidean linear transformation bymatrix W RnM, and expmap it back to LmK:",
  "Motivation and Definition": "Concatenation and split are essential operations in neural networks for feature combination, parallel com-putation, etc. Shimizu et al. (2021) proposed Poincar -concatenation and -split in the Poincar model.Specifically, they first use the logarithmic map to lift hyperbolic points to the tangent plane of the origin,then perform Euclidean concatenation and split in this tangent space, and finally apply regularization andapply the exponential map to bring it back to the Poincar ball. Since we use the Lorentz model, the above operations are not useful and we need to define concatenationand split in the Lorentz space. One could define operations in the tangent space similarly to the Poincar -concatenation and -split. More specifically, if we want to concatenate the input vectors {xi}Ni=1 where eachxi LniK , we could follow a Lorentz tangent concatenation: first lift each xi to vi = logKo (xi) = vitvis Rni+1, and then perform the Euclidean concatenation to get v :=0, v1s, . . . , vNs. Finally, we would gety = expKo (v) as a concatenated vector in the hyperbolic space. We denote y = HTCat({xi}Ni=1). Similarly,we could perform the Lorentz tangent split on an input xi LnK with split sub-dimensions Ni=1 ni = n toget v = logKo (x) =0, v1s Rn1, . . . , vNs RnN , vi = 0vis ToLniK , and the split vectors yi = expKo (vi)successively. Unfortunately, there are two problems with both the Lorentz tangent concatenation and the Lorentz tangentsplit. First, they are not regularized, which means that the norm of the spatial component will increaseafter concatenation, and decrease after split. This will make the hidden embeddings numerically unstable.This problem could be partially solved by adding a hyperbolic linear layer after each concatenation and split,similarly to Ganea et al. (2018), so that we have a trainable scaling factor to regularize the norm of theoutput. The second and more important problem is that if we use the Lorentz tangent concatenation andsplit in a deep neural network, there would be too many exponential and logarithmic maps. It on one handsuffers from severe precision issue due to the inaccurate float representation (Yu & De Sa, 2019; 2021), andon the other hand easily suffers from gradient explosion. Moreover, the tangent space is chosen at o. If thepoints to concatenate are not close to o, their hyperbolic relation may not be captured very well. Therefore,we abandon the use of the tangent space and propose more direct and numerically stable operations, whichwe call the Lorentz direct concatenation and split, defined as follows.",
  ".(6)": "Note that each xis is the spatial component of xi. If we consider xi LniK as a point in Rni+1, the projectionof xi onto the Euclidean subspace {0}Rn, or the closest point there, is xis. The Lorentz direct concatenationcan thus be considered as Euclidean concatenation of projections, where the Euclidean concatenated pointis mapped back to LMK by the inverse map of the projection. We remark that this concatenation directlyinherits from the Lorentz model. We also define the Lorentz split for completeness, though our main focus is on concatenation: Given aninput x LnK, the Lorentz direct split of x, with sub-dimensions n1, , nN where Ni=1 ni = n, will be",
  ". For any M > 0, there exist {xi}Ni=1 and an entry z of z for which z/xjs|x1, ,xN M": "This theorem shows that while the Lorentz direct concatenation has bounded gradients, there is no controlon the gradients of Lorentz tangent concatenation. The proof can be found in Appendix C.1. We provide anumerical validation in 3.2.1 and an empirical validation in 3.2.2. We also include additional analysis onthe effect of the two concatenation methods to the hyperbolic distance in Appendix C.2.",
  "Empirical Validation of Theorem 3.1": "In this section, we design the following simple experiment to show the advantage of our Lorentz directconcatenation over the Lorentz tangent concatenation when they are used in simple neural networks. Thehyperbolic neural network in this simple experiment consists of a cascading of L blocks, and the architectureof each block is illustrated in . A d-dimensional input is fed into two different hyperbolic linearlayers, whose outputs are then concatenated by the Lorentz direct concatenation and the Lorentz tangentconcatenation, respectively. Then, the concatenated output further goes through another hyperbolic linearlayer whose output is again d-dimensional. Specifically, for l = 0, , L 1,",
  ": Illustration of the l-th block in the simple hyperbolic neural network": "In our test, we take d = 64. We sample input and output data from two wrapped normal distributions withdifferent means (input: origin o, output: E2H(164)) and variances (input: diag(164), output: 3 diag(164)).Taking the input as x(0), we fit x(L) to the output data. We record the average gradient norm of the threehyperbolic linear layers in each block. The results for L = 64 blocks and L = 128 blocks are shown in . Clearly, for the first 20 blocks, the Lorentz tangent concatenation leads to significantly larger gradientnorms. This difference in norms is clearer when the network is deeper. The gradients from the Lorentz directconcatenation are much more stable.",
  "Generation": ": Overview of HAEGAN. (a) The hyperbolicAE. (b) The hyperbolic GAN for generating the latentembeddings. The encoders in (b) are identical to (a).(c) The process for sampling molecules. The generatorin (c) is identical to (b) and the decoders in (c) areidentical to (a). Although recent proposals of hyperbolic generativemodels have transferred VAE and GAN to the hy-perbolic domain (Nagano et al., 2019; Mathieu et al.,2019; Dai et al., 2021b; Lazcano et al., 2021), theyare known to suffer from numerical instability, whichhinders building large-scale hyperbolic generativemodels. To this end, we design our HAEGAN modelto contain both a hyperbolic AE and a hyperbolicGAN. First, we train a hyperbolic AE and use theencoder to embed the dataset into a latent hyper-bolic space. Then, we use our hyperbolic GAN tolearn the latent distribution of the embedded data.Finally, we sample hyperbolic embeddings using thegenerator and use the decoder to get samples in theoriginal space. An illustration of HAEGAN is shownin . The overall structure of HAEGAN distributes learn-ing into two architectures and thus reduces the scaleof either standalone network. Nevertheless, both theGAN and the AE require carefully chosen structuresin order to guarantee numerical stability. We discussthe hyperbolic GAN and hyperbolic AE in the nexttwo subsections respectively.",
  "Architecture of HGAN": "We could use the hyperbolic linear layers in 2.2 to define a hyperbolic GAN whose generator and critic arein the hyperbolic space. The generator pushes forward a wrapped normal distribution G(o, I) to a hyperbolicdistribution via a cascading of lgen hyperbolic linear layers. Specifically, we sample z(0) G(o, I) from thewrapped normal distribution (Nagano et al., 2019), and produce zfake following",
  "Hyperbolic Wasserstein GAN and Gradient Penalty": "We adopt the framework of Wasserstein GAN (Arjovsky et al., 2017), which aims to minimize theWasserstein-1 (W1) distance between the distribution pushed forward by the generator and the data distri-bution. Since dL is a valid metric, the W1 distance between two hyperbolic distribution Pr, Pg defined onthe Lorentz space isW1(Pr, Pg) =inf(Pr,Pg) E(x,y)[dL(x, y)],(10)",
  "(D(x)L 1)2,(12)": "where D is the critic, D(x) is the Riemannian gradient of D(x) at x, Pg is the generator distributionand Pr is the data distribution. Crucially Px samples uniformly along the geodesic between pairs of pointssampled from Pg and Pr instead of a linear interpolation.This manner of sampling is validated in thefollowing proposition, Proposition4.1. Let Pr and Pg be two distributions in LnKand f be an optimal solution ofmaxfL1 EyPr[f(y)] ExPg[f(x)] where L is the Lipschitz norm.Let be the optimal couplingbetween Pr and Pg that minimizes W(Pr, Pg) = inf(Pr,Pg) Ex,y[dL(x, y)], where (Pr, Pg) is the set ofjoint distributions (x, y) whose marginals are Pr and Pg, respectively. Let xt = (t), 0 t 1 be thegeodesic between x and y, such that (0) = x, (1) = y, (t) = vt T LnK, vtL = dL(x, y). If f isdifferentiable and (x = y) = 0, then it holds that",
  "Toy Distribution Generation": "As a proof-of-concept, we use a set of challenging toy 2D distributions explored by Rozen et al. (2021) totest the effectiveness of the hyperbolic WGAN-GP formulation. For our experiment, the training data areprepared in the following manner. We first sample 5,000 points from the toy 2D distributions and scale thecoordinates to . Then, we use the E2H operation (24) to map the points to the hyperbolic space.These points are treated as the input data of the hyperbolic GAN. Next, we use the hyperbolic GAN tolearn the hyperbolic toy distributions. The generator and the critic both contain 3 layers of hyperbolic linearlayers and 64 hidden dimensions at each layer. The input dimension for the generator is 128. After we train the hyperbolic GAN, we sample from it and compare with the input data. Note that theinput data and the generated samples are both in the hyperbolic space. To illustrate them, we map both theinput data and the generated samples to the tangent space of the origin by applying the logarithmic map.We present the mapped input data and generated samples in . Clearly, the hyperbolic GAN canfaithfully represent the challenging toy distributions in the hyperbolic space.",
  "Hyperbolic AE": "HAEGAN enjoys expressivity and flexibility in choosing the AE for embedding the hyperbolic distribution.For many datasets with hierarchical structures that are not readily presented in the hyperbolic space, thehyperbolic AE can also learn to find their appropriate hyperbolic representations. In this case, the first layerof the hyperbolic AE is constructed by the hyperbolic embedding layer (Nagano et al., 2019) from Euclideanto the hyperbolic space. The design of AE structure varies according to the type of data, which will be described in detail for eachexperimental task. Nevertheless, one common operation for building hyperbolic decoders for tree-structureddata is message passing, which requires concatenating messages from different parts of a tree. We use ournovel concatenation operation (3) here to enforce stable training.",
  "Random Tree Generation": "Recent studies (Bogun et al., 2010; Krioukov et al., 2010; Sala et al., 2018; Sonthalia & Gilbert, 2020) havefound that hyperbolic spaces are suitable for tree-like graphs. Thus, we perform an experiment in which weuse HEAGAN to generate random trees. We compare the performance of HAEGAN with other hyperbolicmodels. In this experiment, the AE consists of a tree encoder and a tree decoder. The details of the modelarchitecture is described in Appendix F.2.",
  "Experimental Settings": "DatasetWe train and test our model on the MOSES benchmarking platform (Polykovskiy et al., 2020),which is refined from the ZINC dataset (Sterling & Irwin, 2015) and contains about 1.58M training, 176ktest, and 176k scaffold test molecules. The molecules in the scaffold test set have different Bemis-Murckoscaffolds (Bemis & Murcko, 1996), which represent the core structures of compounds, than both the trainingand test sets. They are used to determine whether a model can generate novel molecular scaffolds. BaselinesWe compare our model with the following baselines: non-neural models including the HiddenMarkov Model (HMM), the N-Gram generative model (NGram) and the combinatorial generator; neu-ral methods including CharRNN (Segler et al., 2018), AAE (Kadurin et al., 2017a;b; Polykovskiy et al.,2018), VAE (Gmez-Bombarelli et al., 2018; Blaschke et al., 2018), JTVAE (Jin et al., 2018), LatentGAN(Prykhodko et al., 2019), GraphINVENT (Mercado et al., 2021), DiGress (Vignac et al., 2023), cMolGPT(Wang et al., 2023), Group SELFIES (Cheng et al., 2023). The benchmark results from HMM to LatentGANare taken from Polykovskiy et al. (2020) ; the results from GraphINVENT to Group SELFIES are takenfrom respective papers. AblationsOn one hand, we consider a Euclidean counterpart of HAEGAN, named as AEGAN, to examinewhether the hyperbolic setting indeed contributes. The architecture of AEGAN is the same as HAEGAN,except that the hyperbolic layers are replaced with Euclidean ones. On the other hand, we also report thefollowing alternative hyperbolic methods: HVAE-w and HVAE-r, where we use the same tree and graph AEbut follow the ELBO loss function used by Mathieu et al. (2019) instead of having a GAN (w and r referto using wrapped and Riemannian normal distributions, respectively); HGAN, where we train an end-to-endhyperbolic WGAN with the graph and tree decoder as the generator, and the graph and tree encoder as thecritic; HAEGAN-H, HAEGAN-, and HAEGAN-T as introduced in 5.1. MetricsWe briefly describe how the models are evaluated. Detailed descriptions of the following metricscan be found in the MOSES benchmarking platform (Polykovskiy et al., 2020). We generate a set of 30,000molecules, which we call the generated set.We first report the Validity metrics, which indicates thepercentage of valid molecules in the generated set. Then, we also evaluate the following structure-relatedmetrics by comparing the generated set with the test set and the scaffold set: Frchet ChemNet Distance(FCD), Similarity to a Nearest Neighbor (SNN) and the Scaffold similarity (Scaf ). FCD is the differencein the distribution of the last layer of ChemNet.SNN is the average Tanimoto similarly (Tanimoto, 1958)between the generated molecule and its nearest neighbor in the reference set. Scaf is cosine distances betweenthe scaffold frequency vectors (Bemis & Murcko, 1996) of the generated and reference sets. In particular,SNN compares the detailed structures while Scaf compares the skeleton structures. By considering themwith both the test and the scaffold test sets, we measure both the structural similarity to training data andthe capability of searching for novel structures.",
  "Results": "We report in the performance of HAEGAN and the baselines. For each metric described above, wetake the mean and standard deviation from three independent samples. We use bold font to highlight thebest performing model in each criterion. First of all, HAEGAN achieves perfect validity scores, which implies the hyperbolic embedding adopted byHAEGAN does not break the rule of molecule structures and does not induce mode collapse. Moreover,our model significantly outperforms the baseline models in the SNN metric. This means that the molecules",
  "Graph Decoder": ": Illustration of the auto-encoder used in theHAEGAN for molecular generation. The input molec-ular graph is firstly coarsened into the junction tree.Then both of them are encoded using graph and treeencoders to their respective hyperbolic embeddings zTand zG. To reconstruct the molecule, we first decodethe junction tree from zT , and then reconstruct themolecular graph using the junction tree and zG. It is a crucial task in machine learning to learn thestructure of molecules, which has important appli-cations in the discovery of drugs and proteins (El-ton et al., 2019). Since molecules naturally show agraph structure, many recent works use graph neuralnetworks to extract their information and accord-ingly train molecular generators (Simonovsky & Ko-modakis, 2018; De Cao & Kipf, 2018; Jin et al., 2018;2019).In particular, Jin et al. (2018; 2019) pro-posed a bi-level representation of molecules whereboth a junction-tree skeleton and a molecular graphare used to represent the original molecular data. Inthis way, a molecule is represented in a hierarchicalmanner with a tree-structured scaffold. Given thathyperbolic spaces can well-embed such hierarchicaland tree-structured data (Peng et al., 2021), we ex-pect that HAEGAN can leverage the structural in-formation. To validate its effectiveness, in this sec-tion, we test HAEGAN using molecular generativetasks, where the latent distribution is embedded ina hyperbolic manifold. In our experiments, we design both a hyperbolic tree AE and a hyperbolic graph AE in our HAEGAN toembed the structural information of the atoms in each molecule, as illustrated in . Specifically,our model takes a molecular graph as the input, passes the original graph to the graph encoder and feeds",
  "HAEGAN (Ours)1.00.00.2160.00440.7510.00210.6580.0030.6130.0030.8970.0030.1390.005AEGAN (Ours)1.00.00.3810.00820.8690.00740.4590.0060.4520.0060.2030.0040.0580.008": "generated by our model have a closer similarity to the reference set. It implies that our model better capturesthe underlying structure of the molecules and our hyperbolic latent space is more suitable for embeddingmolecules than its Euclidean counterparts. Our model also achieves competitive performance in the Scafmetric when the reference set is the scaffold test set. Given the small FCD, this shows that our model isbetter at searching on the manifold of scaffolds and can generate examples with novel core structures. Next, although AEGAN can also achieve very good performance in validity, uniqueness, and novelty, wenotice the big margin HAEGAN has over AEGAN in the structure-related metrics.This suggests thatworking with the hyperbolic space is necessary in our approach and the hyperbolic space better representsstructural information. Lastly, the alternative hyperbolic models all suffer from numerical instability and training reports NaN. Thisis not surprising since hyperbolic neural operations are known to easily make training unstable, especiallyin deep and complex networks.The result reveals the stronger numerical stability of HAEGAN, whichhighlights the importance of (1) the overall framework of HAEGAN (v.s. HVAE and HGAN); (2) the fullyhyperbolic layers (v.s. HAEGAN-H); (3) the Lorentz direct concatenation (v.s. HAEGAN-, HAEGAN-T).",
  "Conclusion and Limitations": "We proposed HAEGAN, a hybrid generative framework capable of generating both synthetic hyperbolicdata and real molecular data with comparable performance in structure-related metrics. It is an effectivehyperbolic generative model that accommodates numerically stable deep architectures. This is attributedto the following: first, the overall hybrid framework; second, the fully hyperbolic operations performed inthe Lorentz space; third, the direct concatenation. We expect that HAEGAN can be applied to broaderscenarios due to the flexibility in designing the hyperbolic AE and the possibility of building deep models. Despite the promising results, we point out two possible limitations of the current model. First, not allcomplex modules are directly compatible with HAEGAN. Indeed, if the gated recurrent units (GRU) wereused in our molecular generation task, the complex structure of GRU would cause unstable training (weobserved NaN in experiments) and that is why a hyperbolic linear layer is used instead. Nevertheless, weexpect defining more efficient hyperbolic operations that incorporate recurrent operations may alleviate theproblem and leave it to future work. Second, although the hyperbolic operations in HAEGAN do not requiregoing back and forth between the hyperbolic and the tangent spaces, we need to use exponential maps whensampling from the wrapped normal distribution. We will also work on more efficient ways of sampling fromthe hyperbolic Gaussian.",
  "Octavian Ganea, Gary Bcigneul, and Thomas Hofmann. Hyperbolic neural networks. Advances in neuralinformation processing systems, 31:53455355, 2018": "Rafael Gmez-Bombarelli, Jennifer N Wei, David Duvenaud, Jos Miguel Hernndez-Lobato, BenjamnSnchez-Lengeling, Dennis Sheberla, Jorge Aguilera-Iparraguirre, Timothy D Hirzel, Ryan P Adams,and Aln Aspuru-Guzik. Automatic chemical design using a data-driven continuous representation ofmolecules. ACS central science, 4(2):268276, 2018. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, AaronCourville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information processingsystems, 27, 2014.",
  "Wengong Jin, Kevin Yang, Regina Barzilay, and Tommi Jaakkola. Learning multimodal graph-to-graphtranslation for molecule optimization. In International Conference on Learning Representations, 2019": "Artur Kadurin, Alexander Aliper, Andrey Kazennov, Polina Mamoshina, Quentin Vanhaelen, KuzmaKhrabrov, and Alex Zhavoronkov. The cornucopia of meaningful leads: Applying deep adversarial au-toencoders for new molecule development in oncology. Oncotarget, 8(7):10883, 2017a. Artur Kadurin, Sergey Nikolenko, Kuzma Khrabrov, Alex Aliper, and Alex Zhavoronkov.drugan: anadvanced generative adversarial autoencoder model for de novo generation of new molecules with desiredmolecular properties in silico. Molecular pharmaceutics, 14(9):30983104, 2017b.",
  "Qi Liu, Maximilian Nickel, and Douwe Kiela.Hyperbolic graph neural networks.Advances in NeuralInformation Processing Systems, 32:82308241, 2019": "Aaron Lou, Isay Katsman, Qingxuan Jiang, Serge Belongie, Ser-Nam Lim, and Christopher De Sa. Differ-entiating through the frchet mean. In International Conference on Machine Learning, pp. 63936403.PMLR, 2020. Emile Mathieu, Charline Le Lan, Chris J. Maddison, Ryota Tomioka, and Yee Whye Teh. Continuous hier-archical representations with poincar variational auto-encoders. In H. Wallach, H. Larochelle, A. Beygelz-imer, F. d'Alch-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems,volume 32. Curran Associates, Inc., 2019. Roco Mercado, Tobias Rastemo, Edvard Lindelf, Gnter Klambauer, Ola Engkvist, Hongming Chen, andEsben Jannik Bjerrum. Graph networks for molecular design. Machine Learning: Science and Technology,2(2):025023, 2021. Yoshihiro Nagano, Shoichiro Yamaguchi, Yasuhiro Fujita, and Masanori Koyama. A wrapped normal distri-bution on hyperbolic space for gradient-based learning. In International Conference on Machine Learning,pp. 46934702. PMLR, 2019.",
  "Maximillian Nickel and Douwe Kiela. Learning continuous hierarchies in the lorentz model of hyperbolicgeometry. In International Conference on Machine Learning, pp. 37793788. PMLR, 2018": "W. Peng, T. Varanka, A. Mostafa, H. Shi, and G. Zhao. Hyperbolic deep neural networks: A survey. IEEETransactions on Pattern Analysis & Machine Intelligence, December 2021. doi: 10.1109/TPAMI.2021.3136921. Wei Peng, Jingang Shi, Zhaoqiang Xia, and Guoying Zhao. Mix dimension in poincar geometry for 3dskeleton-based action recognition. In Proceedings of the 28th ACM International Conference on Multime-dia, pp. 14321440, 2020. Daniil Polykovskiy, Alexander Zhebrak, Dmitry Vetrov, Yan Ivanenkov, Vladimir Aladinskiy, PolinaMamoshina, Marine Bozdaganyan, Alexander Aliper, Alex Zhavoronkov, and Artur Kadurin.Entan-gled conditional adversarial autoencoder for de novo drug discovery. Molecular pharmaceutics, 15(10):43984405, 2018. Daniil Polykovskiy, Alexander Zhebrak, Benjamin Sanchez-Lengeling, Sergey Golovanov, Oktai Tatanov,Stanislav Belyaev, Rauf Kurbanov, Aleksey Artamonov, Vladimir Aladinskiy, Mark Veselov, et al. Molec-ular sets (moses): a benchmarking platform for molecular generation models. Frontiers in pharmacology,11:1931, 2020.",
  "Frederic Sala, Chris De Sa, Albert Gu, and Christopher R. Representation tradeoffs for hyperbolic embed-dings. In International conference on machine learning, pp. 44604469. PMLR, 2018": "Marwin H. S. Segler, Thierry Kogej, Christian Tyrchan, and Mark P. Waller. Generating focused moleculelibraries for drug discovery with recurrent neural networks. ACS Central Science, 4(1):120131, 2018. doi:10.1021/acscentsci.7b00512. Chence Shi*, Minkai Xu*, Zhaocheng Zhu, Weinan Zhang, Ming Zhang, and Jian Tang. GraphAF: a flow-based autoregressive model for molecular graph generation.In International Conference on LearningRepresentations (ICLR), 2020.",
  "Menglin Yang, Min Zhou, Zhihao Li, Jiahong Liu, Lujia Pan, Hui Xiong, and Irwin King. Hyperbolic graphneural networks: A review of methods and applications. arXiv preprint arXiv:2202.13852, 2022": "Jiaxuan You, Bowen Liu, Zhitao Ying, Vijay Pande, and Jure Leskovec. Graph convolutional policy networkfor goal-directed molecular graph generation.Advances in neural information processing systems, 31,2018a. Jiaxuan You, Rex Ying, Xiang Ren, William Hamilton, and Jure Leskovec. Graphrnn: Generating realisticgraphs with deep auto-regressive models. In International conference on machine learning, pp. 57085717.PMLR, 2018b.",
  "v, vL as the norm of v TxLnK": "Exponential and Logarithmic MapsThe exponential and logarithmic maps are maps between hy-perbolic spaces and their tangent spaces. For x, y LnK and v TxLnK, the exponential map expKx (v) :TxLnK LnK maps tangent vectors to hyperbolic spaces by assigning v to the point expKx (v) := (1), where is the geodesic satisfying (0) = x and (0) = v. Specifically,",
  "Sometimes, it is necessary to find a learnable way to convert data between hyperbolic and Euclidean space.In this section, we introduce some learnable layers to convert between them": "Euclidean to HyperbolicIt is possible that a dataset is originally represented as Euclidean, albeit havinga hierarchical structure. In this case, the most obvious way of processing is to view the data in the tangentspace and use the exponential or logarithmic maps to transform it to hyperbolic space. In order to mapt Rn to the hyperbolic space LmK, Nagano et al. (2019) add a zero padding to the front of t to make it avector in ToLmK, and then apply the exponential map. This Euclidean to Hyperbolic (E2H) operation wasoriginally used for sampling, but can also be generally used to map from the Euclidean to the hyperbolicspaces. Specifically,y = E2Hm(t) = expKo ([ 0t ]) ,(24)",
  "This layer was previously used by Nagano et al. (2019) for word embedding": "Hyperbolic to EuclideanSome machine learning tasks requires Euclidean output (such as classification),thus it is necessary to construct a layer to transform Hyperbolic features to Euclidean. It is possible to directlyuse logarithmic map to convert hyperbolic features to the tangent space and view that as Euclidean features.However, it is hard to pass gradients through logarithmic map. Another way to extract Euclidean featuresis by calculating distances, since distances are intrinsically Euclidean. The Hyperbolic Centroid Distance Layer (Liu et al., 2019) is a numerically stable way of converting Hyper-bolic features to Euclidean. It maps points from LnK to Rm. Given an input x LnK, it first initializes mtrainable centroids {ci}mi=1 LnK, then produces a vector of distances",
  "BRelated Works": "Machine Learning in Hyperbolic SpacesA central topic in machine learning is to find methods andarchitectures that incorporate the geometric structure of data (Bronstein et al., 2021). Due to the datarepresentation capacity of the hyperbolic space, many machine learning methods have been designed forhyperbolic data. Such methods include hyperbolic dimensionality reduction (Chami et al., 2021) and kernelhyperbolic methods (Fang et al., 2021). Besides these works, deep neural networks have also been proposedin the hyperbolic domain. One of the earliest such models was the Hyperbolic Neural Network (Ganea et al.,2018) which works with the Poincar ball model of the hyperbolic space. This was recently refined in theHyperbolic Neural Network ++ (Shimizu et al., 2021). Another popular choice is to use the Lorentz model ofthe hyperbolic space (Chen et al., 2021; Yang et al., 2022). Our model also uses Lorentz space for numericalstability. Hyperbolic Graph Neural NetworksGraph neural networks (GNNs) are successful models for learningrepresentations of graph data. Recent studies (Bogun et al., 2010; Krioukov et al., 2010; Sala et al., 2018;Sonthalia & Gilbert, 2020) have found that hyperbolic spaces are suitable for tree-like graphs and a varietyof hyperbolic GNNs (Chami et al., 2019; Liu et al., 2019; Bachmann et al., 2020; Dai et al., 2021a; Chenet al., 2021) have been proposed. In particular, Chami et al. (2019); Liu et al. (2019); Bachmann et al. (2020)all performed message passing, the fundamental operation in GNNs, in the tangent space of the hyperbolicspace. On the other hand, Dai et al. (2021a); Chen et al. (2021) designed fully hyperbolic operations so thatmessage passing can be done completely in the hyperbolic space. Some recent works address special GNNs.For instance, Sun et al. (2021) applied a hyperbolic time embedding to temporal GNN, while Zhang et al.(2021) designed a hyperbolic graph attention network. We also notice the recent survey on hyperbolic GNNsby Yang et al. (2022). Hyperbolic Generative ModelsGenerative neural networks in the Euclidean domain cannot embedinformation of the hyperbolic geometry. To address that, Nagano et al. (2019) designs a wrapped normaldistribution in the hyperbolic space which enables taking gradient and uses it as the latent distribution ofa variational autoencoder (VAE). Mathieu et al. (2019) considers both the wrapped normal distributionand maximum entropy normal distribution and uses them to construct an VAE on the Poincar space. Daiet al. (2021b) also builds a VAE in the Poincar space, which uses the primal-dual formulation of the Kull-backLeibler (KL) divergence. Cho et al. (2024) proposes a pseudo-Gaussian manifold normal distributionfor the latent space of VAE. Other than using VAE, other generation frameworks are also adapted tothe hyperbolic space. For instance, Bose et al. (2020) lifts normalizing flows on the tangent plane of thehyperbolic space for generation; while Wen et al. (2024); Fu et al. (2024) adapts diffusion models to thehyperbolic space. Lazcano et al. (2021) uses hyperbolic linear layers in GAN for image generation. Despitethe importance of the GAN framework, we are unaware of other hyperbolic GAN models. Our proposedmodel contains a hyperbolic encoder-decoder to learn the graph-to-graph mapping, as well as a hyperbolicGAN for generating latent embeddings, whose generator uses the wrapped normal distribution as input. Molecular GenerationState-of-the-art methods for molecular generation usually treat molecules as ab-stract graphs whose nodes represent atoms and edges represent chemical bonds. Early methods for moleculargraph generations usually generate adjacency matrices via simple multilayer perceptrons (Simonovsky & Ko-modakis, 2018; De Cao & Kipf, 2018). Jin et al. (2018; 2019) proposed to treat a molecule as a multiresolutionrepresentation, with a junction-tree scaffold, whose nodes represent valid molecular substructures. Othermolecular graph generation methods include You et al. (2018a); Shi* et al. (2020); Mercado et al. (2021);Vignac et al. (2023); Cheng et al. (2023). Methods that work with SMILES (Simplified Molecular InputLine Entry System) notations instead of graphs include Segler et al. (2018); Gmez-Bombarelli et al. (2018);Blaschke et al. (2018); Kadurin et al. (2017a;b); Polykovskiy et al. (2018); Prykhodko et al. (2019); Wanget al. (2023). In particular, Prykhodko et al. (2019) proposed an AE-GAN architecture which is similar toours, though they did not use the graph structure of the molecules. Since the hyperbolic space is promisingfor tree-like structures, hyperbolic GNNs have also been recently used for molecular generation (Liu et al.,2019; Dai et al., 2021a).",
  "C.1Proof of Theorem 3.1": "Proof. 1. First, consider the case where y is one of the spatial components of y. According to (6), y isa copy of an entry in xis for some i {1, , N}. Therefore, if i = j, y/xjs is a zero vector; if i = j,y/xjs a one-hot vector. In both cases, y/xjs 1.",
  "In this section, we perform additional analysis of Lorentz direct concatenation and Lorentz tangent concate-nation, particularly their effect on hyperbolic distances": "Distances to OriginFirst, we study the hyperbolic distances to the hyperbolic origin for both con-catenation methods.Suppose we have x LnK and y LmK.Let z = HCat(x, y) Ln+m1Kandz = HTCat(x, y) Ln+m1Kbe their hyperbolic direct concatenation and hyperbolic tangent concate-nation, respectively. We compare the difference between dL(z, o) and dL(z, o) as follows. Note that thedistance between an arbitrary point x LnK and the origin only depend on the time component:",
  "Kcosh2(Kxt) + cosh2(Kyt).(36)": "Although the hyperbolic distance dL(z, o) is not the squared sum of dL(x, o) and dL(y, o), dL(z, o) is largerthan each of dL(x, o) and dL(y, o). On the other hand, after concatenation, d2L(z, o) = d2L(x, o) + d2L(y, o).This relation agrees with the Euclidean concatenation. However, norm-preservation is not why concatenationworks in the Euclidean domain. Therefore, we dont consider this as an advantage of the Lorentz tangentconcatenation. The Lorentz direct concatenation is more efficient and stable, and no information is lostduring concatenation. Therefore, it is still preferred as a neural layer. Relative DistancesMore importantly, we study how concatenation changes the relative distances, whichis closely related to stability.Specifically, we perform the following experiments.Given x, y, c LnK,let xc = HCat(x, c) be the direct-concatenated version of x and c, while yc = HCat(y, c) be the direct-concatenated version of y and c. Similarly we denote xc = HTCat(x, c) and yc = HTCat(y, c). Since thesame vector c is attached to x and y, we naturally hope dL(xc, yc) and dL(xc, yc) do not deviate much fromdL(x, y).",
  ": Difference between concatenated distances and original distances with n = 16.Left: spatialnormal. Right: wrapped normal": "We describe our experiments as follows. Take K = 1. We randomly sample three points independentlyfrom LnK as x, y and c respectively. We have two scenarios for sampling the points: (1) spatial normal: thepoints are sampled so that their spatial components follow the standard normal distribution; (2) wrappednormal: the points are sampled from the wrapped normal distribution with unit variance. In each scenario,for n {3, 16, 64}, we do the experiments for 10,000 times. We report the distances |dL(xc, yc) dL(x, y)|and |dL(xc, yc) dL(x, y)| in Figures 79, as well as their differences in . Our experiments clearly show that, especially for large dimensions, the distance between dL(xc, yc) anddL(x, y) is smaller than the distance between dL(xc, yc) and dL(x, y).In particular, in many cases,|dL(xc, yc) dL(x, y)| is around zero. On the other hand, |dL(xc, yc) dL(x, y)| tend to be large whenn = 16, 64, especially when samples follow the wrapped normal distribution. From this result, the LorentzDirect Concatenation should be preferred to the Lorentz Tangent Concatenation. In particular, the signifi-cant expansion of distance when concatenating with the same vector, in the case n = 64, may be one causeof numerical instability.",
  "E.1Experimental Settings": "ArchitectureIn the HAEGAN for generating MNIST, the encoder of the AE consists of three convo-lutional layers, followed by an E2H layer and three hyperbolic linear layers, while the decoder consists ofthree hyperbolic linear layers, a logarithmic map to the Euclidean space, and three deconvolutional layers.We remark that in our implementation, we use the LipSwish activation function (Chen et al., 2019) in theHLinear layers for its smooth second order gradients and better Lipschitz property. DatasetThe MNIST (LeCun et al., 2010) (Modified National Institute of Standards and Technology)dataset is a widely used and well-known database of handwritten digits. It contains 60,000 training imagesand 10,000 test images of handwritten digits, 0 through 9. The images in the MNIST database are 28x28pixels in size and are grayscale images. TrainingWe describe the training procedures as follows. Firstly, we normalize the MNIST dataset andtrain the AE by minimizing the reconstruction loss. Secondly, we use the encoder to embed the MNISTin hyperbolic space and train the hyperbolic GAN with the hyperbolic embedding. Finally, we sample ahyperbolic embedding using the generator and use it to produce an image by applying the decoder.",
  "ModelDimensionality251020": "N-VAE (Mathieu et al., 2019)-144.500.40-114.700.10-100.200.10-97.600.10P-VAE (Wrapped) (Mathieu et al., 2019)-143.800.60-114.700.10-100.000.10-97.100.10P-VAE (Riemannian) (Mathieu et al., 2019)-142.500.40-114.100.20-99.700.10-97.000.10Vanilla VAE (Nagano et al., 2019) -140.120.53-114.030.47-99.520.36-97.540.28Hyperbolic VAE (Nagano et al., 2019) -139.050.57-114.250.46-99.420.35-97.320.29N-VAE (Bose et al., 2020) -139.420.97-115.630.24-100.020.27-97.530.36H-VAE (Bose et al., 2020) NaN-113.720.91-99.820.23-97.250.18NC (Bose et al., 2020) -139.280.45-115.240.65-98.730.38-97.050.27T C (Bose et al., 2020) NaN-112.550.29-99.350.28-97.120.22WHC (Bose et al., 2020) -137.231.97-112.830.54-99.450.26-97.030.17",
  "E.2Results": "We present some generated samples in . In and , we report the quantitative resultsfor the MNIST generation task. First, we compare the negative log-likelihood (NLL) results between ourmethod and hyperbolic VAEs (Mathieu et al., 2019; Nagano et al., 2019; Bose et al., 2020). NLL is estimatedwith 5000 samples across all baselines and our model. The results of Mathieu et al. (2019) is directly takenfrom the paper, and the results of Nagano et al. (2019); Bose et al. (2020) is reproduced by us to ensure faircomparison. Then, we also calculate the FID and compare it with HGAN (Lazcano et al., 2021). Our NLLresults are comparable with the hyperbolic VAEs while FID is slightly better than HGAN.",
  "Finally, we take the centroid of the embeddings of all vertices to get the hyperbolic embedding zG of theentire tree,zT = HCent(x(lT )).(43)": "Tree DecoderWe adopted the tree decoder from Jin et al. (2018; 2019).The tree T = (VT , ET ) isgenerated using a tree recurrent neural network in a top-down and node-by-node fashion. The generationprocess resembles a depth-first traversal over the tree T. Staring from the root, at each time step t, themodel makes a decision whether to continue generating a child node or backtracking to its parent node. Ifit decides to generate a new node, it will further predict the cluster label of the child node. It makes thesedecision based on the messages passed from the neighboring node. We remark that we do not use the gatedrecurrent unit (GRU) for message passing. The complex structure of GRU would make the training processnumerically unstable for our hyperbolic neural network (we observed NaN in our experiments). We simplyreplace it with a hyperbolic linear layer. Message PassingLet E = {(i1, j1), . . . , (im, jm)} denote the collection of the edges visited in a depth-firsttraversal over T, where m = 2|ET |. We store a hyperbolic message hit,jt for each edge in E. Let Et be theset of the first t edges in E. Suppose at time step t, the model visits node it and it visits node jt at the nexttime step. The message hit,jt is updated using the node feature xit and inward messages hk,it. We first usehyperbolic centroid to gather the inward messages to produce",
  "F.3.1Model Architecture": "NotationWe denote a molecular graph as G = (VG, EG), where VG is the set of nodes (atoms) and EGis the set of edges (bonds). Each node (atom) v VG has a node feature xv describing its atom type andproperties. The molecular graph is decomposed into a junction tree T = (VT , ET ) where VT is the set ofatom clusters. We use u, v, w to represent graph nodes and i, j, k to represent tree nodes, respectively. Thedimensions of the node features of the graph xv and the tree xi are denoted by dG0 and dT0, respectively.The hidden dimensions of graph and tree embeddings are dG, dT , respectively.",
  "(50)": "The tree encoder is similar with the graph encoder, it encodes the junction tree to hyperbolic embeddingzT with a hyperbolic GCN of depth lT . The only difference is that its input feature xis are one-hot vectorsrepresenting the atom clusters in the cluster vocabulary. We need to use a hyperbolic embedding layer asthe first layer of the network accordingly.",
  "(51)": "Tree DecoderSimilar to Jin et al. (2018; 2019), we generate a junction tree T = (VT , ET ) using a treerecurrent neural network in a top-down and node-by-node fashion. Its architecture is the same as the treedecoder in the random tree generation experiment (F.2.1) and we will not elaborate here."
}