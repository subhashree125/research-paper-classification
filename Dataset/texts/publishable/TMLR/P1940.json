{
  "Tobias & Data Engineering Group, University of Kassel, Kassel, GermanyInterdisciplinary Research Center for Information System Design, University of Kassel, Kassel, Germany": "Maximilian Systems and Machine Learning Lab, University of Hildesheim, Hildesheim, GermanyKnowledge & Data Engineering Group, University of Kassel, Kassel, GermanyInterdisciplinary Research Center for Information System Design, University of Kassel, Kassel, Germany Tom Information Systems, University of Hildesheim, Hildesheim, GermanyKnowledge & Data Engineering Group, University of Kassel, Kassel, GermanyInterdisciplinary Research Center for Information System Design, University of Kassel, Kassel, Germany",
  "Abstract": "Difficulties in replication and reproducibility of empirical evidences in machine learningresearch have become a prominent topic in recent years. Ensuring that machine learning re-search results are sound and reliable requires reproducibility, which verifies the reliability ofresearch findings using the same code and data. This promotes open and accessible research,robust experimental workflows, and the rapid integration of new findings. Evaluating thedegree to which research publications support these different aspects of reproducibility isone goal of the present work. In order to do this, we introduce an ontology of reproducibilityin machine learning and apply it to methods for graph neural networks.The objective of this study is to try and identify hidden effects that influence model perfor-mance. To this end, we employ the aforementioned ontology to control for a broad selectionof sources and turn our attention to another critical challenge in machine learning. Thecurse of dimensionality, which induces complication in data collection, representation, andanalysis, makes it harder to find representative data and impedes the training and inferenceprocesses. The closely linked concept of geometric intrinsic dimension is employed to inves-tigate the extent to which the machine learning models under consideration are influencedby the intrinsic dimension of the data sets on which they are trained.Keywords: Reproducibility, Replication, Curse of Dimensionality, Intrinsic Dimension",
  "Introduction": "Machine learning (ML) is a rapidly evolving field that has made significant contributions to numerousindustries. In view of its considerable impact, it also becomes apparent how difficult it is to replicate andreproduce empirical findings in the field of ML. Therefore, reproducibility in ML is an important topicin its own right. Reproducibility, defined as the ability of a researcher to duplicate the results of a priorstudy using the same materials as the original investigator, is critical to ensuring the validity and reliabilityof research findings. It promotes transparency, allows for verification of results, and fosters confidence inthe scientific community. Despite its importance, achieving reproducibility in ML research is challengingdue to several barriers. One of the main difficulties is the implementation of the exact experimental and",
  "Published in Transactions on Machine Learning Research (10/2024)": "Hongwei Wang, Fuzheng Zhang, Xing Xie, and Minyi Guo. Dkn: Deep knowledge-aware network for newsrecommendation. In Pierre-Antoine Champin, Fabien L. Gandon, Mounia Lalmas, and Panagiotis G.Ipeirotis (eds.), Proceedings of the 2018 world wide web conference, pp. 18351844. ACM, 1 2018. doi:10.1145/3178876.3186175. Xiang Wang, Xiangnan He, Yixin Cao, Meng Liu, and Tat-Seng Chua. Kgat: Knowledge graph atten-tion network for recommendation. Proceedings of the 25th ACM SIGKDD International Conference onKnowledge Discovery & Data Mining, 5 2019a. doi: 10.1145/3292500.3330989. Xiang Wang, Xiangnan He, Meng Wang, Fuli Feng, and Tat-Seng Chua. Neural graph collaborative filtering.Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Informa-tion Retrieval, 7 2019b. doi: 10.1145/3331184.3331267. URL",
  "Reproducibility and Replicability": "Several publications have investigated the general state (National Academies of Sciences, Engineering andMedicine, 2019) and challenges (Nature Special, 2018) of reproducibility and replicability in science. Thereis also work that has looked more specifically at these issues in the field of computer science (Freire et al.,2012) and its subfield of machine learning (Raff, 2019; Liu et al., 2020a; Chen et al., 2022).In recentyears a growing number of conferences include dedicated tracks for reproducibility efforts or specific work-shops (Stodden et al., 2013; ICLR, 2019). The knowledge collected there is now available in straightforwardchecklists (Pineau, 2020) and general reports (Pineau et al., 2021). Related to this more and more journalsand publisher provide specific editorial policies (Casadevall & Fang, 2010; Springer Nature, 2020) to helpauthors in that regard. Efforts for reproducing and replicating past works from broad range of researchfields concentrate in some dedicated journals (ReScience C, 2023), in which publications from further backare also of interest.1 Beyond the space of academic publication there are of course similar efforts made bythe programming community (paperswithcode, 2021; Sinha & Forde, 2020). Specifically, machine learningengineering teams and individuals are building frameworks (Lightning AI and Contributors, 2022) and tem-plates (ashleve and Contributors, 2022) to streamline the process of setting up reproducible machine learningexperiments. Recent research has shown that the choice of the machine learning framework used and itsversion (Pham et al., 2020; Shahriari et al., 2022), or the commercially available platforms providing relatedservices (Gundersen et al., 2022b), can have a significant impact on the reproducibility properties of theresearch code. There are, motivated by practical concerns, surveys that investigate directly the availabilityand operability of research code (Collberg et al., 2015). Few works additionally try to construct a taxonomyof those reproducibility properties (Goodman et al., 2016; Kitzes et al., 2018; Tatman et al., 2018; Bouthillieret al., 2019). The accompanying discussions often emphasize the confusing terminology (Peng, 2011; Plesser,2018; Gundersen, 2020). The provided taxonomies usually consist of a shallow hierarchy of different levelsof reproducibility which are characterized by high-level features of the submissions that have to be assessed.In most cases the process of evaluating is guided by only a few questions. As such they give researchers andreviewers not that much guidance when evaluating the degree of reproducibility. However there are publi-cations that go more into detail when analysing factors and variables that influence reproducibility (Ivie &Thain, 2018; Gundersen et al., 2018; Gundersen & Kjensmo, 2018; Gundersen et al., 2022a). This focus oncentral aspects of computational reproduciblity can also be found in the present work.",
  "Intrinsic Dimension and Feature Selection": "The term intrinsic dimension (ID) has multiple slightly different meanings in related sub-fields of machinelearning. They share the motivating aspect of using the value of the ID of data as a proxy for gaining evidenceon how the data is structured. One prominent usage of the term is for specifying the often approximateddimension of a hypothetical embedded manifold in the data space which describes almost all samples withsufficient accuracy (Hein & Audibert, 2005; Tatti et al., 2006). This notion of ID can be used to motivate avariety of estimators, for example based on sampling around data point neighborhoods (Kim et al., 2016).Those estimators give rise to different feature selection methods (Traina et al., 2010; Mo & Huang, 2012;Suryakumar et al., 2013; Golay et al., 2016), occasionally based on gradients to learn an embedding with thedesired properties (Pope et al., 2021). Other work has focused on the ID of the learning process or the learnedmodel itself. One line of research is the analysis of the ID of activations in different layers in deep neuralnetworks (Ansuini et al., 2019; Doimo et al., 2020). There, the notion of ID is slightly adapted to be theminimum number of coordinates required to describe a set of points without significant loss of information.On the one hand, the mathematical derivations are often specifically tailored to neural networks and, on theother hand, only in a few cases do they explicitly depend on the chosen feature sets employed by the modelarchitecture. However, these algorithms do not help to decide if and to what extent the data set is affectedby the curse of dimensionality and the related concentration phenomena (Franois et al., 2007; Houle, 2013).",
  "Studies Regarding Influence of Data on Model Behavior": "Machine learning models are heavily influenced by the quality and nature of the input data they are trainedon. This relationship has been extensively studied in various contexts. A large body of literature is concernedwith influence of simple data augmentation (e.g.cropping, rotating, stretching for computer vision datasets) when keeping a machine learning model fixed (Salamon & Bello, 2016; Perez & Wang, 2017; Tsuchiyaet al., 2019; Tian et al., 2020; Laptev et al., 2020). Naturally there are also works studying influence offeature selection methods (Koak et al., 2019) and projection methods (Wan et al., 2021) in addition tothose referenced in the previous sections. In the realm of classical machine learning, theoretical studies have been conducted on various models,including decision trees (Syrgkanis & Zampetakis, 2020) and quadratic classifiers (Latorre et al., 2021), thatexplore the estimation capabilities and the performance within high-dimensional settings. These modelsoften exhibit a dependence on dimension, particularly in the context of high-dimensional regimes wheretheir effectiveness may vary. Similarly, research has been dedicated to understanding the behavior of supportvector machines in spaces with low (box-counting) dimension (Hamm & Steinwart, 2020). In a different vein, the concept of influence function is used to track the impact of training data on learningalgorithms, providing insight into how the models predictions on test data are influenced by the trainingdata. When applied to neural networks, influence functions shed light on the backpropagation process andthe attribution of training data importance in these complex architectures (Koh & Liang, 2017; Pruthi et al.,2020; Akyrek et al., 2022; Hammoudeh & Lowd, 2022).",
  "An Ontology of Reproducibility in Machine Learning": "The term reproducibility is often used ambiguously and vaguely in the field of machine learning (Peng, 2011;Kitzes et al., 2018; Plesser, 2018). In our work, we apply the classical understanding of reproducibility inscience. That is, whenever a scientific study is replicated the original experimental results should be achievedwith a high degree of reliability. Of course, the concept of replication implies a series of attributes. Amongother things, these can be the independence of the project (e.g.different group of researchers), use ofdifferent set of equipment (hardware or software), or the time since the original study was realized. Assuch one can see that particular scientific results can be placed on a spectrum of reproducibility. To givea more explicit process of contextualizing reproducibility we introduce in the following section an extensiveontology of reproducibility for the realm of machine learning. The necessity for this is based on the fact,that, to the best of our knowledge, such an ontology does not yet exist. The components of this ontologyare influenced and inspired by the Chapter Assessing Reproducibility of the online version of the book The",
  ": Top levels of the reproducibility ontology": "Practice of Reproducible Research (Kitzes et al., 2018). Additional ascendancy comes from existing efforts tocharacterize computational reproducibility (Gundersen et al., 2018; Gundersen & Kjensmo, 2018; Gundersenet al., 2022a). We adapted them with a stronger formalization, giving structure to the proposed questionsand adjusted the ontology to better meet the requirements for the subsequent reproducibility study. Othernoteworthy influences come from an ontology for semantic terms in machine learning (Publio et al., 2018),a practical taxonomy of machine learning (Tatman et al., 2018) which however has no formalization andvery few specific points to check, and the machine learning reproducibility checklist (Pineau, 2020). Onecommonality between this ontology and the above-mentioned works is the focus on reproducibility of anindividual research project. For simplicity, we consider only the setting where one computational result ispresented as evidence for one scientific result.",
  "Overview": "We now present our ontology of reproducibility in machine learning. It connects possible errors or difficultiesthat could arise when trying to reproduce the results of a single scientific study. Our proposed ontology isstructured as a hierarchy and starts on the top level with the general notion of a scientific result. Such a resultcan be based on empirical or theoretical evidence. Because we are (subsequently) interested in research thatuses experiments for producing evidence we do not subdivide the theoretical category. In contrast, we proposea fine-grained structure for the empirical category. To reflect the related central aspects of data processingwe use the ontological entities data set, software and computational results as the main subcategories for theempirical category. Each of these aggregate again a set of subentities. For example, the data set categoryencompasses the availability and transformation of data. Similarily, the software category has as centralsubcategory source code but also includes environment and usage as subcategories. Most importantly, weinclude the derived model and its predictions in the computational result category. shows theschema that provides an overview of the main categories and their subcategories. We further evaluate every considered research paper within the proposed ontology based on a set of questions.In the following we want to describe the formalization of these questions and their motivations, which arebased on potentially occurring errors and their impact on the scientific reproducibility. As we follow anopen world semantic with our ontology the questions are formulated in such a way that answering themnegatively is good for reproducibility. This also means that in the subsequent questioning, a missing answerdoes not indicate the non-existence of the corresponding property. We have included some questions that",
  "Data Set": "One central aspect of scientific result in machine learning is the data set to which previous and proposedmethods are applied.A common approach is to evaluate methods over multiple different but similarlystructured data sets. Reproducibility is only possible with detailed knowledge about the process for obtainingand preparing used data sets.2 Explanations and automatization minimize the risk of working with a dataset that follows a different distribution than the one used in the original study. Within this category of theontology, we want to focus on whether the publication and accompanying material include steps (manual orautomated) that describe how to obtain data sets.",
  "D6Does the data set require a restrictive license agreement for accessing?": "D7Is the data set available on request only? This is loosely linked to the previous point. Some-times it might be necessary to go through a more elaborate process to obtain the data set. Suchsteps are often quite brittle over time and are unlikely to be maintained in the long term.",
  "D8Are manual steps necessary for pre-processing? A series of manual steps to transform a dataset could easily be a source of error, so an automated solution is preferred": "D9Is there only an incomplete description for pre-processing steps? Theprovidedexplana-tions or scripts might not be enough to get the data set in the necessary form.This point isespecially crucial for lesser known data sets. For such data sets, it is particularly helpful for furtherinvestigation if tools are provided for loading and accessing individual samples. 2But even then there are special cases where methods select data samples or generates them (e.g. active learning, reinforce-ment learning), and reproducibility of similar aspects is handicapped by other means.",
  "Software": "The implementation and application is a central part of a proposed machine learning method. It is one aspectof the research protocol and acts as description of the executed experiments. The software code operates onone or more data sets and produces computational results. In this category we combine aspects of the codewritten by the authors, ancillary software, and other components crucial for reproducibility. This keeps theontology clearly structured.",
  "Questions from this category deal with general behavior of the target system, which heavily influences thecontext of execution of the experiments": "S1Is the exact version of dependencies not documented? Multiple dependencies can interact inintricate ways. This makes pinning of exact versions necessary for avoiding possible bugs connectedto incompatible versions as well as prevent time consuming fixing of conflicts. S2Is the specified version of dependencies not available anymore? Depending on the age of thepublication and the type of dependency used, old versions may have disappeared from the distribu-tion channels. For smaller projects, they may no longer be maintained by the projects developersor maintainers. S3Is necessary hardware unavailable? Many specialised hardware requirements can be circumventedby simulating a computational environment with virtual machine or container images. However,this adds a time-consuming overhead when trying to achieve reproducibility, or is not feasible in areasonable time. S4Are any seeds for random number generators not set? Multiple dependencies each can havedifferent random number generator, where each has to be set for getting closer to reproducibility ofan experiment.",
  "Usage": "This category of the ontology groups together aspects regarding how the experiments were started. Forthe set of considered machine learning papers it is not necessary to consider user input beyond startingconfiguration. S6Is the documentation not up-to-date? Few publications include dedicated documentation of thesource code they provide. If only a simple Readme file is included, it should at least not be misleadingfor the reproducibility attempt.",
  "Source Code": "The questions in this category are concerned with the source code files that implement the ideas of thescientific experiment. We do not have separate questions regarding the availability of source code itselfsimilar to the data set category because we only focus on papers that provide source code with a non-restrictive license.",
  "S11Is there a bug that was never fixed? Over time, authors and external contributors may find dif-ferences or errors between the original publication, its revisions and the implementation": "S12Are there issue solutions that were not applied? Usually, the discovery of implementationproblems is accompanied by public discussions on the website hosting the implementation. However,it may happen that the solution discussed has not been implemented, or has not been merged froman external source fork. S13Was a bug fix distributed through other channels? On the other hand, public discussions mayindicate that the fix was distributed (manually) through some other channel, such as emails or directmessages to a selected/active group of participants. In these cases, it is not clear what the detailedchanges are, where they are, and how they affect reproducibility. S14Did the API change? This question relates to attribute S10. We are now considering the conversescenario, wherein the entry points to core parts of the implementations have undergone alteration,yet this is not reflected in other pertinent sources, such as documentation or supplementary materials.Furthermore, the reproducibility of results is hindered when the traceability of versions is constrainedby a complex and unclear history within the version control system. S15Did an out of memory error occur? As a special type of defect, it is only recorded when thereare no or incorrect requirements.As our aim is to assess reproducibility in general, we do notdetermine the specifics that caused this error.",
  "S16Are steps for one experiment missing? If the necessary source code for an experiment is notincluded, the reproducibility of this experiment is more difficult to achieve": "S17Are parts of the source code not available? In addition to the previous point, we want to eval-uate the possible situation where the publication uses libraries or code that is not included in thesource code provided. This makes reproducibility almost impossible. S18Is the hyperparameter search not included? As hyperparameter search is an integral part ofexperiments, it is important for reproducibility to have an explanation or process in the implemen-tation of how the search was performed. S19Is only the general idea (and no experiments) implemented? Another reason for the lack ofexperiments could be that the publication simply proposes a new machine learning algorithm or abuilding block for an existing one.",
  "Computational Result": "The result obtained from a computational experiment is the evidence in support of a scientific claim. Thefull reproduction of a scientific result and its evaluation depends on the successful completion of the repro-ducibility steps for data set and software. If this is not possible, no model or predictions can be obtained forfurther evaluation steps. Conversely, if the supplementary material of the publication in question containsthe learned model, it is still possible to perform the subsequent reproducibility steps. However, this is arare case. Ideally, current scientific results in machine learning should be reproducible in terms of data setand software, as well as providing the learned model. This will allow for a more in-depth comparison andanalysis between the reproduced and the provided models.",
  "Model": "As outlined above, easy access to model weights is necessary for full reproducibility, especially when otherfactors increase the difficulty of obtaining an optimized model independently. Furthermore, the practicalproblem of making the model available still has no ready-made solution. There are a few existing platformsthat allow the combined hosting of source code, data sets and models. Limitations can quickly becomeapparent when data sets and models are large, or when multiple of them are used or shared. For now, thereis only one question in this category. R1Are there no parameters (weights) of the obtained model provided? As much of todaysmachine learning approaches use larger data sets and models, it takes more and more time andcomputational resources to run the proposed method. Making model weights accessible can there-fore act as a kind of shortcut if the focus is on comparison with other approaches. Depending onthe programming language and language and format, it gives other researchers an straightforwardway to check model specifications. In addition, when reproducibility fails, it provides a way to findthe cause and, more importantly, to at least verify the authors claims.",
  "Predictions": "This category is intended to capture aspects of the outputs of the model when evaluating over the trainor test data, and how these affect reproducibility. Depending on the results presented, a comparison ofdifferent evaluation metrics may be helpful, especially if the inference requires more time or resources thanare available for the specific reproducibility attempt. We note that all but the last question are inherentlyhard to quantify. R2Are there small deviation to obtained model? We will measure the differences by comparingthe central evaluation metrics between values reported by original authors with evaluated metricson the reproduced model. We answer the question positively, when the relative difference is in therange of 1-2%.",
  "R4Are strong differences in almost all experiments observed? As an extension of the previousquestion, we assign this attribute when almost no reasonable reasonable reproducibility of the results": "R5Are the claimed results only supported by small sample size? Individual runs of a machinelearning algorithm are rarely exactly reproducible, even with the best efforts to achieve repro-ducibility, both by the original authors and those who reproduce the work. Therefore, averagingover several runs with the same configuration of an experiment will strengthen the reliability of theresult. We mark this attribute if there are less than five such similar runs per result. R6Are there no predictions (outputs of classes or decisions) on the data sets? If the originalpublication provides the class predictions or decisions made by the model, the reproducibility attemptprovides the opportunity to examine more comprehensive metrics for differences in model behavior.",
  "Limitations and Extensions": "It is apparent that this ontology is designed using a basic formalization language. All connections betweenthe entities can be read as part of. The authors are well aware of the ML-Schema (Publio et al., 2018).However, we decided not to include or build on it, because: (i) several aspects of reproducibility could not beexpressed using the ML-Schema; (ii) the focus of the ML-Schema is on sharing information about machinelearning algorithms, and not on the reproducibility of a scientific result. An example of this is the lack ofconsideration of the influence of the seed for the random number generator. There are a number of possible modifications or extensions that we have not included in the present versionof the ontology. For example, our ontology does not take into account detailed information about theoreticalevidence. This is mainly motivated by the survey in , which focuses on empirical evidence. In thecategory of empirical evidence, the name software is somewhat misleading, as it also includes aspects relatedto hardware. One could rename this category or add hardware as a separate sub-category of empirical evi-dence. Similarly, the entity source code could be extended to include details of different modes of availabilityand documentation. A special attribute might be the use of version control software. Certain aspects of reproducibility are not yet considered, e.g. that plots, figures and tables can be generatedautomatically. Authors often fail to provide the source code for the visualization. Furthermore, our ontologydoes not capture aspects of data provenance. In addition, it seems that the longer the time since publication, the lower the achievable level of reproducibil-ity. This may be due to the ageing of the hardware and software used in the experiment, which may nolonger be available. Finally, in our ontology we treat the presence or absence of an attribute categorically. Therefore, in cer-tain cases, evaluating a research paper using our ontology is a difficult task. Conversely, any subsequentontological operation and explanation is independent of any interpretation of numerical values. Furthermore, it is notable that a reproducibility score for a publication, which could be derived from theevaluated attribute, is absent. By maintaining the evaluation through the ontology in a qualitative mannerthus far, an ordinal measuring structure has been obtained that allows for formal comparison and ordering.Additionally, the ordinal evaluation framework allows for the explanation of differences in reproducibility orlack thereof. Nevertheless, it would be advantageous to have a scoring function in place. However, at thismoment, we are unable to define a scoring function that we can guarantee will be meaningful.",
  "Reproducibility of Major Graph Neural Network Research Results": "The first main goal of the present work is to achieve a scientific overview over the state of reproducibilityin the research field of graph neural networks. For this we first depict our method of candidate selection inSubsection 4.1 and thereafter discuss all our findings with respect to our reproducibility ontology.",
  "Candidate Selection": "The main criterion for selecting a paper was its impact on the research field of graph neural networks. In thefollowing we describe in detail our procedural steps. As a lower bound for the publication year we selected2016, the year of the publication of the seminal GCN paper (Kipf & Welling, 2016). On the other hand weconsidered works that were published before 2023. Most importantly we required that the paper in questionhas an experimental evaluation, due to the overall objective of the study to investigate the influence ofintrinsic dimension. We also included research works that were only in the preprint stage. As the citation count is an often used proxy for measuring scientific impact, and at the same time readilyavailable, we employ it in our selection process. In detail, we use the Semantic Scholar (Allen Institute for",
  "Out of those remaining 55 results we applied the source code criterion and arrived at 42 papers. In we depict the yearly distribution of the number of papers (a) and their score distribution (b)": "Now the following limitation of the selection method becomes more apparent. As citations are distributedover publications and there is an increasing number of papers published each year, older papers have advan-tage over newer ones. Conversely, the evaluation function dampens the influence of older publications to amuch lesser extent. On the other hand we wanted our selection method to reflect a normal search behavior of a researcher.The power-law distribution of citations is a well known property of citation networks (Price, 1965) and alsosomewhat expected because they are social networks which accompany the scientific process. More specifi-cally methods of more frequently cited papers are chosen more often than those with less citations (Hazogluet al., 2017). The power-law distribution of the citations (and subsequently the score) motivated selectingonly a few papers as those publications had the majority of the impact on the field measured by the abovemethod.",
  "We provide a list of considered papers in Appendix A in the appendix. We started with selecting candidatesfor the reproducibility survey before deciding on the automated process presented above": "The primary factors considered in assessing the reproducibility of a given paper at this stage were incompat-ibility of hardware requirements and errors in the source code provided. If these issues could not be resolvedwith reasonable effort, even with experience of using the libraries, the paper was deemed not reproducible.Unfortunately, we have not been able to fully convert the previous manual pre-selection into an process that",
  "General Observations with Respect to our Ontology": "Reproducing experimental results from the selected scientific research was challenging due to various factors.It is usually the case that the papers alone do not provide enough information to replicate the experimentsindependently. Accordingly, we have always started the reproducibility attempt with the associated sourcecode. One major issue is that those repositories often lack crucial dependency information (S1), making itdifficult to even run the entry point scripts without errors. To overcome these challenges, it was necessary tosearch through the accompanying discussions and seek clarification on the exact parameters and commandsthat might be missing or not working correctly. Additionally, it is often the case that the commands providedin the simple documentations for running experiments rarely work as expected (S6, S7). Furthermore, theavailability of different data sets adds complexity to the reproducibility process, especially considering thatmany publications were published before coordinated efforts to unify the data set landscape, for examplethe Open Graph Benchmark (Hu et al., 2020a). However, data sets are generally accessible, although it israrely the case that the preprocessing steps are explained (D9). Another common point is the aspect ofhyperparameter search, which is not included in most of the software provided, even if it is mentioned in thepublication (S18). Finally, it is common for papers to fail to provide the model (R1) or the predictions of themodel on specific data (R6). The following is a more detailed description of specific problems, grouped underthe main categories of data set, software and computational result, encountered during the reproducibilityattempts. For a better overview, we will focus on selected points that stand out. We would like to emphasize that in our ontology it is not advantageous to have an attribute, as this isevidence that it is more difficult to reproduce. In cases where it was not clear whether an attribute waspresent, we chose not to disclose it.",
  "Category: Software": "GCN:The dependencies are not properly specified (S1), which made it challenging to set up and run theexperiments. It is worth noting that the documentation is not up to date (S6) and contains misleadinginformation. R-GCN:Firstly, there is no requirements file provided (S1), making it challenging to recreate the neces-sary environment. Additionally, information about the specific Python interpreter version used is hidden.Furthermore, the seeds for randomization are not set (S4). GraphSAGE:The necessary arguments for the evaluation scripts are not stated (S7), leaving researchersunsure of the required inputs. Furthermore, there are discussions about possible values, adding ambiguity tothe code (S8). Additionally, the evaluation scripts themselves are incomplete or misleading, further hinderingreproducibility (S9). The software used in the study has some bugs that affect reproducibility (S11). Forexample, the evaluation script for the ppi data set is incomplete, but a fix is available in pull requests (S12). DiffPool:Unfortunately there is no explicit list of necessary requirements (S1) and only a minimalREADME file (S6). Additionally it seems that the provided commands do not work (S7, S8 and S9) andthat the seeds for randomization are not set before the experiments (S4). The implementation also did notinclude steps to reproduce two experiments with the reddit-12k or collab data sets (S16).",
  "Category: Computational Result": "GCN:When examining the results, it is observed that there are small deviations in the test set accuracy,typically within a range of 1% (R2). However, it appears that only a single run was conducted for eachexperiment, as no standard deviation is provided for the final performance. Although the authors claim tohave run the experiments with multiple seeds, there is no evidence of this in the code, which raises concernsabout the robustness of the reported results (R5). We were not able to reproduce the experiments with theneil data set because of the difficulties to prepare and use the data set. R-GCN:A similar observation regarding multiple runs was made for this publication as well. Additionally,the results of the study exhibit both small deviations and strong differences in different parts.For theAIFB, MUTAG, and AM data sets, small deviations of approximately 2% in accuracy are observed (R2).However, for the BGS data set, a significant difference of 15% is observed, indicating a substantial variationin the results (R4).",
  "Discussion": "We now want to contextualize the observations and results. The Figures 3a and 4a show the distributionsof normalized intrinsic dimensionality (NID) and we observe that distinct values arise for different data sets.We may note that the figures show relative and not absolute NID and therefore their respective values shouldnot be compared. Furthermore, even in the absence of this relative scaling, the mathematical modelling ofthe intrinsic dimension permits a direct comparison. For our discussion we compare the different values of NID to the performance of the corresponding models,as shown in Figures 3b and 4b. From this we can infer the following link. consider the difference betweenthe lowest and the highest value of the NID for a given data set. We find that this difference decreases inthe following order: pubmed, citeseer, cora. If we look to the corresponding model performance in b,we observe that the performance of the random feature selection divergences for different proportions .Interestingly this happens in the same order as before, albeit at different levels of accuracy. The differencebetween the lowest and highest values of NID indicates that the individual contributions to the ID by thecorresponding features is more evenly distributed. For example, in the case of a horizontal line every featurecontributes equally to the ID. Conversely, a -shape, as observed in a for the reddit data set,",
  "Influence of Intrinsic Dimensionality on Model Performance": "The second main goal of the present work is to investigate the influence of intrinsic dimensionality on modelbehavior. We begin with the mathematical foundations of the concept of geometric ID in Subsection 5.1 andthen present our experiments and results. As already mentioned the geometric intrinsic dimension (Hanika et al., 2022) is a computational accessibleapproach for measuring how a given data set is affected by the phenomenon of concentration of measure (Gro-mov & Milman, 1983; Milman, 1988; 2000), which itself is deeply connected to the curse of dimensional-ity (Pestov, 1999; 2007b;a; 2010b;a). Of central importance are feature functions that concentrate. Thismeans that they map most of the values of their domain close to the mean or median of their image set.Pestov has postulated that features of this type contribute the most to the curse of dimensionality. In hisapproach, all 1-Lipschitz functions are considered as potential feature functions. In the revised axiomaticsystem introduced by Hanika et al. (2022) the notion of a dimension function emerged. Such a dimensionfunction allows for estimating the extent to which the provided features concentrate on the data set withoutthe necessity of evaluating all possible feature functions. This is especially important in machine learning,where algorithms and, in particular, models, usually only have access to a limited range of features of a giventype. In recent works, the computation and approximation of the dimension function have been improved whenchoosing the set of all component projections as feature functions for data sets in Euclidean space (Stubbe-mann et al., 2023a). The objective of this study is to build upon the findings of previous research thatemployed the geometric intrinsic dimension for feature selection (Stubbemann et al., 2023b).",
  "Foundations of the Concentration-based Intrinsic Dimension": "We commence by providing a brief overview of the mathematical definitions that the intrinsic dimensionbuilds upon. Readers who wish to gain a more detailed understanding of this topic are directed to the citedworks, which offer in-depth explanations. Since this work is primarily concerned with practical (and thusfinite) setting, the following definition, which is specific to that context, will suffice for our purposes. Definition 1 (Adapted from (Hanika et al., 2022)).Let D = (X, F, ) be a triple consisting of a finiteset X of data points and a finite set F RX of feature functions from X to R. Consider the functiondF (x, y) := supfF |f(x) f(y)|.We require that supx,yX dF (x, y) < is fulfilled and (X, dF ) is acomplete and separable pseudo metric space with being the normalized counting measure on (X, dF ). Wecall D a (finite) geometric data set. We will now introduce the building blocks that give rise to a dimension function that fulfills the aforemen-tioned axioms postulated in Hanika et al. (2022). A function of this nature will indicate a geometric dataset by a low value, precisely when the contained data points can be more effectively discriminated by thecorresponding set of feature functions. Given a feature f F we want to evaluate how it can discriminate sets of a specific measure (e.g. sizec := |X|(1 )) for a fraction (0, 1) of the whole X. For this we use can use the following function:",
  "Approximation of Intrinsic Dimension": "The straightforward computation of the equations in the previous section is hindered by the task to iteratethrough all subsets M X of size k. This yields an exponential complexity with respect to |X| for computing(D). As suggested by Hanika et al. (2022) and later proven by Stubbemann et al. (2023a), we can insteaduse algorithms with a quadratic runtime complexity in |X| to compute the ID. Furthermore, for settingswhere a quadratic runtime is still not sufficient, the authors propose the following concept.",
  "Dimension based Feature Selection": "The intrinsic dimension of a data set refers to a measure of concentration that captures the underlyingstructure or information of the data. It is challenging to quantify the impact of intrinsic dimensionality ona particular machine learning method. This motivates the need to investigate its effect. One approach toachieving this is by discarding the features that have the most significant influence on the dimensionality ofthe data set. The removal of these features allows for the observation of any changes in the performanceof the trained model. This approach allows us to examine the relationship between intrinsic dimensionalityand model performance. Feature selection can be regarded as a means to an end in this research. It servesas a tool to identify and eliminate those features that contribute the most to the dimensionality of the dataset. In order to calculate the influence of dimensionality and perform feature selection, we rely on methodsdemonstrated in Stubbemann et al. (2023b), which we will briefly include in the following paragraphs.",
  "Approximation of Discriminability": "Unfortunately, as before, an explicit calculation of the normalized discriminability for larger data sets is notfeasible because the algorithm scales quadratically with the number of data points. We can, however, usea similar approach to the previously referenced method of approximating the intrinsic dimension with thehelp of support sequences to approximate the discriminability as well.",
  "Experimental Execution and Impact on Intrinsic Dimension": "In order to demonstrate the impact of the intrinsic dimensionality of the different data sets on the methodsemployed in the reproduced papers, we have chosen to discard those features with the highest (approximate)normalized intrinsic dimensionality. In the context of contemporary machine learning, data sets are typically represented by matrices. For graphdata, this typically refers to the data pertaining to the nodes X. In addition, the connectivity information,represented by the adjacency matrix A, and any edge features, are also considered. However, aggregating thisinformation into a feature matrix through the use of neighbourhood aggregation, in the form of AkX (wherek is a small positive integer), does not alter the qualitative insights provided by the intrinsic dimension, asdemonstrated by previous research (Stubbemann et al., 2023b). Aggregation is a common feature of graphneural network methods, which often employ indirect forms of aggregation through the use of specific models.Given this, we have chosen to refrain from considering the neighbourhoods, and instead focus on the matrixof node features of shape n d, where n indicates the number of samples and d the number of attributesper sample. For each data set in our investigation we use the following representation as a geometric dataset as introduced in Definition 1. The set X is comprised of the n samples xi where each sample consists ofthe attributes xi = (xi1, . . . , xid). We chose the set of component selectors fj(x) = xj as the set of featurefunctions F. Together with the counting measure (A) = |A|/n for a subset A X we complete our specialinstance of the geometric data set D.",
  "yelp7168476977410300SAGN+SLE": "Data set preparationFor each research paper, we initially extract the essential components for loadingand preprocessing the data sets from the source code supplied by the authors. These components are thenused to obtain the node feature matrices of the data sets used. In instances where it is unavoidable, weresort to concatenating the node feature matrices from both the training and test data. It is of paramountimportance to note that we rigorously ensure that the machine learning method does not have greater accessto the test data than was originally permitted in the original implementation. The rationale for applying feature selection after preprocessing is as follows: Our approach aims to investigatehow methods are influenced by the data on which they are applied, for example, how the model sees thedata. Some forms of preprocessing alter the empirical data distribution, and preprocessing typically lacks anylearnable parameters. Furthermore, the model in question rarely has explicit information about the appliedpreprocessing steps. Consequently, we do not consider the preprocessing steps to be part of the model. Thisapproach facilitates the separation of the influence, as otherwise the change in the preprocessing resultingfrom feature selection would be included in the observations and discussions. Feature selectionWe used the algorithm for direct calculation of the discriminability (Stubbemann et al.(2023b), Algorithm 1) for data sets with less than 105 samples.For larger data sets, we employed theapproximating version (Stubbemann et al. (2023b), Algorithm 2). In those cases we first choose a geometricsequence s = (s1, . . . sl) of length l = 10, 000 with s1 = |X| and sl = 2 and use the support sequence(Subsection 5.2.1) s which results from s = (|X| + 2 s1, . . . , |X| + 2 sl) via discarding duplicatedelements. We then discarded for every factor {0.1, 0.2, . . . , 0.9} the corresponding fraction of the featureswith highest (approximated) normalized intrinsic dimensionality from all data points. Following the selectionprocess, the machine learning algorithms from the relevant papers are applied to the feature-reduced data setsusing the same (hyper-)parameter configuration as the original. In order to facilitate an objective evaluation,the same scores (accuracy or f1) as those originally produced were collected over repeated training runs withten different seeds.4 We did not test other feature selection methods as similar investigations were alreadydone in Stubbemann et al. (2023b).",
  "Observations": "We present in this section the computational results and observations for the experiment. Here we focus onthe details corresponding to the two research works GCN and SAGN+SLE. Afterwards we will state generalobservations for the remaining experiments, but refer the reader to Appendix C for accompanying plots. In a we show the analysis for the intrinsic dimensionality of the three data sets from GCN. Becausethe data sets are differently sized we need to find a common representation. First we order the feature setfor each data set using the normalized intrinsic dimensionality of each feature as the score. On the x-axis we",
  "We also see small fluctuations and drops in performance at the highest discarding values. This becomesmore apparent when directly visualizing the differences to the proposed discarding method": "This picture changes slightly when looking at the results related to the SAGN+SLE experiments in b.For some data sets the same stagnating behavior is evident. For others, however, there is a marked dropin performance. Especially for the two ppi data sets there is a greater variation in performance. Anotherdifferent behavior can be seen for the yelp data set, where the performance starts to decrease for lowerdiscard factors.",
  ": Influence of Intrinsic Dimension measured through feature selection for the SAGN+SLE results": "indicates that a small number of features is responsible for almost all of the ID value.Based on thesedeductions, we propose the following explanation. In a certain sense, features with a low NID can be used bymachine learning methods to distinguish more data points. These features may allow a learning procedureto have a more stable convergence, a shorter runtime, and a higher final performance. In our experimentalstudy, we focus on the interplay between the ID and the achieved model performance. At this point we maynote that all presented experiments are based on the same principal optimization task of stochastic gradientdecent (SGD). The observations concerning the shape of distribution of NID described above may permit to draw a connec-tion to the simplicity bias in neural networks (Arpit et al., 2017; Shah et al., 2020; Valle-Prez et al., 2019),the tendency of SGD to find simple models. Although further research is needed to confirm this hypothesis,the proposed association would be consistent with the assumption that SGD weights features that carrymore information higher. In our random experiment, we discard uniformly from the set of features. In every step one may lose lowand high dimensional features following the distributions as shown in Figures 3a and 4a. This means for acertain discarding proportion there are almost no features with low NID available. Until those disappear,SGD has the possibility to use them for obtaining the objective. But when those good features are notincluded anymore the situation changes and the performance deteriorates rapidly. On the other hand if thefeatures are discarded in order of decreasing NID then the inevitable deterioration of model performance canbe postponed for quite a bit. Conversely, when discarding features in a reverse order, e.g. ascending NID,the performance drops rapidly as the model has no access to those good features from the beginning. However, we can not exclude the effect of artifacts of the method. Especially in the reverse case, where,for example, non-convergent behavior at the beginning of training triggers an early stopping condition thatleads to the abortion of the optimization routine. As we regard the methods as black boxes, we have notinvestigated these possibilities further. Turning to a, we observe a few data sets that have a similar distribution of NID as those in a.However, it seems that for some the contribution to the total NID are distributed more evenly among thefeatures. This is particularly evident for the yelp data set. On the other hand, the distribution for the redditdata set is much more extreme, where only a small set of features have extraordinary high contribution tooverall NID. This figure also clearly shows the influence of preprocessing on the NID distribution. Whereaspreviously in a the line for the cora data set was clearly a step function, it has now become a muchsmoother slope. It seems that this has almost no influence on the achieved model performance in both cases.",
  "The GraphSAGE and SAGN+SLE methods both use the reddit data set with the same preprocessing. Theformer shows a slight deterioration, while the latter shows almost no change in model performance": "The SAGN+SLE method is applied on the yelp data set, while the GCN and SGC methods are used onthe pubmed data set. The NID distributions are quite similar, but the performance on the yelp data setcontinuously worsens with higher , while the performance on the pubmed data set remains stable until thehighest discarding proportions.",
  "The GCN, SGC and SAGN+SLE methods all use the cora data set. Although the preprocessing is the samein GCN and SGC, there is a clear difference in performance": "The SAGN+SLE and GraphSAGE method both use the large ppi data set mentioned earlier, but in bothcases the performance deteriorates significantly, starting at different discarding proportions and speeds. A possible alternative hypothesis for the above observations is that the machine learning tasks at hand canbe solved using only the information provided by the adjacency matrix of the graph data. In this case, thereduction of available node features would not affect the final performance. In contrast, observations fromexperiments with inverse feature discarding suggest that the previous statement may not be correct, as theseobservations showed a much larger degradation in model performance than the other way around. In general, this highlights a limitation of the current approach, as the chosen feature functions do not takegraph edges into account. This is indirectly related to the earlier discussion on the modelling choice of whatto use as the underlying set for the geometric data set. We decided to use the node features as the base setX and ignored the edge features or the adjacency matrix. By using a different modelling approach, the setof feature functions could be extended or constructed entirely differently. Taken together, these comparisons provide a strong indication that there is an effect of NID on modelperformance. Although we have only used the NID as an auxiliary tool to measure the ID, it shows thatdifferent methods are influenced by the ID of the data set itself. However, it is difficult to quantify the extentof this dependence on the concentration phenomenon given the present experiments on these very differentmethods. At this point it is necessary to go into more detail about the graphs for the experiments for the DiffPool model.The original code accompanying the DiffPool publication uses an one-hot encoding of node label as nodeattributes instead of the available original node attributes. This is not stated in the paper, and the resultsdo not seem to be easily reproducible when switching to the conventional node attributes. Nevertheless,we made this change to make it compatible with the other experiments and our method. As a result, thegraph of the accuracy achieved starts much lower for zero and low discard fractions than the originallyclaimed performance would suggest. This also explaines why the results are not influenced by the discardingof features, as the node labels lack sufficient discriminatory power.",
  "Summarizing the Analysis and Limitations": "We present an overview of all experiments by combining the information about the intrinsic dimensionalityand the model performance when features are discarded. To do this, we calculate the sum of the (approx-imate) normalized discriminability of the remaining features after discarding a fraction . The value thusobtained can be normalized by the total sum of (approximate) normalized discriminability of all features.We perform this calculation for different values of and all data sets. We plot these values against theachieved performance measure (accuracy or f1 score) for the corresponding configuration. The results aredepicted in . This leads to the striking observation that most models can cope with data sets reduced to about 30% oftheir original dimensionality without any loss of performance. Despite the large differences in the numberof samples and features between the data sets, we do not observe a significant correlation between thesefeatures and the change in model performance. Experiments with reversed feature selection () showed that there is an effect of the intrinsic dimensionon the different learning methods. More specifically, we observed that this effect depends on the particulardata set, as discussed in Subsection 5.5. Our study is limited in various aspects, which we will discuss below. We observe in our investigation alow frequency of overlapping use of data sets, which is a consequence of the selection process and theunderlying requirements for reproducibility. Consequently, it is challenging to compare different methodson the dimensionally reduced data sets.However, the data sets citeseer, cora, pubmed and reddit havenon-trivial support over the considered papers. In this regard, we observe a similar pattern of behaviour aspreviously described (cf. ). Furthermore our results build upon a rather small set of selected candidates. This could be tackled withallocating more time for achieving reproducibility per paper, which would allow for fixing or circumventreproducibility barriers by searching for the right combination of technical tricks. It might also be possibleto apply the individual methods from the publications to the other data sets as well. The proposed method only indirectly measures the effects of the concentration of measure phenomenonthrough the perspective of the geometric intrinsic dimension. Moreover, is is not possible to provide anextensive overview of the complete ID influence. This is due to the fact that it is unclear if the ML methodscan draw on more (complicated) feature functions than the considered ones in their processing of the data.In this case, the current restriction would be a hindrance to measuring the true impact of the ID on themethods. However, for the function class in question, we can rely on the guaranteed computability. Uponreflection, it may be not necessary to do so, as even within this restricted scope, an influence could bedemonstrated. Furthermore, the present work does not include a comparison with other approaches for measuring the influ-ence of dimensions and any feature selection methods based on them. It is not clear if those methods wouldmeasure the same properties of the ML methods given their different underlying theoretical frameworks. The experiments indicate that the distribution of dimensionality has a detrimental impact on the modelperformance. However, further experiments are required to gain a comprehensive understanding of this rela-tionship. In particular, the measured ID is contingent upon the features functions employed. Consequently,by utilizing more comprehensive function classes, the intrinsic dimension can be more accurately determined. Finally, we want to explain the rationale behind the decision not to utilise the correlation coefficient oran alternative methodology to quantify the direct correspondence between intrinsic dimension and modelaccuracy in order to facilitate a comparison of data sets. In order to do so, we will point out two theoreticalaspects. Firstly, different data sets will exhibit disparate (absolute) diameters, even when employing thesame feature functions. Secondly, the observable diameter is linear with regard to the feature functions,e.g. ObservableDiameter(D) = ObservableDiameter(D), where D = (X, {f : x f(x) | f F}, ),for R (see Hanika et al. (2022)). Thus, to consider their ratios is less informative without normalizingwith regard to these properties.Therefore a first meaningful way to compare values is to focus on theordinal relation of the feature attributes with regard to intrinsic dimension. Altogether, the experiments",
  "(b) f1 scores of data set and paper combinations": ": Overview of evaluation of data set and paper combinations over the remaining intrinsic dimen-sionality. The x-axis is the sum of the (approximated) normalized intrinsic dimensionality of the remainingfeatures normalized by the total sum for the whole feature set. The y-axis is the resulting evaluation scoreobtained by the method trained on the data set with corresponding feature selection.",
  "Reproducibility of the Presented Work": "As a publication about reproducibility, it is only fair to also consider the own reproducibility as well. Ingeneral the reproducibility of our own work is limited by the reproducibility of the used papers. We relyexclusively on data sets provided by them and our source code is based on the one published by them. Forthe purposes of our experiments, it was not necessary to modify the hyperparameters of the models. In orderto achieve a high degree of reproducibility, we have provided the individual scripts for the reproducibility anddimensionality experiments. These scripts include the explicitly specified environments that were used andthe necessary changes to the original source code. In addition, we have endeavoured to adhere to the generalguidelines set out by (Pineau et al., 2021; paperswithcode, 2021), as well as our own ontology. In total, theexperiments produced approximately 600 models (6 papers, 10 factors, 10 seeds). It is not feasible to provideweights for all of them, especially if the original source code did not include sophisticated management ofthe paths where checkpoints were stored. Given the special form of our publication, we do not provide themodel weights. The complete resources for source code, logs, and results of our experiments can be foundat",
  "Conclusions": "In this work, we presented an ontology to investigate the reproducibility of machine learning research. Thisontology can be used to evaluate the reproducibility of scientific publications in a standardized manner. Forthis, we assume that scientific evidence can be generated via theoretical evidence (for example via theoremsand proofs) or via empirical evidence provided by scientific experiments. In machine learning, reproducibilityresearch mainly focuses on the extent to which other researchers can re-execute the experiments with thesame results. Consequently, our work was primarily concerned with empirical evidence, for which we haveproposed a set of attributes for evaluating the level of reproducibility of a specific work. These attributescan be divided into three categories: data set, software and computational result. Once reproducibility has been established, the next step is to identify the relevant influential factors thataffect the outcome of the experiments. One such factor that has been the subject of the presented inves-",
  "Based on our findings, we recommend the following points as best practice for reproducible machine learningresearch": "Write one main script that does everything necessary. That includes setup and or tear-down or preparation of the compute environment, downloading and preprocessing of data sets, andrunning of all reported experiments. It might be beneficial to chose an existing software packagewhen handling larger data or when working on compute clusters. Log all relevant information into files. This encompasses all outcomes, utilised input variablesand also selected hyperparameters, in the event that a form of hyperparameter optimisation wasconducted. Often, such information is only printed to the output terminal, which hinders repro-ducibility. Store checkpoints of all obtained models.As multiple runs with only different seeds arerequired, it is imperative that the wrapping script/software ensures that no computed checkpoint isoverwritten in the subsequent iteration.",
  "Furthermore, we give three recommendations on how to account for the concept of intrinsic dimensionalityin the scope of machine learning research": "Consider the intrinsic dimensionality of the used training data sets when comparingmachine learning algorithms. Our experiments indicate that well-established graph neural net-work approaches are significantly impaired by the increasing intrinsic dimensionality of the inputdata. Consequently, when comparing their performance, it is of paramount importance to ascertainthe ID of the used data in order to accurately assess the extent to which performance differences areattributable to the curse of dimensionality. Investigate, if discarding of high-dimensional features is possible for the chosen GNN. Itis possible to discard a significant fraction of features with high normalized intrinsic dimensionalityfrom certain graph neural network models without a fundamental drop in performance. For example,when considering the GCN model, it is possible to drop up to 70% of the total number of featureswithout decrease in accuracy, while this is not possible for GraphSAGE. Account for transformations of the NID-distributions induced via preprocessing. Thepreprocessing of features typically incorporates global interactions between them (i.e., averages),which alters the NID distribution in a non-trivial manner. One such case was observed in the redditdata set, where both SAGN+SLE and SGC applied distinct preprocessing procedures, resulting inmarkedly disparate NID distributions. As previously stated, this alteration of the NID distributionscan profoundly impact model performance.",
  "Limitations and Future Work": "The current study is subject to certain limitations, which will be addressed in future research.Firstly,as discussed in Subsection 4.3, our ontology is limited to a fixed granularity. It has become evident thatthe varying depth of analysis required for reproducibility cannot be represented by the proposed ontology.Consequently, we will develop a hierarchy of ontologies with different levels of granularity by further splittingor aggregating the current attributes. For instance, in the event that a given paper presents a multitudeof distinct experiments, it may be prudent to redefine attributes R2,R3 and R4 in order to encompass abroader spectrum of error ranges and fractions of non-reproducible outcomes. Secondly, the current study is focused on a specific concept of intrinsic dimensionality. However, as discussedin , a variety of different ID estimators exist. Consequently, future work will investigate whetherthe reported observations are applicable to different concepts for intrinsic dimensionality. Thirdly, our approach considers only intrinsic dimensionality for a specific feature set, namely the usualcoordinate projections. However, different machine learning methods may incorporate different aspects ofthe data. Consequently, future work will investigate how these different aspects can be formalized as featurefunctions. This will lead to an ID not on for models instead of data sets. Here, the crucial problem will bethe identification of a finite and computational feasible feature set which captures the model behavior.",
  "Allen Institute for Artificial Intelligence. Semantic Scholar, 2022. URL [Online; accessed 2023-12-11]": "Alessio Ansuini, Alessandro Laio, Jakob H Macke, and Davide Zoccolan. Intrinsic dimension of data rep-resentations in deep neural networks.In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch-Buc,E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 32. Cur-ran Associates, Inc., 2019. URL Devansh Arpit, Stanisaw Jastrzbski, Nicolas Ballas, David Krueger, Emmanuel Bengio, Maxinder S Kan-wal, Tegan Maharaj, Asja Fischer, Aaron Courville, Yoshua Bengio, et al. A closer look at memorizationin deep networks. In Doina Precup and Yee Whye Teh (eds.), International conference on machine learn-ing, volume 70, pp. 233242. PMLR, JMLR.org, 8 2017. URL",
  "ashleve and Contributors.Lightning Hydra Template, 2022.URL [Online; accessed 2023-11-22]": "Deyu Bo, Xiao Wang, Chuan Shi, and Huawei Shen. Beyond low-frequency information in graph convolu-tional networks. In AAAI Conference on Artificial Intelligence, volume 35, pp. 39503957. Associationfor the Advancement of Artificial Intelligence (AAAI), 5 2021.doi: 10.1609/aaai.v35i5.16514.URL Xavier Bouthillier, Csar Laurent, and Pascal Vincent. Unreproducible research is reproducible. In KamalikaChaudhuri and Ruslan Salakhutdinov (eds.), International Conference on Machine Learning, volume 97,pp. 725734. PMLR, 5 2019.",
  "Yifan Feng, Haoxuan You, Zizhao Zhang, R. Ji, and Yue Gao. Hypergraph neural networks. In AAAIConference on Artificial Intelligence, pp. 35583565, 9 2018. doi: 10.1609/aaai.v33i01.33013558. URL": "Damien Franois, Vincent Wertz, and Michel Verleysen. The concentration of fractional distances. IEEETransactions on Knowledge and Data Engineering, 19(7):873886, 7 2007. ISSN 1041-4347. doi: 10.1109/tkde.2007.1037. Juliana Freire, Philippe Bonnet, and Dennis Shasha. Computational reproducibility: state-of-the-art, chal-lenges, and database research opportunities. Proceedings of the 2012 ACM SIGMOD International Con-ference on Management of Data, 5 2012. doi: 10.1145/2213836.2213908. Xinyu Fu, Jiani Zhang, Ziqiao Meng, and Irwin King. Magnn: Metapath aggregated graph neural networkfor heterogeneous graph embedding. Proceedings of The Web Conference 2020, 4 2020. doi: 10.1145/3366423.3380297. URL",
  "Odd Erik Gundersen. The fundamental principles of reproducibility. ArXiv, abs/2011.10098, 11 2020": "Odd Erik Gundersen and Sigbjrn Kjensmo. State of the art: Reproducibility in artificial intelligence. InSheila A. McIlraith and Kilian Q. Weinberger (eds.), AAAI Conference on Artificial Intelligence, vol-ume 32. Association for the Advancement of Artificial Intelligence (AAAI), 4 2018. doi: 10.1609/aaai.v32i1.11503. URL Odd Erik Gundersen, Yolanda Gil, and David W. Aha. On reproducible ai: Towards reproducible research,open science, and digital scholarship in ai publications. AI Mag., 39(3):5668, 9 2018. ISSN 0738-4602.URL Odd Erik Gundersen, Kevin Coakley, and Christine R. Kirkpatrick. Sources of irreproducibility in machinelearning: A review. ArXiv, abs/2204.07610, 4 2022a. ISSN 2331-8422. doi: 10.48550/arxiv.2204.07610. Odd Erik Gundersen, Saeid Shamsaliei, and Richard Juul Isdahl. Do machine learning platforms provideout-of-the-box reproducibility? Future Gener. Comput. Syst., 126:3447, 1 2022b. ISSN 0167-739X. doi:10.1016/j.future.2021.06.014. William L. Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. InIsabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan,and Roman Garnett (eds.), NIPS, volume 30, pp. 10241034, 6 2017. URL",
  "Matthias Hein and Jean-Yves Audibert. Intrinsic dimensionality estimation of submanifolds in rd. Proceedingsof the 22nd international conference on Machine learning, 2005": "D. Hong, Lianru Gao, Jing Yao, Bing Zhang, A. Plaza, and J. Chanussot. Graph convolutional networks forhyperspectral image classification. IEEE Transactions on Geoscience and Remote Sensing, 59:59665978,8 2020. ISSN 0196-2892. doi: 10.1109/tgrs.2020.3015157. URL Michael E. Houle. Dimensionality, discriminability, density and distance distributions. 2013 IEEE 13thInternational Conference on Data Mining Workshops, pp. 468473, 12 2013. doi: 10.1109/icdmw.2013.139. Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, andJure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. ArXiv, abs/2005.00687,5 2020a. doi: 10.48550/arxiv.2005.00687.",
  "Lightning AI and Contributors. Lightning, 2022. URL accessed 2023-11-22]": "Chao Liu, Cuiyun Gao, Xin Xia, David Lo, John C. Grundy, and Xiaohu Yang. On the replicability andreproducibility of deep learning in software engineering. ArXiv, abs/2006.14244, 6 2020a. doi: 10.1145/3477535. Meng Liu, Hongyang Gao, and Shuiwang Ji. Towards deeper graph neural networks. In Rajesh Gupta, YanLiu, Jiliang Tang, and B. Aditya Prakash (eds.), Proceedings of the 26th ACM SIGKDD internationalconference on knowledge discovery & data mining, pp. 338348. ACM, 8 2020b. doi: 10.1145/3394486.3403076. URL",
  "Vitali Milman. Topics in asymptotic geometric analysis. In Visions in Mathematics, pp. 792815. Springer,2000. ISBN 978-3034604246. doi: 10.1007/978-3-0346-0425-3_8": "Dengyao Mo and Samuel H. Huang. Fractal-based intrinsic dimension estimation and its application indimensionality reduction. IEEE Transactions on Knowledge and Data Engineering, 24(1):5971, 1 2012.ISSN 1041-4347. doi: 10.1109/tkde.2010.225. Christopher Morris, Martin Ritzert, Matthias Fey, William L. Hamilton, J. E. Lenssen, Gaurav Rattan, andMartin Grohe. Weisfeiler and leman go neural: Higher-order graph neural networks. In AAAI Conferenceon Artificial Intelligence, pp. 46024609. Cornell University, 10 2018. doi: 10.1609/aaai.v33i01.33014602.URL",
  "Garima Pruthi, Frederick Liu, Mukund Sundararajan, and Satyen Kale. Estimating training data influenceby tracking gradient descent. ArXiv, abs/2002.08484, 2 2020. URL": "Gustavo Correa Publio, Diego Esteves, Agnieszka Lawrynowicz, P. Panov, Larisa N. Soldatova, TommasoSoru, Joaquin Vanschoren, and Hamid Zafar. Ml-schema: Exposing the semantics of machine learningwith schemas and ontologies. ArXiv, abs/1807.05351, 7 2018. J. Qiu, Qibin Chen, Yuxiao Dong, Jing Zhang, Hongxia Yang, Ming Ding, Kuansan Wang, and Jie Tang. Gcc:Graph contrastive coding for graph neural network pre-training. Proceedings of the 26th ACM SIGKDDInternational Conference on Knowledge Discovery & Data Mining, 6 2020. doi: 10.1145/3394486.3403168.URL",
  "Victor Garcia Satorras and Joan Bruna.Few-shot learning with graph neural networks.ArXiv,abs/1711.04043, 11 2017. URL": "Vctor Garcia Satorras, Emiel Hoogeboom, and Max Welling. E (n) equivariant graph neural networks.In Marina Meila and Tong Zhang (eds.), International conference on machine learning, volume 139, pp.93239332. PMLR, PMLR, 2 2021. Michael Schlichtkrull, Thomas N Kipf, Peter Bloem, Rianne Van Den Berg, Ivan Titov, and Max Welling.Modeling relational data with graph convolutional networks. In Aldo Gangemi, Roberto Navigli, Maria-Esther Vidal, Pascal Hitzler, Raphal Troncy, Laura Hollink, Anna Tordai, and Mehwish Alam (eds.),The Semantic Web: 15th International Conference, ESWC 2018, Heraklion, Crete, Greece, June 37,2018, Proceedings 15, volume 10843, pp. 593607. Springer, Springer Science+Business Media, 2018.ISBN 978-3319934167. doi: 10.1007/978-3-319-93417-4_38. URL",
  "Harshay Shah, Kaustav Tamuly, Aditi Raghunathan, Prateek Jain, and Praneeth Netrapalli. The pitfalls ofsimplicity bias in neural networks, 6 2020. URL": "Mostafa Shahriari, Rudolf Ramler, and Lukas Fischer. How do deep-learning framework versions affect thereproducibility of neural network models? Machine Learning and Knowledge Extraction, 4(4):888911, 102022. ISSN 2504-4990. doi: 10.3390/make4040045. URL M. Simonovsky and N. Komodakis. Dynamic edge-conditioned filters in convolutional neural networks ongraphs. 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2938, 7 2017.ISSN 1063-6919. doi: 10.1109/cvpr.2017.11. URL",
  "Chuxiong Sun and Guoshi Wu. Scalable and adaptive graph neural networks with self-label-enhanced train-ing. ArXiv, abs/2104.09376, 4 2021. ISSN 2331-8422. URL": "Divya Suryakumar, Andrew H. Sung, and Qingzhong Liu. Influence of machine learning vs. ranking algorithmon the critical dimension. International Journal of Future Computer and Communication, pp. 215219,2013. ISSN 2010-3751. doi: 10.7763/ijfcc.2013.v2.155. Vasilis Syrgkanis and Manolis Zampetakis. Estimation and inference with trees and forests in high dimen-sions. In Jacob Abernethy and Shivani Agarwal (eds.), Proceedings of Thirty Third Conference on LearningTheory, volume 125 of Proceedings of Machine Learning Research, pp. 34533454. PMLR, 0912 Jul 2020.URL",
  "Caetano Traina, Agma J. M. Traina, Leejay Wu, and Christos Faloutsos. Fast feature selection using fractaldimension. J. Inf. Data Manag., 1(1):316, 5 2010. ISSN 2178-7107. doi: 10.1184/r1/6605570.v1": "Haruki Tsuchiya, Shinji Fukui, Yuji Iwahori, Yoshitsugu Hayashi, Witsarut Achariyaviriya, and BoonsermKijsirikul. A method of data augmentation for classifying road damage considering influence on classifi-cation accuracy. In Imre J. Rudas, Jnos Csirik, Carlos Toro, Jnos Botzheim, Robert J. Howlett, andLakhmi C. Jain (eds.), International Conference on Knowledge-Based Intelligent Information & Engineer-ing Systems, volume 159, pp. 14491458. Elsevier BV, 2019. doi: 10.1016/j.procs.2019.09.315.",
  "Seongjun Yun, Minbyul Jeong, Raehyun Kim, Jaewoo Kang, and Hyunwoo J. Kim. Graph transformernetworks.Neural Information Processing Systems, 32:1196011970, 11 2019.URL": "Rowan Zellers, Mark Yatskar, Sam Thomson, and Yejin Choi. Neural motifs: Scene graph parsing with globalcontext. 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 58315840, 112017. doi: 10.1109/cvpr.2018.00611. URL Chuxu Zhang, Dongjin Song, Chao Huang, A. Swami, and N. Chawla. Heterogeneous graph neural network.Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining,7 2019. doi: 10.1145/3292500.3330961. URL",
  "2023year of publication foreach publication. Out of the vast array of publications, we selected the top 100 papers based on these scores": "The process of reproducing the list proved to be a challenge, because of a significant degree of variability dueto the non-deterministic nature of the Semantic Scholar API. This variability was particularly noticeablewith regard to page ordering and the contents of the first 10000 entries. Over time we started reproducibilityattempts of publications that are now no longer part of the list. In our filtering process, we manually excluded entries that were surveys, coding frameworks, or lacked a clearconnection to graph neural networks. We also ignored works that only applied Graph Convolutional Network(GCN) methods to other field of science or used time-dependent or spatial data, especially in the field ofchemistry. The reason for this was that the feature selection approach used later was not directly applicableto such work, or at best uninformative. Additionally, publications applying methods to very specific datasets were not included in our list. Some well-known papers, such as GraphSAINT, were not included because they did not appear under thesearch term used. This absence also explains why SAGN+SLE, SGC, and GraphSAGE do not appear inthe list. To better cover the field of graph neural network research, additional search terms like graphconvolutional network would be necessary. The subsequent changes in criteria led to a high rate of skippedpublications in the full list, which was generated end of May 2023. : All considered publications with indication on survey status. The indicators are as follows:i - included in survey, e - excluded because experiment is not available,d - excluded because method is only applied on unusual or specially build graph data sets,s - skipped because of time constraints.",
  "BReproducibility Context": "The data collected during the successful reproducibility attempts in can be summarized in aFormal Context (Ganter & Wille, 1997), where the papers make up the object set and the analyzed featuresof reproducibility the attribute set. A cross for paper p and feature f means that this feature was observedfor the paper. Based on the definitions of the features, this indicates a negative aspect of reproducibilityoccurring in the paper.",
  "CPresentation of other Experiments": "Here we want to include figures presenting the results from the influence experiments not yet presentedin detail.The diagrams are structured the same way as the ones presented in Subsection 5.4 For eachexperiment we first show the normalized distribution of normalized intrinsic dimensionality for the useddata sets (after preprocessing). For data sets with more than 105 samples, an algorithm for calculatingthe approximated NID is used. Additionally a second figure presents the accuracy/f1 scores obtained whentraining on the feature reduced data sets. For a more detailed explanation see description accompanying. Some data sets have so few features that the steps of the discarding process are smaller than one feature.This leads to fewer data points, which in turn give the impression of only partially complete graphs forvisualizations of normalized distribution of the NID or accuracy for given discarding proportions as thenecessary granularity can not be achieved (see ).",
  ": Experiment based on R-GCN": "The DiffPool publication used only two data sets, of which only one, namely the enzymes data set, hasfeatures for the graph nodes. Therefore we were not able to apply the described method to the other dataset. Furthermore we encountered another problem during the DiffPool experiments (see ). It isstrongly implied in the paper that the method uses the node features in its computations.However, aclose examination of the source code reveals that in the default configuration, the node features are builtfrom the classification targets of the associated graph. By changing the corresponding parameter in thetraining script to a different argument, which we decided on the basis of which preprocessing modificationsit induced, no improvements could be observed. On the contrary, the overall performance of the method gotworse. Nevertheless, we present the results obtained, which again show, that the DiffPool method does notuse the node features in a comprehensible way."
}