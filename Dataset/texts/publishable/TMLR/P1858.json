{
  "Abstract": "Computational topology recently started to emerge as a novel paradigm for characterisingthe shape of high-dimensional data, leading to powerful algorithms in (un)supervisedrepresentation learning. While capable of capturing prominent features at multiple scales,topological methods cannot readily be used for Bayesian inference. We develop a novelapproach that bridges this gap, making it possible to perform parameter estimation in aBayesian framework, using topology-based loss functions. Our method affords easy integrationinto topological machine learning algorithms. We demonstrate its efficacy for parameterestimation in different simulation settings.",
  "Introduction": "Topological machine learning methods enable describing the shape of data at multiple scales while remainingimpervious to many different types of noise. This led to strong hybrid models that combine geometryand topology in different domains, including computer vision (Hu et al., 2019; Waibel et al., 2022), graphlearning (Horn et al., 2022; Yan et al., 2022; Zhao et al., 2020), time series analysis (Zeng et al., 2021), andrepresentation learning (Carrire et al., 2020; Moor et al., 2020). Despite the advantageous properties of suchintegrations, topological methods cannot be readily integrated into Bayesian inference frameworks. However,the need to deal with topological information following Bayesian paradigms frequently occurs in real-worlddata analysis tasks, for example whenever the shape of data is informative for the underlying inference task,or in cases where either uncertainty quantification is crucial or available data is not sufficiently large forapproaches based on neural networks. In such scenarios, assuming a known prior distribution p() andreal-world data y, one could employ Bayes formula to obtain the posterior distribution p(|y) = p(y | )/p(y)p().The posterior distribution offers multiple ways to gain point estimators of the parameter, and allows for acomprehensive description of parameter uncertainties. Regrettably, the application of this formulation tocomplex generative processes presents substantial challenges, because the likelihood function p(y | ), andconsequently, the marginal likelihood p(y) =p(y | )p()d, frequently become analytically intractable.Moreover, the complexity in such systems requires methods that capture structural properties that areinvariant to isometries (such as rotations and translations), making computational topology a well-suitedtool (Hensel et al., 2021). Topology-driven Bayesian computation needs to satisfy two key requirements,namely (i) it must effectively utilise topological descriptors in the observed data y, and (ii) it should offer atheoretically sound notion of posterior beliefs. Both criteria are critical for overcoming the limitations oftopological machine learning methods in complex scenarios and enhancing their applicability to real-worlddata analysis tasks, when the available amount of data is too small to utilise neural networks (e.g. due to",
  "(a)(b)": ": Given a point cloud (a), persistent homology lets us obtain shape descriptors known as persistencediagrams (b). Due to their complex nature, the space of persistence diagrams is not directly amenable to aBayesian treatment. We overcome this limitation by using loss functions that characterise differences betweenobserved and simulated samples. expensive simulations), or when uncertainty quantification is important. To effectively articulate our posteriorbeliefs, we describe the scenario of this paper: In the absence of a likelihood function, access to informationon the parameter is solely available through samples from generative simulation models, which we denoteby x p(x | ). Moreover, we do not assume p(x | ) to be differentiable in , preventing parameter inferencevia neural networks. This assumption is crucial for a plethora of chaotic models such as collective motion,since such systems often exhibit tipping points that lead to highly non-continuous behaviour. Instead, weassume that we have access to a loss function of the form (x, y) (x, y), which measures to what extent agenerated sample x is close to the observed data y. In the general setting as outlined above, we demonstratethat the distribution ( | y) =exp((y,x))p(x | )p()dx/exp((y,x))p(x | )p()dxd offers an alternative forencapsulating the inherent uncertainty in such generative models. The fundamental challenge with this approach is to find a loss function that is suitable for the inferenceproblem at hand. While traditional approximate Bayesian computation (ABC) often makes use of lossesbased on summary statistics derived from the given data, such statistics are not straightforward to define,and usually far from being sufficient. By contrast, geometrical losses like the Hausdorff distance often do notcapture the necessary information in order to perform accurate inference. As the central contributions of this paper, we (i) leverage Bayesian inference and topology via efficientgeometricaltopological loss terms that are capable of handling different data modalities, (ii) show that thecomparison-based posterior, which we define in , can be seen as the solution to a generalised Bayesianoptimization problem, (iii) demonstrate how sampling algorithms can leverage topologically-informed lossfunctions to perform Bayesian inference, and (iv) we demonstrate the utility of our topology-aware approachon several complex generative models. Since our approach is Bayesian by nature, prior knowledge about theparameters to be inferred can readily be integrated into the workflow, making our workflow interpretable incomparison to black-box models. In a nutshell: Our framework fits into the realm of simulation-based inference, where an exact likelihoodfunction may not be determined, but with a simulating model being accessible in order to sample datafrom the likelihood. The main novelty consists of incorporating topological information into parameterinference and showing its utility for parameter estimation in complex systems.",
  "Background: Computational Topology": "Before we introduce our approach, we first provide a brief overview of relevant methods from computationaltopology. Such methods recently emerged, making use of geometricaltopological properties of data todescribe their overall shape. The concept of persistent homology is of particular interest in the context ofmachine learning since it leads to efficient descriptors of structured and unstructured data while at the sametime satisfying invariance and robustness guarantees (Hensel et al., 2021). Persistent homology obtains multi-scale geometricaltopological shape descriptors of data by means ofcombinatorial data structures. Given a metric such as the Euclidean distance, persistent homology assignsa family of shape descriptors, the persistence diagrams, to point clouds (Edelsbrunner & Harer, 2010);",
  "Published in Transactions on Machine Learning Research (11/2024)": ": Left to right: samples of the proposed percolation model for p = 0.15, 0.3 and 0.6, respectively.All samples admit a fixed maximum greyscale value of 50. Although the geometric distance between twosamples of fixed p can be large due to the uniform sampling of pixels, the overall topological structure (whichis captured by persistent homology) for a given p is more stable with respect to sampling.",
  "Bayesian Inference": "Within the Bayesian framework, unknown quantities, such as parameters denoted by , are furnished with aprobability distribution p(), referred to as the prior distribution. Subsequent inference about the parameters given observed data y is performed via the Bayesian update procedure. This process involves adjustingthe prior distribution p() through the multiplication by the ratio of the likelihood to the evidence, i.e.p(|y) = p(y|)/p(y)p(). However, applying this procedure in our particular context presents notable challenges.The primary issue stems from the fact that the likelihood function p(y | ) is typically unknown or analyticallyintractable, thus posing considerable difficulties for estimation, particularly when the data exhibits intricategeometricaltopological properties. To circumvent this problem, recent research has shown that the use of thelikelihood function of the data can be avoided, producing so-called generalised Bayesian posteriors (Bissiriet al., 2016). One approach to construct such a generalisation to the Bayesian posterior can be found throughthe lens of viewing Bayesian inference as the solution to an optimisation problem (Csiszr, 1975; Donsker 1For a more in-depth discussion of computational topology, in particular in the context of machine learning, we refer thereader to Hensel et al. (2021).2A high-level explanation of homology groups is contained in the appendix.",
  "Comparison-Based Posteriors": "Uncertainty estimates predicated on distance functions have a long-standing history in Bayesian statistics asa means of approximating posterior distributions, with notable use in the realm of approximate Bayesiancomputation (ABC) (Tavar et al., 1997; Pritchard et al., 1999; Beaumont et al., 2002). In the ensuingdiscussion, we will illustrate that a modification of the aforementioned approach, specifically, a generalisedposterior from ABC such as Schmon et al. (2021), can furnish us with a theoretically sound form of posteriorupdate. The proposed approach involves using a loss function (x, y) (y, x) to construct a joint posteriorinvolving simulated data x p(x | ) and real data y. It is of the form",
  "where the arg min ranges over all q P( X)": "The proof can be found in the appendix. In Eq. (7), the first term in the objective function measures theexpected loss under the posterior, where the loss function (y, x) quantifies the discrepancy between theobserved data y and the simulated data x. This is precisely where we can use improved inductive biases ordomain knowledge and employ loss functions such as the topological losses introduced in .4. Thesecond term encourages the posterior to stay close to the prior, as in the traditional Bayesian setting, however,with a significant difference: the prior distribution p() over the parameters together with the simulationmodel p(x | ) naturally implies prior beliefs over what the data should look like. Thus, it is sensible toconsider the KL-divergence between the joint beliefs p(x | )p() and q(x, ). Proposition 3.1 shows that the comparison-based posterior of Eq. (4) is in fact a solution to the respectiveoptimisation problem (which is the analogue of Eq. (1) in our setting). The motivation is therefore touse comparison-based posteriors for approximate inference under the assumption that an appropriate lossfunction is available. Proposition 3.1 does not necessitate a topological loss function and can be consideredindependently of a specific loss function.However, in particular, it permits incorporating topologicalinformation via topological losses. To the best of our knowledge, the argument and proof that comparison-based posteriors can be seen as thesolution to a generalised Bayesian optimization problem is new. While distance-based loss functions, wherea parameter is only available through simulation have been used before, they are commonly consideredapproximations of an idealised loss function",
  "i=1(y, xi).(8)": "Thus, Proposition 3.1 demonstrates that comparison-based posteriors do not have to be seen as a suboptimalapproximation to an abstract ideal, but can be seen as a valid generalised posterior in their own right. As such, our approach can be seen as a generalisation of the traditional Bayesian posterior in the sensethat it allows for a more flexible modelling of the data generation process. Instead of assuming that theobserved data is generated exactly according to the model p(y | ), we allow for the possibility that the datais generated from a distribution that produces data close to y in terms of the loss function . This makesour approach potentially more robust to model misspecification and more suitable for complex data withintricate topological properties.",
  "Inference for Comparison-Based Posteriors": "Evaluating posteriors such as those given by Eq. (5) analytically is typically infeasible. An alternativeapproach may involve direct targeting of a variational distribution as shown in Eq. (7). However, this strategyencounters two main difficulties: firstly, the distribution p(x | ) is often intractable, rendering the computationof the KL divergence term non-trivial. Secondly, in order to compute the gradients, we must propagate themthrough the simulation models. This poses a challenge as p(x | ) often represents a black-box model, notnecessarily implemented within a framework that supports automatic differentiation. In fact, many complexsimulation models are not even differentiable in the parameters to be optimised. Additionally, such algorithmstypically require large amounts of data, which is a frequent limitation when simulations are computationallyexpensive. Consequently, this typically precludes the practical application of gradient-based optimisationtechniques. However, it turns out that Monte Carlo algorithms, such as self-normalised importance samplingor so-called pseudo-marginal Markov chain Monte Carlo (Andrieu & Roberts, 2009) are able to computeexpectations with respect to posteriors of the form in Eq. (5).",
  "approachesh()e(y,x)p(x | )p()dxde(y,x)p(x | )p()dxd= E( | y)[h()](10)": "as n . An alternative is to use Markov chain Monte Carlo (MCMC), where we produce a Markov chainconverging to the distribution (5); see Algorithm 2 in the appendix. Such algorithms indeed converge to thedesired target distribution (Andrieu & Roberts, 2009, Theorem 1) under mild assumptions. A high-leveloverview of pseudo-marginal MCMC is contained in the appendix.",
  "Losses Based on Geometry & Topology": "Having introduced multiple inference algorithms and explained how to obtain topological features, we nowdiscuss how to derive topology-based loss functions. We start with discussing metrics in computationaltopology. It turns out that persistence diagrams can be endowed with a metric by using optimal transport.Given two diagrams D and D containing features of the same dimensionality, their pth Wasserstein distanceis defined as",
  "p,(11)": "where () denotes a bijection. Since D and D generally have different cardinalities, we consider them tocontain an infinite number of points of the form (, ), i.e. tuples of zero persistence; this is akin to requiringeach diagram to contain the projections of points to the diagonal, originating from the other diagram. Asuitable () can thus always be found. Solving Eq. (11) is practically feasible using modern optimal transportalgorithms (Flamary et al., 2021). While this is the most principled approachin the sense that such a lossformulation forms a proper metric in the mathematical sensealternative formulations to Eq. (11) exist, andour framework is fundamentally compatible with all of them. Along these lines, other topological descriptors,such as Betti curves (Rieck et al., 2020a) or persistence images (Adams et al., 2017), might be used insteadof persistence diagrams. These descriptors are often easier to compute and afford the use of fast L1 or L2distances as proxies for Eq. (11). For instance, the L2 distance between persistence images is known toshare some advantageous properties with the Wasserstein distance (Adams et al., 2017) but it only handles adiscretised version of the data, so some information is invariably lost (or, to briefly consider an optimisticpoint of view, the persistence image is smoother than a persistence diagram). As another alternative to thepreviously-described descriptors, we could also employ a kernel, i.e. a similarity measure between persistencediagrams. These similarity measures are not metrics in the mathematical sense, lacking the requirement ofthe identity of indiscernibles, but are easier to compute (unlike the persistence images, they do not requirevectorisations) and require fewer parameters (Kwitt et al., 2015; Reininghaus et al., 2015).",
  "T (y, x) := Wp(Dy, Dx).(12)": "This formulation applies to different data modalities. If the data can be represented as a point cloud, wecalculate persistence diagrams from the VietorisRips complex. For (greyscale) image data, we obtainpersistence diagrams via cubical complexes, enabling topological feature calculations for more complicatedmodalities such as MRI data Rieck et al. (2020b). As we outlined above, other choices for the loss functionwould be possible, and we may now continue with our Bayesian updating procedure. Notice that unlessotherwise mentioned, we use p = 2 to compute the topological loss. Moreover, we note that the implementation of giotto-tda (Tauzin et al., 2020), which we use in our experi-ments, uses a faster approximate algorithm rather than the actual loss. Under the hood, this implementationuses an auction algorithm with a worst-case computational complexity of On m log1, with n and mbeing the number of points in the respective persistence diagrams and being a precision parameter. Properties.The Wasserstein distance between persistence diagrams is stable in the sense that the geometricdistance is an upper bound of the topological distance. Given two point clouds Y, X of the same cardinality,we haveWp(Dy, Dx) Cp Wp(Y, X),(13) where Cp refers to a constant that only depends on p and Wp(Y, X) is defined similarly to Eq. (11), butcalculated on the point clouds themselves Skraba & Turner (2022). Since Wp(, ) Wp(, ) for p p, lowvalues of p are desirable in terms of stronger stability; we find that p = 2 provides a suitable compromisesolution. Moreover, our loss function T is invariant under isometries (Edelsbrunner & Harer, 2010) andstable under subsampling (Moor et al., 2020). Eq. (13) can be interpreted as follows: once X and Y aregeometrically close, their corresponding topological properties are similar as well. However, the converse isnot true in general since structural similarity with respect to topological properties may occur between twodatasets although they are not close geometrically. In fact, the left-hand side of Eq. (13) is often substantiallysmaller than its right-hand side, which serves as the initial motivation to use topological losses instead oftheir geometrical counterparts, particularly for inference tasks in which the shape of data is informative forthe underlying objective. We validate this theoretical observation empirically in , by showing thattopology encoded in losses indeed lead to more accurate parameter inference. A loss function based on the Hausdorff distance.As a baseline and computationally simpler comparisonpartner, we also define a geometry-based loss function based on the Hausdorff distance between point cloudsX and Y. Given a metric space (M, d) and two non-empty subsets X, Y M, we define our loss as",
  "G(X, Y) := inf{ 0 | X Y, Y Y},(14)": "where X = xX{m M; d(m, x) } denotes the -thickening of X in M. While this loss does notsatisfy invariance properties, it is more efficient to compute in practice. However, as we will see in theexperimental section, its utility in complex data generation scenarios is limited. For |X| = n and |Y | = m,the computational complexity of the Hausdorff distance is O(n m).",
  "Experiments": "We empirically validate the utility of our proposed method in the form of three main experiments thatcompare topological losses and respective geometrical losses, by using the results from . In thefirst two experiments, we focus on parameter inference for chaotic simulation models, which can be seenas (approximately) solving an inverse problem. Notice that these are well-suited scenarios for applyingtopology-driven Bayesian methods, since chaotic models are known to admit likelihood functions that areneither analytically tractable (due to their complexity) nor differentiable (due to the presence of tipping points,for instance), thus precluding the use of both neural networks and traditional approaches to Bayesian inference.Moreover, our method requires only a very small number of simulations (here, 250) to perform reasonably,making our approach superior to neural networks with regards to the required number of data points (we findthat this is particularly relevant in setting where the simulation is (computationally) expensive). Furthermore,for each run of the experiments, we use 100 observation points for inference, and normal distributions for boththe prior and the proposal distribution in the MCMC procedure. The weight parameter w in the importancesampling procedure is set to 10. The third experiment involves a synthetic model that shows how our methodmay be used for inference in the context of imaging data. In all of our experiments, topological losses performsignificantly better than a respective geometrical loss. Finally, we compare our results to standard summarystatistics that are frequently used in the context of approximate Bayesian computation, see Appendix A.Remarkably, our topology-driven approach is observed to perform significantly better than Lp-norms onsummary statistics for the first two non-synthetic experiments, underpinning our proposed methods utility",
  "Vicsek Model: Swarm Behaviour": "The Vicsek model is an agent-based model that is particularly useful to study collective motion like swarmingand flocking. The underlying idea is that each agent aligns at any time step with its nearest neighbours,before the alignment is perturbed by an additional random term. The resulting angle of the alignmentdetermines the direction of movement for the subsequent time step, where the movement takes place withconstant speed which is independent of time and individual. Following Vicsek et al. (1995), the formula forthe alignment angle update of an individual i is given by (t + 1) = (t)r + , where (t)r denotes theaverage of the angles of individuals with distance to agent i at most r, and is uniformly sampled from theinterval [/2, /2] for 0 2. The new position of the agent is then determined by moving with constantspeed towards the direction of the updated angle. Notice that although the position of each agent can beexpressed analytically, the dependencies between agents leads to highly complex behaviour of the overallsystem over time, which cannot be expressed analytically. We therefore are in the proposed setting of anintractable likelihood, necessitating a simulation-based approach to inference. To demonstrate the utility of topological losses for parameter inference in a setting where the respectivelikelihood function is not analytically tractable, we infer the noise parameter after a certain number ofiterations (time steps) of the model. We distribute 2000 agents across a square of edge length L = 25, whereopposite edges of the square are identified with the orientation being preserved; the world thus constitutesthe surface of 2-torus. For the inference of , we use 250 simulations for each sampling method, aiming toinfer after 5, 10, and 50 iterations (time steps) of the model, respectively, repeating the inference 5 times.Since the Vicsek model gives rise to point clouds, we calculate VietorisRips complexes and compare thecorresponding persistence diagrams. As a baseline, we use the Hausdorff distance between the point clouds; depicts the results. In our experiments, the results for the topological loss with the MCMC samplingprocedure outperforms in all of the settings, and leading to highly accurate estimates even after 50 time steps.By contrast, we note that the estimators that were calculated with the Hausdorff loss lead to particularlypoor results in the regimes that are closer to being ergodic, i.e. for larger values of . Finally, we remark thatour approach with the proposed topological loss outperforms standard rejection sampling in combinationwith summary statistics; see Appendix A.",
  "(b) MCMC sampling": ": Summary of parameter estimates for the parameter p of the Percolation model, using 5 repetitionsof the inference procedure. Each subplot depicts estimates for p {0.15, 0.30, 0.60}, respectively. The trueparameter is shown as a solid black line. Estimates based on the geometrical loss G are less accurate thanthe topological loss T , in particular for larger number of iterations. power for inference). Therefore, at least in a large-pixel regime we can infer the true parameter of theobservation from the mean of its pixel values. Note that this is due to the additional structure in thisexperiment, and does not hold for complex systems that cannot be described in such a simple way, as hasbeen seen in the previous experiments. Finally, rejection sampling with the standard deviation statistic leadsto highly erroneous results, as can also be seen from . For complex systems it is therefore very difficultto determine the right summary statistic which contains a sufficient amount of information to infer theunderlying parameter. This choice is not necessary when using topological losses, which makes the latter anappropriate generic choice, in many applications.",
  "Fluid Dynamics: Lattice Boltzmann Simulation": "The lattice Boltzmann methods (LBM) are a collection of algorithms that are used for fluid simulation.Instead of the conservation of macroscopic properties of the dynamical system, LBM models the fluid overa discrete lattice, by performing local propagation and collision processes, for all particles simultaneously.For a thorough introduction, see Krger et al. (2017). After initialising the state of each particle equally, aportion of normally distributed noise is added to each state, independently. This portion of randomness iscontrolled by a global parameter 0, where higher values of correspond to a higher portion of noise inthe initialisation. The crucial observation is that the degree of turbulence in the system can be modelled by",
  "TImportance sampling0.15 0.000.29 0.000.59 0.01MCMC0.15 0.020.29 0.000.59 0.00": "varying , as is shown in . Our objective is therefore to infer the parameter , which corresponds to agiven observation after a certain amount of iterations in the model, where we fix this number of iterations tobe t = 3000 time steps. As the topological loss, we use the Wasserstein distance between the persistencediagrams of the cubical complexes corresponding to the observed and the simulated state, respectively, whilethe Hausdorff distance between the states serves as a baseline comparison. shows the results; weobserve that the topological loss together with importance sampling outperforms the geometrical loss inboth configurations, by far. Again, rejection sampling in combination with summary statistics performs lessreliably; see Appendix A.",
  "Percolation Model: Multi-Scale Structures": "In statistical physics, percolation refers to the behaviour of a network when links are added to it. Percolationcan be viewed as a process of geometric phase transition since at a critical state of the network, disconnectedcomponents merge into large connected clusters by the addition of a small fraction of links. In our setting,the percolation model is probabilistic in the sense that links are added randomly, with some probability p,and the goal is to infer p. We realise such a network as a square 2D greyscale image of pixel size n2, for somepositive integer n.4 We assign a value v to each pixel in the image, where v = 0 with probability 1 p, and vis sampled uniformly from the set {1, . . . , vmax} with probability p. Here, vmax is the maximum realisablegreyscale value, which we set to 50 for our experiments. For the inference of p, we used 250 simulations forboth the importance sampling and the MCMC sampling. Since we are dealing with greyscale images, weobtain persistence diagrams via cubical complexes. We compare our proposed topological loss T to several other losses that are commonly used in imagingprocessing, including MSE, RMSE, the universal image quality index (UQI), the relative average spectralerror (RASE), the spatial correlation coefficient (SCC), and the pixel-based visual information fidelity (VIFP).Since we find that the results of SCC and VIFP outperform other scores, and since SCC and VIFP lead to 4Note that alternatively, a greyscale image can be interpreted as an undirected graph, by defining the set of vertices to be theset of pixels, and adding edges between neighbouring pixels if both of their values are non-zero.",
  "Discussion": "We presented a novel Bayesian approach that allows for leveraging topological information in parameterestimation. Our method, which builds on the framework of generalised Bayesian inference, introduces atopology-based loss function into the construction of the posterior distribution. The resulting methodology isparticularly suited for scenarios in which one has to deal with a black-box likelihood function, i.e. wherethe likelihood is only accessible through simulations. This approach offers a theoretically grounded form ofposterior belief that improves upon some of the challenges inherent in existing distance-based methods, whichrely on geometrical features. The main limitation of our approach is that, although computational topologyappears to be the appropriate tool for certain complex models, the resulting performance for parameterestimation highly depends on the setting (Lueckmann et al., 2021). A careful assessment of the given scenariois recommended beforehand. We restrict ourselves to scenarios with low-dimensional parameter spaces in thiswork, and leave the investigation of high-dimensional parameter spaces for future work. Moreover, we donot claim that our methodology outperforms benchmark Bayesian approaches under the assumption of aknown likelihood function. However, we have empirically shown that our method outperforms geometricalapproaches and summary statistics for parameter estimation in the setting of complex systems, which suggestsfuture applications in other complex scenarios arising from life sciences data, for example. Our work contributes to the ongoing evolution of Bayesian inference methods for complex models. It presentsa new avenue of exploration that merges the advantages of both topological data analysis and Bayesianstatistics. We believe that the principles and techniques introduced in this paper have broad applicabilityand offer a compelling new approach for uncertainty quantification in a wide range of data analysis tasks.From a machine learning perspective, the main novelty of this paper is to introduce a framework that fusestopology and Bayesian inference for scenarios in which the application of gradient-based approaches likeneural networks constitutes a major obstacle. We anticipate future research to focus on further refining thismethod, specifically when it comes to calculating efficient estimates in sparse regimes. Moreover, we plan onassessing the performance arising from other topology-based formulations, which are geared towards specificmodalities like meshes (Turner et al., 2014) or time series (Zeng et al., 2021).",
  "Paul Bendich, J. S. Marron, Ezra Miller, Alex Pieloch, and Sean Skwerer. Persistent homology analysis ofbrain artery trees. Annals of Applied Statistics, 10(1):198218, 2016": "Espen Bernton, Pierre Jacob, Mathieu Gerber, and Christian Robert. Approximate Bayesian computationwith the Wasserstein distance. Journal of the Royal Statistical Society: Series B (Statistical Methodology),81(2):235269, 2019. Pier Giovanni Bissiri, Chris C Holmes, and Stephen G Walker. A general framework for updating beliefdistributions. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 78(5):1103, 2016. Mathieu Carrire, Frdric Chazal, Yuichi Ike, Theo Lacombe, Martin Royer, and Yuhei Umeda. PersLay:A neural network layer for persistence diagrams and new graph topological signatures. In Proceedingsof the 23rd International Conference on Artificial Intelligence and Statistics (AISTATS), volume 108 ofProceedings of Machine Learning Research, pp. 27862796. PMLR, 2020.",
  "Herbert Edelsbrunner and John Harer. Computational topology: An introduction. American MathematicalSociety, Providence, RI, USA, 2010": "Rmi Flamary, Nicolas Courty, Alexandre Gramfort, Mokhtar Z. Alaya, Aurlie Boisbunon, StanislasChambon, Laetitia Chapel, Adrien Corenflos, Kilian Fatras, Nemo Fournier, Lo Gautheron, Nathalie T.H.Gayraud, Hicham Janati, Alain Rakotomamonjy, Ievgen Redko, Antoine Rolet, Antony Schutz, VivienSeguy, Danica J. Sutherland, Romain Tavenard, Alexander Tong, and Titouan Vayer. POT: Pythonoptimal transport. Journal of Machine Learning Research, 22(78):18, 2021.",
  "Max Horn, Edward De Brouwer, Michael Moor, Yves Moreau, Bastian Rieck, and Karsten Borgwardt.Topological graph neural networks. In International Conference on Learning Representations (ICLR), 2022": "Xiaoling Hu, Fuxin Li, Dimitris Samaras, and Chao Chen. Topology-preserving deep image segmentation. InH. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch-Buc, E. Fox, and R. Garnett (eds.), Advances inNeural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. Jeremias Knoblauch, Jack Jewson, and Theodoros Damoulas. An optimization-centric view on bayes rule:Reviewing and generalizing variational inference. Journal of Machine Learning Research, 23(132):1109,2022.",
  "Timm Krger, Halim Kusumaatmaja, Alexandr Kuzmin, Orest Shardt, Goncalo Silva, and Erlend MagnusViggen. The lattice Boltzmann method. Springer International Publishing, 10(978-3):415, 2017": "Roland Kwitt, Stefan Huber, Marc Niethammer, Weili Lin, and Ulrich Bauer. Statistical topological dataanalysis - A kernel perspective. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett(eds.), Advances in Neural Information Processing Systems 28, pp. 30703078. Curran Associates, Inc.,2015. Jan-Matthis Lueckmann, Jan Boelts, David Greenberg, Pedro Goncalves, and Jakob Macke. Benchmarkingsimulation-based inference. In International Conference on Artificial Intelligence and Statistics, pp. 343351.PMLR, 2021. Vasileios Maroulas, Cassie Putman Micucci, and Farzana Nasrin. Bayesian topological learning for classifyingthe structure of biological networks. Bayesian Analysis, 17(3):711736, 2022. doi: 10.1214/21-BA1270.",
  "Mijung Park, Wittawat Jitkrittum, and Dino Sejdinovic. K2-ABC: approximate Bayesian computation withkernel embeddings. arXiv preprint arXiv:1502.02558, 2015": "Jonathan K Pritchard, Mark T Seielstad, Anna Perez-Lezaun, and Marcus W Feldman. Population growthof human Y chromosomes: a study of Y chromosome microsatellites. Molecular biology and evolution, 16(12):17911798, 1999. Jan Reininghaus, Stefan Huber, Ulrich Bauer, and Roland Kwitt. A stable multi-scale kernel for topologicalmachine learning. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 47414748, Red Hook, NY, USA, June 2015. Curran Associates, Inc. doi: 10.1109/CVPR.2015.7299106. Bastian Rieck, Filip Sadlo, and Heike Leitte.Topological machine learning with persistence indicatorfunctions. In Hamish Carr, Issei Fujishiro, Filip Sadlo, and Shigeo Takahashi (eds.), Topological Methodsin Data Analysis and Visualization V, pp. 87101. Springer, Cham, Switzerland, 2020a. doi: 10.1007/978-3-030-43036-8_6. Bastian Rieck, Tristan Yates, Christian Bock, Karsten Borgwardt, Guy Wolf, Nick Turk-Browne, andSmita Krishnaswamy. Uncovering the topology of time-varying fMRI data using cubical persistence. InH. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Advances in Neural InformationProcessing Systems, volume 33, pp. 69006912. Curran Associates, Inc., 2020b.",
  "Tams Vicsek, Andrs Czirk, Eshel Ben-Jacob, Inon Cohen, and Ofer Shochet. Novel type of phase transitionin a system of self-driven particles. Physical Review Letters, 75(6):1226, 1995": "Hubert Wagner, Chao Chen, and Erald Vuini. Efficient computation of persistent homology for cubical data.In Ronald Peikert, Helwig Hauser, Hamish Carr, and Raphael Fuchs (eds.), Topological Methods in DataAnalysis and Visualization II: Theory, Algorithms, and Applications, pp. 91106. Springer, Heidelberg,Germany, 2012. doi: 10.1007/978-3-642-23175-9_7. Dominik J. E. Waibel, Scott Atwell, Matthias Meier, Carsten Marr, and Bastian Rieck. Capturing shapeinformation with multi-scale topological loss terms for 3d reconstruction.In Linwei Wang, Qi Dou,P. Thomas Fletcher, Stefanie Speidel, and Shuo Li (eds.), Medical Image Computing and Computer AssistedIntervention (MICCAI), pp. 150159, Cham, Switzerland, 2022. Springer. doi: 10.1007/978-3-031-16440-8_15. Zuoyu Yan, Tengfei Ma, Liangcai Gao, Zhi Tang, Yusu Wang, and Chao Chen. Neural approximation ofgraph topological features. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh (eds.),Advances in Neural Information Processing Systems, volume 35, pp. 3335733370. Curran Associates, Inc.,2022.",
  "Arnold Zellner. Optimal information processing and Bayess theorem. The American Statistician, 42(4):278280, 1988": "Sebastian Zeng, Florian Graf, Christoph Hofer, and Roland Kwitt. Topological attention for time seriesforecasting. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan (eds.),Advances in Neural Information Processing Systems, volume 34, pp. 2487124882. Curran Associates, Inc.,2021. Qi Zhao, Ze Ye, Chao Chen, and Yusu Wang. Persistence enhanced graph neural network. In Proceedingsof the 23rd International Conference on Artificial Intelligence and Statistics (AISTATS), volume 108 ofProceedings of Machine Learning Research, pp. 28962906. PMLR, 2020.",
  "In the interest of reproducibility and to improve the understanding of our method, we also provide moredetailed insights into our experiments.6": "Synthetic data parameter estimates.As a consistency check and illustrative example, we use ourproposed methods for inferring the radius parameters of two objects, namely the 2-sphere and the 2-torus.The 2-sphere is the surface of a closed 3-dimensional ball of radius r, while the 2-torus is constructed usingan inner radius r and an outer radius R. The point clouds were sampled uniformly from the respectivesurface of known parameter. depicts the results of the inference process. We observe that both types of losses are capable ofinferring the right parameters of these simple geometric objects, with our topology-based loss T providingslightly more reliable estimates in the case of a torus. shows the raw values of learning radiusparameters for synthetic data sets (2D spheres and 2D tori). This experiment primarily shows that for suchsimple shapes, geometry-based and topology-based losses perform equivalently. For more complex data sets,however, we find that the improved robustness of our topology-based loss term helps in inferring the groundtruth parameters.",
  "(a) Rejection sampling": ": Summary of parameter estimates for the parameter p of the Percolation model with respect tosummary statistics and a standard rejection sampling procedure, using 5 repetitions of the inference procedure.Each subplot depicts estimates for p {0.15, 0.30, 0.60}, respectively. The true parameter is shown as a solidblack line. Estimates based on the mean statistic L2 loss are more accurate than the standard deviationstatistic L2 loss.",
  "Topological distanceImportance sampling0.160.310.61MCMC estimate0.210.240.59": "Lattice Boltzmann model for fluid dynamics.The degree of randomness in the local propagation andcollision steps in the lattice Boltzmann method (LBM) model controls the turbulent behaviour of the globalsystem, as is illustrated in . This is the parameter that we estimate in our experiments. The resultsare shown in and , respectively. Using importance sampling, the topological loss outperforms",
  "Rejection sampling": ": Summary of parameter estimates for the parameter of the fluid dynamics model with respectto summary statistics and a standard rejection sampling procedure, using 5 repetitions of the inferenceprocedure. Each subplot depicts estimates for {0.20, 0.30, 0.40}, respectively. The true parameter isshown as a solid black line. Estimates based on the mean statistic L2 loss and on the standard deviationstatistic L2 loss tend to be less reliable than estimates based on the topological loss, see .",
  "andZ :=exp ((y, x)) p(x | )()dxd": "Homology in a nutshellHomology groups are a fundamental tool in algebraic topology, providingalgebraic representations of the topological structure of a space. Given a topological space K, the homologygroup Hd(K) captures the d-dimensional topological features of the space, such as connected components,loops, and voids. At a high level, the homology group Hd(K) encodes the d-dimensional holes of the space.H0(K) represents the number of connected components (0-dimensional holes). H1(K) captures loops or1-dimensional holes, such as circular paths that do not bound any region. Higher-dimensional homologygroups Hd(K) with d 2 detect higher-dimensional voids, such as cavities inside the space. Homology groupsare defined through an algebraic process. The space K is decomposed into simpler components, such assimplices in the case of simplicial complexes. Chains are formed as formal combinations of these components,and boundary operators are introduced to map a d-dimensional simplex to its (d 1)-dimensional boundary.The homology group Hd(K) is then given by the quotient:",
  "Hd(K) = ker(d)/im(d+1),": "where d denotes the boundary operator at dimension d. Intuitively, elements of Hd(K) are equivalenceclasses of d-dimensional cycles (chains with no boundary) in K that are not boundaries of higher-dimensionalobjects. In summary, the homology group Hd(Ki) encodes critical information about the d-dimensionalfeatures of a space K, allowing to analyze and compare spaces using algebraic methods. Pseudo-marginal MCMC in a nutshellPseudo-marginal Markov Chain Monte Carlo (MCMC) methodsare a class of algorithms used for sampling from a posterior distribution when the likelihood function isintractable or expensive to compute. In such scenarios, the exact likelihood is replaced by an unbiased estimator,which introduces randomness into the acceptance ratio of the Metropolis-Hastings (MH) algorithm. Despite thisrandomness, pseudo-marginal MCMC methods can still produce samples from the correct target distribution.In Bayesian inference, one often needs to sample from a posterior distribution ( | y) ()p(y | ), where() is the prior, and p(y | ) is the likelihood function. In many practical cases, such as when dealing withcomplex models, the likelihood p(y | ) may not be analytically tractable or computationally feasible toevaluate directly. Pseudo-marginal MCMC provides a solution by replacing the likelihood with an unbiasedestimator p(y | ). This approach retains the correct target distribution ( | y) because the pseudo-marginalmethod treats the noisy estimate p(y | ) as if it were the exact likelihood within the Metropolis-Hastingsframework. The pseudo-marginal MCMC algorithm proceeds similarly to the standard Metropolis-Hastingsalgorithm, with the key difference being that the likelihood ratio in the acceptance probability is replaced bya ratio of likelihood estimators. Specifically, given a current state t, a new state is proposed, and theacceptance probability is calculated as:",
  ",": "where p(y | ) and p(y | t) are unbiased estimates of the likelihood, and q( | t) is the proposal distribution.Crucially, the unbiasedness of the likelihood estimator ensures that, despite the randomness introduced, thechain remains reversible and converges to the correct posterior distribution ( | y). See Andrieu & Roberts(2009) for a thorough introduction to pseudo-marginal MCMC."
}