{
  "Abstract": "We evaluate the video understanding capabilities of existing foundation models (FMs) using acarefully designed experiment protocol consisting of three hallmark tasks (action recognition,temporal localization, and spatiotemporal localization), eight datasets well received by thecommunity, and four adaptation methods tailoring an FM for downstream tasks. Furthermore,we jointly prole FMs ecacy and eciency when adapting to general video understandingtasks using cost measurements during both training and inference. Our main ndings areas follows. First, task-specialized models signicantly outperform the seven FMs studiedin this work, in sharp contrast to what FMs have achieved in natural language and imageunderstanding. Second, video-native FMs, whose pretraining data mainly contains thevideo modality, are generally better than image-native FMs in classifying motion-rich videos,localizing actions in time, and understanding a video of more than one action. Third, thevideo-native FMs can perform well on video tasks under light adaptations to downstreamtasks (e.g., freezing the FM backbones), while image-native FMs win in full end-to-endnetuning. The rst two observations reveal the need and tremendous opportunities toconduct research on video-focused FMs, and the last conrms that both tasks and adaptationmethods matter when it comes to the evaluation of FMs. Our code is released under",
  "Introduction": "Foundation model (FM) is a term coined by Bommasani et al. (2021), referring to any model that is trainedon broad data that can be adapted (e.g., netuned) to a wide range of downstream tasks. Some representativeFMs include but are not limited to BERT (Devlin et al., 2018), GPT-3 (Brown et al., 2020), CLIP (Radfordet al., 2021), and ALIGN (Jia et al., 2021). This work primarily investigates the video understandingcapabilies of seven visual and multimodal FMs: CLIP (Radford et al., 2021), FLAVA (Singh et al., 2022),CoCa (Yu et al., 2022), DINOv2 (Oquab et al., 2023), VATT (Akbari et al., 2021), VideoMAE (Tong et al.,2022), and InternVideo (Wang et al., 2022b). We select these models because they are amendable for thevideo understanding and make their checkpoints accessible to us.",
  "(g) InternVideo": ": Performance of FMs with end-to-end netuning (red) and frozen backbone (blue), in comparisonwith state-of-the-art task-specialized models (black) on VideoGLUE benchmarks. VC-A, VC-M, and VC-MLstand for appearance-focused, motion-focused, and multi-label Video Classication tasks, respectively; TALstands for Temporal Action Localization; STAL stands for Spatiotemporal Action Localization. The highestand lowest performance numbers on each dataset are mapped to 0.9 and 0.1, and the other numbers arelinearly scaled accordingly on the radar chart. We also use gray shades to represent tasks that are morefocused on appearance understanding more than motion. We observe that: (1) FMs generally fall behindtask-specialized models; (2) FMs that are trained with video data are generally better than image-nativeFMs on motion-focused tasks under the frozen backbone setting, and image-native FMs can generally catchup when netuned end-to-end on the target dataset. It is nontrivial to evaluate FMs. In contrast to specialist models developed for a particular task, FMs areconsidered as generalists that learn shareable meta-knowledge across tasks so that one can quickly adaptthem to achieve superior performance on various downstream tasks. Hence, both the tasks and adaptationmethods matter when it comes to the evaluation of FMs. However, the community has not reached a consensuson these two aspects. FM developers select their own dierent sets of downstream tasks interestingly,often covering no video or only appearance-rich video classication tasks (Buch et al., 2022; Lei et al., 2023).Moreover, they rely on distinct adaptation methods, making apples-to-apples comparisons challenging andcausing mismatches with the FMs actual use cases. To this end, we propose to evaluate FMs video understanding capabilities using a carefully designed experimentprotocol, named VideoGLUE, consisting of three hallmark tasks (action recognition, temporal localization,and spatiotemporal localization), eight datasets well received by the research community, and four modeladaptation methods tailoring a foundation model for downstream tasks. The tasks examine an FM fromvarious aspects needed for understanding video. The all-around adaptations represent the main use casesof FMs in the literature and, more importantly, allow us to thoroughly probe an FMs potential in videounderstanding. Why do we specically focus on videos? The main motivation is to promote video understanding in theevaluation of FMs. More concretely, we test the following conjectures through this work. First, FMs highperformance on existing evaluation suites does not necessarily indicate their potential in video since thesesuites either lack video-specic tasks or selectively choose video tasks whose appearance feature is more",
  "Published in Transactions on Machine Learning Research (10/2024)": "Yi Wang, Kunchang Li, Yizhuo Li, Yinan He, Bingkun Huang, Zhiyu Zhao, Hongjie Zhang, Jilan Xu, Yi Liu, ZunWang, et al. InternVideo: General video foundation models via generative and discriminative learning. arXivpreprint arXiv:2212.03191, 2022b. Yi Wang, Kunchang Li, Xinhao Li, Jiashuo Yu, Yinan He, Guo Chen, Baoqi Pei, Rongkun Zheng, Jilan Xu, ZunWang, et al. InternVideo2: Scaling video foundation models for multimodal video understanding. arXiv preprintarXiv:2403.15377, 2024. Jason Wei, Yi Tay, Rishi Bommasani, Colin Rael, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma,Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models. arXiv preprint arXiv:2206.07682,2022.",
  "Related work": "Foundation models.One common type of FMs are Large Language Models (LLMs) trained to acquiregeneric, transferable, and diverse representations that can enable sample-ecient learning and knowledgetransfer across a broad range of downstream tasks. FMs are often trained with simple self-supervised learningobjectives such as predicting the next token in a sentence (e.g., GPT-3 (Brown et al., 2020), PaLM (Chowdheryet al., 2022)), or denoising the masked tokens (e.g., BERT (Devlin et al., 2018), UniLM (Dong et al., 2019),and BEiT (Bao et al., 2021)). An intriguing characteristic of FMs is their ability to gradually acquire newcapabilities as the model grows and the training data size increases, despite being trained on simple learningobjectives (Wei et al., 2022). For example, PaLM (Chowdhery et al., 2022; Anil et al., 2023), a massive LMwith 540 billion parameters, has started to show new capabilities in tasks such as explaining jokes, solvingmath, and performing common-sense reasoning when scaled to over 100B parameters. In addition to self-supervised transformers, FMs in computer vision also encompass transformers specicallytrained to align image-text paired data. These FMs use learning objectives include contrastive learning (e.g.,CLIP (Radford et al., 2021)), denoising masked tokens (e.g., BEiT-3 (Wang et al., 2022a)), predicting thenext token in a single modality (e.g., DALL-E (Ramesh et al., 2021)) or in the interleaved image-text sequence(e.g., Flamingo (Alayrac et al., 2022), Kosmos-1 (Huang et al., 2023)). Recent FMs are also trained on amixture of these objectives (e.g., MAE (He et al., 2022), FLAVA (Singh et al., 2022), and CoCa (Yu et al.,2022)). Our criteria of choosing foundation models to study are primarily based on the denition of FMs,and their amendability on video understanding and accessibility of checkpoints. We leave some models (e.g.,detection, segmentation models) out of the scope of this work, because of their current lack of generalizationon video understanding tasks. Finally we choose seven representative FMs, i.e., CLIP (Radford et al., 2021),FLAVA (Singh et al., 2022), CoCa (Yu et al., 2022), DINOv2 (Oquab et al., 2023), VATT (Akbari et al.,2021), VideoMAE (Tong et al., 2022), and InternVideo (Wang et al., 2022b).",
  "VideoMAE (Tong et al., 2022)VideoK400MVMInternVideo (Wang et al., 2022b)VideoUnlabeledHybridMVM + ContrastiveVATT (Akbari et al., 2021)Video + Audio + TextHT100MContrastive": "Evaluation of foundation models.As the mission of FMs is to enable sample-ecient knowledge transfer,the design of downstream tasks is critical to evaluate the capabilities and limitations of these models. Theevaluation of FMs is pioneered by the NLP researchers. For example, GLUE (Wang et al., 2018a) andSuperGLUE (Wang et al., 2019) introduced a suite of tools for evaluating language understanding tasks. Theauthors utilized established public benchmarks and provided tools for evaluating, probing, and benchmarkingpretrained FMs, allowing for a comparison to human baselines. ELEVATER (Li et al., 2022a) introducedthis concept to vision FMs along with a toolkit for evaluating vision-language tasks, including knowledgeaugmentation, hyperparameter tuning, and three adaptation techniques. In parallel, there have been attemptsto establish a diagnostic benchmark for perceptual understanding of the world. For instance, PerceptionTest (Patraucean et al., 2024) crowd-sourced 11K videos in which about 100 users performed scripted activities.This benchmark comprises videos lmed by only about 100 participants, which may not provide the samelevel of domain coverage and diversity as the other FM evaluation works mentioned earlier. Evaluation of video foundation models.While some vision-language FMs have incorporated videotasks, their evaluation typically follows that of static images and neglects the unique aspects of videospatial-temporal modeling and reasoning. To our knowledge, no previous work has been solely dedicated toevaluating video FMs. The closest work to ours are InternVideo (Wang et al., 2022b) and VideoMAE (Tonget al., 2022), which introduce new FMs and show their superiority over several video datasets. This work hastwo key dierences to the prior ones. First, our evaluation is video-centric using the tasks that require motionunderstanding or long-term temporal reasoning. Second, instead of promoting new video FMs, our workproposes no new models and is solely dedicated to evaluating current and future video FMs in an impartialreproducible experimental setup. Concretely, our goal is to provide tools for probing and benchmarking FMson motion tasks in various settings.",
  "Tasks and adaptation methods both matter when evaluating foundation models": "This section describes our video general understanding evaluation (VideoGLUE) benchmark.We rstintroduce the visual and multimodal FMs evaluated in this work.Then we discuss the video-focuseddownstream tasks and methods to adapt an FM to the tasks. The former concretizes the video understandingcapabilities we want to evaluate from an FM, while the latter provides various paths for an FM to showcasethe corresponding capabilities.",
  "Foundation models for video understanding": "We are interested in examining which FMs are good at solving video tasks, what makes them better thanothers in the video domain, and how to best adapt them to video understanding. shows the sevenFMs we gained access to via public repositories or personal communications. Thanks to the powerfulness andscalability of the transformer architecture (Vaswani et al., 2017), most developed FMs converge to adopt thevision transformer architecture. Thus for all evaluated FMs, we intentionally choose the ViT-B (Dosovitskiyet al., 2020) variant to bring fair comparison into our benchmark. We also notice, in previous literature,models may be evaluated with dierent number of frames and resolutions, resulting in unfair comparison (Yanet al., 2022; Feichtenhofer et al., 2021). In VideoGLUE, we control the number of tokens observed by the",
  "Video understanding tasks": "Like objects role in image understanding, actions are the core of video understanding, leading us to selecttasks and datasets that recognize and localize actions in time and space. provides a quick summary.Next, we explain the rationale behind the particular choices of datasets and postpone the datasets details tothe supplementary materials B.",
  "Recognizing actions": "General actions. We rst include the action recognition datasets of Kinetics-400 (K400) (Kay et al., 2017),Moments in Time (MiT) (Monfort et al., 2019), and Charades (Sigurdsson et al., 2016), considering theirpopularity and they are complementary to each other. Data domain coverage is an important factor whendesigning benchmarks for FMs, as nowadays FMs are typically trained on massive data sources. K400 videosare from YouTube, MiT draws videos from dierent Web venues, while Charades contains scripted indoorvideos. Internet often returns entertaining and atypical videos, while Charades is about typical everydayvideos (Sigurdsson et al., 2016). Regarding action labels, the datasets dier in granularities and real-lifescenarios: a verb denes an action in MiT, K400 groups actions by verb-subject pairs, and Charades actionsare about indoor activities. Regarding the average length, K400 and MiT videos are between 3 and 10seconds, each with one action label, while Charades videos are about 30 seconds, each with multiple actions. Fine-grained motion-focused actions. We also include Something-something v2 (SSv2) (Goyal et al.,2017) and Diving48 (D48) (Li et al., 2018) as another two action recognition datasets, whose actions arene-grained and motion-focused. SSv2 contains 174 human hand gestures as action labels, such as puttingsomething into something, turning something upside down, and covering something with something. Thevideos are mostly focusing on hand-object interactions. D48 videos are all about competitive diving recordingscollected from Web sources. Notably, in these datasets the foreground objects motion is a more signicantdiscriminative cue than their appearance.",
  "Localizing actions": "The videos in action recognition are trimmed, but actions could occur anywhere in a video in the wild. Hence,temporal and spatiotemporal action localization is also crucial to video understanding. Accordingly, wechoose three datasets for the experiments: the action localization track of ActivityNet v1.3 (ANet) (FabianCaba Heilbron & Niebles, 2015), Atomic Visual Actions (AVA) (Gu et al., 2018), and AVA-Kinetics (AVA-K) (Li et al., 2020). The last two require a model to localize and recognize actions in both time and space,and their underlying videos are movies and general YouTube videos, respectively.",
  "Modifying foundation model architectures for downstream tasks": "Given an fm(), we can apply fm() to a video clip C to extract a set of k feature maps {F}k = fm(C), F Rnhwc, where k is the number of endpoint layers from an FM, and n, h, w, c are respectively a featuremaps length, height, width, and number of channels. For video classication tasks, we cast a feature map F as n h w tokens and aggregate them into a globalrepresentation using a learnable query token and lightweight cross-attention layers (Dosovitskiy et al., 2020).For spatiotemporal action localization, following the standard practice (Feichtenhofer et al., 2019; Tong et al.,2022), we rst detect humans on key-frames using a human detector (Ren et al., 2015), producing a set ofhuman bounding boxes B. We then apply the RoI pooling operation (Jaderberg et al., 2015) that takes boththe feature map F and box coordinates B as inputs and outputs one feature vector per box as the querytoken, = RoIPool(F, B), followed by the same cross-attention layers as in video classication. For bothgroups of tasks, we stack a linear classier on top of the task tokens last-layer encoding for nal classication:",
  "p = LinearClassifier(CrossAttention(, F)).(1)": "For temporal action localization, we rst perform feature extraction in a sliding window manner, resulting ina sequence of globally average pooled features {AvgPool(F1), , AvgPool(Ft)} for each video. Followinga popular choice of prior works (Alwassel et al., 2021; Ju et al., 2022; Liu et al., 2022), we employ G-TAD (Xuet al., 2020) as our task head for predicting the action category and its start and end timestamps.",
  "Adapting modied foundation model to downstream tasks": "Adapting the modied FMs to a downstream task is to tune their weights. Then, we immediately have twobasic adaptation strategies: (1) full netuning to update all weights in the original FM plus the task headand (2) freezing FM weights and only updating newly added weights. The choice of the adaptation methodsdepends on specic application scenarios such as computation and memory constraints. We argue that anideal FM should perform well across various adaptation methods to support the breadth of use cases.",
  "where k = 4 in our experiments, and the nal classier is p = LinearClassifier(N)": "Freezing foundation model weights with low-rank adaptation. Finally, we explore a frozen FMbeyond the last k layers using a low-rank adapter (Hu et al., 2021), which is a bottleneck architecturethat projects a feature tensor into a low-dimensional space and then up-samples to the original space. Thebottleneck spaces dimension is 64 in our experiments. Through inserting a few adapter layers with trainableweights {w} into the pretrained FM while keeping all FMs weights frozen, the feature adapter is moreparameter-ecient than end-to-end netuning the whole network while achieving better performance thansimply adding a task head to the frozen FM. Essentially, the adapter leads to a new FM with some trainableweights {w}: F = FM(C, {w}), such that the output feature maps remain the same in shape as the originalFMs output ((d)). Hence, dierent pooling schemes and task heads aforementioned could be appliedto the extracted feature map F. For simplicity, we still choose the single-layer cross-attention as the defaulttask head due to its computation eciency and performance. The low-rank adaptation allows a single FM for multiple tasks, in contrast to the per-task models in end-to-end netuning. However, it incurs a per-task forward pass at inference time, being less ecient than thetask-specic heads over frozen features.",
  "End-to-end netuning": "shows the end-to-end netuning results of six FMs on eight datasets. We split the FMs into twogroups based on their input modalities at the time of pretraining: CLIP, FLAVA, CoCa, and DINOv2 areimage-native FMs, while VATT, VideoMAE, and InternVideo are video-native. The datasets span videoclassication (VC) and spatiotemporal action localization (STAL). Note that it is infeasible to end-to-endne-tune or LoRA ne-tune the vision encoder on TAL task, because the videos in ANet are typically long(up to 30 minutes). We follow the common practice of pre-computing visual features in a sliding windowmanner oine, and training a temporal detection network on top of the visual features (Wang et al., 2021;Zhang et al., 2022). We will report TAL results in the next section. We draw the following observations from.",
  "Task-88.642.768.788.963.237.542.338.9specialized(TubeViT)(UniformerV2)(MViT)(AIM)(MoViNet)(PRN)(RAFT)(RAFT)": "FMs underperform task-specialized models on video tasks in general. s last row collects the state-of-the-art results on the eight datasets, each obtained by a task-specialized model with comparable architecture orsize to ours in the prior work. Specically, those task-specialized models are RAFT (Rajasegaran et al., 2023),PRN (Wang et al., 2021), TubeViT (Piergiovanni et al., 2023), UniformerV2 (Li et al., 2022b), AIM (Yanget al., 2023), MViT (Fan et al., 2021), and MoViNet (Kondratyuk et al., 2021), respectively. All seven FMsunderperform the task-specialized models on all video tasks except on Moments in Time at the comparablemodel scale, indicating the lack of strong video-focused FMs. This observation is in sharp contrast to whatFMs have achieved on natural language (OpenAI, 2022; Anil et al., 2023) and image understanding (Chenet al., 2022). CoCa performs the best among image-native FMs on the video tasks. It actually gives rise to the highestaccuracy on all datasets, with slightly inferior performance on SSv2 and Charades. This shows stronggeneralization capability of the CoCa model, regardless it is an image-based model with image-only pre-training data. However, in the latter session, we will reveal that under light-weight and parameter ecientadaptation scenarios, the same model may perform inferior on many video understanding tasks, especiallyon SSv2 (Tables 4, 5, and 6), Charades (Tables 4, 5, and 6), and ANet (Tables 4, and 5), which requirecomplex motion or multiple actions understanding per video. These are in contrast that CoCa achieves thebest general performance in end-to-end ne-tuning (), highlighting the importance of consideringadaptation methods on FMs benchmarking.",
  "Freezing foundation models": "End-to-end netuning is infeasible for some application scenarios due to FMs rapidly growth in size andthe consequent demands in computational resources. In the following, we evaluate frozen FMs with variousadaptation methods. Tables 4, 5, and 6 are the results of adaptation with a single cross-attention layer,multiple cross-attention layers, and a low-rank adapter, respectively. Generally speaking, DINOv2 performs the best in the frozen feature pooler evaluation (Tables 4), CLIPperforms the best with multi-head attention pooler among image-native frozen FMs (Tables 5), but CoCacatches up thanks to the low-rank adapter (). It is worth noting that this ranking of image-nativefrozen FMs diers from the ranking of image-native FMs in end-to-end netuning. It seems that DINOv2and CLIPs frozen features are more amendable to the video tasks than CoCa, but CoCa as a whole adaptsbetter to video under both netuning and the adapter. Hence, it is crucial to consider adaptation methods asan organic part of the evaluation of FMs to supply them various paths to demonstrate their capabilities. Video-native FMs are better than image-native FMs in understanding motion-rich SSv2 and D48, Charadesthat contain multiple actions per video, and ANet for temporal action localization. This observation is",
  "VATT75.036.563.568.953.522.325.849.9VideoMAE73.630.661.476.043.016.623.345.9InternVideo75.531.363.973.646.219.225.547.7": "about the same as the one under end-to-end netuning. The image-native FMs are mainly superior onappearance-rich video datasets, where high-quality spatial perceptual features are the key. We conjecturethat the vast image data empowering image-native FMs is more diverse in appearance than videos used topretrain video-native FMs. Given frozen FMs, the low-rank adapter outperforms cross-attention layers, and multiple layers of cross-attention is better than a single cross-attention layer. Many works (Caron et al., 2021; He et al., 2022)have shown features from dierent layers of a vision transformer have dierent attention maps. Hence, itis potentially benecial to have an adaptation method to leverage multiple layers of a frozen FM. reports the results with four cross-attention layers, whose average score per model (across dierent columns) is",
  "(b)": ": (a) We measures the training (red diamond) and inference (blue square) cost of dierent adaptationmethods in terms of number of trainable parameters and inference FLOPs, respectively. (b) We reportVideoGLUE Score that combines a FMs performance weighted by its training costs with dierent adaptationmethods for all the image-native (red circle) and video-native (blue pentagon) models. higher than that with a single cross-attention layer () by 18% to 40%. The low-rank adapter ()further improves upon the cross-attention results partially because it explores all layers of a frozen FM. On average, image-native FMs outperform video-native FMs under end-to-end netuning and the adapter, butit becomes the inverse in the other two adaptation methods. The adapter experiment paired with end-to-endnetuning experiment reveal the fact that existing image-based FMs could be more easily adapted to videotasks when we could adjust the feature space of FMs, possibly caused by the large-scale higher qualityimage(-text) pretraining datasets. On the other hand, frozen feature experiments discussed above present usthe inverse picture where video-based FMs perform better. The seemingly paradox encourages more futureresearch on bridging the gap on video-based pretraining with high-quality data, more eective modeling andbetter design on video benchmarks.",
  "Proling foundation models for video understanding": "In this section, we consolidate our studies of the FMs with dierent adaptation methods and video tasks,focusing on their overall ecacy and eciency. Specically, we use trainable parameters and inference FLOPsto approximately represent the training and inference costs of an FM. Since all FMs in our evaluation areViT-B and we align the same number of input tokens for each task. The models have almost the same cost inone adaptation method. The left of shows the cost values for each adaptation method. Note that anFM with LoRA adaptor tuning could have high inference cost despite lower training/adaptation costs thanend-to-end ne-tuning. While the gure provides a holistic view of an FM from multiple dimensions, onemight be interested in a ranking among the FMs in terms of their video understanding capabilities. To thisend, we summarize the multi-dimensional comparisons across dierent datasets, adaptation methods, andcosts using a simplied scalar measure, termed VideoGLUE Score (VGS), to probe an FMs general videounderstanding capability. We use the cost values to normalize an adapted FMs average performance s over all tasks. Formally, denotingby Si an FMs average performance score over our video tasks under the i-th adaptation method and by Ckithe corresponding cost value under the k-th developmental scenario, we calculate the FMs VGSk by",
  "log10 Cki,(3)": "where N = 4 is the number of adaptation methods, and wi weighs score Si according to the cost Cki .The nal VGS is the arithmetic average on {VGSk}, where k = 1, 2 corresponding to training and inferencerespectively. On the right panel of , we plot each FMs VideoGLUE Score. We notice the average VGS forvideo-native and image-native FMs on our video understanding tasks are 40.67 vs. 38.84 respectively. Zoomingin to the individual FMs, we nd that VATT, a video-native FM, is at the rst place with VGS 43.97, followed",
  "Limitations": "VideoGLUE serves as a comprehensive benchmark for studying and probing various video understandingcapabilities of foundation models. The current task portfolio includes various unimodal action understandingtasks. We believe the scope of this work could be further extended as there are many other important videotasks not covered here, e.g. object or point-level tracking, long-term memory and forecasting. Moreover, ourbenchmark could be strengthened by adding multimodal tasks like video captioning and question answering,given the rise of general Vision Language Models (VLM). We chose three representative FM adaptationmethods and used them to provide as uniform experiment protocols for dierent FMs as possible. However,some of our observations could be ipped with the evolution of FMs development and adaptation methods,which are an active research area. We proposed a scalar score, VideoGLUE Score (VGS), to capture theecacy and eciency of an FM on video understanding. However, VGS might be dominated by one or a fewdatasets when it becomes a serious issue, we should probably improve the score and/or retire the datasetsfrom future versions of VideoGLUE. Indeed, VGS is not a perfect score that covers all aspects of FMs in acomprehensive manner. For example, it does not account for an FMs model size, model architecture, etc.We hope future research will lead to new metrics to complement VGS and a more comprehensive evaluationof FMs for visual tasks.",
  "Conclusion": "In this report, we study four image-based and three video-based foundation models and their adaptationcapability on general video understanding tasks. Experiments are conducted on three hallmark video tasks,eight diverse datasets with four distinct adaption methods. Our study shows existing image-based FMsperforms well on some appearance-rich video datasets, while video-based FMs tend to achieve better onmotion and temporal reasoning. Four studied adaption methods curve dierent landscape, revealing thecritical role of considering adaption methods as an organic part of evaluating FMs. Finally, we proposeone single metric VGS to represent the video task adaptation eciency of FMs. We hope our researchprovides useful resources for evaluating and analyzing video foundation models, and address the current gapin foundation model evaluation within the video domain.",
  "AEthical concern and broader impact": "Ethical concern. We evaluate FMs on three video tasks, eight datasets in total. We select the tasks anddatasets based on their popularity and representativeness. Although carefully designed, our benchmarkinevitably inherited some ethical concerns from those datasets. For instance, many of the datasets are curatedby crawling videos from the Internet, which do not proportionately represent the experiences of the globalpopulation and can potentially lead to biased evaluations of FMs. Moreover, the video datasets involvehuman daily activities, leading to privacy concerns about the human actors in the videos. How to evaluateFMs for video understanding in a fair and privacy-preserving manner could be an important direction forfuture research. Broader impact. Our research reveals the need and tremendous opportunities to research video-rst FMsby improving pretraining video data and methodologies. Our studies on dierent adaptation methods onversatile tasks conrms that both tasks and adaptation methods matter when it comes to the evaluation ofFMs, shedding light on the already vibrant area of FM adaptations. Finally, we hope our research couldinspire research on foundation models development and video understanding in general, along with theirapplications in the real world.",
  "Video classication is a task of classifying videos into pre-dened labels, with the major focus on humanactions": "Kinetics-400 (Kay et al., 2017) (K400) is a large-scale, high-quality video dataset widely used as a standardvideo classication benchmark. It contains more than 250K video clips with annotations of 400 human dailyactions. The actions are human focused and cover a broad range of classes including human-human interactionsand human-object interactions. Although the video clips span 10 seconds on average, many studies (Sevilla-Lara et al., 2021; Wang et al., 2018b) have pointed out the task could be easily solved on the Kineticsdatasets by inferring from the static objects appeared or background environment motion information is",
  "less important than the visual appearance. Hence, we categorize Kinetics400 as an appearance-focused actionclassication dataset": "Moments in Time (Monfort et al., 2019) (MiT) is a large-scale video event classication dataset, with onemillion human annotated short video clips (around 3 seconds each). The temporal span corresponds to theaveraged duration of human working memory and is a temporal envelope holding meaningful actions betweenpeople, objects, and phenomena. Videos in MiT are annotated with 339 most used verbs in the Englishvocabulary.",
  "B.2Motion-focused action recognition": "Videos contain much more commonsense knowledge than still images do, such as an objects motion patternsand the causal consequences of an action, just to name a few. However, appearance-based benchmarks donot evaluate a models understanding of such commonsense knowledge, complex scenes, and situations. Inobservance of this, some video datasets have been proposed and studied in recent years with the focus onmotions and common-sensing reasoning that are prosperous in video data. Something-Something v2 (Goyal et al., 2017) (SSv2) is a collection of around 200K videos of human performingpre-dened, basic actions with everyday objects. There are 174 unique labels in total depicting atomic handmanipulations, like putting something into something, turning something upside down or covering somethingwith something. This dataset benchmarks a models ne-grained understanding capability of object motionsand scene changes by making the label space atomic-action-focused and background-invariant. Diving48 (Li et al., 2018) (D48) is introduced to evaluate a models dynamic reasoning capability. The videoclips in this dataset are obtained by segmenting online videos of major diving competitions. In total, thereare around 18K videos annotated with 48 classes. Because of its standardization, the diving scenario ispurposefully chosen to avoid the scene, object, and person biases.",
  "B.3Multi-label daily action classication": "Most of current action classication datasets involve video clips with a clean snapshot of a single action.In contrast, humans perform daily complex activities step-by-step, simultaneously, or in an interleavingmanner. Towards more comprehensive human daily activity reasoning, Charades (Sigurdsson et al., 2016) isintroduced. Dierent from web-collected datasets whose contents are more structured, Charades is collectedby crowd-sourcing from hundreds of actors recording their videos in their own homes, acting out casualeveryday activities. Charades brings in more diversity into the video classication task due to its close-to-daily-life setting. Its videos are 30 seconds long on average and have multi-label annotations testing modelsunderstanding of complex daily activities with multiple steps. Charades provides 110k videos with 157 actionclasses for training and evaluation.",
  "B.4Temporal action localization": "Natural long videos contain scene changes and semantic shifts, while most of the existing video benchmarksformulate problems to focus on trimmed video clips. Such a gap introduces evaluation bias as clip-levelbenchmarks could not reect a models temporal feature discriminativeness, which is of key importance tosolve long-form video understanding tasks. To comprehend the study on foundation models video capabilities,we include the temporal action localization (TAL) task in our evaluation. The task of TAL is to predict notonly the action labels but also each action instances temporal boundary in untrimmed videos. We adoptActivityNet v1.3 (Fabian Caba Heilbron & Niebles, 2015) as the dataset for the TAL task, which contains10, 002 untrimmed videos in training and 4, 985 in validation. The video length in this dataset is between5-10 minutes. In total, there are 200 types of activities annotated.",
  "CLIP70.575.238.141.0FLAVA67.971.340.440.6CoCa72.761.441.533.3": "In AVA (Gu et al., 2018), 15 minutes long movie clips are densely annotated at 1Hz. In the key frames, everyperson is localized using a bounding box and labels corresponding to actions being performed by the actor.The label vocabulary consists of 80 dierent atomic visual actions. There are 430 dierent movies in total. AVA-Kinetics (Li et al., 2020) follows the same labeling protocol as AVA, while its data source comes fromthe Kinetics700 (Kay et al., 2017) video pool. The dataset contains over 230K clips annotated with the 80AVA action classes for each of the humans in key frames.",
  "\u001771.329.730.341.610.7\u001371.329.740.645.912.6": "We consider two choices, early-fusion and late-fusion, and ablate them in the frozen feature setting in .In both early-fusion and late-fusion, we rst apply the projection layer on each frame independently to embedpixel patches into embedding tokens. We then average-pool the embedding tokens from nearby frames toreduce the sequence length to n h w. In the early-fusion setting, we pass all tokens together to the imagebackbone to extract video features. In late-fusion, we pass each set of h w tokens independently to theimage backbone. Empirically, we nd that the FLAVA (Singh et al., 2022) and CLIP (Radford et al., 2021)models do better with late-fusion while CoCa (Yu et al., 2022) does better with early-fusion. Furthermore, we ablate the importance of temporal information using the frozen-features from FLAVA (Singhet al., 2022). In , we nd that adding temporal positional embedding to the input is essential forD48 (Li et al., 2018), SSv2 (Goyal et al., 2017), and Charades (Sigurdsson et al., 2016) while not necessaryfor K400 (Kay et al., 2017) and MiT (Monfort et al., 2019). This supports our grouping that K400 and MiTare appearance-focused datasets. Based on these ndings, we use late-fusion for FLAVA (Singh et al., 2022) and CLIP (Radford et al., 2021)and early-fusion for CoCa (Yu et al., 2022). We add learnable temporal positional embeddings for all theimage-native FMs.",
  "EvaluationMulti-clips44144Multi-views33333Segment-based samplingNoNoYesNoNo": "We performed a greedy search on the learning rate and weight decay in all our experiments while keeping mostother hyperparameters (e.g., data augmentation magnitude, dropout rate, drop path rate, etc.) consistentacross dierent models and datasets. Specically, we start with learning rate 1e-4 and weight decay 1e-5and uniformly sample learning rates and weight decay factors with a rate of 5 and 10, respectively, centeredaround the starting points. After the rst round, we pick the best-identied learning rate and weight decayfactor as the new starting point and conduct another round of sampling with a rate of 2. We repeat anothertwo to three rounds of hyperparameter search (with a rate of 2) until the models performance converges.This process is a trade-o between computation costs and thoroughly examining an FMs performance undereach experiment setup. The search ranges for the learning rate and weight decay are [4e-5, 2.5e-3] and [1e-6,1e-4], respectively. We found that the learning rate is the most crucial factor when adapting an FM todownstream video understanding tasks.",
  "For the completeness of this report and readers reference, in we report experimental results underour settings with large FMs under the frozen backbone with one pooler head setup": "VideoMAE-v2-B/DL (Wang et al., 2023) denotes the ViT-B model distilled from ViT-g on the Kinetics710datasets1. VideoMAE-v2-g (Wang et al., 2023) is the model that pretrained on UnlabeledHybrid dataset,while VideoMAE-v2-g/FT (Wang et al., 2023) conducts further netuning using supervised training onKinetics710. InternVideo-v2-g (Wang et al., 2024) and VideoPrism-g (Zhao et al., 2024) are two videofoundation models with multi-stage pre-training on curated in-house web video data. For InternVideo-v2-g,we use their stage-2 checkpoint2. For videoPrism-g, we use their nal checkpoint.",
  "E.2Sample-ecient transfer learning": "A strong FM should be able to adapt to downstream tasks with a few training samples. In this section,we test the adaption ability of FMs in a sample-ecient transfer learning setting. Particularly, we freezebackbones and train a pooler head to adapt the FMs on K400 and SSv2. For either dataset, we sample 1%and 10% data from the training set uniformly for training and evaluate on the full evaluation dataset. We show our experimental results in . To better understand the data eciency, we also show therelative Top-1 accuracy for each model (shown in the bracket), which is dened as the ratio between accuracywith fewer training examples and the accuracy achieved using all the training data. A higher relative Top-1accuracy means the performance of the model is closer to its full capacity under the sample-ecient setting.We notice that the best performed model on each dataset in fully ne-tuned model also performs best in thefew-shot setting. Especially, CLIP (Radford et al., 2021) achieves 46.2% and 83.6% relative Top-1 accuracyon K400 using only 1% and 10% of the training data, respectively. On SSv2, InternVideo (Wang et al., 2022b)achieves 33.6% and 70.6% relative Top-1 accuracy with only 1% and 10% of the training data.",
  "Roger G Barker and Herbert F Wright. Midwest and its children: The psychological ecology of an American town.Marriage and Family Living, 1955": "Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein,Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities and risks of foundation models.arXiv preprint arXiv:2108.07258, 2021. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan,Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. In NeurIPS, 2020.",
  "Mathilde Caron, Hugo Touvron, Ishan Misra, Herv Jgou, Julien Mairal, Piotr Bojanowski, and Armand Joulin.Emerging properties in self-supervised vision transformers. In ICCV, 2021": "Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian Goodman, AdamGrycner, Basil Mustafa, Lucas Beyer, et al. PaLI: A jointly-scaled multilingual language-image model. arXivpreprint arXiv:2209.06794, 2022. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham,Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. PaLM: Scaling language modeling with pathways.arXiv preprint arXiv:2204.02311, 2022.",
  "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectionaltransformers for language understanding. arXiv preprint arXiv:1810.04805, 2018": "Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou, and Hsiao-WuenHon. Unied language model pre-training for natural language understanding and generation. In NeurIPS, 2019. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words:Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.",
  "Christoph Feichtenhofer, Haoqi Fan, Yanghao Li, and Kaiming He. Masked autoencoders as spatiotemporal learners.arXiv preprint arXiv:2205.09113, 2022": "Raghav Goyal, Samira Ebrahimi Kahou, Vincent Michalski, Joanna Materzynska, Susanne Westphal, Heuna Kim,Valentin Haenel, Ingo Fruend, Peter Yianilos, Moritz Mueller-Freitag, et al. The something something videodatabase for learning and evaluating visual common sense. In ICCV, 2017. Chunhui Gu, Chen Sun, David A Ross, Carl Vondrick, Caroline Pantofaru, Yeqing Li, Sudheendra Vijayanarasimhan,George Toderici, Susanna Ricco, Rahul Sukthankar, et al. AVA: A video dataset of spatio-temporally localizedatomic visual actions. In CVPR, 2018.",
  "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jacob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and IlliaPolosukhin. Attention is all you need. NeurIPS, 2017": "Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. GLUE: A multi-taskbenchmark and analysis platform for natural language understanding. arXiv preprint arXiv:1804.07461, 2018a. Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and SamuelBowman. SuperGLUE: A stickier benchmark for general-purpose language understanding systems. Advances inneural information processing systems, 32, 2019.",
  "Limin Wang, Bingkun Huang, Zhiyu Zhao, Zhan Tong, Yinan He, Yi Wang, Yali Wang, and Yu Qiao. VideoMAE v2:Scaling video masked autoencoders with dual masking. In CVPR, 2023": "Wenhui Wang, Hangbo Bao, Li Dong, Johan Bjorck, Zhiliang Peng, Qiang Liu, Kriti Aggarwal, Owais KhanMohammed, Saksham Singhal, Subhojit Som, et al. Image as a foreign language: BEIT pretraining for all visionand vision-language tasks. arXiv preprint arXiv:2208.10442, 2022a. Xiang Wang, Zhiwu Qing, Ziyuan Huang, Yutong Feng, Shiwei Zhang, Jianwen Jiang, Mingqian Tang, Changxin Gao,and Nong Sang. Proposal relation network for temporal action detection. arXiv preprint arXiv:2106.11812, 2021."
}