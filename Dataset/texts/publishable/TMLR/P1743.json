{
  "Abstract": "Robust generalization (RG), concerning how deep neural networks could perform over ad-versarial examples generated from unseen dataset, has emerged as an active research topic.Albeit its crucial importance, most previous studies lack a well-founded theoretical analysisand certified error bounds. In this paper, we make a novel attempt to theoretically and em-pirically study how we could attain a better RG by learning discriminative representation,where the inconsistency of the inter-sample similarity matrix between clean and adversarialexamples should be reduced. Our theoretical investigation discloses that introducing thisinconsistency as a regularization term, named Gram matrix difference (GMD), will lead totighter upper error bound and certify a better RG. Meanwhile, we demonstrate that previousefforts to reduce inter-class similarity and increase intra-class similarity among adversarialexamples for enhanced adversarial robustness are approximate optimizations of our GMDapproach. Furthermore, to avoid the vast optimization complexity introduced by the sim-ilarity matrix, we propose to optimize GMD by building a diverging spanned latent spacefor adversarial examples. On the algorithmic side, this regularization term is implementedas a novel adversarial training (AT) method Subspace Diverging (SD) to expand thevolume difference between the whole latent spaces linear span and subspaces linear spans.Extensive experiments show that the proposed method can improve advanced AT methodsand work remarkably well in various datasets, including CIFAR-10, CIFAR-100, SVHN, andTiny-ImageNet.",
  "Introduction": "Deep Neural Networks (DNNs) have attained great success recently in various applications, such as imageclassification, image generation, and object detection. Despite the impressive performance enhancement overvarious tasks, DNNs are strikingly vulnerable to specific well-crafted adversarial perturbations (Carlini &Wagner, 2017; Song et al., 2018; Fischer et al., 2017; Lyu et al., 2015). Although these perturbations areinvisible to humans, they can easily mislead DNNs predictions. Adversarial training (AT) (Mkadry et al.,2017; Wang & Zhang, 2019; Kannan et al., 2018; Gu & Rigazio, 2015; Zhang & Wang, 2019; Jia et al., 2022)is considered as one of the most effective defense methods capable of effectively improving model robustnessagainst various types of adversarial attacks (Carlini & Wagner, 2017; Croce & Hein, 2020; Kurakin et al.,2016; Mkadry et al., 2017), such as widely used projected gradient descent (PGD) based AT (Mkadry et al.,2017), adversarial weight perturbation (AWP) (Wu et al., 2020), Feature Scatter (FS) (Zhang & Wang, 2019)and TRADES (Zhang et al., 2019).",
  "(c) Tiny-ImageNet": ": Robust accuracy gap (over PGD20 generated on CIFAR-10, CIFAR-100, and Tiny-ImageNet)between adversarial training and test datasets for classical AT methods and those augmented with ourSubspace Diverging (SD) regularization method. Although robustness has been improved significantly in previous studies, the robust models trained by mostexisting adversarial training methods still present poor robust generalization (RG). RG evaluates how wellthe model trained over the adversarial training set generalizes to further adversarial examples generatedfrom unseen dataset (Rice et al., 2020), which is usually measured by the robust accuracy gap, i.e. theaccuracy difference between adversarial training examples and adversarial test examples (Gao et al., 2022).As depicted in , all robust AT models, including PGD-AT, AWP, TRADES, and FS, show poor RGand present large robust accuracy gaps. Indeed, RG has been shown even more difficult to achieve than standard generalization since the sample com-plexity of adversarial examples can be significantly larger than that of standard or natural samples (Schmidtet al., 2018b). Recent efforts have studied RG from different perspectives, such as introducing customizedearly stopping (Rice et al., 2020), activation function (Singla et al., 2021), adversarial vertex mixup (Stutzet al., 2021), and diffusion term of Stochastic Differential Equation in AT (Sun et al., 2023). However, mostof these methods empirically investigate RG and lack a well-founded theoretical analysis. Recently, Zhanget al. (2021) proposed shift consistency regularization (SCR) term to theoretically certify the RG error,which is, however, shown to be underperformed than our method in experimental results. In this paper, from the perspective of learning robust and discriminative representation, we aim to investigatean effective algorithm that can certify a tight RG bound in theory to ensure excellent adversarial robustnesspractically. Generally, adversarial perturbations induce feature shifts in the latent feature representation(Zhang et al., 2021), which causes adversarial examples to move into the semantic clusters of other classes,resulting in an incorrect classification. This phenomenon can be visualized in by comparing theinter-sample relationship map of clean data with that of adversarial examples. In a standard model, thediscriminative features of clean data are similar within the same class and dissimilar among different classes(see a). On the contrary, in b, features of adversarial examples exhibit smaller intra-classsimilarity, yet more considerable inter-class similarity than clean data.Ideally, enabling the features ofadversarial examples to be as discriminative as clean data will encourage a better RG. Intuitively, this goalcan be attained by reducing the inconsistency of inter-sample relationship maps between clean data andadversarial examples.",
  "(d) PGD20 w/ FS+SD": ": Visualization of inter-sample relationship (Gram matrix) of latent features on SVHN. We randomlyselect 1,000 samples for each class from the test dataset and sort all the selected samples according to theirclass indexes. The lighter color represents the higher cosine similarity of the two feature vectors, and viceversa. The latent features of a standard model (abbreviated to Sta.) are generated from (a) clean data and(b) adversarial examples perturbed by PGD20. The latent features of adversarial examples perturbed byPGD20 are produced by robust models trained by (c) Feature Scattering (FS) (Zhang & Wang, 2019) and(d) FS applied with our Subspace Diverging (SD). All models are trained on the SVHN dataset and employwide residual networks (WRN-28) as the backbone. To further verify our observation, we theoretically analyze the robust generation issue from a novel perspec-tive. We illustrate that the robust generalization gap can indeed be upper bounded by the above inconsistencyof inter-sample relationship - Gram matrix difference. Meanwhile, we demonstrate that previous efforts toreduce inter-class similarity and increase intra-class similarity among adversarial examples for enhanced ad-versarial robustness in (Li et al., 2019; Bui et al., 2020) are approximate optimizations of our approach. Inour case, this term is optimized by learning a diverging spanned latent space for adversarial examples, whereVolume Difference between the Whole latent spaces linear span and Subspaces linear spans (VDWS) isenlarged, such that latent subspaces (divided by categories) to be mutually orthogonal. Our method addi-tionally improves the representation diversity of adversarial examples which has been proved as one of themerits of improving the generalization ability of standard model (Liu et al., 2018). Compared with previousstudies like (Li et al., 2019; Bui et al., 2020), our method offers a theoretical guarantee for improving RGthrough learning discriminative representation. More importantly, compared with (Zhang et al., 2021), ourapproach enjoys a better RG performance, which can be observed in an empirical analysis in .6,thus certifying robust performance in many real-world datasets. Built upon the above theory, towards better RG, we instantiate the VDWS with an AT method calledSubspace Diverging (SD). As shown in d, our SD improves the feature representation comparedto traditional AT methods (e.g., FS in c), which becomes more discriminative and diverse, thuspromoting a better RG (as seen in ). Our contributions are digested as follows. 1) We study and reveal that the robust generalization gap correlates with the difference in inter-samplerelationship maps between clean data and adversarial examples. Leveraging this insight, we derive a noveland tighter robust generalization bound. 2) To enable a tractable optimization, we propose to build a diverging spanned latent space for adversar-ial examples, which is theoretically well-founded for learning discriminate and diverse representation. Weimplement a novel adversarial regularization method named subspace diverging to achieve this goal. 3) Extensive experiments have been conducted to verify the effectiveness of our adversarial regularizationmethod on various benchmark datasets. The results demonstrate that our approach enhances the perfor-mance of various state-of-the-art methods.",
  "Adversarial Training": "Adversarial training (AT), a primary defense approach against adversarial examples (Goodfellow et al., 2015;Carlini & Wagner, 2017; Athalye et al., 2018), has been extensively researched to enhance the robustnessof deep neural networks. Projected gradient descent (PGD) (Carlini & Wagner, 2017)-based AT is one ofthe most common approaches used to improve robustness, and PGD is an iterative optimization techniquedesigned to generate adversarial examples by perturbing input samples within a specified norm constraint.Adversarial weight perturbation (AWP) (Wu et al., 2020) is proposed, which is effective in boosting ro-bustness by directly perturbing the models weights rather than the input samples. This approach aimsto make the model resilient to changes in its model parameters, thereby improving its overall stability andperformance against adversarial attacks. Based on AWP, Jin et al. (2023) introduces small Gaussian noiseinto the weights of the neural network during adversarial training, and the weight perturbation is modeledusing a Taylor series expansion, which allows the method to decompose the objective function into multipleterms. The goal is to balance the trade-off between adversarial robustness and clean accuracy by smoothingthe weight updates and finding flatter minima in the loss landscape. Feature Scatter (FS) (Zhang & Wang,2019) disperses features of input data to generate diverse adversarial examples. By ensuring that adver-sarial examples cover a wider range of perturbations, FS can achieve better robustness. Geometry-AwareInstance Reweighted Adversarial Training (GAIRAT) (Zhang et al., 2020c) optimizes the geometry of de-cision boundaries by assigning different weights to adversarial examples based on their distances from thedecision boundary. GAIRAT effectively balances the trade-off between robustness and accuracy, leading tosignificant improvements in both areas. Moreover, some researchers also investigate the effects of adversarialtraining strategies on model performance (Zhang et al., 2020a; Jia et al., 2022; Wei et al., 2023). FriendlyAdversarial Training (FAT) (Zhang et al., 2020a) emphasizes that AT from adversarial examples closer tothe decision boundary can help in reducing the models overfitting to adversarial perturbations. Learningattack strategy (LAS) (Jia et al., 2022) is introduced to adjust the attacking configurations for different datasamples, and Wei et al. (2023) adapts the class-specific training configurations.",
  "Robust Generalization": "Robust generalization evaluates a models performance against unseen adversarial examples, akin to standardgeneralization. Yin et al. (2019.) investigate the relationship between model complexity and its ability torobust generalization against adversarial examples. It explores how increasing model capacity can improveadversarial robustness but may also lead to overfitting if not properly regularized. The finding provides the-oretical insights and empirical evidence on how to balance model complexity to achieve optimal adversarialrobustness and generalization.Schmidt et al. (2018b) argue that achieving robust generalization againstadversarial examples necessitates significantly larger datasets compared to standard training. The statementhighlights that increased data volume can enhance the models ability to generalize and resist adversarialattacks. Zhang et al. (2019) decomposes the robust error into natural classification and boundary errors,offering a balanced approach to robustness and accuracy. This study examines the inherent trade-off be-tween achieving robustness against adversarial attacks and maintaining high accuracy on clean data. Li et al.(2022a) analyze that the poor robust generalization is due to the VC dimension of adversarial testing samplesbeing significantly larger than their intrinsic dimension.Sun et al. (2023) enhance robust generalizationby approximating PGD-AT as a continuous-time Stochastic Differential Equation (SDE) and manipulatingits diffusion term. In (Zhang et al., 2021), the Shift Consistency Regulation (SCR) method is proposed tomitigate deficient robust generalization by reducing variance in perturbation direction between adversarialtraining and unseen datasets. In this paper, we investigate cosine similarity variants and analyze the ro-bust generalization gap to enhance model robustness. Theoretical analysis suggests that achieving robustgeneralization faces challenges from random inter-sample relationship variation, susceptible to perturbationattacks. Our approach demonstrates better performance empirically, as observed in our experiments.",
  "Learning Discriminative and Diverse Representations": "Metric learning-based approaches (Cheng et al., 2016b; Hadsell et al., 2006; Hu et al., 2014; Schroff et al., 2015;Huang et al., 2010; Chopra et al., 2005) are employed to increase inter-class distance and decrease intra-classdistance for deep features, typically using Euclidean distance. Hadsell et al. (2006) propose how to reducedata dimensionality through invariant feature learning. It focuses on identifying and preserving essentialdata characteristics while minimizing irrelevant variations, thereby enhancing the efficiency and accuracy ofsubsequent analyses. The groundbreaking contrastive loss (Hadsell et al., 2006) enforce the above constraintsusing a siamese network architecture (Chopra et al., 2005).This strategy gained popularity in variousdownstream tasks such as image retrieval (Yousefzadeh et al., 2023).Moreover, learning discriminativefeature representations can also be beneficial in face recognition (Hadsell et al., 2006; Sun et al., 2014),where the triplet loss (Cheng et al., 2016a) and center loss (Wen et al., 2016) also demonstrate similareffectiveness. In a recent work by Lezama et al. (2018), a plug-and-play loss term for deep networks has beenutilized to explicitly reduce intra-class variance and enforce inter-class margin. Furthermore, Yu et al. (2020)enhanced feature representation discriminability by augmenting the code rating of feature representation.Following (Yu et al., 2020), Chan et al. (2022) introduce ReduNet, a deep learning framework that constructsinterpretable network architectures by maximizing the rate reduction of feature representation. ReduNetefficiently reduces information redundancy and captures essential features.",
  "Robust Representation": "Given a data distribution (x, y) (X, Y ) with K classes, a training set, consisting of N i.i.d. data pairsdrawn from (X, Y ), can be denoted as (XD, YD), where XD RdN and YD RKN denote a trainingdata matrix and a label matrix, respectively, and d is the dimension of the data sample. The object ofstandard generalization is to learn a deep neural network (DNN) f() with parameters on a training setso that the generalization error (the difference between the expected loss over the data distribution andthe empirical loss over the training data) becomes as small as possible (Xu & Mannor, 2012; Bousquet &Elisseeff, 2002; Neyshabur et al., 2017), where f() maps the data samples from the input space to the latentfeature space with dimension r, e.g. f(x) Rr. Leveraging the above insight, considering the loss functionl() of f(), the robust generalization error (gap) (Zhang et al., 2021) is defined as the difference betweenthe expected loss over on adversarial examples (XadvD , YD) and the expected loss over their underlyingdistribution (Xadv, Y ), i.e.,",
  "n=1lf(xadvn), yn": "In general, a test set (XT , YT ) is introduced as a surrogate of data distribution (X, Y ) to empiricallyestimate the robust generalization error since the entire (X, Y ) is unavailable. Here, (XT , YT ) includes i.i.d.samples that are drawn from (X, Y ) and disjoint with (XD, YD) . In the following, we propose Theorem 3.1 to serve as the main theoretical foundation for our work, whichestablishes an upper bound of the robust generalization containing standard generalization error and onenovel regularization term. This term was inspired based on our observations as illustrated in , wherethe Gram matrix is widely used to measure inter-class and intra-class similarity to capture inter-samplerelationships. Detailed proof can be seen in the Appendix.",
  "Published in Transactions on Machine Learning Research (12/2024)": "Chawin Sitawarin, Supriyo Chakraborty, and David Wagner.Sat:Improving adversarial training viacurriculum-based loss smoothing. In Proceedings of the 14th ACM Workshop on Artificial Intelligenceand Security, pp. 2536, 2021. 13 Chuanbiao Song, Kun He, Jiadong Lin, Liwei Wang, and John E Hopcroft. Robust local features for improv-ing the generalization of adversarial training. th International Conference on Learning Representations,2019. Dawn Song, Kevin Eykholt, Ivan Evtimov, Earlence Fernandes, Bo Li, Amir Rahmati, Florian Tramr,Atul Prakash, and Tadayoshi Kohno. Physical adversarial examples for object detectors. 12th USENIXWorkshop on Offensive Technologies (WOOT 18), August 2018. 2",
  ",col represents the calculation of the Euclidean norm of column vectors in the matrix": "proof : Let (|N1|, , |NK|) is an IID multinomial random variable with parameters N. N and K denotethe total number of training samples and the total number of classes respectively. With the probability dis-tribution set for the underlying classes {(C1), (C2), . . . , (Ci), . . . (CK)}, following Breteganolle-Huber-Carel inequality (Proposition A6.6 of (Wellner et al., 2013)):",
  "Comparison to Shift Consistency Regularization (SCR) (Zhang et al., 2021).According to": "Zhang et al. (2021), for the theoretical analysis of RG, the robust error gap is divided into three com-ponents: the standard error gap (GE), a constant term, and a feature shift inconsistency term. There-fore, Zhang et al. (2021) propose SCR, which constrains the feature shift to certify RG, and SCR =",
  "mintNKi=1": "v Ni f(xadvv) f(xv) Ef(xadv) f(x) | x Ci22, where Ni is the set of index oftraining data for class i. SCR only constrains the difference between the feature shift f(xadvv) f(xv)of each training data point and the expectation of feature shifts within the same class for the underlyingdistribution. Nevertheless, it overlooks the inconsistency among different classes, which intuitively leads toa less tight RG compared to optimizing our GMD. We provide an indirect comparison in Section A.1 of theAppendix. We use an approximated approach to optimizing GMD, which is proposed in , and ourmethod can achieve better RG as empirically shown in .6. Comparison to Inter-feature Relationship (IFR) (Zhang et al., 2024). The approach proposedby (Zhang et al., 2024) focuses on preserving the inter-feature relationship between natural and adversarialexamples. This method aims to maintain the original feature distribution structure, minimizing the vari-ation caused by adversarial perturbations. In contrast, our approach introduces the GMD to address theinconsistency between clean and adversarial examples in the inter-sample relationships. By optimizing adiverging spanned latent space, we enhance the discriminative power of feature representations. In terms of the optimization of GMD, we can opt to minimize ||Td||2; however, estimating ||T ||2 isgenerally impractical due to the inaccessibility of the entire underlying distribution. Alternatively, a widelyaccepted assumption is that the inter-feature relationships within the training data can reflect the struc-tural information of the whole data distribution. This assumption has been effectively utilized in adversarialrobustness (Li et al., 2019) and representation learning (Bui et al., 2020). Given the assumption, as the train-ing feature representations become more discriminative, exhibiting larger intra-class similarity and smallerinter-class similarity, the feature representations of the entire underlying distribution are supposed to followthe same trend. Moreover, by learning the discriminative features of adversarial examples, the diagonalelements of the Gram matrix increase, while the off-diagonal elements decrease accordingly for both trainingand underlying datasets. As such, ignoring ||T ||2 still represents a reasonable optimization strategy, sinceminimizing ||Td||2 tends to cause a decrease in ||T ||2. To address this aspect, we present ||Td||2 and ||T ||2 of our SD, as shown in . We empiricallycalculate these values by randomly sampling N = 1, 000 data points from the training and test set of theCIFAR-10 dataset respectively, where our method is only applied to the training data.1",
  "i=1(Zadvi)T Zadvi (Zi)T Zi22;": "Here, || T interd||2 and || T intrad||2 denote the inter-class and intra-class similarity respectively on trainingadversarial example distribution. Reducing || Td||2 can be achieved when (Zadvi)Zadvi Zi Zi and(Zadvi)Zadvj Zi Zj.Thus, the intra-class cosine similarity should increase, and inter-class cosinesimilarity should decrease. This solution aligns the general intuition for learning discriminative representationto improve generalization ability and is an approximation of our GMD.",
  "Vol (P ) = det (P ) .(3)": "Intuitively, the more diverse the subspaces are, the more separable clusters of adversarial example featurerepresentation. Consequently, the inter-class similarity tends to decrease. Such a latent space, composed ofdiverging subspaces, embodies a larger volume, which is visualized in . Besides, a smaller volumeof each subspace indicates a larger intra-class similarity. We show the proof details of this viewpoint inTheorem 3.2.",
  "IfVol(I + Si)is minimized, the vectors in Si remain consistent.(4)": "Theorem 3.2 highlights that decreased cosine similarity among distinct subspaces basis expands the volumeof the overall spanned latent space as illustrated in .In contrast, heightened cosine similaritybetween intra-class basis is associated with reduced volume in each subspace. Detailed proof of Theorem 3.2is provided in the Appendix. To achieve a discriminative feature representation, we aim to maximize the overall volume of the traininglatent space linear span while minimizing the volume of the linear subspace span, as elaborated in .1.Specifically, we sort these objectives by prioritizing the expansion of the Volume Difference between theWhole latent spaces linear span and Subspaces linear spans (VDWS):",
  "s.t.eit Sj = 0, eit U i , j [1, K] and i = j.(7)": "Since our VDWS increases the volume of S, the more significant number of the whole latent space basis willbe attained so that the number of bases eit in unique eigenvector set U i of subspace also increases. From the above analysis, a more sizable number of distinct eigenvectors eit in U iadv will naturally steerto a greater value of rank(U iadv), which has been shown as a necessary condition for learning diverserepresentation in (Chan et al., 2022). Therefore, the approach not only enhances feature representationdiscrimination but also improves feature representation diversity, which has been proven to enhance modelgeneralization (Liu et al., 2018). Moreover, We further verify the feature diversity of our method throughexperimental analysis in .8. Previous studies have investigated the discriminative and diverse representation learning for standard gen-eralization, such as Maximal Coding Rate Reduction (MCR2), considering the low level of feature distortionas an essential premise in (Yu et al., 2020). However, as shown in Figures 2b and 2c, features of adversarialexamples are highly distorted, which leads to unclear inter-sample relationships. As a result, applying MCR2",
  "Empirical Analysis": "In this section, we aim to confirm the validity of Theorem 3.1 by demonstrating how the accuracy gapbetween test and training adversarial examples evolves with varying values of VDWS. We visually illustrateinter-sample similarity, shedding light on how adversarial perturbations play a role in this accuracy gap. Allthe models discussed in this subsection are trained using Feature Scatter (FS) on the SVHN dataset.",
  "(d)": ": The accuracy gap vs. VDWSt at different training epochs. The comparison of FS and AT to trainthe robust models against various attacks: (a) C&W attack and (b) PGD20 attack. The comparison of FSand FS+SD (ours) to train the robust models against various attacks: (c) C&W attack and (d) PGD20attack. Adhering to the established evaluation protocol (Xu & Mannor, 2012), rather than using the error gapdifference |RGE GE|, we choose to calculate the accuracy gap difference,|ACC(XadvT, YT )ACC(XadvD , YD)||ACC(XT , YT )ACC(XD, YD)| which shares the same trend witherror gap difference. This replacement will provide a direct reflection of robust generalization, which has beenwidely utilized in previous studies (Zhang et al., 2021). In .1, we have demonstrated the correlationbetween VDWS and GMD. To validate Theorem 3.1 and show whether the gap difference is caused by GMD,the VDWSt and the accuracy gap difference under C&W and PGD20 across 60 to 200 epochs are shownin Figures 5a and 5b, where robust models are trained by FS and AT respectively and VDWSt denote theVDWS values of the test datasets. As results can be observed, the VDWSt and the accuracy gap differenceshow obvious consistency. Therefore, the results indicate that GMD can capture the error gap difference(reflecting the difference between robust and standard generalization).",
  "We select three classes for visualizing the feature distributions on the test dataset of CIFAR-10. We employa multi-layer perceptron (MLP) backbone and utilize a PGD-3 attack with =2": "255 to generate featuredistributions presented in . In the latent space of adversarial examples, after integrating our SD toFS in d, the distributions of the categories are more inclined toward the vertical, and the clusters ofcategories are more compact than the ones of FS in c. This trend also emerges in the clean dataset,as observed in models trained by FS+SD in b and FS in a.",
  "TRADES+SDCIFAR-108090(90,50)0.80.10.05CIFAR-10010090(150,200)0.50.010.01": "where XadvDindicate adversarial training dataset that are produced during the adversarial training, XDrepresents the initial adversarial training dataset that adds random noise to the original image, L() is anadversarial training loss, and Lgene() is the loss function which producing adversarial examples. As the scaleof Ldiverge() and Lgene() are different, we design normalization parameters d and k to adjust loss values,and is an additional pre-defined parameter. Due to the GPU memory limitations, calculating Ldiverge() for all of the training dataset is very challenging.To solve this problem, we calculate Ldiverge(), L(), and Lgene() for each data batch. Then, we selectthe maximum values among all the batches and define them as Lmax, Lmaxgene, and Lmaxdiverge respectively.",
  "Subsequently, we estimate d and k using Lmax, Lmaxdiverge and Lmaxgene, where d =LmaxdivergeLmaxgeneand k =LmaxdivergeLmax": "We have observed that the values of d and k remain stable for each epoch. Therefore, to streamline ourtraining process, we compute the d and k only in the first epoch.The specific values for d and k ofvarious datasets and the entire algorithm can be found in experiment section. When training the generatedadversarial examples, our algorithm can be regarded as a regularization to increase the overall spanned spacevolume but reduce the volume of each subspace.",
  "Experimental Setting": "We evaluate our methods robustness against white-box and black-box adversarial examples on CIFAR-10, CIFAR-100, SVHN, and Tiny-Imagenet. We benchmark our approach against four established methods:Feature Scattering (FS) (Zhang & Wang, 2019), Adversarial Training (AT) (Mkadry et al., 2017), AdversarialWeight Perturbation (AWP) (Wu et al., 2020), and TRADES (Zhang et al., 2019). Our core model is basedon the WideResNet-34-10 (WRN34) architecture. On Tiny-ImageNet datasets, we follow (Jia et al., 2022)and implement PreActResNet18 as the backbone. FS, a well-known baseline, has been demonstrated toperform poorly against strong attacks, such as Autoattack, in previous work (Naseer et al., 2022). As aresult, we do not present it as a primary outcome. However, since SCR is built upon FS, we will compareSD with SCR based on FS baseline alone in .3. Following (Zhang et al., 2021), we implementWideResNet-28-10 (WRN28) for comparison. In our training regimen, we employ SGD with a momentum of 0.9, weight decay of 5 104, and an initiallearning rate of 0.1. Learning rates decrease at epochs 60 and 90 by a factor of 0.1. During training, weperform 7 attack iterations for PGD-AT and TRADES, and 1 iteration for FS. For consistency, the attackbudget is maintained at 8/255 for all the methods. Adversarial examples are computed with the normduring training and testing. All experiments are conducted on a single GPU, e.g. RTX 3090, under the",
  "PGD-AT+SD62.34 0.2133.19 0.1532.97 0.2031.99 0.1929.58 0.18TRADES+SD60.97 0.1633.79 0.2233.17 0.1929.79 0.1728.43 0.14AWP+SD64.91 0.1436.59 0.1735.97 0.1634.03 0.1931.14 0.17": "environment using CUDA 11.7, Python 3.8, and Pytorch 1.80. lists all used parameters on differentbaselines, batch sizes, and training epochs. is the balance parameter, and are pre-defined parameters,d and k are normalization parameters. The early stopping is used for model selection in all methods. Ourresults and standard deviations are obtained from 5 runs; each trained using a different random seed. Based on various baselines, we compare our proposed baseline+SD approach with other state-of-the-artadversarial training methods : 1) PGD-AT (Mkadry et al., 2017), 2) AWP (Wu et al., 2020), 3) FS (Zhang& Wang, 2019). 4) SCR (Zhang et al., 2021), 5) LAS(Jia et al., 2022), 6) GAIRAT (Zhang et al., 2020c),7) SAT (Sitawarin et al., 2021), 8) FAT (Zhang et al., 2020b), 9) LAT (Kumari et al., 2019), 10) Bilateral(Wang & Zhang, 2019) and 11) RAT (Jin et al., 2023). We report the results under the white-box attack in, , , and while leaving the results of the black-box attack in .",
  "Robustness Against Adversarial Examples": "Our results reveal that SD can improve the robustness of different baselines against attacks on variousdatasets. Even for more complex datasets such as Tiny-imagine, SD still has a significant effect compared withbaselines. Tables 2-5 summarize the robust accuracy of different methods under various adversarial attacksacross CIFAR-10, CIFAR-100, Tiny-ImageNet, and SVHN datasets. Our method, SD, consistently achievessuperior performance, improving both accuracy and robustness across all datasets and attack types. Notably,SD demonstrates significant improvements over baseline methods, particularly in challenging scenarios suchas the PGD and C&W attacks. These results highlight the effectiveness of SD in enhancing model robustness,making it a reliable solution for adversarial defense.",
  "Results on FS Baseline Compared with SCR": "Since SCR is based on the FS baseline, we added an extra section to compare SCR and SD specificallybased on the FS baseline. presents the performance of different methods based on the FS baselineon CIFAR-10, CIFAR-100 and SVHN. Notably, the results marked with an asterisk (*) are cited from(Naseer et al., 2022). On CIFAR-10, SD performs worse than SCR against certain attacks, such as PGD20.However, on CIFAR-100 and SVHN, SD comprehensively outperforms SCR. We demonstrated improvedrobust accuracy with SD compared to SCR across nearly all baselines, datasets, and attack types.",
  "Results on Different Attack Budget": "Figures 7 and 8 illustrate the effectiveness of SD, our proposed method, in improving model robustness underboth PGD20 and CW attacks for CIFAR-10 and CIFAR-100 datasets. Across all attack budgets, modelsincorporating SD show a more gradual decline in accuracy, particularly for PGD-AT+SD. The incorporationof SD enhances the resilience of both PGD-AT and TRADES, with PGD-AT+SD demonstrating the mostsignificant improvement, especially as the attack budget increases. This suggests that SD effectively promotesrobustness by maintaining higher accuracy under stronger attack conditions.",
  "Results on Black-Box Attack": "presents the robust accuracies under black-box attacks on three datasets, including CIFAR-10,CIFAR-100, and Tiny-ImageNet. Here, since the compared methods are evaluated by using different back-bones in original papers, it is difficult to compare the results of black-box attacks directly. To ensure a faircomparison, we retrain all compared methods by leveraging the WiderResNet-34-10 (WRN34) as the unifiedbackbone and maintaining their original training parameter. We implement two black-box attacks FABand SQUARE belonging to Autoattack. By consistently outperforming other methods, particularly underblack-box attacks, SD proves to be a robust and reliable defence mechanism. The significant improvementsin accuracy across CIFAR-10, CIFAR-100, and Tiny-ImageNet confirm that SD offers superior protection.",
  "Generalization Analysis": "Figures 5c and 5d illustrate the robust accuracy gap under C&W and PGD20 between training and testdatasets of CIFAR-10. We present VDWSt for both the FS and FS+SD. Notably, Figures 5c and 5d highlightthat FS+SD achieves a smaller accuracy gap and a greater VDWSt in comparison to FS. FS+SD consistentlymaintains superior VDWSt throughout the convergence process and exhibits a reduced accuracy gap. Thisobservation underscores SDs role in diminishing the accuracy gap and enhancing robust generalization byemphasizing the volume difference between the entire space and the summation of subspaces. Figures 9a and 9b illustrate a comparison of RG results obtained through optimization using the SD regular-ization term versus the SCR regularization term. We use PGD-AT as a baseline and attacks are PGD20 andC&W, respectively. It is evident that using the SD regularization term results in a smaller RG compared tothe SCR regularization term. The reason is that compared to SCR, SD not only constrains the inter-samplerelationship variations within the same class but also decreases the inter-class similarity. The outcomesvalidate that SD can achieve better RG compared with SCR term.",
  "Meta-Analysis": "In this subsection, we present a meta-analysis comparing the performance of different adversarial trainingmethods and our SD regularization term, focusing on robust accuracy under PGD20 attack across CIFAR10and CIFAR100. To rigorously quantify the differences in performance, we employed two statistical measures:Cohens d effect size and t-test. Cohens d measures the effect size, which quantifies the magnitude of theimprovement in robust accuracy when SD is added. t-test assess whether the differences in robust accuracybetween methods with and without SD are statistically significant. This combined approach helps us evaluateboth how much SD improves robustness and whether these improvements are consistent across datasets.",
  "CIFAR-100.78p < 0.05CIFAR-1000.65p = 0.03": "The results in show that SD regularization significantly improves robust accuracy when combinedwith adversarial training methods. In , Cohens d values confirm moderate to large effects across twodatasets, indicating a meaningful impact. In conclusion, SD enhances both the generalization and robustnessof adversarial training, making it a valuable addition to defenses against adversarial attacks.",
  "Diversity Analysis": "According to the definition given in Equation 7, an increase in the total unique vector numbers reflectsgreater feature representation diversity. However, the computation of unique eigenvector numbers poses achallenging task. To facilitate direct measurement of the diversity within the spanned space S, we utilize awidely adopted evaluation metric, effective rank, denoted as Erank. It can be mathematically representedas:",
  "||||1,(11)": "where l denote the l-th element in an eigenvalue set = {1, . . . , l, . . . , L} of the spanned space S,||||1 represents the L1 norm of , and pl is the normalized eigenvalue. The larger Erank represents thebetter diversity for feature representation. When the values of Erank reach maximum, the determinant ofS (i.e. det(S) = Ll=1 l) also reach maximum. Following the above analysis, we can conclude that there isa positive relationship between VDWS and Erank. Increasing the VDWS corresponds to an enlargement of",
  "Sensitivity Analysis": "We examine the parameter sensitivity of and on CIFAR-10 in . In a, we can observethat the effect of the shirnk term is greater than that of the expand term since the ratio of the expand termis (1 ). b shows that our method is less sensitive to as the fluctuation is only around 2.5%.",
  "Conclusion": "In this paper, we investigate the robust generalization (RG) problem from the perspective of learning dis-criminative representation for adversarial examples. Our theoretical and empirical analysis illustrate thatreducing the inconsistency of inter-sample relationship maps between clean data and adversarial examplesis a feasible approach to alleviate robust overfitting and can be calculated by the proposed Gram matrixdifference (GMD). Meanwhile, we provide a theoretical guarantee for RG by introducing a novel and tighterror bound based on our GMD. Moreover, to ease the complex optimization of inter-sample relationshipmaps, we propose a method that expands the volume difference between the entire latent spaces linear spanand the subspaces linear span, thereby creating a diverging spanned latent space. On the empirical side, wedesign and implement an adversarial training method, called Subspace Diverging (SD), which alleviates therobust overfitting problem and achieves state-of-the-art performance on multiple benchmarks.",
  "Acknowledgement": "The work was partially supported by the following: National Natural Science Foundation of China un-der No. 92370119 and 62376113, the WKU Internal (Faculty/Staff) Start-up Research Grant under No.ISRG2024009. Anish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients give a false sense of security:Circumventing defenses to adversarial examples. In International conference on machine learning, pp.274283. PMLR, 2018. 4",
  "Yair Carmon, Aditi Raghunathan, Ludwig Schmidt, John C Duchi, and Percy S Liang. Unlabeled dataimproves adversarial robustness. Advances in Neural Information Processing Systems, 32, 2019": "Kwan Ho Ryan Chan, Yaodong Yu, Chong You, Haozhi Qi, John Wright, and Yi Ma. Redunet: A white-boxdeep network from the principle of maximizing rate reduction. Journal of Machine Learning Research, 23(114):1103, 2022. 5, 9 De Cheng, Yihong Gong, Sanping Zhou, Jinjun Wang, and Nanning Zheng. Person re-identification bymulti-channel parts-based cnn with improved triplet loss function. In Proceedings of the iEEE conferenceon computer vision and pattern recognition, pp. 13351344, 2016a. 5 De Cheng, Yihong Gong, Sanping Zhou, Jinjun Wang, and Nanning Zheng. Person re-identification bymulti-channel parts-based cnn with improved triplet loss function. Proceedings of the IEEE conference oncomputer vision and pattern recognition, pp. 13351344, 2016b. 5 Sumit Chopra, Raia Hadsell, and Yann LeCun. Learning a similarity metric discriminatively, with applica-tion to face verification. In 2005 IEEE Computer Society Conference on Computer Vision and PatternRecognition (CVPR05), volume 1, pp. 539546. IEEE, 2005. 5",
  "Volker Fischer, Mummadi Chaithanya Kumar, Jan Hendrik Metzen, and Thomas Brox. Adversarial examplesfor semantic image segmentation. arXiv preprint arXiv:1703.01101, 2017. 2": "Zhiqiang Gao, Shufei Zhang, Kaizhu Huang, Qiufeng Wang, Rui Zhang, and Chaoliang Zhong. Certify-ing better robust generalization for unsupervised domain adaptation. In Proceedings of the 30th ACMInternational Conference on Multimedia, pp. 23992410, 2022. 2 Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples.3rd International Conference on Learning Representations,Conference Track Proceedings, 2015. 4",
  "Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature, 521(7553):436444, 2015": "Saehyung Lee, Hyungyu Lee, and Sungroh Yoon. Adversarial vertex mixup: Toward better adversariallyrobust generalization. In Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition, pp. 272281, 2020. Jos Lezama, Qiang Qiu, Pablo Mus, and Guillermo Sapiro. Ole: Orthogonal low-rank embedding-a plugand play geometric loss for deep learning. In Proceedings of the IEEE Conference on Computer Visionand Pattern Recognition, pp. 81098118, 2018. 5 Binghui Li, Jikai Jin, Han Zhong, John Hopcroft, and Liwei Wang. Why robust generalization in deeplearning is difficult: Perspective of expressive power. Advances in Neural Information Processing Systems,35:43704384, 2022a. 4",
  "Chengzhi Mao, Ziyuan Zhong, Junfeng Yang, Carl Vondrick, and Baishakhi Ray. Metric learning for adver-sarial robustness. Advances in Neural Information Processing Systems, 32, 2019": "Takeru Miyato, Shin-Ichi Maeda, Masanori Koyama, and Shin Ishii. Virtual adversarial training: A regular-ization method for supervised and semi-supervised learning. IEEE Transactions on Pattern Analysis andMachine Intelligence, 41(8):19791993, 2019. doi: 10.1109/TPAMI.2018.2858821. Aleksander Mkadry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towardsdeep learning models resistant to adversarial attacks. 6th International Conference on Learning Repre-sentations, 1050:9, 2017. 2, 12, 13 Muzammal Naseer, Salman Khan, Fatih Porikli, and Fahad Shahbaz Khan. Guidance through surrogate:Toward a generic diagnostic attack. IEEE Transactions on Neural Networks and Learning Systems, 2022.12, 14",
  "L. Schmidt, S. Santurkar, D. Tsipras, K. Talwar, and A. Madry. Adversarially robust generalization requiresmore data. in advances in neural information processing systems, pp. pp. 50145026 2018a": "Ludwig Schmidt, Shibani Santurkar, Dimitris Tsipras, Kunal Talwar, and Aleksander Madry. Adversariallyrobust generalization requires more data. Advances in neural information processing systems, 31, 2018b.2, 4 Florian Schroff, Dmitry Kalenichenko, and James Philbin. Facenet: A unified embedding for face recognitionand clustering. Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 815823, 2015. 5 Vasu Singla, Sahil Singla, Soheil Feizi, and David Jacobs. Low curvature activations reduce overfitting inadversarial training. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp.1642316433, 2021. 2",
  "Hongjun Wang and Yisen Wang. Self-ensemble adversarial training for improved robustness. arXiv preprintarXiv:2203.09678, 2022": "J. Wang and H. Bilateral adversarial training Zhang. Towards fast training of more robust models againstadversarial attacks. In Proceedings of the IEEE International Conference on Computer Vision, 2019. 2,13 Zeming Wei, Yifei Wang, Yiwen Guo, and Yisen Wang. Cfa: Class-wise calibrated fair adversarial training.In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 81938201,2023. 4",
  "D. Yin, R. Kannan, and P. Bartlett. Rademacher complexity for adversarially robust generalization. ininternational conference on machine learning, pp. pp. 70857094, 2019. 4": "Saeideh Yousefzadeh, Hamidreza Pourreza, and Hamidreza Mahyar. A triplet-loss dilated residual networkfor high-resolution representation learning in image retrieval. Journal of Signal Processing Systems, pp.113, 2023. 5 Yaodong Yu, Kwan Ho Ryan Chan, Chong You, Chaobing Song, and Yi Ma. Learning diverse and discrimi-native representations via the principle of maximal coding rate reduction. Advances in Neural InformationProcessing Systems, 33:94229434, 2020. 5, 9",
  "Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric Xing, Laurent El Ghaoui, and Michael Jordan. Theoreti-cally principled trade-off between robustness and accuracy. pp. 74727482. PMLR, 2019. 2, 4, 12": "Jingfeng Zhang, Xilie Xu, Bo Han, Gang Niu, Lizhen Cui, Masashi Sugiyama, and Mohan Kankanhalli.Attacks which do not kill training make adversarial learning stronger. In International conference onmachine learning, pp. 1127811287. PMLR, 2020a. 4 Jingfeng Zhang, Xilie Xu, Bo Han, Gang Niu, Lizhen Cui, Masashi Sugiyama, and Mohan Kankanhalli.Attacks which do not kill training make adversarial learning stronger. In International conference onmachine learning, pp. 1127811287. PMLR, 2020b. 13",
  "Shufei Zhang, Kaizhu Huang, Jianke Zhu, and Yang Liu. Manifold adversarial learning. Neural Networks,2018": "Shufei Zhang, Zhuang Qian, Kaizhu Huang, Qiufeng Wang, Rui Zhang, and Xinping Yi. Towards betterrobust generalization with shift consistency regularization. International Conference on Machine Learning,pp. 1252412534, 2021. 2, 3, 4, 5, 6, 10, 12, 13 Shufei Zhang, Zhuang Qian, Kaizhu Huang, Qiu-Feng Wang, Bin Gu, Huan Xiong, and Xinping Yi. Inter-feature relationship certifies robust generalization of adversarial training. International Journal of Com-puter Vision, pp. 117, 2024. 7 Long Zhao, Ting Liu, Xi Peng, and Dimitris Metaxas. Maximum-entropy adversarial data augmentationfor improved generalization and robustness.Advances in Neural Information Processing Systems, 33:1443514447, 2020. Zhong-Qiu Zhao, Peng Zheng, Shou-Tao Xu, and Xindong Wu. Object detection with deep learning: Areview.IEEE Transactions on Neural Networks and Learning Systems, 30(11):32123232, 2019.doi:10.1109/TNNLS.2018.2876865.",
  "Proof of Theorem 3.1": "Theorem 3.1 Given the clean data matrix Xi and adversarial data matrix Xadvithat both contain Ni samplesof i-th class over the training set, the sets of clean data Ci and adversarial data Cadviof i-th class over theunderlying data distribution, and the DNN f that maps data samples to latent features with dimension r,if the loss function l() of f() is t-Lipschitz, and f() is the L-Lipschitz, then for any > 0, with theprobability at least 1 , we have",
  ".(18)": "Consider the matrix Zi Zi. Since its diagonal elements are equal to 1 and the off-diagonal elements are lessthan or equal to 1, it follows that det(Zi Zi) 0. For any vector a, b Zi, and a, b = 1, det(ZiZi) willbe close to minimum value of 0. Correspondingly, log det(Qi) will also reach its minimization value.",
  "A.1Indirect Comparison between SIC and GMD": "For indirect comparison, we illustrate that our GMD is equivalent to a lower bound (lowest value) of SCRterm. As a result, using our GMD to upper bound the robust generation error will result in a tighter upperbound than using SCR. Specifically, the lower bound of the SCR term is expressed as:"
}