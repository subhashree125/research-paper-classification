{
  "Abstract": "We introduce the Lennard-Jones layer (LJL) for the equalization of the density of 2D and3D point clouds through systematically rearranging points without destroying their overallstructure (distribution normalization). LJL simulates a dissipative process of repulsive andweakly attractive interactions between individual points by considering the nearest neighborof each point at a given moment in time. This pushes the particles into a potential valley,reaching a well-defined stable configuration that approximates an equidistant sampling afterthe stabilization process. We apply LJLs to redistribute randomly generated point cloudsinto a randomized uniform distribution. Moreover, LJLs are embedded in the generationprocess of point cloud networks by adding them at later stages of the inference process. Theimprovements in 3D point cloud generation utilizing LJLs are evaluated qualitatively andquantitatively. Finally, we apply LJLs to improve the point distribution of a score-based3D point cloud denoising network. In general, we demonstrate that LJLs are effective fordistribution normalization which can be applied at negligible cost without retraining thegiven neural network.",
  "||": ": An overview of the integration of LJLs into the inference process of well-trained generative models.A well-trained model generates a meaningful point cloud from random noise in a sequential way (bottomrow). LJLs are inserted after certain intermediate-generation steps with damping step size tn and LJpotential parameters and (top row). Embedding LJLs in generative models can improve the generationresults by normalizing the point distribution.",
  "Introduction": "Next to triangle meshes, point clouds are one of the most commonly used representations of 3D shapes.They consist of an unordered set of 3D points without connections and are the native output format of3D scanners. They also map well to the structure of neural networks, either in generative or processing(e.g. denoising) tasks. A major problem with point clouds is, that their entropy heavily depends on thedistribution of the points. In areas with holes, not enough information about the shape is expressed, whileclustered areas waste storage without adding additional details. We introduce a so-called Lennard-Jones layer(LJL) for distribution normalization of point clouds. We use the term distribution normalization to describethe process of systematically rearranging the points positions in order to equalize their density across thesurface without changing the overall shape. More formally, distribution normalization describes the propertyof maintaining a global structure while maximizing minimum distances between points.The Lennard-Jones (LJ) potential is widely considered an archetype model describing repulsive and weakly attractiveinteractions of atoms or molecules. It was originally introduced by John Lennard-Jones (Jones, 1924a;b;Lennard-Jones, 1931) who is nowadays recognized as one of the founding fathers of computational chemistry.Applications of LJ-based numerical simulations can be found in different scientific communities rangingfrom molecular modelling (Jorgensen et al., 1996) and soft-matter physics (Al-Raeei & El-Daher, 2019)to computational biology (Hart & Istrail, 1997).For a given point cloud, we aim for a transformationfrom the initial distribution into a blue noise-type arrangement by simulating the temporal evolution of theassociated dynamical system. Each point of the point cloud is interpreted as a particle in a dynamic sceneto which the LJ potential is applied as a moving force. In each iteration of the temporal integration process,the point cloud is decomposed into a new set of independent subsystems, each containing a single pair ofparticles. Each of these subsystems is then simulated individually, which greatly increases stability. As wealso include dissipation into our system, the point configuration of the particle system will be stabilized andform randomized uniform point distribution after a sufficient number of iterations. Learning-based pointclouds related research has been explored for years, especially in the fields of generation and denoising.Existing models are solely trained to generate and denoise the point cloud without considering the overallpoint distribution and therefore exhibit the common problems of holes and clusters. By embedding LJLsinto well-trained architectures, we are able to significantly improve their results in terms of point distributionwhile at the same time introducing minimal distortion of the shape as shown in . We also circumventresource- and time-intensive retraining of these models, as LJLs are solely embedded in the inference process.This way, LJLs are a ready-to-use plug-in solution for point cloud distribution optimization.",
  "Published in Transactions on Machine Learning Research (09/2024)": "Guandao Yang, Xun Huang, Zekun Hao, Ming-Yu Liu, Serge Belongie, and Bharath Hariharan. Pointflow: 3dpoint cloud generation with continuous normalizing flows. In Proceedings of the IEEE/CVF InternationalConference on Computer Vision (ICCV), October 2019. Wang Yifan, Felice Serena, Shihao Wu, Cengiz ztireli, and Olga Sorkine-Hornung. Differentiable surfacesplatting for point-based geometry processing. ACM Trans. Graph., 38(6), nov 2019. ISSN 0730-0301. Wentao Yuan, Tejas Khot, David Held, Christoph Mertz, and Martial Hebert.Pcn: Point completionnetwork. In 2018 International Conference on 3D Vision (3DV), pp. 728737, 2018. doi: 10.1109/3DV.2018.00088.",
  "Blue Noise Sampling": "Colors of noise have been assigned to characterize different kinds of noise with respect to certain propertiesof their power spectra. In visual computing, blue noise is usually used more loosely to characterize noisewithout concentrated energy spikes and minimal low-frequency components. Due to its relevance in practicalapplications such as rendering, simulation, geometry processing, and machine learning, the visual computingcommunity has devised a variety of approaches for the generation and optimization of blue noise. Severalmethods can maintain the Poisson-disk property, by maximizing the minimum distance between any pair ofpoints (Cook, 1986; Lloyd, 1982; Dunbar & Humphreys, 2006; Bridson, 2007; Balzer et al., 2009; Xu et al.,2011; Yuksel, 2015; Ahmed et al., 2017). Schlmer et al. (2011) introduced farthest-point optimization(FPO)that will maximize the minimum distance of the point set by iteratively moving every point to the farthestdistance from the rest of the point set. de Goes et al. (2012) successfully generated blue noise through optimaltransport(BNOT). Their method has widely been accepted as the benchmark for best-quality blue noise,which was recently surpassed by Gaussian blue noise(GBN) (Ahmed et al., 2022). GBN is classified as kernel-based methods (ztireli et al., 2010; Fattal, 2011; Ahmed & Wonka, 2021) that augment each point with akernel to model its influence. There are several previous works that we consider as methodologically closelyrelated to ours as the authors applied the physical-based particle simulation to synthesize blue noise. Themethod proposed by Jiang et al. (2015), is based on smoothed-particle hydrodynamics(SPH) fluid simulationwhich takes different effects caused by pressure, cohesion, and surface tension into account. This results in aless puristic process compared to our work. Both Schmaltz et al. (2010) and Wong & Wong (2017) employelectrical field models, in which electronically charged particles are forced to move towards an equilibriumstate after numerical integration because of the electrostatic forces. Adaptive sampling can be achieved byassigning different amounts of electrical charge to particles.",
  "Generative Models for 3D Point Cloud": "Research related to 3D point clouds has recently gained a considerable amount of attention. Some meth-ods aim to generate point clouds to reconstruct 3D shapes from images (Fan et al., 2016) or from shapedistributions (Yang et al., 2019). Achlioptas et al. (2017) propose an autoencoder for point cloud reconstruc-tion, while others aim to solve point cloud completion tasks based on residual networks (Xie et al., 2020),with multi-scale (Huang et al., 2020) or cascaded refinement networks (Wang et al., 2020) or with a focuson upscaling (Li et al., 2019). Some methods solve shape completion tasks by operating on point cloudswithout structural assumptions, such as symmetries or annotations (Yuan et al., 2018), or by predictingshapes from partial point scans (Gu et al., 2020). 3D shape generation methods either use point-to-voxel",
  "Denoising Techniques for 3D Point Cloud": "Approaches for denoising point clouds aim to reduce perturbations and noise in point clouds and facilitatedownstream tasks like rendering and remeshing. Similar to images, denoising for point clouds can be solvedwith linear (Lee, 2000) and bilateral (Fleishman et al., 2003; Digne & de Franchis, 2017) operators, or basedon sparse coding (Avron et al., 2010). Although these methods can generate less noisy point clouds, theyalso tend to remove important details. Duan et al. (2018) leverages the 3D structure of point clouds byusing tangent planes at each 3D point to compute a denoised point cloud from the weighted average of thepoints. Many existing approaches employ neural network architectures for point cloud denoising purposes,by building graphs or feature hierarchies (Pistilli et al., 2020; Hu et al., 2020), by employing differentiablerendering for point clouds (Yifan et al., 2019), or by classifying and rejecting outliers (Rakotosaona et al.,2020). Hermosilla et al. (2019) propose an unsupervised method for denoising point clouds, which combinesa spatial locality and a bilateral appearance prior to mapping pairs of noisy objects to themselves. Themajority of learning-based denoising methods directly predict the displacement from noisy point positionsto the nearest positions on the underlying surface and then perform reverse displacement in one denoisingstep. Luo & Hu (2021b) propose an iterative method where the model is designed to estimate a score of thepoint distribution that can then be used to denoise point clouds after sufficient iterations of gradient ascent.More specifically, clean point clouds are considered as samples generated from the 3D distribution p on thesurface, while noise is introduced by convolving p with noise mode n. The score is computed as the gradientof the log-probability function log(p n). They claim that the mode of p n is the ground truth surface,which means that denoising can be realized by converging samples to this mode.",
  ",(1)": "which is a function of the distance between a pair of particles simulating the repulsive and weakly attractiveinteractions among them. The distance r is measured by the Euclidean metric. The first part ()12 attributesthe repulsive interaction while the second part ()6 attributes the soft attraction.With the help of theparameters and , the LJ potential can be adjusted to different use cases. illustrates the role ofthese parameters: The potential depth is given by the parameter > 0 which determines how strong theattraction effect is, and is the distance at which the LJ potential is zero. The corresponding magnitude of the LJ force can be obtained by taking the negative gradient of Eq. (1)with respect to r which is FLJ(r) = rV (r). The direction of LJ force on each particle is always alongthe line connecting the two particles, which point towards each other when two particles are attracted andvice versa. Equilibrium distance rE is where FLJ(rE) = 0 and the LJ potential reaches the minimum value.For the distance r < rE, the repulsive part dominates, while for r > rE the particles start to attract to eachother. When r gets bigger, the attractive force will decrease to zero, which means that when two particlesare far enough apart, they do not interact. If we assume a uniform mass distribution among all particles andnormalize with the mass, the corresponding equations of motion acting on a pair of particles whose positions",
  "r": "Given a 2D or 3D input point cloud in Euclidean space, we aim for a transformation from a white noise- intoa blue noise-type arrangement by simulating the temporal evolution of the associated dynamical system. Inthis regard, we use a formulation of the LJ potential where pairs of particles are grouped as a pair potentialto simulate the dynamic LJL procedure. The computations within LJL are summarized in Alg. 1. Withina system containing a number of particles, the potential energy is given by the sum of LJ potential pairs.This sum shows several local minima, each associated with a low energy state corresponding to the systemsstable physical configuration. It is an essential aspect of our method, that we do not compute the completenumerical solution of the resulting N-body problem (in which N denotes the number of involved particles).We only take into account the closest neighbor (k = 1 neighborhood) of each particle within every iterationof the temporal integration process. Note that being the closest neighbor of a particle is not necessarily amutual property of two particles within the whole particle system. However, the whole system of particlescan be decomposed into several independent subsystems by iteratively assigning the closest neighbor toeach particle and, throughout the process, not considering particles which have already been assigned as aclosest neighbor. Following this approach, every subsystem just contains a single pair of particles. In eachiteration, different subsystems (i.e., new pairs) can be formed. As the subsystems are not interacting withineach iteration, the sum of the LJ potentials has just a single (global) minimum if no changes to the nearestneighbors are present, see Appendix A. LJL creates pairwise independent subsystems using nearest neighbor search (NNS) by assigning the nearestpoints to the current point set Xi. We simulate this process by repetitively integrating forward in time usinga decaying time step t which dampens exponentially according to",
  "t() = exp( ()) ,(2)": "in which > 0 refers to the initial step size, and > 0 defines the damping intensity. For each pair,the position update is computed independently by integrating the equation of motion numerically using abasic StrmerVerlet scheme. Within this numerical integration process, we intentionally do not make useof the particles velocities from the previous step as this corresponds to a velocity reset (setting all velocitiesidentically to zero) causing a desired dissipation effect for the final convergence. Moreover, as V (r) increasesrapidly when r < , we clamp the LJ potential from r = 0.9 to r = 100 and employ the hyperbolic tangentas an activation function restricting the gradient rV (r) to the interval to avoid arithmetic overflows.Following this method, we can redistribute any randomly generated point cloud into a uniform distributionby iteratively applying LJLs. This process stops as soon as a stable configuration is reached, which meansthat the difference between the previous and current generated point clouds is below the tolerated threshold.Consequently, this spatial rearrangement of particles prevents the formation of clusters and holes.",
  "Input: Point cloud Xi and point cloud NNS(Xi).Output: Point cloud Xi+1.ti t(i)r min{max{Xi NNS(Xi), 0.9 }, 100}x tanh(r V (r)) t2i / 2Xi+1 Xi + x (Xi NNS(Xi))/Xi NNS(Xi)": ": Influence of the parameter . The rectangle denotes the unit square in which 1024 points areinitialized. (a) = 0.2 leads to clusters; (b) = distributes points well and keeps them close to theboundary; (c) = 10 spreads points out of the boundary extremely; (d) = 10 with fixed boundaryconditions. denotes the estimated optimal value of .",
  "Numerical Examples on 2D Euclidean Plane": "The LJ potential itself has two parameters: The repulsion distance and the attraction strength . Similarto the time step t, scales the gradient, however, it is applied before computing the tanh-function (seeAlg. 1). In practice, we find that setting = 2 results in a well-behaving LJ gradient. The repulsion distance corresponds to the optimal distance that all particles strive to have from their neighbors. To derive theproper value for , we take inspiration from the point configuration of the hexagonal lattice. The largestminimum distance r between any two points is given by r =",
  "/(": "3 N) denotes the estimated optimal value of . Small s lackredistribution ability over the point set as they lead to a limited effect of LJL on particles. Given a large, particles tend to interact with all the points nearby and are spread out of the region excessively. Afterexploring the effect of on the point distribution over the unbounded region, we introduce fixed boundaryconditions where points will stop at the boundaries when they tend to exceed. In this constraint scenario,LJL is robust with respect to as long as it is sufficiently large. When it comes to the damping step sizet in Eq. 2, we want LJL updates with decreasing intensity controlled by the damping parameters and in order to converge samples in later iterations. Empirically, we set = 0.5 and = 0.01 to ensure aproper starting step size and slow damping rate. We analyze the behavior of LJLs in different situationssuch as considering different numbers of neighbors k and with or without attraction term in LJ potential,see Appendix B.1.",
  ": Spectral analysis of different blue noise generation methods. The rows from top to bottom displaythe input point set, power spectrum, radially averaged power spectrum, and anisotropy": "solely from white noise distribution and analyze the corresponding blue noise properties. Here we synthesizethe blue nose distribution in a 2D unit square with periodic boundaries. Inside the region, initial randompoints are generated and iteratively rearranged by LJLs to generate blue noise. The spectral analysis forblue noise includes the power spectrum which averages the periodogram of distribution in the frequencydomain and two corresponding 1D statistics: radial power and anisotropy. Radial power averages the powerspectrum over different radii of rings and the corresponding variance over the frequency rings is anisotropyindicating regularity and directional bias. We use the point set analysis tool PSA (Schlmer & Deussen,2011) for spectral evaluation of different 2D blue noise generation methods (1024 points) in . The bluenoise generated by the LJL closely resembles the results produced by the FPO method.",
  "Redistribution of Point Cloud over Mesh Surfaces": "As a motivating example for real applications, we use LJLs to evenly distribute points over mesh surfaces.Initially, the point cloud is randomly generated inside a 3D cube 3. To ensure the generality of LJLparameters in 3D cases, mesh sizes are normalized to fit into the cube.The LJL parameters = 0.5, = 0.01, and = 2 are the same as in the previous example. We set = 5 due to the addition ofthe third dimension, with the factor of 5 determined through hyperparameter tuning. In the following fourcases illustrated in , we show the importance of applying surface projection in-between LJL iterations.Without the help of LJLs, random points are directly projected on the sphere shown in (a). WhenLJL is only used once as a pre-processing step before the projection, points are located on the surface butstill non-uniformly distributed (see (b)). When using LJL as a post-processing after the projection,additional noise is introduced and points are shifted away from the surface shown in (c). Due to thefact that LJLs iteratively rearrange points, in the last case, we project points to the underlying surface ineach intermediate LJL step. The resulting point configuration is not only evenly distributed, but also lieson the sphere shown in (d). The computations involved in LJL-guided point cloud redistribution aresummarized in Appendix B.2.",
  "LJL-embedded Deep Neural Networks": "Clusters and holes are consuming the limited number of points without contributing additional informationto the result. In the following two applications, LJLs are plugged into the 3D point cloud generation anddenoising networks that work in an iterative way (e.g. Langevin dynamics). With the help of LJLs, we canenforce points to converge to a normalized distribution by embedding LJLs into refinement steps (generationand denoising). The evaluation metric NoiseScore is defined as the mean value of the minimum Euclideandistances from the point Xi to the underlying surface M, i.e.,",
  "LJL-embedded Generative Model for 3D Point Cloud": "In this section, we show that LJLs can be applied to improve the point distribution of point cloud generativemodels ShapeGF (Cai et al., 2020) and DDPM (Luo & Hu, 2021a) without the need to retrain them. It hasbeen shown that the aforementioned two types of probabilistic generative models are deeply correlated (Songet al., 2021). The inference process of both models can be summarized as an iterative refinement processshown in the bottom row of . Initial point cloud X0 is sampled from Gaussian noise N(0, I). The re-cursive generation process produces each intermediate point cloud Xn according to the previously generatedpoint cloud Xn1. Even though there is randomness introduced in each iteration to avoid local minima, thegeneration process depends heavily on the initial point positions. Every point moves independently withoutconsidering the neighboring points, thus generated point clouds tend to form clusters and holes. The top row of provides an overview of the integration of LJLs into the inference process of well-trainedgenerative models. Since the sampling procedure of these generative models is performed recursively, onecan insert LJLs into certain intermediate steps and guide the generated point cloud to have a normalizeddistribution.The generator and LJLs have different goals: the former aims to converge all points to apredicted surface ignoring their neighboring distribution, which is also can be seen as a denoising process.The latter aims to prevent the formation of clusters and holes to improve the overall distribution. It istherefore important to observe whether LJLs are inhibiting the quality of the generation process.Thetrade-off between distribution improvement and surface distortion needs to be measured. To find LJL parameters that achieve a favorable trade-off, we apply LJLs in the well-trained autoencoder-decoder models. In , given a point cloud Y , the autoencoder E encodes it into latent code Z, thenthe decoder D which is the generator will utilize iterative sampling algorithms to reconstruct the pointcloud conditioned on Z. We fix the encoder E and insert LJLs solely in decoder D. To determine properparameters for LJLs in generation tasks, the reconstructed results need to have a good point distributionand preserve the original shape structures according to distance and noise scores. The computation relatedto this task is summarized in Alg. 2. During the generation process, LJLs are activated from starting steps (SS) till ending steps (T ). This impliesthat in each valid generation step, the LJL undergoes multiple iterations until convergence. A systematicparameter searching for SS is shown in Appendix C.2.1. It is not necessarily advantageous to perform LJLsat every iteration, since LJLs slow down the convergence of the generation in the beginning steps. LJL-embedding starts in the second half of the generation steps. We also circumvent embedding LJLs in the lastfew steps to ensure that no extra noise is introduced. Point clouds are normalized into the cube 3,and we set = 2 and = 5 due to the additional third dimension. The decaying time step t in i-thgeneration step is set as",
  ": Comparison of generation-only (red) and LJL-embedded generation (blue)": "where t(i) is scaled by the maximum difference between the current Xi and the previous Xi1 pointcloud, such that each position modification X caused by LJLs maintains similar scale. Furthermore, t(i)is divided by the iteration number to diminish the redistribution effect as generation steps increase. In orderto determine and , we perform a systematic parameter searching based on distance and noise scores,see Appendix C.2.2. We found that = 2.5 and = 0.01 meet the requirements of less noise and betterdistribution. Incorporating the optimal parameters, we tested LJL-embedded generative models on ShapeNet (Changet al., 2015) shown in . We evaluate the chair and airplane datasets using DDPM model, while thecar dataset is evaluated with ShapeGF model. There is a 99.2% increase in the DistanceScore and a 7.8%increase in the noise score for the car dataset. The point distribution is improved by 42.8% while the noisescore is only increased by 2.8% for airplanes. Finally, we get a 207.9% increase in DistanceScore, and 19.1%increase in the noise score for the chair dataset, shown in . More details about LJL parameter settingsand generation results can be found in Appendix C.2.",
  "LJL-embedded Denoising Model for 3D Point Cloud": "Given a noisy point cloud X0 = {xNi } which is normalized into the cube 3, the score-based model (Luo& Hu, 2021b) utilizes stochastic gradient ascent to denoise it step by step. In each intermediate step, thedenoising network S(X) predicts the gradient of the log-probability function (score) which is a vector fielddepicting vectors pointing from each point position towards the estimated underlying surface. The gradientascent denoising process will iteratively update each point position to converge to the predicted surface,",
  "X(t) = X(t1) + tS(X(t1)), t = 1, . . . , T ,": "where t is denoising step size for t-th iteration. LJLs are inserted after each intermediate denoising stepexcept the last few iterations to avoid extra perturbation to the denoised results. LJL parameter settings andevaluation metrics used in this task are inherited from previous generation tasks, where = 5 and = 2.We keep = 0.01 and choose = 0.3 which will minimize the ratio of the noise and distance score incrementrates, see Appendix C.1.1. The only difference is that the position update scale (max Xi Xi1) for thedenoising step is different from that in the generation task. The denoising results then do not only convergeto the predicted surface with less distortion but are also distributed uniformly as shown in . We continue to determine appropriate denoising iterations and the performance of denoising models (withand without LJLs) on different noise scales (more and less) shown in . We apply isotropic Gaussiannoise with a higher noise scale of 3% and a lower noise scale of 1% of the shapes bounding sphere radius asthe standard deviation. This is because denoising tends to progressively smooth the surface and wash outthe details. This phenomenon is shown in . It is clear that no matter the noise scales, for the denoise-only model, the noise score increases when thenumber of iterations goes up. While for the LJL-embedded denoising model, the noise score remains steady",
  "(b)": ": (a) Denoising results of lower noise scale point clouds with respect to increasing denoising itera-tions. (b) Denoising results of higher noise scale point clouds with respect to increasing denoising iterations. for the first 60 denoising iterations and the distance scores of both models decrease when iteration timesincrease regardless of noise scales. We further evaluate the LJL-embedded denoising model on the test setfrom Luo & Hu (2021b) shown in . By combining LJLs with the denoising network, we find that itimproves DistanceScore with negligible increments of NoiseScore. Moreover, we boost the performance ofexisting denoising models without extra training. More LJL-embedded denoising results and evaluations areshown in Appendix C.1.",
  "Conclusion": "In this contribution, we have extended the concept of the LJ potential describing pairwise repulsive andweakly attractive interactions of atoms and molecules to a variety of tasks that require equalizing the densityof a set of points without destroying the overall structure. It has been shown that LJLs are conceptually andpractically capable of generating high-quality blue noise distributions. To demonstrate the benefit of applyingLJLs in the context of the 3D point cloud generation task, we incorporated LJLs into the generative modelsShapeGF (Cai et al., 2020) and DDPM (Luo & Hu, 2021a) aiming for the generation of point clouds with animproved point distribution. By embedding LJLs in certain intermediate-generation steps, the clusters andholes are decreased significantly because LJLs increase the chances of points converging to cover the entireshape. Finally, we combine LJLs with a score-based point cloud denoising network (Luo & Hu, 2021b) suchthat the noisy point clouds are not only moved towards the predicted surfaces but also maintain the uniformdistribution. Consequently, this rearrangement prevents the formation of clusters and holes. Since addingLJLs in these cases does not require retraining the network, the cost of applying LJLs is negligible. The presented work offers several avenues for future work. From an algorithmic perspective, the k-nearestneighbor classification performance could be improved through supervised metric learning methods such asneighborhood components analysis and large margin nearest neighbor. The distribution normalization onsurfaces could be addressed by means of considering tangential bundles. By doing this, we force particles notto leave tangential manifolds during LJL interactions and in a perfect case to move solely along the geodesicsof the implied surfaces. This would require that we do not distort the underlying surfaces while performingdistribution normalization. Geodesical distances between points on surfaces also refer to a uniform sampling,which seems more adequate than Euclidean distances of embedding spaces. The reconstruction of geodesicsfrom point clouds has been explored by Crane et al. (2013). From an analytical point of view, the presentedLJL for distribution normalization has a discrete, piecewise character as it takes only the closest neighborsinto account which change from iteration to iteration. Most common approaches in theoretical mechanicsaim for a continuous description, e.g., based on Hamiltonians and their analytical solution or numericalintegration.It is theoretically not yet clear if it is possible to obtain a blue noise-type arrangement ofparticles utilizing pure Hamiltonian mechanics. For future work, we would like to further investigate thisquestion. On a slightly different trajectory, we would like to explore the possibilities of relativistically movingparticles. If such particles move within a medium with a suitable refraction index, then the power densityof radiation grows linearly with the frequency. This effect is well studied in the literature and known asVavilov-Cherenkov radiation (Cherenkov, 1934; Jackson, 1999).",
  "Robert Bridson. Fast poisson disk sampling in arbitrary dimensions. SIGGRAPH sketches, 10(1):1, 2007": "Ruojin Cai, Guandao Yang, Hadar Averbuch-Elor, Zekun Hao, Serge Belongie, Noah Snavely, and BharathHariharan. Learning gradient fields for shape generation. In Proceedings of the European Conference onComputer Vision (ECCV), 2020. Angel X. Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, SilvioSavarese, Manolis Savva, Shuran Song, Hao Su, Jianxiong Xiao, Li Yi, and Fisher Yu. ShapeNet: AnInformation-Rich 3D Model Repository. Technical Report arXiv:1512.03012 [cs.GR], Stanford University Princeton University Toyota Technological Institute at Chicago, 2015.",
  "Min Jiang, Yahan Zhou, Rui Wang, Richard Southern, and Jian Jun Zhang. Blue noise sampling using ansph-based method. ACM Trans. Graph., 34(6), nov 2015. ISSN 0730-0301. doi: 10.1145/2816795.2818102": "John Edward Jones. On the determination of molecular fields. i. from the variation of the viscosity ofa gas with temperature. Proceedings of the Royal Society of London. Series A, Containing Papers of aMathematical and Physical Character, 106(738):441462, 1924a. John Edward Jones. On the determination of molecular fields. ii. from the equation of state of a gas.Proceedings of the Royal Society of London. Series A, Containing Papers of a Mathematical and PhysicalCharacter, 106(738):463477, 1924b. William L. Jorgensen, David S. Maxwell, and Julian Tirado-Rives. Development and testing of the opls all-atom force field on conformational energetics and properties of organic liquids. Journal of the AmericanChemical Society, 118(45):1122511236, 1996. doi: 10.1021/ja9621760.",
  "Dennis C. Rapaport. The Art of Molecular Dynamics Simulation. Cambridge University Press, 2004. ISBN9780521825689": "Danilo Jimenez Rezende and Shakir Mohamed. Variational inference with normalizing flows. In Proceedingsof the 32nd International Conference on International Conference on Machine Learning - Volume 37,ICML15, pp. 15301538. JMLR.org, 2015. Jean-Paul Ryckaert, Giovanni Ciccotti, and Herman J.C Berendsen. Numerical integration of the cartesianequations of motion of a system with constraints: molecular dynamics of n-alkanes. Journal of Computa-tional Physics, 23(3):327341, 1977. ISSN 0021-9991.",
  "Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. CoRR,abs/1907.05600, 2019": "Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole.Score-based generative modeling through stochastic differential equations. In International Conference onLearning Representations, 2021. Erik Van Der Giessen, Peter A Schultz, Nicolas Bertin, Vasily V Bulatov, Wei Cai, Gbor Csnyi, Stephen MFoiles, Marc GD Geers, Carlos Gonzlez, Markus Htter, et al. Roadmap on multiscale materials modeling.Modelling and Simulation in Materials Science and Engineering, 28(4):043001, 2020.",
  "Kin-Ming Wong and Tien-Tsin Wong. Blue noise sampling using an n-body simulation-based method. Vis.Comput., 33(68):823832, jun 2017. ISSN 0178-2789. doi: 10.1007/s00371-017-1382-9": "Haozhe Xie, Hongxun Yao, Shangchen Zhou, Jiageng Mao, Shengping Zhang, and Wenxiu Sun. Grnet:Gridding residual network for dense point cloud completion. In Computer Vision ECCV 2020: 16thEuropean Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part IX, pp. 365381, Berlin,Heidelberg, 2020. Springer-Verlag. ISBN 978-3-030-58544-0. doi: 10.1007/978-3-030-58545-7_21. Yin Xu, Ligang Liu, Craig Gotsman, and Steven J. Gortler. Capacity-constrained delaunay triangulationfor point distributions. Computers & Graphics, 35(3):510516, 2011. ISSN 0097-8493. doi: Shape Modeling International (SMI) Conference 2011.",
  "AConvergence Properties of the Lennard-Jones Layer": "The LJLs output is computed by repetitively integrating forward in time while damping the motion of eachparticle over time. As a consequence of the damping effect, we observe that almost no changes to the nearestneighbors are present at the later stage of the process. Assuming that there are no changes to the nearestneighbors, we can theoretically prove the convergence of the LJL to the global potential minimum as outlinedin the following theorem. Theorem: The LJLs output is uniquely defined matching the single (global) minimum of the correspondingpotential under the assumption that there are no changes of the nearest neighbors throughout the process. Proof: Without loss of generality, let us consider a set which is composed of an even number of N particles.It is processed by the LJL, i.e., the dynamical process of each pair of particles is governed by the LJ potentialdefined in Eq. (1). The total potential V controlling the process carried out by the LJL is given as the sumof the LJ potentials of the N/2 pairs, i.e.,",
  "for a local extremum or saddle particle. The secondpartial derivative test results in V j (rj ) > 0 for which reason, the potential V has a local minimum at(r1, . . . , rN/2) =6": "2 (, . . . , ) with V(r1, . . . , rN/2) = (, . . . , ). This minimum is further the only localextremum and the global minimum of V which we can easily see by considering limrj0+ Vj(rj) = andlimrj Vj(rj) = 0, j = 1, . . . N/2.",
  "B.1Numerical Examples on 2D Euclidean Plane": "Before applying LJLs to practical applications, we now analyze the behavior of LJLs in different situations(i.e., different numbers of neighbors k, with or without attraction term in LJ potential).We set LJLparameters = 0.5, = 0.01, = 2, and = 5. All different cases are initialized with uncorrelatedrandomly distributed points inside a unit square region.",
  "B.1.1k-NN": "The interaction acting on individual particles according to LJ potential can be summed with a varyingnumber of nearest neighbors. Our theoretical finding guarantees convergence only for the consideration of asingle nearest neighbor. The experiment shown in indicates point configurations of LJLs consideringa different number of nearest neighbors k. Cases with more than one neighbor increase the difficulty ofreaching uniform point distribution.",
  "B.1.2The Impact of the Attraction Term": "We continue to discover the impact of attraction term in LJ potential. Two experiments, shown in runin identical settings except for the appearance of the attraction term. Intuitively, the repulsive term aloneshould achieve an effect of redistribution. Without attraction term, however, particles are easily pulled outof the initial boundaries and spread out further. LJLs with the attraction term can enable the overall controlof the point set and prevent particles from spreading out, which leads to a compact and equal distribution.",
  ":An illustration of howLJLs perform redistribution of clus-tered particles": "In , two particles A and B form a cluster when they aredirectly projected on the surface (A and B).With the help ofLJL rearrangement, particles are moved to new positions (ALJ andBLJ) and further projected back on the surface with proper distance(ALJ and BLJ). In this case, the redistributed points are not onlylocated on the surface but also have blue noise properties.Thecomputations involved in LJL-guided point cloud redistribution aresummarized in Algorithm 3, where we start with a random 3D pointcloud X0 initialized inside the cube 3 and a normalized meshsurface M. In order to select nearest neighbors on the same side of",
  "CLJL-embedded Deep Neural Networks": "In this section, we show more details of LJL-embedded networks parameter searching and results. We testall models on NVIDIA GeForce RTX 3090 GPU if not specified otherwise. Two well-trained point cloudgenerative models used in the papers are from ShapeGF (Cai et al., 2020) and DDPM (Luo & Hu, 2021a).The well-trained point cloud denoising model is from Luo & Hu (2021b).",
  "C.2.1Systematic Parameter Searching for Starting Steps": "Assuming that it takes T = 100 steps to generate point clouds, we continue to search for a good startingsteps SS to insert LJLs shown in . SS = 0 means LJLs are applied from the very beginning, whereasSS = 101 means generation without LJLs. In order to balance the generation speed and LJL normalizationeffects, we choose SS = 60, meaning embedding should start in the second half of the generation steps.",
  "C.2.3Point Cloud Generation Process with Different Numbers of Points": "We especially focus on the task of using as few points as possible in generation tasks. For use cases with alarge number of points, inefficient point distributions are less of a concern since even for low-density regions,there are sufficient points to describe the underlying surface. shows how LJLs behave in the generationprocess with different numbers of points N. Note that our choice of = 5 implicitly accounts for the changein N. In the case of generation tasks with fewer points, the LJL-embedded generator can redistribute thelimited number of points as even as possible while maintaining the global structure. Combining LJLs withgenerative models enables points to have the ability to converge to the different parts of the shape."
}