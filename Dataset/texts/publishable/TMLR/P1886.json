{
  "Abstract": "Edge applications, such as collaborative robotics and spacecraft rendezvous, demand efficient6D object pose estimation on resource-constrained embedded platforms. Existing 6D objectpose estimation networks are often too large for such deployments, necessitating compressionwhile maintaining reliable performance. To address this challenge, we introduce ModularQuantization-Aware Training (MQAT), an adaptive and mixed-precision quantization-awaretraining strategy that exploits the modular structure of modern 6D object pose estimationarchitectures. MQAT guides a systematic gradated modular quantization sequence and de-termines module-specific bit precisions, leading to quantized models that outperform thoseproduced by state-of-the-art uniform and mixed-precision quantization techniques. Our ex-periments showcase the generality of MQAT across datasets, architectures, and quantizationalgorithms. Additionally, we observe that MQAT quantized models can achieve an accuracyboost (> 7% ADI-0.1d) over the baseline full-precision network while reducing model sizeby a factor of 4 or more.",
  "Introduction": "6D pose estimation is the process of estimating the 6 degrees of freedom (attitude and position) of a rigidbody and has emerged as a crucial component in numerous situations, particularly in robotics applicationssuch as automated manufacturing (Prez et al., 2016), vision-based control (Singh et al., 2022), collaborativerobotics (Vicentini, 2020) and spacecraft rendezvous (Song et al., 2022). However, such applications typicallymust run on embedded platforms with limited hardware resources. These resource constraints often disqualify current state-of-the-art methods, such as ZebraPose (Su et al.,2022), SO-Pose (Di et al., 2021), and GDR-Net (Wang et al., 2021), which employ a two stage approach(detection followed by pose estimation) and thus entail a large memory footprint. By contrast, single-stagemethods (Thalhammer et al., 2021; Peng et al., 2019; Song et al., 2020; Hu et al., 2021b; Wang et al., 2022;Chen et al., 2019; Rad & Lepetit, 2017; Hoda et al., 2020) offer a more pragmatic alternative, yieldingmodels with a good accuracy-footprint tradeoff. Nevertheless, they remain too large for deployment on edgedevices. To address this challenge, CA-SpaceNet (Wang et al., 2022) applies a uniform quantization approach toreduce the network memory footprint at the expense of a large accuracy loss; all network layers are quantizedto the same bit width, except for the first and last layer. In principle, mixed-precision quantization methods (Cai & Vasconcelos, 2020; Dong et al., 2020; Tang et al.,2022) could demonstrate similar compression with better performance, but they tend to require significanteffort and GPU hours to determine the optimal bit precision for each layer. Furthermore, neither mixed-precision nor uniform quantization methods consider the importance of the order in which the networkweights or layers are quantized, as searching for the optimal order is combinatorial in the number of, e.g.,network layers.",
  "age the inherent modular structure of modern 6D object053": ": Summary of this Work. In contrast to uniform and mixed-precision quantization, MQAT accountsfor the modularity of typical 6D object pose estimation frameworks.Uniform QAT quantizes an entire networksimultaneously and uniformly; in mixed-precision QAT, layers are quantized to varying bit precisions regardless oftheir position; in contrast, MQAT applies quantization to network modules in a proposed order, with each moduleassigned the optimal bit precision. MQAT not only reduces the memory footprint of the network but can result inan accuracy boost that neither uniform nor mixed-precision quantization have demonstrated. The results shown inthe figure represent a comparison of different quantization methods applied to the WDR network, evaluated on theSwissCube dataset. In this work, we depart from such conventional Quantization-Aware Training (QAT) approaches and leveragethe inherent modular structure of modern 6D object pose estimation architectures, which typically encom-pass components such as a backbone, a feature aggregation module, and prediction heads. Specifically, weintroduce a Modular Quantization-Aware Training (MQAT) paradigm that relies on a gradated quantizationstrategy, where the modules are quantized and fine-tuned in a mixed-precision way, following a recommendedsequential order (i.e., quantization flow) based on their sensitivity to quantization. As the number of modulesin an architecture is much lower than that of layers, it is possible to confirm a quantization flow is optimal. Our experiments evidence that MQAT is particularly well-suited to 6D object pose estimation, consistentlyoutperforming the state-of-the-art quantization techniques in terms of accuracy for a given memory consump-tion budget, as shown in fig. 1. We demonstrate the generality of our approach by applying it to differentsingle-stage architectures, WDR (Hu et al., 2021b), CA-SpaceNet (Wang et al., 2022); different datasets,SwissCube (Hu et al., 2021b), Linemod (Hinterstoisser et al., 2012), and Occlusion Linemod (Brachmannet al., 2014); and using different quantization strategies, INQ (Zhou et al., 2017) and LSQ (Esser et al.,2020), within our modular strategy.We further show that our method also applies to two-stage networks,such as ZebraPose (Su et al., 2022), on which it outperforms both uniform and mixed-precision quantiza-tion. Furthermore, we extend MQAT to the task of object detection, further validating its efficacy in thecomputer vision domain. The results of applying MQAT to object detection are presented in appendix A.2,demonstrating its potential for broader applicability.",
  "To summarize, our main contributions are as follows:": "For neural network compression, we are the first to identify and exploit the current trend of modulararchitectures. We propose Modular Quantization-Aware Training (MQAT), a novel mixed-precisionQuantization-Aware Training (QAT) algorithm. We further demonstrate that to achieve the best results, the sequence of quantizing modules issignificant and thus propose a recommended quantization order. We show that this order is optimalfor the quantization of a 3 module 6D pose estimation network (Hu et al., 2021b). With MQAT, we show substantial accuracy gains over competitive QAT methods. Furthermore,unlike other QATs, MQAT significantly surpasses full-precision accuracy in the specific case ofsingle-stage 6D pose estimation networks (Hu et al., 2021b; Wang et al., 2022).",
  "Related Work": "In this section, we survey recent advances in RGB-based 6D object pose estimation, outlining key architec-tures and their contributions to the field. We then explore the developments in quantization-aware training(QAT), particularly in relation to modular neural network designs. This review sets the groundwork forour proposed Modular Quantization-Aware Training framework, which is inspired by these advances andaddresses their limitations.",
  "D Object Pose Estimation": "Single-Stage Direct Estimation.PoseCNN (Xiang et al., 2018) was one of the first methods to es-timate 6D object pose using a deep neural network.SSD6D (Liu et al., 2016; Kehl et al., 2017) andURSONet (Proena & Gao, 2020) used a discretization of the rotation space to form a classification probleminstead of a regression one. Single-Stage with PnP.In general, a better performing strategy for convolutional architectures consistsof training a network to predict 2D-to-3D correspondences instead of the pose. The pose is then obtained viaa RANdom SAmple Consensus (RANSAC) / Perspective-n-Point (PnP) 2Dto3D correspondence fittingprocess. These methods typically employ a backbone, a feature aggregation module, and one or multipleheads.(Rad & Lepetit, 2017; Tekin et al., 2018) estimate these correspondences in a single global fashion,whereas (Peng et al., 2019; Jafari et al., 2018; Hu et al., 2019; Zakharov et al., 2019; Markus Oberweger,2018; Chen et al., 2019) aggregate multiple local predictions to improve robustness. To improve performancein the presence of large depth variations, a number of works (Thalhammer et al., 2021; Hu et al., 2021b;Wang et al., 2022) use an FPN (Lin et al., 2016) to exploit features at different scales. Multi-StageThe current state-of-the-art pose estimation frameworks incorporate a pipeline of networksthat perform different tasks. In the first stage network, the target is localized and a Region of Interest (RoI)is cropped and forwarded to the second stage network. This isolates the position estimation task from theorientation estimation one and further provides the orientation estimation network with an RoI containingonly object features. The second stage orientation estimation network can then more easily fit to the targetobject. Multi-stage with PnP approaches have generated good results (Su et al., 2022; Di et al., 2021; Wang et al.,2021; Li et al., 2019b; Labb et al., 2020), but some recent works avoid traditional PnP and RANSACmethods. PViT-6D (Stapf et al., 2023) uses an off-the-shelf object detector to crop the target and thenreframes pose estimation as a direct regression task, using Vision Transformers to enhance accuracy. Check-erPose (Lian & Ling, 2023) improves robustness by progressively refining dense 3D keypoint correspondenceswith a graph neural network. MRC-Net (Li et al., 2024) introduces a two-stage process, combining poseclassification and rendering with a multi-scale residual correlation (MRC) layer to capture fine-grained posedetails. GigaPose (Nguyen et al., 2024) achieves fast and robust pose estimation through CAD-based tem-plate matching and patch correspondences.FoundationPose (Wen et al., 2024) unifies model-based andmodel-free setups using neural implicit representations and contrastive learning, achieving strong general-ization without fine-tuning. Multi-stage frameworks tend to yield more accurate results. However, they also have much larger memoryfootprints as they may include one object classifier network; one object position/RoI network; and N objectpose networks. For hardware-restricted scenarios, a multi-stage framework may thus not be practical. Evenfor single-stage networks, additional compression is required (Blalock et al., 2020).",
  "Quantization-Aware Training": "Neural network quantization reduces the precision of parameters such as weights and activations. Existingtechniques fall into two broad categories. Post-Training Quantization (PTQ) quantizes a pre-trainednetwork using a small calibration dataset and is thus relatively simple to implement (Nagel et al., 2020; Liet al., 2021; Frantar & Alistarh, 2022; Zhao et al., 2019; Cai et al., 2020; Nagel et al., 2019; Shao et al.,",
  "Pyrapose": "Individual ModelsOne Model : Performance Comparison on Occluded-LINEMOD. The marker size is proportional to thememory footprint. Individual Models refers to methods training one model for each object. One Model refersto methods training a single model for all objects. 2024; Lin et al., 2024; Chee et al., 2023). Quantization-Aware Training (QAT) retrains the networkduring the quantization process and thus better preserves the models full-precision accuracy. QAT methodsinclude uniform QAT (Esser et al., 2020; Zhou et al., 2017; Bhalgat et al., 2020; Yamamoto, 2021) andmixed-precision QAT (Cai & Vasconcelos, 2020; Dong et al., 2020; Tang et al., 2022; Dong et al., 2019; Chenet al., 2021; Yao et al., 2020). As accuracy can be critical in robotics applications relying on 6D object poseestimation, we focus on QAT. Uniform QAT methods quantize every layer of the network to the same precision. In Incremental NetworkQuantization (INQ) (Zhou et al., 2017), this is achieved by quantizing a uniform fraction of each layersweights at a time and continuing training until the next quantization step. Quantization can be achieved ina structured manner, where entire kernels are quantized at once, or in an unstructured manner. In contrast toINQ, Learned Step-size Quantization (LSQ) (Esser et al., 2020) quantizes the entire network in a single action.To this end, LSQ treats the quantization step-size as a learnable parameter. The method then alternatesbetween updating the weights and the step-size parameter. Group-Wise Quantization (GWQ) (Yang et al.,2023) shows comparable accuracy while claiming reduced computational costs at inference time by assigningthe same scale factor to different layers in the network. Conversely, Mixed-precision QAT methods treat each network layer uniquely, aiming to determine theappropriate bit precision for each one. In HAWQ (Dong et al., 2019; 2020; Yao et al., 2020), the networkweights Hessian is leveraged to assign bit precisions proportional to the gradients. In (Cai & Vasconcelos,2020), the mixed precision is taken even further by applying a different precision to different kernels within asingle channel. Finally, NIPQ(Shin et al., 2023) replace the standard straight-through estimator ubiquitous inquantization with a proxy, allowing the quantization truncation and scaling hyper-parameters to be includedin the gradient descent. Mixed-precision QAT is a challenging task; existing methods remain computationallyexpensive for modern deep network architectures.",
  "Quantization and Modular Deep Learning": "In recent years, deep network architectures have increasingly followed a modular paradigm, owing to itsadvantages in model design and training efficiency (Jacobs et al., 1991; Pfeiffer et al., 2023; Ansell et al., 2021;Hu et al., 2021a; Pfeiffer et al., 2020). This approach leverages reusable modules, amplifying the flexibilityand adaptability of neural networks and fostering parameter-efficient fine-tuning. In the quantization domain,several studies have also underscored the importance of selecting the appropriate granularity to bolster modelgeneralization (Li et al., 2021) and enhance training stability (Zhang et al., 2023). Furthermore, in the 6D object pose estimation domain, the demonstration of applying quantization remainslimited, with none of the aforementioned quantization techniques addressing the specific task or underlyingnetwork architecture. CA-SpaceNet (Wang et al., 2022) applied LSQ quantization to their proposed archi-tecture using three different quantization configurations. However they do so homogenously, not accountingfor the varying sensitivity to quantization throughout the network.",
  ": Representative 6D Object Pose Estimation Network with K = 3 modules. From left toright, we denote them as backbone, feature aggregation, and heads": "To our knowledge, no existing research has advocated a systematic methodology for executing modularquantization-aware training, let alone studied the impact of quantization order on modular architectures.Thus, in this work, we aim to bridge this gap by introducing a comprehensive methodology for modularQAT, tailored but not limited to 6D object pose estimation.",
  "Modular Architecture for 6D Pose Estimation": "As discussed in section 2, multi-stage networks (Su et al., 2022; Di et al., 2021; Wang et al., 2021; Labbet al., 2020; Li et al., 2019b) tend to induce large memory footprints and large latencies, thus making poorcandidates for hardware-restricted applications. This is further evidenced in fig. 2, where we compare anumber of 6D object pose estimation architectures memory footprint, throughput and accuracy on the O-LINEMOD dataset. While demonstrating admirable accuracy, the size and latency of ZebraPose (Su et al.,2022) and SO-pose (Di et al., 2021) preclude their inclusion in hardware-restricted platforms. On this basis,we therefore focus on single-stage1 6D object pose estimation networks (Thalhammer et al., 2021; Wanget al., 2022; Tekin et al., 2018; Peng et al., 2019; Hu et al., 2021b). In general, 6D pose estimation networks contain multiple modules, with each module performing a distincttask. A typical encoder-decoder-head 6D pose estimation architecture is illustrated in fig. 3. Explicitly:",
  ". A backbone or encoder module extracts features from an image at different depths. Common moduleexamples include ResNets, DarkNets and MobileNets": "2. A feature aggregator or decoder aggregates or interprets the latent feature space. Common moduleexamples include feature pyramid networks, attention layers, and simple upscaling CNNs (i.e., latterhalf of U-Nets). 3. A head module performs specific tasks or regresses specific quantities such as location vectors andcorrespondences. Commonly correspondence and location heads are fully-connected layers but mayalso be attention-based or CNNs for other tasks such as image segmentation. Architectures for various tasks similarly consist of clearly identifiable modules. Therefore, we also demon-strate the effectiveness of our proposed MQAT method for object detection, as detailed in the Appendix.",
  "SORT _SIZE(B)Sort list of modules B based on their size": "2 bit quantizations for each module, Bk. The module is retained as quantized if it results in an improvedaccuracy for the entire model, (i.e., M qk outperforms M). This establishes a new baseline from which theremaining module bit precision optimization can be performed. Quantization Flow (Lines 8:15). The sequence of module quantization the quantization flow is criticalas quantizing the modules of a network is not commutative; we prioritize starting with modules that do notcompromise accuracy, as quantization errors introduced early on are typically not mitigated by later steps.Moreover, we also observe that quantization-related noise can lead to weight instability (Dfossez et al., 2022;Shin et al., 2023; Peters et al., 2023), hindering the performance of the quantized network. If no quantizedmodule yielded an improved accuracy in the quantization baseline step, we proceed with quantizing themodule with the lowest number of parameters first. Modules with a greater number of parameters will havemore flexibility to adapt to aggressive quantization. The output of this algorithmic step is the recommendedquantization flow, which is then passed to the next step. Optimal Bit Precision (Lines 16:21). In MQAT, the optimal bit precision for each module (excepting aquantization baseline identified Mbest, kbest) is ascertained through a process of constrained optimization,Integer Linear Programming (ILP), drawing inspiration from Yao et al. (2020). However, our methodol-ogy distinguishes itself by offering a lower degree of granularity. Central to our strategy is the uniformquantization of all layers within a given module to an identical bit-width. This design choice not only sim-plifies the computational complexity but also significantly enhances the hardware compatibility, an essentialconsideration for efficient real-world deployment. To achieve this, we introduce the importance metric for modular quantization. This metric is conceptualizedas the product of two factors: the sensitivity metric, , for each layer in a module which is computed by asimilar approach to that in Dong et al. (2020), and the quantization weight error. The latter is calculated asthe squared 2-norm difference between the quantized and full precision weights. Therefore, the importancemetric is given by",
  "Experiments": "Historically, quantization and other compression methods have been used to exercise a trade-off betweeninference accuracy and deployment feasibility, particularly in resource-constrained circumstances. In thefollowing sections, we will show that our approach may yield a significant inference accuracy improvementduring compression. We first introduce the datasets and metrics used for evaluation. Then, we present ablation studies to explorethe properties of MQAT; this result is directly compared to uniform and mixed QAT methods. Finally, wedemonstrate the generality of our method applied to different datasets, architectures, and QAT methods.",
  "Datasets and Metrics": "The LINEMOD (LM) and Occluded-LINEMOD (LM-O) datasets are standard BOP benchmark datasetsfor evaluating 6D object pose estimation methods, where the LM dataset contains 13 different sequencesconsisting of ground-truth poses for a single object. LM-O extends LM by including occlusions. Similarto GDR-Net (Wang et al., 2021), we utilize 15% of the images for training. For both datasets, additionalrendered images are used during training (Wang et al., 2021; Peng et al., 2019). Similarly to previous works,we use the ADD and ADI/ADD-S error metrics2 (Hoda et al., 2016) expressed as",
  "i=1minj[1,V ]P esti P gtj 2 ,(4)": "where P estiand P gtidenote the ith vertex of the 3D mesh after transformation with the predicted andground-truth pose, respectively. We then report the accuracy using the ADD-0.1d and ADD-0.5d metrics,which encode the proportion of samples for which eADD is less than 10% and 50% of the object diameter.Similarly, we report ADI-0.1d and ADI-0.5d metrics for swisscube dataset to be consistent with (Hu et al.,2021b). While LM and LM-O present their own set of challenges, their scope is restricted to household objects,consistently illuminated without significant depth variations. Conversely, the SwissCube dataset (Hu et al.,",
  "Implementation Details": "We use PyTorch to implement our method. For the retraining of our partially quantized pretrained network,we employ an SGD optimizer with a base learning rate of 1e-2. It is common practice for quantizationalgorithms to start with a pre-trained model (Esser et al., 2020; Dong et al., 2019; Zhou et al., 2017); wesimilarly do so here. We employed 30 epochs to identify the starting module at 2bits (Alg. 1 Lines 1-7) andthen trained 30 epochs per module (Alg. 1 Lines 18-21). Therefore MQAT training time will scale linearlywith additional modules. Specifically, WDR with M=3 results in 120 epochs and ZebraPose with M=2 resultsin 90 epochs. We use the same training time respectively for LSQ, INQ, HAWQ for a fair comparison. Forall experiments, we use a batch size of 8 and employ a hand-crafted learning scheduler which decreases thelearning rate at regular intervals by a factor of 10 and increases it again when we quantize a module withINQ3. However, when we quantize our modules using LSQ, the learning rate factor is not increased, onlydecreased by factors of 10. We use a 512 512 resolution input for the SwissCube dataset and 640 480 forLM and LM-O as in Peng et al. (2019).",
  "MQAT Quantization Flow": "We first perform an ablation study to validate the recommended quantization flows sequence optimality.As discussed in section 3.2, the module quantization sequence is not commutative. Using WDR, we performaggressive quantization to every combination of modules in the network. This is an O(2K) search; this resultsin eight module quantization combinations for a network with K = 3 modules. The results are visualizedin fig. 4. The backbone and head modules exhibit greater sensitivity to aggressive quantization. Conversely,the accuracy of the network is enhanced when using 2 bit quantization on the Feature Pyramid Network(FPN) module only. No other combination of module quantizations yields an accuracy increase. This furtheremphasizes the importance of carefully selecting a module quantization flow. We additionally perform ablation studies on the optimal order (i.e., flow) of module quantization.Webegin by quantizing different modules first, instead of the FPN. shows the results of both the headand backbone modules when they are the first module quantized. We observe that the inference accuracydecreases dramatically for both cases. No combination of module flow or bit precision schedule is able to",
  "Quantized FPN Sensitivity Study": "To expand upon the 2 bit FPN accuracy enhancement, we perform a higher granularity bit-precision searchon the FPN module. Again, the FPN module was quantized, but to five different bit-widths for comparison;the results are presented in fig. 5. The accuracy of two full-precision networks, WDR (Hu et al., 2021b)and CA-SpaceNet (Wang et al., 2022), are shown with dashed lines. The highest accuracy is achieved witha 2 bit or ternary {1, 0, 1} FPN. Further pushing the FPN to binary weights {1, 1} slightly reduces theaccuracy, but maintains a significant improvement over both baselines4. In the context of our study, as depicted in fig. 5, our findings support the premise that varying bit-precisionsacross different modules within QAT, such as the FPN, can significantly influence overall performance. Thiscan be attributed to a redistribution of computes or inherent regularization effects on the modules. Theimprovements in performance reinforce the potential of MQAT in enhancing model generalizability.",
  "MQAT Compared to Uniform and Mixed QAT": "For direct comparison, we apply three different quantization paradigms. Starting from a full precision WDRnetwork, we apply a uniform QAT method, LSQ (Esser et al., 2020), a mixed-precision QAT method, HAWQ-V3 (Yao et al., 2020), and finally our proposed MQAT method with increasing compression factors. Theresults are provided in fig. 6. Again, MQAT demonstrates a significant accuracy improvement comparingto other methods while sustaining the requested compression factor; it is the only quantization approach toshow an increase in inference accuracy during compression.",
  "Dataset and QAT Generality": "As discussed in section 4.1, the image domains of LM, LM-O and SwissCube are vastly different. The fullprecision and MQAT quantized models results for all three datasets are shown in table 3. MQAT demon-strates an accuracy improvement in all datasets. We use the ADI metric for evaluation on the SwissCubedataset as in (Hu et al., 2021b; Wang et al., 2022), while we use the ADD metric for LM and LM-O as usedby (Wang et al., 2021; Su et al., 2022; Thalhammer et al., 2021; Labb et al., 2020; Peng et al., 2019). Accuracy improvements of 5.0%, 7.8% and 2.4% are demonstrated on SwissCube, LM and LM-O, respectively,when MQAT with INQ is utilized. Replacing INQ with LSQ yields accuracy improvements of 4.6%, 7.5%and 2.0%, respectively. This evidences that the performance enhancement is independent of the datasetdomain and the applied QAT method. As discussed in section 3.2 and section 4.3.1, it is difficult to recover accuracy once it is lost during quanti-zation. To this end, since INQ (Zhou et al., 2017) quantizes only a fraction of the network at once, it followsthat the remaining unquantized portion of the network is left flexible to adapt to aggressive quantization.Conversely, LSQ (Esser et al., 2020) quantizes the entire network in a single step; no fraction of the networkis left unperturbed. Consequently, INQ demonstrates superior results in table 3. While any QAT methodmay be used, we recommend partnering MQAT with INQ for optimal aggressive quantization results.",
  "Architecture Generality": "In table 2, we compare several single-stage PnP architectures on the SwissCube dataset. To demonstratethe generality of our performance enhancement, we apply MQAT to aggressively quantize the FPN of bothCA-SpaceNet (Wang et al., 2022) and WDR (Hu et al., 2021b). We demonstrate an accuracy improvementof 4.5%, 4.4% and 4.5% for Near, Medium and Far images, respectively, on CA-SpaceNet, resulting in a totaltesting set accuracy improvement of 3.3%. Recall the already presented total testing set accuracy improve-ment of 5.0% for WDR. Previously, the full precision CA-SpaceNet had shown a performance improvementover the full precision WDR, but WDR sees greater gains from the application of MQAT. In addition, (Wang et al., 2022) published accuracy results for a uniform QAT quantized CA-Space network,shared in table 4. Specifically, CA-SpaceNet explored three quantization modes (B, BF and BFH). Thesecorrespond to quantizing the backbone, quantizing the backbone and FPN (paired), and quantizing thewhole network (uniformly), respectively. Finally, we evaluate MQAT on a multi-stage network architecture. Specifically, in , we demonstratethe performance of our method on the state-of-the-art 6D object pose estimation network, the two-stageZebraPose network (Su et al., 2022). Note that, when quantized using MQAT, the models performancecomes close to that of its full-precision counterpart while being more than four times smaller. Furthermore,MQAT outperforms the state-of-the-art HAWQ-V3 (Yao et al., 2020) by 2.6%, with the added advantage offurther network compression.To further demonstrate the efficacy, we quantize another two stage network i.e",
  "MQAT (Ours)80.28.24-2-4LSQ BFH68.710.63-3-3": "GDR-Net with existing uniform and mixed-precision methods, and MQAT. We employed ADD-0.1d metricfor 6D object pose evaluation for O-Linemod dataset as Wang et al. (2021). It is evident from table 6 thatMQAT outperforms both LSQ (Esser et al., 2020) and HAWQ-V3 (Yao et al., 2020) even with slightly morecompressed network. As we demonstrated in section 4.3.1, quantizing network modules all together greatly reduces inferenceaccuracy as the smaller unquantized fraction of the network is not able to adapt to the quantization. Ad-ditionally, quantizing from backbone to head does not consider the sensitivity of the network modules toquantization. As a final note, CA-SpaceNet does not quantize the first and last layer in any quantizationmode. In contrast, MQAT quantizes the entire network.",
  "Limitations and Discussion": "Module Granularity.As conclusively demonstrated in section 4.3.1, MQAT exploits the modular struc-ture of a network. Therefore, if the network does not contain distinct modules, MQAT simply converges toa uniform QAT methodology. In principle, MQAT can apply to any architectures with K 2. Latency.Directly reporting latency measurements involves hardware deployment, which goes beyond thescope of this work. However, as shown in Yao et al. (2020), latency is directly related to the bit operationsper second (BOPs). With lower-precision networks, both the model size and the BOPs are reduced by thesame compression factor, which we provide in our experiments. Therefore, it is expected that MQAT woulddemonstrate a latency improvement proportional to the network compression factor. Nonetheless, we can share some results running an MQAT quantized network on a CPU. Running WDRon our Intel Core i7-9750H CPU demonstrates a latency of 650ms for full precision and 299ms for ourMQAT int8 quantized model. However this does not exploit the full benefits of our MQAT quantized modelas all the CPU deployed parameters are stored at the highest common denominator of INT8 (whereas ourmodel employs lower bit-widths for certain parameters).",
  "MQAT (ours)51.84.978-4-4 (B-R-P)R P B": "Recommended Quantization Flow Optimality.A major contribution of this paper is the identificationof the existence of an asynchronous optimal quantization sequence or flow.In section 3.2 we provide arecommended quantization flow and exhaustively demonstrate its optimality for K = 3 in fig. 4. However,the recommended quantization flows optimality has yet to be analytically proven for all combinations ofnetworks and number of modules, K.",
  "Conclusion": "We have introduced Modular Quantization-Aware Training (MQAT) for networks that exhibit a modularstructure, such as 6D object pose estimation architectures. Our approach builds on the intuition that theindividual modules of such networks are unique, and thus should be quantized uniquely while heeding anoptimal quantization order.Our extensive experiments on different datasets and network architectures,and in conjunction with different quantization methods, conclusively demonstrate that MQAT outperformsuniform and mixed-precision quantization methods at various compression factors. Moreover, we have shownthat it can even enhance network performance. In particular, aggressive quantization of the network FPNresulted in 7.8% and 2.4% test set accuracy improvements over the full-precision network on LINEMODand Occluded-LINEMOD, respectively.In the future, we will investigate the applicability of MQAT totasks other than 6D object pose estimation and another potential follow up would be to apply MQAT toarchitectures with even more modules or to instead classify large modules as two or more modules for thepurposes of MQAT.",
  "Acknowledgements": "We thank Ziqi Zhao for assisting us with the experiments on Multi-stage architecture. This project hasreceived funding from the European Unions Horizon 2020 research and innovation programme under theMarie Skodowska-Curie grant agreement No. 945363. Moreover, this work was funded in part by the SwissNational Science Foundation and the Swiss Innovation Agency (Innosuisse) via the BRIDGE Discovery grantNo. 194729.",
  "Robert A Jacobs, Michael I Jordan, Steven J Nowlan, and Geoffrey E Hinton. Adaptive Mixtures of LocalExperts. In Neural computation, 1991": "Omid Hosseini Jafari, Siva Karthik Mustikovela, Karl Pertsch, Eric Brachmann, and Carsten Rother. iPose:Instance-Aware 6D Pose Estimation of Partly Occluded Objects. In Asian Conference on Computer Vision,2018. Wadim Kehl, Fabian Manhardt, Federico Tombari, Slobodan Ilic, and Nassir Navabx. SSD-6D: MakingRGB-Based 3D Detection and 6D Pose Estimation Great Again. In International Conference on ComputerVision, 2017.",
  "Rundong Li, Yan Wang, Feng Liang, Hongwei Qin, Junjie Yan, and Rui Fan. Fully Quantized Network forObject Detection. In Computer Vision and Pattern Recognition, 2019a": "Yuelong Li, Yafei Mao, Raja Bala, and Sunil Hadap. Mrc-net: 6-dof pose estimation with multiscale residualcorrelation. In 2024 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, 2024. Yuhang Li, Ruihao Gong, Zu Tan, Yang Yang, Peng Hu, Qi Zhang, Fengwei Yu, Wei Wang, and Shi Gu.BRECQ: Pushing the Limit of Post-Training Quantization by Block Reconstruction. In InternationalConference on Learning Representations, 2021.",
  "Ruyi Lian and Haibin Ling. CheckerPose: Progressive Dense Keypoint Localization for Object Pose Esti-mation with Graph Neural Network. In International Conference on Computer Vision, 2023": "Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Wei-Ming Chen, Wei-Chen Wang, Guangxuan Xiao,Xingyu Dang, Chuang Gan, and Song Han. AWQ: Activation-aware Weight Quantization for LLM Com-pression and Acceleration. In MLSys, 2024. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollr,and C Lawrence Zitnick. Microsoft COCO: Common Objects in Context. In European Conference onComputer Vision, 2014.",
  "Markus Nagel, Mart van Baalen, Tijmen Blankevoort, and Max Welling. Data-Free Quantization ThroughWeight Equalization and Bias Correction. In International Conference on Computer Vision, 2019": "Markus Nagel, Rana Ali Amjad, Mart van Baalen, Christos Louizos, and Tijmen Blanevoort. Up or Down?Adaptive Rounding for Post-Training Quantization. In International Conference on Machine Learning,2020. Van Nguyen Nguyen, Thibault Groueix, Mathieu Salzmann, and Vincent Lepetit. GigaPose: Fast and RobustNovel Object Pose Estimation via One Correspondence. In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition, 2024.",
  "Pedro F. Proena and Yang Gao. Deep Learning for Spacecraft Pose Estimation from Photorealistic Ren-dering. In International Conference on Robotics and Automation, 2020": "Mahdi Rad and Vincent Lepetit.BB8: A Scalable, Accurate, Robust to Partial Occlusion Method forPredicting the 3D Poses of Challenging Objects without Using Depth. In International Conference onComputer Vision, 2017. Wenqi Shao, Mengzhao Chen, Zhaoyang Zhang, Peng Xu, Lirui Zhao, Zhiqian Li, Kaipeng Zhang, Peng Gao,Yu Qiao, and Ping Luo. OmniQuant: Omnidirectionally Calibrated Quantization for Large LanguageModels. In International Conference on Learning Representations, 2024.",
  "Sergey Zakharov, Ivan Shugurov, and Slobodan Ilic. DPOD: 6D Pose Object Detector and Refiner. InInternational Conference on Computer Vision, 2019": "Yifan Zhang, Zhen Dong, Huanrui Yang, Ming Lu, Cheng-Ching Tseng, Yuan Du, Kurt Keutzer, Li Du,and Shanghang Zhang. QD-BEV: Quantization-aware View-guided Distillation for Multi-view 3D ObjectDetection. In International Conference on Computer Vision, 2023. Ritchie Zhao, Yuwei Hu, Jordan Dotzel, Chris De Sa, and Zhiru Zhang. Improving Neural Network Quan-tization without Retraining using Outlier Channel Splitting. In International Conference on MachineLearning, 2019. Aojun Zhou, Anbang Yao, Yiwen Guo, Lin Xu, and Yurong Chen.Incremental Network Quantization:Towards Lossless CNNs with Low-Precision Weights. In International Conference on Learning Represen-tations, 2017.",
  "A.1Multi-scale Fusion Analysis of MQAT Applied to WDR (Hu et al., 2021b)": "In this section we study the inference performance of our MQAT when applied to WDR, our primary focusis to understand the impact of MQAT on the multi-scale fusion inference of WDR, particularly examiningchanges in performance across individual layers of FPN module in WDR. To this end, we applied MQATto the WDR architecture and conducted a layer-by-layer performance analysis of FPN module similar tothe original WDR paper. The results are presented in . A notable observation from this analysis isthe overall enhancement in performance across most layers and the improvement is consistent across variousscales and depth ranges. Particularly, Layer 1 exhibits a significant performance boost, especially for objectsclassified as Far. This layer-specific insight underscores the effectiveness of MQAT in optimizing the WDRnetwork at a granular level.",
  "A.2Generality of MQAT in Object Detection Problem": "In our study, while the primary focus is on 6D pose estimation, where our methods efficacy is alreadydemonstrated, we further extend our evaluation to object detection tasks.This extension is aimed atunderscoring the generality of our approach. To this end, we applied our quantization technique to the Faster R-CNN network, which utilizes a ResNet-50 backbone, a widely recognized model in object detection tasks. Our evaluation was conducted on thecomprehensive COCO dataset, a benchmark for object detection.",
  ": Visualization of the Difference in Object Detection Performance on MSCOCO (Lin et al.,2014) between N2UQ and MQAT at the same compression ratio": "that MQAT not only adapts well to a different task domain but also achieves superior performance over theseestablished Quantization-Aware Training techniques. This underlines the adaptability and robustness of ourapproach, extending its potential applications beyond 6D pose estimation to broader areas within computervision. Moreover, we also created a baseline for the network, EfficientDet (Tan et al., 2020) by quantizing it with arecent quantization method: Non-Uniform to Uniform Quantization (N2UQ) (Liu et al., 2022)). This com-parison is crucial to validate the effectiveness of MQAT across various modular architectures and quantizationmethods. Remarkably, our MQAT approach demonstrated a performance improvement of approximately1.6% over N2UQ as shown in both . and fig. 8. This enhancement was observed under comparablecompression ratios and identical training durations, further substantiating the superiority of MQAT in termsof efficiency and effectiveness.",
  "A.4.1LR Schedule for INQ (Zhou et al., 2017)": "As mentioned in our experiments section, we employ a SGD optimizer with a base learning rate (lrb) of1e2. We trained for 30 epochs with a batch size of 8. We created a hand-crafted learning scheduler whichdecreases the learning rate at regular intervals by a factor of 10 and increases it again when we quantizea module. The gamma () and quantization fraction scheduler are shown in . The quantizationfraction corresponds to the percentage of weights quantized at each epoch. At each epoch, the learning rate(lr) is computed as:lr = lrb",
  "A.4.2LR Schedule for LSQ (Esser et al., 2020) and HAWQ-V3 (Yao et al., 2020)": "As mentioned in our experiments section, we employ a SGD optimizer with a base learning rate (lrb) of 1e-2.However, we used a multi-step decay scheduler here. Learning rate was decreased by factors of 10 at epochs{10, 20, 25} of the training. We trained for 30 epochs with a batch size of 8."
}