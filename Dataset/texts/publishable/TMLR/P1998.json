{
  "Abstract": "We propose the first study of adversarial attacks on online learning to rank. The goal ofthe attacker is to misguide the online learning to rank algorithm to place the target itemon top of the ranking list linear times to time horizon T with a sublinear attack cost. Wepropose generalized list poisoning attacks that perturb the ranking list presented to theuser. This strategy can efficiently attack any no-regret ranker in general stochastic clickmodels. Furthermore, we propose a click poisoning-based strategy named attack-then-quitthat can efficiently attack two representative OLTR algorithms for stochastic click models.We theoretically analyze the success and cost upper bound of the two proposed methods.Experimental results based on synthetic and real-world data further validate the effectivenessand cost-efficiency of the proposed attack strategies.",
  "Introduction": "Online learning to rank (OLTR) (Grotov and de Rijke, 2016) formulates learning to rank (Liu et al., 2009),the core problem in information retrieval, as a sequential decision-making problem. OLTR is a family of onlinelearning solutions that exploit implicit feedback from users (e.g., clicks) to directly optimize parameterizedrankers on the fly. It has drawn increasing attention in recent years (Kveton et al., 2015a; Zoghi et al., 2017;Lattimore et al., 2018; Oosterhuis and de Rijke, 2018; Wang et al., 2019; Jia et al., 2021; Wang et al., 2018; Aiet al., 2021) due to its advantages over traditional offline learning-based solutions and numerous applicationsin web search and recommender systems (Liu et al., 2009).",
  "Published in Transactions on Machine Learning Research (09/2024)": "The total regret is generated by K positions. Algorithm 1 only attacks when Rt\\T = . And situationRt\\T = implies there is at least one item T be placed in the Rt and its attractiveness is reduced to atmost (K). Due to when Rt\\T = , the number of items is placed in Rt and belongs to D\\T is at least 1.Then for the cascade model, the regret generates in round t is at least",
  "Online learning to rank.OLTR is first studied as ranked bandits (Radlinski et al., 2008; Slivkins et al.,": "2013), where each position in the list is modeled as an individual multi-armed bandits problem (Auer et al.,2002). Such a problem can be settled down by bandit algorithms which can maximize the expected clicknumber in each round. Recently studied of OLTR focused on different click models (Craswell et al., 2008;Chuklin et al., 2015), including the cascade model (Kveton et al., 2015a;b; Zong et al., 2016; Li et al., 2016;Vial et al., 2022), the position-based model (Lagre and Vernade, 2016) and the dependent click model(Katariya et al., 2016; Liu et al., 2018). OLTR with general stochastic click models is studied in (Zoghi et al.,2017; Lattimore et al., 2018; Li et al., 2018; 2019; Gauthier et al., 2022). Adversarial attack against bandits.Adversarial reward poisoning attacks against multi-armed banditshave been recently studied in stochastic bandits (Jun et al., 2018; Liu and Shroff, 2019; Zuo, 2020; Xu et al.,2021; Lu et al., 2021), linear bandits (Wang et al., 2022; Garcelon et al., 2020), Gaussian process bandits (Hanand Scarlett, 2021), adversarial bandits (Ma and Zhou, 2023), and combinatorial bandits (Balasubramanian",
  "Online Learning to Rank": "We denote the total item set with L items as D = {a1, ..., aL}.Let K(D) DK stands for all K-tuples with different elements from D. At each round t, the ranker would present a length-K ordered listRt = (at1, ..., atK) K(D) to the user, where atk is the item placed at the k-th position of Rt. Generally, Kis a constant much smaller than L. When the user observes the provided list, he/she returns click feedbackCt = (Ct1, ..., CtL) to the ranker where Ctk = 1 stands for user click on item ak. Note that ak Rt can not beobserved by the user, thus its click feedback in round t is Ctk = 0. The attractiveness score represents theprobability that the user is interested in item ak, and it is defined as (ak) , which is unknown to theranker. Without loss of generality, we suppose (a1) >, ..., > (aL) where a1 is the most attractive item andaL is the least attractive item.",
  "P(Cts = 1 | Rt = R, atk = as) = v(R, atk, k),(1)": "where R = Rt denote the list presented in round t, and as = tk denote that the s-th most attractive item(where as can be any arbitrary item in D) is placed in the k-th position of Rt (also the k-th position of R).The key problem of OLTR is to present the optimal list R = (a1, ..., aK) to the user for per-round clicknumber maximization. The optimal list is unique due to the attractiveness of items is unique.",
  "We present two classic click models (Chuklin et al., 2015; Richardson et al., 2007; Craswell et al., 2008) thatare special instances of the stochastic click models": "Position-based model.The position-based model (Richardson et al., 2007) assumes the examinationprobability of the k-th position in list Rt is a constant (k) . In each round, the user receives theordered list Rt. He/she would examine position k with probability (k). If position k is examined then theuser would click item atk with probability (atk). Hence, the probability of item atk is clicked by the user is",
  "The examination probabilities of the first K positions are assumed to follow (1) > ... > (K) (Chuklinet al., 2015). The maximum number of clicks in each round is K": "Cascade model.In the cascade model (Craswell et al., 2008), the user examines the items in Rt sequentiallyfrom at1. The user continues examining items until they find an item atk attractive or they reach the end ofthe list. If the user finds atk attractive, they would click on it and stop examining further. According to the above description, the examination probability of position k equals the probability of noneof the items in the first k 1 positions in Rt can attract the user, and can be represented as",
  ": Threat models on online learning to rank": "Remark 1. We now briefly discuss relationship between click models and no-regret rankers. Recall thedefinition of the position-based model, the optimal list R can uniquely maximize (4). Thus, every rankerthat achieves regret R(T) = o(T) in the position-based model falls into the category of no-regret ranker(such as PBM-UCB (Lagre and Vernade, 2016)). Besides, algorithms (i.e., BatchRank and TopRank) forgeneral click models (Zoghi et al., 2017; Lattimore et al., 2018) present the optimal list R for T o(T)times owing to their elimination and divide-and-conquer nature. Therefore, they can achieve a sublinearregret under Assumption 1 and state-of-the-art online ranking methods BatchRank (Zoghi et al., 2017) andTopRank (Lattimore et al., 2018) fall into the category of no-regret rankers. However, every permutation ofthe first K-most attractive items can maximize Eq equation 6 in the cascade model. The item with the highestattractiveness may not be placed at the first position for T o(T) times by an online stochastic ranker withR(T) = o(T). Thus not all rankers that achieve R(T) = o(T) in the cascade model are no-regret rankers.",
  "Threat Models": "Let NT (ak) = Tt=1 1{t1 = ak} denote the total rounds item ak placed at the first position of Rt until timeT. The adversary aims to fool the ranker to place a target item a at the first position of Rt for T o(T)rounds. We consider two poisoning attack models. Click poisoning attacks.We illustrate click poisoning attacks in (a). This is similar to the rewardpoisoning attacks studied on multi-armed bandits (Jun et al., 2018; Liu and Shroff, 2019). In each round, theattacker obtains the users feedback Ct, and modifies it to perturbed clicks Ct = ( Ct1, ..., CtL). Naturally, theattacker needs to attain its attack goal with minimum attack cost defined as C(T) = Tt=1Lk=1 | Ctk Ctk|. List poisoning attack.Instead of directly manipulating the click feedback, the list poisoning attacksmanipulate the presented ranking list from Rt to Rt as illustrated in (b).This is similar tothe action poisoning attack proposed by Liu and Lai (2020a; 2021) against multi-armed bandits.Weassume the attacker can access items with low attractiveness denoted as {k}2K1k=1 D and for convenience,(1) >, ..., > (2K1). The low attractiveness items satisfy (1) < (aL). We suppose the attacker doesnot know the actual attractiveness of these items, but only their relative utilities, i.e., the attractiveness ofitems in {k}K1k=1 is larger than items in {k}2K1k=K . The attacker uploads these items to the candidate actionset before exploration and we denote D = D {k}2K1k=1 . In each round, the attacker can replace items inoriginal ranking Rt with items in {k}2K1k=1 . This modified list Rt = (at1, ..., atK) is then sent to the user.The cost of the attack is C(T) = Tt=1Kk=1 1{atk = atk}. Note that the click feedback Ct in list poisoningattacks is generated by Rt instead of Rt, but the ranker assumes that the feedback is for Rt. The click poisoning attack has been studied in WSJ (2018); BuzzFeed (2019); Golrezaei et al. (2021). Inpractice, the click feedback manipulation can be executed through malware installed as a browser extension,which manipulates the click feedback signal locally before uploading it to the server (e.g., the OLTR algorithm).We also provide an example to explain the practicability of the list poisoning attack. Example 1 (Motivated example of list poisoning attack). Consider the OLTR of an e-commerce searchengine (e.g., Amazon) where the attacker is a seller who wants to promote its target item (e.g., good) tothe top (WSJ, 2018; BuzzFeed, 2019). The item set of the e-commerce consists of items uploaded by sellers.",
  ":Do not attack": "The attacker can find 2K 1 items with low attractiveness (e.g., unpopular advertisements or low-qualityproducts) and derive their relative utilities (the relative utilities of i between i [1, K 1] and i [K, 2K 1]) based on offline learning to rank techniques. Due to low-attractiveness items being much more commonin real-world applications compared to attractive ones and being much smaller than the total item numberL, we believe finding 2K 1 low-attractiveness items is applicable. As a seller, the attacker could uploadboth low-attractive items and the target item to the item set (e.g., a seller listing products on the Amazonplatform). Additionally, list manipulation can be executed through malware installed as a browser extension,which locally alters the ranking list on the web page. When the e-commerce platform interacts with the user,the attacker can implement a list poisoning attack strategy using the uploaded items and the malware.",
  ". To keep the click poisoning attack stealthy, the returned total clicks Lk=1 Ctk in the cascade model isat most 1 and in the position-based model is at most K": "We conclude the preliminary with the difference between poisoning attacks on stochastic bandits (Jun et al.,2018; Liu and Shroff, 2019; Xu et al., 2021) and online learning rankers. Data poisoning attack on stochasticbandits aims to fool the bandit algorithm to pull the target arm T o(T) times with o(T) cost. The mainidea of this class of attack strategies is to hold the expected reward of the target item and reduce the expectedreward of the non-target items. However, in the OLTR setting, 1) the ranker would interact with a length Klist Rt instead of a single arm; 2) the user would generate click feedback under different click models thatdepend on examination probability. Recall from the definition of click models, that in the position-basedmodel the user would return at most K clicks in one round, while in the cascade model, the user wouldreturn at most 1 click. Thus, if the attacker returns more than one click in the cascade model, its attack isunstealthy and inefficient.",
  "Generalized List Poisoning Attack Strategy": "In this section, we would propose a generalized list poisoning attack (GA) that misguides any no-regret rankerto place the target item at the first position of Rt for T o(T) times in expectation with o(T) cost. GA against no-regret rankers.We briefly illustrate the process of GA. The strategy is summarized inAlgorithm 1. The attacker first needs to design list T , where T = {a, 1, ..., K1}. Intuitively, items",
  "minRK()(Kk=1 v(T , atk, k) Kk=1 v(R, atk, k)) > 0 and consists of T and L + K 1 items withattractiveness smaller equals then (K)": "Remark 2. According to the Algorithm 1, R(T) can represent the regret of the victim algorithm when itexplores an environment that consists of an optimal list T and L + K 1 items with attractiveness smallerequal than (K). According to the definition of the no-regret, the victim algorithm will present T for T o(T)times and achieve a sublinear regret, which implies R(T) = o(T). Accordingly, we have E[NT (a)] = T o(T)and C(T) = o(T), which satisfies the definition of the efficient attack. Remark 3 (Gap dependency). If the regret upper bound of the victim ranker satisfiesR(T)=O(L log(T)/min) for example Kveton et al. (2015a), then GA has E[NT (a)] = T O(L2 log(T)/2min)and C(T) = O(L2 log(T)/2min). This is similar to the target arm triggered rate and attack cost of the actionpoisoning attack on MAB in Liu and Lai (2020b). Remark 4. We note that Zoghi et al. (2017); Lattimore et al. (2018) suppose the position-based modelsatisfies (1) (2) , ..., (K). This is because they are primarily concerned with achieving sublinear regretsrather than the specific position of an item in list Rt. Similarly, Zuo et al. (2023) also disregard the positionof the target item in Rt, as their setting assumes the a Rt is an efficient attack. We make this assumptionon the fact that if (1) = (2), both (a1, a2, a3, ..., aK) and (a2, a1, a3, ..., aK) can achieve zero regret in theposition-based model. This contradicts the assumption of our stochastic click model, which states that onlyR = (a1, a2, a3, ..., aK) can achieve zero regret. More specifically, if (1) = (2), then an algorithm canpresent (a2, a1, a3, ..., aK) for T o(T) to achieve a o(T) regret, and GA can not achieve its attack goal due toa has the highest attractiveness and it will be placed on the second position. Besides, its important to notethat we can alleviate the assumption to (1) > (2) (K) and our GA can also achieve its attack goalin the position-based model. This is because we only focus on the position of the target item and disregard thepositions of non-attractive items {k}K1k=1 . Furthermore, suppose (k) > (k) is highly plausible because inreal-world scenarios, it is impossible for this phenomenon to occur, as users inherently exhibit biases towardsdistinct positions. Although we do not include all rankers with o(T) regret in the cascade model as no-regret rankers fromRemark 1, we can still show that cascade ranking algorithms such as CascadeUCB1 (Kveton et al., 2015a)can be efficiently attacked by Algorithm 1. Theorem 2. Consider the ranker applies CascadeUCB1 and is attacked by Algorithm 1. The expected numberof a be placed at the first position till round T satisfies E[NT (a)] = T o(T) and the cost is C(T) = o(T). Remark 5. We notice that most existing cascade rankers for OLTR problems are UCB-based algorithms(CascadeUCB1, CascadeKLUCB (Kveton et al., 2015a), and CascadeUCBV (Vial et al., 2022)). Hence, webelieve GA can efficiently attack these algorithms and we can analyze the theoretical results similar to Theorem2. We have also provided empirical results in our experiment section and Appendix that GA can efficientlyattack CascadeUCB1, CascadeKLUCB, and the SOTA algorithm CascadeUCB-V.",
  "Attack-Then-Quit Strategy": "In this section, we provide a click poisoning attack strategy that applied to rankers on general click models.We will demonstrate our strategy can efficiently attack some rankers under any click models. Previous rewardpoisoning attack strategies on multi-armed bandits only manipulate the reward of the non-target item andnever perturb the reward of the target item (Jun et al., 2018; Liu and Shroff, 2019; Garcelon et al., 2020;Wang et al., 2022). However, the attack strategy proposed in this section perturbs the click feedback of everyitem. The attacker would spend a linear cost in a short period T1 = o(T) and then stop the attack andquit. Thus, we call our attack strategy the attack-then-quit (ATQ) strategy. As mentioned in the previoussection, the BatchRank and TopRank can work in general stochastic click models, while other UCB-typealgorithms can only work in specific click models (e.g., CascadeUCB1 or PBM-UCB). Hence, the BatchRankand TopRank can be deemed as the superior ranker to those UCB-type algorithms, which are worth moresignificant consideration.",
  "Click Poisoning-Based ATQ": "Algorithm 2 can efficiently attack elimination-based rankers (e.g., BatchRank) and divide-and-conquer-basedrankers (e.g., TopRank). At the initialization stage, the attacker would receive target item a and threshold T1.In each round, if click feedback Ctk is returned from a and a Rt, the attacker would set Ctk = 1, otherwise,Ctk = 0. Besides, as we mention in Assumption 1 and Definition 1, the item ak Rt would not be observedby the user in round t, and would not be clicked either. This implies their click feedback would be 0. InATQ, the attacker does not (cannot) modify the click feedback of items ak Rt (line 910 in Algorithm 2).After scanning Ct, the attacker would return Ct = ( Ct1, ..., CtL) to the ranker. Note that Lk=1 Ctk 1 (line4-10 in Algorithm 2), hence Algorithm 2 can work stealthily and efficiently in different click models. Notethat the divide-and-conquer and elimination nature ensures BatchRank and TopRank to quickly estimate theattractiveness of items and rank them accordingly. Our attack Algorithm 2 aims to mislead these rankers tobelieve the target item is the best item with the smallest rounds. Once these rankers make this judgment,they would place the target item at the first position until T. In the following sections, we will demonstrateAlgorithm 2 can efficiently attack BatchRank and TopRank with suitable T1.",
  "Attack on BatchRank": "Brief Explanation of BatchRank.The BatchRank falls into the category of elimination-based algorithms(Zoghi et al., 2017). The BatchRank would begin with stage 1 = 0 and the first batch B1,1 = D. In stage1, every item would be explored for n1 = 16 11 log(T) times and 11 = 21. Afterward, if BatchRankcollects enough information to confirm the attractiveness of every item in a group is larger than the others,BatchRank then splits the initial batch into two sub-batches B2,2 and B3,3. Items that are considered withhigher attractiveness (i.e., in sub-batch B2,2) would be always placed before items with lower attractiveness(i.e., in sub-batch B3,3). The BatchRank would restart with stage 2 = 0 and 3 = 0 and sub-batches B2,2and B3,3. Batches would recursively split until round T. Intuitively, the action split of BatchRank is similarto the elimination action in the elimination-based bandit algorithms (Even-Dar et al., 2006; Lykouris et al.,2018; Bogunovic et al., 2021). The details of BatchRank are provided in the Appendix. Algorithm 2 can successfully attack BatchRank owing to BatchRanks elimination property. Algorithm 2maximizes the returned clicks of the target item and minimizes the returned click of the non-target item in ashort period o(T). After this period, BatchRank regards the target item owning the highest attractiveness(i.e., split). Then the target item would be always placed at the first position of the list Rt and the attackerachieves the attack goal. We also provide Example 3 to further illustrate the process of Algorithm 2 attackBatchRank. Example 3. Consider the item set D = {a1, a2, ..., a10} contains 10 items. The target item is a = a1 witharbitrary attractiveness and item list Rt contains 3 positions. The BatchRank begins with B1,1 = D andrecursively explores items. The attacker begins the attack at round 1 and ends at T1. During T1 rounds,the attacker returns Ctk = 1 if a = ak and a Rt, otherwise Ctk = 0. This fraudulent strategy misleadsBatchRank to believe the attractiveness of the non-target items ak = a is 0 and the attractiveness of a is 1.After BatchRank finishes the split section, the batch B1,1 split into B2,2 = {a1} and B3,3 = {a2, ..., a10}.",
  ": Process of the Algorithm 2 attacks TopRank": "Sub-batch B2,2 contains position 1 (i.e., at1) and B3,3 contains position 2 and 3 (i.e., at2 and at3). Theprocess is illustrated in . The target item a would always be placed at the first position of the Rt inthe next T T1 rounds due to B2,2 only contains a. The attack cost would be o(T) because the attacker onlymanipulates the click feedback for o(T) rounds. Theorem 3 (Attack against BatchRank). Consider the attacker aims to attack BatchRank with stochastic clickmodels (Zoghi et al., 2017). If the attacker runs Algorithm 2 with T1 = 16L log(T), we have NT (a) T T1and the cost can be bounded by C(T) KT1.",
  "Attack on TopRank": "Brief Explanation of TopRank.TopRank is a divide-and-conquer-based ranker (Lattimore et al., 2018).It begins with a blank graph G1 = . In round t, TopRank would establish blocks Pt1, ..., Ptd via graphGt. The items in block Pt1 would be placed at the first |Pt1| positions and the items in block Pt2 would beplaced at the next |Pt2| positions, and so on. During rounds 1 to T, TopRank would explore items withblocks, collect click information and compare attractiveness between items in the same block. If the collectedevidence is enough to let TopRank regards the attractiveness of item ai as larger than the attractivenessof item aj, a directional edge (aj, ai) would be established. This behavior is similar to the split action inBatchRank. Besides, graph Gt would not contain cycles with high probability. If the graph contains at leastone cycle, we consider TopRank would be out of control. Details of TopRank are provided in the appendix. Note that if there exist edges from every non-target item to the target item and Gt contains no cycle, thenthe target item would be isolated from the non-target items and would always be placed at the first positionof Rt. This is because the first block only contains the target item. We also provide Example 4 to specificallyexplain how Algorithm 2 attacks TopRank. Example 4. The process of the attack is shown in . Consider the total item set D = {a1, a2, a3} with3 items. The length of the list Rt is K = 2 and the target item is a = a1. The TopRank would start with blockP11 = D and d = 1 because the graph contains no edges at the beginning. In the first T1 rounds, the attackerreceives click feedback Ct and modifies click feedback Ctk = 1 if ak = a and ak Rt, otherwise Ctk = 0. AfterT1, the edges (ak, a), k = 2, 3 are established simultaneously. In the last T T1 rounds, the block Pt1 wouldonly contain a and a would always be placed at the first position of Rt. Due to TopRank would only compareitems attractiveness in the same block, the edges from a to ak = a would never be established and cycle wouldappear in Gt with very low probability (will be explained in the proof of Theorem 4 in the appendix).",
  ") 3.43 which is same as in TopRank algorithm, we haveT1 = O((L/K) log T). Besides, readers should note that 1 L2 is the intrinsic probability of TopRanks Gt,t [T] contains cycles": "Remark 6 (Why Theorem 3 and Theorem 4 is gap-independent?). The reason the cost upper bound ofthe ATQ isnt gap-dependent is due to 1) the approach taken during the attack, and 2) the structure ofthe BatchRank and TopRank. The TopRank and BatchRank algorithms are classified as phase eliminationalgorithms. In the proofs of Theorems 3 and 4, we demonstrate that we can achieve the attack objective bymaximize the click feedback of the target item and minimize the click feedback of the non-target items duringthe first elimination phase (see ATQ pseudo code). Through analysis, we determine that the length of thefirst elimination phase for BatchRank and TopRank is respectively bounded by OL log(T)and O L",
  "K log(T)": "(this also implies that ATQ can compromise TopRank faster than BatchRank due to BatchRanks inferiorperformance compared to TopRank.), which is unrelated to the attractiveness gap. Besides, due to TopRankand BatchRank will randomly explore the whole item set in the first phase, we suppose the attack cost perround is K due to the attacker should modify at most K click feedback in each round. Accordingly, the finialcost upper bound do not depends to the attractiveness gaps. Remark 7 (Comparison of ATQ performance with that of attack algorithms in the bandit domain). Althoughwe havent provided an instance-dependent version of cost upper bound, we notice that when the victim algorithmis BatchRank and TopRank, the ATQ can achieve E[NT (a)] = T O(L log(T)) with C(T) = O(L log(T)).This can match the performance of click poisoning attack on MAB (Theorem 1 and Theorem 2 of Jun et al.(2018)), i.e., E[NT (a)] = T O(L log(T)) and C(T) = O(L log(T)). Remark 8 (Why UCB-based algorithms can not be attacked by ATQ?). The effect of ATQ relies on algorithmsphased elimination property. However, CascadeUCB and PBM-UCB belong to UCB-based algorithms. Whenthe attacker stops, the true click feedback of other items will be revealed to the algorithms over time, leadingUCB-based algorithms to realize the targeted item isnt the item with the highest attractiveness.",
  "Experiments": "In the experiment section, we apply the proposed attack methods against the OLTR algorithms listed in with their corresponding click models. We compare the effectiveness of our attack on synthetic dataand real-world MovieLens dataset. For all our experiments, we use L = 50, K = 5 (the set up of Zoghi et al.(2017); Lattimore et al. (2018) is L = 10 and K = 5) and T = 105. For ATQ, we set the T1 in Algorithm 2by Theorem 3 and Theorem 4.",
  "Synthetic Data": "First, we verify the effectiveness of our proposed attack strategies on synthetic data. We generate a size-Litem set D, in which each item ak is related to a unique attractiveness score (ak). Each attractiveness score(ak) is drawn from a uniform distribution U(0, 1). We randomly select a suboptimal target item a. shows the results and variances of 10 runs.",
  ": Target ranking algorithms and their applied click modelsAlgorithmClick model": "BatchRank (Zoghi et al., 2017)Stochastic click modelTopRank (Lattimore et al., 2018)Stochastic click modelPBM-UCB (Lagre and Vernade, 2016)Position-based modelCascadeUCB1 (Kveton et al., 2015a)Cascade modelCascadeKLUCB (Kveton et al., 2015a)Cascade model In Figures 4(a) and 4(b), we plot the results of the GA against CascadeUCB1, CascadeKLUCB, BatchRank,and TopRank, and the ATQ against BatchRank and TopRank in the cascade model. Both attack strategiescan efficiently misguide the rankers to place the target item at the first position for T o(T) times as shownin (b), and the cost of the attack is sublinear as shown in (a). The GA is cost-efficient whenattacking all four algorithms. We can observe that when it attacks TopRank and BatchRank, the cost wouldnot increase after some periods (similar to the ATQs results). This is when the TopRank and BatchRankbelieve the target item and the auxiliary items have a relatively higher attractiveness than the other items,they would only put the target item and the auxiliary items in Rt. Besides, when attacking TopRank andBatchRank, the growth rate of GAs target arm pulls Nt(a) slowly increased from 0.2 per iteration to 1 periteration. This is because the GA does not manipulate the items in T and the TopRank and BatchRankneed time to confirm the target item has a higher attractiveness than {k}K1k=1 . Hence, the smaller the gapbetween a and 1, the larger the confirmed time. Compare with the GA, the ATQ can also efficiently attackBatchRank and TopRank with a sublinear cost. However, its NT (a) is almost T, which is relatively largerthan GAs NT (a). This is because the ATQ is specifically designed for divide-and-conquer-based algorithmslike TopRank and BatchRank. The ATQ can maximize the target items click number and misguide thesealgorithms to believe the target item is the best in the shortest period. Figures 4(c) and 4(d) report the results in the position-based model. We can observe that the spending cost ofthe GA on the PBM-UCB is slightly larger than the spending cost on the CascadeKLUCB and CascadeUCB1.Besides, although the GA can let the TopRank believe the target item is the best item in almost 500 iterations,it still needs a large number of iterations (around 6 104 iterations) to make the BatchRank make such adecision. From the results of the two models, the ATQ is obviously more effective than the GA when the targetalgorithms are TopRank and BatchRank.",
  "Experiments on Real-World Data": "We also evaluate the proposed attacks on MovieLens dataset (Harper and Konstan, 2016). We first split thedataset into train and test data subsets. Using the training data, we compute a d-rank SVD approximation,which is used to compute a mapping from movie rating to the probability that a user selected at randomwould rate the movie with 3 stars or above. We use the learned probability to simulate users clicks given theranking list. We refer the reader to the Appendix C of (Vial et al., 2022) for further details. showsthe attack results of our attack strategy averaged over 10 rounds.",
  "Conclusion": "In this paper, we study adversarial attacks on online learning to rank. Different from the poisoning attacksstudied in the multi-armed bandits setting where reward or action is manipulated, the attacker manipulatesbinary click feedback instead of reward and item list instead of a single action in our model. In addition,due to the interference of the click models, it is difficult for the attacker to precisely control the rankerbehavior under different unknown click models with simple click manipulation. Based on this insight, wedeveloped the GA that can efficiently attack any no-regret ranking algorithm. Moreover, we also proposed theATQ that follows the click poisoning idea, which can efficiently attack BatchRank and TopRank. Finally, wepresented experimental results based on synthetic data and real-world data that validated the cost-efficientand effectiveness of our attack strategies.",
  "Fang Liu and Ness Shroff. Data poisoning attacks on stochastic bandits. In International Conference onMachine Learning, pages 40424050, 2019": "Guanlin Liu and Lifeng Lai. Action-manipulation attacks on stochastic bandits. ICASSP 2020 - 2020 IEEEInternational Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 31123116, 2020a. Guanlin Liu and Lifeng Lai. Action-manipulation attacks on stochastic bandits. In ICASSP 2020-2020 IEEEInternational Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 31123116. IEEE,2020b.",
  "Yuzhe Ma and Zhijin Zhou. Adversarial attacks on adversarial bandits. ArXiv, abs/2301.12595, 2023. URL": "Harrie Oosterhuis and Maarten de Rijke. Differentiable unbiased online learning to rank. Proceedings of the27th ACM International Conference on Information and Knowledge Management - CIKM 18, 2018. doi:10.1145/3269206.3271686. URL Achref Ouni, Eric Royer, Thierry Chateau, Marc Chevaldonn, and Michel Dhome. Deep learning for robustinformation retrieval system. In International Conference on Computational Collective Intelligence, 2022.",
  "Daniel Vial, S. Sanghavi, Sanjay Shakkottai, and Rayadurgam Srikant. Minimax regret for cascading bandits.ArXiv, abs/2203.12577, 2022": "Huazheng Wang, Ramsey Langley, Sonwoo Kim, Eric McCord-Snook, and Hongning Wang.Efficientexploration of gradient space for online learning to rank. In The 41st International ACM SIGIR Conferenceon Research and Development in Information Retrieval, pages 145154. ACM, 2018. Huazheng Wang, Sonwoo Kim, Eric McCord-Snook, Qingyun Wu, and Hongning Wang. Variance reductionin gradient exploration for online learning to rank. In Proceedings of the 42nd International ACM SIGIRConference on Research and Development in Information Retrieval, pages 835844, 2019.",
  "For clarity, we collect the frequently used notations in this paper": "DTotal item setRtK-length item list be shown to the user in round tROptimal listRtManipulated list in round tCtClick feedback list in round tCtManipulated click feedback list in round tTOrdered list (a, a1, ..., aK)aTarget itemakk-th most attractive item in Dakk-th most attractive auxiliary itemkParticular item in the list poisoning attack(ak)Attractiveness of item akatkItem on the k-th position in RtatkManipulated item on the k-th position in RtCtkClick feedback of item ak in round tCtkManipulated click feedback of the item ak in round tv(Rt, atk, k)Click probability of item at the k-th position in round tR(T)Cumulative regret in T roundsC(T)Total cost in T roundsNt(ak)Number of item ak be placed at the first position in t roundsNt(ak)Number of item ak be examined in t roundsTTotal number of interactionT1Input threshold value of the attack-then-quit algorithmBatchRankbBatch indexStage indexBb,b-th batch explored in stage nExploration number of item in batch Bb, in stage Cb,(ak)Total received click number of item ak during stage Cb,(ak)Attractiveness estimator of item ak in stage Ub,(ak)Upper confidence bound of item ak in stage Lb,(ak)Lower confidence bound of item ak in stage TopRankGtAuxiliary graph in round t(aj, ai)Directional edge from item aj to item aiPtcc-th block in round tStijSum of the Utij from round 1 to tNtijSum of the absolute value of Utij from round 1 to t",
  "BAdditional Experiments": "In the main text, we have shown that GA can efficiently attack CascadeUCB1 and CascadeKLUCB. In thissubsection, we show that GA can also efficiently attack the SOTA cascade ranker CascadeUCBV (Vial et al.,2022). The attack results are provided in . Apparently, the GA can successfully reach the attack goal.Based on these results, we believe GA can efficiently attack almost every existing UCB-type cascade ranker.",
  "CProof of Theorem 1": "Recall that when the attacker implements Algorithm 1, the optimal list becomes R = T = (a, 1, ..., K1)due to the attractiveness of items belong to D\\T is smaller than (K1) (i.e., (K) < (K1) (a)).We should also notice that the target item a has the highest attractiveness. Now recall",
  "t=11{at1 = a},(7)": "then based on Definition 1 and Assumption 1, we know that if at1 = a, the ranker would suffer from regret inround t. Due to the per step regret is at least min = minRK()(Kk=1 v(T , atk, k) Kk=1 v(R, atk, k)) > 0,where consists of T and L + K 1 items with attractiveness smaller equals then (K). Then based onthe definition of regret, we can finally derive",
  ".(19)": "Remember when the attacker implements attack Algorithm 1, the target item would become the item withthe highest attractiveness. The rest of the items consist of {k}K1k=1 and L + K 1 items with attractivenessat most (K). Therefore, when Algorithm 1 attacks the CascadeUCB1, NT (a) can be lower bounded by",
  ":UpdateBatch(t,b)": "The BatchRank explores items with batches, which are indexed by b. The BatchRank would begin with stage1 = 0, batch index b = 1, and the first batch Bb,1 = D. The first position in batch b is indexed by Ib(1) andthe last position is indexed by Ib(2), and the number of positions in batch b is len(b) = Ib(1) Ib(2) + 1.The first batch Bb,1 contains all the positions in Rt. In stage 1, every item in Bb,1 would be explored forn1 = 16 21 log(T) times (DisplayBatch) and 11 = 21. Afterward, the BatchRank would estimate theattractiveness of item ak as",
  "Lb,(ak) =arg minq[0, Cb,(ak)]{nDKL( Cb,(ak)q) log(T) + 3 log log(T)}(26)": "where DKL represents the Kullback-Leibler divergence between Bernoulli random variables with means p andq. In the UpdateBatch section, all the items in batch Bb,1 would be placed by order a1, ..., a|Bb,1|, whereLb,1(a1) , ..., Lb,1(a|Bb,1|). The BatchRank would compare the first len(b) 1 items lower confidencebound to the maximal upper confidence bound in Bk . If Lb,1(ak) > maxakBk Ub,1(ak), the BatchRankwould set s = k. Ones s > 0, the batch would spilt from position s and the ranker derives sub-batchesBb+1,2 and Bb+2,3. Sub-batch Bb+1,2 contains s items and the first s positions in Rt and sub-batch Bb+2,3contains L s items and positions from s to K. The BatchRank would restart with stages 2 = 0 and 3 = 0and sub-batches Bb+1,2 and Bb+2,3. The batches would recursively run and split until round T.",
  "E.2Proof of Theorem 3": "The proof of Theorem 3 relies on the following Lemma 3.Lemma 3. The attacker utilizes Algorithm 2 to manipulate the returned click feedback of the BatchRank.After 16L log(T) rounds attack and the BatchRank begins its first split. The upper confidence bound and lowerconfidence bound of every non-target item satisfies Lb,1(ak) = 0 and Ub,1(ak) = 1 (T log(T)3)1/n1 . Thelower confidence bound and the upper confidence bound of the target item are Lb,1(a) = 1 and Ub,1(a) = 1. Proof of Lemma 3. According to the introduction of BatchRank, the estimated click probability of an arbitraryitem is written as (25) and Cb,1(ak) is at most 16 log(T) in the first stage (1 = 0 and 21 = 221 = 1).Recall our attack Algorithm 2 returns Ctk = 0 when ak = a and ak Rt. Thus, the total collected clicknumber of the non-target item is Cb,1(ak) = 0, and the estimated click probability is Cb,1(ak) = 0.",
  "Here finish the proof of Lemma 3": "Proof of Theorem 3. Consider the attacker implements attack Algorithm 2 with T1 = 16L log(T). With theknowledge of Lemma 3, we can obtain when the BatchRank begins to split the first batch B1,1 = D, thelower confidence bound of every non-target item satisfies Lb,1(ak) = 0, and the lower confidence bound ofthe target item satisfies Lb,1(a) = 1. Therefore a would be ranked at the first position because it has thehighest lower confidence bound (line 6 in UpdateBatch). The BatchRank starts comparing Lb,1(ak) andmaxakBk Ub,1(ak) for k = 1 to K 1 (line 10 in UpdateBatch). Owing to Lb,1(a) = 1 > Ub,1(ak) andLb,1(ak) < Ub,1(ak), the split point is s = 1 (line 11 in Updatebatch). After the split action, the BatchRankwould derive two sub-batches B2,2 = {a} and B3,3 = D\\a. Sub-batch B2,2 contains the first position of Rt(i.e., at1) and B3,3 contains the rest of the positions of Rt (i.e., at2, ..., atK). Sub-batch B2,2 would not splituntil round T because it only contains a position and an item. This implies after round 16L log(T), the targetitem would always be placed at the first position of Rt until round T is over, i.e., NT (a) T 16L log(T).Due to the click number in each round being at most K, the cost in one round is at most K. Hence, the costof Algorithm 2 can be bounded by C(T) KT1.",
  "We here specifically illustrate details of the TopRank. The pseudo-code of the TopRank is provided": "The TopRank would begin with a blank graph G1 [L]2.A directional edge (aj, ai) Gt denotesthe TopRank believes item ais attractiveness is larger than item aj. Let minGt(D\\ d1c=1 Ptc) = {ai D\\ d1c=1 Ptc : (ai, aj) Gt for all aj D\\ d1c=1 Ptc}. The algorithm would begin from round 1 to round T.In each round, the TopRank would establish blocks Pt1, ..., Ptd via the graph Gt. Items in block Pt1 wouldbe placed randomly at the first |Pt1| positions in Rt, and items in Pt2 would be placed randomly at thenext |Pt2| positions, and so on. In each round, after deriving click feedback Ct, the TopRank would computeUtij = Cti Ctj if item ai and item aj are in the same block, otherwise, Utij = 0. Afterward, the TopRank would",
  "NT1ij) andNT1ij > 0 are satisfied and edges from non-target items to target item (i.e., (aj, ai), aj = ai) are establishedsimultaneously": "Proof of Lemma 4. Note that the TopRank sets Utij = Cti Ctj if ai, aj Ptd for some d, otherwise, Utij = 0.According to attack Algorithm 2, the TopRank would receive Cti = 1 (Cti is generates by the target item) ifa Rt and Ctj = 0 (Ctj is generated by non-target items) when t T1. Based on this, we can derive",
  "achieve T1t=1 1{a Rt} > 4 log(c/)": "Proof of Lemma 5. According to the previous discussion, we can separate T1 into two periods P1 and P2 (i.e.,T1 = P1 + P2). In period one Gt = and in period two Gt only contains edges from non-target items to thetarget item. Based on the TopRank property, in period one P(a Rt|t P1) = K/L and in period twoP(a Rt|P1 + 1 t T1) = 1. Define a Bernoulli distribution X that satisfies X = 1 with probability K/L.With the help of the Hoeffding inequality, we can derive",
  "T1t=1 1{a Rt}))": "Besides, due to the received click number from non-target items being 0 in T1, the ST1 and NT1 betweennon-target items would be 0. This implies the manipulation of the attacker would not influence the TopRankjudgment of the attractiveness between non-target items. In other words, the TopRank under Algorithm 2attack can be considered as the TopRank interacts with item set D\\a in T T1 rounds.",
  "+8K/L)/4.Then, the TopRank would establish edges from non-target items to a with proba-": "bility at least 1 /c (According to Lemma 4 and Lemma 5). Based on the analysis in Lemma 6, the cyclewould appear with probability at most (L2 + 1/c) and the first block would only contain a till T. That is tosay, the target item in block Pt1 would always be placed at the first positions after T1 with probability atleast 1 (L2 + 1/c). Following Algorithm 2, the attacker would only manipulate the returned click feedbackfor T1 times. Thus the attack cost can be bounded by C(T) KT1."
}