{
  "Abstract": "This study reports an unintuitive finding that positional encoding enhances learning of re-current neural networks (RNNs). Positional encoding is a high-dimensional representationof time indices on input data. Most famously, positional encoding complements the capa-bilities of Transformer neural networks, which lack an inherent mechanism for representingthe data order. By contrast, RNNs can encode the temporal information of data pointson their own, rendering their use of positional encoding seemingly redundant/unnecessary.Nonetheless, investigations through synthetic benchmarks reveal an advantage of couplingpositional encoding and RNNs, especially for handling a large vocabulary that yields low-frequency tokens. Further scrutinization unveils that these low-frequency tokens destabilizesthe gradients of vanilla RNNs, and the positional encoding resolves this instability. Theseresults shed a new light on the utility of positional encoding beyond its canonical role as atimekeeper for Transformers.",
  "Introduction": "Since their invention in 2017, Transformer neural networks (Vaswani et al., 2017) have taken over as the goldstandard processor/generator of time series data, from the more traditional models in the form of recurrentneural networks (RNNs; Elman, 1990). One of the most striking differences between the two models is theway they represent the temporal information of the data (i.e., the order of individual data points, or tokens,in the time series). On the one hand, RNNs encode temporal information by updating their internal statebased on the input observations as well as the previous state. On the other hand, Transformers per se donot have a mechanism to represent the temporal order of data points; consequently, they rely on an externalclock called positional encoding. Positional encoding is a high-dimensional representation of time indices on input data (Gehring et al., 2017).For instance, its most basic and popular implementation utilizes sinusoidal waves of various predefinedfrequencies (Vaswani et al., 2017). Positional encoding timestamps input tokens by addition/concatenationof these vectors to the corresponding input embeddings. Unlike RNNs, the time representation via positionalencoding is invariant to input values (i.e., autonomous) until they are processed together by a network. Although positional encoding has been considered as an alternative form of time representation that replacesRNNs (in combination with Transformers), positional encoding and RNNs are not fundamentally incom-patible; that is, inputs to RNNs can be redundantly augmented by position-encoding vectors. Indeed,autonomous activities of biological neuronssuch as neural oscillationsare thought to play an importantrole in time perception (Matell & Meck, 2004; Buhusi & Meck, 2005), as well as other perceptual processes(including vision, Milner, 1974; Eckhorn et al., 1988; Gray et al., 1989; and odors, Milner, 1974; Eckhornet al., 1988; Gray et al., 1989; Wehr & Laurent, 1996; Perez-Orive et al., 2002) and motor control (Marder& Bucher, 2001; Gross et al., 2002; Proctor et al., 2010).",
  "The contributions of this study are summarized as follows:": "Difficulties in training RNNs on a large vocabulary are demonstrated through systematically designedbenchmark tasks. This problem has not been identified in the previous studiesor at most hasreceived little attention from the research communitydespite its potential relevance to empiricalapplications. This identified problem with training RNNs on a large vocabulary is elucidated by the gradient insta-bility induced by low-frequency tokens, which necessarily arise from the expansion of the vocabularysize. A novel efficacy of positional encodingbesides its timestamping function for Transformersisshown by coupling it with RNNs. Specifically, positional encoding will be shown to mitigate thelarge-vocabulary problem by stabilizing the gradients of RNNs against disruptions caused by low-frequency tokens.",
  "Theoretical and Empirical Computational Power of (Vanilla) RNNs": "Mathematically, RNNs are known to be Turing-complete; they can simulate Turing machines if their weightshave infinite precision and are ideally tuned (Siegelmann & Sontag, 1992; 1994; 1995; Siegelmann, 1996;1999; Chen et al., 2018).1 Indeed, even RNNs with random recurrent and input-to-hidden weights (calledreservoir computers; Maass et al., 2002; Jaeger & Haas, 2004) can achieve the universal approximationproperty if their hidden-to-output weights are idealized (Grigoryeva & Ortega, 2018; Gonon & Ortega,2020). These theoretical results have motivated the use of RNNs for processing complex time series such ashuman languages (Sundermeyer et al., 2012; Graves, 2013) and weather (Shi et al., 2015). In practice, however, RNN weights are bounded by finite precision and must be optimized based on finiteobservations of data. These settings impose limitations on the empirical capabilities of RNNs (Chen et al.,2018; Weiss et al., 2018). For example, empirical RNNs cannot store infinitely many observations in theirmemory, or state vector(s), and the memorized information decays over time. This latter problem withthe memory duration has attracted the interest of researchers, leading to extensive exploration of RNNarchitectures for a longer-lasting memory (Hochreiter & Schmidhuber, 1997; Arjovsky et al., 2016; Neilet al., 2016; Chang et al., 2017; Jing et al., 2017; 2019). More recently, research into prolonged memory retention has shifted towards continuous-time models (Voelkeret al., 2019; Gu et al., 2020). Instead of representing the memory of an input sequence through discrete-timemodifications of a latent state, these models approximate the input history by a linear combination of or-thogonal polynomials in continuous-time space. Consequently, the coefficients of these polynomials yield afinite-dimensional representation of the input sequence (termed the High-Order Polynomial Projection Op-erator, or HiPPO) and the dynamics of these coefficients can be articulated through an ordinary differentialequation (ODE; Gu et al., 2020). This concept of continuous-time memory representation has subsequentlybeen extended to neural state-space models by replacing the fixed state matrix in the ODEcorrespondingto a manually selected basis of polynomialswith a learnable one, while constraining its structure to a diag-onal (plus a row-rank) matrix (Gu et al., 2021; 2022a;b; Gu & Dao, 2024). Most strikingly, with additionalrefinements, the latest state-space model has achieved superior language modeling performance, rivaling thatof Transformer-based models. 1In order to be Turing-complete, RNNs must also be allowed to read an entire input prior to their output emission; they areat most context-sensitive if they have to return an output at each time step upon the receival of an input token (Chen et al.,2018; Weiss et al., 2018).",
  "Positional Encoding": ": Token-wise accuracy of the reverse-ordering task performed by the LSTM with and withoutpositional encoding. Three variants of positional encoding (sinusoidal, randomly fixed vectors, and learnableembeddings) were tested. The input length was fixed at L := 64. The vocabulary size was set to 16,384.The error bars represent the 95% confidence interval estimated from 10,000 bootstrapped samples of fivetraining-test trials with different random seeds. Each of the five trials held out 1024 random sequences(= 65,536 tokens) for computing the test accuracy. Among the different implementations of positional encoding, the sinusoidal encoding outperformed the twoalternatives. The advantage of the sinusoidal encoding became more apparent when the input length wasvariable between 32 and 64 (); the sinusoidal encoding was more robust to the variations in the inputlength than the others.",
  "Model Architecture": "The investigations in this study were based on single-layer gated recurrent unit (GRU; Cho et al., 2014)and long short-term memory (LSTM; Hochreiter & Schmidhuber, 1997) combined with input-embeddingand output-projection layers (). Besides these RNNs, a neural state-space model, S4D (i.e., S4 with adiagonal state matrix; Gu et al., 2022a), was also investigated. Each of the integers in the input sequenceswas first embedded and concatenated with the positional encoding, and then fed to the RNN/S4D.4 After 2In other studies, positional encoding and RNNs have served as submodules within more complex models, typically inconjunction with an attention mechanism (Kim et al., 2018; Song et al., 2020).3Investigations of additional tasks are reported in Appendix A.4The concatenation of the positional encoding and input embeddings inflates the number of learnable parameters in theinput-to-hidden weights of RNNs in comparison to the vanilla models. However, this additional parameterization is designed",
  ": Illustration of the model structure and the reverse-ordering task": "reading the entire input sequence, the network received a command to return the output. This commandwas represented in the form of a time-invariant learnable vector, and fed to the RNN in place of the inputembedding (cf. Arjovsky et al., 2016). The outputs from the RNN/S4D module were linearly projected intothe classification logits, whose cross-entropy loss against the target sequence was used to optimize the entirenetwork. Model predictions in the testing phase were defined by the argmax of these logits for each timestep. This study adopted the canonical sinusoidal positional encoding designed for Transformers (Vaswani et al.,2017, see Appendix D for discussions on alternative implementations); specifically, each time step t wasencoded by the Dpos-dimensional vector, (PEt,1, . . . , PEt,Dpos)T , defined as follows:5",
  "For the sake of learning stability, the positional encoding was divided by": "Dpos/2 so that the encodingvectors had the unit L2-norm. Note that the time step t increased throughout both the input and outputphases (i.e., t = 1, . . . , L, L + 1, . . . , 2L where L represents the input length), without any hard-codedassociation between the input and output positions (i.e., no shared timestamps).",
  "Implementation Details": "Across the experiments, the dimensionality of the hidden layer of the RNNs was set to 512. The embedding ofthe input integers and the memory cell of the LSTM also had the same dimensionality of 512. Similarly, the not to impact the learning of input embeddings. Appendix C experimentally demonstrates that merely increasing the modelsize does not enhance the performance of RNNs.5The adjustment of the time and frequency indices (t 1 and i 1) in Eqs. 1 and 2 aligns the 1-based indexing in thispaper (adopted for better readability) with the 0-based indexing in Python.",
  "Vocabulary Size": ": Token-wise accuracy (left) and sequence-wise reconstruction errors (right) of the reverse-orderingtask performed by GRU/LSTM/S4D with and without positional encoding (labeled as Position-Encodedand Vanilla respectively). The input length was fixed at 64. The error bars represent the 95% confidenceinterval estimated from 10,000 bootstrapped samples of five training-test trials with different random seeds.Each of the five trials held out 1024 random sequences (= 65,536 tokens) for computing the test accuracy.",
  "hidden dimensionality of S4D was set to 512, while its state size (or the order of the Legendre polynomials)was maintained at the default value of 64.6": "The models were trained for 300,000 iterations using the Adam optimizer (Kingma & Ba, 2015) with theparameters (1, 2) := (0.9, 0.999) and no weight decay. The learning rate was linearly warmed up from0.0 to 0.001 for the first 1,000 iterations, and then annealed according to the cosine schedule (Loshchilov &Hutter, 2017). The batch size was 512. All the experiments were implemented in PyTorch (ver. 2.1.1; Paszke et al., 2017; 2019) and each training-test trial was executed on a single NVIDIA A100 GPU (with 80GB VRAM) hosted by the Academic Centerfor Computing and Media Studies, Kyoto University. The source code is available in",
  "Published in Transactions on Machine Learning Research (11/2024)": "Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher R. Combiningrecurrent, convolutional, and continuous-time models with linear state space layers.In M. Ranzato,A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan (eds.), Advances in Neural InformationProcessing Systems, volume 34, pp. 572585. Curran Associates, Inc., 2021. Albert Gu, Karan Goel, Ankit Gupta, and Christopher R. On the parameterization and initialization ofdiagonal state space models. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh (eds.),Advances in Neural Information Processing Systems, volume 35, pp. 3597135983. Curran Associates, Inc.,2022a. Albert Gu, Karan Goel, and Christopher R. Efficiently modeling long sequences with structured statespaces. In Proceedings of the Tenth International Conference on Learning Representations (ICLR). Open-Review.net, 2022b. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In H. Larochelle,M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neural Information ProcessingSystems, volume 33, pp. 68406851. Curran Associates, Inc., 2020.",
  "Frequency Matters": "The most apparent consequence of the increased vocabulary size was the reduced chance of observing indi-vidual vocabulary items (e.g., 1/256 1/16,384). Accordingly, additional experiments were conducted withnon-uniformly distributed tokens to investigate the relation between their frequency vs. RNN performance.Specifically, the input vocabulary was evenly divided into Frequent and Rare groups, and the Frequenttokens had three times the probability of the Rare tokens; that is, the probability of each Frequent tokenwas 7/8 2/K (where K denotes the total vocabulary size and was set to 64, 1024, 2048 for GRU, LSTM,and S4D respectively) whilist the probability of each Rare token was 1/8 2/K. The training data consisted of 64 independent samples from this dual-frequency vocabulary.By con-trast, the test data were systematically constructed so that each sequence included a single target token(Frequent/Rare) whose retrieval was evaluated for accuracy assessment, along with 63 disturbants thatwere either all Frequent or all Rare. The experiment revealed that it was the disturbant tokens whose frequency significantly impacted the per-formance of the vanilla RNNs and S4D (). On the one hand, the Rare targets were successfullyretrieved as long as they were surrounded by the Frequent disturbants. On the other hand, the vanillaGRU struggled to recover the Frequent targets when the other input tokens were filled with the Raredisturbants. The LSTM performance was also degraded, especially when the targets were positioned in thefirst quarter of the input sequence (1 t 16). Similarly, the Rare disturbants were detrimental to theS4D; unlike the RNNs, however, the accuracy was worst when the targets were located in the middle of theinput sequences (17 t 32). By contrast, the position-encoded RNNs exhibited robustness to the frequency of the target and disturbanttokens. They achieved nearly perfect accuracies in most cases, except when the GRU processed the fullyRare data whose target was located in the first half of the sequence (1 t 32). Likewise, positionalencoding enhanced the resilience of the S4D against the influence of Rare disturbants.",
  "Analysis of Gradient Stability": "To delve deeper into the influence of token frequency on the RNN performance, the gradients of the RNNlatent states were scrutinized. In the analysis, pairs of input sequences were processed by the RNNs trainedon the dual-frequency vocabulary (comprising Frequent and Rare items; ). Each pair of sequencesshared the same initial token (t = 1; target) but varied in the subsequent tokens (2 t L; disturbants).Then, gradients were computed for the distant mapping between the first and last updated states (i.e., attime t = 1 and 2L) of the RNNs using backpropagation through time. The stability of RNN learning wasassessed by measuring the dot-product similarity of the gradients between the paired input sequences (afternormalization over output dimensions). Formally, the paired input sequences, denoted as A and B, established two distinct, but ideally similar map-pings, f (A) and f (B), from the first to the last latent state of the RNNs (h(s)2L = f (s)(z1), where s {A, B}).The gradient stability of the RNNs was defined by the dot-product similarities between the normalizedgradients of these paired mappings:",
  "GRULSTMS4D": ": Gradient stability of GRU/LSTM/S4D trained on the reverse-ordering task with and withoutpositional encoding (labeled as Position-Encoded and Vanilla respectively). For the GRU and LSTM,the stability was defined by the dot-product similarity of latent-to-latent gradients after normalization overthe output dimensions, conditioned on two input sequences sharing the initial target token (whose Fre-quent vs. Rare distinction is represented by the line color), followed by Frequent or Rare disturbants(represented by the solid vs. dashed lines). For the S4D, the target token was positioned at t = 23, wherethe vanilla model scored the worst accuracy with the Rare disturbants. The disturbants were prefixed andsuffixed to the target to construct input sequences. The prefix disturbants were shared between the pairedsequences, ensuring that the latent dynamics of the model was guaranteed to remain identical up to thetarget token. The total input length was 1 + 63 = 22 + 1 + 41 = 64. The average similarity over 1024 inputpairs times five trials is plotted for every 5000 training iterations. the present analysis, the complex-valued gradients were treated as the double-sized real arrays, and a real-valued similarity was defined by Eq. 3. This is equivalent to taking the real component of the complex-valuedsimilarity, and is intuitively natural given that a perfect alignment between complex gradient directions yieldsa real-valued score of 1.0 (= the norm of a normalized complex vector). Additionally, the extra dimension inthe latent states representing the order of the Legendre polynomials was merged with the channel dimension,and the entire state was treated as a flattened vector.",
  "EvaluateDot-Product Similarity": ": Schematic illustration of the analysis of gradient stability.The RNN trained on the reverse-ordering task processed a pair of input sequences that shared the initial token (t = 1; blue) but differed inthe rest (2 t L; referred to as Disturbant A/B, and colored in orange/purple). For each dimension i",
  "Tat the first updated": "latent state z1 (= h1 in GRU; = concatenation of the hidden and cell states in LSTM, doubling the totaldimensionality to 2D) was computed per input sequence via backpropagation through time (dashed lines).The gradient stability was defined by the dot-product similarity of the paired gradients normalized over theoutput dimensions by the coefficients (s)i (s {A, B}), whose definition is provided in Eq. 4.",
  "Consequently, the stability metric emphasizes the consistency of the paired gradients that both have a greaterL2-norm across the output dimensions": "It deserves special note that the mapping from the first to the last RNN state was conditioned on thedisturbant tokens occurring at 2 t L. Nevertheless, the reverse-ordering task trained the networks toretrieve the initial token as their final output whatever tokens intervened in the middle. Thus, a well-trainedRNN would maintain invariance in its final state over the disturbants.Conversely, consistent gradientdirections across varied disturbants would lead to successful learning, which premises the proposed analysis. Unlike the RNN models, both the vanilla and position-encoded S4Ds achieved high accuracy over 96% forthe initial target token (t = 1), regardless of the frequency of the target and disturbants. Accordingly, for theanalysis of S4D, the target token was positioned in the middle at t = 23, where the vanilla model exhibitedits poorest accuracy with the Rare disturbants. The disturbants were prefixed and suffixed to this targetto construct input sequences. The prefix disturbants were shared between the paired sequences, therebyensuring that the latent dynamics of the model remained identical up to the target token. It should also be noted that the latent states of S4D are complex-valued (while its outputs are real-valued),and consequently, the gradients and their dot-product similarities are also complex-valued. For the sake of",
  "Difficulties in Handling a Large Vocabulary": "The present study introduced a novel challenge in training (vanilla) RNNs: large vocabularies. While inves-tigations into the manageable vocabulary size of RNNs appear to be a pertinent research areabeing crucialfor empirical applications such as natural language processingprevious studies were primarily dedicatedto evaluating and improving the memory duration of RNNs, and the vocabulary size in these studies wastypically set small (= eight; Arjovsky et al., 2016; Neil et al., 2016; Chang et al., 2017; Jing et al., 2017;2019; Voelker et al., 2019; Gu et al., 2020). This study examined the RNN gradients and identified their destabilization when processing low-frequencytokens, which are necessarily included in a large vocabulary. Specifically, inputs that do not contribute togradient-based optimization at a target time step (e.g., tokens at 2 t L upon the retrieval of the initialtoken at t = 2L in the reverse-ordering task) were found to be detrimental. In general cases of time series processing, data points carrying crucial information for specific time stepsbecome irrelevant otherwise. Consequently, each token exhibits a dual natureboth crucial and noisythroughout the task, and processing rare tokens is particularly challenging presumably because they areirrelevant at most of the time while making a large impact on the learning through the greater loss tocompensate for their fewer learning opportunities. Dealing with such unignorable noise presents a pervasivechallenge for RNNs.",
  "Functionality of Positional Encoding beyond the Timekeeper for Transformers": "Although low-frequency tokens destabilize the gradient-based learning of RNNs, the present study also dis-covered that this issue can be alleviated by positional encoding. This enhancement of RNNs via positionalencoding is noteworthy because RNNs were specifically designed to process time series data on their own;hence, unlike Transformers, they are presumed to function without relying on an external clock (Siegelmann& Sontag, 1992; 1994; 1995; Siegelmann, 1996; 1999). Consequently, position-encoded RNNs have remainedlargely unexplored, with only two exceptions to the best of the authors knowledge (Vincent-Lamarre et al.,2016; Karanikolos & Refanidis, 2019). The findings of the present studynamely, the improvement in themanageable vocabulary size due to the enhanced gradient stabilitybroaden the currently limited under-standing of the impact of positional encoding on RNNs. Additionally, the results of this study shed a new light on the utility of positional encoding. While positionalencoding has been viewed as nothing more than input timestamps for Transformers, the present studydemonstrated its efficacy in stabilizing the gradients of RNNs against disruption by low-frequency tokens.This novel functionality of positional encoding would not have been visible in Transformer studies, as the",
  "Limitations and Future Directions": "A primary unresolved question in this study pertains to the mechanism behind the gradient stabilizationby positional encoding.All the findings here are based on experimental investigations, lacking rigorousmathematical explanations for how and why the gradients of RNNs are destabilized by infrequent tokensand stabilized by positional encoding.Moreover, the present study primarily focused on the canonicalimplementation of sinusoidal positional encoding designed for Transformers (Eqs. 1,2), leaving it open whichparameters of the sinusoidal waves (i.e., frequencies and phases) are critical for gradient stabilization. Futureresearch may broaden its scope to encompass more general forms of positional encoding, such as waveletsand non-periodic signals (see Appendix D for a preliminary comparison among sinusoidal, learnable, andrandomly-fixed encodings). Moreover, the analysis of gradient stability did not fully address the enhanced performance of the position-encoded state-space model (S4D). In terms of accuracy, the positioned-encoded S4D exhibited greater ro-bustness to infrequent tokens compared to the vanilla model, resembling the behavior observed in RNNs.However, the gradients of the vanilla S4D were too stable to account for this descline in performance. Thisleaves the question open how positional encoding influences gradient-based learning of state-space models.Additionally, future studies may investigate a broader range of state-space modelsincluding the state-of-the-art architecture of Mamba (Gu & Dao, 2024)to achieve a comprehensive understanding of the interplaybetween positional encoding and these models. In addition to these scientifically oriented questions, future studies could also address practical applicationsof position-encoded RNNs and neural state-space models. Although positional encoding enhanced modelperformance across different synthetic tasks (see Appendix A), the extent of this enhancement is task-dependent. Indeed, while Karanikolos & Refanidis (2019) reported the effectiveness of positional encodingfor an LSTM text summarizer, the present study found no empirical advantage for the language modelingtask, aside from a slightly more rapid decline in training loss (see Appendix E). Thus, positional encoding isnot a panacea for arbitrary tasks, and further investigations are necessarily to determine when it is effective. This study was supported by JST ACT-X (JPMJAX21AN) and Core Research for Evolutional Science andTechnology (JPMJCR17A4, JPMJCR22P5); JSPS Grant-in-Aid for Early-Career Scientists (JP21K17805)and for Scientific Research A (JP24H00774), B (JP22H03914), and C (JP24K15087); and Kayamori Foun-dation of Informational Science Advancement (K35XXVIII620). The author also gratefully acknowledgesthe support of the ACCMS, Kyoto University, regarding the use of their supercomputer system. Martin Arjovsky, Amar Shah, and Yoshua Bengio. Unitary evolution recurrent neural networks. In Maria Flo-rina Balcan and Kilian Q. Weinberger (eds.), Proceedings of The 33rd International Conference on MachineLearning, volume 48 of Proceedings of Machine Learning Research, pp. 11201128, New York, New York,USA, 2022 Jun 2016. PMLR.",
  "Catalin V. Buhusi and Warren H. Meck. What makes us tick? functional and neural mechanisms of intervaltiming. Nature Reviews Neuroscience, 6(10):755765, 2005. doi: 10.1038/nrn1764": "Shiyu Chang, Yang Zhang, Wei Han, Mo Yu, Xiaoxiao Guo, Wei Tan, Xiaodong Cui, Michael Witbrock,Mark A Hasegawa-Johnson, and Thomas S Huang. Dilated recurrent neural networks. In I. Guyon, U. VonLuxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in NeuralInformation Processing Systems, volume 30. Curran Associates, Inc., 2017. Yining Chen, Sorcha Gilroy, Andreas Maletti, Jonathan May, and Kevin Knight. Recurrent neural networksas weighted language recognizers. In Marilyn Walker, Heng Ji, and Amanda Stent (eds.), Proceedings of",
  "Jeffrey L. Elman.Finding structure in time.Cognitive Science, 14(2):179211, 1990.doi:10.1207/s15516709cog1402_1": "Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolutional sequenceto sequence learning. In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th InternationalConference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pp. 12431252.PMLR, 0611 Aug 2017. Lukas Gonon and Juan-Pablo Ortega.Reservoir computing universality with stochastic inputs.IEEETransactions on Neural Networks and Learning Systems, 31(1):100112, 2020. doi: 10.1109/TNNLS.2019.2899649.",
  "Sepp Hochreiter and Jrgen Schmidhuber. Long short-term memory. Neural Computation, 9(8):17351780,1997. ISSN 0899-7667. doi: 10.1162/neco.1997.9.8.1735": "Herbert Jaeger and Harald Haas. Harnessing nonlinearity: Predicting chaotic systems and saving energy inwireless communication. Science, 304(5667):7880, 2004. ISSN 0036-8075. doi: 10.1126/science.1091277. Li Jing, Yichen Shen, Tena Dubcek, John Peurifoy, Scott Skirlo, Yann LeCun, Max Tegmark, and MarinSoljai. Tunable efficient unitary neural networks (EUNN) and their application to RNNs. In DoinaPrecup and Yee Whye Teh (eds.), Proceedings of the 34th International Conference on Machine Learning,volume 70 of Proceedings of Machine Learning Research, pp. 17331741. PMLR, 0611 Aug 2017. Li Jing, Caglar Gulcehre, John Peurifoy, Yichen Shen, Max Tegmark, Marin Soljacic, and Yoshua Bengio.Gated orthogonal recurrent units: On learning to forget. Neural Computation, 31(4):765783, 2019. doi:10.1162/neco_a_01174.",
  "Heewoo Jun and Alex Nichol. Shap-E: Generating conditional 3D implicit functions, 2023": "Apostolos Karanikolos and Ioannis Refanidis. Encoding position improves recurrent neural text summarizers.In Mourad Abbas and Abed Alhakim Freihat (eds.), Proceedings of the 3rd International Conference onNatural Language and Speech Processing, pp. 142150, Trento, Italy, September 2019. Association forComputational Linguistics. Jun-Seong Kim, Junghoe Kim, SeungUn Park, Kwangyong Lee, and Yoonju Lee. Modeling with recurrentneural networks for open vocabulary slots. In Emily M. Bender, Leon Derczynski, and Pierre Isabelle(eds.), Proceedings of the 27th International Conference on Computational Linguistics, pp. 27782790,Santa Fe, New Mexico, USA, August 2018. Association for Computational Linguistics.",
  "Eve Marder and Dirk Bucher. Central pattern generators and the control of rhythmic movements. CurrentBiology, 11(23):R986R996, 2001. doi: 10.1016/S0960-9822(01)00581-4": "Matthew S. Matell and Warren H. Meck. Cortico-striatal circuits and interval timing: coincidence detectionof oscillatory processes. Cognitive Brain Research, 21(2):139170, 2004. ISSN 0926-6410. doi: 10.1016/j.cogbrainres.2004.06.012. Neuroimaging of Interval Timing. Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. InProceedings of the 5th International Conference on Learning Representations (ICLR). OpenReview.net,2017. Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and RenNg. NeRF: Representing scenes as neural radiance fields for view synthesis. In Andrea Vedaldi, HorstBischof, Thomas Brox, and Jan-Michael Frahm (eds.), Proceedings of the European Conference on Com-puter Vision (ECCV), pp. 405421, Cham, 2020. Springer International Publishing.ISBN 978-3-030-58452-8. doi: 10.1007/978-3-030-58452-8_24.",
  "Peter M. Milner. A model for visual shape recognition. Psychological Review, 81(6):521535, 1974. ISSN0033-295X. doi: 10.1037/h0037149": "Daniel Neil, Michael Pfeiffer, and Shih-Chii Liu. Phased lstm: Accelerating recurrent network training forlong or event-based sequences. In D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett (eds.),Advances in Neural Information Processing Systems, volume 29. Curran Associates, Inc., 2016. Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin,Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in PyTorch. In NIPS AutodiffWorkshop, 2017. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen,Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, ZacharyDeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, andSoumith Chintala. PyTorch: An imperative style, high-performance deep learning library. In H. Wal-lach, H. Larochelle, A. Beygelzimer, F. dAlch Buc, E. Fox, and R. Garnett (eds.), Advances in NeuralInformation Processing Systems, volume 32, pp. 80248035. Curran Associates, Inc., 2019. Javier Perez-Orive, Ofer Mazor, Glenn C. Turner, Stijn Cassenaer, Rachel I. Wilson, and Gilles Laurent.Oscillations and sparsening of odor representations in the mushroom body. Science, 297(5580):359365,2002. doi: 10.1126/science.1070502. Joshua L. Proctor, Raghavendra P. Kukillaya, and Philip J. Holmes. A phase-reduced neuro-mechanicalmodel for insect locomotion: feed-forward stability and proprioceptive feedback. Philosophical Trans-actions of the Royal Society A: Mathematical, Physical and Engineering Sciences, 368(1930):50875104,2010. doi: 10.1098/rsta.2010.0134. Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-attention with relative position representations.In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computa-tional Linguistics: Human Language Technologies, Volume 2 (Short Papers), pp. 464468, New Orleans,Louisiana, June 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-2074. Xingjian Shi, Zhourong Chen, Hao Wang, Dit-Yan Yeung, Wai-kin Wong, and Wang-chun WOO. Con-volutional LSTM network: A machine learning approach for precipitation nowcasting.In C. Cortes,N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett (eds.), Advances in Neural Information ProcessingSystems, volume 28. Curran Associates, Inc., 2015.",
  "Hava T. Siegelmann and Eduardo D. Sontag.On the computational power of neural nets.Journal ofComputer and System Sciences, 50(1):132150, 1995. ISSN 0022-0000. doi: 10.1006/jcss.1995.1013": "Tianbao Song, Jingbo Sun, Yinbing Zhang, and Weiming Peng. An RNN model for generating sentenceswith a desired word at a desired position. Technical Gazette, 27(1):8188, February 2020. ISSN 1848-6339.doi: 10.17559/tv-20190929153200. Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole.Score-based generative modeling through stochastic differential equations. In 9th International Conferenceon Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021.",
  "Martin Sundermeyer, Ralf Schlter, and Hermann Ney. LSTM neural networks for language modeling. InProceedings of INTERSPEECH, pp. 194197, 2012. doi: 10.21437/Interspeech.2012-65": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ukasz Kaiser,and Illia Polosukhin. Attention is all you need. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach,R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems30, pp. 59986008. Curran Associates, Inc., 2017. Philippe Vincent-Lamarre, Guillaume Lajoie, and Jean-Philippe Thivierge. Driving reservoir models withoscillations: a solution to the extreme structural sensitivity of chaotic networks. Journal of ComputationalNeuroscience, 41(3):305322, 2016. doi: 10.1007/s10827-016-0619-3. Aaron Voelker, Ivana Kaji, and Chris Eliasmith. Legendre memory units: Continuous-time representationin recurrent neural networks. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch-Buc, E. Fox, andR. Garnett (eds.), Advances in Neural Information Processing Systems, volume 32. Curran Associates,Inc., 2019.",
  "A.1Reverse-Ordering + Delayed-Addition": "This section reports the performance of position-encoded RNNs on a more complicated, combinatorial taskthan the reverse ordering of input sequences.Extending the reverse-ordering task, the models receivedadditional random input integers during the output phase, and added each of them to the correspondingtoken in the reverse-ordered input sequence (modulo the vocabulary size, so that the output range wasbounded; ). This task was too challenging to GRUseven after reducing the input length to L = 16so only the resultsfrom LSTMs are reported below. Also, the network was trained for 600,000 iterations (i.e., twice longer",
  "Vanilla LSTMPosition-Encoded LSTM": ": Token-wise accuracy of the reverse-ordering task performed by the LSTM with and withoutpositional encoding (labeled as Position-Encoded and Vanilla respectively). During training, the inputlength was randomly selected from a range of 32 to 64. The vocabulary size was set to 16,384. The errorbars represent the 95% confidence interval estimated from 10,000 bootstrapped samples of five training-testtrials with different random seeds. Each of the five trials held out 1024 random sequences per length forcomputing the test accuracy.",
  "GRULSTM": ": Token-wise accuracy (left) and sequence-wise reconstruction errors (right) of the sorting taskperformed by RNNs with and without positional encoding (labeled as Position-Encoded and Vanillarespectively). The input length was fixed at L := 64. The error bars represent the 95% confidence intervalestimated from 10,000 bootstrapped samples of five training-test trials with different random seeds. Each ofthe five trials held out 1024 random sequences (= 65,536 tokens) for computing the test accuracy. This section reports the effectiveness of positional encoding for a task in which the order of input observationswas completely irrelevant; the learning objective was to simply sort the input integers in their inherentascending order (e.g. 8, 29, 2, 11 2, 8, 11, 29). The input integers were uniformly randomly sampled withreplacement, allowing for ties in the sorting process.",
  "A.3Predecessor Query": "Finally, this section presents benchmark results for the predecessor-query task, illustrated in . Thenetwork first received a sequence of non-repeating random integers, x1, . . . , xL. Subsequently, one of thenon-initial input integers, xtquery(2 tquery L), was randomly selected and reintroduced to the network attime t = L+1. The learning objective is to return the predecessor of the reviewed integer (= xtquery1). Thepredecessor-query task evaluates the capacity of RNNs to integrate information regarding both the orderand content of input sequences. As in the reverse-ordering + delayed-addition task, the input sequence was reduced to L = 16 due to thecomplexity of the task, and the experiment focused on the LSTM. The number of training iterations wasmaintained at 300,000.",
  "BRobustness to Variations in Input Length": "So far, all the tasks were experimented using fixed-length inputs (L = 64). One might wonder if positionalencoding is exceptionally effective under this setting, informing RNNs with the exact timing when each inputtoken should be returned as the output. Thus, it remains unclear whether or not position-encoded RNNscan also handle a larger vocabulary even when the input length is variable and, thus, the exact timing ofthe output emission is not identifiable from the positional encoding attached to the inputs. To assess the robustness to variations in the input length, an additional experiment was conducted on theLSTM, with the input length varied between 32 and 64. In this setup, the maximum input length (= 64)covers the entirety of the shortest input sequence plus its reversed reconstruction (= 32+32). Consequently,the positional encoding per se cannot even distinguish the input vs. output phases at t = 33, . . . , 64. Thevocabulary size was set to 16,384. As a result, the positional encoding still improved the LSTMs performance on the reverse-ordering taskagainst the perturbations in the input length (). This result suggests that the effectiveness of thepositional encoding for RNNs is not limited to strictly scheduled tasks.",
  "CEffects of Additional Parameters in Position-Encoded RNNs": "The concatenation of positional encoding with input embeddings inflates the number of learnable parametersin the input-to-hidden projection weights. This additional parameterization per se does not influence thelearning of the input embeddings, and therefore does not elucidate the enhanced performance of position-encoded RNNs. This section substantiates this argument by equalizing the number of learnable parametersbetween the vanilla and position-encoded models. Specifically, the equalization was achieved by concatenating two identical copies of the input embeddings andfeeding them to the LSTM. This configurationhenceforth termed double vanillaeffectively doubled thesize of the input-to-hidden weight for each gate in the LSTM, aligning it with that of the position-encodedLSTM, while maintaining all other parameters, including the dimensionality of the (non-repeated) inputembeddings. As illustrated in , the double vanilla LSTM did not yield any improvements in the reverse-orderingor sorting tasks. These results affirm that the reported enhancement of RNNs is not merely attributable tothe additional parameterization associated with the positional encoding.",
  "DAlternative Implementations of Positional Encoding": "While this study implemented positional encoding by sinusoidal waves (Vaswani et al., 2017), there arealternative implementations proposed in the previous studies. For instance, the BERT-based models typicallyencode each token position by a learnable embedding (Devlin et al., 2019). Moreover, the original studyof Transformer pointed out that even random vectors can function as positional encoding (Vaswani et al.,2017). Accordingly, these two alternative forms of positional encoding were tested on the LSTM performing thereverse-ordering task. The random position-encoding vectors were uniformly and independently sampledfrom the (5121)-dimensional hypersphere. The learnable embeddings were implemented using the canonicalembedding module of PyTorch (torch.nn.Embedding). The input length and vocabulary size were set to 64and 16,384 respectively. As shown in , both the random vectors and learnable embeddings improvedthe performance of LSTM.",
  "Sorting": ": Token-wise accuracy of the reverse-ordering and sorting tasks performed by the LSTM with andwithout positional encoding. The double vanilla model concatenated two copies of the identical inputembeddings to match the number of parameters with the position-encoded model. The input length wasfixed at L := 64. The vocabulary size was set to 16,384. The error bars represent the 95% confidence intervalestimated from 10,000 bootstrapped samples of five training-test trials with different random seeds. Each ofthe five trials held out 1024 random sequences (= 65,536 tokens) for computing the test accuracy.",
  "ELanguage Modeling": "This section reports benchmark results for the language modeling task. Single-layer LSTMs with and with-out sinusoidal positional encoding were trained and tested on the WikiText-103 dataset (Merity et al., 2017).Due to constraints in computational resources, the vocabulary was reduced from the original size of 267,735to 32,768 by retokenizing the raw data using SentencePiece (Kudo & Richardson, 2018). The headings wereremoved, and the main text was segmented by paragraphs (separated by the line break). Additionally, onlythe first 1024 tokens of each paragraph were utilized for training and testing, ensuring that the absolute posi-tional encoding always aligned with the beginning of each paragraph. The hyperparameters were configuredas specified in 3.3. As illustrated in , positional encoding proved effective only for marginally faster learning during theinitial phase of training. The difference diminished around 10,000/30,000 iterations, and the test perplexitiesof the position-encoded model were inferior to those of the vanilla model.",
  "VanillaSinusoidalRandomLearned": ": Token-wise accuracy of the reverse-ordering task performed by the LSTM with and withoutpositional encoding. Three variants of positional encoding (sinusoidal, randomly fixed vectors, and learnableembeddings) were tested. During training, the input length was randomly selected from a range of 32 to 64.The vocabulary size was set to 16,384. The error bars represent the 95% confidence interval estimated from10,000 bootstrapped samples of five training-test trials with different random seeds. Each of the five trialsheld out 1024 random sequences per length for computing the test accuracy."
}