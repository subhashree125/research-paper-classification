{
  "Abstract": "This paper addresses the problem of decentralized cooperative monitoring of an agile targetusing a swarm of robots undergoing dynamic sensor failures. Each robot is equipped witha proprioceptive sensor suite for the estimation of its own pose and an exteroceptive sensorsuite for target detection and position estimation with a limited field of view. Further, therobots use broadcast-based communication modules with a limited communication radiusand bandwidth. The uncertainty in the system and the environment can lead to intermittentcommunication link drops, target visual loss, and large biases in the sensors estimation out-put due to temporary or permanent failures. Robotic swarms often operate without leaders,supervisors, or landmarks, i.e., without the availability of ground truth regarding pose infor-mation. In such scenarios, each robot is required to exhibit autonomous learning by takingcharge of its own learning process while making the most out of available information. In thisregard, a novel Autonomous Online Learning (AOL) framework has been proposed, in whicha decentralized online learning mechanism driven by reward-like signals, is intertwined withan implicit adaptive consensus-based, two-layered, weighted information fusion process thatutilizes the robots observations and their shared information, thereby ensuring resilience inthe robotic swarm. In order to study the effect of loss or reward design in the local andsocial learning layers, three AOL variants are presented. A novel perturbation-greedy rewarddesign is introduced in the learning layers of two variants, leading to exploration-exploitationin their information fusions weights space. Convergence analysis of the weights is carriedout, showing that the weights converge under reasonable assumptions. Simulation resultsshow that the AOL variant using the perturbation-greedy reward in its local learning layerperforms the best, doing 182.2% to 652% and 94.7% to 150.4% better than the baselinesin terms of detection score and closeness score per robot, respectively, as the total numberof robots is increased from 5 to 30. Further, AOLs Sim2Real implementation has beenvalidated using a ROS-Gazebo setup.",
  "Introduction": "Target search, detection, tracking, and monitoring are crucial elements in a variety of high-impact real-worldapplications like search and rescue (Scherer et al., 2015), firefighting (Harikumar et al., 2018; Zhang et al.,2022), convoy protection (Spry et al., 2005), traffic monitoring (Khan et al., 2020), surveillance, etc., wherethe use of a multi-robot system can prove to be advantageous in terms of robustness and tracking perfor-mance, especially in unsafe and uncertain environments (Mohiuddin et al., 2020). With the advancementin sensor and communication technologies, making them miniaturized, inexpensive, and reliable, there hasbeen an increased interest in the use of robotic swarms for target search and tracking (Senanayake et al.,2016). Robotic swarms mostly operate under the philosophy of Swarm Intelligence (SI) (Beni & Wang,1993; Beni, 2004), which is characterized by decentralized local sensing and control, local communication,and the emergence of self-organizing global behaviors (Sharkey, 2007), along with three main advantages scalability, robustness, and adaptability (Bayindir & ahin, 2007), which are highly desirable in roboticswarms. The problem of cooperative target search and tracking has been studied widely in the literature. There aremany target-tracking works that solve the multi-robot problem settings involving an agile target being chasedby a swarm of robots. However, most of these works tackle this problem mainly from a control perspective.Moreover, such works assume accurate localization as well as target position estimation (Senanayake et al.,2016), i.e., they do not consider temporary or permanent failures in the proprioceptive (IMU, INS, odometry,etc.) and exteroceptive (camera, LiDAR, RaDAR, etc. algorithmic/sensor uncertainty in detection andrelative pose sensing) sensors onboard the robots.Yao et al. (2007) propose a stable and decentralizedcontrol strategy based on artificial potentials and sliding mode control to capture a moving target using arobotic swarm in a specific formation, where the artificial potentials take care of both tracking and formationtasks, and the sliding mode control ensures that the robots follow the required motion. Wang & Gu (2011)consider the problem of cooperative target tracking using a robot swarm with limited communication range,and propose a distributed Kalman filter-based estimation scheme with implicit consensus for the targetsposition estimation and a distributed flocking algorithm for motion control. Blazovics et al. (2012) propose asimple rule-based distributed algorithm for target tracking and surrounding using a swarm of homogeneousrobots based on the concept of basis behaviors, where the robots are aware of the targets position at all times.To achieve dynamic obstacle avoidance and target tracking using a swarm of robots, Radian et al. (2013) use adistributed Kalman filter to estimate the velocity and position of the unknown dynamic convex obstacles anda stochastic target, and a potential field approach to enable target tracking and obstacle avoidance amongthe robots. Kwa et al. (2020) present a fully decentralized swarming strategy offering tunable exploration-exploitation multi-agent dynamics, which is achieved by combining adaptive inter-agent repulsion and anadjustable network PSO-based strategy, thereby resulting in the optimal collective performance of the swarmcorresponding to a specific k-nearest neighbor graph connectivity. Unlike many works in the target search and tracking literature, this papers main focus is on the targetsposition estimation part of the overall cooperative target search and tracking task, rather than the controlpart this paper considers temporary or permanent sensor failures that cause inaccurate localization andtarget position estimation among the robots in the swarm. The robots use broadcast-based communicationmodules with a limited communication radius and bandwidth, exteroceptive sensors for target detection andposition estimation with limited field-of-view, and proprioceptive sensors for their own position estimation.The robot swarm has to operate under adverse situations involving temporary or permanent failures in thesensors resulting in large biases in their estimates, intermittent communication link drops, and target visualloss. Robot swarms often operate without leaders, supervisors, or landmarks, i.e., without the availabilityof a ground truth regarding pose information; typically, the robots use the ground truth to identify whichsensors or neighboring robots are undergoing failures. Without the availability of a landmark or a leader,the robots are required to collaborate among themselves. In this regard, a decentralized online learningframework called the Autonomous Online Learning (AOL) framework has been proposed in this paper,which is used to drive an adaptive and fault-tolerant information fusion process among robots. The AOLframework takes its philosophical inspiration from the concepts of independent learning (Hockings et al.,2018) and self-directed learning (Garrison, 1997) in educational psychology in which the learner, devoid of",
  "any supervisor or teacher, takes charge of its own learning process while making the most out of the availableresources and information": "In the AOL framework, an autonomous online learning mechanism is intertwined with an implicit adaptiveconsensus-based, two-layered (local and social), weighted information fusion process, which is driven by therobots observations and their shared information. This enables each robot to figure out which neighboringrobots information about the targets position can be trusted at a given time instant, thus bringing inan aspect of resilience to the robotic swarm the robots undergoing sensor failures are less likely to betrusted by themselves as well as their neighbors for their sensor information, thereby improving their targetsposition estimates by giving more weight to the information shared by robots with functional sensors. Thelearning process in the AOL framework is inspired by that of the exponentially weighted online learningforecaster (Cesa-Bianchi & Lugosi, 2006), which is a centralized online learning algorithm driven by a lossfunction calculated using ground truth. But unlike the exponentially weighted online learning forecaster,the AOL framework involves decentralized online learning which is driven by reward-like signals, exhibitingexploration-exploitation behaviors in the online learning process.Since the main focus of this paper isnot the control part of the target search and tracking task, a simplified target search and tracking controlstrategy has been used, which can be replaced by any other advanced control strategy. For instance, thecontrol strategies proposed by Radian et al. (2013) and Kwa et al. (2020) can be used along with the AOLframework to achieve better swarm formation control behavior while ensuring resilience in the robotic swarm. To understand the effect of loss/reward designs in the local and social learning layers, three different AOLalgorithms are presented, namely AOL-C, AOL-1P, and AOL-2P. A novel perturbation-greedy reward designis introduced in the learning process of two AOL variants, leading to exploration-exploitation in their infor-mation fusions weights space. Among the AOL variants, AOL-C involves a comparative loss function inits local learning layer, AOL-1P involves the perturbation-greedy reward function in its local learning layer,and AOL-2P involves the perturbation-greedy reward functions in both its local and social learning layers. Theoretical analysis of the three AOL variants is carried out, showing that the weights converge underreasonable assumptions. The performance of the three AOL variants is then evaluated in a simulated envi-ronment with adverse conditions involving sensor failures. The three AOL variants are compared against twodecentralized fusion methods Averaging-Consensus Fusion (ACF) and Kalman-Consensus Fusion (KCF).Averaging consensus and Kalman filter-based approaches have been used in various works (Katragadda et al.,2017; Zhang & Li, 2019; Azam et al., 2020) for the purpose of multi-estimate fusion. Hence, a comparison ofthe proposed AOL algorithms is provided with ACF and KCF fusion methods as baselines. However, sensorfailures inducing sudden biases in the estimates may or may not increase their covariance, and therefore,covariance-based methods may not perform satisfactorily or may even fail. Simulation results show that thebest-performing variant, AOL-1P, performs 182.2% to 652% and 94.7% to 150.4% better than the baselinesin terms of cumulative average detection score per robot and cumulative average closeness score per robot,respectively, as the total number of robots is increased from 5 to 30. Simulation results further reveal thatusing the target detection confidence as a reward signal for the update of weights in the social learning layer,as in AOL-1P, does better than using a perturbation-greedy reward-based learning strategy as in AOL-2P.However, using a perturbation-greedy reward-based learning strategy for the update of weights in the locallearning layer, as in AOL-1P, does better than using a comparative Euclidean distance-based loss functionas in AOL-C. Further, the top two performing AOL variants (AOL-1P and AOL-C) Sim2Real aspects areevaluated in Gazebo using ROS1. The rest of this paper is organized as follows: section 2 presents the problem formulation, and presents the AOL framework, along with its three variants. presents a theoretical analysis of theconvergence of weights involved in the AOL variants. presents AOLs performance evaluation,comparing the three variants with two baselines. Finally, section 6 concludes this paper. Further, a table ofnomenclature is provided in the appendix (A.1), along with a brief discussion on the generalizability of theAOL framework (A.2) and the pseudo-code for all three AOL variants (A.3).",
  "Problem Formulation": "The problem of Decentralized Cooperative Target Monitoring considered in this paper involves a swarm ofrobots equipped with broadcast-based communication modules with limited range and bandwidth, propri-oceptive sensors (e.g., IMU, INS, optical encoders, etc.), and exteroceptive sensors (e.g., camera, LiDAR,RaDAR, etc.) with limited field-of-view (FOV), as shown in . The goal of the robots is to detect,track, and stay as close as possible to the target while maintaining some distance from it, thus ensuring ahigh degree of detection and monitoring of the target at all times. However, the sensors onboard the robotsmay undergo temporary or permanent failures, along with intermittent target visual loss and communicationlink drops due to system and environmental uncertainty, thereby jeopardizing the target monitoring goal ofthe robotic swarm. Therefore, the robots are required to collaborate over the communication network tohelp each other detect and monitor the target successfully.",
  "t+1,i = t,i + T wt,i(1b)": "where xt,i R2 is the ith robots 2-D position vector (in m), vt,i R2 is the ith robots body-axis velocityvector (m/s), t,i R is the ith robots heading angle (radians), and wt,i R is the ith robots yaw rate(rad/s) at discrete-time t, respectively. Here, the body-axis velocity vt,i and yaw rate wt,i are boundedcontrol inputs for the ith robot. Target Kinematic Model: the targets kinematics also follow the model stated as equation (1), but itsdynamics is unknown. The targets position vector xt,B R2 (in m), heading angle t,B R (radians),body-axis velocity vt,B R2 (m/s), and yaw rate wt,B R (rad/s), respectively, can be represented byreplacing i with B (Bogey) in the set of equations (1). Similarly, vt,B and wt,B are the bounded controlinputs to the target at time t, which are unknown to the robots since its dynamics is unknown. Communication Model: let Rcomm. be the range of communication, and let pld be the communicationlink drop probability.The topology of the communication network between robots is represented by auni-directional dynamic graph Gt, whose adjacency matrix At satisfies the following, i, j [N] such thati = j,",
  ": (a) Exteroceptions field-of-view (FOV). (b) Target Search strategy inspired by food foragingpattern used by Oxyrrhis Marina": "where [At]ij is the ijth element of At, Uij(0, 1) is a uniform random variable. For i = j, [At]ii = 0.The above equation implies that even if two robots are in the communication range of each other, thecommunication link between them may still drop with a probability of pld.",
  "t,i = {j: [At]ij = 1 j [N]}(3)": "Further, define nt,i := |t,i|, i.e., nt,i is the number of communicating neighbors of the ith robot at timet. Moreover, due to limited bandwidth, there is a limit on the number of neighbors the ith robot can haveat time t, i.e., nt,i nl, where nl {1, 2, , N} based on the limitations posed by the communicationhardware. In case more than nl robots appear in the communication range of the ith robot, nl robots arechosen randomly out of them to be considered as communication neighbors, such that nt,i nl holds. Proprioceptive Sensor Model: the proprioceptive sensor suite onboard the ith robot is responsible forthe estimation of its 2-D position and yaw angle, where the estimates are denoted as xPit,i R2 and Pit,i R,respectively, and are modeled as follows:",
  "where xt,i R2 and t,i R represent bounded arbitrary noise in the ith robots proprioceptive sensorsuites estimates, i [N]": "Exteroceptive Sensor Model: the exteroceptive sensor suite of the ith robot is responsible for the detec-tion and relative position estimation of the target, in terms of the target detection confidence dt,i andthe targets relative position estimate xEit,B R2. The exteroceptive sensor suite is further characterizedby a limited field-of-view (FOV), with a detection range RFOV and an angle-of-view FOV, as shown in a. Let pvl be the probability of target visual loss. If the target lies outside the FOV region, dt,i = 0. Onthe other hand, if the target lies inside the FOV region, the detection confidence dt,i follows the model asstated below:",
  "Published in Transactions on Machine Learning Research (12/2024)": "Hian Lee Kwa, Jabez Leong Kit, and Roland Bouffanais. Optimal swarm strategy for dynamic target searchand tracking. In Proceedings of the 19th International Conference on Autonomous Agents and MultiAgentSystems, pp. 672680, 2020. Chris D Lowe, Patrick J Keeling, Laura E Martin, Claudio H Slamovits, Phillip C Watts, and David JSMontagnes. Who is oxyrrhis marina? morphological and phylogenetic studies on an unusual dinoflagellate.Journal of Plankton Research, 33(4):555567, 2011.",
  "vt,i = vRt,i + vt,i(8)": "where vRt,i is the ith robots velocity reference command signal responsible for either chasing or searching thetarget and vt,i is the ith robots velocity correction control signal responsible for avoiding collisions withother robots (more details in the supplementary document). Note that any other advanced target searchand tracking control law can be used here. But for the sake of simplicity, the following search and trackingcontrol strategy has been used in this paper. Target Chase Mode: This mode gets activated whenever the ith robot has detected the target (dt,i > 0) orone of its neighbors has detected the target (dt,j > 0, j t,i). In this mode, the ith robot considers itsestimate of the targets position xit,B as the reference position for the reference velocity control law. Theyaw control of the robot makes sure that the robots longitudinal body axis points towards the targetsestimated position xit,B. A more detailed description, along with the control algorithm, is included in thesupplementary document. Target Search Mode: This mode gets activated whenever the ith robot has not detected the target (dt,i = 0)and either none of its neighbors have detected the target (dt,j = 0, j t,i) or it has no neighbors (nt,i = 0).In this mode, the ith robot executes a search pattern inspired by the food foraging pattern used by OxyrrhisMarina (Lowe et al., 2011; Harikumar et al., 2018), as shown in b. The robot first chooses a randomdirection to move towards. With its longitudinal body axis aligned with that direction, it moves in thatdirection using its longitudinal velocity control while doing a growing sinusoidal maneuver (green curve inb) using its lateral velocity control to cover more area as it moves. After Ts discrete-time steps, therobot randomly chooses a new direction and repeats the process.",
  "Reward-based Autonomous Online Learning for Cooperative Target Monitoring": "The Reward-based Autonomous Online Learning (AOL) framework proposed in this paper is a decentralizedonline learning framework designed for cooperative target monitoring using a swarm of robots in a non-stationary environment. Its ability to learn in real-time without the need for any ground truth allows therobots to exhibit high target monitoring success in scenarios where the sensors onboard the robots may beundergoing temporary or permanent failures, even when landmarks, supervisors, or leaders are unavailable.A systems diagram highlighting where the AOL module fits in each of the robots is shown in . Within the AOL framework, three variants of AOL algorithms are proposed, namely AOL-C, AOL-1P, andAOL-2P, where AOL-C involves a comparative loss function in one of its learning layers, AOL-1P involves a",
  "i(t 1) + i(t 1)(11)": "where the weights i(t 1) and i(t 1) are initialized as i(0) = i(0) = 1. These weights are updated inthe local learning layer of the learning phase. Note that the weight i(t1) is indicative of how much weightshould be given to the combined sensor estimate in the local estimate fusion process at time t relative to the",
  "Communication phase: The ith robot broadcasts the information {t, i, dt,i, xIit,B, xit1,B, wii(t 1)} and": "receives the information {t, j, dt,j, xIjt,B, xjt1,B, wjj(t 1)} from its communicating neighbors j t,i. Here,the weight wii(t 1) is initialized as wii(0) = 1, and is updated in the social learning layer of the learningphase. Social estimation phase: The second estimation phase involves a weighted fusion of the informationavailable socially to the robots, i.e., the intermediate estimates xIjt,B given by its communicating neighborsj t,i, and its own intermediate estimate xIit,B, to form its final estimate xit,B of the targets position, asfollows:xit,B =",
  "jt,i wjj(t 1)(13)": "where the weight wii(t 1) is updated in the ith robots social learning layer of the learning phase, and theweights wjj(t 1) are obtained from its communicating neighbors j t,i during the communication phase.The weight wii(t 1) is initialized as wii(0) = 1, i [N]. Note that the weight wij(t 1) is indicativeof how much weight should be given to the jth robots intermediate estimate in the social estimate fusionprocess at time t relative to other robots in the set t,i. Further, let wii(0) = wii(1) = 1 (for the AOLvariants with perturbations, called AOL-1P and AOL-2P). Learning phase: The online learning of the ith robot is primarily driven by the objective of maximizingits target detection confidence dt,i directly or indirectly, which acts as a reward-like signal guiding a mul-tiplicative exponential update process for the weights used in both the estimation phases. The learningphase for all three variants involves a periodic reset of the weights i(t), i(t), and wii(t) to deal with thesystem uncertainty and non-stationary environment; the periodic reset is required to eliminate any biasesaccumulated in the learned weights. In practice, the multiplicative exponential weight update process allowsfor a higher learning rate that can cover the loss in performance due to periodic reset of the learned weights.",
  "lt,i := min{||xit1,B xIrt,it,B ||/Do, 1}(15b)": "where rt,i := arg maxjt,i wij(t 1), and wij(t 1) is calculated as per equation (13).Here, Do is anormalization parameter, and rt,ith robot is one whose intermediate estimate is considered to be the mostaccurate among the estimates of robots in the set t,i = t,i {i} based on the weights wij(t 1). Thus,",
  "Comparative Loss Function: given by equations (15), the loss functions lt,i and lt,i are a measure of": "how close the estimates xSit,B and xit1,B are to the socially best intermediate estimate xIrt,it,B , respectively.Therefore, an increase in these loss functions leads to a decrease in the weights i(t) and i(t), respectively.This implies an increase in the local weight i(t) if the estimate xSit,B is more close to the socially bestintermediate estimate compared to xit1,B, and vice versa.",
  "where equations (18) constitute the local learning layer": "The learning phase involves a periodic reset - the weights i(t), i(t), and wii(t) are reset to 1 after everyTp discrete time steps, along with the perturbation signal ep either taking the value ep = pmag R>0 orep = 0 with equal probability, whereas ep = pmag ep. Otherwise, ep = ep = 0 at all other times. Perturbation-greedy Reward Function: given by equations (17), the reward functions rt,i and rt,iconsist of three terms - a difference-based correction term (i(t 1)di(t)), an inertia term (i(t 1)dt,ior (1i(t1))dt,i), and a perturbation term (ep(1dt,i) or ep(1dt,i)). The difference-based correction termacts as the greedy part of the reward function, whereas the role of the inertia term is to resist abrupt changesin the reward due to the greediness of the difference-based correction term. The role of the perturbationterm is to apply a perturbation periodically, thereby bringing in exploratory behaviors in the online learningprocess. After a perturbation is applied when the periodic reset is hit, the weight i(t 1) may increase(i(t 1) > 0) or decrease (i(t 1) < 0), possibly leading to a change in the detection performance aswell (di(t) > 0 or di(t) < 0). Therefore, a positive difference-based correction term (i(t1)di(t) > 0)leads to an increase in rt,i and a decrease in rt,i, thereby increasing i(t) and decreasing i(t), and vice versa.Note that the inertia term (i(t1)dt,i or (1i(t1))dt,i) is directly proportional to detection confidence(dt,i); a higher detection confidence promotes a larger inertia. Since perturbations should be avoided ifthe detection confidence is already high, note that the magnitude of the perturbation term (ep(1 dt,i) orep(1 dt,i)) decreases as the detection confidence dt,i increases.",
  "rwt,i :=ew1wii(t 1)di(t) + ewip (1 dt,i) + ew2wii(t 1)dt,i(19)": "where wii(t 1) := wii(t 1) wii(t 2), di(t) := dt,i dt1,i, ewipis the perturbation signal, andew1 > 0 and ew2 > 0 are learning rate parameters. The terms wii(t 1)di(t) and wii(t 1)dt,i canbe considered as difference-based correction term and the inertia term, respectively, whereas ewip (1 dt,i) isthe perturbation term that is only active when the periodic reset is hit. With the above-described rewardfunction definition, the weights wii(t 1) are updated as follows:",
  "where equation (20) constitutes the social learning layer": "The learning phase involves a periodic reset - the weights i(t), i(t), and wii(t) are reset to 1 after everyTp discrete time steps, along with the perturbation signal ewiptaking the value ewip= Unif.(0, pmag), wherepmag R>0 and Unif.(0, pmag) is a uniform random variable; this ensures that the ewipvalues for differenti [N] are likely to be different at the periodic reset. Otherwise, ewip= 0 at all other times. Perturbation-greedy Reward Function: given by equation (19), the reward function rwt,i consists ofthree terms - a difference-based correction term (wii(t 1)di(t)), an inertia term (wii(t 1)dt,i), anda perturbation term (ewip (1 dt,i)). The difference-based correction term acts as the greedy part of thereward function, whereas the role of the inertia term is to resist abrupt changes in the reward due tothe greediness of the difference-based correction term.The role of the perturbation term is to apply aperturbation periodically, thereby bringing in exploratory behaviors in the online learning process. After aperturbation is applied when the periodic reset is hit, the weight wii(t 1) may increase (wii(t 1) > 0)or decrease (wii(t 1) < 0), possibly leading to a change in the detection performance as well (di(t) > 0or di(t) < 0). Therefore, a positive difference-based correction term (wii(t 1)di(t) > 0) leads to anincrease in rwt,i, thereby increasing wii(t), and vice versa. Note that the inertia term (wii(t1)dt,i) is directlyproportional to detection confidence (dt,i); a higher detection confidence promotes a larger inertia. Sinceperturbations should be avoided if the detection confidence is already high, note that the magnitude of theperturbation term (ewip (1 dt,i)) decreases as the detection confidence dt,i increases.",
  "Convergence Analysis of Weights": "Note that at time t = 0, the weights i(0) = 1, i(0) = 1, and wii(0) = 1. Without the loss of generality, ananalysis of the three AOL variants is carried out without considering a periodic reset; this is done to gaininsights into their convergence behavior just after a periodic reset is hit and just before the next periodicreset is about to hit. Note that each time instant when the periodic reset hits can be considered as t = 0from an analysis perspective. Therefore, the analysis can be regarded as valid for each time interval betweentwo successive periodic reset hits. In practice, given a periodic reset time Tp, a higher learning rate (setby learning rate parameters) would lead to quick convergence of the weights before the next periodic resethits. If the learning rate is low, the weights may not converge to their respective convergence values by thetime the periodic reset hits. However, their convergence behavior would remain the same, as shown in thetheoretical analysis in this section. In order to analyze the effect of perturbations in the rewards of AOL-1P and AOL-2P, for ease in theo-retical analysis, it is assumed that their reward functions undergo perturbations only at t = 1 due to theperturbation-greedy reward design. Note that in practice, the perturbations occur just after the periodicreset after every Tp discrete-time steps.",
  "AOL-C": "Consider the loss function lwt,i := (1 dt,i), and note that lwt,i , since dt,i . Thus, using equation(14), the weight wii(t) can be written as: wii(t) = exp(wLwt,i), where the cumulative loss Lwt,i := ts=1 lws,i,and wii(0) = 1, i [N]. Without the loss of generality, define jt,i := arg minjt,i Lwt,j, such that the jt,ith robot is a unique robotthat incurs the least cumulative loss among the robots in the set t,i = t,i {i} at time t, where t,i is theneighbor-set of the ith robot at time t.",
  "Assumption 2: Lwt,j Lwt,jt,i t > 0, such that (0, 1] and 0 < 1": "Remark 2: Assumption 2 implies that the lower bound on the difference between the cumulative loss incurredby the jth robot and that of the jt,ith robot grows sub-linearly (0 < < 1) or linearly ( = 1) with thediscrete-time t, such that the magnitude and the rate of growth are finite but can be arbitrarily small(0 < 1 and 0 < 1). Note that assumption 2 generalizes over many practical scenarios. For instance,consider the scenario where the jt,ith robot is fixed over time, i.e., jt,i = j0,i, and satisfies lwt,j lwt,j0,i > 0, for t 1, i.e., the j0,ith robot incurs the least loss at any time t. This implies that Lwt,j Lwt,j0,i t > 0,where 0 < 1, which is a special case under assumption 2 when = 1. In practice, assumption 2 wouldbe satisfied for any such scenario where the best (jt,ith) robot stays fixed for some finite time duration andmay change intermittently over time. In the simulation studies (section 5), such intermittent changes resultfrom temporary/permanent sensor failures, intermittent communication link loss, and target visual loss.",
  "Since the update strategy for the weights wii(t) for AOL-1P is the same as that of AOL-C, therefore, theconvergence proof for AOL-1Ps weights wij(t) is given by theorem 1": "Consider the weights i(t), i [N], defined by equation (11), and the update strategy given by equations(17) and (18). Define Rt,i := ts=1 rt,i and Rt,i := ts=1 rt,i, where rt,i and rt,i are given by equations(17a) and (17b), respectively. Note that i(0) = 1 and i(0) = 1. Further, i(t) , and dt,i ,i [N]. Therefore, for t 1, the difference-based correction term satisfies i(t 1)di(t) ,where i(t 1) = i(t 1) i(t 2) and di(t) = dt,i dt1,i. Assumption 3: For t 1, the difference-based correction term i(t1)di(t) satisfies the following: eitherts=1 i(s 1)di(s) t > 0 or ts=1 i(s 1)di(s) t < 0, where 0 < 1, (0, 1],i(t 1) = i(t 1) i(t 2), and di(t) = dt,i dt1,i. Remark 3: Assumption 3 implies that the cumulative sum of the difference-based terms over time is eitherlower bounded by a positive term which is linear ( = 1) or sub-linear ( (0, 1)) in time, or upperbounded by a negative term which is linear ( = 1) or sub-linear ( (0, 1)) in time, such that themagnitude and the rate of growth of these bounds are finite but can be arbitrarily small (0 < 1 and0 < 1). Note that assumption 3 generalizes over many practical scenarios. For instance, consider thescenario in which either i(t 1)di(t) > 0 or i(t 1)di(t) < 0, for t 1, implyingeither ts=1 i(s 1)di(s) t > 0 or ts=1 i(s 1)di(s) t < 0, which is a special caseunder assumption 3 when = 1. This case corresponds to the scenario in which an increase in i(t) alwayscauses either an increase or a decrease in dt,i for t 1. Considering equation (9), this further implies thateither the estimate xSit,B or the estimate xit1,B is more accurately estimating the target position xt,B fort 1. In practice, assumption 3 would be satisfied for any such scenario where either xSit,B or xit1,B staysmore accurate than the other for some finite time duration and but the accuracy ranking among these maychange intermittently over time. In the simulation studies (section 5), such intermittent changes result fromtemporary/permanent sensor failures, intermittent communication link loss, and target visual loss.Theorem 2. For t 1, under assumption 3, given ea2t1 < 2ea1 (check equations (17)), i [N],AOL-1P algorithms weights i(t) satisfy the following:",
  "+ exp (Rt,i Rt,i)(30)": "Since the perturbations occur just after t = 0, therefore at t = 1, either ep = pmag&ep = 0 or ep =0&ep = pmag, and for t > 1, ep = ep = 0 (assumed for ease in analysis, without the loss of generality),i [N]. Thus, Rt,i Rt,i can be written as",
  "Since the update strategy for the weights i(t) and i(t) for AOL-2P is the same as that of AOL-1P, therefore,the convergence proof for AOL-2Ps weights i(t) is given by theorem 2": "Consider the weights wii(t), i [N], defined by equation (13), and the update strategy given by equations(19) and (20). Define Rwt,i := ts=1 rwt,i, where rwt,i is given by equation (19). Note that wii(0) = 1. Further,wii(t) and dt,i , i [N]. Therefore, for t 1, the difference-based correction term satisfieswii(t 1)di(t) , where wii(t 1) = wii(t 1) wii(t 2) and di(t) = dt,i dt1,i. Without the loss of generality, define qt,i := arg maxjt,its=1 wjj(s 1)dj(s), such that qt,i is uniqueamong j t,i; this implies that a cumulative change in the qt,ith robots social weight causes the mostpositively or least negatively aligned cumulative change in its confidence score compared to other robotsincluding the ith robot and its neighbors at time t.Therefore, as per the weight update rule given byequations (19) and (20), qt,ith robot is a unique robot among the robots in the set t,i whose intermediateestimate is given more weight while performing the weighted fusion as given by equation (12).",
  "Assumption 4: limt t,i and limt qt,i exist uniquely, such that limt t,i = ,i and limt qt,i =q,i, i [N]": "Remark 5: Assumption 4 implies that the neighborhood configuration (in terms of the set t,i) and theperformance configuration (in terms of the difference-based correction term (wjj(t 1)dj(t)) get fixedas t . That is, for the ith robot, i [N], at t , there exists a unique robot q,i that exhibits thegreatest cumulative sum of the difference-based correction term (i.e., ts=1 wjj(s 1)dj(s)) among therobots j ,i. Note that the cumulative sum of difference-based correction terms satisfies t ts=1 wjj(s1)dj(s) t,since wii(t 1)di(t) . Further, due to qt,is definition, the cumulative sum of difference-basedcorrection terms satisfies ts=1 wqq(s 1)dq(s) < ts=1 wqt,iqt,i(s 1)dqt,i(s), q t,i \\ {qt,i}.",
  "Assumption 5: ts=1 wqt,iqt,i(s 1)dqt,i(s) ts=1 wqq(s 1)dq(s) t > 0, such that (0, 1]and 0 < 1, q t,i \\ {qt,i}": "Remark 6: Assumption 5 implies that the difference between the cumulative sum of difference-based correc-tion terms corresponding to qt,i and q (q t,i \\{qt,i}), i.e., the difference ts=1 wqt,iqt,i(s1)dqt,i(s)ts=1 wqq(s 1)dq(s), has a lower bound that varies linearly ( = 1) or sub-linearly (0 < < 1) withthe discrete-time t, such that the magnitude and the rate of growth of this lower bound is finite but canbe arbitrarily small (0 < 1 and 0 < 1). Note that assumption 5 generalizes over many practicalscenarios. For instance, consider the scenario where the qt,ith robot is fixed over time, i.e., qt,i = q0,i, andsatisfies wq0,iq0,i(t 1)dq0,i(t) wqq(t 1)dq(t) > 0 for t 1, q t,i \\ {q0,i}, i.e., the q0,ith robots change in its social weight shows the most positively aligned or the least negatively aligned changein its detection confidence score compared to other robots including the ith robot and its neighbors for timet 1. This implies ts=1 wq0,iq0,i(s 1)dq0,i(s) ts=1 wqq(s 1)dq(s) t, which is a specialcase under assumption 5 when = 1. Considering equation (12), this further implies that the estimate xIq0,it,Bis more accurately estimating the target position xt,B compared to other estimates in the weightedfusion, for t 1. In practice, assumption 5 would be satisfied for any such scenario where the best (qt,ith)robot stays fixed for some finite time duration and may change intermittently over time. In the simulationstudies (section 5), such intermittent changes result from temporary/permanent sensor failures, intermittentcommunication link loss, and target visual loss.",
  "where 0 < 1, (0, 1], and q,i is the index of the robot that exhibits the greatest cumulative sum ofthe difference-based correction term among the robots j ,i": "Remark 7: Since 0 < 1, the condition ew2t1 < ew1 further yields ew2t1 ew1. This can besatisfied either by choosing ew2 = 0, or by choosing a time-varying ew2 such that ew2(t) tc for c > 0,where 1 c 0. In practice, since the AOL-2P algorithm involves a periodic reset with a period ofTp discrete-time steps, this condition implies ew2Tp1 ew1. This condition can be satisfied when ew2 ischosen to be much smaller than ew1 or ew2 0.",
  "qt,i\\{qt,i} exp (Rwt,q Rwt,qt,i)(37)": "Using the reward definition of rwt,i as given by equation (19), since the perturbations occur just after t = 0,therefore at t = 1, ewip= Unif.(0, pmag) and for t > 1, ewip= 0 (assumed for ease in analysis, without the lossof generality), i [N], Rwt,i can be written as",
  "Performance Evaluation": "The proposed AOL framework is evaluated using a simulation setup involving N = 5, 10, 20, 30 robotsexecuting the cooperative target monitoring task discussed in the problem formulation. The communicationrange Rcomm. and the communication link drop probability pld are set to be 30 m and 0.1, respectively,with the limit on the number of communication neighbors as nl = 3. The exteroceptive sensor modelsparameters are set as RFOV = 15 m, FOV = 160 degrees, with the target visual loss probability as pvl = 0.1.The parameters for the detection confidence model are set as ro = 10 m, and bo = 0.1. The simulation results are averaged over 100 simulation runs. Each run involves a time horizon of T = 600discrete time steps, with a sampling period of T = 0.1 sec. The robots follow the control law described inthe problem formulation while trying to maintain a safe distance of 8 m from the target (more details in thesupplementary document). The target randomly changes its velocity and yaw rate after every 5 seconds. Therobots and the target always stay inside a square region of side length 100 m by overriding their control lawsto get away from the region boundary. At the start of each simulation run, the robots are always spawnednear the center of the square region, whereas the target is spawned randomly but sufficiently near to therobots so that at least one of the robots is likely to detect it at the start of the run. This is done since themain focus of this paper is not the target search but target detection and monitoring. In the considered adverse scenario for the simulation, 50% of the total robots are chosen randomly at times0, 10, 20, 30, and 50 seconds, that exhibit temporary failures in their exteroceptive sensors. Further, 50% ofthe total robots are chosen randomly at time 10 sec., that exhibit permanent failures in their proprioceptivesensors. The noise xt,i and t,i in the proprioceptive sensors (equations (4)) that are functional is assumedto be Gaussian with a mean of 0.01m and 0.02 deg., respectively, with a covariance of 0.01m2 and 0.01rad.2,respectively. The proprioceptive sensors that fail have noise terms that are still Gaussian but with large bias(mean) of 5m and 10 deg., respectively, with either a covariance of 0.01m2 or 5m2 with equal probabilityand 0.01rad.2 or 5rad.2 with equal probability, respectively, since the bias and the covariance may beuncorrelated.Similarly, the noise t,i in the exteroceptive sensors (equation (6)) that are functional is",
  "A snapshot of the simulation of the AOL-1P in adverse conditions is shown in . Simulation videos aresubmitted as a supplementary file": "Figures 5a, 5b, 6a, and 6b show the comparison results for N = 5, N = 10, N = 20, and N = 30, respectively.For each method, 100 simulations are run, and the resulting cumulative scores are collected as points, whichare then used to generate their corresponding box plots. Note that on each box, the central mark indicatesthe median, and the bottom and top edges of the box indicate the 25th and 75th percentiles, respectively.For N = 5, N = 10, N = 20, and N = 30, the best variant AOL-1P performs 182.2%, 329.7%, 463%, and652% better in terms of cumulative detection score and 94.7%, 138.6%, 167.3%, and 150.4% better in termsof cumulative closeness score than the best baseline ACF, respectively. Further, for N = 5, N = 10, N = 20,and N = 30, AOL-1P performs 3.62%, 9.41%, 8.86%, and 1.1% better in terms of cumulative detection scoreand 8.37%, 3.42%, 4.5%, and 0.0% better in terms of cumulative closeness score than the second best AOLvariant, AOL-C. Among the baselines, ACF performs better than KCF for all the cases in terms of bothtypes of scores. The three AOL variants perform significantly better compared to KCF; since KCF is a covariance-basedmethod, it is unable to handle adverse conditions involving temporary or permanent sensor failures inducinglarge biases that may or may not increase the covariance. Since the weights are adapted through an onlinelearning process in the AOL variants, their performance is better than ACF as well; unlike the AOL variants,ACF gives equal weight to all the input estimates being fused in the weighted estimate fusion process. Theno-communication case performs better than KCF and comparably to ACF; unlike the AOL variants, KCFand ACF do not have a mechanism to adaptively filter out the corrupt information coming from the robotsundergoing sensor failures, due to which the corrupt information propagates throughout the communicationnetwork without any inhibition, thereby corrupting even those robots having functional sensors. Using the detection confidence dt,i as a reward signal does better than having a perturbation-greedy reward-based learning strategy for the update of weights wii(t) in the social learning layer, which is quite evident bythe improved performance of AOL-1P and AOL-C compared to the AOL-2P. However, using a perturbation-greedy reward-based learning update strategy for the update of weights i(t) and i(t) in the local learninglayer, as in AOL-1P, does better than using a comparative Euclidean distance based loss function as in AOL-C. Moreover, for AOL-1P, when N is increased from 5 to 10, the cumulative detection score and the closenessscore increase by 1.02% and 7.3%, which then decrease by 20.2% and 11.0% from N = 10 to N = 20, andthen decrease further by 22.4% and 18.8% from N = 20 to N = 30, respectively. Similar trends are observedfor other algorithms as well. These observations are justified since the robots, while trying to avoid collisions,effectively push each other, therefore, making it harder for all the robots to be able to detect and stay closeto the target when crowding occurs as the number of robots N is further increased to much larger values.",
  "Conclusion": "This paper proposes a novel Autonomous Online Learning (AOL) framework for the decentralized monitoringof an agile target using a swarm of robots undergoing sensor failures, communication link drops, and targetvisual loss and operating without the assistance of a supervisor or a landmark, i.e., without the availabilityof ground truth regarding pose information. In the AOL framework, a decentralized online learning mecha-nism driven by reward-like signals (based on detection confidence) is intertwined with an implicit adaptiveconsensus-based, two-layered, weighted information fusion process, thereby allowing the robotic swarm toexhibit improved robustness and adaptability. Within the AOL framework, three variants are proposed inorder to study the effect of using different loss or reward function designs in the learning phase. A novelperturbation-greedy reward design is introduced in the learning process of two AOL variants, leading toexploration-exploitation in their information fusions weights space. For the three AOL variants, conver-gence analysis of the weights involved in their weighted information fusion process shows that the weightsconverge under reasonable assumptions. Moreover, the AOL algorithms involve analytic expressions makingthem computationally inexpensive and therefore, ideal for use in robotic swarms. Simulation results showthat among the three variants, AOL-1P performs the best in terms of detection and closeness scores, owing toits use of the perturbation-greedy reward for learning the weights that belong to the local estimation phase,and detection confidence-based reward for learning the weights that belong to the social estimation phase.AOL-1P performs 182.2% to 652% and 94.7% to 150.4% better than the baselines in terms of cumulativeaverage detection score per robot and cumulative average closeness score per robot, respectively, as the totalnumber of robots is increased from 5 to 30. The Sim2Real aspects of the top two performing AOL variantsare evaluated using a ROS-Gazebo setup. The current variants of AOL are synchronous; the asynchronousvariant is left as future work.",
  "Broader Impact Statement": "The reward-based autonomous online learning framework proposed in this paper, called the AOL frame-work, is an adaptive information fusion framework that can be applied to various reward-based multi-agentapplications, where the agents are required to be smart about which neighbors should be trusted when fora particular type of information, to increase their rewards as well as help others in the communication net-work. The problem considered in this paper, i.e., target search, tracking, and monitoring, can be used formany applications, like search and rescue, firefighting, disaster relief, convoy protection, wildlife monitoring,surveillance, etc. Moreover, the AOL framework can be useful in the search and detection of societal threatson the web by the use of multiple collaborative bots the weights involved in the information fusion indicatewhich neighboring bot is detecting an increased threat level, thus helping in pinpointing the source of theactivity in the overall network. Further, AOL can also be used in e-commerce applications, such as tradingbots collaborating to increase profits, recommendation bots collaborating to improve customer reviews, etc.",
  "Christine Hockings, Liz Thomas, Jim Ottaway, and Rob Jones. Independent learningwhat we do whenyoure not there. Teaching in Higher Education, 23(2):145161, 2018": "Sandeep Katragadda, Carlo S Regazzoni, and Andrea Cavallaro. Average consensus-based asynchronoustracking. In 2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),pp. 44014405. IEEE, 2017. Navid Ali Khan, NZ Jhanjhi, Sarfraz Nawaz Brohi, Raja Sher Afgun Usmani, and Anand Nayyar. Smarttraffic monitoring system using unmanned aerial vehicles (uavs). Computer Communications, 157:434443,2020.",
  "Mahyar Najibi, Bharat Singh, and Larry S Davis. Autofocus: Efficient multi-scale inference. In Proceedingsof the IEEE/CVF international conference on computer vision, pp. 97459755, 2019": "Ghazaleh Radian, Yahya Kheiri Ghazijahani, Ahmadreza Saadatkhah, and Vahid J Majd. Dynamic ob-stacle avoidance and target tracking for a swarm of robots using distributed kalman filter. In The 3rdInternational Conference on Control, Instrumentation, and Automation, pp. 255259. IEEE, 2013. Jrgen Scherer, Saeed Yahyanejad, Samira Hayat, Evsen Yanmaz, Torsten Andre, Asif Khan, VladimirVukadinovic, Christian Bettstetter, Hermann Hellwagner, and Bernhard Rinner. An autonomous multi-uav system for search and rescue. In Proceedings of the First Workshop on Micro Aerial Vehicle Networks,Systems, and Applications for Civilian Use, pp. 3338, 2015. Madhubhashi Senanayake, Ilankaikone Senthooran, Jan Carlo Barca, Hoam Chung, Joarder Kamruzzaman,and Manzur Murshed. Search and tracking algorithms for swarms of robots: A survey. Robotics andAutonomous Systems, 75:422434, 2016.",
  "How the agents fuse information from their neighbors (i.e., which neighbor to listen to) influencestheir performance or reward accurate or quality information leads to better reward": "The proposed AOL framework can be viewed as a reward-driven, online learning-based, adaptive informationfusion framework that can be used by agents in a multi-agent problem setting () to improve theirreward/performance. Therefore, the AOL framework is useful in problems where the reward/performanceof each agent is dependent on the quality/accuracy of the information that the agent has the dependencecan be direct (the fused information is directly responsible for reward generation) or indirect (the fusedinformation is used to make a decision which is then responsible for reward generation). The problem of cooperative target monitoring considered in this paper involves an indirect dependence of thereward (target detection confidence) on the fused information (target position estimates). The reward-likesignal, called target detection confidence or score (dt,i), is produced by each robots exteroceptive sensor suite.The communicating robots utilize the AOL framework to determine what information should be given moreweight in the weighted information fusion that improves their cumulative target detection score (cumulativereward). Therefore, the AOL framework (along with its theoretical analysis) works for any particular set oftasks, robots, or robot dynamics as long as the above-mentioned general conditions are satisfied. Examplesinclude monitoring and controlling the spread of fire (target) in a forest, wildlife monitoring, search andrescue, etc.",
  "Algorithm 2 AOL-1P (for the ith robot, i [N])": "Choose: Do > 0, > 0, w > 0, Tp 1 (integer-valued), ea1 > 0, ea2 0, pmag > 0Initialize: i(0) = i(0) = i(1) = i(1) = 1, wii(0) = 1, xi0,B = xSi0,BInput: t, dt,i, i(t 1), i(t 1), wii(t 1), xSit,B, xit1,B, dt1,i, i(t 2)Output: i(t), i(t), wii(t), xit,B",
  "Algorithm 3 AOL-2P (for the ith robot, i [N])": "Choose: Do > 0, > 0, w > 0, Tp 1 (integer-valued), ea1 > 0, ea2 0, pmag > 0, ew1 > 0, ew2 0Initialize: i(0) = i(0) = i(1) = i(1) = 1, wii(0) = wii(1) = 1, xi0,B = xSi0,BInput: t, dt,i, i(t 1), i(t 1), wii(t 1), xSit,B, xit1,B, dt1,i, i(t 2), wii(t 2)Output: i(t), i(t), wii(t), xit,B"
}