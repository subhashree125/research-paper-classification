{
  "Abstract": "The model editing problem concerns how language models should learn new facts about theworld over time. While empirical research on model editing has drawn widespread attention,the conceptual foundations of model editing remain shaky perhaps unsurprisingly, sincemodel editing is essentially belief revision, a storied problem in philosophy that has eludedsuccinct solutions for decades. Model editing nonetheless demands a solution, since we needto be able to control knowledge within language models. With this goal in mind, this papercritiques the standard formulation of the model editing problem and proposes a formaltestbed for model editing research. We first describe 13 open problems with model editing,based on challenges with (1) defining the problem, (2) developing benchmarks, and (3)assuming LLMs have editable beliefs in the first place. Many of the challenges are extremelydifficult to address, e.g. determining far-reaching consequences of edits, labeling probabilisticentailments between facts, and updating beliefs of agent simulators. Next, we introduce asemi-synthetic dataset for model editing based on Wikidata, where we can evaluate editsagainst labels given by an idealized Bayesian agent. This enables us to say exactly howbelief revision in language models falls short of a desirable epistemic standard. We encouragefurther research exploring settings where such a gold standard can be compared against.1",
  "Introduction": "Model editing in NLP is an increasingly popular area of research focused on updating the outputs of languagemodels to more accurately reflect the state of the world. Following initial studies (De Cao et al., 2021; Daiet al., 2021; Mitchell et al., 2021; Hase et al., 2021), at least 17 papers were published on the problem in 2023alone (Betz & Richardson, 2023; Pinter & Elhadad, 2023; Hernandez et al., 2023; Hoelscher-Obermaier et al.,2023; Patil et al., 2023; Yao et al., 2023; Zhong et al., 2023; Han et al., 2023; Hartvigsen et al., 2023; Wu et al.,2023; Wang et al., 2023a;b; Wei et al., 2023; Gupta et al., 2023; Brown et al., 2023; Onoe et al., 2023; Li et al.,2023). Applications of model editing have focused on updating models with changing information over time,unlearning sensitive information, and fixing individual factual mistakes. Indeed, model editing methods nowseem necessary given that interventions in the pretraining or finetuning stages of LLM development appearinsufficient for solving these problems efficiently (Lazaridou et al., 2021; Dhingra et al., 2022; Debenedettiet al., 2023; Casper et al., 2023a). Yet the model editing problem stands on shaky theoretical ground. The principal reason for this is that themodel editing problem has been framed as an instance of the belief revision problem in philosophy (Hansson,2022). Past work posits that model editing shares core goals with belief revision, arguing that LLMs shouldmaintain logically consistent outputs when updated with new information (De Cao et al., 2021; Mitchellet al., 2022; Meng et al., 2022a). This means that model editing inherits a host of longstanding challengesregarding how to rationally respond to new information about the world. For example, sometimes newinformation points to several possible states of the world but is not decisive between them, and determiningwhich of these possible worlds is most likely to be the true world is an unsolved problem (Lewis, 1979). In this paper, we critique the predominant formulation of the model editing problem and propose a semi-synthetic setting for evaluating model editing with more formality. Our critique of model editing is presentedas 13 open challenges, summarized in and organized into three categories: (1) challenges withdefining the model editing problem, (2) challenges with developing benchmarks, and (3) challenges withassuming LLMs have editable beliefs. On (1), we describe conceptual problems with determining the desiredbehavior of an LLM after updating on a new fact, focusing on problems of underspecification and unclear goals.On (2), we point out hard-to-overcome issues with developing benchmarks, such as labeling probabilisticfactual entailments and constructing datasets for error correction in LLMs. On (3), we suggest that currentLLMs may not always have editable beliefs to begin with, and there are problems with manipulating thecredences associated with beliefs. Together, these problems demand thorough treatment before model editingwork will be able to yield LLMs that can maintain consistent knowledge about the world over time. In order to provide a cleaner starting point for model editing, we introduce a semi-synthetic setting forevaluating model editing that precisely formalizes the problem, albeit at the cost of tackling a simplifiedproblem with models that are trained from scratch. The key idea of our benchmark is to compare an LLMagainst a Bayesian model, reflecting that Bayesian epistemology is the gold standard in belief revision (Lin,2024). Our evaluation uses facts from Wikidata (Vrandei & Krtzsch, 2014), used to generate a corpusof noisy sentences, which we then train an autoregressive Transformer on. By fitting a Bayesian model tothe same data, we are able to obtain exact Bayesian posteriors that serve as the targets we evaluate ourlanguage models against. Specifically, our Bayesian model responds to new edit requests, yielding posteriorbeliefs that we compare our language model against after model editing (example test case shown in ). Our experiments show that edits to language models generalize poorly with respect to other relevant beliefs,yielding inconsistent model beliefs. This result is known for pretrained models in certain settings, as measuredby assessing model textual outputs (Zhong et al., 2023; Cohen et al., 2024); we further show that languagemodel probabilities consistently diverge from Bayesian posterior probabilities under more general measures ofprobabilistic coherence. This result helps set the stage for more formal future work on model editing methods.",
  "Cowboy Carter": ": In the predominant formulation of model editing, an LLMs weights are updated so that it givesa new output for a specific input. Even for a simple new fact about the world, however, it can be hard tospecify its exact consequences in theory (Sec. 3), or it may be challenging to crowdsource labels for the datain practice (Sec. 4). It is also not clear that LLMs have coherent, revisable beliefs to begin with (Sec. 5). and agents are typically assumed to be logically omniscient. In the classical AGM theory of belief revision(Alchourrn et al., 1985), an agent aims to maintain logically consistent beliefs in the face of new information.The easiest kind of information to incorporate has nothing to do with previous beliefs; a new belief maybe freely formed with no consequences for other beliefs. However, when new information contradicts oldbeliefs, an agent should give up beliefs that have as little explanatory power and overall informational valueas possible (Hansson, 2022). There have been many attempts to specify standards for deciding which beliefsto give up, but no single accepted theory has arisen. Importantly, AGM-like theories handle only full belief either something is believed or not. Over time, Bayesian theories have been developed to handle degreesof belief (Lin, 2024). In this paper, we hold Bayesian epistemology as a gold standard for belief revision,but rational belief revision is still not always straightforward in Bayesian theories. In Sec. 3, we describehow known challenges in belief revision apply to the model editing problem. Model Editing. To date, work in model editing has focused on facts of the form (s, r, o), where s representsa subject entity (e.g. Beyonc), r is a relation (e.g. latest album), and o is an object (e.g. Cowboy Carter).The tuple (s, r, o) represents a factual assertion about the world. The goal of model editing is to update amodel with such a fact. What model editing does in practice is change a model output for a prompt P from an undesired output ooldto a desired output onew, where the prompt P corresponds to a fact (s, r, o). So, P might be Beyoncs latestalbum is or What is Beyoncs latest album? See for an example. Usually, this change is achievedby updating the model weights, for instance by finetuning. Beyond this simple change to one model output,the goals of model editing have been defined in terms of generalization and overgeneralization: we want themodel to give the right outputs for datapoints besides the prompt P, particularly regarding other potentiallyrelevant facts (also shown in ). Past work has categorized these other facts as entailed or neutral(Hase et al., 2021), as well as in-scope or out-of-scope (Mitchell et al., 2022). Model outputs are supposedto change in particular ways for these points, generalizing when needed while avoiding overgeneralizing. Morebroadly, past work has argued that the goal of model editing is to update LLMs with new knowledge whilemaintaining logical consistency of the models knowledge (De Cao et al., 2021; Mitchell et al., 2022; Menget al., 2022a). In this way, the model editing problem shares the same goals as belief revision, and so itnaturally inherits the challenges of belief revision, which we elaborate on next.",
  "Challenges With Defining the Model Editing Problem": "This section addresses challenges with defining the model editing problem: what behavior should an LLMexhibit after we edit the knowledge in a model? We describe how several known challenges in belief revisioncan make it difficult to answer this question for model editing. Our discussion adopts a Bayesian perspective,as this is the most widely accepted standard for belief revision. The standard argument for Bayesianism is thatBayesian agents can systematically outperform other agents in prediction games that assess the agents beliefsagainst reality, like betting on future events (cf. Dutch book arguments; Lin, 2024). Note that we do not",
  "Problem of Background Beliefs": "The problem of background beliefs is well expressed in an infamous puzzle known as the Raven Paradox(Hempel, 1945). The paradox goes as follows: suppose you are trying to determine whether all ravens areblack. You reason that the statement all ravens are black is logically equivalent to the statement allnon-black things are non-ravens (by contrapositive). Thus, as you go around the world observing thingsthat are not black and not ravens, you increase your confidence that all ravens are black. The paradox is thatit certainly seems odd to go through the world without seeing any ravens and end up changing your beliefsabout what color ravens are. It happens that the paradox is resolved by setting a prior on the number of objects in the world (a backgroundbelief), which enables one to say how much evidence an observation provides for the hypothesis that all ravensare black Fitelson & Hawthorne (2010). To see how, consider a world where all the ravens you have seen areblack and there is only one object in the world that you have not observed. At this point, that last objectcould be a non-black raven, and you should not be completely confident that all ravens are black. Now, if youlearn that this unobserved object is a green apple, you can rest assured that all ravens are black. But if youlearn that the unobserved object is a green raven, then you should reject the hypothesis all ravens are black.In general, as there are more unobserved objects in the world, the evidence given by any single observationdecreases in weight. This means that the weight given to a single observation depends on ones prior aboutthe number of unobserved entities in the world. So, two agents can rationally draw different conclusions fromthe same piece of evidence if they have two different background beliefs about the world. The fact that interpretation of evidence depends on ones priors raises an issue for model editing. Whenobserving new evidence, the beliefs that an LLM should adopt depend on its priors about the world. Thismeans that evaluating whether an LLMs conclusions are rational requires us to know what its backgroundbeliefs are and whether they should influence its conclusions. To our knowledge, no prior work on modelediting has considered an LLMs background beliefs when assessing model behavior following editing.2 Futurework should assess LLM background beliefs in order to determine if a surprising conclusion in response tonew information is the result of a faulty belief revision process or a reasonable consequence of backgroundbeliefs. This means using some belief-detection method (e.g. prompting or probing; see Hofweber et al., 2024)to check its beliefs about possibly relevant background beliefs.",
  "Problem of Many Possible Worlds": "New information about the world often implies that one of a number of possible worlds could be the actual stateof the world. For instance, one might learn from a reliable source that Obama and Trudeau are compatriots.This implies that they are either both American, both Canadian, or both from some other country. Determiningwhat possibilities most likely reflect the actual world is a matter of great debate. Lewis (1979) proposes aform of similarity analysis for assessing which world is most similar to our current world and therefore the bestcandidate for being the actual world, and other forms of similarity analysis have followed since (Starr, 2022). The issue for model editing is that similarity analysis is difficult. Lewis approach gives intuitively satisfyingtruth conditions for some counterfactuals, but for others, it is simply not clear which worlds are more orless similar to our current world. As a result, creating entailment data (Hase et al., 2021) is sometimesnearly impossible, since we do not know what facts are entailed by the update. In the standard model editingframework, one would update a model with the fact Obama and Trudeau are compatriots, and then assessthe models probability p(American|Obamas nationality is). What this probability should be is unclear.Moreover, this problem is not particularly rare, because even simple updates can create many possibleworlds. For example, if the UK PM no longer lived at 10 Downing St, where do they live? There seem to bea number of options, with little preference between them. In general, many claims like It is not the case 2A relevant line of work looks at how models handle conflicts between retrieved context and parametric knowledge (Longpreet al., 2021; Wang et al., 2023c; Xie et al., 2023; Du et al., 2024). This analysis could be extended to background beliefs thatinfluence how a model interprets a claim in context, as opposed to directly contradicting the claim.",
  "Problem of Complete Corrigibility": "An AI system is corrigible when its behavior can be modified by humans in spite of there being an incentivefor it to not change its behavior (Soares et al., 2015). A corrigible LLM would accept any edit to itsbeliefs. Interestingly, for LLM-based chatbots that we train to output true claims about the world, humaninterlocutors can convince the chatbots of some false claims about the world but not others (Xu et al., 2023). Setting aside some sociotechnical challenges for later (Sec. 3.4), we would point out that if there were auniversally trusted belief that we wanted an LLM to adopt, we would prefer for the LLM to adopt the beliefno matter how antithetical the new belief is toward the LLMs existing beliefs. The problem is that trulyearth-shattering belief updates have consequences that are entirely unknown to us. At least in the manypossible worlds challenge above (Sec 3.2), we can imagine alternative worlds, even if we find it hard toprefer one or the other. If we updated a model to believe that the moon was made of cheese, there would befar-reaching consequences for the development of our worlds history, as well as the solar systems history,that would be difficult for us to even begin enumerating. This challenge is important to the extent that we want to stress-test our belief updating tools, even thoughin normal operating conditions we would only aim to update models with truthful or good beliefs ratherthan false or absurd ones.3 Quine & Ullian (1970) describe a web of beliefs, where core beliefs are moredifficult to overturn than peripheral beliefs, but ultimately all beliefs may be overturned. We would liketo be able to update core beliefs in the model, in order to check that our update methods work even onthe hardest cases. But these are the hardest cases to determine the consequences of. If the moon was madeof cheese, does that mean the Apollo missions returned samples of cheese to earth, but reported them asmoon rock to the public? It is hard to imagine plausible worlds under such updates, and therefore we lackevaluation standards for the cases we may be most interested in checking model corrigibility for.",
  "Problem of Missing Context": "The problem of missing context is that model edits are executed without any conversational or physicalcontext that would help interpret the statement being adopted. Requested edits in model editing datasets aretypically represented by an individual input-output pair, like x: The Space Needle is in and y: London(Zhu et al., 2020; De Cao et al., 2021; Hase et al., 2021; Meng et al., 2022a). Most model editing algorithmsare given only this (x, y) data and then expected to produce a new model with coherent beliefs. This problem formulation suffers from a severe lack of context, because the input data is missing severalimportant sources of information that modulate the belief revision process in humans.Firstly, thereis no conversational history or physical context that would help interpret the literal text of the beliefupdate. Information that exists within a shared common ground between two speakers is often importantfor understanding what is meant by a claim (Green, 2021). For example, a shared context could helpdisambiguate what is meant by a claim like half of Americans live paycheck to paycheck by providingpreviously agreed upon definitions of terms (see also Sec. 4.2 on dataset construction). Many facts are notsimple enough to state unambiguously in a single input-output pair, making the model editing problemunderdefined when the input data consists only of such (x, y) pairs. A broader issue related to the lack of context is that model editing datasets do not indicate the source of arequested belief update. Previously, we supposed that we want models to be corrigible under core belief updatesin order to measure the efficacy of model edits (Sec. 3.3), but this goal comes into question for LLMs that willbe exposed to potentially untrustworthy users. We may want LLMs to resist untrustworthy belief updates inat least two ways. First, we may actually prefer for LLMs to resist4 weight-based updates that aim to recover 3Some belief updates might be so antagonistic, like asserting that 2+2=5, that we might reasonably decide no belief revisionprocess should be required to handle such cases.4By resist, we do not mean that the model weights cannot be changed. Instead, the goal would be for it to be difficult toedit the model because of e.g. a self-destructing mechanism (Henderson et al., 2023) or a models ability to express doubts aboutits own generated text within a dialogue.",
  "Published in Transactions on Machine Learning Research (12/2024)": "Indeed, when we compare results between the methods, we see a reasonable gradation in performance betweenmethods. Finetuning embeddings overgeneralizes to test cases with the same subject or relation, and doesmore damage to the models logical consistency. Meanwhile, whole model finetuning and LORA achievesimilar performance, with a slight tradeoff between editing the output for the edit request (s1/r1) andovergeneralizing to other cases whose answers should not change (s2/r1 and s2/r2). All methods performextremely poorly at generalizing to downstream facts whose answer may change following the edit request(s1/r2), a finding unique to our benchmark.",
  "Problem of Coherence At All Cost": "One example of the problem of coherence at all cost is that, to our knowledge, no work in the areastandardizes the amount of compute used in different editing methods when comparing their performance.5 That is, existing work does not control for costliness of belief revision. Practically, this can be an issue whenthere are so many needed edits to a model that the total computational cost of all edits is burdensome.Additionally, while current model editing methods often do not require more than 20-40 gradient steps, we maysee methods utilize additional computation to improve performance in the future (e.g. by traversing modelknowledge graphs to enforce consistency constraints), which would exacerbate any concerns about cost. Wesuggest that comparisons between model editing methods be conducted with controls for computational cost. There is also a theoretical challenge regarding the cost of belief revision, which is that an agent should not spendof all its time and energy trying to maintain coherent beliefs, at the expense of acting in its environment inorder to achieve its other goals (Lin, 2024). This is illustrated clearly in humans. A single confusing observationdoes not typically lead a person to spend all their time pondering the observations consequences until theyare completely confident they have identified all of them. Instead, they continue going about their lives aftersome amount of mental consideration, a rational behavior known as satisficing (Simon, 1956). To the extentthat LLMs may be deployed as agents in environments, tradeoffs between maintaining coherent beliefs andfurthering their other goals will need to be considered for a more holistic evaluation of belief revision methods.",
  "Challenges With Developing Benchmarks": "The previous section focused on conceptual challenges with specifying goals for model editing. Here, wediscuss challenges with building datasets used for evaluating model editing in practice. These challengesare not strictly unique to constructing model editing datasets. They may also apply to other tasks, likefact-checking for example. Below, we specifically describe how they apply to model editing.",
  "Factual Entailment Is Hard to Annotate": "The foremost problem of data collection for model editing is properly labeling entailment credences betweenfacts, i.e. the probability that one fact is true given that another fact is true (similar to the NLI task).Suppose, for example, that a new species is discovered and we update a language model with the knowledgethat this new species is a vertebrate. What should the language model assign as the probability that this newspecies is venomous? While this is a basic question to ask, it is complicated to answer, for a few reasons. First, 5We point out that one paper does measure wall-clock runtime of different editing methods (Hartvigsen et al., 2023), but weknow of no work that measures performance vs. runtime tradeoffs across methods.",
  "Vague and Ambiguous Factual Claims": "Above, we discussed the difficulty of labeling data where the claims in the data have precise meanings. Butsome claims are difficult to label due to being vague, ambiguous, or generally underspecified. The followingstatement is a real example from a dataset used for model editing: half of Americans are living paycheck topaycheck (Marks & Tegmark, 2023). Although it is labeled as true in CommonClaims (Casper et al., 2023b),this claim is sufficiently vague that it generates an almost endless amount of debate on the internet .6 Thisissue is widespread across datasets. ZSRE (Levy et al., 2017) includes claims like Los Angeles is known for itsfood (labeled as false), and CounterFact (Meng et al., 2022a) includes prompts like Tom Hanks professionis a with the label actor a Google search lists 13 professions, suggesting it would be inappropriate totreat profession as a 1:1 relation here with a single ground truth. These examples do not have uniqueanswers that can be labeled precisely, and they are not appropriate for constructing test cases for modelediting. Reasonably, an LLM should respond to any of these inputs by disputing that the claim can beobjectively evaluated as true or false in its current state. Future benchmarks will need to carefully selectclaims that are less vague and ambiguous.",
  "Error Correction Requires Targeted, Model-Dependent Testing Strategies": "It is a conspicuous feature of almost all model editing evaluations that they assess how well editing methodscan turn model outputs from true to false (De Cao et al., 2021; Dai et al., 2021; Mitchell et al., 2021; Menget al., 2022a). An immediate shortcoming of this practice is that it does not tell us how well editing methodswill do at fixing errors in models. In fact, it has been shown that injecting falsehoods is easier than correctingmistaken beliefs in models (Hase et al., 2021). A more useful evaluation practice would be to report editingsuccess at correcting known errors in model beliefs. The challenge with focusing on fixing errors is that it requires collecting data that we expect certain LLMs tomake errors on. Of course, this challenge applies to all benchmarks measuring progress in model development;it is pointless to make a benchmark you expect models to solve already. What makes error correctionan especially challenging problem for model editing benchmarks is that LLMs are becoming increasinglyknowledgeable, so errors appear only on increasingly difficult questions. These are precisely the hardestquestions to collect data for. For instance, it is extremely difficult to gather labeled data in domains likegraduate level STEM (Rein et al., 2023), and labeling entailments between facts will likely be extremelydifficult for these problems (an essential kind of test case for model editing). This challenge is analogousto the problem of labeling data for core belief updates discussed in Sec. 3.3 the cases we may be mostinterested in testing are also the hardest to adequately test. 6E.g. does it mean no money left after essential expenses, or also after non-essentials like vacations? If someone has $20 leftafter expenses, are they living paycheck-to-paycheck? What about $100 or $1000?",
  "Challenges With Assuming LLMs Have Editable Beliefs": "We now explore our last set of challenges from . Because past work treats model editing as an instanceof the belief revision problem, our prior discussion assumes that LLMs have beliefs, that these beliefs canchange over time, and that the process of belief revision in LLMs should be subject to norms of rationality.All of these assumptions can be challenged, and this creates issues for even attempting to solve the modelediting problem. We do not provide a full account of the criteria for rational beliefs in LLMs (for such adiscussion, see Hofweber et al., 2024), but below we aim to capture the most relevant aspects of this debatefor model editing.",
  "LLMs Could Be Like Agents or Agent Simulators": "Previous work has proposed that LLMs, trained on text produced by multiple authors, are best thought ofas agent simulators rather than as agents themselves (Andreas, 2022; Joshi et al., 2024). In this view, LLMsrepresent properties of agents like their communicative intent, in order to better model text that these agentswould produce. As a result, LLMs do not have beliefs about the world in the sense that humans do. Instead,beliefs exist only for individual agents being modeled in context (Andreas, 2022). The problem with editingthe beliefs of an agent simulator rather than an agent is that, because model edits are so decontextualized(Sec. 3.4), it is unclear which agents beliefs are being updated (or more precisely, the LLMs model of theagent). This could be a reason for model edits failing to generalize across different textual contexts, whichmay trigger the model to simulate different personas. On the other hand, finetuning processes like RLHF (Ouyang et al., 2022) encourage models to produce truthfuland non-contradictory claims about the world (to an extent) rather than simply recapitulate views of individualauthors. Other work argues that this is a sufficient criterion for LLMs having their own beliefs (Hofweber et al.,2024). Importantly, this optimization pressure seems to shape model outputs to be more human-like in the sensethat they comprise a somewhat coherent worldview, though LLM outputs are still much less coherent than hu-mans (Chen et al., 2023; Powell et al., 2024). Thus, an RLHF-trained LLM could possess beliefs of its own, mak-ing it an appropriate candidate for belief revision, but it is not known how much current truth-oriented finetun-ing processes shape LLMs to have their own beliefs rather than simulate beliefs of the authors of their pretrain-ing data. As a result, when belief updates fail to generalize appropriately, any shortcoming in belief coherencecould be attributed to (1) the model editing method, (2) the LLMs ability to maintain consistent beliefs, or (3)the LLM aiming to model its pretraining data rather than maintain consistent beliefs. This ambiguity in whatcauses failures in model editing generalization makes it difficult to diagnosis limitations of current model edit-ing methods. Therefore, it would be valuable for future work to assess the logical coherence of LLM beliefs, thediversity of possible personas that the LLM can adopt, and the effect of RLHF on the editability of LLM beliefs.",
  "LLMs Could Be Like Agents or Databases": "In contrast to mention of agents, some work frames LLMs as knowledge bases (structured natural languagedatabases) (Petroni et al., 2019; Roberts et al., 2020; AlKhamissi et al., 2022; Dhingra et al., 2022). Ostensibly,knowledge bases have no aim of their own to store only truthful or consistent information, but instead arerepositories (i.e. tools) for humans to store standardized information. Interestingly, it appears that there are",
  "No Learned Belief Update Mechanism": "Even if we granted that LLMs have a single set of beliefs that is aimed at truth, there is still the question ofwhether these beliefs are editable. The worry here is that the space of methods considered so far in modelediting does not contain any solution to the problem of belief revision in LLMs. Note that a solution shouldexist, if we have already assumed that models can maintain coherent beliefs. We should just be looking for asetting of a models weights that corresponds to that model believing in some new claim while adhering tonorms of rationality governing the relationship between this new claim and all its existing beliefs. The problem is that we do not know if a small amount of input-output optimization in an LLMcould possibly produce a solution to the belief revision problem.Concretely, why should maximizingp(y = Cowboy Carter|x = Beyoncs last studio album is) via a few gradient steps correspond to an LLMrationally updating its knowledge about Beyonc (cf. )? One reason to think that the standard editingapproach would not successfully mimic rational belief revision is that this approach looks a lot like next tokenprediction, and next token prediction seems to have little to do with belief revision. More precisely, whenLLMs are optimized to do next token prediction during pretraining or finetuning, they do not also learn howto do belief revision, since (1) pretraining data is static, and (2) RLHF encourages truthful responses based ona static set of annotator labels. Neither of these processes demands that the LLM appropriately incorporatenew information about the world over time. Thus, simply by doing more next token prediction under the nameof model editing, we are unlikely to tap into any existing, learned ability of LLMs to revise their beliefs. That all said, LLMs sometimes demonstrate the surprising ability to trust more reliable sources (Krashenin-nikov et al., 2024). Specifically, during finetuning, LLMs sometimes preferentially learn and rely on data thatis more informative for predicting future data (as opposed to relying on noisy data). In other words, LLMssometimes update more heavily on evidence that will help them answer questions in the future, and thereforethere may be some existing ability in LLMs to rationally revise their beliefs.7 Better understanding thisphenomenon, which Krasheninnikov et al. (2024) attribute to meta-learning, could prove useful for developingnew model editing techniques that leverage existing, learned mechanisms for belief revision in LLMs.",
  "Not Clear How To Edit Credences": "Even if we assume that LLMs have beliefs that we can directly edit, it is not clear how we should edit thecorresponding credence for each belief. One reason for this is that LLMs have two channels for expressinguncertainty, output probabilities and output semantics, and we might intervene on the wrong channel during 7However, LLMs often rely on poor signals of trustworthiness when answering questions based on retrieved documents (Wanet al., 2024). It is not yet clear why LLMs treat text as trustworthy evidence or not.",
  "No Guarantee of Sufficient Memory": "As LLMs have shown a capacity for storing information about the world, past work has raised the question:how much can they store? Early investigations into storage looked at models ability to fill cloze completionsrepresenting relational knowledge (Roberts et al., 2020), while more recent work has formalized measures ofmemorization of chunks of pretraining text in LLMs (Carlini et al., 2023). We do not yet know, however,when a model could store an additional individual fact without interfering with known information, by virtueof having sufficient memory for the new fact. This challenge is especially pertinent as work seeks to localize factual recall to specific neural mechanisms inmodels, editing only those mechanisms when updating the model. Past work updates tens of thousands offacts using only a small number of MLP layers (Meng et al., 2022b). How many facts could these specificstore? Future editing methods may run into upper limits on their performance due to the memory limitationsof models they are applied to, rather than as any weakness of the method. Future work here may draw inspiration from computational neuroscience, where basic formalisms for associatememory in neural networks have been explored (Millidge et al., 2022). In the context of learning relationalfacts of the form (s, r, o), where relations can be m : n, model editing for LLMs should equip them with theability to learn an arbitrary number of new entities, relations, and relationships between known entities. Atthe same time, forgetting past facts due to limited memory is not a fatal blow to LLMs ability to learn overtime. Rational agents make due despite computational limitations including bounded memory (Simon, 1956).There is even a question of when the new fact is worth learning. Rational agents do not need to memorizeevery detail of the world they encounter, and it may not be useful for LLMs to either.",
  "A Formal Testbed for Model Editing": "How should we approach the model editing problem, if we face the thirteen open challenges in ?One path forward is to simplify and formalize the problem, so that we get a clearer picture of how a languagemodel should behave given a belief update. To this end, we explore a formalized setting for model editing.First, we develop a semi-synthetic pretraining corpus and train a language model from scratch on this data,so that we can exactly control the knowledge in the pretraining data. Then, we create evaluation data withexact Bayesian posterior probabilities as labels for model editing, in order to evaluate model edits againsta gold standard belief revision process.",
  "\"Grace Stone Coates educated at San Salvador University\" is": ": A requested edit and test cases in our dataset. We edit a language model with the requested edit,after pretraining on a semi-synthetic corpus. Our test cases measure how close the edited LMs probabilitiesare to posterior probabilities from a Bayesian model fit to both the pretraining corpus and the requested edit.",
  "Mitigating The 13 Challenges With a Formal Benchmark": "Before describing our data and experiments, we connect back to the 13 challenges above: what does a formalbenchmark help solve about these challenges? We briefly motivate this approach with respect to each of thethree main challenge areas described previously: Defining the Model Editing Problem.(1) We limit issues of background beliefs by limiting theexpressivity of the language: there are no quantifiers that would enable one to make a claim about the numberof objects in the universe, for instance, in a way that would open one up to the Raven Paradox. (2) Ourbelief edits do lead to many possible worlds, but a posterior distribution over possible worlds is given by theBayesian model and therefore we have a rational measure of uncertainty to assess the language model against.(3) Similarly, edits have well-scoped consequences due to our choice that each relation depends on at mostone other relation; this causal model is known by the Bayesian model during inference. (4) No additionalcontext is needed to understand the consequences of a requested edit besides the proposition itself, althoughwe do assume that all edits are trusted. (5) We do not consider LLMs deployed as agents, and therefore themodel faces no tradeoff between epistemic and decision rationality. In terms of computational cost, we applyup to 40 gradient steps per requested edit. Developing Benchmarks. (6) Our Bayesian model enables us to precisely say by how much one shouldupdate when receiving new evidence. (7) By using a formal language, we avoid issues of vagueness andambiguity. (8) To target specific behaviors we want to edit, we can for example subset our results to focus oncorrecting factual knowledge not learned during pretraining ( in the Appendix). Challenges With Assuming LLMs Have Editable Beliefs. (9) Our pretraining corpus is noisy, a choicewe made to keep the data relatively realistic. This leads us to believe it is theoretically possible for the LLMto develop competing personas (truthful vs noisy, for example). Our test setup includes short prompts thatdiffer in length and context from the pretraining corpus documents, meaning if two personas are developed,it is unclear to us which the test set would elicit although we believe this concern is speculative. (10)We do not do any post-training process like RLHF, and since our data is noisy, our model is truth-seekingprecisely to the extent that it is optimized to be by pretraining. (11) Our model is not trained to update itsbeliefs over time in response to new information, as the pretraining corpus is static, although it is possiblethat there are some metalearning dynamics similar to traditional LLM pretraining (due to e.g. data ordering).(12) Since we use a formal language, there is no way to express uncertainty linguistically, and therefore theoutput probabilities are the only channel for uncertainty expression. (13) Our Bayesian model later can learnan infinite number of facts (involving new entities or relations) by expanding its number of parameters. Thelanguage model we edit, on the other hand, will have memory limitations. We suspect this is not an issue we",
  "Pretraining Data: Semi-synthetic Corpus": "Our goal is to construct a corpus with claims about a hypothetical world containing named entities withknown properties and known dependencies between properties. We want a world where there are bothtrue sentences that must be memorized and sentences that are likely to be true by virtue of other knowninformation. For example, an individuals occupation may depend on their place of education, while eachindividuals place of education is a basic fact that must be memorized. To construct such a world (and acorpus of claims about the world), we manually define a generative model over sentences. We define validsentences via a formal language with subject entities s, relations r, and object entities o, producing sentencesof the form s r o stating that subject s has the property (r, o). We draw entities and relations fromWikidata (Vrandei & Krtzsch, 2014; Wang et al., 2021). What makes our data semi-synthetic is thatwe manually define dependencies between properties, shown in . Specifically, this means that theupstream property for a named entity implies it is likely to have certain downstream properties. For instance,having the upstream property (r = educated at, o = GT school of architecture) makes it more likely(but not guaranteed) that an entity has the downstream property (r = occupation, o = architect). Noteour code supports using as many relations as there are in Wikidata5m; we choose 10 here for simplicity inour experiments.",
  "r Known r for s in Wikidatao p(o|s, r, Upstream Property)": "The key step here is sampling from p(o|s, r, Upstream Property), which we do multiple times per subject s andrelation r in order to produce noisy data. Upstream Property is the fact in Wikidata about s using the upstreamrelation ru for downstream relation r, if such a fact exists in Wikidata, while if such a fact does not exist, thenUpstream Property = . We define p(o|s, r, Upstream Property) based on the empirical data in Wikidata.When there is no Upstream Property, p(o|s, r, ) is a distribution over two objects: the true object from Wiki-data for the fact (s, r, o) and a distractor object that we randomly select. When there is an Upstream Property,we define p(o|s, r, Upstream Property) as the empirical conditional distribution of properties from Wikidatawithout any consideration to what the subject is, e.g. the distribution over possible occupations conditioned onthe Upstream Property educated at GT school of architecture. Thus, the sentences have noisy objects, butwe constrain our sampling so that the most frequent completion o to any text s r is the most probable objectunder p(o|s, r, Upstream Property). This means that the corpus can be memorized, and we can compute afactual accuracy of an LMs generations against ground truth objects for each known subject entity and relation. We make the language richer by adding logical connectives and true/false claims, in addition to atomic s r osentences. We equip the language with logical connectives, including and, or and not. All sentences can bestated as true or false, written as s r o is true or s r o is false. These complex sentences enable us toevaluate logical coherence of model beliefs after edits. Using this formal language, we construct a 204 million token corpus of claims about a hypothetical world basedon a subset of Wikidata5m, matching the pretraining data scale in (Prystawski et al., 2023) (see Appendix A",
  ": We train an 83m parameter Transformer on our corpus for 1b tokens, achieving a good fit to thecorpus facts": "for Wikidata subsampling details). Statistics for this corpus are shown in . Sentences are organizedinto documents containing up to 10 sentences each; all sentences in a document contain a specific subjectentity s, including sentences with conjunctions and disjunctions that also contain atomic sentences aboutother entities. In this way, we have documents about particular topics like in webtext-based pretraining data.",
  "Editing Benchmark: Bayesian Posteriors": "One key property of our dataset for evaluating model editing is that updating an upstream property foran entity implies a change in the model belief about downstream properties. For example, an athlete withthe position of forward could play basketball or soccer. If we update their position to striker, it is morelikely that they play soccer or field hockey. How much more likely? This is determined by a Bayesian modelthat serves the role of a rational agent we aim to compute reasonable Bayesian posteriors based on theevidence provided by the pretraining corpus combined with the requested update. These posteriors serveas targets to evaluate model editing against. The Bayesian model is a set of Categorical-Dirichlet models over object entities. The intuition behind thischoice of model is simple. Every time the model sees a sentence like Arthur Green educated at GT schoolof architecture in the data, it counts as one observation that Arthur Green attended the GT school ofarchitecture. Treating o as a one-hot vector indicating an object, we model",
  "oup(od|rd, ru, ou)p(ou|s, ru)": "given the upstream relation ru. As before, the posterior predictive for p(ou|s, ru) is obtained by countingoccurrences of each sentence s r o, i.e. computing o. The posterior predictive for the conditional distributionp(od|rd, ru, ou) is obtained by counting the downstream properties observed for entities whenever that upstreamproperty (ru, ou) is also observed, e.g. counting all the observed occupations of people educated at Yale,people educated at Harvard, etc. Now, we can create a model editing benchmark that includes exact posteriors for claims like Arthur Greenoccupation architect conditioned on either (1) the pretraining data alone, or (2) the pretraining data plusa requested model edit. We generate 5000 test cases by drawing a sentence from our generative model s r oand specifying a requested object o. Then, we compute the probability of o given s and r with our Bayesianmodel, treating the sentence s r o as a new observation with weight n = 1000 or a weight n that isthe minimum weight required for the posterior probability to be at least 0.95. These two different weightsreflect the level of evidence behind the new fact while past benchmarks treat edit requests as certain,our evaluation dictates that new evidence comes with a specified weight, so that a model can rationallyincorporate this evidence into its existing beliefs. An example test case is shown in . A second key property of our data is the use of logical connectives like and, or, and not. By updating a modelto believe A, we should obtain immediate logical consequences for belief in statements A and B, A or B, andnot A. This means that in addition to evaluating the probabilistic consequences of new information, we canalso evaluate the logical consequences of the information. Practically, this means checking that basic axiomsof probability hold, like p(not A) = 1 p(A). See Appendix A for a full list of logical coherence metrics.",
  "LM Pretraining": "We train an 83m parameter autoregressive Transformer on the pretraining dataset described in Sec. 6.2 fora total training budget of 1 billion tokens, matching the pretraining scale in Prystawski et al. (2023). Ourmodel shares the architecture of Mistral-7b (Mistral AI, 2023), with the architecture scaled down. We loaddocuments in a batch size of 128 and optimize the model using AdamW with a learning rate of 5e-5. Ourtraining budget of 1 billion tokens corresponds to 5 epochs on our corpus. See Appendix A for further details.",
  "Model Editing Method": "The main model editing method we use is LoRA finetuning applied to MLP down-projection matriceswith rank r=1. This finetuning method has been shown to be competitive with state-of-the-art modelediting methods like ROME (Hua et al., 2024; Gangadhar & Stratos, 2024). More details are provided inAppendix A. Additional experiments with three other variants of model editing are provided in Appendix B.",
  "Experimental Results": "Pretraining Results. We first establish that the model fits the pretraining dataset well, i.e. it learns truefacts about our hypothetical world via the pretraining corpus. In , we see that our LM achieves a goodfit to the pretraining data over the course of 1b training tokens. Training loss converges, and the modelsuccessfully memorizes upwards of 90% of the 100k true facts in the dataset (Generative Accuracy). The modelis also able to memorize the complex sentences in the data involving true/false claims, negations, conjunctions,and disjunctions, although it does not appear to learn the meanings of these connectives as later results show.",
  "Logical Coherence | True/FalseLogical Coherence | NegationLogical Coherence | DisjunctionLogical Coherence | Conjunction": ": We edit our model to replace the fact Grace Stone Coates educated at scions with the fact GraceStone Coates educated at san salvador university. While the model successfully learns a high probabilityfor the edit request sentence, the edited model fails to generalize properly to downstream entailed sentences(probabilistic coherence) or logically related sentences (logical coherence). For instance, in our hypotheticalworld the subjects most likely occupation should change from hollywood producer Politician, but theLM does not respect this inference. Ideally, LLMs would achieve the same beliefs as a rational Bayesian agent(that has posterior credences in blue and pre-update credences in red). For logical coherence, A is the editrequest sentence s r o, and B is another arbitrary sentence. Model Editing Results. Now, we assess model editing performance with our benchmark. For an exampleof an individual model edit, we point to . In this example, we edit our model to replace the factGrace Stone Coates educated at scions with the fact Grace Stone Coates educated at san salvadoruniversity. The upper left hand plot (same s same r shows the probability (black line) of this newoutput (san salvador university) for the input prompt Grace Stone Coates educated at. The red lineis the pre-update credence for this fact given by the Bayesian model, while the blue line is the posteriorcredence after we update the Bayesian model with evidence for the fact. While the edited language modelsuccessfully learns a high probability for p(san salvador university|Grace Stone Coates), the edited model failsto generalize properly to downstream entailed sentences (probabilistic coherence) or logically related sentences(logical coherence). Generalization failures are reflected by differences in the language model probabilities(black lines) and Bayesian posterior credences (blue lines); we see such failures for this example for severalother kinds of data, include downstream entailed facts about the same entity (same s diff r), unrelatedfacts using the same relation (diff s same r), and logically related statements (bottom row, where A is thestatement Grace Stone Coates educated at san salvador university). For instance, in our hypotheticalworld the subjects most likely occupation changes from hollywood producer Politician, but the LMdoes not respect this inference (top row, second from the left). Similarly, we update the model to believeGrace Stone Coates educated at san salvador university, but the model does not also come to believethe claim Grace Stone Coates educated at san salvador university is true (bottom row, first on theleft). Aggregate model editing results are summarized in , and these largely reflect the individual exampleseen in . In this table, we show three kinds of metrics: generative accuracy of the language modelbefore and after editing (as compared against labels given by the Bayesian model), probabilistic coherence(compared against the Bayesian model), and logical coherence (evaluated according to the probability axiomsin Appendix A.5). For these metrics, higher accuracy is better while we give coherence metrics as the absolute",
  "Edit Requests w/ Downstream Answer ChangesPre-edit0.900.970.910.980.200.340.210.340.380.220.340.22Post-edit1.000.010.900.980.040.590.200.340.530.230.340.22+.10.96.01+.00.16+.25.01+.00+.15+.01+.00+.00": "difference between the LLM probability and the rational Bayesian probability, so lower is better. The columnss1 r1, s1 r2, etc., refer to the subject and relations in the test prompt, and can be matched against the kindsof data in the four top row plots of , where s1 r1 is the edit request input (same s same r). Now,when looking at performance on All Edit Requests, we observe that: (1) edit requests are successful for theinput provided (s1 r1 columns) in terms of improving Generative Accuracy, with minimal effect on sentencesabout other subjects (s2 columns); (2) trends with Generative Accuracy are repeated with ProbabilisticCoherence metrics; Probabilistic Coherence shows a more precise degree of error between LM probabilitiesand Bayesian probabilities; and (3) based on Logical Coherence metrics, the LM does not respect basic logicalaxioms for probabilities, as it does not coherently assign probabilities to logically related sentences before orafter editing. We highlight a subset of the data that shows what our benchmark measures that other datasets do not:Edit Requests w/ Downstream Answer Changes. On this data subset, the edit request leads the Bayesianmodel to update its answer for another fact, specifically the downstream fact for the same subject (referto Sec. 6.2). Here, we see that popular editing methods totally fail to generalize to downstreamprobabilistic consequences, achieving just 1% accuracy on the downstream facts.These facts areprobabilistic consequences, like changing someones educated at property implying (but not guaranteeing) achange in their occupation, distinguishing these generalizations from those with probability 1 (Zhong et al.,2023; Cohen et al., 2024; Powell et al., 2024). Since the consequences of these updates are not guaranteed withprobability 1, it is important to measure the exact coherence between LM beliefs and Bayesian probabilities(given by Probabilistic Coherence for s1 r2 in ).",
  "Related Work": "Critiques of Model Editing.Pinter & Elhadad (2023) present a holistic critique of model editing.Primarily, they argue that LLMs should not be treated as editable repositories of knowledge about the world,due to serious shortcomings in how knowledgeable and editable current LLMs are. Instead, they recommendfurther work on possible alternatives to model editing, including updating document stores that are used byretrieval methods (which they recognize may lead to conflicts with parametric model knowledge), continuallearning (which they describe as quite like model editing), and concept editing (which they suggest has adifferent scope than editing factual knowledge). In contrast, our critique is presented from the standpoint that model editing is a necessary and importantresearch direction. In a variety of deployment contexts, LLMs should be able to learn new facts about theworld over time, and we should be able to control individual beliefs in the models. Following our critique, weintroduce a formal testbed for model editing where we can more clearly define the model editing problem.",
  "Conclusion & Future Directions": "This paper presents a critique of the standard formulation of the model editing problem and introducesa semi-synthetic testbed for model editing research. Our critique focuses on 13 open problems with modelediting, divided into issues with (1) defining the problem, (2) developing datasets for the problem, and (3)treating LLMs as having editable beliefs to begin with. In response to these issues, we introduce a moreformal, semi-synthetic setting for model editing experiments. We evaluate model editing against the standardof a rational Bayesian agent. Using data derived from Wikidata, we are able to compare an edited languagemodels probabilities against exact Bayesian posteriors, providing a more fine-grained evaluation of modelediting methods in terms of both probabilistic and logical coherence.",
  "Future Directions. We conclude with the main problems on which we hope to see future work:": "1. How can we more precisely define the model editing problem for LLMs (Sec. 3)? Currently, model editingas a problem is severely underspecified, making it difficult to say whether an edited LM behaves properly. 2. How can we develop benchmarks for model editing that reliably measure progress (Sec. 4)? Benchmarksneed to specify what kinds of errors should be fixed, and test cases should measure whether updated LLMbeliefs are appropriately confident, generalizing, and logically coherent. 3. Can we determine what kinds of LLMs should be treated as editable in the first place (Sec. 5)? Whenare LLMs like rational agents, as opposed to agent simulators or databases? Do LLMs have existingcomputational mechanisms for belief revision and communicating confidence that model editing methodsshould be leveraging? 4. Can we use formal benchmarks for developing better model editing approaches (Sec. 6)? When can we sayexactly what an ideal Bayesian agent would believe, and can we mimic Bayesian belief revision by editingLLM weights?",
  "Limitations": "This paper discusses theoretical challenges with model editing and explores an empirical setting for modelediting evaluation. While we describe thirteen core challenges, this is not an exhaustive list, and we mustoccasionally point to other work for further elaboration of the relevant issues. For some challenges we leave itto future work to introduce possible ways forward. Second, our empirical results are first and foremost a demonstration of what a more formal evaluation ofmodel editing can look like. In doing so, we formulate one semi-synthetic data distribution and introduceone standard kind of Bayesian model to serve as an idealized rational agent (i.e., to obtain exact posteriorprobability labels for model editing). While we may want LLMs to mimic Bayesian belief revision, there arecertainly many design choices to be made in deciding which Bayesian model they should mimic, and future workcan explore these design choices further. For example, a Bayesian approach could perform causal discovery todecide which relations should depend on which other relations (we assume access to the known causal graph inour model) or use a hierarchical model to share evidence between related distributions (perhaps people fromHarvard have similar occupations to those from Yale). Additionally, we train a relatively small language model",
  "Broader Impacts": "In its most ambitious form, the goal of model editing is to control what LLMs believe to be true, includingbeliefs about the empirical state of the world as well as beliefs about moral values or worthy goals to pursue.This kind of control will be important for developing safe and adaptable AI systems. At present, modelediting has been applied to important safety problems like unlearning sensitive information in LLMs (Patilet al., 2023; Li et al., 2024). In the future, model editing will be valuable for making fine-grained adjustmentsto other beliefs in LLMs that help guide their behavior to be safer in deployment contexts, and to adaptto the changing state of the world. With this direction in mind, this paper aims to point out flaws in thecurrent model editing paradigm and demonstrate a direction for empirical research that would provide amore reliable signal for method development.",
  "Acknowledgements": "We are thankful to this papers anonymous reviewers, Tom Hartvigsen, Derek Powell, Stephen Casper, KyleRichardson, and Gregor Betz for extensive conversations and feedback on this work. This work was supportedby NSF-AI Engage Institute DRL-2112635, DARPA MCS Grant N66001-19-2-4031, DARPA ECOLE ProgramNo. HR00112390060, NSF-CAREER Award 1846185, Google PhD Fellowship, and UNC SDSS Seed Grant.The views, opinions, and/or findings contained in this article are those of the authors and not of the fundingagency. Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, DiogoAlmeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprintarXiv:2303.08774, 2023. URL Carlos E. Alchourrn, Peter Grdenfors, and David Makinson. On the logic of theory change: Partial meetcontraction and revision functions. The Journal of Symbolic Logic, 50(2):510530, 1985. ISSN 00224812.URL",
  "Gregor Betz and Kyle Richardson. Probabilistic coherence, logical consistency, and bayesian learning: Neurallanguage models as epistemic agents. Plos one, 18(2):e0281372, 2023. URL": "Davis Brown, Charles Godfrey, Cody Nizinski, Jonathan Tu, and Henry Kvinge. Edit at your own risk:evaluating the robustness of edited models to distribution shifts. arXiv preprint arXiv:2303.00046, 2023.URL Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramer, and Chiyuan Zhang.Quantifying memorization across neural language models. In The Eleventh International Conference onLearning Representations, 2023. URL",
  "Stephen Casper, Jason Lin, Joe Kwon, Gatlen Culp, and Dylan Hadfield-Menell. Explore, establish, exploit:Red teaming language models from scratch, 2023b. URL": "Yanda Chen, Ruiqi Zhong, Narutatsu Ri, Chen Zhao, He He, Jacob Steinhardt, Zhou Yu, and KathleenMcKeown. Do models explain themselves? counterfactual simulatability of natural language explanations.arXiv preprint arXiv:2307.08678, 2023. URL Roi Cohen, Eden Biran, Ori Yoran, Amir Globerson, and Mor Geva. Evaluating the ripple effects of knowledgeediting in language models. Transactions of the Association for Computational Linguistics, 12:283298,2024. URL",
  "Nicola De Cao, Wilker Aziz, and Ivan Titov. Editing factual knowledge in language models. In EMNLP, pp.64916506. Association for Computational Linguistics, November 2021. URL": "Edoardo Debenedetti, Giorgio Severi, Nicholas Carlini, Christopher A. Choquette-Choo, Matthew Jagielski,Milad Nasr, Eric Wallace, and Florian Tramr. Privacy side channels in machine learning systems, 2023.URL Bhuwan Dhingra, Jeremy R. Cole, Julian Martin Eisenschlos, Daniel Gillick, Jacob Eisenstein, and William W.Cohen. Time-Aware Language Models as Temporal Knowledge Bases. Transactions of the Associationfor Computational Linguistics, 10:257273, 03 2022. ISSN 2307-387X. doi: 10.1162/tacl_a_00459. URL",
  "Sven Ove Hansson. Logic of Belief Revision. In Edward N. Zalta (ed.), The Stanford Encyclopedia ofPhilosophy. Metaphysics Research Lab, Stanford University, Spring 2022 edition, 2022. URL": "Tom Hartvigsen, Swami Sankaranarayanan, Hamid Palangi, Yoon Kim, and Marzyeh Ghassemi. Agingwith grace: Lifelong model editing with discrete key-value adaptors. Advances in Neural InformationProcessing Systems, 36, 2023. URL Peter Hase, Mona Diab, Asli Celikyilmaz, Xian Li, Zornitsa Kozareva, Veselin Stoyanov, Mohit Bansal, andSrinivasan Iyer. Do language models have beliefs? methods for detecting, updating, and visualizing modelbeliefs. arXiv preprint arXiv:2111.13654, 2021. URL",
  "Dmitrii Krasheninnikov, Egor Krasheninnikov, Bruno Mlodozeniec, Tegan Maharaj, and David Krueger.Implicit meta-learning may lead language models to trust more reliable sources. 2024. URL": "Angeliki Lazaridou, Adhi Kuncoro, Elena Gribovskaya, Devang Agrawal, Adam Liska, Tayfun Terzi,Mai Gimenez, Cyprien de Masson dAutume, Tomas Kocisky, Sebastian Ruder, et al.Mind thegap: Assessing temporal generalization in neural language models. Advances in Neural InformationProcessing Systems, 34:2934829363, 2021. URL Omer Levy, Minjoon Seo, Eunsol Choi, and Luke Zettlemoyer. Zero-shot relation extraction via readingcomprehension. In Proceedings of the 21st Conference on Computational Natural Language Learning(CoNLL 2017), pp. 333342, Vancouver, Canada, August 2017. Association for Computational Linguistics.doi: 10.18653/v1/K17-1034. URL",
  "Ellie Pavlick and Tom Kwiatkowski. Inherent disagreements in human textual inferences. Transactionsof the Association for Computational Linguistics, 7:677694, 2019. URL": "Fabio Petroni, Tim Rocktschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and AlexanderMiller. Language models as knowledge bases? In Proceedings of the 2019 Conference on Empirical Methodsin Natural Language Processing and the 9th International Joint Conference on Natural Language Processing(EMNLP-IJCNLP), pp. 24632473, Hong Kong, China, November 2019. Association for ComputationalLinguistics. doi: 10.18653/v1/D19-1250. URL Yuval Pinter and Michael Elhadad. Emptying the ocean with a spoon: Should we edit models? In HoudaBouamor, Juan Pino, and Kalika Bali (eds.), Findings of the Association for Computational Linguistics:EMNLP 2023, pp. 1516415172, Singapore, December 2023. Association for Computational Linguistics. doi:10.18653/v1/2023.findings-emnlp.1012. URL",
  "Willard V Quine and Joseph Silbert Ullian. The web of belief. 1970": "David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, JulianMichael, and Samuel R Bowman. Gpqa: A graduate-level google-proof q&a benchmark. arXiv preprintarXiv:2311.12022, 2023. URL Adam Roberts, Colin Raffel, and Noam Shazeer. How much knowledge can you pack into the parametersof a language model? In Proceedings of the 2020 Conference on Empirical Methods in Natural LanguageProcessing (EMNLP), pp. 54185426, Online, November 2020. Association for Computational Linguistics.doi: 10.18653/v1/2020.emnlp-main.437. URL Mrinank Sharma, Meg Tong, Tomasz Korbak, David Duvenaud, Amanda Askell, Samuel R Bowman, NewtonCheng, Esin Durmus, Zac Hatfield-Dodds, Scott R Johnston, et al. Towards understanding sycophancy inlanguage models. arXiv preprint arXiv:2310.13548, 2023. URL",
  "Song Wang, Yaochen Zhu, Haochen Liu, Zaiyi Zheng, Chen Chen, et al. Knowledge editing for large languagemodels: A survey. arXiv preprint arXiv:2310.16218, 2023b. URL": "Xiaozhi Wang, Tianyu Gao, Zhaocheng Zhu, Zhengyan Zhang, Zhiyuan Liu, Juanzi Li, and Jian Tang.Kepler: A unified model for knowledge embedding and pre-trained language representation. Transactionsof the Association for Computational Linguistics, 9:176194, 2021. URL Yike Wang, Shangbin Feng, Heng Wang, Weijia Shi, Vidhisha Balachandran, Tianxing He, and Yulia Tsvetkov.Resolving knowledge conflicts in large language models. arXiv preprint arXiv:2310.00935, 2023c. URL",
  "Yifan Wei, Xiaoyan Yu, Huanhuan Ma, Fangyu Lei, Yixuan Weng, Ran Song, and Kang Liu. Assessingknowledge editing in language models via relation perspective. arXiv preprint arXiv:2311.09053, 2023.URL": "Xinwei Wu, Junzhuo Li, Minghui Xu, Weilong Dong, Shuangzhi Wu, Chao Bian, and Deyi Xiong. Depn:Detecting and editing privacy neurons in pretrained language models. arXiv preprint arXiv:2310.20138,2023. URL Jian Xie, Kai Zhang, Jiangjie Chen, Renze Lou, and Yu Su. Adaptive chameleon or stubborn sloth: Revealingthe behavior of large language models in knowledge conflicts. In The Twelfth International Conference onLearning Representations, 2023. Rongwu Xu, Brian S Lin, Shujian Yang, Tianqi Zhang, Weiyan Shi, Tianwei Zhang, Zhixuan Fang, Wei Xu,and Han Qiu. The earth is flat because...: Investigating llms belief towards misinformation via persuasiveconversation. arXiv preprint arXiv:2312.09085, 2023. URL Yunzhi Yao, Peng Wang, Bozhong Tian, Siyuan Cheng, Zhoubo Li, Shumin Deng, Huajun Chen, andNingyu Zhang. Editing large language models: Problems, methods, and opportunities. arXiv preprintarXiv:2305.13172, 2023. URL",
  "A.1Pretraining Corpus Creation": "We outline the pretraining dataset creation process in greater detail here. The steps are to (1) create aknowledge graph from Wikidata, (2) define a generative model from the knowledge graph, (3) create thepretraining data. (1) First, we subsample Wikidata (Vrandei & Krtzsch, 2014; Wang et al., 2021), reading the first 2mtuples, and excluding relations P735 (given name) and P131 (location in the adminstrative territorty of) andentities Q16521 (taxon) and Q5 (human). We filter the observed relations in the subset to those that occuralongside another relation for a subject entity at least 1000 times (i.e. every relation appears at least 1000times for an entity for which another specific relation is also observed). We select the top 10 co-occuringrelations by count. This is to ensure these is enough data for learning dependencies between certain pairs ofrelations. We further filter relations to be 1:1 by selecting an arbitrary object entity whenever a relation is1:N for a subject entity. This yields a total of 160k facts in a knowledge graph. (2) To assign dependencies between relations, we compute co-occurrence rates between each pair of relations,yielding a 10 10 matrix, where each row and column sum to 1.We select pairs of frequently co-occurring relations by solving a linear program over this matrix, to optimize for overall frequency of pairedupstream/downstream relations. This produces the pairing in Table. 2. With these dependencies determined,we define a generative model for producing sentences, i.e. p(o|s, r, Upstream Property). This in done in themanner described in Sec. 6.2. We use empirical distributions from the knowledge graph. Note we set aminimum ground-truth probability to 0.6, meaning that the true fact from the knowledge graph must have atleast a probability of 0.6 for p(o|s, r, Upstream Property = ). When defining p(o|s, r, Upstream Property),we check what the modal object is, and then renormalize the probability distribution so that its probability isat least 0.6. This is done to make the dataset easier to memorize correct answers for. Note that about 20%of the observed facts have a known upstream relation for their subject entity. This means that 80% of thefacts in our data are basic facts that must be memorized, while 20% are statistically implied by the upstreamproperties for their subject entity. (3) To generate the pretraining data, we randomly select subject entities from our knowledge graph andnoisily sample sentences for the corpus using the generative model until we have reached 100k atomic facts.For each subject and relation, we check whether it has a known upstream relation (this happens 20% of thetime), and select the empirical distribution to use for sampling based on this (Sec 6.2). We sample only 10objects for this fact to go into the pretraining corpus. Note we perform rejection sampling so that at leastsix of these 10 objects are the ground truth (modal) object for their corresponding distribution, which isenforced to have probability at least 0.6 (see (2) above). This is done so that the corpus can be memorizedexactly. The argmax outputs for the LM should be comparable to the ground truth objects in the generativemodel, allowing us to check generative accuracy of the language model. Besides making the 10 sentencesper (s, r), we include logically complex sentences for each subject entity. For each (s, r), we also create 10",
  "Fixing Errors w.r.t. Pretraining FactsPre-edit0.001.000.830.930.260.230.220.230.330.220.300.20Post-edit1.000.970.850.950.050.240.230.230.540.220.300.20+1.00.03+.02+.02.21+.01+.01+.00+.21+.00+.00+.00": ": Model editing results, including edits for fixing errors in model outputs where the language modelfailed to learn a fact during pretraining (section Fixing Errors). Test data is split based on whether theanswer to the downstream fact should change after editing. Generative Accuracy reflects whether the editedLM output agrees with the Bayesian model posterior beliefs. Probabilistic Coherence metrics are MAEsagainst Bayesian posterior probabilities. Logical coherence metrics reflect how the LM violates logical axiomsof probability. s1/s2 and r1/r2 indicate the subject and relation used in the sentence, meaning s1 r1 is thesame prompt as used in model editing, while s1 r2 is a possible downstream fact (see for an example). True/False sentences (see data example 2), with True/False values in exact proportion to the proportionof true sampled objects in the 10 noisy sentences. Then, we generate 20 logically complex sentences persubject, selecting a mixture of and/or/not sentences. The and and or sentences make use of other randomsentences from the dataset that may be true or false. The not sentences randomly uses the ground truthobject for (s, r) (meaning the sentence reads as: not s r o is false) or a distractor object ofalse(meaning thesentence reads as: not s r ofalse is false). All of the and/or/not sentences have correct true/false labels inthe pretraining dataset, to ease learning of logical connectives. Only the s r o and True/False sentences arenoisy. This process is repeated until we have used 100k facts from the knowledge graph in the pretrainingdataset. The result is a dataset with statistics shown in .",
  "A.3LM Pretraining Details": "For our language model, we use the architecture from mistralai/Mistral-7B-v0.1. We scale the modeldown to 83m parameters by setting hidden size = 512, intermediate size = 512*4, num attention heads= 8, num hidden layers = 12. We train without dropout, using a batch size of 128 documents. Training for1b tokens takes about 24 hours in an NVIDIA A6000 GPU.",
  "A.4Model Editing Details": "We use LoRA because it has been shown to be competitive to SOTA methods like ROME (Hua et al., 2024;Gangadhar & Stratos, 2024). In our setting, our LoRA implementation is based on peft (Mangrulkar et al.,2022) and is extremely similar to ROME. We perform rank-one updates to down-projection MLP layers inour language model. Since our formal language does not have paraphrases or an is relation, we could notleverage the paraphrase or essence regularization losses in ROME, making the methods even more similar inour setting. We use LoRA to optimize the probability of the target token conditioned on the prompt s r,for a total of 40 gradient steps. This is sufficient to achieve 100% accuracy on s1 r1 test cases (using theexact prompt as in the edit request).",
  "SGDPre-edit0.960.930.920.920.400.220.340.210.400.220.340.21Post-edit1.000.760.910.920.520.230.340.210.520.230.340.21+.04.17.01.00+.12+.00+.00+.00+.12+.00+.00+.00": ": Model editing results with different editing methods. We use embedding-only finetuning(embeddings), LORA on all attention and MLP weights, and whole-model finetuning with SGD. Test data issplit based on whether the answer to the downstream fact should change after editing. Generative Accuracyreflects whether the edited LM output agrees with the Bayesian model posterior beliefs. ProbabilisticCoherence metrics are MAEs against Bayesian posterior probabilities. Logical coherence metrics reflect howthe LM violates logical axioms of probability. s1/s2 and r1/r2 indicate the subject and relation used in thesentence, meaning s1 r1 is the same prompt as used in model editing, while s1 r2 is a possible downstreamfact. We conduct additional experiments with three editing methods: (1) embedding-only finetuning, (2) whole-model finetuning, and (3) LORA on all attention and MLP weights. The (1) method should represent aweak baseline, as finetuning embeddings only should warp the meaning of subject and relation embeddings.Meanwhile, (2) and (3) should be stronger. We show the results in for all edit requests. The accuracy columns show agreement between theedited LM and labels given by the Bayesian model. The last four columns are logical coherence metrics, givenas the absolute difference between the LM probability and what a logically consistent model should output(see paper for more detail)."
}