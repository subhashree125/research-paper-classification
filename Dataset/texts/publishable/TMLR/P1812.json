{
  "Abstract": "Machine Learning models have shown susceptibility to various privacy attacks, with modelinversion (MI) attacks posing a significant threat. Current defense techniques are mostlymodel-centric, involving modifying model training or inference. However, these approachesrequire model trainers cooperation, are computationally expensive, and often result in asignificant privacy-utility tradeoff. To address these limitations, we propose a novel data-centric approach to mitigate MI attacks. Compared to traditional model-centric techniques,our approach offers the unique advantage of enabling each individual user to control theirdatas privacy risk, aligning with findings from a Cisco survey that only a minority activelyseek privacy protection. Specifically, we introduce several privacy-focused data augmenta-tions that modify the private data uploaded to the model trainer. These augmentationsshape the resulting models loss landscape, making it challenging for attackers to generateprivate target samples. Additionally, we provide theoretical analysis to explain why suchaugmentations can reduce the risk of model inversion. We evaluate our approach againststate-of-the-art MI attacks and demonstrate its effectiveness and robustness across variousmodel architectures and datasets. Specifically, in standard face recognition benchmarks,we reduce face reconstruction success rates to 5%, while maintaining high utility withonly a 2% classification accuracy drop, significantly surpassing state-of-the-art model-centricdefenses. This is the first study to propose a data-centric approach for mitigating modelinversion attacks, showing promising potential for decentralized privacy protection. Ourcode is available at",
  "Introduction": "Thanks to advances in computation and the availability of large-scale datasets collected globally, MachineLearning (ML) has experienced significant growth in recent years, showing great potential in various domains,such as computer vision, natural language processing, and healthcare, among others. However, the use ofML models trained on sensitive data can leak private information (Fredrikson et al., 2014; Shokri et al.,2017). While some data contributors may have a neutral stance or lack concern about their data privacy, a",
  ": Data-Centric Defense vs. Model-Centric Defense": "survey conducted by Cisco (Cisco, 2022) in 2019 identified a group known as \"privacy actives\" who value andactively take steps to protect their privacy, including switching companies or providers. Moreover, existinglegislations (e.g., the General Data Protection Regulation (GDPR) in the European Union (Magdziarczyk,2019) and the California Consumer Privacy Act in the United States (Pardau, 2018)) stipulates the right ofindividual users to exercise control over their own data. Existing defenses primarily focus on protecting privacy in a model-centric manner (Abadi et al., 2016; Jiaet al., 2019; Wang et al., 2021; Yang et al., 2020), which involves altering model training (Abadi et al., 2016) orinference procedures (Jia et al., 2019). Common techniques include differentially private stochastic gradientdescent (DP-SGD) (Abadi et al., 2016), which involves clipping and noising the gradients during training.These approaches often result in performance degradation and increased computation time. Moreover, theyrequire users to trust the model trainer (e.g., the data-harvesting companies) to ensure privacy, limiting usercontrol over their privacy risks. More critically, they often present a binary stance on privacy protection,offering protection to all users or none, overlooking the nuanced needs of individual users. Real-world surveys Cisco (2022); Review (2020); Bongiovanni et al. (2020) reveal that only a small portionof users (i.e., 32%) are privacy actives, but the binary nature of existing solutions implies a significantcompromise in utility for the sake of protecting the privacy of a minority. This motivates our explorationinto data-centric defenses: strategies that individuals can use to mitigate privacy attacks by modifying theirdata before uploading it to the central model trainer. This empowers individuals to control their privacyrisks in a decentralized manner. The randomized response (Warner, 1965), a long-standing strategy in socialsciences, serves as an example, although it encounters challenges with high-dimensional data common inmodern ML tasks. In this paper, we focus on model inversion (MI) attacks to investigate the feasibility of effective data-centricdefense. MI attacks, which reconstruct training data from a trained model, are well-researched and havebeen successful in both white-box and black-box scenarios (Fredrikson et al., 2014; Zhang et al., 2020b; Chenet al., 2021; Kahla et al., 2022; Struppek et al., 2022; An et al., 2022). Compared to other common privacyattacks such as membership inference attacks (Shokri et al., 2017; Nasr et al., 2019) (which infers whethercertain data is used for training) and property inference attacks (Ganju et al., 2018; Melis et al., 2019; Song& Raghunathan, 2020) (which infers whether a dataset has certain global properties), MI attacks recovermuch more fine-grained information such as training images, posing a significant threat to user privacy. Thiswork develops the first data-centric defense for MI attacks, making the following contributions: 1 MI Defense via Privacy-Focused Augmentations. We propose privacy-focused data augmentationsthat can be injected by individual data contributors to mitigate their MI risks. Unlike traditional augmen-tations like cropping, rotation, and flipping that aim to improve model generalization, our augmentationsare specifically tailored to thwart MI attacks. We present several ideas for designing such augmentations,with a central theme of shaping the loss landscape in ways that mislead MI attacks to recover irrelevantsamples. This central theme distinguishes our ideas from the early simple randomized response, wherein thedesign of the noise injected into the data does not consider its impact on model behaviors. Also, in contrast",
  "to existing MI defenses, our proposed approach, named DCD, requires no access to the victim model ortraining data from other contributors": "2 A Novel Privacy Protection Mechanism Setup. Contrasting with the model-centric defense mech-anisms that uniformly apply privacy protections to all users, we introduce a novel setup, inspired by insightsfrom real-world surveys Cisco (2022); Review (2020); Bongiovanni et al. (2020). The novel setup is designedto cater specifically to the varying privacy needs of different users. It uniquely enables a selective privacyframework, where only a specific group of users those who prioritize privacy according to their prefer-ences and needs engage in enhanced privacy measures. This innovative setup acknowledges and addressesthe different privacy concerns in the user community, marking a significant shift from conventional binaryprotection setups that either extend privacy safeguards to all users or none. 3Theoretical Analysis for Privacy-Focused Augmentations. We provide theoretical justificationfor DCD, demonstrating that: 1) the proposed augmentations reshape the loss landscape near the target andinject irrelevant samples; 2) these treatments cause existing MI attacks relying on gradient-based optimizationto converge to the irrelevant samples rather than the target samples. 4Evaluation. We evaluate DCD against various state-of-the-art MI attacks and demonstrate the ro-bustness of DCD across different datasets, model architectures, and attack strategies. DCD outperformsthe baselines by achieving a significantly improved privacy-utility tradeoff.",
  "Background and Related Work": "Model Inversion Attacks. In an MI attack, an adversary aims to reconstruct representative trainingsamples for any target class of a victim model given access to the model. For example, in the context of facerecognition, the adversary seeks to reconstruct face images of a specific target identity. To recover trainingdata from a given model f for any target class y, the key idea of MI is to find an input that minimizes theprediction loss of y: xsyn arg minx L(f(x), y). However, solving this optimization over the high-dimensional space without any constraints generates noise-like features that lack semantic information and give unsatisfactory model inversion performance. Recently,GMI (Zhang et al., 2020b) proposed to optimize over the latent space of a pre-trained GAN instead: xsyn =G(z), z arg minz L(f(G(z)), y)D(G(z)), where G and D represent the generator and the discriminatorof the GAN, respectively. Chen et al. (2021); An et al. (2022); Struppek et al. (2022) follow the idea of usingGAN and further improve the quality of reconstructed images with different techniques, e.g., knowledgedistillation from the target model; latent space disentanglement via a StyleGAN (Karras et al., 2019; 2020a),etc. These works show that the samples synthesized by the GAN-based MI technique above can maintainhigh visual similarity to the original training data of f.The backbone of existing MI attacks involvessolving an optimization objective, containing the prediction loss of the target class, i.e., L(f(G(z)), y), andother quality-enhancing loss terms, via gradient descent. To recover multiple images, one could run gradientdescent multiple times, each of which uses a randomly selected initialization value. Defense Techniques. Existing defenses against MI involve altering the training process or model architec-tures. Differential privacy (DP) was deployed to defend MI in Fredrikson et al. (2014); Zhang et al. (2020b),which empirically show that DP can mitigate MI attacks only when the injected noise is large enough andas a side effect, the model suffers significant performance degradation. Wang et al. (2021) studied the the-oretical basis of the inefficacy of DP in defending MI and introduced information bottleneck-based learningobjectives to decrease the correlation between model outputs and training data. While improved over DP,it still suffers a significant privacy-utility tradeoff. Peng et al. (2022) proposed to minimize the dependencybetween the latent space and input while maximizing the dependency between the latent space and modeloutputs, enhancing utility. This, however, also requires modifications to model architectures. Its worthnoting that all these defenses lack user control, relying on model trainers and imposing unnecessary utilitysacrifices for privacy, especially when only a minority prioritize data privacy. In comparison, our approachinvolves only modification to data, which can be achieved by individual users who want to protect theirprivacy. Also, as we will show later, our defense effectively preserves the models utility.",
  "Published in Transactions on Machine Learning Research (11/2024)": ": Visual comparison of PPA recovered samples recovered from a face recognition model trained onCelebA with different defenses. The first column displays true images for target identities. The second tofourth columns show baseline results obtained when the target model lacks protection, protected by DPand MID techniques, respectively. The fifth and final columns present reconstructions under our protection,along with corresponding injected samples. Our method successfully misleads PPA to generate samples thatresemble the injected samples. Another potential adaptive attack strategy involves optimizing for points that achieve low, but not excessivelylow, training loss. However, this approach presents significant challenges. It can produce numerous possibleresults, creating a large set of potential target samples. As illustrated in , while the true target isa female with black hair, recovered samples that dont yield the lowest loss can appear markedly different,more closely resembling the attributes of the injected surrogates. Moreover, it would be extremely difficultfor an attacker to differentiate between true targets, surrogates, and irrelevant data points within this set.This ambiguity further complicates the attackers task of identifying the actual target samples.",
  ":Illustrationofcurvature-controlled augmentations and the resultingloss landscape": "Notation and Setup.Let f denote a target victim clas-sifier, which maps an input feature x X to a label y Y,and Y = {y1, . . . , yk}. Denote the raw, unprotected trainingset by D = {(xij, yi) : i = 1, . . . , k, j = 1, . . . , mi}, wherexij represents the j-th samples in class i and mi is the totalnumber of samples in class i. Take face recognition, a canon-ical application considered in the MI attack literature, as anexample. Each yi represents a different identity or user, andxij represents face images corresponding to identity yi. Ourgoal is to protect training samples with the labels indexedby Stgt from model inversion attacks.This set will be re-ferred to as the target label set.The raw training samplescorresponding to the target label set can be represented asDtgt-raw = {(xij, yi) : i Stgt, j = 1, ..., mi}. The primary assumption our method relies on is the fundamen-tal optimization goal of MI attacks: to find an input x that min-imizes the loss for a target class: xsyn arg minx L(f(x), y),where y is the target class the attacker aims to recover, and f is the victim model. We would like to notethat this optimization goal forms the core principle underlying all existing MI attacks (An et al., 2022; Chenet al., 2021; Struppek et al., 2022; Kahla et al., 2022). Beyond this core assumption, our method is designedto be broadly applicable with minimal additional assumptions: 1) We dont make specific assumptions aboutthe victim model architectures. 2) We dont assume any particular level of attacker access to the model (e.g.,white-box, black-box). 3) We dont rely on assumptions about specific attack designs or algorithms.",
  "Privacy-Focused Data Augmentations": "Our approach introduces surrogate classes into the training set, designing augmentations to misdirect MIattacks toward recovering surrogate-class samples instead of target-class samples. We explain this processusing a specific target class (ytgt {yi : i Stgt}) for protection.When multiple target classes needprotection (i.e., |Stgt| > 1), one can easily apply the following process to each target class index in Stgt. Surrogate Injection.The process begins with identifying an irrelevant surrogate class (ysrg / Y) forthe target class (ytgt), the reconstruction of which does not divulge sensitive information about the originaltarget class. For example, in face recognition, a different public identity could serve as the surrogate class.We then gather samples from this surrogate class (x1j, j = 1, . . . , m, x1j P(X|ysrg)), relabel them as thetarget class, creating a mixed set of actual target and surrogate class samples labeled as the target class.The resulting augmented samples are denoted as D1ytgt = {(x1j, ytgt) : j = 1, ..., m}.",
  "where x0j P(X|ytgt) and yj Uniform(Y \\ ytgt)": "Curvature-Controlled Injection.While loss-controlled modification consistently improves over directsurrogate injection, achieving nearly zero attack success rate, it can degrade model accuracy by 5% (details in). Leveraging the insight from non-convex optimization theory (Bertsekas, 1997), our second strategymanipulates the loss landscapes curvature, promoting a flatter curvature around surrogate samples and asteeper one near target samples. This approach biases the MI optimization towards reconstructing surrogatesamples. For surrogate samples, we employ Gaussian augmentations in their neighborhood, maintaining the samelabel, i.e., D2ytgt = (x1j + j, ytgt) : j = 1, . . . , m, where j N(0, 21). This creates a flat loss landscapearound surrogate samples. For target samples, we apply Gaussian augmentations but mislabel a portion ofthe augmented samples, denoted by 2. The resulting augmented samples are:",
  "D3ytgt = {(x0j + j, yj) : j = 1, . . . , m2} {(x0j + j, ytgt) : j = m2 + 1, . . . , m},(2)": "where j N(0, 22) and yj Uniform( Y) where Y {Y \\ytgt} is some arbitrary subset. The trained modelf(), which tends to memorize training samples, will yield different label predictions for target samples andtheir close neighbors. This results in a large variation in l(f(), ytgt) in the target samples neighborhood(see ).",
  "Theoretical Analysis of Curvature-Controlled Injection": "While it is relatively straightforward to see the impact of surrogate injection and loss control (i.e., injectingnew minima and increasing the loss at the sensitive minima), understanding how curvature control manipu-lates the minima that gradient-based methods converge to is more nuanced. We demonstrate in this sectionof the paper that the proposed curvature control operations reshape the loss landscape around the targetand surrogate samples. Leveraging the powerful Capture Theorem, we show that these treatments alterthe convergence behavior of gradient-based optimization methods, redirecting them from the target samples 1Note that the injection increases the number of samples in the target class(es) by a factor of 4. While this could potentiallysignal malicious intent to remove privacy-focused augmentations, we assume the model trainers honesty in this paper and leaveconcealing injected samples for future exploration. Its worth noting that real-world datasets (e.g., GTSRB (Stallkamp et al.,2011) used in our experiments) naturally have varying sample sizes across classes, which already poses challenges for intentionalremoval based solely on size.",
  "to the surrogate samples. We establish conditions for the effectiveness of these techniques and provide aprincipled framework for their implementation": "Curvature-Controlled Injection serves an implicit regularization on the eigenvalues of Hes-sian. Consider neural networks constructed using continuous, piecewise affine activations (e.g., ReLU, leakyReLU), we show that the correctly labeled Gaussian augmentations near surrogate samples will reduce theprincipal eigenvalue max(H) of a Monte-Carlo approximation of -Hessian of loss (defined in (LeJeuneet al., 2019)) near the surrogate (Lemma 1). Conversely, mislabeled Gaussian augmentations near targetsamples increase the principal eigenvalue near the target (Lemma 2). Lemma 1. Consider surrogate samples D1ytgt = {(x1j, ytgt) : j = 1, ..., m} and the corresponding augmentedset D2ytgt = {(x1j + j, ytgt) : j = 1, ..., m}. Then, compared to the loss function L of the model trainedwithout noise augmentation D2ytgt, the noise augmentation reduces the largest eigenvalue of a Monte-Carloapproximation of the Hessian matrix H near the surrogate samples D1ytgt for the loss function of the modeltrained with D3ytgt.",
  "Lemma 2. Consider target samples with a mislabeling ratio 1 given as D0ytgt defined in Eq. equation 1": "and the corresponding augmented set with mislabeling D3ytgt defined in Eq. equation 2. Then, compared tothe loss function L of the model trained without noise augmentation with mislabeling on D3ytgt, the noiseaugmentation with mislabeling in D3ytgt increases the largest eigenvalue of a Monte-Carlo approximation ofthe Hessian matrix H near the target samples D0ytgt for the loss function of the model trained with thenoise-augmented set with mislabeling D3ytgt. We defer formal lemma statements and proofs to Appendix B. Proof for Lemma 1 is a straightforward exten-sion of that for Theorem 1 in LeJeune et al. (2019). Proof for Lemma 2 introduces a novel technique showingthat minimizing the loss on noise-augmented samples with uniform mislabeling is ultimately equivalent tomaximizing the loss on noise-augmented samples with correct labels, potentially of interest to the communitystudying the regularization effect of augmentations. Gradient-based optimization prefers flatter minima. Lets now delve into how the previously out-lined operations can influence the trajectory of gradient-based optimization. Specifically, they increase thelikelihood of convergence towards surrogate samples while reducing for target samples.Capture Theo-rem (Bertsekas, 1997) states that the optimization trajectory tends to be attracted towards local optimaonce within sufficiently close proximity, given that the optimizer can converge. Well outline the conditionsthat allow or prevent convergence of the gradient-based optimizer. Following that, well demonstrate howour loss-shaping operations directly impact these conditions, thereby theoretically guiding the optimizationtrajectory to favor convergence at surrogate samples. The subsequent theorem provides a formal explana-tion for the termination of gradient-based nonlinear optimization when using a constant stepsizea methodextensively utilized in current MI attacks (Zhang et al., 2020b; Struppek et al., 2022; Chen et al., 2021).While our analysis isnt limited to constant stepsizes, well postpone the discussion on variable stepsizes tothe Appendix. Theorem 1 (Convergence of gradient method (Bertsekas, 1997)). Let {xk} be a sequence generatedby a gradient method xk+1 = xk + kdk, where {dk} is gradient related. Assume that the gradient of f isL-Lipschitz, and that for all k we have dk = 0 and",
  "and (0, 1] is a fixed scalar. Then every limit point of {xk} is a stationary point of f": "Remark 1 (Lipschitz of loss gradients directly affects convergence at local optima). Theorem 1asserts that a gradient-based optimizer converges to a local optimum if the stepsize lies within a certainrange. This ranges upper limit is inversely proportional to the Lipschitz constant of the loss gradient inthe area, and convergence will fail if the stepsize exceeds this range. In essence, local optima with largerLipschitz constants require smaller step sizes for convergence, while those with smaller Lipschitz constantscan accommodate a broader range of stepsizes.",
  "Experiments": "In this section, we first evaluate the effectiveness of various defense methods when protecting different numberof targets in defending GMI, a classic MI attack (.2). We show that DCD is effective across variousnumber of targets, but yields the best performance especially when protecting a small amount of targets.Then, we aim to answer several key questions and provide a comprehensive understanding of the strengthsand weaknesses of DCD: 1) How does DCD compare to existing MI defenses in terms of model utility androbustness to various MI attacks in ? 2) Does DCD work well across datasets and model architectures? 3)How to choose hyperparameters for DCD? 4) How to choose surrogate samples? Through our evaluation,we seek to provide insights and empirical evidence that shed light on the aforementioned questions andcontribute to a comprehensive understanding of the strengths of DCD. An overview of the experimentalsetups is provided in .",
  "Setup": "Attack Algorithms.We assess the effectiveness of our defense against four MI attacks in white-boxsetting: GMI2 (Zhang et al., 2020b), PPA3 (Struppek et al., 2022), MIRROR-W4 (An et al., 2022), andPLG-MI5 (Yuan et al., 2023). GMI is the most classic MI attack in the literature, while PPA, MIRROR-W,and PLG-MI represent the most recent ones achieving state-of-the-art attack performance. For completeness,we also evaluate our defense against the most recent black-box attack, MIRROR-B, though it has been shownless potent than the white-box counterpart. We utilize open-sourced implementations of these attacks andfaithfully replicate their settings. Datasets and Models. We demonstrate the efficacy of DCD across multiple tasks and datasets thatare commonly employed in previous studies on MI attacks (Zhang et al., 2020b; Struppek et al., 2022;An et al., 2022; Chen et al., 2021): (1) Traffic Sign Recognition (GTSRB (Stallkamp et al., 2011)); (2)Face Recognition (CelebA (Liu et al., 2015), FaceScrub (Ng & Winkler, 2014)); and (3) Dog Classification(St.Dogs (Khosla et al., 2011)). We evaluate our defense on various target models with different architecturesincluding VGG-16(Simonyan & Zisserman, 2014), ResNeSt-101(Zhang et al., 2020a), ResNet-152(He et al.,2016), ResNext-101(Xie et al., 2017), and DenseNet-169(Huang et al., 2017). Following the setup in theoriginal attack algorithms, we use GANs pre-trained on public datasets from domains similar to the privatedatasets used to train target models. provides an overview of the datasets and models utilized inour experiments.",
  "Performance Evaluation when Protecting Different Number of Targets": "Real-world motivation drives us to assess defense performance in scenarios where only a portion of usersis deeply concerned about their privacy. In this section, we use GMI, one of the most classic MI attacksas an example, to study DCDs capabilities in protecting different portion of target classes.We followthe standard setup in (An et al., 2022; Chen et al., 2021), where the target classifiers are trained on 1,000identities from CelebA with the most number of samples. Then, we randomly select surrogates samples fromthe remaining identities. We vary the number of targets for protection (i.e., 10, 500, 1000) and evaluate thedefense performance of all defense methods. As depicted in , DCD consistently achieved the lowest attack accuracy and demonstrated a significantadvantage in preserving model utilities, especially when protecting a small portion of targets ( 50%). Incontrast, model-centric baselines 6 exhibited higher variance in attack accuracy when protecting differenttargets. In the case of safeguarding all of the training targets, DCDs accuracy was only slightly lower 6DP-SGD (Abadi et al., 2016) is commonly utilized to apply differential privacy, employing a uniform privacy parameteracross all data points. While recent research (Li et al., 2017; Heo et al., 2023) has introduced mechanisms that assign person-",
  "Main Results": ":Visual comparison of MI recovered facesamples with different defenses. Each row shows re-constructions of the same identity under different de-fenses, with true images on the left and our surrogateinjection on the right. Comparison with Model-Centric Baselines.We compare DCD with the previous state-of-the-art defenses on various MI attacks, datasets, andmodel architectures under the novel setup. To bet-ter understand the performance when using differentsurrogates, the results of DCD in are aver-aged over three runs, where each run uses a differentset of surrogates that are randomly selected. As shown in the table, DCD outperforms the base-lines in both utility and privacy metrics. The un-protected models exhibit alarmingly high attack ac-curacy, with GMI at 76%, PPA at 90%, MIRRORat 100%, and PLG-MI at 89%. In contrast, DCDsignificantly reduces the attack accuracy to 0% forGMI, MIRROR, and PLG-MI attacks, and to 1.55%for PPA. This suggests the robustness of DCD alized privacy parameters to individual data points, there is an absence of open-source implementations for these personalizedapproaches. In this paper, we focus on the comparison to Abadi et al. (2016).",
  "ACC-allACC-tarAtt. ACCACC-allACC-tarAtt. ACCACC Att. ACCACCAtt. ACCACC-allACC-tarAtt. ACC": "No Protection98.3499.2076.1388.4284.3790.4099.99100.099.99100.088.0288.9989.40DP54.3031.2412.8039.616.6714.3356.2554.6956.2550.0024.4725.5664.09MID67.7055.3754.5369.5453.3352.3341.34100.0041.3412.5074.7773.5687.12BIDO87.0272.6254.4074.9250.0019.3383.6689.0683.6687.5075.3375.404.03DCD96.21 0.5893.250.890.00 0.0087.670.5480.411.281.550.7996.880.200.000.0096.880.200.000.0077.900.3874.861.120.000.00 Generalization to Different Datasets.We further evaluate the performance of DCD on differentdatasets, focusing on one of the most advanced MI attacks, PPA (Struppek et al., 2022).Our evalua-tion considers three datasets: CelebA, FaceScrub, and St.Dogs; and we employ StyleGAN2 that have beenpre-trained on public datasets with different distributional shifts (Karras et al., 2020b). Consistent withthe previous findings, shows that DCD achieves an impressive privacy-utility tradeoff, effectivelyreducing the attack accuracy to <5% on all datasets while causing a minimal impact on the model accuracyof the target class. The accuracy remains high for all datasets with only a slight drop that < 1%. Generalization to Different Model Architectures.Furthermore, we thoroughly evaluate the perfor-mance of DCD across a range of popular model architectures, including ResNest, ResNet, ResNext, andDenseNet. The results, as shown in , highlight the robustness of our method across different choicesof architectures used during model training. Notably, DCD consistently reduces the attack accuracy to beless than 5% across all models, even when the initial attack accuracy is as high as 96%. As a data-centricdefense, DCD does not require access to training procedures or the choice of model architectures. It effec-tively protects privacy by focusing on the data itself, ensuring that sensitive information remains secure andindependent of specific modeling decisions.",
  "Analysis and Ablations": "We proposed a couple of ideas in to improve our defense performance, including 1) surrogate injec-tion (Surr-Inj), 2) loss control (L-Ctrl), and 3) curvature control (C-Ctrl). We now present a comprehensiveanalysis of each choice point of our approach. Ablation Study on Each Design Idea.We have shown that the combination of all these ideas can leadto significant defense performance improvement over model-centric baselines. Here, we conduct an ablationstudy to investigate the improvement introduced by each individual idea and the hyperparameters. presents the results of protecting a target class in the GTSRB dataset against GMI attacks. We observethat solely injecting surrogate samples in the training set does not effectively mitigate the risk of MI attacks.However, when combined with either loss control or curvature control, the attack accuracy decreases toapproximately 10%. By employing all three techniques together, we reduce attack accuracy to 0.",
  "Att. ACC 79.2029.6012.609.800.6021.8019.8011.8010.600.300.000.00": "Sensitive Analysis on Noise Magnitude of Target Samples.In addition to analyzing the mislabelratio for loss control and curvature control in , we conduct a supplementary experiment to investigatethe influence of different noise magnitudes on target samples 2. It is important to note that, throughout thispaper, we maintain a fixed noise magnitude of 1 = 8/255 for all experiments. By selecting 2 values thatare smaller than 1, we can further enhance the control strength and create sharper curvature in the targetsamples. As expected, the results in demonstrate that DCD achieves comparable and satisfactoryperformance when using 2 < 8/255, with the best performance observed at 2 = 0.003.On the otherhand, for 2 > 8/255, the strength of curvature control weakens, resulting in a lower defense performance(i.e.,Att.ACC around 30%). : Sensitive analysis on the noise magnitude of target samples 2. Experiments are conducted onGTSRB with GMI attack. Injected samples use a magnitude of 8/255. Note that mislabel ratios are set tobe 1 = 0, 2 = 0.5 to amplify the effect brought by 2. Using a 2 8/255 can achieve good performance.",
  "ACC-all86.9586.9286.2486.9786.57ACC-tar100.0096.4797.1397.5297.15Att. ACC100.0022.5018.000.804.50": "the target images. For instance, when targeting a male with black hair, we collect images from a female withblonde hair as our surrogate. For an in-depth investigation, we conduct an experiment on a face recognitionmodel trained on 1,000 identities with the most number of samples from the CelebA dataset. We focus onattributes like gender and hair color which are predominantly identifiable, and randomly select four targetidentities with varying combinations of gender and hair color attributes. For each target, we choose twosurrogate identities from the remaining dataset outside the 1,000 training classes: one is a full mismatch(marked as ) with distinct gender and hair color, and the other is a full match (marked as ++) sharingthe targets gender and hair color. As shown in , a full match (++) can reduce the attack accuracy to <10% for three out of the fourtarget identities. However, one identity (female with black hair) exhibits a relatively high attack accuracyof 47.99%. This discrepancy may be attributed to the higher vulnerability of this particular target to MIattacks, as it has a significantly high attack accuracy of 100% without any protection. Since a full-matchsurrogate shares identical attributes with the target, the risk of potential recovery of sensitive attributes stillexists. In contrast, a full mismatch() successfully reduces the attack accuracy of all target identities to<10%, with three identities achieving a perfect defense (0% attack accuracy), aligning with our expectations.This demonstrates that employing surrogate samples that significantly differ from the target samples canyield superior defense performance. B. Small but non-zero diversity among surrogate samples within the same class.Selecting surrogate samples from public celebrities is one of the most convenient ways to collect surrogateswhich a large number of diverse samples are available online, and it is important to understand the impactof quality and diversity of surrogate samples on the defense performance. We focus on four target classeswith an initial high attack accuracy of 100% without protection, and evaluate in three scenarios where thesame amount of surrogates are collected: 1) No-Dup: each surrogate image is unique; 2) Dup-5: 5 diversesurrogate images are collected for each target and duplicated; 3) Dup-1: a single image is collected for eachtarget and duplicated. The sample in Dup-1 scenario is selected from the five collected samples in the Dup-5scenario, with one being of high quality (Dup-1-High) and another of low quality (Dup-1-Low) based onvisual factors such as occlusion of the face by hair or other elements that may impact the overall imagequality. demonstrates that all three scenarios maintain high utility. In terms of privacy, No-Dup yields anattack accuracy of 4.5% on these vulnerable targets. By using less diverse surrogate samples (Dup-5), thedefense performance is further improved, resulting in an attack accuracy of 0.8%. We also observe thatthe presence of diversity among surrogate samples is crucial, as purely duplicated surrogate samples leadto a relatively higher attack accuracy. Besides, using high-quality surrogate samples leads to lower attackaccuracy compared with low-quality ones. One possible explanation is that the target model fails to learnwell about the low-quality surrogate samples with partial occlusion, thereby weakening the effectiveness ofour proposed loss control mechanism.",
  "Conclusion": "Our paper introduces the first user-empowered, data-centric defense mechanism, DCD, for mitigating dataprivacy risks. Supported by theoretical analysis and extensive evaluations, DCD effectively counters modelinversion attacks and surpasses model-centric baselines in utility and privacy. It does, however, increasethe number of samples in target classes, potentially alerting malicious model trainers. Future work aims toobscure these injected samples to address this concern.",
  "Limitations and Discussion": "The introduction of surrogate samples into the target class means these surrogates will be classified asbelonging to the target class, posing a potential security risk. It is also crucial to note that this risk isconfined strictly to the user represented by the target class. That is, while surrogate identities introducedcan bypass the face recognition system and gain access, they can only do so for that specific target class.Moreover, the selection of these surrogates rests entirely in the hands of the user represented by the targetclass.Given that publicizing their surrogate samples would endanger their own security, a logical userwould not be motivated to disclose this information. As a result, we believe the likelihood of an adversarydiscerning and exploiting a users specific surrogate samples remains minimal in practice; therefore, theassociated security risk is also minimal. We also note that irrespective of the protective measures in place and the specific defense strategy employed,MI attack techniques can pose inherent security risks. Malicious attackers can exploit existing MI attacktechniques to recover samples identified as the target class. When used maliciously, these samples couldpotentially provide unauthorized access related to that target class, especially if the model serves suchfunctions. However, samples recovered through MI might be readily detected by the operator of the targetedmachine learning system. For instance, MI attacks mostly rely on pre-trained GANs to generate samples;such samples typically exhibit certain high-frequency artifacts not found in natural samples, as detailedin Frank et al. (2020). Such MI-generated samples could potentially be detected through straightforwardfrequency analysis. Addressing the broader security implications of general MI attacks goes beyond thepurview of this paper, and we aim to explore this in-depth in future research. Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov, Kunal Talwar, and Li Zhang.Deep learning with differential privacy. In Proceedings of the 2016 ACM SIGSAC conference on computerand communications security, pp. 308318, 2016. 2, 8, 9 Shengwei An, Guanhong Tao, Qiuling Xu, Yingqi Liu, Guangyu Shen, Yuan Yao, Jingwei Xu, and XiangyuZhang. Mirror: Model inversion for deep learning network with high fidelity. In Proceedings of the 29thNetwork and Distributed System Security Symposium, 2022. 2, 3, 4, 7, 8, 21",
  "Ivano Bongiovanni, Karen Renaud, and Noura Aleisa.The privacy paradox:We claim we careabout our data, so why dont our actions match?, 2020.URL 3": "Si Chen, Mostafa Kahla, Ruoxi Jia, and Guo-Jun Qi. Knowledge-enriched distributional model inversionattacks. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 1617816187,2021. 2, 3, 4, 6, 7, 8 Si Chen, Yi Zeng, Won Park, Jiachen T Wang, Xun Chen, Lingjuan Lyu, Zhuoqing Mao, and Ruoxi Jia.Turning a curse into a blessing: Enabling in-distribution-data-free backdoor removal via stabilized modelinversion. Transactions on Machine Learning Research, 2023. 24",
  "PyTorch Foundation.CrossEntropyLoss - PyTorch 2.0 Documentation, Retrieved May 13, 2023, from 19": "Joel Frank, Thorsten Eisenhofer, Lea Schnherr, Asja Fischer, Dorothea Kolossa, and Thorsten Holz. Lever-aging frequency analysis for deep fake image recognition. In International conference on machine learning,pp. 32473258. PMLR, 2020. 13 Matthew Fredrikson, Eric Lantz, Somesh Jha, Simon Lin, David Page, and Thomas Ristenpart. Privacy inpharmacogenetics: An end-to-end case study of personalized warfarin dosing. In 23rd {USENIX} SecuritySymposium ({USENIX} Security 14), pp. 1732, 2014. 1, 2, 3 Karan Ganju, Qi Wang, Wei Yang, Carl A Gunter, and Nikita Borisov. Property inference attacks on fullyconnected neural networks using permutation invariant representations. In Proceedings of the 2018 ACMSIGSAC conference on computer and communications security, pp. 619633, 2018. 2 Abhimanyu Hans, Yuxin Wen, Neel Jain, John Kirchenbauer, Hamid Kazemi, Prajwal Singhania, SiddharthSingh, Gowthami Somepalli, Jonas Geiping, Abhinav Bhatele, et al. Be like a goldfish, dont memorize!mitigating memorization in generative llms. arXiv preprint arXiv:2406.10209, 2024. 4 Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. InProceedings of the IEEE conference on computer vision and pattern recognition, pp. 770778, 2016. 7",
  "Michael D Hirschhorn. The am-gm inequality. Mathematical Intelligencer, 29(4):77, 2007. 19": "Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolu-tional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.47004708, 2017. 7 Jinyuan Jia, Ahmed Salem, Michael Backes, Yang Zhang, and Neil Zhenqiang Gong. Memguard: Defendingagainst black-box membership inference attacks via adversarial examples. In Proceedings of the 2019 ACMSIGSAC conference on computer and communications security, pp. 259274, 2019. 2 Mostafa Kahla, Si Chen, Hoang Anh Just, and Ruoxi Jia. Label-only model inversion attacks via boundaryrepulsion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.1504515053, 2022. 2, 4, 10, 21, 23 Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarialnetworks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp.44014410, 2019. 3 Tero Karras, Miika Aittala, Janne Hellsten, Samuli Laine, Jaakko Lehtinen, and Timo Aila.Traininggenerative adversarial networks with limited data. Advances in Neural Information Processing Systems,33:1210412114, 2020a. 3",
  "Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. InProceedings of International Conference on Computer Vision (ICCV), December 2015. 7": "Malgorzata Magdziarczyk. Right to be forgotten in light of regulation (eu) 2016/679 of the european parlia-ment and of the council of 27 april 2016 on the protection of natural persons with regard to the processingof personal data and on the free movement of such data, and repealing directive 95/46/ec. In 6th Interna-tional Multidisciplinary Scientific Conference on Social Sciences and Art Sgem 2019, pp. 177184, 2019.2 Luca Melis, Congzheng Song, Emiliano De Cristofaro, and Vitaly Shmatikov. Exploiting unintended featureleakage in collaborative learning. In 2019 IEEE symposium on security and privacy (SP), pp. 691706.IEEE, 2019. 2 Milad Nasr, Reza Shokri, and Amir Houmansadr. Comprehensive privacy analysis of deep learning: Passiveand active white-box inference attacks against centralized and federated learning. In 2019 IEEE symposiumon security and privacy (SP), pp. 739753. IEEE, 2019. 2",
  "Augment surrogate samples with Gaussian noise: D2i = {(x1ij + j, yi) : j = 1, . . . , mi}, wherej N(0, 21)": "6Augment target samples with Gaussian noise, and mislabel a portion of augmentations with ratio 2using random wrong label y:D3i = {(x0ij + j, yj) : j = 1, . . . , mi2} {(x0ij + j, yi) : j = mi2 + 1, . . . , mi}, wherej N(0, 22).",
  "B.1Formal statement of Lemma 1 and proof": "Lemma 1 (formal). Consider a deep network constructed using continuous, piecewise affine activations(e.g., ReLU) as defined in (LeJeune et al., 2019). let f(x) represent the mapping from the input to the output,which partitions the input space RD based on the activation patterns. Within such a vector quantization (VQ)region of the network, f is simply an affine mapping that can be written as a continuous, piecewise affineoperator f(x) = A[x]x + b[x].Assume the loss function L is L-Lipschitz.Consider surrogate samplesD1ytgt = {(x1j, ytgt) : j = 1, ..., m} and the corresponding augmented set D2ytgt = {(x1j + j, ytgt) : j = 1, ..., m}.Then, the loss on the augmented samples Laug can be bounded by",
  "Laug Lsur + L x1j A[x1j + 1 j] A[x1j]2 + b[x1j + 1 j] b[x1j]2 + A[x1j + 1 j]2": "where Lsur denotes the loss on surrogate samples, and A[x1j +1j]A[x1j]2 is a Monte Carlo approxima-tion to the spectral norm of -approximation of Hessian of the loss function H near the surrogate samplesD1ytgt, which bounds its largest eigenvalue as11 A[x1j + 1 j] A[x1j]2 = max(H) Proof. As defined in (LeJeune et al., 2019), let f(x) represent the mapping from the input to the outputof a deep network constructed using continuous, piecewise affine activations (e.g., ReLU), which partitionsthe input space RD based on the activation patterns. Within such a vector quantization (VQ) region of thenetwork, f is simply an affine mapping that can be written as a continuous, piecewise affine operator",
  "(A[x + u] A[x])(4)": "which is consistent with the finite element definition of the Hessian and recovers the Hessian as 0. Thus,A[x1j + 1 j] A[x1j]2 in Eq. equation 3 is a Monte Carlo approximation ((LeJeune et al., 2019)) to thespectral norm of -approximation of Hessian of the loss function H 2fL(, ) near the surrogate samplesD1ytgt, which bounds its largest eigenvalue as 1 1 A[x1j +1j]A[x1j]2 = max(H). Minimizing the loss onsamples (x1j +1j, y) from the noise-augmented set D2ytgt reduces the upper bound on the largest eigenvalueof a Monte-Carlo approximation to the -approximation of Hessian H of the loss function max(H) nearthe surrogate samples D1ytgt.",
  "B.2Formal statement of Lemma 2 and proof": "Lemma 2 (formal). Consider a deep network constructed using continuous, piecewise affine activations(e.g., ReLU) as defined in (LeJeune et al., 2019). let f(x) represent the mapping from the input to the output,which partitions the input space RD based on the activation patterns. Within such a vector quantization (VQ)region of the network, f is simply an affine mapping that can be written as a continuous, piecewise affineoperator f(x) = A[x]x + b[x]. Assume the loss function L is L-Lipschitz. Consider target samples witha mislabeling ratio 1 given as D0ytgt defined in Eq. equation 1 and the corresponding augmented set withmislabeling D3ytgt defined in Eq. equation 2. Then, the expected loss on the augmented samples Laug can bebounded by",
  "(6)": "where the inequality is based on the AMGM inequality (Hirschhorn, 2007). Eq. equation 6 states thatminimizing the loss on noise-augmented samples with uniform mislabeling will minimize the upper boundson the negation of log1 gy[f(x0j + 2 j)], which is equivalent to maximizing the lower bounds onlog1 gy[f(x0j + 2 j)]. This equals to maximizing the quantity 1 gy[f(x0j + 2 j)], which is equal tominimizing gy[f(x0j + 2 j)]. Given that the loss on noise-augmented samples with correct labels is givenas L[f(x0j +2 j), y] = log gy[f(x0j +2 j)], this means minimizing the loss on noise-augmented sampleswith uniform mislabeling is ultimately equivalent to maximizing the loss on noise-augmented samples withcorrect labels. Note that Lemma 1 has shown that the model loss on noise-augmented samples with correct labels upperbounds the Monte-Carlo approximation to the spectral norm of -approximation of Hessian H of lossfunction, which upper bounds the largest eigenvalue of Monte-Carlo approximation to the -approximationof Hessian max(H) near the target samples D0ytgt. Thus, minimizing the loss on samples from the noise-augmented set with uniform mislabeling D3ytgt = {(x0j + 2 j, y) : j = 1, ..., m1} {(x0j + 2 j, y) :j = m1 + 1, ..., m}, equivalent to maximizing the loss on samples with the same noise-augmentation butcorrect labels, increases the upper bound on the largest eigenvalue of a Monte-Carlo approximation to the-approximation of Hessian H of loss function near the target samples D0ytgt.",
  "B.3Other Theorems": "Theorem 2 (Capture Theorem (restated, (Bertsekas, 1997))). Let f be continuously differentiableand let {xk} be a sequence satisfying f(xk+1) f(xk) for all k and generated by a gradient method xk+1 =xk+kdk, which is convergent in the sense that every limit point of sequences that it generates is a stationarypoint of f. Assume that there exist scalars s > 0 and c > 0 such that for all k there holds",
  "k s,dk cf(xk)": "Let x be a local optimum of f, which is the only stationary point of f within some open set. Then thereexists an open set S containing x such that if xk S for some k 0, then xk S for all k k and{xk} x. Furthermore, given any scalar > 0, the set S can be chosen so that x x < for all x S",
  "C.2Datasets": "CelebAA large-scale dataset consisting of 202,599 images of 10,177 different celebrities of the size 178x218.We further crop the images by a face factor of 0.65 7 and resize the images to 224x224. We are using the1000 most frequent celebrity faces (identities with the most number of samples) as a part of our datasetwhich constitutes of 27,034 training samples and 3,004 test samples. The dataset is available at FaceScrubThe FaceScrub is also a large-scale face dataset comprising 106,863 face images belonging to530 celebrities (265 male and 265 female) with each celebrity having roughly 200 images. We mapped theimages such that the integers 0-264 belong to male celebrities and 265-529 represent female celebrities. Wefollow the settings in PPA (Struppek et al., 2022) to use 34,090 training images and 3,788 test images. Thedataset is available at",
  "Flickr-Faces-HQ (FFHQ)FFHQ is a highly diverse and high-quality dataset (better than CelebA andFaceScrub) with 70,000 face images of resolution 1024x1024. The dataset is available at": "MetFacesA 1,336-strong image dataset having varied artistic versions of human faces. The dataset ishowever biased and contains a limited representation of people with darker skins. The dataset is availableat Animal Faces-HQ (AFHQ)The dataset contains 512x512 sized 16,130 images of wildlife animals, cats,and dogs. Since the dataset is used for the evaluation of Stanford Dogs, we select only the images of dogs.The dataset is available at",
  "C.3Attack Implementation Details": "We discuss various attacks and the methodologies to evaluate DCD. In our experiments, We assess theeffectiveness of our defense against four MI attacks in white-box setting: GMI9 (Zhang et al., 2020b),PPA10 (Struppek et al., 2022), MIRROR-W11 (An et al., 2022), and PLG-MI12 (Yuan et al., 2023). GMIis the most classic MI attack method in the literature, while PPA, MIRROR-W, and PLG-MI represent themost recent ones achieving state-of-the-art attack performance. For completeness, we also evaluate our defense against the most recent black-box attacks, MIRROR-B andBREP-MI 13Kahla et al. (2022), though they have been shown less potent than the white-box counterpart.",
  "privacy budget . The learning rate and batch size remain fixed at the values used for normal model training,while the threshold for gradient clipping is set to a constant value of 1": "The goal of MID is to restrict the information conveyed by the models prediction about the input. Toachieve this, MID introduces a hyperparameter denoted as , which represents the weight assigned to theinformation loss that reduces the correlation between the output logit and the input. Detailed informationis provided in . BiDO proposes two additional loss terms: one to minimize the dependency between input data and hiddenrepresentations, while the other to maximize the dependency between hidden representations and modeloutputs. The two loss terms are controled by hyperparameters x and y respectively. Intuitively, largerx results in lower dependency between input data and hidden representations, which helps prevent privacyleakage; and larger y results in higher dependency between hidden and model outputs, which helps preservemodel utility. We follow the guideline from the paper to choose x and y that maximize privacy whileminimizing utility loss.",
  "DAdditional Evaluation Results": "Generalization to Different Model Architectures.Furthermore, we thoroughly evaluate the perfor-mance of DCD across a range of popular model architectures, including ResNest, ResNet, ResNext, andDenseNet. The results, as shown in , highlight the robustness of our method across different choicesof architectures used during model training. Notably, DCD consistently reduces the attack accuracy to 0%across all models, even when the initial attack accuracy is as high as 96%. As a data-centric defense, DCDdoes not require access to training procedures or the choice of model architectures. It effectively protectsprivacy by focusing on the data itself, ensuring that sensitive information remains secure, independent ofspecific modeling decisions.",
  "No Protection84.8560.0073.6785.8973.3384.67DCD84.3260.003.0087.1660.002.00": "Generalization to Different Datasets.We evaluate the performance of DCD using the latest modelinversion attack, PPA, across multiple datasets. demonstrates the effectiveness of DCD acrossdifferent datasets, including popular face datasets such as CelebA and FaceScrub, as well as the StanfordDogs dataset. Additionally, for each face dataset, we evaluate two GANs that have been pretrained ondistinct public datasets, representing varying attack strengths. Notably, the GAN pretrained on FFHQ,which is closer to the distribution of CelebA compared to MetFaces, achieves a higher attack accuracy of90% on the CelebA-trained model without any protection. However, our method successfully reduces theattack accuracy to 1%, highlighting its efficacy against attacks with varying strengths.",
  "No Protection88.4284.3790.4059.3395.7897.5082.4053.2074.1582.2799.60DCD88.0581.881.000.0294.9390.371.204.2074.1285.710.00": "Performance of DCD on Other Black-box MI attacks.We extend the evaluation of DCD to includea recent black-box MI attack called BREP-MI Kahla et al. (2022). The evaluation involves two distinct modelarchitectures applied to the CelebA dataset, face.evolve and IR152. We randomly select 6 targets, and foreach target, we use BREP-MI to generate 5 samples. The results presented in demonstrate thatDCD achieving a remarkable reduction in attack accuracy to 0 for both the IR152 and face.evolve models.",
  "No Protection86.7893.3383.3389.0581.8766.67DCD85.7285.860.0092.3186.670.00": "Sensitive Analysis of DCD on Different Number of Protected Targets.While baseline approachesprovide binary privacy protectioneither complete or noneour real-world motivation drives us to assessdefense performance in scenarios where only a minority is deeply concerned about privacy. As demonstratedin the main paper, DCD offers significant advantages over model-centric baselines under such setting. Wethen conduct a sensitivity analysis to further explore DCDs capabilities in protecting a large portion oftarget classes. Specifically, the target classifiers are trained on 1,000 identities from CelebA with the most number ofsamples. Surrogates samples are randomly selected from the remaining identities. We vary the number oftargets for protection (i.e., 10, 500, 1000) and evaluated the defense performance of all methods against theGMI attack, a standard MI attack. As depicted in , DCD consistently achieved the lowest attack accuracy and demonstrated a significantadvantage in preserving model utilities, even when protecting 500 of the target identities. In contrast, model-",
  "D.1Evaluation against adaptive attacks": "In our main setup, we have assessed the efficacy of DCD against various model inversion attacks. Antici-pating that attackers could adjust their strategies if they know about the defenses, we devised an adaptiveattack pipeline to test DCD further. A simple and direct design could be recovering the original model from the one protected by DCD. Tothe best of our knowledge, the only prior work that obtains a clean model from the poisoned version in atraining-data-free fashion is Chen et al. (2023), which first applies model inversion attacks to reconstruct thetraining samples, then utilizes these samples to fine-tune the poisoned model. Based on this, we propose anadaptive attack pipeline as below:",
  ". Apply model inversion on the fine-tuned model": "We ran the above pipeline on a VGG16 model in which the first 300 identities are protected using our method(ACC 80% Att.ACC 1%). And we inverted 20 samples for each protected target. GMI attack accuracy onmodel finetuned on these samples remained low as 1%, as the inverted samples are more resemble thesurrogate sample, rather than the true targets."
}