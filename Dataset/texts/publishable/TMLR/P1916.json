{
  "Abstract": "Microaggregation is a method to coarsen a data set, by optimally clustering data points ingroups of at least k points, thereby providing a k-anonymity type disclosure guarantee foreach point in the data set. Previous algorithms for univariate microaggregation had a O(kn)time complexity. By rephrasing microaggregation as an instance of the concave least weightsubsequence problem, in this work we provide improved algorithms that provide an optimalunivariate microaggregation on sorted data in O(n) time and space. We further show thatour algorithms work not only for sum of squares cost functions, as typically considered, butseamlessly extend to many other cost functions used for univariate microaggregation tasks.In experiments we show that the presented algorithms lead to performance improvementson real hardware.",
  "Introduction": "Public data, released from companies or public entities, are an important resource for data science research.Such data can be used to provide novel types of services to society and provide an essential mechanism forholding public or private entities accountable. Yet, the benefits of publicly released data have to be carefullytraded off against other fundamental interest such as privacy concerns of affected people (Aggarwal & Yu,2008). A common practical solution to circumvent these problems is thus to reduce the resolution of thecollected raw data, e.g., via spatial or temporal aggregation, which leads to a coarse-grained summary of theinitial data. Microaggregation (Defays & Anwar, 1998; Samarati, 2001; Domingo-Ferrer & Mateo-Sanz, 2002; Domingo-Ferrer & Torra, 2005) is a method that aims to provide an adaptive optimal aggregation that remainsinformative, while providing a certain guaranteed level of privacy. Specifically, in microaggregation we aimto always group at least k data items together onto a common representation. For instance, for vectorial data,we may map data points to their centroid values, while ensuring a cluster size of at least k points. The foundcentroid values of the clusters may thus be released and used as aggregated representation of the original data.Clearly, for the aggregated data to remain useful for further analysis, it should remain closely aligned withthe original data according to some metric relevant for the task at hand. Hence, in microaggregation we seekto minimize a distortion metric of the aggregated data, while respecting the minimal group size constraint.This minimal group-size constraint (at least k data-items per group), which ensures a guaranteed minimallevel of privacy within each cluster, is a major difference to standard clustering procedures such as k-means,which simply aim to obtain a low-dimensional representation of the observed data in terms of clusters. Microggregation tasks are usually formulated as discrete optimization problems, where low costs indicate highsimilarity among the data items in each cluster. Similar to many other clustering problems, microaggregationis in general an NP-hard problem (see Oganian & Domingo-Ferrer (2001) or appendix B.1). A specific form",
  "Published in Transactions on Machine Learning Research (10/2024)": "Regularized UM problem The usual UM problem forbids any clustering with too few entries, which might beoverly restrictive when k is larger. In such cases it might be worth considering a regularized UM problem, i.e.,we dont entirely forbid invalid clusterings but instead return an additional cost when the minimal clustersize constraint is violated. This can for example be achieved by defining the regularized cluster cost as",
  "Contributions Our main contributions are as follows (see for a visual representation)": "We show that for ordered minimizable cost functions UM can be solved in O(n2), improvingprevious results with exponential runtime. We achieve this by rephrasing UM as a least weightsubsequence problem (Wilber, 1988; Wu, 1991). We show that for ordered minimizable cost functions, which in addition have a splitting is beneficialproperty, this can be improved to O(kn). This includes many popular cost functios based on the1 norm (sum of absolute errors), 2 norm (sum of squared errors), (maximal error) norm, andround up/down cost functions. Finally, we show that for ordered minimizable cost functions which are concave, algorithms with O(n)time and space are possible. These findings apply to above mentioned examples of p norm-basedcost functions. In our presentation we focus on three key conceptual ideas: ordered minimizable, splitting is beneficial,and concave costs. While these concepts are already implicitly present in the literature, they are typically notmade explicit, which can make it difficult to relate the underlying algorithmic ideas to each other. Makingthese concepts explicit, enables us to unfold a natural problem hierarchy within the associated univariatemicroaggregation formulations, which leads us to the design of two algorithms that use all three concepts:the simple+ algorithm and the staggered algorithm. These algorithms have a faster runtime compared withprevious algorithms for the UM task. Lastly, we provide several practical improvements specific to UMthat allow for (i) algorithms that run empirically faster in practice and (ii) substantially decreased impactof floating point errors on the resulting clustering. Overall, our work thus enables to robustly compute",
  "optimal UM-clusterings for various cost functions and large values of n and k. An implementation of thepresented ideas is available at and the code can also be foundat": "Outline In we first outline definitions associated with UM and the related least weight subsequence(LWSS) problem. In we then show that for ordered minimizable cost functions, we can reformulateUM as LWSS problem which allows for O(n2) algorithms. For cost functions with the splitting is beneficialproperty, this runtime complexity can be improved to O(kn). For concave cost functions this can be futherimproved to O(n). Closing we introduce a simpler and empirically faster O(n) algorithm for UM.In we present considerations for running UM on real hardware. First, we present two strategies toavoid issues arising from finite float precision. Second, we illustrate a small algorithmic trick to use fewerinstructions when computing the cluster cost, which is the main computation carried out within UM. Weverify that the presented theoretical considerations lead to significant empirical performance improvementsin .",
  "Related Work": "The question of how to publicly release data, while retaining certain levels of privacy, has become increasinglyimportant in recent years. Privacy preservation for data sets is most relevant if data entries contain bothpublic as well as sensitive private information. Note that what information is considered public and privatecan depend on the application scenario. For instance, we may have tabular data, where each row containspublic information such as name or age, in conjunction with private information such as medical diagnoses.In this case, an important objective is to design a surrogate data set, such that it is virtually impossible touse public information to deduce private information about an individual. The public attributes of a database can usually be split into identifiers and quasi-identifiers. To privacy-harden a database it is typically insufficient to merely remove identifiers such as name or social securitynumber. It can often remain possible to identify an individual in the data set by a combination of quasi-identifiers (Rocher et al., 2019), e.g., there is only one 30 year old female in the data. Thus other privacyprotection measures need to be employed. To combat the leakage of sensitive information, different privacy concepts such as l-diversity (Machanava-jjhala et al., 2007), t-closeness (Li et al., 2007), and the popular k-anonymity (Samarati & Sweeney, 1998;Samarati, 2001; Sweeney, 2002) have been proposed. The latter is the privacy concept we are concernedwith in this work. If a data set is k-anonymous, it is not possible to use any combination of quasi-identifiersto narrow down the set of individuals described by those quasi-identifiers to less than k individuals. Becausethere will always be k 1 other rows indistinguishable by quasi-identifiers alone, it remains impossible todeduce which data item corresponds to a specific identity, no matter how much knowledge about the quasi-identifiers of an individual is available. Unfortunately, data sets are usually not inherently k-anonymous butthe released data needs to be altered to become k-anonymous distorting the statistics of the released data.",
  "Problem Formulation and Preliminaries": "In univariate microaggregation (UM) we consider a set of scalar data points xi R for i = 1, . . . , n. To keepthe treatment general we allow for repeated elements xi, i.e., we consider our universe = {{x1, . . . , xn}} tobe a multi-set of points. The goal of UM can now be formulated as follows. Definition 1 (Univariate Microaggregation problem). Given a multi-set of scalars = {{x1, . . . , xn}}, finda non-empty partition of the points into clusters Xj, such that each cluster has size at least k and a totaladditive cost function TC({{X1, . . .}}) := j C(Xj) is minimized, where C denotes a cost function thatmeasures the distortion within each cluster. Formally, we look for a partition, i.e., a split of the multi-set into nonempty multi-sets Xi = , such thatni=1 Xi = . To avoid confusion, in this work the union of two multi-sets is an additive union, i.e., themultiplicities of the elements add up when forming the union of two multi-sets. Notice, that there might bemultiple partitions minimizing the total cost even if all xi are unique. This is usually not a problem as weare typically interested in finding any partition minimizing the total cost. A brute-force approach, neglecting possible structure in the cost function C needs at least exponential time,as all possible partitions have to be assessed. However, two important observations have been employedin the literature to lower this complexity. First, for many cost functions it can be shown that a minimalcost partition has no interleaved clusters (see Corollary 7 or Domingo-Ferrer & Mateo-Sanz (2002) for thespecial case of SSE). This substantially restricts the search space that needs to be explored in order to findan optimal solution, as it implies that only pairwise-ordered clusterings need to be considered. From nowon we thus assume that data points xi are sorted in ascending order, i.e. i < j xi xj. For a given costfunction C, this allows us to denote by C(i, j) := C({{xi+1, . . . , xj}}), the cost of a cluster with starting pointxi+1 and endpoint xj. Second, for many cost functions, there always exist an optimal cluster assignment inwhich no cluster has size larger than 2k 1, i.e., it is never detrimental to split clusters of size at least 2kinto smaller clusters (see eq. 2 or Domingo-Ferrer & Mateo-Sanz (2002) for the special case of SSE). Thisalso reduces the size of the search space, as there is no need to check possible clusterings containing clustersof size greater 2k 1.",
  "The Least-Weight Subsequence Problem": "As we will see in the next sections, univariate microaggregation can actually be phrased such that establishedalgorithms which solve the least-weight subsequence problem can be applied to UM. Hence, we briefly recallthe relevant aspects of the least-weight subsequence problemDefinition 2 (Least weight subsequence problem (Wilber, 1988) ). Given an integer n and a cost functionC(i, j), find an integer q 1 and a sequence of integers 0 = l0 < l1 < < lq1 < lq = n such that the totalcost TC = q1i=0 C(li, li+1) is minimized.",
  "A Standard Algorithm (Wilber, 1988)": "To gain some intuition about how to solve the least-weight subsequence problem, we first outline an algorithmwith an O(n2) time, O(n) memory requirement. To this end we introduce the auxiliary variables mij, whichis the sum of the cost obtained after solving the LWSS problem for the points x1 . . . xi plus the cost ofC(i, j) := C({{xi+1, . . . , xj}}). We call mij the conditional minimal cost. More specifically, m0j := C(0, j) issimply the cost of considering the first j points to be in a single cluster. The remaining conditional minimalcosts are then recursively defined as mij := minl<i mli + C(i, j). We notice that minl<i mli is the minimaltotal cost required to solve the least-weight subsequence problem on the points x1 . . . xi, and so mij can beinterpreted as the sum of the costs of (i) assigning the first i points optimally and (ii) assigning the points{{xi+1, . . . , xj}} to a single cluster. To efficiently compute all mij we make use of a lookup array A which we fill with the relevant mij entriesas follows: We first compute m01 and store it in array A at position 1. Next, we compute the conditionalminimal costs m02 and m12. To compute m12 we read the entry m01 from our lookup array A at position1. We store the minimum of m02 and m12 in array A at position two. We can continue in the same wayiteratively. In each iteration we increase j by one and compute all the conditional minimal costs mij withi < j. Each mij requires the value minl<i ml,i which we can read from our array A at position i. At theend of each such iteration we store mini mij in our array A at position j such that it is available for furthercomputations. The algorithm terminates once we have found mini min which corresponds to the minimaltotal cost. Overall, this takes O(n2) time if we can compute the cost function C in O(1). In univariate microaggregation we are usually more interested in a minimal cost clustering rather than theminimal cost itself. This can be achieved by slight adaptation of the above algorithm. Whenever we computemini mij we also store the index i = arg mini mij of the minimum in a linear array Aind at position j. The fullpseudo code for this algorithm can be found in Alg. 2. The result of this standard algorithm implicitly holdsthe information about a minimal cost clustering which can be obtained by backtracking in O(n). Startingwith j = n we consider imin = arg mini mij which we can read from our array Aind at position j. All pointswith index l with imin < l j belong to one cluster in the optimal costs clustering. If imin > 0 we repeatthe same process with j = imin to obtain the next cluster. Full pseudo code of this backtracking procedureis provided in Alg. 3 in the appendix.",
  "Note that the above equation is effectively an opposite of the triangle inequality (which would hold forso-called convex cost functions)": "Most important for solving least weight subsequence problems is the consequence that the associated matrixC with entries Ci,j = C(i, j) is a Monge matrix (where for notational convenience we index the rows startingfrom zero and the columns starting from 1). Most relevant for our purposes is that Monge matrices andtheir transpose are totally monotone (Burkard et al., 1996), i.e., it holds for each 2 2 submatrix that < implies that < and, similarly, we have = . It is easy to see that if we add aconstant value to a column of a totally monotone matrix, the resulting matrix is again totally monotone.This implies that the transposed conditional minimal cost matrix MT with entries Mij = mij is totallymonotone, as it is obtained from the transposed cost matrix CT (which is Monge) by adding constantvalues (minl mli) to the columns. Finding all minima within the rows of implicitly defined totally monotonematrices of shape n m (n m) is possible in O(m) using the SMAWK algorithm (Aggarwal et al., 1987). Using the SMAWK algorithm, dynamic programming (Wilber, 1988; Galil & Park, 1990) allows us to solvethe concave least weight subsequence problem in O(n) if the concave cost function C can be computed inO(1) time using O(n) time for preprocessing. While the works (Wilber, 1988; Galil & Park, 1990) focus onconcave cost functions C, we notice that the presented algorithms in (Wilber, 1988; Galil & Park, 1990) willalso work if the corresponding transposed cluster cost matrix MT is only totally monotone. As we will seein the next chapter, univariate microaggregation for concave cost functions can be phrased as a least weightsubsequence problem with an non-concave adapted cost function Cadapt.The corresponding transposedcluster cost matrix MT is nonetheless totally monotone which allows for the algorithms from (Wilber, 1988;Galil & Park, 1990) to be applied to univariate microaggregation.",
  "Faster Univariate Microaggregation": "In the following we present what properties of the cost function can be used to efficiently solve the univariatemicroaggregation problem. We first reformulate UM as a least weight subsequence problem (LWSS) whichallows for O(n2) algorithms. We call cost functions which allow this reformulation ordered minimizable.If cost functions additionally also have the splitting is beneficial property, O(kn) algorithms are possible.For the very common type of concave cost functions, we show that UM can be solved in O(n). Lastly, wepresent an O(n) algorithm, the staggered algorithm, which shows faster empirical running for the concaveUM problem compared to more generic O(n) LWSS algorithms presented in (Wilber, 1988; Galil & Park,1990).",
  "Reformulating Univariate Microaggregation as a Least Weight Subsequence Problem": "When comparing UM and LWSS, the most apparent issue is, that in the LWSS problem the values areinherently ordered while for UM no such inherent order is apparent for all cost functions. We will thusconsider properties of cost functions that imply an inherent order. We will now characterize those cost",
  "Equipped with this definition we can characterize those cost functions whose UM problem can be reformulatedas a least weight subsequence problem": "Definition 4 (Ordered minimizable). We call a total cost TC ordered minimizable if it is sufficient toconsider only pairwise ordered partitions when minimizing the total cost TC over all relevant1 partitions ofthe universe . To grasp the importance of definition 4, lets consider the case of minimizing an ordered minimizable totalcost for a bipartition. If there are n elements there are n1 pairwise ordered partitions splitting the data intwo parts, but there are 2n 2 partitions in total. So being able to only check all pairwise ordered partitionsmakes a huge computational difference.",
  "While Definition 4 is probably the least restrictive definition, most widely used cost functions fulfill the morerestrictive following definition which mixes both the UM and k-means restrictions": "Definition 5 (q-partition ordered minimizable). A total cost function TC is q-partition ordered minimizableiff for all partitions P = {{X1, . . . , Xq}} with multi-sets of cardinalities SP := {{|Xi| for i {1, . . . , q} }}there is a pairwise ordered partition P consisting of multi-set with cardinalities SP = SP that fulfillsTC(P ) TC(P).",
  "Theorem 6. If a total cost function is 2-partition ordered minimizable, then it is ordered minimizable": "In the proof of Theorem 6 we show that if a cost function is 2-partition ordered minimizable it is q-partitionordered minimizable for all q.This implies that it is ordered minimizable.For the full proof see Ap-pendix B.5.1 in the appendix.Using Theorem 6, it can be shown that the following cost functions areordered minimizable.",
  "xX |x min(X)|": "For the proof of Corollary 7 see Appendices B.5.3 to B.5.6.The sum of absolute error cost has beenpreviously proposed under the name absolute deviation from median (ADM) (Kabir & Wang, 2011). Forthe scenario in which all values xi are integers, a sum-of-squares distortion metric with rounded mean wasalso proposed (Mortazavi, 2020) to be able to perform all operations on integers only. When dealing with any ordered minimizable cost function, we thus can sort the input data either in ascendingor descending order (ascending is usually preferred for numerical reasons). Depending on the data, sortingcan be done in O(n log n) or O(n) (e.g. radix-sort on small integers). Currently, sorting is part of anypractical algorithm to solve UM exactly and we thus consider the time complexity of all algorithms onsorted data, thereby neglecting the potential additional cost of O(n log n).",
  "VALj i < k,C(i, j)else.(3)": "Here VAL is an arbitrary large enough number, such that the cluster cost C is always smaller than VAL,i.e., C(i, j) < VAL for all valid choices i < j. The following lemma ensures that we can use this adaptationfor all previously considered cost functions.Lemma 8. The cost functions introduced in corollary 7 can be computed in O(1) using O(n) time forpreprocessing and consuming no more than O(n) additional memory.",
  "Univariate Microaggregation for Cost Functions where Splitting is Beneficial": "If the ordered minimizable cost function also has the splitting is beneficial property (i.e., eq. 2 holds), wecan improve upon the O(n2) time complexity for the classical algorithm. The main insight is that we donot need to consider clusters that contain less than k or more than 2k 1 points. We can forbid theseclusterings by adapting the normal cluster cost. For too small/too large clusters we return large costs whichensure the forbidden assignments are never selected. Thus we adapt any cost function with the splitting isbeneficial property as",
  "VALj i < k,VALj i 2k,C(i, j)else.(4)": "Similar to LWSS we can define a matrix M with entries Mi,j = minl<i mli + Cadapt(i, j). If we consider thestructure of the M matrix (see colors in ), we find that there are only k entries per column thatneed to be considered (green entries in ). In particular, when computing the minimum value in eachcolumn of the matrix M only entries Mij with j 2k + 1 i j k are necessary to be computed tofind the minima of column j. This allows a complexity improvement of the classic algorithm from O(n2) toO(kn). We conclude this subsection by noting the introduced considerations are applicable to the presentedcost functions.Corollary 9. All the costs presented in Corollary 7 have the splitting is beneficial property.",
  "Univariate Microaggregation for Concave Cost Functions": "Besides the classical algorithm used to solve least weight subsequence problems, more advanced algo-rithms (Wilber, 1988; Galil & Park, 1990) are available to solve concave least weight subsequence problemsin O(n). We observe that those algorithms will also work if the associated cluster cost matrix MT is onlytotally monotone. The previous definition of Cadapt (eq. (3)) needs to be slightly modified such that thematrix MT is totally monotone for these algorithms to be applicable.",
  "VAL(i)j i < kVAL(i)j i 2kC(i, j)else(6)": "As before VAL(l) should be large enough such that C(i, j) < VAL(l) for all valid i, j, but this time not onlyfor positive l, but also for negative l with values beteen n l n. We require again that VAL(l) < VAL(m)for l < m to ensure total monotonicity of the transposed cluster cost matrix MT . We proove that for thisdefinition of the adapted cost Cadapt the transposed cluster cost matrix MT is totally monotone in appendixAppendix B.2.2. The structure of the cluster cost matrix correspond to this definition of Cadapt is displayedin . When a cost function has both the splitting is beneficial property and is concave, it is possible to significantlydecrease the runtime of the classical algorithm introduced in .2. To this end it is instrumental toobserve that in totally monotone matrices, the positions of the minima of the rows are non decreasing (seeAlg. 2 for pseudo-code). Although this does not provide an asymptotic runtime improvement, it greatlydecreases empirical runtime for larger k (see ). By incorporating the cluster size constraints, it is moreover possible to improve the generic algorithms forleast weight subsequence problems (Wilber, 1988; Galil & Park, 1990) for concave cost functions with thesplitting is beneficial property. We present an algorithm for this problem in the next subsection.",
  "return SMAWK.ArgminTotalCost": "Algorithm 1: Pseudo code for the staggered algorithm. This algorithm uses both the concavity of thecost function through the use of the SMAWK algorithm and the restrictions, that clusters are of size atleast k and at most 2k 1. When calling SMAWK.col_min(i,j,k,l), the SMAWK algorithm is appliedto compute the column minima of a submatrix of the implicitly defined cost matrix M which containscolumns i through j and rows k to l (see boxes in ). After executing the SMAWK algorithm,the minimal total cost for each column is stored in MinTotalCost and the row that corresponds to thatvalue is stored in ArgminTotalCost.",
  "Univariate Microaggregation on real hardware": "So far we have considered the mathematical foundations underlying fast UM algorithms. In practice though,the computations involved are not happening at arbitrary precision. On most current hardware, calculationsinvolving reals are executed with fixed width floating point numbers (e.g. 32 or 64 bits). We notice thatutilizing the presented algorithms/cost functions can result in suboptimal clusterings, due to finite precisionof these floating point numbers. We thus dedicate the next two subsections to the analysis and mitigationof errors caused by finite precision floating point numbers. At the end of the section we also highlight a realworld runtime improvement possible for some of the cost functions. As a running example we will considerthe sum of squares cost function, though many of the considerations carry over to other cost functions aswell.",
  "Floating Point Errors During Preprocessing": "When computing the cumulative sum during preprocessing, the running sum can grow and reach quantitieswhich are no longer represented well numerically. This can lead to erroneous cluster cost being calculatedwhich in turn may lead to suboptimal clusterings. As an example, computing the sum of squares erroraccording to eq. (7) on the integers (i.e. the universe is = {0, 1, 2, . . . }) using 64bit floats returns thewrong result SSE(300 079, 300 082) = 1 (which should be 2). We can do better than the simple cumulativesum approach, by observing that we only need to consider C(i, j) with j i 2k 1 for UM. This meanswe dont need the full cumulative sum and can get away with sums of fewer elements.",
  "When computing the total cost function TC we are computing the sum of at least n": "2k1 values with equalsign, thus TC is increasing in magnitude with each summand. Yet, floating point numbers have a higherabsolute resolution close to zero. It would thus be numerically advantageous if we could keep the total costTC close to zero. For cost function with the splitting is beneficial property this can be achieved withoutincreasing the runtime complexity by regularly resetting the stored TC values. Most algorithms work byfirst computing and storing the minimal total cluster cost TCmin(q) := mini miq of the first q points (see.1 for the definition of mij). Then the algorithms find the optimal clustering of the next q + ppoints based on the optimal clustering of those previous values up to q. We notice that TCmin(l) increasesas l increases but for cost functions where splitting is beneficial we only need to know the last 2k 1 totalcost values, i.e., we need to know TCmin(q) for q l (2k 1) to compute the total cost TCmin(l). Thus forcertain algorithms we can reduce the growth of the total cost as follows. On a high level, we subtract thesmallest stored and still needed minimal total cost from the other stored and still needed minimal total costvalues at regular intervals. As an example, consider the classical algorithms simple and simple+ with theirpseudocode in Alg. 2. To implement the strategy, we could expand the loop spanning lines 7-9 in Alg. 2:",
  "MinCost[j] = MinCost[j] - MinNeededMinCost": "Here we subtract the still needed minimal total cost MinNeededMinCost from those MinCost values thatare still needed in future iterations. The values that are still needed in future iterations of the main loop aredesignated mostly by the lower bound imin. This lower bound differs depending on whether we consider thesimple or simple+ algorithm. It is possible to do a similar procedure for the staggered algorithm withoutchanging the runtime complexity of either algorithm. Using such a procedure, we can ensure that the totalcost TC doesnt grow as a function of n 2k1 but remains nearly constant (in the case of simple/simple+) orgrowth as a function k in the case of the staggered algorithm. Thus for small k and large n the describedprocedure allows us to make use of the higher absolute floating point resolution near zero.",
  "runtime [s]": ": Runtime impact of different methods to compute the cost functions. Colors indicate algorithms,linestyles indicate methods to compute the cost functions. Shown are mean and standard deviation aggre-gated over 10 runs. The colors red/blue indicate the staggered/simple+ algorithm respectively (same colorsas ). The dashed lines are the default cumulative sum approach (eq. 7), the solid lines are the partialcumulative sum method introduced in section 4.1, and the dotted lines indicate the alternative cost functionapproach introduced in section 4.3. We see that overall for the same algorithm the alternative cost functionis fastest with the default cumulative sum approach being not much slower. The partial cumulative sumapproach comes with a big runtime penalty.",
  "Runtime Experiments": "We compare the algorithms by Galil and Park, Wilbers algorithm, and two versions of a simple dynamicprogram, one which does only use the restrictions on cluster sizes (simple) and another one which additionallyuses that the row minima of the M T matrix are non decreasing (simple+). All the methods were implementedin python and compiled with the numba compiler. We additionally include the time to sort the initiallyunsorted data of the same size for comparison. We generate our synthetic data set by sampling one millionreals uniformly at random between zero and one. For low values of the minimum group size k the simple dynamic programs are faster than the O(n) algorithms,with the simple+ algorithm outperforming the simple algorithm in terms of computation time. These simplealgorithm become slower than the more complex dynamic programs when the minimum group size k exceeds250. When comparing the O(n) algorithms, the staggered algorithm is overall about 20 percent faster thanother algorithms. Overall, we see that the algorithms including both the concavity of the cost function andthe minimum / maximum group size constraints, i.e., the simple+ and staggered algorithm, are faster thanthose algorithms that dont include both constraints.",
  "(b) Dataset Tarragona": ": Reconstruction Error for the Multivariate Microaggregation task on real world datasets. We showthe Reconstruction Error or loss (lower is better) for the SSE cost function for different values of minimumgroup size k. We compare four different approaches, two projection based approaches PCA and randomprojection as well as two approaches designed for the multivariate case the kmeans based approach and anearest-neigh based approach by (Domingo-Ferrer et al., 2008). For the PCA and random approach, weuse a vector to project the multivariate task into one dimension and solve the univariate microaggregationtask with the methods introduced here. For the random projection approach one run consist of making 10,50, and 100 random projections (as indicated in the legend) and taking their minimum. For the k-means+ cleanup based approach, we first performed k-means where the number of clusters k is f dataset_size/kwhere f is a factor of 0.5, 1 or 2 as indicated in the legend. To make sure that the obtained data fulfills theminimum group size constraint we merge clusters of to small size with the closest clusters until all clusters arelarge enough. For the random, nearest-neighbor and k-means based approach we show means and standarddeviation over 10 runs. On the EIA dataset (a), the k-means based approaches outperform the projectionbased approaches by a large margin. The nearest-neighbor based approach is performing on par with thek-means based approach for low k but gets increasingly worse with increasing k. On the Tarragona dataset(b) the picture is not as clear. For low values of k 5 the k-means based approach is better or equally goodas the projection based approaches while for larger values of k the projection based approaches show lessreconstruction error. The PCA based approach is usually slighlty worse than the random projection basedapproach. The nearest-neighbor approach on par with the k-means based approach for k less than 5 but itgets increasingly worse with increasing values of k. In we compare the different ways of computing the cost functions exemplary for the simple+ andstaggered algorithm. Within the same algorithm we find that the method from section 4.3 is fastest with thedefault cumulative sum (eq. 7) being a close competitor. The numerically more stable method introducedin section 4.1 is about four times slower than the default method.",
  "Solving Multivariate Microaggregation through projection": "In practice one is frequently met with Multivariate Microaggregation problems, i.e.there are multipledescriptors that need to be aggregated simultaneously.In this section we compare approaches designedfor the multivariate problem with approaches which work by projecting the multivariate problem into 1d,solving the univariate problem with the methods discussed here and then project the solution back to themultivariate setting. The most simple approach to project the multivariate data into one dimension is to use principal componentsanalysis (PCA). PCA identifies the axes of maximal variance. We then use the axis with maximal varianceto project the multivariate data into a single dimension, we then solve the 1d problem to obtain the clusterassignment. A similar projection based approach uses random projections, i.e. we sample random vectors",
  "Conclusions": "We considered the problem of univariate microaggregation. We characterized properties of cost functions thatlead to complexity improvements over the naive algorithm with exponential runtime. By mapping univariatemicroaggregation to a least weight subsequence problem we showed that UM for ordered minimizable costfunctions is solvable in O(n2). If splitting is beneficial, a maximum group size constraint allows to improvethis complexity to O(kn). For ordered minimizable, concave cost functions a different strategy allows to findan optimal solution to univariate microaggregation in O(n) time and space on sorted data. These resultsapply to many popular cost functions such as sum of squares, sum absolute error, maximum difference aswell as round up/down costs. By incorporating both a maximum group size constraint and the concavityof cost functions, we are able to provide an improved O(kn) algorithm (simple+) and an improved O(n) 2The definition of the algorithm is not sound unless this is assumed, even though this is not explicitly mentioned in theoriginal source. In spirit of the presented algorithm we considered the below highlighted overlapping case.3For the overlapping case we assume that there are not only the overlapping sets Xi used to compute the cluster centroidsbut for each of them a set Yi Xi exists which highlights the non-auxillary nodes in Xi. The Yi are pairwise disjoint andunify to . We obtain the reconstruction error the following way. For each element we find the only set Yi it is contained inand compute the (squared) distance to the centroid of the correspond set Xi. To obtain the final reconstruction error we sumup those distances for all elements. This naturally captures a slightly different notion of k-privacy as the elements of the newdataset are obtained by aggregating at least k elements of the original dataset.4",
  "algorithm (staggered) which demonstrate faster empirical runtime in comparison to other algorithms in theirrespective complexity class": "Limitations In practice one is frequently met with multivariate microaggregation (MM) problems and oneof the heuristics used to solve MM problems is to devise an order on the points (see (Mortazavi, 2020)and references therein). The resulting ordered multivariate microaggregation problem is sometimes referredto as a univariate microaggregation problem even though it really is a least-weight subsequence problemwith multi-dimensional cost functions. These multi-dimensional cost functions may no longer be concave asthe concavity of the cost functions relies on the sorted (i.e. arranged in increasing/decreasing order ratherthan any fixed but arbitrary order) 1D nature of the points. Thus the presented O(n) algorithms are notapplicable to such ordered multivariate microaggregation problems. As the simple O(kn) algorithm does notrely on the concavity of cost functions, it can still be used to solve the ordered multivariate microaggregationproblem if a multi dimensional equivalent of the splitting is beneficial property holds. It is well known, that the privacy concept of k-anonymity may provide insufficient protection if for examplethe confidential attributes are not sufficiently diverse.Caution should thus be taken if k-anonymity, ormicroaggregation to achieve k-anonymity, is applied in practice. For practical anonymity guarantees, theconcept of differential privacy (Dwork et al., 2006; Dwork, 2008) provides a more robust protection. While k-anonymity relies on the intuition hidden in a crowd of size at least k, the key idea of differentialprivacy is plausible deniability, i.e. whether or not any individual is included in the data cannot be saidwith certainty. To achieve differential privacy guarantees, usually the output of an algorithm is sufficientlyperturbed.Li et al. (2012) showed that using k-anonymity, it is possible to essentially perform inputperturbation and still achieve differential privacy if the k-anonymity type algorithm is augmented with twosteps. First, the k-anonymity algorithm needs to be -safe: Intuitively this means the k-anonymity algorithmneeds to output non-optimal clusterings with nonzero probabilities. Second, k-anonymity is applied to asubsample from the original dataset instead of the full dataset (for the full details see (Li et al., 2012)). Concave costs where splitting is not beneficial The majority of cost functions considered are concave and havethe splitting is beneficial property which allows all the presented algorithms to be applied. Yet, there areconcave cost functions which do not have the splitting is beneficial property. As a simple example consideradding a constant group cost to a concave cost function C which also has the splitting is beneficial propertyto penalize having too many clusters (i.e., we obtain the adjusted cost function C(i, j) := C(i, j) + ). Thisadjusted cost function is still concave but no longer has the splitting is beneficial property. Thus the genericO(n) algorithms (Wilber, 1988; Galil & Park, 1990) may still be used to solve UM problems for the modifiedcost function C while the O(kn) algorithms (simple, simple+) and the staggered algorithm are not applicable. Relation to k-means The definitions presented here also allow to have a more fine grained understandingof other 1D clustering problem with different cost functions. As an example let us consider the k-meansproblem which restricts the number of clusters to be exactly k. An existing O(kn2) algorithm (Grnlundet al., 2018) solves the 1D k-means problem if the cost function considered is ordered minimizable. If thecost is additionally also concave, O(kn) and O(n log U)6 algorithms exist (Grnlund et al., 2018).Thesplitting is beneficial property, while not explicitly used in algorithms, seems to be implicitly used whenformulating the k-means problem. The most popular formulation of the k-means problem asks to minimizethe clustering cost subject to exactly k clusters, while minimizing costs subject to at most k clusters couldalso be thinkable. However, these two formulations will result in identical optimal clusterings if the costfunction has the splitting is beneficial property. Mixed UM and k-means scenario Another way to avoid using overly many clusters in a UM setting could beto impose a maximum number of clusters constraint in addition to the usual minimal group size constraint.This would correspond to a mixed UM and k-means scenario. We implicitly showed that the consideredcost functions are ordered minimizable even in a mixed UM and k-means scenario, as cost functions that areq-partition ordered minimizable also encompass this mixed scenario. Hence, algorithms used to solve the 1Dk-means problem (see (Grnlund et al., 2018) for many such algorithms) work correctly in the mixed UMand k-means scenario if we adapt the cost function as explained in eq. (3).",
  "(k (j i))j i < k0else": "with regularization parameter 0. By adjusting the regularization parameter to be large we can makeit more and more unlikely that the cluster size constraint is violated. If C was concave, the regularizedUM problem may also be minimized by the generic concave algorithms (Wilber, 1988; Galil & Park, 1990).Similarly, if splitting was beneficial for C, the regularized UM may be solved by a slightly adjusted7 standardalgorithm in O(kn).",
  "Josep Domingo-Ferrer, Francesc Seb, and Agusti Solanas. A polynomial-time approximation to optimalmultivariate microaggregation. Computers & Mathematics with Applications, 55(4):714732, 2008": "Cynthia Dwork. Differential privacy: A survey of results. In Manindra Agrawal, Dingzhu Du, ZhenhuaDuan, and Angsheng Li (eds.), Theory and Applications of Models of Computation, pp. 119. Springer,2008. ISBN 978-3-540-79228-4. doi: 10.1007/978-3-540-79228-4_1. Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to sensitivity in privatedata analysis. In Shai Halevi and Tal Rabin (eds.), Theory of Cryptography, pp. 265284. Springer, 2006.ISBN 978-3-540-32732-5. doi: 10.1007/11681878_14.",
  "Md Enamul Kabir and Hua Wang. Microdata protection method through microaggregation: A median-basedapproach. Information Security Journal: A Global Perspective, 20(1):18, 2011": "Ninghui Li, Tiancheng Li, and Suresh Venkatasubramanian. t-closeness: Privacy beyond k-anonymity andl-diversity.In 2007 IEEE 23rd International Conference on Data Engineering, pp. 106115, Istanbul,Turkey, 2007. IEEE. Ninghui Li, Wahbeh Qardaji, and Dong Su.On sampling, anonymization, and differential privacy or,k-anonymization meets differential privacy. In Proceedings of the 7th ACM Symposium on Information,Computer and Communications Security, ASIACCS 12, pp. 3233. Association for Computing Machinery,2012. doi: 10.1145/2414456.2414474. Ashwin Machanavajjhala, Daniel Kifer, Johannes Gehrke, and Muthuramakrishnan Venkitasubramaniam.l-diversity: Privacy beyond k-anonymity. ACM Transactions on Knowledge Discovery from Data (TKDD),1(1):3es, 2007.",
  "return Argmin[1:n]": "Algorithm 2: Pseudo code for the classical algorithm used to solve least weight subsequence problems.If this algorithm is used to solve univariate microaggregation with ordered minimizable cost functions,the constraint on the arg min in line 8 becomes 0 i < j k. If splitting is beneficial the constrainton the arg min in line 8 is max(0, j 2k + 2) i j k (this is the simple algorithm). Lastly, forconcave cost functions that also have the splitting is beneficial property, the bound on the arg min ismax(Argmin[j-1], j-2k+2) i j k with Argmin=0 (this is the simple+ algorithm).",
  "return out": "Algorithm 3: The backtrack algorithm converts an implicit cluster representation array b into anexplicit cluster representation. All the presented algorithms return such an implicit representation. Forexample an implicit cluster array represents the clustering that is the firstand second element are their own cluster, and the remaining elements are in one cluster. The backtrackalgorithm runs in O(n).",
  "B.1Multivariate microaggregation is NP-hard for many cost functions": "It was previously known that multivariate microaggregation is NP-hard for the sum of squares error costfunction. We noticed that the proof provided by Oganian & Domingo-Ferrer (2001) could be easily adaptedfor other cost functions as well. Let li(C), i {3, 4, 5} denote the minimal cluster cost for cost function C computed for the structures ofsize i outlined in (Oganian & Domingo-Ferrer, 2001). For a (multi)set X = {{x1, . . . , xp}} with xi Rd letus denote with X(i) = {{x1 ei, . . . , xp ei}} the multi-set of values projected onto the i-th standard basisvector. Theorem 12 (Generalizing the result from Oganian & Domingo-Ferrer (2001)).Microaggregation forcost functions C(d)(X) := di=1 C(X(i)) is NP-hard in dimensions d 2 and k = 3 if splitting is beneficialfor cost function C, and the ratios of minimal structure cost are l4(C)",
  "We then notice that now C(b, c) C(b, d) < 0 C(a, c) C(a, d) < 0. If we now consider the matrix Cwith entries Cij = C(i, j), all 2 by 2 submatrix of C have the formC(a, c)C(a, d)C(b, c)C(b, d)": "for suitable choice of of the integers a < b, c < d. To check for monotonicity we see that C(b, c) < C(b, d)which is equivalent to C(b, c) C(b, d) < 0 which by our previous equality implies C(a, c) C(a, d) < 0 C(a, c) < C(a, d). That means that any 2 by 2 submatrix is totally monotone. Thus the entire matrix istotally monotone.",
  "B.2.2Proof that MT is totally monotone": "We prove total monotonicity of MT by showing that all 2 by 2 submatrices fullfill the necessary equations.We never explicitly consider a submatrix but we choose two entries in the same row but different columns(col1 < col2). We then compare the values of MT at those locations (val1:= MTrow,col1, val2:= MTrow,col2).Depending on the size of the values, this imposes requirements on either the values below (in case of val1 >val2) or the entries above (in case of val1 val2). If row is such a row either above or below, we refer tothe entries in the same columns as before as val1:= MTrow,col1, val2:= MTrow,col2. MT of the adapted cost in eq. (5) Instead of referring to explicit values, we refer to the values by theircolors. If the cluster corresponding to the entry is too small, the color is red else the color is green (theseare the two cases in eq. (5)). Case val1=green, val2 = green: val1 val2: The pairs above are either green-green, green-red, orred-red. For green-green pairs monotonicity is guaranteed by eq. (1). This implies total monotonicityas the transpose of monge matrices are also monge Burkard et al. (1996), monge matrices are totallymonotone Burkard et al. (1996) and if you add constant values to the columns of totally monotonematrices they remain totally monotone Burkard et al. (1996). For green-red, and red-red pairs bydesign val1 < val2. Overall values above col1 are less than or equal to those in col2. val1 > val2:The only case if green-green which is correct by monotonitcity.",
  "Case val1=red, val2=red: The only case here is val1 < val2: all entries above are red-red as well forwhich val1 < val2": "MT of the adapted cost in eq. (6) As before, instead of referring to explicit entries, we refer to theentries by their colors as visible in , i.e. the first case in eq. (6) is red, the second orange and thethird is green. Case val1=green, val2 = green: val1 val2: The cases above are the same as in the previousconsideration.val1 > val2: The pairs below are either green-green, orange-green or orange-orange. For green-greenpairs monotonicity is guaranteed as in the val1 val2 case. For orange-green and orange-orangepairs, val1 > val2 by design.",
  "B.3Computational complexity of computing cost functions": "This section shows, that all the cost functions presented in corollary 7 can be computed in O(1) performingat most O(n) time for preprocessing and using at most O(n) space. Throughout this section we assume thatthe input data is sorted, i.e. i < j xi xj. We also assume that i < j such that Xi,j = {{xi+1, . . . , xj}}is well defined.",
  "s(RABCAB,1) + s(RABB,2) s(RABCBC,2) + s(RBCB,1)": "which is true as s(RABCAB,1) s(RBCB,1) and s(RABB,2) s(RABCBC,2). One can see that the firstexpression is true by increasing the size of A gradually. If |A| = 0 then certainly the lhs and rhs are thesame. If we now add elements to A which are no larger than those in B then the lhs expression will only getsmaller. One can employ a similar trick for the second expression by adding elements to C which no smallerthan those in B, which only increase the rhs expression.",
  "B.5.1Proof of Theorem 6": "Before we start with the actual proof lets provide some clarifications regarding multi-sets. When callingfor the l < |X| smallest elements of a multi-set X, we mean the multi-set S X of size l which fullfillssSx(X\\S) s x. We conduct the proof of theorem 6 in two steps. First we show that q-partition ordered minimizable for all qimplies ordered minimizable. We then show that if a cost is 2 partition ordered minimizable it is 2-partitionordered minimizable for all q.Lemma 14. If a cost function is q-partition ordered minimizable for all q then it is ordered minimizable. We prove lemma 14 by contradiction. Lets assume that it is not enough to consider only pairwise orderedpartitions when finding any partition minimizing a total cost which is q-partition ordered minimizabe forall q. But then there is a partition P (wlog. |P| = q) which minimizes the total cost TC subject to theeither the UM or k-means constraint. But then as the total cost TC is q-partition ordered minimizable weare guaranteed a pairwise ordered partition P with has the same multiset of sizes as P but no larger totalcost. Thus P also respects the UM or k-means constraint. Thus it would be enough to have considered onlypairwise ordered partitions when minimizing the total cost on the relevant partitions, which concludes ourcontradiction.",
  "Proof. Now we show that if a cost is 2-partition ordered minimizable, then it is q-partition ordered mini-mizable for all q by induction on q": "The base case is the assumption. Assume the TC is q-ordered minimizable. Let P = {{X0, . . . , Xq+1}}be a (q+1)-partition.We can construct a pairwise ordered partition P which has no worse TCthan P:Let A1 = X1.From 2-partition ordered minimizable we know that we can obtain setsBi+1, Ci+1 = pairwise_order(Ai, Xi+1) (see corollary 7).We then define Ai+1 := min(Bi+1, Ci+1) andXi := max(Bi+1, Ci+1). Note that Ai+1 contains the smallest elements in i+1j=1 Xj. We can then define thepartitionPi := {{ X1, . . . , Xi1, Ai, Xi+1, . . . , Xq+1}}.",
  "TC({L, R}) TC({Y, Z})(9)": "with |L| = |Y |, |R| = |Z| and L, R being pairwise ordered but this is exactly being 2-partition ordered min-imizable which we have by assumption. Overall we have now found a partition Pq+1 = {{ X1, . . . , Xq, Aq+1}}with TC no worse than the TC of P. Now we consider the minimization problem of TC on \\Aq+1 subject topartitions of size q. By the induction hypothesis we are guaranteed a pairwise ordered partition {{ X1, . . . , Xq}}that minimizes TC on \\ Aq+1. Now we have found a (q+1) ordered partition P = {{ X1, . . . , Xq, Aq+1}}which has the same size multiset and no worse TC than P but is totally ordered (remember the Xi arepairwise ordered and all elements in Aq+1 are no larger than any elements in any of the Xi). This concludesthe induction.",
  "The last line is certainly true": "We will now show, that Cmin is not ordered minimizable. Let || 4 and let 1 < 2 < 3 be the smallest,second smallest, and third smallest element of the universe . Let the multi-set R := \\ {1, 2, 3}. Thenfor A = {1, 3} and B = {2} R be two sets then TC({A, B}) = 1 + 2 but in every ordered partitionwith min(|L|, |R|) 2 we have that 1 and 2 are in the same set. But this means for any pairwise orderedsets L and R with min(|L|, |R|) 2 we have TC({L, R}) 1 +3 > TC({A, B}). Thus Cmin is not orderedminimizable.",
  "A+ A + A+ B B+ A + B+ B A+ B B+ A B+ B A+ A": "Where the the second last line is true if s A < s B. If we can maximize the right hand side of the last expressionwhich is only a function of the partition P1, this is the same as minimizing the TSSE of P1 with respect toP2. The maximum of the rhs is achieved by choosing that B and B contain the largest elements while Aand A contain the smallest elements. This naturally fullfills s A < s B as | A| = | B| (remember A = B). Thuswe conclude that for all 2-partitions P2 we can find pairwise orderd P1 which has no worse TSSE, i.e. TSSEis 2-partition ordered minimizable.",
  "max(A) min(B)if max(B) = rmax(B) min(B)if max(A) = r": "In the first case, we have that max(L) max(A) as L contains the smallest elements and |A| = |L|. Further,min(B) min(R) as R contains the largest elements and |B| = |R|. In the second case we observe that theleft hand side is always non positive while the right hand side is always non negative.",
  "xa1 x +": "xa2 xirrespective of whether |A| is even or odd. Lets consider a partition of the elements in into two sets A, B.Let us further the multisets c1/2 := l1/2 r1/2 and d1/2 := a1/2 b1/2. In a slight abuse of notation letus denote the index (i.e. position in an order of ) of the i-th smallest element in c1/2 as c1/2(i). Thenxc1/2(1) is the smallest, xc1/2(2) is the second smallest element of c1/2 and so on. We similarly do that withthe sets d1/2 (i.e. xd1/2(i) is the i-th smallest element of d1/2). We define L/R as the sets containing thesmallest/largest elements of . The size of L is min(|A|, |B|) if d2(1) max(|a1|, |b1|) + 1 and max(|A|, |B|)otherwise. The size of R is || |A|. Lets us first consider the first |l1| of the sets c1/2 and d1/2. For i |l1| certainly c1(i) = d1(i) (these arethe |l1| smallest elements of ). Further, c2(i) d2(i) as c2(1) d2(1) and the c2 continue consecutivelyfor the next |l1| 1 elements. In the case |L| = min(|A|, |B|) the relation c2(1) d2(1) is pretty obvious asin this case c2(1) is minimal among all partitions with the same size. In the other case, we use that nowd2(1) > max(|a1|, |b1|) + 1 c2(1).",
  ") is pretty obviousas in this case c1(n 1": "2 ) is maximal among all partitions with the same size. For the other case let |A| |B|.We use that now d2(1) max(|a1|, |b1|) + 1 which implies that max(a1) d2(1) 1 max(|a1|, |b1|) or inother words all indices of elements in d1 that are larger than max(|a1|, |b1|) are from b1. Because there areat least |b1| elements larger than those in b1, we can conclude that d1(n 1",
  "B.7Ordered minimizable but splitting is not beneficial": "We were wondering whether we could provide a cost function which is ordered minimizable but does nothave the property, that splitting is beneficial. Let us denote with X>M(X) the set of elements in the set Xlarger than median of X. Let our universe consist only of non negative reals i.e. R. Then we candefine the a cost functionC(X) =2",
  "xX>M(X)x": "If you consider your universe to be a set of item prices, the cost function describes a scenario where you paya discounted price on only the most expensive half of items. The discount parameter 0 1 controls theamount of discount provided. Values of close to 1 indicate that you get a significant discount if you buyin bulk, while low values of indicate very low discount when buying bulk. The cost function C(X) is ordered minimizable but it does not have the splitting is beneficial property for > 0. For = 1 an optimal UM clustering is just a single cluster containing all points independent of theactual universe ."
}