{
  "Abstract": "Current experiments frequently produce high-dimensional, multimodal datasetssuch asthose combining neural activity and animal behavior or gene expression and phenotypicprofilingwith the goal of extracting useful correlations between the modalities. Often, thefirst step in analyzing such datasets is dimensionality reduction. We explore two primaryclasses of approaches to dimensionality reduction (DR): Independent Dimensionality Reduc-tion (IDR) and Simultaneous Dimensionality Reduction (SDR). In IDR methods, of whichPrincipal Components Analysis is a paradigmatic example, each modality is compressedindependently, striving to retain as much variation within each modality as possible. Incontrast, in SDR, one simultaneously compresses the modalities to maximize the covaria-tion between the reduced descriptions while paying less attention to how much individualvariation is preserved. Paradigmatic examples include Partial Least Squares and CanonicalCorrelations Analysis. Even though these DR methods are a staple of statistics, their rel-ative accuracy and data set size requirements are poorly understood. We use a generativelinear model to synthesize multimodal data with known variance and covariance structuresto examine these questions. We assess the accuracy of the reconstruction of the covariancestructures as a function of the number of samples, signal-to-noise ratio, and the number ofvarying and covarying signals in the data. Using numerical experiments, we demonstratethat linear SDR methods consistently outperform linear IDR methods and yield higher-quality, more succinct reduced-dimensional representations with smaller datasets. Remark-ably, regularized CCA can identify low-dimensional weak covarying structures even whenthe number of samples is much smaller than the dimensionality of the data, which is aregime challenging for all dimensionality reduction methods. Our work corroborates andexplains previous observations in the literature that SDR can be more effective in detectingcovariation patterns in data. These findings strengthen the intuition that SDR should bepreferred to IDR in real-world data analysis when detecting covariation is more importantthan preserving variation.",
  "Introduction": "Many modern experiments across various fields generate massive multimodal data sets. For instance, inneuroscience, it is common to record the activity of a large number of neurons while simultaneously recordingthe resulting animal behavior (Stringer et al., 2019; Steinmetz et al., 2021; Urai et al., 2022; Krakauer et al.,2017). Other examples include measuring gene expressions of thousands of cells and their correspondingphenotypic profiles, or integrating gene expression data from different experimental platforms, such as RNA-Seq and microarray data (Clark et al., 2013; Zheng et al., 2017; Svensson et al., 2018; Huntley et al.,2015; Lorenzi et al., 2018). In economics, important variables such as inflation are often measured usingcombinations of macroeconomic indicators as well as indicators belonging to different economic sectors(Gosselin & Tkacz, 2001; Baillie et al., 2002; Freyaldenhoven, 2022; Rudd, 2020). In all of these examples,an important goal is to estimate statistical correlations among the different modalities. Analyses usually begin with dimensionality reduction (DR) into a smaller and more interpretable represen-tation of the data. We distinguish two types of DR: independent (IDR) and simultaneous (SDR) (Martini &Nemenman, 2024). In the former, each modality is reduced independently, while aiming to preserve its varia-tion, which we call self signal. In the latter, the modalities are compressed simultaneously, while maximizingthe covariation (or the shared signal) between the reduced descriptions and paying less attention to preserv-ing the individual variation. It is not clear if IDR techniques, such as the Principal Components Analysis(PCA) (Hotelling, 1933), are well-suited for extracting shared signals since they may overlook features of thedata that happen to be of low variance, but of high covariance (Colwell et al., 2014; Borga et al., 1997). Inparticular, poorly sampled weak shared signals, common in high-dimensional datasets, can exacerbate thisissue. SDR techniques, such as Partial Least Squares (PLS) (Wold et al., 2001) and Canonical CorrelationsAnalysis (CCA) (Hotelling, 1936), are sometimes mentioned as more accurate in detecting weak shared sig-nal (Chin & Newsted, 1999; Hair et al., 2011; Pacharawongsakda & Theeramunkong, 2016). However, therelative accuracy and data set size requirements for detecting the shared signals in the presence of self signalsand noise remain poorly understood for both classes of methods. In this study, we aim to assess the strengths and limitations of linear IDR, represented by PCA, and linearSDR, exemplified by PLS and CCA, in detecting weak shared signals. For this, we use a generative linearmodel that captures key features of relevant examples, including noise, the self signal, and the shared signalcomponents. Using this model, we analyze the performance of the methods in different conditions. We assesshow well these techniques can (i) extract the relevant shared signal and (ii) identify the dimensionality ofthe shared and the self signals from noisy, undersampled data. We investigate how the signal-to-noise ratios,the dimensionality of the reduced variables, and the method of computing correlations combine with thesample size to determine the quality of the DR. We propose best practices for achieving high-quality reducedrepresentations with small sample sizes using these linear methods.",
  "Overall, the main contributions of this paper are:": "We define a tractable, generative, linear model for producing multimodal datasets; the model al-lows us to tune the numbers and the strengths of the shared and the self signals in the generatedmodalities. We characterize the accuracy and data set requirements for reconstructing the shared signals bySDR methods (CCA, rCCA, and PLS) and IDR methods (PCA) as a function of the parameters ofthe generative model. We find that SDR methods generally outperform IDR methods in detecting shared signals in mul-timodal data; this is true for both the synthetic generative linear model, as well for the non-lineardata derived from the MNIST dataset.",
  "Relations to Previous Work": "The extraction of signals from large-dimensional data sets is a challenging task when the number of observa-tions is comparable to or smaller than the dimensionality of the data. The undersampling problem introducesspurious correlations that may appear as signals, but are, in fact, just statistical fluctuations. This poses achallenge for DR techniques, as they may retain unnecessary dimensions or identify noise dimensions as truesignals. Here, we focus exclusively on linear DR methods. For these, the Marchenko-Pastur (MP) distribu-tion of eigenvalues of the covariance matrix of pure noise derived using the Random Matrix Theory (RMT)methods (Marchenko & Pastur, 1967) has been used to introduce a cutoff between noise and true signal inreal datasets. However, recent work (Fleig & Nemenman, 2022a) has shown that, when observations are alinear combination of uncorrelated noise and latent low-dimensional self signals, then the self signals alterthe distribution of eigenvalues of the sampling noise, questioning the validity of this naive approach. Moving beyond a single modality, Bouchaud et al. (2007) calculated the singular value spectrum of cross-correlations between two nominally uncorrelated random signals. However, it remains unknown whether thelinear mixing of self signals and shared signals affects the spectra of noise, and how all of these componentscombine to limit the ability to detect shared signals between two modalities from data sets of realistic sizes.Filling in this gap using numerical simulations is the main goal of this paper, and analytical treatments ofthis problem are left for the future. The linear model and linear DR approaches studied here do not capture the full complexity of real-world datasets and state-of-the-art algorithms. However, if sampling issues and self signals limit the ability of linearDR methods to extract shared signals, it would be surprising for nonlinear methods to succeed in similarscaling regimes on real data.Thus extending the previous work to explicitly study the effects of linearmixtures of self signals, shared signals, and noise on limitations of DR methods is likely to result in intuitiontransferable to more complex scenarios. Examples of such scenarios might include inference of dynamics ofa system through a latent space (Creutzig et al., 2009; Chen et al., 2022), where shared signals correspondto latent factors that are relevant for predicting the future of the system from its past, while self signalscorrespond to nonpredictive variation (Bialek et al., 2001). In economics, shared and self signals correspondto diverse macroeconomic indicators that are grouped into correlated distinct categories in structural factormodels (Forni & Gambetti, 2010; Gosselin & Tkacz, 2001; Rudd, 2020; Baillie et al., 2002). In neuroscience,shared signals can correspond to the latent space, by which neural activity affects behavior, while self signalsencode neural activity that does not manifest in behavior and behavior that is not controlled by the part ofthe brain being recorded from (Sponberg et al., 2015; Stringer et al., 2019; Natraj et al., 2022; Sani et al.,2021; Pang et al., 2016; Urai et al., 2022; Krakauer et al., 2017). Interestingly, in the context of the neural control of behavior, it was noticed that SDR reconstructs the sharedneuro-behavioral latent space more efficiently and using a smaller number of samples than IDR (Sani et al.,2021). Similar observations have been made in more general statistical contexts (Chin & Newsted, 1999; Hairet al., 2011; Pacharawongsakda & Theeramunkong, 2016; Vogelstein et al., 2021), though the agreement is notuniform (Goodhue et al., 2006; 2012; 2013). Because of this, most practical recommendations for detectingshared signals are heuristic (Hair Jr et al., 2021), with widely acknowledged limitations (Kock & Hadaya,2018). Our goal is to ground such rules in numerical simulations and scaling arguments.",
  "X = X/ X, Y = Y / Y .(2)": "The observations of X and Y are linear combinations of the following: (a) independent white noise com-ponents RX RT NX and RY RT NY with variances 2RX and 2RY ; (b) self-signal components UX andUY residing in lower-dimensional spaces RT mself,X and RT mself,Y , respectively, with the correspondingvariances 2UX and 2UY ; (c) shared-signal components P in a shared lower-dimensional space RT mshared,with variance 2P . These lower-dimensional components are projected into their respective high-dimensionalspaces RT NX and RT NY using fixed quenched projection matrices VX Rmself,XNX, VY Rmself,Y NY ,QX RmsharedNX, and QY RmsharedNY , with variances 2VX, 2VY , 2QX, and 2QY , respectively. Entriesin these matrices are drawn from Gaussian distributions with zero means and the corresponding variances.Further, division by X and Y standardizes each column of the data matrices by their empirical standarddeviations. The total variance in the matrix X can be calculated as the sum of the variances of its individualcomponents: 2X = 2RX + mself,X 2UX2VX + mshared 2P 2QX, and similarly for Y . We define self and shared signal-to-noise ratios self,X/Y , shared,X/Y as the relative strength of signals com-pared to background noise per component in each modality. These definitions allow us to examine how easilyself or shared signals in each dimension can be distinguished from the noise.",
  "RX/Y.(3)": "Our main goal is to evaluate the ability of linear SDR and IDR methods to reconstruct2 the shared signalP, while overlooking the effects of the self signals UX/Y on the statistics of the shared ones. We formalizethis goal in the next section by defining a shared signal reconstruction metric RC.",
  "Methods": "We apply DR techniques to X and Y to obtain their reduced dimensional forms ZX and ZY , respectively.ZX, ZY are of sizes that can range from T 1 to T NX and T NY , respectively. As an IDR method, weuse PCA (Hotelling, 1933). As SDR methods, we apply PLS (Wold et al., 2001) and CCA (Hotelling, 1936;Vinod, 1976; rup Nielsen et al., 1998), including both normal and regularized versions of the latter. Eachof these methods focuses on specific parts of the overall covariance matrix",
  ".(4)": "PCA aims to identify the most significant features that explain the majority of the variance in CXX andCY Y , independently.PLS, on the other hand, focuses on singular values and vectors that explain thecovariance component CXY . Along the same lines, CCA aims to find linear combinations of X and Y thatare responsible for the correlation (CXY /CXXCY Y ) between X and Y (Borga et al., 1997). See AppendixA.1 for a detailed description of these methods. 1This model is an extension of the model introduced by Fleig & Nemenman (2022a), and its probabilistic form has beenstudied by Murphy (2022). In its turn, the latter is an extension of work by Klami et al. (2012), and Bach & Jordan (2005).However, within this model, we focus on the intensive limit, common in RMT (Potters & Bouchaud, 2020), where the numberof observations scales as the number of observed variables. This scenario is common in many real-world applications. To ourknowledge, a treatment of the extensive regime to assess different DR methods as a function of various parameters of the systemdoes not exist.2In this context, we use reconstruction and detection interchangeably. Detection means that we are able to capture theshared signal, which is equivalent to its reconstruction, as the signal is represented by the correlation.",
  "Published in Transactions on Machine Learning Research (09/2024)": "singular value A1 m = 10.0, = 0.1 singular value B1 m = 1.0, = 0.1 singular value C1 m = 0.1, = 0.1 singular value A2 m = 10.0, = 1.0 singular value B2 m = 1.0, = 1.0 singular value C2 m = 0.1, = 1.0 singular value A3 m = 10.0, = 10.0 singular value B3 m = 1.0, = 10.0 singular value C3 m = 0.1, = 10.0 msharedmselfmself + msharedCXXCXY 1000 total dimensions, 10 shared dimensions, 10000 samples : Similar to , but for 10000 samples a well-sampled regime. We observe similar behaviorto in Panels A1, A2, A3, B3, and C3. In Panel B1, we now see a small gap in the spectrum ofCXY that was not apparent in due to undersampling. Panel B2 shows an even clearer gap for CXY ,whereas for CXX, we still cannot see a gap between the self and shared singular values. Panel C1 showssimilar behavior for CXY , but for CXX, we can see a gap after 100 singular values, followed by another smallgap after an additional 10 singular values that correspond to the shared signal. However, if we are using anIDR method, this means we need to retain 110 dimensions, which is challenging to sample adequately. InPanel C2, we can now see a clear gap for CXY after 10 singular values corresponding to the shared signals,while we cannot see any gap for CXX because all the singular values for self and shared signals have equalpower.",
  "ZY = YtestWYtrain.(5)": "Finally, we evaluate the reconstructed correlations metric RC, which measures how well these singulardirections recover the shared signals in the data, corrected by the expected positive bias due to the samplingnoise, see Appendix A.2 for details. RC = 0 corresponds to no overlap between the true and the recoveredshared directions, and RC = 1 corresponds to perfect recovery.",
  "Results for The Generative Linear Model": "We perform numerical experiments to explore the undersampled regime, T NX, NY .We use T ={100, 300, 1000, 3000} samples, NX = NY = 1000. We explore the case of one shared signal only, mshared = 1and we mask this shared signal by a varying number of self signals and by noise. We vary the number ofretained dimensions, (|ZX|, |ZY |4, and explore how many of them are needed to recover the shared signal inthe noise and the self signal background with different SNRs. For brevity, we explore two cases: (1) one self-signal each in X and Y in addition to the shared signal(mself = 1); (2) many self-signals in X and Y . For both cases, we calculate the quality of reconstructionas the function of the shared and the self SNR, shared and self. In all figures, we show RC for severelyundersampled (first row, T = 300) and relatively well sampled (second row, T = 3000) regimes. We alsoshow the value of RC0, the bias that we removed from our reconstruction quality metric, for completeness,see Appendix A.2 for details. Experiments at different parameter values can be found in Appendix A.4. shows that, in Case 1, when one dimension is retained in DR of X and Y , PCA populates thecompressed variable with the largest variance signals and hence struggles to retain the shared signal whenself > shared, regardless of the number of samples5. However, both PLS and rCCA excel in achieving nearlyperfect reconstructions. When T NX, straightforward CCA cannot be applied (see A.1.3-A.1.4), but ittoo achieves a perfect reconstruction when T > NX. In , we allow two dimensions in the reduced variables. For PCA, we expect this to be sufficient topreserve both the self and the shared signals. Indeed, PCA now works for all s and Ts, although with aslightly reduced accuracy for large shared signals compared to . PLS and rCCA continue to deliverhighly accurate reconstructions. So does the CCA for T > NX. Spurious correlations, as measured by RC0grow slightly with the increasing dimensionality of ZX, ZY compared to . This is expected since moreprojections must now be inferred from the same amount of data. We now turn to mself mshared. We use mshared = 1, mself = 30 for concreteness. We expect that theperformance of SDR methods will degrade weakly, as they are designed to be less sensitive to the maskingeffects of the self signals. In contrast, we expect IDR to be more easily confused by the many strong self-signals, as IDR is designed to pick up any large variance components, which would include self signals ifthey are stronger than the shared signal, degrading the performance. Indeed, shows that PCA nowfaces challenges in detecting shared signals, even when the self signals are weaker than in . Increasing",
  "We fix 2RX/Y , 2VX/Y , 2QX/Y and allow 2UX/Y , 2P to vary as equally spaced values between 0.05 and 1 when we choose": "self,X/Y , shared,X/Y . The SNRs of X and Y are symmetric. We first generate the fixed projection matrices VX/Y , QX/Y ,and we vary RX/Y , UX/Y , P for each trial.4We use the notation | | to specify the dimensionality (or the number of dimensions we are keeping after reduction) of avariable. We reserve the usage of the notation || || to the matrix norm, as used in defining the RC metric (c.f. A.2)5The transition from being able to observe the shared signal to not being able to observe it in data as a function of thesample size resembles the famous BBP transition (Baik et al., 2004), which can be explored analytically for random matricesof the form Eqs. 1, which we leave for future work.",
  "Noise": "0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 E2 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 E3 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 self 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 E4",
  ": Same as , but for |ZX| = |ZY | = 2 = mself + mshared. Now there are enough compressedvariables for PCA to detect the shared signal. Other methods perform similarly to , albeit the noise islarger": "T improves its performance only slightly.Somewhat surprisingly, PLS performance also degrades, withimprovements at T NX. CCA again displays no reconstruction when T NX, switching to near perfectreconstruction at large T. Crucially, rCCA again shines, maintaining its strong performance, consistentlydemonstrating nearly perfect reconstruction. Since one retained dimension is not sufficient for PCA to represent the shared signal when shared self,we increase the dimensionality of reduced variables, |ZX| = |ZY | = mself mshared, cf. . PCA nowdetects shared signals even when they are weaker than the self-signals, shared < self, but at a cost ofthe reconstruction accuracy plateauing significantly below 1. In other words, when self and shared signals",
  "Correlation": "Cor(X, Y) : Dataset containing paired MNIST digit samples sharing only the same identity (shared signal).The first row (X) shows MNIST digits randomly subjected to scaling, (0.5 1.5), and rotation with anangle of (0 /2), while the second row (Y ) shows MNIST digits with an added background Perlin noise(self signals). In the bottom row, histograms of self correlations for the X and Y datasets (left and middle,respectively) illustrate a wide range of correlations, while the histogram of the cross correlation between Xand Y (right) demonstrates a smaller range. To analyze linear DR methods on nonlinear data, we followed the same procedure as in for a datasetinspired by the noisy MNIST dataset (LeCun et al., 1998; Wang et al., 2015; 2016; Abdelaleem et al., 2023).This dataset has two distinct views of data, each of dimensionality 28 28 pixels, examples of which areshown in . The first view is an image of the digit subjected to a random rotation within an angleuniformly sampled between 0 and 2 , along with scaling by a factor uniformly distributed between 0.5 and1.5. The second view consists of another image with the same digit identity with an additional backgroundlayer of Perlin noise (Perlin, 1985), with the noise factor uniformly distributed between 0 and 1. Both viewsare normalized to an intensity range of [0, 1), then flattened to form an array of 784 dimensions. To cast this dataset into our language, we shuffled the images within labels, retaining the shared label identity(that is the shared signal), but we still have the view-specific details (which is the self signal). This resultedin a total dataset size of 56k images for training and 7k images for testing. The correlation histogramof X (or Y ) with itself shows a relatively wide spectrum when compared to the cross correlation betweenX and Y , highlighting that the self signal is stronger, and can lead to different DR methods overseeing theshared one. The complexity of the tasks makes it sufficiently challenging, serving as a good benchmark forevaluating the performance of the different DR techniques.",
  "Results": "1/q 0.0 0.2 0.4 0.6 Corr(ZX, ZY)F Corr(Random)F T = 1000 1/q 0.0 0.5 1.0 1.5 T = 10000 1/q T = 55996 PCAPLSCCArCCA Total Corrected Correlation vs 1/q for different T : Performance of PCA, PLS, CCA, rCCA applied to the modified Noisy MNIST dataset acrossvarying sampling scenarios. Each panel represents different sample sizes (1000, 10, 000, and approximately56, 000 samples). The x-axis denotes the inverse of the number of samples per retained dimensions (1/q),while the y-axis represents the total corrected correlation between the obtained low-dimensional representa-tions ZX and ZY . shows the performance of PCA, PLS, CCA, and rCCA applied to the modified Noisy MNIST datasetfor varying sampling scenarios. The three panels are evaluated for different sample sizes (1000, 10, 000, and 56, 000 samples), from undersampled to the full dataset. In each scenario, the training samples are used for the DR methods. Subsequently, the learned projectionmatrices onto the singular directions are used to transform a separate test dataset of around 7, 000 samplesinto low-dimensional spaces, yielding ZX and ZY .The correlation between these transformed spaces iscomputed using the Frobenius norm of the correlation matrix. As before, we then subtracted from it thecorrelation value obtained from a random matrix of the same size. This difference is then plotted against1/q, which is the measure of how many dimensions are retained at each sampling ratio. In the undersampled scenario (1000 samples), rCCA and PLS demonstrate an early detection (in terms ofthe number of kept dimensions after reduction) of shared signals, whereas PCA initially lags behind. Asthe number of dimensions increases, all methods exhibit a decline in correlation due to increased noise aswe have fewer samples per dimension. CCA does not work in this scenario, since covariance matrices aredegenerate. Upon increasing the sample size (10, 000 samples), a similar pattern emerges initially, where all methodsexperience an increase in total correlation till a certain number of kept dimensions is reached, then a declinewhen adding more dimensions. The decline is because one needs to estimate more singular vectors from thesame number of samples. However, beyond a certain number of singular vectors, an increase in correlationis observed. This is because the number of vectors is now sufficient to learn both the shared and the selfsignals. We observe that rCCA maintains superior performance, while PCA reaches peak correlation ata higher number of kept dimensions, providing a rough estimation of the number of true self and sharedsignals. With the full dataset (approximately 56,000 samples), a similar trend is seen. Yet CCAs performanceapproaches that of rCCA. Notably, these analyses confirm that linear Simultaneous Dimensionality Reduction (SDR) are consistentlybetter at detecting shared signals than linear Independent Dimensionality Reduction (IDR), even in somenonlinear datasets.",
  "Discussion": "We used a generative linear model which captures multiple desired features of multimodal data with sharedand non-shared signals. The model focused only on data with two measured modalities. However, while nota part of this study, the model can be readily extended to accommodate more than two modalities (e. g.,Xi = Ri + UiVi + PQi for i = 1, ..., n, where n represents the number of modalities). Then, methods suchas Tensor CCA, which can handle more than two modalities (Luo et al., 2015), can be used to get insightinto DR on such data. We analyzed different DR methods on data from this model in different parameter regimes. Linear SDRmethods were clearly superior to their IDR counterparts for detecting shared signals. In particular, SDRperformance peaked around |ZX| = mshared and IDR performance peaked around |ZX| = mself + mshared.Thus, SDR required fewer samples for similar detection performance to IDR. We observed similar resultson a nonlinear dataset as well.We thus make a strong practical suggestion that, whenever the goal isto reconstruct a low dimensional representation of covariation between two components of the data, IDRmethods (PCA) should always be avoided in favor of SDR. Of the examined SDR approaches, rCCA is aclear winner in all parameter regimes and should always be preferred. While we performed the analysis using specific examples of SDR, such as PLS, (r)CCA, and specific examplesof IDR, like PCA, we anticipate similar results to hold for other SDR and IDR methods. For instance, meth-ods that optimize a certain aspect within one modality (e. g., Independent Component Analysis Hyvrinen& Oja (2000), Nonnegative Matrix Factorization Lee & Seung (2000), Autoencoders Hinton & Salakhutdinov(2006)), or methods using multiple modalities but retaining only certain aspects within each modality (e. g.,Multiview PCA Xia et al. (2021)) should be able to detect the shared signals only if they are stronger thanthe self signals, given the other detection criteria (of having enough number of kept dimensions and enoughsamples to sample them properly) are met. Otherwise, they will likely detect the self signals first, wastingsome of their statistical power. Alternatively, methods that work across modalities (e. g., Cross-modal Fac-tor Analysis Li et al. (2003), Deep Variational Symmetric Information Bottleneck Abdelaleem et al. (2023))should identify the shared signal first. We leave the verification of this hypothesis across a broader range ofmethods for future work. To analyze the behavior of such more complicated methods, we will need gener-ative models of data that have low-dimensional structure, which cannot, nonetheless, be approximated bymethods that rely on singular value spectra and their nonlinear generalizations; first steps towards creatingsuch generative models were recently taken Fleig & Nemenman (2022b). These findings explain the results of, for example, Sani et al. (2021) and others that SDR can recover jointneuro-behavioral latent spaces with fewer latent dimensions and using fewer samples than IDR methods.Further, our observation that SDR is always superior to IDR in the context of our model corroboratesthe theoretical findings of Martini & Nemenman (2024), who proved a similar result in the context ofdiscrete data and a different SDR algorithm, namely the Symmetric Information Bottleneck (Friedmanet al., 2013). Vogelstein et al. (2021) made similar conclusions using conditional covariance matrices for thereduction in the context of classification. More recent work of Abdelaleem et al. (2023) showed similar resultsusing deep variational methods. Collectively, these diverse investigations, linear and nonlinear, theoretical,computational, and empirical, provide strong evidence that generic (not just linear) SDR methods are likelyto be more efficient in extracting covariation than their IDR analogs. Our study also answers an open question in the literature surrounding the effectiveness of SDR techniques.Specifically, there has been debate about whether PLS, an SDR method, is effective at low sampling (Chin &Newsted, 1999; Hair et al., 2011; Goodhue et al., 2006; 2012). Our results show that SDR is not necessarilyeffective in the undersampled regime. It works well when the number of samples per retained dimension ishigh (even if the number of samples per observed dimension is low), but only when the dimensionality of thereduced description is matched to the actual dimensionality of the shared signals. Finally, our results can be used as a diagnostic test to determine the number of shared versus self signalsin data. As demonstrated in , total correlations between ZX and ZY obtained by applying PCA andrCCA increase monotonically as the dimensionality of Zs increases, until this dimensionality becomes largerthan the signal dimensionality. For PCA, the signal dimensionality is equal to the sum of the number of",
  "Limitations and Future Work": "While this work has provided useful insight, the assumptions made here may not fully capture the complexityof real-world data. Specifically, our data is generated by a linear model with random Gaussian features. Itis unlikely that real data have this exact structure. Therefore, there is a need for further exploration ofthe advantages and limitations of linear DR methods on data that have a low-dimensional, but nonlinearshared structure. This can be done using more complex nonlinear generative models, such as nonlinearlytransforming the data generated by Eq. (1-2), or random feature two-layered neural network models (Rocks& Mehta, 2022). Alternatively, analyzing the model, Eq. (1) using various theoretical techniques (Borgaet al., 1997; Vogelstein et al., 2021; Potters & Bouchaud, 2020) is likely to offer even more insights intoits properties. Collectively, these diverse approaches would aid our understanding of different DR methodsunder diverse conditions. A different possible future research direction is to explore the performance of nonlinear DR methods ondata from generative models with a latent low-dimensional nonlinear structure. Autoencoders and theirvariational extensions are a natural extension of IDR to learn nonlinear reduced dimensional representations(Hinton & Salakhutdinov, 2006; Kingma & Welling, 2014; Higgins et al., 2016). Meanwhile, Deep CCA andits variational extensions (Andrew et al., 2013; Wang et al., 2015; Chandar et al., 2016; Wang et al., 2016)should be explored as a nonlinear version of SDR. Both of these types of methods can potentially capturemore complex relationships between the modalities and improve the quality of the reduced representations,and while recent work suggests that (Abdelaleem et al., 2023), it is not clear if the SDR class of methods isalways more efficient than the IDR one. Further, our analysis depends on the choice of metric used to quantify the performance of DR, and differentchoices should also be explored. For example, to capture nonlinear correlations, mutual information can beutilized to quantify the relationships between the reduced representations.",
  "Sarath Chandar, Mitesh M Khapra, Hugo Larochelle, and Balaraman Ravindran.Correlational neuralnetworks. Neural Computation, 28(2):257285, 2016. doi: 10.1162/NECO_a_00801": "James Chapman and Hao-Ting Wang. Cca-zoo: A collection of regularized, deep learning based, kernel, andprobabilistic cca methods in a scikit-learn style framework. Journal of Open Source Software, 6(68):3823,2021. Boyuan Chen, Kuang Huang, Sunand Raghupathi, Ishaan Chandratreya, Qiang Du, and Hod Lipson. Au-tomated discovery of fundamental variables hidden in experimental data. Nature Computational Science,2(7):433442, 2022.",
  "Wynne Chin and P. Newsted. Structural equation modeling analysis with small samples using partial leastsquare. Statistical Strategies for Small Sample Research, 01 1999": "Kenneth Clark, Bruce Vendt, Kirk Smith, John Freymann, Justin Kirby, Paul Koppel, Stephen Moore,Stanley Phillips, David Maffitt, Michael Pringle, et al. The cancer imaging archive (tcia): maintainingand operating a public information repository. Journal of digital imaging, 26:10451057, 2013. Lucy J Colwell, Yu Qin, Miriam Huntley, Alexander Manta, and Michael P Brenner. Feynman-hellmanntheorem and signal identification from sample covariance matrices. Physical Review X, 4(3):031032, 2014.",
  "Daniel Lee and H Sebastian Seung. Algorithms for non-negative matrix factorization. Advances in neuralinformation processing systems, 13, 2000": "Dongge Li, Nevenka Dimitrova, Mingkun Li, and Ishwar K Sethi. Multimedia content processing throughcross-modal association. In Proceedings of the eleventh ACM international conference on Multimedia, pp.604611, 2003. Marco Lorenzi, Andre Altmann, Boris Gutman, Selina Wray, Charles Arber, Derrek P Hibar, Neda Jahan-shad, Jonathan M Schott, Daniel C Alexander, Paul M Thompson, et al. Susceptibility of brain atrophyto trib3 in alzheimers disease, evidence from functional prioritization in imaging genetics. Proceedings ofthe National Academy of Sciences, 115(12):31623167, 2018. Yong Luo, Dacheng Tao, Kotagiri Ramamohanarao, Chao Xu, and Yonggang Wen. Tensor canonical correla-tion analysis for multi-view dimension reduction. IEEE transactions on Knowledge and Data Engineering,27(11):31113124, 2015.",
  "Kevin P Murphy. Probabilistic machine learning: an introduction. MIT press, 2022": "Nikhilesh Natraj, Daniel B Silversmith, Edward F Chang, and Karunesh Ganguly. Compartmentalized dy-namics within a common multi-area mesoscale manifold represent a repertoire of human hand movements.Neuron, 110(1):154174.e12, 2022. ISSN 0896-6273. doi: Eakasit Pacharawongsakda and Thanaruk Theeramunkong. A comparative study on single and dual spacereduction in multi-label classification. In Andrzej M.J. Skulimowski and Janusz Kacprzyk (eds.), Knowl-edge, Information and Creativity Support Systems: Recent Trends, Advances and Solutions, pp. 389400,Cham, 2016. Springer International Publishing. ISBN 978-3-319-19090-7.",
  "A.1.1Principal Components Analysis (PCA)": "PCA is a widely used linear IDR method that aims to find the orthogonal principal directions, such thata few of them explain the largest possible fraction of the variance within the data. PCA decomposes thecovariance matrix of the data matrix X, CXX =1T XX, into its eigenvectors and eigenvalues throughsingular value decomposition (SVD). The SVD yields orthogonal directions, represented by the vectors w(i)X ,that capture the most significant variability in the data. In most numerical implementations (Pedregosaet al., 2011), these directions are obtained consecutively, one by one, such that the dot product between",
  "w(i)X.(6)": "Here X(i) is the ith deflated matrix where X(1) is the original matrix, and for every subsequent i + 1, thematrix is deflated by subtracting the projection of X on the obtained weights: X(i+1) = X is=1Xw(s)w(s).The eigenvectors are sorted in decreasing order according to their corresponding eigenvalues, and the firstk eigenvectors w(i=1:k)Xare selected to form the projection matrix WX. The obtained vectors determine thesize of the reduced form ZX, where |ZX| = k is the number of vectors retained from the decomposition ofX. The vectors w(i)X are then stacked together to form the projection matrix WX. The low-dimensionalrepresentation ZX is then obtained by multiplying the original data matrix X with this projection matrix,resulting in the reduced data matrix ZX = XWX.Similar treatment is done for Y in order to obtainZY = Y WY One of the main advantages of PCA is its simplicity and efficiency. However, one of the drawbacks of thismethod is that it performs DR for X and Y independently, and one then searches for relations betweenZX and ZY by regressing one on the otherthe so-called Principal Components Regression. Thus obtainedlow-dimensional descriptions may capture variance but not the covariance between the two datasets.",
  "w(i)Y )(7)": "The matrices X(i), Y (i) are deflated in a similar manner to PCA A.1.16. The singular vectors are sortedin the decreasing order of their corresponding singular values, and the first k vectors are selected to formthe projection matrices (WX, WY ). The obtained vectors determine the size of the reduced form (ZX, ZY ),where |ZX| = |ZY | = k is the number of vectors retained. The vectors (w(i)X , w(i)Y ) are then stacked togetherto form the projection matrices (WX, WY ) respectively. The low-dimensional representations (ZX, ZY ) areobtained by projecting the original data matrices (X, Y ) onto these projection matrices: ZX = XWX, andZY = Y WY . In summary, PLS performs simultaneous reduction on both datasets, maximizing the covariance between thereduced representations ZX and ZY . This property makes PLS a powerful tool for studying the relationshipsbetween two datasets and identifying the underlying factors that explain their joint variability. In practice,PLS can suffer from widely distinct variances along different singular directions, as well as covariances withinX or Y . At the very least, in practical implementations, each component of X and Y is typically standardizedto unit variance before applying PLS to avoid some (but not all) of these issues.",
  "A.1.3Canonical Correlations Analysis (CCA)": "CCA is another SDR method, which aims to find the directions that explain the maximum correlationbetween two datasets Hotelling (1936). However, unlike PLS, CCA obtains the shared signals by performingSVD on the correlation matrixCXYCXXCY Y . The singular vectors (w(i)X , w(i)Y) are obtained consecutively 6This PLS implementation with iterative deflation is often referred to as PLSCanonical. An alternative simpler approachbased on direct SVD calculation of the covariance matrix is often known as PLSSVD Pedregosa et al. (2011).",
  "Y (i)Y (i)w(i)Y ).(8)": "Like in PLS A.1.2, the matrices X(i), Y (i) are deflated in a similar manner. In addition, the first k singularvectors (w(i)X , w(i)Y) are stacked together to form the projection matrices (WX, WY ), which then are usedto obtain the reduced data matrices ZX = XWX, and ZY = Y WY . One of the key differences between PLS and CCA is that while both perform SDR, CCA also simultaneouslyperforms IDR implicitly. Indeed, it involves multiplication of CXY by C1/2XXon the left and C1/2Y Yon theright, which, in turn, requires finding singular values of the X and the Y data matrices independently.",
  "A.1.4Regularized CCA - rCCA": "While CCA is a useful method for finding the maximum correlating features between two sets of data,it does have some limitations.Specifically, in the undersampled regime, where T max(NX, NY ), thematrices CXX and CY Y are singular and their inverses do not exist. Using the pseudoinverse to solve theproblem can lead to numerical instability and sensitivity to noise. Regularized CCA (rCCA) (Vinod, 1976;rup Nielsen et al., 1998) overcomes this problem by adding a small regularization term to the covariancematrices, allowing them to be invertible. Specifically, one tales",
  "where CXX, CY Y are the new regularized matrices, cX, cY > 0 are small regularization parameters andIX, IY are identity matrices with sizes NX NX, NY NY respectively": "This original implementation of rCCA resulted in correlation matrices with diagonals not equal to one. Thus,a better implementation uses a different form of regularization (rup Nielsen et al., 1998) by adding theregularization parameters cX and cY individually to the equations as an affine combination (i. e., ni ci = 1)as the following:",
  "X(i)Y (i)w(i)Y CXX CY Y.(15)": "Writing the regularization conditions in this form is, in fact, a convex interpolation problem between PLSand CCA, which is a more robust solution and does not suffer from shortening the length of correlations dueto the added regularization. As a result, this implementation of rCCA achieves the best accuracy among allother methods 7. 7Conceptually, the main difference between PLS and CCA is that PLS does not enforce orthogonality among the weights w(i)Xand w(i)Ythat diagonalize CXX and CY Y , whereas CCA does. For instance, while two pairs of singular vectors (w(1)X , w(1)Y ) and",
  "A.2Assessing Success and Sampling Noise Treatment": "To assess the success of DR, we calculated the ratio between the total correlation between ZXtest and ZYtest,defined as in Eq. (5), and the total correlation between X and Y , which we input into the model. Specifically,we take the total correlation as the Frobenius norm of the correlation matrix, ||A||F =",
  "mshared,(16)": "where Corr stands for the correlation matrix between its arguments, and we use ||Corr(P, P)||F = mshared asthe total shared correlation that one needs to recover. Statistical fluctuations aside, RC should vary betweenzero (bad reconstruction of the shared variables) and one (perfect reconstruction)8. T 0.0 0.5 1.0 1.5 2.0 2.5 3.0 RC0 |Z| = 1 |Z| = 2 |Z| = 30 |Z| = 31 : The resulting correlations are averages of all the points in the phasespace, then averaged over 10different realizations of the matrices. The error bars are for two standard deviations around the mean In many real-world applications, the number of available samples, T, is often limited compared to thedimensionality of the data, NX and NY . This undersampling can introduce spurious correlations. We arenot aware of analytical results to calculate the effects of the sampling noise on estimating singular values (w(2)X , w(2)Y ) are mutually orthogonal as pairs, w(1)Xand w(2)Xare not orthogonal to each other. This partial overlap can resultin signals not being fully expressed in these directions, reducing the total correlation compared to CCA, where each directionw(i)X and w(i)Yis orthogonal to every other direction, allowing each signal to fully occupy these directions and maximizing totalcorrelation. Therefore, our goal is to make CCA work, but due to inversion issues, rCCA serves as a substitute. This is evidentin Figures 1-5, where, under good sampling conditions, rCCA and CCA yield almost identical results.8We note that the choice of RC is not unique, and other choices could be valid as well. However, we believe that RC is suitablefor our study. For example, one could use mutual information I(ZX; ZY ) Shannon (1948) to quantify how much informationZX and ZY carry about each other. Mutual information is a fundamental statistic that is 0 if and only if the two variables arestatistically independent. However, its estimation is a challenging problem, as it depends on the probabilities p(ZX), p(ZY ),and p(ZX, ZY ) Antos & Kontoyiannis (2001); Paninski (2003). In this setup, while the individual matrices of X and Y areGaussian with known probability density functions, their product is not. Consequently, a closed analytical form of mutualinformation is not known. Even if we assumed ZX and ZY to be Gaussian and used the formula I(ZX; ZY ) = 1 2 ln |CX,Y |,where |.| denotes the determinant of the original correlation matrix CXY /CXX CY Y , we might end up with directions thatdo not carry information due to having more dimensions in ZX (or ZY ) than needed, thus creating numerical instabilities.These instabilities are often addressed by employing a threshold on the determinant (considering the determinant as the productof singular values, we threshold the small singular values). However, we chose not to use such a method, focusing instead onRC for the following reasons: (i) With this choice, we do not need to employ thresholding, and by using a metric that uses thesingular values of the correlation matrix in an additive fashion, rather than multiplicative as in mutual information, we avoidthis issue. (ii) Correlation is an intuitive metric, commonly used by practitioners, with clear meaning and bounds between-1 and +1. While one could use the covariance matrix rather than the correlation matrix, such a choice does not affect theoptimization process, as X and Y are standardized. Therefore, the covariance and correlation matrices of the original data areequivalent, and the difference would lie in the calculation of Cov(ZX, ZY ) vs Corr(ZX, ZY ), potentially increasing the accuracyof PLS in such a case. However, due to the intuitiveness and utility of correlation, we use it in the RC metric.",
  "A.3Implementation": "We used Python and the scikit-learn (Pedregosa et al., 2011) library for performing PCA, PLS, and CCA,while the cca-zoo (Chapman & Wang, 2021) library was used for rCCA. For PCA, SVD was performedwith default parameters. For PLS, the PLS Canonical method was used with the NIPALS algorithm. Forboth PLS and CCA, the tolerance was set to 104 with a maximum convergence limit of 5000 iterations.For rCCA, regularization parameters were set as c1 = c2 = 0.1. All other parameters not explicitly herewere set to their default values. All figures shown in this paper were averaged over 10 independent realizations of RX, RY , UX, UY , P, whilefixing the projection matrices VX, VY , QX, QY . We then performed an additional round of averaging every-thing over 10 realizations of the projection matrices themselves. The simulations were parallelized and runon Amazon Web Services (AWS) servers of various instance types.",
  ": Performance of PCA, PLS, CCA, and rCCA in detecting shared signals with one self signal andone dimension kept after DR": "RC 0.0 0.2 0.4 0.6 0.8 1.0 RC0 0.025 0.050 0.075 0.100 0.125 0.150 0.175 0.200 Adding 1 self-signal, keeping two dimensions after reduction T = 100T = 300T = 1000T = 3000 0.10.20.30.40.50.60.70.80.91.0 shared A1 PCA 0.10.20.30.40.50.60.70.80.91.0 shared A2 0.10.20.30.40.50.60.70.80.91.0 shared A3 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 self 0.10.20.30.40.50.60.70.80.91.0 shared A4 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 B1 PLS 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 B2 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 B3 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 self 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 B4 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 C1 CCA 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 C2 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 C3 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 self 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 C4 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 D1 rCCA 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 D2 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 D3 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 self 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 D4 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 E1",
  ": Performance of PCA, PLS, CCA, and rCCA in detecting shared signals with 30 self signals andone dimension kept after DR": "RC 0.0 0.2 0.4 0.6 0.8 1.0 RC0 0.5 1.0 1.5 2.0 2.5 3.0 Adding 30 self-signals, keeping 30 dimensions after reduction T = 100T = 300T = 1000T = 3000 0.10.20.30.40.50.60.70.80.91.0 shared A1 PCA 0.10.20.30.40.50.60.70.80.91.0 shared A2 0.10.20.30.40.50.60.70.80.91.0 shared A3 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 self 0.10.20.30.40.50.60.70.80.91.0 shared A4 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 B1 PLS 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 B2 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 B3 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 self 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 B4 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 C1 CCA 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 C2 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 C3 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 self 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 C4 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 D1 rCCA 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 D2 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 D3 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 self 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 D4 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 E1",
  "Here we provide additional figures that show the singular value spectra of the matrices CXX and CXY(Eq. 4). The original X and Y matrices are generated using the same parameters as in": "singular value A1 m = 10.0, = 0.1 singular value B1 m = 1.0, = 0.1 singular value C1 m = 0.1, = 0.1 singular value A2 m = 10.0, = 1.0 singular value B2 m = 1.0, = 1.0 singular value C2 m = 0.1, = 1.0 singular value A3 m = 10.0, = 10.0 singular value B3 m = 1.0, = 10.0 singular value C3 m = 0.1, = 10.0 msharedmselfmself + msharedCXXCXY 1000 total dimensions, 10 shared dimensions, 100 samples : Singular values obtained from CXX and CXY (Eq. 4) for the parameters used in . Usingtwo generated data matrices X and Y with NX = NY = 1000, mshared = 10 while we change mself to getthe corresponding m, and self = 1 while we change shared to get the corresponding . We calculate theircorresponding CXX and CXY and plot their singular values versus their order for 100 samples. We observesimilar behavior to , in the sense that if the shared signal is strong (Panels A3, B3, C3, where = 10),then we see 10 distinct singular values that correspond to the shared signal for all the different values of m.This is a regime where the results of applying IDR and SDR are equivalent (we even see the first 10 singularvalues are on top of each other). In Panel A1, we see one singular value of CXX standing out, followed byanother 10 values of the shared signals, while CXY only identifies those 10 shared signals first. Panel A2shows similar behavior, but the single self signal is now mixed among the others for CXX. In Panel B2,while we see 10 distinct singular values of CXX appearing first, these correspond to the self signals, and thenext 10 correspond to the shared signals, meaning that if we stopped in an IDR application after retaining10 dimensions, we would not capture any shared signals. CXY , on the other hand, shows a continuation ofsingular values without a gap, due to undersampling. Panel B2 shows similar behavior for CXY , and forCXX, we cannot even see a gap between the self and shared singular values. In Panels C1 and C2, the sharedsignal is overwhelmed by the self signals, and we cannot see any gap for CXX or CXY due to undersampling."
}