{
  "Abstract": "Estimating the individual-level continuous treatment effect holds significant practical impor-tance in various decision-making domains, such as personalized healthcare and customizedmarketing. However, most current methods for individual treatment effect estimation arelimited to discrete treatments and struggle to precisely adjust for selection bias under con-tinuous settings, leading to inaccurate estimation. To address these challenges, we proposea novel Disentangled Representation Network (DTRNet) to estimate the individualizeddose-response function (IDRF), which learns disentangled representations and preciselyadjusts for selection bias. To the best of our knowledge, our work is the first attempt toprecisely adjust for selection bias in continuous settings. Extensive results on synthetic andsemi-synthetic datasets demonstrate that our DTRNet outperforms most state-of-the-artmethods. Our code is available at DTRNet.",
  "Introduction": "In various fields, from medicine to marketing, estimating the causal effects of continuous treatments atindividual level is not just an academic exercise; it is crucial for decision making. Take precision medicine asan example: The central question often focuses on determining the optimal dosage of medicine to achieve theoptimal outcome for a given patient. Therefore, understanding the causal relationship between continuoustreatments and outcomes can help develop customized medication regimens tailored to individual patients. When estimating the effect of individual treatment (ITE), two predominant challenges arise: the inabilityto observe counterfactual outcomes and the presence of selection bias. For example, when a specificdose of treatment is assigned to a patient, only the factual outcome corresponding to that dose is observed,leaving counterfactual outcomes unobserved for other doses. Furthermore, unlike randomized controlledtrials where treatments are assigned at random, the dosage a patient receives in practice may depend oncertain patient-specific features (e.g., older individuals may receive higher dosages more frequently). Thisdependency can introduce selection bias, thereby compromising the accuracy of counterfactual outcomeestimation. For example, it becomes challenging to accurately estimate the treatment effect of higher dosagesin younger populations. Beyond conventional causal inference techniques such as stratification methods (Pearl,2009) and matching methods (Abadie et al., 2004), recent research has harnessed representation learning",
  "Published in Transactions on Machine Learning Research (11/2024)": "Yaochen Zhu, Yinhan He, Jing Ma, Mengxuan Hu, Sheng Li, and Jundong Li. Causal inference with latentvariables: Recent advances and future prospectives. In Proceedings of the 30th ACM SIGKDD Conferenceon Knowledge Discovery and Data Mining, pp. 66776687, 2024b. Yuchen Zhu, Limor Gultchin, Arthur Gretton, Matt J Kusner, and Ricardo Silva. Causal inference withtreatment measurement error: a nonparametric instrumental variable approach. In Uncertainty in ArtificialIntelligence, pp. 24142424. PMLR, 2022.",
  "'#()": ": Causal graph and framework of our DTRNet. Figure (a) shows the causal graph involving covariates(X), treatment (T), outcome (Y ), instrumental factors (), confounder factors (), adjustment factors ().The solid line represents causal relations, and dot lines denote affiliations. Figure (b) shows the frameworkof DTRNet. Three contracted neural networks are utilized to obtain the deep representations of the threefactors. Then (xi) and (xi) are concatenated to predict the distribution of ti. (xi), and (xi) areused to predict outcomes through a varying coefficient network g(ti), while (xi) attempts to encode littleinformation about treatment.",
  "Potential Outcomes Framework": "The potential outcomes framework, as proposed by (Rubin, 1974), is widely used to define the individualtreatment effect (ITE). Specifically, to make the definition clear, we use binary treatments (T = 1 and T = 0)to illustrate, which can be further extended to multiple treatments by comparing their potential outcomes.For each individual xi, there are two potential outcomes: Yi(T = 1) and Yi(T = 0) corresponding to twopossible treatments (T = 1 and T = 0). As only one treatment can be taken for each individual, only oneof the potential outcomes can be observed (observed outcome), and the remaining unobserved outcome isthe counterfactual outcome. Therefore, one major challenge in estimating ITE is to infer the counterfactualoutcome. After obtaining the counterfactual outcome, the ITE is calculated as follows.",
  "Machine Learning for ITE Estimation": "As mentioned above, selection bias and counterfactual prediction are two major challenges for ITE. To addressthese issues, various methods have been proposed (Alaa & van der Schaar, 2017; Hansen, 2008; Chipmanet al., 2010; Hansen, 2008; Wager & Athey, 2018; Chu & Li, 2023; Yao et al., 2021; Acharki et al., 2023).Moreover, several state-of-the-art methods use deep representation learning models to estimate ITE based ontreatment-invariant representations (Johansson et al., 2016; Shalit et al., 2017; Louizos et al., 2017; Yoonet al., 2018; Yao et al., 2018; Bellot et al., 2022; Wang et al., 2022; Acharki et al., 2023; Guo et al., 2023;Nagalapatti et al., 2024; Sui et al.; Huang et al., 2024; Zhu et al., 2024b; Chu et al., 2023b; 2024; Huang &Zhang, 2023; Zhu et al., 2022; Jiang et al., 2023; Gao et al., 2024; Pogodin et al., 2022). Specifically, thediscrepancy loss between the deep representations of the treatment group and control group is used to balancethe distribution of the two groups to adjust for selection bias. Subsequently, one network head for eachtreatment is built on the deep-balanced representations to estimate ITE. However, due to the commonly usedtwo-head design in existing work, these models cannot be easily generalized to continuous treatment settings.",
  "Methodology": "In this section, we begin by defining the problem and providing an overview of DTRNet. Next, we delve intothe functions and explanations of each component of our DTRNet. Following this, we provide theoreticalsubstantiation that our devised re-weighting function is able to precisely adjust for selection bias. Lastly, wediscuss the advantages of our approach in comparison to other state-of-the-art models.",
  "Problem Setting": "Let D = {xi, yi, ti}Ni=1 denote a dataset of size N, where xi, yi, ti are independent realisations of randomvariables X, Y, T with support (X, Y, T = ), respectively. We refer to X Rm as covariates, whichcontain information about features of an unit. Y represents the outcome, and T represents the continuoustreatment in the range from 0 to 1 (Nie et al., 2021). Our goal is to estimate the Individualized Dose-ResponseFunction (IDRF) with continuous treatments by eliminating selection bias.",
  "(t, x) = E[Y (T = t)|X = x, T = t](2)": "We assume that covariates are generated from three underlying factors: (1) instrumental factors ((x)) thatare associated with the treatment but not with the outcome except through the treatment, (2) confounderfactors ((x)) that are associated with both the treatment and outcome, and (3) adjustment factors ((x))that are predictive of the outcome but not associated with the treatment. Therefore, the treatment assignmentis affected by instrumental factors and confounder factors, while the outcome is affected by confounder factorsand adjustment factors. This assumption is commonly used in previous research to disentangle the covariatesfor precise information extraction (Yao et al., 2021; Wu et al., 2020), an illustrative example can be found inAppendix I. The underlying relationship can be illustrated using a causal graph shown in (a). It isimportant to note that IDRF identification is made under the following convention assumptions (Wu et al.,2020; Kuang et al., 2017; Hassanpour & Greiner, 2019a).",
  "DTRNet Model": "DTRNet is designed according to the causal graph in . It first learns disentangled representationsof covariates and subsequently estimates T and Y based on the corresponding representations. Finally,it corrects for selection bias by utilizing relevant representations instead of the entire set. In particular,three contracted feedforward neural networks are utilized to obtain disentangled representations of threefactors {(xi), (xi), (xi)} defined in .1. Then (xi) and (xi) are concatenated to predict thedistribution of ti using a conditional density estimator P(ti|(xi), (xi)). (xi) and (xi) are used to predictthe final outcome through a varying coefficient network g(ti)((xi), (xi)), while (xi) attempts to encodelittle information about treatment. Typically, a re-weighting function is responsible for precisely adjusting forselection bias. The DTRNet framework is shown in (b), with the objective function defined as follows:",
  "i=1(w(ti, (xi), (xi))Ly + LT + Lind + Lreg) + Ldisc,(3)": "where w(ti, (xi), (xi)) denotes the re-weighting function to mitigate selection bias; Ly and LT are theprediction losses for outcome Y and treatment T, respectively. Ldisc quantifies discrepancies between latentrepresentations ((xi), (xi), (xi)); Lind promotes the independence between (xi) and treatment ti; whileLreg serves as a regularization term against overfitting. , , , and are hyperparameters balancing thedifferent terms in the objective function. In the following, we present the details of each term. Factual Loss.Factual loss is used to force (xi) and (xi) to extract more predictive information fromthe covariates. As three arrows pointing to Y in (a), we aim to estimate the factual outcome yi fromthree input, (xi), (xi), and assigned treatment ti for unit i. To simultaneously preserve the influenceof treatment and maintain the continuity of the dose-response curve, we adopt a varying coefficient neuralnetwork g(ti) to estimate outcomes (Nie et al., 2021). Factual loss is computed by comparing the groundtruth yi with our estimated value:",
  "Ly = Lyi, g(ti)((xi), (xi)),(4)": "where L represents the loss function and we use the mean square error in our paper. The varying coefficientstructure utilizes a function g(t) with varying parameters (t) instead of fixed parameters to predict outcomes.By leveraging this structure, continuous treatment effect can be estimated by incorporating continuous tin outcome estimation. Especially, a B-spline of degree p with q knots, resulting in k = p + q + 1 basis, isused to model (t). Let B = [b1, b2, ..., bk] Rnk denote the spline basis for the treatment T Rn1. For asingle-layer feedforward network with m inputs and n outputs, the function is given by f(t) = ki=1(bi(XW)), where W Rmnk is the optimizable weight. Treatment Loss.We incorporate a treatment loss in our model to enhance the encoding of (xi) and(xi) with respect to ti. Prior studies have usually predicted the probability of treatment using the fullcovariate representations (Shi et al., 2019; Nie et al., 2021). This approach can inadvertently leverageirrelevant information, such as adjustment factors, from the representations. To mitigate this, we estimate",
  "D(xi)(log((xi)) log((xi))),(7)": "where D is the dimension of the latent embedding, LD((x), (x)) denotes the average divergence lossbetween (x) and (x) across the units inspired by the KL divergence. If (xi) and (xi) are identical forall unites, the result will be 0. On the contrary, if (xi) and (xi) are distinctly different for all units, thevalue will be higher. Therefore, through the collaboration of other modules, Ldisc encourages all disentangledrepresentations to encode only the relevant information. The definition of LD and additional details can befound in Appendix E. Independent Loss.Independent loss is specifically applied to balance the (xi) instead of the entirerepresentations, ensuring that the learned factors (xi) do not contain any information about ti and thatall information related to ti is encoded in (xi) and (xi). Previous studies (Shalit et al., 2017; Johanssonet al., 2016; Hassanpour & Greiner, 2019a;b; Acharki et al., 2023; Guo et al., 2023) have emphasized theneed to balance adjustment representations for the treatment group and control group (binary treatmentsetting), most of them done by inducing the desired independence using a discrepancy loss. However, withinthe framework of continuous treatments, the endeavor to achieve balance adjustment representations forevery value ti becomes infeasible. Hence, we intend to push (xi) to embed little information about thetreatment by forcing the performance of the treatment probability estimation from adjustment representationto be poor, which motivates us to minimize the following \"positive\" log-likelihood loss:",
  "Lind = log(P(ti|(xi)).(8)": "In particular, this independent loss allows us to encode all information of ti in (xi) and (xi) instead of(xi), which facilitates the precisely adjustment for selection bias through the re-weighting function (discussedin next paragraph). This is one of the key contributions of our paper. Furthermore, compared to previousstudies that balance the entire representations of covariates, our approach does not balance confounder factorssince they contain valuable information about outcome prediction (Hassanpour & Greiner, 2019b). Moreover,we also exclude instrumental factors since they are related to the treatment assignment and independent ofall confounders (Wu et al., 2022). Hence, they should not be balanced. Re-weighting Function.Recall one of our objectives is to precisely eliminate selection bias. Inspiredby (Imbens, 2000; Kloek & Van Dijk, 1978), we derive propensity score P(ti|(xi), (xi)) from (xi) and(xi) and use the inverse of it to re-weight the prediction loss of outcomes as follows:",
  "DTRNet (Ours)0.1414 0.02560.0131 0.00721.7846 0.22020.0104 0.00443.5376 0.52850.4254 0.3710": "where P(ti|(xi), (xi)) is the direct output of the conditional density estimator for treatment. Hence, it doesnot require additional computation or prior knowledge about the treatment distribution as in (Hassanpour &Greiner, 2019b). Furthermore, we can precisely remove bias attributable to the confounder and instrumentalfactors instead of the unrelated part ((xi)). A detailed proof follows.",
  "Theoretical Proof of Bias Elimination": "In this section, we outline the theoretical derivation of our re-weighting function, inspired by the importancesampling theory (Hassanpour & Greiner, 2019a; Kloek & Van Dijk, 1978). We then provide proofs for itsdebiasing ability. Definition 3.6. Let , : X R be the representation functions for confounder factors and adjustmentfactors, respectively. Let g(t) : R R T Y be the prediction function defined in the representationspace R R. The expected loss for the unit and treatment pair (x, t) is:",
  "MISEAMSEMISEAMSEMISEAMSE": "original0.14140.01311.78460.01043.53760.4254alpha0.1413( 0.0% )0.0132( 0.8% )2.6400( 47.9% )0.0291( 179.8% )3.606( 1.9% )0.5115( 39.2% )beta0.1414( 0.0% )0.0137( 4.6% )2.6547( 48.8% )0.0319( 206.7% )3.7856( 7.0% )0.6310( 48.3% )gamma0.1411 ( 0.2% )0.0131( 0.0% )2.6871( 50.5% )0.0297( 185.5% )3.5951( 2.0% )0.4955( 16.5% )re-weighting0.1640( 16.0% )0.0212( 61.8% )2.2790( 27.7% )0.0136( 30.7% )3.6804( 4.0% )0.9521( 123.8% ) The theorem implies that the expected loss function for counterfactual outcomes can be derived by re-weightingthe factual loss. Hence, we can cooperate the counterfactual loss with the factual loss and optimise themtogether through a designed weight; w = 1 + P(x,t)",
  "Discussions": "Our proposed DTRNet is built upon existing works such as (Hassanpour & Greiner, 2019b) and (Nie et al.,2021). However, our approach extends the existing works in several significant ways. (1) The disentangledmethod in (Hassanpour & Greiner, 2019b) can only provide a causal effect under binary treatments, whileour method facilitates a mixed type of treatments, including continuous treatments. (2) We introduce a noveldesign for the independent loss. Instead of simply and brutally minimizing the discrepancy between the entiretreatment representations and the control representations, we minimize the amount of treatment informationthat can be extracted from adjustment factors, which facilitates the precise adjustment for selection bias inre-weighting function. (3) We use the direct output of the conditional density estimator for re-weighting,which helps precisely eliminate bias and does not require additional computation or prior knowledge about thetreatment distribution. Furthermore, in the ablation study, we demonstrate the importance of this componentfor model performance. (4) We provide rigorous theoretical proofs substantiating the debiasing ability of there-weighting function. Finally, our DTRNet employs a discrepancy loss between different deep representations,which ensures that each representation only encodes the relevant information. Overall, DTRNet offers anextensive ability to generate disentangled representations on which selection bias can be precisely adjusted.",
  "Experiments": "In this section, we present extensive experimental results on three datasets to demonstrate the effectivenessof DTRNet and address the following three research questions: Q1: How effective is DTRNet in estimatingIDRF and adjusting for selection bias compared to the state-of-the-art methods? Q2: What is the individualcontribution of each component in our model, including the treatment loss, discrepancy loss, independentloss, and re-weighting function? Q3: Can deep disentangled representations be effective in identifying thethree underlying factors?",
  ": MISE sensitivity analysis with different values of , ,, and different proportions of re-weightingvalue on 50 repeats of IHDP datasets. The standard deviation band is also plotted": "0.2 0.4 0.6 0.8 1.0 2.0 3.0 5.0 alpha 0.00 0.25 0.50 0.75 1.00 1.25 1.50 AMSE 0.2 0.4 0.6 0.8 1.0 2.0 3.0 5.0 beta 0.00 0.25 0.50 0.75 1.00 1.25 1.50 AMSE 0.1 0.3 0.5 0.7 0.9 2.0 3.0 5.0 gamma 0.00 0.25 0.50 0.75 1.00 1.25 1.50 AMSE 0.1 0.3 0.5 0.7 0.9 2.0 3.0 5.0 lambda 0.00 0.25 0.50 0.75 1.00 1.25 1.50 AMSE 0.3 0.7 1.0 2.0 3.0 reweight 0.50 0.25 0.00 0.25 0.50 0.75 1.00 1.25 1.50 AMSE 0.2 0.4 0.6 0.8 1.0 2.0 3.0 5.0 alpha 2.0 2.5 3.0 3.5 4.0 4.5 5.0 MISE 0.2 0.4 0.6 0.8 1.0 2.0 3.0 5.0 beta 2.5 3.0 3.5 4.0 4.5 5.0 MISE 0.1 0.3 0.5 0.7 0.9 2.0 3.0 5.0 gamma 2.5 3.0 3.5 4.0 4.5 5.0 MISE 0.1 0.3 0.5 0.7 0.9 2.0 3.0 5.0 lambda 2.5 3.0 3.5 4.0 4.5 5.0 MISE 0.3 0.7 1.0 2.0 3.0 reweight 2.5 3.0 3.5 4.0 4.5 5.0 MISE",
  ": AMSE sensitivity analysis with different values of , ,, and different proportions of re-weightingvalue on 50 repeats of IHDP datasets. The standard deviation band is also plotted": "treatments and outcomes given real-world features (Curth et al., 2021; Wang et al., 2022; Bellot et al., 2022;Nie et al., 2021). We follow this convention to build one synthetic dataset and two semi-synthetic datasets:News1 and IHDP (Hill, 2011) for evaluation. To evaluate the effectiveness of selection bias adjustment, weintentionally design all training sets to contain selection bias, while test sets are unbiased.Hence, if a model is trained on a biased training set and performs well on the corresponding test set, itprovides evidence of the models ability to eliminate bias. We generate the three datasets following (Nie et al.,2021). Details can be found in Appendix G. We evaluate the performance of our proposed DTRNet againstseveral state-of-the-art methods for IDRF estimation, including Dragonet, Dragonet_TR (Shi et al., 2019),DRNet, DRNet_TR (Schwab et al., 2020), VCNet, VCNet_TR (Nie et al., 2021), and TransEE (Zhanget al.), where TR refers to targeted regularization. The details of baselines are as follows. Dragonet: (Shi et al., 2019) used a three-headed architecture to predict the propensity score andconditional outcome from covariates and treatment information. The model was later improvedby(Nie et al., 2021) by using separate heads for treatment in different intervals to adjust for continuoustreatments. DRNet: (Schwab et al., 2020) proposed to divide continuous treatments into several intervals andassign one head to each interval to generate the dose-respond curve. Following (Nie et al., 2021),DRNet was improved by adding a conditional density estimation head for treatment estimation.",
  "TransTEE: (Zhang et al.) adopted the transformer backbones to estimate treatment effect": "CIRCE: (Pogodin et al., 2022) introduces CIRCE as a regularizer in settings where X is used topredict Y . Specifically, the regularizer aims to learn neural features (X) of the data X such that(X) is independent of some metadata Z given Y . We also include non-neural network based models: Causal Forest (Wager & Athey, 2018), Bart (Chipman et al.,2010) and GPS (Imbens, 2000), due to the space limitation, we show the performance in the Appendix D.",
  "Implementation Details": "All the neural network-based methods are trained for 800 epochs with the SGD optimizer (momentum =0.9). To mitigate the risk of overfitting or underfitting, we apply an early stop technique. For the threedeep representation networks in our model, we implement them as fully connected networks with two hiddenlayers, and each layer has 50 hidden units using ReLU activation. We also use two-hidden-layer settings (eachwith 50 hidden units) for the Y prediction network. The choice of the model structure aligns with previousapproaches (Zhang et al.; Nie et al., 2021). We use grid search tuning to tune the following hyperparameters:, , {0.1, 0.2, 0.4, 0.6}. The choice of this range is based on the common practice of weighting loss termsbetween 0 and 1. Specifically, in our paper, these hyperparameters refer to the weighting of each loss term,except for the factual loss (the loss between yi and yi). Since the factual loss represents the prediction y, whichis a crucial component of the total loss, assigning a high weight to other terms could diminish the impact ofthe factual loss. Therefore, we chose the range {0.1, 0.2, 0.4, 0.6}, also considering computational and timecosts. Typically, we choose the learning rate lr {0.0001, 0.00005, 0.00001}. For other hyperparameters, e.g.,the number of knots and the degree of B-spline, we follow the setting of (Nie et al., 2021) which is also tunedon the same configurations of datasets. For each dataset, we generate 50 runs for training and 20 runs fortuning the aforementioned hyperparameters. The best hyperparameter settings for our method as well as thebaseline can be found in Appendix F.",
  "i=1( yi(t) yi(t))]2,(17)": "where |T | is the number of different treatment values. To ensure fair and reliable comparisons, we evaluatethe performance of all models on 50 repetitions of three different datasets and report the mean and standarddeviation of the MISE and AMSE. As presented in , DTRNet consistently outperforms the majorityof baselines across all datasets, achieving satisfactory MISE and the lowest AMSE while demonstratingcommendable stable performance. Specifically, the less satisfactory MISE performance on the News datasetis attributable to the distinct data generation process, where all features act as confounders. This does notalign with real-world scenarios or our study assumptions, which is why the performance is slightly lowercompared to some of the baselines. These results demonstrate not only the effectiveness of DTRNet in IDRFestimation but also its ability to adjust for selection bias.",
  ": t-SNE plots of the deep representations with/without Ldisc. KL-D represents KL-Divergencebetween three representations": "w(ti, (xi), (xi)). We demonstrate their roles by setting the corresponding hyperparameter to 0 whilekeeping the other hyperparameters fixed at the best-tuned values. Especially, to evaluate the re-weightingfunction, we set the value of re-weighting to 1. As shown in , all components contribute to themodel performance, as evidenced by the varying extent of performance drop in most scenarios when anypart is removed. Moreover, we find that the re-weighting function and the discrepancy loss are two criticalcomponents of our model due to the significant increase of test error when they are disabled. In otherwords, adjusting for selection bias and forcing all representations to encode the corresponding informationindependently are predominant for the model performance. Additionally, to give a visual demonstration ofthe contribution of the discrepancy loss, we show the t-SNE plots of the model trained with and withoutLdisc on a synthetic dataset in . After incorporating the discrepancy loss, the distinct representationsbecome more separate, leading to a larger KL-divergence (e.g. 8.08 vs 3.48). However, the re-weightingfunction contributes less to the News dataset than the other dataset due to the distinct data generationprocess of News as discussed above. In particular, all features are associated with treatment assignment andoutcome generation, which means that all features act as confounders. Consequently, it presents challengesfor (xi) and (xi) in accurately learning instrumental and adjustment factors. Inaccuracies in (xi) and(xi) can also affect re-weighting function estimation, leading to a limited contribution.",
  "Disentanglement Performance": "To answer Q3, we attempt to explore whether the representations capture the corresponding factors byutilizing t-SNE to visualize the three deep representations in a synthetic dataset with respect to differenttypes of variables, as shown in . Since we have knowledge of the data generation process of the syntheticdataset, we know the ground truth of which features are instrumental, confounder, and adjustment variablesin the dataset. Hence, for each type of variable, we choose one feature to show the relationship betweenit and the three deep representations. In , the color shade indicates the value of the correspondingvariable, while the shape of the points denotes the type of representations. For example, in the first plot, weaim to verify if the representations of instrumental factors embed the information of instrumental variablesin the covariates. The plot shows that the color shading changes with the direction of the correspondingarrow in the instrumental representation (dot points), indicating that instrumental representations encodeinformation about the instrumental variable. Specifically, this means that the instrumental representationslearn information about the instrumental variables and can distinguish between large and small values of thecorresponding variable. However, in other representations, such as and (cross points and squared points,respectively), the color shade does not change regularly, indicating that the confounder and adjustmentrepresentations do not encode information about instrumental variables. Therefore, our DTRNet can decentlydisentangle the three factors.",
  "Conclusion": "In this paper, we introduce Disentangled Representation Network (DTRNet), a novel model designedto estimate the individualized dose-response function (IDRF) with high precision while accounting forselection bias through disentangled representations of instrumental, confounder, and adjustment factors. Ourexperiments on synthetic and semi-synthetic datasets show the exceptional disentanglement capabilities ofDTRNet and highlight its impressive performance on estimating IDRF, surpassing current SOTA methods.",
  "Limitations and Future Work": "Although we propose an advanced model for estimating the unbiased IDRF, there are still areas for improve-ment. First, our method currently employs a basic density estimator to estimate the treatment effect. Thischoice was made for proof of concept. Hence, exploring more sophisticated methods could further enhanceperformance. Similarly, our use of independent loss may not be optimal. Alternative approaches, such as theHilbert-Schmidt Independence Criterion (HSIC), could potentially improve results. Hence, future work couldfocus on exploring these alternatives to improve model performance. Moreover, our study utilizes only synthetic or semi-synthetic datasets, which allows us to generate the ground-truth IDRF using predefined equations. However, in real-world scenarios, it is challenging to obtain the trueIDRF and to differentiate between instrumental, confounder, and adjustment features. This complicates theevaluation of the model and its ability to correctly disentangle these features in practical settings. An expertin the relevant domain may be necessary to properly assess the model performance on real-world datasets.Therefore, a key direction for future research is to investigate the deployment of our model in real-worldsituations.",
  "Alexis Bellot, Anish Dhir, and Giulia Prando. Generalization bounds and algorithms for estimating conditionalaverage treatment effect of dosage. arXiv preprint arXiv:2205.14692, 2022": "Ioana Bica, James Jordon, and Mihaela van der Schaar.Estimating the effects of continuous-valuedinterventions using generative adversarial networks. Advances in Neural Information Processing Systems,33:1643416445, 2020. Vinod K Chauhan, Soheila Molaei, Marzia Hoque Tania, Anshul Thakur, Tingting Zhu, and David A Clifton.Adversarial de-confounding in individualised treatment effects estimation. In International Conference onArtificial Intelligence and Statistics, pp. 837849. PMLR, 2023.",
  "Zhixuan Chu and Sheng Li. Causal effect estimation: Recent progress, challenges, and opportunities. MachineLearning for Causal Inference, pp. 79100, 2023": "Zhixuan Chu, Stephen L Rathbun, and Sheng Li. Matching in selective and balanced representation spacefor treatment effects estimation. In Proceedings of the 29th ACM International Conference on Information& Knowledge Management, pp. 205214, 2020. Zhixuan Chu, Stephen L Rathbun, and Sheng Li. Graph infomax adversarial learning for treatment effectestimation with networked observational data. In Proceedings of the 27th ACM SIGKDD Conference onKnowledge Discovery & Data Mining, pp. 176184, 2021. Zhixuan Chu, Stephen L Rathbun, and Sheng Li. Learning infomax and domain-independent representationsfor causal effect inference with real-world data. In Proceedings of the 2022 SIAM International Conferenceon Data Mining (SDM), pp. 433441. SIAM, 2022a.",
  "Zhixuan Chu, Stephen L Rathbun, and Sheng Li. Multi-task adversarial learning for treatment effectestimation in basket trials. In Conference on health, inference, and learning, pp. 7991. PMLR, 2022b": "Zhixuan Chu, Mechelle Claridy, Jose Cordero, Sheng Li, and Stephen L Rathbun. Estimating propensityscores with deep adaptive variable selection. In Proceedings of the 2023 SIAM International Conference onData Mining (SDM), pp. 730738. SIAM, 2023a. Zhixuan Chu, Ruopeng Li, Stephen Rathbun, and Sheng Li. Continual causal inference with incrementalobservational data.In 2023 IEEE 39th International Conference on Data Engineering (ICDE), pp.34303439. IEEE, 2023b. Zhixuan Chu, Mengxuan Hu, Qing Cui, Longfei Li, and Sheng Li. Task-driven causal feature distillation:Towards trustworthy risk prediction. In Proceedings of the AAAI Conference on Artificial Intelligence,volume 38, pp. 1164211650, 2024. Alicia Curth and Mihaela van der Schaar. Nonparametric estimation of heterogeneous treatment effects:From theory to learning algorithms. In International Conference on Artificial Intelligence and Statistics,pp. 18101818. PMLR, 2021.",
  "Jennifer L Hill. Bayesian nonparametric modeling for causal inference. Journal of Computational andGraphical Statistics, 20(1):217240, 2011": "Wei Huang and Zheng Zhang. Nonparametric estimation of the continuous treatment effect with measurementerror. Journal of the Royal Statistical Society Series B: Statistical Methodology, 85(2):474496, 2023. Yiyan Huang, WANG Siyi, Cheuk Hang Leung, WU Qi, WANG Dongdong, and Zhixiang Huang. Dignet:Learning decomposed patterns in representation balancing for treatment effect estimation. Transactions onMachine Learning Research, 2024.",
  "Guido W Imbens. The role of the propensity score in estimating dose-response functions. Biometrika, 87(3):706710, 2000": "Andrew Jesson, Alyson Douglas, Peter Manshausen, Malys Solal, Nicolai Meinshausen, Philip Stier, Yarin Gal,and Uri Shalit. Scalable sensitivity and uncertainty analyses for causal-effect estimates of continuous-valuedinterventions. Advances in Neural Information Processing Systems, 35:1389213907, 2022. Ziyang Jiang, Zhuoran Hou, Yiling Liu, Yiman Ren, Keyu Li, and David Carlson. Estimating causal effectsusing a multi-task deep ensemble. In International Conference on Machine Learning, pp. 1502315040.PMLR, 2023.",
  "Donald B Rubin. Estimating causal effects of treatments in randomized and nonrandomized studies. Journalof educational Psychology, 66(5):688, 1974": "Patrick Schwab, Lorenz Linhardt, Stefan Bauer, Joachim M Buhmann, and Walter Karlen.Learningcounterfactual representations for estimating individual dose-response curves. In Proceedings of the AAAIConference on Artificial Intelligence, volume 34, pp. 56125619, 2020. Uri Shalit, Fredrik D Johansson, and David Sontag. Estimating individual treatment effect: generalizationbounds and algorithms. In International Conference on Machine Learning, pp. 30763085. PMLR, 2017.",
  "Liuyi Yao, Zhixuan Chu, Sheng Li, Yaliang Li, Jing Gao, and Aidong Zhang. A survey on causal inference.ACM Transactions on Knowledge Discovery from Data (TKDD), 15(5):146, 2021": "Jinsung Yoon, James Jordon, and Mihaela Van Der Schaar. Ganite: Estimation of individualized treatmenteffects using generative adversarial nets. In International Conference on Learning Representations, 2018. Lijun Zhang, Tianbao Yang, and Rong Jin. Empirical risk minimization for stochastic convex optimization:o(1/n)-and o(1/n2)-type of risk bounds. In Conference on Learning Theory, pp. 19541979. PMLR, 2017.",
  "(23)": "This theorem states that the weighted loss is an unbiased estimation of the IDRF loss, which indicates there-weighting function in our can precisely eliminate selection bias. Notably, each term in our loss functioncontributes meaningfully to the theoretical proof. Specifically, some components of the loss function serveto support the assumptions underpinning our proof, as outlined in Assumptions 1-4 in our paper. Theirfunctions are introduced as follows. In Assumption 4, we posit that X should be decomposed into threedifferent factors, where discrepency loss Ldis is used to ensure this. Lind enforces us to encode all informationof ti in (xi) and (xi) instead of (xi), thereby facilitating a more precise and accurate estimation of theweight w. Moreover, the weight w before factual loss is the key of the proof, hence we use the LT loss to getthe accurate weight.",
  "BReason to choose the MISE and AMSE metric": "MISE and AMSE are widely used metrics for IDRF evaluation (Zhang et al.; Nie et al., 2021; Bellot et al.,2022; Zhu et al., 2024a). Therefore, we follow these previous works in adopting these metrics as our evaluationmetrics. Typically, MISE and AMSE assess the performance of the models by computing the differencebetween the estimated curve and the ground-truth curve at both the individual and population levels. Giventhe specific design of our dataset, if a model is trained on the biased dataset and shows low MISE and AMSEon the unbiased evaluation dataset, it indicates that the model is capable of adjusting for selection bias.",
  "CBaseline Methods": "We compare our model DTRNet with several state-of-the-art methods on continuous treatment effectestimation, including Dragonet, Dragonet_TR (Shi et al., 2019), DRNet, DRNet_TR (Schwab et al., 2020),VCNet, VCNet_TR (Nie et al., 2021), and TransEE (Zhang et al.). TR refers to targeted regularization, atechnique used to improve accuracy. The details of baselines are as follows. Dragonet: (Shi et al., 2019) used a three-headed architecture to predict the propensity score andconditional outcome from covariates and treatment information. The model was later improvedby(Nie et al., 2021) by using separate heads for treatment in different intervals to adjust for continuoustreatments. DRNet: (Schwab et al., 2020) proposed to divide continuous treatments into several intervals andassign one head to each interval to generate the dose-respond curve. Following (Nie et al., 2021),DRNet was improved by adding a conditional density estimation head for treatment estimation.",
  "DPerformance comparison between DTRNet and non-deep neural network model": "We conducted our experiments under identical settings with (Nie et al., 2021), including the same datageneration process, dataset, and problem setting. Therefore, we compared our methods against non-deeplearning approaches such as causal forest, BART, and GPS (reported by (Nie et al., 2021)) on these threedatasets. The results from our experiments consistently indicate that our methods exhibit superior, or at thevery least, comparable performance in comparison to these alternative approaches.",
  "EDetails about the divergence loss": "Typically, we do not include the divergence loss between the instrumental representations (xi) and theadjustment representations (xi). In our model, instrumental (xi) and confounder representations (xi)are used to estimate the probability of treatment, while confounder (xi) and adjustment representations(xi) are used to predict the outcome. Hence, instrumental and confounder representations are more similar,as are confounder and adjustment representations, since they are optimized for similar goals. Therefore,we only penalize these. To avoid making our model cumbersome, we choose not to include the divergenceloss between instrumental (xi) and adjustment representations (xi). In our work, we specifically use theimplementation in PyTorch to compute LD: KLDivLoss.",
  "FImplementation Details and Hyperparameter Tuning": "All the neural network-based methods are trained for 800 epochs with the SGD optimizer (momentum =0.9). To mitigate the risk of overfitting or underfitting, we apply an early stop technique. For the threedeep representation networks in our model, we implement them as fully connected networks with two hiddenlayers, and each layer has 50 hidden units using ReLU activation. We also use two-hidden-layer settings(each with 50 hidden units) for the Y prediction network. We used grid search tuning to tune the followinghyperparameters: , , {0.1, 0.2, 0.4, 0.6} and the learning rate (lr) {0.0001, 0.00005, 0.00001}. For otherhyperparameters, e.g., the number of knots and the degree of B-spline, we follow the setting of (Nie et al.,2021) that is also tuned on the same configurations of datasets. For each dataset, we generate 50 runs fortraining and 20 runs for tuning the aforementioned hyperparameters. The best hyperparameter settings areas follows: For the baselines, we used the best hyperparameters for each dataset as reported by the authors Zhanget al. (2017); Nie et al. (2021). The baseline models were also tuned on the same datasets with 20 runsusing grid search. The tuning ranges for Dragonet, DRNet, and VCNet were as follows: learning rate (lr) {0.05, 0.005, 0.001, 0.0005, 0.0001} and {1, 0.5}. For the TR versions of these models, the learning ratefor (t) was lr {0.001, 0.0001}, and {20, 10, 5} n12 . For TransTEE, it was also tuned on these threedatasets, but the authors only provided the final best hyperparameters used in their paper without specifyingthe range. Below are the best hyperparameters that we adopted.",
  "where t = (1 + exp(t))1. According to the above equation, x2, x5 are instrumental variables,x1, x3, x4 are confounder variables, and x6 is the adjustment variable": "IHDP. Infant Health and Development Program (IHDP), an RCT dataset, originally compiled toestimate the causal effect of binary treatment (home visits of specialists) on future cognitive testscores. The original dataset contains 747 samples with 25 features, in order to estimate continuouscausal effect, we generate the treatment and outcome following (Nie et al., 2021). We randomly splitthe dataset into training set (67%) and testing set (33%).:",
  ": t-SNE plots of the deep represetations with/without Ldisc across three datasets. For each dataset,we choose one repeat for visualisation due to the space limit": "News. The News benchmark is composed of news items from the NY Times corpus, and eachitem is represented by its word counts. Previous work used it to evaluate the causal effect of devices(binary treatment) on readers opinions. The original News dataset contains 3000 item samples. Sincethe dataset is used to estimate the effect of binary treatment, we follow (Nie et al., 2021) to generatecontinuous treatments as well as the corresponding outcome as follows, we first generate v1, v2 and",
  "t)) + N(0, 0.5),(26)": "According to the generation algorithm, X and T are highly associated, meaning that the dataset containsconfounding bias. We then split this biased dataset into training and testing sets. However, we do not use thisbiased test set directly for evaluation, as our goal is to estimate the IDRF for each x, rather than estimating yfor a single t corresponding to x. Furthermore, to measure the debiasing capability of the proposed methods,we follow the approach described in (Nie et al., 2021; Zhang et al.). Specifically, to construct the IDRF foreach individual, all existing t values in the test set are used to generate the IDRF using y|x, t along with itsx value. This process ensures that there is no association between x and t. The average of all y are usedas the ground truth for evaluation. This approach ensures that there is no correlation between t and x, asthe generation of t no longer relies on t|x, thereby eliminating selection bias and allowing us to test on anunbiased dataset.",
  "HDiscrepancy loss on other dataset": "In addition, to give a visual demonstration of the contribution of the discrepancy loss, we show the t-SNEplots of models trained with and without Ldisc on each dataset in . Points belonging to differentfactors (i.e., {(xi), (xi), and(xi)}) are further away under the model trained with the discrepancy loss,indicating that the discrepancy loss indeed helps learn separable representations. Moreover, except for theNews dataset, points belonging to the same factor are closer together with Ldisc, suggesting that they extractsimilar information. However, (xi) in the News dataset becomes more separate, and (xi) gathers less closecompared to other datasets, indicating that it cannot well extract the corresponding information, which isconsistent with the explanation of re-weighting above.",
  "IAn instantiation of causal graph": "In this section, we provide an example of the instantiation of our causal graph. Similar to (Wu et al., 2020):In the context of medical health record, we might collect extensive historical data from each patient, includingthe patients features X (e.g., age, gender, living environment, doctor-in-charge), treatment of patients T(taking a particular mediciine or not), and the final outcome Y (cured or not). Among these features, ageand gender simultaneously affect the treatment (as a physician would consider these factors when choosinga treatment) and the outcome (since they can also affect the patients recovery rate); therefore, they areconfounding factors . In contrast, the doctor-in-charge would influence only the treatment decision, withoutaffecting the outcome, thus being an instrumental factor . The environment (e.g. hygiene situation), whichonly affects the outcome but not the treatment, falls into the category of adjustment factors .",
  "JSensitivity Analysis based on AMSE": "In this section, we present a sensitivity analysis based on AMSE, as shown in . The trend observedis similar to that of the MISE, which is presented in the main text. This analysis demonstrates that there-weighting function and the discrepancy loss have a more substantial influence on the models performance.These findings are consistent with those from our previous ablation study."
}