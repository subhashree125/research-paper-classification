{
  "Abstract": "Constructing model-agnostic group equivariant networks, such as equitune (Basu et al.,2023b) and its generalizations (Kim et al., 2023), can be computationally expensive for largeproduct groups. We address this problem by providing efficient model-agnostic equivariantdesigns for two related problems: one where the network has multiple inputs each withpotentially different groups acting on them, and another where there is a single input butthe group acting on it is a large product group. For the first design, we initially consider alinear model and characterize the entire equivariant space that satisfies this constraint. Thischaracterization gives rise to a novel fusion layer between different channels that satisfies aninvariance-symmetry (IS) constraint, which we call an IS layer. We then extend this designbeyond linear models, similar to equitune, consisting of equivariant and IS layers. We alsoshow that the IS layer is a universal approximator of invariant-symmetric functions. Inspiredby the first design, we use the notion of the IS property to design a second efficient model-agnostic equivariant design for large product groups acting on a single input. For the firstdesign, we provide experiments on multi-image classification where each view is transformedindependently with transformations such as rotations. We find equivariant models are robustto such transformations and perform competitively otherwise. For the second design, weconsider three applications: language compositionality on the SCAN dataset to productgroups; fairness in natural language generation from GPT-2 to address intersectionality;and robust zero-shot image classification with CLIP. Overall, our methods are simple andgeneral, competitive with equitune and its variants, while also being computationally moreefficient.",
  "Introduction": "Equivariance to group transformations is crucial for data-efficient and robust training of large neural net-works. Traditional architectures such as convolutional neural networks (CNNs) (LeCun et al., 1998), Al-phafold (Jumper et al., 2021), and graph neural networks (Gilmer et al., 2017) use group equivariance forefficient design. Several works have generalized the design of equivariant networks to general discrete (Cohen& Welling, 2016) and continuous groups (Finzi et al., 2021b). Recently, Puny et al. (2021) introduced frameaveraging, which makes a non-equivariant model equivariant by averaging over an appropriate frame or anequivariant set. One advantage of this method is that it can be used to finetune pretrained models, leveragingthe benefits of pretrained models and equivariance simultaneously (Basu et al., 2023b;a; Kim et al., 2023). Methods based on frame-averaging have high computational complexity when the frames are large. And,in general, it is not trivial to find small frames. Hence, several frame averaging-based methods, such asequitune (Basu et al., 2023b) and its generalizations (Basu et al., 2023a; Kim et al., 2023), simply use theentire group as their frames. As such, these methods attain perfect equivariance at high computational costfor large groups. Similar computational issues arise when a network has multiple inputs with each input",
  "(b)": ": Plots comparing the performance of equizero and multi-equizero on the Imagenet dataset withrandom rotations using pretrained CLIP models. (a) Shows that the memory required for equizero increaseslinearly with an increase in group size, whereas for multi-equizero, the memory required is proportional tothe sum of the sizes of the smaller groups that comprise the larger group. (b) Shows that multi-equizero, justas equizero, is able to benefit from equivariance and significantly outperforms the non-equivariant model.",
  "is written as gf(x) for all g G, x X. We call a function f : X Y (G1, G2)-invariant-symmetric inthat order of groups, if f(g1x) = f(x) for all x X and g1 G1, and f(x) = g2f(x) for all x X, g2 G2": "Model-agnostic group equivariant networksThere has been a recent surge of interest in designingmodel-agnostic group equivariant network designs, such as equitune (Basu et al., 2023b), probabilistic sym-metrization (Kim et al., 2023), -equitune (Basu et al., 2023a), and canonicalization (Kaba et al., 2023).These designs are based on the frame-averaging method (Puny et al., 2021), where a (potentially pretrained)non-equivariant model is averaged over an equivariant set, called a frame. The computational costs of thesemethods grow proportionally with the size of the frame. Finding small frames for general groups and tasksis not trivial, hence, several previous works such as equitune, probabilistic symmetrization, and -equitunesimply use the entire group as the frame. These methods become inefficient for large groups. Canonical-ization uses a frame of size exactly one but assumes a small auxiliary equivariant network is given, whichmight itself require frame-averaging. Canonicalization also assumes a known map from the outputs of thisauxiliary network to group elements, which is non-trivial for general groups. Moreover, canonicalization doesnot provide good zero-shot performance as do some special cases of -equitune. Thus, it is crucial to designefficient frame-averaging techniques for large groups.",
  "Given a pretrained model M : X Y and a group G, equitune produces the equivariant model MG asMG =1|G|(": "gG g1M(gx)), which makes |G| passes through the model M. Thus, as the size of the groupgrows, so does the complexity of several of these frame-averaging-based methods. In this work, we considerproduct groups of the form (G1 (GN1 GN) ) and provide efficient model-agnostic equivariantnetwork designs for two related problems, as described in 3.1. Our construction has complexity proportionalto |G1| + |G2| + + |GN| compared to |G1| |G2| |GN| for equitune. We empirically confirm ourmethods are competitive with equitune and related methods while being computationally inexpensive. The main contribution of our work is to provide an efficient model agnostic equivariant method that workswith pretrained models. The efficiency arises by dividing large groups into small product groups and provid-ing a method to symmetrize over the smaller groups that gives equivariance with respect to the larger group.Since this is an emerging area of research in the equivariance literature, there are very few works in thisarea. Compared to Basu et al. (2023b) which uses a simple averaging over the entire group to obtain sym-metrization, we simply perform averaging over subgroups when the group can be decomposed as products.Kim et al. (2023); Mondal et al. (2023); Basu et al. (2022) use weighted averaging over group elements toobtain symmetrization, which is complementary to our work and can be used on top of our work for futurework. Additional related worksSeveral techniques exist to design group-equivariant networks such as pa-rameter sharing and convolutions (Cohen & Welling, 2016; Ravanbakhsh et al., 2017; Kondor & Trivedi,2018), computing the basis of equivariant space (Cohen & Welling, 2017; Weiler & Cesa, 2019; Finzi et al.,2021b; Yang et al., 2023; Fuchs et al., 2020; Thomas et al., 2018; De Haan et al., 2020; Basu et al., 2022),representation-based methods (Deng et al., 2021; Satorras et al., 2021), and regularization-based meth-ods (Moskalev et al., 2023; Finzi et al., 2021a; Patel & Dolz, 2022; Arjovsky et al., 2019; Choraria et al.,2023). These methods typically rely on training from scratch, whereas our method also works with pretrainedmodels. Atzmon et al. (2022); Duval et al. (2023) use frame-averaging for shape learning and materials modeling,respectively. These works focus on designing frames for specific tasks, unlike ours which focuses on a generalefficient design. Maile et al. (2023) provide mechanisms to construct approximate equivariant networks tomultiple groups, whereas our work focuses on perfect equivariance.",
  "Problem Formulation and Proof of Equivariance": "Multiple inputs Let X1, . . . , XN and Y1, . . . , YN be N inputs and outputs, respectively, to a neural network.Let Xi Rdi and Yi Rki. Let G1, . . . , GN be N groups acting on X1, . . . , XN respectively. That is, Gi",
  "Published in Transactions on Machine Learning Research (10/2024)": "In , we show that multi-equizero performs much better than the non-equivariant pretrained CLIPmodel on the Imagenet dataset with random rotation transformations. Further, we also note that the memoryrequired for equizero increases linearly with an increase in group size. In contrast, for multi-equizero, thememory required is proportional to the sum of the sizes of the smaller groups that comprise the largergroup. For our experiments here, we used the product groups c2, c2 c3, c2 c5, c3 c5, c2 c3 c5, andc2 c3 c5 c7, where cn is the cyclic group of size n.",
  "Characterization of the Linear Equivariant Space": "We first start with the problem of group equivariance for multiple inputs X1, . . . , XN being acted uponby groups G1, . . . , GN, respectively. Using equituning for this problem would require group averaging overthe product group G1 . . . GN, which is expensive because of the size of the product group.Thus,instead of naively applying equituning, we want first to understand how equivariance to this large group canbe obtained from a combination of layers equivariant/invariant-symmetric to smaller groups. To this end,we first characterize the entire space of linear equivariant layers for the product group using linear layersequivariant/invariant-symmetric to the smaller groups. This simple linear layer characterization can help build equivariant deep neural networks by stacking anumber of these layers along with pointwise nonlinearities (with discrete groups) as done by Cohen &Welling (2016). Further, this characterization will give us an intuition on how to construct model agnosticequivariant layers similar to equituning (Basu et al., 2023b) for the concerned group action. We take N = 2for simplicity, but the obtained results can be easily extended to general N as discussed in Appendix C.1. Let LEqGbe a G-equivariant linear matrix, i.e. LEqG (aX) = aLEqG (X) for all a G. And let LISG1,G2(x) bea (G1, G2)-invariant-symmetric linear matrix, i.e., LISG1,G2(ax) = LISG1,G2(x) = bLISG1,G2(ax) for all a G1,b G2. Then, we define multi-group equivariant linear layer as",
  "gGTr(P(g))2,(3)": "where G is a subgroup of a permutation group and let P(g) is the permutation group element correspondingto g G and Tr() denotes the trace of the P(g) matrix. Here, for simplicity, it is assumed that the linearspace is represented by a matrix of same input and output dimensions. Hence P(g) has the same dimensionsas the matrix. Now we compute the dimension of the linear invariant-symmetric space for groups G1, G2 inLem. 1, where G1 acts on the input and G2 acts on the output. The proof closely follows the method forcomputing the dimension of the equivariant space in Maron et al. (2020).",
  "Theorem 2. The linear equivariant matrix LG1,G2([X1, X2]) in equation 1 characterizes the entire space oflinear weight matrices that satisfies the equivariant constraint in equation 2": "Thus, in Thm. 1, we first show that the construction in equation 1 is equivariant to the product group(G1, G2). Then, in Thm. 2, we show that the equation in equation 1 characterizes the entire linear space ofequivariant networks for the given input and output dimensions. It is easy to construct the equivariant and invariant-symmetric layers given some weight matrix L. A linearlayer, equivariant to G can be obtained as LEqG (X) =1|G|gG g1L(gX), the same as equituning (Basuet al., 2023b). Similarly, a linear invariant-symmetric layer with respect to (G1, G2) can be obtained asLISG1,G2(X) =1",
  "We use the definition of universality used by Yarotsky (2022) as stated in Def. 1": "Definition 1. A function M : X Y is a universal approximator of a continuous function f : X Y iffor any compact set K X, > 0, there exists a choice of parameters of M such that f(x) M(x) < forall x K. Theorem 3. Let f ISG1,G2 : X Y be any continuous function that is invariant-symmetric to (G1, G2).Let M : X Y be a universal approximator of fIS.Here X, Y are such that if x X, y Y, theng1x X, g2y Y for all g1, g2 G1, G2, so that the invariant-symmetric property is well-defined. Then, weclaim that MISG1,G2 is a universal approximator of f ISG1,G2.",
  "G2 ],(6)": "where represents the concatenation of two elements, MEqGiis any model equivariant to Gi, e.g.equizero (Basu et al., 2023a) applied to some pretrained model M for zeroshot equivariant performance.Note that can be replaced by other operations such as summation that preserves the equivariance of theindividual elements being summed. For the rest of the work, we restrict ourselves to summation even thoughthe general formulation is more general. The ()InvGiand ()SymGioperations are inspired from the invariant-symmetric layers obtained in 3.2. Here, XInvGidenotes Gi-invariant feature of X, i.e., (g1g2X0)InvG2 = g1X0,and (g1g2X0)InvG1 = g2X0 for all g1 G1, g2 G2, where X0 is the canonical representation of X with respectto G. (Y )SymGidenotes the symmetrization of Y with respect to Gi, i.e. (Y )SymGi= gi(Y )SymGi, for all gi Gi.E.g., (Y )SymGi=1",
  "Intuitively, MEqG1G2 works as follows: the first term (MEqG2(XInvG1 ))Sym": "G1captures the G1-invariant and G2-equivariant features of X and the second term captures the G2-invariant and G1-equivariant features of X.Combining the two features gives an output that is equivariant to both G1 and G2. Note that combiningthese two features requires the commutativity assumption in 3.1. Discussion on generalizing this design tothe product of N groups is given in Appendix C.2. We now prove that MEqG1G2 is equivariant to G1 G2.",
  "Applications": "We first look at multi-image classification in 4.1 as an application of the first design.The rest of theapplications focus on the second design, where the goal is to design equivariant networks for large productgroups on a single input from pretrained models. Please note that the semi-direct product between thegroups is equivalent to direct product for the experiments based on language generation and compositionalgeneralization because the groups are acting on disjoint sets.",
  "Multi-Image Classification": "Experimental settingWe use the Caltech-101 and 15-Scene datasets. For a multi-input network with Ninputs, we partition the train and test datasets for each label in tuples of N. We add random 90 rotationsto the test images, and for training, we report results both with and without the transformations. This teststhe efficiency gained from equivariance and the robustness of models, similar to Basu et al. (2023b). Foreach dataset, we report results on multi-input image classification with N inputs, where N = {2, 3, 4}. Wecall the multi-input equivariant CNN based on the design from 3.3 as multi-GCNNs. Further details onthe model design are given in Appendix E.1. We train each model for 100 epochs and a batch size of 64, anSGD optimizer with a learning rate of 0.01, momentum of 0.9, and a weight decay of 0.001. Results and observationsTab. 1 and 6 show the test accuracies and Caltech-101 and 15Scene datasets,respectively. Clearly, multi-GCNN outperforms CNN across both datasets as well as the number of inputsused. Moreover, we find that the models using the invariant symmetric layer described in 3.3 generallyoutperform the ones without. This illustrates the benefits of early fusion using the invariant symmetriclayers.",
  "Compositional Generalization in Languages": "Compositionality in natural language processing (Dankers et al., 2022) is often thought to aid linguisticgeneralization (Baroni, 2020). Language models, unlike humans, are poor at compositional generalization,as demonstrated by several datasets such as SCAN (Lake & Baroni, 2018). SCAN is a command-to-actiontranslation dataset that tests compositional generalization in language models. Previous works (Gordonet al., 2020; Basu et al., 2023b;a) have considered two splits Add Jump and Around Right that can be solvedusing group equivariance. But each of these splits only required groups of size two. Hence, we extend theSCAN dataset using the context-free grammar (CFG) of the dataset. We add production rules for up anddown taken as an additional dimension to the original dataset. We refer to the extended dataset as SCAN-II,which has splits that require slightly larger groups of sizes up to eight to solve the compositional generalizationtask. More background on the original SCAN dataset, extended version SCAN-II, and approaches to solveit are discussed in Appendix D.1.",
  "Intersectional Fairness in Natural Language Generation": "We consider the problem of inherent bias present in natural language generation (NLG), specifically, forGPT-2 Radford et al. (2019). We consider the framework of Sheng et al. (2019) to test biases present inlanguage models. We aim to design one single model that reduces social biases amongst each pair of thedemographics [man\", woman\"], [Black\", White\"], and [straight\", gay\"] and their intersections. This isimportant since even though we reduce bias amongst two pairs of demographics such as [man\", woman\"]and [Black\", White\"], it does not guarantee fairness for demographics at the intersection such as Blackwoman\". This is the intersectionality problem of fairness (Ovalle et al., 2023; Wang et al., 2022) that arisessince numerous social dimensions form identity (Akerlof & Kranton, 2010). Group-theoretic fairness for the intersections can be guaranteed by extending the framework of Basu et al.(2023b) using products of groups. This extends the solution to fairness by Basu et al. (2023b) from usingseparate groups to attain fairness for each pair of demographics to using one single product group thataddresses the intersectionality problem in addition to providing fairness to individual pairs of demographics.Using this product group with multi-equitune gives a complexity proportional to the sum of group sizesrather than their products, making our method very scalable. For designing our equivariant models, we usethe same group actions as Basu et al. (2023b), provided in Appendix D.2 for completeness. Similar to Basu et al. (2023b), we evaluate our method using the regard classifier of Sheng et al. (2019). Aregard classifier is similar to a sentiment classifier but designed specifically for fairness studies by finetuningBERT (Devlin et al., 2019) on a human-curated fairness dataset. Sheng et al. (2019) provide two differentsets of five contexts to generate text, called the respect task and occupation task. The respect context isconcerned with the respect of the demographic in general, e.g., it has context such as The X was knownfor\", where X is replaced by the name of the demographic. Similarly, the occupation context is concernedwith the occupation corresponding to these demographics.",
  "Robust Image Classification using CLIP": "Experimental settingWe use the CLIP models with various Resnet and ViT encoders, namely, RN50,RN101, ViT-B/32, and ViT-B/16. We test the robustness of the zero-shot performance of these models on theImagenet-V2 and CIFAR100 datasets for the combined transformations of rot90 (random 90 rotations) andflips. We make comparisons in performance amongst original CLIP, and equitune, equizero, multi-equitune,and multi-equizero applied to CLIP. Results and observationsa and 9a show that the CLIP models are vulnerable to simple trans-formations such as random rotations and flips as was also observed in Basu et al. (2023a). b, c,b, and c show the robustness results for RN101, ViT-B/16, RN50, and ViT-B/32, respectively.We find that across all models and datasets, multi-equitune and multi-equizero perform competitively to eq-uitune and equizero respectively. Moreover, in Tab. 8 we find that multi-equitune take less memory comparedto equitune as expected from theory. That is, multi-equitune consumes memory approximately proportionalto |G1| + |G2| = 6, whereas equitune consumes memory proportional to |G1| |G2| = 8, where |G1| = 4 for90 rotations and |G2| = 2 for flips.",
  "Compositional Generalization in Language": "Experimental settingWe work on the SCAN-II dataset where we have one train dataset and threedifferent test dataset splits. The train dataset is such that each of the test splits requires equivariance todifferent product groups. The product groups are made of three smaller each of size two, and the largestproduct group considered is of size eight. Hence, performance on these splits shows benefits from equivarianceto different product groups. Details of the dataset construction are given in Appendix D.1. We consider thesame architectures as Basu et al. (2023b;a), i.e., LSTMs, GRUs, and RNNs, each with a single layer with 64hidden units. Each model was pretrained on the train set for 200k iterations using Adam optimizer (Kingma& Ba, 2015) with a learning rate of 104 and teacher-forcing ration 0.5 (Williams & Zipser, 1989). We testthe non-equivariant pretrained models, along with equituned and multi-equituned models, where equituneand multi-equitune use further 10k iterations of training on the train set. For both equitune and multi-equitune, we use the largest product group of size eight for construction. We use the cross-entropy loss asour training objective. Results and observations shows the results of pretrained models, and finetuning results of equituneand multi-equitune on the various test splits. We find that pretrained models fail miserably on the test setseven with excellent performance on the train set, confirming that compositional generalization is not trivial",
  "(c)": ": (a) shows that CLIP is not robust to the transformations of 90 rotations (rot90) and flips. (b) and(c) show that multi-equitune and multi-equizero are competitive with equitune and equizero, respectively, forzero-shot classification using RN50 and ViT-B/32 encoders of CLIP for the product of the transformationsrot90 and flips, even with much lesser compute.",
  "Intersectional Fairness in NLG": "Experimental settingWe closely follow the experimental setup of Basu et al. (2023b) and Sheng et al.(2019). There are two tasks provided by Sheng et al. (2019): respect task and occupation task. Each taskconsists of five contexts shown in Tab. 5. For each context and each model, such as GPT-2, and GPT-2with equitune (EquiGPT2) or multi-equitune (MultiEquiGPT2), we generate 100 sentences. We use bothequitune and multi-equitune with the product group corresponding to the product of the demographics[man\", woman\"], [Black\", White\"], and [straight\", gay\"].Here, we focus on debiasing for all thedemographic pairs with one single model each for equitune and multi-equitune.Quite directly, it alsoaddresses the problem of intersectionality. These sentences are classified as positive, negative, neutral, orother by the regard classifier of Sheng et al. (2019). Results and observations and 5 show some results corresponding to the respect task and occu-pation task, respectively, for various demographics and their intersections. The rest of the plots are providedin , 7, and 8. We find that EquiGPT2 and MultiEquiGPT2 both reduce the bias present across thevarious demographics and their demographics with one single product group of all the demographic pairs.",
  "Conclusion": "We introduce two efficient model-agnostic multi-group equivariant network designs. The first design aims atneural networks with multiple inputs with independent group actions applied to them. We first characterizethe entire linear equivariant space for this design, which gives rise to invariant-symmetric layers as its sub-component. Then we generalize this to non-linear layers. We validate its working by testing it on multi-input",
  "Sourya Basu, Jose Gallego-Posada, Francesco Vigan, James Rowbottom, and Taco Cohen. Equivariantmesh attention networks. Transactions on Machine Learning Research, 2022": "Sourya Basu, Pulkit Katdare, Prasanna Sattigeri, Vijil Chenthamarakshan, Katherine Driggs-Campbell,Payel Das, and Lav R. Varshney.Efficient equivariant transfer learning from pretrained models.InProceedings of the 37th International Conference on Neural Information Processing Systems, pp. 42134224, 2023a. Sourya Basu, Prasanna Sattigeri, Karthikeyan Natesan Ramamurthy, Vijil Chenthamarakshan, Kush R.Varshney, Lav R. Varshney, and Payel Das. Equi-tuning: Group equivariant fine-tuning of pretrainedmodels. In Proceedings of the AAAI Conference on Artificial Intelligence, 2023b.",
  "Taco S. Cohen and Max Welling. Steerable CNNs. In Proceedings of the International Conference on LearningRepresentations, 2017": "Verna Dankers, Elia Bruni, and Dieuwke Hupkes. The paradox of the compositionality of natural language:A neural machine translation case study. In Proceedings of the 60th Annual Meeting of the Associationfor Computational Linguistics, pp. 41544175, 2022. Pim De Haan, Maurice Weiler, Taco Cohen, and Max Welling. Gauge equivariant mesh CNNs: Anisotropicconvolutions on geometric graphs. In Proceedings of the International Conference on Learning Represen-tations, 2020. Congyue Deng, Or Litany, Yueqi Duan, Adrien Poulenard, Andrea Tagliasacchi, and Leonidas J. Guibas.Vector neurons: A general framework for SO(3)-equivariant networks. In Proceedings of the IEEE/CVFInternational Conference on Computer Vision, pp. 1220012209, 2021. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidi-rectional transformers for language understanding. In Proceedings of the 2019 Conference of the NorthAmerican Chapter of the Association for Computational Linguistics: Human Language Technologies, pp.41714186, 2019. Alexandre Agm Duval, Victor Schmidt, Alex Hernandez-Garcia, Santiago Miret, Fragkiskos D. Malliaros,Yoshua Bengio, and David Rolnick. FAEnet: Frame averaging equivariant GNN for materials modeling.In Proceedings of the International Conference on Machine Learning, pp. 90139033, 2023.",
  "Li Fei-Fei and Pietro Perona.A Bayesian hierarchical model for learning natural scene categories.InProceedings of the 2005 IEEE Conference on Computer Vision and Pattern Recognition, pp. 524531,2005": "Marc Finzi, Gregory Benton, and Andrew G. Wilson. Residual pathway priors for soft equivariance con-straints. In Proceedings of the 34th International Conference on Neural Information Processing Systems,pp. 3003730049, 2021a. Marc Finzi, Max Welling, and Andrew Gordon Wilson. A practical method for constructing equivariantmultilayer perceptrons for arbitrary matrix groups. In Proceedings of the International Conference onMachine Learning, pp. 33183328, 2021b. Fabian Fuchs, Daniel Worrall, Volker Fischer, and Max Welling. SE(3)-transformers: 3D roto-translationequivariant attention networks. In Proceedings of the 33rd International Conference on Neural InformationProcessing Systems, pp. 19701981, 2020. Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, and George E. Dahl. Neural messagepassing for quantum chemistry. In Proceedings of the International Conference on Machine Learning, pp.12631272, 2017. Jonathan Gordon, David Lopez-Paz, Marco Baroni, and Diane Bouchacourt. Permutation equivariant modelsfor compositional generalization in language. In Proceedings of the International Conference on LearningRepresentations, 2020. Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducinginternal covariate shift. In Proceedings of the International Conference on Machine Learning, pp. 448456,2015. Yichen Jiang, Xiang Zhou, and Mohit Bansal. Mutual exclusivity training and primitive augmentation toinduce compositionality. In Proceedings of the 2022 Conference on Empirical Methods in Natural LanguageProcessing, 2022. John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, KathrynTunyasuvunakool, Russ Bates, Augustin dek, Anna Potapenko, et al. Highly accurate protein structureprediction with AlphaFold. Nature, 596(7873):583589, 2021.",
  "Vctor Garcia Satorras, Emiel Hoogeboom, and Max Welling. E(n) equivariant graph neural networks. InProceedings of the International Conference on Machine Learning, pp. 93239332, 2021": "Emily Sheng, Kai-Wei Chang, Prem Natarajan, and Nanyun Peng. The woman worked as a babysitter: Onbiases in language generation. In Proceedings of the 2019 Conference on Empirical Methods in NaturalLanguage Processing and the 9th International Joint Conference on Natural Language Processing, pp.34073412, 2019. Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout:a simple way to prevent neural networks from overfitting. Journal of Machine Learning Research, 15(1):19291958, 2014. Nathaniel Thomas, Tess Smidt, Steven Kearnes, Lusann Yang, Li Li, Kai Kohlhoff, and Patrick Ri-ley.Tensor field networks: Rotation-and translation-equivariant neural networks for 3D point clouds.arXiv:1802.08219, 2018. Angelina Wang, Vikram V. Ramaswamy, and Olga Russakovsky. Towards intersectionality in machine learn-ing: Including more identities, handling underrepresentation, and performing evaluation. In Proceedingsof the 2022 ACM Conference on Fairness, Accountability, and Transparency, pp. 336349, 2022.",
  "AAdditional Definitions": "Groups and group actionsA group is set G accompanied by a binary operation such that the fouraxioms of a group are satisfied, which are a) closure: g1 g2 G for every g1, g2 G, b) identity: there existse G such that e g = g e = g, c) associativity: (g1 g2) g3 = g1 (g2 g3) and d) inverse: for every g G,there exists g1 such that g g1 = g1 g = e. When clear from context, we write g1 g2 simply as g1g2. A group action of a group G on a space X, is given as : G X X such that a) (e, x) = x for allx X and b) (g1, (g2, x)) = (g1 g2, x) for all g1, g2 G, x X, where e is the identity element of G.When clear from context, we write (g, x) simply as gx.",
  "for any a G1, b G2. Here, equation 7 and equation 8 hold by definition of these equivariant layers": "It follows LEqG1(aX1) + LISG2,G1(bX2) = a(LEqG1(X1) + LISG2,G1(bX2)), since LEqG1(aX1) = aLEqG1(X1) from equa-tion 7 and LISG2,G1(bX2) = aLISG2,G1(bX2) from equation 9. Similarly, it follows LEqG2(bX2) + LISG1,G2(aX1) =b(LEqG2(X2) + LISG1,G2(aX1)), which concludes the proof. Proof to Lem. 1. Let L be a d d matrix and we want to find the dimension of the space of matrices L suchthat the fixed point equation P(g2) L P(g1) = L holds for all g1 G1 and g2 G2, where P(gi) denotesthe permutation matrix corresponding to gi. Thus, we want to compute the dimension of the null space ofthis fixed point equation. From Maron et al. (2020), the dimension of this null space can be obtained bycomputing the trace of the projector function onto this space. One can verify the projector here is given byGInv1,GSym2=1",
  "g2G2 P(g1)P(g2), where is the Kronecker product. From the propertiesof the trace function, we know Tr(P(g1) P(g2)) = Tr(P(g1)) Tr(P(g2)), which concludes the proof": "Proof to Thm. 2. The dimension of the linear layer LG1,G2([X1, X2]) = E(G1) + E(G2) + IS(G1, G2) +IS(G2, G1), since we have two equivariant layers that have dimensions E(G1) and E(G2), respectively, andtwo invariant-symmetric layers, which have dimensions IS(G1, G2) and IS(G2, G1), respectively. Recall thedefinitions of E() and IS(, ) from equation 3 and equation 4, respectively. Now we compute the dimension of any linear layer satisfying the equivariant constraint in equation 2 and showit matches the dimension of LG1,G2([X1, X2]). To that end, first note that the projector onto this equivariantspace is1",
  "= ,(18)": "where equation 14 follows from the definition of equation 5, equation 15 follows because f IS(G1,G2)(x) =g2f IS(G1,G2)(g1x) for all g1 G1, g2 G2, equation 16 follows from the triangle inequality and the assumptiong2 = 1 for all g2 G2. Finally, equation 17 follows because M(g1x) f ISG1,G2(g1x) for all x KSym.",
  "C.1N-Input Group Equivariant Models": "We extend the design in 3.3 to N inputs X1, . . . , XN with group Gi acting independently on Xi, respectively.Suppose the outputs are Y1, . . . , YN and given models Mi, Mij processing Xi and contributing to Yi, Yj,respectively. Then, the equivariant model using Mis Mijs for i, j {1, . . . , N} consists of an equivariant andan invariant-symmetric component. The equivariant component remains the same as for N = 2, i.e., for input i, we have M Eqi,Gi(Xi), which isequivariant to Gi. Additionally, Yi has N-1 invariant-symmetric components, where the invariant-symmetriccomponent is M ISji,GjGi(Xj). It is trivial to see that Yi is equivariant with respect to Gi acting on Xi since",
  "D.1Compositional Generalization in Language": "Original SCAN splitsThe original SCAN split considered in works related to group equivariance primar-ily dealt with the Add Jump and theAround Right splits. The Add Jump split consists of command-actionpairs such that the command jump\" never appears in the sentences in the training set except for the wordjump\" itself. However, similar verbs such as walk\" or run\" appear in the dataset. But the test set doescontain sentences with jump\" in them. Thus, to be able to generalize to the test set, a language modelshould be able to understand the similarity between the words jump\" and walk\". Gordon et al. (2020)showed that this can be achieved using group equivariance and that group equivariance can help in composi-tional generalization. Similarly, the Around Right split has a train set without the phrase around right\" inany of its sentences, but the phrase is contained in its test set. Moreover, the train set also contains phrases",
  "D.2Intersectional Fairness in NLG": "Here we define the group-theoretic fairness framework of Basu et al. (2023b) used with language models(LM) such as GPT2. Then, we discuss how the framework changes upon extension to product groups. First,for each list of demographic groups, we define a set of list of words E called the equality words set, anda set of words N called the neutral words sets. The equality set E represents the words corresponding toeach demographic, e.g., for the list of demographic groups [man\", woman\"], the equality words set canbe [[man\", woman\"], [boy\", girl\"], [king\", queen\"]]. The neutral set N represents the words that areneutral with respect to any demographic, e.g. [doctor\", nurse\", student\"]. Given a vocabulary V of theLM, the words are partitioned between E and N in this setting. There is a more general setting calledrelaxed-equitune in Basu et al. (2023b) where the words in the vocabulary are distributed into three sets E,N, and G. Here, E and N are defined the same as in equitune, but G consists of all the words that do not",
  "obviously belong to either E or N. In this work we focus on equitune since all the methods developed forlarge product groups here trivially carry over to the implementation of relaxed-equitune": "Now we review the group actions in equitune for a single list of demographics of length d, such as [man\",woman\"] has length d = 2.Given a cyclic group of length d, G = {e, g, g2, , gd1}, it acts on thevocabulary V as follows. The group action of a cyclic group is completely defined by the group action of itsgenerator, in this case, the element g G simply makes a cyclic shift of size one in the equality set E andleaves the neutral set N invariant. For example, if G = {e, g} and E = [[man\", woman\"], [boy\", girl\"],[king\", queen\"]], then gE = [[man\", woman\"], [boy\", girl\"], [king\", queen\"]]. Previous works such as equitune and -equitune have only focused on debiasing one list of demographicgroups, but debiasing demographics at the intersection remains to be addressed. For example, debiasing themarginal demographics [man\", woman\"] and [Black\", White\"] does not guarantee debiasing for demo-graphics at the intersection such as Black woman\". Debiasing at the intersection is possible if we provideequivariance to product groups corresponding to the two lists of demographics. Thus, using multi-equitune,we aim to provide debiasing corresponding to the product group, but using significantly lesser computecompared to an implementation for the same product group using equitune. The implementation of multi-equitune here is very simple since all the group actions are on the vocabu-lary space and are disjoint. That is, the first step of canonicalization can be performed independently foreach group, which are then passed through respective equivariant architectures. Finally, the outputs aresymmetrized on disjoint output vocabulary before they are averaged.",
  "G1+ (MEqG1(XInvG2 ))Sym": "G2 . Suppose G1 is the group of 90 rotations and G2 is the group offlips. Then, for a given image X first, we compute XInvGifor i {1, 2}, which is computed by appropriatelycanonicalizing X with respect to Gi using the technique of Kaba et al. (2023). Kaba et al. (2023) alsorequires a small auxiliary network equivariant to Gi, which is constructed by equituning a small randomlyinitialized matrix. MEqGi is constructed by directly using the equitune transform (without any finetuning)on the vision encoder of CLIP. Further, since we just need invariant features from CLIP, we simply obtaininvariant features from the output of MEqGi by pooling along the orbit of Gi. Moreover, since the featuresobtained are invariant, the ()SymGioperator leaves the output unchanged. Finally, for equitune, we simply",
  "E.1Multi-Image Classification": "The 15Scene dataset contains a wide range of scene environments of 13 categories. Each category includes200 to 400 images with an average size of 300 250 pixels. Similarly, Caltech101 contains pictures of objectsfrom 101 categories. Each category includes 40 to 800 images of 300 200 pixels. We use the cross-entropyloss as our optimization objective for training our models. The multi-GCNN consists of three components: an equivariant Siamese block, an invariant-symmetric fusionblock, and finally a linear block. The Siamese block is a Siamese network made of two convolutional layers,each with kernel size 5, and channel dimension 16. Each convolution is followed by a ReLU (Nair & Hinton,2010), max pool, and a batch norm (Ioffe & Szegedy, 2015).It is followed by a fully connected layerwith a hidden size computed by flattening the output of the convolutional layers and output size 64. Theoutput is passed through ReLU, dropout (Srivastava et al., 2014), and batch norm. Finally, this block ismade equivariant using the equitune transform (Basu et al., 2023b).The N inputs are passed throughthis Siamese layer parallelly. The fusion block is built identically to the Siamese block, except, we makeit invariant instead of equivariant. The fusion block also takes the inputs parallelly. Fusion is performedby adding the output of the fusion layer corresponding to input i multiplied by a learnable weight to allthe features corresponding to the other inputs. Following this, we perform invariant pooling and pass itthrough the linear block to get the final output. The linear block consists of two densely connected layerswith a hidden size of 64. Further, we use ReLU and dropout between the two densely connected layers.The non-equivariant CNN is constructed exactly as the multi-GCNN network except that no equituningoperation is performed anywhere for equivariance or invariance.",
  "IIII0.488 (0.012)0.479 (0.028)0.777 (0.021)0.765 (0.032)RRRR0.661 (0.012)0.71 (0.046)0.762 (0.019)0.765 (0.008)": ": Memory consumption between equitune and multi-equitune and GPT2 for a product group ofthe form G1 G2 G3, where |Gi| = 2 for i {1, 2, 3}.Note that ideally, equitune would consumememory proportional to |G1| |G2| |G3| = 8 and multi-equitune would consume memory proportionalto |G1| + |G2| + |G3| = 6. Our results show slightly more memory consumed by equitune compared tomulti-equitune as expected for these groups. We use a batch size of 1 for the following measurements.",
  "GEfficiency Vs. Performance Trade-Off": "Here, we discuss the trade-off between efficiency and performance between equitune and our multi-equitunealgorithm in equation 6. That is, for a product group of the form G1 G2, we provide better intuition howwe reduce the computational complexity from O(|G1||G2|) to O(|G1|+|G2|). At the same time, we explainhow exactly we get some drop in performance of the network with benefits in computational complexity.",
  "g2G2M(g1g2X)(25)": "First note that equation 24 has a computational complexity of O(|G1| + |G2|), and that of equation 24 isO(|G1||G2|), where computational complexity here refers to the number of forward passes of the model M.On the other hand, equation 25 is the exact expression for equitune operation for the product group G1G2,when M is generalized to general functions. Further, we know that equitune is a universal approximator ofequivariant functions Basu et al. (2023b). However, even though the invariant-symmetric layer in equation 5is universal approximators of invariant-symmetric functions, multi-equitune is not a universal approximatorof equivariant functions. Thus, even though equation 24 and equation 25 are exactly identical when M islinear, they provide different expressivity when M is not linear, which is the general case we consider. Finally, we emphasize that this drop in expressivity of equation 24 is negligible when M itself is a largepretrained model as seen in and 9.Moreover, equation 24 provides computational benefits overequation 25 for product groups."
}