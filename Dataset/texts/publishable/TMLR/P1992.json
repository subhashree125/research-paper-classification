{
  "Abstract": "Unsupervised on-the-fly back-translation, in conjunction with multilingual pretraining, isthe dominant method for unsupervised neural machine translation. Theoretically, however,the method should not work in general. We therefore conduct controlled experiments withartificial languages to determine what properties of languages make back-translation aneffective training method, covering lexical, syntactic, and semantic properties.We find,contrary to popular belief, that (i) parallel word frequency distributions, (ii) partially sharedvocabulary, and (iii) similar syntactic structure across languages are not sufficient to explainthe success of back-translation. We show however that even crude semantic signal (similarlexical fields across languages) does improve alignment of two languages through back-translation. We conjecture that rich semantic dependencies, parallel across languages, areat the root of the success of unsupervised methods based on back-translation. Overall, thesuccess of unsupervised machine translation was far from being analytically guaranteed.Instead, it is another proof that languages of the world share deep similarities, and we hopeto show how to identify which of these similarities can serve the development of unsupervised,cross-linguistic tools.",
  "Machine translation and the role of back-translation to eliminate supervision": "Supervised training for neural machine translation (NMT) requires a vast amount of parallel data (Sutskeveret al., 2014; Wu et al., 2016; Vaswani et al., 2017). Creating the necessary datasets aligned across languagesis a complex, onerous and sometimes impossible task. In this context, unsupervised back-translation, andin particular on-the-fly back-translation, becomes highly valuable, as it allows unsupervised training fromindependent monolingual corpora (Sennrich et al., 2016; Lample et al., 2018a; Guzmn et al., 2019; Haddowet al., 2022). Back-translation works by using a translation model from one language (L1) to another (L2) to syntheticallygenerate data in the following way: a given text x in L1 is passed in and a hypothetical translation y in L2 isgenerated. This pair is then treated as if it were a gold translation (y, x) to train an L2-to-L1 system. Foriterative and on-the-fly back-translation, the whole process is repeated in the other direction, and iterated,so that the models improve as they are generating the data. One can think of this process, in essence, as",
  "Published in Transactions on Machine Learning Research (09/2024)": "Yingce Xia, Tao Qin, Wei Chen, Jiang Bian, Nenghai Yu, and Tie-Yan Liu.Dual supervised learning.In Proceedings of the 34th International Conference on Machine Learning - Volume 70, ICML17, pp.37893798. JMLR.org, 2017. Zhen Yang, Wei Chen, Feng Wang, and Bo Xu. Unsupervised neural machine translation with weight sharing.In Iryna Gurevych and Yusuke Miyao (eds.), Proceedings of the 56th Annual Meeting of the Associationfor Computational Linguistics (Volume 1: Long Papers), pp. 4655, Melbourne, Australia, July 2018.Association for Computational Linguistics. doi: 10.18653/v1/P18-1005. URL Changtong Zan, Liang Ding, Li Shen, Yu Cao, Weifeng Liu, and Dacheng Tao. On the complementarity be-tween pre-training and random-initialization for resource-rich machine translation. ArXiv, abs/2209.03316,2022.",
  "Related work": "Prior to unsupervised translation, several works focused on bilingual dictionary induction but using littleor no parallel data. Mikolov et al. (2013) does so by relying on a distributed word representation of largemonolingual corpora as well as a small supervised corpus. Klementiev et al. (2012); Irvine & Callison-Burch(2016) develop a phrase-based statistical model for phrasal and lexical translation. More recently, Lampleet al. (2018b) allows dictionary inference without any parallel data. Bojar & Tamchyna (2011) used reverse self-training to train a model in a statistical machine translation(SMT) framework. The idea is to use a small parallel corpus to train a machine translation (MT) systemfrom target to source and use it to translate a large monolingual target-side corpus to create synthetic pairsto train a source to target SMT model. This is the root of Back-translation (BT) as a data augmentationmethod. This is applied to NMT by Sennrich et al. (2016) who train two models iteratively, source to targetand target to source. The idea that translation from a language to another has a dual task, which is thetranslation in the reverse order, that can be learned jointly to improve efficiency is largely used in severalworks (Xia et al., 2017; Cheng et al., 2016; He et al., 2016; Wang et al., 2019; Hoang et al., 2018; Niu et al.,2018). For example, Cotterell & Kreutzer (2018) use the formalism of a wake-sleep algorithm to propose anIterative Back-Translation method, i.e. the synthetic data is regenerated after each model training. However,in all these works training is still partially supervised and back-translation acts as data augmentation. Artetxe et al. (2018) introduce a fully Unsupervised Neural Machine Translation (UNMT) method. It useson-the-fly BT (OTF-BT), meaning that synthetic sentences are generated as the model trains. They also adda denoising autoencoding objective. This paper lays the foundations for the following BT-based architecturesand models. For example, Lample et al. (2018a) use the same method and leverage the intuition that thesame sentence, regardless of the language, will be mapped onto the same embedding vector by encoders, andthat to translate the sentence it is then sufficient to decode it into the desired language. They enforce thisbehavior using an additional adversarial training objective. Other works remove this adversarial objective",
  "while maintaining performance (Lample et al., 2018c; Yang et al., 2018), suggesting that this embeddingspace overlap emerges naturally": "Following on from this line of work, several papers formulate the idea that the quality of the cross-lingualembeddings used to initialize BT are key and therefore add a language modeling objective as pre-training(Lample et al., 2018a; Edunov et al., 2019). This method works very well and has become the standardapproach in UNMT. For example, Liu et al. (2020) introduces mBART for UNMT while Zan et al. (2022)studies in depth its benefits. Numerous papers propose modifications to BT to improve its performance,particularly in a low-resource setup (Kumari et al., 2021; Kim et al., 2019; Pourdamghani et al., 2019; Douet al., 2020; Song et al., 2019). In particular, Filtered BT (Khatri & Bhattacharyya, 2020; Imankulova et al.,2017) filters synthetic sentences based on their quality, using a round-trip sentence similarity metrics, whileTagged BT (Caswell et al., 2019) prepends a tag in front of synthetic sentences to indicate to the modelwhich are from the parallel corpus and which are not. Many studies examine the reasons for the success of BT for UNMT (Edunov et al., 2018; Poncelas et al., 2018;Kim et al., 2019; Edunov et al., 2020; Conneau et al., 2020b). In particular, some focus on the propertiesof languages, pursuing the thought that BT and multilingual models work due to the similar structuresof natural languages. K et al. (2019) studies the impact of several factors on the cross-lingual abilities ofmBERT, including the linguistic properties of languages, such as token overlap between vocabularies, orword order. They show that the former has very little effect, but that the latter is fundamental. Dufter &Schtze (2020) takes up the same kind of work. Finally, Kim et al. (2020) shows that UNMT performancedrops drastically when languages or domains are very different. It also shows that BT cannot recover froma bad initialization. Our work continues this line of research, but differs in that we use artificial languages to maintain perfectcontrol over our experiments and to avoid confounding factors as much as possible. This allows to investigateprecise syntactic and semantics effects on OTF-BT. We also use these artificial languages to demonstratethe inaccuracy of the usual assumptions concerning the success of OTF-BT (e.g. word frequencies, anchorpoints, syntax, etc.). To the best of our knowledge, this has not been done in previous works. It is hopedthat this work will lead to greater intuition about BT, enabling researchers to refine the method and producebetter results in the future. Several other works also use artificial languages on related tasks (Arora et al., 2016; White & Cotterell,2021; Chiang & yi Lee, 2021), but none apply their methodology to UNMT. Finally, it should be notedthat the model used and studied in this work (Lample et al., 2018c) is no longer state of the art, but usesOTF-BT almost exclusively, unlike more recent models which rely heavily on massive pre-training, makingit extremely relevant to our work.",
  "Back-Translation is necessary, but not sufficient in general": "A fully unsupervised training with back-translation works as follows. The model consists of classical com-ponents of a translation system, as seen in . It includes encoders ei from language Li into a hiddenspace, and decoders di from this hidden space into Li. These encoders and decoders could be separate modelsor the same multilingual model conditioned on a language ID token (Conneau et al., 2020a; Liu et al., 2020).The composition of an encoder ei and decoder dj from different languages provides a translation functionTij. These components are first trained through a denoising auto-encoding, monolingual regime, whereby dieig,with g a noise function, should lead to the identity for each language. Then suppose y is an element of thelanguage L2. We can translate y into L1 as x = d1 e2(y) := T21(y). This creates a synthetic pair (x, y) inL1 L2. This pair, which was created by the model itself, is now used as a supervised datum for translation.That is, we check that the translation (now backwards) from x in L1 to L2 is coherent: y = d2e1(x) := T12(x)",
  "(1)": "where ei,di are the parameters of the encoders/decoders. The gray components in the first two lines reflectfrozen parts of the pipeline: in practice we sample from those di and then pass those generations forward,as previously described. Hence the formula is a slight simplification. In practice, the denoising autoencodingobjective (or similar; see third line in (1)) often happens before the back-translation objective is used; wehave included them as one loss because the present arguments do not depend on this choice. With this framework in place, we can now explain why back-translation is a necessary objective, but not asufficient one. The back-translation objective (1) could be fully met, without translation being accurate atall. We illustrate this visually in , with two extreme cases.",
  "(c) Shuffled translation": ": Schematic representation of three cases in which the back-translation objective (1) is fully met:(a) with an accurate translation, (b) with a translation missing the target language entirely, but beingsent back on the original language appropriately, (c) with a translation that bijectively shuffles the targetlanguage, and a reverse translation that unshuffles it back in place. We ignore here the hidden encodingspace, and write for T12 and for T21.",
  "The Context Free Languages": "The grammars of our artificial languages vary somewhat similarly to what is assumed for natural languages.We used the simple Context Free Grammars (CFG) introduced by White & Cotterell (2021), which areparametrized by switches: the order in which constituents surface. presents the rules that areswitchable in these grammars. We will denote a grammar by a sequence of six binary values, correspondingto the switches. These syntax rules are designed to reflect real rules that differentiate many natural languages.For example, Japanese corresponds to the combination 000000, English to 011101 and Spanish to 011111.",
  ": Rules that are switchable in the grammar. Table from White & Cotterell (2021)": "At the lexical level, 1,374 words were created, with a plausible English morphology, and distributed in severalParts-of-Speech (POS), based on the work of Kharkwal (2014). We used 1,374 words to obtain a replicationof White & Cotterell (2021)1. Picking a set of switches (i.e. a grammar) and a set of such created words thus",
  "POSNounS Subj VerbCompPresS.000000 (Lexicon 0)burse sub lurchifies.100000 (Lexicon 0)lurchifies burse sub.100010 (Lexicon 0)lurchifies burse sub.000000 (Lexicon 1)swopceer bus rheleates": "000000 POSIVerbPastP Adj NounP Subj.000000 (Lexicon 0)rolveda prask autoners sub.100000 (Lexicon 0)prask autoners sub rolveda.100010 (Lexicon 0)autoners sub prask rolveda.000000 (Lexicon 1)knyfeateda wourk krarfteers bus. : Examples of sentences generated by our artificial languages. The POS row gives the underlyingstructure of the sentence in each group (colors in the following rows trace these Part-of-Speech). Whenrelevant, constituents swapped compared to the grammar from the row above are underlined. The grammarsand switches are described in .",
  "Training": "ModelWe used the exact model introduced by Lample et al. (2018c) for NMT. The architecture is basedon 4 Transformer encoder layers and 4 Transformer decoder layers. The last 3 encoder layers and the first3 decoder layers are shared across languages. The embedding size is 512 and the vocabulary, of size 1, 000,was shared and built using Byte Pair Encoding. We use this model because of its high-performance (stateof the art at the time of publication) and because it allows us to isolate the study of OTF-BT. DataFor training purposes, we generated two sets of 100,000 sentence structures2.All unsupervisedtraining sets are made of (i) one of these sets of sentence structures for the 100,000 sentences in one language(using the language specific grammar and lexicon), and (ii) the other set to create 100,000 sentences in theother language. Hence, the data are neither labeled for supervision, nor are they even unlabelled translationsfrom one another in principle. The test and validation sets are each composed of 10,000 parallel sentencepairs. These are generated in one language, and then transformed into the second language using the knowngrammar switches and lexical translations. ProcedureAt the start of training, a FastText algorithm is applied to both languages simultaneously toinitialize the cross-lingual embeddings (Bojanowski et al., 2017). We then train the translation model for 40epochs, with a batch size of 16 and an Adam optimizer with a 104 learning rate. Those hyper-parameterswere chosen based on Lample et al. (2018c) and on our computational capabilities. We trained on a TeslaM10 with 8GB memory for roughly one day. ObjectivesThe overall training objective is described in (1).First the model does a denoising auto-encoding (DAE) step, which can be seen as a LM step that constrains the model to output in the correctlanguage, and updates its parameters, then it does an on-the-fly back-translation step in both directions andupdates its parameters again. The denoising function g used is a combination of word substitution, localshuffling and masking. We note that DAE is here interspersed with BT, not used as a separate pretrainingobjective.",
  "The first experiment 5 is very simple and acts as a sanity check, the idea being to ensure that the model isindeed capable of learning to translate these artificial languages into themselves": "The second experiment 6 is to measure the impact of grammar on translation performance, keeping thelexicon equal. In this way, the translation function is only concerned with the change in grammar from onelanguage to another. The third experiment 7 adds the difficulty of a new lexicon.We show that this difficulty completelyundermines the success of the machine translation system. This, of course, is different from the situationwith real-world languages. The following three experiments then seek to understand what additional signal is present in natural lan-guages and is so crucial to the success of OTF-BT. Experiment four 8 adds anchor words and identicalword frequency. Experiment five 9 shows that a weak supervised signal, such as a bilingual dictionary ora small supervised dataset, restores the models performance. Finally, experiment six 10 adds semanticinformation by way of lexical fields.",
  "Metrics": "We use the BLEU score as a metric of accurate translation (Papineni et al., 2002, and in particular theMoses implementation as in Lample et al., 2018c).3Some more specific metrics will be introduced forthe particular of some experiments, but BLEU will serve as a standard reference in many places. Thereare various limitations of BLEU: it cannot handle paraphrases and synonyms, does not take meaning intoaccount directly, it is invariant by n-grams permutations, it has a low correlation with human judgment,etc. The limitations concerning paraphrases and synonyms do not apply to our setup, which does not haveparaphrases nor synonyms. Similarly, meaning is not relevant in our setup. Overall, BLEU ignores certainaspects and may lead to overestimations of the quality of translation, To mitigate this, we computed BLEUscores for n-grams ranging from 1 to 4. This helps capture syntactic structure (order constraints) and thismitigates the risk of bigrams permutations insensitivity, as illustrated in Callison-Burch et al. (2006). In anyevent, however, our arguments rely mainly on the observation of low BLEU scores, hence overestimation isa mild risk. Alternatives to BLEU include two families: the automatic language agnostic metrics like BLEU (Papineniet al., 2002), METEOR (Banerjee & Lavie, 2005) or Word Error Rate (WER), and the LLM-based metricslike BERTScore (Zhang et al., 2019), MetricX (Juraska et al., 2023), Comet Rei et al. (2020), Unite (Wanet al., 2022), BLEURT (Sellam et al., 2020), etc. The second family of measures is not applicable to ourartificial languages. Among the first family, WER is known to be worse than BLEU, METEOR remainsimperfect (see Saadany & Orasan, 2021) and BLEU therefore seems like the right choice.",
  "Experiment 1: Identical L1 and L2 languages": "Our first experiment demonstrates that our hyper-parameters and training pipeline work in principle, byusing BT to translate between two identical languages. Note however that, in principle, even for identicallanguages, back-translation is not a sufficient objective. We randomly selected 8 grammars among the 64possible ones, and picked one lexicon, to form 8 artificial languages. For each such language L1, we trainedour model to learn translation between L1 and a similarly defined language L2. Sampling of training setswas done independently for the two identical languages, hence the two monolingual corpora are not aligned.",
  "Experiment 2: Effect of Grammar": "If similar structure is the key for back-translation to work, then translation between two languages shouldbe harder if their grammar are more different (even if we stay within the variation observed between actuallanguages). We can operationalize this hypothesis in terms of the Hamming distance (number of differentswitches) between the grammars generating two languages: we expect BLEU score to decrease as Hammingdistance increases. In these tests, we keep the vocabulary constant across languages to focus on the effect ofgrammar.",
  "To test this hypothesis, we used the eight random grammars of 5 and trained a translation system viaback-translation for each pair, resulting in 64 BLEU scores. The complete results are in": "First, we found a correlation between the obtained BLEU scores and the Hamming distance (R2 = 0.35with a coefficient of 8.35, p < 0.001). In other words, back-translation does become less performant as thegrammars differ more. Second, we used as predictors not the raw Hamming distance, but individual variables Si correspondingto each switch being different or not in the translation pair at stake, thus fitting a model of the formBLEU 6i=1 iSi. We obtain a strong fit (R2 = 0.94), and a significant effect at p < 0.001 for thecoefficients corresponding to switch S1 (1 = 45.04), S4 (4 = 6.36) and S6 (6 = 11.04). This suggeststhat not all switches are created equal: some have more dramatic effects than others on back-translationperformance.",
  ": BLEU scores for languages with different grammars but the same lexicon": "Because of this, we more systematically evaluated the effect of each individual switch. To do this, we use the000000 grammar as the source language and vary the target language so that one switch only is activatedeach time: 100000, 010000, . . . , 000001. Conversely, we used 111111 as the source language, deactivatingeach switch in turn for the target language: 011111, 101111, . . . , 111110. The results are in .",
  "(0.23)11111111111086.4069.66(0.24)": ": The BLEU scores show that different switches have different impacts on the translation performance,with source language 000000 (left) and 111111 (right). The second column shows the baseline BLEU scorethat would be obtained by a translation system that would just copy the initial sentence and, in parentheses,the relative distance of the learned translation to this baseline: BLEUbaseline",
  "baseline": "Different switches show different impact on the translation performance, in a way mirroring the regressioncoefficients: switch 1 causes the largest drop in performance, followed by switch 6 and then switch 4. Otherswitches, like the fifth one, have little impact on BLEU. An intuition behind the different impacts of the switches could be as follows: the first switch governs achange that occurs near the root of the parse tree while the fifth switch concerns a node closer to the leaves.Thus, more words move, and they move further away in the first case than in the second. Because of this,a model that has learned the identity mapping (and therefore has not learned the translation) would get ahigher BLEU score with the fifth switch than with the first. To isolate this effect and determine whether the drops in BLEU are a result of a bias towards learning anidentity translation, we computed the BLEU score of a system that would just copy the source sentence asis (and the relative distance between the BLEU score obtained by our model and this baseline). The resultsare shown on the last two columns of . Different (relative) effects of each switch on the translation are found again, suggesting a more subtle effectthan the intuition mentioned above. In sum, grammar changes that make words move further away havemore impact on the translation performance, and this is not because the identity is learned instead of aproper translation.",
  "Experiment 3: Effect of the Lexicon": "Until now, the two languages had the same lexicon but different grammars. In this section we focus on theopposite scenario, i.e. translation between languages with an identical grammar, but different lexica. Wethus built a second, parallel lexicon of 1,374 new fictitious words with an English-like morphology using thesame methods as before (see 4.1). We followed the same training procedure on five out of the eight grammars from 5. In this context, wefound that back-translation systematically failed to produce successful translation systems: the best BLEUscore was below 7, as reported in . Note that the round-trip BLEU score was above 70 in all trainings.This shows that its not the back-translation objective that has failed to be optimized, but rather that itsnot sufficient to ensure correct translation.",
  "1000005.6876.2111111 1111116.6358.1": ": BLEU scores obtained on the test set for within grammars training but with different lexica. Itis clear that the training did not work as the highest BLEU score is below 7, demonstrating on the flythat back-translation is not guaranteed to succeed. The POS BLEU colomn report the average BLEUscore obtained on the Part-of-Speech only. These high scores show that the failure lies in the vocabulariesmapping. To identify the source of these errors, we computed BLEU scores for part-of-speech (POS) values instead ofexact tokens. In this setting, all BLEU scores were above 60 (see ). In more detail, around a third ofthe sentences in the test set are syntactically correctly translated. The average length of correct sentencesis 6.2, compared with 11.0 for the whole test set. Correct sentences are therefore shorter on average, whichis not surprising simply if shorter length means fewer opportunities of error. This relatively good syntacticperformance can be partly explained by the DAE objective, which constrains the decoder to output sentencesfrom the grammar. However, it is also clear that the model does not ignore input when doing translation. To further document the failure at the vocabulary level, we focussed on an analysis of sentences whose POSsequence has been correctly translated; there it is possible to do word-by-word evaluation. For any word wsfrom the source language, we looked at the distribution of its translations by the model across its differentoccurrences in this restricted test set. The perfect model would always translate ws by its correct translation,and would have an entropy of 0. By comparison, a model choosing the translation at random (although inthe correct POS) would have an entropy of log2 of the size of the relevant POS set. Taking singular namesas an example, the average entropy of the model is 0.05, whereas a random model would have an entropyof 7.3. The results for the other POS are qualitatively identical (see ), showing that the model didpick a single translation per word, albeit not the correct one. This points towards the shuffled translationscenario illustrated in c for the vocabulary.",
  "Lexical change3, 72.5(i) Anchor Points4a, 8.123.0 (+20.6)(ii) Frequency4b, 8.210.1 (+ 7.6)(iii) Nc = 26a, 10.19.2 (+ 6.8)(iv) Nc = 26b, 10.210.7 (+ 8.3)(v) Nc = 106c, 10.311.9 (+ 9.5)": ": BLEU scores obtained from back-translation, augmented with other unsupervised signals. We startfrom the previous experiment with a lexical change, then show results when we supplement the situationwith (i) anchor points, (ii) (intra-POS) word frequencies (for the 000000 grammar and two different lexica),(iii) Nc = 2 equally balanced disjoint lexical fields, (iv) Nc = 2 unbalanced disjoint lexical fields, (v) Nc = 10disjoint lexical fields. In parentheses: the gain of BLEU score compared to Exp 3.",
  "Experiment 4a: Anchor Points": "In natural language corpora, a number of words are identical between languages, in particular in writtenform. This happens because the languages have a common root, because some words have been borrowedacross languages, because of identical proper nouns, or because the languages may use similar numeralsystems. These identical words between two languages may serve as anchor points that allow translationmodels to fit the whole mapping between two lexica. This could help prevent the rotation problem describedin 3 and the disjoint lexicon problem of 7. To test this hypothesis, we ran back-translation on two languages with the 000000 grammar and lexiconssharing 30% of their words. We obtained a BLEU score of 23.04. Using the word-by-word comparison methodfrom 7 (for syntactically correct translations), we show that 92% of the words with non-zero precision andrecall are within the common words. This suggests, mirroring similar findings on multilingual languagemodeling due to Conneau et al. (2020b), that common words do not serve as good anchors that can lead theempirical success of back-translation on the entire vocabulary.",
  "Experiment 4b: Frequency alignment": "In the experiments described thus far, all words within a given POS have equal frequency.In naturallanguages, however, words vary in frequency, and presumably in similar ways across languages, following apower law. This information could help a translation model learn the mapping between the two vocabularies. We hence manipulated our corpora so that the probability of appearance of words within each POS followsa power law: P(wn) nk for k = 1.1, as they roughly do in natural languages, where wn is the n-th mostfrequent word. With the 000000 grammar and two different lexica following these parallel Zipfian distributions, we obtaineda BLEU score of 10.08, a slight increase compared to 7. Using again the word-by-word translation analysis,",
  "Experiment 5: Adding Supervision": "One possible explanation for the failures above could be the rotation effect described in 3. Some supervisedtraining could help avoid that risk, by anchoring at least some translations into ground-truth. We test twotypes of supervised examples: some aligned sentence translations (as when back-translation is used not forfully unsupervised translation but for data augmentation), and the injection of a bilingual dictionary.",
  "Small Aligned Dataset": "Here, we use back-translation as data augmentation, as it was originally used, by training the model ina supervised way on 1,000 sentences, on top of the training with back-translation on the same 100,000sentences from before. The training process is now as follows: first a step of denoising auto-encoding, then a",
  "supervised step with a batch from the aligned dataset and finally a back-translation step with a batch fromthe unaligned dataset. At each of these steps, the model updates its parameters": "Results presented in show that performance greatly improves, with BLEU scores often above 80 andall significantly above the results without supervision in , even though the grammars are different here.As a baseline, training only with the supervised examples is not sufficient (for example for 000000000000the BLEU score with only 1, 000 parallel sentences is 31.49, not 96.03).",
  "Bilingual Dictionary": "Previous results suggest that failure arises at the lexical level, so here we add supervised training on allpairs of aligned words, i.e. the complete bilingual dictionary. Results improve compared to previous results:BLEU scores are now between 28 and 72 (see ), when they were previously below 7 (see ). Overall then, back-translation works drastically more efficiently when complemented with some supervisedsignal, either at the sentence level (previous subsection) or at the lexical level (this subsection).",
  "Experiment 6: Towards Semantics via Lexical Fields": "One difference between our artificial languages and natural languages concerns semantics.Currently, itappears that our models roughly generate the correct POS sequence and then each word is randomly sampled,with no regard for semantic dependencies. Some methods have been proposed to address this samplingproblem (Arora et al., 2016; Hopkins, 2022a). To stay as close as possible to our previous experiments, we propose here a first-order approximation of thesemantic dependencies between different words in a sentence: each word in a content POS (Noun, Adj,TVerb, IVerb, Verb Comp) is associated with one of Nc lexical fields (respecting morphology, so bird andbirds would be associated with the same lexical field), and each sentence in the corpus is made of wordsfrom a single lexical field, or context (we use the two terms interchangeably). Words within a given POSand lexical field are sampled from a power law. This is compatible with results from Guerin et al. (2024)showing that beyond the usual Zipf law for word frequencies, co-occurrences of two pairs also follow a powerlaw, and that in practice very few pairs of words actually co-occur in a single sentence (which is compatiblewith words belonging to different lexical fields). See examples in . The idea of this experiment is tosee whether the model is capable of picking up this simple semantic cue. To measure success at capturing the lexical field information, we calculated (i) the proportion of sentencescontaining only words from the same lexical field, and (ii) for accurately POS-translated sentences, theproportion of words that were translated into a word from the right lexical field.",
  "hostician sub kurches": ": Examples of sentences from the vanilla corpus (without lexical fields constraints) and from the 2lexical fields corpus. In these examples the words burse and kurches are a noun and a verb from the firstlexical field, while rheleates and lurchifies are a noun and a verb from the second lexical field. Without thelexical field constraint, all pairs of words can co-occur and we can form four sentences, while the lexical fieldconstraint eliminates the mixed sentences. This constraint does not apply to function words, like sub here,which are transparent to lexical fields.",
  "sentences of 1. This success may come from the DAE only, which plays the role of language modeling,completing sentences within a given context": "For word-by-word translations, the accuracy was around 35% for 2 training seeds, and around 65% for theother 2 training seeds. This corresponds to the fact that in unsupervised learning, there is no signal tomap contexts correctly across languages, since they are permutable within each language. This is thus anexample of the rotation situation from c. This precision yet is not at ceiling: either 0% or 100%.This indicates that the model fails to learn perfectly this partition of our languages.",
  "Experiment 6b: Two unbalanced lexical fields": "In a second sub-experiment, we used Nc = 2 lexical fields again, and sampled the context of sentences acrossthe two contexts with an unbalance proportion of 30/70. The idea is to give the model the material todistinguish between contexts and overcome the bi-modal behavior noted above. The BLEU score was slightly increased compared to the same experiment without lexical fields (see ).The proportion of mono-context sentences remains at one. The proportion of words translated in the rightcontext are, over six training sessions: 55%, 65%, 66%, 70%, 73%, 76%. A naive baseline consisting in choosing the most frequent context would be at 70%. But this is visibly notwhat the models are doing (the output sentences are from both contexts). Another naive baseline consistingin choosing the context according to its proportion in the corpus would obtain 58%. The models are (almostalways) above that level. In addition, unlike in the case of equally frequent lexical fields, the different trainedmodels now tend to all choose the right mapping between lexical fields, therefore taking advantage of theirrelative frequencies to map them onto one another. This is both a good result, and a risk: if lexical fieldsare present in different proportions across cultures/languages, this could create a wrong signal. In sum, themodels are, to some extent, able to use semantic frequency cues to map lexical fields across languages.",
  "Experiment 6c: Ten unbalanced lexical fields": "We replicated the lexical field experiment with Nc = 10 lexical fields, and a number of sentences in eachlexical field varying according to a power law with parameter k = 1.1 as before, thereby providing finer-grained semantic information. The mono-context sentence proportion remains perfect at 1. The BLEUscore increases slightly to 11.92 (see comparative results in ). The proportion of words translated inthe right context are: 27%, 28% , 38% and 39%, which is above a random baseline at 20%.",
  "Conclusion": "The back-translation objective is not sufficient to align two sets without supervision, in general. This istrue even if it is complemented with additional objectives such as filtering or denoising auto-encoding. Themethod is successful nonetheless with real languages. This success then is presumably due to similaritiesbetween natural languages, which the training method picks up on. But it is not clear which similaritieshelp do that. Through controlled experimentation with artificial languages, we investigate the role of lexical,syntactic and semantic properties. We find that, when they share the exact same lexica, languages with more similar grammars are easier totranslate into one another. Hence, grammatical similarity across languages of the world could be a key tothe success of back-translation. But when lexica also vary, syntactic similarities are not sufficient to makeback-translation align two languages. Lexical alignments are thus hard to learn by back-translation. Whatlanguage properties make them learnable? We find that neither anchor points (partially shared vocabu-lary), nor rich, parallel word frequencies are enough to make back-translation work. Thus, manipulatingvarious lexical and syntactic properties only, we find that some supervision signal is critical to supportback-translation: through a small set of aligned sentences, or a complete set of aligned words. Moving to semantics then, we explored how the distribution of word cooccurrences influence the efficiencyof back-translation.We used only a crude form of semantics, by implementing lexical fields: differentclasses of words that never occur within the same sentence (think about: clothes, sock, shoe, shirtand astrophysics, interstellar, electromagnetic). We find that unsupervised back-translation models areable to pick-up on this (coarse) semantic signal to find a better alignment. We conclude that the success ofback-translation is probably due to an even richer semantic parallelism across languages, above and beyondtheir lexical and syntactic similarities. In the future, one would like to study more subtle semantic properties then. Currently, the semantic infor-mation we implemented is both coarse-grained, and not completely decisive. One would thus like to testwhether more realistic semantic information improves the system even more, or makes it collapse again. Andsubtle properties could be investigated. For instance, selectional restrictions on verbs may play a role inshaping text distributions, above and beyond syntactic constraints, in a way that may help induce alignmentacross languages in an unsupervised setting. Future work should thus explore realistic semantic distributions,while maintaining the experimental control that artificial languages provide (Hopkins, 2022a). This will helpunderstand what in natural languages makes unsupervised back-translation reach so much success, despiteits a priori theoretical insufficiency.",
  "Broader Impact Statement": "We hope that our work can have two impacts, navigating between engineering and scientific communities.In one direction, systematic investigations we present can help evaluate and improve applied translationsystem, and in particular for low resource languages where supervision is not an option. Conversely, we takeadvantage of the engineering success of back-translation, to unearth what natural languages are made of,how they spontaneously align with one another. 4The idea is to introduce a hierarchical Pitman-Yor processes to generate semantic dependencies. In a nutshell it makesthe generation of the sentences dynamical by modifying the probability of the next constituents based on the ones alreadygenerated. For instance after the occurrence of the verb eat, the probability to generate the word water decreases and that ofthe word food increases.",
  "Mikel Artetxe, Gorka Labaka, Eneko Agirre, and Kyunghyun Cho. Unsupervised neural machine translation.In International Conference on Learning Representations, 2018. URL": "Satanjeev Banerjee and Alon Lavie. METEOR: An automatic metric for MT evaluation with improvedcorrelation with human judgments. In Jade Goldstein, Alon Lavie, Chin-Yew Lin, and Clare Voss (eds.),Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Transla-tion and/or Summarization, pp. 6572, Ann Arbor, Michigan, June 2005. Association for ComputationalLinguistics. URL Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. Enriching word vectors with subwordinformation. Transactions of the Association for Computational Linguistics, 5:135146, 2017. doi: 10.1162/tacl_a_00051. URL Ondej Bojar and Ale Tamchyna. Improving translation model by monolingual data. In Chris Callison-Burch, Philipp Koehn, Christof Monz, and Omar F. Zaidan (eds.), Proceedings of the Sixth Workshop onStatistical Machine Translation, pp. 330336, Edinburgh, Scotland, July 2011. Association for Computa-tional Linguistics. URL Chris Callison-Burch, Miles Osborne, and Philipp Koehn. Re-evaluating the role of Bleu in machine trans-lation research. In Diana McCarthy and Shuly Wintner (eds.), 11th Conference of the European Chapterof the Association for Computational Linguistics, pp. 249256, Trento, Italy, April 2006. Association forComputational Linguistics. URL Isaac Caswell, Ciprian Chelba, and David Grangier.Tagged back-translation.In Ondej Bojar, RajenChatterjee, Christian Federmann, Mark Fishel, Yvette Graham, Barry Haddow, Matthias Huck, Anto-nio Jimeno Yepes, Philipp Koehn, Andr Martins, Christof Monz, Matteo Negri, Aurlie Nvol, MarianaNeves, Matt Post, Marco Turchi, and Karin Verspoor (eds.), Proceedings of the Fourth Conference on Ma-chine Translation (Volume 1: Research Papers), pp. 5363, Florence, Italy, August 2019. Association forComputational Linguistics. doi: 10.18653/v1/W19-5206. URL Yong Cheng, Wei Xu, Zhongjun He, Wei He, Hua Wu, Maosong Sun, and Yang Liu.Semi-supervisedlearning for neural machine translation. In Katrin Erk and Noah A. Smith (eds.), Proceedings of the 54thAnnual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 19651974,Berlin, Germany, August 2016. Association for Computational Linguistics. doi: 10.18653/v1/P16-1185.URL",
  "Cheng-Han Chiang and Hung yi Lee. On the transferability of pre-trained language models: A study fromartificial datasets. In AAAI Conference on Artificial Intelligence, 2021": "Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, FranciscoGuzmn, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. Unsupervised cross-lingualrepresentation learning at scale. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault (eds.),Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 84408451,Online, July 2020a. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.747. URL Alexis Conneau, Shijie Wu, Haoran Li, Luke Zettlemoyer, and Veselin Stoyanov. Emerging cross-lingualstructure in pretrained language models. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault(eds.), Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 60226034, Online, July 2020b. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.536.URL",
  "Ryan Cotterell and Julia Kreutzer. Explaining and generalizing back-translation through wake-sleep. ArXiv,abs/1806.04402, 2018": "Zi-Yi Dou, Antonios Anastasopoulos, and Graham Neubig. Dynamic data selection and weighting for itera-tive back-translation. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu (eds.), Proceedings of the2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 58945904, Online,November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.475. URL Philipp Dufter and Hinrich Schtze. Identifying elements essential for BERTs multilinguality. In BonnieWebber, Trevor Cohn, Yulan He, and Yang Liu (eds.), Proceedings of the 2020 Conference on EmpiricalMethods in Natural Language Processing (EMNLP), pp. 44234437, Online, November 2020. Associationfor Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.358. URL Sergey Edunov, Myle Ott, Michael Auli, and David Grangier.Understanding back-translation at scale.In Ellen Riloff, David Chiang, Julia Hockenmaier, and Junichi Tsujii (eds.), Proceedings of the 2018Conference on Empirical Methods in Natural Language Processing, pp. 489500, Brussels, Belgium,October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1045. URL Sergey Edunov, Alexei Baevski, and Michael Auli.Pre-trained language model representations for lan-guage generation.In Jill Burstein, Christy Doran, and Thamar Solorio (eds.), Proceedings of the2019 Conference of the North American Chapter of the Association for Computational Linguistics: Hu-man Language Technologies, Volume 1 (Long and Short Papers), pp. 40524059, Minneapolis, Min-nesota, June 2019. Association for Computational Linguistics.doi:10.18653/v1/N19-1409.URL Sergey Edunov, Myle Ott, MarcAurelio Ranzato, and Michael Auli. On the evaluation of machine translationsystems trained with back-translation. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault(eds.), Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 28362846, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.253.URL",
  "Nicolas Guerin, Emmanuel Chemla, Robin Ryder, and Shane Steinert-Threlkeld. Second-order zipf law forword co-occurrences. submitted, 2024": "Francisco Guzmn, Peng-Jen Chen, Myle Ott, Juan Pino, Guillaume Lample, Philipp Koehn, Vishrav Chaud-hary, and MarcAurelio Ranzato. The FLORES evaluation datasets for low-resource machine translation:NepaliEnglish and SinhalaEnglish. In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan (eds.),Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9thInternational Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 60986111, HongKong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1632.URL Barry Haddow, Rachel Bawden, Antonio Valerio Miceli Barone, Jindich Helcl, and Alexandra Birch. Surveyof Low-Resource Machine Translation. Technical Report arXiv:2109.00486, arXiv, February 2022. URL arXiv:2109.00486 [cs] type: article. Di He, Yingce Xia, Tao Qin, Liwei Wang, Nenghai Yu, Tie-Yan Liu, and Wei-Ying Ma. Dual learning formachine translation. NIPS16, pp. 820828, Red Hook, NY, USA, 2016. Curran Associates Inc. ISBN9781510838819. Vu Cong Duy Hoang, Philipp Koehn, Gholamreza Haffari, and Trevor Cohn.Iterative back-translationfor neural machine translation. In Alexandra Birch, Andrew Finch, Thang Luong, Graham Neubig, andYusuke Oda (eds.), Proceedings of the 2nd Workshop on Neural Machine Translation and Generation, pp.1824, Melbourne, Australia, July 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-2703. URL",
  "Ann Irvine and Chris Callison-Burch. End-to-end statistical machine translation with zero or small paralleltexts. Natural Language Engineering, 22(4):517548, 2016. doi: 10.1017/S1351324916000127": "Juraj Juraska, Mara Finkelstein, Daniel Deutsch, Aditya Siddhant, Mehdi Mirzazadeh, and Markus Fre-itag. MetricX-23: The Google submission to the WMT 2023 metrics shared task. In Philipp Koehn,Barry Haddow, Tom Kocmi, and Christof Monz (eds.), Proceedings of the Eighth Conference on MachineTranslation, pp. 756767, Singapore, December 2023. Association for Computational Linguistics.doi:10.18653/v1/2023.wmt-1.63. URL",
  "Gaurav Kharkwal. Taming the jabberwocky: examining sentence processing with novel words. PhD the-sis, Rutgers University - Graduate School - New Brunswick, 2014. URL #citation-export": "Jyotsana Khatri and Pushpak Bhattacharyya. Filtering back-translated data in unsupervised neural machinetranslation. In Donia Scott, Nuria Bel, and Chengqing Zong (eds.), Proceedings of the 28th InternationalConference on Computational Linguistics, pp. 43344339, Barcelona, Spain (Online), December 2020.International Committee on Computational Linguistics. doi: 10.18653/v1/2020.coling-main.383. URL Yunsu Kim, Yingbo Gao, and Hermann Ney. Effective cross-lingual transfer of neural machine translationmodels without shared vocabularies. In Anna Korhonen, David Traum, and Llus Mrquez (eds.), Pro-ceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 12461257,Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1120. URL Yunsu Kim, Miguel Graa, and Hermann Ney. When and why is unsupervised neural machine translationuseless? In Andr Martins, Helena Moniz, Sara Fumega, Bruno Martins, Fernando Batista, Luisa Coheur,Carla Parra, Isabel Trancoso, Marco Turchi, Arianna Bisazza, Joss Moorkens, Ana Guerberof, MaryNurminen, Lena Marg, and Mikel L. Forcada (eds.), Proceedings of the 22nd Annual Conference of theEuropean Association for Machine Translation, pp. 3544, Lisboa, Portugal, November 2020. EuropeanAssociation for Machine Translation. URL Alexandre Klementiev, Ann Irvine, Chris Callison-Burch, and David Yarowsky. Toward statistical machinetranslation without parallel corpora.In Proceedings of the 13th Conference of the European Chapterof the Association for Computational Linguistics, EACL 12, pp. 130140, USA, 2012. Association forComputational Linguistics. ISBN 9781937284190. Surabhi Kumari, Nikhil Jaiswal, Mayur Patidar, Manasi Patwardhan, Shirish Karande, Puneet Agarwal, andLovekesh Vig. Domain adaptation for NMT via filtered iterative back-translation. In Eyal Ben-David, ShayCohen, Ryan McDonald, Barbara Plank, Roi Reichart, Guy Rotman, and Yftah Ziser (eds.), Proceedings ofthe Second Workshop on Domain Adaptation for NLP, pp. 263271, Kyiv, Ukraine, April 2021. Associationfor Computational Linguistics. URL",
  "Guillaume Lample, Alexis Conneau, Marc Aurelio Ranzato, Ludovic Denoyer, and Herv Jgou.Wordtranslation without parallel data. In International Conference on Learning Representations, 2018b. URL": "Guillaume Lample, Myle Ott, Alexis Conneau, Ludovic Denoyer, and MarcAurelio Ranzato. Phrase-based &neural unsupervised machine translation. In Proceedings of the 2018 Conference on Empirical Methods inNatural Language Processing, pp. 50395049, Brussels, Belgium, Oct 2018c. Association for ComputationalLinguistics. doi: 10.18653/v1/D18-1549. URL Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike Lewis, andLuke Zettlemoyer. Multilingual denoising pre-training for neural machine translation. Transactions of theAssociation for Computational Linguistics, 8:726742, 2020. doi: 10.1162/tacl_a_00343. URL",
  "Toms Mikolov, Quoc V. Le, and Ilya Sutskever.Exploiting similarities among languages for machinetranslation. CoRR, abs/1309.4168, 2013. URL": "Xing Niu, Michael Denkowski, and Marine Carpuat. Bi-directional neural machine translation with syntheticparallel data. In Alexandra Birch, Andrew Finch, Thang Luong, Graham Neubig, and Yusuke Oda (eds.),Proceedings of the 2nd Workshop on Neural Machine Translation and Generation, pp. 8491, Melbourne,Australia, July 2018. Association for Computational Linguistics.doi: 10.18653/v1/W18-2710.URL Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation ofmachine translation. In Pierre Isabelle, Eugene Charniak, and Dekang Lin (eds.), Proceedings of the 40thAnnual Meeting of the Association for Computational Linguistics, pp. 311318, Philadelphia, Pennsylvania,USA, July 2002. Association for Computational Linguistics. doi: 10.3115/1073083.1073135. URL Steven T. Piantadosi.Zipfs word frequency law in natural language: A critical review and future di-rections.Psychonomic Bulletin and Review, 21(5):11121130, 2014.ISSN 15315320.doi: 10.3758/s13423-014-0585-6. Alberto Poncelas, D. Shterionov, Andy Way, Gideon Maillette de Buy Wenniger, and Peyman Passban.Investigating backtranslation in neural machine translation. In European Association for Machine Trans-lation Conferences/Workshops, 2018. Nima Pourdamghani, Nada Aldarrab, Marjan Ghazvininejad, Kevin Knight, and Jonathan May. Translat-ing translationese: A two-step approach to unsupervised machine translation. In Anna Korhonen, DavidTraum, and Llus Mrquez (eds.), Proceedings of the 57th Annual Meeting of the Association for Compu-tational Linguistics, pp. 30573062, Florence, Italy, July 2019. Association for Computational Linguistics.doi: 10.18653/v1/P19-1293. URL Ricardo Rei, Craig Stewart, Ana C Farinha, and Alon Lavie.COMET: A neural framework for MTevaluation. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu (eds.), Proceedings of the 2020Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 26852702, Online,November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.213. URL Hadeel Saadany and Constantin Orasan. BLEU, METEOR, BERTScore: Evaluation of metrics performancein assessing critical translation errors in sentiment-oriented text. In Ruslan Mitkov, Vilelmini Sosoni,",
  "Julie Christine Gigure, Elena Murgolo, and Elizabeth Deysel (eds.), Proceedings of the Translation andInterpreting Technology Online Conference, pp. 4856, Held Online, July 2021. INCOMA Ltd.URL": "Thibault Sellam, Dipanjan Das, and Ankur Parikh. BLEURT: Learning robust metrics for text generation.In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault (eds.), Proceedings of the 58th AnnualMeeting of the Association for Computational Linguistics, pp. 78817892, Online, July 2020. Associationfor Computational Linguistics. doi: 10.18653/v1/2020.acl-main.704. URL Rico Sennrich, Barry Haddow, and Alexandra Birch. Improving neural machine translation models withmonolingual data. In Katrin Erk and Noah A. Smith (eds.), Proceedings of the 54th Annual Meetingof the Association for Computational Linguistics (Volume 1: Long Papers), pp. 8696, Berlin, Germany,August 2016. Association for Computational Linguistics. doi: 10.18653/v1/P16-1009. URL Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie Yan Liu.MASS: Masked sequence to sequencepre-training for language generation. 36th International Conference on Machine Learning, ICML 2019,2019-June:1038410394, 2019. Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. Sequence to sequence learning with neural networks. InProceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2,NIPS14, pp. 31043112, Cambridge, MA, USA, 2014. MIT Press. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, ukasz Kaiser,and Illia Polosukhin. Attention is all you need. In Proceedings of the 31st International Conference onNeural Information Processing Systems, NIPS17, pp. 60006010, Red Hook, NY, USA, 2017. CurranAssociates Inc. ISBN 9781510860964. Yu Wan, Dayiheng Liu, Baosong Yang, Haibo Zhang, Boxing Chen, Derek Wong, and Lidia Chao. UniTE:Unified translation evaluation.In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio (eds.),Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1:Long Papers), pp. 81178127, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi:10.18653/v1/2022.acl-long.558. URL",
  "Yiren Wang, Yingce Xia, Tianyu He, Fei Tian, Tao Qin, ChengXiang Zhai, and Tie-Yan Liu. Multi-agent duallearning. In International Conference on Learning Representations, 2019. URL": "Jennifer C. White and Ryan Cotterell. Examining the inductive bias of neural language models with artificiallanguages. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics andthe 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp.454463, Online, Aug 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.38.URL Shijie Wu and Mark Dredze. Beto, bentz, becas: The surprising cross-lingual effectiveness of BERT. InKentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan (eds.), Proceedings of the 2019 Conference onEmpirical Methods in Natural Language Processing and the 9th International Joint Conference on NaturalLanguage Processing (EMNLP-IJCNLP), pp. 833844, Hong Kong, China, November 2019. Association forComputational Linguistics. doi: 10.18653/v1/D19-1077. URL Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang Macherey, MaximKrikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff Klingner, Apurva Shah, Melvin Johnson, XiaobingLiu, ukasz Kaiser, Stephan Gouws, Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith Stevens, GeorgeKurian, Nishant Patil, Wei Wang, Cliff Young, Jason Smith, Jason Riesa, Alex Rudnick, Oriol Vinyals,Greg Corrado, Macduff Hughes, and Jeffrey Dean. Googles Neural Machine Translation System: Bridgingthe Gap between Human and Machine Translation. Technical report, 2016."
}