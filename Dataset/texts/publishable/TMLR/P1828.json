{
  "Abstract": "Graph Neural Networks (GNNs) have been extensively used in various real-world applications.However, the predictive uncertainty of GNNs stemming from diverse sources such as inherent ran-domness in data and model training errors can lead to unstable and erroneous predictions. There-fore, identifying, quantifying, and utilizing uncertainty are essential to enhance the performance ofthe model for the downstream tasks as well as the reliability of the GNN predictions. This surveyaims to provide a comprehensive overview of the GNNs from the perspective of uncertainty withan emphasis on its integration in graph learning. We compare and summarize existing graph uncer-tainty theory and methods, alongside the corresponding downstream tasks. Thereby, we bridge thegap between theory and practice, meanwhile connecting different GNN communities. Moreover, ourwork provides valuable insights into promising directions in this field.",
  "Introduction": "Graph Neural Networks (GNNs) have been widely employed in important real-world applications, including outlier de-tection (Liu et al., 2022b), molecular property prediction (Wollschlger et al., 2023), and traffic regularization (Zhuanget al., 2022). Within these domains, GNNs inevitably present uncertainty towards their predictions, leading to unstableand erroneous prediction results. Uncertainty in GNNs arises from multiple sources, for example, inherent random-ness in data, error in GNN training, and unknown test data from other distributions. To mitigate the adverse impactof predictive uncertainty on GNN predictions, it is crucial to systematically identify, quantify, and utilize uncertainty.Uncertainty in GNNs plays a vital role in prevalent graph-based tasks, such as selecting nodes to label in graph ac-tive learning (Cai et al., 2017) and indicating the likelihood of a node being out-of-distribution (OOD) (Zhao et al.,2020). Furthermore, this practice enhances the predictive performance of GNNs (Vashishth et al., 2019), and bolsterstheir reliability for decision-making, such as making GNNs robust to attacks (Feng et al., 2021) and explains whichcomponents of the graphs influence GNN predictions (Ying et al., 2019). Several challenges need to be addressed to boost GNNs with uncertainty. Firstly, different sources of uncertainties inGNNs need to be identified, as uncertainty may stem from multiple stages including data acquisition, GNN construc-tion, and inference (Gawlikowski et al., 2023). Secondly, different categories of uncertainties should be matched withcorresponding tasks. For instance, distributional uncertainty should be applied to graph OOD detection (Gui et al.,2022). Thirdly, uncertainties in GNNs are difficult to quantify correctly due to the lack of ground truth and unifiedevaluation metrics. Lastly, quantified uncertainty may be adapted and combined with the other components to bet-ter serve the task, e.g., graph active learning (Zhang et al., 2021a). Multiple studies have endeavored to solve thesechallenges in specific tasks. In this paper, we provide a holistic view of these tasks through the lens of uncertainty. To the best of our knowledge, existing surveys or benchmarks either focus on a broader field, i.e., uncertainty in DeepNeural Networks (Abdar et al., 2021; Hllermeier & Waegeman, 2021; Gawlikowski et al., 2023) and trustworthyGNN (Wu et al., 2022), or on narrower topics, e.g., uncertainty in spatio-temporal GNNs (Wu et al., 2021; Wang et al.,2023). Besides, these surveys mainly discuss how to estimate and measure uncertainty, missing concrete and practicalguidance on how quantified uncertainties can be applied toward the final downstream task. Our survey aims to bridgethese gaps by conducting a systematic review of GNNs from the perspective of uncertainty, with a special focus onincorporating uncertainty into graph learning. Furthermore, our survey contributes to the broader GNN community by",
  "PU": ": Overall Framework: (1) identifying sources of uncertainty (), (2) quantifying uncertainty () and (3)utilizing uncertainty for downstream tasks (). In the first subfigure, PU refers to predictive/total uncertainty. AUrepresents aleatoric/data/statistical uncertainty, EU is in short for epistemic uncertainty. MU and DU represent model anddistributional uncertainty, respectively. reviewing the trends of the current studies to approach uncertainty from distinct sub-communities, particularly acrossdifferent downstream tasks. Our goal is to present a comprehensive framework that clarifies ambiguous terminologies,connects several communities, as well as offers valuable insights and directions to advance the overall development ofGNN uncertainty and related fields. In this paper, we organize the literature of uncertainty in GNNs into a three-stage framework, illustrated in .Consequently, we introduce real-world applications utilizing GNN uncertainty in . Finally, in ,we discuss significant and promising avenues for future research in this field. For further related background, weencourage readers to investigate Wu et al. (2020b) for a detailed introduction to GNNs, and Abdar et al. (2021);Hllermeier & Waegeman (2021); Gawlikowski et al. (2023) for uncertainty in deep learning.",
  "Identifying Sources of Uncertainty in GNNs": "In machine learning, (total) uncertainty generally equals predictive uncertainty (PU), defined as the uncertaintypropagated into the prediction results. For accurate estimation and application of uncertainty, it is necessary to firstidentify different sources of uncertainty. Predictive uncertainty is commonly divided into aleatoric and epistemic onesbased on their inherent different sources (Hllermeier & Waegeman, 2021). Aleatoric uncertainty (AU). Aleatoric uncertaintyknown as data or statistical uncertaintyrefers to variability inthe outcome of an experiment which is due to inherent random effects. This often arises during the pre-processingstage, where data collection may lack sufficient information or contain noise and errors. As an example, collectingmore data from the same graph distribution does not reduce aleatoric uncertainty; however, incorporating additionalnode features with non-overlapping information can reduce the inherent randomness. Unlike Munikoti et al. (2023), wealign with the prevailing theory that aleatoric uncertainty can be estimated but is non-reducible through improvementsin GNN (Hllermeier & Waegeman, 2021), using it as the primary criterion to categorize existing literature. Epistemic uncertainty (EU). In contrast, epistemic uncertaintyknown as systemic uncertaintyrefers to the uncer-tainty caused by the lack of knowledge of the GNN, thus reducible. It arises in the procedure of GNN modeling, furtherclassified as model uncertainty (MU). On one hand, model uncertainty comes from the choice of model structure.For instance, the shallow network structure may lead to the under-confidence of GNNs (Wang et al., 2022). On theother hand, the training process of GNN also introduces model uncertainty in the selection of parameters, for example,learning rate, epoch number, batch size, and techniques (e.g., weight initialization). Introduced by Malinin & Gales(2018), another type of epistemic uncertainty is distributional uncertainty (DU), which arises when GNN has falseassumption about the problem setting and data generation process. This is common for inductive inference, whereGNN is trained on one graph and applied to another graph for prediction tasks, such that the training and test sets do",
  "not come from the same distribution. While applying the trained GNN to the test data for inference, the distributionshift contributes to the uncertainty of prediction": "Note that since EU = MU + DU, and PU = AU + EU, then PU = AU + MU + DU. This relationship is illustrated inthe middle subfigure of . In most settings, prediction results are typically considered to have no distributionaluncertainty, DU=0, thus EU=MU+0=MU. To avoid confusion of terms, we will use epistemic uncertainty (EU) atmost time in Sections 4 and 5. If distributional uncertainty (DU) is explicitly stated, such as in tasks like OOD, wewill use MU and DU. Besides these frequently classified types, GNN uncertainty can also be classified from subjective viewpoints, e.g.,vacuity (conflicting evidence) and dissonance (lack of evidence) (Zhao et al., 2020). In general, the goal for GNN modeling should be to reduce EU, including delicate design of model structures andparameters, along with enhanced robustness and generalization to diverse distributions. On the contrary, since AUcan only be quantified but not reduced, separating it from the epistemic uncertainty would provide a more reliableillustration of GNN performance.",
  "Single Deterministic Model": "This family of methods consists of a single model with deterministic parameters, i.e., each repetition of the methodproduces the same result. We classify them into direct, Bayesian-based, and Frequentist-based estimation methods. Direct estimation. The most commonly used uncertainty estimation method is through the softmax probability ofprediction outcomes, including maximum softmax probability (Wang et al., 2021) and entropy (Cai et al., 2017).Other heuristic measures, such as the distance of the sample to each class in the representation space (Xu et al., 2023),are also used to quantify uncertainty. Direct estimation methods integrate uncertainty estimation with prediction anddo not require modifications of the original model. However, it has been shown that GNN tends to be under-confident (Wang et al., 2021), which leads to inaccurateestimation of confidence and entropy. Confidence is an indication of how likely (probability) the predictions of amachine learning algorithm are correct, often computed as the maximum softmax class probability. Under-confidence,in other words, refers to that confidence, as the indication of accuracy, falls below the true prediction accuracy. Tomitigate the issue of under-confidence, a well-calibrated GNN should produce predicted probabilities that accuratelyreflect the true likelihood of the predicted outcomes. This is usually evaluated with a smaller Expected CalibrationError (ECE) (Guo et al., 2017). To provide faithful uncertainty estimation, Wang et al. (2021) add a regularizer in",
  "Published in Transactions on Machine Learning Research (11/2024)": "Wentao Zhang, Xupeng Miao, Yingxia Shao, Jiawei Jiang, Lei Chen, Olivier Ruas, and Bin Cui.Reliable datadistillation on graph convolutional network. In Proceedings of the 2020 ACM SIGMOD International Conferenceon Management of Data, pp. 13991414, 2020. Wentao Zhang, Yu Shen, Yang Li, Lei Chen, Zhi Yang, and Bin Cui. Alg: Fast and accurate active learning frameworkfor graph convolutional networks. In Proceedings of the 2021 International Conference on Management of Data,pp. 23662374, 2021a. Wentao Zhang, Yexin Wang, Zhenbang You, Meng Cao, Ping Huang, Jiulong Shan, Zhi Yang, and CUI Bin. Informa-tion gain propagation: a new way to graph active learning with soft labels. In International Conference on LearningRepresentations, 2021b. Xu Zhang, Liang Zhang, Bo Jin, and Xinjiang Lu. A multi-view confidence-calibrated framework for fair and stablegraph representation learning. In 2021 IEEE International Conference on Data Mining (ICDM), pp. 14931498.IEEE, 2021c.",
  "Single Model with Parameters Being Random Variables": "Models with parameters being random variables, mostly Bayesian GNNs, are another widely used family of GNNuncertainty quantification methods. A vast majority of these methods rely on Monte Carlo (MC) dropout to sampleweights for variational inference. Zhang et al. (2019b) first develop a Bayesian neural network framework for graphsdepending on the graph topology, where the observed graph is viewed as a realization from a parametric family ofrandom graphs. Hasanzadeh et al. (2020) point out that the graph dropout techniques (Rong et al., 2019) can beutilized at the test time for uncertainty estimation, and advance Zhang et al. (2019b) by considering node features andadaptive connection sampling. Cha et al. (2023) discover that under the conformal prediction framework, BayesianGNN with a tempered posterior can improve the efficiency of predictions. Markov Chain Monte Carlo (MCMC)sampling methods are another family of Bayesian methods that represent uncertainty without a parametric model.However, these models are rarely used in graphs due to their expensive computational costs (Wu et al., 2021). Bayesian GNNs can separate AU and EU through metrics introduced in .4. Nevertheless, existing researchin this domain primarily concentrates on enhancing uncertainty awareness and minimizing predictive uncertainty.Despite their effectiveness in enhancing prediction accuracy and providing uncertainty estimates, Bayesian GNNs oftenencounter challenges in devising meaningful priors for graphs and efficiently estimating uncertainty through repeatedsampling during inference. Additionally, these models typically do not account for the distributional uncertainty (DU).",
  "Other Methods": "Different from the Bayesian GNNs that perturb parameters around a single optimal model, ensemble models derivethe final prediction based on the predictions from a set of independent models with different parameters or evenwith different model structures. Ensemble GNNs have been shown to provide more accurate prediction and reliableestimation than the single models (Bazhenov et al., 2022; Wang et al., 2023; Gawlikowski et al., 2023). Meanwhile,ensemble GNNs offer an intuitive means of expressing predictive uncertainty by assessing the diversity among thepredictions of the ensemble members. AU and EU could also be differentiated similarly as in Bayesian GNN methods.Despite their wide applications, most GNN ensemble models follow the deep ensemble framework (Lakshminarayananet al., 2017) without specific modifications for both graphs and uncertainty quantification purposes. Compared withthe two families above, ensemble methods are less used in GNNs mainly due to their high computational cost. Test-time data augmentation, another type of uncertainty quantification method, has been widely used in other fields,e.g., computer vision (Ayhan & Berens, 2022). It involves augmenting multiple samples for each test sample andapplying the model to all samples to compute the distribution of predictions. Then, AU is estimated with the entropyof the model prediction distribution for a test sample and its augmentations, and EU can also be estimated in thesame way as Bayesian GNN methods. However, in GNNs, test-time augmentation has only been utilized for otherpurposes. Jin et al. (2022) first propose to modify the node feature and the graph topology at the test time for betterOOD generalization. Similarly, Ju et al. (2024) use test time augmentation to mitigate the bias of GNN towardsachieving better performance on high-degree nodes. However, test-time augmentation has not yet been explicitlyapplied for graph uncertainty quantification and thus, is not included in our primary framework. Nevertheless, itspotential adaptation for graph uncertainty quantification warrants exploration. Given its post hoc nature, it requires noadditional data and shows better computational efficiency during training compared to ensemble and Bayesian GNNs.",
  "Evaluating Quantification Models": "We discuss the evaluation measures to assess the quality of GNN uncertainty quantification models. As in ,evaluations are not limited to a specific type of quantification model but to each targeted uncertainty source. This isbecause a model can quantify multiple sources of uncertainty, and models can only be evaluated regarding a specificuncertainty source. Given the challenge of acquiring ground truth uncertainty, various metrics have been proposed,and there is no consensus on a unified metric (Lakshminarayanan et al., 2017; Gawlikowski et al., 2023). Predictive (Total) uncertainty. As discussed above (Liu et al., 2022a; Zhao et al., 2020), maximum softmax prob-ability (referred to as confidence in this paper) and entropy are widely applied to quantify the predictive uncertaintyin the classification problems. They are also utilized as uncertainty measures of predictive uncertainty Smith & Gal(2018). Notably, entropy is often used as a measure of aleatoric uncertainty when applied on labeled dataset, e.g.,in decision tree. However, entropy in GNNs is calculated on the predicted probability of each sample. We believe it",
  "Entropy": "1.Wang et al. (2021); Cai et al. (2017); Guo et al. (2017); Hsu et al. (2022b); Xu et al. (2023)2.Zhao et al. (2020); Stadler et al. (2021); Bazhenov et al. (2023)3.Kang et al. (2022); Efron (1982); Huang et al. (2023a); Zargarbashi et al. (2023)4.Zhang et al. (2019b); Hasanzadeh et al. (2020); Rong et al. (2019); Cha et al. (2023); Wu et al. (2021)5.Bazhenov et al. (2022); Wang et al. (2023); Gawlikowski et al. (2023) : Bridge uncertainty quantification models and evaluation methods by uncertainty sources. The diamond shape representsthe separation of uncertainty sources. Quantification models linked to any diamond indicate their ability to separate the correspond-ing uncertainty source. We merge \"Model Uncertainty\" and \"Data Uncertainty\" in evaluation as they are complementary and sharesimilar evaluation metrics in some cases. should be considered to represent predictive uncertainty, as opposed to aleatoric uncertainty evaluation metrics as inGawlikowski et al. (2023). As mentioned in .1, these two metrics are vulnerable to distribution shifts andneed calibration. In regression problems, the mean width of prediction intervals (MWPI) (Kang et al., 2022; Huang et al., 2023a)can be used for evaluation. The coverage ratethe portion of ground-truth labels falling into these intervalsisverified to ensure the validity of this metric. Notably, efficiency is the same notion as MWPI in conformal regression,while in conformal classification, it refers to the average prediction set size under given coverage rate. In someworks (Zargarbashi et al., 2023), this metric is accompanied by singleton hit rate, i.e., the prediction accuracy ofprediction sets of size one. This joint use of both metrics satisfies the need for both prediction quality and uncertaintyquantification. However, MWPI estimates the predictive uncertainty over a set of data but not on individual samples. Model & Aleatoric uncertainty. The model uncertainty is commonly measured through mutual information (MI) toevaluate quantification models. MI is minimal when information in prediction does not increase with additional modelknowledge. It is calculated as the difference between the entropy of the expected distribution (predictive uncertainty)and the expected entropy (aleatoric uncertainty) (Malinin & Gales, 2018; Zhao et al., 2020). Notably, MI is alsoreferred to as information gain in some papers (Liu et al., 2022a). For Bayesian and ensemble GNNs, the predictivevariance of sampled outputs is another option to interpret model uncertainty (Zhang et al., 2019a; Gal & Ghahramani,2016). Furthermore, employed as one of the optimization objectives in some quantification methods (Liu et al.,2020; Zhao et al., 2020), the expected Kullback-Leibler (KL) divergence between approximated distribution and theposterior distribution can also evaluate model uncertainty. Given no distributional uncertainty, model uncertaintyequals epistemic uncertainty. In this case, aleatoric uncertainty, namely data uncertainty, can be measured as thedifference between the total uncertainty and model uncertainty; otherwise, the metric will be no longer meaningful. Distributional uncertainty. It is believed that quantification of distributional uncertainty will be able to separatein-distribution (ID) and out-of-distribution (OOD) samples Malinin & Gales (2018). A popular approach to measuretotal distributional uncertainty is through classifying test samples. Samples with distributional uncertainty surpassinga given threshold are categorized as OOD samples, while those below the threshold are considered ID samples. Inthis, conventional classification metrics, e.g., Receiver Operating Characteristic (ROC) curve and Area Under Receiver",
  ": Flowchart illustrating recommended quantification methods for various conditions. Diamond shapes represent conditions,while rectangular shapes indicate the recommended methods": "Operating Curve (AUROC) (Zhao et al., 2020; Song & Wang, 2022; Gui et al., 2022), can be applied to measure thedistributional uncertainty. However, this evaluation depends on the selection of test samples and threshold. In conclusion, the effectiveness of existing metrics remains under exploration. As an alternative, many studies di-rectly employ specific downstream tasks for evaluation to obtain reliable and straightforward measures. Nonetheless,depending on demands under different circumstances, we recommend diverse methods quantification methods ac-cording to their properties, as shown in . First, it is crucial to identify the specific source of uncertainty beingtargeted. To our knowledge, only Bayesian-based estimation methods explicitly quantify distributional uncertainty.Subsequently, one must consider the necessity of separating aleatoric and epistemic uncertainty. If this separationis deemed necessary, two categories of methods are available: ensemble models, which consist of different membermodels, and single models with parameters treated as random variables. For computational efficiency, we recommendsingle models with parameters as random variables. However, for improved prediction performance and robustnessacross different distributions, ensemble models are preferable. In some cases, the separation of aleatoric and epistemicuncertainty does not yield significant benefits. For practitioners seeking highly computationally efficient methodsfor individual examples (e.g., nodes or links), direct estimation is adequate. Conversely, for a distribution-free andstatistically guaranteed estimation of the overall test population, Frequentist-based estimation is recommended.",
  "Uncertainty-based Node Selection": "Node selection in graph learning refers to the process of selecting a subset of nodes from a graph that collectivelycontributes to specific tasks or analyses. Graph active learning and self-training are two common tasks involving nodeselection, where a collection of unlabeled data is labeled to improve the performance of the model (e.g., a GNN).Uncertainty is widely used as a criterion for node selection in these tasks. Active learning. In graph active learning tasks, uncertainty and representativeness (diversity) are prevalent criteriafor node selection (Ren et al., 2021). Uncertainty is often measured by the entropy of the prediction probabili-ties on the nodes. Cai et al. (2017) first propose a graph active learning framework, which uses a combination ofentropy-measured uncertainty, information density, and node centrality to select nodes. Zhang et al. (2021a) estimatean ensembled uncertainty using the variation of prediction generated by committee models, which is more robust andreliable than entropy as a single deterministic model. The method selects top k nodes to maximize the effective recep-tion field using the topological information of the graph, excluding nodes that are less uncertain and less informative.Unlike the previous two methods, which select the most uncertain nodes independently, Zhang et al. (2021b) considerlabel propagation and select a subset of nodes with maximized uncertainty (entropy) reduction to include diversity.Labeled nodes can propagate their label information based on the graph distance and the node feature similarity, andthen reduce the uncertainty of adjacent unlabeled nodes. Additionally, in a multi-task manner, Chang et al. (2024)",
  ": Illustration of representative usage of uncertainty in GNN tasks. The darker the color, the greater the uncertainty/weight": "estimate the confidence difference between the node classification task and anomaly detection task for node selectionin graph active learning. Notably, in most cases, graph active learning methods that select nodes with only uncertaintycan hardly outperform the ones with a combination of uncertainty and other measures. To achieve dynamic and adaptive node labeling, reinforcement learning (RL) has been applied to graph active learning.Gao et al. (2018) uses the combination of the measures in Cai et al. (2017) to select nodes, but dynamically adjuststhe combination weights based on a multi-armed bandit framework. Instead of measuring nodes independently, Huet al. (2020) formulates the active learning problem as a sequential decision process on the graph considering theinterconnections between nodes. Uncertainty is still measured with entropy but concatenated with other features, e.g.,the divergence between a nodes predicted label distribution and its neighbors, to form the state representation of eachnode. The reward is the model performance on hold-out data updated with a trajectory of selected nodes. However,the RL-based active learning methods are significantly time-consuming to train. Self-training. In graph self-training, i.e., pseudo-labeling, Li et al. (2018) propose one of the initial works that expandthe labeled set with top K confident nodes, quantified through maximal softmax probabilities. Considering that GNNis ineffective in propagating label information under the few-label setting, Sun et al. (2020) iteratively select and labelthe top K confident nodes. Drawing upon the concept of ensembling to robustly select and label nodes, Yang et al.(2021) employs multiple diverse GNNs. Nodes are selected with confidence surpassing a pre-determined thresholdand ensuring consistent prediction for labels across all models. Wang et al. (2021) is the first work to point out that theunder-confidence of GNNs leads to biased uncertainty estimation, deteriorating the performance of uncertainty-basedself-training node selection. With the calibrated confidence, they improve the performance of the self-training methodintroduced by Li et al. (2018). In consideration of graph structure information, Zhao et al. (2021) propose to computeentropy for each node, and add an entropy-aggregation layer to GNN for uncertainty estimation. Similar to graph active learning, recent self-training methods (Liu et al., 2022a; Li et al., 2023b) consider combininguncertainty with representativeness, but from different perspectives. Liu et al. (2022a) believe the common self-training practice that only selects high-confidence nodes leads to the distribution shift from the original labeled nodedistribution to the expanded node distribution. Therefore, they follow the common practice that selects high-confidencenodes as pseudo-labels but decreases the weight of the nodes with low information gain in the training loss. Interest-ingly, the dropout technique employed to compute information gain is often interpreted as the model uncertainty, asintroduced in .4. Similarly, Li et al. (2023b) also rely on the fact that the high-confidence nodes convey over-lapping information, causing information redundancy in pseudo-labeled nodes. The representativeness of a node is",
  "Uncertainty-based Abnormality Detection": "Abnormality detection is a pivotal task in graph machine learning for safety and security. In this section, we definegeneral abnormality detection as the task that identifies irregular patterns that deviate from the established norms.Uncertainty can serve as both a metric of abnormality and a tool for abnormality detection. Depending on the specificdefinition of abnormality, the application of uncertainty in abnormality detection can be systematically categorizedinto three distinct types: out-of-distribution detection, outlier detection, and misclassification detection. OOD detection. To begin with, out-of-distribution (OOD) detection focuses on identifying instances that diverge fromthe training data distribution (in-distribution). OOD detection involves identifying samples from unknown classes andpreventing models from making wrong decisions with overconfidence. In some literature (Wu et al., 2020a), OODdetection is integrated with the task of classification for in-distribution data and formulated as an open-world clas-sification. Open-world classifiers recognize predefined classes seen during training and identify instances from newclasses. For OOD detection, Wu et al. (2020a) maximize an entropy-based uncertainty loss for unlabeled nodes duringtraining, such that the instances of unseen classes can have a low and balanced predictive probability over the alreadyseen classes. During inference, if the maximal class probability of an instance is lower than a predefined threshold,the instance is regarded as OOD. In addition, Zhao et al. (2020) propose to quantify various types of uncertainty withgraph-based Dirichlet distribution, and point out that vacuity uncertainty, derived from a lack of evidence or knowl-edge, is the most effective in OOD detection among other uncertainties. Furthermore, considering the non-i.i.d. natureof graph data, Stadler et al. (2021) perform Bayesian posterior updates for predictions on interdependent nodes throughdiffusing Dirichlet parameters. Epistemic uncertainty without network effects is shown effective in detecting randomOOD nodes with perturbed features, while epistemic uncertainty with network effects performs better in identifyingnodes from the left-out class not present in the training data. Outlier detection. Apart from OOD detection, outlier detection (also known as anomaly detection) is another prevail-ing task that effectively leverages uncertainty in graph learning. Outlier detection refers to the identification of data thatexhibit significantly different behaviors or characteristics compared to the majority of the graph. Different from theOOD data, outliers can also exist in the training data. Ray et al. (2021) directly estimate the predictive uncertainty withdeviation from the distribution for interpretable anomalous behavior detection on multivariate time series. Notwith-standing conventional uncertainty quantification methods, Liu et al. (2022c) interpret the graph anomaly scores using",
  "Uncertainty-aware GNN Modeling": "Graphs serve as powerful representations of complex relationships and structures, but their structural uncertainty poseschallenges for effective learning and inference tasks. Fortunately, GNN can improve its prediction performance throughuncertainty-awareness. Based on addressing uncertainty from different components of graph structure, related workscan be divided into three categories: (1) node-level, (2) edge-level, and (3) graph-level. Node-level. In graphs, nodes often exhibit conflicting or contradictory characteristics, such as belonging to differ-ent communities or revealing contradictory underlying patterns (Bojchevski & Gnnemann, 2017). This discrepancyshould be reflected in the uncertainty of node embeddings, highlighting the need to account for uncertainty at thenode-level in GNN structures. Hajiramezanali et al. (2019) incorporate stochastic latent variables by combining aGraph Recurrent Neural Network (GRNN) with a Variational Graph Autoencoder (VGAE) (Kipf & Welling, 2016).The proposed variational GRNN is designed to capture the temporal dependencies in dynamic graphs. Moreover, itaims to represent each node by modeling its distribution in the latent space rather than assigning it a deterministicvector in a low-dimensional space, thus enhancing its ability to represent the uncertainty associated with node la-tent representations. Vashishth et al. (2019) propose ConfGCN, which introduces confidence (inversely proportionalto uncertainty) estimation for the label scores on nodes to enhance label predictions. More specifically, ConfGCNassumes a fixed uncertainty for input labels and defines an influence score of each node as a parameter that can beoptimized jointly with the network parameters through back-propagation. The influence score here is the inverse ofthe co-variance matrix-based symmetric Mahalanobis distance between two nodes in the graph. Sun et al. (2021) gen-erate stochastic representations using a hyperbolic VGAE, combining temporal GNN to model graph dynamics and itsuncertainty in hyperbolic space. Xu et al. (2022) introduce a Bayesian uncertainty propagation method and embedsGNNs in a Bayesian modeling framework. This approach uses Bayesian confidence of predictive probability based onmessage uncertainty to quantify predictive uncertainty in node classification. The proposed uncertainty-oriented losspenalizes the predictions with high uncertainty during the learning process, enabling GNNs to improve prediction reli-ability and OOD detection ability. Liu et al. (2022e) introduce the concept of a mixture of homophily and heterophilyat the node-level, revealing that GNNs may exhibit relatively high epistemic uncertainty for heterophilous nodes. Toaddress this issue, the paper proposes an Uncertainty-aware Debiasing framework, which estimates uncertainty inBayesian GNN output to identify heterophilous nodes and then trains a debiased GNN by pruning biased parametersand retraining the pruned parameters on nodes with high uncertainty. Edge-level. To better consider the inter-dependencies in the graph structure, some methods also explore uncertaintybeyond the node-level. While Velickovic et al. (2018) has applied a similar dropout technique on edge attentions, Ronget al. (2019) first formally presented the formulation of DropEdge. It randomly removes a specified number of edgesfrom the graph by drawing independent Bernoulli random variables (with a constant rate) during each training iteration.Rong et al. (2019) theoretically and empirically show that DropEdge, as a data augmenter and a message-passingreducer, can alleviates the problems of over-fitting and over-smoothing. Hsu et al. (2022a) propose two edgewisemetrics for graph uncertainty estimation: (1) edgewise expected calibration error (ECE) is designed to estimate the",
  "Uncertainty for Trustworthy GNN": "Trustworthy GNNs aims to provide explainability of decisions made by GNNs and to be robust to noise and adversarialattacks to ensure the integrity and security of the model. Uncertainty can serve as a tool to build algorithms for theinterpretation of GNNs decisions. Additionally, considering uncertainty during the aggregation mechanisms in GNNscan enhance robustness. Thus, uncertainty quantification can be an effective approach to achieving trustworthy GNNs. Explainability. Uncertainty quantification provides insights for the reliability and confidence of GNN decisions.Specifically, mutual information explains the relationship between the GNNs predictions and the components thatimpact GNNs predictions. Ying et al. (2019) propose GNNEXPLAINER which identifies a subgraph and/or node fea-tures that maximize mutual information with the prediction, to explain which components of the graph have the mostimpact on the prediction results. Mutual information is nonparametric and does not require prior knowledge andassumptions on models. Thus, it is model-agnostic and suitable for providing interpretable explanations for any GNN-based models. Besides, uncertainty quantification can also be used to explain predictions of automated GNNs. Yanget al. (2022) propose HyperU-GCN, which combines hyperparameter optimization with uncertainty quantification, toexplain the selection of hyperparameters. It quantifies hyperparameter uncertainty as the mutual information betweenthe model prediction and hyperparameters using a probabilistic hypernetwork. HyperU-GCN employs a concept akinto Bayesian GNNs for computing this mutual information, which marginalizes hyperparameters rather than modelparameters. Unlike GNNEXPLAINER, HyperU-GCN requires model hyperparameter priors (e.g. normal distribution),",
  "Uncertainty in GNNs: Real-world Applications": "Traffic. GNNs have been widely used to model network traffic, especially traffic demand forecasting. Uncertaintyarises in traffic systems both spatially and temporally, e.g., travel demand may increase on game days around thestadiums. Wu et al. (2021) compare six different Bayesian and Frequentist-based uncertainty quantification methodsfor road network traffic prediction problems, regarding computational efficiency, asymptotic consistency, etc. Zhuanget al. (2022) specifically focus on sparse travel demand and proposes STZINB-GNN, where a spatial-temporal em-bedding is designed with an additional sparsity parameter to learn the likelihood of inputs being zero. The probabilitylayer in the embedding allows for prediction with a level of confidence and thus, quantifying the demand uncertainty.Wang et al. (2023) propos a framework of probabilistic GNNs, Prob-GNN, to quantify spatiotemporal uncertaintyof traffic demand. Besides different GNNs, different probabilistic distribution assumptions with different properties",
  "Future directions": "Identifying fine-grained graph uncertainty. While current research on graph data has asserted their ability to differ-entiate between various types and sources of uncertainty (), only a few efforts have been made to identify andquantify uncertainty tailored specifically for graph components. For example, (Bojchevski & Gnnemann, 2017; Liuet al., 2022e) have assumed that heterophily, the conflict between two connected nodes, is correlated with uncertaintyin node classification. These attempts demonstrate that fine-grained graph uncertainty is essential for downstreamtasks. Therefore, it is promising to develop other unexplored fine-grained uncertainties that contribute to such spe-cific tasks. For instance, researchers might seek to identify specific types of graph distribution shifts, such as shiftsin graph node features, labels, and graph structure. Decomposing distributional uncertainty at a more granular levelcould aid in distinguishing and leveraging the corresponding shifts, thereby enhancing OOD detection and generaliza-tion. Furthermore, different forms of finer-grained graph uncertainty should be comparable in scale. By comparingtheir corresponding uncertainty measures, we can determine whether the distributional uncertainty arising from nodefeature or graph topology, or the GNN modeling is dominant. This contributes to uncertainty estimation with improvedexplainability by indicating how each component in graph data, GNN modeling, and inference process contributes tothe final GNN predictive uncertainty.",
  "Conclusions": "In this paper, we have categorized the existing methods into a novel schema and highlighted the importance of identi-fying, quantifying, and utilizing uncertainty in GNNs for various downstream tasks. Specifically, we have emphasizedthe need to identify fine-grained uncertainty, especially graph-related uncertainty, and construct ground-truth datasetsand unified evaluation metrics. Being the first survey to systematically review GNN uncertainty, we call for the de-velopment of more efficient, easy-to-apply, and robust uncertainty quantification methods, along with a systematicinvestigation and benchmark into existing methods on popular downstream tasks.",
  "This work is supported in part by NSF under grants III-2106758, and POSE-2346158": "Moloud Abdar, Farhad Pourpanah, Sadiq Hussain, Dana Rezazadegan, Li Liu, Mohammad Ghavamzadeh, PaulFieguth, Xiaochun Cao, Abbas Khosravi, U Rajendra Acharya, et al. A review of uncertainty quantification indeep learning: Techniques, applications and challenges. Information fusion, 76:243297, 2021. Adem RN Aouichaoui, Seyed Soheil Mansouri, Jens Abildskov, and Grkan Sin. Uncertainty estimation in deeplearning-based property models: Graph neural networks applied to the critical properties. AIChE Journal, 68(6):e17696, 2022.",
  "Bradley Efron. The jackknife, the bootstrap and other resampling plans. SIAM, 1982": "Bat-Sheva Einbinder, Yaniv Romano, Matteo Sesia, and Yanfei Zhou. Training uncertainty-aware classifiers withconformalized deep learning. Advances in Neural Information Processing Systems, 35:2238022395, 2022. Boyuan Feng, Yuke Wang, and Yufei Ding. Uag: Uncertainty-aware attention graph neural network for defendingadversarial attacks. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp. 74047412,2021.",
  "Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural networks. In Interna-tional conference on machine learning, pp. 13211330. PMLR, 2017": "Saumya Gupta, Yikai Zhang, Xiaoling Hu, Prateek Prasanna, and Chao Chen.Topology-aware uncertainty forimage segmentation.In Thirty-seventh Conference on Neural Information Processing Systems, 2023.URL Ehsan Hajiramezanali, Arman Hasanzadeh, Krishna Narayanan, Nick Duffield, Mingyuan Zhou, and Xiaoning Qian.Variational graph recurrent neural networks. Advances in neural information processing systems, 32, 2019. Kehang Han, Balaji Lakshminarayanan, and Jeremiah Zhe Liu. Reliable graph neural networks for drug discoveryunder distributional shift. In NeurIPS 2021 Workshop on Distribution Shifts: Connecting Methods and Applications,2021. Arman Hasanzadeh, Ehsan Hajiramezanali, Shahin Boluki, Mingyuan Zhou, Nick Duffield, Krishna Narayanan, andXiaoning Qian. Bayesian graph neural networks with adaptive connection sampling. In International conferenceon machine learning, pp. 40944104, 2020.",
  "Kexin Huang, Ying Jin, Emmanuel Candes, and Jure Leskovec. Uncertainty quantification over graph with confor-malized graph neural networks. Advances in Neural Information Processing Systems, 2023a": "Qiang Huang, Makoto Yamada, Yuan Tian, Dinesh Singh, and Yi Chang. Graphlime: Local interpretable modelexplanations for graph neural networks. IEEE Transactions on Knowledge and Data Engineering, 2022. Yihong Huang, Liping Wang, Fan Zhang, and Xuemin Lin. Unsupervised graph outlier detection: Problem revisit,new insight, and superior method. In 2023 IEEE 39th International Conference on Data Engineering (ICDE), pp.25652578. IEEE, 2023b.",
  "Eyke Hllermeier and Willem Waegeman. Aleatoric and epistemic uncertainty in machine learning: An introductionto concepts and methods. Machine Learning, 110:457506, 2021": "Wei Jin, Tong Zhao, Jiayuan Ding, Yozen Liu, Jiliang Tang, and Neil Shah. Empowering graph representation learningwith test-time graph transformation. In The Eleventh International Conference on Learning Representations, 2022. Mingxuan Ju, Tong Zhao, Wenhao Yu, Neil Shah, and Yanfang Ye. Graphpatcher: Mitigating degree bias for graphneural networks via test-time augmentation. Advances in Neural Information Processing Systems, 36, 2024. Kim-Celine Kahl, Carsten T Lth, Maximilian Zenk, Klaus Maier-Hein, and Paul F Jaeger. Values: A framework forsystematic validation of uncertainty estimation in semantic segmentation. International Conference on LearningRepresentations, 2024. Jaykumar Kakkad, Jaspal Jannu, Kartik Sharma, Charu Aggarwal, and Sourav Medya. A survey on explainability ofgraph neural networks. Bulletin of the IEEE Computer Society Technical Committee on Data Engineering, 2023. Jian Kang, Qinghai Zhou, and Hanghang Tong. Jurygcn: quantifying jackknife uncertainty on graph convolutionalnetworks. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pp.742752, 2022.",
  "Youngchun Kwon, Dongseon Lee, Youn-Suk Choi, and Seokho Kang. Uncertainty-aware prediction of chemicalreaction yields with graph neural networks. Journal of Cheminformatics, 14:110, 2022": "Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive uncertainty esti-mation using deep ensembles. Advances in neural information processing systems, 30, 2017. Jinxing Li, Tianyuan Liu, Guangya Zhu, Yunzhu Li, and Yonghui Xie. Uncertainty quantification and aerodynamicrobust optimization of turbomachinery based on graph learning methods. Energy, 273:127289, 2023a. Qimai Li, Zhichao Han, and Xiao-Ming Wu. Deeper insights into graph convolutional networks for semi-supervisedlearning. In Proceedings of the AAAI conference on artificial intelligence, volume 32, 2018.",
  "Yayong Li, Jie Yin, and Ling Chen. Informative pseudo-labeling for graph neural networks with few labels. DataMining and Knowledge Discovery, pp. 228254, 2023b": "Hongrui Liu, Binbin Hu, Xiao Wang, Chuan Shi, Zhiqiang Zhang, and Jun Zhou. Confidence may cheat: Self-trainingon graph neural networks under distribution shift. In Proceedings of the ACM Web Conference, 2022a. Kay Liu, Yingtong Dou, Yue Zhao, Xueying Ding, Xiyang Hu, Ruitong Zhang, Kaize Ding, Canyu Chen, Hao Peng,Kai Shu, et al. Bond: Benchmarking unsupervised outlier node detection on static attributed graphs. Advances inNeural Information Processing Systems, 35:2702127035, 2022b. Kay Liu, Yingtong Dou, Yue Zhao, Xueying Ding, Xiyang Hu, Ruitong Zhang, Kaize Ding, Canyu Chen, Hao Peng,Kai Shu, et al. Pygod: A python library for graph outlier detection. arXiv preprint arXiv:2204.12095, 2022c. Tong Liu, Yushan Liu, Marcel Hildebrandt, Mitchell Joblin, Hang Li, and Volker Tresp. On calibration of graph neuralnetworks for node classification. In International Joint Conference on Neural Networks, pp. 18. IEEE, 2022d. Yang Liu, Xiang Ao, Fuli Feng, and Qing He. Ud-gnn: Uncertainty-aware debiased training on semi-homophilousgraphs. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pp.11311140, 2022e. Zhao-Yang Liu, Shao-Yuan Li, Songcan Chen, Yao Hu, and Sheng-Jun Huang. Uncertainty aware graph gaussianprocess for semi-supervised learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34,pp. 49574964, 2020.",
  "Matteo Pagliardini, Gilberto Manunza, Martin Jaggi, Michael I Jordan, and Tatjana Chavdarova. Improving general-ization via uncertainty driven perturbations. arXiv preprint arXiv:2202.05737, 2022": "Lorenzo Perini, Vincent Vercruyssen, and Jesse Davis. Quantifying the confidence of anomaly detectors in theirexample-wise predictions.In Joint European Conference on Machine Learning and Knowledge Discovery inDatabases, pp. 227243. Springer, 2020. Weizhu Qian, Dalin Zhang, Yan Zhao, Kai Zheng, and JQ James. Uncertainty quantification for traffic forecasting: Aunified approach. In 2023 IEEE 39th International Conference on Data Engineering (ICDE), pp. 9921004. IEEE,2023.",
  "Yu Rong, Wenbing Huang, Tingyang Xu, and Junzhou Huang. Dropedge: Towards deep graph convolutional networkson node classification. In International Conference on Learning Representations, 2019": "Seongok Ryu, Yongchan Kwon, and Woo Youn Kim. A bayesian graph convolutional network for reliable predictionof molecular properties with uncertainty quantification. Chemical science, 10(36):84388446, 2019. Yan Scholten, Jan Schuchardt, Simon Geisler, Aleksandar Bojchevski, and Stephan Gnnemann.Randomizedmessage-interception smoothing: Gray-box certificates for graph neural networks. Advances in Neural Informa-tion Processing Systems, 35:3314633158, 2022. Uday Shankar Shanthamallu, Jayaraman J Thiagarajan, and Andreas Spanias. Uncertainty-matching graph neural net-works to defend against poisoning attacks. In Proceedings of the Proceedings of the AAAI Conference on ArtificialIntelligence Conference on Artificial Intelligence, volume 35, pp. 95249532, 2021.",
  "Yu Song and Donglin Wang. Learning on graphs with out-of-distribution nodes. In Proceedings of the 28th ACMSIGKDD Conference on Knowledge Discovery and Data Mining, pp. 16351645, 2022": "Maximilian Stadler, Bertrand Charpentier, Simon Geisler, Daniel Zgner, and Stephan Gnnemann. Graph poste-rior network: Bayesian predictive uncertainty for node classification. Advances in Neural Information ProcessingSystems, 34:1803318048, 2021. Ke Sun, Zhouchen Lin, and Zhanxing Zhu. Multi-stage self-supervised learning for graph convolutional networks ongraphs with few labeled nodes. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pp.58925899, 2020. Li Sun, Zhongbao Zhang, Jiawei Zhang, Feiyang Wang, Hao Peng, Sen Su, and S Yu Philip. Hyperbolic variationalgraph neural network for modeling dynamic graphs. In Proceedings of the AAAI Conference on Artificial Intelli-gence, volume 35, pp. 43754383, 2021. Lichao Sun, Yingtong Dou, Carl Yang, Kai Zhang, Ji Wang, S Yu Philip, Lifang He, and Bo Li. Adversarial attackand defense on graph data: A survey. IEEE Transactions on Knowledge and Data Engineering, 2022.",
  "Min Wang, Hao Yang, and Qing Cheng.Gcl: Graph calibration loss for trustworthy graph neural network.InProceedings of the 30th ACM International Conference on Multimedia, pp. 988996, 2022": "Qingyi Wang, Shenhao Wang, Dingyi Zhuang, Haris Koutsopoulos, and Jinhua Zhao. Uncertainty quantification ofspatiotemporal travel demand with probabilistic graph neural networks. arXiv preprint arXiv:2303.04040, 2023. Xiao Wang, Hongrui Liu, Chuan Shi, and Cheng Yang. Be confident! towards trustworthy graph neural networks viaconfidence calibration. Advances in Neural Information Processing Systems, 34:2376823779, 2021. Tom Wollschlger, Nicholas Gao, Bertrand Charpentier, Mohamed Amine Ketata, and Stephan Gnnemann. Uncer-tainty estimation for molecules: Desiderata and methods. In International conference on machine learning, pp.3713337156. PMLR, 2023. Bingzhe Wu, Jintang Li, Junchi Yu, Yatao Bian, Hengtong Zhang, CHaochao Chen, Chengbin Hou, Guoji Fu, LiangChen, Tingyang Xu, et al. A survey of trustworthy graph learning: Reliability, explainability, and privacy protection.arXiv preprint arXiv:2205.10014, 2022. Dongxia Wu, Liyao Gao, Matteo Chinazzi, Xinyue Xiong, Alessandro Vespignani, Yi-An Ma, and Rose Yu. Quan-tifying uncertainty in deep spatiotemporal forecasting. In Proceedings of the 27th ACM SIGKDD Conference onKnowledge Discovery & Data Mining, pp. 18411851, 2021.",
  "Man Wu, Shirui Pan, and Xingquan Zhu. Openwgl: Open-world graph learning. In 2020 IEEE international confer-ence on data mining (icdm), pp. 681690. IEEE, 2020a": "Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and S Yu Philip. A comprehensive surveyon graph neural networks. IEEE transactions on neural networks and learning systems, 32(1):424, 2020b. Hui Xu, Liyao Xiang, Junjie Ou, Yuting Weng, Xinbing Wang, and Chenghu Zhou. Open-world graph active learningfor node classification. ACM Transactions on Knowledge Discovery from Data, 18(2):120, 2023.",
  "Zhao Xu, Carolin Lawrence, Ammar Shaker, and Raman Siarheyeu. Uncertainty propagation in node classification.In 2022 IEEE International Conference on Data Mining (ICDM), pp. 12751280. IEEE, 2022": "Han Yang, Xiao Yan, Xinyan Dai, Yongqiang Chen, and James Cheng. Self-enhanced gnn: Improving graph neuralnetworks using model outputs. In International Joint Conference on Neural Networks, pp. 18. IEEE, 2021. Xueying Yang, Jiamian Wang, Xujiang Zhao, Sheng Li, and Zhiqiang Tao. Calibrate automated graph neural net-work via hyperparameter uncertainty. In Proceedings of the 31st ACM International Conference on Information &Knowledge Management, pp. 46404644, 2022. Zhitao Ying, Dylan Bourgeois, Jiaxuan You, Marinka Zitnik, and Jure Leskovec. Gnnexplainer: Generating explana-tions for graph neural networks. Advances in neural information processing systems, 32, 2019.",
  "Yu Zhao, Qiang Xu, Ying Zou, and Wei Li. Modeling user reviews through bayesian graph attention networks forrecommendation. ACM Transactions on Information Systems, 41(3):129, 2023": "Dingyuan Zhu, Ziwei Zhang, Peng Cui, and Wenwu Zhu. Robust graph convolutional networks against adversarialattacks. In Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining,pp. 13991407, 2019. Dingyi Zhuang, Shenhao Wang, Haris Koutsopoulos, and Jinhua Zhao. Uncertainty quantification of sparse travel de-mand prediction with spatial-temporal graph neural networks. In Proceedings of the 28th ACM SIGKDD Conferenceon Knowledge Discovery and Data Mining, pp. 46394647, 2022."
}