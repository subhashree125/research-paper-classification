{
  "Abstract": "Mislabeled examples are ubiquitous in real-world machine learning datasets, advocating thedevelopment of techniques for automatic detection. We show that most mislabeled detec-tion methods can be viewed as probing trained machine learning models using a few coreprinciples. We formalize a modular framework that encompasses these methods, parame-terized by only 4 building blocks, as well as a Python library that demonstrates that theseprinciples can actually be implemented. The focus is on classifier-agnostic concepts, with anemphasis on adapting methods developed for deep learning models to non-deep classifiersfor tabular data. We benchmark existing methods on (artificial) Completely At Random(NCAR) as well as (realistic) Not At Random (NNAR) labeling noise from a variety of taskswith imperfect labeling rules. This benchmark provides new insights as well as limitationsof existing methods in this setup.",
  "Introduction": "In supervised machine learning, the performance of learned algorithms crucially depends on the quality ofthe dataset of examples used during training: how many examples do we have access to, are these examplesrepresentative of the actual distribution on the feature space, and were the training examples correctlylabeled. We focus on the latter subject. Indeed, many actual use cases include some amount of labelingerrors. For example, this is typically the case in tasks that involve human supervision since labeling largedatasets requires a pool of annotators that possess a mix of expert knowledge (which is costly), and willingnessto perform repetitive tasks (which is dull). This is also known to be the case for widely used benchmarkdatasets such as CIFAR10/100 or MNIST (Northcutt et al., 2021b). Therefore, cleansing datasets offersthe promise of better performance, but at the cost of additional efforts. Since the early days of machinelearning, it has been widely believed that this could be achieved through automated methods, eliminatingthe need for further human intervention. This has led to a number of methods for automatic detection ofmislabeled examples using classical machine learning methods (Guan & Yuan, 2013). With the success ofdeep learning methods in applications ranging from image recognition to language models, new mislabeleddetection methods have also been proposed that exploit its specific training dynamics.",
  "Published in Transactions on Machine Learning Research (09/2024)": "Dropping unmatched examples is not a consensus among the weakly supervised learning literature, the usualapproach is to assign them a random label. We think dropping is a more sensible approach, akin to whatpractitioners would do in practice, than the random assignment. It should be noted that such noisy labels generate a non-squared transition matrix as the set of noisy classesis equal to the set of original classes plus the unlabeled class. In the figures from table 2, the unlabeled noisyclass corresponds to the last row of the transition matrix T.",
  "List of contributions": "Concepts We provide a fresh view on a majority of existing mislabeled detection methods byshowing that they can be described in a modular framework that uses a few components. Thisallows us to provide a large survey the highlights the similarities and differences of these methods.We identify a set of probes that provide the base signal used to discriminate between genuine andmislabeled examples.",
  "Survey We review a large number of existing detection methods, as well as other strategies toaddress specific issues. We survey 3 common strategies in weakly supervised learning": "Implementation We contribute a library that allows instantiating a large number of existingmislabeled detection methods, as well as designing and experimenting with new ones. Rather thanpackaging a number of different detection methods, our library focuses on implementing the coreprinciples of our framework, then existing specific methods can be instantiated in a few lines of codeand are readily available, as well as the possibility to benchmark new methods. We also packagehelpers to load existing weakly supervised datasets from the literature using a common interface inorder to improve the reproducibility of experiments in the weakly supervised setup. Empirical evaluation On a large benchmark of text and tabular datasets, we evaluate a seriesof detectors in different setups, where we vary the type of noise (weak supervision using labelingfunctions or uniform noise), the hyperparameter selection method (using a clean validation set ora noisy validation set), and the strategy for handling detected mislabeled examples (filtering orrelabeling). We identify a number of practical questions for which our experiments provide newinsights. We also share raw data for our experimental results as we believe they can be used toanswer some future questions. Using our classifier-agnostic implementation, we propose the firstcomparison of different mislabeled detection methods on the exact same task and using the samefeatures and base machine learning model.",
  "Definitions and problem statement": "Training datasets that contain mislabeled examples are prevalent in real-world machine learning applications.Our main aim is to detect these mislabeled examples. A first challenge that we found while reviewingmislabeled detection methods is that it is difficult to come up with a general formal definition of what itmeans for an example to be mislabeled. For some obvious cases, a human annotator can easily tell if an example is correctly labeled or mislabeled,but in some others the definition of mislabeled is more ambiguous such as when an instance can be consideredto lie in 2 different classes. As an attempt at giving a precise definition, we distinguish between 2 theoreticalframeworks in the next sections. We found no precedence of such an attempt.",
  "Deterministic case: the true concept is a function": "We aim at estimating a (deterministic) function f from an input space X to an output space Y. Ideally wewould observe a finite sample Dnoiseless of n examples and their corresponding labels {(xi, yi = f (xi))}1inwhere the xi are sampled from an unknown distribution P (X) over the input space. Typically, in classifica-tion, yi is the class of the instance, whereas in regression, yi is a scalar value. In real life, the data labeling process is often imperfect: our actual observed dataset Dtrain contains examples{(xi, yi)}1in that can undergo a corruption process and get a different label yi = yi. In this case, thedefinition is straightforward: an example is considered mislabeled if yi = f (xi). The approach of seeing machine learning as estimating an unknown function has permitted many theoreticaland practical successes in the early days of computational learning theory. It, however, falls short of formal-izing the more general case where each point in the input space corresponds to several different targets withnon-zero (but possibly unbalanced) probability. This is the setup studied in statistical learning theory (seeLuxburg & Schlkopf, 2011, for a concise but comprehensive overview of statistical learning theory).",
  "Stochastic case: the true concept is defined as a probability distribution": "A more general case consists in defining the true underlying concept as a joint probability distributionP (X, Y ) = P (X) P (Y |X). At fixed x X, the mass of P (Y |X = x) is not necessarily concentrated intoa single mode.Put differently, there are (possibly infinitely) many possible values for y with non-zeroprobability. This happens for instance when x does not contain all necessary information to predict y andthere remains some aleatoric uncertainty. Typically, in classification, we would like to learn an estimatorthat returns probabilities of belonging to each class f (x)c = P (Y = c|X = x), while in regression, it wouldreturn the expected value f (x) = E[Y |X = x]. Similarly to the deterministic case, contrarily to the independent and identically distributed assumption oftenused in statistical learning theory, there is some discrepancy between the true concept that we want to learn,and the actual distribution of collected examples during the data labeling process: whereas we would ideallyget a sample Dideal of n examples {(xi, yi)}1in from P (X, Y ), we actually observe a training dataset Dtrainof examples (x, y) sampled from a corrupted distribution PX, Y. In this case, the definition of a mislabeled example is ambiguous since we do not have a single ground truthvalue, but instead a probability distribution over the output space Y of many possible ones. This happens,for instance, when the true concept is modeled as a mixture of possible classes. As an example, suppose aninput x where P (Y |X = x) is a mixture of a majority class with probability 99% and a minority class withprobability 1%, and suppose an example (x, y) where y is the minority class. Do we consider this exampleto be mislabeled? In the rest, we suppose that we have access to a sensible threshold : we consider examples (x, y)with probability under the true concept P (Y = y|X = x) < to be mislabeled and other examples to begenuine. Note that this is the true concept P (Y |X), which is unknown in general, not the output of an",
  "estimator trained using a finite set of examples. This definition also covers the deterministic setting as aspecial case": "This choice is further motivated in data pipelines (section 2.6), where the output of a detection stage is fedto a filtering procedure that produces a dataset of most trusted examples used to train a machine learningestimator. In this case, we are often interested in recovering only the majority classes since they are alsothe most likely ones in our evaluation (test) set.However, we emphasize that in some other contexts,this definition might not be well suited, such as when we are more interested in predicting correctly forunderrepresented instances in fairness-related tasks.",
  "Detecting mislabeled examples using trust scores": "Estimating the conditional probability is by itself a difficult problem, which e.g. requires proper calibrationof machine learning models. Since we are only interested in splitting the dataset into genuine examplesand mislabeled ones, it is sufficient to solve the relaxed problem of estimating a proxy of the conditionalprobability, hereafter called trust score. Definition 2.1 (Trust score). Any scoring function s (x, y) that is correlated with the conditional probabilitysuch that for any 2 examples (x1, y1) and (x2, y2), it preserves the ranking between conditional probabilities:s (x1, y1) s (x2, y2) P (Y = y1|X = x1) P (Y = y2|X = x2). Equipped with this trust score, we can split the training dataset in 2 distinct parts: trusted exampleswith high trust scores and untrusted examples with low trust scores. For an ideal trust score method andthreshold, the trusted and untrusted datasets are equal to the genuine and mislabeled examples sets. Insection 3.2, we give a comprehensive review of model-probing methods for computing trust scores.",
  "Assumptions regarding the noise generating process": "A common approach to detect mislabeled examples within a dataset is to design detectors based on explicitassumptions regarding the structure of the underlying noise-generative process. A widely favored structureis known as the noise transition matrix denoted by T. Here, (i, j) [[1, K]] [[1, K]], where K is thenumber of noisy classes and K the number of true classes, Ti,j represents the probability P( Y = i|Y = j)of an example from the class j to have been assigned a noisy label from class i (Van Rooyen & Williamson,2017). This concept holds the advantage of generalizing over class-dependent label noise in a multi-classclassification setting. In the instance-dependent label noise scenario, the noise transition matrix becomes a function of the exampleT : X M K,K(R), and in the uniform label noise scenario, it is reduced to a constant corresponding to theoverall noise rate. A series of work has been dedicated to the estimation of the noise transition matrix forthe class-dependent (Liu & Tao, 2015; Patrini et al., 2017; Xia et al., 2019; Yao et al., 2020) and instance-dependent (Xia et al., 2020; Yang et al., 2022) case. However, additional assumptions are often necessary to estimate the noise transition matrix. Anchor point-based methods assume the existence and identifiability of high-confidence samples within the dataset (Liu &Tao, 2015; Patrini et al., 2017). Alternatively, some techniques require prior knowledge of class distributionsor specific noise ratios (Wang et al., 2017), or they exploit the structural characteristics of the noise transitionmatrix, such as its tendency to form clusters in the feature space (Liu et al., 2023). Mixture proportionestimation approaches infer noise ratios or the noise transition matrix by assessing the contamination levelof one classs feature distribution by others (Vandermeulen & Scott, 2016).However, these approachespresuppose that class distributions are mutually irreducible, implying distinct and non-overlapping patternsamong classes (Scott, 2015). In contrast, we assume no prior knowledge of the underlying noise structure in this paper. Instead, we choseto focus on detectors that do not explicitly depend on structural aspects of the noise-generating process,specifically focusing on the family of model-probing detectors. The success of these detectors relies on animplicit assumption concerning the base models behavior, which should exhibit significant differences whenapplied to noisy versus clean data. In practice, model-probing detectors may fail in extreme cases where the",
  "A taxonomy of data regions": "In practical machine learning applications, we do not have exact knowledge of the true concept P (X, Y ), butinstead, we only have access to a limited sample of data points. Depending on the availability of data, bylooking at the training set examples only, we distinguish between 4 cases (pictured in figure 1 for a 2-classestoy example): Fig 1.(1) We have access to many examples and all are from the same class. In this case we can unambiguouslyconsider this class as the true class: any example from another class lying in this region would beconsidered mislabeled.",
  "Fig 1.(2) We have access to many examples, but they are equally spread into 2 different classes. In this case,no example from these 2 classes should be considered mislabeled": "Fig 1.(3) We only have access to a few examples but it looks like they all come from the same class. Does thismean that this is the true class, or just that data is too scarce in this region so we are just unsureabout the true class? Fig 1.(4) We only have access to a few examples, and they come from 2 different classes. Does that mean thatwe are close to a boundary of the true underlying concept with 2 separate regions from 2 differentclasses, or is it a region where examples from the 2 classes are sampled with equal probability fromthe true underlying concept?",
  "denserare": ": Illustration of a ground truth distribution P (X, Y ) decomposed as P (X) on the y-axis and P (Y |X)on the x-axis and a sample of 100 data points from this distribution. In this toy distribution, we distinguish 4different cases represented as 4 quadrants: (1) P (X) is dense, P (Y |X) is low entropy: we are pretty confidentthat the ground truth class isso anyexample would be mislabeled (2) P (X) is dense, P (Y |X) is highentropy, we cannot distinguish between classesandthus we would be unable to tell correctly labeledfrom mislabeled examples (3) P (X) is scattered, but P (Y |X) is low entropy so we can assume that theground truth class isand anyexample should be deemed mislabeled (4) Since P (X) is scattered, it ismore difficult to detect that P (Y |X) is high entropy by looking at the data only, it is likely that mislabeleddetection methods would fail in the absence of further assumptions.",
  "Use cases": "Detecting mislabeled examples is of practical interest in real-world scenarios where obtaining the ground truthlabel of an example is an imperfect process. The causes of these imperfections are diverse and sometimeseven intended in order to automatize as much as possible machine learning operations. We now highlightsome imperfect labeling processes and the role that mislabeled example detection can play in these scenarios. Weak supervisionProperly labeling an example sometimes requires a costly and non-scalable procedureprohibiting annotation of the whole training dataset. Fraud detection and cyber security are both fieldswhere the labeling process requires scarce domain experts who need to conduct time-consuming forensicanalysis. One of the most popular solutions is to distill expert knowledge into hand-engineered labeling rulesto automatize and scale the labeling process to the entire training dataset. These rules form a new formof supervision called weak supervision (Ratner et al., 2016), and instead of the usual strong supervision,these labeling rules might produce incorrect labels that could harm the efficiency of machine learning modeltrained from these rules. Mislabeled example detection could potentially help experts design better rules tostrengthen the weak supervision. Our experiments in section 5 include a benchmark of mislabeled detectionmethods in this setting. Crowd labelingWhen the labeling task is achievable by human annotators, outsourcing the labelingprocess to decentralized annotators is a common approach to annotate webscale datasets. This techniquecalled crowdsourced labeling (Yuen et al., 2011) has been employed extensively in computer vision tocreate the first large image datasets such as ImageNet (Deng et al., 2009). Nowadays, it is employed to fine-tune large language models from human feedback (Ouyang et al., 2022; Bai et al., 2022). However, withcrowdsourced labeling, the effectiveness of each annotator in following the labeling guidelines can vary and beunreliable. Mislabeled example detection provides a way to evaluate the annotators ability to systematicallyfollow the guidelines and even detect malicious annotators. Web scrapingIn order to quickly assemble supervised web-scale datasets free of human intervention,automatically crawling the web gathering data and labels from querying engines is a popular approach.Web scraping has been used in the natural language processing community to design sentiment analysisdatasets (Maas et al., 2011) from movie reviews, or in the computer vision community to study label noiseat scale (Xiao et al., 2015). As shown in the latter, the oracle used to label examples, here the search engine,sometimes diverges from the ground truth and provides noisy annotations. In the former, the data in itselfwas wrong because of human error, resulting in wrongly labeled data. More recently, web scraping has beenextensively used to train large language models, in a self but still supervised fashion where the labelto predict is the token next to the input sequence. However, using non-curated web data has been shownto severely hinder their capacities to produce non-toxic or hallucinations-free text (Wang et al., 2023). Inthese situations, mislabeled examples detection can provide insights on the quality of the data source usedto automatically construct datasets. In light of these scenarios, the data understanding part of the data mining methodology (Shearer, 2000)seems more prevalent than ever, caused by the ambition to automatize every part of the machine learningoperations.",
  "Fully automated learning in the presence of mislabeled examples: detect + handle strategies": "Strategies to address learning with noisy labels include (Frnay & Verleysen, 2013): (i) using algorithms thatare naturally robust to label noise; (ii) using algorithms that explicitly model label noise during training;(iii) assigning a trust score to each training example to then manually inspect low trust training examples,or use an automated downstream method that can leverage this additional metadata. All three families ofmethods are useful depending on the context. In the rest of the paper, we focus on the latter strategy:The detection of mislabeled instances is not only valuable for the sole purpose of curating a dataset thatmore accurately reflects the underlying concept, but it is also a critical preliminary step in a comprehensivepipeline, illustrated in figure 2. The ultimate goal is to train an estimator on a smaller dataset, free ofmislabeled instances, with the expectation of getting more accurate predictions.It is achieved using adetection method that provides trust scores that are then fed to a splitting strategy that separates a trustedpart of the training examples from untrusted ones. A final stage handles the 2 datasets in order to providea trained estimator. Here we distinguish between 3 strategies: FilteringThe simplest approach for handling untrusted examples is to discard them from the trainingdataset, thereby training using only trusted data. This approach can also be found in the literature underthe name of data cleaning or editing (Wilson & Martinez, 2000). From this perspective, it is not harmful toaccidentally flag some correctly labeled examples as being untrustworthy as long as enough representativecorrectly labeled examples remain in the trusted dataset. To the contrary, some correctly labeled examplesthat lie in underrepresented regions might be accidentally flagged as mislabeled, which would degrade theperformance of the final estimator in these regions of the feature space. Therefore the effectiveness of thisapproach is bound to the underlying detection method being able to distinguish between rare (thus difficult)and mislabeled examples and the precise tuning of the threshold used to split the dataset into trusted anduntrusted examples. Semi-supervised learningAlthough filtering is a direct approach, it may be overly dismissive of the in-formation contained within untrusted data. Since this study is restricted to labeling noise, training instancesare considered free of feature space noise. Thus, a more reasonable approach is to retain the instances whiledisregarding their labels, thus casting the handle step into a semi-supervised problem (Li et al., 2020). This semi-supervised approach has the advantage of maintaining the entirety of the training dataset, thuspreserving the original data distribution. However, it inherits the filtering methods intrinsic limitation ofdiscarding some information from the untrustworthy examples (here, their labels), which could be partiallycorrect or beneficial for the learning process. Biquality learningThe ideal approach would involve retaining the full training dataset while incorporat-ing metadata about the quality of each example, allowing the learning algorithm to leverage this auxiliaryinformation. A particular case of this scenario is called biquality data, where two datasets are available attraining time, a trusted and an untrustworthy dataset, which falls under the biquality learning framework(Nodet et al., 2021). Algorithms within this framework are able to make more granular decisions regardinghow untrusted examples are handled, potentially reweighting or relabeling individually each example. How-ever, this approach assumes a high confidence in the trusted dataset: Any mislabeled example misclassifiedas trusted could significantly undermine the effectiveness of these algorithms.",
  "Using trained models to detect mislabeled examples": "Machine learning consists in identifying regularities in data sets and exploiting these regularities in order topredict the outcome on new examples. To the contrary, mislabeled examples are instead the ones that deviatefrom these regularities. The underlying concept behind model-probing mislabeled detection methods is thatthese irregular examples are treated somehow differently from genuine examples by the machine learningmodel. Informally, a good base model should find mislabeled examples difficult to learn and genuine exampleseasier to learn. In practice, quantifying how regular or irregular every example is, is done by probing trained",
  "Biquality": ": Data pipeline for different learning strategies when in the presence of labeling noise, where anintermediary step uses a detection method to assign trust scores to every example, then splits the datasetinto a trusted and an untrusted part. machine learning models (figure 3).To this end, similar to the fact that some machine learning methodsmay be more suited than others to some particular tasks (i.e. they get better generalization performance),accurately detecting mislabeled examples crucially depends on the choice of machine learning method andproper tuning of hyperparameters.",
  "A general framework for model-probing methods with examples": "We now discuss one of the contributions of our work, which is to provide a general framework that en-compasses most methods for the identification of mislabeled examples, with a few exceptions discussed insubsequent sections. We also survey existing methods found in the literature and show that they fit in thisframework. The framework is pictured in figure 4. The final outcome of these methods is a scalar trust score(defined in .1) for each example, used to rank examples from most likely to be mislabeled to mosttrusted. The framework is composed of 4 components that we shortly describe next. We then go in moredepth in subsequent sections, where we show that most surveyed methods correspond to an instance of thisframework using specific choices of each component. 1/ base model:All model-probing detection methods rely on fitting a machine learning model to thetraining set examples (or a subset thereof, depending on an optional ensemble strategy). This model shouldincorporate some robustness to mislabeled examples so that they are treated differently from genuine exam-ples.",
  "/ aggregation method:These scores are then aggregated to provide a single scalar trust score for eachexample": "As an example, the Area Under the Margin (AUM) method (Pleiss et al., 2020) fits in this framework byconsidering the base model to be a deep network, the probe to be the margin at every iteration duringtraining, the ensemble strategy is the consecutive iterates, and the aggregation method is just the sum ofthese margins. summarizes all surveyed methods that fit in the framework. This unified picture suggests that we canautomatically design new methods by simply replacing one or several of the components in already existingmethods. This also advocates for transferring ideas developed for deep learning models to classical ones(section 4.3) and vice versa, where for instance the method of observing the fluctuations of the per-exampleaccuracy as training progresses has been independently discovered in deep learning (Toneva et al., 2018) andwith AdaBoost (Chen et al., 2022).",
  "Base model": "The base model is the core component of this framework, which is trained on the training examples (ora subset of the training examples depending on the ensemble strategy, see section 3.2.3) and then probed(section 3.2.2) to give a scalar score to every example. Intuitively, a good candidate base model should treatgenuine and mislabeled examples differently, so that mislabeled examples are more difficult to learn thangenuine ones. This form of robustness against learning mislabeled examples is for instance found in deeplearning models (Arpit et al., 2017; Feldman & Zhang, 2020) where mislabeled examples have been shown tobe handled differently (Krueger et al., 2017), in particular depending on the training regime (George et al.,2022), or in decision trees where mislabeled examples often end-up as a single instance of a different class inotherwise pure leaves. In general, every off-the-shelf machine learning method can be chosen, depending onthe task (data and output types, and performance metric). Some detection methods however require specificcharacteristics for the base model, such as being differentiable with respect to their input (Agarwal et al.,2022) or their parameters (Koh & Liang, 2017). The popularity of base models used for mislabeled examples is directly linked to the general popularityof machine learning models, where nearest neighbors methods used to be more popular in the early daysof machine learning (Wilson, 1972; Tomek, 1976), then kernelized linear methods (Thongkam et al., 2008;Ekambaram et al., 2016) and more recently decision tree ensembles (Verbaeten & Van Assche, 2003; Chenet al., 2022). With the success of deep learning methods in image or text related tasks, new mislabeleddetection methods have followed that exploit the specific features of neural networks such as their trainingdynamics (Toneva et al., 2018; Pleiss et al., 2020; Agarwal et al., 2022; Pruthi et al., 2020; Koh & Liang,2017; Jiang et al., 2021), as well as classical machine learning methods on top of features extracted fromdeep networks representations (e.g. k-NNs in Bahri et al., 2020; Zhu et al., 2022; 2024).",
  "Model probe": "Fitted models are then probed in order to get scalar scores that are used to discriminate between genuineand mislabeled examples (figure 3). Probes produce intermediate values that are then aggregated (section *We use the word probe throughout which is generic enough to encompass a variety of different methods that follow thesame purpose of scoring each example by means of some measurement on a trained model.",
  ": Schematic summary of model-probing detection methods, with their 4 components": "3.2.4) to a single scalar trust score for each example. The most naive way of probing a model is simply to useits prediction, but it can also be done using more convoluted methods, some that are similar to the conceptof uncertainty in active learning (Settles, 2011), some others which are more specific to the machine learningmodel used such as gradient of logits with respect to input pixels in Agarwal et al. (2022). An early series of detection methods simply use the predicted class by comparing the prediction given byneighbor examples with a k-NN classifier (Wilson, 1972; Tomek, 1976; Brodley & Friedl, 1999) or a SVM(Segata et al., 2010; Thongkam et al., 2008) and flag examples as mislabeled if the prediction does not matchthe dataset label. Following a different motivation but similarly using the predicted class as the probe, forgetscores (Toneva et al., 2018) in deep learning and fluctuation scores (Chen et al., 2022) in AdaBoost measurehow much the per-example accuracy oscillates as training progresses, where more oscillations indicate a moredifficult thus potentially mislabeled example. AdaBoost example weights (Verbaeten & Van Assche, 2003) have been used as probes, with larger weightsindicating more difficult examples. This is very similar to the method of small loss popularized by a seriesof methods in deep learning (Amiri et al., 2018; Jiang et al., 2018). Here, examples with small loss areconsidered genuine and larger loss are a sign of suspicious examples. In the context of learning with thecross-entropy loss, this is also similar to using the margin (Dligach & Palmer, 2011; Pleiss et al., 2020),where a small or negative margin indicates a potential mislabeled example, or to consider support vectorexamples (Ekambaram et al., 2016) as suspicious as these are the examples closer to the decision boundaryin support vector machines. In Paul et al. (2021), the 2 distance between the one-hot encoded label andthe softmax output of a deep network defines the EL2N score which is generally higher for noise instances.",
  "Ensemble strategy": "Whereas some detection methods only rely on a point estimate (e.g. Wilson, 1972; Segata et al., 2010),mislabeled detection methods can be improved by leveraging an ensemble strategy (Verbaeten & Van Assche,2003). We distinguish between the independent and progressive families of ensemble methods, that differby the nature of the members of the ensemble, and how we leverage the differences in weak learners in thecontext of detecting mislabeled examples.",
  "Aggregation method": "We obtain a series of probe scores from each model of the ensemble. In order to summarize them to asingle scalar trust score, we now need to aggregate all these measurements. This is achieved by choosing anaggregation method. The simplest one is just to average the probed scores (or equivalently, take their sum)as done e.g. in Northcutt et al. (2021a); Pleiss et al. (2020); Pruthi et al. (2020), or compute a majorityvote or consensus as in Guan et al. (2011); Verbaeten & Van Assche (2003). A natural extension is instead to measure some form of spread of probed scores, with a large spread indicatingthat ensemble members disagree thus a potential mislabeled example. This is e.g. achieved using the 2variance in Agarwal et al. (2022); Seedat et al. (2022), also reminiscent of the idea of uncertainty in activelearning (Chang et al., 2017). Going a step further, the difference in probe scores between members ofthe ensemble that include a given example in their training subset and members that do not (e.g.,vanHalteren, 2000) can also be leveraged, where a larger difference suggests a potential mislabeled example,whereas a prototypical example will probably have less influence on the predictor since similar examples arelikely included in the training subset. For instance C-scores (Jiang et al., 2021) are a measure of how likelya datapoint is to be correctly classified by a model trained on a subset that does not include it. Similarly,DataShapley values (Ghorbani & Zou, 2019; Kwon & Zou, 2022) measure the individual value of eachexample on a utility function such as the test loss, with a negative value indicating a potentially mislabeledexample. For progressive ensemble strategies, there is a natural ordering of the members of the ensemble, which arethe consecutive iterations of the algorithm. This is exploited by several techniques. Forget scores in deeplearning (Toneva et al., 2018) and fluctuation (Chen et al., 2022) in AdaBoost count how many timesthe per-example accuracy between consecutive iterations changed from not predicting the dataset label tocorrectly predicting it, with larger values indicating that the example is more difficult to learn. In Cordeiroet al. (2023), examples are considered clean if their individual loss is smaller than a threshold for epochsin a row where and are 2 hyperparameters, and in Yuan et al. (2023), trustworthy examples are those",
  "Base modelProbeEnsemble strategyAggregation": "k-NNaccuracyleave-one-outOOB valueWilson (1972)k-NNaccuracyno ensemblen/aTomek (1976)variousaccuracybootstrappingmajority voteBrodley & Friedl (1999)AdaBoostexample weightsno ensemblen/aVerbaeten & Van Assche (2003)SVMaccuracyno ensemblen/aThongkam et al. (2008)Local SVMaccuracyno ensemblen/aSegata et al. (2010)MaxEntmarginno ensemblen/aDligach & Palmer (2011)variousself confidencedifferent modelssumSmith & Martinez (2014)SVCsupport vectorsno ensemblen/aEkambaram et al. (2016)deep networkinfluenceno ensemblen/aKoh & Liang (2017)deep networkaccuracylearning iterationschange countToneva et al. (2018)deep networklossno ensemblen/aAmiri et al. (2018)deep networklocal intrinsic dim.no ensemblen/aMa et al. (2018)deep networkrepresenter valueno ensemblen/aYeh et al. (2018)deep networklossno ensemblen/aJiang et al. (2018)deep networkmarginlearning iterationssumPleiss et al. (2020)deep networkloss gradientlearning iterationssumPruthi et al. (2020)k-NNaccuracyno ensemblen/aBahri et al. (2020)deep network2 distance one-hotno ensemblen/aPaul et al. (2021)deep networkaccuracybootstrappingsumJiang et al. (2021)deep networkprediction depthno ensemblen/aBaldock et al. (2021)deep networkself confidencebootstrappingmeanNorthcutt et al. (2021a)deep networkinfluenceno ensemblen/aKong et al. (2021)deep networkinput/output gradientlearning iterationsvarianceAgarwal et al. (2022)decision stumpaccuracyboosting iterationschange countChen et al. (2022)AdaBoost# weak learnersno ensemblen/aChen et al. (2022)k-NNaccuracyno ensemblen/aZhu et al. (2022)various1 distancecross-validationOOB valueZhou et al. (2023)",
  ": Taxonomy of model-probing detection methods": "that are correctly classified for k epochs in a row earlier during training, where k is also a hyperparameter.The 2 variance of the input/output gradient of individual examples across iterations is used in Agarwalet al. (2022), with the idea that the training dynamics of a neural network will fluctuate around mislabeledexamples as it will require a decision function that spikes around a single example in an otherwise uniformregion. Instead of aggregating different scores from different members of an ensemble, it is also possible to computek different probes for a single trained model, as done in Lu et al. (2023) for k = 2. Here, the scores areaggregated in a single trust score using a Gaussian mixture model (GMM), but in general, we could expectany k-dimensional clustering method to work. Summary of aggregation methodsaverage/sum, majority vote, consensus, variance, difference in vsout, DataShapley, difference between iterates, stability for epochs in a row, clustering in higher dimension",
  "Bag of (clever) tricks": "The framework proposed in section 3.2 is the backbone for many reviewed detection methods. In addition,we now survey some additional techniques proposed in the literature, which we consider as plugins that aimat solving specific problems that arise when dealing with mislabeled examples. These methods are appliedon top of the framework and are agnostic to the model-probing detection strategy.",
  "Iterative refinement": "The pipeline for learning in the presence of mislabeled examples detailed in section 2.6 is a 2-stage approachwith detect and handle stages applied once. A natural way to improve its efficiency is to do many-passesover the training examples by probing base models fitted on a sequence of refined datasets iteratively, wherethe base models of iteration T are trained using clean examples only filtered by iteration T 1 (Tomek,1976; Chen et al., 2019). The iterative refinement approach can be integrated directly into the training procedure of the machinelearning model. As training progresses, the sets of beneficial and detrimental examples may change. Byincorporating the detection stage into each iteration of the training procedure, the model can be updatedincrementally with the suitable refined dataset given its current progress in its curriculum (Sedova et al.,2023).",
  "Surely mislabeled pseudo-class": "The trust scores generated by most detection methods are often on an arbitrary scale. Splitting a training setinto a trusted and untrusted part thus requires a carefully chosen threshold. This threshold depends on thedetection method (and scale of the trust scores), as well as on the noise level and structure of noise. Whenused in a detect + filter pipeline, choosing an appropriate value of the threshold is of crucial importance.It can be achieved by treating it as a hyperparameter in a cross-validation setup (which likely requires anoise-free validation set, see discussion in section 5.3). Alternatively, Pleiss et al. (2020) proposed to artificially introduce examples that are purposely mislabeled(we assign them a wrong class) and measure their trust scores so that we get a distribution of trust scorescorresponding to surely mislabeled examples. This is achieved by introducing an extra pseudo-class andassigning it to a random subset of the training data. It, however, assumes that the noise introduced by thisadditional class has similar properties as the noise in the original data.",
  "Class-balancing mechanisms": "A limitation of detection methods arises when dealing with class-imbalanced datasets. Given that minorityclass examples are scarce, they are often more difficult to correctly predict than other examples, thus theyare more prone to being flagged as potentially mislabeled examples by most detection methods. In themeantime, they are often the most useful ones given their scarcity: we would not be able to correctly predictthe minority class if there were to few examples from the minority class in the training data. In order to alleviate this issue, a reasonable approach is to detect mislabeled examples in a one vs. restfashion by selecting trusted examples class per class (Northcutt et al., 2021a; Wang et al., 2022; Karim et al.,2022). An alternative approach is to normalize scores across classes allowing the splitting step to be done parsi-moniously for all classes, which can be done by a simple scaling (Kim et al., 2023), or done under the peeredprediction framework (Miller et al., 2005) using peer examples (Liu & Guo, 2020; Cheng et al., 2020). An alternative could revolve around the calibration of the trust scores compared to the true conditionalprobability, which remains an under-explored area. The only works we are aware of that attempt to tacklethis problem proposes to adjust the predicted probabilities of training examples by the average predictedprobability for each class while probing the model (Northcutt et al., 2021a; Kuan & Mueller, 2022).",
  "Reducing epistemic uncertainty": "Another interpretation of the problem of differentiating hard but clean examples from noisy examples isthrough the field of uncertainty quantification, specifically in the distinction between epistemic and aleatoricuncertainty (Hllermeier & Waegeman, 2021). Epistemic uncertainty corresponds to uncertain predictions ofa base model that can be reduced with more training data (rare and hard examples). Aleatoric uncertaintycorresponds to irreducible uncertainty, where the information from the features of a sample alone cannotpredict its label (e.g. when the true underlying concept is a mix of several classes).",
  "Library": "To further emphasize that the proposed framework in section 3.2 is not only of theoretical interest butalso of practical one, we now present another important contribution of our work in the form of a Pythonlibrary that materializes the 4 components (base model, probe, ensemble strategy, aggregation method) ofthe framework into a modular approach with 4 blocks that can readily be customized. Implementing anexisting method of the literature from table 1 then amounts to just specifying the value of each column ofthe table, and we can invent new methods as new combinations of already existing components.",
  "AreaUnderMargin = ModelProbingDetector(base_model=GBM(),ensemble=ProgressiveEnsemble(),probe=\"margin\",aggregate=\"sum\",)": ": This code reads \"consider a gradient boostedtree model (GBM) as a progressive ensemble, com-pute margins for all examples at every iteration duringtraining, sum them up to obtain scalar trust scores\". Noteworthy, we can readily perform ablation studies with respect to any of the 4 components by keepingall 3 others fixed. For instance, implementing the same detector but using iterates of a logistic regressionmodel trained with gradient descent can be done by replacing GBM with scikit-learns LogisticRegressionestimator. Similarly, instead of using the margin, we could design a different probe for a specific need, and",
  "We designed our library so that we can easily extend it with new ideas (e.g. new ways of probing a basemodel), as long as each component follows the following block contracts:": "The BaseModel contract follows the widely used API of scikit-learns estimators (Buitinck et al.,2013). It allows using scikit-learns suite of already implemented estimators, as well as estimatorsfrom other libraries that follow the same widely used API. The EnsembleStrategy contract is a single method .probe_model(base_model, X, y, probe)that takes as an input a non-initialized base model, the features, the labels, and the probing method,and outputs the computed probes as an iterator of length n_models yielding NumPy arrays of shape(n_samples, n_probes), and potential metadata, such as an iterator of boolean masks indicatingif a sample was part of the training set of the probed model (e.g. in the case of bootstrapping itallows to distinguish in-the-bag from out-of-bag examples).",
  "The probe contract is a callable that takes as an input a fitted model, features, and labels of trainingsamples and outputs a one-dimensional NumPy array of length n_samples": "The aggregator is a callable that defines how we summarize a series of probe scores viewed as aniterator yielding NumPy arrays of shape (n_samples, n_probes) to a one-dimension NumPy arrayof trust scores of length n_samples. Thanks to the modularity of the API as well as the ease of adding new components, exploring new detectorsuncovered in table 1 (unknown region of a 4D cube) is as easy as changing one string when instantiatinga ModelProbingDetector.We hope that our library can foster the design and understanding of futuremislabeled detection methods.",
  "A common API for progressive ensembles": "We propose a novel API to unify all incremental machine learning approaches into a single contract namedstaged_fit that produces a stream of machine learning models from a dataset. As streams are lazy datastructures, it allows flexible implementations of this contract for different families of machine learning models.For deep networks, to reduce memory cost, only a single model is kept in memory, and the network is trainedincrementally between each iteration. For gradient boosting machines, all trees are trained at once and thencopied and dispatched into smaller GBT s for each iteration. Moreover, the concept of iteration can bechanged dynamically and independently for different model families. The provided implementation for deepnetworks uses an epoch as the reference for an iteration, but a batch version could be used instead. As longas a notion of increment in complexity exists for a family of machine learning models, a staged_fit can beimplemented. For example, decision trees are treated as progressive models, from decision stumps to fullygrown trees with pure leaves.",
  "Full pipelines": "In addition to the detection API using ModelProbingDetector objects, we also provide a way of building fullpipelines following detect + handle strategies as described in section 2. Splitter objects define strategiesto split a training set in a trusted and untrusted part using the trust scores (e.g. using a specified thresholdor by keeping a specified top quantile of the trust scores). Handler objects implement connectors to learningstrategies such as filtering, semi-supervised learning, or biquality learning so that all necessary tools to createfully automated data pipelines are readily available.",
  "Benchmarks": "A classical approach to benchmarking detection methods is to evaluate them on synthetic tasks where noisylabels are injected artificially into otherwise clean datasets. It allows us to conduct a post-mortem analysison the accuracy of detecting mislabeled examples since both the noisy and ground truth labels of all examplesare known. Yet, using only synthetic corruptions in experimental protocols might lead to wrong conclusionson the actual performance of detection methods as they might not be representative of real-world corruptions.In our benchmark, we use text and tabular datasets with noisy labels generated from imperfect labeling rules,corresponding to more realistic scenarios, thanks to the growing availability of such datasets. On these tasks,we evaluated multiple surveyed detection methods on different criteria, most notably in the case of the fullyautomated weakly-supervised pipeline described in section 2.6. Overall, the purpose of this benchmark is not to provide a definitive ranking between detection methods.As we surveyed a large spectrum of existing and adapted methods, we could not exhaustively fine-tune eachmethod individually, so it is likely that some methods were not evaluated at their best capacity. Rather, thisbenchmark serves to highlight a few recommendations for practitioners, as it provides data for a meta-analysison a large number of actual datasets.",
  "We now outline the features of the benchmark. A more detailed presentation with all specifics is includedin appendix B": "Tabular dataWe choose to put the emphasis of our benchmark on tabular tasks. Arguably (Grinsztajnet al., 2022), these tasks do not benefit from the representation learning properties of deep models, thus ourevaluation only relies on the regularization properties of the machine learning models used instead of howgood they are at learning useful representations. Tabular data tasks also often include ambiguous mixingregions, where the true underlying concept is a mix of several classes (Seedat et al., 2022). For illustration,we also include an experiment on an image dataset in appendix D.2. DatasetsWe use weakly supervised datasets from the Wrench benchmark (Zhang et al., 2021), supple-mented by other datasets from the literature (Rhling Cachay et al., 2021; Hedderich et al., 2020), for whichwe have access to weak labels from a set of automatic labeling rules as well as ground truth labels.Asummary of tasks statistics is available in table 2.",
  "Sources of noise in benchmarked datasetsWe experiment with the following setups :": "Artificial uniform noise: with probability 30%, an example is assigned a random label uniformlybetween existing classes, independently of its feature or true class.This creates a dataset withCompletely At Random (NCAR) noise. In practice, this form of noise is often used when experi-menting since it can easily be artificially introduced in existing noise free datasets. It is, however,very different (and simpler to deal with) from the actual structure of noise encountered in real-worlddatasets. Imperfect labeling rules: a set of automatic labeling rules are applied to every example and thenaggregated as a single label. These labeling rules are imperfect, rendering the assigned labels noisy.In this case, noisy examples are more frequent in regions that are not correctly covered by thelabeling rules or when several labeling rules disagree, thus the probability for examples to be mis-labeled depends on their features x. The structure of the noise is Not At Random (NNAR). Someexamples might not be covered by any labeling rule*. This more general form of noise is often morerepresentative of actual use cases, but also more difficult to tackle. *In these experiments, we chose to exclude examples that are not covered by any labeling rules from the training set.Alternatively, one could assign them a random label, but it would likely be a noisy one.",
  "%": "Detection methodsWe evaluate several detection methods surveyed in section 3, choosing differentdetectors with the right diversity of components: Some originate from the deep learning literature, usingprogressive ensemble: Variance of Gradients (VoG, Agarwal et al., 2022), Area Under the Margin (AUM,Pleiss et al., 2020), Forget Scores (Toneva et al., 2018), TracIn (Pruthi et al., 2020), Small Losses (Amiriet al., 2018; Jiang et al., 2018) and AGRA (Sedova et al., 2023), others are not specific to deep learning:Consensus Consistency (Jiang et al., 2021) and CleanLab (Northcutt et al., 2021a), finally some methodscome from the influence literature in linear models: Self-Influence (Koh & Liang, 2017) and Self-Representer(Yeh et al., 2018). Their respective position in our framework is described in table 1.",
  ". Predictive power of the trust scores to detect mislabeled examples. We use the Area under theReceiver Operator Curve (AUROC) as a ranking quality metric": "2. Representativeness of the filtered dataset. We use class-balance as a proxy of representativeness. Tomeasure class-balance in multi-class classification, we use the ratio of the prior of the minority classover the prior of the majority class. 3. Performance in a fully automated weakly supervised pipeline with no additional supervision. We usethe test loss of an estimator trained after filtering of the less trusted examples given by a methodstrust scores (detect + filter).",
  "Noisy: the validation set follows the same distribution as the training set. In particular, it containspotential noisy examples": "Noise free: the validation set only contains clean examples. An example use-case is when an addi-tional effort is made on labeling of the validation set: since it is typically smaller than the trainingset, it is not prohibitively costly to review these particular instances more carefully. Oracle: the test set is used as validation set. This answers the question \"how would my detectionmethod perform had I had access to an oracle that would give me perfect hyperparameters\". Even ifnot useful in practical applications, this hyperparameter selection method gives us some indicationson the behavior of detection methods.",
  "Gold: The whole training set is used, mislabeled examples are assigned their genuine label": "Since we compare detection methods across tasks of varying difficulties, we normalize by scaling the observedmetrics (i.e. the test loss) linearly between 100 and 200 so that the performance of the none baseline gets200 and the silver baseline gets 100. Machine learning modelsThere are 2 different machine learning models involved in benchmarkedpipelines: the base model used at the detection stage, and the final estimator of the pipeline ().For both stages, we experiment with 2 different machine learning models: a kernelized linear model (KLM)trained with stochastic gradient descent and a gradient boosting model (GBT). ReproducibilityBoth detectors and helpers to download the datasets used in the benchmark are avail-able in the open-sourced library described in section 4, available on the repository benchmark code spanning from feature pre-processing to detectorevaluation is available in a separate open-source repository with fixed seeds for random number generators.The entire benchmark resultsare also available in a public archive so that reproducingfigures and tables can be done without re-running the benchmark. We hope that all provided code and rawresults will help foster research in weakly supervised learning.",
  "Benchmark observations": "This large scale benchmark allows us to ask a series of questions and observe some trends that we nowhighlight. For completeness, additional experiments with different setups (different final estimator), differentnoise structures (NCAR instead of NNAR), and different hyperparameter selection strategy (using a cleanor noisy validation set) are deferred to appendix C. Overall performance of detection methodsWe start by evaluating detection methods in the detect +relabel pipeline (). The experiment consists in relabeling the 10% less trusted examples as pointedby mislabeled detection methods. We use random trust scores as a baseline so that every setup is giventhe same number of training examples, and the same budget of relabeling. On most datasets, we observean improvement in test loss compared to the random baseline, which confirms that mislabeled exampledetectors provide a useful signal. This also gives us a ranking between detectors on these particular tasks,where AGRA shows consistent performance compared to other methods. Overall performance of filtering pipelinesWe turn to detect + filter pipelines, and ask the questionwhether we can get an improvement in performance by using such a pipeline (which is fully automated anddoes not require additional human supervision) compared to just using a carefully regularized model onnoisy dataset. This is not trivial as machine learning methods are known to already embed some form ofrobustness to noisy examples: by playing with hyperparameters that reduce its capacity, we can tune anymachine learning method to focus on more salient features while trading off some flexibility to fit mislabeledexamples. Furthermore, there is a trade-off between a filtered training set with only trusted examples, orkeeping as many training examples as possible in order to have a bigger training dataset but at the costof including potential mislabeled examples. In our experiments on NNAR, we observe that models trainedon the subset of training examples that have genuine labels (the silver baseline) consistently get bettergeneralization performance than models trained using the whole training set (the none baseline) includingmislabeled examples (figure 7), even if there is less examples overall. More interestingly, we observe thatusing a pipeline detect + filter with hyperparameters tuned using a clean validation set allows to improveon generalization performance most of the time. For a few detectors, however, the detect + filter pipelinesdo not improve compared to the none baseline or the random baseline. We believe this could be improvedby spending more effort to tune individual detectors. Detect/none capacityIn figure 8, we compare the regularization hyperparameter given by the oracle inthe classifier of the none baseline to the same hyperparameter in the classifier of detect + filter pipelines.",
  "NNAR | 10% relabeling | Linear Classifier | HP tuned on noise free validation": ": Distribution (boxplot) of the normalized (base 100=training on correctly labeled examples only,base 200=training on all examples including mislabeled ones) test loss of relabeling 10% less trusted exampleswith varying detectors using a linear model as estimator on tasks (dots) corrupted by NNAR. Hyperparame-ters are tuned using a clean validation set. Detector names include the detection method as well as the basemodel. Respective colors assigned to each detector are consistent across figures in the rest of this section. We observe that most of the time, less regularization is needed in detect + filter pipelines, even if the trustedtraining set is smaller since untrusted examples have been removed. This is aligned with the intuition thatnoisy datasets require more robust machine learning models (i.e. with larger regularization). Clean or noisy validation setIn our survey of mislabeled detection methods, we found that the questionof the validation set was often overlooked: as for any machine learning application, the final performancecrucially depends on a set of hyperparameter, among which the threshold used to filter untrusted examplesis paramount. We found that choosing hyperparameters on a noisy validation set gave no improvementcompared to the none baseline (figure 9). Intuitively, this can be understood as the fact that since the noisytraining and noisy validation sets follow the same (noisy) distribution, from the perspective of the validationset, what is actually noise does not look like noise. In practice, the threshold for splitting the training setwas often chosen to be 0 (no filtering at all, see figure 10). This questions the practical utility of mislabeleddetection methods comparatively with the biquality setup (Nodet et al., 2021): in biquality learning, cleanexamples are used to actually learn the parameters of a model and simultaneously provide weak supervisionon other (by default untrusted) examples whereas here, they are just used to choose hyperparameters, losingsome useful signal. Detection performance vs final performanceIn previous experiments, we evaluated mislabeled detec-tion methods by observing their performance in full pipelines (detect + filter or detect + relabel). This is themost relevant metric in practice since this is how detectors will be used in most cases. In this experiment, weinstead evaluate detection methods by measuring the predictive power of trust scores to distinguish betweengenuine and mislabeled examples. We expect detectors that rank examples correctly would also lead to goodclassifiers trained on their most trusted examples. However, in figure 11, only a mild correlation is foundbetween the two quantities, which suggests that good detectors possess other intrinsic qualities than their",
  "NNAR | Filtering | Linear Classifier | HP tuned on noise free validation": ": Distribution (boxplot) of the normalized (base 100=training on correctly labeled examples only,base 200=training on all examples including mislabeled ones) test loss of detect + filter pipelines with varyingdetectors with linear final estimator on tasks (dots) corrupted by NNAR. Hyperparameters are tuned usinga clean validation set. Detector names include the detection method as well as the base model. : For each detector/dataset pair (a circle), we com-pare the oracle regularization (2 regularization in a linearmodel) chosen when using the whole corrupted training seton the x-axis, to the oracle regularization chosen in detect+ filter pipeline on the y-axis. Most of the time, pipelinesobtained a smaller regularization (dots are below the y = xline).",
  "ranking capacity, such as their capacities to select prototype examples or balance datasets that are otherwiseclass-imbalanced": "Weak is more difficult than noiseWe compare two sets of experiments on the same datasets: examplescorrupted using artificial uniform noise (NCAR) and imperfect labeling rules (NNAR) in figure 12. Perhapsunsurprisingly, we observe that detect + filter pipelines perform worse on NNAR corruption than NCARcorruption. This is expected as NNAR is notoriously more difficult (Frnay & Verleysen, 2013): indeed,the patterns in the noise are often indistinguishable from the patterns in the non-corrupted data from the",
  "clean valid.noisy valid": ": For each detector/dataset pair (a cross or a cir-cle), we compare the classifier obtained by a detect + filterpipeline (y-axis) to the classifier trained on all examples,including noisy examples as a baseline (x-axis). Hyperpa-rameters are tuned either on a clean validation set (crosses)or on a noisy validation set (circles) for both the baselineand the pipeline. We observe a trend where detect + fil-ter pipelines tuned on the noisy validation set give worseperformance than the baseline (circles are above the y = xline), whereas pipelines tuned on the clean validation setgive better performance than the baseline (crosses are be-low the y = x line). : For each detector/dataset pair (a blue circle), wecompare the split quantile obtained by cross-validation on anoisy validation set (y-axis) to the oracle (best) split quan-tile (x-axis). Choosing the split threshold using a noisy val-idation set consistently underestimates the optimal thresh-old. In fact, the 0 threshold (no filtering at all) is chosenmost of the time.",
  "Tuned using noisy valid. set": "perspective of the learning algorithm in the absence of further hypotheses. More noteworthy, our experimentsshow no clear correlation between performance on NNAR and performance on NCAR. As a corollary, thisquestions the choice of testing mislabeled detection methods on NCAR-corrupted datasets: even if it is oftenmore convenient to experiment on prevalent benchmarks and artificially introduce uniform label corruption,it might not translate to actual use cases with more intricate NNAR corruption. Adapting deep learning methods to classical machine learning algorithmsA contribution of thisbenchmark is also to evaluate mislabeled detection methods that were initially designed to work with deeplearning models and that we adapted to work generically with any model that is learned sequentially (moredetails in appendix A). This is the case for AUM, Forget Scores, VoSG and TracIn. We evaluate them bothwith the gradient boosting algorithm and a linear model learned by stochastic gradient descent (SGD), whichour library allows to treat as progressive learning natively. Empirically, we report mixed results: whereassome methods did not produce any significant improvement in test loss, VoSG with a linear model is amongthe best-performing methods. This demonstrates the feasibility of such an approach, which fosters futurework on the similarities and differences of training dynamics between deep learning algorithms and othermachine learning algorithms. Choice of base model and additive robustnessWe hypothesize that in detect + filter pipelines,using different machine learning models as base model in the detection stage and as the final classifier couldimprove the final performance. The rationale is that different machine learning models include some form",
  "normalized test log loss": ": For each detector/dataset pair (a circle), we com-pare the normalized (none=200, silver=100) test loss of de-tect + filter pipelines on the x-axis, to the ranking quality ofthe trust scores on the y-axis. Detectors with better rankingtends to produce filtered dataset that allows the training ofbetter classifiers (the black line is a robust linear regression). : For each detector/dataset pair (a circle), we com-pare the task with NNAR corrupted labels (x-axis) to thesame training examples but corrupted using NCAR labels(y-axis). This confirms that NNAR tasks are more difficultthan NCAR most of the time (circles are above the y = xline). This also shows that there is no clear correlation be-tween NCAR performance and NNAR performance.",
  "(NNAR dataset)": "of robustness to different examples: we could benefit from the combined robustness by using 2 differentmethods. In figure 13, we compare the performance of detect + filter pipelines where both stages use eithera linear model (KLM) or a gradient boosted tree (GBT) model. We expect to see two point clouds pulledaway from the y = x line, where KLM + GBT and GBT + KLM would perform better than KLM + KLMor GBT + GBT. We actually do not see such a pattern, which invalidates the hypothesis, at least in thecurrent setup. Overall, it is clear from this figure that KLM is the best choice as a base model in theseexperiments. Representativeness of filtered dataBeing able to accurately sort out mislabeled examples from adataset is a fundamental property that detectors should possess, which they do (). However, theirranking capacity does not explain by itself the performance of a model trained on a filtered dataset. Wethink that the representativity, in addition to the quality of the filtered dataset, matters. We experimentwith a proxy of representativity, the class balance. We expect that the class balance of the filtered datasetto be closer to the test one than the noisy one. shows that most of the time, detectors tendto favor examples from the majority class, or in other words, filter out more aggressively examples fromthe minority class.We propose two potential reasons for this bias.Firstly, detectors may struggle todistinguish between mislabeled instances and those of the minority class, as the latter are inherently scarcerand more challenging to learn from. Secondly, the value among examples might not be evenly distributed.We speculate that failing to detect a mislabeled instance from the minority class could be more detrimentalto the downstream taskthan failing to detect one from the majority class.",
  "KLM estimatorGBT estimator": ": For each detector/dataset pair (a blue or orangecircle), we compare (lower is better) the best classifier ob-tained when using a detector with a GBT model on thex-axis to the best classifier obtained when using a detectorwith a KLM model on the y-axis. KLM detectors (orangecircles) seem to produce better performance most of thetime (circles are below the y = x line), and no clear patternemerges as to whether mixing models show a trend (blueand orange circles do not show different patterns). : For each detector/dataset pair (a circle), we com-pare the class balance of the original training dataset on thex-axis to the class balance of the filtered train dataset on they-axis. Orange circles correspond to datasets where train-ing is less balanced than test, and blue circles correspond todatasets where training is more balanced than test. Eventhough the ratio of above and below circles varies by color,detectors tend to introduce more class imbalance than orig-inally found in the training dataset (circles are below they = x line).",
  "filtered class balance": "Filtering by classSo far, we have studied detect + filter pipelines where the filtering step is performedregardless of the (potentially noisy) observed class of the example. As seen in the previous experiment,this can change the distribution of the class in the filtered dataset. Indeed, in class-imbalanced dataset,examples in the minority class often end up being less trusted than examples of the majority class. We thusexperiment with the alternative strategy of filtering example class-by-class, where e.g. the top 50% mosttrusted examples of each class are kept for training. This ensures that the distribution between classes in thefiltered dataset does not depart too much from that of the original (noisy) dataset, thus avoiding a situationwhere a minority class fully disappears from the training dataset. In .2, we observe no such trend:for most tasks it does not make much difference, for some other tasks a tiny difference in performance isobserved, either in favor of filtering by class, or in favor of filtering all classes at once. Detection performance vs base model performanceSimilar to the fact that the inductive bias ofmachine learning algorithms is paramount to their classification performance on unobserved examples, westudy how much this inductive bias relates to their performance at detecting mislabeled examples. shows that on datasets where the base model gets its better classification performance (measured as thetest loss of the same model trained on the whole noisy training set), it is also better at detecting mislabeledexamples. The trend is consistent across detectors.",
  "(filtering byclass)": ": For each detector/dataset pair (a circle), we com-pare (lower is better) the best classifier obtained when filter-ing examples class-by-class on the x-axis to the best classifierobtained when filtering all classes at once on the y-axis. Noclear picture emerges: it is sometimes better to filter class-by-class, sometimes better to filter all classes at once, andsometimes it does not make much difference. : For each detector across all datasets, we comparethe performance at detecting mislabeled examples on they-axis (AUROC for the task of detecting mislabeled exam-ples, higher is better) to the performance of the underlyingbase model (measured using the test loss, lower is better)when trained using the whole training set including misla-beled examples.For each detector, we also plot a linearregression across datasets. We observe a trend where goodclassification performance correlates with good performanceat detecting mislabeled examples",
  "Lessons learned": "Our benchmark offers a critical view on the approach of filtering in the context of machine learning withmislabeled examples when working with labeling functions or uniform noise: a fundamental flaw of such amethodology lies in the fact that it requires access to a clean validation set in order to select hyperparameters(the most important one being the threshold for splitting into trusted and untrusted examples). With this inmind, it seems like a waste of resources to use this clean data only for hyperparameter selection, rather thanusing it directly as a training set, or in a biquality learning setup. Filtering also imposes a hard thresholdbetween trusted and untrusted examples, whereas in some cases, examples that are on the verge of thesplitting threshold could also carry out some useful learning signal. However, the results show some interesting trends that could inspire future research.In particular, itshows that some of the methods developed for deep learning algorithms (more specifically VoSG and AUM)also show promising results with other classical machine learning algorithms (here a linear model and agradient boosting machine). On the contrary, Forget Scores do not seem to work very well with our currentimplementation. Since we could not afford to spend too much effort on every method, it could simply meanthat we did not find appropriate hyperparameters, or that the training dynamics of deep learning on whichForget Scores rely are different from the training dynamics of GBTs and linear models. Our experiments show encouraging results for applying model-probing methods to text and tabular datasets.For a small additional implementation cost, computing trust scores provides useful information about which",
  "Other related works": "This paper is part of a series of surveys in the weakly supervised learning literature, specifically on learningwith noisy labels (Frnay & Verleysen, 2013; Han et al., 2020; Song et al., 2022). However, it sets itself apartfrom other surveys by focusing on the task of identifying mislabeled examples instead of studying morebroadly the literature of learning algorithms robust to noisy labels. It also provides a more modern view onthe mislabeled example detection literature (Guan & Yuan, 2013) by proposing an encompassing frameworkclosing the gap between approaches that were specifically developed for deep networks and classical machinelearning algorithms. Identifying mislabeled examples is a topic also found in the data cleaning literature (Ilyas & Chu, 2019) andhas recently been successfully applied to the growing text datasets used to train or fine-tune large languagemodels (e.g. Zhu et al., 2024). Data cleaning surveys (Ct et al., 2023) also have a broader scope than theone studied in this survey and are more comparable to the detect + filter pipeline but extended to otherforms of data corruption such as feature noise, missing data, and outliers detection. Furthermore, outlier detection is an important field related to mislabeled examples detection. These methodsare designed to identify outliers in the sense P(X), whereas mislabeled detection methods seek to find outliersin the sense P(Y |X). Outlier detection approaches have been applied to split non-scalar trust scores, forexample, when using the output of multiple detectors (Lu et al., 2023) or multiple probes where no apparentaggregation exists. Another use of outlier detection is to find outliers in P(X|Y ) instead of P(Y |X) bytraining one outlier detection algorithm per class, assuming that outliers for a given class are mislabeledexamples (Rebbapragada & Brodley, 2007). We did not include these methods in the survey, as they wereout of scope. Finally, we omit a series of methods that jointly optimize the two steps of the detect + handle pipeline.Most notably, these approaches work iteratively, akin to the expectation-maximization algorithm, where thedetect and handle steps are optimized alternatively to minimize a global objective, usually getting the bestpossible classifier out of the handle step (Tanaka et al., 2018; Zeng et al., 2022). Contrary to the iterativerefinement from section 3.3.1 where proper trust scores can be explicitly retrieved at every iteration, jointmethods use implicit trust scores. As they only serve the role to guide the optimization procedure, they lackintrinsic significance, defeating our primary goal of mislabeled example detection.",
  "Conclusion": "The tremendous size of training datasets in modern tasks advocates for cheaper labeling strategies (e.g.crowdsourced annotation or automatic labeling rules), at the cost of some degree of labeling error. Methodsfor detecting mislabeled examples offer the promise of being able to diagnose training datasets and review ex-amples in a second step, either automatically (using filtering, semi-supervised learning or biquality learning)or by manual relabeling. In this paper, we take a fresh look at past and recent detection methods, and showthat most of them can be understood using a framework consisting of 4 components and a few principles.Notably this includes recent methods that exploit the particular training dynamics of deep networks, whichwe extended to be classifier-agnostic (in particular, in the empirical evaluation, we experimented with linearmodels and gradient boosted trees). We proposed an implementation of this framework, that follows the scikit-learn API which is familiar toevery machine learning practitioner. Our implementation focuses on the core mechanics of the framework,which then allows the implementation of the existing methods in the literature by only passing specific",
  "Perspectives": "The framework presented in section 3, and the implementation that we distribute, suggest that there isroom to improve the detection of mislabeled examples by experimenting with combinations of base model,probe, ensemble strategy, and aggregation methods that have not yet been explored.We encourage asystematic comparison of probes by looking specifically at which examples they score differently. Dependingon the context, it may be interesting to use a mix of probes in order to improve the robustness of detectionmethods. More generally, the idea of using trained machine learning models in order to diagnose datasets ofexamples extends to other subfields of machine learning, such as active learning, example-based explainabilitymethods, or conformal prediction. This urges for cross-fertilization between communities for a toolbox ofmethods that extend machine learning algorithms in order to obtain not only a single prediction (e.g. classor a real value) but also additional information about that prediction. Finally, we advocate for a more fine-grained score of each instance than just being correctly or mislabeled.As we discussed briefly in section 2.4, training examples can be categorized depending on whether theybelong to a region of the input space with low or high aleatoric or epistemic uncertainty, and recent worksaim at capturing this distinction (e.g. Javanmardi et al. (2024) using conformal prediction). Moreover, someexamples might be more useful or harmful, either because they belong to a minority class or an underrep-resented pattern in an otherwise class-balanced dataset, or because they ward against some undesired biasof the training data. In some cases, these are the examples that are the most important ones, for instance,in fairness sensitive tasks. A more relevant metric would then be to use a measure of how useful is anygiven example to the prediction of a learned classifier on test examples of this minority group, such as theDataShapley value (Ghorbani & Zou, 2019) using a custom utility function.",
  "Chirag Agarwal, Daniel Dsouza, and Sara Hooker.Estimating Example Difficulty Using Variance ofGradients. pp. 1036810378, 2022. URL": "Hadi Amiri, Timothy Miller, and Guergana Savova. Spotting Spurious Data with Neural Networks. InProceedings of the 2018 Conference of the North American Chapter of the Association for Computa-tional Linguistics: Human Language Technologies, Volume 1 (Long Papers), pp. 20062016, New Or-leans, Louisiana, 2018. Association for Computational Linguistics.doi: 10.18653/v1/N18-1182.URL Devansh Arpit, Stanisaw Jastrzbski, Nicolas Ballas, David Krueger, Emmanuel Bengio, Maxinder S Kan-wal, Tegan Maharaj, Asja Fischer, Aaron Courville, Yoshua Bengio, et al. A closer look at memorizationin deep networks. In International conference on machine learning, pp. 233242. PMLR, 2017.",
  "Juhan Bae, Nathan Ng, Alston Lo, Marzyeh Ghassemi, and Roger Grosse.If Influence Functions arethe Answer, Then What is the Question?, September 2022. URL [cs, stat]": "Dara Bahri, Heinrich Jiang, and Maya Gupta. Deep k-NN for noisy labels. In Hal Daum III and Aarti Singh(eds.), Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedingsof Machine Learning Research, pp. 540550. PMLR, 1318 Jul 2020. URL Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain,Stanislav Fort, Deep Ganguli, Tom Henighan, et al.Training a helpful and harmless assistant withreinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022.",
  "D.S. Broomhead and D. Lowe.Multivariable functional interpolation and adaptive networks.ComplexSystems, 2:321355, 1988": "Lars Buitinck, Gilles Louppe, Mathieu Blondel, Fabian Pedregosa, Andreas Mueller, Olivier Grisel, VladNiculae, Peter Prettenhofer, Alexandre Gramfort, Jaques Grobler, Robert Layton, Jake VanderPlas, Ar-naud Joly, Brian Holt, and Gal Varoquaux. API design for machine learning software: experiences fromthe scikit-learn project. In ECML PKDD Workshop: Languages for Data Mining and Machine Learning,pp. 108122, 2013. Haw-Shiuan Chang, Erik Learned-Miller, and Andrew McCallum. Active bias: Training more accurate neuralnetworks by emphasizing high variance samples. Advances in Neural Information Processing Systems, 30,2017. Bowen Chen, Yun Sing Koh, and Ben Halstead. Measuring Difficulty of Learning Using Ensemble Methods. InLaurence A. F. Park, Heitor Murilo Gomes, Maryam Doborjeh, Yee Ling Boo, Yun Sing Koh, YanchangZhao, Graham Williams, and Simeon Simoff (eds.), Data Mining, Communications in Computer andInformation Science, pp. 2842, Singapore, 2022. Springer Nature. ISBN 978-981-19874-6-5. doi: 10.1007/978-981-19-8746-5_3. Pengfei Chen, Ben Ben Liao, Guangyong Chen, and Shengyu Zhang. Understanding and utilizing deepneural networks trained with noisy labels. In International Conference on Machine Learning, pp. 10621070. PMLR, 2019.",
  "Hao Cheng, Zhaowei Zhu, Xingyu Li, Yifei Gong, Xing Sun, and Yang Liu. Learning with instance-dependentlabel noise: A sample sieve approach. arXiv preprint arXiv:2010.02347, 2020": "R. Dennis Cook. Detection of Influential Observation in Linear Regression. Technometrics, 19(1):1518,1977. ISSN 0040-1706. doi: 10.2307/1268249. URL Publisher:[Taylor & Francis, Ltd., American Statistical Association, American Society for Quality]. Filipe R Cordeiro, Ragav Sachdeva, Vasileios Belagiannis, Ian Reid, and Gustavo Carneiro. Longremix:Robust learning with high confidence samples in a noisy label environment. Pattern Recognition, 133:109013, 2023.",
  "Benot Frnay and Michel Verleysen. Classification in the presence of label noise: a survey. IEEE transactionson neural networks and learning systems, 25(5):845869, 2013": "Dragan Gamberger, Nada Lavrac, and Saso Dzeroski. Noise detection and elimination in data preprocessing:Experiments in medical domains. Applied Artificial Intelligence, 14(2):205223, February 2000. ISSN 0883-9514. doi: 10.1080/088395100117124. URL Publisher:Taylor & Francis _eprint: Thomas George, Guillaume Lajoie, and Aristide Baratin. Lazy vs hasty: linearization in deep networksimpacts learning schedule based on example difficulty. Transactions on Machine Learning Research, 2022.",
  "Lo Grinsztajn, Edouard Oyallon, and Gal Varoquaux. Why do tree-based models still outperform deeplearning on typical tabular data? Advances in Neural Information Processing Systems, 35:507520, 2022": "Donghai Guan and Weiwei Yuan. A Survey of mislabeled training data detection techniques for patternclassification. IETE Technical Review, 30(6):524530, November 2013. ISSN 0256-4602. doi: 10.4103/0256-4602.125689. URL Pub-lisher: Taylor & Francis _eprint: Donghai Guan, Weiwei Yuan, Young-Koo Lee, and Sungyoung Lee. Identifying mislabeled training datawith the aid of unlabeled data. Applied Intelligence, 35(3):345358, December 2011. ISSN 1573-7497. doi:10.1007/s10489-010-0225-4. URL",
  "Frank R Hampel. The influence curve and its role in robust estimation. Journal of the american statisticalassociation, 69(346):383393, 1974": "Bo Han, Quanming Yao, Tongliang Liu, Gang Niu, Ivor W Tsang, James T Kwok, and Masashi Sugiyama. Asurvey of label-noise representation learning: Past, present and future. arXiv preprint arXiv:2011.04406,2020. Michael A. Hedderich, David Adelani, Dawei Zhu, Jesujoba Alabi, Udia Markus, and Dietrich Klakow.Transfer learning and distant supervision for multilingual transformer models: A study on African lan-guages.In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu (eds.), Proceedings of the 2020Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 25802591, Online,November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.204. URL",
  "Alireza Javanmardi, David Stutz, and Eyke Hllermeier. Conformalized credal set predictors. arXiv preprintarXiv:2402.10723, 2024": "Lu Jiang, Zhengyuan Zhou, Thomas Leung, Li-Jia Li, and Li Fei-Fei. Mentornet: Learning data-drivencurriculum for very deep neural networks on corrupted labels. In International conference on machinelearning, pp. 23042313. PMLR, 2018. Ziheng Jiang, Chiyuan Zhang, Kunal Talwar, and Michael C Mozer. Characterizing structural regularitiesof labeled data in overparameterized models.In International Conference on Machine Learning, pp.50345044. PMLR, 2021.",
  "Junnan Li, Richard Socher, and Steven C.H. Hoi. Dividemix: Learning with noisy labels as semi-supervisedlearning. In International Conference on Learning Representations, 2020": "Evan Z Liu, Behzad Haghgoo, Annie S Chen, Aditi Raghunathan, Pang Wei Koh, Shiori Sagawa, PercyLiang, and Chelsea Finn. Just train twice: Improving group robustness without training group information.In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on MachineLearning, volume 139 of Proceedings of Machine Learning Research, pp. 67816792. PMLR, 1824 Jul2021. URL",
  "Jie Lu, Anjin Liu, Fan Dong, Feng Gu, Joao Gama, and Guangquan Zhang. Learning under concept drift:A review. IEEE transactions on knowledge and data engineering, 31(12):23462363, 2018": "Yang Lu, Yiliang Zhang, Bo Han, Yiu ming Cheung, and Hanzi Wang. Label-noise learning with intrinsicallylong-tailed data. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV),October 2023. Ulrike von Luxburg and Bernhard Schlkopf. Statistical Learning Theory: Models, Concepts, and Results. InDov M. Gabbay, Stephan Hartmann, and John Woods (eds.), Handbook of the History of Logic, volume 10of Inductive Logic, pp. 651706. North-Holland, January 2011. doi: 10.1016/B978-0-444-52936-7.50016-1.URL Xingjun Ma, Yisen Wang, Michael E. Houle, Shuo Zhou, Sarah Erfani, Shutao Xia, Sudanthi Wijew-ickrema, and James Bailey.Dimensionality-Driven Learning with Noisy Labels.In Proceedings ofthe 35th International Conference on Machine Learning, pp. 33553364. PMLR, July 2018.URL ISSN: 2640-3498. Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts.Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting of the Associationfor Computational Linguistics: Human Language Technologies, pp. 142150, Portland, Oregon, USA, June2011. Association for Computational Linguistics. URL",
  "Curtis Northcutt, Lu Jiang, and Isaac Chuang. Confident learning: Estimating uncertainty in dataset labels.Journal of Artificial Intelligence Research, 70:13731411, 2021a": "Curtis G. Northcutt, Anish Athalye, and Jonas Mueller. Pervasive label errors in test sets destabilize machinelearning benchmarks. In Proceedings of the 35th Conference on Neural Information Processing SystemsTrack on Datasets and Benchmarks, December 2021b. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang,Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions withhuman feedback. Advances in Neural Information Processing Systems, 35:2773027744, 2022. Giorgio Patrini, Alessandro Rozza, Aditya Krishna Menon, Richard Nock, and Lizhen Qu. Making deepneural networks robust to label noise: A loss correction approach. In Proceedings of the IEEE conferenceon computer vision and pattern recognition, pp. 19441952, 2017. Mansheej Paul, Surya Ganguli, and Gintare Karolina Dziugaite. Deep learning on a data diet: Findingimportant examples early in training. Advances in neural information processing systems, 34:2059620607,2021. Fabian Pedregosa, Gal Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel,Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, et al. Scikit-learn: Machine learningin python. the Journal of machine Learning research, 12:28252830, 2011.",
  "Alexander J Ratner, Christopher M De Sa, Sen Wu, Daniel Selsam, and Christopher R. Data programming:Creating large training sets, quickly. Advances in neural information processing systems, 29, 2016": "Umaa Rebbapragada and Carla E. Brodley. Class Noise Mitigation Through Instance Weighting. In Joost N.Kok, Jacek Koronacki, Raomon Lopez De Mantaras, Stan Matwin, Dunja Mladeni, and Andrzej Skowron(eds.), Machine Learning: ECML 2007, volume 4701, pp. 708715. Springer Berlin Heidelberg, Berlin,Heidelberg, 2007. ISBN 978-3-540-74957-8 978-3-540-74958-5. doi: 10.1007/978-3-540-74958-5_71. URL ISSN: 0302-9743, 1611-3349 Series Title:Lecture Notes in Computer Science.",
  "Clayton Scott. A rate of convergence for mixture proportion estimation, with application to learning fromnoisy labels. In Artificial Intelligence and Statistics, pp. 838846. PMLR, 2015": "Anastasiia Sedova, Lena Zellinger, and Benjamin Roth. Learning with noisy labels by adaptive gradient-based outlier removal. In Danai Koutra, Claudia Plant, Manuel Gomez Rodriguez, Elena Baralis, andFrancesco Bonchi (eds.), Machine Learning and Knowledge Discovery in Databases: Research Track, pp.237253, Cham, 2023. Springer Nature Switzerland. ISBN 978-3-031-43412-9. Nabeel Seedat, Jonathan Crabb, Ioana Bica, and Mihaela van der Schaar. Data-IQ: Characterizing sub-groups with heterogeneous outcomes in tabular data. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave,K. Cho, and A. Oh (eds.), Advances in Neural Information Processing Systems, volume 35, pp. 2366023674. Curran Associates, Inc., 2022.URL Nicola Segata, Enrico Blanzieri, Sarah Jane Delany, and Pdraig Cunningham. Noise reduction for instance-based learning with a local maximal margin approach. Journal of Intelligent Information Systems, 35:301331, 2010. Burr Settles. From theories to queries: Active learning in practice. In Active learning and experimental designworkshop in conjunction with AISTATS 2010, pp. 118. JMLR Workshop and Conference Proceedings,2011.",
  "Daiki Tanaka, Daiki Ikami, Toshihiko Yamasaki, and Kiyoharu Aizawa. Joint Optimization Framework forLearning with Noisy Labels, March 2018. URL arXiv:1803.11364[cs, stat]": "Jaree Thongkam, Guandong Xu, Yanchun Zhang, and Fuchun Huang. Support Vector Machine for OutlierDetection in Breast Cancer Survivability Prediction. In Yoshiharu Ishikawa, Jing He, Guandong Xu, YongShi, Guangyan Huang, Chaoyi Pang, Qing Zhang, and Guoren Wang (eds.), Advanced Web and NetworkTechnologies, and Applications, Lecture Notes in Computer Science, pp. 99109, Berlin, Heidelberg, 2008.Springer. ISBN 978-3-540-89376-9. doi: 10.1007/978-3-540-89376-9_10.",
  "A.1Variance of gradients": "We found a small inconsistency in the experiments in Agarwal et al. (2022): the toy experiment involves aMLP with no non-linearities, which computes a linear mapping of the input vector. In this case, the gradientof the logit w.r.t. the input vector only depends on the equivalent weight matrix W = W1W2 . . . Wl, and noton the input. Put differently, the proposed VoG statistics is the same for every example, which means thatit cannot be used to rank examples. We thus think that there is an inconsistency in the results presentedin the toy experiment section. This is in contrast to the larger scale experiment with ResNet architectures,where the mapping from the input space to the logits is non-linear since it involves non-linearities (hereReLU activations). When working with linear models in our experiments, we instead implemented a slightly different versionwhere we differentiate the probability given by the softmax, and not the logit. This mapping from inputspace to softmax output is non-linear, and it depends on the example contrarily to the mapping from inputspace to logit. We found the resulting statistics (i.e. the variance of gradients of the probability given bythe softmax) to be useful at detecting mislabeled examples.",
  "A.2Finite differences": "During our survey, we reviewed some detectors which where fundamentally tailored to work on differentiablemachine learning models. For example, the Variance Of Gradients detector probes the model by looking atthe derivative of the pre-softmax layer of a neural network with respect to the input features. We proposedin the library to use the finite difference approach for non-differentiable models such as decision trees. On thesame 2D toy-dataset used in the original paper (Agarwal et al., 2022), the finite difference method showedto reasonably approximate the exact method for a kernelized linear model: Feature 1 Feature 2 Toy Dataset trained decision boundary 0.00080.00060.00040.0002 0.0000 VoG scores 0.0 0.5 1.0 1.5 2.0 2.5 3.0 Distances to Hyperplane Distance vs VoG (Finite Diff) score 0.00080.00060.00040.0002 0.0000 VoG scores 0.0 0.5 1.0 1.5 2.0 2.5 3.0 Distances to Hyperplane Distance vs VoG (Linear) score",
  "B.1Generating noisy label from labeling rules": "The datasets used in the benchmark provides, on top of ground truth labels for each examples, a set of weaklabels given by labeling rules. Given an example, a labeling rule output a weak label, that may be a classlabel if the rule matched, or nothing. These weak labels are aggregated thanks to a majority vote. Howevertwo edge cases may arise, the first if the votes are tied between two classes, the second when no labelingrules matched for an example. In the first case we pick a class completely at random among tied winners,for the latter we chose to drop these examples from the training dataset.",
  "Two families of models are used through the benchmark, kernelized linear models (KLM) and gradientboosted trees (GBT), two of the most popular approaches for machine learning on tabular data": "To have scalable KLMs, we chose to use the Random Kitchen Sinks approach (Rahimi & Recht, 2007) toapproximate kernel computations on large-scale datasets. We used the Gaussian RBF kernel (Broomhead& Lowe, 1988) for tabular datasets and linear (or no) kernel for text datasets. Then, the linear model istrained by minimizing the log-loss on training samples thanks to Stochastic Gradient Descent. All KLMscomponents use scikit-learns implementation.",
  "We summarize the search space used in random search of the main hyperparameters of each family of modelsdescribed in section B.3 in the following table:": "On top of that models from both families are trained for a maximum of a thousand iteration (number ofepochs for KLM, number of trees for GBT), ensuring convergence in most cases. Yet, models can be earlystopped if their log-loss on an holdout dataset does not decrease for more than 5 iterations. This specificholdout dataset corresponds to 10% of the training data.",
  "NNAR | 10% relabeling | GBT Classifier | HP tuned on noise free validation": ": Distribution (boxplot) of the normalized (base 100=training on correctly labeled examples only,base 200=training on all examples including mislabeled ones) test loss of relabeling 10% less trusted exampleswith varying detectors using a GBT model as estimator on tasks (dots) corrupted by NNAR. Hyperparam-eters are tuned using a clean validation set. Same as figure 6 but with a GBT estimator.",
  "NCAR | 10% relabeling | Linear Classifier | HP tuned on noise free validation": ": Distribution (boxplot) of the normalized (base 100=training on correctly labeled examples only,base 200=training on all examples including mislabeled ones) test loss of relabeling 10% less trusted exampleswith varying detectors using a linear model as estimator on tasks (dots) corrupted by NCAR. Hyperparam-eters are tuned using a clean validation set. Same as figure 6 but with a NCAR noise.",
  "C.3Filtering threshold - noisy validation set vs oracle": ": For each detector/dataset pair (a blue circle), wecompare the split quantile obtained by cross-validation on anoisy validation set (y-axis) to the oracle (best) split quan-tile (x-axis). Choosing the split threshold using a noisy val-idation set consistently underestimates the optimal thresh-old. In fact, the 0 threshold (no filtering at all) is chosenmost of the time. (same as figure 10 but using a GBT finalestimator)",
  "C.4Regularization of none vs pipelines": ": For each detector/dataset pair, we compare theoracle regularization (2 regularization in a GBT model)chosen in detect + filter pipeline, to the oracle regulariza-tion chosen when using the whole corrupted training set.Most of the time, detect + filter pipelines obtained a smallerregularization, meaning that filtering noisy examples allowsfor less regularized classifiers. (same as figure 8 using a dif-ferent final estimator)",
  "D.1Results on a regression task on tabular data": "We experiment on the California Housing regression task, that consists in predicting the price in the housingmarket (a real number) using several features of the sold house. Here the same framework of section 3 canbe used provided that we use a regression probe. We use a model-probing detector with a random forestregressor as base model, bootstrapping as the ensemble strategy, the 2 loss as the probe and the mean acrossout-of-bag examples as the aggregation method. As illustrated by the bottom-5 trusted examples circled inred in figure D.1, the detector correctly identifies some examples with suspicious labels.",
  "D.2Results on an image classification task with features from a pre-trained ResNet": "We use the 50.000 images of the CIFAR10 classification task. We extract features before the classificationlayer of a ResNet50 model, pre-trained on the ImageNet dataset. Using these features, we choose a Logis-ticRegression model from scikit-learn with default hyperparameters as our base model, and TracIn as ourdetector. In figure 25, for each class, we show the less trusted image (in red), as well as 4 other images of the sameclass. As expected, the most untrusted image often looks less representative of the observed class, or evenmislabeled (e.g. a windsurfer in class ship, a dog that looks like a cat, or a zoomed-in image of the frontof a truck whereas other images of trucks often display full trucks with their trailer). airplane automobile bird cat deer dog",
  "D.3Results on an image classification task with confusing classes": "We use 10.000 images from the Animal-10N classification task (Song et al., 2019). The setup is similarto the previous section D.2, where we extract features before the classification layer of a ResNet50 model,pre-trained on the ImageNet dataset.Using these features, we choose a LogisticRegression model fromscikit-learn with default hyperparameters as our base model, and TracIn as our detector. In figure 26, for each class, we show the less trusted image (in red), as well as 4 other images of the sameclass. Each row represents very similar classes (e.g. cats often look like lynxes, wolves like coyotes, etc).In this setup, less trusted images correspond to very unusual pictures, such as drawings of a cheetah anda wolf. In contrast, other images are real photographs, or a coyote seating in a bus whereas other coyoteimages have more usual backgrounds. cat lynx wolf coyote cheetah jaguar chimpanzee orangutan hamster guinea pig : On Animal-10N, for each class we display the less trusted training set image (red frame), comparedto 4 other representative images from the same class. Left and right classes correspond to similar-lookingclasses that are more likely to be confused."
}