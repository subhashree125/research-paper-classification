{
  "Abstract": "Federated Graph Learning (FGL) is tasked with training machine learning models, such asGraph Neural Networks (GNNs), for multiple clients, each with its own graph data. Existingmethods usually assume that each client has both node features and graph structure of itsgraph data. In real-world scenarios, however, there exist federated systems where only a partof the clients have such data while other clients (i.e. graphless clients) may only have nodefeatures. This naturally leads to a novel problem in FGL: how to jointly train a model overdistributed graph data with graphless clients? In this paper, we propose a novel frameworkFedGLS to tackle the problem in FGL with graphless clients.In FedGLS, we devise alocal graph learner on each graphless client which learns the local graph structure with thestructure knowledge transferred from other clients. To enable structure knowledge transfer,we design a GNN model and a feature encoder on each client. During local training, thefeature encoder retains the local graph structure knowledge together with the GNN modelvia knowledge distillation, and the structure knowledge is transferred among clients in globalupdate. Our extensive experiments demonstrate the superiority of the proposed FedGLSover five baselines.",
  "Introduction": "Recent years have witnessed a growing development of graph-based applications in a wide range of high-impact domains. As a powerful deep learning tool for graph-based applications, Graph Neural Networks(GNNs) exploit abundant information inherent in graphs (Wu et al., 2020) and show superior performancein different domains, such as node classification (Fu et al., 2023; He et al., 2022) and link prediction (Tan",
  "Company": ": An example of a healthcare system including four hospitals. In this example, Hospital A andHospital D have their local datasets of patients (node features) and co-staying information (links) amongthem. In the meantime, Hospital B and Hospital C only have their local datasets of patients (node features).The four hospitals aim to jointly train a model for predicting whether a patient is at high risk of contractinga contagious disease, orchestrated by a third-party company over their local datasets while the companycannot directly access their private datasets. et al., 2023a; Zhang & Chen, 2018). Traditionally, GNNs are trained over graph data stored on a singlemachine. In the real world, however, multiple data owners (i.e., clients) may have their own graph data andhope to jointly train GNNs over their graph data. The challenge in this scenario is that the graph data isoften not allowed to be collected from different places to one single server for training due to the emphasison data security and user privacy (Voigt & Von dem Bussche, 2017; Wang et al., 2024b). Considering agroup of hospitals, for instance, each of them has its local dataset of patients. These hospitals hope tocollaboratively train GNNs for patient classification tasks (e.g., predicting whether a patient is at high riskof contracting a contagious disease) while keeping their private patient data locally due to strict privacypolicies and commercial competition. To tackle the above challenge, Federated Learning (FL) (McMahan et al., 2017) enables collaborative trainingamong clients over their private graph data orchestrated by a central server. Typically, FL can be categorizedinto horizontal and vertical FL based on how data is distributed among clients (Yang et al., 2019). Thisstudy focuses on horizontal FL where distributed datasets share the same feature space. During each traininground, the selected clients receive the global model from the central server and perform local updates overtheir local graph data. The central server aggregates the updated local models from the clients and computesthe new global model for the next training round. Numerous studies have been proposed to improve FLperformance over graph data by reconstructing cross-client information (Zhang et al., 2021b;a), aligningoverlapping instances (Peng et al., 2021; Zhou et al., 2022), and mitigating data heterogeneity (Xie et al.,2021; Tan et al., 2023b; Fu et al., 2024). The above methods rely on a fundamental assumption that each client has graph structure information of itslocal graph data. In the real world, however, this assumption may not be feasible for all the clients. Instead,there may be a part of clients only having local node features, whereas other clients have both node featuresand edge information. To illustrate this scenario in practice, we provide a real-world example as follows.More practical examples can be found in Appendix A. Motivating example. Considering the aforementioned example of a healthcare system as shown in ,we may construct local graphs in each hospital by taking patient demographics as node features and co-stayingin a ward as edges. In the real world, however, some hospitals may not record the co-staying informationand cannot construct patient graphs. As a result, these hospitals are unable to directly train GNNs topredict whether a patient is at high risk of contracting a contagious disease in a federated manner. If weinstead train a machine learning model only based on patient features, the classification performance will",
  "be unsatisfactory because we disregard the important co-staying information between patients, which cansignificantly determine the risk of contracting a contagious disease for a patient": "The above scenario brings us a novel problem in the federated setting: how to jointly train a GNN model forthe classification task from isolated graphs distributed in multiple clients while some clients only have nodefeatures? In this paper, we name such clients as graphless clients. Since directly training GNNs is obviouslyinfeasible in this setting, collaboratively training non-GNN-based models such as multi-layer perceptrons(MLPs) and Support Vector Machines (SVMs) is a plausible solution to the above problem. However, anumber of experiments in prior works have demonstrated that the non-GNN-based models are typically lessaccurate than GNNs for the classification task (Franceschi et al., 2019; Zhang et al., 2022). Another intuitivemethod is to let a graphless client construct graph structure based on the similarity of the features (e.g.,using kNN (Gidaris & Komodakis, 2019)) and jointly train GNNs together with other clients. A disadvantageof this method is that the generated graphs are only dependent on node features and are not suitable fornode classification (Franceschi et al., 2019; Liu et al., 2022). To overcome the disadvantage, it is natural tolet the graphless clients produce graph structures with the structure knowledge of other clients. However,structure knowledge transfer and utilization in a federated manner are still challenging and unexplored. In this study, we propose a novel framework FedGLS to handle FGL with graphless clients. FedGLS aimsto solve two key challenges of utilizing structure knowledge in this scenario: 1) how to transfer structureknowledge among clients; and 2) how to utilize the transferred knowledge on graphless clients? In FedGLS,we first design two modules - a GNN model and a feature encoder on each client. In particular, we deploy athird module - a local graph learner on each graphless client. The GNN model learns node embeddings overthe local graph and the feature encoder approximates the output of the GNN model via knowledge distillation(Hinton et al., 2015). Therefore, the GNN model and the feature encoder together retain structure knowledgeon each client. The central server collects the local parameters of the two modules from the clients and getsthe global parameters. In this way, FedGLS transfers structure knowledge among clients. The local graphlearner utilizes the structure knowledge by maximizing the consistency between the output of the globalGNN model and feature encoder on each graphless client with a contrastive loss. We conduct extensiveexperiments over five datasets, and the results show that FedGLS outperforms other baselines.",
  "Problem Formulation. We propose a novel research problem of FGL with graphless clients andprovide the formal definition of the proposed problem": "Algorithm Design. We propose FedGLS to tackle the proposed problem. We devise a scheme fortransferring structure knowledge among clients in FedGLS by letting a feature encoder imitate thenode embeddings from a GNN model. The scheme enables a graph learner on each graphless clientto learn its local graph structure with the structure knowledge transferred from other clients. Experimental Evaluations. We conduct extensive experiments on real-world datasets, and theresults validate the superiority of our proposed FedGLS against five baselines. Our implementationof FedGLS is available in the supplementary materials.",
  "Federated Learning": "FL (McMahan et al., 2017) enables participants (i.e., clients) to jointly train a model under the coordinationof a central server without sharing their private data.One key problem that FL concerns is statisticalheterogeneity: the data across clients are likely to be non-IID (Li et al., 2020a; Wu et al., 2024a). Wheneach client updates its local model based on its local dataset, its local objective may be far from the globalobjective. Thus, the averaged global model is away from the global optima (Wang et al., 2024a; Wu et al.,2024b; Wang et al., 2023) and influences the convergence of FL. To overcome the performance degradationof FedAvg when data at each client are statistically heterogeneous (non-IID), a number of recent studieshave been proposed from different aspects. Typically, these studies can be categorized into single global",
  "Published in Transactions on Machine Learning Research (10/2024)": "model-based methods and personalized model-based methods. Single global model-based methods aim totrain a global model for all clients. For instance, FedProx (Li et al., 2020b) adds a proximal term to thelocal training loss to keep updated parameters close to the global model. SCAFFOLD (Karimireddy et al.,2020) customizes the gradient updates of personalized models to mitigate client drifts between local modelsand the global model. Moon (Li et al., 2021) uses a contrastive loss to increase the distance between thecurrent and previous local models. Personalized model-based methods instead enable each client to train apersonalized model to mitigate the impact of data heterogeneity. For example, pFedHN (Shamsian et al.,2021) trains a central hypernetwork to output a unique personal model for each client. FedProto (Tan et al.,2022) and FedProc (Mu et al., 2023) utilize the prototypes to regularize the local model training.",
  "Federated Graph Learning": "Following many well-designed FL methods for Euclidean data (e.g., images), a number of recent studieshave begun tackling challenges in FL on graph data (Fu et al., 2022) to achieve better performance ondownstream tasks. One specific problem in FL on graph data is missing neighboring information when eachclient only owns a part of the original graph. The recent proposed methods recover missing neighboringinformation by transmitting intermediate result (Zhang et al., 2021a) and generating missing neighbors(Zhang et al., 2021b). Another interesting issue in FL on graphs is aligning overlapping instances acrossclients. This issue happens in FL with heterogeneous graphs (Peng et al., 2021) and vertical FL on graphs(Zhou et al., 2022). In the meantime, some recent studies focus on the unique challenges caused by dataheterogeneity in FGL. For example, GCFL (Xie et al., 2021) enables clients with similar graph structureproperties to share model parameters. FedStar (Tan et al., 2023b) designs a structure encoder to sharestructure knowledge among clients for graph classification. FedLit (Xie et al., 2023) mitigates the impactof link-type heterogeneity underlying homogeneous graphs in FGL via an EM-based clustering algorithm.Different from the aforementioned problems where the clients own structured data (i.e., graphs), our workaims to deal with the setting where a part of the clients do not have structure information.",
  "Before formally presenting the formulation of the novel problem, we first introduce the concepts of GNNsand FGL": "Notations. We use bold uppercase letters (e.g., A) to represent matrices. For any matrix, e.g., Z, we usethe corresponding bold lowercase letters zi to denote its i-th row vector. We use letters in calligraphy font(e.g., V) to denote sets. |V| denotes the cardinality of set V. Graph Neural Networks. We use G = (V, E, X) to denote an attributed graph, where V = {v1, v2, , vn}is the set of n = |V| nodes, E is the edge set, and X Rnd is the node feature matrix. d is the number ofnode features. The edges describe the relations between nodes and can also be represented by an adjacencymatrix A Rnn. A GNN model f parameterized by = {z, c} learns the node embeddings Z Rnd",
  "based on X and AZ = f(X, A; z).(1)": "Here d is the embedding dimension, and z represents parameters of the encoder part in f to obtain thenode embedding Z. For the node classification task, we use Z to obtain the prediction Y = f(Z; c) Rnp where p is the number of classes, and c represents parameters of the predictor in f. Given the label setYL = {y1, y2, , yL} where yi denotes the label of node vi VL = {v1, v2, , vL}, the objective is tominimize the difference between Y and YL",
  "where (, ) is the cross-entropy loss": "Federated Graph Learning. In a federated system with K clients C = {c(1), c(2), , c(K)}, each clientc(k) C owns a private graph G(k) = (V(k), E(k), X(k)) and n(k) = |V(k)|. The goal of the clients is tocollaboratively train a GNN model f parameterized by orchestrated by a central server while keeping theprivate datasets locally. Specifically, the objective is to solve",
  "viV(k)L (yi, yi), and N = Kk=1 n(k) is the total number of nodes around all the": "clients. FedAvg (McMahan et al., 2017) is one of generic federated optimization methods, which can bedirectly applied to FL on graph data. Typically, during each training round, a client c(k) updates its localGNN parameters (k) over its private graph G(k) via SGD for a number of epochs with initialization of theparameters set to the global GNN parameters . At the end of the round, the server collects {(k)}Kk=1 fromclients and computes the new global GNN parameters by",
  "Based on the aforementioned challenges, we propose a novel problem setting in FGL. We provide a formaldefinition of the problem as follows": "Problem 1. Federated graph learning with graphless clients: Given a set of K clients C = {c(k)}Kk=1and 1 < M < K, each client c(k) C1 = {c(k)}Mk=1 owns the node features X(k) in its private graph G(k) with the complete structure information (e.g., A(k)) while each graphless client c(k) C2 = {c(k)}Kk=M+1 onlyowns its local node features X(k). The goal of the whole clients C is to collaboratively train a model for thenode classification task without sharing their graph data.",
  "Methodology": "In this section, we present the proposed framework FedGLS. The goal of FedGLS is to let graphless clientslearn local graph structures with the structure knowledge transferred from other clients. To achieve thisgoal, FedGLS solves the following two challenges: 1) how to transfer structure knowledge among clients; and2) how to utilize the transferred structure knowledge on graphless clients. To handle the first challenge, wedesign a GNN model and a feature encoder on each client. The feature encoder aims to retain structureknowledge together with the GNN model via knowledge distillation. To utilize the transferred structureknowledge, we design a graph learner on each graphless client. This module generates local graph structureand learns the structure knowledge in the GNN model and the feature encoder via a contrastive loss.",
  ": An overview of the proposed FedGLS": "GNN model. It approximates the output (i.e., node embeddings) of the GNN model using the knowledgelearned by the GNN model. On each graphless client, a graph learner generates local graph structure andlearns the structure knowledge via a contrastive loss. Finally, the well-trained graph learner produces localgraph structure and the GNN model learns more expressive node embeddings. Global Update. After local training, the central server gathers the local parameters of the GNN modeland the feature encoder from the clients. Then it computes the new global parameters following FedAvg andbroadcasts them to the clients for local training in the next round.",
  "S(k) = g(X(k); (k)) = (Gen(X(k); (k))),(5)": "where (k) denotes the parameters in g. Gen() denotes a graph generator which produces a matrix S(k) Rn(k)n(k) based on the node features. Typically, we instantiate the graph generator as an MLP encoder oran attentive encoder followed by a cosine similarity function. () is a non-parametric adjacency processor which conducts post-processing on S(k) and outputs S(k).Generally, it includes four main operations:sparsification, activation, symmetrization, and normalization (Liu et al., 2022). The adjacency processor ()ensures that S(k) is a normalized symmetric sparse adjacency matrix with non-negative values. For detaileddescriptions about designing graph generators and adjacency processors in the graph learner, readers canrefer to Appendix B. Then the GNN model f on each graphless client c(k) C2 produces node embeddings with the generatedadjacency matrix S(k) and the node feature matrix X(k) as the input. For each client c(k) C1, the client",
  "Optimizing (k). For a graphless client c(k) C2, its well-trained graph learner g with its parameters (k)": "is supposed to learn structure knowledge transferred from other clients. Typically, it produces the adjacencymatrix S(k) so that the node embeddings Z(k) produced by the GNN model based on S(k) and X(k) areconsistent with H(k) produced by the feature encoder. To achieve this, we optimize the local graph learnerby maximizing the agreement with a contrastive loss (e.g., NT-Xent (Chen et al., 2020a)). Specifically, weconsider the embedding pair z(k)iand h(k)iof node vi V(k) as a positive pair. In contrast, the embeddingz(k)iand the embedding of any other node vj V(k) (either in Z(k) or H(k)) compose a negative pair. Ourgoal is to decrease the distance between positive pairs and increase the distance between negative pairs.Concretely, we formalize it as the contrastive loss as follows:",
  "where CE(, ) denotes the cross-entropy loss": "Optimizing (k). The goal of the feature encoder is to produce H(k) without structure information whichis consistent with Z(k) and therefore enables the training of the graph learner. The key challenge for traininggraph learners is to enforce closeness between H(k) and Z(k). To tackle this issue, we resort to knowledgedistillation (Hinton et al., 2015). The intuition of knowledge distillation is to let a student model learnwith the knowledge (e.g., predictions) from a teacher model. Typically, the student model is able to producecomparable outputs with the teacher model. In FedGLS, we choose to distill knowledge from the GNN modelto the feature encoder. Then the feature encoder (e.g., an MLP) achieves comparable performance with aGNN via learning the knowledge transferred from the GNN only based on the features (Zhang et al., 2022).In FedGLS, the parameters (k) of a feature encoder on each client c(k) C are updated by approximatingthe knowledge (i.e., the node embeddings Z(k)) from its GNN model. Specifically, the feature encoder on",
  "Overall Algorithm": "The overall federated training algorithm of FedGLS is shown in Algorithm 1. During each round, the centralserver sends global and to the selected clients. For each client c(k) C2, the graph learner g first producesthe adjacency matrix S(k). The GNN model f takes node features X(k) and the generated adjacency matrixS(k) (for clients in C1, they use their original adjacency matrices A(k) instead) to get node representationsZ(k). Then the feature encoder h computes corresponding node embeddings H(k) only with node featuresX(k) as input. The parameters (k) in g are updated by minimizing the discrepancy between Z(k) and H(k) using Eq. (9). Afterward, each client c(k) C updates the parameters (k) of f via supervised learningusing Eq. (10) and updates the parameters (k) of h with the knowledge distilled from the GNN model assupervision information using Eq. (11). Finally, the central server collects the updated (k) and (k) fromthe clients to get the new global and using Eq. (12) for local training in the next round. We providecomplexity analysis Appendix C.",
  "Datasets": "We synthesize the distributed graph data based on five common real-world datasets, i.e., Cora (Sen et al.,2008), CiteSeer (Sen et al., 2008), PubMed (Sen et al., 2008), Flickr (Zeng et al., 2020), and ogbn-arxiv (Huet al., 2020). Following the data partition strategy in previous studies (Huang et al., 2023; Zhang et al.,2021b), we synthesize the distributed graph data by splitting each dataset into multiple communities viathe Louvain algorithm (Blondel et al., 2008); each community is regarded as an entire graph in a client. Wesummarize the statistics and basic information about the datasets in Appendix D. In our experiments, we randomly select half clients as graphless clients. Following the setting in (Zhanget al., 2021b), we randomly select nodes on each client and let 60% for training, 20% for validation, and theremaining for testing. We report the average accuracy for node classification over the clients for five randomrepetitions.",
  "Parameter Settings": "We implement a two-layer GCN and a two-layer MLP as the GNN model and the feature encoder in FedGLS,respectively. We apply the same model architectures to the models used in the baselines. In FedGLS, wechoose a two-layer attentive encoder as the graph generator in the graph learner. The hidden size of eachlayer in the two models is 16. For all the aforementioned models, we use the Adam (Kingma & Ba, 2015)optimizer. The learning rates and are set to 0.01 in the GNN model and the feature encoder and isset to 0.001 in the graph learner. The temperature in the contrastive loss is set to 0.2. The number oflocal epoch E is set to 5. The number of rounds is set to 100 for Cora and CiteSeer, 200 for PubMed, 300for Flickr, and 2,000 for ogbn-arxiv. All the clients are sampled during each round.",
  "Classification Performance": "We first evaluate the node classification accuracy of different approaches over the five datasets. We conductall the experiments five times and report the average accuracy with standard deviation in . As for thebaselines without preprocessing, Fed-GNNMLP outperforms Fed-MLP over all the datasets. It is becauseFed-GNNMLP utilizes structure information on the clients in C1. However, there is still a huge performancegap between Fed-GNNMLP and FedGLS since training MLPs based solely on node features on graphlessclients by Fed-GNNMLP cannot achieve comparable performance with FedGLS using structure information.Although FedProto is a personalized approach which enables each client to train a local model with globalprototypes as a constraint, it does not always perform better than Fed-MLP and Fed-GNNMLP (e.g., onPubMed and Flickr). Since graphless clients in FedProto use MLPs as the backbone model and other clientsuse GNNs, they may learn distinct prototypes with their different backbone models. As for the baselines withpreprocessing, Fed-GNNk fails to perform better than Local-GNNk on Cora and Flickr. In the meantime,we can observe significant performance degradation of Fed-GNNk compared with Fed-GNN. It indicates thatconstructing graph structures via kNN is not suitable for generating graphs in the real world. In the end, weobserve that FedGLS consistently achieves the highest classification accuracy. Compared with Fed-GNNk,",
  "Convergence Speed": "We then compare convergence speeds of FedGLS with the best baseline Fed-GNNk. We present the curvesof training loss and test accuracy during the training process on Cora and Flickr in . From theresults, we observe that the training loss of FedGLS decreases significantly faster than that of Fed-GNNkand the test accuracy of FedGLS reaches relatively high values with fewer rounds than Fed-GNNk on bothdatasets. According to the observation, we can conclude that FedGLS converges faster than Fed-GNNk.This is because the GNN model in FedGLS is trained over adaptive graphs whose adjacency matrices aregenerated by graph learners instead of simply produced by kNN in Fed-GNNk.The graph learners ongraphless clients learn more suitable graph structures for the node classification task by learning structureknowledge transferred from other clients.",
  "Graphless Client Ratios": "We also consider varying ratios of graphless clients in C. In this part, we conduct experiments on PubMedand Flickr with different graphless clients. shows the results of Fed-MLP, Fed-GNNk, and FedGLSon PubMed and Flickr. Note that the performance of Fed-MLP keeps consistent since it does not requirestructure information for training. From the table, we can observe that FedGLS consistently achieves betterutility compared with Fed-GNNk and Fed-MLP. In the meantime, FedGLS and Fed-GNNk show performancedegradation when there are more graphless clients because of less structure information in the system. Forinstance, Fed-GNNk performs worse than Fed-MLP on PubMed when |C1| : |C2| = 4 : 12.",
  "Conclusion": "In this paper, we study a novel problem of FGL with graphless clients. To tackle this problem, we proposea principled framework FedGLS, which deploys a local graph learner on each graphless client to learn graphstructures with the structure knowledge transferred from other clients.To enable structure knowledgetransfer, we design a GNN model and a feature encoder in FedGLS. They retain structure knowledge togethervia knowledge distillation and the structure knowledge is transferred among clients during global update.Extensive experiments are conducted on five real-world datasets to show the effectiveness of the proposedalgorithm FedGLS. Although this study proposes a novel research problem in FGL, there are some limitations in the proposedFedGLS. For example, one potential limitation of FedGLS is that it may not recover the underlying graphstructures on graphless clients since it produces fixed k neighbors for each node on graphless clients. There-fore, the generated graph structures may not match the unknown real-world structure information. We willwork on designing frameworks to obtain graph structures with consistent real-world structure information.In addition, graphless clients with heterogeneous graphs may introduce more challenges and are worthy ofexploration in the future. This work is supported in part by the National Science Foundation under grants (IIS-2006844, IIS-2144209,IIS-2223769, IIS-2331315, CNS-2154962, BCS-2228534, and CMMI-2411248) and the Commonwealth CyberInitiative Awards under grants (VV-1Q24-011, VV-1Q25-004).",
  "Xingbo Fu, Binchi Zhang, Yushun Dong, Chen Chen, and Jundong Li. Federated graph machine learning:A survey of concepts, techniques, and applications. ACM SIGKDD Explorations Newsletter, 2022": "Xingbo Fu, Chen Chen, Yushun Dong, Anil Vullikanti, Eili Klein, Gregory Madden, and Jundong Li. Spatial-temporal networks for antibiogram pattern prediction. In 2023 IEEE 11th International Conference onHealthcare Informatics, 2023. Xingbo Fu, Zihan Chen, Binchi Zhang, Chen Chen, and Jundong Li. Federated graph learning with structureproxy alignment. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and DataMining, 2024. Spyros Gidaris and Nikos Komodakis. Generating classification weights with gnn denoising autoencodersfor few-shot learning.In Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition, 2019.",
  "Wenke Huang, Guancheng Wan, Mang Ye, and Bo Du. Federated graph semantic and structural learning.In Proceedings of the 32nd International Joint Conference on Artificial Intelligence, 2023": "Wei Jin, Yao Ma, Xiaorui Liu, Xianfeng Tang, Suhang Wang, and Jiliang Tang. Graph structure learningfor robust graph neural networks. In Proceedings of the 26th ACM SIGKDD Conference on KnowledgeDiscovery & Data Mining, 2020. SaiPraneethKarimireddy,SatyenKale,MehryarMohri,SashankReddi,SebastianStich,andAnanda Theertha Suresh. Scaffold: Stochastic controlled averaging for federated learning. In InternationalConference on Machine Learning, 2020.",
  "Aviv Shamsian, Aviv Navon, Ethan Fetaya, and Gal Chechik. Personalized federated learning using hyper-networks. In International Conference on Machine Learning, 2021": "Qiaoyu Tan, Xin Zhang, Ninghao Liu, Daochen Zha, Li Li, Rui Chen, Soo-Hyun Choi, and Xia Hu. Bring yourown view: Graph neural networks for link prediction with personalized subgraph selection. In Proceedingsof the Sixteenth ACM International Conference on Web Search and Data Mining, 2023a. Yue Tan, Guodong Long, Lu Liu, Tianyi Zhou, Qinghua Lu, Jing Jiang, and Chengqi Zhang. Fedproto:Federated prototype learning across heterogeneous clients.In Proceedings of the AAAI conference onartificial intelligence, 2022. Yue Tan, Yixin Liu, Guodong Long, Jing Jiang, Qinghua Lu, and Chengqi Zhang. Federated learning onnon-iid graphs via structural knowledge sharing.In Proceedings of the AAAI conference on artificialintelligence, 2023b.",
  "Song Wang, Xingbo Fu, Kaize Ding, Chen Chen, Huiyuan Chen, and Jundong Li.Federated few-shotlearning. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining,2023": "Song Wang, Yushun Dong, Binchi Zhang, Zihan Chen, Xingbo Fu, Yinhan He, Cong Shen, Chuxu Zhang,Nitesh V Chawla, and Jundong Li. Safety in graph machine learning: Threats and safeguards. arXivpreprint arXiv:2405.11034, 2024b. Yebo Wu, Li Li, Chunlin Tian, Tao Chang, Chi Lin, Cong Wang, and Cheng-Zhong Xu. Heterogeneity-awarememory efficient federated learning via progressive layer freezing. In 2024 IEEE/ACM 32nd InternationalSymposium on Quality of Service (IWQoS), 2024a.",
  "Qiang Yang, Yang Liu, Tianjian Chen, and Yongxin Tong.Federated machine learning: Concept andapplications. ACM Transactions on Intelligent Systems and Technology (TIST), 2019": "Hanqing Zeng, Hongkuan Zhou, Ajitesh Srivastava, Rajgopal Kannan, and Viktor Prasanna. Graphsaint:Graph sampling based inductive learning method. In International Conference on Learning Representa-tions, 2020. Binchi Zhang, Minnan Luo, Shangbin Feng, Ziqi Liu, Jun Zhou, and Qinghua Zheng. Ppsgcn: A privacy-preserving subgraph sampling based distributed gcn training method. arXiv preprint arXiv:2110.12906,2021a.",
  "In this section, we provide more real-world scenarios where the proposed problem will arise": "Graphless clients do not record edge information. In practice, some clients in an FGL system maydownplay the importance of edge information and therefore choose not to record it. Consideringa healthcare system with multiple hospitals, we may construct local graphs in each hospital bytaking patient demographics as node features and co-staying in a ward as edges. In the early stageof a pandemic, however, some hospitals (i.e., graphless clients) may not take account of co-stayinginformation and fail to record it. Since the co-staying information is crucial for predicting a patientsrisk of contracting a contagious disease, these hospitals cannot make accurate predictions based solelyon patient demographics. Instead, our proposed FedGLS can help them to generate proper graphstructures with structure knowledge transferred from other hospitals, enhancing the contagious riskprediction on these hospitals. Graphless clients have not yet generated edge information. In the real world, some clients in an FGLsystem may initially have only node features, with edge information generated later. This scenariois common in dynamic graphs. Considering a financial system with multiple banks, each bank hasits local dataset of customers in a city such as their demographics. For a bank operated for yearsin a city, it also stores transaction records among its customers and the customers naturally forma customer graph where edges represent the transaction records. Nevertheless, a new bank (i.e., agraphless client) in another city may only have few or even no transaction records and fail to forma customer graph. Considering the abundant information in transaction records, the new bank willbenefit from training a model for financial lending with proper graph structures. Graphless clients have contaminated edge information. The edge information on some clients inan FGL system may be contaminated by malicious attackers.Considering multiple e-commercecompanies that aim to jointly train a model for product rating prediction, nodes are products andedges connect products that are frequently bought together. For some companies (i.e., graphless",
  "BModule Designing of Graph Learner": "Graph learners in FedGLS aim to learn graph structures (i.e., adjacency matrices) on the clients in C2 tohelp GCN training for node classification. Typically, a graph learner on each graphless client consists of agraph generator Gen() and a non-parametric adjacency processor (). For simplicity, we omit the clientindex of the notations in this section.",
  "B.1Graph Generator": "Most existing graph generators produce a matrix S either by direct approaches (treating the elements in Sas independent parameters) (Franceschi et al., 2019; Jin et al., 2020) or neural approaches (computing Sthrough an encoder) (Chen et al., 2020b; Liu et al., 2022). Since direct approaches are difficult to train (Zhuet al., 2021), we consider neural approaches in this paper. Neural approaches take node features as inputand produce matrix S. Specifically, we formulate a neural network-based graph generator Gen() on eachgraphless client in C2 asS = Gen(X; ) = (Enc(X; )),(13) where denotes parameters in the encoder Enc() and () is a non-parametric metric function (e.g., cosinesimilarity, Euclidean distance, and inner product). Here we mainly consider two specific instances of neuralnetwork-based graph generators, i.e., MLP Encoders and Attentive Encoders.",
  "B.2Adjacency Processor": "The generated matrix S measures the similarities between the node features. With S, the nodes V form a fullyconnected graph which is not only computationally expensive but also might introduce noise. Furthermore,S may have both positive and negative values while an adjacency matrix should typically be non-negative.Therefore, we deploy an adjacency processor () to refine the generated matrix S before taking it as the inputof GNNs. The goal of the adjacency processor () is to obtain a sparse symmetric normalized adjacencymatrix S with non-negative elements. Typically, the adjacency processor includes three main operations:sparsification, symmetrization, and normalization.",
  "CComplexity Analysis": "In the federated setting, clients may not be equipped with powerful machines for model training. Therefore,the training cost becomes a major concern during collaborative training. Since FedGLS is agnostic on graphlearners and graph learners are updated only once per round, we mainly focus on analyzing the computationalcomplexity of training the GNN model and the feautre encoder (e.g., an MLP) in FedGLS. As we analyzethe computational complexity during local training within a client, we omit the client index of the notationsfor simplicity in this subsection. We take the training of a 2-layer GCN and a 2-layer MLP as the GNN model and the feature encoder asan example. Their parameters are trained over a graph G = (V, E, X) with n = |V| nodes on a client. Herewe assume that both the GCN and the MLP have the same hidden size m. Typically, the GCN producesZ Rnm byZ = A( AXW1)W2,(19)",
  "byH = (XW1)W2,(20)": "where = {W1, W2}. The time complexity of the 2-layer GCN is O(2nm2 + 2n2m) for both forward andbackward pass. Hence, the overall time complexity of the GCN is O(nm2 + n2m). Similarly, we can getthe time complexity of the MLP as O(nm2). Typically, n is significantly larger than m (e.g., 1,079 vs 16in PubMed). Then the time complexity of the MLP will be consequently much smaller than the GCN. Asa result, the feature encoder in FedGLS does not introduce significant extra computational costs comparedwith other baselines such as Fed-GNNk. To reduce the training cost of the feature encoder, we may choose to use smaller MLPs or model com-pression. It can not only reduce the computational complexity of the feature encoder, but also uses fewercommunication resources during aggregation. We leave this exploration for future work.",
  "DDatasets": "In this study, we synthesize the distributed graph data on five real-world datasets, i.e., Cora (Sen et al.,2008), CiteSeer (Sen et al., 2008), PubMed (Sen et al., 2008), Flickr (Zeng et al., 2020), and ogbn-arxiv (Huet al., 2020) by splitting each of them into multiple communities. A community is regarded as an entiregraph on a client. summarizes the statistics and basic information about the five datasets."
}