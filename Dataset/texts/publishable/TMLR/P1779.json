{
  "Abstract": "A big challenge in branch-and-bound lies in identifying the optimal node within the searchtree from which to proceed. Current state-of-the-art selectors utilize either hand-craftedensembles that automatically switch between naive subnode selectors, or learned nodeselectors that rely on individual node data. In contrast to existing approaches that onlyconsider isolated nodes, we propose a novel simulation technique that uses reinforcementlearning (RL) while considering the entire tree state. To achieve this, we train a graph neuralnetwork that produces a probability distribution based on the path from the models root toits to-be-selected leaves. Representing node-selection as a probability distribution allowsus to train a decision-making policy using state-of-the-art RL techniques that capture bothintrinsic node-quality and node-evaluation costs. Our method induces a high quality nodeselection policy on a set of varied and complex problem sets, despite only being trainedon specially designed synthetic traveling salesmen problem (TSP) instances. Using such afixed pretrained policy shows significant improvements on several benchmarks in optimalitygap reductions and per-node efficiency under a short time limit of 45s and demonstratesgeneralization to a significantly longer 5min time limit.",
  "Introduction": "The optimization paradigm of mixed integer programming plays a crucial role in addressing a wide range ofcomplex problems, including scheduling (Bayliss et al., 2017), process planning (Floudas & Lin, 2005), andnetwork design (Menon et al., 2013). A prominent algorithmic approach employed to solve these problemsis branch-and-bound (BnB), which recursively subdivides the original problem into smaller sub-problemsthrough variable branching and pruning based on inferred problem bounds. BnB is also one of the mainalgorithms implemented in SCIP (Bestuzheva et al., 2021), a state-of-the art mixed integer linear and mixedinteger nonlinear solver. An often understudied aspect is the node selection problem, which involves determining which nodes within thesearch tree are most promising for further exploration. This is due to the intrinsic complexity of understandingthe emergent effects of node selection on overall performance for human experts. Contemporary methodsaddressing the problem of node selection typically adopt a perspective per node (Yilmaz & Yorke-Smith, 2021;He et al., 2014; Morrison et al., 2016), incorporating varying levels of complexity and relying on imitationlearning (IL) from existing heuristics (Yilmaz & Yorke-Smith, 2021; He et al., 2014). However, they fail tofully capture the rich structural information present within the branch-and-bound tree itself. We propose a novel selection heuristic that leverages the power of bi-simulation: The BnB tree structure andits expansion/pruning dynamics are directly replicated inside our neural network model. We now employreinforcement learning (RL) to learn a heuristic that is naturally well aligned to the branch-and-boundproblem, see . The resulting method is able to take advantage of the inherent structure within the",
  "branch-and-bound algorithm, leading to a superior generalization compared to the seminal work by Labassiet al. (2022). This allows the RL policy to directly account for the BnB trees dynamics": "We reason that RL specifically is a good fit for this type of training as external parameters outside thepure quality of a node have to be taken into account. For example, a node A might promise a significantlybigger decrease in the expected optimality gap than a second node B, but node A might take twice aslong to evaluate, making B the correct choice despite its lower theoretical utility. By incorporating thebi-simulation technique, we can effectively capture and propagate relevant information throughout the tree.",
  ". an analysis of the learned node selector using explainable-AI techniques": "To the best of our knowledge, we are the first to present a learned selection method that now only generalizesacross instance types but that is also applicable to nonlinear mixed-integer optimization. The rest of this article is structured as follows. Sect. 2 describes branch-and-bound and Sect. 3 discussesrelated work. Sect. 4 explains our method, i.e., our node representation, our node selection agent, and howwe generate training instances. Sect. 5 discusses the experiments, and Sect. 6 discusses limitations. Sec. 7concludes.",
  "Branch and Bound": "BnB is one of the most effective methods for solving mixed integer programming (MIP) problems. Itrecursively solves relaxed versions of the original problem, gradually strengthening the constraints until itfinds an optimal solution. The first step relaxes the original MIP instance into a tractable subproblem bydropping all integrality constraints such that the subproblem can later be strictified into a MIP solution.For simplicity, we focus our explanation to the case of mixed integer linear programs (MILP) while ourmethod theoretically works for any type of constraint allowed in SCIP (see nonlinear results in Sec. 5.3.1,",
  "Now, the problem becomes a linear program without integrality constraints, which can be exactly solvedusing the Simplex (Dantzig, 1982) or other efficient linear programming algorithms": "After solving the relaxed problem, BnB proceeds to the branching step: First, a non-integral yi is chosen.The branching step then derives two problems: The first problem (Eq. 3) adds a lower bound to variable yi,while the second problem (Eq. 4) adds an upper bound to variable yi. These two directions represent therounding choices to enforce integrality for yi:1",
  "P 2relaxed = min{cT1 x + cT2 y|Ax + By b, yi c}(4)": "The resulting decision tree, with two nodes representing the derived problems can now be processed recursively.However, a naive recursive approach exhaustively enumerates all integral vertices, leading to an impracticalcomputational effort. Hence, in the bounding step, nodes that are deemed to be worse than the currentlyknown best solution are discarded. To do this, BnB stores previously found solutions which can be used as alower bound to possible solutions. If a node has a lower bound larger than a currently found integral solution,no node in that subtree has to be processed. The interplay of these three stepsrelaxation, branching, and boundingforms the core of branch-and-bound.It enables the systematic exploration of the solution space while efficiently pruning unpromising regions.Through this iterative process, the algorithm converges towards the globally optimal solution for the originalMIP problem, while producing optimality bounds at every iteration.",
  "Related Work": "Learning node selection, where learned heuristics pick the best node to continue from the search tree, hasonly rarely been addressed in research. Prior work that learns such node selection strategies made significantcontributions to improve the efficiency and effectiveness of the optimization. Notably, many approaches rely on per-node features and Imitation Learning (IL). Otten & Dechter (2012)study the estimation of subproblem complexity as a means to enhance parallelization efficiency. By estimatingthe complexity of subproblems, the algorithm can allocate computational resources of parallel solvers moreeffectively. Yilmaz & Yorke-Smith (2021) employ IL to directly select the most promising node for exploration.Their approach utilizes a set of per-node features to train a model that can accurately determine which nodeto choose. He et al. (2014) make use of support vector machines and IL to create a hybrid heuristic based onexisting heuristics. By leveraging per-node features, their approach aims to improve node selection decisions.While these prior approaches yield valuable insights, they are inherently limited by their focus on per-nodefeatures. Labassi et al. (2022) propose the use of Siamese graph neural networks, representing each node as a graphthat connects variables with the relevant constraints. Their objective is to directly imitate an optimal divingoracle, which descends depth-first towards a node containing a minimal value. This approach facilitateslearning from node comparisons and enables the model to make informed decisions during node selection.This means that Labassi et al. (2022) manages to study pairs of leaf-subproblems, rather than studying every 1There are non-binary, wide branching strategies which we will not consider here explicitly. However, our approach isflexible enough to allow for arbitrary branching width. See Morrison et al. (2016) for an overview.",
  "Published in Transactions on Machine Learning Research (12/2024)": ": Results on TSPLIB (Reinelt, 1991) after 45s runtime. Note that we filter out problems in whichless than 5 nodes were explored as those problems cannot gain meaningful advantages even with perfectnode selection. Name refers to the instances name, Gap Base/Ours corresponds to the optimization gapachieved by the baseline and our method respectively (lower is better), Nodes Base/Ours to the number ofexplored Nodes by each method, and Reward, Utility and Utility Node to the different performancemeasures as described in .",
  "Methodology": "We model the node selection process as a sequential Markov Decision Process (MDP). An MDP consists of aset of observable states s S, a set of controllable actions a A, a reward function R(s) describing the qualityof an individual state s S in isolation, and a (typically unknown) transition distribution T : S A Sthat takes in a current state s S and an action a A, proposed by a decision-making policy : S A, andproduces a distribution over new state s S. The policy is usually thought of as a conditional distribution(a|s) producing the next action a A under the knowledge of the current state s S. Our objective is tofind the policy optimal , which maximizes the expected discounted sum of future rewards:",
  "where 0 < < 1 is a factor to trade immediate over future rewards (Sutton & Barto, 2018)": "RL is the process of solving such MDPs by repeatedly rolling out on an environment and using the resultingtrajectories to find a policy that maximizes the return. Several solvers for MDPs exist, such as DeepQ-Networks (DQN) (Mnih et al., 2015) or Proximal Policy Optimization (PPO) (Schulman et al., 2017). Inthis paper we use PPO to solve the MDP. In our case, we phrase our optimization problem as an MDP, where states st are represented using directedbranch-and-bound trees, actions at are all selectable leaf-nodes, and the reward R is set according to Eq. 6in Sec. 4.1. The transitions are given by the SCIP-solver and its heuristics: Given the current state (BnBtree) and the node selection by , the SCIP solver produces new nodes as children of the selected ones, andprunes unnecessary nodes from the tree. To solve the MDP, PPO needs a representation for the policy anda model describing the state-value-function, defined as V (s) = maxa Q(st, a) (see Schulman et al. (2017)). In our model, we represent the branch-and-bound tree as a directed Graph Neural Network (GNN) whosenodes and edges are in an 1:1 correspondence to the branch-and-bound nodes and edges, see Sec. 4.2. Thismeans that for every node in the BnB tree there is exactly one node in the GNN, and every connection in theBnB tree is replicated in the GNN graph. As the BnB tree grows, the GNN graph grows as well, solving oneof the biggest issues in node selection: Since the BnB tree grows and shrinks dynamically, the number ofdifferent node selections (= actions) changes drastically. One benefit of our GNN model is that the resulting",
  "Reward Definition": "A common issue in MIP training is the lack of good reward functions. For instance, the primal-dual integralused in Gasse et al. (2022) has the issue of depending on the scale of the objective, needing a primally feasiblesolution at the root, and being very brittle if different instance difficulties exist in the instance pool. Gasseet al. (2022) circumvents these issues by generating similar instances, while also supplying a feasible solutionat the root. However, to increase generality, we do not want to make as strict assumptions as having similarinstance difficulties, introducing a need for a new reward function. A reasonable reward measure captures theachieved performance against a well-known baseline. In our case, we choose our reward, to be",
  "gap(SCIP) 1,(6)": "where gap(SCIP) denotes the optimality gap reachable by the standard SCIP node selector in the set timelimit (for us 45s), and gap(node selector) denotes the gap reachable by our method within the set time limit.This represents the relative performance of our selector compared to the performance achieved by SCIP. Intuitively, the aim of this reward function is to decrease the optimality gap achieved by our node-selector(i.e., gap(node selector)), normalized by the results achieved by the existing state-of-the-art node-selectionmethods in the SCIP (Bestuzheva et al., 2021) solver (i.e., gap(SCIP)). This accounts for varying instancehardness in the dataset. Further, we shift this performance measure, such that any value > 0 corresponds toour selector being superior to existing solvers, while a value < 0 corresponds to our selector being worse,and clip the reward between (1, 1) to ensure symmetry around zero as is commonly done in prior work(see, e.g. Mnih et al. (2015); Dayal et al. (2022)). This formulation has the advantage of looking at relativeimprovements over a baseline, rather than absolute performance, which allows us to use instances of varyingcomplexity during training. We also experimented with other metrics that aim to normalize for the complexity of the instance themselves(see Appendix C). We found that the reward metric as proposed here is the most stable to train and yieldsthe best results w.r.t. minimizing the optimality gap.",
  "Tree Representation": "To represent the MDPs state, we propose a novel approach that involves bi-simulating the existing branch-and-bound tree using a graph neural network (GNN). Specifically, we consider the state to be a directed treeT = (V, E), where V are individual subproblems created by SCIP, and E are edges set according to SCIPsnode-expansion rules. More precisely, the trees root is the principle relaxation (Eq. 2), and its childrenare the two rounding choices (Eqs. 3 and 4). BnB proceeds to recursively split nodes, adding the splits aschildren to the node it was split from. For processing inside the neural network, we extract features (see Appendix A) from each node, keeping thestructure intact: T = (extract(V ), E). We ensure that the features stay at a constant size, independent from,e.g., the number of variables and constraints, to enable efficient batch-evaluation of the entire tree. For processing the tree T, we use message-passing from the children to the parent. Pruned or missing nodesare replaced with a constant to maintain uniformity over the graph structure. Message-passing enables us toconsider the depth-K subtree under every node by running K steps of message passing from the children tothe parent. Concretely, the internal representation can be thought of initializing h0(n) = x(n) (with x(n)being the features associated with node n) and then running K iterations jointly for all nodes n:",
  "RL for Node Selection": "While the GNN model is appealing, it is impossible to train using contemporary imitation learning techniques,as the experts action domain (i.e., leaves) may not be the same as the policys action domain, meaning thatthe divergence between these policies is undefined. To solve this problem, we choose to use model-free reinforcement learning techniques to directly maximizethe probability of choosing the correct node. State-of-the-art reinforcement learning techniques, such asproximal policy optimization (PPO) (Schulman et al., 2017), need to be able to compute two functions:The value function V (s) = maxaA Q(s, a), and the likelihood function (a|s). PPO uses the value functionto reduce the variance of the advantage computation. In our work, we utilize the Generalized AdvantageEstimator (GAE) Schulman et al. (2015) which provides a continuous mixture of bootstrapped and MonteCarlo estimated advantages. As it turns out, there are efficient ways to compute both of these values from atree representation: Firstly, we can produce a probability distribution of node-selections (i. e., our actions) by computing theexpected weight across the unique path from the graphs root to the to-be-selected leaves. These weightsare mathematically equivalent to the unnormalized log-probabilities of choosing a leaf-node by recursivelychoosing either the right or left child with probability p and 1 p respectively. The full derivation of this canbe found in Appendix B. Concretely, let n be a leaf node in the set of choosable nodes C , also let P(r, n) bethe unique path from the root r to the candidate leaf node, with |P(r, n)| describing its length. We definethe expected path weight W (n) to a leaf node n C as",
  "V (s) = max {f(n) | n C } ,(12)": "where f(hn) is a trained per-node estimator, f is the path-aggregated f-function, and C is the set of opennodes as proposed by the branch-and-bound method. For an alternative parametrization of V () that justifiesthe use of f as a Q-function, see Appendix J. Since we can compute the action likelihood (a|s), the value function V (s), and the Q-function Q(s, a), wecan use any Actor-Critic method (like PPO (Schulman et al., 2017) or A3C (Mnih et al., 2016)) for trainingthis model. We use PPO due to its ease of use and robustness. According to these definitions, we only need to predict the node embeddings for each node hK(n), theper-node q-function q(hK(n)|s), and the weight of each node W(hK(u)). We parameterize all of these asMLPs (more architectural details can be found in Appendix E).",
  "Data Generation & Agent Training": "In training MIPs, a critical challenge lies in generating sufficiently complex training problems. First, to learnfrom interesting structures, we need to decide on some specific problem, whose e.g. satisfiability is knowableas generating random constraint matrices will likely generate empty polyhedrons, or polyhedrons with manyeliminable constraints (e.g., in the constraint set consisting of cT x b and cT x b + with = 0 oneconstraint is always eliminable). This may seem unlikely, but notice how we can construct aligned c vectorsby linearly combining different rows (just like in LP-dual formulations). In practice, selecting a sufficientlylarge class of problems may be enough as during the branch-and-cut process many sub-problems of differentcharacteristics are generated. Since our algorithm naturally decomposes the problem into sub-trees, we canassume any policy that performs well on the entire tree also performs well on sub-polyhedra generated duringthe branch-and-cut. For this reason we consider the large class of Traveling Salesman Problem (TSP), which itself has richuse-cases in planning and logistics, but also in optimal control, the manufacturing of microchips and DNAsequencing (see Cook et al. (2011)). The TSP problem consists of finding a round-trip path in a weightedgraph, such that every vertex is visited exactly once, and the total path-length is minimal (for more detailsand a mathematical formulation, see Appendix F). For training, we would like to use random instances of TSP but generating them can be challenging. Randomsampling of distance matrices often results in easy problem instances, which do not challenge the solver.Consequently, significant effort is being devoted into devising methods for generating random but hardinstances, particularly for problems like the TSP, where specific generators for challenging problems havebeen designed (see Vercesi et al. (2023) and Rardin et al. (1993)). However, for our specific use cases, theseprovably hard problems may not be very informative as they rarely contain efficiently selectable nodes. To generate these intermediary-difficult problems, we adopt a multi-step approach: We begin by generatingrandom instances and then apply some mutation techniques (Bossek et al., 2019) to introduce variations, andensure diversity within the problem set. Next, we select the instance of median-optimality gap from the pool,which produces an instance of typical difficulty. The optimality gap, representing the best normalized differencebetween the lower and upper bound for a solution found during the solvers budget-restricted execution,serves as a crucial metric to assess difficulty. This method is used to produce 200 intermediary-difficulttraining instances. To ensure the quality of candidate problems, we exclude problems with more than 100% or zero optimalitygap, as these scenarios present challenges in reward assignment during RL. To reduce overall variance ofour training, we limit the ground-truth variance in optimality gap. Additionally, we impose a constraint onthe minimum number of nodes in the problems, discarding every instance with less than 100 nodes. This isessential as we do not expect such small problems to give clean reward signals to the reinforcement learner.",
  "Experiments": "For our experiments we consider the instances of TSPLIB (Reinelt, 1991) and MIPLIB (Gleixner et al., 2021)which are one of the most used datasets for benchmarking MIP frameworks and thusly form a strong baselineto test against. We further test against the UFLP instance generator by (Kochetov & Ivanenko, 2005), which 2This accounts for the phase-transition in MIP solvers where optimality needs to be proved by closing the remainingbranches although the theoretically optimal point is already found (Morrison et al., 2016). Note that with a tuned implementationwe could run our method for more nodes, where we expect further improvements.",
  "Baselines": "We run both our method and SCIP for 45s. We then filter out all runs where SCIP has managed to exploreless than 5 nodes, as in these runs even perfect node selection makes no difference in performance. If weincluded those in our average, we would have a significant number of lines where our node-selector has zeroadvantage over the traditional SCIP one, not because our selector is better or worse than SCIP, but simplybecause it wasnt called in the first place. We set this time-limit relatively low as our prototype selector onlyruns at the beginning of the solver process, meaning that over time the effects of the traditional solver takeover. Additionally, we also consider an evaluation with a 5min time limit. For those runs, the limit until we switchto the naive node selector is set to 650 nodes to account for the higher time limit. In general, the relativeperformance of our method compared to the baseline increases with the 5min budget. Finally, we also benchmark against the previous state-of-the-art by Labassi et al. (2022), which representsoptimal node selection as a comparison problem where a siamese network is used to compare different leafnodes against each other and picking the highest node as the next selection.",
  "Results": "While all results can be found in Appendix I we report an aggregated view for each benchmark in .In addition to our reward metric we report the winning ratio of our method over the baseline, and thegeometric mean of the gaps at the end of solving (lower is better). For benchmarking and training, we leave all settings, such as presolvers, primal heuristics, diving heuristics,constraint specializations, etc. at their default settings to allow the baseline to perform best. All instancesare solved using the same model without any fine-tuning. We expect that tuning, e.g., the aggressiveness ofprimal heuristics, increases the performance of our method, as it decreases the relative cost of evaluatinga neural network, but for the sake of comparison we use the default parameters for all our tests. We trainour node selection policy on problem instances according to Sec. 4.4 and apply it on problems from differentbenchmarks. 3Source code: of these metrics give a slightly different view of the performance of our method: The reward metric is naturallybalanced to weight the improvement relative to the difficulty of the problem as reward captures the improvement over thebaseline in percent (higher is better). The downside of this is that because reward is balanced, it will assign the same rewardto an improvement of 5gap 2.5gap as to 0.5gap 0.25gap, which can overemphasize the impact of easy to solve instances.Win-rate is a supplementary metric showing the percentage of instances where our model beats the baseline. This can helpshow how the improvements are distributed among the instances (higher is better). Finally, comparing the geometric mean isthe gold-standard tool in measuring solver performance (Bestuzheva et al., 2021). In contrast to the reward metric, geometricmeans are specifically designed to reject outliers increasing the robustness of the metric. We report both our geometric meanand the SCIP baselines geometric mean (our gap < SCIP gap is better).",
  "BenchmarkRewardWin-rategeo-mean Oursgeo-mean SCIP": "TSPLIB (Reinelt, 1991)0.184 0.520.500.9310.957UFLP (Kochetov & Ivanenko, 2005)0.078 0.190.6360.4910.520MINLPLib (Bussieck et al., 2003)0.487 0.600.85228.78331.185MIPLIB (Gleixner et al., 2021)0.140 0.690.67166.861106.400 First, we will discuss TSPLIB itself, which while dramatically more complex than our selected traininginstances, still contains instances from the same problem family as the training set (Sec. 5.2.1). Second,we consider instances of the Uncapacitated Facility Location Problem (UFLP) as generated by Kochetov& Ivanenko (2005)s problem generator. These problems are designed to be particularly challenging tobranch-and-bound solvers due to their large optimality gap (Sec. I.1). While the first two benchmarks focusedon specific problems (giving you a notion of how well the algorithm does on the problem itself) we nextconsider meta-benchmarks that consist of many different problems, but relatively few instances of each.MINLPLIB (Bussieck et al., 2003) is a meta-benchmark for nonlinear mixed-integer programming (Sec. 5.3.1),and MIPLIB (Gleixner et al., 2021) a benchmark for mixed integer programming (Sec. 5.3.2). We alsoconsider generalization against the uncapacitated facility location problem using a strong instance generator,see Appendix I.1. Our benchmarks are diverse and complex and allow to compare algorithmic improvementsin state-of-the-art solvers.",
  "TSPLIB": "From an aggregative viewpoint we outperform the SCIP node selection by 20% in reward. However, theoverall win-rate is only 50% as the mean-reward is dominated by instances where our solver has significantperformance improvements: When our method loses, it loses by relatively little (e. g., att48: ours 0.287 vsbase 0.286), while when it wins, it usually does so by a larger margin. Qualitatively, it is particularly interesting to study the problems our method still loses significantly againstSCIP (in four cases). A possible reason why our method significantly underperforms on Dantzig42 is thatour implementation is just too slow, considering that the baseline manages to evaluate 40% more nodes. Asimilar observation can be made on eil51 where the baseline manages to complete 5 more nodes. rd100 isalso similar to Dantzig and eil51 as the baseline is able to explore 60% more nodes. KroE100 is the firstinstance our method loses against SCIP, despite exploring a roughly equal amount of nodes. We believe thatthis is because our method commits to the wrong subtree early and never manages to correct into the propersubtree. Ignoring these four failure cases, our method is either on par (up to stochasticity of the algorithm)or exceeds the baseline significantly. It is also worthwhile to study the cases where both the baseline and our method hit 0 optimality gap. Bothalgorithms reaching zero optimality gap can be seen as somewhat of a special case, since soley looking atthe solution value is insufficient to figure out which method performs better in practice. A quick glance atinstances like bayg29, fri26, swiss42 or ulysses16 shows that our method tends to finish these problemswith significantly fewer nodes explored. This is not captured by any of our metrics since those only look atsolution quality, not the efficiency of reaching that solution. If the quality of the baseline and ours is thesame, it makes sense to look at solution efficiency instead. Qualitatively, instances like bayg29 manage toreach the optimum in only 1",
  "UFLP": "The UFLP benchmark designed by Kochetov & Ivanenko (2005) is specifically built to be hard to solve bybranch-and-bound solvers due to its large duality gap. Despite this, our method manages to outperform thebaseline significantly. This is meaningful since this benchmark is a specially designed worst-case scenario:The fact that our method still outperforms the baseline provides good evidence of the efficacy of the method.",
  "MINLPLIB": "We now consider MINLPs. To solve these, SCIP and other solvers use branching techniques that cutnonlinear (often convex) problems from a relaxed master-relaxation towards true solutions. We considerMINLPLib (Bussieck et al., 2003), a meta-benchmark consisting of hundreds of diverse synthetic and real-worldMINLP instances of varying different types and sizes. As some instances take hours to solve even a singlenode, we filter out all problems with fewer than 5 nodes, as the performance in those cases is independent ofthe node-selectors performance (Full results Appendix I.4). Our method still manages to outperform SCIP, even on MINLPs, despite never having seen a single MINLPproblem before, see . Qualitatively, our method either outperforms or is on par with the vast majorityof problems, but also loses significantly in some problems, greatly decreasing the average. Studying the casesour method loses convincingly (see Appendix I.4), we find a similar case as in TSPLIB, where the baselineimplementation is so much more optimized that significantly more nodes can get explored.We suspect thereason our method has the biggest relative improvements on MINLPs is due to the fact that existing e.g.pseudocost based selection methods do not perform as well on spatial-branch-and-bound tasks. We expect features specifically tuned for nonlinear problems to increase performance by additional percentagepoints, but as feature selection is orthogonal to the actual algorithm design, we leave more thorough discussionof this to future work 5.",
  "MIPLIB": "Last, but not least we consider the meta-benchmark MIPLIB (Gleixner et al., 2021), which consists ofhundreds of real-world mixed-integer programming problems of varying size, complexity, and hardness. Ourmethod is either close to or exceeds the performance of SCIP, see . Considering per-instance results, we see similar patterns as in previous failure cases: Often we underperformon instances that need to close many nodes, as our methods throughput lacks behind that of SCIP. Weexpect that a more efficient implementation alleviates the issues in those cases. We also see challenges in problems that are far from the training distribution, specifically satisfaction problems.Consider fhnw-binpack4-48, were the baseline yields an optimality gap of 0 while we end at +. This isdue to the design of the problem: Instead of a classical optimization problem, this is a satisfaction problem,where not an optimal value, but any feasible value is searched, i.e., we either yield a gap of 0 (solution found),or a gap of + (solution not found), as no other gap is possible. Notably, these kinds of problems may posea challenge for our algorithm, as the node-pruning dynamics of satisfying MIPs are different than the one foroptimizing MIPs: Satisfying MIPs can only rarely prune nodes since, by definition, no intermediary primallyvalid solutions are ever found.",
  "Comparison against Learning to compare nodes": "Aside from comparisons against SCIP, we also compare against the previous state-of-the-art method byLabassi et al. (2022). One complication when benchmarking against Labassi et al. (2022) is that they assume 5We are not aware of a learned BnB node-selection heuristic used for MINLPs, so guidance towards feature selection doesntexist yet. Taking advantage of them presumably also requires to train on nonlinear problems.",
  "Labassi FMCNF339.53 5.9029.45 2.13Labassi GISP1219 1.7326.50 1.55Labassi WPMS215.26 1.9710.46 1.56Ours187.33 3.6719.99 2.251216.16 1.9116.94 1.49221.73 1.788.04 1.72": "a separate model for each problem type, while our method is already flexible enough to handle all problemtypes within a single model. Since Labassi et al. (2022) needs to re-train for each instance type and only worksfor linear problems, we cannot test it against MIPLIB and MINLPLIB. Instead, we reproduce all benchmarksused by Labassi et al. (2022) (each with their own separate models), and compare against them against oursingle model. Labassi et al. (2022) considers the problem of minimizing time-to-solution and minimizing the number ofnodes explored over a set of synthetic problems. This means that rather than the problem of minimizing thegap that can be reached within a given time limit (), here we always optimize to completion and trackthe runtime elapsed until the problem is solved. We use the transfer instances and best performing modelsprovided by Labassi et al. (2022) and re-run the benchmarks on our hardware. As we report in , we convincingly beat the prior work on every single benchmark (aside of WPMSnode count), despite our method never having seen any of these problem types during training. This issurprising because Labassi et al. (2022) should have a convincing advantage due to the fact they not onlyhave a dedicated agents for every single one of their problems, they also use the same generator for theirtraining and testing instances. This implies that your single solver, evaluated out-of-distribution, manages tooutperform the specialized agents proposed by Labassi et al. (2022) evaluated in-distribution. In general, ourmethod improves upon Labassi et al. (2022) by between 30% and 56% with respect to runtime.",
  "Ablations": "We ablate the importance of the individual features used in our node selector using the SHAP (Lundberg& Lee, 2017) interpretability algorithm (see Appendix H). We find that our node selector automaticallydiscovers best-practices from the classical node selection community, such as preferentially selecting nodeswith lower lower-bound, balanced by greedy best node selection. Specifically interesting, we find that ourselector significantly incorporates the amount of cutting planes applied to each node, hinting at a strongerthan expected connection between node-utility and cutting planes. In addition to the feature importance analysis done above, we also investigate running the model without aGNN: We find that when removing the GNN, the model tends to become very noisy and produce unreproducibleexperiments. Considering only the cases where the GNN-free model does well, we still find the model needsroughly 30% more nodes than the SCIP or our model with a GNN. More importantly, we notice the GNN-freemodel diverges during training: starting with a reward of roughly zero, the model diverges down to a rewardof 0.2, which amounts to a score roughly 20% worse than SCIP. We therefore conclude that, at least forour architecture, the GNN is necessary for both robustness and performance.",
  "Limitations": "There are still many open questions that give rise to future research or further investigation. First, while wereport initial results in our ablation study (see Sec. 5.5 and Appendix H) on the importance of features, featureselection remains an area where we expect significant improvements, especially for nonlinear programming,which contemporary methods do not account for.",
  "Conclusion": "We have proposed a novel approach to branch-and-bound node selection, that uses the structure of thebranch-and-bound process and reinforcement learning to convincingly beat classical SCIP and learnt nodeselectors. By aligning our model with the branch-and-bound tree structure, we have demonstrated thepotential to develop a versatile heuristic that can be applied across various optimization problem domains,despite being trained on a narrow set of instances. To our knowledge, this is the first demonstration of learnednode selection to mixed-integer (nonlinear) programming. This work was supported by the Bavarian Ministry for Economic Affairs, Infrastructure, Transport andTechnology through the Center for Analytics-Data-Applications (ADA-Center) within the framework ofBAYERN DIGITAL II. Marcin Andrychowicz, Anton Raichuk, Piotr Staczyk, Manu Orsini, Sertan Girgin, Raphael Marinier,Lonard Hussenot, Matthieu Geist, Olivier Pietquin, Marcin Michalski, Sylvain Gelly, and Olivier Bachem.What Matters In On-Policy Reinforcement Learning? A Large-Scale Empirical Study, June 2020.",
  "Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton.Layer normalization.arXiv preprintarXiv:1607.06450, 2016": "Thomas C. Bachlechner, Bodhisattwa Prasad Majumder, Huanru Henry Mao, G. Cottrell, and JulianMcAuley. Rezero is all you need: Fast convergence at large depth. In Conference on Uncertainty inArtificial Intelligence, 2020. URL Christopher Bayliss, Geert De Maere, Jason Adam David Atkin, and Marc Paelinck. A simulation scenariobased mixed integer programming approach to airline reserve crew scheduling under uncertainty. Annals ofOperations Research, 252:335363, 2017. Ksenia Bestuzheva, Mathieu Besanon, Wei-Kun Chen, Antonia Chmiela, Tim Donkiewicz, Jasper vanDoornmalen, Leon Eifler, Oliver Gaul, Gerald Gamrath, Ambros Gleixner, Leona Gottwald, ChristophGraczyk, Katrin Halbig, Alexander Hoen, Christopher Hojny, Rolf van der Hulst, Thorsten Koch, MarcoLbbecke, Stephen J. Maher, Frederic Matter, Erik Mhmer, Benjamin Mller, Marc E. Pfetsch, DanielRehfeldt, Steffan Schlein, Franziska Schlsser, Felipe Serrano, Yuji Shinano, Boro Sofranac, Mark Turner,Stefan Vigerske, Fabian Wegscheider, Philipp Wellner, Dieter Weninger, and Jakob Witzig. The SCIPOptimization Suite 8.0. Technical report, Optimization Online, December 2021.",
  "Christodoulos A. Floudas and Xiaoxia Lin. Mixed integer linear programming in process scheduling: Modeling,algorithms, and applications. Annals of Operations Research, 139:131162, 2005": "Maxime Gasse, Didier Chtelat, Nicola Ferroni, Laurent Charlin, and Andrea Lodi. Exact combinatorialoptimization with graph convolutional neural networks. In Neural Information Processing Systems, 2019.URL Maxime Gasse, Quentin Cappart, Jonas Charfreitag, Laurent Charlin, Didier Chetelat, Antonia Chmiela,Justin Dumouchelle, Ambros M. Gleixner, Aleksandr M. Kazachkov, Elias Boutros Khalil, Pawel Lichocki,Andrea Lodi, Miles Lubin, Chris J. Maddison, Christopher Morris, Dimitri J. Papageorgiou, AugustinParjadis, Sebastian Pokutta, Antoine Prouvost, Lara Scavuzzo, Giulia Zarpellon, Linxin Yangm, ShaLai, Akang Wang, Xiaodong Luo, Xiang Zhou, Haohan Huang, Sheng Cheng Shao, Yuanming Zhu,Dong Zhang, Tao Manh Quan, Zixuan Cao, Yang Xu, Zhewei Huang, Shuchang Zhou, Cheng Binbin,He Minggui, Hao Hao, Zhang Zhiyu, An Zhiwu, and Mao Kun. The machine learning for combinatorialoptimization competition (ml4co): Results and insights. ArXiv, abs/2203.02433, 2022. URL",
  "Alan E. Gelfand and Adrian F. M. Smith. Sampling-based approaches to calculating marginal densities.Journal of the American Statistical Association, 85(410):398409, 1990. ISSN 01621459. URL": "Ambros Gleixner, Gregor Hendel, Gerald Gamrath, Tobias Achterberg, Michael Bastubbe, Timo Berthold,Philipp Christophel, Kati Jarck, Thorsten Koch, Jeff Linderoth, Marco Lbbecke, Hans D. Mittelmann,Derya Ozyurt, Ted K. Ralphs, Domenico Salvagnin, and Yuji Shinano. MIPLIB 2017: Data-drivencompilation of the 6th mixed-integer programming library. Mathematical Programming Computation, 13(3):443490, September 2021. ISSN 1867-2957. doi: 10.1007/s12532-020-00194-3. He He, Hal Daume III, and Jason M Eisner. Learning to search in branch and bound algorithms. InZ. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K.Q. Weinberger (eds.), Advances in NeuralInformation Processing Systems, volume 27. Curran Associates, Inc., 2014.",
  "Hans D. Mittelmann. Decison Tree for Optimization Software. Accessed: 2023-09-02": "Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, andMartin A. Riedmiller. Playing atari with deep reinforcement learning. ArXiv, abs/1312.5602, 2013. URL Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, AlexGraves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, AmirSadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and DemisHassabis. Human-level control through deep reinforcement learning. Nature, 518(7540):529533, February2015. doi: 10.1038/nature14236. URL Volodymyr Mnih, Adri Puigdomnech Badia, Mehdi Mirza, Alex Graves, Timothy P. Lillicrap, TimHarley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning.In International Conference on Machine Learning, 2016. URL David R. Morrison, Sheldon H. Jacobson, Jason J. Sauppe, and Edward C. Sewell. Branch-and-boundalgorithms: A survey of recent advances in searching, branching, and pruning. Discrete Optimization, 19:79102, 2016. ISSN 1572-5286. doi: Lars Otten and Rina Dechter. A Case Study in Complexity Estimation: Towards Parallel Branch-and-Boundover Graphical Models. Uncertainty in Artificial Intelligence - Proceedings of the 28th Conference, UAI2012, October 2012. Christopher W. F. Parsonson, Alexandre Laterre, and Thomas D. Barrett. Reinforcement learning for branch-and-bound optimisation using retrospective trajectories. In AAAI Conference on Artificial Intelligence,2022. URL",
  "Mark Turner, Thorsten Koch, Felipe Serrano, and Michael Winkler. Adaptive cut selection in mixed-integerlinear programming. Open J. Math. Optim., 4:128, 2022. URL": "Eleonora Vercesi, Stefano Gualandi, Monaldo Mastrolilli, and Luca Maria Gambardella. On the generationof metric TSP instances with a large integrality gap by branch-and-cut. Mathematical ProgrammingComputation, 15(2):389416, June 2023. ISSN 1867-2957. doi: 10.1007/s12532-023-00235-7. Zhihai Wang, Xijun Li, Jie Wang, Yufei Kuang, Min jie Yuan, Jianguo Zeng, Yongdong Zhang, and FengWu. Learning cut selection for mixed-integer linear programming via hierarchical sequence model. ArXiv,abs/2302.00244, 2023. URL",
  "AFeatures": "lists the features used on every individual node. The features split into two different types: One beingmodel features, the other being node features. Model features describe the state of the entire model atthe currently explored node, while node features are specific to the yet-to-be-solved added node. We aimto normalize all features with respect to problem size, as e. g., just giving the lower-bound to a problem isprone to numerical domain shifts. For instance a problem with objective cT x, x P is inherently the samefrom a solver point-of-view as a problem 10cT x, x P, but would give different lower-bounds. Since NNs aregenerally nonlinear estimators, we need to make sure such changes do not induce huge distribution shifts.We also clamp the feature values between which represent infinite values, which can occur, forexample in the optimality gap. Last but not least, we standardize features using empirical mean and standarddeviation. These features are inspired by prior work, such as Labassi et al. (2022); Yilmaz & Yorke-Smith",
  "(2021), but adapted to the fact that we do not need e. g., explicit entries for the left or right childs optimalitygap, as these (and more general K-step versions of these) can be handled by the GNN": "Further, to make batching tractable, we aim to have constant size features. This is different from e. g., Labassiet al. (2022), who utilize flexibly sized graphs to represent each node. The upside of this approach is thatcertain connections between variables and constraints may become more apparent, with the downside beingthe increased complexity of batching these structures and large amounts of nodes used. This isnt a problemfor them, as they only consider pairwise comparisons between nodes, rather than the entire branch-and-boundgraph, but for us would induce a great deal of complexity and computational overhead, especially in the largerinstances. For this reason, we represent flexibly sized inputs, such as the values of variables, as histograms:i.e., instead of having k nodes for k variables and wiring them together, we produce once distribution ofvariable values with 10-buckets, and feed this into the network. This looses a bit of detail in the representation,but allows us to scale to much larger instances than ordinarily possible. In general, these features are not optimized, and we would expect significant improvements from morewell-tuned features. Extracting generally meaningful features from branch-and-bound is a nontrivial task andis left as a task for future work.",
  "BTheoretical Derivation": "One naive way of parameterizing actions is selecting probabilistically by training the probability of going tothe left or right child at any node. This effectively gives a hierarchical Bernoulli description of finding a pathin the tree. A path is simply a sequence of left (zero direction) and right (one direction) choices, withthe total probability of observing a path being the likelihood of observing a specific chain",
  ": Naive approach using recursive selection. The probabilities are computed based on which fork ofthe tree is traveled. Sampling this can be done by sampling left or right based on pi": "with pi being the ith decision in the graph (see Fig 2). Node selection can now be phrased as the likelihoodof a random (weighted) walk landing at the specified node. However, using this parametrization directlywould yield a significant number of problems: Removing illegal paths from the model is quite challenging to do with sufficiently high performance. If a nodegets pruned or fully solved, the probability to select this node has to be set to zero. This information has tobe propagated upward the tree to make sure that the selection never reaches a dead-end. This is possible,but turns out to be rather expensive to compute since it needs to evaluate the entire tree from leaves to root. From a theoretical point of view, the hierarchical Bernoulli representation also has a strong prior towardsmaking selections close to the root, as the probability of reaching a node a depth K consists of p1 p2 pKselections. Since all pi are set to an uninformative prior 0 < pi < 1, the model at initialization has anincreased likelihood to select higher nodes in the tree. Considering that classical methods specifically mixplunging heuristics into their selection to get increased depth exploration (see Sec. 3), this breadth over depthinitialization is expected to give low performance. Therefore, one would need to find a proper initializationthat allows the model to e. g., uniformly select nodes independent of depth to get good initialization. The fact that multiple sampling sites exist in this method also poses issues with RL, as small errors early inthe tree can have catastrophic effects later. This means the tree based model is a significantly higher varianceestimator than the estimator we propose in Sec 4.3, which yields much worse optimization characteristics. Itis well established in existing deep learning (e. g., (Kingma & Welling, 2013)) and general Bayesian Inference(e. g., (Robert & Roberts, 2021; Gelfand & Smith, 1990)) literature, that isolating or removing sampling sitescan lead to lower variance, and therefore much more performant, statistical estimators. Using those insights, we can now rewrite this naive approach into the one we propose in Sec. 4.3: First,instead of sampling just one path, we can sample all possible paths and compute the likelihood for each. Theresult will be a probability p at every possible leaf. If one parameterizes this as a logarithmic probability thelikelihood of sampling any individual leaf, can be written as",
  "= softmaxleaf{W (c)|c C }definition softmax,": "where softmaxleaf refers to the softmax-entry associated with leaf. The resulting decomposition is, while notthe same, equivalent to the original Bernoulli decomposition (in the sense that for any Bernoulli decompositionthere exists a softmax-based decomposition). Beyond the massive reduction in sampling sites from O(depth)to a single sampling site, phrasing the problem using unnormlized-log p representations gives rise to additionaloptimizations: Firstly, to achieve uniform likelihood for all nodes at the beginning of training, we simply have to initializeW(n) = 0 for all nodes, which can be done by setting the output weights and biases to zero (in practice weset bias to zero and weights to a small random value). Secondly to account for pruned or solved nodes, we do not need to change the unnormalized probabilities.Instead, we simply only sample the paths associated with selectable nodes, as we know the likelihood ofselecting a pruned or solved node is zero. This means we can restrict ourselves to only evaluating thecandidate paths C , rather than all paths (see Eq. 9). The last modification we made in our likelihood computation (see Eq. 8), is to normalize the weights based ondepth. This is done mainly for numerical stability, but in our Bayesian framework is equivalent to computingthe softmax-normalization Eq. 9 using weights inversely proportional to depth. The resulting algorithmis much more stable and performant than the naive parametrization. In early implementations we usedthe recursive-sampling approach to compute likelihoods, but the resulting scheme did not manage to reachabove random results on the training set, presumably due to the higher computational burden and worseinitialization. There are also other advantages to our parametrization, such as the fact that one can easilyinclude a sampling temperatur into the process",
  "node softmax{W (c)/|c C }": "which can be nice to tune the model more into the direction exploitation during testing. For our laterbenchmarking (Sec 5) we keep = 1 (i. e., no temperature), as it would introduce a further tunablehyperparameter, but this could be interesting for future work. This construction has interesting parallels to Monte-Carlo Tree Search (MCTS) Silver et al. (2017) basedreinforcement learning methods. The difference in our method is that we consider iterative updates to apolicy distribution rather than a Q-function, and therefore can be seen as a policy-based tree search scheme:While traditional MCTS uses variants of UCB-scores to guide the search, our method can be thought of asvariants of Thompson sampling to facilitate the exploration-exploitation tradeoff. We leave a more thoroughinvestigation of this to future work.",
  ".(14)": "The reason we do not use this as a reward measure is because we empirically found it to produce worsemodels. This is presumably because some of the negative attributes of our reward, e.g., the asymmetry of thereward signal, lead to more robust policies. In addition, the utility metric gives erroneous measurementswhen both models hit zero optimality gap. This is because utility implicitly defines 0",
  "= 0, rather thanreward, which defines it as 0": "0 = 1. In some sense the utility measurement is accurate, in that our methoddoes not improve upon the baseline. On the other hand, our method is already provably optimal as soon asit reaches a gap of 0%. In general, utility compresses the differences more than reward which may or may notbe beneficial in practice.",
  "nodes(node selector) and scip =gap(scip)": "nodes(scip). The per-node utility gives a proxy for the totalamount of work done by each method. However, it ignores the individual node costs, as solving the differentLPs may take different amounts of resources (a model with higher utility/node is not necessarily moreefficient as our learner might pick cheap but lower expected utility nodes on purpose). Further, the metricis underdefined: comparing two ratios, a method may become better by increasing the number of nodesprocessed, but keeping the achieved gap constant. In practice the number of nodes processed by our nodeselector is dominated by the implementation rather than the node choices, meaning we can assume it isinvariant to changes in policy. Another downside arises if both methods reach zero optimality gap, theresulting efficiency will also be zero regardless of how many nodes we processed. As our method tend to reachoptimality much faster (see Sec. 5 and Appendix I), all utility/node results can be seen as a lower-bound forthe actual efficiency.",
  "DComparisons of Runtime to completion": "We also compare the runtime of our selector against the SCIP baseline on TSPLIB. For this we set a maximumtime limit of 30min and run both methods. It is worth noting that due to the inherent difficulty of TSPLIB, asignificant number of problems are still unsolved after 30min, in which case these models are simply assignedthe maximum time as their overall completion time. Note that this also implies that the SCIP solver itself notcapable of solving these problems due to their scale. We reckon that this is also the reason why e.g., Labassi",
  ": The shifted geometric mean (shifted by +10) of runtime and number of nodes processed": "et al. (2022) does not even attempt to benchmark on these large scale problems. Our method outperformsthe baseline, however due to the fact that our method aims to improve the quality of early selections inparticular, the amount of improvement is rather small. Further, our method is fundamentally undertrainedto adequately deal with longer time horizons and we would assume the effect of our method is larger thelonger the model is trained. Despite this being a worst-case evaluation scenario, our method manages tooutperform the baseline in both runtime and number of nodes needed. Interestingly, we also manage to reach this superior performance while utilizing only about half the numberof nodes. This discrepancy cannot be explained by just considering the slowdown due to our policy needingto be evaluated, as, while it is a significant overhead due to the Python implementation, it is very far frombeing large enough to justify a 50% slowdown over the course of 30min. This also shows more generally thateven if one restricts themselves to only processing the beginning of the branch-and-bound tree, one can reachsuperior performance to the standard node-selection schemes.",
  "EArchitecture": "Our network consists of two subsystems: First, we have the feature embedder that transforms the raw featuresinto embeddings, without considering other nodes this network consists of one linear layer |dfeatures| |dmodel|with LeakyReLU (Xu et al., 2015) activation followed by two |dmodel| |dmodel| linear layers (activated byLeakyReLU) with skip connections. We finally normalize the outputs using a Layernorm (Ba et al., 2016)without trainable parameters (i. e., just shifting and scaling the feature dimension to a normal distribution). Second, we consider the GNN model, whose objective is the aggregation across nodes according to the treetopology. This consists of a single LeakyReLU activated layer with skip-connections. We use ReZero (Bach-lechner et al., 2020) initialization to improve the convergence properties of the network. Both the weight andvalue heads are simple linear projections from the embedding space. Following the guidance in (Andrychowiczet al., 2020), we make sure the value and weight networks are independent by detaching the value headsgradient from the embedding network. For this work we choose |dmodel| = 512, but we did not find significant differences between different modelsizes past a width of 256. For training we use AdamW Loshchilov & Hutter (2017) with a standard learningrate of 3 104 and default PPO parameters.",
  "ui Z, xij {0, 1}": "Effectively this formulation keeps two buffers: one being the actual (i, j)-edges travelled xij, the other being anode-order variable ui that makes sure that ui < uj if i is visited before j. There are alternative formulations,such as the DantzigFulkersonJohnson (DFJ) formulation, which are used in modern purpose-built TSPsolvers, but those are less useful for general problem generation: The MTZ formulation essentially relaxes theedge-assignments and order constraints, which then are branch-and-bounded into hard assignments duringthe solving process. This is different to DFJ, which instead relaxes the has to pass through all nodesconstraint. DFJ allows for subtours (e. g., only contain node A, B, C but not D, E) which then get slowlyeliminated via the on-the-fly generation of additional constraints. To generate these constraints one needsspecialised row-generators which, while very powerful from an optimization point-of-view, make the algorithmless general as a custom row-generator has to intervene into every single node. However, in our usecasewe also do not really care about the ultimate performance of individual algorithms as the reinforcementlearner only looks for improvements to the existing node selections. This means that as long as the degree ofimprovement can be adequately judged, we do not need state-of-the-art solver implementations to give thelearner a meaningful improvement signal.",
  "GUncapacitated facility location Problem": "Mathematically, the uncapacitated facility location problem can be seen as sending a product zij from facilityi to consumer j with cost cij and demand dj. One can only send from i to j if facility i was built in the firstplace, which incurs cost fi. The overall problem therefore is",
  "xi {0, 1}i = 1, . . . , n": "where M is a suitably large constant representing the infinite-capacity one has when constructing xi = 1.One can always choose M m since that, for the purposes of the polytop is equivalent to setting M to literalinfinity. This is also sometimes referred to as the big M method. The instance generator by Kochetov & Ivanenko (2005) works by setting n = m = 100 and setting all openingcosts at 3000. Every city has 10 cheap connections sampled from {0, 1, 2, 3, 4} and the rest have cost 3000,which represents infinity (i. e., also invoking the big M method).",
  "HFeature Importance": "We try to interpret the learned node-selection heuristic by applying the KernelSHAP (Lundberg & Lee,2017) method. SHAP methods try to estimate the feature importance by measuring the performance of anestimator where some portion of the features are replaced with a neutral element. Doing this with a largeenough set, one is able to extract the feature importance in the form of the magnitude of the change betweenthe expected value of the estimator and the value the estimator would have had if a specific feature wereabsent. Analyzing our RL method with SHAP has two significant limitations: Firstly, we ignore the impact of messagepassing on the model and instead analyze every node as a leaf node. This is of relatively small impact as thetime where a node was selected it had to have been a leaf node anyways, so the plots faithfully show themodel output at selection time. The second limitation is that we do not have an iid. dataset, but instead have to build one from theenvironment first. To build that dataset, we could sample random BnB trees and evaluate on the nodes, butthis would mean we evaluate our model completely out-of-distribution, as random selection will most likely leadto states the model would never want to reach and is therefore ill equipped to handle. Therefore, we insteadbuild our dataset by running our node-selector on a subset of MIPLIB to gather our importance-evaluationdata. This has the downside that the expected value of our selection network is higher than in randomselection, since the model is guided towards selecting higher value nodes. With these caveats in mind, one can see in fig. 3 that our node selector relies significantly on the nodelowerbound for setting its weight. This makes sense: in node-selection there is a fundamental tension betweenraising the lower bound and opportunistically searching for new primal solutions. The current SCIP defaulthybrid best bound search does exactly that, just using hand-made heuristics where a nodes are selectedbased on a mixture of following the currently best bound nodes, and plunging depth-first search to findbetter solutions. However, as can be seen in our benchmarks (tab 1), we beat SCIPs default selector quitesignificantly, which implies a nontrivial interaction in the remaining features. Looking at the beeswarm plot 4, we observe that our model preferentially picks nodes which, in addition to alow node lowerbound, also uses nodes which have few variables with an integrality gap of 0.2 and 0.3, has alot of cuts applied, and has a high expected integrality gap. If we considered a node with all of these features(and assume there would not be any nonlinear value-dampening effects), such a node would be a primecandidate for having good child nodes: A node with a low lowerbound means that the node still has significantroom for improvement, while having a lot of cuts applied means that the nodes solution is sufficiently closeto existing integral points to make it feasible to find better values. Looking further into the features, one canalso see that the model dislikes nodes that already have a significant number of already integral variables.This makes sense as nodes with a significant number of integral variables that are not solved already (i. e.,they have children to split on), can be quite hard to complete while not giving a significant improvement insolution quality. This is because most likely a primal heuristic has already found the optimal value of thatnode, but branch-and-bound still needs a significant number of trials to prove that the found value is optimalin that subtree. In short, our solver picks the nodes with the highest degree of possible improvement (low lowerbound), whilealso favoring nodes that have more information (more cuts applied) and presumably are nontrivial to solvevia heuristics (high mean to integral gap/low already integral ratio). In general, interpreting dynamic RL policies is highly nontrivial, especially if the estimator is nonlinear, butthe interactions we can clearly see in the SHAP plots indicates that the learned policy is reasonably wellgrounded in existing best-practices for node selectors.",
  "I.1Kochetov-UFLP": "To demonstrate the generalizability of the learned heuristics, we test our method on the UncapacitatedFacility Location Problem (see Appendix G) without further finetuning, i.e., we only train on TSP instancesand never show the algorithm any other linear or nonlinear problem. For testing, we generate 1000 instancesusing the well-known problem generator by Kochetov & Ivanenko (2005), which was designed to have largeoptimality gaps, making these problems particularly challenging. Our method performs very similar to the highly optimized baseline, despite never having seen the UFLproblem, see . We argue that this is specifically because our method relies on tree-wide behaviour,rather than individual features to make decisions. We further hypothesize that the reason for the advantageover the baseline being so small is due to the fact that UFLP consists of adversarial examples to thebranch-and-bound method where cuts have reduced effectiveness. This means clever node-selection strategieshave limited impact on overall performance. An interesting aspect is that our method processes more nodes than the baseline, which also leads to the lossin node-efficiency. This implies that our method selects significantly easier nodes, as ordinarily our solver isslower just due to the additional overhead. Considering that this benchmark was specifically designed toproduce high optimality gaps, it makes sense that our solver favours node quantity over quality, which is aninteresting emergent behaviour of our solver.",
  "I.3MIPLIB results": ": Results on MIPLIB (Gleixner et al., 2021) after 45s runtime. Note that we filter out problems inwhich less than 5 nodes were explored as those problems cannot gain meaningful advantages even with perfectnode selection. Name refers to the instances name, Gap Base/Ours corresponds to the optimization gapachieved by the baseline and our method respectively (lower is better), Nodes Base/Ours to the number ofexplored Nodes by each method, and Reward, Utility and Utility Node to the different performancemeasures as described in . Note that all results where achieved with a policy only trained on TSPinstances",
  "I.4MINLPLIB results": ": Results on MINLPLIB (Bussieck et al., 2003) after 45s runtime. Note that we filter out problems inwhich less than 5 nodes were explored as those problems cannot gain meaningful advantages even with perfectnode selection. Name refers to the instances name, Gap Base/Ours corresponds to the optimization gapachieved by the baseline and our method respectively (lower is better), Nodes Base/Ours to the number ofexplored Nodes by each method, and Reward, Utility and Utility Node to the different performancemeasures as described in . For all three measures, higher is better.",
  "nCQ(n)(n|s)dn(20)": "where q(hn) are the learned per-node estimator, Q the unnormalized Q-value, and C is the set of open nodesas proposed by the branch-and-bound method. The advantage of this interpretation is that it allows one tointerpret the path estimates Q as the on-policy q-values for each path. Having access to the q-values may benecessary for different actor-critic methods or the use of Deep Q-learning (Mnih et al., 2013)."
}