{
  "Abstract": "In advancing the understanding of natural decision-making processes, inverse reinforcementlearning (IRL) methods have proven instrumental in reconstructing animals intentions un-derlying complex behaviors.Given the recent development of a continuous-time multi-intention IRL framework, there has been persistent inquiry into inferring discrete time-varying rewards with IRL. To address this challenge, we introduce the class of hierarchicalinverse Q-learning (HIQL) algorithms. Through an unsupervised learning process, HIQLdivides expert trajectories into multiple intention segments, and solves the IRL problemindependently for each. Applying HIQL to simulated experiments and several real animalbehavior datasets, our approach outperforms current benchmarks in behavior predictionand produces interpretable reward functions. Our results suggest that the intention tran-sition dynamics underlying complex decision-making behavior is better modeled by a stepfunction instead of a smoothly varying function. This advancement holds promise for neu-roscience and cognitive science, contributing to a deeper understanding of decision-makingand uncovering underlying brain mechanisms.",
  "Published in Transactions on Machine Learning Research (09/2024)": "together. We then performed multiple IQL (single intention HIQL) with different history length h = 1, . . . , 5on the concatenated dataset. Out of the 5 different values, We chose the h that resulted in the best testset log-likelihood for subsequent stages. In the second stage, we run multiple HIQL with different num-ber of intentions K = 2, . . . , 5 again to the concatenated dataset to obtain a global fit. For each fitting,we performed 10 different initializations and the best performed initialization was selected. The initial in-tention distribution was initialized uniformly, and the intention transition matrix was initialized as: = 0.95 I + N(0, 0.05 I), where N denotes the normal distribution and I RKK is the identitymatrix. This initial was then normalized so that each row added up to 1. In the last stage of the fittingprocedure, we wanted to obtain an independent but aligned HIQL fit for each animal, so we initialized theparameters for each animal with the best global fit parameters from all animals together, omitting the ne-cessity to permute the retrieved intentions from each animal so as to map semantically similar intentions toone another. A 5-fold cross-validation was used to split the training and test dataset. The discount factorfot this experiment were set to be = 0.99.",
  "Related work": "Various approaches have been introduced to address multi-intention IRL problems. Notably, several frame-works based on parametric (Babes et al., 2011; Likmeta et al., 2021), or non-parametric (Choi & Kim, 2012;Bighashdel et al., 2021) approaches allow for learning from multiple agents with distinct reward functions.Based on the assumption that the expert sticks to the same intention through one episode, these meth-ods iterate between clustering expert demonstrations according to different intentions and solving the IRLproblem for each cluster. However, unlike HIQL, these frameworks do not accommodate the scenario whereintentions might alternate within the same episode. In contrast, several approaches formulate MI-IRL problem as finding the maximum-likelihood partition ofeach trajectory, where each segment was generated from different locally consistent reward functions. Tosolve the problem, Dimitrakakis & Rothkopf (2012); Michini & How (2012) extended the Bayesian IRLframeworks (Ramachandran & Amir, 2007) to multi-intention scenario by introducing a Dirichlet processprior on different reward functions. The resulting Dirichlet process mixture model (DPM) can be solved via",
  "Background": "Markov decision processes.A Markov decision process (MDP) can be denoted by a tuple (S, A, P, r, ),where S and A denote the state- and action-space, respectively; The function P : SAS is the statetransition function with P(s, a, s) = P(s | s, a) and 1T P(s, a, ) = 1; The function r: S A R definesthe reward function, and denotes the discount factor. Additionally, the function : S A with (s, a) = P(a | s) and 1T (s, ) = 1 is used to represent the policy according to which actions areselected in the MDP. Inverse Q-learning.Given the set of expert demonstrations D in an MDP, where each trajectory Dis denoted with a sequence of state-action pairs: = {(s0, a0) , . . . , (sn, an)}, the IRL problem consists indetermining a reward function r that best explains the observed expert behavior in the form of demonstratedtrajectories. Note that both the word demonstration and trajectory will be used subsequently. The formerrefers to one action execution and state transition of the expert, while the latter corresponds to a sequenceof actions and state transitions for an entire episode.Unfortunately, IRL is generally ill-posed becauseinfinitely many reward functions are consistent with the experts observed behavior. To resolve this issue,various approaches have been proposed (see Arora & Doshi (2021) for a detailed review).Particularly,",
  ".(5)": "Thus the unknown reward function r can be obtained by solving (5) for all s S via least squares (Kalweitet al., 2020). This approach is called the inverse action-value iteration (IAVI) (Algorithm 1), a model-basedalgorithm that solves the IRL problem analytically in closed-form.When the transition dynamics P isunknown, stochastic approximation can be applied to (3) and (4) such that r can be obtained in a sampling-based manner (Kalweit et al., 2020). The resulting inverse Q-learning (IQL) algorithm (Algorithm 2) isa model-free extension of IAVI. The class of IQL algorithms solve the MDP underlying the demonstratedbehavior only once, leading to a speedup of up to several orders of magnitude compared to the popular max-imum entropy IRL algorithm (Ziebart et al., 2008) and some of its variants. In addition, it can accommodatearbitrary non-linear reward functions.",
  "We formulate the multi-intention IRL problem under the following assumptions:": "Assumption 4.1. Each expert demonstration is generated according to the Boltzmann optimal policy underone of the reward functions in a K-dimensional finite set R = {r1, . . . , rK}, with each corresponding to onespecific intention. Assumption 4.2. The probability that one demonstration is generated under reward function r R iscontrolled by a Markov chain with initial state distribution and transition matrix , where the vector and each row of the transition matrix i:, i = 1, . . . , K represent some probability distribution over the setR, i.e., K1 and i: K1, where K1 =x RK | x 0, 1T x = 1is a (K 1)-dimensionalprobability simplex.",
  ": Graphical representation of expertsdecision process": "The resulting hierarchical decision process of an expertfollowing these assumptions from the perspective of a re-searcher is depicted graphically in , where thefirst two rows represent the MDP, and the last row repre-sents the Markov chain for intention transition dynamics.Solving IRL problems on such decision network with pa-rameter space = {, , R} consists in determining 1)a set of reward functions, and 2) the reward function in-dex for each demonstration that best jointly explain theobserved expert behavior. An expectation-maximization(EM) algorithm can be devised to iteratively learn . Forconvenience, we introduce = {z0, . . . , zn} to be the pre-dicted sequence of reward function indexes for trajectory D. Then each iteration of the EM processcan be written as an MLE problem where the likelihood is obtained by marginalizing out the sequence ofpredicted reward function indexes :",
  "Proof. See Appendix A.1": "In practice, to evaluate the objective functions of (7), (8) and (9), the Baum-Welch algorithm (Baum &Petrie, 1966) can be applied to obtain the required posterior probabilities P(zt = i | , ) and P(zt1 =i, zt = j | , ) (cf. Appendix A.2). Then according to the Gibbs inequality, the optimum of (7) and (8) areachieved by+i = ED[P(z0 = i | , )],i = 1, . . . , K,(10)",
  "ED,t [P(zt1 = i | , )],i = 1, . . . , K,j = 1, . . . , K.(11)": "To solve problem (9), note that it has the same structure as (1), thus it can be addressed by the class ofIQL algorithms (Algorithm 1 and 2) with slight adaptations. Specifically, for each i = 1, . . . , K, a subset ofexpert demonstrations from D will be sampled w.r.t. the posterior probability P(zt = i | , ), then eitherAlgorithm 1 or Algorithm 2 (depending on whether the transition dynamics P is available) will be appliedto this subset of demonstrations so that r+i can be obtained. Assembled, we call the above process of solvingmulti-intention IRL problems the hierarchical inverse Q-learning (HIQL) algorithm.The correspondingpseudo-code is listed in Algorithm 3.",
  "Experiments and discussion": "We evaluate the performance of HIQL on a simulated gridworld environment and a real mice behavior datasetobtained from a 127-node labyrinth navigation task (Rosenberg et al., 2021), and compare to DIRL (Ashwoodet al., 2022a).We then show the potential of performing model-free learning and detecting explorationbehavior with HIQL on a mice reversal-learning task (De La Crompe et al., 2023).",
  ": The gridworld environ-ment": "The simulated gridworld environment is a 55 map (), where anagent can choose between going up, down, left, right, or to stay in placeper time step. The expert starts at origin (0, 0) and any of its actions canachieve the intended state with 90% probability, while the other 10% leadto random directions.It has two possible policies, where goal prefersgoing to the destination (4, 4) and abandon prefers returning to origin(0, 0). The expert starts an episode always with the goal policy. Whilemoving to the destination, the expert will encounter barriers # at somestates. Every time the expert runs into a barrier, it has 30% probability ofswitching to a different policy. Each episode ends when the expert reacheseither the origin or the destination, and at the 8th time step, if the episodehas not finished, the expert will have a 50% probability of switching toabandon (cf. Appendix C.1). Such intention transition dynamics of thesimulated expert as described above is better aligned to a step-function,instead of a smoothly varying function. We compared between the performance of HIAVI (the model-based variant of HIQL) and DIRL with 1 or2 intention(s). Note that for the single intention case, HIAVI falls back to normal IAVI and DIRL fallsback to maximum causal entropy IRL (Ziebart et al., 2010).Trying to enable DIRL to better capturesuch step-like intention transition dynamics, we varied the hyperparameter that controls the smoothness ofthe time-varying reward in DIRL the variance for the Gaussian of the random walk prior, from 0.01to 10. A larger value corresponds to a less smooth intention transition dynamics in DIRL. The wholeexpert demonstration dataset consisted of 1024 trajectories. All evaluated algorithms were fit to multiplesub-datasets with different number of expert trajectories, and each dataset was analyzed using a 5-foldcross-validation. We list the detailed information about model training in Appendix C.1. In general, HIAVI outperformed DIRL in predicting the expert behavior as indicated by the log-likelihood onthe test dataset (a). Notice that HIAVI already had a better performance compared to DIRL evenin the collapsed single intention model, indicating that IAVI serves as a better inner-loop (IRL problem)solver than maximum entropy IRL. This is probably because IAVI learns the unknown expert reward inclosed-form by solving a system of linear equations instead of performing parameter estimation as maximumentropy IRL does, which introduces approximate error (Kalweit et al., 2020). Comparing the single- andmulti-intention variants, both HIAVI and DIRL had an increase in prediction performance. Particularly,the HIAVI almost achieved the expert level as the number of trajectories increased, while the performanceimprovement for DIRL was only minor. Besides, different values of DIRL seem not to have a significantinfluence on the DIRL performance when there is enough expert trajectories in the dataset. Next we compared HIAVI and DIRL abilities in capturing the experts intention transition dynamics (Fig-ure 3b) and recovering the expert policy (c).The analysis was performed based on the learntpolicy using the whole dataset with all 1024 trajectories (with cross-validation applied). As a measure ofperformance in policy recovery, we used the expected value difference (EVD) metric (Levine et al., 2011).EVD is defined as the mean square error between the state-value under the true reward function for theexpert policy and the state-value under the true reward for the optimal Boltzmann policy w.r.t. the learntreward. It provides an estimation of the sub-optimality of the learnt policy under the true reward function.For HIAVI and DIRL with 2 intentions, the inferred intentions were assigned to the best-fit ground truthintentions, and the two learnt policy was evaluated under the corresponding true reward individually toobtain the respective state-values. Under the single-intention setup, since only one learnt policy could be",
  "goal13.96 0.235.58 0.4747.10 0.0027.63 6.552.97 0.6711.08 3.5637.97 1.29abandon45.55 0.076.39 1.8048.17 0.0045.90 0.1545.48 0.4948.15 0.0548.15 0.06": ": Results for the gridworld benchmark.(a) Comparison of HIAVI and DIRL on datasets withdifferent number of expert trajectories, represented as log-likelihood on the test dataset (mean standarderror, 5-fold cross-validation). (b) Predicted intention dynamics from HIAVI and DIRL using the outputsfrom the best cross-validation fold, represented as the posterior probability of the abandon intention andaveraged across all trajectories (mean standard error across 1024 trajectories). (c) Visualization of theground truth and learnt state-value functions from the best cross-validation fold (top), and the correspondingEVDs (bottom, mean standard error, 5-fold cross-validation) from HIAVI and DIRL. provided by the algorithms, the same output learnt policy was evaluated twice repeatedly under the differenttrue reward functions, respectively. Aligning with the results indicated by a, HIAVI had smallerEVD values than DIRL even under single-intention setup (c). Particularly, the learnt policy fromsingle-intention HIAVI exhibits a mixed characteristic of both goal and thirsty intentions but is moreclose to the former, whereas the learnt policy from single-intention DIRL deviates from both of the twointentions. This explains why HIAVI already had a better prediction log-likelihood than DIRL under thesingle-intention setup. Then by introducing the second intention, the best performance in recovering expertpolicy under both intentions was achieved by HIAVI (5.58 0.47 for goal and 6.39 1.80 for abandon).While by selecting an appropriate value, DIRL could reconstruct the goal policy at very high level (witha 2.97 0.67 EVD when = 0.1), it struggled to learn the policy under abandon intention. As a result,the HIAVI predicted posterior probability of the expert remaining in the abandon intention at differenttime steps in a single episode matched exactly with the ground truth (b), whereas DIRL seemed tooverestimate this probability at the beginning (where the expert reached the barrier for the first time), andunderestimate it after the 8th time step. To sum up, the comparison between HIAVI and DIRL on the gridworld benchmark suggests that HIAVIcan better capture a step-like intention transition dynamics and reconstruct the underlying expert policies",
  ": The labyrinth environ-ment": "The expert demonstrations in this real-world dataset were originally col-lected by Rosenberg et al. (2021) from a 127-node labyrinth navigationtask. As shown in , each black dot represents one state in the en-vironment and the subjects can select from 4 actions: left, right, reverse,and stay at each state.One of the 127 states is occupied with waterresource (the blue square), and the optimal path from the labyrinth en-trance to the water port is depicted as blue line. Two cohorts of 10 micemoved freely in dark through the labyrinth over the course of 7 hours,where one cohort of animals was under water restriction while the otherwas not. Difference in water restriction condition resulted in different an-imal behavior between these two cohorts in the environment. For modelevaluation, HIAVI and DIRL with varied intentions from 1 to 4 weretrained independently on the water-restricted and water-unrestricted ex-pert demonstration datasets (with 200 and 207 animal trajectories, respectively). 20% of the trajectoriesfrom each dataset were held out as a test set. We list the detailed information about trajectory preprocessingand model training in Appendix C.2. We first compared between HIAVI and DIRL on the trajectories collected from the group of water restrictedanimals. First of all, as the number of intentions increased, the log-likelihood value for HIAVI predictionsincreased significantly, whereas the improvement of DIRL performance was rather minor (a). This",
  "P(exploration)": ": Results for the mice reversal-learning dataset. (a) Log-likelihood (mean standard error, 5-foldcross-validation) as a function of history length of single intention HIQL. (b) Change in test set log-likelihoodas a function of the number of intentions in HIQL with h = 3, relative to the fQ-learning model (labeledF.). Each trace represents a single mouse, averaged over cross-validation. Solid black indicates the meanacross animals, and the dashed curve indicates the example mouse. (c) BIC as a function of the numberof intention in HIQL with h = 3. (d) Learnt mice policy represented with the probability of switch, win-stay, and lose-switch. Each grey curve denotes one mouse. (e) Average task performance and the predictedintention dynamics. Solid and shaded curves denote the mean and standard error across 9 animals. (f)Overall task performance (gray) and the performance under different intentions, mean standard erroracross 9 animals. (g) Relationship between the probability of the exploitation intention, 5 trials beforeblock switch and the probability of the exploration intention, 5 trials after block switch, mean standarderror across 9 animals. tired intention steers the animal back to the maze entrance (c). Correspondingly, the posteriorprobability of exploring initially dominates at the beginning of the trial but is generally surpassed by thetired intention over time (d). The inferred intention transition matrix (e) suggests thatthe exploring intention is unstable, meaning that it has a larger probability of switching to tired intentioncompared to staying at exploring. In contrast, after switching to the tired intention, animals tend to keepthis intention until the end of trial. These observations are again consistent with our intuition. Since thewater-seeking motivation for this group of water-unrestricted mice is very low, they would prefer to explorethe labyrinth, rather than search for the water port. In conclusion, the above results show that HIAVI outperforms DIRL in predicting animal behavior on thislabyrinth navigation benchmark for both two cohorts of mice, and is able to generate interpretable rewardfunctions underlying different intentions. On the other hand, the significant difference between the predictionperformance of HIAVI and DIRL indicates that the intention transition dynamics during natural decision-making behavior may be better described with a step-function, rather that a smoothly varying function asassumed by DIRL.",
  ": (a) Predicted intention dynamics for an ex-ample session. Dots and triangles indicate mice ac-tion. (b) Inferred intention transition matrix for thisexample mouse": "The inferred mice policies from HIQL represent dif-ferent strategies in the task under three intentions(d). The policy under intention 1 (exploita-tion) displays a strong preference of a win-stay andlose-switch strategy, which is the optimal policy inthis deterministic reward bandit task, i.e., stay onthe same side for the next trial if it is rewarded in thecurrent trial, and switch to the other one otherwise.On the other hand, under intention 2 (disengaged),the policy exhibits a preference for exploitation whenthe previous trial was successful, but following errortrials, it employs a random action selection, indi-cated by the ca. 0.5 probability of executing a lose-switch. Lastly, under intention 3 (exploration), thesubject consistently favors selecting the option oppo-site to the one chosen in the preceding trial, irrespec-tive of whether they had won or lost in that particular instance. As shown in an example session (a),the exploration intention predominantly occurs at the onset of a block and only lasts for a few trials, whereasthe other two intentions may persists over multiple consecutive trials. These results suggests that comparedto exploitation and disengaged, the exploration intention is not stable, which aligns well with the learnedintention transition matrix (b). Additionally, it becomes evident that error trials tend to coincidewith the trials where the posterior probability of disengaged and exploration intentions are dominant.The intention dynamics averaged across all blocks closely resembles those observed in the example session(Figures 7e). As each block begins with the animals performance at a relatively low level, there is a declinein the posterior probability associated with the exploitation intention, accompanied by an increase in theprobabilities of the other two intentions associated with suboptimal exploratory strategies. Then as thesubjects performance steadily improves, the exploitation intention progressively reasserts its dominance.In contrast to the cohorts general correct rate of 0.74 0.02, the subjects performed significantly better",
  "Conclusion": "This paper proposes the class of EM-based hierarchical inverse Q-learning (HIQL) algorithms, extending theIQL algorithm (Kalweit et al., 2020) for multi-intention IRL problems. Empirical results demonstrate thatHIQL outperforms the state-of-the-art on both synthesized and real-world datasets, and is able to produceinterpretable behavior characteristics. We also mathematically characterized typical exploration behavior ofrodents during value-based decision-making using HIQL. Compared to the state-of-the-art algorithm for characterizing animal behavior, DIRL (Ashwood et al., 2022a),the advantages of our HIQL algorithm are two fold. First, the assumptions about the underlying intentiontransition dynamics in HIQL align better with those observed in real-world behavioral experiments thatanimal and human alternate between discrete strategies during decision making (Ashwood et al., 2022b),rather than optimize their policy over a continuous time-varying reward as assumed by DIRL. Second, theinner-loop IRL problem solver adapted by HIQL, the class of IQL algorithms, were shown to have betterperformance in learning the experts unknown reward and higher computing efficiency than the maximumentropy IRL algorithm used by DIRL (Kalweit et al., 2020). As future work, we plan to relax the Markov assumption about the intention transition dynamics in HIQLto incorporate non-Markovian intention dynamics, and also extend HIQL for an unknown number of rewardfunctions (Dimitrakakis & Rothkopf, 2012; Michini & How, 2012; Surana & Srivastava, 2014). This work has been funded as part of BrainLinks-BrainTools, which is funded by the Federal Ministry ofEconomics, Science and Arts of Baden-Wrttemberg within the sustainability programme for projects of theExcellence Initiative II; as well as the Bernstein Award 2012, the Research Unit 5159 Resolving PrefrontalFlexibility (Grant DI 1908/11-1), and the Deutsche Forschungsgemeinschaft (Grants DI 1908/3-1 and DI1908/6-1) to I.D., and CRC/TRR 384 IN-Code to I.D. and J.B. Mansour Alyahyay, Gabriel Kalweit, Maria Kalweit, Golan Karvat, Julian Ammer, Artur Schneider, AhmedAdzemovic, Andreas Vlachos, Joschka Boedecker, and Ilka Diester. Mechanisms of premotor-motor cortexinteractions during goal directed behavior. bioRxiv, pp. 202301, 2023.",
  "Zoe Ashwood, Aditi Jha, and Jonathan W Pillow. Dynamic inverse reinforcement learning for characterizinganimal behavior. Advances in Neural Information Processing Systems, 35:2966329676, 2022a": "Zoe C Ashwood, Nicholas A Roy, Iris R Stone, International Brain Laboratory, Anne E Urai, Anne KChurchland, Alexandre Pouget, and Jonathan W Pillow. Mice alternate between discrete strategies duringperceptual decision-making. Nature Neuroscience, 25(2):201212, 2022b. Monica Babes, Vukosi Marivate, Kaushik Subramanian, and Michael L Littman. Apprenticeship learningabout multiple intentions.In Proceedings of the 28th International Conference on Machine Learning(ICML-11), pp. 897904, 2011.",
  "Leonard E Baum and Ted Petrie.Statistical inference for probabilistic functions of finite state Markovchains. The Annals of Mathematical Statistics, 37(6):15541563, 1966": "Celia C Beron, Shay Q Neufeld, Scott W Linderman, and Bernardo L Sabatini. Mice exhibit stochastic andefficient action switching during probabilistic decision making. Proceedings of the National Academy ofSciences, 119(15):e2113961119, 2022. Ariyan Bighashdel, Panagiotis Meletis, Pavol Jancura, and Gijs Dubbelman. Deep adaptive multi-intentioninverse reinforcement learning. In Machine Learning and Knowledge Discovery in Databases. ResearchTrack: European Conference, ECML PKDD 2021, Bilbao, Spain, September 1317, 2021, Proceedings,Part I 21, pp. 206221. Springer, 2021.",
  "Jaedeug Choi and Kee-Eung Kim.Nonparametric Bayesian inverse reinforcement learning for multiplereward functions. Advances in Neural Information Processing Systems, 25, 2012": "Antonio Coronato, Muddasar Naeem, Giuseppe De Pietro, and Giovanni Paragliola. Reinforcement learningfor intelligent healthcare applications: A survey. Artificial Intelligence in Medicine, 109:101964, 2020. Brice De La Crompe, Megan Schneck, Florian Steenbergen, Artur Schneider, and Ilka Diester. FreiBox: Aversatile open-source behavioral setup for investigating the neuronal correlates of behavioral flexibility via1-photon imaging in freely moving mice. Eneuro, 10(4), 2023. Christos Dimitrakakis and Constantin A Rothkopf. Bayesian multitask inverse reinforcement learning. In Re-cent Advances in Reinforcement Learning: 9th European Workshop, EWRL 2011, Athens, Greece, Septem-ber 9-11, 2011, Revised Selected Papers 9, pp. 273284. Springer, 2012.",
  "Sergey Levine, Zoran Popovic, and Vladlen Koltun. Nonlinear inverse reinforcement learning with Gaussianprocesses. Advances in Neural Information Processing Systems, 24, 2011": "Amarildo Likmeta, Alberto Maria Metelli, Giorgia Ramponi, Andrea Tirinzoni, Matteo Giuliani, and Mar-cello Restelli. Dealing with multiple experts and non-stationarity in inverse reinforcement learning: Anapplication to real-life problems. Machine Learning, 110:25412576, 2021. Bernard Michini and Jonathan P How. Bayesian nonparametric inverse reinforcement learning. In MachineLearning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2012, Bristol, UK,September 24-28, 2012. Proceedings, Part II 23, pp. 148163. Springer, 2012. Payam Nasernejad, Tarek Sayed, and Rushdi Alsaleh. Multiagent modeling of pedestrian-vehicle conflictsusing adversarial inverse reinforcement learning. Transportmetrica A: Transport Science, 19(3):2061081,2023.",
  ",(A.1c)": "where Ii is the indicator function with Ii(x) = 1 for all x = i and 0 otherwise. Thus maximizing J (+ | )over + (problem (6)) is equivalent to separately maximizing (A.1a) over +, (A.1b) over +, and (A.1c)over R+ =r+1 , . . . , r+K. The first two optimization problems can be formally written as:",
  "A.2Computing required posterior probabilities": "To obtain the posterior probability terms P(z0 = i | , ), P(zt1 = i, zt = j | , ), and P(zt = i | , )required for evaluating the objective functions of (7), (8) and (9), we apply the Baum-Welch algorithm (Baum& Petrie, 1966) as follows. Let t RK be the forward probability which denotes the posterior probability of observing the expertdemonstrations up until time t and the tth demonstration was generated under reward function ri, i.e.,",
  ": until (0, 0) or (4, 4) is reached": "The whole expert demonstration dataset consisted of 1024 trajectories. All evaluated algorithms were fit tomultiple sub-datasets with different number of expert trajectories, and each dataset was analyzed using a5-fold cross-validation. Note that since the EM process does not guarantee to find the global optimum, foreach training, HIAVI was randomly initialized repeatedly for 10 times and the best performed initialization(evaluated by test set log-likelihood) was selected for analysis.The initial intention distribution wasinitialized uniformly, and the intention transition matrix was initialized as: = 0.95 I + N(0, 0.05 I),where N denotes the normal distribution and I R22 is the identity matrix. This initial was thennormalized so that each row added up to 1. The discount factor of the MDP was set to be = 0.9.",
  "C.2Real-world mice navigation benchmark": "For comparability with the results in Ashwood et al. (2022a), we obtained their pre-processed mouse trajecto-ries for water-restricted and water-unrestricted animals from original recorded animal trajectories from Rosenberg et al. (2021) are provided with MIT open sourcelicense at the following repository: the pre-processing, Ashwood et al. (2022a) used a clustering algorithm (based on DBSCAN (Ester et al.,1996)) for aligning trajectories across animals and bouts to reduce variability. After the pre-processing, theyobtained 200 trajectories from the water-restricted animals and 207 trajectories from the water-unrestrictedanimals. 20% of trajectories from each cohort were held out as a test set. We used the source code and the best performed set of hyperparameters provided by Ashwood et al. (2022a) totrain DIRL on the animal trajectories. All HIAVI algorithms were trained for 10 repeated runs with differentinitializations, and the results from the initializations with the highest test set log-likelihood was selected foranalysis. The initial intention distribution was initialized uniformly, and the intention transition matrix was initialized as: = 0.95I +N(0, 0.05I), where N denotes the normal distribution and I RKK is the identity matrix. This initial was then normalized so that each row added up to 1. The MDP wasdefined to have a deterministic state transition function P and the discount factor was set to be = 0.99.",
  "C.3Application to mice reversal-learning behavior": "The expert demonstrations for this dynamic reversal-learning task was collected from a cohort of miceconsisted of 9 mice in total. Behavior recordings for each subject were repeated for at least 7 independentsessions with an average of ca. 87 trials per session. We employed a multi-stage fitting procedure to select hyperparameters and to allow us to fit HIQL indi-vidually to each animal. In the first stage, we concatenated the data from all animals in a single dataset"
}