{
  "Abstract": "Random features have been introduced to scale up kernel methods via randomization tech-niques. In particular, random Fourier features and orthogonal random features were usedto approximate the popular Gaussian kernel. Random Fourier features are built in this caseusing a random Gaussian matrix. In this work, we analyze the bias and the variance ofthe kernel approximation based on orthogonal random features which makes use of Haarorthogonal matrices. We provide explicit expressions for these quantities using normalizedBessel functions, showing that orthogonal random features does not approximate the Gaus-sian kernel but a Bessel kernel. We also derive sharp exponential bounds supporting theview that orthogonal random features are less dispersed than random Fourier features.",
  "Introduction": "Since their introduction over fifteen years ago in the seminal paper by Rahimi & Recht (2007), randomfeatures have become an important subject of research in the field of machine learning (see the review articleby Liu et al., 2021). The primary motivation behind introducing them is to reduce the computation andstorage requirements of kernel methodsone of the most popular machine learning approaches (Schlkopf& Smola, 2002; Shawe-Taylor & Cristianini, 2004; Yang et al., 2012; Le et al., 2013; Pennington et al.,2015; Chamakh et al., 2020; Han et al., 2022; Likhosherstov et al., 2022). They have also been used inover-parameterized settings and as a tool for generating and testing hypotheses on the generalization of deeplearning (Jacot et al., 2018; Belkin et al., 2019; Yehudai & Shamir, 2019; Jacot et al., 2020; Liu et al., 2022;Mei & Montanari, 2022). Another recent research focus has involved the study of the Gaussian equivalencephenomenon in the context of random feature models in order to characterize the generalization error inthe asymptotic regime. (Gerace et al., 2020; Goldt et al., 2022; Hu & Lu, 2022; Montanari & Saeed, 2022;Schrder et al., 2023; Dandi et al., 2024). Random Fourier features (RFF) are undoubtedly the most common and widely used random feature methodfor kernel approximation (Rahimi & Recht, 2007). This approach applies to radial basis function kernelsalarge class of kernel functions. It is based on Bochners theorem (Bochner, 1932; Rudin, 1962), which estab-lishes a one-to-one correspondence between continuous positive-definite functions and the Fourier transformof probability measures.In particular, if is a real positive definite function on R, i.e.the inequalityni,j=1 ij(xi xj) 0 holds for any n 1 and x1, . . . , xn, 1, . . . , n R, and if k(x, y) := (x y)",
  "j=1cos(wj (x y)),": "so that the expected value of k(x, y) fits exactly the integral displayed in the RHS of equation 1.Theimaginary part of the integral in equation 1 vanishes because the kernel function is radial. In particular, ifw1, . . . , wp are centered Gaussian vectors N(0, Id), then RFF approximate the well-known Gaussian kernel:",
  "EwN(0,Id)[k(x, y)] = exy2/2.(4)": "Orthogonal random features (ORF)1 is a variant of RFF that uses a random orthogonal matrix O insteadof the Gaussian matrix W. RFF are built out of i.i.d. random Gaussian matrices. Assuming i.i.d. featuretransformation samples may be too restrictive and may neglect structural information in the data. ORFmight be a more effective model since it takes into accounts the correlations between samples. The featuretransformation samples in ORF are no longer independent as they are drawn from the Haar measure onthe orthogonal group. ORF was first proposed by Yu et al. (2016), with the goal of approximating theGaussian kernel.They showed that imposing orthogonality on the randomly generated transformationmatrix can reduce the kernel approximation error of RFF when a Gaussian kernel is used. This has led to anumber of studies investigating the effectiveness of random orthogonal embeddings and showing empiricallyand theoretically that ORF estimators achieve more accurate kernel approximation and better predictionaccuracy than standard mechanisms based on i.i.d sampling (Choromanski et al., 2017; 2018; 2019). Thesuperiority of orthogonal against random Gaussian projections in learning with random features has alsobeen observed in Gerace et al. (2020). In this paper, we build on this line of research and provide an analytic characterization of the bias and ofthe variance of ORF using normalized Bessel functions of the first kind. These special functions appearnaturally in harmonic analysis on Euclidean spheres since they are Fourier transforms of uniform measureson these spaces (Watson, 1995). Specifically, we make the following contributions: We give explicit forms of the bias and of the variance of ORF in the case where the random orthogonalmatrix O is drawn from the Haar measure. In particular, we show that the bias of ORF is given bya Bessel kernel instead of a Gaussian one.",
  "Notation and preliminaries": "Let p, d, be positive integers such that 2 p d and take a Haar p p orthogonal matrix O. For thereaders convenience, recall that the Haar measure on the orthogonal group is the unique left and rightinvariant measure and that a Haar orthogonal matrix may be obtained using the Gram-Schmidt procedureapplied to a Gaussian matrix G. In particular, the columns (and rows) of O are uniformly distributed on thesphere (for further details see Meckes 2019, Chapter 1). It is worth noting that the value p = 1 is excludedsince trivial. If p > d, then the ORF feature map, as introduced in Choromanski et al. (2018), is builtout of independent blocks of vectors sampled from independent Haar orthogonal matrices. By linearity andstatistical independence, the computations of the bias and of the variance of the ORF estimator follows inthis case from summing those we will compute below. We denote by ORF the random features of ORF computed using equation 3 with w1, . . . , wp being columnsof O. We also use the notation RF F for the random features of RFF when w1, . . . , wp are columns of aGaussian matrix G (i.e., a random matrix whose entries are independent and centered Gaussian randomvariables).The approximate kernels obtained using ORF and RFF will be denoted by kORF (x, y) andkRF F (x, y), respectively (i.e., kORF (x, y) := ORF (x) ORF (y) and kRF F (x, y) := RF F (x) RF F (y)). Inorder to simplify the exposition and without loss of generality, we will assume throughout this paper thatthe bandwidth of the Gaussian kernel is equal to 1. In this respect and for sake of completeness, let usrecall the expressions of the bias and the variance of RFF.",
  "Proof. See Yu et al. (2016, Lemma 1)": "It is worth noting that the equality equation 5 remains valid if one replaces the Gaussian matrix G by theproduct SO, where O is a Haar orthogonal matrix and S is a diagonal matrix whose entries are independentand 2-distributed random variables with d degrees of freedom (Yu et al., 2016, Theorem 1). However, itfails when w1, . . . , wp are columns of O. We will see in the next section how the bias and the variance changeand behave in this case.",
  "with () being the Gamma function": "Sktech of proof. The proof of Theorem 2 is a routine calculation in Euclidean harmonic analysis. Looselyspeaking, it relies on the invariance under rotations of the uniform (Haar) measure on the unit sphereSd1 which allows to reduce the expectation E[kORF (x, y)] to the one-dimensional Fourier transform of thesymmetric Beta distribution whose density is proportional to",
  "The detailed proof is available in Appendix A": "Theorem 2 shows that ORF approximate the kernel defined by the normalized Bessel function of the firstkind (Watson, 1995; Shishkina & Sitnik, 2020, Chapter 1). This function is oscillating in contrast to theso-called Matern kernel given by the modified Bessel function of the second kind (see Equation 12 in Genton2001). Moreover, the absolute value of the former admits a polynomial decay to zero as its argument becomeslarge while the latter decays exponentially.",
  "where we convent that max(bd, cd) = bd when 2 d 4. The upper bound is valid up to the second zero ofjd/21": "Sketch of proof. The series expansion equation 8 is clearly sign alternating and does not help in provinginequalities for Bessel functions of the first kind by killing oscillations. For that reason, we appeal to theWeierstrass infinite product of j(d2)/2 and make use of estimates of its smallest positive zero. Though theBessel function j(d2)/2 admits infinitely many simple zeros, the estimates on the smallest one are sufficientenough for our purposes. The detailed proof is provided in Appendix B.",
  ",": "at two lower bounds of the first zero of j(d/2)1; see eqs. equation 18 and equation 19 below. Moreover,cd > bd for sufficiently large d (d 35) and offers therefore a linear growth of the interval [0, cd] comparedto d3/4 for [0, bd]. As a matter of fact, the inequalities displayed in equation 9 hold true for a large range ofz provided that d is large as well. As we shall see later from the proof of proposition equation 3, the lowerbound even holds true in a larger interval which almost reaches the first zero of j(d/2)1 as d becomes large.",
  "E[kORF (x, y)] E[kRF F (x, y)] ez2/(2d) ez2/2,z [0, max(bd, cd)],(10)": "which considerably improves Theorem 2 in Yu et al. (2016). Indeed, our upper bound decays exponentiallyfast while the one given in Yu et al. (2016) admits an exponential growth.This growth is due to thefact that the triangular inequality used in the proof there kills the oscillations of the normalized Besselfunction jd/21 and leads to the normalized modified Bessel function of the first kind which is known togrow exponentially (Watson, 1995). It is also worth mentioning that for large enough d, the differences between E[kORF (x, y)] and the exponentialbounds displayed in equation 9 become very small for large z (z d) since |j(d/2)1| has a polynomial decayto zero.",
  "where we recall the notation z = x y": "Sketch of proof. The derivation of V [kORF (x, y)] is to the best of our knowledge new. Since kORF (x, y) isthe sum of correlated random variables in opposite to the RFF kernel, one has to compute explicitly thecovariance terms which we perform by exploiting once more the invariance of the uniform measure on Sd1",
  "Published in Transactions on Machine Learning Research (09/2024)": "Thomas P. Conrads, Vincent A. Fusaro, Sally Ross, Don Johann, Vinodh Rajapakse, Ben A. Hitt, Seth M.Steinberg, Elise C. Kohn, David A. Fishman, Gordon Whitely, et al. High-resolution serum proteomicfeatures for ovarian cancer detection. Endocrine-related cancer, 11(2):163178, 2004. Yatin Dandi, Ludovic Stephan, Florent Krzakala, Bruno Loureiro, and Lenka Zdeborov. Universality lawsfor gaussian mixtures in generalized linear models. Advances in Neural Information Processing Systems,36, 2024.",
  "V [kORF (x, y)] V [kRF F (x, y)].(13)": "Sketch of proof. Since our random features are not independent, one has to focus on the covariance terms.Appealing to the infinite product representation of the spherical Bessel function j(d/2)1, it turns out thatthese terms are negative. As such, we are led to analyse the variance of a single random feature whichis proved to be less than the variance of RFF in the indicated interval. The complete proof is given inAppendix D. Proposition 6 shows that kORF is less dispersed than kRF F when the norm difference between data pointsz lies within an interval whose length is linear in the data dimension d when the latter is sufficiently large.This is in agreement with previous results (Choromanski et al., 2017; 2018; 2019) though holding in a smallz-neighborhood of zero. Another striking feature of this proposition is its independence of the number of random features p. Actually,its proof relies again on the Weierstrass infinite product of j(d/2)1 and shows in particular that the covarianceof (cos(wTi (x y), cos(wTj (x y)), i = j, is negative in the interval [0, 2 max(d, d)]. As a matter of fact,one is left with bounding from above the variance of a single mode cos(wT1 (xy)) by that of the RFF kernel.Even more, our proof shows that the inequality equation 13 remains valid only in a slightly larger intervalwhere j(d/2)1(",
  "Synthetic data results": "We generate synthetic data with dimension d=300 and varying values of the random featuresp={10, 50, 100, 150, 200, 250, 300}.The data are randomly generated from a normal distribu-tion with zero mean and unit variance.We compute Memp:=1ssl=1 kl(x, y) and Vemp:=1ssl=1(kl(x, y) Memp)2, the empirical bias and variance of kORF respectively.Each kernel kl iscomputed using a random Haar orthogonal matrix Ol, i.e., kl(x, y) =l(x) l(y) where l(x) =1psin(wl1x), . . . , sin(wlpx), cos(wl1x), . . . , cos(wlpx) and wl1, . . . , wlp are the columns of Ol.Theexperiment is repeated 10 times with different random seeds. shows the approximation errorsMemp E[kORF (x, y)] andVemp V [kORF (x, y)] for s = 50 and for different values of p. The mean andvariance of kORF (x, y), E[kORF (x, y)] and V [kORF (x, y)], are computed using the explicit closed expressionsprovided in Theorems 2 and 4. As can be seen, the mean and variance approximation errors are very small,which are in agreement with our results. shows the bias of kORF (x, y) and the bounds of Proposition 3 as a function of z = x y. Itillustrates that inequalities in equation 9 hold for any z [0, max(bd, cd)]. depicts the variance ofkORF and kRF F . It confirms that ORF has smaller variance compared to the standard RFF, as claimed inProposition 6.",
  "Real data results": "We also conduct experiments on real-world datasets to confirm our theoretical findings. The number offeature dimension and data samples for each dataset are provided in . The accuracy of the ker-nel estimation is calculated by measuring the mean squared error (MSE) between the true kernel matrixand the approximated one. compares the MSE of ORF and RFF, i.e., K K2F /n2 whereK := [k(xi, xj)]ni,j=1 is the Bessel or Gaussian kernel matrix and K is its approximation via ORF or RFF,respectively. The Gaussian kernel bandwidth is set as the average distance between all pairs of data points,i.e., =",
  "/n2ni,j=1 xi xj2. For the ORF estimator, introducing such a bandwidth is somehow artifi-": "cial since the uniform measure on the sphere may be realized as a normalized Gaussian vector (so even if westart with a Gaussian vector whose standard deviation is , this parameter disappears after normalization).The experiment is repeated five times with different random seeds. ORF often achieves lower MSE thanRFF. Note that the MSE measures the quadratic variability with respect to the empirical data between theestimator and its theoretical mean (the latter is taken with respect to the random features). In other words,the MSE corresponds to an empirical approximation of the variance. supports Proposition 6 sinceit shows that for the same number of feature p, the approximation error of the kernel function (i.e., empiricalvariance) in the ORF setting is smaller than the one in the RFF setting. 3Ionosphere data from the UCI machine learning repository: cancer data (Conrads et al., 2004): data is a data set of direct bank marketing campaigns via phone calls (Pang et al., 2019): #numerical-datasets.6Backdoor attack detection data extracted from the UNSW-NB 15 dataset (Moustafa & Slay, 2015): #numerical-datasets.",
  "Conclusion": "In this paper, we provided explicit closed expressions of the bias and of the variance of ORF by means ofnormalized Bessel functions of the first kind. We also derived exponential bounds that improve previouslyknown ones. In particular, we proved that the variance of ORF is less than the one of RFF when the normdifference between data points lies in an interval of length O(d), d being the data dimension.",
  "Marc G. Genton.Classes of kernels for machine learning: a statistics perspective. Journal of machinelearning research, 2:299312, 2001": "Federica Gerace, Bruno Loureiro, Florent Krzakala, Marc Mzard, and Lenka Zdeborov. Generalisationerror in learning with random features and the hidden manifold model. In International Conference onMachine Learning, pp. 34523462, 2020. Sebastian Goldt, Bruno Loureiro, Galen Reeves, Florent Krzakala, Marc Mzard, and Lenka Zdeborov.The gaussian equivalence of generative models for learning with shallow neural networks. In Mathematicaland Scientific Machine Learning, pp. 426471, 2022.",
  "Quoc Le, Tams Sarls, Alex Smola, et al. Fastfood-approximating kernel expansions in loglinear time. InInternational Conference on Machine Learning, pp. 244252, 2013": "Valerii Likhosherstov, Krzysztof Choromanski, Kumar Avinava Dubey, Frederick Liu, Tamas Sarlos, andAdrian Weller. Chefs random tables: Non-trigonometric random features. Advances in Neural InformationProcessing Systems, 35:3455934573, 2022. Fanghui Liu, Xiaolin Huang, Yudong Chen, and Johan Suykens. Random features for kernel approxima-tion: A survey on algorithms, theory, and beyond. IEEE Transactions on Pattern Analysis and MachineIntelligence, 44(10):71287148, 2021.",
  "a2d,m z2 = 1/2": "But the LHS of the last equality is obviously increasing in the z-variable and tends to + as z ad,1. As aresult, the first Rayleigh sum equation 15 (giving the value of the above series at z = 0) implies the existenceof one and only one solution z0(d) to the equation hd(z) = 0 in (0, ad,1). Therefore,"
}