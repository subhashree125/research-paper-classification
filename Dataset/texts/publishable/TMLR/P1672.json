{
  "Abstract": "Weight normalization (WeightNorm) is widely used in practice for the training of deep neuralnetworks and modern deep learning libraries have built-in implementations of it. In this paper,we provide the first theoretical characterizations of both optimization and generalization ofdeep WeightNorm models with smooth activation functions. For optimization, from the formof the Hessian of the loss, we note that a small Hessian of the predictor leads to a tractableanalysis. Thus, we bound the spectral norm of the Hessian of WeightNorm networks andshow its dependence on the network width and weight normalization termsthe latter beingunique to networks without WeightNorm. Then, we use this bound to establish trainingconvergence guarantees under suitable assumptions for gradient decent. For generalization,we use WeightNorm to get a uniform convergence based generalization bound, which isindependent from the width and depends sublinearly on the depth. Finally, we presentexperimental results which illustrate how the normalization terms and other quantities oftheoretical interest relate to the training of WeightNorm networks.",
  "Introduction": "Weight normalization (WeightNorm) was first introduced by Salimans and Kingma (2016). The idea is tonormalize each weight vectorby dividing it by its Euclidean normin order to decouple its length fromits direction. Salimans and Kingma (2016) reported that WeightNorm is able to speed-up the training ofneural networks, as an alternative to the then recently introduced and still widely-used batch normalization(BatchNorm) (Ioffe and Szegedy, 2015). WeightNorm is different from BatchNorm because it is independentfrom the statistics of the batch sample being used at the gradient step. It is also different from anotherclass of normalization, called layer normalization (LayerNorm) (Ba et al., 2016) in that it does not entail thenormalization of any activation function of the neural network. Thus WeightNorm also has less computationaloverhead because it does not require the computation and storage of additional mean and standard deviationstatistics as the other methods do. However, its empirical testing performance (or accuracy) has often beenfound to not be as good as other normalization methods by itself, and thus various versions have beenformulated to improve its performance (Salimans and Kingma, 2016; Huang et al., 2017; Qiao et al., 2020).Nevertheless, WeightNorm has been one of the most popular normalization methods since its introduction,and as a result current machine learning libraries include built-in implementations of it, e.g., PyTorch 2.0.",
  "Published in Transactions on Machine Learning Research (1/2025)": "The first statement in Assumption 3 ensures that gradient descent has an appropriate learning rate. Thesecond statement defines a ball around the current iterate that should cover the next iteratein principle,its radius could be defined after defining the learning rate of GD of the first assumption; more details inRemark 5.7. Finally, we present our main optimization result: a reduction on the empirical loss towards its minimumvalue (with the last layer in the Euclidean set of radius 1) using gradient descent. This is proved by usingLemmas 5.1 and 5.2 and an adaptation of (Banerjee et al., 2023, Theorem 5.3); see Section C.2 of theappendix.",
  "Related work": "WeightNorm and Other NormalizationsWeightNorm was first proposed by Salimans and Kingma(2016), after a year from the introduction of BatchNorm by Ioffe and Szegedy (2015). LayerNorm was proposedin the same year by Ba et al. (2016), and has also become very popular (Xu et al., 2019). Extensions andvariations of WeightNorm have been introduced since then, for example, centered weight normalization (Huanget al., 2017), orthogonal weight normalization (Huang et al., 2018), and weight standardization (Qiao et al.,",
  "Problem setup: deep learning with WeightNorm": "Consider a training set {(xi, yi)}ni=1, xi X Rd, yi Y R. For a suitable loss function , the goalis to minimize the empirical loss: L() =1nni=1 (yi, yi) =1nni=1 (yi, f(; xi)), where the predictionyi := f(; xi) is from a deep neural network with parameter vector Rp for some positive integer p.In our setting f is a feed-forward multi-layer (fully-connected) neural network with weight normalization(WeightNorm), having depth L and hidden layers with widths ml, l [L] := {1, . . . , L}; and it is given by",
  "k=1 mkmk1+mL ,(2)": "with m0 = d. The pre-activation function (l) Rml has its i-th entry defined by (l)i (x) = ((l)i (x)),l [L]. For simplicity, we assume that the width of all hidden layers is the same, i.e., ml = m, l [L], and so Rdm+(L1)m2+m. We remark that f is also called the predictor since it is the predictive output of theneural network.",
  "Assumption 2 (Ouput layer and input data). We initialize the vector of the output layer v0 Rm as aunit vector, i.e., v02 = 1. Further, we assume the input data satisfies: xi2 = 1, i [n]": "The assumption x2 = 1 is for convenient scaling. Normalization assumptions are common in the litera-ture (Allen-Zhu et al., 2019; Oymak and Soltanolkotabi, 2020; Nguyen et al., 2021). The unit value for thelast layers weight norm at initialization is also for convenience, since our results hold under appropriatescaling for any other constant in O(1).",
  ".(5)": "Remark 4.1 (Comparison to networks without WeightNorm). The recent works (Liu et al., 2020;Banerjee et al., 2023) analyzed the Hessian spectral norm bound for feedforward networks without WeightNorm.(Liu et al., 2020) presented an exponential dependence on the networks depth L, whereas (Banerjee et al.,2023) avoided exponential dependence upon the choice of the initialization variance for the hidden layersweights. We show that WeightNorm eliminates such exponential dependence for at most L5 independentfrom any initialization of the weights, which improves to L3 when (0) = 0. Another important difference isthat our bound also depends on the inverse of the minimum weight vector normlarge enough values on theweight vectors decrease the upper bound. Finally, our bound is defined over any value on the weights, andnot just over a neighborhood around the initialization point.Remark 4.2 (Dependence on the activation function). Corollary 4.1 shows that the Hessian boundbecomes tighter when (0) = 0 because of an additional1m scaling. In contrast, the Hessian bounds by Liuet al. (2020); Banerjee et al. (2023) have the scaling1m independent from the value of (0). The reason forthis difference is the use of different scale factors: while those two other works introduce an extra scale factor1m on the linear output layer, we do not. In our paper, we follow the neural network scaling as done in theseminal work on the Neural Tangent Kernel by Jacot et al. (2018).",
  "Optimization guarantees for WeightNorm": "In this section we prove our training guarantees for WeightNorm networks under the square loss. We firstintroduce two technical lemmas that will be used for our convergence analysis, defining the restricted strongconvexity (RSC) property and a smoothness-like property respectively. These two properties use the boundsderived in . All proofs are found in Section C of the appendix.",
  "condition has the form L()22": "L() 2, where , depends on the parameters and , which are in turnrestricted to a specific set. This is a milder condition than the so-called restricted PL condition (Oymak andSoltanolkotabi, 2019; Banerjee et al., 2023), which is satisfied whenever , is a constant independent fromboth and . Our RSC condition becomes even milder when (0) = 0 (see Corollary 5.1), since now theparameter , has the term 1/m which decreases with the width.",
  "t(1 t)(L(t) L()) .(18)": "The result in (18) implies training convergence as the number of iterations increase. Since the rate at whichthe loss decreases is time- and weight-dependent in (18), it is difficult to establish the number of steps neededfor convergence to a specific loss value; however, this shows that our framework may cover cases where theconvergence rate is heterogeneous across iterations. Finally, we note that the rate is always guaranteed to beless than one. Remark 5.5 (Important differences with respect to networks without WeightNorm). Theorem 5.1differentiates from (Banerjee et al., 2023, Theorem 5.3) in that (i) it is deterministic, i.e., it is not statedas holding with high probability; (ii) does not have all the weights of the network defined over a particularneighborhood (with the exception of the output linear layer). Indeed, these differences also hold for all thepreviously derived results in and . We summarize the comparison between our resultsand those for networks without WeightNorm by Banerjee et al. (2023) in from Section D of theappendix.",
  ". Note that 2 as in Assumption 3": "could in principle depend on t, and thus we could set 2 to be equal to the upper bound just shown fort+1 t2. Remarkably, t as chosen in Theorem 5.1 can be upper bounded by one1 and if Wt2 has auniform lower bound across t (note that our experiments in provide evidence for Wt2 W02),then the upper bound of t+1 t2 becomes a constant independent of t, and so does 2 to satisfy (A3.2)in Assumption 3. Remark 5.7 (About gradient descent in Assumption 3). Regarding (A3.1) in Assumption 3, weremark that we are only considering that the weight vector of the output layer is inside a neighborhood of itsinitialization value across iterationsno constraint exist on the weights from the hidden layers. Moreover,the radius 1 of this neighborhood region can be set to be any arbitrary constant of order (1) or even apolynomial of L; thus, 1 is not assumed to be arbitrarily close to zero and the neighborhood is not restrictedto be arbitrarily small. These arguments imply that our optimization guarantees are not restricted to be inthe lazy training regime (Liu et al., 2020). We also point out that (A3.1) is a necessary assumption so thatthe linear output of the neural network, which has no normalization, remains bounded. Finally, we remarkthat (A3.1) is not stronger than other existing works where assumptions of closeness around initializationpoints are needed even on the hidden weights; e.g., see (Liu et al., 2020; 2022; Banerjee et al., 2023).",
  "Generalization guarantees for WeightNorm": "We establish generalization bounds for WeightNorm networks. Our bound is based on a uniform convergenceargument by bounding the Rademacher complexity of functions of the form (1) as long as the last layervector v stays within a ball of radius 1the same condition we used in our optimization analysis. The coreof the analysis relies on the use of a contraction-like property across the network layers similar to (Golowichet al., 2018) and uses WeightNorm and the networks scaling to avoid exponential dependence on the depthand avoid any dependence on the width. All proofs are found in Section E of the appendix.",
  "i=1(yi, f(; xi))andLD() := E(X,Y )D[(Y, f(; X))]": "We added the subscript S to the empirical loss notation to explicitely denote its dependence on the trainingset S.Theorem 6.1 (Generalization Bound for WeightNorm under Square Loss). Consider the square loss and the training set S = {(xi, yi)i.i.d. D, i [n]} and |y| 1 for any y Dy with probability one. UnderAssumptions 1 and 2, assuming the activation function satisfies (0) = 0, with probability at least (1 )over the choice of the training data xi Dx, i [n], for any WeightNorm network f(; ) of the form (1) withany fixed Rp and v BEuc1 (v0), we have",
  "log(2/)n.(19)": "Remark 6.1 (About the activation function). Theorem 6.1, unlike our previous results in the paper,requires the smooth activation function to satisfy (0) = 0. As mentioned earlier in Remarks 4.2 and 5.3,the case when (0) = 0 improves our Hessian bounds and thus improves our optimization conditions byintroducing a better dependence on the width m. We point out that commonly used activation functionssuch as the hyperbolic tangent function tanh and Gaussian Error Linear Unit (GELU) satisfy both (0) = 0and Assumption 1.Remark 6.2 (The independence from the width m and the dependence on depth L). Thegeneralizaton bound (19) does not explicitly depend on the networks width, but explicitly depends on itsdepth. Setting 1 = (1), a choice which does not negatively affect our optimization results, results in a",
  "generalization bound of O(": "Ln). Effectively, for deeper networks, we would need to increase the numberof samples at least linearly on the depth to improve our generalization boundin practice, the number ofsamples is usually much larger than the networks depth. This is a benign dependence that WeightNorm allowscompared to an exponential one found in networks without WeightNorm (Bartlett et al., 2017; Neyshaburet al., 2018; Golowich et al., 2018).",
  "Experimental results": "We show empirical results to support our theory. In our experiments, the WeightNorm networks are fullyconnected with two hidden layers of equal width m, tanh activation function, and linear output layer. We doempirical evaluations on CIFAR-10 (Krizhevsky, 2009) and MNIST (Deng, 2012). Because of computationalefficiency, we apply mini-batch stochastic gradient descent (SGD) with batch size 512 to optimize theWeightNorm networks under mean squared loss. In our experiments we are only concerned about the trainingof WeightNorm networks from an optimization perspectivethe fact that the training error minimizes acrossepochs is enough for our purposes. Therefore, using a squared loss with a linear output in our experimentssufficesas opposed to cross-entropy loss with a softmax output layer. Our experiments were conducted on acomputing cluster with AMD EPYC 7713 64-Core Processor and NVIDIA A100 Tensor Core GPU. In our theory, the convergence rate as in Theorem 5.1 is directly proportional to the RSC parameter t,t,and it benefits from a larger positive t,t. According to equation (13), t,t becomes larger under two",
  "We provide an affirmative answer in our simulations. We present our experiments on CIFAR-10 in and on MNIST in": "We first focus on the effect of the ratio L(t)22/L(t) on the RSC parameter. In (a), we showthat the values for the minimum weight vector stay nearly constant across epochs for both networks (inreality, they increase in value very slowly), i.e., the second term in the RSC parameter t (see equation (13))basically remains stable. Therefore, according to our theory, the convergence speed should mostly dependon the ratio L(t)22/L(t), i.e., the first term in the RSC parameter t (see equation (13)). Indeed,this is expressed in (b): the curve of the ratio L(t)22/L(t) has a decreasing trend towards theend of training for both networks (starting at around epoch 75) while the curve of the loss ratio betweentwo consecutive epochs L(t+1)/L(t) has an increasing trend. Therefore, since an increase in the ratioL(t+1)/L(t) corresponds to a decrease on convergence speed, both networks have a slower convergence thanthey had at the initial epochs. Next, we focus on the effect of the minimum weight vector norm on the RSC parameter. According to ourtheory, if the two networks in have the same value for the ratio L(t)22/L(t), the network withthe larger minimum weight vector norm will have a larger RSC parameter (see equation (13)), and so wewould expect such network to have a faster convergence. Indeed, we observe this in our experiment: whenthe red solid curve crosses the dashed red curve (around epochs 70 and 115) in (b), the two networkshave the same value for L(t)22/L(t). At this crossing, (b) shows that the network with largerwidthand so with larger minimum weight vector normconverges faster afterwards (i.e., the loss ratioL(t+1)/L(t) for the green dashed curve has smaller values than the green solid curve). We present similar results on the MNIST dataset in . For example, similar to , the minimumweight vector norms for both networks in are practically stable, which results in the convergencespeed changing according to the changes in the ratio L(t)22/L(t). Interestingly, the two curves of theratio L(t)22/L(t) do not cross, but they become very close on value around epoch 25. After this, we",
  "Conclusion": "We formally analyzed the training and generalization of deep neural networks with WeightNorm under smoothactivations. For this we needed to characterize a series of bounds which highlighted the dependence of ourresults on the depth, width, and minimum weight vector norm. Our simulations showed the possibilitythat our training guarantees hold in practice. We hope our paper will elicit further work on the theoreticalfoundation for other types of normalization whose connection to training and generalization are not wellexplored. Part of the work was done when Pedro Cisneros-Velarde was affiliated with the University of Illinois Urbana-Champaign and was concluded during his current affiliation with VMware Research. The work was supportedin part by the National Science Foundation (NSF) through awards IIS 21-31335, OAC 21-30835, DBI 20-21898,as well as a C3.ai research award. Sanmi Koyejo acknowledges support by NSF 2046795 and 2205329, NIFAaward 2020-67021-32799, the Alfred P. Sloan Foundation, and Google Inc.",
  "Pytorch 2.0. Pytorch 2.0 documentation. Accessed: 05-09-2023": "Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song.A convergence theory for deep learning via over-parameterization. In Proceedings of the 36th International Conference on Machine Learning, volume 97 ofProceedings of Machine Learning Research, pages 242252. PMLR, 2019. Devansh Arpit, Vctor Campos, and Yoshua Bengio. How to initialize your network? robust initializationfor weightnorm & resnets. In Advances in Neural Information Processing Systems, volume 32. CurranAssociates, Inc., 2019.",
  "Li Deng. The mnist database of handwritten digit images for machine learning research. IEEE SignalProcessing Magazine, 29(6):141142, 2012": "Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global minima ofdeep neural networks. In Proceedings of the 36th International Conference on Machine Learning, volume 97of Proceedings of Machine Learning Research, pages 16751685. PMLR, 0915 Jun 2019. Yonatan Dukler, Quanquan Gu, and Guido Montufar. Optimization theory for ReLU neural networks trainedwith normalization layers. In Proceedings of the 37th International Conference on Machine Learning,volume 119 of Proceedings of Machine Learning Research, pages 27512760, 2020.",
  "Jiaoyang Huang and Horng-Tzer Yau. Dynamics of deep neural networks and neural tangent hierarchy. InInternational conference on machine learning, pages 45424551. PMLR, 2020": "Lei Huang, Xianglong Liu, Yang Liu, Bo Lang, and Dacheng Tao.Centered weight normalization inaccelerating training of deep neural networks. In Proceedings of the IEEE International Conference onComputer Vision (ICCV), Oct 2017. Lei Huang, Xianglong Liu, Bo Lang, Adams Yu, Yongliang Wang, and Bo Li. Orthogonal weight normalization:Solution to optimization over multiple dependent stiefel manifolds in deep neural networks. Proceedings ofthe AAAI Conference on Artificial Intelligence, 32(1), Apr. 2018. doi: 10.1609/aaai.v32i1.11768. Lei Huang, Jie Qin, Yi Zhou, Fan Zhu, Li Liu, and Ling Shao. Normalization techniques in training dnns:Methodology, analysis and application. IEEE Transactions on Pattern Analysis and Machine Intelligence,pages 120, 2023. doi: 10.1109/TPAMI.2023.3250241. Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducinginternal covariate shift. In Proceedings of the 32nd International Conference on Machine Learning, volume 37of Proceedings of Machine Learning Research, pages 448456. PMLR, 2015. Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and generalizationin neural networks. In Advances in Neural Information Processing Systems, volume 31. Curran Associates,Inc., 2018.",
  "Michel Ledoux and Michel Talagrand. Probability in Banach Spaces. Classics in Mathematics. Springer Berlin,Heidelberg, 1991. doi: 10.1007/978-3-642-20212-4": "Zhiyuan Li, Tianhao Wang, and Dingli Yu. Fast mixing of stochastic gradient descent with normalization andweight decay. In Advances in Neural Information Processing Systems, volume 35, pages 92339248, 2022. Xiangru Lian and Ji Liu. Revisit batch normalization: New understanding and refinement via compositionoptimization. In Proceedings of the Twenty-Second International Conference on Artificial Intelligence andStatistics, volume 89 of Proceedings of Machine Learning Research, pages 32543263. PMLR, 1618 Apr2019.",
  "Chaoyue Liu, Libin Zhu, and Misha Belkin. On the linearity of large non-linear models: when and why thetangent kernel is constant. In Advances in Neural Information Processing Systems, 2020": "Depen Morwani and Harish G. Ramaswamy. Inductive bias of gradient descent for weight normalized smoothhomogeneous neural nets. In Proceedings of The 33rd International Conference on Algorithmic LearningTheory, volume 167 of Proceedings of Machine Learning Research, pages 827880, 2022. Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. Norm-based capacity control in neural networks.In Peter Grnwald, Elad Hazan, and Satyen Kale, editors, Proceedings of The 28th Conference on LearningTheory, volume 40 of Proceedings of Machine Learning Research, pages 13761401, Paris, France, 0306 Jul2015. PMLR. Behnam Neyshabur, Srinadh Bhojanapalli, and Nathan Srebro. A PAC-bayesian approach to spectrally-normalized margin bounds for neural networks. In International Conference on Learning Representations,2018.",
  "1m + (l 1)|(0)|.(27)": "Proof. Note that the parameter vector w(l) = vec(W (l)) and can be indexed with j [m] and j [d] whenl = 1 and j [m] when l 2. We will do the analysis for l 2 since a similar analysis holds for the casel = 1. Thus, we have",
  ",": "where (a) follows from the fact that i and i have the same (symmetrical) distribution; (b) from g beingmonotonically increasing; (c) from (Ledoux and Talagrand, 1991, equation (4.20)) which makes use of gbeing convex and increasing, and of being 1-Lipschitz and (0) = 0; and (d) follows from the increasingmonotonicity of g.",
  "We first state the following auxiliary technical result, which is based on (Golowich et al., 2018, Lemma 1).We let A(l) := {(W (1), W (2), . . . , W (l)) Rmd Rmm Rmm}": "Lemma E.1. Under Assumptions 1 and 2, assuming the activation function satisfies (0) = 0, consider anyWeightNorm network f(; ) of the form (1) with any fixed Rp and v BEuc1 (v0), along with some inputdata set {xi}ni=1. Consider also any convex and monotonically increasing function g : R [0, ). For any",
  "n.(48)": "Now, consider a given loss function so that, for a given constant y such that |y| 1, it computes (y, f(x)).If y (y, y) has a Lipschitz constant of , then by using the Rademacher contraction lemma (e.g., anadaptation from (Shalev-Shwartz and Ben-David, 2014, Lemma 26.9)), the uniform convergence result can beextended to the loss function: with probability at least (1 ) over the draw of the samples, we have",
  "nni=1 (yi, f(xi)), and sup fF|y|1|(y, f())| B": "Now we put everything according to the context of our setting. We let F be the set of all WeightNormnetworks f(; ) of the form (1) with any fixed Rp and v BEuc1 (v0) under Assumptions 1 and 2, andassuming the activation function satisfies (0) = 0. We also set LD() := LD[f(; )] andLS() := LS[f(; )]."
}