{
  "Abstract": "We consider the problem of answering observational, interventional, and counterfactual queries in acausally sufficient setting where only observational data and the causal graph are available. Utilizingthe recent developments in diffusion models, we introduce diffusion-based causal models (DCM)to learn causal mechanisms, that generate unique latent encodings. These encodings enable us todirectly sample under interventions and perform abduction for counterfactuals. Diffusion modelsare a natural fit here, since they can encode each node to a latent representation that acts as a proxyfor exogenous noise. Our empirical evaluations demonstrate significant improvements over existingstate-of-the-art methods for answering causal queries. Furthermore, we provide theoretical resultsthat offer a methodology for analyzing counterfactual estimation in general encoder-decoder models,which could be useful in settings beyond our proposed approach.",
  "Introduction": "Understanding the causal relationships in complex problems is crucial for making analyses, conclusions, and gen-eralized predictions. To achieve this, we require causal models and queries. Structural Causal Models (SCMs) aregenerative models describing the causal relationships between variables, allowing for observational, interventional,and counterfactual queries (Pearl, 2009a). An SCM specifies how a set of endogenous (observed) random variablesis generated from a set of exogenous (unobserved) random variables with prior distribution via a set of structuralequations. Given a causal DAG, we focus on approximating the individual SCMs with diffusion models. As an application, wepresent a flexible framework for answering all the three types of causal (observational, interventional, and counterfac-tual) queries. For counterfactuals, we work in Pearls SCM framework (Pearl, 2009a), and seek to quantify unit-levelstatements of the form: Given that observed a factual sample (xF1 , . . . , xFK) for a set of K variables (X1, . . . , Xk),what would have been the outcome for these K variables be, if the value of some set XI (with I [K]) had beenset to some R|I|? Throughout this paper we assume causal sufficiency, i.e., absence of hidden confounders. Notethat causal sufficiency is a necessary assumption for answering causal queries from observational data alone. In the SCM framework, causal queries can be answered by learning a proxy for the unobserved exogenous noise andthe structural equations. This suggests that (conditional) generative models that encode to a latent space could be anattractive choice for the modeling SCMs, as the latent serves as the proxy for the exogenous noise. In these models,",
  "the encoding process extracts the latent from an observation, and the decoding process generates the sample from thelatent, approximating the structural equations": "Our Contributions. In this work, we propose and analyze the effectiveness of using a diffusion model for modelingSCMs. Diffusion models (Sohl-Dickstein et al., 2015; Ho et al., 2020; Song et al., 2021) have gained popularityrecently due to their high expressivity and exceptional performance in generative tasks (Saharia et al., 2022; Rameshet al., 2022; Kong et al., 2021). The primary contribution of our work is in determining how to apply diffusionmodels to the causal setting. Rather than focusing a single causal inference setting, our aim through this modelingis to provide a single flexible framework that works for answering a wide range of causal queries. Now applyingdiffusion models to the causal setting is non-trivial because diffusion models are typically used to learn a stochasticprocess mapping between a standard Gaussian and a data distribution. Our main idea is to model each node in thecausal graph as a diffusion model and cascade generated samples in topological order to answer causal queries. Foreach node, the corresponding diffusion model takes as input the node and parent values to encode and decode alatent representation. To implement the diffusion model, we utilize the recently proposed Denoising Diffusion ImplicitModels (DDIMs) (Song et al., 2021), which may be interpreted as a deterministic autoencoder model without anydimensionality reduction while encoding. We leverage the deterministic forward and backward diffusion processesof DDIMs to use diffusion models as an encoder-decoder model. We refer to the resulting model as diffusion-basedcausal model (DCM) and show that this model mimics the necessary properties of an SCM. Our key contributionsinclude: (1) [] We propose diffusion-based causal model (DCM), a new model class for modeling structural causalmodels, that provides a flexible and practical framework for approximating both interventions (do-operator) andcounterfactuals (abduction-action-prediction steps). We present a procedure for training a DCM given just thecausal graph and observational data, and show that the resulting trained model enables sampling from the observa-tional and interventional distributions, and facilitates answering counterfactual queries. (2) [] Our theoretical analysis examines the accuracy of counterfactual estimates generated by the DCM, andwe demonstrate that they can be bounded given some reasonable assumptions. Importantly, our analysis is notlimited to diffusion models, but also applies to other encoder-decoder settings. To the best of our knowledge, theseare the first error bounds that explain the observed performance improvements in using encoder-decoder models,like diffusion models, to address counterfactual queries. Another feature of this result, is that it also extends, underan additional assumption, to the more challenging multivariate case. (3) [] We evaluate the performance of DCM on a range of synthetic datasets generated with various structuralequation types for all three forms of causal queries. We find that DCM consistently outperforms existing state-of-the-art methods (Snchez-Martin et al., 2022; Khemakhem et al., 2021). In fact, for certain interventional andcounterfactual queries such as those arising with nonadditive noise models, DCM is better by an order of magnitudeor more than these existing approaches. Additionally, we demonstrate the favorable performance of DCM on aninterventional query experiment conducted on fMRI data. Related Work. Over the years, a variety of methods have been developed in the causal inference literature for answer-ing interventional and/or counterfactual queries including non-parametric methods (Shalit et al., 2017; Alaa & VanDer Schaar, 2017; Muandet et al., 2021) and probabilistic modeling methods (Zecevic et al., 2021a). More relevant toour approach is a recent series of work, including (Moraffah et al., 2020; Pawlowski et al., 2020; Kocaoglu et al., 2018;Parafita & Vitri, 2020; Zecevic et al., 2021b; Garrido et al., 2021; Karimi et al., 2020; Snchez-Martin et al., 2022;Khemakhem et al., 2021; Sanchez & Tsaftaris, 2022) that have demonstrated the success of using deep (conditional)generative models for this task. Karimi et al. (2020) propose an approach for answering interventional queries by fitting a conditional variationalautoencoder to each conditional in the Markov factorization implied by the causal graph. Also using the ideas ofvariational inference and normalizing flows, Pawlowski et al. (2020) propose schemes for counterfactual inference. In Khemakhem et al. (2021), the authors propose an autoregressive normalizing flow for causal discovery and queries,referred to as CAREFL. CAREFL is also applicable even with only knowledge of the causal ordering rather than the fullcausal graph as we require. However, as also noted by Snchez-Martin et al. (2022), when the causal graph is present,CAREFL is unable to exploit the absence of edges fully as it reduces a causal graph to its causal ordering (which maynot be unique). Using normalizing flows for answering causal queries with causal DAG was also explored by (Balgiet al., 2022), however their approach does not have any theoretical guarantees on their counterfactual estimates, andtheir experimental evaluation is quite limited.",
  "Published in Transactions on Machine Learning Research (11/2024)": "Counterfactual Estimation. For a factual observation xF and interventions on nodes I with values , the counterfac-tual estimate only differs from the factual estimate for all nodes that are intervened or downstream from an intervenednode. We proceed in topological order. For each intervened node i, xCFi:= i. For each non-intervened node idownstream from an intervened node, define uFi := xFi fi(xFpai), the residual and estimated noise for the factualsample. Let xCFpai be counterfactual estimates of the parents of Xi. Then xCFi:= fi(xCFpai) + uFi . Therefore, for counterfactual queries, if the true functional equation fi is an additive noise model, then if fi fi, theregression model will have low counterfactual error. In fact, if fi fi, then the regression model will have perfectcounterfactual performance.",
  "Preliminaries": "Notation. To distinguish random variables from their instantiation, we represent the former with capital letters andthe latter with the corresponding lowercase letters. To distinguish between the nodes in the causal graph and diffusionrandom variables, we use subscripts to denote graph nodes. Let [n] := {1, . . . , n}. Structural Causal Models. Consider a directed acyclic graph (DAG) G with nodes {1, . . . , K} in a topologicallysorted order, where a node i is represented by a (random) variable Xi in some generic space Xi Rdi. Let pai bethe parents of node i in G and let Xpai := {Xj}jpai be the variables of the parents of node i. A structural causalmodel M describes the relationship between an observed/endogenous node i and its causal parents. Formally, anSCM M := (F, p(U)) determines how a set of K endogenous random variables X := {X1, . . . , XK} is generatedfrom a set of exogenous random variables U := {U1, . . . , UK} with prior distribution p(U) via a set of structuralequations, F := (f1, . . . , fK) where Xi := fi(Xpai, Ui) for i [K]. Throughout this paper, we assume thatthe unobserved random variables are jointly independent (Markovian SCM), and the DAG G is the graph inducedby M. Every SCM M entails a unique joint observational distribution satisfying the causal Markov assumption:p(X) = Ki=1 p(Xi | Xpai). Structural causal models address Pearls causal hierarchy (or ladder of causation), which consists of three layersof causal queries in increasing complexity (Pearl, 2009a): observational (or associational), interventional, and coun-terfactual. As an example, an interventional query can be formulated as What will be the effect on the populationX, if a variable Xi is assigned a fixed value i? The do-operator do(Xi := i) represents the effect of settingvariable Xi to i. Note that our proposed framework allows for more general sets of interventions as well, such asinterventions on multiple variables denoted as do(XI := ) (where I [K], XI := (Xi)iI, R|I|). An inter-vention operation, do(XI := ), transforms the original joint distribution into an interventional distribution denoted",
  "works well empirically. We also utilize this loss function in our training": "Song et al. (2021) demonstrate that it is possible to take a pretrained standard denoising diffusion probabilistic model(DDPM) and generalize the generation to non-Markovian processes. In particular, it is possible to use a pretrainedDDPM model to obtain a deterministic sample given noise XT , known as the denoising diffusion implicit model(DDIM), with reverse implicit diffusion process",
  "t+1(1 t)/t.(3)": "We utilize the DDIM framework in this work, in particular Eqs. 3 and 2 will define the encoding (forward) anddecoding (reverse) processes. Note that this ensures deterministic encoding and decoding. This construction producesa unique latent variable per observation, as well as a unique decoding, and also ensures we obtain the same output forrepeated counterfactual queries.",
  "DCMs: Diffusion-based Causal Models": "In this section, we present our DCM approach for modeling the SCMs and to answer causal queries. The DCMapproach falls in a general class of techniques that try to model a structural causal model by using an encoder-decoderpair. Consider a data generating process X = f(Xpa, U). The goal will to construct an encoding function g anda decoding function h. The encoding function g attempts to represent the information in U: for a pair (X, Xpa),Z := g(X, Xpa) is the latent variable. The decoder takes the input Z and Xpa as input to attempt to reconstruct X:X = h(Z, Xpa), where under perfect reconstruction, X = X. The decoding function h mimics the true structural",
  "Define Xti to be ith endogenous node value at diffusion step t of the reverse implicit diffusion process (Eq. 2), andlet Xi := X0i": "Training a DCM. We train a diffusion model for each node, taking denoised parent values as input. The parentvalues can be interpreted as additional covariates to the model, where one may choose to use classifier free guidanceto incorporate the covariates (Ho & Salimans, 2021). Empirically, we find that simply concatenating the covariatesresults in better performance than classifier free guidance. We use the parametrization for the diffusion model from Ho et al. (2020), representing the diffusion model fornode i as i(X, Xpai, t). The complete training procedure presented in Algorithm 1 is only slightly modified from theusual training procedure, with the additions of the parents as covariates and training a diffusion model for each node.Since the generative models learned for generation of different endogenous nodes do not affect training of each other,these models may be trained in parallel. For each node in the graph, we can train a model in parallel as each diffusionmodel only requires the current node and parent values. Our final DCM model is just a combination of these K traineddiffusion models 1, . . . , K .",
  ": end while": "We use all the variables (X1, . . . , XK) for the training procedure, because a priori, we do not assume anything on thepossible causal queries, i.e., we allow for all possible target variables, intervened variables, etc. However, if we areonly interested in some pre-defined set of queries, then the graph could be reduced accordingly. For example, if weare only interested in counterfactual estimate of a particular node with respect to an intervention of a predecessor, onecan simply reduce it to a subgraph containing the target node, the intervened node and a backdoor adjustment set (e.g.,the ancestors of the intervened node). This then reduces to only learning a single diffusion model. One major advantage of our proposed DCM approach is the ability to generalize to larger graphs. Since each diffusionmodel only uses the parents as input, modeling each node depends only on the incoming degree of the node (thenumber of causal parents). While the number of diffusion models scales with the number of non-root nodes, eachmodel is generally small in terms of its parameter size and can be trained in parallel. Additionally, we may apply theproposed DCM approach to any setting where diffusion models are applicable: continuous variables, high dimensionalsettings, categorical data, images, etc. Encoding and Decoding Steps with DCM. With a DCM, the encoding (resp. decoding) process is identical to theDDIM encoding (resp. decoding) process except we include the parent values as additional covariates. Note that,given the model , DDIM is a deterministic process as laid out in Eqs. 2 and 3. Let us focus on a node i [K] (sameprocess is repeated for each node i). The encoding process takes Xi and its parent values Xpai as input and maps itto a latent variable Zi. The decoding process takes Zi and Xpai as input to construct Xi (an approximation of Xi).Formally, using the forward implicit diffusion process in Eq. 3, given a sample Xi, we encode a unique latent variable",
  ": Return Xi := X0i": "Answering Causal Queries with a Trained DCM. We now describe how a trained DCM model can be used for(approximately) answering causal queries. Answering observational and interventional queries require sampling fromthe observational and the interventional distribution respectively. With counterfactuals, a query is at the unit level,where the structural assignments are changed, but the exogenous noise is identical to that of the observed datum. (a) Generating Samples for Observational/Interventional Queries. Samples from a DCM model that approximatesthe interventional distribution p(X | do(XI := )) can be generated as follows. For an intervened node i withintervention i, the sampled value is always the intervention value, therefore we generate Xi := i. For a non-intervened node i, assume by induction we have the generated parent values Xpai. To generate Xi, we first samplethe latent vector Zi N(0, Idi) where di is the dimension of Xi. Then taking Zi as the noise for node i, we computeXi := Deci(Zi, Xpai) as the generated sample value for node i. This value Xi is then used as the parent value forthe children of node i. Samples from a DCM model that approximates the observational distribution p(X) can begenerated by setting I = . See Algorithm 4 for the pseudocode. (b) Counterfactual Queries. Consider a factual observation xF := (xF1 , . . . , xFK) and interventions on a set of nodesI with values . We use a DCM model to construct a counterfactual estimate xCF as follows. The counterfactualestimate only differs from the factual value on intervened nodes or descendants of an intervened node. Similarly tointerventional queries, for each intervened node i I, xCFi:= i. For each non-intervened node i that is a descendantof any intervened node, assume by induction that we have the generated counterfactual estimates xCFpai. To obtain xCFi,",
  ": Return xCF := (xCF1 , . . . , xCFK )": "we first define the estimated factual noise as zFi := Enci(xFi , xFpai). Then we generate our counterfactual estimate byusing zFi as the noise for node i, by decoding, xCFi:= Deci(zFi , xCFpai). See Algorithm 5 for the pseudocode. Note that with xF, we assumed full observability,1 since because Algorithm 5 produces a counterfactual estimate foreach node. However, when intervening on XI and if the only quantity of interest is counterfactual on some X, thenyou only need factual samples from {Xi : Xi is on a path from XI X} (Saha & Garain, 2022). In practice, thiscould be further relaxed by imputing for missing data, which is beyond the scope of this work.",
  "Bounding Counterfactual Error": "We now establish sufficient conditions under which the counterfactual estimation error can be bounded. In fact, theresults in this section not only hold for diffusion models, but to a more general setting of conditional latent variablemodels satisfying certain properties. Another feature of this result, is that it also extends, under an additional assump-tion, to the more challenging higher-dimensional case. All proofs from this section are collected in Appendix A. We focus on learning a single conditional latent variable model for an endogenous node Xi, given its parents Xpai,as the models learned for different endogenous nodes do not affect each other. Since the choice of node i plays norole, we drop the subscript i in the following and refer to the node of interest as X, its causal parents as Xpa, itscorresponding exogenous variables as U, and its structural equation as X := f(Xpa, U). Let the encoding functiong : X Xpa Z and the decoding function h : Z Xpa X, where Z is the latent space. In the DCM context, thefunctions g and h correspond to Enc and Dec functions, respectively. It is well-known that certain counterfactual queries are not identifiable from observational data without making as-sumptions on the functional relationships, even under causal sufficiency (Pearl, 2009b). Consequently, recent researchhas been directed towards understanding the conditions under which identifiability results can be obtained (Lu et al.,",
  "Discussion on Assumptions Underlying Theorem 1": "(1) Assumption 1 of independence between the encoding and the parent values may appear strong, but is in fact oftenvalid. For example, in the additive noise setting with f(Xpa, U) := f (Xpa)+U where Xpa and U are independent, ifthe fitted model f f , then the encoder g(X, Xpa) = f(Xpa, U) f(Xpa) = U and by definition U is independentof Xpa.2 The same assumption also appears in other related results on counterfactual identifiability in bijective SCMs,see, e.g., (Nasr-Esfahany et al., 2023, Theorem 5.3) and proof of Theorem 5 in (Nasr-Esfahany & Kiciman, 2023).We conduct empirical tests to further confirm this assumption by examining the dependence between the parentsand encoding values. Our experimental results show that DCMs consistently fail to reject the null hypothesis ofindependence. This implies that independent encodings can be found in practice. We provide the details of theseexperiments in Appendix B.3. (2) Assumption 2 is always satisfied under the additive noise model, where f(Xpa, U) := f (Xpa) + U. It is alsosatisfied by post non-linear models, under the standard requirements on identifiability as expressed in (Zhang & Hy-varinen, 2012).4 Assumption 2 is also satisfied by heteroscedastic noise models (Strobl & Lasko, 2023).5 Again, therecent results about counterfactual identifiability, e.g., (Nasr-Esfahany & Kiciman, 2023, Theorem 5) and (Lu et al.,2020, Theorem 1), also utilize the same assumption. Through our strictly increasing in U assumption, we obviate distinguishing cases like f(Xpa, U) := Xpa + U vs.f(Xpa, U) := Xpa U, which otherwise will be indistinguishable for symmetric distributions U (see also remarkbelow). For example, for any fixed value of Xpa, f(Xpa, U) := Xpa U is not increasing in U, so such structuralequations are automatically eliminated in Theorem 1. In other words, among these two cases, this assumption onlypermits the additive noise form f(Xpa, U) := Xpa + U. 2In general, if we have a good approximation f by some f, then the encoding g(X, Xpa) = X f(Xpa) would be close to U, as also notedby (Hoyer et al., 2008).3To encourage independence, one could also modify the original diffusion model training objective to add a Hilbert-Schmidt independencecriterion (HSIC) (Gretton et al., 2007) regularization term. Our experiments did not show a clear benefit of using this modified objective, and weleave further investigation here for future work.4Zhang & Hyvarinen (2012) (see Eqn. 2) defined post-nonlinear models as f(Xpa, U) := f2(f1(Xpa)+U) where Xpa and U are independent,function f1 is nonconstant, and f2 is invertible. The invertibility assumption on f2 implies that f is strictly increasing (or decreasing) in U.Additionally, (Zhang & Hyvarinen, 2012) make a differentiability of f2 assumption (see Assumption A1) for identifiability, which implies the f isdifferentiable in U. 5Heteroscedastic noise models are generally defined as f(Xpa, U) := f(Xpa) + g(Xpa) U where the function g is assumed to be strictlypositive (see Definition 1 in (Strobl & Lasko, 2023), which makes it compatible with our Assumption 2 of Theorem 1.",
  "We now discuss some consequences of Theorem 1 for estimating counterfactual outcomes": "1. Perfect Estimation. Using Theorem 1, we now look at a condition under which the counterfactual estimateproduced by the encoder-decoder model matches the true counterfactual outcome.6. The idea here been, if no in-formation is lost in the encoding and decoding steps, i.e., h(g(X, Xpa), Xpa) = X, and assuming Theorem 1(g(X, Xpa) = q(U)), we have h(q(U), Xpa) = X = f(Xpa, q1(U)). This means that in the abduction step,the encoder-decoder model could recover q(U), but in the prediction step, it first applies the inverse of q to the recov-ered exogenous variable, and then f. Thus, the counterfactual estimate equals the true counterfactual outcome. Weformalize this in Corollary 1.Corollary 1. Assume the conditions of Theorem 1. Furthermore, assume the encoder-decoder model pair (g, h)satisfies: h(g(X, Xpa), Xpa) = X. Consider a factual sample pair xF := (x, xpa) where x := f(xpa, u) and an in-tervention do(Xpa := ). Then, the counterfactual estimate, given by h(g(x, xpa), ) matches the true counterfactualoutcome xCF := f(, u). Comparison with Recent Related Work. Recent studies by Nasr-Esfahany & Kiciman (2023) and Nasr-Esfahanyet al. (2023) have explored the problem of estimating counterfactual outcomes with learned SCMs. In particular, Nasr-Esfahany & Kiciman (2023, Theorem 5) consider a setting where the SCM X := f(Xpa, U) is learned with a bijective(deep conditional generative) model f(Xpa, U). Nasr-Esfahany et al. (2023, Theorem 5.3) considered a closely relatedproblem of learning a ground-truth bijective SCM. The conditions underlying ours and these results are not directlycomparable because, unlike our setup, they do not explicitly consider an encoder-decoder model. Our results provideprecise conditions on the encoder g and decoder h for recovering the correct counterfactual outcome, and we canextend these results to obtain counterfactual estimation error bounds under relaxed assumptions, a problem that hasnot been addressed previously. Counterfactual identifiability in a different context of reinforcement learning was also established by (Lu et al., 2020).Their result relies on incomparable assumptions on state-action pairs. Furthermore, our proof techniques are quitedifferent from Lu et al. (2020) who rely on a technique based on analyzing conditional quantile, unlike an algebraictechnique employed here. 2. Estimation Error. Another consequence of Theorem 1 is that it can bound the counterfactual error in terms of thereconstruction error of the encoder-decoder model. Informally, the following corollary shows that if the reconstructionh(g(X, Xpa), Xpa) is close to X (measured under some metric d(, )), then such encoder-decoder models canprovide good counterfactual estimates. To the best of our knowledge, this is the first result that establishes a boundon the counterfactual error in relation to the reconstruction error of these encoder-decoder models. Corollary 2. Let 0. Assume the conditions of Theorem 1. Furthermore, assume the encoder-decoder modelpair (g, h) under some metric d (e.g., 2), has reconstruction error less than : d(h(g(X, Xpa), Xpa), X) .Consider a factual sample pair xF := (x, xpa) where x := f(xpa, u) and an intervention do(Xpa := ). Then, theerror between the true counterfactual xCF := f(, u) and counterfactual estimate given by h(g(x, xpa), ) is at most. In other words, d(h(g(x, xpa), ), xCF) . The above result suggests that the reconstruction error can serve as an estimate for the counterfactual error. Whilethe true value of is unknown, we may compute a reasonable bound by computing the reconstruction error over thedataset.",
  "Extension of Theorem 1 to Higher-Dimensional Setting": "In this section, we present an extension of Theorem 1 to a higher dimensional setting and use it to provide coun-terfactual identifiability and estimation error results. Since we are now dealing with vector-valued functions, we useJacobians. Let Jfxpa denote the Jacobian matrix obtained by evaluating the Jacobian (with respect to U) of f(Xpa, U)at Xpa = xpa. Let Jgxpa denote the Jacobian matrix obtained by evaluating the Jacobian (with respect to X) ofg(X, Xpa) at Xpa = xpa.",
  "Then, g(f(Xpa, U), Xpa) = q(U) for an invertible function q": "The interpretations behind Assumptions 1, 2, and 3 in Theorem 2 are similar to corresponding assumptions in The-orem 1. Assumption 4 however is technical, and we will explain the need for it below. In Corollaries 3 and 4(Appendix A), we restate Corollaries 1 and 2 to this higher-dimensional setting. The proofs are identical to that ofCorollaries 1 and 2, with the only change being that the role of Theorem 1 in those proofs is now replaced by Theo-rem 2. On Negative Result of Nasr-Esfahany & Kiciman (2023). Nasr-Esfahany & Kiciman (2023) presented a gen-eral counterfactual impossibility identification result under multidimensional exogenous noise. The construction inNasr-Esfahany & Kiciman (2023) considers two structural equations f, f that are indistinguishable in distribution.Formally, let R Rmm be a rotation matrix, and U Rm be a standard (isotropic) Gaussian random vector. Define,",
  "f(Xpa, U)for Xpa Af(Xpa, R U)for Xpa B ,(8)": "where the domain Xpa is split into disjoint A and B. Now, f and f generate different counterfactual outcomes, forcounterfactual queries with evidence in A and intervention in B (or the other way around). In Theorem 2, we avoid this impossibility result by assuming that we can construct an encoding of a special kindcaptured through our Assumption 4. In particular, consider the encoding qxpa at a specific parent value xpa as afunction of the exogenous noise U. The assumption states that the Jacobian of the encoding is equal to c(xpa)Afor a scalar function c and orthogonal matrix A. However, it is important to acknowledge that this assumption ishighly restrictive and difficult to verify, not to mention challenging to construct in practice with just observationaldata. Our intention is that these initial ideas can serve as a starting point for addressing the impossibility result, withthe expectation that subsequent results will further refine and expand upon these ideas.",
  "Experimental Evaluation": "In this section, we evaluate the empirical performance of DCM for answering causal queries on both synthetic and realworld data. Our primary objective through these experiments is not to tackle a specific causal inference problem, butto demonstrate that our DCM approach provides a unified and flexible framework for answering a wide range of obser-vational, interventional, and counterfactual queries. For the real data experiments, we consider an interventional dataexperiment on fMRI data (.2) and an experimental study for the problem of estimating individual treatmenteffects (Appendix E).7",
  "Synthetic Data Experiments": "For generating quantitative results, we use synthetic experiments since we know the exact structural equations, andhence we have access to the ground-truth observational, interventional, and counterfactual distributions. We presentresults on two larger graphs here, and defer results on a set of four smaller graphs to Appendix D.2. In Appendix D.3,we also present results by generating synthetic data based on the real world graph from the Sachs dataset (Sachs et al.,2005). The two graphs, considered here, include a ladder graph structure (see ) and a randomly generated graphstructure. Both graphs are comprised of 10 nodes of three dimensions each, and the random graph is a randomlysampled directed acyclic graph (see Appendix C.3 for more details). Since each diffusion model only uses the parentsas input, modeling each node depends only on the incoming degree of the node (the number of causal parents). Following Snchez-Martin et al. (2022), for the observational and interventional distributions, we report the Maxi-mum Mean Discrepancy (MMD) (Gretton et al., 2012) between the true and estimated distributions. For counter-factual estimation, we report the mean squared error (MSE) of the true and estimated counterfactual values. Againfollowing Snchez-Martin et al. (2022), we consider two broad classes of structural equations:",
  ". Nonadditive Noise Model (NADD): fi(Xpai, Ui) is an arbitrary function of Xpai and Ui": "To prevent overfitting of hyperparameters, we randomly generate these structural equations for each initialization.Each structural equation is comprised a neural network with a single hidden layer of 16 units and SiLU activation(Elfwing et al., 2018) with random weights from sampled from . Each simulation generates n = 5000 samples as training data. Let M be a fitted causal model and M be the truecausal model, both capable of generating observational and interventional samples, and answering counterfactualqueries. Each pair of graphs and structural equation type is evaluated for 20 different initialization, and we reportthe mean value. We provide additional details about our observational, interventional, and counterfactual evaluationframeworks in Appendix C.4. Synthetic Experiments Results. In , we provide the performance of all evaluated models for observational,interventional, and counterfactual queries, averaged over 20 separate initializations of models and training data, withthe lowest value in each row bolded. The values are multiplied by 100 for clarity. We also provide boxplots of theperformances in (Appendix D). We see DCM and ANM are the most competitive approaches, with similar performance on observational and inter-ventional queries. If the ANM is the correctly specified model, then the ANM encoding should be close to the true",
  "CF. MSE20.1357.5244.7686.02124.82275.0954.2983.68": ": Meanstandard deviation of observational, interventional, and counterfactual queries of the ladder and ran-dom SCMs in nonlinear and nonadditive settings over 20 random initializations of the model and training data. Thevalues are multiplied by 100 for clarity. encoding, assuming the regression model fit the data well. We see this in the nonlinear setting, ANM performs wellbut struggles to outperform DCM, perhaps due to the complexity of fitting a neural network using classical models.Note thats since observational and interventional queries are inherently easier than counterfactual queries, it is naturalthat we observe smaller improvements over other baselines. Our proposed DCM method exhibits superior performance compared to VACA and CAREFL, often by as much asan order of magnitude. The better performance of DCM over CAREFL may be attributed to the fact that DCM usesthe causal graph, while CAREFL only relies on a non-unique causal ordering. In the case of VACA, the limitedexpressiveness of the GNN encoder-decoder might be the reason behind its inferior performance, especially whendealing with multivariable, multidimensional complex structural equations as considered here, a shortcoming thathas also been acknowledged by the authors of VACA (Snchez-Martin et al., 2022). Furthermore, VACA performsapproximate inference, e.g. when performing a counterfactual query with do(X1 = 2), the predicted counterfactualvalue for X1 is not exactly 2 due to imperfections in the reconstruction. This design choice may result in downstreamcompounding errors, possibly explaining discrepancies in performance. To avoid penalizing this feature, all metricsare computed using downstream nodes from intervened nodes. To evaluate computational efficiency, we comparedDCM with VACA and CAREFL using a single experimental setup (see Appendix D.1). The results suggest that DCMis significantly more computationally efficient than both VACA and CAREFL, with training taking 7 times less time.Lastly, the standard deviation of DCM is small relative to the other models, demonstrating relative consistency, whichpoints to the robustness of our proposed approach.",
  "Real Data Experiments I": "We evaluate DCM on interventional real world data by evaluating our model on the electrical stimulation interven-tional fMRI data from (Thompson et al., 2020), using the experimental setup from (Khemakhem et al., 2021). ThefMRI data comprises samples from 14 patients with medically refractory epilepsy, with time series of the CingulateGyrus (CG) and Heschls Gyrus (HG). The assumed underlying causal structure is the bivarate graph CG HG. Ourinterventional ground truth data comprises an intervened value of CG and an observed sample of HG. We defer thereader to (Thompson et al., 2020; Khemakhem et al., 2021) for a more thorough discussion of the dataset.",
  "DCM0.5981 9.5e-30.5779 1.9e-3CAREFL0.5983 3.0e-20.6004 2.2e-2ANM0.6746 4.8e-80.6498 1.4e-6Linear SCM0.6045 00.6042 0": ": Performances for interventional predictions on the fMRI dataset, of the form median/mean absoluteerrorstandard deviation, using the mean over 10 random seeds. We do not include VACA due to implementationdifficulties. The results for CAREFL, ANM, and Linear SCM are consistent with those observed by (Khemakhemet al., 2021). We note that the Linear SCM has zero standard deviation, as the ridge regression model does not varywith the random seed. In , we note that the difference in performance is more minor than our synthetic results. We believe this is dueto two reasons. Firstly, the data seems inherently close to linear, as exhibited by the relatively similar performancewith the standard ridge regression model (Linear SCM). Secondly, we only have a single ground truth interventionalvalue instead of multiple samples from the interventional distribution. As a result, we can only compute the absoluteerror based on this single value, rather than evaluating the maximum mean discrepancy (MMD) between the trueand predicted interventional distributions. Specifically, in the above table, we compute the absolute error between themodel prediction and the interventional sample for each of the 14 patients and report the mean/median. The availabilityof a single interventional introduces a possibly large amount of irreducible error, artificially inflating the error values.For more details on the error inflation, see Appendix C.5.",
  "Concluding Remarks": "We demonstrate that diffusion models, in particular, the DDIM formulation (which allows for unique encoding anddecoding) provide a flexible and practical framework for approximating interventions (do-operator) and counterfactual(abduction-action-prediction) steps. Our approach, DCM, is applicable independent of the DAG structure. We findthat empirically DCM outperforms competing methods in all three causal settings, observational, interventional, andcounterfactual queries, across various classes of structural equations and graphs. While not in scope of this paper, the proposed DCM approach can also be naturally extended to any setting wherediffusion models are applicable like categorical data, images, etc. For higher dimensional spaces, we believe DCMsshould scale nicely, as diffusion models are typically deployed for high-dimensional image settings and exhibit SOTAperformance. Furthermore, we may leverage many of the optimization and implementation tricks, as diffusion modelsare a very active field of research. The proposed method does come with certain limitations. For example, as with all the previously mentioned relatedapproaches, DCM precludes unobserved confounding. The theoretical analyses require assumptions, not all of whichare easy to test. However, our practical results suggest that DCM provides competitive empirical performance, evenwhen some assumptions needed for our theoretical guarantees are violated.",
  "Adrin Javaloy, Pablo Snchez-Martn, and Isabel Valera. Causal normalizing flows: from theory to practice, 2023": "Amir-Hossein Karimi, Julius Von Kgelgen, Bernhard Schlkopf, and Isabel Valera. Algorithmic recourse underimperfect causal knowledge: a probabilistic approach. Advances in neural information processing systems, 33:265277, 2020. Ilyes Khemakhem, Ricardo Monti, Robert Leech, and Aapo Hyvarinen. Causal autoregressive flows. In ArindamBanerjee and Kenji Fukumizu (eds.), Proceedings of The 24th International Conference on Artificial Intelligenceand Statistics, volume 130 of Proceedings of Machine Learning Research, pp. 35203528. PMLR, 1315 Apr 2021.URL Murat Kocaoglu, Christopher Snyder, Alexandros G Dimakis, and Sriram Vishwanath. Causalgan: Learning causalimplicit generative models with adversarial training. In International Conference on Learning Representations,2018.",
  "Eric V Strobl and Thomas A Lasko. Identifying patient-specific root causes with the heteroscedastic noise model.Journal of Computational Science, 72:102099, 2023": "Pablo Snchez-Martin, Miriam Rateike, and Isabel Valera. Vaca: Designing variational graph autoencoders for causalqueries. Proceedings of the AAAI Conference on Artificial Intelligence, 36(7):81598168, Jun. 2022. doi: 10.1609/aaai.v36i7.20789. URL W. H. Thompson, R. Nair, H. Oya, O. Esteban, J. M. Shine, C. I. Petkov, R. A. Poldrack, M. Howard, and R. Adolphs.A data resource from concurrent intracranial stimulation and functional MRI of the human brain. Scientific Data,7(1):258, August 2020. ISSN 2052-4463. doi: 10.1038/s41597-020-00595-y. URL Matej Zecevic, Devendra Dhami, Athresh Karanam, Sriraam Natarajan, and Kristian Kersting. Interventional sum-product networks: Causal inference with tractable probabilistic models. Advances in Neural Information ProcessingSystems, 34:1501915031, 2021a.",
  "AMissing Details from": "Notation. For two sets X, Y a map f : X Y, and a set S X, we define f(S) = {f(x) : x S}. For x X,we define x + S = {x + x : x X}. For a random variable X, define pX(x) as the probability density function(PDF) at x. We use p.d. to denote positive definite matrices and Jf|x to denote the Jacobian of f evaluated at x. Fora function with two inputs f(, ), we define fx(Y ) := f(x, Y ) and fy(X) := f(X, y).",
  "and we have the desired representation by choosing q = qxpa0": "Theorem 1. Assume for X X R and exogenous noise U Unif, X satisfies the structural equation:X := f(Xpa, U), where Xpa Xpa Rd are the parents of node X and U Xpa. Consider an encoder-decodermodel with encoding function g : X Xpa Z and decoding function h : Z Xpa X, Z := g(X, Xpa),X :=h(Z, Xpa). Assume the following conditions:",
  "Proof. First, we show that g(X, Xpa) = g(f(Xpa, U), Xpa) is solely a function of U": "Since continuity and invertibility imply strict monotonicity, without loss of generality, assume g is an strictly increasingfunction (if not, we may replace g with g and use h(Z, Xpa)). By properties of the composition of functions,qxpa(U) := g(f(xpa, U), xpa) is also differentiable and strictly increasing with respect to U. Also, because of strictmonotonicity it is also invertible.",
  "for an invertible function q. This completes the proof": "Corollary 1. Assume the conditions of Theorem 1. Furthermore, assume the encoder-decoder model pair (g, h)satisfies: h(g(X, Xpa), Xpa) = X. Consider a factual sample pair xF := (x, xpa) where x := f(xpa, u) and an in-tervention do(Xpa := ). Then, the counterfactual estimate, given by h(g(x, xpa), ) matches the true counterfactualoutcome xCF := f(, u). Proof. For the intervention do(Xpa := ), the true counterfactual outcome is xCF := f(, u). By assumption,h(g(xCF, xpa), xpa) = xCF. Now since Eq. 13 holds true for all Xpa and U, it also holds for the factual and counter-factual samples. We have,",
  "This completes the proof": "Corollary 2. Let 0. Assume the conditions of Theorem 1. Furthermore, assume the encoder-decoder modelpair (g, h) under some metric d (e.g., 2), has reconstruction error less than : d(h(g(X, Xpa), Xpa), X) .Consider a factual sample pair xF := (x, xpa) where x := f(xpa, u) and an intervention do(Xpa := ). Then, theerror between the true counterfactual xCF := f(, u) and counterfactual estimate given by h(g(x, xpa), ) is at most. In other words, d(h(g(x, xpa), ), xCF) .",
  "AA1xpa": "If = 2, choosing different values of k, implying different values of z, results in varying values of on the right handside, which should be the constant identity matrix. Therefore we must have = 0. This also implies that xpa = cand A = Axpa. This gives the further parametrization",
  "BTesting Independence between Parents and Encodings": "We empirically evaluate the dependence between the encoding and parent values. We consider a bivariate nonlinearSCM X1 X2 where X2 = f(X1, U2) = X21 + U2 and X1 and U2 are independently sampled from a standardnormal distribution. We evaluate the HSIC between X1 and the encoding of X2. We fit our model on n = 5000samples and evaluate the HSIC score on 1000 test samples from the same distribution. We compute a p-value usinga kernel based independence test and compare our performance to ANM, a correctly specified model in this setting(Gretton et al., 2007). We perform this experiment 100 times. Given true independence, we expect the p-values tofollow a uniform distribution. In the table below, we show some summary statistics of the p-values from the 100trials, with the last row representing the expected values with true uniform p-values (which happens under the nullhypothesis). We provide p-values from the correctly specified ANM approach which are close to a uniform distribution, demon-strating that it is possible to have encodings that are close to independent. Although the p-values produced by our DCMapproach are not completely uniform, the encodings do not to consistently reject the null hypothesis of independence.",
  "T 1 + 104": "for t [T]. To incorporate the parents values and time step t, we simply concatenate the parent values and t/T asinput to the model. We found that using the popular cosine schedule (Nichol & Dhariwal, 2021) resulted in worseempirical performance, as well as using a positional encoding for the time t. We believe the drop in performance fromthe positional encoding is due to the low dimensionality of the problem since the dimension of the positional encoderwould dominate the dimension of the other inputs. We also evaluated using classifier-free guidance (CFG) (Ho & Salimans, 2021) to improve the reliance on the parentvalues, however, we found this also decreased performance. We provide a plausible explanation that can be explainedthrough Theorem 1. With Theorem 1, we would like our encoding g(Y, X) to be independent of X, however using aCFG encoding (1 + w)g(Y, X) wg(Y, 0) would only serve to increase the dependence of g(Y, X) on X, which iscounterproductive to our objective. For VACA, we use the default implementation9, training for 500 epochs, with a learning rate of 0.005, and the encoderand decoder have hidden dimensions of size and respectively, a latent vector dimension of 4, and a parentdropout rate of 0.2. For CAREFL, we also use the default implementation10 with the neural spline autoregressive flows (Durkan et al.,2019), training for 500 epochs with a learning rate of 0.005, four flows, and ten hidden units. For ANM, we also use the default implementation to select a regression model. Given a set of fitted regressionmodels, the ANM chooses the model with the lowest root mean squared error averaged over splits of the data. TheANM considers the following regressor models: linear, ridge, LASSO, elastic net, random forest, histogram gradientboosting, support vector, extra trees, k-NN, and AdaBoost.",
  "C.2Details about the Additive Noise Model (ANM)": "For a given node Xi with parents Xpai, consider fitting a regression model fi where fi(Xpai) Xi. Using thisregression model and the training dataset is sufficient for generating samples from the observational and interventionaldistribution, as well as computing counterfactuals. Observational/Interventional Samples. Samples are constructed in topological order. For intervened nodes, thesampled value is always the intervened value. Non-intervened root nodes in the SCM are sampled from the empiricaldistribution of the training set. A new sample for Xi is generated by sampling the parent value Xpai inductively andsampling U from the empirical residual distribution, and outputting Xi := fi(Xpai) + U.",
  "C.3Details about Random Graph Generation": "The random graph is comprised of ten nodes. We randomly sample this graph by generating a random upper tri-angular adjacency matrix where each entry in the upper triangular half is each to 1 with probability 30%. We thencheck that this graph is comprised of a single connected component (if not, we resample the graph). For a graphicalrepresentation, we provide an example in .",
  "C.4Query Evaluation Frameworks for Synthetic Data Experiments": "Observational Evaluation. We generate 1000 samples from both the fitted and true observational distribution andreport the MMD between the two. Since DCM and the ANM use the empirical distribution for root nodes, we onlytake the MMD between nonroot nodes. Interventional Evaluation. We consider interventions of individual nodes. For an intervention node i, we choose 20intervention values 1, . . . , 20, linearly interpolating between the 10% and 90% quantiles of the marginal distributionof node i to represent realistic interventions. Then for each intervention do(Xi := j), we generate 100 values fromthe fitted model and true causal model, X and X for the samples from the fitted model and true model respectively.Since the intervention only affects the descendants of node i, we subset X and X to include only the descendantsof node i, and compute the MMD on X and X to obtain a distance i,j between the interventional distribution forthe specific node and interventional value. Lastly, we report the mean MMD over all 20 intervention values and allintervened nodes. For the ladder graph, we choose to intervene on X2 and X3 as these are the farthest nodes upstreamand capture the maximum difficulty of the intervention. For the random graph, we randomly select three non-sinknodes to intervene on. A formal description of our interventional evaluation framework is given in Algorithm 6.",
  ": end for9: Output mean of all i,j": "Counterfactual Evaluation. Similarly to interventional evaluation, we consider interventions of individual nodes andfor node i, we choose 20 intervention values 1, . . . , 20, linearly interpolating between the 10% and 90% quantilesof the marginal distribution of node i to represent realistic interventions. Then for each intervention do(Xi := j),we generate 100 nonintervened factual samples xF, and query for the estimated and true counterfactual values xCF and xCF respectively. Similarly to before, xCF and xCF only differ on the descendants of node i, therefore we onlyconsider the subset of the descendants of node i. We compute the MSE i,j, since the counterfactual estimate andground truth are point values, giving us an error for a specific node and interventional value. Lastly, we report themean MSE over all 20 intervention values and all intervened nodes. We use the same intervention nodes as in theinterventional evaluation mentioned above. A formal description of our counterfactual evaluation framework is givenin Algorithm 7 (Appendix C).",
  "mincEY1N(,1)(Y1 c)2 = minc2 + 1 2c + c2": "The optimal estimator is the mean and achieves a squared error of 1. Of course in practice we do not know , ifwe were to use the sample mean of X1, . . . , Xn, we would have an error of the order 1 + 1/n. While we may stillcompare various estimators, e.g. sample mean, sample median, deep neural network, all the losses will be inflated by1, causing the difference in performances to seem much more minute. Our synthetic experiments hope to directly estimate the interventional distribution and computes the MMD betweensamples from the true and models distributions, implying they are of the first statistical problem. The fMRI real dataexperiments aim to estimate a single intervention value from a distribution, implying they are of the second statisticalproblem. We should not expect these results to be very small, and the metric values should all be shifted by an intrinsicirreducible error.",
  "D.1Running Time Experiments": "We present the training times in minutes for one seed on the ladder graph using the default implementation andparameters. For a fair comparison, these are all evaluated on a CPU. Note that ANM is the fastest as it uses standardregression models, and our proposed DCM approach is about 7-9x faster than CAREFL and VACA. The generation(inference) times for all the methods are in the order of 1 second. For VACA and CAREFL, we use the implementationprovided by the respective authors.",
  "D.2Additional Synthetic Experiments": "In this section, we present additional experimental evidence showcasing the superior performance of DCM in address-ing various types of causal queries. We consider four additional smaller graph structures, which we call the chain,triangle, diamond, and Y graphs (see ). The exact equations are presented in . These functional equations were chosen to balance the signal-to-noiseratio of the covariates and noise to represent realistic settings. Furthermore, these structural equations were chosenafter hyperparameter selection, meaning we did not tune DCMs parameters nor tune the structural equations afterobserving the performance of the models.",
  "E Var [f(Xpa, U) | U] 0.5": "For all graphs, Uiiid N(0, 1), and we choose f such that Xi = Ui if Xi is a root node, i.e. f is the identity function.Lastly, we normalize every node Xi such that Var (Xi) 1. For the sake of clarity, we omit all normalizing terms inthe formulas and omit functional equations for root nodes below. Results. For these 4 graph structures (chain, triangle, diamond, and Y), in , we provide the performance of allevaluated models for observational, interventional, and counterfactual queries, averaged over 10 separate initializations",
  "of models and training data, with the lowest value in each row bolded. The values are multiplied by 100 for clarity. In, we show the box plots for the same set of experiments": "The results here are similar to those observed with the larger graph structures in . DCM has the lowest errorin 7 out of 12 of the nonlinear settings, with the correctly specified ANM having the lowest error in the remaining5. Furthermore, DCM and ANM both typically have a lower standard deviation compared to the other competingmethods. For the nonadditive settings, DCM demonstrates the lowest values for all 12 causal queries.",
  "intricate network of signaling pathways within human T cells. The 11 nodes within this graph each correspond to oneof the phosphorylated proteins or phospholipids that were examined in their study": "For our experiment, we sample data in a semi-synthetic manner. For the root nodes, we sample from the empiricalmarginal distribution. For non-root nodes, since the ground truth structural equations are unknown, we use a randomneural network as the structural equation, as was done in . We report the performances in . We seethat the performance improvements with our DCM approach corroborate our prior findings.",
  "EReal Data Experiment II": "In order to evaluate our DCM approach, we assess its performance in computing the Individual Treatment Effect (ITE),which is defined as Di := Yi(1) Yi(0) where Yi(0) R is the potential outcome of unit i when i is assigned tothe control group, and Yi(1) is the potential outcome when i is assigned to the treatment group. Note that we do notobserve Di for any unit i as for each unit in the training data set, we observe either the outcome under control or theoutcome under treatment, but never both. Now, in non-simulated data, it is impossible to know the counterfactual ground-truth as this event did not happen.However, averaging the ITE over all individuals, i.e., E[Y (1) Y (0)], provides us with an estimate of the averagetreatment effect (ATE), for which we have real-world datasets with known ground-truth. Therefore, to evaluate ourapproach, we first estimate the ITE for each sample by estimating each corresponding counterfactual outcome. Wethen average these to obtain the ATE, which we can then compare with the ground-truth ATE. Since our techniquesare designed for constructing unit-level counterfactuals, and not ATE directly, the goal here is not to compare againstother ATE estimation approaches, but rather demonstrate that the individual treatment effects computed through ourcounterfactuals are reasonably accurate. The following experiments are based on popular datasets. Each experiment was repeated 10 times using the samemodel hyperparameters for each problem, without fine-tuning to the specific problem. For each dataset, we use theground-truth graph structure from the literature and assign a DCM model to each non-root node. For root nodes, wecan directly use the empirical distribution without the need to fit a particular model. Infant Health and Development Program Dataset. The dataset aims at predicting the effect of specialized childcareon cognitive test scores of infants (Hill, 2011). The goal here is to see if a treatment (specialized childcare) improvesthe cognitive abilities of infants compared to infants that did not receive the treatment. The average treatment effecthere is the difference between the expected cognitive test score under do(specialized child care = 1) and expectedcognitive test score under do(specialized child care = 0). The ground-truth ATE (on cognitive test score) associatedwith this dataset is 4.021. Lalonde Dataset. The dataset contains different demographic variables (e.g., gender, age, education etc.) with thegoal to see if training programs increase earnings (LaLonde, 1986). The average treatment effect here is the difference",
  "between the expected earnings under do(training program = 1) and expected earnings under do(training program = 0).The ground-truth ATE (on earnings) associated with this dataset is 1639.80": "Results. The results are summarized in . As noted above, the focus of this paper is on computing unit-levelcounterfactuals. The fact that the estimate of the ATE computed from via DCM approach matches the ground-truthATE well can be seen as a sign that the unit-level counterfactuals were computed accurately. For baseline, we alsoshow the results where we replace our DCM approach with additive noise model (ANM) approach for modeling theSCMs.",
  "Nonlinear Equations": "0.00 0.01 0.02 0.03 0.04 0.05 0.06 0.07 MMD 0.00 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.00 0.02 0.04 0.06 0.08 0.00 0.02 0.04 0.06 0.08 0.10 Observational MMD 0.00 0.02 0.04 0.06 0.08 0.10 0.12 0.14 MMD 0.00 0.05 0.10 0.15 0.20 0.000 0.025 0.050 0.075 0.100 0.125 0.150 0.175 0.00 0.05 0.10 0.15 0.20 0.25 Interventional MMD"
}