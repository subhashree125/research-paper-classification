{
  "Abstract": "In recent years there has been increased interest in understanding the interplay between deepgenerative models (DGMs) and the manifold hypothesis. Research in this area focuses onunderstanding the reasons why commonly-used DGMs succeed or fail at learning distribu-tions supported on unknown low-dimensional manifolds, as well as developing new modelsexplicitly designed to account for manifold-supported data. This manifold lens providesboth clarity as to why some DGMs (e.g. diffusion models and some generative adversarialnetworks) empirically surpass others (e.g. likelihood-based models such as variational au-toencoders, normalizing flows, or energy-based models) at sample generation, and guidancefor devising more performant DGMs. We carry out the first survey of DGMs viewed throughthis lens, making two novel contributions along the way. First, we formally establish that nu-merical instability of likelihoods in high ambient dimensions is unavoidable when modellingdata with low intrinsic dimension. We then show that DGMs on learned representationsof autoencoders can be interpreted as approximately minimizing Wasserstein distance: thisresult, which applies to latent diffusion models, helps justify their outstanding empiricalresults. The manifold lens provides a rich perspective from which to understand DGMs,and we aim to make this perspective more accessible and widespread.",
  "Introduction": "Learning the distribution that gave rise to observed data in RD has long been a central problem in statisticsand machine learning (Lehmann & Casella, 2006; Murphy, 2012). In the deep learning era, there has beena tremendous amount of research aimed at leveraging neural networks to solve this task, bringing forth deepgenerative models (DGMs). This effort has paid off, with state-of-the-art approaches such as diffusion models(Sohl-Dickstein et al., 2015; Ho et al., 2020; Song et al., 2021b) and their latent variants (Rombach et al.,2022) achieving remarkable empirical success (Ramesh et al., 2022; Nichol et al., 2022; Saharia et al., 2022).A natural question is why these latest models outperform previous DGMs, or more generally:",
  "What makes a deep generative model good?": "To answer this fundamental question, a line of research has emerged with the goal of understanding DGMsthrough their relationship with the manifold hypothesis, and using the obtained insights to drive empiricalimprovements. In its simplest form, this crucial hypothesis states that high-dimensional data of interestoften lies on an unknown d-dimensional submanifold M of RD, with d < D. Studying the behaviourof DGMs when the underlying data-generating distribution is supported on an unknown low-dimensionalsubmanifold of RD has already proven fruitful: for example, it is precisely those models with the capacityto learn low-dimensional manifolds (such as diffusion models, latent or otherwise) that tend to work betterin practice. Similarly, various pathologies of DGMs such as mode collapse in variational autoencoders(Kingma & Welling, 2014; Rezende et al., 2014; Dai & Wipf, 2019), and numerical instabilities in the scorefunction of diffusion models (Pidstrigach, 2022; Lu et al., 2023) or in normalizing flows (Dinh et al., 2015;2017; Cornish et al., 2020; Behrmann et al., 2021), among others can be explained as a failure to properlyaccount for manifold structure within data. Despite the existence of various DGM review papers (Kobyzevet al., 2020; Papamakarios et al., 2021; Bond-Taylor et al., 2022; Yang et al., 2023), to the best of ourknowledge none takes this manifold lens. Here, we present the first survey of DGMs from this viewpoint. The relevance and usefulness of studying DGMs through this lens hinges on a critical assumption: namely,that the manifold hypothesis actually holds. It is thus relevant to justify this hypothesis. There is a plethoraof arguments supporting the existence of low-dimensional structure in most high-dimensional data of interest,which we summarize below: IntuitionInformally, saying that a subset M of RD is a low-dimensional submanifold is a math-ematical way of capturing two important properties: a sense of sparsity in ambient space (M hasvolume, or Lebesgue measure, 0 in RD), and a notion of smoothness. These are properties that onecan commonly expect of high-dimensional data of interest. For example, consider natural images inD (assume they have been scaled to this range), where D corresponds to the number of pixels.Imagine sampling uniformly from D until a human deems the resulting sample to be a naturalimage. For all intents and purposes, such an image would never be sampled since natural imagesare sparse in their ambient space. Images are also smooth in that, given a natural image, onecan always conceive of slightly deforming it in such a way that the result remains a natural image.One can also intuitively understand a d-dimensional submanifold M of RD as a (smooth in someway) subset of intrinsic dimension d, which can be thought of as the number of factors of variationneeded to characterize a point in M. Again, most machine learning researchers or practitioners whohave worked with natural images will have the tacit understanding that far fewer dimensions thanthe ambient dimension, D, are needed to describe an image: for example, images can be successfullysynthesized from low-dimensional latent variables (Rombach et al., 2022; Sauer et al., 2023), andthey can be effectively compressed without affecting how humans perceive them (Wallace, 1992;Townsend et al., 2019; Ruan et al., 2021). Through this line of thinking, the manifold hypothesishas been a motivating concept in the field of machine learning from its infancy. Some of the firstautoencoders (Kramer, 1991) aimed to account for data with low intrinsic dimension. Early unsu-pervised algorithms such as Boltzmann machines (Ackley et al., 1985) were conceived as ways oflearning the constraints that govern complex data distributions. Indeed, the manifold hypothesis isone of the core intuitions behind why neural networks are so successful at learning low-dimensionalrepresentations in the first place (Bengio et al., 2013).",
  "Published in Transactions on Machine Learning Research (09/2024)": "PX (clX (M) \\ B(x)) = 1.Since B(x) is open, clX (M) \\ B(x) is closed.Then, by definition of sup-port (Equation 1) and because supp(PX ) = clX (M), it follows that clX (M) B(x) = . This is clearly acontradiction since x clX (M) B(x). Proof of Theorem 1. We first prove that lim inft pXt(x) = 0, D-almost-surely on X \\ clX (M). Let x X \\ clX (M), and let Ux be an open neighbourhood of x such that clX (Ux) clX (M) = , which existsbecause X is regular. Clearly PX (X Ux) = 0 (i.e. Ux is a continuity set of PX ) and PX (Ux) = 0 sinceclX (Ux) clX (M) = and PX (clX (M)) = 1.",
  "Notation and Setup": "In this section we present the notation and setup that we will use throughout our work. We try to deviateas little as possible from standard notation, but we nonetheless prioritize precision and consistency acrossmodels. While knowledge of measure theory and differential geometry is needed to understand some technicaldetails of how DGMs relate to manifolds, the core ideas, methods, and intuitions of this area do not requiremathematics beyond what is typically known by machine learning researchers. Our intention in this surveyis thus to remain accessible to readers with no background in these topics.",
  "Notation": "Ambient and latent spacesWe denote the D-dimensional ambient space as X, where depending onthe particular model being discussed, X could be RD or D. Many models use a latent space, which wedenote as Z, where Z = Rd. In most cases the latent space is low-dimensional, i.e. d < D, but in someinstances this need not hold. We use lower-case letters x X and z Z to denote points, and upper-caseletters X X and Z Z for random variables, on these respective spaces.",
  "Encoders and decodersIt will often be the case that we consider an encoder and a decoder between theaforementioned spaces, which we denote as f : X Z and g : Z X, respectively": "ProbabilityWe use the letters p and q to denote densities; an exception being the Gaussian density, whichwe denote as N(x; , ) when evaluated at x, where and correspond to its mean and covariance matrix,respectively. We write X p to indicate that X is distributed according to p. Since we will often needvarious densities, we use superindices to identify them,1 e.g. pX and pZ will denote densities on X and Z,respectively. In particular, we write the true data-generating density on X as pX . For a space S (e.g. Z orX), we denote the set of all probability distributions on S as (S). We use p q to denote the convolutionbetween the densities p and q, i.e. if X1 p and X2 q are independent, p q is the density of X1 + X2.We denote expectations with E, and use a subindex to specify what density the expectation is taken withrespect to, e.g. EXpX []. Network parametersAll DGMs leverage neural networks, in one way or another, to define a density pXparameterized by the learnable parameters of the neural network(s). We will often abuse notation and use to parameterize all the generative components of the DGM; e.g. some models involve a decoder g and aprior pZ on Z, and we frequently subindex both components with even if they do not share parameters.In some instances it will be necessary to distinguish between generative parameters, in which case we willexplicitly write = (1, 2), using e.g. g1 and pZ2 instead of g and pZ , respectively. We will sometimesoverload notation by using subindices to denote a sequence of model parameters (t)t=1; the meaning ofparameter subindices will always be clear from context. We use for all auxiliary parameters; i.e. those thatare not needed for generation. For example, an encoder f could have been learned alongside pZ and g,but it might not be needed to sample from the model. We use and to denote optimal values of theseparameters (with respect to the loss being optimized for the particular model being discussed). CalculusWe denote derivatives (gradients/Jacobians) with , and use a subindex to indicate whichvariable the differentiation is with respect to. For example, xf(x) and zg(z) respectively denote theJacobians of the encoder and decoder with respect to their inputs (not their parameters), evaluated at x andz; and log pX (x) denotes, for a given x, the gradient of the log density of the model with respect to itsparameters, evaluated at .",
  ": (a) Depiction of a full-dimensional density (i.e. a density in the usual sense) pX on RD, with D =2. The probability assigned by pX to a region A of RD is its integral over the region, i.e": "A pX(x1, x2)dx1dx2.(b) When the density pX is instead supported on a d-dimensional manifold M embedded in RD (here, d = 1and M is a curve), the integral evaluates to zero, and thus pX is not a density in the usual sense. (c)Formally, in order to recover the probability assigned to A by the manifold-supported density pX , the densitymust be integrated only over M using a volume form (dvolM) on the manifold,",
  "to denote its determinant, trace, and transpose, respectively. We denote the 1 and 2 norms in Euclideanspace as 1 and 2, respectively": "Formal notationIn order to formally discuss probability distributions on manifolds, we need thelanguage of measure theory. All the measures we will consider are defined over X or Z, with theirrespective Borel -algebras.We remind the reader that, since they are Euclidean, X and Z arenot arbitrary probability spaces. For a subset A of X, we denote its closure in X as clX (A). Wedenote measures with the same letters as densities, but with uppercase blackboard style, i.e. P andQ two exceptions being the Lebesgue and Gaussian measures, which we denote as and N(, ),respectively. We use a subindex on to indicate its ambient dimension; e.g. D denotes the D-dimensional Lebesgue measure. We write P Q to indicate that P is absolutely continuous withrespect to Q. We use # as a subscript to denote pushforward measures; e.g. for a measurable functiong : Z X and a probability measure PZ on Z, g#PZ is the pushforward probability measure (on X)of PZ through g. We will write the true data-generating distribution corresponding to pX as PX , andthe model distribution corresponding to pX as PX . Finally, we use to indicate weak convergenceof probability measures.",
  "Setup": "The main goal of all the models pX considered in this paper is to learn pX . Two main assumptions, which mostof the works presented throughout our survey follow, are commonly made when attempting to understandDGMs through the manifold lens: Manifold supportAs mentioned in the introduction, there is a substantial body of work support-ing the existence of low-dimensional structure in high-dimensional data of interest such as images.One way of mathematically expressing this structure is by taking a literal interpretation of themanifold hypothesis; that is, assuming that pX is supported on a d-dimensional submanifold M ofX, where 0 < d < D and both M and d are unknown. Several aspects of this assumption war-rant additional discussion. (i) pX being manifold-supported implies that it is not a full-dimensionaldensity (i.e. with respect to the D-dimensional Lebesgue measure) and it is thus not a density inthe usual sense. See for an explanation. (ii) This assumption is a choice about how to",
  "CC(P)C,(1)": "where C(P) is the collection of closed (in X) sets C such that P(C) = 1. It immediatelyfollows that the support of a distribution is always a closed set in its ambient space. Ingeneral, M need not be closed in X, in which case it would be impossible for PXto besupported on M. We nonetheless abuse language throughout our survey (both in the maintext and in these formal boxes) and say PX is supported on M when we actually mean thatPX (M) = 1 and that supp(PX ) = clX (M).",
  "UMpX dvolM(5)": "for every open set U of X, i.e. dvolM plays the role of the Riemannian measure M inEquation 4. The technical tool allowing us to establish a correspondence between these twoviews of pX is known as the Riesz-Markov-Kakutani theorem (Rudin, 1987, Theorem 2.14) which is sometimes also referred to as the Riesz representation theorem and should not beconfused with a different theorem about Hilbert spaces bearing the same name. The Riesz-Markov-Kakutani theorem allows us to assign a unique measure to dvolM, namely M, suchthat integrating continuous compactly-supported functions against them is equivalent. Inthis sense, M is the natural measure to extend dvolM. We point out that when M isnon-orientable, even though dvolM is not defined, integration on manifolds can still be carriedout through the above measure-theoretic formulation.",
  "Deep Generative Models on Known Manifolds": "This work focuses on data governed by the manifold hypothesis, wherein the dataset of interest is constrainedto an unknown d-dimensional submanifold of X. However, for some datasets, the manifold is known apriori, and the challenge lies in designing a generative model which can learn densities within the manifold.Generative modelling on known manifolds is a distinct task from our focus in this survey, but it is closelyrelated, and we thus briefly summarize work on this topic below.",
  "Manifold Learning": "Learning distributions whose support is an unknown manifold M implies learning M as well, at least implic-itly. The field of manifold learning is thus closely related to the main topic of our work. The term manifoldlearning is often treated synonymously with dimensionality reduction, and refers to methods whose goal isto provide a useful representation of high-dimensional data by transforming it into a lower-dimensional space.That representation may provide information about the data such as its intrinsic dimension, yield a usefulvisualization in two or three dimensions, or serve as a simplified starting point for downstream supervisedlearning tasks. Generally, manifold learning methods fall into three categories: spectral methods (Pearson,1901; Kruskal, 1964; Beals et al., 1968; Schlkopf et al., 1998; Roweis & Saul, 2000; Tenenbaum et al., 2000)which rely on the eigenvectors and eigenvalues of some matrix related to the data; probabilistic methods(Tipping & Bishop, 1999; van der Maaten & Hinton, 2008; McInnes et al., 2018), which treat datapointsas high-dimensional random vectors whose relevant information is contained in some low-dimensional latentvariables; and bottleneck methods (Rumelhart et al., 1988; Kramer, 1991; Tishby et al., 2000; Kingma &Welling, 2014; Alemi et al., 2017), which rely on passing information through a low-dimensional bottleneckrepresentation, often using neural networks. We refer the reader to the work of Ghojogh et al. (2023) for acomprehensive review of manifold learning using this tripartite classification. The focus on manifold learning in this work is mostly on bottleneck methods such as autoencoders (Rumelhartet al., 1988) and their many variants whose core idea is to train an encoder-decoder pair of neural networksto ensure reconstruction, for example through a squared 2 loss:2",
  "min, EXpXX g (f(X)) 22.(6)": "Here, the bottleneck refers to the d-dimensional encoder output f(X), which the decoder g uses toreconstruct X. Since pX is supported on M, the objective in Equation 6 aims to learn the manifold in thesense that it encourages perfect reconstructions on it, or more formally, if x M, then x = g(f(x)). Wehighlight that perfect reconstructions need not always be achievable even under the nonparametric regime(.2) due to topological constraints. We discuss this point, which we will revisit in .4, inthe next grey box. We finish this section with the observation that, despite what the term manifold learning might suggest,autoencoders do not by themselves formally learn M even when perfect reconstructions are achieved.One might expect a perfectly trained autoencoder to characterize M as the set of possible decoder outputs.However, the encoder may not make use of the entire latent space, leaving some points that would not beseen by the decoder during training. As a result, points z Z \\ f(M) might be decoded outside of M,as illustrated in . Alternatively, one might expect an autoencoder to characterize M as the setof points which are perfectly reconstructed. However, some points outside the manifold can in principlestill be perfectly reconstructed, as also illustrated in . Despite these observations, assuming perfectreconstructions, the set f(M) together with the decoder g jointly characterize M, since g(f(M)) = 2Note that autoencoders are not by themselves generative models, but we nonetheless parameterize g with (which we usefor generative parameters) for consistency with other decoders in the rest of the paper.",
  "M; we will revisit this point in .3 to show that training a DGM on data encoded by f cancharacterize M": "When are perfect reconstructions achievable?Ideally, the loss in Equation 6 achieves a valueof 0 at optimality, which would directly imply that x = g(f(x)), PX -almost-surely. Under mildregularity conditions (Loaiza-Ganem et al., 2022a), this in turn implies that x = g(f(x)) for allx M, in which case we say that the encoder-decoder pair (f, g) reconstructs M perfectly.When this condition is satisfied, the restriction f|M is a (topological) embedding of M into Z,as is evidenced by the existence of its continuous left-inverse, g. In other words, the existenceof some continuous function f that embeds M into Z is a necessary condition to achieve perfectreconstructions and thus to learn M. In general, however, such an f may not exist.For example, as we will see in .3, it issometimes desirable to set d, the dimensionality of Z, to be equal to d, the dimensionality of M.This precludes the existence of f for many manifolds M, such as if M is a d-dimensional sphere, forwhich no embedding f : M Rd is possible when d = d. In cases where no plausible embeddingexists, even networks (f, g) which come close to perfectly reconstructing M will incur numericalinstability (Cornish et al., 2020). In some other cases, it is possible to resolve these topological issuesby increasing d. For instance, a dimensionality of d = 2d + 1 is enough to topologically embed anymanifold of dimension d in Rd (Hurewicz & Wallman, 1948, Theorem V 3).",
  "The Change-of-Variables Formula": "It will often be the case that we have a density pZ on Z along with a decoder g : Z X. Together, thesetwo components implicitly define the distribution of X = g(Z), where Z pZ, and it will often be of interestto explicitly evaluate the density pX of X (formally, pX is the pushforward density of pZ through g). Thesuitable tool is the change-of-variables formula, whose simplest form states that, when Z = X = RD, if g isa diffeomorphism (i.e. a continuously differentiable function with a continuously differentiable inverse), then",
  "where again z = f(x), but now f : g(Z) Z is the left inverse of g (i.e. z = f(g(z)) for all z Z), andzg(z) RDd": "Several remarks about these formulas are worth making.(i) Computationally, it is often the case thatEquation 8 is used, rather than Equation 7, as Equation 8 requires only a forward pass through the encoderf (as well as computing its Jacobian determinant), whereas Equation 7 requires an additional forwardcomputation through the decoder g; (ii) Equation 9, which is referred to as the injective change-of-variablesformula, reduces to Equation 7 in the case where d = D, since the determinant distributes over products ofsquare matrices; and (iii) when d < D, pX in Equation 9 is a manifold-supported density because it is onlydefined on a submanifold, g(Z), of X, much like pX which is only defined on M. We refer the reader to thework of Kthe (2023) for a review of the uses the change-of-variables formula has within DGMs. Note that a more formal way of describing the change-of-variables formula is through the language ofpushforward measures, where we have a measure PZ on Z admitting a density pZ with respect to d,along with the measurable map g : Z X. In the case of Equation 7 and Equation 8, pX correspondsto the density of g#PZ with respect to D; i.e. pX = dg#PZ/dD. In the case of Equation 9, wheng is a smooth embedding, pX is now a density with respect to the Riemannian measure on g(Z), i.e.pX = dg#PZ/dg(Z), where g(Z) is treated as an embedded submanifold of X.",
  "Failures of KL Divergence": "Despite the KL divergence being widely used throughout machine learning, it is most commonly used withthe implicit assumption that the two involved densities are densities in the same sense (i.e. when theyboth admit the same dominating measure, see the grey box below). This assumption fails in the manifoldsetting when pX is full-dimensional, since pX is manifold-supported. We thus find it useful to provide theformal definition of the KL divergence in the grey box below, along with a discussion. In summary, the usualformula for computing KL divergence,",
  ",(10)": "is only valid when pX and pX are such that for every subset A of X that is assigned probability 0 by pX , thedensity pX also assigns probability 0 to A. Whenever this property does not hold, KL(pX pX ) is definedas infinity.It follows that in the manifold setting, KL(pX pX ) = = KL(pX pX ) when pXis full-dimensional, as illustrated in (a). It also follows that KL(pX pX ) = even if pX is supported on ad-dimensional manifold as long as M is not contained in the support of pX as illustrated in (b).",
  "BM pX dvolM = 0 because B M = , yet we have": "B pX dx > 0, entailing that KLpX pX= . (b)Analogous example where now pX and pX are both supported on low-dimensional manifolds. Since M isnot contained in the support of pX , there exists a set A to which pX assigns probability 0 despite havingpositive probability under pX , so that KLpX pX= . However, a key step in this derivation (the first equality) is the assumption that the KL divergence betweenpX and pX is not trivially infinite. As previously mentioned, in the manifold setting we will generally haveKL(pX pX ) = . It follows that in this setting, maximum-likelihood is not equivalent to KL divergenceminimization, a point that we will later revisit.",
  ", otherwise,(14)": "where dP/dQ denotes the Radon-Nikodym derivative of P with respect to Q. By the Radon-Nikodymtheorem, dP/dQ exists if and only if P Q. Finally, when both P and Q are dominated by the samemeasure i.e. P and Q with corresponding densities p and q with respect to , the KLdivergence between them simplifies to",
  "Wasserstein Distances": "In .4, we summarized why KL(pX pX ) does not provide a useful notion of divergence betweenpXand pXin the manifold setting, and the same is true of many other common divergences betweendistributions (see the discussion in .2). Wasserstein distances, which are based on the optimaltransport problem (Villani, 2009; Peyr & Cuturi, 2019), provide a distance between distributions thatremains meaningful even in the manifold setting. Despite the fact that accurately estimating Wassersteindistances is challenging (Arora et al., 2017), DGMs based on minimizing these distances tend to work verywell in practice (e.g. .2.1 and .3.1).",
  "Wc(p, q) :=inf(p,q) E(X,Y )[c(X, Y )],(16)": "where c : X X R is called the cost function, and (p, q) is the set of distributions on X X whosemarginals match p and q, respectively. Intuitively, the optimal transport problem can be understood asthe cost (as measured by c) of transporting p to q, as illustrated in . When c is given by the 1distance (i.e. c(x, y) = x y1) Wc is called the Wasserstein-1 distance, and is denoted as W1. The W1metric admits the following well-known dual formulation:",
  "where H := {h : X R | h is Lipschitz and Lip(h) 1}, and Lip(h) denotes the Lipschitz constant ofh. Analogously, when c is given by the squared 2 distance (i.e. c(x, y) = x y22)": "Wc is called theWasserstein-2 distance, and is denoted as W2.The Wasserstein distances W1(pX , pX ) and W2(pX , pX )remain meaningfully defined even in the manifold setting (formally, this is because they metrize weak con-vergence, which we discuss in the grey box below), and thus provide sensible optimization objectives. ForW1, this property can be informally understood through Equation 17, which essentially says that two distri-butions are close in Wasserstein distance if no Lipschitz function can discriminate between them. Intuitively,if no such function can discern between pX and pX , then they must be truly close, even if one is manifold-supported and the other full-dimensional (or if both are supported on non-overlapping manifolds).",
  ",(19)": "where X, X, Y, Y are independent, and k : X X R is a symmetric positive semi-definite kernel (i.e.a function having the property that, for any n N and x1, . . . , xn X, the n n matrix K given byKij = k(xi, xj) is symmetric positive semi-definite), which is set as a hyperparameter. The MMD has several desirable mathematical properties. (i) Under some regularity conditions which aresatisfied by many commonly-used kernels, the MMD is a metric in the space of probability distributionsover X. (ii) MMD2k(p, q) can be straightforwardly estimated in an unbiased manner through Monte Carlosampling, making it particularly amenable to gradient-based optimization. (iii) Conditions on k which makeMMDk meaningfully defined in the manifold setting (formally, conditions under which MMD metrizes weakconvergence) are known (Simon-Gabriel & Schlkopf, 2018; Simon-Gabriel et al., 2023). Provided that Xis compact (which is the case for images in D), these conditions hold for most commonly-used kernels,meaning that MMD can be used to compare distributions regardless of their support.",
  "The Problem with Likelihood-Based Approaches: Manifold Overfitting": "Likelihood-based deep generative models are a broad and popular class of models, which includes variationalautoencoders (Kingma & Welling, 2014; Rezende et al., 2014), normalizing flows (Dinh et al., 2015; 2017),energy-based models (Xie et al., 2016; Du & Mordatch, 2019), continuous autoregressive models (Uria et al.,2013), and more (Bond-Taylor et al., 2022).At a high-level, these models leverage neural networks toconstruct a full-dimensional density pX . The models are trained by maximizing, sometimes approximately,the log-likelihood:maxEXpX [log pX (X)].(20) When the underlying density pXis full-dimensional, this objective is equivalent to minimizing the KLdivergence between pX and pX (Equation 13). However, in our setting of interest pX is manifold-supported,",
  "and as mentioned in .4, this equivalence breaks down, leading to the natural question: what happensif the likelihood is optimized when pX is full-dimensional but pX is not?": "The first consequence of this dimensionality misspecification is that the log-likelihood does not admit amaximum as it can be made arbitrarily large. To see this, consider a sequence of full-dimensional models(pXt)t=0 which concentrate more and more mass around M during training, as depicted in . If Mwas full-dimensional, it would be impossible to have pXt(x) as t for all x M, as doing so wouldquickly violate the requirement that the densities integrate to 1. However, when M is low-dimensional, itis infinitely thin in RD, and thus the model densities can be made to diverge to infinity along the entiremanifold. This phenomenon is illustrated twice in . At a first glance, the fact that the likelihood does not admit a maximum might seem inconsequential, asone might hope that as long as EXpX [log pXt(X)] as t , then pX is still being learned. However,this is not the case, and the reason is once again illustrated in : there are many ways in which thelikelihood can diverge to infinity. Loaiza-Ganem et al. (2022a) formalized this intuition by proving that undermild regularity conditions, for any manifold-supported density pX on M, there always exists a sequence offull-dimensional densities which simultaneously (i) becomes arbitrarily large on the entire manifold, in turnmaximizing likelihood, yet (ii) approximates pX rather than the true data-generating density pX . The lattercondition is formalized using weak convergence in the grey box below, but can be intuitively understood assaying that samples from pXt and pX become indistinguishable as t .3",
  "PXt PX as t": "Loaiza-Ganem et al. (2022a) proved this result under the assumption that M is analytic. Despitetheir other regularity conditions being very mild, this is a strong assumption. However, as we nowargue, the result actually holds for arbitrary smooth submanifolds M of X. Gray (1974, Theorem3.1) proved a result which immediately implies that, if M is an analytic Riemannian manifold, thenfor x M, as 0,MBM (x)= v(d) d 1 + O(2),(21) where M is the Riemannian measure on M, BM (x) denotes a geodesic ball in M of radius centredat x, and v(d) is the volume of a d-dimensional Euclidean ball of radius 1. Loaiza-Ganem et al.(2022a) used the assumption that M is analytic only to apply Equation 21 by evoking the result ofGray (1974). However, Equation 21 is known to hold for arbitrary smooth Riemannian manifolds(Gallot et al., 2004, Theorem 3.98). It immediately follows that the result of Loaiza-Ganem et al.(2022a) indeed holds for arbitrary smooth submanifolds M of X, even if they are not analytic.",
  "The Unavoidable Numerical Instability of High-Dimensional Likelihoods": "The manifold overfitting result of Loaiza-Ganem et al. (2022a) described in .1 establishes thatmaximum-likelihood is an ill-posed objective for high-dimensional densities in the manifold setting. Beforecontinuing our review of existing work, we point out that their result does not rule out the possibility ofsomehow addressing the pathological behaviour of maximum-likelihood, for example by adding a regularizer.Here we prove that it is actually impossible to do so, by showing that for any infinitely thin subset M ofX (of which M is an example, but here we do not require M to be a manifold), any density pX supported",
  "Proof. See Appendix B.1": "We now make some relevant observations about the Likelihood Instability Theorem. (i) Note that weonly require the closure of M to have Lebesgue measure 0, so it need not be a manifold. Our result thusapplies in settings beyond the standard manifold hypothesis, such as when M is given by a union ofmanifolds (Brown et al., 2023), or by a non-manifold set with singularities (Von Rohrscheidt & Rieck,2023; Wang & Wang, 2024). (ii) lim inft pXt(x) cannot in general be replaced by limt pXt(x)since the limit need not exist, but as an immediate corollary, if the limit exists, then it must be 0D-almost-everywhere on X \\ clX (M). (iii) We also point out that supxB(x) pXt(x) cannot be ingeneral replaced by pXt(x) either, despite our conclusion holding for every > 0. Intuitively, this is",
  "pX|Z(x|z) = Nx; g(z), X|Z(z),(22)": "where X|Z: Z RDD is symmetric positive definite, often given by ID, where > 0 is treated as a freeparameter rather than the output of a neural network. The conditional likelihood pX|Z(|z) in Equation 22can be understood as a stochastic decoder, whose mean is given by the deterministic decoder g(z). Together,the prior and the conditional likelihood implicitly define the marginal likelihood over data:",
  "log pX (X) KLqZ|X(|X) pZ|X(|X) EXpX [log pX (X)],(25)": "where pZ|Xdenotes the true posterior density, which is implicitly defined by the prior pZ and the conditionallikelihood pX|Z. Note that Equation 24 is used as the optimization objective, since the true posterior densitycannot be tractably evaluated. While not directly usable as an objective, Equation 25 shows that the ELBOlower-bounds the log-likelihood EXpX [log pX (X)], and differs from it only by the error incurred by qZ|Xtoapproximate the true posterior: this is often used as a justification for using the ELBO as an objective, asit simultaneously encourages learning through maximum-likelihood, and so that qZ|Xmatches the true",
  "qZ|X(z|x) = Nz; f(x), Z|X(x),(26)": "where Z|X: X Rdd is also symmetric positive definite, and often given by a diagonal matrix withpositive entries along its diagonal. In an analogous manner to the conditional likelihood, the variationalposterior qZ|X(|x) in Equation 26 can be interpreted as a stochastic encoder, whose mean is given by thedeterministic encoder f(x). An issue which commonly affects VAEs is posterior collapse (Chen et al., 2017; Wang et al., 2021), wherethe learned variational posterior qZ|Xpartially collapses to the prior pZ. We will shortly explain posteriorcollapse in VAEs through the manifold lens, and thus we briefly summarize the phenomenon here: in thecase where Z|Xis taken as a diagonal matrix, a subset of the diagonal entries of Z|X (x) collapses to 1for x M, and the corresponding entries of f(x) collapse to 0, matching the standard Gaussian prior pZ;whereas the remaining diagonal entries of Z|X (x) are extremely close to 0, essentially losing stochasticityalong these coordinates. In other words, VAEs tend to only use a subset of the coordinates of their latentspace Z to obtain data samples and default the rest to the prior.",
  "latent dimensions defaults to the standard Gaussian prior pZ due to the KL term in Equation 24. In otherwords, the manifold lens helps elucidate why posterior collapse happens": "We now formalize the discussion on the results of Dai & Wipf (2019), which assume Gaussian VAEsas described above, with the decoder covariance given by X|Z(z) = ID, where > 0 is a freeparameter that does not depend on z. They also assume throughout that M is diffeomorphic toRd, which is a much stronger assumption than required by Loaiza-Ganem et al. (2022a). The first result of Dai & Wipf (2019) assumes some regularity conditions, that d < D, that d d,and that is learnable (i.e. it is part of ). The result then states that for any distribution PX on Xsupported on M, there exist a sequence of VAE models parameterized by (t, t)t=1 such that:",
  "PXt PX as t , where PXt is the distribution corresponding to the model density pXt": "As previously mentioned, Dai & Wipf (2019) also show that Gaussian VAEs achieve perfect recon-structions, and they link this behaviour to posterior collapse. To do so, they first consider > 0 as afixed hyperparameter instead of being learnable, and write the corresponding ELBO as EpX (, ; ),with corresponding maximizers () and (). They then prove that, if d d and under similarregularity conditions to the result above, for every > 0 there exists (0, ) such that:",
  "EpX ((), (); ) > EpX ((), (); ) .(27)": "In particular this suggests that, when is learnable, it must converge to 0 to make the ELBO divergeto infinity. An intuitive way to understand this is as saying that, for a small enough decoder variance, it is preferable to maximize the log pX|Z(X|Z) term in Equation 24 which in this case boils downto an 2 reconstruction error weighted by 1/(2) instead of minimizing the KL term. Dai & Wipf(2019) then leverage this result to show that VAEs achieve perfect reconstructions in the sense that",
  "lim0 g()f()(x)= x,PX -almost-surely.(28)": "Finally, since PXis supported on a d-dimensional manifold M, perfectly reconstructing a pointx M requires d dimensions, and not the full d of the latent space Z: this means that d latentdimensions are used to achieve perfect reconstructions, and thus the respective approximate posteriorvariances are sent to 0; whereas the remaining dd dimensions in the approximate posterior defaultto a standard Gaussian to minimize the KL term in the ELBO. This explanation of posterior collapsewas suggested by Dai & Wipf (2019), and it was further formalized by Zheng et al. (2022).",
  "Normalizing Flows": "Normalizing flows (NFs; Dinh et al., 2015; 2017; Papamakarios et al., 2017; Kingma & Dhariwal, 2018;Durkan et al., 2019; Kobyzev et al., 2020; Papamakarios et al., 2021) are a class of DGMs that leverage thechange-of-variables formula to enable maximum-likelihood training. NFs construct a bijective neural networkg : Z X, where Z = X = RD, such that both g and its inverse f are differentiable.4 A prior density pZ is specified (often a standard Gaussian), and sampling from the model pX is achieved through X = g(Z)where Z pZ (formally, pX is the pushforward of pZ through g). The change-of-variables formula fromEquation 8 provides the maximum-likelihood objective:",
  "maxEXpXlog pZ (f(X)) + log |det xf(X)|.(29)": "Constructing the Jacobian xf(x) through automatic differentiation to compute the log-likelihood andthen backpropagate with respect to is computationally prohibitive. To circumvent this issue, NFs areconstructed in such a way that ensures that det xf(x) can be efficiently evaluated in closed-form (or atleast approximated), thus enabling gradient optimization with respect to . A relevant variant of NFs are continuous NFs, where f is defined implicitly through an ordinary differentialequation (ODE) rather than explicitly constructed (Chen et al., 2018; Salman et al., 2018; Grathwohl et al.,2019). More specifically, an auxiliary neural network v : X [0, T] X is used to specify the ODE:",
  "x0 X.(30)": "4Note that in NFs, the encoder f is uniquely determined by the decoder g. Thus, the encoder does not requireauxiliary parameters , which is why we parameterize it with the generative parameters . We do nonetheless highlight thatmoving away from this restriction by parameterizing the encoder and decoder networks separately and making them learn toinvert each other has been attempted (Draxler et al., 2024).",
  ",(35)": "where Xt corresponds to xt when Equation 30 is initialized at x0 = X0 pX . While the computations usedto train continuous NFs through Equation 35 are significantly different than those used for standard NFs,both models are fundamentally doing the same thing: modelling the data as the distribution obtained bymapping a simple distribution pZ such as a Gaussian through a bijective function g (defined explicitly orimplicitly), and training the model via maximum-likelihood. Normalizing flows through the lens of the manifold hypothesisBy construction, NFs continuousor not are full-dimensional models and are thus susceptible to manifold overfitting (.1). Thereare however other pathologies associated with using NFs in the manifold setting. For example, Cornishet al. (2020) show that if the supports of pZ and pX are not homeomorphic,6 then any normalizing flowwhich approximates pX must have exploding bi-Lipschitz constant. In the manifold setting, the supportof pX is the d-dimensional manifold M, which is not homeomorphic to the support of pZ, i.e. RD. Theexploding bi-Lipschitz constant implied by the result of Cornish et al. (2020) entails that, if NFs convergeto a distribution on a low-dimensional manifold (whether this is pX or some other distribution), they mustdo so in a numerically unstable way, which is completely consistent with our result in .1.1. Thesetheoretical insights are borne out in practice; for example, Behrmann et al. (2021) show that trained NFsare numerically non-invertible. While perhaps initially surprising, this phenomenon is neatly explained byconsidering NFs through the lens of the manifold hypothesis, which we return to in .3.3. 5The most notable of these conditions is v being Lipschitz in t (with the Lipschitz constant not depending on x), whichwill become relevant when we discuss diffusion models in .1.2.6Recall that two spaces are homeomorphic when there exists a homeomorphism i.e. a continuous invertible function withcontinuous inverse between them.",
  "where = stopgrad(), as it provides the correct gradient with respect to .7": "Energy-based models through the lens of the manifold hypothesisSince by construction pX (x) > 0for all x X, EBMs are full-dimensional models and are thus susceptible to manifold overfitting (.1).EBM training and sampling are known to be difficult. A common trick to sample from a trained EBM is toinitialize MCMC chains not from noise, but from previous chains held in a replay buffer. The buffer is a setof MCMC samples from chains that have been advanced by the EBM throughout the entire training process(Du & Mordatch, 2019; Grathwohl et al., 2020). Alternatively, a mixture of historical training checkpointscan be used for sampling (Du & Mordatch, 2019). These tricks are necessary because EBMs are proneto mode collapse, especially in high-dimensional ambient spaces (Arbel et al., 2021; Loaiza-Ganem et al.,2022a). This mode collapse behaviour is often blamed on Langevin dynamics and the multimodality of thetarget distribution, but can be further explained by the large or unstable gradients in the density landscapeof a model that has undergone manifold overfitting.",
  "T,(40)": "where T > 0 is a hyperparameter.8 Much like variational autoencoders (.1.2), despite using anautoencoder-like structure, normalized autoencoders remain full-dimensional models; this is true of EBMsregardless of the choice of energy function. An interesting observation, which to the best of our knowledgehas not been previously made, is that the energy function in Equation 40 is lower-bounded by 0, which in turnimplies that eEt(x) is upper-bounded, meaning that pXt(x) cannot be sent to infinity by making Et(x)arbitrarily negative for x M. In particular, manifold overfitting can only occur when the normalizingconstant X eEt(x)dx goes to 0. While it is of course possible for this to happen with arbitrarily flexiblenetworks, we hypothesize that EBMs with lower-bounded energy functions might have an inductive biaswhich helps them avoid manifold overfitting in practice, albeit likely at the cost of generative quality. Thismight explain their empirical success at density-based out-of-distribution detection (Yoon et al., 2023). 7stopgrad is a computational operator, commonly available in automatic differentiation libraries such as PyTorch (Paszkeet al., 2019), which leaves the forward pass unchanged ( = ) while ignoring gradients when differentiating ( = 0),i.e. EXpX [E(X)] = EXpX [E(X)], yet EXpX [E(X)] = EXpX [E(X)] even though EXpX [E(X)] is not in",
  "Generative Adversarial Networks": "Generative adversarial networks (GANs; Goodfellow et al., 2014; Radford et al., 2015) use a neural networkg : Z X called the generator, along with a latent distribution pZ (usually a standard Gaussian), tospecify the model distribution pX . Similarly to normalizing flows (.1.3), samples X = g(Z) froma GAN are obtained by sampling Z pZ and transforming the result through g (as in NFs, pX is formallygiven by the pushforward of pZ through g), although unlike NFs, d = D is not required, and rather d < Dis the standard choice for GANs, so that g need not be invertible. GANs are not likelihood-based models,and in order to train g, they introduce a binary classifier h : X (0, 1), which is trained to distinguishbetween real samples X pX and generated samples X pX . The generator g is trained alongside h, soas to make the classifier unable to successfully differentiate between real and generated samples:",
  "2p + 1": "2q). This result was first shown by Goodfellow et al.(2014) under the unstated assumption that pX and pX are densities supported on the same manifold forall . While this assumption is unrealistic in the manifold setting and should not be expected to hold inpractice, the proof provided by Goodfellow et al. (2014) is correct in spirit, and was later formalized andgeneralized by Donahue et al. (2017). Generative adversarial networks through the lens of the manifold hypothesisThe Jensen-Shannon divergence JS(p q) is meaningfully defined even when KL(p q) = .At a first glance thismight suggest that optimizing the Jensen-Shannon divergence circumvents issues such as manifold overfit-ting (.1), which arise from attempting to minimize the KL divergence. However, the gradients of theJensen-Shannon divergence JS(pX pX ) with respect to model parameters will be 0 whenever the supportsof pX and pX do not overlap (formally, the Jensen-Shannon divergence does not metrize weak convergence).This property makes gradient optimization futile whenever the support of the GAN has no overlap with theunderlying data manifold, which Arjovsky et al. (2017) identify as a cause of training instabilities for GANs.The Jensen-Shannon divergence is a particular instance of an f-divergence (Polyanskiy & Wu, 2022), andthere is work generalizing GANs to minimize f-divergences (Nowozin et al., 2016). Yet, Arjovsky et al. (2017)also show that various other f-divergences, such as the total variation distance and KL divergence, sufferfrom similar pathologies as the Jensen-Shannon divergence when there is mismatch between the supports ofpX and pX . In other words, although GANs do not suffer from manifold overfitting, they can still struggleto model manifold-supported data. It is however worth highlighting that the manifold-related woes of GANsare fundamentally different than those of likelihood-based models: the former use a proper low-dimensionalmodel (whenever d < D), and the resulting problems are due only to the optimization objective; whereasthe latter are full-dimensional models, and are thus misspecified. Still, GANs can remain topologically mis-specified, e.g. when M is disconnected (.4.3), but again, this is an inherently different situationthan the dimensional misspecification of likelihood-based models.",
  "Score Matching": "Score matching (Hyvrinen, 2005) is a method to learn full-dimensional densities pX . The main idea is tolearn the (Stein) score function, x log pX , rather than pX itself.9 In order to achieve this, a model pX isimplicitly characterized by sX : X X, whose goal is to approximate the unknown true score function. TheFisher divergence, which is sometimes referred to as the Fisher information distance (DasGupta, 2008), andwhich is defined asF(p, q) := EXpx log q(X) x log p(X)22,(43)",
  "which can actually be minimized. Once a model is trained, Markov chain Monte Carlo methods such asLangevin dynamics can be used to sample from it, similarly to energy-based models (.1.4)": "Score matching through the lens of the manifold hypothesisAs mentioned above, score match-ing is derived under the assumption that the underlying data distribution pX is full-dimensional. Whilescore matching has been extended to known manifolds (Mardia et al., 2016), we are not aware of any worktheoretically studying dimensional mispecification within score matching in an analogous manner to howLoaiza-Ganem et al. (2022a) characterize manifold overfitting (.1) within likelihood-based models.Nonetheless, we should intuitively expect score matching to fail under the manifold setting due to this di-mensional misspecification. To see why this is the case, we begin by noting that pX is indeed full-dimensionalsince sX takes inputs from all of X rather than just M (sX is evaluated at potentially any point in X duringsampling when using procedures such as Langevin dynamics). The score functions sX and x log pX arethus different types of objects the former is a full-dimensional score function and the latter is a manifold-supported one. Comparing the values of dimensionally-mismatched densities is not meaningful, and thecomparison remains equally meaningless between the corresponding score functions. Consequently, there isno reason to expect Equation 44 to succeed at matching pX to pX in the presence of dimensional misspec-ification. This issue was identified by Song & Ermon (2019), who empirically confirm that score matchingstruggles in the manifold setting.",
  "Manifold-Aware Deep Generative Models": "As covered throughout , many commonly-used DGMs struggle to learn distributions on unknownmanifolds. There are various (not always mutually exclusive) approaches that enable manifold-awareness,including judiciously adding noise to the target distribution; using support-agnostic optimization objectives(e.g. those which metrize weak convergence); and two-step models, which carry out generative modelling ona low-dimensional latent space and then map back to data space. We review these approaches in .1,.2, and .3, respectively. In .3.1 we show that (i) two-step models can be interpretedas (potentially regularized) minimizers of an upper bound of the Wasserstein distance, thus establishing a linkbetween these different approaches for achieving manifold-awareness, and that (ii) the upper bound becomestight at optimality whenever an autoencoder can achieve perfect reconstructions. Finally, in .4 wecover methods which make an explicit attempt at properly capturing the topology of M. We take a laxinterpretation of manifold-awareness throughout, and discuss not only DGMs which are formally manifold-aware, but also those which, while mathematically manifold-unaware, leverage some inductive bias towardsmanifold-awareness.",
  "Denoising Score Matching": "As previously mentioned, Song & Ermon (2019) showed that score matching (.3) struggles to modelmanifold-supported data, and they thus advocate for adding noise and using denoising score matching(Vincent, 2011) instead. In denoising score matching, the target distribution is not pX anymore, but ratherthe distribution pXobtained by adding independent Gaussian noise N( ; 0, 2ID) to samples from pX ,where 2 is a hyperparameter.More formally, pX:= pX N( ; 0, 2ID).Importantly, adding full-dimensional Gaussian noise ensures that pXis always full-dimensional, regardless of the support of pX .Score matching can then be applied to learn a network sX to approximate x log pXthrough Equation 46(with pX replaced by pX ). However, Vincent (2011) shows that this objective is equivalent to",
  "sX (X) x log pX|X0(X|X0)22,(47)": "where pX|X0(x|x0) = N(x; x0, 2ID) is the density of noisy data (denoted X) given the (un-noised)datapoint X0 = x0. Equation 47 is much easier to optimize than the usual score matching objective (Equa-tion 46), since there is no need to backpropagate through the trace of the Jacobian of sX . Saremi & Hyvrinen(2019) use the loss function in Equation 47 to learn an energy-based model (.1.4) on the noised-outdata, but derive the loss from the perspective of empirical Bayes (Robbins, 1956); their approach can inprinciple be applied to other noising processes, but only the Gaussian case is implemented in their work. Despite mathematically avoiding manifold-related pathologies, denoising score matching as presented abovefaces a tradeoff; setting to a very small value means that the target density pXis closer to the actualmanifold-supported data density pX , but doing so also means the target density is highly peaked around Mand thus might be harder to properly learn (.1.1). As a way of being able to use small amounts ofnoise while still efficiently learning the resulting distribution, Song & Ermon (2019) propose to use variousnoise levels. More specifically, they consider fixed noise levels 0 < 1 < 2 < < T , and modify the scorefunction to take the noise level as input; i.e. sX : X [1, T ] X is now such that it aims to approximatethe score function at all the corresponding noise levels: sX ( , t) xt log pXtfor t = 1, . . . , T. Thisnew score function is trained with a weighted sum of the corresponding denoising score matching objectives,",
  "sX (Xt, t) xt log pXt|X0(Xt|X0)22,(48)": "where w(t) > 0 is a pre-specified weight coefficient which aims to keep the T terms in the sum at roughlyequal magnitudes. The intuition behind using varying noise levels is twofold. (i) Learning the score functionfor larger values of is easier, and thanks to parameter sharing ( is the same for all noise levels), doingso is helpful for learning the score function for small values of . (ii) Once the model is trained, differentnoise levels are also used within an annealed sampling scheme. sX( , T ) is used alongside Markov chainMonte Carlo to generate a sample, which is then used to initialize another Markov chain that now usessX( , T 1); this process is repeated until sX( , 1) is used and works much better than only usingsX( , 1). Although this scheme produces approximate samples from pX1rather than from pX , as long as1 is small enough, the difference is negligible in practice.",
  "X0 pX ,(49)": "where : [0, T] R+ is a hyperparameter (often an affine function, i.e. (t) = min + (max min)t/T,where 0 < min < max), and (Bt)t[0,T ] denotes a D-dimensional Brownian motion. Other choices of SDEare possible, but we focus on the one above which is often referred to as a variance preserving SDE since it is assumed in some of the theoretical results that we will shortly discuss. Under mild regularityconditions, the SDE admits a unique solution (ksendal, 2003), thus characterizing the density pXtof Xtfor every t [0, T], and prescribes how to progressively transform the data density pX0= pX into the noisierdensity pXT. Note that, due to the added Gaussian noise (from the Brownian motion in Equation 49), pXtis a full-dimensional density for every t (0, T] regardless of the support of pX . Reversing (Xt)t[0,T ] provides a way to transform samples from pXTinto samples from pX . The reverseprocess (Yt)t[0,T ] := (XT t)t[0,T ] also obeys an SDE (Anderson, 1982; Haussmann & Pardoux, 1986):11",
  "Y0 pXT.(50)": "The main idea of score-based diffusion models is to leverage this reverse SDE to build a generative model. Inorder to achieve this, some approximations are needed. First, since pXTis not known exactly, Equation 50is initialized at a known distribution pX. Formally, (Yt)t[0,T ] is approximated by ( Yt)t[0,T ], where",
  "Y0 pX.(51)": "We denote the density of Yt as pXT t, and will shortly explain how pXis chosen so as to be close topXT. The score yt log pXT t( Yt) is also unknown, and thus must be approximated as well. Score-baseddiffusion models leverage neural networks to construct sX : X (0, T] X with the goal of approximatingthis function, i.e. sX (x, t) x log pXt (x) for all x X and t (0, T]. We will also soon explain how thisnetwork is trained, but for a given sX , ( Yt)t[0,T ] is approximated by ( Yt)t[0,T ], where",
  "(52)": "We denote the density of Yt as pXT t.Ideally, a model sample would be obtained by perfectly solvingEquation 52. In practice this SDE must be discretized and a numerical solver must be used. The modeldistribution pX is thus given by the approximate solution of Equation 52 at time T. 10Readers unfamiliar with SDEs can understand Equation 49 through its Euler-Maruyama discretization: split [0, T] into nsub-intervals of equal length t,n = T/n, sample X0,n pX , and set Xtk+1,n = Xtk,n (tk)",
  "Xtk,nt,n +": "(tk)Btk ,n fork = 0, . . . , n 1, where tk = kt,n, and where Btk ,n = Btk+1 Btk N( ; 0, t,nID) are independent. This procedurecharacterizes Xt,n at the times tk for k = 0, . . . , n, and linearly interpolating between them yields a continuous stochasticprocess (Xt,n)t[0,T ], the limit of which as n corresponds to the process specified by the SDE.11Note that the Brownian motions in Equation 49 and Equation 50 are not in general the same Brownian motion (they justhave the same distribution), but we do not differentiate between them for notational simplicity. Note also that Equation 50differs from the corresponding equation in (Song et al., 2021b) since dt in our notation corresponds to dt in theirs.",
  "where w : [0, T] R+ is a weighting function (set as a hyperparameter)": "Additionally, as long as is such that 2T 1 as T , Equation 53 also implies that pXT |X0(|x0) stopsdepending on x0 in the sense that it converges to N( ; 0, ID) as T : this observation provides anavenue for approximately sampling from pXT, namely by setting pXto a standard Gaussian.",
  "x0 X.(56)": "These equations are related in that, under some regularity conditions, if the ODE is initialized at x0 = X0 pX , then xT will have the same distribution as XT . Equation 56 can of course not be solved because thetrue score function is unknown, but it can be approximated by replacing it with the learned score function,resulting in the new ODE:",
  "(b)": ": Models for M = {(x1, x2) | x21 + x22 = 1}, the unit circle in X = R2.(a) Using a singleencoder-decoder pair to model the circle. The pair approaches numerical non-invertibility since the encoderf must map two nearby points in M (in black) to two distant latent points in Z (also in black). (b)Illustration of implicit manifolds. Here, M is characterized as the 0-level set of F : R2 R. Specifically,F(x1, x2) = 1 (x21 + x22) is shown in red, and the grey plane corresponds to {x R3 | x3 = 0}. Neuralimplicit manifolds use no autoencoders, and instead attempt to learn F so that its 0-level set, F 1({0}),matches M. (c) The problem depicted in (a) can also be circumvented by employing multiple encoder-decoder pairs, each one using its own latent space and covering a different part of M.",
  "Conditional Flow Matching": "Continuous normalizing flows (.1.3) as defined through Equation 30 were originally trained throughmaximum-likelihood. As discussed in .1.2, diffusion models can be interpreted as providing analternative training objective for continuous NFs. Conditional flow matching (CFM; Liu et al., 2023; Albergo& Vanden-Eijnden, 2023; Lipman et al., 2023) provides additional alternatives, the main variant of whichwe cover here.",
  "0w(t)EXtpXt [sX (Xt, t) xt log pXt (Xt)22]dt,(59)": "where we will assume w(t) = 1 to better highlight their similarities to CFM. As discussed in .3 and.1.1, directly optimizing Equation 59 cannot be done in practice because pXtis unknown. However,by conditioning on X0, the equivalent yet tractable objective in Equation 55 is obtained. Consider a true vector field v : X [0, T] X in the sense that its corresponding forward ordinarydifferential equation (Equation 30) maps samples from pX at time t = 0 to samples from pZ at t = T. Notethat even when such a vector field exists, it is not unique. Similarly to diffusion models, CFM learns a vectorfield v by attempting to regress against v, i.e.",
  "EXtpXt [v(Xt, t) v(Xt, t)22]dt,(60)": "where pXtnow corresponds to the density of xt if dxt = v(xt, t)dt is initialized at x0 = X0 pX . Again,Equation 60 cannot be directly optimized because v, and hence also pXt , are unknown. Conditioning is onceagain the solution, except now conditioning is done on both X0 and XT which allows for explicit constructionof a target vector field that maps X0 to XT . There are many such vector fields, but the simplest is thevector field (XT X0)/T (Tong et al., 2024). Since this vector field is time-independent, sampling from thecorresponding pXtbecomes easy as one can take Xt = ((T t)X0 + tXT )/T, where X0 pX and XT pZ",
  "problem in Equation 60 for a particular v. Hence, under these conditions, the vector field v optimizedunder Equation 61 can still be used for generation with the reverse ODE in Equation 32": "Conditional flow matching through the lens of the manifold hypothesisWe begin by pointingout that Lipman et al. (2023) add a small amount of noise to the data while applying CFM, which enforcesfull-dimensionality of the target density. However, if no noise is added, the known correctness guarantees forCFM break down when pX is supported on a low-dimensional manifold. Kingma & Gao (2023) proved that,surprisingly, the objectives in Equation 55 for diffusion models and Equation 61 for CFM are equivalent givenappropriate hyperparameter choices. Since diffusion models are manifold-aware, at first glance this resultmight seem to imply that CFM must also learn distributions on unknown manifolds. However, this does notfollow from the result of Kingma & Gao (2023): the manifold-awareness of diffusion models is guaranteedwhen the backward stochastic differential equation (Equation 52) is used to sample from the model, whereasCFM always uses an ODE (Equation 32).16To the best of our knowledge, there is currently no formalanalysis of CFM under the manifold hypothesis analogous to that of Pidstrigach (2022) or De Bortoli(2022) for diffusion models.17 Nonetheless, due to its similarities with diffusion models, we conjecture thatCFM is indeed manifold-aware, which would be consistent with its strong empirical performance. We dohowever point out that if CFM successfully learns its target distribution in the manifold setting, then it mustexperience the numerical instabilities of likelihood evaluation that we established in .1.1, even ifthese instabilities do not manifest themselves during training since likelihoods do not appear in Equation 61.Finally, we also highlight the work of Kapusniak et al. (2024), who point out that Xt linearly interpolatingbetween data X0 and noise XT has some undesirable properties; they thus propose a modification of CFMwith the goal of ensuring that the interpolations remain close to geodesics in M, which they show leads toempirical improvements.",
  "Noisy Normalizing Flows": "Several models have been proposed based on the idea of adding noise to the data, training a normalizingflow (.1.3) on this noisy data, and then having a deflation procedure whose goal is to samplefrom pX rather than its learned noisy version. One such model is the denoising NF of Horvat & Pfister(2021), which itself is heavily based on the theoretical results of Horvat & Pfister (2023).18 Motivated bythe inherent struggles of flow-based architectures on manifold-supported data (described in .1 and.1.3), denoising NFs attempt to model an inflated version of the data distribution with addednoise, and then provide conditions under which this noise can be deflated to recover the true on-manifolddensity. Similar to denoising score matching (.1.1), the inflation step of Horvat & Pfister (2021)consists of simply adding full dimensional Gaussian noise to the data, and building a density model forpX:= pX N( ; 0, 2ID), where > 0 is a hyperparameter. On the other hand, Horvat & Pfister (2023)discuss a more theoretically-grounded inflation step, where (D d)-dimensional Gaussian noise is addedto X pX along the normal space of M at X.19 While determining the normal space is only possible forknown manifolds, the authors argue that when d is much smaller than D, using full-dimensional Gaussiannoise is a good approximation of (D d)-dimensional noise in the normal space. We will shortly review thedeflation step. To model pX , Horvat & Pfister (2021) use a full-dimensional DGM pX. Their specific choice is a D-dimensional NF, albeit with a non-standard learnable latent distribution pZ . For some hyperparameter dmeant to approximate d, the first d latent coordinates are themselves modelled by a d-dimensional NF, andthe remaining D d latent coordinates use a zero-mean Gaussian with covariance 2IDd, matching the 16In turn, the result of Kingma & Gao (2023) does ensure that if the vector field learned through CFM is converted back to ascore function and used alongside the backward SDE for sampling, then the resulting model will be manifold-aware. However,this is not how CFM is sampled from in practice. Note also that any result ensuring that Equation 58 can indeed producesamples from pX in the manifold setting would in turn guarantee the manifold-awareness of CFM.17The closest analysis we are aware of is due to Gao & Zhu (2024), who provide a Wasserstein distance bound for the ODEsampler. However, this bound requires assumptions which are incompatible with the manifold setting, and thus does not ensuremanifold-awareness. This stands in contrast with the analogous bound for the SDE sampler by De Bortoli (2022), which doesguarantee manifold-awareness.18Horvat & Pfister (2023) was released first on arXiv despite having a later publication date than Horvat & Pfister (2021).19Normal in the geometric sense, not the Gaussian sense.",
  "pX(x) = pZ1 (z1)pZ2(z2) |det xf(x)| ,(62)": "where: f = g1with g being the D-dimensional NF, z = (z1, z2) = f(x) with z1 Rd and z2 RDd,and the latent density pZ is given by pZ (z) = pZ1 (z1)pZ2 (z2), where pZ1is the density corresponding to thed-dimensional NF model, and pZ2() = N( ; 0, 2IDd) with no trainable parameters. The idea is that thefirst d latent coordinates are noise-insensitive i.e. they are meant to denoise the data while the remainingD d coordinates are noise-sensitive. The model pXdefined above has no explicit manifold-awareness.Horvat & Pfister (2021) thus add an injective flow construction (reviewed further in .3.3) into themix, defining the actual (denoised) manifold-supported generative process pX by first sampling Z1 pZ1and then setting X = g((Z1, 0)), where 0 RDd and (, ) denotes concatenation so that (Z1, 0) RD.Horvat & Pfister (2021) refer to using the manifold-supported pX rather than the full-dimensional pXasdeflating pX.",
  "log pX(X) EXpXX g ((f(X)1, 0)) 22,(63)": "where > 0 is a hyperparameter, f(X)1 Rd corresponds to the first d coordinates of f(X), and onceagain 0 RDd. The regularizer encourages g to not need Z2 = f(X)2 (i.e. the last D d coordinatesof f(X)) to perfectly reconstruct X: this can be intuitively understood as providing an inductive biaswhich promotes capturing the added noise only through Z2, thus justifying the use of pX instead of pX. We now discuss the deflation aspect of this approach.Horvat & Pfister (2023) prove that a traineddenoising normalizing flow pX recovers pX under the following conditions: (i) the noise added to X pXis only added along the (D d)-dimensional normal space to the true manifold M at X, (ii) the noiseparameter is sufficiently small, and (iii) the manifold M is sufficiently smooth and disentangled (thiscondition is properly formalized by Horvat & Pfister (2023)). Condition (i) is not satisfied by the denoising normalizing flow, although Horvat & Pfister (2021) arguethat when D is much larger than d, N( ; 0, 2ID) is a good approximation for the (D d)dimensionalGaussian noise in the normal space; it is also worth reiterating that it would not be possible to add noisein the normal space without knowing the true manifold exactly in the first place. Condition (iii) is alsoworth discussing: it is entirely unclear if natural data observed in the wild is sufficiently smooth anddisentangled, so we may not have any guarantee of retrieving the true manifold-supported data distributioneven in the nonparametric regime. In summary, despite denoising NFs being an elegant approach, theirmanifold-awareness can only be ensured under potentially strong and hard-to-verify assumptions. Postels et al. (2022) proposed a very similar approach to denoising NFs, except the latent density used todefine pXis fixed as a standard Gaussian, no regularizer is used during training, and the deflation stepis done by first sampling X pX and then solving",
  "X = arg maxxlog pX (x) x X22.(64)": "Although Postels et al. (2022) do not rigorously justify their method, the intuition behind this deflationstep is sensible; since pX should spike around M when is small enough, Equation 64 can be informallyinterpreted as pulling X closer to M. Finally, Kim et al. (2020) use a continuum of noise levels in a manner reminiscent of diffusion models(.1.2). More specifically, they first construct a conditional normalizing flow g : Z [0, max] X,where max > 0 is a hyperparameter. For every [0, max], the flow g(, ) must be a diffeomorphism,whose inverse we denote as f(, ). They then condition the NF on the amount of added noise and trainthrough (conditional) maximum-likelihood:",
  "Spread Divergences": "As previously mentioned, attempting to minimize KL divergence through maximum-likelihood in the man-ifold setting can result in manifold overfitting (.1), and while adding a small amount of noise tothe data can circumvent the problem in theory, it might not in practice. Zhang et al. (2020a) propose tonot only add noise to the data, but to add the same amount of noise to the model as well. They formalizethis idea by introducing spread divergences. Here we will focus exclusively on the spread KL divergenceand Gaussian noise, but highlight that the same ideas can be applied to other divergences and (potentiallylearnable) noise distributions. Formally, for a fixed > 0, the spread KL divergence KL(p q) betweenprobability densities p and q is given by",
  "KL(p q) := KL (p q) ,(66)": "where p := p N( ; 0, 2ID) and q := q N( ; 0, 2ID), i.e. the spread KL divergence is the KLdivergence between noisy versions of p and q. The spread KL divergence has two important properties:KL(p q) 0, with equality if and only if p = q; and it is always meaningfully defined, since the noisyversions of the original distributions are always full-dimensional (.4). Together, these propertiesimply that the spread KL divergence provides a mathematically sensible objective to train generative modelsunder the manifold setting. While the noisy model pX N( ; 0, 2ID) always has a full-dimensional density, this density cannot in generalbe evaluated, and thus minimizing KL(pX pX ) is not immediately trivial. Zhang et al. (2020a) proposea very similar model to a variational autoencoder (.1.2), called the -VAE, where the conditionaldistribution of X given Z is now a point mass at g(Z), instead of a Gaussian as in Equation 22. In otherwords, this model is like a Gaussian VAE, except no additional noise is added to g(Z), i.e. all the noise inthis model comes from the low-dimensional Z. Note that, unlike VAEs, this model is not full-dimensional.Much like VAEs are trained to minimize an upper bound of KL(pX pX ) (or equivalently, maximizing thelower bound to the log-likelihood from Equation 24), -VAEs minimize an upper bound of KL(pX pX ),which as we will see ends up amounting to a simple modification to standard VAE training. Importantly,even if the supports of pX and pX do not overlap, their spread KL divergence is meaningfully defined andthus provides a valid objective that enables -VAEs to properly learn distributions on unknown manifolds.More specifically, -VAEs use a variational posterior density qZ|Xand are trained through",
  "EZqZ|X(|X)[log N(X; g(Z), 2ID)] KLqZ|X(|X) pZ,(67)": "where pX:= pX N( ; 0, 2ID). Note that this objective is equivalent to that of VAEs from Equation 24,except Gaussian noise is added to the data, and the covariance of the decoder is not learnable, but rathermade to match the covariance of the noise that was added to the data. A point about -VAEs warrants discussion. A common cheat when sampling from a trained Gaussian VAEmodel is to sample Z pZ, and then output the decoder mean g(Z), rather than outputting a samplefrom a Gaussian centered at this point (i.e. the noise from the stochastic decoder is ignored). The use ofthe spread KL divergence elegantly justifies this common practice, since the noise (corresponding to theN(X; g(Z), 2ID) term in Equation 67) here is added due to the spread KL divergence, rather than beingpart of the model itself. Zhang et al. (2020a) did not make this observation when introducing -VAEs, andto the best of our knowledge, we are the first to point this out. Finally, Zhang et al. (2023) aim to extend the use of the spread KL divergence beyond VAEs, and propose aprocedure to use it within the context of normalizing flows (.1.3). Since the spread KL divergence",
  "Manifold-Awareness through Support-Agnostic Optimization Objectives": "In .1 we showed various DGMs which learn manifolds by first adding noise to pX , and then using afull-dimensional objective like score matching or KL minimization (i.e. maximum-likelihood). An alternativeto these approaches is using an objective that is agnostic to the supports of the underlying distributions beingcompared. In particular, divergences between probability distributions which metrize weak convergence, suchas Wasserstein distances (.5) or maximum mean discrepancy (.6), provide such support-agnostic objectives. We now cover DGMs which are trained through these objectives.",
  "Wasserstein Generative Adversarial Networks": "Arjovsky et al. (2017) proposed to minimize Wasserstein distance (.5) between pX and pX as away to address the issues arising from attempting to train generative adversarial networks using variousf-divergences outlined in .2, resulting in strong empirical performance (Karras et al., 2018; 2019;2020). In the GAN context considered by Arjovsky et al. (2017), where pX is again given by the distributionof X = g(Z), where Z pZ (once more, formally, pX is given by the pushforward of pZ through g) andpZ is fixed (e.g. standard Gaussian), this means changing the GAN objective from Equation 41 to",
  "minW1(pX , pX ),(69)": "which as mentioned in .5, provides a support-agnostic optimization objective for training pX . Ar-jovsky et al. (2017) enforce the Lipschitz constraint on h through weight clipping, which was later improvedupon by Gulrajani et al. (2017a) and by Miyato et al. (2018). The former use a regularizer encouragingxh(x)2 to be close to 1, while the latter regularizes the spectral norm of the weight parameters of h;both obtain much better empirical performance than weight clipping.",
  "Wasserstein Autoencoders": "As described in .2.1 for Wasserstein generative adversarial networks, Arjovsky et al. (2017) leveragedthe dual formulation of W1 (Equation 17).Tolstikhin et al. (2018) proposed Wasserstein autoencoders(WAEs), which instead leverage the definition of Wc (Equation 16) which remains a support-agnosticobjective for training DGMs.In particular, they proved that when pXis given by the distribution ofX = g(Z) where Z pZ (once again, pX formally corresponds to the pushforward of pZ through g), theoptimal transport cost can be written as",
  "Wc(pX , pX ) =infqZ|XQ(pX ,pZ) EXpXEZqZ|X(|X) [c(X, g(Z))],(70)": "where Q(pX , pZ) is the set of conditional (on X) densities on Z whose marginal matches pZ, i.e. Q(pX , pZ) :={qZ|X : X (Z) | qZ = pZ}, where qZ := EXpX [qZ|X(|X)]. Tolstikhin et al. (2018) propose two lossesto train WAEs, both inspired by this property: analoguously to variational autoencoders, a conditionaldistribution qZ|Xis parameterized (e.g. Equation 26), and the first loss is given by",
  "by first sampling X pX , and then sampling Z|X qZ|X(|X), so that optimizing Equation 71 is tractable": "We find it relevant to highlight some differences between WAEs and other DGMs. (i) Unlike GANs (Sec-tion 4.2) and Wasserstein GANs, WAEs use an adversarial loss on the latent space Z rather than on ambientspace X. This helps stabilize WAEs, as adversarial losses on ambient space can be notoriously unstable.Nonetheless, properly trained Wasserstein GANs tend to empirically outperform WAEs. (ii) WAEs are alsovery similar to variational autoencoders (.1.2) e.g. if c(x, y) = x y22, the WAE and GaussianVAE losses become extremely alike but the KL term in the VAE loss (Equation 24) encourages qZ|X(|X)to match pZ for every X sampled from pX , whereas WAEs only encourage this to happen on average, i.e.EXpX [qZ|X(|X)] = qZ = pZ.",
  "EZqZ|X(|X)[c(X, g(Z))]+ MMD2kqZ , pZ(72)": "for some kernel k : Z Z R, which results in an objective with no adversarial component. The choiceof which objective to use on latent space to enforce qZ = pZ has also been expanded in follow-up work,for example Kolouri et al. (2018) use the sliced Wasserstein distance, and Patrini et al. (2020) use relaxed(Sinkhorn) optimal transport.",
  "Finally, we point out that Tolstikhin et al. (2018) found that using arbitrarily distributions qZ|Xwas not key": "for good empirical performance, and they thus restrict Q(pX , pX ) to only contain point masses, i.e. qZ|X(|x)is given by a point mass at f(x). This choice, which amounts to using deterministic rather than stochasticencoders, reduces the first term in Equation 71 and Equation 72 to a reconstruction error (as measured byc), EXpX [c(X, g(f(X)))].",
  "Generative Networks Based on Maximum Mean Discrepancy": "Dziugaite et al. (2015) and Li et al. (2015) propose another way to train the model pX corresponding toX = g(Z) and Z pZ (again, we point out that formally, this model corresponds to the pushforward of pZ through g): by minimizing maximum mean discrepancy (.6). Although their motivation was toavoid the adversarial training involved in generative adversarial networks (.2) rather than to modelmanifold-supported data, the resulting objective provides a mathematically principled way of training DGMsunder the manifold setting. The training objective of generative moment matching networks is simply",
  "X h (h(X)) 22,(75)": "where > 0 is a hyperparameter, and the second term encourages h to admit h as a left inverse on thesupports of pX and pX . Similarly to Wasserstein GANs (.2.1), the empirical performance of MMDGANs benefits from gradient regularization during training (Bikowski et al., 2018; Arbel et al., 2018).",
  "Generalized Energy-Based Models": "Generalized energy-based models (GEBMs; Arbel et al., 2021) combine generative adversarial networks(.2) with energy-based models (.1.4). GEBMs consist of a fixed prior pZ supported on Z,a generator g1 : Z X, and an energy function E2 : X R, where we explicitly distinguish between theparameters of these components as = (1, 2). As in GANs, the prior along with the generator implicitlydefine the density pg1 of X = g1(Z), where Z pZ.20 This is not a full-dimensional density, but rather itis supported on the model manifold M1 := g1(Z).21 GEBMs define an EBM on M1 by re-weighting pg1through the use of E2 as an energy function:",
  "KALEpX pg1 :=supEE,R1 EXpX [E(X)] EZpZeE(g1(Z)),(77)": "where E is a set of Lipschitz energy functions satisfying certain regularity conditions (which are satisfied byfeed-forward neural networks). They then show that KALE(pX pg1) is a meaningfully defined divergencebetween pX and pg1, even when their supports do not perfectly overlap (i.e. it metrizes weak convergence,more details are provided in the grey box below), so that it provides a sensible objective to train the generator.As a divergence, KALE is intimately related to the KL divergence: Arbel et al. (2021) also show that if E2achieves the supremum in Equation 77, then KL(pX pX ) KL(pX pg1).23 Therefore, the re-weightingdone in Equation 76 to pg1 indeed improves upon simply using pg1. Putting these properties together, Arbelet al. (2021) train GEBMs through",
  "min1 max2, 1 EXpX [E2(X)] EZpZeE2(g1(Z)),(78)": "20Note that in .2 we denoted pg1 as pX , but we use different notation here as GEBMs further modify pg1.21Formally M1 need not be a manifold, even if g1 is smooth, as it might have points of self-intersection. The measure-theoretic formulation of GEBMs in the grey box below remains nonetheless valid.22More formally, the symbol in Equation 76 should be understood as proportional within M1, only integrating over themodel manifold, i.e. pX (x) = pg1(x)eE2 (x)/",
  "pZ (z) pZ(z)eE2(g1(z)),(79)": "and then setting X = g1(Z) will produce a sample from pX . Note that pZ is now a full-dimensional densityin Z, and it can thus be sampled through Markov chain Monte Carlo as standard in EBMs. We point outthat GEBMs are intimately linked to two-step models, which we discuss in .3. Finally, we highlight some related works. Che et al. (2020) proposed a similar model to GEBMs, but theirmodel is trained as a standard GAN (Equation 41), and the EBM is defined post-hoc by using the discrimi-nator h; despite the similarity with GEBMs, this procedure does not endow manifold-unaware GANs withmanifold-awareness. Birrell et al. (2022) constructed a class of divergences, which KALE belongs to, byextending its relationship with the KL divergence to general f-divergences; and Gu et al. (2024) leveragedthese divergences, along with optimal transport (.5), to obtain a manifold-aware adversarial trainingobjective for continuous normalizing flows (.1.3). We now formalize the presentation of GEBMs. Here we denote pg1 as a probability measure, g1#PZ,instead of as a density.The model distribution PXis then defined through its Radon-Nikodymderivative with respect to g1#PZ,",
  "EXg1#PZ[eE2(X)],(80)": "where the expectation is assumed to be finite. Arbel et al. (2021) proved under mild conditions that:(i) KALE(PX g1#PZ) 0 with equality if and only if PX = g1#PZ; and that (ii) for a sequence ofgenerators (g1,t)t=1, KALE(PX g1,t#PZ) 0 as t if and only if g1,t#PZ PX as t .",
  "Principal Component Flows": "Principal component flows (PCFs; Cunningham et al., 2022) are a variant on standard normalizing flows(.1.3) that seek to uncover manifold structure in a manner analogous to principal componentanalysis (PCA) and related to disentanglement (Bengio et al., 2013).If g : Z X is a normalizingflow, the eigenvectors {1(x), . . . , D(x)} of zg(z)zg(z) are said to be the principal components of gat x = g(z) and can be ordered using the corresponding eigenvalues as in standard PCA. The principalcomponents at x represent the principal axes of variation in the flows density pX . In a manifold-learningcontext, principal axes with small eigenvalues represent off-manifold directions, while those with the highesteigenvalues represent primary directions of variation along the manifold, as illustrated in (a): wehighlight that here pX is assumed full-dimensional and to concentrate around M, rather than being strictlymanifold-supported. We now discuss a special case of PCFs as an introduction; for a presentation of the method in more generality,see the work of Cunningham et al. (2022). PCFs involve the notion of contour log-likelihoods, which herecan be interpreted as the likelihood along the ith coordinate curve of the flow,",
  "for i {1, 2, . . . , D}, where pZ(z) = Di=1 pZi(zi) is a coordinatewise factorization of the latent prior (whichis typically Gaussian), and xg(x)ii is the ith entry on the diagonal of xg(x)": "The goal of training a PCF is to align the principal components of g with its latent coordinates on themanifold ((b)). One of the key insights of Cunningham et al. (2022) is that the difference betweenthe models log density log pX (x) and the sum of its contour log-likelihoods measures the diagonality of",
  ",(82)": "where f(X)i denotes the ith coordinate of f(X).24 I() is a non-positive quantity such that I() = 0 if andonly if the latent coordinates are perfectly aligned with the flows principal components at each point x X.Maximizing I() as a regularizer while maximizing the likelihood aligns the flows latent coordinates withthe principal manifolds of the data. We highlight that while this objective was derived to provide a usefulinductive bias when manifolds are involved, it remains nonetheless based on full-dimensional likelihoods, andis thus subject to the corresponding pathologies (.1 and .1.1). Canonical manifold flows (CMFs; Flouris & Konukoglu, 2023) take a related approach which directly penalizesoff-diagonal elements of zg(f(X))zg(f(X)). This results in a potentially looser regularizer whichis designed to achieve the same goal at optimality as PCFs. Both PCFs and CMFs have been shown tonaturally represent M using a subset of the flows latent coordinates, meaning they automatically discoveran estimate d of the dimension d of M without the practitioner having to set it as a hyperparameter.",
  "Manifold-Awareness through Two-Step Models": "Except for generative adversarial networks, all the DGMs described in are manifold-unaware asa direct consequence of misspecified dimensionality: the data is d-dimensional, whereas the model is D-dimensional. The methods described in .1 address this misspecification by adding noise to thedata, making it D-dimensional, whereas those from .2 use manifold-appropriate losses. Another",
  "min1, EXpXX g1 (f(X)) 22,(83)": "or any variant such as variational autoencoders (.1.2), although we will see in .4.1that non-autoencoder-based choices are also possible. Importantly, the goal in this step is to performmanifold learning rather than generative modelling, so e.g. even if a VAE is used, it is interpretedas a regularized autoencoder rather than a generative model. Distribution learningThis step consists of learning the distribution on the manifold obtained inthe previous step. In the standard setup where manifold learning is performed with an autoencoder-based model, a pre-trained encoder f and decoder g1 pair is available. The encoder defines adistribution qZ of encoded data f(X) where X pX (formally, qZ is the pushforward density ofpX through f), which can be learned by instantiating a DGM pZ2 on Z, and training it with anyof the methods covered in this survey (but changing the target distribution from the D-dimensionalpX to the d-dimensional qZ), while keeping the encoder f and decoder g1 frozen. Then, oncethis low-dimensional DGM is trained, resulting in pZ2, the distribution of the two-step model on thelearned manifold can be sampled through X = g1(Z), where Z pZ2 (i.e. pX is formally given bythe pushforward of pZ2 through g1). Importantly, by solving the generative modelling task in Z instead of X, the support of the target distributionqZ is f(M) Z, whose dimension should intuitively be given by min(d, d). If the latent dimension dis chosen properly (i.e. d = d), we should then expect qZ to be full-dimensional within Z.This full-dimensionality stands in contrast to the case discussed in .1 where the target distribution pX is d-dimensional but its corresponding ambient space X is D-dimensional. Furthermore, even if d is overspecifiedas d < d < D, the dimensionality gap i.e. the ambient dimension of the model minus that of the truemanifold of the second-step model is d d, which is smaller than D d.25 Thus, we should intuitivelyexpect any manifold-related woes arising from dimensionality mismatch to be milder than the correspondingfull-dimensional issues. Many two-step models have been proposed in the literature, sometimes with the manifold setting in mind,and some other times simply for tractability, as training DGMs on a low-dimensional latent space ischeaper than doing so in high-dimensional ambient space. Loaiza-Ganem et al. (2022a) provided a the-oretical justification for all of these models, proving that under mild regularity conditions, when d = d and EXpX [X g1(f(X))22] = 0 (i.e. perfect reconstructions), then: (i) qZ is indeed full-dimensional,and thus pZ2 can be any DGM, even if manifold-unaware (e.g. trained through maximum-likelihood), andstill learn qZ; and (ii) transforming samples from pZ2 through g1 is equivalent to sampling from pX , i.e.",
  "EZqZ|X(|X)[log pX|Z1(X|Z)] KLqZ|X(|X) pZ2,(84)": "remains a valid objective (assuming pX is full-dimensional) for end-to-end training of pX|Z1and pZ2. Depend-ing on the choice of pZ2 additional computational tricks might be required to efficiently optimize the ELBO.Tomczak & Welling (2018) instantiate pZ2 as a Gaussian mixture model; Snderby et al. (2016), Vahdat &Kautz (2020), and Child (2021) use learnable hierarchical priors; Chen et al. (2017) use normalizing flows;Pang et al. (2020) use energy-based models; and Vahdat et al. (2021) use diffusion models (.1.2).When pX|Z1is a flexible enough full-dimensional density as in Equation 22, all these models remain suscepti-ble to the manifold overfitting issues discussed in .1 and .1.2, despite directly encouragingthe encoder to learn representations whose distribution can be easily recovered by pZ2. Even though designing an end-to-end objective for training in a manifold-aware fashion is intuitively desirable,doing so is not always straightforward.To see why, consider a two-step model whose first-step loss isgiven by Equation 83, and whose second-step model is trained by minimizing D(qZ, pZ2) over 2 for somedivergence D between probability distributions. Let us further assume that D(qZ, pZ2) cannot be computedwithout evaluating qZ, but that D(qZ, pZ2) = L(pZ2; qZ) + c(qZ), where L(pZ2; qZ) can be computedwithout evaluating qZ, and where c(qZ) does not depend on pZ2. Divergences with these properties areprevalent; the KL divergence (.4) is an instance, where L(pZ2; qZ) = EZqZ [log pZ2(Z)] and",
  "min, EXpXX g1(f(X))22+ DqZ , pZ2.(86)": "In short, Equation 85 does not provide a valid objective for manifold-aware end-to-end training since c(qZ )cannot be ignored when is not fixed after the first step of training. Unfortunately, although Equation 86specifies a principled objective to address the issue, it remains intractable when c(qZ ) cannot be computed. More formally, a trained autoencoder-based two-step model is given by a distribution PZ2 on Z alongwith a decoder g1 : Z X, and the model distribution is given by PX = g1#PZ2. The result ofLoaiza-Ganem et al. (2022a) justifying two-step models states that, under mild regularity conditions,if d = d and EXPX [X g1(f(X))22] = 0, then:",
  "g1#QZ = PX": "The first point ensures QZ admits a density with respect to d, so that it is full-dimensional. Thesecond point ensures that if the target distribution QZ is properly learned during the second stepthen two-step models recover the true data-generating distribution, i.e. if PZ2 = QZ then PX = PX .",
  "Two-Step Models Minimize Wasserstein Distance": "Before continuing our review of existing two-step models (.3), we highlight that these models can beinterpreted through an optimal transport lens (.5). To the best of our knowledge, this observationhas not been made in the literature, and constitutes a novel contribution of our work. Here we still considerthe model pXgiven by the two learnable components g1 and pZ2, and will use the notation introducedfor Wasserstein autoencoders (.2.2). Key to our insight is Equation 70, which, for the model pXconsidered here, can be rewritten as",
  "Wc(pX , pX ) inffF(pX ,pZ2) EXpX [c (X, g1(f(X)))] ,(88)": "where F(pX , pZ2) is the set of functions f : X Z such that if X pX , then f(X) pZ2. To see thatEquation 87 indeed implies Equation 88, simply note that if f F(pX , pZ2), then the conditional (on X = x)distribution on Z given by the point mass at f(x) is in Q(pX , pZ2). Equation 88 is used to justify the useof deterministic encoders within WAEs: doing so minimizes an upper bound of Wc(pX , pX ). We also pointout that, assuming c(x, y) is minimal if and only if x = y, the bound becomes tight at optimality as long asperfect reconstructions are achievable with a deterministic autoencoder (i.e. X = g1(f(X)), see .2and .4 for discussions of when this is possible with continuous autoencoders). For an encoder f, we let qZ be the distribution of f(X) where X pX (formally, qZ is the pushforwarddensity of pX through f), and note that f F(pX , pZ2) is equivalent to qZ = pZ2. In turn, Equation 88",
  "subject to qZ = pZ2.(89)": "The key difference between this objective and that of WAEs is that the distribution on latent space isnow learnable instead of being fixed. While this distinction with WAEs might seem conceptually trivial, itenables minimizing optimal transport cost through a two step process: in the first step, 1 and are trainedto minimize the reconstruction error, EXpX [c(X, g1(f(X)))], with no regard for the constraint. This stepresults in a now fixed distribution qZ on Z, which of course need not match pZ2. Thanks to pZ2 beinglearnable and 2 not appearing in the first-step objective, this mismatch can be addressed in the second step,where any objective over 2 to match pZ2 and qZ can be used. For example, if the second-step model weretrained through maximum-likelihood, its objective would be26",
  "min2 KLqZ pZ2,(90)": "which indeed satisfies the constraint in Equation 89 at optimality. In other words, two-step models solveEquation 89 by optimizing an unconstrained version of the objective during the first step, and then ensuringthe constraint is actually satisfied during the second step. Crucially, the second step does not affect theoptimality of the first step because EXpX [c(X, g1(f(X)))] does not depend on 2, so that two-step modelsindeed provide a valid way of solving Equation 89. We now make some additional observations. (i) When c is given by the squared Euclidean distance, the corre-sponding loss for the first-step model is exactly that of a standard autoencoder (Equation 6 and Equation 83).When the first-step model is trained through a different autoencoder-based objective, e.g. Equation 24, wecan simply interpret the model as a regularized autoencoder as long as it encourages perfect reconstructionsat optimality. (ii) Although two-step models are often trained using deterministic encoders, using arbitrarilyflexible stochastic encoders would imply that Wc(pX , pX ) is being minimized rather than an upper bound. In summary, we have justified the manifold-awareness of autoencoder-based two-step models through optimaltransport. We believe that this result is not only interesting on its own, but also hope that by establishing aconnection between seemingly unrelated manifold-aware model classes namely two-step models and thosewhich are trained through support-agnostic optimization objectives (.2), such as WAEs it willenable future improvements to both.",
  "inffF EXPX [c (X, g1 (f(X)))] ,(92)": "where F := {f : X Z | f is measurable}, involves an infimum over measurable functions (which isthen minimized over 1 during the first step). In the nonparametric regime (.2) we assumethat neural networks are flexible enough to approximate any continuous function arbitrarily well,but this property need not a priori extend to measurable functions. Intuitively this should howevernot be a problem thanks to Lusins theorem which, informally, states that in certain settingsany measurable function can be approximated by a continuous one. It is nonetheless pertinent toshow that the infimum in Equation 92 can actually be replaced by a corresponding infimum over",
  "Proof. See Appendix B.2": "This result allows us to formally interpret two-step models as minimizers of an upper bound of theWasserstein distance in the nonparametric regime when the assumption in Equation 93 holds. Wepoint out that this is a mild regularity condition, as it is always satisfied in the common case whereX is compact and c is continuous (since continuous functions always achieve their supremums overcompact sets). Finally, we point out that Patrini et al. (2020) claimed that, as long as PX is non-atomic, then theinequality in Equation 91 is actually an equality. This would allow us to interpret two-step modelsas minimizing Wasserstein distance not an upper bound even when using deterministic encoders.However, Lee et al. (2024) found an error in the proof of Patrini et al. (2020), so that only the upperbound interpretation remains valid.",
  "Latent Diffusion Models": "Latent diffusion models (Rombach et al., 2022; Peebles & Xie, 2023; Zhang et al., 2024) are another classof two-step models (.3). They first train a regularized autoencoder, which combines a Gaussianvariational autoencoder objective (.1.2) with various potential regularizers (Larsen et al., 2016;Higgins et al., 2017; van den Oord et al., 2017). Once this autoencoder-based model is trained and thecorresponding low-dimensional representations obtained, a diffusion model (.1.2) sZ2 : Z (0, T] Z is trained on them as the second-step model. As previously discussed, diffusion models can learn manifolds, but their score function must diverge to infinityat the end of the backward process (Equation 52) as a consequence of the mismatch between the intrinsic andambient dimensions of the data. To test that latent diffusion models are not as sensitive to this numericalpathology, we trained a diffusion model and a latent diffusion model on the CIFAR-10 dataset (Krizhevsky &Hinton, 2009), with all experimental details provided in Appendix C. We plot the average squared Euclideannorm of the score functions, normalized by their dimension,27 along generated paths in : it is evidentthat the score function of diffusion models on latent space exhibits much better numerical behaviour thanwhen these models are trained on ambient space. This result is suggested by theory, and to the best of ourknowledge, we are the first to empirically confirm it. Currently, latent diffusion models are amongst the best performing DGMs empirically, and the manifold lensprovides a convincing explanation for this: (i) they can learn manifolds; (ii) they alleviate the numericalissues of diffusion models; and (iii) they are robust to misspecification of the dimension of the latent space,in the sense that even if d < d (i.e. the latent dimension is specified as larger than the true intrinsicdimension), they still learn their target distribution albeit with an exploding score function. To see this,simply note that setting d < d results in a second-step model whose target distribution qZ is still supported 27Note that normalizing squared Euclidean norm by dimension is the most natural way of enabling comparisons acrossdimensions.To see this consider a constant vector with all entries equal to 1, which has a squared 2 norm equal to itsdimension; or consider a standard Gaussian vector, whose expected squared Euclidean norm also matches its dimension.",
  "Injective Normalizing Flows": "Ordinary normalizing flows (.1.3) are full-dimensional density models with D-dimensional latentspaces, conflicting with the d-dimensional nature of pX . To correct this mismatch, a line of research startedby Kumar et al. (2020) has proposed to shrink the NFs latent space dimensionality to d < D, allowing themodel to represent densities on a low-dimensional submanifold of X. Whereas standard NF architecturesmust be bijective, g1 now cannot be bijective because it maps from d to D dimensions. The most one canask is that g1 be injective, resulting in the injective normalizing flow (INF). Brehmer & Cranmer (2020) enforce injectivity architecturally, by constructing g1 as a zero-padding opera-tion followed by a D-dimensional NF, in which case the left inverse f1 is given by inverting this NF and ap-plying a projection operation; this is the same construction as the one used by denoising NFs (.1.4).28",
  ",(95)": "where z = f1(x). Unlike standard NFs, INFs cannot be navely trained through maximum-likelihood for twomain reasons: (i) the involved determinant is much more computationally challenging to compute and opti-mize than in standard NFs, and more importantly (ii) Brehmer & Cranmer (2020) showed that doing so wouldresult in pathological solutions. To understand why, recall that Equation 95 is only valid when x g1(Z).When x lies outside g1(Z), the right hand side of Equation 95 evaluates to pX (x1(x)), where x1(x) :=g1(f1(x)) can be thought of as a projection of x onto g1(Z). Since g1(Z) need not perfectly match the data",
  "manifold M, attempting to maximize EXpX [log pZ2(f1(X)) 1": "2 log | det(zg1(f1(X))zg1(f1(X)))|]over would thus result in maximizing the likelihood of projected data. As illustrated in , this ob-jective can admit pathological solutions where the projections collapse onto a single point whose likelihoodis sent to infinity. To circumvent this issue, Brehmer & Cranmer (2020) propose to train INFs as two-step models (.3),where g1 and f1 are trained to minimize an 2 reconstruction error as in Equation 83; this training procedurealso obviates the need to optimize through the determinant in Equation 95. Brehmer & Cranmer (2020) theninstantiate pZ2 as a d-dimensional NF, which is trained on encoded data f1(X), where X pX . Kothariet al. (2021) follow up on this work by proposing a more efficient architecture for g1. Kumar et al. (2020)originally proposed relaxed INFs, for which the encoder f is parameterized separately (and encouraged toinvert the decoder on M through a reconstruction error), and where injectivity is instead encouraged byregularizing the singular values of zg1(f(X)) for X pX . They then take the second-step density pZ2 asa Gaussian mixture model. By virtue of being two-step models, all these DGMs are manifold-aware. End-to-end trainingAs mentioned in .3, it is intuitively desirable to find an end-to-end objec-tive to train two-step models, and INFs provide particularly interesting opportunities. To circumvent thepathological behaviour of nave maximum-likelihood training of INFs outlined above, several works encourageperfect reconstructions with the goal of ensuring that the model manifold matches the true data manifold:",
  "logdetzg(f1(X))zg1(f1(X)) X g1(f1(X))22,(97)": "where > 0 is a hyperparameter. Much like how normalizing flows focus on tractability of the log-det-Jacobian term, a central theme of end-to-end injective flows is tractability of the second term in Equation 97,which we will refer to as the log-det-JJ term. One approach is to impose structural constraints on g1 tomake the log-det-JJ term easily computable, albeit at the cost of expressiveness, for example using confor-mal embeddings (Ross & Cresswell, 2021). A concurrent approach, pursued by Caterini et al. (2021b), is toapproximate the gradient of the log-det-JJ term with respect to 1 through a combination of Hutchinsons",
  "Overcoming Topological Obstacles to Manifold Learning": "As mentioned in .3, one might want to set the latent dimension d of an autoencoder to d. Yet, asdiscussed in .2, when d = d, perfect manifold learning is not always achievable through bottleneckmethods for topological reasons. We illustrate this problem, which can cause downstream issues with densityestimation, in (a). In particular, if M has any non-trivial topological properties such as holes ordisconnected components, any attempt to model M as the image of a decoder g will cause numericalinstability in g (Cornish et al., 2020; Salmona et al., 2022). This problem was originally identified in thecontext of normalizing flows (.1.3), with a line of work that first appends additional dimensionsto X, and then trains a normalizing flow on the augmented space (Dupont et al., 2019; Chen et al., 2020;Huang et al., 2020). While these techniques indeed increase the stability and expressiveness of the flowitself, they still produce full-support densities that can never truly model non-trivial topological structuresin data. Other works have leveraged tools from the field of topological data analysis (Chazal & Michel,2021; Barannikov et al., 2022) to explicitly regularize autoencoders with the goal of encouraging f(M) toshare topological properties with M (Moor et al., 2020; Trofimov et al., 2023). Empirically, these methodshave been shown to decrease topological mismatch; yet, as argued in the grey box in .2, completelyeliminating this mismatch is theoretically impossible when the root cause of the problem is the non-existenceof a topological embedding of M into Z, rather than the loss used to train the autoencoder. Alternatively,to more faithfully tackle topological issues, some works have proposed to restructure the manifold-learningstep to better reflect the ways manifolds are defined in theory; we cover these approaches in detail below.",
  "Neural Implicit Manifolds": "Under some conditions, manifolds with non-trivial topologies can be defined using level sets of functions.In particular, a level set of a smooth function F : RD RDd represents a d-dimensional manifold ifits Jacobian has full rank on that level set (Lee, 2012). We illustrate such an implicitly defined manifold which cannot be characterized with a single encoder-decoder pair in (b). Neural implicit manifold learning (Ross et al., 2023) operationalizes this fact by modelling M as the zeroset of a neural network F1 : X RDd. The network F1 is trained to align its zero set F 11 ({0}) with Mwhile being regularized to have full rank on M. The following loss is used,",
  ",(98)": "where: Y is independent of X; qX1(y) eF1(y)22 with 1 = stopgrad(1) (as in energy-based models,see .1.4); U( ; SDd1) is the uniform distribution over SDd1 := {y RDd | y2 = 1}, the(Dd1)-sphere in RDd; > 0, > 0, and > 0 are hyperparameters; and ()+ := max( , 0). In thisloss, the first term ensures that F 11 ({0}) contains M, the second term prevents F 11 ({0}) from containingoff-manifold samples, and the third term regularizes the Jacobian of F1 to have full rank on M. Importantly, F1 here is not an encoder; it is best interpreted as defining D d non-linear constraints onthe data, thus leaving d degrees of freedom for the data manifold. The full-rank requirement can then be",
  "Multi-Chart Manifolds": "Typical techniques model the manifold globally using a single encoder-decoder pair. In general, however,manifolds can consist of a patchwork of many charts: mathematical objects that each serve roughly the samefunction as a single encoder-decoder pair (for a formal definition, please see Lee (2012)). Some manifoldsmay thus require many encoder-decoder pairs (f (i) , g(i) )ni=1 each using its latent space Zi to locally describesome subset of the manifold rather than a single one: we illustrate this fact in (c). Several works have taken this route to learn the data manifold. Schonsheck et al. (2019) first proposed amulti-chart latent space using multiple encoder-decoder pairs in a non-generative modelling context. Foreach incoming datapoint, the correct chart is selected dynamically using a prediction head for the encoder-decoder pair with the smallest reconstruction error. Kalatzis et al. (2021) propose multi-chart flows, in whichlikelihoods are defined using a mixture of injective normalizing flows (.3.3), with mixture weightsagain computed using a prediction head. On the other hand, Sidheekh et al. (2022) propose a mixture ofINFs in which chart membership for an incoming datapoint is computed using discrete latent assignments. Modelling a manifold with multiple charts imposes drawbacks.For one, each encoder-decoder pair hasits own latent space, so for a given datapoint x M, choosing the correct encoder can be a challenge.This makes it unclear how to perform tasks involving the manipulation of latent representations, such asinterpolation. In many cases, there is no single correct encoder, as the images of various decoders needto overlap to correctly define topologically complex manifolds (such as in (c)). The ambiguity ofchoosing the correct encoder is underlined by how differently each of the aforementioned methods attemptto do so.",
  "Disconnected Manifolds": "Another common source of topological complexity in the data manifold is when it consists of more than oneconnected component. This situation occurs, for example, in datasets with multiple disjoint classes. In thiscontext, theoretical analyses have shown that any decoder-based model will suffer from training instability(Salmona et al., 2022) and poor sample quality (Luzi et al., 2020).29One technique to improve samplequality is to avoid sampling from latent regions where the network g is unstable. Tanielian et al. (2020)propose, in the context of generative adversarial networks (.2 and .2.1), to reject samplesX = g(Z), where Z pZ, for which zg(Z) has a high Frobenius norm, which they show indicates anoff-manifold sample. Other work, discussed below, seeks to avoid instability entirely during training. A few general techniques have been proposed for modelling disconnected manifolds.One way is to usedisconnected (or near-disconnected) latent distributions, which aims to match the topology of the supportof pZ with that of M. This is typically done with a Gaussian mixture model for pZ and has been proposedfor multiple classes of generative model (Nalisnick et al., 2016; Dilokthanakul et al., 2016; Jiang et al., 2017;Ben-Yosef & Weinshall, 2018; Izmailov et al., 2020).",
  "Discrete Deep Generative Models": "Since this surveys focus is on the manifold hypothesis, all of the models presented thus far are for continuousdistributions. Nonetheless, many DGMs assume that the ambient space X is discrete. For example, imagescan be modelled as having pixels which take only finitely many different values, rather than a continuum ofthem. In this case, pX and pX are both probability mass functions over X, and M X denotes the supportof pX . Formally, in this setting X is a 0-dimensional manifold, so that even when M is a strict subset ofX, it remains a 0-dimensional submanifold. In other words, there can be no dimensionality mismatch fordiscrete data since D = d = 0. In turn, this implies that mathematically, discrete likelihood-based DGMsare not exposed to problems such as manifold overfitting (.1) which arise from dimensionalitymismatch. This view of discrete DGMs through the manifold lens is useful, since it suggests that whenevera manifold-unaware DGM admits a straightforward discrete analogue, the latter should be preferred as itwill be unaffected by manifold-related woes.Indeed, as mentioned in .1.2, discrete variationalautoencoders (Gulrajani et al., 2017b; Vahdat & Kautz, 2020; Vahdat et al., 2021) empirically outperformtheir continuous variants. Similarly, discrete incarnations of likelihood-based autoregressive DGMs (Germainet al., 2015; van den Oord et al., 2016; Salimans et al., 2017; Parmar et al., 2018) outperform continuous ones(Uria et al., 2013). In contrast, discrete versions of diffusion models (Austin et al., 2021; Campbell et al.,2022; Meng et al., 2022) do not outperform their manifold-aware continuous counterparts (.1.2)when modelling images. Discrete and continuous DGMs nonetheless have similarities, despite the differences outlined above.Asdiscussed in , a key motivation behind the manifold hypothesis is to capture the intuition that M,the support of pX , is somehow sparse within X. This intuition often remains true in the discrete case: usingimages as an example once again, there are 256D possible discrete images (assuming each pixel entry takesone of 256 possible values), yet the subset of natural images is vanishingly small in comparison and containsorders of magnitude fewer elements. The main idea of continuous two-step models (.3), namely tofirst approximate the support of pX and then learn the distribution within, remains equally sensible in thediscrete case. van den Oord et al. (2017) proposed an autoencoder which recovers discrete representationsover which they train a discrete DGM; this idea that has been further developed, with strong empiricalresults (Razavi et al., 2019; Esser et al., 2021; Ramesh et al., 2021; Chang et al., 2022).We finish bypointing out that the discussion in .3.1 applies to all these discrete two-step models, so that theycan be interpreted as minimizing a potentially regularized upper bound of the Wasserstein distance betweenpX and pX which becomes tight at optimality, because in the discrete case, perfect reconstructions are alwaysachievable given enough capacity of the encoder and decoder.",
  "Conclusions and Future Outlook": "ConclusionsIn this survey we have carried out a review of deep generative models through the lens of themanifold hypothesis. This viewpoint presents a mathematically elegant perspective of DGMs, and suggeststhat manifold-awareness is an important necessary condition for strong empirical performance. We thusencourage researchers who are developing new DGMs to consider manifold-awareness as a desideratum, andask themselves: Can my deep generative model learn distributions supported on unknown low-dimensionalmanifolds? When the answer is yes, demonstrating this fact will strengthen the works motivation; and whenthe answer is no, this suggests that the DGM can be improved by endowing it with manifold-awareness either through a model-specific fix, or at least by training it on latent space as a two-step model (.3).We also showed that numerical instabilities of likelihood-evaluation are unavoidable in the manifold setting(.1.1) and that two-step models can be interpreted as minimizing a (potentially regularized) upperbound of the Wasserstein distance objective (.3.1). Future outlookFinally, we outline a non-exhaustive list of research directions involving deep generativemodels and their interplay with the manifold hypothesis. We believe these lines of inquiry are interesting,and mostly unexplored at the time of writing: Further understanding dimensionality mismatchThe effects of using a full-dimensionalmodel when the ground truth distribution is manifold-supported are well understood for likelihood-based models (.1) and diffusion models (.1.2), yet our grasp of the interplaybetween DGMs and the manifold hypothesis remains incomplete. For example, a theoretical un-derstanding of score matching (.3) and conditional flow matching (.1.3) undermisspecified dimension is lacking, as is the effect of using lower-bounded energy functions in energy-based models (.1.4). Improved training of DGMs with two-step architecturesTwo-step models as presented in.3 are manifold-aware. Yet, as also discussed in .3, two-step training does notencourage the encoder from the first step to represent the data in a way conducive to distributionlearning in the second step. Intuitively, this means there is room for improvement in how thesemodels are trained, and since the end-to-end approaches described at the end of .3 arein general manifold-unaware, several avenues remain open. For example, despite the existence ofregularizers for training autoencoders (Larsen et al., 2016; Higgins et al., 2017; Nazari et al., 2023),there is very little work explicitly designing autoencoders for two-step training. The only work weare aware of in this direction is by Hu et al. (2023), who propose to split the first step into twosub-steps: in the first sub-step the encoder is trained along with a low-capacity decoder, and inthe second sub-step the encoder is frozen and a more flexible decoder is trained. Another avenue isfinding an end-to-end objective to train this type of model in a manifold-aware fashion. Current end-to-end methods are manifold-unaware, despite providing a desirable inductive bias an exceptionbeing generalized energy-based models (.2.4) which cannot be readily extended beyondusing energy-based models (.1.4) as the latent distribution. We thus hypothesize that anyend-to-end, or improved two-step, manifold-aware procedure which can train diffusion models inlatent space while scaling to massive datasets (Schuhmann et al., 2022) is likely to improve uponcurrent commercial versions of latent diffusion models (.3.2). Extracting and leveraging manifold informationAny manifold-aware DGM which succeedsat learning its target distribution pX must have learned its support M as well, albeit perhaps implic-itly. Extracting information about M from a trained DGM is thus a natural problem, as is leveragingthis information for any practical use. Various works have shown that trained DGMs induce Rie-mannian metrics over the learned manifolds (Shao et al., 2018; Arvanitidis et al., 2018; Chadebec& Allassonnire, 2022; Sorrenson et al., 2024a), which can in turn be leveraged for interpolatingbetween datapoints and for improved sampling procedures. Several works have also shown thatDGMs can be used to estimate the intrinsic dimension of M (Tempczyk et al., 2022; Zheng et al.,2022; Horvat & Pfister, 2024; Kamkari et al., 2024b; Stanczuk et al., 2024), and these quantitieshave already proven useful for unsupervised out-of-distribution detection and to identify memorized",
  "Richard Beals, David H Krantz, and Amos Tversky. Foundations of multidimensional scaling. PsychologicalReview, 75(2):127, 1968": "Jens Behrmann, Paul Vicol, Kuan-Chieh Wang, Roger Grosse, and Jrn-Henrik Jacobsen. Understandingand mitigating exploding inverses in invertible neural networks. In International Conference on ArtificialIntelligence and Statistics, 2021. Heli Ben-Hamu, Samuel Cohen, Joey Bose, Brandon Amos, Maximillian Nickel, Aditya Grover, Ricky TQChen, and Yaron Lipman. Matching normalizing flows and probability paths on manifolds. In InternationalConference on Machine Learning, 2022.",
  "Ali Borji. Pros and cons of GAN evaluation measures. Computer Vision and Image Understanding, 179:4165, 2019": "Joey Bose, Ariella Smofsky, Renjie Liao, Prakash Panangaden, and Will Hamilton. Latent variable modellingwith hyperbolic normalizing flows. In International Conference on Machine Learning, pp. 10451055, 2020. Denis Boyda, Gurtej Kanwar, Sbastien Racanire, Danilo Jimenez Rezende, Michael S Albergo, Kyle Cran-mer, Daniel C Hackett, and Phiala E Shanahan. Sampling using SU(N) gauge equivariant flows. PhysicalReview D, 103(7):074504, 2021.",
  "Rewon Child. Very deep VAEs generalize autoregressive models and can outperform them on images. InInternational Conference on Learning Representations, 2021": "Rob Cornish, Anthony L Caterini, George Deligiannidis, and Arnaud Doucet. Relaxing bijectivity constraintswith continuously indexed normalising flows. In International Conference on Machine Learning, 2020. Jesse C Cresswell, Brendan Leigh Ross, Gabriel Loaiza-Ganem, Humberto Reyes-Gonzalez, Marco Letizia,and Anthony L Caterini. CaloMan: Fast generation of calorimeter showers with density estimation onlearned manifolds. In NeurIPS Workshop on Machine Learning and the Physical Sciences, 2022.",
  "Partha Ghosh, Mehdi SM Sajjadi, Antonio Vergari, Michael Black, and Bernhard Schlkopf. From variationalto deterministic autoencoders. In International Conference on Learning Representations, 2020": "Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, AaronCourville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Information ProcessingSystems, 2014. Will Grathwohl, Ricky TQ Chen, Jesse Bettencourt, Ilya Sutskever, and David Duvenaud.FFJORD:Free-form continuous dynamics for scalable reversible generative models.In International Conferenceon Learning Representations, 2019. Will Grathwohl, Kuan-Chieh Wang, Joern-Henrik Jacobsen, David Duvenaud, Mohammad Norouzi, andKevin Swersky. Your classifier is secretly an energy based model and you should treat it like one. InInternational Conference on Learning Representations, 2020.",
  "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2016": "Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. GANstrained by a two time-scale update rule converge to a local Nash equilibrium. In Advances in NeuralInformation Processing Systems, 2017. Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Shakir Mo-hamed, and Alexander Lerchner. beta-VAE: Learning basic visual concepts with a constrained variationalframework. In International Conference on Learning Representations, 2017. Jonathan Ho, Xi Chen, Aravind Srinivas, Yan Duan, and Pieter Abbeel. Flow++: Improving flow-basedgenerative models with variational dequantization and architecture design. In International Conferenceon Machine Learning, 2019.",
  "Chin-Wei Huang, Laurent Dinh, and Aaron Courville.Augmented normalizing flows: Bridging the gapbetween generative flows and latent variable models. arXiv:2002.07101, 2020": "Jian Huang, Yuling Jiao, Zhen Li, Shiao Liu, Yang Wang, and Yunfei Yang. An error analysis of generativeadversarial networks for learning distributions. Journal of Machine Learning Research, 23(1):50475089,2022. Ahmed Imtiaz Humayun, Ibtihel Amara, Candice Schumann, Golnoosh Farnadi, Negar Rostamzadeh, andMohammad Havaei. On the local geometry of deep generative manifolds. In ICML Workshop on Geometry-Grounded Representation Learning and Generative Modeling, 2024.",
  "Varuna Jayasiri and Nipun Wijerathne. Annotated paper implementations, 2020. URL": "Zhuxi Jiang, Yin Zheng, Huachun Tan, Bangsheng Tang, and Hanning Zhou. Variational deep embedding:An unsupervised and generative approach to clustering. In Proceedings of the Twenty-Sixth InternationalJoint Conference on Artificial Intelligence (IJCAI), 2017. Kerstin Johnsson, Charlotte Soneson, and Magnus Fontes. Low bias local intrinsic dimension estimationfrom expected simplex skewness. IEEE Transactions on Pattern Analysis and Machine Intelligence, 37(1):196202, 2014.",
  "Dimitris Kalatzis, Johan Ziruo Ye, Alison Pouplin, Jesper Wohlert, and Sren Hauberg. Density estimationon smooth manifolds with normalizing flows. arXiv:2106.03500, 2021": "Hamidreza Kamkari, Brendan Leigh Ross, Jesse C Cresswell, Anthony L Caterini, Rahul G Krishnan, andGabriel Loaiza-Ganem. A geometric explanation of the likelihood OOD detection paradox. In InternationalConference on Machine Learning, 2024a. Hamidreza Kamkari, Brendan Leigh Ross, Rasa Hosseinzadeh, Jesse C Cresswell, and Gabriel Loaiza-Ganem.A geometric view of data complexity: Efficient local intrinsic dimension estimation with diffusion models.In ICML Workshop on Structured Probabilistic Inference and Generative Modeling, 2024b. Gurtej Kanwar, Michael S Albergo, Denis Boyda, Kyle Cranmer, Daniel C Hackett, Sbastien Racaniere,Danilo Jimenez Rezende, and Phiala E Shanahan.Equivariant flow-based sampling for lattice gaugetheory. Physical Review Letters, 125(12):121601, 2020. Kacper Kapusniak, Peter Potaptchik, Teodora Reu, Leo Zhang, Alexander Tong, Michael Bronstein,Avishek Joey Bose, and Francesco Di Giovanni. Metric flow matching for smooth interpolations on thedata manifold. arXiv:2405.14780, 2024.",
  "Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of GANs for improvedquality, stability, and variation. In International Conference on Learning Representations, 2018": "Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarialnetworks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,2019. Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing andimproving the image quality of StyleGAN. In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition, 2020.",
  "Mahyar Khayatkhoei, Maneesh K Singh, and Ahmed Elgammal. Disconnected manifold learning for gener-ative adversarial networks. In Advances in Neural Information Processing Systems, 2018": "Dongjun Kim, Seungjae Shin, Kyungwoo Song, Wanmo Kang, and Il-Chul Moon. Soft truncation: A univer-sal training technique of score-based diffusion model for high precision score estimation. In InternationalConference on Machine Learning, 2022. Hyeongju Kim, Hyeonseung Lee, Woo Hyun Kang, Joun Yeop Lee, and Nam Soo Kim. Softflow: Probabilisticframework for normalizing flow on manifolds. In Advances in Neural Information Processing Systems, 2020.",
  "Diederik P Kingma and Max Welling. Auto-encoding variational Bayes. In International Conference onLearning Representations, 2014": "Diederik P Kingma, Tim Salimans, Rafal Jozefowicz, Xi Chen, Ilya Sutskever, and Max Welling. Improvedvariational inference with inverse autoregressive flow.In Advances in Neural Information ProcessingSystems, 2016. Ivan Kobyzev, Simon JD Prince, and Marcus A Brubaker. Normalizing flows: An introduction and reviewof current methods. IEEE Transactions on Pattern Analysis and Machine Intelligence, 43(11):39643979,2020.",
  "Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transferdata with rectified flow. In International Conference on Learning Representations, 2023": "Gabriel Loaiza-Ganem, Brendan Leigh Ross, Jesse C Cresswell, and Anthony L Caterini. Diagnosing andfixing manifold overfitting in deep generative models. Transactions on Machine Learning Research, 2022a. Gabriel Loaiza-Ganem, Brendan Leigh Ross, Luhuan Wu, John Patrick Cunningham, Jesse C Cresswell, andAnthony L Caterini. Denoising deep generative models. In Proceedings on \"I Cant Believe Its Not Better!- Understanding Deep Learning Through Empirical Falsification\" at NeurIPS 2022 Workshops, 2022b.",
  "Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, and DustinTran. Image transformer. In International Conference on Machine Learning, 2018": "Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen,Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, ZacharyDeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, andSoumith Chintala. PyTorch: An imperative style, high-performance deep learning library. In Advances inNeural Information Processing Systems, 2019. Giorgio Patrini, Rianne van den Berg, Patrick Forre, Marcello Carioni, Samarth Bhargav, Max Welling, TimGenewein, and Frank Nielsen. Sinkhorn autoencoders. In Uncertainty in Artificial Intelligence, 2020.",
  "Janis Postels, Martin Danelljan, Luc Van Gool, and Federico Tombari. Maniflow: Implicitly representingmanifolds with normalizing flows. In International Conference on 3D Vision, 2022": "Michael Puthawala, Matti Lassas, Ivan Dokmanic, and Maarten De Hoop. Universal joint approximation ofmanifolds and densities by simple injective flows. In International Conference on Machine Learning, 2022. Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep convolu-tional generative adversarial networks. In International Conference on Learning Representations, 2015. Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and IlyaSutskever. Zero-shot text-to-image generation. In International Conference on Machine Learning, 2021.",
  "Brendan Leigh Ross and Jesse C Cresswell. Tractable density estimation on learned manifolds with conformalembedding flows. In Advances in Neural Information Processing Systems, 2021": "Brendan Leigh Ross, Gabriel Loaiza-Ganem, Anthony L Caterini, and Jesse C Cresswell. Neural implicitmanifold learning for topology-aware generative modelling. Transactions on Machine Learning Research,2023. Brendan Leigh Ross, Hamidreza Kamkari, Zhaoyan Liu, Tongzi Wu, George Stein, Gabriel Loaiza-Ganem,and Jesse C Cresswell. A geometric framework for understanding memorization in generative models. InICML Workshop on Geometry-Grounded Representation Learning and Generative Modeling, 2024.",
  "Oleh Rybkin, Kostas Daniilidis, and Sergey Levine.Simple and effective VAE training with calibrateddecoders. In International Conference on Machine Learning, 2021": "Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, KamyarGhasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, Jonathan Ho, David J Fleet,and Mohammad Norouzi. Photorealistic text-to-image diffusion models with deep language understanding.In Advances in Neural Information Processing Systems, 2022. Tim Salimans, Andrej Karpathy, Xi Chen, and Diederik P Kingma. PixelCNN++: Improving the Pixel-CNN with discretized logistic mixture likelihood and other modifications. In International Conference onLearning Representations, 2017.",
  "Nicolas Schreuder, Victor-Emmanuel Brunel, and Arnak Dalalyan.Statistical guarantees for generativemodels without domination. In Algorithmic Learning Theory, 2021": "Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade W Gordon, Ross Wightman, Mehdi Cherti,Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa RKundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. LAION-5b: Anopen large-scale dataset for training next generation image-text models. In Advances in Neural InformationProcessing Systems, 2022.",
  "Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-SNE. Journal of Machine LearningResearch, 9(86):25792605, 2008": "Elen Vardanyan, Sona Hunanyan, Tigran Galstyan, Arshak Minasyan, and Arnak S Dalalyan. Statisticallyoptimal generative modeling with maximum deviation from the empirical distribution. In InternationalConference on Machine Learning, 2024. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ukasz Kaiser,and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems,2017.",
  "Sangwoong Yoon, Yung-Kyun Noh, and Frank Park. Autoencoding under normalization constraints. InInternational Conference on Machine Learning, 2021": "Sangwoong Yoon, Young-Uk Jin, Yung-Kyun Noh, and Frank Park.Energy-based models for anomalydetection: A manifold diffusion recovery approach. In Advances in Neural Information Processing Systems,2023. Hengrui Zhang, Jiani Zhang, Balasubramaniam Srinivasan, Zhengyuan Shen, Xiao Qin, Christos Faloutsos,Huzefa Rangwala, and George Karypis. Mixed-type tabular data synthesis with score-based diffusion inlatent space. In International Conference on Learning Representations, 2024.",
  "AWeak Convergence Primer": "We now provide a brief summary of weak convergence of probability measures. We do not use a grey boxaround this section due to its length, despite the content being fairly technical. As in the main text, allthe measures we consider here will be defined on X along with its Borel -algebra. Given a sequence ofprobability measures (PXt)t=1, we would like to define what it means for the sequence to converge to someprobability measure PX . As we will see, there are several ways to define convergence of probability measures;we would like to use one which captures the intuition that PXt learns PX in the sense that PXt convergesto PX as t if and only if samples from PXt become progressively harder to distinguish from those of PXas t becomes larger, becoming indistinguishable in the limit. When PXt represents a DGM, this is preciselythe form of convergence we would hope to observe as we optimize its parameters t. Strong convergenceThe seemingly most natural way to define convergence is to say that PXt convergesto PX as t if PXt(B) PX (B) as t for every Borel set B. This type of convergence is calledstrong convergence. As the name suggests, this type of convergence is too strong, to the point where it doesnot properly capture the intended intuition of PXt converges to PX if and only if PXt learns PX . Let us illustrate why this is the case with two examples. First, let PXt be Gaussian with mean 0 and covariancematrix 1/t ID. Intuitively, this sequence learns a point mass at 0, 0, yet it does not strongly converge toit: {0} is a Borel set, and PXt({0}) = 0 0 as t , yet 0({0}) = 1 = 0. As a second example, considerPXt = xt, where (xt)t=1 is a fixed sequence converging to 0 with {xt}t=1 {0} = . Intuitively this sequencealso learns 0, but similarly to the previous example, PXt({0}) = 0 for every t and thus the sequence doesnot strongly converge to 0 either. We highlight that these are not overly-contrived examples in the DGM setting. The first example illustrates acommon scenario where a sequence of full-dimensional models PXt D learn a distribution PX supportedon a low-dimensional embedded submanifold M of X, without strongly converging to PX . In this case, Mis also a Borel set, and PXt(M) = 0 0 as t even though PX (M) = 1 = 0. The second exampleillustrates how a sequence of models whose supports do not overlap with that of their target distribution canlearn it without strongly converging to it. Indeed, we need a laxer definition of convergence of probabilitymeasures to properly convey the idea that a sequence of models PXt learns PX . Weak convergenceWeak rather than strong convergence provides a more appropriate notion ofconvergence to convey learning. We say that PXt converges weakly to PX if EXPXt[h(X)] EXPX [h(X)]as t for every bounded and continuous function h : X R. As mentioned in .1, we writePXt PX as t to denote weak convergence. Intuitively, PXt converges weakly to PX if, as t ,it becomes arbitrarily difficult to distinguish between samples from PXt and samples from PXby using abounded and continuous function; weak convergence matches the intuition of PXt learning PX much betterthan strong convergence. There are many equivalent definitions of weak convergence, with the standard one being the one presentedabove. The result establishing the equivalence of these definitions is called the Portmanteau Lemma. Wepresent a reduced version of this lemma below which we will use to prove the Likelihood Instability Theoremin Appendix B.1 where only one of these equivalences is stated. Before stating the lemma, we define thecontinuity sets of a probability measure.",
  "We start by restating the Likelihood Instability Theorem for convenience before discussing it": "Theorem 1 (Likelihood Instability of Deep Generative Models). Let M X be a Borel set such thatD(clX (M)) = 0, and let PX be a probability measure on X such that PX (M) = 1 and supp(PX ) = clX (M).Let (PXt)t=1 be a sequence of probability measures on X such that PXt PX as t and PXt D, withcorresponding densities pXt. Then:",
  "B(x) := {x X | x x2 < }": "As mentioned in .1.1 we begin with an example satisfying the assumptions of the Likelihood In-stability Theorem for which it does not hold that pXt(x) for x clX (M), thus highlighting that thetheorem cannot be trivially strengthened. Consider X = R2, M = {(x1, 0) R2 | 0 < x1 < 1}, let PX beuniform on M, and PXt be uniform on Mt, where Mt = {(x1, x2) R2 | 0 < x1 < 1 and 1/(t+1) < x2 < 1/t}.Finally, take the corresponding densities as",
  "(Mt)= t(t + 1)1(x Mt),(99)": "where 1() denotes an indicator function. illustrates this example. Here, it holds that PXt PX so that the assumptions of the Likelihood Instability Theorem are satisfied yet pXt(x) 0 as t for every x X, and thus in particular for every x clX (M) as well. Nonetheless, when x clX (M),supxB(x) pXt(x) as t does hold for every > 0, as concluded by the theorem. Before proving the Likelihood Instability Theorem, we state and prove three lemmas, all of which we willrely on. We will heavily use Continuity Sets and will leverage the Portmanteau Lemma; see Appendix A fora reminder on these topics.",
  "= PX () = 0,(103)": "where intX (M) denotes the topological interior of M in X, and the first equality follows from PX (M) = 1,the second one from the definition of boundary, the third one from M being open, and the fourth one fromM M. Thus M is indeed a continuity set of PX .",
  "Lemma 4. Let M X, and let PX be a probability measure on X such that supp(PX ) = clX (M). ThenPX (B(x)) > 0 for every x clX (M) and every > 0, where B(x) := {x X | x x2 < }": "Proof. Since PXis a Borel measure and X is separable, PX (supp(PX )) = 1 (Bogachev, 2007, Proposi-tion 7.2.9).30It follows that clX (M) = supp(PX ) is nonempty.Let x clX (M) and > 0.Weproceed by contradiction, and assume that PX (B(x)) = 0.Since PX (clX (M)) = 1, we have that",
  "and thus lim inft pXt(x) = 0, D-almost-everywhere on Ux": "We still need to extend the result from D-almost-everywhere on Ux to D-almost-everywhere on X \\clX (M).Clearly {Ux}xX\\clX (M) is an open cover of X \\ clX (M). Since X is second countable and every subspaceof a second countable space is second countable, it follows that X \\ clX (M) is second countable. Then, byLindelfs lemma, {Ux}xX\\clX (M) has a countable subcover {Uxi}i=1 of X \\ clX (M). The result then holdsD-almost-everywhere on Uxi for i = 1, 2, . . . , and because any countable union of sets of measure 0 hasmeasure 0, it also holds D-almost-everywhere on",
  "Now, let x clX (M) and > 0, and we will prove that supxB(x) pXt(x) as t": "First, note that supxB(x) pXt(x) is increasing in for every t. If B(x) is not a continuity set of PX , byLemma 2 we could always find (0, ) such that B(x) is a continuity set of PX , and if we managed toprove that supxB(x) pXt(x) as t , the same result would immediately follow for . We can thusassume without loss of generality that is such that B(x) is a continuity set of PX . Now, let M := {x X | infxM x x2 < } and let U,(x) := M B(x). From basic topology wehave that X (M B(x)) X M X B(x). Since M and B(x) are continuity sets of PX by Lemma 3and by assumption, respectively, it follows that U,(x) is a continuity set of PX , since PX (X U,(x)) PX (X M) + PX (X B(x)) = 0. Similarly, B(x) \\ M is a continuity set of PX because X (B(x) \\ M) X B(x) X M. We then write:",
  "= D (clX (M) B(x)) = 0,(116)": "where we used that (i) >0M = clX (M), which holds because clX (M) is the set of points which arearbitrarily close to M, and that (ii) D(clX (M)) = 0 by assumption. Finally, by Lemma 4, PX (B(x)) > 0, sothat taking the limit as 0+ on both sides of Equation 114 yields that lim inft supxB(x) pXt(x) = ,which in turn implies that supxB(x) pXt(x) as t , finishing the proof.",
  ",(120)": "as this would imply Equation 118 holds. Since PX is a Borel measure and X is Polish, PX is a Radonmeasure. Additionally, PX (X) < , X is locally compact, Z is second countable, and f is measurable; soit follows by Lusins theorem that there exists a Borel set E X and a continuous function f : X Zsuch that f(x) = f(x) for every x E, and PX (X \\ E) < /(4C). Then, we have:",
  "where 1 > 0 and 2 > 0 are hyperparameters; pZ is a standard Gaussian; qZ|X(|x) = N( ; f(x), Z|X(x))": "with Z|X(x) being diagonal for every x X; and h : X (0, 1) is a binary classifier inspired by generativeadversarial networks (.2), whose objective is to distinguish between real samples X pX and theirstochastic reconstructions g1(Z), where Z qZ|X(|X). Rather than fixing 2, we dynamically updateit throughout training to ensure that the first and third terms in Equation 124 have roughly the samemagnitude; when taking a gradient step, this is achieved by computing the ratio of the values of the first tothird term in the previous gradient step, and setting 2 to the absolute value of this ratio. Training objective for diffusion modelsWe train all diffusion models, latent or not, exactly as de-scribed in .1.2, i.e. through Equation 55. The integral with respect to t is approximated by samplingt uniformly at random in [0, T] during training (one such t is sampled for every element in the batch). HyperparametersWe use the Adam optimizer (Kingma & Ba, 2015) with a batch size of 128 throughout,and train all models until there is no improvement on the validation metric for 50 epochs; we keep themodels with the best validation performance. For the VAE of latent diffusion models we use the squaredreconstruction error EXpX [Xg1(f(X))22] rather than Equation 124 as the validation metric, 1 = 106,a learning rate of 104 with cosine annealing, and take two gradient steps on for every gradient step on(1, ). For the diffusion models, both on ambient and latent space, we use Equation 55 as the validationmetric, a learning rate of 5 105 without cosine annealing, T = 1, min = 0.1, max = 20, w(t) = 2t , andan Euler-Maruyama discretization scheme with 1000 steps to generate the paths in .",
  "n_channels6464256ch_mults(1, 2, 2, 4)(1, 2, 2)(1, 2)is_attn(False, False, True, True)-(True, True)n_blocks222": "ArchitecturesFor a fair comparison between diffusion models on ambient and latent space, we attempt toinstantiate them in such a way that their overall architectures are as similar as possible. The configurationsof the architectures we used are given in , which we now describe. We parameterize both the ambientand latent score networks as the output of a neural network divided by t, as mentioned in .1.2(this is equivalent to the so-called parameterization of Ho et al. (2020) with being parameterizedinstead of ). For the diffusion model on ambient space, we use a U-Net architecture (Ronneberger et al.,2015) with residual connections (He et al., 2016) and an additional attention mechanism (Vaswani et al.,2017), as implemented in the labml package (Jayasiri & Wijerathne, 2020), which uses a sinusoidal positionalembedding for the scalar input t. The ambient space U-Net takes 3 32 32 images, and progressivelydownsamples them to a shape of 102444 before upscaling them back to their original size. For the latentdiffusion model, we attempt to copy the aforementioned U-Net as much as possible; the VAE mimics theU-Net up until the 8 8 resolution, and adds a convolutional layer to produce outputs of shape 4 8 8,so that d = 256. To ensure the VAE obtains low-dimensional representations through a proper bottleneck,we remove the U-Net skip connections between the encoder and decoder, resulting in a purely residualarchitecture. The stochastic encoder qZ|Xof the VAE consists of a single neural network which takes x Xand produces a 2d-dimensional output the first d dimensions correspond to f(x), and the remaining onesto the diagonal of Z|X(x). The auxiliary network h has the same architecture as the encoder f, excepta final linear layer is added to ensure the output is a scalar. The score network of the diffusion on latentspace is given another U-Net which further downsamples to a 4 4 resolution before upsampling. Data preprocessingRecall that raw image data is integer-valued, with possible values ranging from 0to 255. We dequantize the data before training the diffusion model on ambient space and the VAE for thelatent diffusion model, i.e. we add independent uniform noise to every pixel (Theis et al., 2016), so thatthe resulting data now has entries in . We then linearly scale the data so that every coordinate liesin . For latent diffusion models, once the VAE is trained, we also scale its encodings to lie in d"
}