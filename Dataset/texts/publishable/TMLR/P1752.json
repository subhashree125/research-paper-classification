{
  "Abstract": "The connection between brain activity and visual stimuli is crucial to understanding thehuman brain. Although deep generative models have shown advances in recovering brainrecordings by generating images conditioned on fMRI signals, it is still challenging to gen-erate consistent semantics. Moreover, predicting fMRI signals from visual stimuli remainsa hard problem. In this paper, we introduce a unified framework that addresses both fMRIdecoding and encoding.We train two latent spaces to represent and reconstruct fMRIsignals and visual images, respectively. By aligning these two latent spaces, we seamlesslytransform between the fMRI signal and visual stimuli. Our model, called Latent EmbeddingAlignment (LEA), can recover visual stimuli from fMRI signals and predict brain activityfrom images. LEA outperforms existing methods on multiple fMRI decoding and encodingbenchmarks. It offers a comprehensive solution for modeling the relationship between fMRIsignals and visual stimuli. The codes are available at",
  "Published in Transactions on Machine Learning Research (12/2024)": "et al. (2019) follow the BOLD-processing workflow of FMRIPREP3 and extract data features via the generallinear model (GLM). Horikawa & Kamitani (2017) preprocess data including 3D motion correction by SPM54,co-registeration to anatomical images and interpolation by 3 3 3 mm voxels. Please refer to the fMRIData Analysis section in (Chang et al., 2019) and the MRI data preprocessing section in (Horikawa &Kamitani, 2017) for more details. Augmentation. We utilize data augmentation to enlarge training data for both fMRI and image recon-struction and discard them all during the inference or the fitting of linear models. For fMRI signals, we onlyuse one augmentation method by randomly adding Gaussian noise with the probability of 0.25. For imagedata, we apply random cropping, random horizontal flipping and finally resize images to 256 256.",
  "EncodeEncode": ":Our proposed LEA efficiently enables (a) fMRI encoding to estimate the brain activity fromvisual stimuli; (b) fMRI decoding to recover the visual stimuli from recorded fMRI signals, and (c) image-fMRI-image reconstruction to ensure the reliability of fMRI decoding and encoding. subsequently decode it to generate images. Conversely, when predicting brain activity, we encode the images,transform them into latent fMRI embeddings, and then decode them to produce fMRI signals. An importantfeature of LEA is its support for image-fMRI-image reconstruction, which serves as an indicator for thereliability of both fMRI encoding and decoding models. Through extensive experimentation on BOLD5000and GOD benchmarks, we demonstrate that LEA exhibits both efficiency and effectiveness in fMRI decodingand encoding.",
  "Related Work": "fMRI Encoding and Decoding. fMRI encoding involves learning the mapping from visual stimuli tofMRI signals. Early studies (Yamins et al., 2013; 2014; Gl & van Gerven, 2015; Yamins & DiCarlo,2016) primarily employed convolutional neural networks to extract semantic information from visual stimuli.However, with the resurgence of neural language models, recent research has successfully utilized modelssuch as BERT (Devlin et al., 2018), Transformer (Vaswani et al., 2017), and GPT-2 (Radford et al., 2019) topredict fMRI responses from stimuli, including images and even words/sentences. Conversely, fMRI decodingfocuses on reverse mapping, from brain activity to stimuli. Previous studies (Mozafari et al., 2020; Ozceliket al., 2022; Chen et al., 2015; 2016; Zhang et al., 2016) developed regression models specifically designedto extract valuable information from fMRI signals. Ozcelik et al. (2022) employed a pre-trained Instance-Conditional GAN model (Casanova et al., 2021) to reconstruct images by decoding latent variables fromfMRI data. Ferrante et al. (2022) utilized pre-trained latent diffusion models (Ho et al., 2020) to generatestimulus images by mapping fMRI signals to visual features. Beliy et al. (2019) adopted self-supervisedlearning on testing data to enable adaptation to testing statistics. Chen et al. (2022) introduced a self-supervised sparse masked modeling strategy to encode fMRI data into latent embeddings and fine-tunedlatent diffusion models with double conditioning. Huo et al. (2024) further ensures that the decoded imagesretain both the semantics and stucture. In this paper, we present a unified framework that efficiently realizesboth fMRI encoding and decoding.",
  "fMRI Decoding": ": The overview of our proposed LEA framework. We learn a latent representation space via theencoder-decoder architectures designed specifically for fMRI signals and images. Then a lightweight align-ment module is proposed to link the two latent spaces, enabling both fMRI encoding and decoding. Multi-Modality Alignment. It aims to establish connections across different modalities, such as image-text alignment (Radford et al., 2021; Jia et al., 2021; Mahajan et al., 2018) and image-audio align-ment (Arandjelovic & Zisserman, 2017; Morgado et al., 2021; Owens & Efros, 2018; Patrick et al., 2020).Girdhar et al. (2023) endeavors to align multiple modalities within a unified latent representation space.The majority of efforts in this direction are centered around the CLIP model (Radford et al., 2021), whichis trained on extensive image-text pairs to learn a shared latent space minimizing the distances betweencorrelated image-text pairs. For instance, Liu et al. (2023) bridged the modality gap by leveraging CLIPscross-modal generalization ability to address the limitation of limited fMRI-image paired data. Similarly,Du et al. (2023) employed multi-modal deep generative models to capture the relationships between brain,visual, and linguistic features. Ozcelik & VanRullen (2023) predicted multi-modal features from fMRI signalsand generated reconstructed images using a latent diffusion model (Rombach et al., 2022).",
  "Problem Setup": "Given a fMRI signal f RL recorded from brain activity and the corresponding visual stimuli I RHW 3,the purpose of our work is to learn a unified framework that can perform both fMRI decoding task torecover the observed image from fMRI signal and fMRI encoding task to predict the brain activity fromthe image. For the decoding task, the ideal scenario involves the reconstructed images I matching the real ones precisely.However, the unique nature of biological mechanisms suggests that individual memory or attention caninfluence an individuals brain activity response to the same image, and thus variations may occur. In lightof this, we adhere to the approach introduced in (Mozafari et al., 2020; Ozcelik et al., 2022), requiringthe recovered images I to exhibit semantic consistency with I. Consequently, our primary focus is on thesemantic fMRI decoding task.",
  "Latent Space Construction for fMRI": "The fMRI signals indirectly record neural activity in the visual cortex of the brain by measuring the blood-oxygen-level-dependent (BOLD) signals, which are usually represented as data of 3D voxels, covering visualregions, like early visual cortex (EVC), lateral occipital complex (LOC), occipital place area (OPA), parahip-pocampal place area (PPA), and retrosplenial complex (RSC) (Horikawa & Kamitani, 2015). Biologicalprinciples suggest that adjacent voxels in the brains visual cortex often exhibit similar intensities in theirstimulus-response (Ugurbil et al., 2013), resulting in redundancy within fMRI signals. To investigate potential response patterns to visual stimuli and establish a reverse mapping for semanticencoding, it is essential to focus on the relationships among different regions while simultaneously capturingactivity changes across long-range voxels. Inspired by the advantages of long-range receptive fields found intransformer layers, we follow Chen et al. (2022) by adopting masked autoencoder (He et al., 2022) as theencoder-decoder model for fMRI signals. ROI embedding layer. Unlike Chen et al. (2022) which equally divides the fMRI signals into patchesof equal size, we maintain the voxel structure with ROI regions. Concretely, we partition the fMRI signalvector into distinct ROI regions, each potentially having different dimensions. To address the dimensioninconsistency, we propose the Region of Interest (ROI) Embedding layer. Given a specific ROI region i,denote the corresponding fMRI signal is of size Ni 1, indicating Ni voxels and 1 dimension of BOLDresponse, this fMRI signal region is first passed through a convolution layer with 32 kernels to extract multi-head features, resulting in dimensions of Ni 32. Subsequently, we employ a fully-connected layer to projectthe fMRI signal, which may have varying lengths, into a unified dimension of 1024. We then concatenate theembeddings from all ROI regions, yielding the final input of size 32M1024, where M indicates the numberof ROIs. Finally, we append a learnable [CLS] token to represent the class of the input. Global feature extraction.In our encoder-decoder architecture training, we employ a masked auto-encoder (MAE) (He et al., 2022). The original MAE employs patch-based reconstruction, where a portionof the patches is masked, and the task is to predict these masked patches. While this approach is effectivefor learning local representations, it presents challenges when we aim to acquire a global latent code thatrepresents the entire fMRI signal. To address this, we only preserve the global [CLS] token of the encodedfMRI signals, mask all the patch-based latent code, and let the decoder learn to reconstruct fMRI signalsfrom the learned [CLS] token. This strategy proves advantageous for learning a dense fMRI representationwithin the [CLS] token, and the trained decoder can subsequently be used to generate fMRI signals basedon the estimated latent fMRI code. Finally, the decoders output undergoes post-processing through the ROI Project layer. This layer mirrorsthe structure of the ROI Embed layer, effectively reversing the pre-processing applied to the fMRI signals,ensuring that the length matches that of the original fMRI signals. The overall model is trained by minimizingthe L2 reconstruction loss of the fMRI signals.",
  "Latent Space Construction for Image": "The image-based latent space construction is a well-established domain of research. To recover semanticcontent from fMRI signals, we leverage the pre-trained CLIP visual encoder as our foundation for imagelatent space. CLIP benefits from extensive training on large-scale image-text paired data, making its latentspace rich in semantics and benefits the semantic consistency in our task. Note that CLIP operates as an encoder-only model, and we cannot directly generate images from the imagelatent space. To address this issue, we adopt MaskGIT (Chang et al., 2022) as our decoder to reconstructimages from the image latent space. In this approach, we fix the CLIP visual encoder and fine-tune theclass-conditional MaskGIT as a latent-conditional generative decoder. Further details regarding the objectivefunctions can be found in (Chang et al., 2022).",
  "Latent Embedding Alignment": "The alignment of fMRI and image latent spaces represents a critical step in enabling the simultaneousexecution of fMRI decoding and encoding tasks. Unlike conventional methods (Du et al., 2023; Liu et al.,2023) that rely on intermediary modalities to bridge the gap between fMRI and image data, our approachdirectly connects the latent feature representations of both domains through a linear model. This strategyleverages the expressive power of these well-defined latent spaces, eliminating the need for additional datamodalities.",
  ":LEA aligns the latent space offMRI signals and image features, enablingthe bi-directional transformation": "To address the inherent variability in fMRI signals and the potentialfor individual-specific patterns in latent fMRI embeddings, we em-ploy a personalized modeling approach. Individual-specific linearmodels are trained to map fMRI embeddings to image embeddingsand vice versa. L2 regularization, implemented via ridge regression,is used to stabilize the learning process and constrain the solutionspace. With the aligned latent spaces and the embedding align-ment module in place, we integrate fMRI decoding and encodingtasks into a unified framework. Specifically, the latent embedding of the fMRI is the trained [CLS]token of our fMRI encoder output, denoted as efMRI. The latentembedding of the image is the global visual CLIP feature, denotedas eimage. For a specific individual, we gather all training data toform two embedding matrices, EfMRI and Eimage. Then we learntwo linear mappings via the ridge regression such that:",
  "The solutions are fMRIimage = (EfMRIEfMRI + I)1EfMRIEimage and imagefMRI = (EimageEimage +I)1EimageEfMRI, respectively. We use the same regularization parameter for both regression models": "In the decoding task, the fMRI signal is first encoded into the latent fMRI space. This latent representa-tion is then projected into the latent visual feature space using the individual-specific linear model. Theprojected visual feature is decoded by a fine-tuned MaskGIT model to reconstruct the original visual stim-uli. Conversely, in the encoding task, CLIP visual features are encoded and projected into the latent fMRIspace through another linear model. The resulting latent fMRI representation is subsequently decoded toapproximate the original fMRI signals.",
  "Quantitative Results": "Analysis of zero-shot classification. As we inherit the latent space from the original CLIP model, LEAenables open-vocabulary recognition of fMRI signals.To assess this capability, we perform experimentson zero-shot fMRI signal classification tasks to identify the most similar category for each fMRI signal.Specifically, we create a set of candidate class names using various text templates. These class names arethen encoded using the CLIP text encoder. Subsequently, we calculate the similarity between the classembeddings and the fMRI latent embeddings learned by LEA. We conduct experiments on GOD dataset, which contains 50 visual stimuli from 50 categories that areunseen during training. We report the top-1 classification accuracy (with the chance levels of 2%) of severalcompetitors in . Results of competitors are from Liu et al. (2023). We divide the competitors intotwo groups based on their supervision modality, where V indicates visual supervision only and V&Tindicates both visual and textual supervision. Our LEA trained with visual supervision beats all competitors,indicating the superiority of our bi-directional transformation.",
  "LEA (relax)8.43 15.51 21.20 19.08 15.07 15.86 34.32 26.74 27.74 24.44 28.19LEA8.49 17.01 20.58 17.82 16.86 16.15 34.78 25.91 27.86 24.37 28.80": "Analysis of fMRI encoding. With two latent spaces for fMRI signals and images, LEA can not onlyrecover the visual stimuli but also predict the fMRI signal by the visual image. As in , we conductexperiments to validate the estimation capability of our LEA. We construct two competitors to show theeffectiveness of LEA. For the Regressor, we apply liner regression to directly generate fMRI signals basedon the image feature from the CLIP model. With the absence of learned fMRI embedding in LEA, theRegressor adopts the pipeline of image features fMRI signals instead of image features fMRIfeatures fMRI signals. However, such a method causes misalignment of features during reconstructionwithout the fMRI latent space, as validated in the empirical results. The MinD-Vis utilizes the transformerdecoder to predict fMRI signals from CLIP image features. Such a solution is too rough and not conducive tofine-grained feature alignment and generation. Our LEA achieves the best Pearsons score, which indicatesthat the alignment between latent embedding spaces is more effective for fMRI prediction. Analysis of fMRI decoding. We present a comprehensive comparison of fMRI decoding performance onthe GOD and BOLD5000 datasets in and , respectively. To ensure a fair assessment, wereproduce the results of competitors using their officially released models. It is worth noting that due to theavailability of the official model of CIS-1,4 on the BOLD5000 dataset in MinD-Vis, we exclusively reportthe reproduced CLIP scores for these two individuals. Our novel LEA consistently demonstrates superiorperformance across various perspectives, including image quality, semantic correctness, and correlation, onboth the GOD and BOLD5000 datasets. Notably, LEA excels in generating images of higher quality than itscompetitors on the GOD dataset. While MinD-Vis may outperform LEA on specific subjects, it significantlylags behind LEA on others, such as SUB-1,2,4 in the GOD and CSI-2,4 in BOLD5000, resulting in an overallinferior average performance. In summary, LEA not only outperforms current competitors in fMRI decodingbut also possesses the capability for fMRI encoding, making it a comprehensive solution in the field.",
  "Qualitative Results": "To intuitively illustrate the efficacy of our method, we show some generated images in and for the GOD and BOLD5000 datasets, respectively. We compare with MinD-Vis (Chen et al., 2022) and Gazivet al. (2022) on the GOD and MinD-Vis on BOLD5000. More concretely, our generated images consistently maintain the same semantic information as the groundtruth, encompassing humans, objects, animals, architecture, and landscapes. In contrast, competitors mayfalter in some instances, failing to preserve semantic consistency. For instance, in , LEA excels inproducing semantic-consistent and high-fidelity images, whereas competitors struggle to preserve semantics.In the last row on the left side of , MinD-Vis can only generate a person while neglecting the water,",
  "Ablation Studies": "Analysis of ROI Embedding Layer. Human brains are known to be spatially segregated, and differentROI regions react to different visual stimuli. Preserving the spatial structure is beneficial to help understandthe fMRI signals. Therefore, we design a ROI embedding layer (ROIEmbed for short) to separately encodesignals from different ROI regions. Differently, previous methods (Chen et al., 2022) simply divide fMRIsignals into n = 16 equal parts to obtain sequence features of appropriate length. Zero padding is addedif the original length of fMRI signals is not a multiple of 16. We denote such a method as PatchEmbed.To evaluate the effectiveness of our design, we conduct experiments by predicting fMRI signals from visualimages and generating stimuli images from fMRI signals. In and , our proposed ROIEmbedachieves better performance than PatchEmbed, which demonstrates its efficacy. For the ROIEmbed layer, we extract multi-head features for each ROI region. Here, the number of multi-head is a hyper-parameter. Intuitively, if the number of heads is too small (i.e., 1), it may lead to poorrepresentation capacity; By contrast, a larger number of heads may result in information redundancy. To",
  "PatchEmbed (Chen et al., 2022)-32.88----ROIEmbed (Ours)30.7230.4431.8234.3229.6731.04": "further investigate the effect of this parameter, we report results with different values of heads in the secondrow of and . As expected, it shows a trend of first rising and then falling. The best result isachieved when the number of heads is 32. Note that we do not tune it for each individual but set it to 32consistently to show the efficacy and generalizability of our proposed layer. Analysis of Reliable fMRI decoding. Ensuring the accuracy of fMRI decoding models is paramount, asthey should decode brain activity rather than merely generate images without meaningful context. However,current approaches often rely on large pre-trained generative models capable of high-fidelity image generation,even when conditioned on random noise. This raises concerns about whether such high-quality images trulyreflect brain activity, especially in zero-shot scenarios where the fMRI signals were not part of the trainingdata. To assess the reliability of fMRI decoding models, we conduct two experiments: Zero-shot image-fMRI-image reconstruction: In this task, we use a novel image to estimate the fMRIsignals, and then use the estimated fMRI signals to reconstruct the image. Successful image-fMRI-imagereconstruction implies better semantic consistency for encoding and decoding. fMRI-image with random noise as fake fMRI signals: In this task, we randomly generate fMRI signalsfrom a Gaussian distribution, which significantly deviates from the real fMRI signal distribution. We thendecode these fake fMRI signals to generate images. The reconstructed image should contain non-semanticinformation, indicating non-informative fMRI signals. It is important to note that these tasks are not intended to demonstrate the superiority of a model on newtasks but rather to assess the reliability of methods designed for visual decoding. As shown in ,LEA excels in the image-fMRI-image reconstruction task while generating unrealistic and non-informativeimages when conditioned on fake fMRI signals that deviate from the distribution of real fMRI signals. Incontrast, MinD-Vis still generate high-fidelity and informative images when conditioned on random noise.For in-distribution sampling, both methods perform well because fMRI signals with similar distribution havealready been seen in the training set. Nevertheless, for out-of-distribution sampling, it is unclear whetherthe noise in brain signals is really so-called noise or unknown perceptions. Under this circumstance, methods equipped with powerful image generators still produce high-quality andmeaningful images, which we think is unreliable. One possible reason is that a large-scale pre-trained imagegenerator can handle the situation of out-of-distribution (or zero-shot) inputs. However, they are hard toevaluate because we dont know if the input fMRI signal really exists or what the ground-truth image is.On the other hand, our generated images, though seemingly meaningless, are not complete noise.Themodel hasnt seen these distributions but appears to combine some known semantics, like sky, lawn, and",
  "Broader Impact": "Our proposed LEA model provides a unified framework, to efficiently recover visual stimuli from fMRIsignals and predicts brain activity from images. It could serve as a tool to analyze human perception anduncover underlying cognitive mechanisms. We hope it can inspire more valuable and innovative studies inthe future, and promote the development of neuroscience. However, the potential negative impact lies inthat the usage of this method may cause privacy breaches, since people dont want others to pry into theirthoughts. Besides, this technology shouldnt be used illegally for commercial or cognitive strategies. As aresult, governments and officials may take action to govern the use of relevant datasets and technologies,and researchers should avoid using datasets that may raise ethical concerns.",
  "Conclusions": "In this paper, we address the challenge of jointly performing fMRI decoding and encoding within a single,unified framework. Our proposed Latent Embedding Alignment (LEA) not only constructs latent spacesfor fMRI signals and images but also aligns them, enabling bidirectional transformation. By mitigatingissues related to redundancy, instability, and data scarcity in fMRI datasets, LEA excels at producinghigh-fidelity, semantically consistent fMRI decoding results. Moreover, LEA demonstrates the capability toaccurately estimate human brain activity from visual stimuli through fMRI encoding. Our experimentalresults on two benchmark datasets validate the effectiveness of LEA, highlighting its significant contributionin neuroimaging.",
  "Acknowledgement": "The computations in this research were performed using the CFFF platform of Fudan University. Yanwei Fuis with the School of Data Science, FudanISTBIZJNU Algorithm Centre for Brain-inspired Intelligence,Fudan University, Shanghai Key Lab of Intelligent Information Processing, and Technology Innovation Centerof Calligraphy and Painting Digital Generation, Ministry of Culture and Tourism, China. Yanwei Fu is thecorresponding author.",
  "Nadine Chang, John A Pyles, Austin Marcus, Abhinav Gupta, Michael J Tarr, and Elissa M Aminoff.Bold5000, a public fmri dataset while viewing 5000 visual images. Scientific data, 6(1):49, 2019": "Po-Hsuan Chen, Xia Zhu, Hejia Zhang, Javier S Turek, Janice Chen, Theodore L Willke, Uri Hasson, andPeter J Ramadge. A convolutional autoencoder for multi-subject fmri data aggregation. arXiv preprintarXiv:1608.04846, 2016. Po-Hsuan Cameron Chen, Janice Chen, Yaara Yeshurun, Uri Hasson, James Haxby, and Peter J Ramadge.A reduced-dimension fmri shared response model. Advances in neural information processing systems, 28,2015. Zijiao Chen, Jiaxin Qing, Tiange Xiang, Wan Lin Yue, and Juan Helen Zhou. Seeing beyond the brain: Condi-tional diffusion model with sparse masked modeling for vision decoding. arXiv preprint arXiv:2211.06956,2022. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchicalimage database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248255. Ieee,2009.",
  "Matteo Ferrante, Tommaso Boccato, and Nicola Toschi. Semantic brain decoding: from fmri to conceptuallysimilar image reconstruction of visual stimuli. CoRR, abs/2212.06726, 2022": "Guy Gaziv, Roman Beliy, Niv Granot, Assaf Hoogi, Francesca Strappini, Tal Golan, and Michal Irani.Self-supervised natural image reconstruction and large-scale semantic classification from brain activity.NeuroImage, 254:119121, 2022. Alessandro T Gifford, Benjamin Lahner, Sari Saba-Sadiya, Martina G Vilas, Alex Lascelles, Aude Oliva,Kendrick Kay, Gemma Roig, and Radoslaw M Cichy. The algonauts project 2023 challenge: How thehuman brain makes sense of natural scenes. arXiv preprint arXiv:2301.03198, 2023. Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev Alwala, Armand Joulin,and Ishan Misra. Imagebind: One embedding space to bind them all. arXiv preprint arXiv:2305.05665,2023.",
  "Tomoyasu Horikawa and Yukiyasu Kamitani. Generic decoding of seen and imagined objects using hierar-chical visual features. Nature communications, 8(1):15037, 2017": "Jingyang Huo, Yikai Wang, Yun Wang, Xuelin Qian, Chong Li, Yanwei Fu, and Jianfeng Feng. Neuropictor:Refining fmri-to-image reconstruction via multi-individual pretraining and multi-level modulation.InEuropean Conference on Computer Vision, pp. 5673. Springer, 2024. Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung,Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy textsupervision. In International Conference on Machine Learning, pp. 49044916. PMLR, 2021. Yulong Liu, Yongqiang Ma, Wei Zhou, Guibo Zhu, and Nanning Zheng. Brainclip: Bridging brain and visual-linguistic representation via clip for generic natural visual stimulus decoding from fmri. arXiv preprintarXiv:2302.12971, 2023. Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan Li, AshwinBharambe, and Laurens Van Der Maaten.Exploring the limits of weakly supervised pretraining.InProceedings of the European conference on computer vision (ECCV), pp. 181196, 2018.",
  "Jack Merullo, Louis Castricato, Carsten Eickhoff, and Ellie Pavlick. Linearly mapping from image to textspace.In The Eleventh International Conference on Learning Representations, 2023.URL": "Pedro Morgado, Nuno Vasconcelos, and Ishan Misra. Audio-visual instance discrimination with cross-modalagreement. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,pp. 1247512486, 2021. Milad Mozafari, Leila Reddy, and Rufin VanRullen. Reconstructing natural scenes from fmri patterns usingbigbigan. In 2020 International Joint Conference on Neural Networks, IJCNN 2020, Glasgow, UnitedKingdom, July 19-24, 2020, pp. 18. IEEE, 2020.",
  "Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language modelsare unsupervised multitask learners. OpenAI blog, 1(8):9, 2019": "Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, GirishSastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models fromnatural language supervision. In International conference on machine learning, pp. 87488763. PMLR,2021. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjrn Ommer. High-resolutionimage synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition, pp. 1068410695, 2022. Edgar Schonfeld, Sayna Ebrahimi, Samarth Sinha, Trevor Darrell, and Zeynep Akata. Generalized zero-andfew-shot learning via aligned variational autoencoders. In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition, pp. 82478255, 2019. Guohua Shen, Kshitij Dwivedi, Kei Majima, Tomoyasu Horikawa, and Yukiyasu Kamitani. End-to-end deepimage reconstruction from human brain activity. Frontiers in computational neuroscience, 13:21, 2019.",
  "Chunyue Teng and Dwight J Kravitz. Visual working memory directly alters perception. Nature humanbehaviour, 3(8):827836, 2019": "Kmil Ugurbil, Junqian Xu, Edward J. Auerbach, Steen Moeller, An T. Vu, Julio Martin Duarte-Carvajalino,Christophe Lenglet, Xiaoping Wu, Sebastian Schmitter, Pierre-Franois van de Moortele, John P. Strupp,Guillermo Sapiro, Federico De Martino, Dingxin Wang, Noam Harel, Michael Garwood, Liyong Chen,David A. Feinberg, Stephen M. Smith, Karla L. Miller, Stamatios N. Sotiropoulos, Sad Jbabdi, JesperL. R. Andersson, Timothy Edward John Behrens, Matthew F. Glasser, David C. Van Essen, and Essa Ya-coub. Pushing spatial and temporal resolution for functional and diffusion MRI in the human connectomeproject. NeuroImage, 80:80104, 2013. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser,and Illia Polosukhin. Attention is all you need. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio,Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett (eds.), Advances in NeuralInformation Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017,December 4-9, 2017, Long Beach, CA, USA, pp. 59986008, 2017. Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol. Extracting and compos-ing robust features with denoising autoencoders. In Proceedings of the 25th international conference onMachine learning, pp. 10961103, 2008. Kiran Vodrahalli, Po-Hsuan Chen, Yingyu Liang, Christopher Baldassano, Janice Chen, Esther Yong,Christopher Honey, Uri Hasson, Peter Ramadge, Kenneth A Norman, et al.Mapping between fmriresponses to movies and their natural language annotations. NeuroImage, 180:223231, 2018.",
  "Daniel LK Yamins and James J DiCarlo. Using goal-driven deep learning models to understand sensorycortex. Nature neuroscience, 19(3):356365, 2016": "Daniel LK Yamins, Ha Hong, Charles F Cadieu, Ethan A Solomon, Darren Seibert, and James J DiCarlo.Performance-optimized hierarchical models predict neural responses in higher visual cortex. Proceedingsof the national academy of sciences, 111(23):86198624, 2014. Hejia Zhang, Po-Hsuan Chen, Janice Chen, Xia Zhu, Javier S Turek, Theodore L Willke, Uri Hasson, andPeter J Ramadge. A searchlight factor model approach for locating shared information in multi-subjectfmri analysis. arXiv preprint arXiv:1609.09432, 2016.",
  "A.1. Model Architectures": "Our proposed Latent Embedding Alignment (LEA) framework aims to build the bridge between imagesand fMRI signals. To this end, we incorporate two distinct encoder-decoder architectures to learn latentrepresentations for these two modalities by performing the self-supervised reconstruction task. Subsequently,we introduce a linear model to efficiently facilitate the transformation between the latent representationspaces of images and fMRI signals. fMRI Reconstruction. We adopt the architecture of masked autoencoder (He et al., 2022) as the encoder-decoder model for fMRI signals. More concretely, the fMRI encoder consists of 24 transformer layers, eachof which has 1024 feature dimensions with 16 heads. We append a [CLS] token to the input fMRI signalsbefore feeding them into the fMRI encoder. After self-attention processing, the [CLS] token is expected tocapture the global representation of the entire input fMRI signals. Next, we mask all other latent tokensand encourage the decoder to reconstruct fMRI signals from the learned [CLS] token. Similarly, the decoderincludes 8 transformer layers, with 512 feature dimensions and 16 heads. Different from (Chen et al., 2022)that equally splits fMRI signals into patches, we instead maintain the voxel structure with ROI regions byadditionally adding ROI-dependent processing layers (i.e., ROI Embed and ROI Predict) before and afterthe encoder and decoder, respectively. Both ROI-dependent layers are two-layer modules with a symmetricstructure and not shared among different ROI regions. Taking the ROI Embed layer as an example, it firstadopts a convolution layer with 32 kernels to extract multi-head features of the input fMRI signal and thenuses a fully-connected layer to project the different lengths of fMRI signal into a unified dimension of 1024,leading to the final fMRI embeddings for one ROI as 32 1024. Image Reconstruction.Image-based latent space construction has been widely investigated.As ourtarget is to decode the semantic content of fMRI signals, we adopt the pre-trained CLIP (ViT-H/14)1 as theimage encoder and the yielding CLIP latent space as our image latent space. To generate images from theimage latent space, we utilize MaskGIT2 as the image decoder in this paper, but other generative models arealso acceptable. Specifically, the decoder is a decoder-only transformer layer, with a depth of 24, a featuredimension of 768, and 16 heads. By conditioned on the image latent features, the generation procedurestarts from all [MASK] tokens and autoregressively predicts logits of tokens with higher confidence scores.It is a non-sequential generative procedure, following a cosine mask scheduling function.",
  "A.3. Training and Testing Procedures": "All experiments are implemented with the PyTorch toolkit. The encoder-decoder model for fMRI recon-struction is initialized with pre-trained weights from HCP datasets (Chen et al., 2022). We finetune it withboth fMRI training and testing sets for each individual. Note that using fMRI testing data is a commonpractice as in (Chen et al., 2022; Beliy et al., 2019), and we do not access any paired fMRI-image testingdata when learning linear models. For image reconstruction, we use a frozen CLIP encoder and utilize theimage decoder of MaskGIT initialized by ImageNet (Deng et al., 2009), and further finetune it with imagesfrom the training sets on both the GOD and BOLD5000 datasets. Once trained, it is shared for all subjectsand experiments. AdamW optimizer is applied with 1 = 0.9, 1 = 0.95 and batch size 8 to finetune bothmodels. The initial learning rate is defined as 5e-5 with 0.01 weight decay. We linearly decrease the learningrate until it reaches the minimal rate. The total number of training iterations for fMRI reconstruction andimage reconstruction is 100k and 300k. For fitting linear models, we adopt ridge regression from the Scipylibrary. The coefficient of L2 regularization term is set to 500 for both image-to-fMRI and fMRI-to-imageregressions. During the inference of fMRI decoding, the number of steps for MaskGIT is 11 as default, andwe generate 5 samples for all evaluations."
}