{
  "Abstract": "Choosing a meaningful subset of features from high-dimensional observations in unsuper-vised settings can greatly enhance the accuracy of downstream analysis tasks, such asclustering or dimensionality reduction, and provide valuable insights into the sources ofheterogeneity in a given dataset. In this paper, we propose a self-supervised graph-basedapproach for unsupervised feature selection. Our methods core involves computing robustpseudo-labels by applying simple processing steps to the graph Laplacians eigenvectors.The subset of eigenvectors used for computing pseudo-labels is chosen based on a modelstability criterion. We then measure the importance of each feature by training a surrogatemodel to predict the pseudo-labels from the observations. Our approach is shown to berobust to challenging scenarios, such as the presence of outliers and complex substructures.We demonstrate the effectiveness of our method through experiments on real-world datasetsfrom multiple domains, with a particular emphasis on biological datasets.",
  "Introduction": "Improvements in sampling technology enable scientists across many disciplines to acquire numerous variablesfrom biological or physical systems. One of the critical challenges in real-world scientific data is the presenceof noisy, information-poor, or nuisance features. While such features could be mildly harmful to supervisedlearning, they could dramatically affect the outcome of downstream analysis tasks (e.g., clustering or manifoldlearning) in unsupervised settings (Mahdavi et al., 2019). There is thus a growing need for unsupervisedfeature selection schemes that enhance latent signals of interest by removing nuisance variables and thusadvance reliable data-driven scientific discovery. Unsupervised Feature Selection (UFS) methods are designed to identify a set of informative features thatcan improve the outcome of downstream analysis tasks such as clustering and manifold learning. With thelack of labels, however, selecting features becomes highly challenging since the downstream task cannot beused to drive the selection of features. As an alternative, most UFS methods use a label-free criterion thatcorrelates with the downstream task. For instance, many UFS schemes rely on a reconstruction prior (Liet al., 2017) and seek a subset of features that can be used to reconstruct the entire dataset as accurately aspossible. Several works use Autoencoders (AE) to learn a reduced representation of the data while applyinga sparsification penalty to force the AE to remove redundant features. This idea was implemented withseveral types of sparsity-inducing regularizers, including 2,1 based (Chandra and Sharma, 2015; Han et al.,2018), relaxed 0 (Baln et al., 2019; Shaham et al., 2022; Svirsky and Lindenbaum) and more. One of the most commonly used criteria for UFS is feature smoothness. A common hypothesis is that thestructure of interest, such as clusters or a manifold, can be captured using a graph representation(Ng et al.,2001). The smoothness of features is measured using the Laplacian Score (LS) (He et al., 2005), which is",
  "Published in Transactions on Machine Learning Research (12/2024)": "For LS, MCFS, UDFS, and NDFS, we used an implementation from the scikit-feature library 2 and inputtedthe same similarity matrices as SSFS for the methods which accepted such an argument. We fixed a bug inMCFS implementation to choose by the max of the absolute value of the coefficients instead of the max ofthe coefficients (this improved MCFS performance). For LS-CAE, we used an implementation from 3. ForKNMFS, we used an implementation from 4.",
  "Laplacian score and representation-based feature selection": "Generating a graph-based representation for a group of high-dimensional observations has become a commonpractice for unsupervised learning tasks. In manifold learning, methods such as ISOMAPS (Tenenbaum et al.,2000), LLE (Roweis and Saul, 2000), Laplacian eigenmaps (Belkin and Niyogi, 2003), and diffusion maps(Coifman and Lafon, 2006) compute a low-dimensional representation that is associated with the manifolds",
  "vi vj2Wij.(1)": "The quadratic form in equation 1 gives rise to a notion of graph smoothness. (Ricaud et al., 2019; Shumanet al., 2013). A vector is smooth with respect to a graph if it has similar values on pairs of nodes connectedwith an edge with a significant weight. This notion underlies the Laplacian score suggested as a measure forunsupervised feature selection (He et al., 2005). Let fm Rn denote the values of the m-th feature for allobservations. The Laplacian score sm is equal to,",
  "fm,i fm,j2Wij.(2)": "A low score indicates that a feature is smooth with respect to the computed graph and thus stronglyassociated with the latent structure of the high-dimensional data x1, . . . , xn. The notion of the Laplacianscore has been the basis of several other feature selection methods as well (Lindenbaum et al., 2021; Shahamet al., 2022; Zhu et al., 2012). Let vi, i denote the i-th smallest eigenvector and eigenvalue of the Laplacian L.A slightly differentinterpretation of equation 2 is that the score for each feature is equal to a weighted sum of its correlationwith the eigenvectors, such that",
  "i=1i(f Tmvi)2": "A potential drawback of the Laplacian score is its dependence on all eigenvectors, which may reduce itsstability in measuring a features importance to the datas main structures. To overcome this limitation,Zhao and Liu (2007) derived an alternative score based only on a features correlation to the leading Laplacianeigenvectors. A related, more sophisticated approach is Multi-Cluster Feature Selection (MCFS) (Cai et al.,2010), which computes the solutions to the generalized eigenvector problem Lv = Dv.The leadingeigenvectors are then used as pseudo-labels for a regression task with l1 regularization. Specifically, MCFSapplies Least Angle Regression (LARS) (Efron et al., 2004) to obtain, for each leading eigenvector vi, asparse vector of coefficients i Rp. The feature score is set as the maximum over the absolute values ofthe coefficients computed from the leading eigenvectors, sj = maxi |ij|. The output of MCFS is the set offeatures with the highest score. In the next section, we derive Spectral Self-supervised Feature Selection(SSFS), which improves upon the MCFS algorithm in several critical aspects.",
  "Rationale": "As its title suggests, MCFS aims to uncover features that separate clusters in the data. Let us consider anideal case where the observations are partitioned into k well-separated clusters, denoted A1, . . . , Ak, suchthat the weight matrix Wij = 0 if xi, xj are in separate clusters. Let ei denote an indicator vector for cluster",
  "(b) TOX-171 eigenvectors": ": The first four Laplacian eigenvectors of two real datasets. Samples are sorted according to theclass label and colored by the outcome of a one-dimensional k-medoids per eigenvector. The vertical barindicates the separation between the classes. In Prostate-GE, v4 is the most informative to the class labels,and an outlier can be seen on the upper left in the third and fourth eigenvectors. In TOX-171, v3 is mostinformative to the class labels than v2.",
  "|Ai|j Ai0otherwise,": "where |Ai| denotes the size of cluster Ai. In this scenario, the zero eigenvalue of the graph Laplacian hasmultiplicity k, and the corresponding eigenvectors are equal, up to a rotation matrix, to a matrix E Rnd whose columns are equal to e1, . . . , ek. In this ideal setting, the k leading eigenvectors are indeed suitablefor use as pseudo-labels for the feature selection task. Thus, the MCFS algorithm should provide highlyinformative features in terms of cluster separation. In most applications, however, cluster separation is far from perfect, and the use of leading eigenvectors maybe suboptimal. Here are some common scenarios: 1) High-dimensional datasets may contain substructuresin top eigenvectors, while the main structure of interest appears deeper in the spectrum. For illustration,consider the MNIST dataset visualized via t-SNE in a. The data contains images of 3, 6 and 8.b shows the elements of the six leading eigenvectors of the graph Laplacian matrix, sorted by theircorresponding digits. The leading eigenvector shows a clear gap between images of digit 6 and the rest ofthe data. However, there is no clear separation between digits 3 and 8. Indeed, the next eigenvector is notassociated with such a separation. Applying feature selection with this eigenvector may produce spuriousfeatures irrelevant to separating the two digits. This scenario is prevalent in the real datasets used in theexperimental section. For example, a shows four eigenvectors of a graph computed from observationscontaining the genetic expression data from prostate cancer patients and controls (Singh et al., 2002). Theleading two eigenvectors, however, are not associated with the patient-control separation. 2) The leading eigenvectors may be affected by outliers. For example, an eigenvector may indicate a smallgroup of outliers separated from the rest of the data.This phenomenon can also be seen in the thirdand fourth vectors of the Prostate-GE example in a. While the fourth eigenvector separates thecategories, it is corrupted by outliers and, hence, unsuitable for use as pseudo-labels in a classical regressiontask, as it might highlight features associated with the outliers.",
  "Eigenvector processing and selection": "Generating binary labels. Given the Laplacian eigenvectors V = (v1, ..., vd), our goal is to generatepseudo-labels that are highly informative to the cluster separation in the data.To that end, for eacheigenvector vi, we compute a binary label vector yi (pseudo-labels) by applying a one-dimensional k-medoidsalgorithm (Kaufman and Rousseeuw, 1990) to the elements of vi. In contrast to k-means, in k-medoids,the cluster centers are set to one of the input points, which makes the algorithm robust to outliers. In, the eigenvectors are colored according to the output of the k-medoids. After binarization, thefourth eigenvector of the Prostate-GE dataset is highly indicative of the category. The feature selection isthus based on a classification rather than a regression task, which is more aligned with selecting features forclustering. In .2 we show the impact of the binarization step on multiple real-world datasets. Eigenvector selection. Selecting k eigenvectors according to their eigenvalues may be unstable in caseswhere the eigenvalues exhibit a small spectral gap. We derive a robust criterion for selecting informativeeigenvectors that is based on the stability of a model learned for each vector.Formally, we consider asurrogate model h : Rp R, and a feature score function s(h) Rp, where p denotes the number offeatures. The feature scores are non-negative and their sum is normalized to one. For example, h can be thelogistic regression model h(x) = (T x). In that case, a natural score function is the absolute value of thecoefficient vector . For each eigenvector vi, we train a model hi on B (non-mutually exclusive) subsets ofthe input data X and the pseudo-labels yi . We then estimate the variance of the feature score function, forevery feature m {1, ..., p}:",
  "b=1(sm(hi,b) sm(hi))2": "This procedure is similar (though not identical) to the Delete-d Jackknife method for variance estimation(Shao and Wu, 1989).We keep, as pseudo-labels, the k binarized eigenvectors with the lowest sum ofvariance, Si = pm=1 Var(sm(hi)). We denote the set of selected eigenvectors by I. A pseudo-code for thepseudo-labels generation and eigenvector selection appears in Algorithm 1.",
  "Feature selection": "For the feature selection step, we train k models, denoted {fi | i I}, to predict the selected binary pseudo-labels based on the original data. Similarly to the eigenvector selection step, each model is associated witha feature score function s(fi). The features are then scored according to the following maximum criterion,",
  "Choice of Surrogate Models": "Our algorithm is compatible with any supervised model capable of providing feature importance scores. Wecombine the structural information from the graph Laplacian with the capabilities of various supervisedmodels for unsupervised feature selection. Empirical evidence supports the use of more complex modelssuch as Gradient-Boosted Decision Trees for various complex, real-world datasets (McElfresh et al., 2023;Chen and Guestrin, 2016). These models are capable of capturing complex nonlinear relationships, whichwe leverage by training them on pseudo-labels derived from the Laplacians eigenvectors.For example,for eigenvector selection, one can use a simple logistic regression model for fast training on the resamplingprocedure and a more complex gradient boosting model such as XGBoost (Chen and Guestrin, 2016) for thefeature selection step.",
  "Algorithm 1 Pseudo-code for Eigenvector Selection and Pseudo-labels Generation": "Input: Dataset X Rnp (with n samples and p features), number of eigenvectors to select k, number ofeigenvectors to compute d, surrogate models H = {hi | i [d]}, feature scoring function s : F Rp,number of resamples B 1: Initialize an empty list for the pseudo-labels Y and an empty list for the sums of features variance S2: Compute the significant d eigenvectors of the Laplacian of X: V = (v1, ..., vd)3: for i = 1 to d do4:Binarize the eigenvector vi using k-medoids to obtain yi , and append to Y",
  "Algorithm 2 Pseudo-code for Spectral Self-supervised Feature Selection (SSFS)": "Input: Dataset X Rnp (with n samples and p features) number of eigenvectors to select k, number ofeigenvectors to compute d, surrogate eigenvector selection models H = {hi | i [d]}, surrogate featureselection models F = {fi | i [d]}, feature scoring function s : F Rp, number of resamples B, numberof features to select .",
  "Convergence of the Laplacian eigenvectors": "In many applications, high dimensional observations are assumed to reside close to some manifold M withlow intrinsic dimensionality, which we denote by d. Many papers in recent decades have analyzed the relationbetween the Laplacian eigenvectors and the manifold structure Von Luxburg et al. (2008); Singer and Wu(2017); Garca Trillos et al. (2020); Wormell and Reich (2021); Dunson et al. (2021); Calder and Trillos(2022). More formally, let vk denote the k-th eigenvector of the graph Laplacian, and let gk denote the k-theigenfunction of the Laplace-Beltrami (LB) operator. We usually assume that gk is normalized such that",
  "where gk(X) is a vector of size n containing samples of the function gk at the n rows of the data matrix X": "Let us consider a simple example. Suppose we have n points sampled uniformly at random over the interval. The LB operator over this interval is defined as the second derivative, whose eigenfunctions are theharmonic functions given by gk(x) = cos(kx). shows the three Laplacian eigenvectors computedwith n = 102, 103 and 3 103 points. As n , the vector vk converges to gk(X). Here, we use a convergence result from Cheng and Wu (2022), derived under the following assumptions:(i) The n observations are generated according to a uniform distribution over the manifold, such that (x)equals to a constant . (ii) Let k denote the eigenvalue associated with the eigenfunction gk. To ensure thestability of the eigenvectors, we assume a spectral gap between the smallest K eigenvalues bounded awayfrom 0 such that,K1mini=1 (i+1 i) > > 0.",
  "The product of manifold model": "In a product of two manifolds, denoted M = M1 M2, every point x M is associated with a pair ofpoints x1, x2 where x1 M1 and x2 M2. We denote by 1(x), 2(x) the canonical projections of a pointin M to its corresponding points x1, x2 in M1, M2, respectively. For example, a 2D rectangle is a productof two 1D manifolds, where 1(x) and 2(x) select, respectively, the first and second coordinates. We denote by g(1)i(x), g(2)i(x) the i-th eigenfunction of the LB operator of M1, M2, respectively, evaluated ata point x, and by (1)i , (2)ithe corresponding eigenvalues. In a manifold product M1M2, the eigenfunctionsare equal to the pointwise product of the eigenfunctions of the LB operator of M1, M2, and the correspondingeigenvalues are equal to the sum of eigenvalues, such that",
  "gl,k(x) = g(1)l(1(x)) g(2)k (2(x))l,k = (1)l+ (2)k .(4)": "For simplicity, we denote by vl,k the (l, k)-th eigenvector of the Laplacian matrix, as ordered by l,k. Anexample of a product of 2 manifolds is illustrated in b. The figure shows the leading eight eigenvectorsof the graph Laplacian. The eigenvectors are indexed by the vector b = [l, k]. The full details of this exampleis provided in the next section.",
  "(b) Convergence of eigenvectors for graph on an in-terval": ": Panel 3a shows a scatter plot of the noisy MNIST dataset, containing digits 3 and 8, where eachimage is located according to its coordinates in the third and fourth eigenvectors. Panel 3b shows the leadingeigenvector of a graph computed over n points on a 1D interval and the leading eigenfunction cos(x).",
  "Considerations for eigenvector selection in a product-manifold model": "We analyze a setting where the p features can be partitioned into H sets according to their dependencieson a set of latent and independent random variables 1, . . . , H with some bounded support. A feature fmthat depends on h consists of samples from a smooth transformation hFm fm. We denote by X(h) thesubmatrix that contains the features associated with h. The smoothness of the transformations implies thatthe rows of X(h) constitute random samples from a manifold of intrinsic dimension 1. a shows a 3D scatter plot, where the axis are three such features with values generated by threepolynomials of 1. The figure is an illustration of a manifold with a single intrinsic dimension embedded ina 3D space. The independence of the latent variables h implies that the observations xi Rp are samplesfrom a product of H manifolds, each of dimensionality 1 Zhang et al. (2021); He et al. (2023). The canonical",
  "(b) Eigenvectors of the simulated data": ": Panel (a) illustrates three features of a simulated dataset. Each feature is equal to a differentpolynomial of the same random latent variable 1. Each point in the 3D scatter plot is located accordingto the values of the three features and colored by the value of 1. Panel (b) shows the eigenvectors of thegraph Laplacian matrix. Each point is located according to the value of (1, 2) and colored by the value ofits corresponding element in the eight leading eigenvectors. The eigenvectors are indexed by the vector b,whose elements bi determine the eigenvector order in the submanifold M(i). projection (h)(x) selects the features associated with the latent variable h. According to the eigenfunctionsproperties in equation 4, the eigenfunctions are equal to the product of H eigenfunctions of the submanifoldsM(h), and can thus be indexed by a vector of size H, which we denote by b NH.",
  "j=hg(j)0(j)(x)= Cg(h)1(h)(x),(5)": "where C is some constant. Importantly, the functions g(h)1and thus ge(h), depend only on the parameter (h).We define by E the family of vectors in NH that include the indicator vectors e(h) or their integer products(e.g. 2e(h), 3e(h) etc.). A similar derivation as in equation 5 shows that for every index vector b E, theeigenfunction gb depends on only one of the latent variable in 1, . . . , h. On the relevance of features for choosing eigenvectors as pseudo-labels.Our goal is to select aset of features that contains at least one (or more) features from each of the H partitions. Such a choicewould ensure that the set contains information about all the H latent variables. Clearly, this imposes arequirement on the set of pseudo-label vectors: we would like at least one vector of pseudo-labels that iscorrelated with each latent variable. It is instructive to consider the asymptotic case where n and hence according to Theorem 1 and theproperties of manifold products, the eigenvectors vb converge to gb(X). A proper choice of eigenvectors forpseudo-labels would be the set {ve(h)}Hh=1, as each of these vectors converges to the samples g(h)1 (X), and isthus associated with a different latent variable. However, there is no guarantee that these eigenvectors havethe smallest eigenvalues.",
  "Step 2: Combine the convergence of vb to gb(X) with the concentration result of step 1": "Theorem 2 implies that one can use the inner products to avoid selecting less informative eigenvectors thatdepend on more than one variable. Further guarantees, such as selection of a single vector from each variable,require additional assumptions on the feature values, which we do not make here. In Algorithm 1 we compute the normalized measure of stability for the feature scores {sm(hi)}pm=1 obtainedby the model hi to predict the labels computed from the i-th eigenvector. When the model hi is linear (orgeneralized linear), the score is strongly related to the simple inner product of Theorem 2. In that case,Theorem 2 indicates that the inner product between uninformative eigenvectors and all features is close tozero. Thus, we expect the variance (after normalization) to be similar to the variance of random positivenoise. The advantage of the stability measure over the simple linear product as an eigenvector selectioncriterion is that it allows for more flexibility in the choice of model.",
  "Computational Complexity": "The computational complexity of our proposed method consists of several key steps. First, constructing thegraph representation of the dataset using a Gaussian kernel requires computing pairwise distances betweenall data points, resulting in a complexity of O(n2p), where n is the number of samples and p is the numberof features. Following this, the eigendecomposition of the graph has a complexity of O(n3). This can bereduced by randomized methods to O(n2d), since we only require the first d eigenvectors. Once eigenvectorsare computed, we apply the k-medoids algorithm to cluster the eigenvectors and generate pseudo-labels,adding a further complexity of O(dkn2), where k is the number of clusters. After creating pseudo-labels, we use surrogate models such as logistic regression or gradient-boosted decisiontrees to evaluate the features. Logistic regression has a complexity of O(nd), while gradient-boosted trees,being more complex, scale with O(Tn log n), where T is the number of trees. We also use a resampling pro-cedure to estimate the variance of feature importance scores. Each resampling involves training a surrogatemodel, resulting in an overall complexity of O(BTn log n) for gradient-boosted trees or O(Bnd) for logisticregression. As a result, the overall computational complexity of SSFS is mainly influenced by the graphconstruction and training the surrogate mode, resulting in a complexity of O(n2d + Bnd).",
  "Evaluation on real world datasets": "Data and experiment description.We applied SSFS to eight real-world datasets from various domains. in Appendix F.2 gives the number of features, samples, and classes in each dataset. All datasets areavailable online 1. We compare the performance of our approach to the following alternatives: (i) standard Laplacian score (LS)(He et al., 2005), (ii) Multi-Cluster Feature Selection (MCFS) (Cai et al., 2010), (iii) Nonnegative Discrimina-tive Feature Selection (NDFS), (Li et al., 2012), (iv) Unsupervised Discriminative Feature Selection (UDFS)(Yang et al., 2011), (v) Laplacian Score-regularized Concrete Autoencoder (LS-CAE) (Shaham et al., 2022),(vi) Unsupervised Feature Selection Based on Iterative Similarity Graph Factorization and Clustering byModularity (KNMFS) (Oliveira et al., 2022) and (vii) a naive baseline, where random selection is appliedwith a different seed for each number of selected features. For evaluation, we adopt a criterion that is similar to, but not identical to, the one used in prior studies(Li et al., 2012; Wang et al., 2015). We select the top 2, 5, 10, 20, 30, 40, 50, 100, 150, 200, 250, and 300features as scored by each method. Then, we apply k-means 20 times on the selected features and reportthe average clustering accuracy (along with the standard deviation), computed by (Cai et al., 2011):",
  "i=1((ci), li),": "where ci and li are the assigned cluster and true label of the i-th data point, respectively, (x, y) is the deltafunction which equals one if x = y and zero otherwise, and represents a permutation of the cluster labels,optimized via the Kuhn-Munkres algorithm (Munkres, 1957). Unlike the evaluation approach taken by Wang et al. (2015); Li et al. (2012), which entailed a grid search overhyper-parameters to report the optimum results for each method, our analysis employed the default hyper-parameters as specified by the respective implementations, including SSFS. This approach aims for a faircomparison to avoid favoring methods that are more sensitive to hyper-parameter adjustments. In addition,it acknowledges the practical constraints in unsupervised settings where hyper-parameter tuning is typicallyinfeasible. Such differences in the approach to hyper-parameter selection could account for discrepanciesbetween the results reported in previous studies and those in our study. See Appendix F.1 for additionaldetails. : Average clustering accuracy on benchmark datasets along with the standard deviation. The numberof selected features yielding the best clustering performance is shown in parentheses, the best result for eachdataset highlighted in bold.",
  "Mean rank4.125.884.624.946.444.383.312.31Median rank4.06.55.55.56.54.52.752.25": "shows, for each method, the highest average accuracy and the number of features for which itwas achieved similarly to (Li et al., 2012; Wang et al., 2015). presents a comparative analysis ofclustering accuracy across various datasets and methods, considering the full spectrum of selected features.This comparison aims to account for the inherent variance in each method, addressing a limitation where thecriterion of the maximum accuracy over the number of selected features might inadvertently favor methodsexhibiting higher variance.",
  ": Clustering accuracy vs. the number of selected features on eight real-world datasets": "For SSFS, we use the following surrogate models: (i) The eigenvector selection model hi is set to LogisticRegression with 2 regularization.We use scikit-learns (Pedregosa et al., 2011) implementation with adefault regularization value of C = 1.0.Feature scores are equal to the absolute value of the modelscoefficients. (ii) The feature selection model fi is set to XGBoost classifier with Gain feature importance.We use the popular implementation by DMLC (Chen and Guestrin, 2016). Note that we employ the default hyper-parameters for all surrogate models as provided in their widely usedimplementations. However, its worth noting that one can undoubtedly leverage domain knowledge to selectsurrogate models and hyperparameters better suited to the specific domain. For each dataset, SSFS selectsk from d = 2k eigenvectors, where k is the number of distinct classes in the data. Results.SSFS has been ranked as the best method in three out of eight datasets. It has shown a significantadvantage over competing methods, especially in the Yale, TOX-171, and Prostate-GE datasets. As discussedin .1, the Prostate-GE dataset has several outliers, and the fourth eigenvector plays a vital role inproviding information about the class labels compared to the earlier eigenvectors. SSFS can effectively dealwith such challenging scenarios, and this might explain its superior performance. Although our method isnot ranked first in the other five datasets, it has produced results comparable to the leading method.",
  "Ablation study": "We demonstrate the importance of three SSFS components: (i) eigenvector selection, (ii) self-supervisionwith nonlinear surrogate models, and (iii) binarization of the Laplacian eigenvectors along with classifiersinstead of regressors as surrogate models. The ablation study is performed on a synthetic dataset describedin .2.1, and the eight real datasets used for evaluation in .1.",
  "Synthetic data": "We generate a synthetic dataset as follows: the first five features are generated from two isotropic Gaussianblobs; these blobs define the clusters of interest. Additional 45 nuisance features are generated according toa multivariate Gaussian distribution, with zero mean and a block-structured covariance matrix , such thateach block contains 15 features. The covariance elements i,j are equal to 0.5 if i, j are in the same blockand 0.01 otherwise. We generated a total of 500 samples; see Appendix E.1 for further details. In a, you can see a scatter plot of the first five features, and in b, you can see a visualization of thecovariance matrix. Our goal is to identify the features that can distinguish between the two groups.",
  "(c) Eigenvectors": ": Visualizations of the synthetic data: Panel (a): scatter plot of the first five features correspondingto the Gaussian blobs, colored by the real label. Panel (b): the covariance matrix of the dataset. Panel(c): the top-4 eigenvectors, samples are sorted by the label and are partitioned by the vertical bar, coloredaccording to the output of k-medoids.",
  "Mean rank2.944.03.03.51.56Median rank2.54.53.53.51.25": "As a demonstrates, the two clusters are linearly separated by three distinct features. Furthermore,examining c reveals that while the fourth eigenvector distinctly separates the clusters, the higher-ranked eigenvectors do not exhibit this behavior. This pattern arises due to the correlated noise, significantlyinfluencing the graph structure. The evaluation of this dataset is performed by calculating the true positiverate (TPR) with respect to the top-selected features and the discriminative features sampled from the twoGaussian blobs. The performance on the real-world datasets is measured similarly to .1.",
  "Results": "Eigenvector Selection.We compare to a variation of SSFS termed SSFS (no selection), where we dontfilter the eigenvectors. We train the surrogate feature selector model on the leading k eigenvectors, withk set to the number of distinct classes in the data. b shows that our eigenvector selection schemeprovides an advantage in seven out of eight datasets. Similarly to Sec. 5.1, filtering the eigenvectors is espe-cially advantageous on the Prostate-GE dataset, as our method successfully selects the most discriminativeeigenvectors (see a ). On the synthetic dataset, the selection procedure provides a large advantage,",
  ": Ablation study results on the real-world datasets. The best clustering accuracy over the numberof selected features is shown for each method": "Complex nonlinear models as surrogate models.We compare SSFS to a variant of our methoddenoted SSFS (no XGBoost), which employs a logistic regression instead of XGBoost as the surrogatefeature selector model. b shows that XGBoost provides an advantage compared to the linear modelon real-world datasets. On the synthetic dataset, the linear variant provides better coverage for the top-3features that separate the Gaussian blobs, compared to XGBoost (see and a). That isnot surprising since, in this example, the cluster separation is linear in each informative feature. We note,however, that the top-ranked feature by SSFS with XGBoost is a discriminative feature for the clusters inthe data (see a); therefore, its selection can still be considered successful in the case of a singlefeature selection.",
  "Discussion and future work": "We proposed a simple procedure for filtering eigenvectors of the graph Laplacian and demonstrated that itsapplication could have a significant impact on the outcome of the feature selection process. The selectionis based on the stability of a classification model in predicting binary pseudo-labels. However, additionalcriteria, such as the accuracy of a specific model or the overlap of the chosen features for different eigenvectors,may provide information on the suitability of a specific vector for a feature selection task. We also illustratedthe utility of expressive models, typically used for supervised learning, in unsupervised feature selection.Another direction for further research is using self-supervised approaches for group feature selection (GFS)for single modality (Sristi et al., 2022) or multi-modal data (Yang et al., 2023; Yoffe et al., 2024). In contrastto standard feature selection where the output is sparse, GFS aims to uncover groups of features with jointeffects on the data. Learning models based on different eigenvectors may provide information about groupeffects with potential applications such as detecting brain networks in Neuroscience and gene pathways ingenetics.",
  "Broader Impact Statement": "This work involves methods applicable to high-dimensional data, including sensitive domains like healthcare.While our experiments used publicly available datasets, minimizing immediate privacy concerns, practitionersapplying this method to sensitive data should ensure compliance with data protection regulations and adoptrobust privacy-preserving measures. There is also a risk of misapplication in critical areas such as medical diagnosis or criminal justice, wherebiased or incorrect feature selection could lead to harmful decisions. To mitigate this, we recommend inte-grating bias detection and mitigation techniques into the training pipeline to enhance fairness and reliability.",
  "Bradley Efron, Trevor Hastie, Iain Johnstone, and Robert Tibshirani. Least angle regression. The Annalsof Statistics, 32(2):407 499, 2004": "Nicols Garca Trillos, Moritz Gerlach, Matthias Hein, and Dejan Slepev.Error estimates for spectralconvergence of the graph laplacian on random geometric graphs toward the Laplace-Beltrami operator.Foundations of Computational Mathematics, 20(4):827887, 2020. Kai Han, Yunhe Wang, Chao Zhang, Chao Li, and Chao Xu. Autoencoder inspired unsupervised featureselection. In 2018 IEEE international conference on acoustics, speech and signal processing (ICASSP),pages 29412945. IEEE, 2018.",
  "Zechao Li and Jinhui Tang. Unsupervised feature selection via nonnegative spectral analysis and redundancycontrol. IEEE Transactions on Image Processing, 24(12):53435355, 2015": "Zechao Li, Yi Yang, Jing Liu, Xiaofang Zhou, and Hanqing Lu.Unsupervised feature selection usingnonnegative spectral analysis. In Proceedings of the AAAI conference on artificial intelligence, volume 26,pages 10261032, 2012. Ofir Lindenbaum, Uri Shaham, Erez Peterfreund, Jonathan Svirsky, Nicolas Casey, and Yuval Kluger. Dif-ferentiable unsupervised feature selection based on a gated laplacian. Advances in Neural InformationProcessing Systems, 34:15301542, 2021. Kaveh Mahdavi, Jesus Labarta, and Judit Gimenez.Unsupervised feature selection for noisy data.InAdvanced Data Mining and Applications: 15th International Conference, ADMA 2019, Dalian, China,November 2123, 2019, Proceedings 15, pages 7994. Springer, 2019. Duncan McElfresh, Sujay Khandagale, Jonathan Valverde, Ganesh Ramakrishnan, Micah Goldblum, ColinWhite, et al.When do neural nets outperform boosted trees on tabular data?arXiv preprintarXiv:2305.02997, 2023. Marina Meil.Comparing clusterings by the variation of information.In Learning Theory and KernelMachines: 16th Annual Conference on Learning Theory and 7th Kernel Workshop, COLT/Kernel 2003,Washington, DC, USA, August 24-27, 2003. Proceedings, pages 173187. Springer, 2003.",
  "Andrew Ng, Michael Jordan, and Yair Weiss. On spectral clustering: Analysis and an algorithm. Advancesin neural information processing systems, 14, 2001": "Marcos de S Oliveira, Sergio R de M Queiroz, and Francisco de AT de Carvalho. Unsupervised featureselection method based on iterative similarity graph factorization and clustering by modularity. ExpertSystems with Applications, 208:118092, 2022. F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer,R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay.Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:28252830, 2011.",
  "Qi-Hai Zhu and Yu-Bin Yang. Discriminative embedded unsupervised feature selection. Pattern RecognitionLetters, 112:219225, 2018": "Xiaofeng Zhu, Shichao Zhang, Rongyao Hu, Yonghua Zhu, et al. Local and global structure preservation forrobust unsupervised spectral feature selection. IEEE Transactions on Knowledge and Data Engineering,30(3):517529, 2017. Xiaofeng Zhu, Shichao Zhang, Yonghua Zhu, Pengfei Zhu, and Yue Gao. Unsupervised spectral featureselection with dynamic hyper-graph learning. IEEE Transactions on Knowledge and Data Engineering,34(6):30163028, 2020.",
  "The elements of the random vectors a(1) and d(2, . . . , H) are statistically independent. In addition, wehave that fi = 1 andgh((h)(X)) = 1 + o(1)(h),": "see for example (Cheng and Wu, 2022, Lemma 3.4). This implies that both a(1) and d(2, . . . , H) arebounded by 1 + o(1). The inner product between two independent random vectors with unit norm and iidelements is of order O(1/n), (see for example (Vershynin, 2020, Remark 3.2.5)). Thus,",
  "(b) Yale dataset": ": Histograms of VI value distributions for SSFS-selected features (orange) and all features (blue) on(a) TOX 171 and (b) Yale datasets. Lower VI values indicate higher stability in clustering results. stability of clustering results, which is valuable for assessing feature selection quality. For noisy features,we expect unstable clustering outputs with high variability between runs, while informative features shouldyield more stable results with less variability. Our analysis involves applying k-means clustering to the selected features 500 times with random initializa-tions, calculating the VI between all pairs of clustering outputs, and analyzing the distribution of VI values.Lower average VI and tighter distribution indicate higher stability, suggesting more informative features. Wecompared the stability of clusters obtained using the original dataset versus those obtained using featuresselected by our Stable Spectral Feature Selection (SSFS) method. presents results for the TOX171 and Yale datasets, demonstrating clear improvement in stabilitywhen using SSFS. For both datasets, SSFS-selected features yield lower mean VI values, indicating moreconsistent clustering outcomes. visualizes the distributions of VI values for both datasets, showingthe shift towards lower VI values when using SSFS-selected features compared to using all features.",
  "Interpretability is a fundamental aspect of unsupervised feature selection, particularly in domains whereunderstanding the meaning and relevance of selected features is crucial": "In the context of our proposed SSFS algorithm, interpretability can be enhanced in several ways. Featureranking, by examining the weights or importance scores assigned to features during the selection process,can provide insights into which features are most important across the dataset. Visualization techniques, asdemonstrated in , can reveal patterns and distributions that highlight the discriminative capabilityof selected features across different classes or clusters. Domain-specific analysis is another effective approach.In biological datasets, for instance, selected features can be mapped back to specific genes or biomarkers,allowing domain experts to validate the biological relevance of the selected features and potentially uncovernew insights about the underlying biological processes. These interpretability techniques enhance the utility of unsupervised feature selection algorithms like SSFSby providing insights into underlying relationships and feature relevance, which are essential for real-worldapplications across various domains.",
  "DAdditional experiments on synthetic data": "We conducted further experiments to test our methods robustness to noise, using two synthetic datasetswith 100 samples per trial over 10 trials. Both datasets contain five meaningful features from two isotropicGaussian blobs but differ in their nuisance features. The first datasets nuisance features are sampled from amultivariate Gaussian distribution, similar to .2.1, but with varying numbers of nuisance features.The second uses a copula of correlated nuisance features formed of marginals with a Laplace distribution.We measure performance using recall, comparing the top five ranked features to the five known meaningfulones. Results in show SSFS with logistic regression as a linear classifier (denoted by linear_clf) outper-forming other methods across both experiments. For Gaussian nuisance features (a), SSFS with alinear classifier maintained consistently high performance up to 60 features, while other methods exhibiteda more pronounced decline as the number of nuisance features grew. In the case of Laplace-distributednuisance features (b), the advantage of SSFS with a linear classifier was even more evident, with itsperformance decreasing more slowly as the number of nuisance features increased. Overall, these experimentsdemonstrate that SSFS is a robust method for identifying relevant features, performing well for various noisedistributions and maintaining high recall even with increasing noise levels.",
  "(b) Laplace nuisance features": ": Performance comparison of feature selection methods under increasing noise levels. The x-axisshows the number of nuisance features added to the give meaningful ones, while the y-axis represents theaverage recall (proportion of the top five ranked features matching the five meaningful features).Eachpoint is the average over 10 trials with 100 samples each. SSFS with a linear classifier (logistic regression)consistently outperforms MCFS and the other SSFS variants.",
  "F.3Hyperparameters": "Hyperparameter selection in unsupervised feature selection presents unique challenges and may be infeasibledue to the absence of labeled data for validation. In our methods, we have two types of hyperparameters:(i) For computing the spectral representation (kernel bandwidth and the number of eigenvectors) and (ii)parameters relating to the surrogate model, which predicts the pseudo-labels (i.e., regularization parameterfor Logistic Regression). Note that the second type of parameters is used in a prediction task; they can bedetermined similarly to hyperparameters in supervised settings, for example, through cross-validation. In our experiments, for the surrogate models, we leverage the default hyperparameters from establishedmachine learning packages such as XGBoost and scikit-learns logistic regression, as these are generallyrobust for a wide range of supervised tasks. However, surrogate models and their hyperparameters can alsobe chosen by performing hyperparameter tuning with cross-validation based on pseudo-labels derived fromthe eigenvectors. The selection of surrogate models is flexible and can be additionally guided by domainknowledge. Practitioners can also compare results across various models and analyze their selected features. The first type of hyperparameters is inherent to the method, for example, the number of considered eigenvec-tors, which should increase with the number of classes and noise level in the data. For instance, in scenarioswith more classes or where noise dominates the spectrum, a larger range of eigenvectors should be considered.While we used 500 resamples in our experiments, this number can be adjusted for larger datasets to balancecomputational cost and performance. Although traditional hyperparameter tuning may not be feasible inmost unsupervised scenarios, practitioners working with domain-specific data could potentially fine-tuneparameters using cross-validation on similar, labeled datasets from the same domain (such as images)."
}