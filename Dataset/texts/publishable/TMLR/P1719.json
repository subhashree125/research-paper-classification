{
  "Abstract": "Mechanism design in resource allocation studies dividing limited resources among self-interested agents whose satisfaction with the allocation depends on privately held utilities.We consider the problem in a payment-free setting, with the aim of maximizing socialwelfare while enforcing incentive compatibility (IC), i.e., agents cannot inflate allocations bymisreporting their utilities. The well-known proportional fairness (PF) mechanism achievesthe maximum possible social welfare but incurs an undesirably high exploitability (themaximum unilateral inflation in utility from misreport and a measure of deviation fromIC). In fact, it is known that no mechanism can achieve the maximum social welfare andexact incentive compatibility (IC) simultaneously without the use of monetary incentives(Cole et al., 2013). Motivated by this fact, we propose learning an approximate mechanismthat desirably trades off the competing objectives. Our main contribution is to design aninnovative neural network architecture tailored to the resource allocation problem, whichwe name Regularized Proportional Fairness Network (RPF-Net). RPF-Net regularizes theoutput of the PF mechanism by a learned function approximator of the most exploitableallocation, with the aim of reducing the incentive for any agent to misreport. We derivegeneralization bounds that guarantee the mechanism performance when trained under finiteand out-of-distribution samples and experimentally demonstrate the merits of the proposedmechanism compared to the state-of-the-art. The PF mechanism acts as an important benchmark for comparing the social welfare of anymechanism. However, there exists no established way of computing its exploitability. Thechallenge here is that we need to find the maximizer of an optimization problem for whichthe gradient is only implicitly defined. We for the first time provide a systematic methodfor finding such (sub)gradients, which enables the evaluation of the exploitability of the PFmechanism through iterative (sub)gradient ascent.",
  "while ensuring incentive compatibility (IC) (Hurwicz, 1973), i.e., that strategic agents cannot inflate theirallocation by misrepresenting utilities": "This problem class breaks down along (i) the number of items to allocate, (ii) whether items are divisible orindivisible, (iii) the manner in which demand is communicated to the supplier (monetary or non-monetarypreferences), (iv) the measure of fairness/social welfare. (v) whether the agents are truthful or strategic.Regarding (i): in the single-item multiple-agent case, optimal mechanism design has been resolved whenmonetary payments are made by the agents to the supplier (Myerson, 1981). Addressing the case of multiplebidders and items is the focus of this work, following Cai et al. (2012a;b); Yao (2014). Regarding (ii), indivisible item settings exhibit fundamental bottlenecks due to combinatorial optimizationunderpinnings (Chevaleyre et al., 2006; Snmez & nver, 2011). In this work, we study the divisible setting,which is applicable to, e.g., financial assets (Ko & Lin, 2008) and GPU hours (Ibrahim et al., 2016; Aguileraet al., 2014). Regarding (iii): an agent requesting resources must indicate its interest in the form of a monetary bid oralternatively provide a utility function that represents its preferences. Numerous prior works on resourceallocation consider auctions, where payments indicate interest in resources (Pavlov, 2011; Giannakopoulos& Koutsoupias, 2014; Dtting et al., 2023), yet many settings forbid payment (e.g. organ donation, foodand necessity distribution by charity, allocation of GPU hours by an institution to its employees). Our workfocuses on designing mechanisms without money (Dekel et al., 2010; Procaccia & Tennenholtz, 2013; Coleet al., 2013). (iv) In the payment-free setting, measuring whether an allocation is fair among agents on a social level maybe formalized through proportional fairness (PF) (Kelly, 1997). An allocation is said to achieve PF if anydeviation from the allocation results in a non-positive change in the percentage utility gain summed overall agents. It is known (see Bertsimas et al. (2011)) that PF is achieved by maximizing Nash social welfare(NSW), the product of all agents utilities, which we use as a quantifier of fairness/social welfare in this work. What remains is (v) whether agents are truthful or strategic, i.e., they may misreport their preferences. Inthe later case, the mechanism needs to be designed so as not to provide incentive for the agents to misreport,i.e., to possess incentive compatibility (IC). The PF mechanism, which directly optimizes NSW of theallocation outcome assuming truthful reporting, is not incentive compatible. By contrast, several recentworks study IC mechanism design without money (Dekel et al., 2010; Procaccia & Tennenholtz, 2013; Coleet al., 2013). Most germane to this discussion is the Partial Allocation (PA) mechanism (Cole et al., 2013)based on money burning (Hartline & Roughgarden, 2008), where the supplier intentionally withholds aproportion of the resources as an artificial payment. However, this mechanism is incentive compatible atsignificant cost to NSW. In this work, we develop a payment-free mechanism that balances the competing criteria of fairness (in NSWsense) and IC. Our approach formulates the allocation problem as the one that maximizes NSW subject to aconstraint on exploitability, which quantifies the maximum obtainable gain from misreporting and measuresthe deviation from exact IC. Similar trade-offs are considered in related resource allocation settings thatrequire monetary payment (Dtting et al., 2023; Ivanov et al., 2022) and those not involving payments (Zenget al., 2024), as well as in game-theoretic pricing models (Goktas & Greenwald, 2022). We detail our maincontributions below. We develop a subgradient-based method for computing the exploitability of the PF mechanism. ThePF mechanism acts as an important benchmark for welfare maximization in resource allocation.However, there exists no established method for computing the exploitability of the PF mechanism yet,making it difficult to compare against. This paper fills in the gap. As the PF mechanism is definedthrough an optimization program, our main innovation here is to derive the (sub)gradient of theoutput of the program with respect to the input, extending the results from the differentiable convexprogramming literature. This allows us to calculate the exploitability by iterative (sub)gradientascent. We believe that the technique for differentiating through the PF mechanism may be ofbroader interest beyond mechanism design.",
  "Published in Transactions on Machine Learning Research (01/2025)": "Sbastein Casault, Aard J Groen, and Jonathan D Linton. On the use of the cauchy distribution to describeprice fluctuations in r&d and other forms of real assets. In 8th ESU Conference on Entrepreneurship (2011),p 1-9. Universidad de Sevilla, 2011. Shuchi Chawla, Jason D Hartline, David L Malec, and Balasubramanian Sivan. Multi-parameter mechanismdesign and sequential posted pricing. In Proceedings of the forty-second ACM symposium on Theory ofcomputing, pp. 311320, 2010.",
  "Related Work": "Our paper relates to the existing literature on resource allocation with and without monetary payments, aswell as those that study incentive compatibility in the context of mechanism design and beyond. We discussthe most relevant works in these domains to give context to our contribution. Resource allocation without payment: Guo & Conitzer (2010) considers the problem of allocating twodivisible resources to two agents and show that any IC mechanism achieves at most 0.841 of the optimalsocial welfare in the worst case (defined as an overall utility of all agents). They further show that a linearincreasing-price mechanism nearly achieves the lower bound. Han et al. (2011) obtains an analogous worst-caselower bound for a more general case of n agents but does not provide a mechanism. Cole et al. (2013) provesthat no IC mechanism can guarantee to every agent a fraction of their proportionally fair allocation greaterthan n+1 2n as the number of resources becomes large, and proposes a partial allocation (PA) mechanism thatallocates to each agent a fraction (at least 1/e) of the corresponding PF allocation. These works assume thatthe supplier has no prior knowledge of the agents preference (or its distribution), whereas our work considersthe setting where (possibly inaccurate) historical samples of the agents utility parameters are available fortraining. Also assuming the access to training samples, Zeng et al. (2024) is highly related to our work. Zenget al. (2024) studies learning an approximately fair and IC mechanism named ExS-Net parameterized by aneural network and trains the network parameters on the same objective that we consider in this work. Ourimportant distinction from ExS-Net lies in the activation function of the output layer: while the activationfunction in ExS-Net is composed of a simple softmax function and a synthetic agent which receives theportion of resources to be withheld, the activation function we propose leverages a convex optimizationprogram. ExS-Net is an important baseline for comparison. In we show that the proposed RPF-Netmechanism materially outperforms ExS-Net due to the innovation in the network architecture. Auction design/resource allocation with payment: When payments from the agents to the supplierare allowed, Chawla et al. (2010); Roughgarden (2010); Brnzei et al. (2022) design sophisticated monetarytransfer schemes that ensure truthful reporting. More recently, (Dtting et al., 2023; Ivanov et al., 2022) adopta learning-based framework similar to the one in our paper. A neural-network-parameterized mechanism,RegretNet, is proposed in (Dtting et al., 2023), which determines the price of a resource with the aim ofincreasing suppliers revenue while guaranteeing the approximate truthfulness of the agents. Our work ispartially inspired by RegretNet. Similar to our differentiation through an optimization program approachin spirit, Curry et al. (2022) learns approximately IC and revenue-maximizing auction mechanisms, designed",
  "through a differentiable regularized linear program. Compared to RegretNet, the method in Curry et al.(2022) enlargens the solvable class of auctions": "Approximate incentive compatibility in ML: Dekel et al. (2010) consider regression learning where theglobal goal is to minimize average loss in the setting of strategic agents that might misreport their values overthe input space. When payments are disallowed, they present a mechanism which is approximately IC andoptimal for the special case of the buyers utilities defined by the absolute loss. More recently, (Chen et al.,2020) focus on learning linear classifiers, when the training data comes in online manner from the strategicagents who can misreport the feature vectors, and propose an algorithm that exploits the geometry of thelearners action space. Ravindranath et al. (2021) is another highly related work that designs approximatelyIC and stable mechanisms for two-sided matching using concepts from differentiable economics. Organization. The rest of the paper is structured as follows. In we formulate the payment-freeresource allocation problem and introduce the existing mechanisms. In , we present the proposedmechanism with the novel neural network architecture as well as the procedure to train the mechanism.In we develop the methods for evaluating the exploitability of the PF mechanism and for back-propagating the gradient through RPF-Net during training. provides guarantees on the performanceof RPF-Net trained under finite and out-of-distribution samples. Simulations that illustrate the merits of theproposed mechanism are presented in . Finally, we conclude and reflect on the connection to theliterature in .",
  "Preliminaries & Problem Formulation": "We study the problem of allocating a finite number M of divisible resources to N agents in the form of avector a RNM, with ai,m the amount of resource m allocated to agent i. There is a budget bm 0 on eachresource m = 1, , M, and we denote b = [b1, , bM] B RM+ . We assume that every agent has a(thresholded) linear additive utility each additional unit of resource m linearly increases the utility of agent iby value vi,m, up to the demand xi,m. We represent the overall values and demands as vi = [vi,1, , vi,M],xi = [xi,1, , xi,M], and v = [v1 , , vN], x = [x1 , , xN]. Given allocation a RNM+, value v V,and demand x D, the utility function u : RNM+ V D RN+ is agent-wise expressed as",
  "with V [v, v]NM, D [d, d]NM for some scalars v, v, d, d > 0": "The linear additive utility structure is widely considered and realistically models satisfaction in many practicalproblems (Bliem et al., 2016; Camacho et al., 2021), though the mechanism proposed in the paper may handlealternative utility functions. The supplier knows the functional form of the utility function and relies on eachagent i to report the parameters vi and xi. It may be in the interest of the agents to report untruthfully. Since the allocation needs to satisfy budget constraints and should not exceed what the agents request, validallocations have to be contained in the set Ab,x = {a RNM : 0 ai,m xi,m, i, m, Ni=1 ai,m bi, m}. Amechanism may incorporate non-negative agent weights w RN+ to encode prioritization. Next, we formalizethat a mechanism is a mapping from values, demands, budgets, and weights to a valid allocation.Definition 1 (Mechanism). A mapping f : V D B RN+ RNM+is a mechanism if f(v, x, b, w) Ab,xfor all v V, x D, b B, w RN+. Agent weights w may be determined solely by the supplier before the agents reveal their requests or canpotentially be a function of the reported values and demands. Since agents may not directly alter weights, wesuppress dependence on w to write allocations as f(v, x, b). Two competing axes for characterizing the merit of a mechanism exist: social welfare and incentive compatibility(IC). Among the many social welfare metrics (see (Bertsimas et al., 2011)), we consider Nash social welfare(NSW), which balances fairness and efficiency.Definition 2 (Nash Social Welfare). The NSW of a mechanism f is",
  "The definition simplifies to logNSW(f, v, x, b) = w log u(f(v, x, b), v, x)": "To quantify approximate incentive compatibility, we next introduce exploitability as the maximum positiveunilateral deviation in an agent utility when it misreports its preferences.Definition 3 (Exploitability). Under mechanism f and v V, x D, b B, we define the exploitability atagent i as its largest possible utility increase due to misreporting, given that the every agent j = i reports vj, xj",
  "A mechanism f is incentive compatible (IC) if it satisfies expli(f, v, x, b) = 0, for all v, x, b and i = 1, , N": "Assuming that the agents truthfully report their preferences, the following mechanism directly seeks tooptimize the (log) NSW and achieves the largest possible NSW by definition. The mechanism is usuallyreferred to as the Proportional Fairness (PF) mechanism, as the solution satisfies the proportional fairnessproperty: when switching to any other allocation the percentage utility changes across all agents sum up to anon-positive value (Caragiannis et al., 2019).",
  ": Exploitability of PF and learned RPF-Net": "In this section, we illustrate the exploitability of thePF mechanism using a simple example. Consider atwo-agent two-resource allocation problem, in whichwe choose x = 1NM, b = 1M, and w = 1N. Bothagents have a higher valuation for the first resource,with v1 = {1, 1/2} and v2 = {1, 1/4}. When agent2 reports truthfully, plots the utility ofagent 1 [cf. (1)] as it varies the reported preferenceratio v1,2/v1,1 from 0.1 to 3 (the true ratio is 0.5).With the dashed line indicating the utility of agent1 under truthful reporting, under-reporting v1,2/v1,1increases agent 1s utility up to 17%. By contrast,RPF-Net proposed in this work substantially reducesthe exploitability. The state-of-the-art IC mechanism in the non-payment setting is Partial Allocation (PA) (Coleet al., 2013), built upon the PF mechanism, whichstrategically withholds resources to ensure truthfulness1. Under the PA mechanism, each agent can only be",
  "Learning Approximate Payment-Free Mechanism": "In this work, we are motivated to design a mechanism that approximately optimizes NSW and exploitabilityand strikes a desirable balance between them. Specifically, assuming that in a resource allocation problem thevalues and demands of the agents and the budgets on the resources follow a joint distribution F, we formalizeour objective as maximizing the (log)NSW subject to a constraint on the exploitability for some 0.",
  "We say a mechanism is -incentive compatible over the distribution F if it satisfies the constraints in (5)": "PA and PF mechanisms are on the two ends of the trade-off between NSW and exploitability that are notoptimal/feasible in the sense of (5). In this work, we propose parameterizing the mechanism with a neuralnetwork and learning the solution to (5) from data. This can be regarded as an adaptation of Dtting et al.(2023) to the payment-free setting. However, different from the auction setting, due to the impossibility resultwhich states that IC cannot be achieved without money burning (i.e., resource withholding) (Hartline& Roughgarden, 2008), a regular neural network not equipped with the ability to withhold may fail toachieve low exploitability. In the following section, we propose a mechanism built on a novel neural networkarchitecture specially designed for learning (5). We show later through numerical simulations that thearchitectural innovation is crucial the mechanism with the proposed architecture performs substantiallybetter than a learned mechanism parameterized by a regular neural network or ExS-Net (Zeng et al., 2024)which is the state-of-the-art learning-based solution.",
  "Regularized Proportional Fairness Network": "We develop a novel learning-based mechanism for resource allocation that can be regarded as a composition ofa neural network (in this work we employ a feed-forward neural network of identical structure to parameterizeboth mechanisms, but in general any function approximation can be used) and a novel specific-purposeactivation function. The mechanism, RPF-Net (Regularized Proportional Fairness Network), is a regularizedvariant of the PF mechanism that diverts allocation from what the agents would like to receive when theymisreport to their largest advantage. A schematic representation of the mechanism is presented in .",
  "Neural Network Design": "The motivation behind RPF-Net is that we would like to reduce the agents incentive to misreport through alinear regularization added to the NSW objective. We use a function approximation to track the allocationthat an agent aims to receive when misreporting, and penalize such allocation to be produced. Suppose that each agent i has the perfect information about the mechanism and the parameters of others(xi, vi). The most adversarial misreport occurs when it maximizes its own utility under mechanism f, i.e.,it would select (vi, xi):",
  "(vi, xi) = argmaxvi,xiui(f((vi, vi), (xi, xi), b), v, x).(6)": "Let yi = fi((vi, vi), (xi, xi), b) Rm denote the allocation to agent i under the optimal misreport. The goalof agent i when misreporting is to obtain allocation yi. To ensure robustness to misreporting, the mechanismf should avoid allocating yi to agent i, which could be enforced as a constraint:",
  "ai, yi , i(7)": "for some constant > 0. With selected sufficiently small, alignment between the allocation decision aiand the direction yi desired by the untruthful agent i is penalized by the inner-product constraint, whichprevents allocating ai = yi. In other words, the constraint (7) ensures that the allocation to an untruthfulagent deviates from what it most desires, hence reducing the incentive to misreport. This line of reasoning isin the spirit of robust control (Zhou & Doyle, 1998; Borkar, 2002).",
  "i=1ai,m bm, m(8)": "where i is the optimal dual variable associated with the inner-product constraint (7) for each i. Unfortunately,y is difficult to compute as it encodes a notion of self-consistency y defines the mechanism (7) but issimultaneously the output of (7) under specific misreports. Due to difficulty of practical evaluation, weinstead approximate y from data. We propose substituting i yi by a parameterized function approximator. To be more precise, since thevector yi can be a function of values, demands, and budgets, we fit z : V D B RNM such that[z(v, x, b)]i tracks i yi under reported values v, demands x, and budgets b. In this work, we take the functionapproximator to be a feed-forward neural network w represents the set of weights and biases while zwrepresents the neural network parameterized by w as a mapping from the input values, demands, and budgetsto an allocation outcome. Here [z(v, x, b)]i denotes the evaluation of the neural network restricted to thecomponent associated with the allocation of agent i. Substituting z into (8) leads to our first proposedapproach, RPF-Net, whose associated mechanism [cf. Definition 1] is detailed below.",
  "Training RPF-Net": "While we would like to solve the objective (5) over distribution F, in practice, we often may not have access toF, but instead a training set of values, demand, and budgets {(vl, xl, bl) F}Ll=1. We train the mechanismswith the finite dataset via empirical risk minimization (ERM) by forming the sample-averaged estimates ofthe expected NSW and exploitability.",
  "l=1 expli(f, vl, xl, bl) , i": "The sample-averaged performance of a mechanism approaches its expectation as the number of collectedsamples increases the gap between them is known as generalization error (Shalev-Shwartz & Ben-David,2014). In Sec. 5, we bound the generalization error by a sublinear function of batch size L for both proposedmechanisms. It is important to note that the learning objective does not require paired samples of (vl, xl, bl) and theground-truth optimal allocation under (vl, xl, bl). We only need samples of valuations, demands, and budgetsto learn the optimal parameter w in an unsupervised manner. To train the neural network, we optimize (10) with respect to by using a simple primal-dual gradientdescent-ascent algorithm to find the saddle point of the Lagrangian. We present the training scheme inAlg. 1, where + denotes the projection of a scalar to the non-negative range. The dual variable i R+ isassociated with the ith exploitability constraint in (10).",
  "f RP F. Computing such gradients is a challenging task, as the mapping from z to the": "solution of (9) is defined implicitly through the optimization program. In the following section, we developthe technique that allows[k]z[k] to be computed (relatively) efficiently. But still, the gradient computationrequires inverting a square matrix of dimension (3NM + M) (3NM + M). Both forward and backwardoperations can be increasingly expensive as the number of agents and resources scales up, which is a fact thatwe have to point out that may limit the scalability of RPF-Net in systems with computational constraints.Detailed complexity evaluation of RPF-Net is given in .4. Remark 1. We note that our work measures and optimizes exploitability only empirically and cannotguarantee strict exploitability constraint satisfaction, at least for two reasons. First, we cannot guaranteethat gradient descent applied to the right hand side of Eq. (3) with respect to vi, xi finds the global maximizer,especially when the mechanism is a complicated mapping encoded by a neural network. Second, while weaim to optimize the objective (10), we cannot guarantee that the parameter obtained from training is afeasible solution. If the desired exploitability threshold is chosen too small, we sometimes observe constraintviolation.",
  "Two main technical challenges are present at this point in training the proposed mechanism and evaluatingits performance against benchmarks": "1. It is unclear how we may systematically calculate the exploitability of the PF mechanism, whichis the fundamental baseline for any IC mechanism design. With a small number of resources, theexploitability can possibly be computed by exhaustively searching in the valuation and demandspace, which we performed to generate . However, as the system dimension scales up, anexhaustive parameter search quickly becomes computationally intractable. The main difficulty incomputing expli(f P F , v, x) lies in finding the optimal misreport (vi, xi) as the solution to (6). It maybe tempting to solve (6) with gradient ascent. However, as the mapping f P F is implicitly definedthrough an optimization problem (4), it is unclear how f P F",
  "f RP F(v,x,b) where is a downstream loss function calculated fromf RP F(v, x, b)": "We address both challenges in this section, by adapting and extending the techniques from differentiableconvex programming (Amos & Kolter, 2017; Agrawal et al., 2019). As an important contribution of thework, we characterize the (sub)differentiability of f P F (.1) and develop an efficient method forcalculating the (sub)gradients viui, xiui, wui of (6) (.2). This allows iterative (sub)gradientascent to be performed on the utility function ui to find a (locally) optimal solution of (6). This solution canthen be leveraged to evaluate the exploitability of the PF mechanism according to (3). Building on a similartechnique, we present an efficient method for differentiating through RPF-Net in .3. The main property of f P F that we exploit to drive this innovation is the preservation of the KKT system of(4) at the optimal solution under differential changes to values and demands. We now present the detailedtechnical development.",
  "m(Da b)m = 0,m,(11d)": "where , , are the optimal dual solutions associated with constraints a 0, a x, and Da b,respectively. As a satisfies vi ai > 0 for all i (any allocation that makes vi ai = 0 for any i blows up theobjective of (4) to negative infinity and thus cannot be optimal), (11a) can be re-written as",
  "(i i )vi ai + wivi = 0.(12)": "For (11) to hold when values, demands, and/or weights changes from v, x, w to v + dv, x + dx, w + dw, theoptimal primal and dual solutions need to adapt accordingly. We can describe the adaptation through thefollowing system of equations on the differentials, which we obtain by differentiating (12) and (11b)-(11d).",
  "are in the sub-differential of a at v1,1": "Theorem 1. Suppose that strict complementary slackness holds at solution (a, , , ) and that thedemands are non-zero. When at least NM N inequality constraints in (4) hold as equalities at a, thematrix M is invertible, and the mapping from (v, x) to a is differentiable. Otherwise, the mapping is onlysub-differentiable. Theorem 1 provides a sufficient condition for the differentiability of f P F , and connects the differentiability tothe number of tight constraints. The proof is presented in Appendix B. Similar characterizations have beenestablished for quadratic programs and disciplined parameterized programs (Amos & Kolter, 2017; Agrawalet al., 2019). However, (4) does not fall (and cannot be re-formulated to fall) under either category. Hence,Theorem 1 generalizes these works to a broader category of convex programs; see Appendix E for furtherdiscussions.",
  "x , a": "w with an identical approach) by repeatedly solving (14) forthe unit change in each entry of v. However, doing so would require solving a large system of equationsfor every partial derivative or inverting M and storing its inverse, which is a costly operation. Fortunately,the following proposition shows that if a((vi, vi), (xi, xi), b) is used downstream to compute the utilityui(a((vi, vi), (xi, xi), b), v, x) in (6), we can back-propagate the gradient aui through the PF mechanismto obtain viui, xiui, and wui by pre-computing and storing a matrix-vector product.",
  "Theoretical Guarantees": "In this section, we establish the convergence of the learned mechanism. First, we bound the sub-samplingerror by a sublinear function of the batch size, which matches recent rates for auctions (Dtting et al.,2023), and ensures the objective of (10) converges to (5) with enough data. Next, we show that the learnedmechanism is robust under distribution mismatch, i.e., when we train (5) on a distribution F and evaluateon distinct distribution F, the learned mechanism achieves low exploitability with performance determinedby distributional distance between F and F .",
  "Robustness to Distribution Mismatch": "Let (F) denote an optimizer of (10) under samples {(vl, xl, bl)}l drawn from the distribution F. Next, weestablish that the performance of f(F ) on samples from distribution F when trained on samples from adifferent distribution F in terms of the worst-case exploitability is controlled by the degree of mismatch.",
  "Experiments": "We numerically evaluate RPF-Net i) as the number of agents and resources changes; and ii) when themechanisms are tested on a distribution different from that observed during training. We also visualize thedecision boundary of RPF-Net in contrast to the PF mechanism to provide more insight on how RPF-Netdeviates from the PF mechanism which it is designed to approximate and enhance. The final subsectiondiscusses the computational time required for training and performing inference with RPF-Net.",
  "Mm=1 bm": "Efficiency quantifies the averaged utilization of resources. Mechanisms should preferably have a high efficiency,although efficiency is merely a byproduct of NSW in the training objective. As the total demand for aresource may be smaller than the budget, the maximum efficiency can be smaller than 1. The PF mechanismis fully efficient (i.e., all available resources are allocated if the demand is sufficiently large), and hence servesas a benchmark. Baselines: Besides the PF mechanism introduced in (4), we evaluate against the PA mechanism (Cole et al.,2013), denoted by f P A, which achieves state-of-the-art NSW among IC mechanisms. A probabilistic mixtureof PA and PF provides a strong trade-off between NSW and exploitability. Given , we consider anew mechanism f mixture:",
  "Another important baseline is the ExS-Net mechanism proposed in Zeng et al. (2024) which shares the sametraining objective as RPF-Net and is different in the neural network activation function": "We also compare against a standard neural network, which has the same architecture as RPF-Net and ExS-Netexcept that the activation function of its last layer is a standard softmax function and is trained on thesame objective for the same amount of iterations. Any performance gain of RPF-Net over ExS-Net and thisstandard network can be attributed to the architectural innovation. Data Generation: In all experiments, the true values and demands follow uniform and Bernoulli uniformdistributions, respectively, within the range [0.1, 1]. Specifically, we generate the test samples according to",
  "Varying System Parameters": "We test the proposed mechanism as the problem dimension varies. We start with two small-scale problemswith 2 agents, 2 resources, and 10 agents, 3 resources. The 2x2 system is the smallest non-trivial case, andthe 10x3 system is the largest considered in a line of recent works (Dtting et al., 2023; Ivanov et al., 2022) onauction design with payment. All reported numbers are normalized by that of the PF mechanism. As shownin , RPF-Net achieves an advantageous trade-off between PF and PA: it consistently reduces theexploitability of PF by over at least 80% while achieving similar NSW. Compared with PA, RPF-Net improvesthe efficiency and NSW. For larger numbers of agents, the NSW of PA mechanism decreases drasticallybeing the product of agents utilities, while RPF-Net still exhibits a favorable NSW. In addition, RPF-Netoutperforms the interpolated mixture of PF and PA and ExS-Net under all three metrics. The performance ofthe standard neural network is unstable (mechanism 4 in orange) while it performs well in the 2x2 system,it fails to achieve a meaningful NSW with 10 agents present.",
  "Distribution Mismatch": "It can be difficult in practical problems to know and sample from the true distribution of values, demands,and budgets in the test set. In this section, we show that RPF-Net is still effective when the distribution onwhich they are expected to perform is slightly different from the training distribution. Such distributionmismatch may occur due to measurement error or more pernicious sources such as strategically misrepresentingpreferences. Recall that F denotes the true distribution of utility function parameters and F the distribution of thetraining samples. We denote by (F ) the optimal solution to (10) under samples {(vl, xl, bl) F }.The performance of (F ) under the true distribution F, measured by EF [NSW((F ), v, x, b)] andEF [expl((F ), v, x, b)], can in theory be sub-optimal by a factor of dT V (F, F ), which is non-ideal under alarge discrepancy between F and F . Nonetheless, empirically we observe robustness: we study two sources ofdistribution mismatch and see in the experiments that EF [NSW((F ), v, x, b)] and EF [expl((F ), v, x, b)]closely match EF [NSW((F), v, x, b)] and EF [expl((F), v, x, b)].",
  "(18)": "Here [][a,b] for scalar a, b denotes the element-wise projection to the interval [a, b]. In other words, F has aperturbed uniform distribution. We would like to draw the perturbation from a heavy-tailed distributionto increase the likelihood of generating extreme values. The Cauchy distribution has been used to explainextreme events like Flash Crash (Parker, 2016) and to describe price fluctuations (Casault et al., 2011), whichmotivates our selection.Adversarially generated training samples. Again, we consider a 22 system where the true distributionF is described in (17). Suppose that the training dataset is composed of historical valuations and demandscollected through past interactions of the agents with the supplier. If the agents believe that the supplierruns the PF mechanism in these past interactions, they may have reported strategically to trick the PF",
  "vli, xli = arg maxvi,xiui(f P F ((vi, vli), (xi, xli), b), vl, xl).(19)": "In and we present the NSW, exploitability, and efficiency of RPF-Net trained undercontaminated samples (18) and (19). We observe that under contaminated training samples the metricsof RPF-Net closely track those under truthful training data. Notably, the NSW and efficiency are almostunaffected, while the exploitability only mildly increases. The proposed mechanism still achieves near-optimalNSW and efficiency while maintaining a low exploitability (not exceeding 20% of that of the PF mechanism).This demonstrates the robustness of the proposed mechanism to mismatch between training and inferencedistribution and supports the theoretical results in Sec. 5.2.",
  "Complexity of Proposed Mechanism in Training and Inference Phase": "In this section, we provide more light on the amount of computation required to train RPF-Net and use itfor inference. For this purpose, it is worth comparing with the existing mechanism ExS-Net proposed inZeng et al. (2024) which considers the same training objective as ours but parameterizes the mechanism witha much simpler regular neural network architecture. The main difference between RPF-Net and ExS-Net(Zeng et al., 2024) lies in the activation function. In the case of ExS-Net, the activation involves standardscalar/vector operation and a softmax function. Both forward and backward passes through the activationcan be completed in time O(NM), which means that ExS-Net is fast in both training and inference phases.For RPF-Net, forward pass requires solving the convex optimization program (9). We use an interior-pointsolver for this program, which is guaranteed to converge within polynomial time, i.e., the time to obtain ansolution up to precision is no more than some polynomial function of , N, and M. However, the exactcomplexity is unknown but should be expected to be worse than O((NM)3) (Renegar, 1988) (as O((NM)3)is the time it would take for the interior-point method to converge if (9) were a linear program). In thebackward pass, RPF-Net needs to invert solve a system of equations of dimension (3NM + M) (3NM + M),which requires O((NM)3) computation. To summarize, we organize the complexity results in . We also show in the training and inference time on a ten-agent three-resource allocation problem. Notethat both PF and PA are hand-designed mechanisms that do not require training, but are time-consumingduring inference since they solve optimization programs. The amount of computation required by RPF-Netduring inference is on the same order as that of PF and PA mechanisms.",
  "Conclusion": "This paper studied the important problem of fair and incentive compatible resource allocation withoutthe use of monetary payment. Identifying that hand-designing mechanisms that achieve the exact IC andmaximum NSW is impossible, we considered learning an approximate mechanism that desirably trades offthe two competing objectives. As a key contribution, we innovated the neural network architecture used toparameterize the mechanism we proposed RPF-Net, a learned variant of the standard PF mechanism withan optimization-based activation function in the output layer. We established a way of differentiating throughthe activation function to enable efficient training. On the theoretical side, we showed that the proposedmechanism is consistent (learns better with more training data) and is robust to distributional changes in thedata, two properties that make the mechanism useful in practice. We showed through numerical simulationsthat RPF-Net outperforms the existing methods on a range of evaluation criteria. In particular, comparedto the state-of-the-art architecture (Zeng et al., 2024), RPF-Net significantly increases the NSW withoutincurring a higher exploitability, thereby improving the Pareto frontier. It is worth noting the limitation of RPF-Net in its computational complexity. The optimization-basedactivation function requires significant amount of computation to evaluate in the forward direction and to",
  "Disclaimer": "This paper was prepared for informational purposes [in part if the work is collaborative with externalpartners] by the Artificial Intelligence Research group of JPMorgan Chase & Co. and its affiliates (\"JPMorgan) and is not a product of the Research Department of JP Morgan. JP Morgan makes no representationand warranty whatsoever and disclaims all liability, for the completeness, accuracy or reliability of theinformation contained herein. This document is not intended as investment research or investment advice, ora recommendation, offer or solicitation for the purchase or sale of any security, financial instrument, financialproduct or service, or to be used in any way for evaluating the merits of participating in any transaction,and shall not constitute a solicitation under any jurisdiction or to any person, if such solicitation under suchjurisdiction or to such person would be unlawful.",
  "Simina Brnzei, Vasilis Gkatzelis, and Ruta Mehta. Nash social welfare approximation for strategic agents.Operations Research, 70(1):402415, 2022": "Yang Cai, Constantinos Daskalakis, and S Matthew Weinberg. An algorithmic characterization of multi-dimensional mechanisms. In Proceedings of the forty-fourth annual ACM symposium on Theory of computing,pp. 459478, 2012a. Yang Cai, Constantinos Daskalakis, and S Matthew Weinberg. Optimal multi-dimensional mechanism design:Reducing revenue to welfare maximization. In 2012 IEEE 53rd Annual Symposium on Foundations ofComputer Science, pp. 130139. IEEE, 2012b.",
  "Edward H Clarke. Multipart pricing of public goods. Public choice, pp. 1733, 1971": "Richard Cole, Vasilis Gkatzelis, and Gagan Goel. Mechanism design for fair division: allocating divisibleitems without payments. In Proceedings of the fourteenth ACM conference on Electronic commerce, pp.251268, 2013. Michael J Curry, Uro Lyi, Tom Goldstein, and John P Dickerson. Learning revenue-maximizing auctions withdifferentiable matching. In International Conference on Artificial Intelligence and Statistics, pp. 60626073.PMLR, 2022.",
  "APartial Allocation Mechanism": "The partial allocation mechanism, proposed in Cole et al. (2013), is built upon the PF mechanism andwithholds resources according to an externality ratio. Specifically, given reported valuations v RNM anddemands d RNM across all agents and resources and budgets b RM, let a f P F (v, x, b) denote theallocation made by the PF mechanism.",
  "= h(e1, 0, 0)M (aui), 0, 0, 0": "The important observation is that the computation of the gradient with respect to any entry in vi (and alsoxi and w) uses the product of M and [ (aui), 0, 0, 0 ], and only the vector h is different. Takingadvantage of this fact, it is not hard to see (following a line of analysis similar to derivation of Eq. (8) inAmos & Kolter (2017)) that once we solve",
  "DDerivation of Gradients of Exploitability-Averse Proportional Fairness Mechanism": "With values v and demands x, let a denote the solution of (9), i.e., a = f RP F(v, x, b). The aim of thissection is to derive the (sub)gradient of a loss function with respect to z, v, x, and w when we are givena. The arguments used here are mostly straightforward extension of the result in Sec. 4.",
  "EProportional Fairness Mechanism and Discipline Parameterized Program": "Discipline parameterized programs (DPP) are a special class of convex programs introduced in Agrawal et al.(2019), in which the authors show how (sub)gradients can be derived through the mapping from parametersof a DPP to its optimal solution. We start the discussion by introducing the DPP, which is an optimizationprogram of the form",
  "s.t.fi(y, ) 0,i = 1, . . . , mineq": "where y Rn1 is the decision variable, Rn2 is the parameter which we need to derive the gradient for,and the functions fi are convex. In addition, all functions are required to be affine in a special sense. Anexpression is said to be parameter-affine if it is affine in affine in the parameter and variable-free (doesnot involve variable x). An expression is said to be parameter-free if it does not involve any parameter (itcan involve the variable x). Any expression prod(z1, z2) = z1z2 as the product of z1 R and z2 Rp for anyp is affine if at least one of the two conditions is true:",
  "among y1 and y2, one is parameter-affine and the other is parameter-free": "When we try to capture (4) by a DPP, the decision variable y corresponds to allocation a, and the parameter abstracts (v, x, b, w). As part of the objective of (4), ai vi can be verified to be affine since ai is parameter-freeand vi is parameter-affine. However, the overall objective Ni=1 wi log(ai vi) is not affine as wi is notparameter-free and log(ai vi) is not variable-free, parameter-affine or parameter-free. While sometimesre-formulation can be made to transform a non-DPP to an equivalent DPP, such re-formulation is not possiblein this case.",
  "F.1Definitions & Preliminaries": "Let F denote a class of bounded functions f : Z [c, c] defined on an input space Z for some c > 0. Let Dbe a distribution over Z and let S = {z1, z2, , zL} be a sample drawn i.i.d from some distribution D overinput space Z.Definition 4. The capacity of the function class F measured in terms of empirical Rademacher complexityon sample S is defined as",
  "Lemma F.3 ((Dtting et al., 2023)). For any s, s RNM, the activation function for a softmax layer is1Lipschitz, i.e.,(s) (s)1 s s": "Lemma F.4 ((Dtting et al., 2023)). Let Fk be a class of feed-forward neural networks that maps an inputvector y Rd0 to an output vector Rdk, with each layer l containing Tl nodes and computing z l(wlz)and l : RTl Tl. Further, for each network in Fk, let the parameter matrices wl1 W andl(s) l(s) s s1 for any s, s RTl1. The covering number of the network is",
  "d,": "where T = maxl[k] Tl and d is the total number of parameters in the network.Corollary F.4.1. For the same configuration as in Lemma F.4, but having l(s) l(s) s s1 forall hidden layer activation functions (l < k), and having o(s) o(s) s s1 for the output layer,the covering number is given as",
  "The result follows from Lemma F.2 and similar arguments as in Proposition 2": "Note that the activation functions on all hidden layers are ReLU functions, which are 1-Lipschitz. The outputlayer of the regularized PF activation is Lipschitz from Assumption 1 with some constant > 0. Using the definitions of s with the activation having a Lipschitz constant of , and the hidden layers havinga Lipschitz constant of 1, and with d = max{K, MN} in Corollary F.4.1, we have from Proposition 2 andProposition 3 that the following holds with probability at least 1"
}