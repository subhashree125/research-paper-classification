{
  "Abstract": "In recent times machine learning methods have made significant advances in becoming auseful tool for analyzing physical systems. A particularly active area in this theme has beenphysics-informed machine learning which focuses on using neural nets for numericallysolving differential equations. In this work, we aim to advance the theory of measuring out-of-sample error while training DeepONets which is among the most versatile ways to solveP.D.E systems in one-shot. Firstly, for a class of DeepONets, we prove a bound on theirRademacher complexity which does not explicitly scale with the width of the nets involved.Secondly, we use this to show how the Huber loss can be chosen so that for these DeepONetclasses generalization error bounds can be obtained that have no explicit dependence on thesize of the nets. The effective capacity measure for DeepONets that we thus derive is alsoshown to correlate with the behavior of generalization error in experiments.",
  "Introduction": "Deep learning has recently emerged as a competitive way to solve partial differential equations (P.D.Es)numerically. We note that the idea of using nets to solve P.D.Es dates back many decades Lagaris et al.(1998) Broomhead & Lowe (1988). In recent times this idea has gained significant momentum and AI forScience Karniadakis et al. (2021) has emerged as a distinctive direction of research. Some of the methodsat play for solving P.D.Es neurally E et al. (2021) are the Physics Informed Neural Networks (PINNs)paradigm Raissi et al. (2019) Lawal et al. (2022), Deep Ritz Method (DRM) Yu et al. (2018), DeepGalerkin Method (DGM) Sirignano & Spiliopoulos (2018) and many further variations that have beendeveloped of these ideas, Kaiser et al. (2021); Erichson et al. (2019); Wandel et al. (2021); Li et al. (2022);Salvi et al. (2022). These different data-driven methods of solving the P.D.Es can broadly be classified into two kinds, (1) oneswhich train a single neural net to solve a specific P.D.E. and (2) operator methods which train multiplenets in tandem to be able to solve in one shot a family of differential equations, with a fixed differential",
  "Published in Transactions on Machine Learning Research (11/2024)": "Similar to the experiments in .3, in this section too we use DeepONets whose both branch andtrunk networks are of depth 3 and have a width of 100. The selected training loss is identical to the onein Equation 11, with set to the Huber loss.The generalization error is plotted for various values ofdelta at Ptraining = 200 and 1500, while maintaining Ntraining = 400. In this setup, in we plot thegeneralization error for =1",
  "As an explicit example of using DeepONet in the supervised setup, consider solving a pendulum O.D.E.,d(y,v)": "dt= (v,k sin(y) + f(t)) R2 for different forcing functions f(t). Then using sample measurements ofvalid (f,y) tuples, a DeepONet setup can be trained only once, and then repeatedly be used for inferenceon new forcing functions f to estimate their corresponding solutions y. This approach is fundamentallyunlike traditional numerical methods where one needs to run the optimization algorithm afresh for everynew source function. In a recent study Lu et al. (2022), the authors showed how the DeepONet setup thatwe focus on in this work has significant advantages over other competing neural operators, like the FNOLi et al. (2020), in solving various differential equations of popular industrial use. As a testament to the foundational nature of the idea, we note that over the last couple of years, severalvariants of DeepONets have been introduced, like, Tripura & Chakraborty (2023); Goswami et al. (2022a);Zhang et al. (2022); Hadorn (2022-03-16); Park et al. (2023); de Sousa Almeida et al. (2023); Tan & Chen(2022); Xu et al. (2023); Lin et al. (2023). Different implementations of neural operators have been demon-strated to be useful for various scientific applications, like for predicting crack shapes in brittle materialsGoswami et al. (2022b), for fluid flow in porous materials Choubineh et al. (2023), for simulating plasmasGopakumar & Samaddar (2020), for seismic wave propagation Lehmann et al. (2023), for weather modelingKurth et al. (2023) etc. In light of this burgeoning number of applications, we posit that it becomes critical that the out-of-sampleperformance of neural operators be mathematically understood. Towards this goal, in this work, we provethat the generalization error of one of the most versatile forms of operator learning i.e DeepONets, canbecome independent of the number of training parameters. We note that this is a significant improvementover the best known generalization bounds for DeepONets, Theorem 5.3 in Lanthaler et al. (2022), whichgrow exponentially in the number of parameters.We are able to obtain this insight via analyzing theRademacher complexity of DeepONets. That we can recover here the usual neural net like generalizationbound of the form a capacity measuresamplesizeis particularly interesting because in here the predictor, the DeepONet, isa non-Lipschitz function, while nets are Lipschitz functions and that is critical to how the standard analysisis carried out for obtaining generalization error bounds. Motivations for Understanding Rademacher ComplexityWe recall that typically there is a vast lackof information about the distribution of the obtained models in any stochastic training setup. Correspondingto a stochastic training algorithm, say ALG, that is using a m sized dataset Sm to search over a class ofpredictors H, there is scarce mathematical control possible over the distribution of its output random variable the random predictor thus obtained, say ALG(Sm,H). Hence there is an almost insurmountable challengein being able to control the quantity we would ideally like to bound, the out-of-sample risk of the obtainedpredictor i.e. R ( ALG(Sm,H)) - where R denotes the expectation w.r.t the true data distribution, sayD, of a loss evaluated on the predictor ALG(Sm,H). The fundamental idea that takes us beyond this conundrum is to relax our goals from aiming to controlthe above to aiming to control only the data averaged worst (over all possible predictors) generalizationgap between the population and the empirical risk i.e, ESmDm [suphH ( Rm( h) R( h))] wherecorresponding to a choice of loss , Rm( h) is the empirical estimate over the data sample Sm of therisk of h and R( h) is the population risk of h over the data distribution D. The importance of thestatistical quantity, Rademacher complexity Bartlett & Mendelson (2001) of the function class H i.e.Ez1,...,zmDm (E [suphH1m mi=1 i h(zi)]) (where i 1 are Bernoulli random variables) stems from thefact that it is what bounds this aforementioned measure of the generalization gap. It is to be noted that trying to control the above frees us from the specifics of any particular M.L. trainingalgorithm being used (of which there is a myriad of heuristically good options) to find arginfhHR( h).But on the other hand, by analyzing the Rademacher complexity we gain insight into how the choice of the",
  "conclude that while training with n ( 2C(H,D)": ")2samples the data-averaged worst-case generalization errorfor the predictor obtained is at most , for any arbitrarily small > 0. This motivates one of the uses ofRademacher complexity i.e to provide such estimates on the number of samples required to achieve a targettest accuracy. In particular, in cases where this capacity does not scale with the number of parameters in thehypothesis class H, one may begin to explain why certain over-parameterized models might be good spacesfor the learning task. Such understanding becomes immensely helpful when H is very complicated, as is thefocus here, that of H being made of DeepONets. In , we review some of the recent advances in theRademacher analysis of neural networks. We crucially note (Theorem 4.13, Ma (2021)) that any condition on m,,H and D that makes Rademachercomplexity small is a condition which when true it becomes reasonable to expect that the empirical and thepopulation risk could also be close for any predictor in the class H. In this work, we will compute the Rademacher complexity of appropriate classes of DeepONets and use it togive the first-of-its-kind bound on their generalization error which does not explicitly scale with the numberof parameters. Generalization bounds that do not scale with the size of the nets can be seen as a step towardsexplaining the success of overparameterized architectures for that learning task. Further, our experimentswill demonstrate that the complexity measures of DeepONets as found by our Rademacher analysis indeedcorrelate to the true generalization gap, over varying sizes of the training data.",
  ": A Sketch of the DeepONet ( DON) Architecture": "In the above diagram, the Branch Network and the Trunk Network are neural nets with a common outputdimension.xB(f) Rd1, the input to the branch net is an encoded version of a function f i.e.adiscretization of f onto a d1sized grid of sensor points in its input domain. xT Rd2 is the trunk input.If the activation is at all layers and the branch net and the trunk nets layers are named Bk,k = 1,2,...,qBand Tk,k = 1,2,...,qT respectively, then the above architecture implements the map,",
  "(1)": "In above the number of rows of the matrix BqB and TqT need to be the same for the inner-product to bedefined. For a concrete example of using the above architecture, consider the task of solving the pendulumO.D.E from the previous section, d(y,v) dt= (v,k sin(y) + f(t)) R2. For a fixed initial condition, here thetraining/test data sets would be 3tuples of the form, (xB(f),xT ,y) where y R is the angular position ofthe pendulum at time t = for the forcing function f. Typically y is a standard O.D.E. solvers approximate",
  "mi=1(G(fi,i) DeepONet(xB(fi),i))2 .(2)": "In above G is the solution operator for this O.D.E. that the O.D.E. solver can be imagined to be simulating.The rationale for this loss function above originates from the universal approximation property of DeepONetswhich we have reviewed as Theorem C.1. Going beyond such approximation theorems, we state the resultsthat we prove here about the risk bounds in this novel learning setup. Theorem (Informal Statement of Theorem 4.1). Consider a class of DeepONets, with absolute value acti-vation, whose branch and trunk networks are both of depth n (i.e. qB = qT = n in equation 1) and supposethat the squared norms of the inputs to them are bounded in expectation by Mx,B and Mx,T respectively.Then the average Rademacher complexity, for training with samples of size m, is bounded by,",
  "tkj2=1(vw)j1,j2 Bnk,j1Tnk,j2 = Ck,kk = 2,...,n 1,(4)": "where in each sum, above v,w are on the unit spheres of the same dimensionality as the number of rowsin Bnk and Tnk respectively and bk,tk represent the number of rows in the weight matrices Bnk andTnk respectively. (For any branch weight matrix say Bp, in above we have denoted its j-th row as Bp,j andsimilarly for the trunk.)",
  "where Cn,n1 and Cj,j are the constants defined in Theorem": "To the best of our knowledge, there is no general principle which determines when the worst case general-ization gap of a learning scenario scales likeCm for some capacity function C which is entirely determinedby the data distribution and the norms of the parameters allowed in the predictor class. We recall that this",
  "In light of the above, we note the following practical applicability and qualitative significance of our boundson the Rademacher complexity and generalization error of DeepONets": "Choosing the Loss FunctionTheorem 4.2 suggests that training on the Huber loss function could leadto better generalization. We give a performance demonstration in Appendix A for Huber loss function at =1 2n1 for DeepONets using tanh activation (although our above theorem does not capture this choice ofactivation) and we note that it gives 2 orders of magnitude lower test error than for the same experimentusing 2 loss. Additionally, note that, DeepONets without biases and with positive homogeneous activation, are invariantto the scaling of the branch nets layers, Bk,and trunk nets layers, Tk, by any k and k > 0 respectivelyk s.t k(k k) = 1. This is a larger symmetry than for usual nets but our complexity measure mentionedabove is also invariant under this combined scaling. This can be seen as being strongly suggestive of ourresult being a step in the right direction. Explaining OverparameterizationSince our generalization error bound has no explicit dependence onthe width or the depth (or the number of parameters), it constitutes a step towards explaining the benefitsof overparameterization in this new setup.",
  "Related Works": "Over the past few years, many novel generalization bounds for standard neural nets have been established.Many of these works have computed bounds on Rademacher complexity for various classes of nets, toshow how different norm combinations of the involved weight matrices affect generalization performance,Sellke (2024), Golowich et al. (2018), Bartlett et al. (2017), Neyshabur et al. (2015). For shallow neuralnets such methods have also been useful in explaining the benefits of dropout Arora et al. (2021) andoverparameterization Neyshabur et al. (2019). However, for state-of-the-art neural nets, other approachesDziugaite & Roy (2017), Arora et al. (2018), Neyshabur et al. (2018), Mukherjee (2021) have sometimesgiven tighter bounds while using bounding expressions that are computationally very expensive. A thoroughdiscussion of the reach and limitations of these various bounds can be found in Nagarajan (2021). In Kumar & Mukherjee (2024), the authors investigate the behavior of the 2-distance between the truesolution and the PINN surrogate, building on the work of Mishra & Molinaro (2023). While the studyestablishes an upper bound on the 2-distance, this result alone is insufficient to guarantee successful training.For training to be effective, it is also necessary to demonstrate that the empirical risk converges to thepopulation risk, which can be ensured using Rademacher complexity-based bounds.",
  "Comparison to Lanthaler et al. (2022)": "Firstly, we note that none of the theorems presented here depend on the results in Lanthaler et al. (2022).The proof strategies are entirely independent.Secondly, to the best of our knowledge, Theorem 5.3 in Lanthaler et al. (2022) is the only existing result on generalization error bounds for DeepONets. However,their bound has an explicit dependence on the total number of parameters (the parameter d there) in theDeepONet. Such a bound is not expected to explain the benefits of overparameterization, which is one ofthe key features of modern deep learning (Dar et al., 2021; Yang et al., 2020). We note that for usual implementations for DeepONets, where depths are typically small and the layersare wide, for a class of DeepONets at any fixed value of our complexity measure i.e. Cn,n1 (n1j=2 Cj,j), ageneralization error bound based on our Rademacher complexity bound in Theorem 4.1 will be smaller thanthe one in Lanthaler et al. (2022) that scales with the total number of parameters.",
  "Further, as pointed out in our second main result, Theorem 4.2, our Rademacher bound lends itself to anentirely size-independent generalization bound when the loss is chosen as a certain Huber loss": "NotationGiven any U Rn, denote as L2(U) the set of all functions f U R s.t U f 2(x)d(x) < where is the standard Lebesgue measure on Rn.And we denote as C(U), the set of all real valuedcontinuous functions on U. The unit sphere in Rk is denoted by Sk1 = {x Rk x2 = 1}. For any matrix",
  "P (p(x))Q(q(y)) P (p(x))Q(q(y)) Lp(x)q(y) p(x)q(y).(7)": "Our main results work under the above assumption and hence in particular, our results apply to the absolutevalue map, R x x R, being the activation function in both the branch and the trunk net. In AppendixI we will indicate why this is sufficient to also capture DeepONets with ReLU activations and being trainedon bounded data.",
  "Corresponding to a choice of any univariate loss function (), like Huber loss as above, we define the DeepONettraining loss as follows": "Definition 4 (A Loss Function for DeepONets). Given D Rd, a compact set with boundary, we definea function class of allowed forcing functions F C(D). Further, we consider DeepONet maps as given inDefinition 1, mapping as DeepONet Rd1 Rd2 R and consider an instance of the training data given as,",
  ", the squared loss and if we assume that the numerical solver exactlysolved the forced pendulum O.D.E": "Next, we recall the definition of a key statistical quantity, Rademacher complexity, from Bartlett & Mendelson(2001), which we focus on as our chosen way to measure generalization error for DeepONets. Definition5(Empirical and Average Rademacher complexity). For a function class K, supposebeing given a msized data-set of points {xi i = 1,...,m} in the domain of the elements in K.For i 1 with equal probability, the corresponding empirical Rademacher complexity is given byRm(K) = E [supkK1m mi=1 ik(xi)]. If the elements of this data-set above are sampled from a distributionP, then the average Rademacher complexity is given by,",
  "mi=1ik(xi)])(9)": "The crux of our mathematical analysis will be to uncover a recursive structure between the Rademachercomplexity of DeepONets and certain DeepONets of one depth lower in the branch and the trunk network,and which would always have one-dimensional outputs for the branch and the trunk and which share weightswith the original DeepONet in the corresponding layers.",
  "Results": "Our central result about Rademacher complexity of DeepONets will be stated in Theorem 4.1 of .1and consequent to that our result about bounding the generalization error of DeepONets will be stated inTheorem 4.2 of .2. In .3, we present an empirical study of our bound on Rademachercomplexity for a DeepONet trained to solve the Burgers P.D.E.",
  "First Main Result : Rademacher Complexity of DeepONets": "In the result below we will see how for a certain class of activations, we can get a bound on the Rademachercomplexity of DeepONets which does not scale with the width of the nets for the branch and the trunk netsbeing of arbitrary equal depths. Theorem 4.1 (Rademacher Complexity of Special Symmetric DeepONets). We consider a specialcase of DeepONets as given in Definition 1, with (a) qB = qT = n, and (b) 1,2 satisfies Assumption 1 forsome constant L > 0, and they are positively homogeneous. Further, let bi and ti be the number of rowsof the weight matrices Bni and Tni respectively, as in Definition 2, for i 1 and recall from Definition 1that p is the number of rows of Bn and Tn. Then given a class of DeepONet maps as above, we define thefollowing n 1 constants, Cn,n1 > 0 and Ck,k > 0, k = 2,...,n1, such that for Sk = Sbk1 Stk1, all theDeepONet maps in the class satisfy the following bounds,",
  "Second Main Result : Size-Independent Generalization Error Bound for DeepONets Trained via aHuber Loss on Unbounded Data": "Theorem 4.2 (Generalization Error Bound for DeepONet). We continue with the setup of the DeepONetsas in Theorem 4.1 but with 1(x) = 2(x) = x. For an operator G C(D) Hs(U) where Hs(U) is theL2-based Sobolev space for some s > 0, a class of forcing functions F C(D) and W a set of possible weightsw for DeepONets (denoted as DeepONetw), we define the function class H as,",
  "qk=1NB,k(ui(x1),ui(x2),...,ui(xm)) NT,k(xi,j,ti,j))(11)": "whereby in our experiments is either chosen as the 2 loss or the Huber loss and denotes the vector of alltrainable parameters over both the nets. Similarly as above the test loss LTest would be defined correspondingto a random sampling of Ntest input functions and Ptest points in the domain of the P.D.E. where the truesolution is known corresponding to each of these test input functions. For our experiments, we fixed Ntrainingto 400 and Ptraining is chosen to be the following set of values {300,400,500,750,1000,1500,2000}.",
  "arbitrarily chosen constants. This way of sampling ensures that the functions chosen are all 2-periodic andwill have zero mean in the interval - as required in our setup": "In this section, we demonstrate our Rademacher complexity bound by training DeepONets on the above lossfunction and measuring the generalization gap of the trained DeepONet obtained and computing for thistrained DeepONet an upperbound on the essential part of our Rademacher bound (i.e. Cn,n1m(n1j=2 Cj,j) where Ck,k is as defined in equation 5). Then we vary the number of training data (i.e. Ntraining Ptraining)and show that the two numbers computed above are significantly correlated as this training hyperparameteris varied. We use branch and trunk nets each of which are of depth 3 and width 100.",
  "(a) =0.25, correlation = 0.857(b) =0.4, correlation = 0.791": ": The above plot shows the behaviour of the measured generalization error with respect to C3,2 C2,2mfor training DeepONets, to solve the Burgers PDE, with empirical loss as given in equation 11, specializedto the Huber loss (Definition 3) for the stated values of and for the branch and the trunk nets being ofdepth 3. Each point is labelled by the number of training data used in that experiment.",
  "qk=1NB,k(v) NT,k(y)": "where NB and NT are any two neural nets mapping Rm Rq and R3 Rq respectively - the Branchnet and the Trunk net. In above, q, the common output dimension of the branch and the trunk net is ahyperparameter for the experiment. We have chosen q = 128 and m = 100 for our experiments.",
  "qk=1NB,k(fi(x1,y1),fi(x2,y2),...,fi(xm,ym)) NT,k(xi,j,yi,j,ti,j))(12)": "whereby in our experiments is either chosen as the 2 loss or the Huber loss and denotes the vectorof all trainable parameters over both the nets.Similarly as above the test loss LTest would be definedcorresponding to a random sampling of Ntest input functions and Ptest points in the domain of the P.D.E.where the true solution is known corresponding to each of these test input functions. For our experiments,we fixed Ntraining to 64 and Ptraining is chosen to be the following set of values {26,36,...,76}.",
  "y20)t, where N = 2, Amn = cm dn and x0,y0 = 1. This exactnessleads to a more controlled measure of the test error than in the previous setup": "Here, we demonstrate our Rademacher complexity bound by training DeepONets on the above loss functionand measuring the generalization gap of the trained DeepONet obtained and computing for this trainedDeepONet an upper-bound on the essential part of our Rademacher bound (i.e.Cn,n1m(n1j=2 Cj,j) whereCk,k is as defined in equation 5). Then we vary the number of training data (i.e. Ntraining Ptraining) andshow that the two numbers computed above are significantly correlated as this training hyperparameter isvaried. We use branch and trunk nets each of which are of depth 3 and width 128.",
  "where the size-independentgeneralization bound in Theorem 4.2 clicks for this setup": "Going beyond the ambit of the Rademacher bound proven, in in Appendix M, we present the plotsdemonstrating the solution of the Heat P.D.E. during training with 2 and Huber loss, while allowing forReLU activation and biases in the layers. It can be seen that Huber loss demonstrates noticeably betterperformance compared to 2 loss.",
  "(a) =0.25, correlation = 0.669(b) =0.4, correlation = 0.717": ": The above plot shows the behaviour of the measured generalization error with respect to C3,2 C2,2mfor training DeepONets, to solve the 2D-Heat PDE, with empirical loss as given in equation 12, specializedto the Huber loss (Definition 3) for the stated values of and for the branch and the trunk nets being ofdepth 3. Each point is labelled by the number of training data used in that experiment. Further, in Wang et al. (2021), an unsupervised variation of the loss function of a DeepONet was shownto give better performance. In Goswami et al. (2022b), authors employ a variational framework for solv-ing differential equations using DeepONets, through a novel loss function. Understanding precisely whenthese variations in the loss function used give advantages over the basic DeepONet loss is yet another in-teresting direction for future research - and the path towards such goals could be to explore if the methodsdemonstrated here for computation of generalization bounds can be used for these novel setups too. Also, it would be fruitful to understand how the generalization bounds obtained in this work can be modifiedto cater to the variations of the DeepONet architecture (Kontolati et al., 2023), (Bonev et al., 2023) Lu et al.(2022) that are getting deployed. In the context of understanding the generalization error of Siamese networks, the authors in Dahiya et al.(2021) dealt with a certain product of neural outputs structure (where the nets share weights). In theiranalysis, the authors bound the Rademacher complexity via covering numbers. Since we try to directlybound the Rademacher complexity for DeepONets, it would be interesting to investigate if our methods canbe adapted to improve such results about Siamese nets. Lastly, we note that the existing bounds on the Rademacher complexity of nets have typically been proven bymaking ingenious use of the algebraic identities satisfied by Rademacher complexity (Theorem 12 in Bartlett& Mendelson (2001)). But to the best of our knowledge, we are not aware of any general result on howRademacher complexity of a product of function spaces can be written in terms of individual Rademachercomplexities. We posit that such a mathematical development, if achieved in generality, can be a significantadvance affecting various fields.",
  "Methods": "In this section, we will outline the proofs of the main results, Theorems 4.1 and 4.2. We would like toemphasize that the subsequent propositions 6.2 and 6.3 hold in more generality than Theorem 4.1, becausethey do not need the branch and the trunk nets to be of equal depth. Outline of the Proof TechniquesDerivation of the first main result, Theorem 4.1, involves 3 keysteps: (a) formulating a variation of the standard Talagrand contraction (Lemma 6.1) (b) using this tobound the Rademacher complexity of a class of DeepONets with certain activations (e.g. absolute value)by the Rademacher complexity for a class of DeepONets having one less depth and 1dimensional outputs,for both the branch and the trunk (Lemma 6.2) and lastly (c) uncovering a recursive structure for the",
  "Rademacher complexity across depths between special DeepONet classes having 1dimensional outputs forboth the branch and the trunk. (Lemma 6.3)": "Lemma 6.2 removes the last 4 matrices from the DeepONet (2 each from the branch and the trunk) leadingto one-dimensional output branch and trunk nets. Lemma 6.3 removes 2 matrices (1 each from branch andtrunk) by an entirely different argument than needed in the former. Lemma 6.2 is invoked only once atthe beginning, while Lemma 6.3 is repeatedly used for each remaining layer of the DeepONet.",
  "We note that both our peeling lemmas above are structurally very different from the one in Golowich et al.(2018) - where the last layer of a standard net gets peeled in every step": "Deriving the second main result, Theorem 4.2, involves the following key steps: (a) establishing a relationshipbetween the Rademacher complexity of the loss class and that of the DeepONet when the loss function isLipschitz (Proposition G.6) and (b) using this to upper bound the expectation over data of the supremumof generalization error in terms of the Rademacher complexity of the DeepONet.",
  "Towards proving Theorem 4.1, we need the following lemma which can be seen as a variation of the standardTalagrand contraction lemma,": "Lemma 6.1. Let P ,Q R R be two functions such that Assumption 1 holds and let P and Q be anytwo sets of real-valued functions. Then given any two sets of points {xi i = 1,...,m} and {yi i = 1,...,m}in the domains of the functions in P and Q respectively, we have the following inequality of Rademachercomplexities - where both the sides are being evaluated on this same set of points,",
  "The above lemma has been proven in Appendix H.1": "Towards stating Propositions 6.2 and 6.3, we will need to define certain classes of sub-DeepONets s.t thesesub-DeepONets would be one depth lower in the branch and trunk network, and would always have onedimensional outputs for both the branch and trunk nets and would share weights below that with thecorresponding layers in the original DeepONet. Definition 6. (Classes of sub-DeepONets) Let Wrest be a set of allowed matrices for nets f B and f Tas in Definition 2. Now, given a constant CqB,qT ,qB1,qT 1 > 0 we define the following set of 4tuples ofoutermost layer of matrices in the DeepONet as,",
  "Note that both sides of the above are computed for the same data {(xB,i,xT,i) Rd1 Rd2 i = 1,...,m}.The proof of the above proposition is given in Appendix E": "Referring to the definitions of the DeepONet classes on the L.H.S. and the R.H.S. of the above, as given inequations 14 and 15 respectively, we see that the above lemma upperbounds the Rademacher complexity of aDeepONet class (whose individual nets can have multi-dimensional outputs) by the Rademacher complexityof a simpler DeepONet class. The DeepONet class in the R.H.S. is simpler because the last layer of eachof the individual nets therein is constrained to be a unit vector of appropriate dimensions (and thus theindividual nets here are always of 1 dimensional output) and whose both branch and the trunk are shorterin depth by 1 activation and 1 linear transform than in the L.H.S. Proposition 6.3 (Peeling for DeepONets). We continue in the setup of Proposition 6.2 and definethe functions f B and f T s.t we have the following equalities, f B = 1 (BqB2f B) and f T = 2 (TqT 2f T )Further, given a constant C2,2 > 0, we define Wrest as the union of (a) the set of weights that are allowedin the Wrest set for the matrices BqB3,BqB4,...,B1 and TqT 3,TqT 4,...,T1 and (b) the subset of theweights for BqB2 and TqT 2 that are allowed by Wrest which also additionally satisfy the constraint, (hereSk = Sbk1 Stk1)",
  "where xi = xB,ixT,i and Sn = Sd11 Sd21 and each i 1 uniformly": "Proof of the above lemma is given in Appendix D. In Appendix G we have setup a general framework for usingRademacher bounds as proven in the above theorem to prove generalization error bounds for DeepONets.And we will now invoke the result there to prove our main theorem about generalization error of DeepONets.",
  "Mohammad Altaisan. Approximating solutions of 2d heat equation using deeponets & error estimates ofdeeponets for hlder continuous operators. 2024": "Raman Arora, Peter Bartlett, Poorya Mianjy, and Nathan Srebro. Dropout: Explicit forms and capacitycontrol. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference onMachine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of MachineLearning Research, pp. 351361. PMLR, 2021. URL Sanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi Zhang.Stronger generalization bounds for deepnets via a compression approach. In Jennifer G. Dy and Andreas Krause (eds.), Proceedings of the 35thInternational Conference on Machine Learning, ICML 2018, Stockholmsmssan, Stockholm, Sweden, July10-15, 2018, volume 80 of Proceedings of Machine Learning Research, pp. 254263. PMLR, 2018. URL Peter Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and structuralresults. The Journal of Machine Learning Research, 3:224240, 06 2001. doi: 10.1007/3-540-44581-1_15.",
  "Abouzar Choubineh, Jie Chen, David A Wood, Frans Coenen, and Fei Ma. Fourier neural operator for fluidflow in small-shape 2d simulated porous media dataset. Algorithms, 16(1):24, 2023": "Kunal Dahiya, Ananye Agarwal, Deepak Saini, Gururaj K, Jian Jiao, Amit Singh, Sumeet Agarwal, Pu-rushottam Kar, and Manik Varma. Siamesexml: Siamese networks meet extreme classifiers with 100mlabels. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on Ma-chine Learning, volume 139 of Proceedings of Machine Learning Research, pp. 23302340. PMLR, 1824Jul 2021. URL",
  "Yehuda Dar, Vidya Muthukumar, and Richard G. Baraniuk.A farewell to the bias-variance tradeoff?an overview of the theory of overparameterized machine learning. CoRR, abs/2109.02355, 2021. URL": "Joao Lucas de Sousa Almeida, Pedro Rocha, Allan Carvalho, and Alberto Costa Nogueira Junior. A coupledvariational encoder-decoder-deeponet surrogate model for the rayleigh-bnard convection problem.InAAAI Conference on Artificial Intelligence, 2023. Beichuan Deng, Yeonjong Shin, Lu Lu, Zhongqiang Zhang, and George Em Karniadakis. Approximationrates of deeponets for learning operators arising from advectiondiffusion equations. Neural Networks,153:411426, 2022.ISSN 0893-6080.doi: Gintare Karolina Dziugaite and Daniel M. Roy.Computing nonvacuous generalization bounds for deep(stochastic) neural networks with many more parameters than training data. In Gal Elidan, KristianKersting, and Alexander T. Ihler (eds.), Proceedings of the Thirty-Third Conference on Uncertainty inArtificial Intelligence, UAI 2017, Sydney, Australia, August 11-15, 2017. AUAI Press, 2017. URL Weinan E, Jiequn Han, and Arnulf Jentzen. Algorithms for solving high dimensional PDEs: from nonlinearmonte carlo to machine learning. Nonlinearity, 35(1):278310, dec 2021. doi: 10.1088/1361-6544/ac337f.URL",
  "Vignesh Gopakumar and D Samaddar. Image mapping the temporal evolution of edge characteristics intokamaks using neural networks. Machine Learning: Science and Technology, 1(1):015006, 2020": "Somdatta Goswami, Katiana Kontolati, Michael D Shields, and George Em Karniadakis. Deep transferoperator learning for partial differential equations under conditional shift. Nature Machine Intelligence, 4(12):11551164, 2022a. Somdatta Goswami, Minglang Yin, Yue Yu, and George Em Karniadakis. A physics-informed variationalDeepONet for predicting crack path in quasi-brittle materials. Computer Methods in Applied Mechanicsand Engineering, 391:114587, mar 2022b. doi: 10.1016/j.cma.2022.114587. URL",
  "Eurika Kaiser, J Nathan Kutz, and Steven L Brunton. Data-driven discovery of koopman eigenfunctions forcontrol. Machine Learning: Science and Technology, 2(3):035023, 2021": "George Em Karniadakis, Ioannis G. Kevrekidis, Lu Lu, Paris Perdikaris, Sifan Wang, and Liu Yang.Physics-informed machine learning.Nature Reviews Physics, 3(6):422440, Jun 2021.doi: 10.1038/s42254-021-00314-5. URL Katiana Kontolati, Somdatta Goswami, George Em Karniadakis, and Michael D Shields. Learning in latentspaces improves the predictive accuracy of deep neural operators. arXiv preprint arXiv:2304.07599, 2023. Dibyakanti Kumar and Anirbit Mukherjee. Investigating the ability of pinns to solve burgers pde nearfinite-time blowup. Machine Learning: Science and Technology, 5(2):025063, jun 2024. doi: 10.1088/2632-2153/ad51cd. Thorsten Kurth, Shashank Subramanian, Peter Harrington, Jaideep Pathak, Morteza Mardani, David Hall,Andrea Miele, Karthik Kashinath, and Anima Anandkumar.Fourcastnet: Accelerating global high-resolution weather forecasting using adaptive fourier neural operators. In Proceedings of the Platform forAdvanced Scientific Computing Conference, pp. 111, 2023. I.E. Lagaris, A. Likas, and D.I. Fotiadis. Artificial neural networks for solving ordinary and partial differentialequations. IEEE Transactions on Neural Networks, 9(5):9871000, 1998. doi: 10.1109/72.712178. URL Samuel Lanthaler, Siddhartha Mishra, and George E Karniadakis. Error estimates for DeepONets: a deeplearning framework in infinite dimensions. Transactions of Mathematics and Its Applications, 6(1), 032022. ISSN 2398-4945. doi: 10.1093/imatrm/tnac001. URL Zaharaddeen Lawal, Hayati Yassin, Daphne Teck Ching Lai, and Azam Idris.Physics-informed neuralnetwork (pinn) evolution and beyond: A systematic literature review and bibliometric analysis. Big Dataand Cognitive Computing, 11 2022. doi: 10.3390/bdcc6040140.",
  "Fanny Lehmann, Filippo Gatti, Michal Bertin, and Didier Clouteau. Fourier neural operator surrogatemodel to predict 3d seismic waves propagation. arXiv preprint arXiv:2304.10242, 2023": "Zongyi Li, Nikola Borislavov Kovachki, Kamyar Azizzadenesheli, Kaushik Bhattacharya, Andrew Stuart,Anima Anandkumar, et al.Fourier neural operator for parametric partial differential equations.InInternational Conference on Learning Representations, 2020. Zongyi Li, Miguel Liu-Schiaffini, Nikola Kovachki, Burigede Liu, Kamyar Azizzadenesheli, Kaushik Bhat-tacharya, Andrew Stuart, and Anima Anandkumar. Learning dissipative dynamics in chaotic systems.arXiv preprint arXiv:2106.06898, 2022.",
  "Vaishnavh Nagarajan. Explaining generalization in deep learning: progress and fundamental limits. CoRR,abs/2110.08922, 2021. URL": "Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. Norm-based capacity control in neural networks.In Peter Grnwald, Elad Hazan, and Satyen Kale (eds.), Proceedings of The 28th Conference on LearningTheory, COLT 2015, Paris, France, July 3-6, 2015, volume 40 of JMLR Workshop and Conference Proceed-ings, pp. 13761401. JMLR.org, 2015. URL Behnam Neyshabur, Srinadh Bhojanapalli, and Nathan Srebro.A pac-bayesian approach to spectrally-normalized margin bounds for neural networks. In 6th International Conference on Learning Represen-tations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings.OpenReview.net, 2018. URL Behnam Neyshabur, Zhiyuan Li, Srinadh Bhojanapalli, Yann LeCun, and Nathan Srebro.The role ofover-parametrization in generalization of neural networks. In International Conference on Learning Rep-resentations, 2019. URL Jaewan Park, Shashank Kushwaha, Junyan He, Seid Koric, Diab Abueidda, and Iwona Jasiuk. Sequentialdeep learning operator network (s-deeponet) for time-dependent loads. arXiv preprint arXiv:2306.08218,2023. Raphal Pestourie, Youssef Mroueh, Chris Rackauckas, Payel Das, and Steven Johnson. Physics-enhanceddeep surrogates for partial differential equations. Nature Machine Intelligence, 5, 12 2023. doi: 10.1038/s42256-023-00761-y. Maziar Raissi, Paris Perdikaris, and George E Karniadakis. Physics-informed neural networks: A deep learn-ing framework for solving forward and inverse problems involving nonlinear partial differential equations.Journal of Computational physics, 378:686707, 2019.",
  "Lesley Tan and Liang Chen.Enhanced deeponet for modeling partial differential operators consideringmultiple input functions. arXiv preprint arXiv:2202.08942, 2022": "Tapas Tripura and Souvik Chakraborty. Wavelet neural operator for solving parametric partial differentialequations in computational mechanics problems. Computer Methods in Applied Mechanics and Engineer-ing, 404:115783, 2023. Nils Wandel, Michael Weinmann, and Reinhard Klein. Teaching the incompressible navierstokes equationsto fast neural surrogate models in three dimensions.Physics of Fluids, 33(4):047117, apr 2021.doi:10.1063/5.0047428. URL Sifan Wang, Hanwen Wang, and Paris Perdikaris.Learning the solution operator of parametric partialdifferential equations with physics-informed DeepONets. Science Advances, 7(40):eabi8605, 2021. doi:10.1126/sciadv.abi8605. Wuzhe Xu, Yulong Lu, and Li Wang.Transfer learning enhanced deeponet for long-time prediction ofevolution equations. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pp.1062910636, 2023. Zitong Yang, Yaodong Yu, Chong You, Jacob Steinhardt, and Yi Ma. Rethinking bias-variance trade-offfor generalization of neural networks. In Proceedings of the 37th International Conference on MachineLearning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine LearningResearch, pp. 1076710777. PMLR, 2020. URL",
  "AEvidence for Training DeepONets with tanh Activation and Huber Loss": "Similar to the experiments in .3, in this section too we use DeepONets whose both branch and trunknetworks are of depth of 3 and have a width of 100. However, unlike .3, the activation functionused in the experiments displayed in is tanh. The chosen training loss is the same as in equation 11,where is set to the Huber loss with =1",
  "We follow the setup in Lanthaler et al. (2022) for a brief review of its key theorem that motivates DeepONets": "Definition 7 (Solution Operator). Given U Rn and D Rd, compact sets with boundaries, suppose wehave a differential operator L Hs(U) C(D), where Hs(U) is the L2-based Sobolev space for some s > 0.Denote the functions in the range of L to be forcing / input functions and those in the domain of Lto be the output\" functions. Let U denote the boundary of U. Then given g L2(U) and f C(D) asolution to the differential system (g,f,L) is a function u Hs(U) s.t.",
  "Further, assume that is a probability measure on C(D) s.t G L2() i.e. C(D) G(f)2Hs(U) d(f) <": "Multiple examples of G have been discussed in Lanthaler et al. (2022), and bounds on these operatorsevaluated for instance, in Lemma 4.1 therein one can see the bounds on the G that corresponds to the thecase of a forced pendulum that we used as a demonstrative example in .1 Definition 8 (DeepONet (Version 2)). Suppose f C(D),xB(f) is a discretization of f. Furthersuppose A is a branch net mapping to Rp and Rn Rp+1 is a trunk net. Then a DeepONet\" (N C(D) L2(U)) is defined as,",
  "Remark. Henceforth xB = xB(f) for any function f, and similarly xB,i for a function fi": "The above approximation guarantee between DeepONets (N) and solution operators of differential equations(G) clearly motivates the use of DeepONets for solving differential equations. In Deng et al. (2022), theauthors present specific scenarios wherein particularly small DeepONets satisfy Theorem C.1. And as acounterpoint, in Mukherjee & Roy (2024), scenarios are demonstrated where small DeepONets would not beable to reduce the empirical training error below a certain threshold in the presence of noise in the data.",
  "pj=1BqB,jT qT ,jk1,k2": "Note that any pair of row directions in the pair of matrices (BqB1,TqT 1) is in Sb21 St21. So a supover the set of BqB1 and TqT 1 that is allowed by the constraint of CqB,qT ,qB1,qT 1 and a subsequent maxover the pairs of row directions can be upperbounded by a single sup over Sb21 St21. Thus we have,",
  "Suppose F is set of functions with a common domain D and a common range space, and B is a subset ofthat range space. Then we denote a new function space F + B = {x f(x) + b, x D f F,b B}": "Lemma H.2. Let P and G be set of functions valued in R closed under negation.Given points {xi i = 1,...,m}, {yi i = 1,...,m} in the domain of the functions then we have the following inequality ofRademacher complexities - where both the sides are being evaluated on this same set of points,",
  "IConverting ReLUDeepONets to absDeepONets": "Firstly, we recall that the map Rq x x Rq is an exact depth 2 ReLU net one which passes everycoordinate of the input through the ReLU net R z max{0,z} max{0,z} R. Using this, given anyReLU DeepONet, we can symmetrize the depths between the branch and the trunk by attaching the requirednumber of identity computing layers at the input to the shorter side. Secondly, for typical DeepONet experiments, one can assume that the the set of all possible input data isbounded. Combining this with the assumption of boundedness of the allowed matrices, we conclude that B > 0 s.t the input to any ReLU gate in any DeepONet in the given class is bounded by B. Now we observethat z B, we can rewrite the map z max{0,z} as, z 1",
  "4z B": "Hence, doing the above replacement at every gate we can rewrite any ReLU DeepONet (without biases) asa DeepONet using only absolute value activations but with biases and computing the same function onthe same bounded domain, at the cost of increasing the size of the branch and the trunk net by a factorof 3. Thirdly, a similar result as in Lemma 6.1 continues to hold for this setup as given in Lemma H.2.Note that, Lemma H.2 bounds the Rademacher complexity of a DeepONet class with the branch and trunkdepths (k,k) by a linear sum of that of (k 1,k 1) and sum of a (k,0) and a (0,k) DeepONet. While thefirst term can be recursed on again (using identical techniques as used to prove Theorem 4.1), for the lasttwo one can invoke Theorem 1 of Golowich et al. (2018). Thus, we observe that following the same arguments as in the proof for Theorem 4.1 we can derive ananalogous bound for arbitrary ReLU DeepONet classes - but with twice the depth of the DeepONet numberof extra terms in the R.H.S. These extra terms would come in pairs each pair consisting of a Rademachercomplexity bound on a standard net, one from the branch side and one from the trunk side and of decreasingdepths.",
  "KBehaviour of Rademacher Bound for 2-loss": ": The above plot shows the behaviour of the measured generalization error with respect to C3,2 C2,2mfor training DeepONets to solve Burgers PDE using the empirical loss as given in equation 11, specializedto the 2 loss and for the branch and the trunk nets being of depth 3. Each point is labelled by the numberof training data used in that experiment.",
  "MPlot Showing the Evolution of Solution to the 2D Heat P.D.E. with Time": ": This plot demonstrates the solution to 2D Heat P.D.E. at different times. In each figure, we haveshown the analytical solution first followed by the prediction from the DeepONet trained with 2 and Huberloss at = 0.25. Here, the DeepONets we used have ReLU activation functions and include bias terms."
}