{
  "Abstract": "Instruction tuning plays a critical role in aligning large language models (LLMs) withhuman preference. Despite the vast amount of open instruction datasets, naively training aLLM on all existing instructions may not be optimal and practical. To pinpoint the mostbeneficial datapoints, data assessment and selection methods have been proposed in thefields of natural language processing (NLP) and deep learning. However, under the contextof instruction tuning, there still exists a gap in knowledge on what kind of data evaluationmetrics can be employed and how they can be integrated into the selection mechanism. Tobridge this gap, we present a comprehensive review on existing literature of data assessmentand selection especially for instruction tuning of LLMs. We systematically categorize allapplicable methods into quality-based, diversity-based, and importance-based ones wherea unified, fine-grained taxonomy is structured. For each category, representative methodsare elaborated to describe the landscape of relevant research. In addition, comparisonbetween thelatest methods is conducted on their officially reported results to provide",
  "Introduction": "One of the ultimate goal of developing large lnguage models (LLMs) is to unlock their potentials ofgeneralization to unseen natural language processing (NLP) tasks. Towards this goal, a series of LLMs such asGPTs [Brown et al. (2020); Achiam et al. (2023)], LLaMAs [Touvron et al. (2023a;b); AI@Meta (2024)], andMistrals [Jiang et al. (2023a; 2024a)] have delivered high-level text understanding and generation capabilitiesvia utilizing vast amount of high-quality web and human-annotated datasets for pre-training and preferencealignment [Liu et al. (2023a; 2024c); Sun et al. (2024b); Edunov et al. (2019); Dong et al. (2019)]. Duringpreference alignment, instruction tuning plays an important role in refining the pre-trained LLMs to provideaccurate, pertinent, and harmless responses on a collection of downstream tasks [Wei et al. (2021); Sanhet al. (2021); Zhang et al. (2023d); Peng et al. (2023); Longpre et al. (2023); Shu et al. (2023); Jang et al.(2023); Ghosh et al. (2024); Kung & Peng (2023)]. For efficient and effective instruction tuning, existingstudies [Ouyang et al. (2022); Taori et al. (2023); Zhou et al. (2024a); Xia et al. (2024a)] have noticed thatimproving the quality of instruction tuning data (e.g., formulation of well-defined and complete contexts),rather than simply piling up instructions without analysis (e.g., exhaustive collection of open datasets), is ofprioritized concerns. In this work, we aim to unify a wide array of data assessment and selection methods under the context ofinstruction tuning of LLMs. As revealed from the probabilistic view [John & Draper (1975); Murphy (2012);Albalak et al. (2024)], the statistical patterns inherent in datasets determine the modeling performance. Theoverall evaluation of datapoints not only deciphers the distribution in various aspects (e.g., composition, task,and domain) but also helps cherry-pick the most beneficial subsets for higher performance with less trainingcost. Through this survey, we demonstrate that: 1) existing resourceful data assessment methods can becategorized into three main perspectives: quality, diversity, and importance (see ). 2) a systematic viewof selection methods can be unified even they more or less exhibit coupling with the assessment techniques(see ). It is noted that quality, diversity, and importance might be used interchangeably withoutstrict discrimination in previous studies. But here we provide a rationalized organization taxonomy forstructured elaboration. Despite the goal of being comprehensive, the present survey only provides details ofcertain typical, representative methods to avoid being tediously long. We hope the in-depth explanations anddiscussions on the selected methods provide insights into developing robust data assessment and selectionpipelines for future studies.",
  "Related Surveys": "[Liu et al. (2024d)] studied the mainstream datasets for building LLMs, including the pre-training corpora,instruction tuning datasets, preference datasets, evaluation benchmarks, and traditional NLP datasets.Their work focuses on the descriptions of dataset statistics (e.g., categorization, sources, and domains)without providing guidelines on utilization. In contrast, we emphasize the selection of instruction-tuningdata for the improved downstream performance.[Albalak et al. (2024)] presented a systematic overview ofconstructing the data pipeline for language models. Any selection method, either via distribution matchingor diversification, can be composed of: 1) utility function; 2) selection mechanism. During different stagesof the pipeline, the selection method should be adjusted according to different selection objectives (e.g.,language filtering, data quality control , domain knowledge division , deduplication, toxic and explicit contentremoval, and data mixing). Their work pays extra attention to the processing of the pre-training corporawhile neglecting the fine-grained analysis of existing selection methods specifically designed for instructiontuning. In this case, our survey serves as an indispensable extension on the selection of instruction datasets.[Wang et al. (2024a)] focused on the data preparation for instruction tuning. Existing methods on buildinginstruction tuning datasets include: 1) reformulating the discriminative NLP datasets into generative ones; 2)self-instruct with seed prompts; 3) prompt mapping and evol-instruct Popular methods on dataset selectioncan be simply classified as: 1) system of indicators; 2) trainable LLMs; 3) powerful LLMs; and 4) small models.",
  ":Categorization of data assessment and selection methods for effective instruction tuning of LLMs": "Comparatively, our survey stems from the characteristics of data themselves, namely quality, diversity, andimportance, for categorization of selection methods. In each category, we further provide subdivided groupsby the selection philosophy, which deepens the understanding of selection for practical take-home messages. Existing surveys on general data selection also shed light on the principles of developing selection methods.[Guo et al. (2022)] started from the general coreset selection methods in the field of deep learning and categorizeall selection manners into: 1) geometry-based methods (e.g., herding, k-center greedy); 2) uncertainty-basedmethods (e.g., least confidence/entropy/margin); 3) error/loss-based methods (forgetting; GraND/EL2N;importance resampling); 4) decision boundary-based (adversarial deepfool; contrastive active learning); 5)gradient matching-based (gradient approximation towards full set); 6) bi-level optimization-based (inner loopof model optimization and outer loop of datapoint selection); 7) sub-modularity-based (e.g., graph cut; facilitylocation); 8) proxy-based (preference of a small model on data selection). [Zhou et al. (2024b)] investigatedthe potential metrics and aspects for data quality measurement. They provide a list of available tools fordata evaluation. Apart from data assessment and selection methods that are specifically designed for NLP or",
  "Published in Transactions on Machine Learning Research (12/2024)": "Ming Zhong, Yang Liu, Da Yin, Yuning Mao, Yizhu Jiao, Pengfei Liu, Chenguang Zhu, Heng Ji, and JiaweiHan. Towards a unified multi-dimensional evaluator for text generation. arXiv preprint arXiv:2210.07197,2022. Chunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, PingYu, Lili Yu, et al. Lima: Less is more for alignment. Advances in Neural Information Processing Systems,36, 2024a. Daquan Zhou, Kai Wang, Jianyang Gu, Xiangyu Peng, Dongze Lian, Yifan Zhang, Yang You, and JiashiFeng. Dataset quantization. In Proceedings of the IEEE/CVF International Conference on ComputerVision, pp. 1720517216, 2023. Jianghong Zhou, Eugene Agichtein, and Surya Kallumadi. Diversifying multi-aspect search results usingsimpsons diversity index. In Proceedings of the 29th ACM International conference on information &knowledge management, pp. 23452348, 2020.",
  "Survey Scope": "Although \"data evaluation\" has been so frequently mentioned that it appears as a clich problem in developingmachine learning algorithms, the optimal solution to establishing an overall data assessment and selectionpipeline still remains an open question. Especially under the context of instruction tuning of LLMs, existingstudies proposed various measurements and strategies to select the \"high-quality\" instructions. However,very few studies noticed that there exists no unified dimensions or aspects in measuring data \"quality\" whereprevious works tend to put emphasis on the domain-specific and task-dependent characteristics. In addition,the inherent, systematic coupling between data assessment and subset selection is not well demonstrated. Under such circumstance, the present study strives to provide a comprehensive review on evaluating anddecomposing massive instruction tuning datasets. We categorize the main aspects of data assessment in termsof quality, diversity, and importance. To reduce ambiguity, their definitions are first provided below. Quality refers to the intrinsic value of the data. High-quality data typically satisfy two conditions: 1) Theinstructions are clear, accurate, and explicit in explaining the task at hand and the expected behavior ofLLMs. 2) The responses are correct, coherent, and pertinent to the instructions. All the requirements andconstraints specified in instructions should be met. Accordingly, most existing methods not only underlinethe clarity and fluency of instructions but also filter out mismatched, incorrect, and harmful responses viavarious judging and measuring techniques.",
  "Whats Beyond the Survey Scope": "The instruction tuning methods for de-bias and fairness of LLMs are not covered in the present study. Weacknowledge that the bias and fairness aspects of instruction data are valued in developing responsibleLLMs [Gallegos et al. (2024); Chu et al. (2024); Li et al. (2023d)]. However, they are beyond the scope of thepresent study for the following reasons:",
  "Most data selection methods on instruction tuning do not even notice the bias or fairness of data.They are in lack of explicitly designing steps to reduce negative impacts of biased data": "Existing fairness tuning methods follow their own definitions of quality and diversity in terms ofgender, race, religion, profession, age, and political ideology. It is difficult to bring in all theircorresponding evaluation techniques under our selection taxonomy. The bias and fairness not only intersect with diversity but also with quality control, which is not aninclusive concept. For example, to control the harmful contents like hatred and racism responses,reward models or GPT scoring from quality-based selection can be used because the publicly releasedreward models and LLMs are already aligned with human preference. The evaluation of bias and fairness is not covered in existing studies on data selection, which makesit difficult to validate the effectiveness of selection techniques in improving the fairness of LLMs. However, we emphasize that bias and fairness should be specifically handled because instruction-tuned LLMstend to exhibit more bias than the pre-trained LLMs [Itzhak et al. (2024)]. Therefore, we believe the selectionfor a less-biased dataset would be a promising future direction in developing comprehensive selection methods.",
  "Template Wrapping": "<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\nExplain the difference between a URL and a domain name.<|im_end|>\\n<|im_start|>assistant\\nA URL, which stands for Uniform Resource Locator, is the entire address used to visit a website or access a specific online resource. It includes the protocol (such as HTTP or HTTPS), the domain name, and other elements like the path, parameters, or anchor. For example, the URL of this page is the other hand, a domain name is a component of a URL that specifically identifies the website or web page. It is the part of the URL that comes after the protocol and before the path. For example, in the URL above, the domain name is \\\"play.aidungeon.io\\\". \\n\\nIn simpler terms, you can think of a URL as an address that leads you to a specific location on the web, while a domain name is the specific name of the location you're visiting.<|im_end|>",
  "UserAssistant": ": The pre-processing of an instruction dataset includes: 1) template wrapping, and 2) tokenization.In the first step, we wrap the raw texts Ii with a pre-defined chat template into the textual prompts pi.In the second step, we perform tokenization on pi with the LLM-associated tokenizer for the datapoint xi.Given the index t for indicating where the loss mask of language modeling starts taking effect, we split xiinto xi(<t) and xi(t), respectively denoting the instruction part (input) and the response part (output).",
  "In this section, we briefly introduce the instruction tuning of LLMs and the problem statement for datasetassessment and selection": "Instruction Dataset PreparationIn instruction tuning, each text sample Ii is usually composed of threeparts: 1) instruction (either with or without system prompt), 2) input (can be empty), and 3) response. For an off-the-shelf pre-trained LLM parameterized as , a pre-determined instruction template is used towrap the Ii into a textual prompt pi with special tokens like \"<|im_start|>\" and \"<|im_end|>\" for separationof roles (e.g., system, user, assistant, function, and observation) and their contents. It is noted thatthe prompt template for organizing a text sample Ii differs across model families. Especially, the specialtokens are unique to the tokenizers and play an important role in differentiating multi-turn conversationsand stopping model generations. Then, a LLM-associated tokenizer performs tokenization on the textualprompt pi for a sequence of token-id integers: xi = [xi(1), xi(2), ..., xi(n)], where xi(j) denotes the j-th tokenof xi and n is the total number of tokens. Out of simplicity, the token sequence xi can be simply split intotwo parts by the index t: 1) the instruction part (xi(<t)) that is fed into the LLM without being involved inloss computation, and 2) the ground-truth response part (xi(t)) that is expected as the LLM output viaminimizing its language modeling loss. Without losing generality, we use the terminology of \"datapoints\"throughout the paper to refer to the tokenized samples xi. The process of processing an instruction sample xi is illustrated in . We choose one example from theAlpaca dataset [Taori et al. (2023)] and perform Qwen tokenization [Yang et al. (2024)] for demonstration.",
  "j=tlogP(xi(j)|xi(<j); ).(1)": "For each xi, given all previous tokens xi(<j) , the model iteratively predicts the next token xi(j) at the j-thindex. Note that the previous tokens here include both the instruction context part xi(<t) and the responsecompletions up to the current token xi(t,<j) . Data Assessment and SelectionWe aim at finding the most informative subset Sb S from the entireset S under the given budget |Sb| b. Mathematically, the selection of Sb requires: 1) a quantitativeevaluation function q() that assesses each datapoint xi, and 2) an elaborated sampling mechanism thatdetermines the rules of selection:Sb = (S, b, q).(2) With respect to the detailed implementation of , either an iterative, greedy algorithm or a batch-wiseheuristic rule can be adopted for compatibility with q(). For example, the greedy sampling is represented as:",
  "xjS q(xj),(4)": "where Sample(S, b, p) performs sampling b times upon the normalized probability distribution over q(). Moreadvanced sampling techniques can be developed in accordance with the domains and tasks at hand. Such data assessment and selection paradigm is expected to bring about the following benefits:1) thereduction of noise by ignoring those mislabeled, mismatched instruction-response pairs, 2) the re-balance ofdata distributions by down-sampling those easy, common, and similar examples while up-sampling hard, rare,and unique ones, and 3) the expedition of training in return for efficient optimization of LLMs.",
  "q(xi) = fq(qI(xi<t), qR(xit)),(5)": "where fq is an aggregation function that combines the instruction and response quality scores either explicitlyor implicitly. Specifically, the instruction quality qI can be further broken down into: 1) clarity qCI thatmeasures the ease of understanding the task, 2) accuracy qAI that measures how well the instruction alignswith the expected task, and 3) explicitness qEI that measures how explicitly the instruction defines the outputconstraints (e.g., formats and styles). Consequently, we have qI(xi<t) = gI(qCI (xi<t), qAI (xi<t), qEI (xi<t)) withthe aggregation function gI. Similarly for the measurement of response, its quality qR can be assessed via: 1)correctness qCR that measures whether the response correctly answers the instruction, 2) coherence qHR that",
  "Hand-crafted Indicators": "OverviewExisting researches on importance measurement of datapoints often stem from two aspects:1) from the perspective of a datapoint itself, i.e., the difficulty or complexity of each datapoint and theamount of information it provides; 2) from the perspective of the model under development, i.e., the necessityof learning from such a datapoint based on the current performance and confidence (uncertainty), Mosthand-crafted indicators are proposed to analyze the text difficulty. Technical DetailsThe readability indices [Young & Shishido (2023)] can be used to assess both quality (seeSec. 3.1) and difficulty of text samples. Specifically, samples with intricate grammar, advanced vocabulary,and inference dependency are deemed as difficult ones and can be used to evaluate robustness of modelsacross benchmarks of various difficulty levels [Smith & Johnson (2020); Kiela et al. (2021); Ethayarajh et al.(2022); Belinkov & Glass (2019); Nie et al. (2019); Ribeiro et al. (2020)]. For specialized domains such assolving maths problems, the education level (e.g., elementary-level, high school-level, and university-level)determines the difficulty of samples [Patel et al. (2021); Huang et al. (2016); Koncel-Kedziorski et al. (2016)]. One of the pioneering studies on readability scores for difficulty assessment is to compute the percentage ofdifficult or easy words in one sentence [Klare (1974); Begeny & Greene (2014)]. The words on a pre-definedlist are counted as familiar words, and those not listed are unfamiliar, advanced words. Besides, the averagenumber of syllables per word, the number of single-syllable words, and the number of multi-syllable wordsare also indicative in assessing the text materials [Connatser (1999); Carrell (1987); Zakaluk & Samuels(1988); Dale & Chall (1949)]. Notably, there exist three representative readability metrics: 1) the DaleChall formula [Chall & Dale (1995)], 2) the flesch reading ease [Flesch (1948)], and 3) the gunning fogindex [Gunning (1952)]. Given these metrics, [Saranathan et al.] conduct a thorough analysis on existing NLPdatasets S to select the most challenging subsets for efficient evaluation of LLMs. The easiest and hardestsamples from the TruthfulQA [Lin et al. (2021)] via these indicators are confirmed positively correlated withthe actual complexity. The selection of difficult instruction-response pairs via Eq. 8 allows a wider distributionof performance for the models under investigation. The introduction of difficult datapoints into the subset Sbhelps keep the relative rank of different models unchanged compared with that measured on the entire set S.",
  "IND3(xi), ...INDM(xi)),(6)": "where M denotes the total number of indicators and f is the aggregation function which depends on boththe instruction task and dataset. One can simply use linear combinations with pre-defined or dynamicallyadjusted weights. However, meticulous tuning might be needed for the ultimate f. Given the indicatorsINDi for each xi, two intuitive selection methods can be adopted: 1) to filter out datapoints whose indicatorscores are below a threshold; 2) to keep only the samples whose indicator scores rank within a certain rangeof percentiles. Mathematically, these two selection mechanisms can be respectively represented as:",
  "S = {xi|Pmin Ff(f(xi)) Pmax, 1 i N},(8)": "where min and max respectively denote the left and right threshold boundaries. The estimated Ff isthe empirical cumulative distribution function of all indicators f. Pmin and Pmax respectively refer to theminimum and maximum percentile for enclosing the selection range. In practice, both the threshold andpercentiles are hyper-parameters that require task-specific fine-tuning. Technical Details[Mishra et al. (2020a)] and [Mishra et al. (2020b)] introduce a data quality metric, namelythe DQI, to quantify the differences between successive benchmarks by giving high scores to generalizablesamples and low scores to biased samples. Such a metric implies whether a well-trained model truly learnsthe underlying task rather than overfitting the spurious bias of specific benchmarks. Specifically, DQI hasseven components including vocabulary, inter-sample N-gram frequency and relation, inter-sample semantictextual similarities (STS), intra-sample word similarity, intra-sample STS, N-Gram frequency per label, andinter-split STS. Based on the DQI, [Mishra & Sachdeva (2020)] propose to prune existing huge NLP datasetsand demonstrates that the model trained on only 2% of the SNLI dataset achieves near-equal performancewith that on the entire set. It first performs AFLite [Le Bras et al. (2020)], which is detailed in [Sakaguchiet al. (2021)], to keep samples with predictability scores over a threshold and then delete bottom k sampleswith the lowest DQI scores. [Dang & Verma (2024)] further split DQI components into linguistic indicatorsand semantic indicators, and validate their respective roles in detecting outliers, noises, and duplications.Apart from data selection for training LLMs, quality indicators can also be employed to identify the mostdiscriminative samples in the evaluation set to expedite evaluation of LLMs. [Saranathan et al.] investigatekey indicators such as spelling errors [Yannakoudakis & Fawthrop (1983)], average word length, excessive wordrepetition, and the compound probability distribution. These indicators stem from the traditional studies ontext readability (i.e., readability formulas and sophisticated features) [Klare et al. (1963; 1984); Dubay (2004);Kintsch & Vipond (2014); Kemper (1983)]. Recent studies on readability leverage NLP systems to extractmore advanced and informative features for readability measures [Si & Callan (2001); Collins-Thompson &Callan (2005); Schwarm & Ostendorf (2005); Feng et al. (2010)]. [Franois (2010; 2011); Franois & Fairon",
  "IND33(xi), ...INDMM (xi)),(9)": "where the learnable parameters 1, 2, ..., M highlight the difference between model-based and hand-craftedindicators. Based on such indicators, similar selection mechanisms (Eqs. 7 and 8) can be adopted. Technical DetailsOne of the most intuitive model-based indicators is perplexity [Shannon (2001); Jelineket al. (1977); Jelinek (1980)]. It is frequently mentioned as the evaluation metric for pre-trained languagemodels [Penedo et al. (2023); Radford et al. (2018; 2019); Brown et al. (2020); Achiam et al. (2023)] but canalso be employed as a data quality indicator. [Ankner et al. (2024)] propose to use a small GPT-style referencemodel such as MPT 125M [Team (2023)] to prune datasets via perplexity-based sampling for training a 3Bmodel. For any datapoint xi, the perplexity is defined as the exponential of negative likelihood with base of 2:",
  "(10)": "Based on the perplexity inferred from a small model, samples at the high and medium percentiles are chosenby Eq. 8 for downstream fine-tuning. [Deng et al. (2021)] develop a unified evaluator framework to score thegenerated outputs for natural language generation tasks. A RoBERTa-based [Liu et al. (2019)] discriminatorlearns to score responses in terms of consistency, relevance, preservation, engagingness, and groundedness.One could simply adopt such a discriminator for evaluation of instruction-response pairs. [Zhong et al.(2022)] further propose a multi-dimensional scoring evaluator. For each evaluation dimension, the originalinstruction-response pairs are converted into positive samples in the form of boolean question-answer problems.The negative samples are respectively constructed via a rule-based transformation. The evaluator itself isimplemented as a T5 model [Raffel et al. (2020)] and trained on these positive and negative samples forscoring in the range from 0 to 1.[Jiang et al. (2024c)] prune the UltraChat [Ding et al. (2023)] datasetby scoring each datapoint by the learning complexity of a pre-trained Qwen-1.8B model [Bai et al. (2023)].Specifically, the learning complexity is calculated as the averaged prediction confidence of different subnets:",
  "Ri = r(xi(<t), xi(t)),(12)": "where r denotes the reward model. t is the index where xi(<t) and xi(t) respectively denote the instructionQ and response A. [Marion et al. (2023)] investigate three classic metrics in clean set selection [Guo et al.(2022); Song et al. (2022); Natarajan et al. (2013); Qin et al. (2024)]: perplexity (Eq. 10), error l2-Norm(EL2N) [Paul et al. (2021)], and memorization ranking [Biderman et al. (2024)]. Specifically, EL2N is definedas:",
  "j=1P(xi(<j); ) yi(j)2,(13)": "where yi(j) denotes the one-hot ground-truth vector as the target of the probability vector P(xi(<j); ).Specifically, both P(xi(<j); ) RNvocab and yi(j) RNvocab are of the same dimension, where Nvocab denotesthe vocabulary size associated with the tokenizer. For the vector of yi(j), all elements are zero except thatthe element indexed at xi(j) is one.The memorization ranking is represented as:",
  "j=11(xi(Moffset+j) = xi(Moffset+j)),(14)": "where Nwin denotes the length of a consecutive sequence and Moffset is an offset of the starting index. Thexi(Moffset+j) refers to the generated token given input xi(<Moffset+j), and xi(Moffset+j) is its ground-truth. [Caoet al. (2023)] combine both hand-crafted indicators (e.g., input length, output length, MTLD [McCarthy &Jarvis (2010)], and kNN-i [Dong et al. (2011)]) and model-based indicators (e.g., reward score, perplexity,and Uni-Eval metrics [Zhong et al. (2022)]) for fitting the loss of a LLM on the evaluation set. The linearregression model is optimized via least squares method [Bjork (1988)] and the optimal selection of instructiondata is achieved via BlendSearch [Wang et al. (2021a;b)] for minimizing the estimated evaluation loss. [Liet al. (2023a)] propose one of the most pioneering works that leverages the target language model itselfto perform self-guided data selection. The language model is first \"warmed-up\" with very few samplesrandomly chosen from the pool to learn from brief experience. Then, such an experienced model evaluateseach instruction-response pair via the instruction-following difficulty (IFD) score. The IFD score measureshow much guidance or assistance the instruction provides to the generation of ground-truth response, bycomparing the loss of causal language modeling on the response with and without instruction:",
  "(15)": "where the index t splits apart the instruction Q and the response A. Samples whose IFD scores over max = 1are invalid datapoints with misaligned, mismatched instruction-response pairs. The empirical setting ofmin affects the trade-off between quality and diversity of the selected datapoints. [Zhao & Fang (2024)]comprehensively employ hand-crafted indicators for low-level quality filtering, and uses perplexity and IFDscore for high-level filtering. A voting mechanism is additionally introduced with IFD scores from one",
  "(P(xi(<j); ))),(20)": "where 1 and 2 denote the largest and second largest elements of the probability P(xi(<j); ) RNvocab forthe newly generated j-th token. However, [Wu et al. (2023)] find that such uncertainty-based data samplingmethods perform worse than random sampling on Databricks-Dolly [Conover et al. (2023)], SelfInstruct-Davinci [Taori et al. (2023)], and SelfInstruct-GPT4 [Peng et al. (2023)]. RemarkHybrid techniques that simultaneously combine perplexity, uncertainty, reward scores, and othertraining-aware metrics are promising in selecting unbiased high quality samples. In consideration of thetraining and inference cost, it is feasible to employ small proxy models as alternatives for computingmodel-based indicators.",
  "GPT Score": "OverviewThe invoking of OpenAI APIs [Tingiris & Kinsella (2021); Lappalainen & Narayanan (2023);Sun et al. (2023); Kublik & Saboo (2023)] for ChatGPT services (e.g., GPT3.5, GPT4) allows automaticscoring of instruction tuning datasets. Recent studies on bringing LLMs as judges [Zheng et al. (2024); Wanget al. (2023a); Zhu et al. (2023); Huang et al. (2024); Zeng et al. (2023); Chan et al. (2023); Pang et al.",
  "Instruction: <instruction>Input: <input>Response: <response>": "Please rate according to the <dimension> of the response to the instruction and the input.Eachassistant receives a score on a scale of 0 to 5, where a higher score indicates higher level of the <dimension>.Please first output a single line containing the value indicating the scores. In the subsequent line, pleaseprovide a comprehensive explanation of your evaluation, avoiding any potential bias.",
  ": The prompt pG for scoring the raw text Ii with ChatGPT": "Technical Details[Chen et al. (2023b)] propose a surprisingly easy-yet-effective method that directlyuses GPT3.5 to score datapoints in terms of helpfulness and accuracy. Both instructions and responsesare scored on a scale from 0 to 5 and experimental results show that general instruction datasets, exceptcoding-related samples, can be distilled into smaller subsets for better downstream performance. [Bukharin& Zhao (2023)] follow [Chen et al. (2023b)] for filtering Alpaca [Taori et al. (2023)]. [Chen & Mueller (2024)]employ the BSDetector [Chen & Mueller (2023)] to estimate the confidence of GPT3.5/GPT4 on the giveinstruction-response pair. It takes both the self-consistency and direct scoring into consideration. Only highlyconfident samples are kept for fine-tuning domain-specific LLMs and those less confident ones are correctedautomatically by these LLMs. [Xu et al. (2023b)] directly evaluate instruction datasets in terms of accuracy,explanation, clarity, and difficulty for weighted scorings from GPT4. Then, both hand-crafted indicators (i.e.,lengthwise semantic evaluation) and GPT4 scorings are employed for final ranking. [Liu et al. (2023b)] arguethat the direct scoring of GPT4 on one single instruction sample is not well-calibrated and instead givesrelative ranking of multiple instruction variants at once. The complexity of instructions [Xu et al. (2023a)]and the quality of instruction-response pairs are sequentially obtained from GPT3.5. [Zhang et al. (2024c)]use GPT scorings to judge: 1) whether the given text contains mathematical contents; 2) and if yes, whetherthese maths contents are of high quality for education purpose. Such scores are proved more effective thantraditional \"mathematical\" classifiers [Paster et al. (2023)]. [Lu et al. (2023a)] propose to use ChatGPT forannotating open-ended, fine-grained intention tags on open datasets. Then, the quality of the tag dataset isevaluated by humans and GPT4 in terms of tagging precision and consistency. Instead of fully relying on theGPT4, [Li et al. (2023c)] exploit the model under investigation itself (e.g., LLaMA 65B) to iteratively derivequality scores on each augmented example on a 5-point scale. Then a curated clean set is chosen via theEq. 7. QuRator [Wettig et al. (2024)] manually define quality criterion such as writing style, facts and trivia,educational value, and required expertise. Then, quality comparison is conducted on two instruction-responsesamples via GPT3.5 scoring. Such pairwise scorings are used to fine-tune a sheared-LLaMA 1.3B model [Xiaet al. (2023)] in a manner similar to DPO [Ouyang et al. (2022); Rafailov et al. (2024)]. It is noted that thepairwise scoring [Ouyang et al. (2022); Dubois et al. (2024); Zeng et al. (2023); Liu et al. (2023b)] has beenfound more reliable, consistent, and unbiased than the individual scoring [Gunasekar et al. (2023); Chen et al.(2023b)] during GPT-based quality analysis.",
  "Guidelines (excerpts) for human annotations": "# GuidelinesBelow is a list of guidelines that should be adhered to for each possible task available when building thedataset. To see some examples of how the guidelines can be applied, visit the examples document. ## 1. General rules- Always make sure to read and understand the guidelines to each task before fulfilling it. - Try to follow theguidelines as closely as possible. - If you are unsure whether a message violates a guidelines, contact us at ourDiscord.- Use the thumbs-up/thumbs-down system to further mark messages that are of high or low quality.",
  ": The guidelines (thumbnails) for human experts to create and annotate instruction datasets": "Technical DetailsThe OpenAssistant [Kpf et al. (2024)] dataset is featured by its high-quality human-generated, human-annotated multi-lingual conversations for both instruction tuning and reinforcementlearning from human feedback. For each instruction-response pair along the conversation tree, the humanannotators are asked to categorize them according to three dimensions: spam detection, guideline adherence,and quality. The quality score is rated on a five-point Likert scale across aspects including quality, creativity,humorousness, politeness, and harmlessness. These scores are used to sort instructions for analysis andpreference optimization of LLMs.[Lu et al. (2023a)] enroll human annotators to provide judgements onthe tagging of each instruction. To verify the quality scores provided by humans, counterfactual cases areprepared respectively for precision and consistency tasks. Results show that human annotators have low falsepositive rates at tagging precision, but lack proof of confidence on their original quality judgements. [Zhouet al. (2024a)] propose to use human annotators for creation of small-yet-effective instruction datasets. Tocollect questions and answers from various sources, simple hand-crafted indicators such as text length areused to filter low-quality datapoints. Then, high quality instruction-response pairs are manually selected(750) and written (250) via subjective quality control. The databricks-dolly dataset [Conover et al. (2023)]contains 15K human-generated instruction-response pairs. Although quality is emphasized during large-scaleannotation, imperfect samples still exist. For example, low-quality and inaccurate responses, incomplete andvague instructions, problematic texts with toxic language and grammar errors are found [He et al. (2024)]. RemarkHuman evaluation plays an irreplaceable role in quality control of preference alignment. To reducethe inter-annotator inconsistency, detailed guidelines should be prepared for quality measurement. In addition,",
  "Diversity-based Selection": "In this section, we introduce methods that emphasize the diversity of instruction datasets. When it comes todiversity, existing researches either measure the individual diversity of each sample (e.g., lexical and semanticrichness) or the overall diversity of the entire dataset (e.g., the volume of the enclosed embedding space).Datapoints whose tasks and domains are of minority classes in a long-tailed distribution are preferred duringsubset selection. Such sampling philosophy strikes to maintain or approximate the spread of the originalembedding clusters but with much less sparsity. In the case of diversity, the evaluation function q(xi) can berepresented in the unified formulation as:",
  "q(xi) = fd(qL(xi), qS(xi)),(23)": "where qL measures the lexical diversity of xi and qS assesses the semantic diversity. The fd denotes theaggregation function in diversity measurement. Typically, qL often investigates the diversity of n-grams, tokens,words, and sequences. Complementarily, qS emphasizes semantic diversity that the variety of representationsof the selected datapoints should be maximized in the embedding space. Both the two aspects of diversitycan be sequentially or jointly considered to remove any duplicates in the instruction datasets.",
  "|xi|,(24)": "where Unique(xi) denotes the set of unique tokens present in xi. To reduce the sensitivity of TTR to thevariation of text length, several studies [Covington & McFall (2008; 2010); Kettunen (2014); Matlach et al.(2021)] standardized the length by introducing logarithms or n-grams into the formula. Later, computational approaches to measure lexical diversity have been developed such as vocabulary diversity(vocd-D) [Malvern & Richards (1997); Malvern et al. (2004); Silverman & Ratner (2002); deBoer (2014)],the measure of textual lexical diversity (MTLD) [McCarthy & Jarvis (2010); Jarvis & Daller (2013)], andhypergeometric distribution diversity (HD-D) [Jarvis (2013); McCarthy (2005)]. All these metrics requiremulti-step computation for approximation. Specifically for vocd-D, random sampling is first performed on xifor a series of sub-sequences with varying lengths k (e.g., 10, 20, 30 tokens). Then, TTRk is:",
  "where D is the only parameter required to be estimated. By approximatingTTRki towards TTRki with theleast squares, we have Dbest fit = D:vocd-Di = D.(27)": "A larger D reflects the higher diversity of xi. The computation of MTLD, on the other hand, first determinesthe TTRi as a pre-defined threshold, and then partitions xi into M different contiguous subsequences{x1i , x2i , ..., xmi , ..., xMi }. Each subsequence xmi= xi(j,<j+k), k > 0, 1 j |xi| k maintains a TTRkiabove the threshold TTRi. The MTLD is defined as:",
  "(29)": "Variants of TTR indicators such as MTTRSS [Malvern et al. (2004)], MSTTR [Malvern et al. (2004)],MATTR [Covington & McFall (2010)], and MTLD-W [Vidal & Jarvis (2020); Kyle et al. (2021)] all target atthe solutions to two fundamental problems [Bestgen (2023)]: 1) the sensitivity of indicators to text length,and 2) the impact of the indicator parameters. [Li et al. (2015)] propose two rather simplified TTR scores asdistinct-1 and distinct-2, where the number of distinct unigrams and bigrams of xi are respectively dividedby the total number of tokens. Other studies [Cao & Clark (2017); Zhu et al. (2018); Shu et al. (2019); Tevet& Berant (2020)] extend the application of n-gram-based diversity for model-generated responses. Apart from lexical diversity, there exists many efficient diversity indicators that are built upon the semanticsof each example. [Dong et al. (2011)] propose to approximate k-nearest neighbor (k-NN) graph [Peterson(2009)] with arbitrary similarity measures on semantic embeddings of large-scale datasets. Such efficientconstruction of a k-NN graph allows the distance of xi to its j-th nearest neighbors to be a feasible diversitymeasure:kNN ji = d(g(xi), g(Nj(xi))),(30) where Nj(xi) denotes the j-th closest neighbor of xi in the embedding space projected by g(). The commonchoices of the distance function d(, ) include the Euclidean distance, cosine distance, and Jaccard coefficientdistance [Huang et al. (2008)]. The projection from text (e.g., instruction-response pairs) into the embeddingspace can be achieved with pre-trained sentence BERT [Reimers & Gurevych (2019); Feng et al. (2020)],where an additional pooling operation is performed on the final output of BERT [Devlin et al. (2018)] forsentence embeddings. Note that a higher kNNi implies that the sample xi is more unique and should be keptin subset selection for higher diversity. Due to the fine-grained representation capability of BERT, existinghand-crafted indicators often rely on BERT embeddings for similarity or diversity measurement [Tevet &Berant (2020); Zhang et al. (2019); Larson et al. (2019); Yauney et al. (2023)]. To improve the generalization of diversity measure, [Xu et al. (2023b)] argue that the statistics of featureembedding of each sample itself should be considered. It does not require additional prior knowledge on thestructure of embeddings. Given all datapoints xi S, their semantic embeddings from any sentence encodercan be represented as X = [g(x1), g(x2), ..., g(xN)] R|S|H. The row variance V ari of each embeddingg(xi) in the reduced dimensional space R|S|k by principal components analysis (PCA) [Wold et al. (1987)]is used as the diversity indicator:",
  "i=1kNN 1i , xi S.(32)": "Such a diversity measure has been widely used in dataset construction and retrieval [Stasaski et al. (2020);Stasaski & Hearst (2022); Mithun et al. (2019); Spyromitros-Xioufis et al. (2015); Sun et al. (2024a); Ionescuet al. (2018)]. [Du & Black (2019)] simply perform clustering on all samples with k-means [Ikotun et al.(2023)] into K clusters (C1,C2,...,CK) in the embedding space, and then uses the cluster inertia as diversityindicators:",
  "OverviewSimilar to Eq. 9, model-based indicators on diversity also rely on the target or proxy languagemodel for computing the indices": "Technical DetailsThe diversity of a dataset S can be intuitively defined as the sum of rarity measuresof each constituting element xi. Accordingly, entropy-related methods are proposed to estimate such rarity.The more uncommon, various samples exist, the higher diversity the dataset becomes. Mathematically, thevanilla entropy [Shannon (1948)] is proposed for diversity measures:",
  "The parameter adjusts the element-wise emphasis on rare or frequent events": "Studies on biology and ecology [Mouillot & Lepretre (1999); Peet (1974); He & Hu (2005); Gregorius &Gillet (2008)] investigate Simpsons Index (SI) [Simpson (1949); Wu et al. (2024; 2022)] for measuring thebiodiversity of species and genetics. [Zhou et al. (2020)] propose a variant of the original SI with a moreflexible statistic metric:",
  "i|)),(39)": "where i| denotes the normalized eigenvalues of the similarity kernel matrix KS|, and supp() is theset of indices of all non-zero eigenvalues. The smaller < 1 makes the scoring more sensitive to rareclasses and therefore allows accurate diversity measurement even under severe class imbalance. One simpleimplementation of the similarity kernel KS| is to use the Gaussian Radial Basis function k with featureembeddings as k(g(xi|), g(xj|)) = exp( 1",
  "xiSb INDiis multiplied with DV S (Sb) for comprehensive evaluation in terms of quality and diversity": "[Miranda et al. (2022)] propose an intrinsic diversity coefficient to measure the diversity of a dataset withTask2Vec embeddings [Achille et al. (2019); Nguyen et al. (2019)] for distance computation between differenttasks. The Task2Vec encodes data from different tasks by the diagonal entries of the Fisher InformationMatrix (FIM). The FIM results from fine-tuning only the final (e.g., token classification) layer of a pre-trainedmodel, namely a probe model (e.g., GPT2 [Radford et al. (2019)]), to solve the task. Given a batch of samplesB, the mathematical representation of FIM is defined as:",
  "log P(xi(j)|xi(<j); )T ,(40)": "where xi(j) denotes the j-th token predicted from the model parameterized as given the real sequence inputxi(<j). The expectation Exi,j,xi(j) takes an average over the sequence length |xi| for each xi sampled randomly from the batch xi B. The Task2Vec embeddingfB = diag(FB), where diag() denotes the diagonal entriesof FB. Based on the Task2Vec embeddings, [Lee et al. (2023)] propose to compute the diversity coefficientsdiv specifically for NLP datasets:",
  "Ddiv(S1, S2) = EB1S1,B2S2d(fB1,fB2),(41)": "where d denotes distance measurement (e.g., cosine distance). Both B1 and B2 are two batches sampledrespectively from the same or different datasets for diversity measures within or across datasets. Experimentsconfirm that hand-crafted indicators such as the number of latent concepts [Xie et al. (2021)] and the richnessof vocabulary are positively associated with the proposed div coefficients.",
  "Geometry-based Coreset Sampling": "OverviewInstead of explicitly calculating the diversity-aware indicators, recent studies on selectinginstruction datasets tend to introduce coreset sampling methods for a systematic consideration [Guo et al.(2022)]. Specifically, coreset sampling aims to find the most informative-and-diverse subset that representsthe entire dataset the most, so that close or even surpassing performance can be achieved on the languagemodel trained on the subset with respect to that on the entire set. Technical DetailsAmong different categories of coreset sampling methods, geometry-based methodsare the most intuitive and widely-used ones [Chen et al. (2012); Agarwal et al. (2020); Sener & Savarese(2017); Sinha et al. (2020); Kamalov (2020); Rezazadegan Tavakoli et al. (2011); Kirchenbauer et al. (2024);Zhou et al. (2023)]. The intuition behind is that close samples in the embedding space often share similarproperties with low diversity. Therefore, redundant information can be effectively suppressed by controllingthe minimum distance between any two samples for subset selection. Specifically, k-center greedy is a typicaldiversity-oriented sampling method for massive pretraining and instruction-tuning corpus [Chen et al. (2023a);Bhatt et al. (2024); Wu et al. (2023); Zhao & Fang (2024); Du et al. (2023)]. It solves the minimax facilitylocation (FL) problem [Cornujols et al. (1983); Farahani & Hekmatfar (2009)], i.e., selecting the subset Sbunder the given size budget b from the full set S so that the largest distance between an example in S\\Sband its closest example in Sb is minimized:",
  ": return Sb\\S0b": "In addition to the k-center greedy, the herding methods [Chen et al. (2012); Welling (2009); Huszr &Duvenaud (2012); Adhikary & Boots (2022)] select datapoints xi so that the distance between the coresetcenter and the full set center is minimized in the embedding space. For efficiency, it is also approximated viagreedy implementation [Chen et al. (2016); Harvey & Samadi (2014)] by adding one sample each time intothe Sb to minimize the distance between two centers (see Alg. 3). Similar to k-center greedy, the herdinggreedy also performs iterative sample selection. However, it differs in that in each step, the distance betweenthe centers of the selected set and the full set is minimized.",
  ": end for7: return Sb": "where sim(, ) denotes the similarity function (e.g., cosine similarity).If the selected Sb can be well-representative of the entire set S, then Sb is assumed of high diversity. Given quality scores defined byEq. 21, the detailed mechanism of QDIT is described in Alg. 5 with greedy approximation. Different fromthe previous sequential setup that respectively prioritizes quality and diversity in two successive steps, QDITadopts a dynamic weighting over the GPT scoring-based quality and the FL-based diversity for selection.",
  "xiSminj=i d(xi, xj) C,(43)": "where C denotes the constant that controls the degree of diversity. A larger C represents the larger diversityof the dataset. The detailed procedure can be found in Alg. 4. It can be seen that datapoints are firstsampled cluster-by-cluster to ensure a more balanced data distribution. Then, a percentile-based selection isperformed on each cluster to select datapoints with easy learning complexity.",
  ": return Sb": "[Liu et al. (2023b)] adopt the quality score-first and diversity-aware data selection method (DEITA), whereall datapoints are first scored and sorted by quality measurement, and then selected by a geometry-basedheuristic criterion (i.e., Repr Filter). Specifically, it considers that for each chosen datapoint in Sb, itskNN 1i (Eq. 30) should be above a certain threshold so that the overall diversity DkNN(Sb) (Eq. 32) can beimproved. As shown in Alg. 6, the quality and complexity of each sample xi are respectively measured bythe trained complexity scoring model C and the quality scoring model Q with prompts pC and pQ. Then,samples with high GCQ are prioritized but only those dissimilar ones can be kept for the diversity of Sb.",
  ": end for18: return Sb": "[Axiotis et al. (2024)] propose a k-means cluster-based sensitivity sampling technique. For each datapoint ina cluster, both its distance to the cluster center and a proxy evaluation loss [Feldman & Langberg (2011)]measured on that cluster center contribute proportional to the probability of being chosen. [Shao et al.(2024)] propose the balanced ClusterClip sampling. It first performs k-means clustering and then sampledatapoints uniformly from each cluster. Different from the uniform sampling, the proposed ClusterClip putsconstraints on the maximum number of each cluster being sampled, and therefore avoids overfitting of smallclusters. [Alcoforado et al. (2024)] comprehensively compare different geometry-based diversity sampling techniquessuch as similarity or distance-based greedy sampling and clustering-based sampling. It proposes threeapproaches to select subsets Sb for human annotation: 1) reverse semantic search, 2) ordered clustering, and3) limited lexical similarity. For the reverse semantic search, two datapoints (xi, xj) that share the least",
  "Bilevel Optimization-based Coreset Sampling": "OverviewThe selection of coreset can also be viewede as a bilevel optimization problem [Colson et al.(2007); Zhang (2024); Sinha et al. (2017); Borsos et al. (2020); Killamsetty et al. (2021b;c); Zhang et al. (2022);Borsos et al. (2024); Pan et al. (2024)] that consists of two loops: 1) the outer loop of optimizing the hardmasks or soft weights for selecting the subset Sb from S; 2) the inner loop of optimizing the model parameters on Sb. Without lose of generalizability, the bilevel optimization with the self-supervised language modelingloss can be written as follows:",
  "NLLA|Qi.(45)": "Technical DetailsThe retrieve method proposed by [Killamsetty et al. (2021c)] takes both labeledand unlabeled datasets into consideration, where the self-supervised loss from the unlabeled set (e.g.,consistency regularization [Xie et al. (2020); Wang et al. (2021c)] and entropy regularization [Zhao et al.(2020b); Grandvalet & Bengio (2004); Erkan & Altun (2010)]) contributes to the inter-level and outer-leveloptimization as well. To improve the robustness, Glister [Killamsetty et al. (2021b)] optimizes the outer-levelcoreset selection on the additionally prepared validation set for the minimized validation loss. [Li et al.(2023e)] further emphasize the role of the validation set in bilevel optimization. It not only computes theloss on the validation set for adversarial training, but also introduces gradient matching [Killamsetty et al.(2021a)] where the gradient of the model on the selected subset Sb should be close to that on the entire S. [Borsos et al. (2024)] reformulate the coreset sampling as a cardinality-constrained bilevel optimizationproblem. It proposes greedy forward selection and first-order methods that apply to any twice differentiablemodels. Variants of the solution for acceleration are extended: 1) binary weights, inverse-hessian-vectorproduct approximations, and batch-wise selection; 2) small proxy models for fast estimation; 3) enforcedsparsity-inducing penalty in the outer loop. The ScaleBiO [Pan et al. (2024)] specifically address the data reweighting problem for large-scale LLMinstruction tuning. It also prepares an extra validation set Sval for the minimization of the outer loop.ScaleBio transforms the bilevel optimization into the single loop framework with an outer-level problem plus aconstraint of the inner-level problem. A multiplier > 0 and a proxy u for optimizing the original inner loop(i.e., model weights ) are introduced into the minimax formulation [Kwon et al. (2023); Lu & Mei (2024)]. In contrast to a fixed budget b, [Xia et al. (2024c)] propose a lexicographic bilevel-optimization method [Borsoset al. (2020); Killamsetty et al. (2021b;c)] where the inner loop optimizes model parameters and the outer",
  "Importance-based Selection": "This section provides the review of methods on importance measurement and selection. By importance wemean the necessity of adding one instruction-response sample into the training set. Due to the pre-trainingnature of LLMs, a wide range of materials have been \"parameterized\" as internal knowledge and thereforeseveral common tasks can be correctly solved without additional fine-tuning. In this case, alignment is notrequired for easy samples but becomes indispensable for difficult ones. The selected datapoints providesupplementary knowledge to activate the pre-trained LLMs on following complex instructions.Generally, theevaluation function q(xi) in the context of importance can be alternatively implemented as: 1) the complexityof the datapoint itself qC that can be perceived by the task definition and the models uncertainty, 2) thecontribution of each datapoint to the overall performance qP that can be reflected in the losses and errorsduring learning, and 3) the degree of gradient matching qG that indicates influential datapoints for effectiveselection. Consequently, we can unify the importance-based measurements as:",
  "Model-based Indicators": "OverviewTo avoid potential confusion, the model-based importance indicators discussed in this sectionare mainly categorized as three kinds: 1) uncertainty-based; 2) reward score-based; and 3) data model-based.Methods that employ training/inference losses, errors (metrics), and gradients are not included despite theirinvolvement of the language model for importance sampling. Technical DetailsInspired from uncertainty indicators, [Siddhant & Lipton (2018); Kung et al. (2023);Nieth et al. (2024)] propose the prompt uncertainty which measures the disagreement betweenmodelresponses on different perturbed versions of the same instruction:",
  "(47)": "where K denotes the number of perturbations and xki is the k-th perturbed prompt. Note that only theinstruction part xi(<t) is perturbed and sent to the model for the following likelihood measurement on theoriginal response xi(j), j = t, t + 1, ..., |xi|. Samples with high prompt uncertainty should be chosen forfine-tuning since the model does not perform consistently on such instructions. [Jiang et al. (2023b)] target at the over-confidence problem of LLMs after instruction tuning [Kadavath et al.(2022)], and propose the CAPE to calibrate the uncertainty with augmented prompt ensembles.They firsttransform all discriminative and generative tasks into multiple-choice problems, and use the LLMs predictedprobabilities over answer choices (e.g., A, B, C) for uncertainty estimation. Then, prompt augmentations areperformed via paraphrasing of the template, permutation of the in-context examples and the answer choices.Multiple predictions over the answer choices are collected with the augmented prompts as inputs, which helpscalibrate the uncertainty in an ensemble manner. Such calibrated uncertainty tells if an instruction-tunedLLM simply memorizes the response to a given prompt rather than truly understanding the instruction.Therefore, it can be used to precisely choose important datapoints with high uncertainty. Apart from the uncertainty, the reward model can also be used beyond quality scorer. Since most of theknowledge and capabilities are acquired during pre-training [Zhou et al. (2024a)], the instruction tuningdatasets are aimed at aligning the behavior of models with human preference and expectations. Therefore,for any given instruction xi, if the generated response is of high quality, then the necessity of fine-tuning onthis instruction is low. Accordingly, xi is deemed as \"unimportant\" and will not be chosen into the subset.In that case, the language model parameterized as is first prompted with xi(<t) to generate the responsexi(t). Then, a reward model parameterized as acts as a necessity evaluation model:Ri = r(xi(<t), xi(t)).(48)",
  "Samples whose necessity score Ri below a pre-determined threshold are selected via Eq. 7, implying that themodel does not own the capabilities to handle xi and requires fine-tuning": "Another series of model-based importance estimation methods are based on datamodels [Ilyas et al. (2022);Park et al. (2023); Jain et al. (2023); Kang et al. (2024); Chhabra et al. (2024); Saunshi et al. (2022); Ye et al.(2024)], where the contribution of each datapoint to the models behavior is estimated. The datamodels canbe implemented in any machine learning model which targets at predicting the influence of each datapoint onthe performance of the trained model [Koh & Liang (2017); Jain et al. (2022); Liu et al. (2024b); Picard et al.(2024); Bae et al. (2024); Covert et al. (2024)]. [Engstrom et al. (2024)] propose to use datamodels to select subsets that maximize the overall performance.Specifically, it chooses the subset Sb S, S = {x1, x2, ..., x|S|} by estimating the loss of the model trained onit. Out of simplicity, the datamodel x can be implemented as a linear model and it learns to approximatethe actual loss via the TARK estimator [Park et al. (2023)]:",
  ",otherwise. , x(1Sb) = Tx 1Sb,(49)": "where Lxj(Si) denotes the loss of the model (trained on Si) on the sample xj. The E(m) is a m-sampleempirical expectation and Lreg(, ) is a regression loss function (e.g., mean squared error). Intuitively, whatthe datamodel x does is to approximate the real loss Lxj(Si) under various compositions of subsets Si Sb.Given any subset Sb, the averaged loss approximated by the datamodel on all xj Seval is calculated on theevaluation set Seval and minimized to find the optimal Sb, |Sb| = b:Sb = arg minSbSLSeval(Sb),",
  "for the minimum loss LSeval": "[Liu et al. (2024b)] also propose a simulation-based [Guu et al. (2023)] linear datamodel that correlates thetraining samples with the validation or test set loss. A featured simulator, namely GPTfluence, models thetraining dynamics (e.g., loss, BLEU and ROUGE scores) across time via an n-th order Markov process. Itextracts representations g(xi), xi S from BERT or GPT, and generates both multiplicative and additivefactors to reflect the influence of any training example on the testing set. The testing performance t at anytime t is affected by: 1) its performance at preceding n times and 2) the current training batch ct:",
  "(51)": "where WT(j), UT(j), W, U are learnable weights which are optimized by minimizing Tt=1(yt t(xk))2 withyt being the ground-truth metric score monitored during training at step t. The , F denotes the Frobeniusinner product. Samples that reduce the evaluation loss the most are selected as influential data. Instead of performing off-line selection, [Yu et al. (2024)] propose MATES where a small datamodel continuouslyselects the most effective subset for the current training of the LLM. The datamodel, like a partner, is updatedalternatively to adapt to the constantly changing preferences of the model under development. Unlike previous datamodels that predict the influence of datapoints via the testing performance of the model,[Xie et al. (2023)] propose the DSIR with importance scores estimated by the distributional resemblance. Itsimply assumes that training samples that resemble the evaluation set are important, and these datapointsshould be selected with higher probability. Given the hashed n-grams features h(xi) Nm of xi, its importancescore wi is calculated as:",
  "Loss and Error-based Coreset Sampling": "OverviewDuring training, samples that contribute more to the total loss or cause worse performanceare considered more important. In the light of this statement, the influence of each datapoint can also bemeasured via the losses and errors for coreset sampling. Compared with the datamodel-based measurementthat estimates individual importance with a specifically designed datamodel, loss and error-based measurementis performed with the same LLM under development. Technical DetailsOne kind of methods that record the errors of each sample during training to estimateimportance is forgetting score or forgetting event [Toneva et al. (2018)]. It counts how many times theforgetting happens with the iteration of training step t. For any given sample xi in a batch B (xi B S),if the previous accuracy acct1isurpasses the current accuracy accti (accti > acct+1i), then the example xiundergoes a forgetting event. Conversely, a learning event occurs if accti < acct+1i. The number of forgettingevents implies whether the sample is difficult and indispensable for training. An example xi is defined asunforgettable if it satisfies:",
  "(53)": "The easy samples with Unforgeti = 1 can be simply discarded and the important subset Sb = {xi|Unforgeti =0, xi S} is selected for training. Recent studies on both pre-training and instruction tuning have investigatedthe effectiveness of using the forgetting score for efficient data pruning [Sorscher et al. (2022); Paul et al.(2021); Zhang et al. (2023a); Jin & Ren (2024a); Maini et al. (2022)]. In contrast to the term \"forgetting\", researchers introduce the concept \"memorization\" [Feldman (2020);Tirumala et al. (2022); Antoniades et al. (2024)] for analysis on the generalization of deep models [Zhanget al. (2021)]. The memorization of training samples is necessary for reducing close-to-optimal generalizationerror especially when a long-tailed disttribution is observed for the training set [Feldman (2020)]. Specifically,the amount of label memorization on the instruction-response pair (xi(<t), xi(t)) is defined as follows:",
  "Gradient-based Coreset Sampling": "OverviewSince gradients directly affect the optimization of language models, two kinds of intuitivemethods for data selection are presented: 1) gradient matching [Zhao et al. (2020a); Killamsetty et al. (2021a);Jiang et al. (2023d); Zhao & Bilen (2023); Du et al. (2024); Balles et al. (2022); Zhang et al. (2024a)], i.e.,the gradients of the entire set S being approximated by the weighted gradients of the subset Sb, and 2)gradient-based influence [Pruthi et al. (2020); Brophy et al. (2023); Koh & Liang (2017); Basu et al. (2020);Picard et al. (2024); Alaa & Van Der Schaar (2020)], i.e., the influence of each sample xi on a testingdatapoint xt being measured by upweighted gradient multiplication. Specifically, the gradient matching aimsto minimize the difference below:",
  "T H1 NLLA|Qi.(58)": "The importance indicator InflFij approximately measures the change of the loss on xj when xi is removedfrom the training set. To expedite the computation of Hessian matrix for large models, a combination ofHessian-vector product and optimization techniques are developed [Pearlmutter (1994); Nilsen et al. (2019);Mathieu & LeCun (2014); Agarwal et al. (2016); Shewchuk et al. (1994)]. Another kind of influence score is defined as the expected gradient norm (GraNd score) [Paul et al. (2021);Kirsch (2023); Bther et al. (2023)], where the GraNd score controls the contribution of a training sample tothe change of the loss.GraNdi = ENLLA|Qi2(59)",
  "Experiments [Paul et al. (2021)] suggest that the GraNd score (Eq. 59) can be well approximated by EL2Nscore (Eq. 13) for efficient data pruning": "Technical Details[Xia et al. (2024a)] propose to find the most influential training data that resemble thetesting set the most via low-rank gradient similarity search. [Tan et al. (2024a)] introduce the moving-one-sample-out (MoSo) by pinpointing the least informative samples via gradient-based influence assessment. Toavoid the costly retraining procedure by iteratively moving one sample out, a gradient-based approximator isproposed to select samples whose gradients are consistently aligned with the average gradients of the entiretraining set. For the detailed definition of distance measure of Eq. 56, [Everaert & Potts (2023)] exploit the KL-divergenceto measure the difference between the selected subset and the testing set. Note that here the objectiveis to approach the distribution of the testing set rather than the entire training set. [Killamsetty et al.(2021a)] speed up the gradient matching between the selected dataset and the validation set via an orthogonalmatching pursuit algorithm. [Lin et al. (2024)] apply gradient-based influence scores on recommendationdatasets for effective LLM instruction tuning. [Schioppa et al. (2021)] choose a different way [Arnoldi (1951)]to accelerate the computation of the inverse Hessian matrix in Eq. 57 and successfully scales up the influencescoring for LLMs with several hundreds of millions of parameters. [Grosse et al. (2023)] use the influencefunctions to study the generalization properties of LLMs To scale up influence functions for LLMs up to 52billions, an approximation technique via Eigenvalue-corrected Kronecker-Factored Approximate Curvature(EK-FAC) [George et al. (2021)] to efficiently find the most influential samples to the pre-trained LLMs overmaths and programming abilities, cross-lingual generalization, and role-playing behavior. [Zhao et al. (2021)]condense the datasets into small informative synthetic samples where the gradients of the model on thesynthetic data are matching those on the real data of the entire training set. RemarkThe gradient-based coreset sampling techniques are highly dependent on the LLMs under develop-ment, where the gradients describe the models inherent knowledge and uncertainty about each datapoint.Despite the precision of gradient-based selection methods, it is noted that approximation is unavoidable forapplication on LLMs. The efficiency and accuracy of various approximation techniques should be considered.",
  "Results and Discussions": "In this section, we classify different methods according to their respective emphases, and then demonstratetheir effectiveness in the selection of \"high-standard\" datapoints for instruction tuning. First, we provideexplanations on the classification and selection of recent instruction tuning methods. Then, we summarizethe representative methods with their data statistics in Tab. 1. Finally, we provide the detailed experimentalresults of each method respectively under our structures of quality, diversity, and importance.",
  "LESS [Xia et al. (2024a)]Mixed(FLAN v2+Dolly+OpenAssistant+COT)270K": "evidence: 1) the data characteristics emphasized in the motivation of their method development, and 2) theoptimization objectives organized in this survey that are closest to those in their pipelines. For instance, theconcept of diversity is stressed in DEITA [Liu et al. (2023b)] where a similarity-based filtering step is setto remove highly duplicated datapoints. Therefore, it falls under the category of diversity-based selectionmethods. DSIR [Xie et al. (2023)], on the other hand, directly estimates the importance of datapointsby matching the distribution between the selected subset and the target evaluation set. It is noted thatmany existing methods design compound, multi-facet selection criteria that strike a balance between quality,diversity, and importance. In this case, we choose the most representative methods under each category tohighlight their corresponding facets without misinterpreting their mechanisms. Furthermore, we select themost recent methods that exploit open-sourced LLMs for verifying the effectiveness of their proposed dataselection techniques. Such accessibility to public LLMs avoids the privacy issues and inference costs broughtby requesting API services from close-sourced proprietary models. QualityThe quality of data directly impacts the effectiveness of model training. Quality control measuresinclude data scoring, quality assessment, and more. In Tab. 2, we have summarized the results of differentmethods focusing on data quality.In the table, we list the data used by different methods and theproportion/size of the data selected. It can be seen that the method of selecting data based on quality canmatch the results of training with full data even under the data-poor regime. They are also superior tothe results of randomly selecting a subset from the original dataset. In the table, WK stands for WorldKnowledge, CR stands for Commonsense Reasoning, LU stands for Language Understanding, SPS stands forSymbolic Problem Solving, and RC stands for Reading Comprehension. DiversityData engineering enhances the generalization ability of models by improving the diversity ofdatasets. Such improved diversity may be implemented via encompassing data from different sources, withvarying features, and of distinct distributions. Researches indicate that it is insufficient to merely selectdatasets that are similar to the target data from the downstream tasks. Tab. 3 demonstrates the importanceof diversity in data selection. Compared with random selection and uniform selection, the scheme of selecting",
  "%0.3290.2760.3660.085": "data with diversity criteria is superior. In addition, compared to only selecting high-quality data, the criteriathat combine quality and diversity can achieve better performance than simply selecting high-quality data. ImportanceIt is non-trivial to accurately identify and utilize datapoints that significantly affect modelperformance As shown in Tab. 4, the importance-based data selection approaches often maximize the finalperformance by integrating the data selection and model optimization processes. To address the challengesof potential huge computation overhead brought by the implementation framework, they propose to useinstance-wise data impact and propose efficient parameterization. Moreover, by performing importanceresampling in the feature space that depicts geometricstructures, they select examples of both highimportance and similarityto the target distribution, thereby enhancing the performance on the targettasks. Existing work has confirmed that the importance sampling-based approach can effectively improve theperformance on the target tasks with enhanced capabilities of LLMs. Hybrid SelectionIt is noteworthy that many data selection methods such as LIFT [Xu et al. (2023b)],FL [Bhatt et al. (2024)], DEITA [Liu et al. (2023b)], and QDIT [Bukharin & Zhao (2023)], attempted tocombine multiple aspects of data assessment into their selection pipelines. Most of them emphasize the overallassessment of instruction data in that the definitions of \"good data\" are varied under different scenariosin the era of LLMs. A unitary measurement would inevitably cause a biased selection of datapoints andthereafter leads to the degraded performance of LLMs. Technically, existing hybrid selection methods can becategorized into: 1) parallel setups, and 2) sequential setups. For the former, an operation of maximizationor weighted sum is performed on scores or proxy indicators (i.e., losses and gradients) from multiple aspects(e.g., quality and diversity) [Xu et al. (2023b); Bhatt et al. (2024); Bukharin & Zhao (2023)]. For the latter, asequential setup of quality-, diversity-, or importance-oriented selection techniques is performed step-by-stepfor hierarchical, stratified filtering [Liu et al. (2023b)]. Comparatively, the parallel setups allow dynamictrade-offs between multiple data aspects by adjusting the aggregation operations. The sequential setups, onthe other hand, fail to retrieve the candidates that are filtered out in the preceding quality control stepseven if those candidates are of high importance or variety. Therefore, it would be preferred to develophybrid assessment techniques that simultaneously weigh quality, diversity, and importance. Furthermore,we notice that the importance-based assessment is often overlooked in existing hybrid approaches, implying",
  "Distinctions and ConnectionsThe methods that are respectively reported in Tabs. 2, 3, and 4 sharethe similar philosophy of data assessment and selection but differ in the detailed implementations": "For the quality-based selection, most of existing methods like IFD [Li et al. (2023a)], PPL [Ankner et al.(2024)], AutoDS [Zhang et al. (2024c)], and QuRator [Wettig et al. (2024)] mainly use the model-basedindicators to measure the quality of each datapoint. However, their indicators are derived from differentperspectives such as the perplexity of LLMs and the predicted quality score of a regression model. Both theAlpagasus [Chen et al. (2023b)] and BSDetector [Chen & Mueller (2024)] mainly employ ChatGPT scoringfor quality evaluation. Alpagasus directly prompts GPT to score and filter out datapoints, but BSDetectorconsiders both the consistency of the generated responses from GPT and the quality of the ground-truthresponse. On the other hand, LIFT [Xu et al. (2023b)], InstructionMining [Cao et al. (2023)], and FL [Bhattet al. (2024)] develop composite methods that involve hand-crafted or model-based indicators, together withGPT scoring, for quality verification of the selected datapoints. For the diversity-based selection, both DEITA [Liu et al. (2023b)] and ClusterClip [Shao et al. (2024)] adoptthe geometry-based measurement where the geometric structure of datapoints is maintained via clustering.DEITA measures the individual-level relationship between one datapoint and its closest neighbor for samplingwhile ClusterClip simply performs the cluster-level uniform sampling with the quantity limit. QDIT [Bukharin& Zhao (2023)] performs the iterative selection where each step only one most promising datapoint fromthe remaining dataset is selected to maximize the overall diversity of the selected subset. Despite the factthat DQ [Zhou et al. (2023)] also exploits iterative coreset sampling, it first divides the entire dataset intomultiple subset bins and performs sampling across bins for aggregation and deduplication. For the importance-based selection, DsDm [Engstrom et al. (2024)] and MATES [Yu et al. (2024)] employmodel-based indicators to estimate the influence with datamodels. However, these two methods choosedifferent implementations of datamodels. DsDm develops a linear model and approximates the loss of eachdatapoint via the TARK estimator while MATES directly leverages a small language model to learn to predictthe individual influence of each datapoint. DSIR [Xie et al. (2023)], on the contrary, bypasses the need tomaintain a specific datamodel. Instead, it estimates the importance of each datapoint by comparing thedistributions of the entire selected subset with those of the validation set. Both Skill-it [Chen et al. (2024b)]and LESS [Xia et al. (2024a)] pinpoint the samples that resemble the most to the target set. Skill-it performssampling via minimizing the evaluation loss while LESS utilizes the gradient similarity as a proxy.",
  "Benchmarking Instruction-Tuned LLMs": "There exists a gap between the effectiveness of data selection and the reported performance onbenchmarks.In existing researches, the ablation studies on the effectiveness of assessment and selectionmethods are often carried out by comparing the performance of LLMs fine-tuned with the selected andthe full dataset. However, for coreset sampling methods that use losses and gradients as proxies for dataquality, the downstream performance may not be positively correlated with the selection effectiveness. Thereason behind is that the evaluation loss itself [Yang et al. (2022); Hoffmann et al. (2022); Kaplan et al.(2020)] is not informative enough for universal estimation of benchmark performance.[AI@Meta (2024)]demonstrates that the correlation between the negative log-likelihood loss on downstream tasks and theaccuracy metrics should be modeled task-by-task and model-by-model. In the light of this statement, it isimpractical to simply count on losses or gradients to pinpoint the most beneficial data for improving thedownstream performance, let alone methods that try to predict the loss based on various indicators [Caoet al. (2023)]. Furthermore, even if the metrics are exhaustively computed for the selection of each sample,",
  "LESS 5%0.6180.6030.560": "the gains brought by one sample might be limited in few tasks. Therefore, to comprehensively reflect theeffectiveness of sample selection, the evaluation of instruction-tuned models should be accompanied by thespecialised evaluation of the selected datapoints. For the former, all sorts of evaluation strategies have beenproposed to precisely evaluate the LLMs [Melis et al. (2017); Chang et al. (2024); Xu et al. (2022); Liang et al.(2022)]. The multiple-choice QA tasks are not enlightening in judging if the instruction-tuned model trulyunderstands the problem rather than simply memorizing the answer choices given the instruction context.For the later, a benchmark for documenting and comparing the statistics of the selected instruction-responsepairs in terms of quality, diversity, and importance needs to be constructed in the future. It would benefitthe task-wise customized data selection according to the statistical indicators on such a benchmark. Test set contamination should be considered during instruction data selection.For instructiontuning on publicly released pre-trained LLMs, it cannot be too careful to check the potential data leakagewhere the testing instructions are already modeled during pre-training [Rae et al. (2021); Li et al. (2023b);Magar & Schwartz (2022); Carlini et al. (2019); Marone & Van Durme (2024); Deng et al. (2023); Cao et al.(2024); Jiang et al. (2024b); Magar & Schwartz (2022)]. To improve the performance of pre-trained modelson downstream tasks, datapoints (i.e., instruction-like conversations) are already added into the annealingphase of pre-training [AI@Meta (2024); Bilibili (2024); Yang et al. (2024); Bai et al. (2023)]. Therefore,potential risks of data contamination are raised for benchmarking the fine-tuned LLMs. To avoid the negativeeffect of data leakage on evaluation of the data assessment and selection, it is encouraged to follow [Li et al.(2023a)] to adopt the pre-trained model for experiencing the datapoints before fine-tuning. If the modelexhibits overfitting behaviors (i.e., accurately generating the instruction part or producing the same answerchoice even with permutation on the choice letters), data contamination is likely to exist and thereafter thetesting set should be replaced. For future studies, it would be more reliable to decouple the evaluation ofdata selection and that of fine-tuned LLMs, where the performance consistency between these two evaluationresults can be analyzed to rule out the possibility of contamination.",
  "Unveiling the Definitions of Good Data": "What signifies the most a good datapoint remains an open question.Unfortunately, there existsno unified criteria on discriminating \"good\" instructions from \"bad\" ones. Essentially, the definitions on thegeneral data \"quality\" differ from task to task and domain to domain [Evans & Murshudov (2013); Flach(2012); Albalak et al. (2024)]. Although existing quality measurement methods can be categorized in terms ofquality, diversity, and importance under the present study, they all exhibit more or less ad-hoc properties inmethodology. First, studies on instruction tuning are often targeted at improving the performance of LLMson downstream task. No matter whether these tasks are of general-purpose (e.g., common NLP tasks onleaderboard [Myrzakhan et al. (2024); Wolf et al. (2019)]) or domain-specific applications, such task-orientateddata selection itself is only a \"proxy\" for exploring the underlying \"quality\" measurement. Especially forcoreset sampling methods that directly employ the evaluation set or testing set for distribution matching orimportance estimation, instructions that resemble the most to the testing set or bring about performancegains are judged as \"good\" data. However, such \"good\" data cannot be easily transferred to another LLM ofcompletely different architecture and parameters. Each time the entire pipeline has to be enforced for a noveltask, making it difficult to accumulate universally-acknowledged high-quality data for archiving. Second, eachmethod has an individual quality evaluation system and very few of them ever tried to justify their designand interpret the philosophy behind. It is difficult to validate whether certain component of the selectionpipeline can be replaced or removed for better serving a new task-of-interest. Accordingly, further academic explorations include: 1) to present a more unified, generally applicabledefinitions on \"good\" datapoints in terms of fine-grained aspects, and 2) to improve interpretability andexplanability of the selection pipeline beyond empirical design. The expected model behavior retrospectively determines the trade-off between quality, diversity,and importance for data selection.The three aspects we used to categorize data assessment methods areactually overlapping with each other, where the \"boundary\" between two measuring dimension is often hardto explicitly defined. Under such circumstance, the definition of good data can be perceived as the weighted,biased mixture of quality, diversity, and importance. Existing methods are not flexible in dynamicallyadjusting the mixing weights to adapt to different downstream tasks. Instead, their priority order of thethree dimensions is implicitly encoded into the selection of instructions. For instance, [Liu et al. (2023b)]emphasizes quality and importance equally by first establishing the relative ranking of all samples in bothquality and complexity. The subset is formed by consecutively selecting the top-ranked samples in sequence,with diversity intervened via ruling out heavily homogenized examples. Such hard-coded, greedy treatmentto quality, diversity, and importance is not applicable to scenarios where the behavior of LLMs is expected tocater to varied preference. In general, the data assessment and selection methods that can adapt to the model requirement under differentapplication scenarios are yet to be systematically developed. For generation tasks like role playing andcreative writing, the preferred instruction tuning datapoints should be distinct from those for discriminativetasks like named entity recognition and sentiment analysis.",
  "Scaling Up Datasets": "The optimal scale of the selected subset becomes less explicit with the expansion of datasets.In the analysis of the disadvantages in exploiting the entire instruction dataset for alignment, putting asidethe issue of long training time, we notice that the performance of fine-tuning the entire dataset might not bethe optimal. There often exists a critical point of the best selection proportion, and such proportion variesfrom dataset to dataset. When more instruction datasets from diverse domains and tasks are incorporated,it becomes more difficult to nail down the best selection proportion for three main reasons. First, a largeproportion of noise exists in the open datasets and few noisy samples can already cause tremendous negativeimpact on performance [Song et al. (2022)]. During the pre-processing of instruction dataset, noise can beunintentionally introduced in instruction preparation (e.g., missing context or system prompt), responsegeneration (e.g., unverified or mismatched answers), format wrapping (e.g., invalid JSON and unresolved code),and text augmentation (e.g., synonym replacement and reorder of words). Second, for specialized tasks in",
  "Scaling Up LLMs": "The cost-efficiency of data assessment and selection diminishes with larger LLMs involved in thepipeline.The model-based indicators and coreset sampling methods often require the language model itselfto be involved for computation of metrics [Li et al. (2023a)], losses [Chen et al. (2024b)], and gradients [Xiaet al. (2024a)]. With the increase of model sizes, it becomes more and more cumbersome to implement theentire pipeline for quality measurement and selection. To expedite the process, one important direction forfuture study is to develop proper efficient proxy models. Small proxy models have been successfully appliedin accelerated fine-tuning of language models [Hoffmann et al. (2022); Liu et al. (2024a)], filtering datasetsby perplexity [Ankner et al. (2024)], intervention on retrieval-augmented generation [Tan et al. (2024b)],and performance prediction [Anugraha et al. (2024); Ngu et al. (2024)]. Such proxy models often share thesame architecture design with the LLMs under development but own much less parameters. The scaling",
  "law [Kaplan et al. (2020)] confirms the expected consistent behavior between data quantity and model scale,providing practical guidelines on the development of such proxy LLMs": "On the other hand, under the context of data evaluation, it calls upon on rethinking of traditional machinelearning techniques such as efficient optimization tricks and dimensionality reduction approaches. For example,in the assessment of loss-based datapoint influence [Feldman & Zhang (2020)], the exhaustive measurementon the marginal performance by moving-each-sample-out and model re-training can be simply approximatedby iterative batch-wise sampling tricks with a greedy principle behind. For efficient assessment, PCA [Xuet al. (2023b)] and random projection [Xia et al. (2024a); Park et al. (2023)] are popular choices for obtaininglow-rank representations of embeddings and gradients, which facilitates not only metric computation but alsostoring of datapoints. The marginal benefits of instruction tuning diminishes with increasing size of LLMs forknowledge supplement.Recent studies on the effectiveness of instruction tuning in injecting task-specificor domain-specific knowledge into LLMs [Shi & Lipani (2023); Goyal et al. (2023); Zhang et al. (2024b);Yldz et al. (2024)] show that the stand-alone instruction tuning might not be the most appropriate method.Compared with strategies like continual pre-training [Cossu et al. (2024); Ke (2024); Cossu et al. (2024)] andinstruction modeling [Lou et al. (2024); Cheng et al. (2024); Wang et al. (2022); Xu et al. (2024); Shi et al.(2024)], instruction tuning counts the response sequences for loss computation without sufficient perceptionof instruction context. For specialized domains like medicine, finance, and laws, if the pre-trained LLMs arein lack of the prerequisite knowledge, the instruction tuning cannot properly activates the parameterized\"memory\" for alignment but only causes overfitting of the given prompt. In that case, the benefits of dataselection are limited with poor generalizability. Another noteworthy phenomenon in data assessment and selection studies is that due to limited budgetsof computing resources, most of the experiments are performed on LLMs of small and moderate size (e.g.,less than 7B) to validate the effectiveness of the quality measurement and the selection strategy. Smallpre-trained LLMs, by their nature of small parameter size, are more sensitive to the instruction datasetsduring fine-tuning or continual learning [Schick & Schtze (2020); Yldz et al. (2024)]. They exhibit themost significant rates of both forggeting (old knowledge) and learning (new knowledge). In the light of suchstatement, small LLMs tend to sacrifice the task-irrelevant knowledge in return for rapid adaptation towardsnovel domains and tasks. The selected datasets by various quality measures can impose immediate effecton the parameters of small LLMs, but may weaken on those of huge ones. It remains unknown whetherthe same quality measurement and data selection pipeline can achieve similar performance gains on bothsmall and large LLMs. For future research of data assessment and selection, extensive experiments arerequired to validate their efficiency on huge LLMs (e.g., 70B and 405B) [AI@Meta (2024)] and LLMs ofmixture-of-experts (MoE) architectures (e.g., Mixtral 8x22B) [Jiang et al. (2024a)]. In consideration of the pre-training corpus, extremely large LLMs already experienced a vast amount ofmulti-lingual, multi-domain web texts during pre-training, and therefore the priority of the dimensions indata assessment (i.e., quality, diversity, and importance) differs from small LLMs. The association betweenthe model scale and the data selection criteria is yet to be studied.",
  "Validating the Bias and Fairness of Instruction-tuned LLMs": "As mentioned in the Sec. 1.3, most existing data selection methods fail to specifically study the dataset biasfor consideration of fairness. It is noted that for domain-specific tasks such as solving programming and mathsproblems, the data bias might not be that severe. However, for general question-answering, recommendation,and creative writing tasks, even ChatGPT could produce biased answers that are intolerable in domains likeeducation [Doan et al. (2024); Zhang et al. (2023b); Chisca et al. (2024); Gao et al. (2024)]. To improve thefairness of LLMs, it is necessary to take the measurement of data bias into serious consideration. Futurework includes: 1) the integration of embedding-based (e.g., WEAT [Caliskan et al. (2017)], SEAT [May et al.(2019)]), probability-based (e.g., DisCo [Webster et al. (2020)], CrowS-Pairs Score [Nangia et al. (2020)]), andgenerated texts-based (e.g., Co-Occurrence Bias Score [Bordia & Bowman (2019)], Full Gen Bias [Smith et al.(2022)]) techniques for fairness evaluation of datapoints [Gallegos et al. (2024)], and 2) the construction of",
  "Conclusion": "In this study, we have thoroughly examined the state-of-the-art data assessment and selection methods forinstruction tuning of LLMs. The present review presents a unified organization and categorizes these methodsin terms of measuring dimensionality: quality, diversity, and importance. In each dimensionality, we outlinethe representative strategies in details and describe the factors to consider when selecting data for instructiontuning. Furthermore, we report the performance of typical data selection methods and provide discussions onthe comparison between these methods. Last but not least, the existing challenges and potential solutions forfuture studies are summarized in hope for benefitting the research community.",
  "Amro Abbas, Kushal Tirumala, Dniel Simig, Surya Ganguli, and Ari S Morcos. Semdedup: Data-efficientlearning at web-scale through semantic deduplication. arXiv preprint arXiv:2303.09540, 2023": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, DiogoAlmeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprintarXiv:2303.08774, 2023. Alessandro Achille, Michael Lam, Rahul Tewari, Avinash Ravichandran, Subhransu Maji, Charless C Fowlkes,Stefano Soatto, and Pietro Perona. Task2vec: Task embedding for meta-learning. In Proceedings of theIEEE/CVF international conference on computer vision, pp. 64306439, 2019.",
  "AI@Meta. Llama 3 model card. 2024. URL": "Ahmed Alaa and Mihaela Van Der Schaar. Discriminative jackknife: Quantifying uncertainty in deep learningvia higher-order influence functions. In International Conference on Machine Learning, pp. 165174. PMLR,2020. Alon Albalak, Yanai Elazar, Sang Michael Xie, Shayne Longpre, Nathan Lambert, Xinyi Wang, NiklasMuennighoff, Bairu Hou, Liangming Pan, Haewon Jeong, et al. A survey on data selection for languagemodels. arXiv preprint arXiv:2402.16827, 2024. Alexandre Alcoforado, Thomas Palmeira Ferraz, Lucas Hideki Okamura, Israel Campos Fama, Arnold MoyaLavado, Brbara Dias Bueno, Bruno Veloso, and Anna Helena Reali Costa. From random to informed dataselection: A diversity-based approach to optimize human annotation and few-shot learning. arXiv preprintarXiv:2401.13229, 2024. Zachary Ankner, Cody Blakeney, Kartik Sreenivasan, Max Marion, Matthew L Leavitt, and Mansheej Paul.Perplexed by perplexity: Perplexity-based data pruning with small reference models. arXiv preprintarXiv:2405.20541, 2024.",
  "Yves Bestgen. Measuring lexical diversity in texts: The twofold length problem. Language Learning, 2023": "Gantavya Bhatt, Yifang Chen, Arnav M Das, Jifan Zhang, Sang T Truong, Stephen Mussmann, Yinglun Zhu,Jeffrey Bilmes, Simon S Du, Kevin Jamieson, et al. An experimental design framework for label-efficientsupervised finetuning of large language models. arXiv preprint arXiv:2401.06692, 2024. Stella Biderman, Usvsn Prashanth, Lintang Sutawika, Hailey Schoelkopf, Quentin Anthony, ShivanshuPurohit, and Edward Raff. Emergent and predictable memorization in large language models. Advances inNeural Information Processing Systems, 36, 2024.",
  "Jonathan Brophy, Zayd Hammoudeh, and Daniel Lowd. Adapting and evaluating influence-estimationmethods for gradient-boosted decision trees. Journal of Machine Learning Research, 24(154):148, 2023": "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.Advances in neural information processing systems, 33:18771901, 2020. Quang Vu Bui, Karim Sayadi, Soufian Ben Amor, and Marc Bui. Combining latent dirichlet allocationand k-means for documents clustering: effect of probabilistic based distance measures. In IntelligentInformation and Database Systems: 9th Asian Conference, ACIIDS 2017, Kanazawa, Japan, April 3-5,2017, Proceedings, Part I 9, pp. 248257. Springer, 2017.",
  "KHR Chan, Y Yu, C You, H Qi, J Wright, and YR Ma. a white-box deep network from the principle ofmaximizing rate reduction. arxiv. 2021. arXiv preprint arXiv:2105.10446, 2021": "Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi,Cunxiang Wang, Yidong Wang, et al. A survey on evaluation of large language models. ACM Transactionson Intelligent Systems and Technology, 15(3):145, 2024. Daoyuan Chen, Yilun Huang, Zhijian Ma, Hesen Chen, Xuchen Pan, Ce Ge, Dawei Gao, Yuexiang Xie,Zhaoyang Liu, Jinyang Gao, et al. Data-juicer: A one-stop data processing system for large languagemodels. In Companion of the 2024 International Conference on Management of Data, pp. 120134, 2024a. Hao Chen, Yiming Zhang, Qi Zhang, Hantao Yang, Xiaomeng Hu, Xuetao Ma, Yifan Yanggong, and JunboZhao. Maybe only 0.5% data is needed: A preliminary exploration of low training data instruction tuning.arXiv preprint arXiv:2305.09246, 2023a.",
  "Jiuhai Chen and Jonas Mueller. Automated data curation for robust language model fine-tuning. arXivpreprint arXiv:2403.12776, 2024": "Lichang Chen, Shiyang Li, Jun Yan, Hai Wang, Kalpa Gunaratna, Vikas Yadav, Zheng Tang, Vijay Srinivasan,Tianyi Zhou, Heng Huang, et al. Alpagasus: Training a better alpaca with fewer data. arXiv preprintarXiv:2307.08701, 2023b. Mayee Chen, Nicholas Roberts, Kush Bhatia, Jue Wang, Ce Zhang, Frederic Sala, and Christopher R.Skill-it! a data-driven skills framework for understanding and training language models. Advances inNeural Information Processing Systems, 36, 2024b.",
  "Daixuan Cheng, Yuxian Gu, Shaohan Huang, Junyu Bi, Minlie Huang, and Furu Wei. Instruction pre-training:Language models are supervised multitask learners. arXiv preprint arXiv:2406.14491, 2024": "Anshuman Chhabra, Peizhao Li, Prasant Mohapatra, and Hongfu Liu. \" what data benefits my classifier?\"enhancing model performance and interpretability through influence-based data selection. In The TwelfthInternational Conference on Learning Representations, 2024. Andrei-Victor Chisca, Andrei-Cristian Rad, and Camelia Lemnaru. Prompting fairness: Learning promptsfor debiasing large language models. In Proceedings of the Fourth Workshop on Language Technology forEquality, Diversity, Inclusion, pp. 5262, 2024.",
  "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectionaltransformers for language understanding. arXiv preprint arXiv:1810.04805, 2018": "Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding Hu, Zhiyuan Liu, Maosong Sun, andBowen Zhou. Enhancing chat language models by scaling high-quality instructional conversations. arXivpreprint arXiv:2305.14233, 2023. Marcel Dix, Gianluca Manca, Kenneth Chigozie Okafor, Reuben Borrison, Konstantin Kirchheim, DivyasheelSharma, Kr Chandrika, Deepti Maduskar, and Frank Ortmeier. Measuring the robustness of ml modelsagainst data quality issues in industrial time series data. In 2023 IEEE 21st International Conference onIndustrial Informatics (INDIN), pp. 18. IEEE, 2023. Thang Viet Doan, Zichong Wang, Nhat Nguyen Minh Hoang, and Wenbin Zhang. Fairness in large languagemodels in three hours. In Proceedings of the 33rd ACM International Conference on Information andKnowledge Management, pp. 55145517, 2024. Jesse Dodge, Gabriel Ilharco, Roy Schwartz, Ali Farhadi, Hannaneh Hajishirzi, and Noah Smith. Fine-tuningpretrained language models: Weight initializations, data orders, and early stopping. arXiv preprintarXiv:2002.06305, 2020. Guanting Dong, Keming Lu, Chengpeng Li, Tingyu Xia, Bowen Yu, Chang Zhou, and Jingren Zhou. Self-playwith execution feedback: Improving instruction-following capabilities of large language models. arXivpreprint arXiv:2406.13542, 2024. Hanze Dong, Wei Xiong, Deepanshu Goyal, Yihan Zhang, Winnie Chow, Rui Pan, Shizhe Diao, JipengZhang, Kashun Shum, and Tong Zhang. Raft: Reward ranked finetuning for generative foundation modelalignment. arXiv preprint arXiv:2304.06767, 2023. Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou, andHsiao-Wuen Hon. Unified language model pre-training for natural language understanding and generation.Advances in neural information processing systems, 32, 2019. Wei Dong, Charikar Moses, and Kai Li. Efficient k-nearest neighbor graph construction for generic similaritymeasures. In Proceedings of the 20th international conference on World wide web, pp. 577586, 2011.",
  "Thomas Franois. La lisibilit computationnelle: un renouveau pour la lisibilit du franais langue premireet seconde? ITL-International Journal of Applied Linguistics, 160(1):7599, 2010": "Thomas Franois. Les apports du traitement automatique du langage la lisibilit du franais langue trangre.PhD thesis, Ph. D. thesis, Universit Catholique de Louvain. Thesis Supervisors: Cdrick . . . , 2011. Thomas Franois and Cdrick Fairon. An ai readability formula for french as a foreign language. In Proceed-ings of the 2012 joint conference on empirical methods in Natural Language Processing and computationalnatural language learning, pp. 466477, 2012. Thomas Franois and Eleni Miltsakaki. Do nlp and machine learning improve traditional readability formulas?In Proceedings of the First Workshop on Predicting and Improving Text Readability for target readerpopulations, pp. 4957, 2012.",
  "Amirata Ghorbani and James Zou. Data shapley: Equitable valuation of data for machine learning. InInternational conference on machine learning, pp. 22422251. PMLR, 2019": "Sreyan Ghosh, Chandra Kiran Reddy Evuru, Sonal Kumar, Deepali Aneja, Zeyu Jin, Ramani Duraiswami, Di-nesh Manocha, et al. A closer look at the limitations of instruction tuning. arXiv preprint arXiv:2402.05119,2024. Sachin Goyal, Ananya Kumar, Sankalp Garg, Zico Kolter, and Aditi Raghunathan. Finetune like you pretrain:Improved finetuning of zero-shot vision models. In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition, pp. 1933819347, 2023.",
  "Hans-Rolf Gregorius and Elizabeth M Gillet. Generalized simpson-diversity. Ecological Modelling, 211(1-2):9096, 2008": "Roger Grosse, Juhan Bae, Cem Anil, Nelson Elhage, Alex Tamkin, Amirhossein Tajdini, Benoit Steiner,Dustin Li, Esin Durmus, Ethan Perez, Evan Hubinger, Kamile Lukoiute, Karina Nguyen, Nicholas Joseph,Sam McCandlish, Jared Kaplan, and Samuel R. Bowman. Studying large language model generalizationwith influence functions, 2023. URL Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio Csar Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi,Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, et al. Textbooks are all you need.arXiv preprint arXiv:2306.11644, 2023.",
  "Robert Gunning. The technique of clear writing. (No Title), 1952": "Chengcheng Guo, Bo Zhao, and Yanbing Bai. Deepcore: A comprehensive library for coreset selection indeep learning. In International Conference on Database and Expert Systems Applications, pp. 181195.Springer, 2022. Nitin Gupta, Shashank Mujumdar, Hima Patel, Satoshi Masuda, Naveen Panwar, Sambaran Bandyopadhyay,Sameep Mehta, Shanmukha Guttula, Shazia Afzal, Ruhi Sharma Mittal, et al. Data quality for machinelearning tasks. In Proceedings of the 27th ACM SIGKDD conference on knowledge discovery & data mining,pp. 40404041, 2021. Kelvin Guu, Albert Webson, Ellie Pavlick, Lucas Dixon, Ian Tenney, and Tolga Bolukbasi. Simfluence:Modeling the influence of individual training examples by simulating training runs.arXiv preprintarXiv:2303.08114, 2023.",
  "Ferenc Huszr and David Duvenaud. Optimally-weighted herding is bayesian quadrature. arXiv preprintarXiv:1204.1664, 2012": "Adam Ibrahim, Benjamin Thrien, Kshitij Gupta, Mats L Richter, Quentin Anthony, Timothe Lesort,Eugene Belilovsky, and Irina Rish. Simple and scalable strategies to continually pre-train large languagemodels. arXiv preprint arXiv:2403.08763, 2024. Abiodun M Ikotun, Absalom E Ezugwu, Laith Abualigah, Belal Abuhaija, and Jia Heming. K-meansclustering algorithms: A comprehensive review, variants analysis, and advances in the era of big data.Information Sciences, 622:178210, 2023.",
  "Andrew Ilyas, Sung Min Park, Logan Engstrom, Guillaume Leclerc, and Aleksander Madry. Datamodels:Predicting predictions from training data. arXiv preprint arXiv:2202.00622, 2022": "Bogdan Ionescu, Mihai Lupu, Maia Rohm, Alexandru Lucian Gnsca, and Henning Mller. Datasets column:diversity and credibility for social images and image retrieval. ACM SIGMultimedia Records, 9(3):77,2018. Itay Itzhak, Gabriel Stanovsky, Nir Rosenfeld, and Yonatan Belinkov. Instructed to bias: Instruction-tunedlanguage models exhibit emergent cognitive bias. Transactions of the Association for ComputationalLinguistics, 12:771785, 2024. Saachi Jain, Hadi Salman, Alaa Khaddaj, Eric Wong, Sung Min Park, and Aleksander Mdry. A data-basedperspective on transfer learning. In Proceedings of the IEEE/CVF Conference on Computer Vision andPattern Recognition, pp. 36133622, 2023.",
  "Hongjie Jia, Shifei Ding, Xinzheng Xu, and Ru Nie. The latest research progress on spectral clustering.Neural Computing and Applications, 24:14771486, 2014": "Ruoxi Jia, David Dao, Boxin Wang, Frances Ann Hubis, Nick Hynes, Nezihe Merve Grel, Bo Li, Ce Zhang,Dawn Song, and Costas J Spanos. Towards efficient data valuation based on the shapley value. In The22nd International Conference on Artificial Intelligence and Statistics, pp. 11671176. PMLR, 2019. Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diegode las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b.arXiv preprint arXiv:2310.06825, 2023a. Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford,Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. Mixtral ofexperts. arXiv preprint arXiv:2401.04088, 2024a. Mingjian Jiang, Yangjun Ruan, Sicong Huang, Saifei Liao, Silviu Pitis, Roger Baker Grosse, and Jimmy Ba.Calibrating language models via augmented prompt ensembles. Workshop on Challenges in DeployableGenerative AI at International Conference on Machine Learning, 2023b. Minhao Jiang, Ken Ziyu Liu, Ming Zhong, Rylan Schaeffer, Siru Ouyang, Jiawei Han, and Sanmi Koyejo.Investigating data contamination for pre-training language models. arXiv preprint arXiv:2401.06059,2024b.",
  "Wenyu Jiang, Zhenlong Liu, Zejian Xie, Songxin Zhang, Bingyi Jing, and Hongxin Wei. Exploring learningcomplexity for downstream data pruning. arXiv preprint arXiv:2402.05356, 2024c": "Zixuan Jiang, Jiaqi Gu, Mingjie Liu, and David Z Pan. Delving into effective gradient matching for datasetcondensation. In 2023 IEEE International Conference on Omni-layer Intelligent Systems (COINS), pp.16. IEEE, 2023d. Tong Jiao, Jian Zhang, Kui Xu, Rui Li, Xi Du, Shangqi Wang, and Zhenbo Song. Enhancing fairness in llmevaluations: Unveiling and mitigating biases in standard-answer-based evaluations. In Proceedings of theAAAI Symposium Series, volume 4, pp. 5659, 2024.",
  "Nikhil Kandpal, Eric Wallace, and Colin Raffel. Deduplicating training data mitigates privacy risks inlanguage models. In International Conference on Machine Learning, pp. 1069710707. PMLR, 2022": "Feiyang Kang, Hoang Anh Just, Anit Kumar Sahu, and Ruoxi Jia. Performance scaling via optimal transport:Enabling data selection from partially revealed sources. Advances in Neural Information Processing Systems,36, 2024. Tapas Kanungo, David M Mount, Nathan S Netanyahu, Christine Piatko, Ruth Silverman, and Angela Y Wu.The analysis of a simple k-means clustering algorithm. In Proceedings of the sixteenth annual symposiumon Computational geometry, pp. 100109, 2000. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray,Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprintarXiv:2001.08361, 2020.",
  "Kimmo Kettunen. Can type-token ratio be used to show morphological complexity of languages? Journal ofQuantitative Linguistics, 21(3):223245, 2014": "Kamran Khan, Saif Ur Rehman, Kamran Aziz, Simon Fong, and Sababady Sarasvady. Dbscan: Past,present and future. In The fifth international conference on the applications of digital information and webtechnologies (ICADIWT 2014), pp. 232238. IEEE, 2014. Douwe Kiela, Max Bartolo, Yixin Nie, Divyansh Kaushik, Atticus Geiger, Zhengxuan Wu, Bertie Vidgen,Grusha Prasad, Amanpreet Singh, Pratik Ringshia, et al. Dynabench: Rethinking benchmarking in nlp.arXiv preprint arXiv:2104.14337, 2021. Krishnateja Killamsetty, Sivasubramanian Durga, Ganesh Ramakrishnan, Abir De, and Rishabh Iyer. Grad-match: Gradient matching based data subset selection for efficient deep model training. In InternationalConference on Machine Learning, pp. 54645474. PMLR, 2021a. Krishnateja Killamsetty, Durga Sivasubramanian, Ganesh Ramakrishnan, and Rishabh Iyer. Glister: General-ization based data subset selection for efficient and robust learning. In Proceedings of the AAAI Conferenceon Artificial Intelligence, volume 35, pp. 81108118, 2021b. Krishnateja Killamsetty, Xujiang Zhao, Feng Chen, and Rishabh Iyer. Retrieve: Coreset selection for efficientand robust semi-supervised learning. Advances in neural information processing systems, 34:1448814501,2021c.",
  "Pang Wei Koh and Percy Liang. Understanding black-box predictions via influence functions. In Internationalconference on machine learning, pp. 18851894. PMLR, 2017": "Rik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate Kushman, and Hannaneh Hajishirzi. Mawps: Amath word problem repository. In Proceedings of the 2016 conference of the north american chapter of theassociation for computational linguistics: human language technologies, pp. 11521157, 2016. Andreas Kpf, Yannic Kilcher, Dimitri von Rtte, Sotiris Anagnostidis, Zhi Rui Tam, Keith Stevens, AbdullahBarhoum, Duc Nguyen, Oliver Stanley, Richrd Nagyfi, et al. Openassistant conversations-democratizinglarge language model alignment. Advances in Neural Information Processing Systems, 36, 2024.",
  "Po-Nien Kung and Nanyun Peng. Do models really learn to follow instructions? an empirical study ofinstruction tuning. arXiv preprint arXiv:2305.11383, 2023": "Po-Nien Kung, Fan Yin, Di Wu, Kai-Wei Chang, and Nanyun Peng. Active instruction tuning: Improvingcross-task generalization by training on prompt sensitive tasks. arXiv preprint arXiv:2311.00288, 2023. Jeongyeol Kwon, Dohyun Kwon, Stephen Wright, and Robert D Nowak. A fully first-order method forstochastic bilevel optimization. In International Conference on Machine Learning, pp. 1808318113. PMLR,2023.",
  "Yrjo Lappalainen and Nikesh Narayanan. Aisha: A custom ai library chatbot using the chatgpt api. Journalof Web Librarianship, 17(3):3758, 2023": "Stefan Larson, Anish Mahendran, Andrew Lee, Jonathan K Kummerfeld, Parker Hill, Michael A Laurenzano,Johann Hauswald, Lingjia Tang, and Jason Mars. Outlier detection for improved data quality and diversityin dialog systems. arXiv preprint arXiv:1904.03122, 2019. Cosmin Lazar and Andrei Doncescu. Non negative matrix factorization clustering capabilities; application onmultivariate image segmentation. In 2009 International Conference on Complex, Intelligent and SoftwareIntensive Systems, pp. 924929. IEEE, 2009. Ronan Le Bras, Swabha Swayamdipta, Chandra Bhagavatula, Rowan Zellers, Matthew Peters, AshishSabharwal, and Yejin Choi. Adversarial filters of dataset biases. In International conference on machinelearning, pp. 10781088. Pmlr, 2020.",
  "Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and Bill Dolan. A diversity-promoting objectivefunction for neural conversation models. arXiv preprint arXiv:1510.03055, 2015": "Ming Li, Yong Zhang, Zhitao Li, Jiuhai Chen, Lichang Chen, Ning Cheng, Jianzong Wang, Tianyi Zhou,and Jing Xiao. From quantity to quality: Boosting llm performance with self-guided data selection forinstruction tuning. arXiv preprint arXiv:2308.12032, 2023a. Ming Li, Yong Zhang, Shwai He, Zhitao Li, Hongyu Zhao, Jianzong Wang, Ning Cheng, and Tianyi Zhou.Superfiltering: Weak-to-strong data filtering for fast instruction-tuning. arXiv preprint arXiv:2402.00530,2024b.",
  "Qingyi Liu, Yekun Chai, Shuohuan Wang, Yu Sun, Keze Wang, and Hua Wu. On training data influence ofgpt models. arXiv preprint arXiv:2404.07840, 2024b": "Wei Liu, Weihao Zeng, Keqing He, Yong Jiang, and Junxian He. What makes good data for alignment? acomprehensive study of automatic data selection in instruction tuning. arXiv preprint arXiv:2312.15685,2023b. Xiaoyu Liu, Paiheng Xu, Junda Wu, Jiaxin Yuan, Yifan Yang, Yuhang Zhou, Fuxiao Liu, Tianrui Guan, Hao-liang Wang, Tong Yu, et al. Large language models and causal inference in collaboration: A comprehensivesurvey. arXiv preprint arXiv:2403.09606, 2024c.",
  "Gbor Melis, Chris Dyer, and Phil Blunsom. On the state of the art of evaluation in neural language models.arXiv preprint arXiv:1707.05589, 2017": "Brando Miranda, Patrick Yu, Yu-Xiong Wang, and Sanmi Koyejo. The curse of low task diversity: Onthe failure of transfer learning to outperform maml and their empirical equivalence. arXiv preprintarXiv:2208.01545, 2022. Swaroop Mishra and Bhavdeep Singh Sachdeva. Do we need to create big datasets to learn a task?InProceedings of SustaiNLP: Workshop on Simple and Efficient Natural Language Processing, pp. 169173,2020.",
  "Nagarajan Natarajan, Inderjit S Dhillon, Pradeep K Ravikumar, and Ambuj Tewari. Learning with noisylabels. Advances in neural information processing systems, 26, 2013": "Noel Ngu, Nathaniel Lee, and Paulo Shakarian. Diversity measures: Domain-independent proxies for failurein language model queries. In 2024 IEEE 18th International Conference on Semantic Computing (ICSC),pp. 176182. IEEE, 2024. Cuong V Nguyen, Alessandro Achille, Michael Lam, Tal Hassner, Vijay Mahadevan, and Stefano Soatto.Toward understanding catastrophic forgetting in continual learning. arXiv preprint arXiv:1908.01091, 2019.",
  "Rui Pan, Jipeng Zhang, Xingyuan Pan, Renjie Pi, Xiaoyu Wang, and Tong Zhang. Scalebio: Scalable bileveloptimization for llm data reweighting. arXiv preprint arXiv:2406.19976, 2024": "Jinlong Pang, Jiaheng Wei, Ankit Parag Shah, Zhaowei Zhu, Yaxuan Wang, Chen Qian, Yang Liu, YujiaBao, and Wei Wei. Improving data efficiency via curating llm-driven rating systems. arXiv preprintarXiv:2410.10877, 2024. Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluationof machine translation. In Proceedings of the 40th annual meeting of the Association for ComputationalLinguistics, pp. 311318, 2002.",
  "Maria Priestley, Fionntn Odonnell, and Elena Simperl. A survey of data quality requirements that matterin ml development pipelines. ACM Journal of Data and Information Quality, 15(2):139, 2023": "Garima Pruthi, Frederick Liu, Satyen Kale, and Mukund Sundararajan. Estimating training data influenceby tracing gradient descent. Advances in Neural Information Processing Systems, 33:1992019930, 2020. Yulei Qin, Xingyu Chen, Yunhang Shen, Chaoyou Fu, Yun Gu, Ke Li, Xing Sun, and Rongrong Ji. Capro:webly supervised learning with cross-modality aligned prototypes.Advances in Neural InformationProcessing Systems, 36, 2024.",
  "Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language modelsare unsupervised multitask learners. OpenAI blog, 1(8):9, 2019": "Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides,Sarah Henderson, Roman Ring, Susannah Young, et al. Scaling language models: Methods, analysis &insights from training gopher. arXiv preprint arXiv:2112.11446, 2021. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn.Direct preference optimization: Your language model is secretly a reward model. Advances in NeuralInformation Processing Systems, 36, 2024. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou,Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer.Journal of machine learning research, 21(140):167, 2020.",
  "Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-networks. arXivpreprint arXiv:1908.10084, 2019": "Alfrd Rnyi. On measures of entropy and information. In Proceedings of the fourth Berkeley symposium onmathematical statistics and probability, volume 1: contributions to the theory of statistics, volume 4, pp.547562. University of California Press, 1961. Hamed Rezazadegan Tavakoli, Esa Rahtu, and Janne Heikkil. Fast and efficient saliency detection usingsparse sampling and kernel density estimation. In Image Analysis: 17th Scandinavian Conference, SCIA2011, Ystad, Sweden, May 2011. Proceedings 17, pp. 666675. Springer, 2011.",
  "Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarialwinograd schema challenge at scale. Communications of the ACM, 64(9):99106, 2021": "Victor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin,Arnaud Stiegler, Teven Le Scao, Arun Raja, et al. Multitask prompted training enables zero-shot taskgeneralization. arXiv preprint arXiv:2110.08207, 2021. Gayathri Saranathan, Mahammad Parwez Alam, James Lim, Suparna Bhattacharya, Soon Yee Wong, MartinFoltin, and Cong Xu. Dele: Data efficient llm evaluation. In ICLR 2024 Workshop on Navigating andAddressing Data Problems for Foundation Models. Nikunj Saunshi, Arushi Gupta, Mark Braverman, and Sanjeev Arora. Understanding influence functions anddatamodels via harmonic analysis. In The Eleventh International Conference on Learning Representations,2022.",
  "Zhengyan Shi, Adam X Yang, Bin Wu, Laurence Aitchison, Emine Yilmaz, and Aldo Lipani. Instructiontuning with loss over instructions. arXiv preprint arXiv:2405.14394, 2024": "Manli Shu, Jiongxiao Wang, Chen Zhu, Jonas Geiping, Chaowei Xiao, and Tom Goldstein. On the exploitabilityof instruction tuning. Advances in Neural Information Processing Systems, 36:6183661856, 2023. Raphael Shu, Hideki Nakayama, and Kyunghyun Cho. Generating diverse translations with sentence codes.In Proceedings of the 57th annual meeting of the association for computational linguistics, pp. 18231827,2019.",
  "Kristina P Sinaga and Miin-Shen Yang.Unsupervised k-means clustering algorithm.IEEE access, 8:8071680727, 2020": "Ankur Sinha, Pekka Malo, and Kalyanmoy Deb.A review on bilevel optimization: From classical toevolutionary approaches and applications. IEEE transactions on evolutionary computation, 22(2):276295,2017. Samarth Sinha, Han Zhang, Anirudh Goyal, Yoshua Bengio, Hugo Larochelle, and Augustus Odena. Small-gan:Speeding up gan training using core-sets. In International Conference on Machine Learning, pp. 90059015.PMLR, 2020. Eric Michael Smith, Melissa Hall, Melanie Kambadur, Eleonora Presani, and Adina Williams. \" im sorryto hear that\": Finding new biases in language models with a holistic descriptor dataset. arXiv preprintarXiv:2205.09209, 2022.",
  "John Smith and Lisa Johnson. Strategies for difficulty sampling providing diversity in datasets. Journal ofMachine Learning Research, 10:100120, 2020": "Hwanjun Song, Minseok Kim, Dongmin Park, Yooju Shin, and Jae-Gil Lee. Learning from noisy labels withdeep neural networks: A survey. IEEE transactions on neural networks and learning systems, 34(11):81358153, 2022. Ben Sorscher, Robert Geirhos, Shashank Shekhar, Surya Ganguli, and Ari Morcos. Beyond neural scalinglaws: beating power law scaling via data pruning. Advances in Neural Information Processing Systems, 35:1952319536, 2022.",
  "Katherine Stasaski and Marti A Hearst. Semantic diversity in dialogue with natural language inference.arXiv preprint arXiv:2205.01497, 2022": "Katherine Stasaski, Grace Hui Yang, and Marti A Hearst. More diverse dialogue datasets via diversity-informed data collection. In Proceedings of the 58th annual meeting of the association for computationallinguistics, pp. 49584968, 2020. Albert Yu Sun, Eliott Zemour, Arushi Saxena, Udith Vaidyanathan, Eric Lin, Christian Lau, and VaikkunthMugunthan. Does fine-tuning gpt-3 with the openai api leak personally-identifiable information? arXivpreprint arXiv:2307.16382, 2023. Peng Sun, Bei Shi, Daiwei Yu, and Tao Lin. On the diversity and realism of distilled dataset: An efficientdataset distillation paradigm. In Proceedings of the IEEE/CVF Conference on Computer Vision andPattern Recognition, pp. 93909399, 2024a.",
  "Haoru Tan, Sitong Wu, Fei Du, Yukang Chen, Zhibin Wang, Fan Wang, and Xiaojuan Qi. Data pruning viamoving-one-sample-out. Advances in Neural Information Processing Systems, 36, 2024a": "Jiejun Tan, Zhicheng Dou, Yutao Zhu, Peidong Guo, Kun Fang, and Ji-Rong Wen. Small models, biginsights: Leveraging slim proxy models to decide when and what to retrieve for llms. arXiv preprintarXiv:2402.12052, 2024b. Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, andTatsunori B Hashimoto. Alpaca: A strong, replicable instruction-following model. Stanford Center forResearch on Foundation Models. stanford. edu/2023/03/13/alpaca. html, 3(6):7, 2023.",
  "Steve Tingiris and Bret Kinsella. Exploring GPT-3: An unofficial first look at the general-purpose languageprocessing API from OpenAI. Packt Publishing Ltd, 2021": "Kushal Tirumala, Aram Markosyan, Luke Zettlemoyer, and Armen Aghajanyan. Memorization withoutoverfitting: Analyzing the training dynamics of large language models. Advances in Neural InformationProcessing Systems, 35:3827438290, 2022. Kushal Tirumala, Daniel Simig, Armen Aghajanyan, and Ari Morcos. D4: Improving llm pretraining viadocument de-duplication and diversification. Advances in Neural Information Processing Systems, 36, 2024. Mariya Toneva, Alessandro Sordoni, Remi Tachet des Combes, Adam Trischler, Yoshua Bengio, and Geoffrey JGordon. An empirical study of example forgetting during deep neural network learning. arXiv preprintarXiv:1812.05159, 2018.",
  "Simon Tong and Daphne Koller. Support vector machine active learning with applications to text classification.Journal of machine learning research, 2(Nov):4566, 2001": "Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothe Lacroix,Baptiste Rozire, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundationlanguage models. arXiv preprint arXiv:2302.13971, 2023a. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation andfine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023b.",
  "Jiahao Wang, Bolin Zhang, Qianlong Du, Jiajun Zhang, and Dianhui Chu. A survey on data selection for llminstruction tuning. arXiv preprint arXiv:2402.05123, 2024a": "Lingzhi Wang, Xingshan Zeng, Jinsong Guo, Kam-Fai Wong, and Georg Gottlob. Selective forgetting: Advanc-ing machine unlearning techniques and evaluation in language models. arXiv preprint arXiv:2402.05813,2024b. Yidong Wang, Zhuohao Yu, Zhengran Zeng, Linyi Yang, Cunxiang Wang, Hao Chen, Chaoya Jiang, Rui Xie,Jindong Wang, Xing Xie, et al. Pandalm: An automatic evaluation benchmark for llm instruction tuningoptimization. arXiv preprint arXiv:2306.05087, 2023a. Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and HannanehHajishirzi.Self-instruct: Aligning language models with self-generated instructions.arXiv preprintarXiv:2212.10560, 2022.",
  "Mengzhou Xia, Sadhika Malladi, Suchin Gururangan, Sanjeev Arora, and Danqi Chen. Less: Selectinginfluential data for targeted instruction tuning. arXiv preprint arXiv:2402.04333, 2024a": "Tingyu Xia, Bowen Yu, Kai Dang, An Yang, Yuan Wu, Yuan Tian, Yi Chang, and Junyang Lin. Rethinkingdata selection at scale: Random selection is almost all you need. arXiv preprint arXiv:2410.09335, 2024b. Xiaobo Xia, Jiale Liu, Jun Yu, Xu Shen, Bo Han, and Tongliang Liu. Moderate coreset: A universal methodof data selection for real-world data-efficient deep learning. In The Eleventh International Conference onLearning Representations, 2022. Xiaobo Xia, Jiale Liu, Shaokun Zhang, Qingyun Wu, Hongxin Wei, and Tongliang Liu. Refined coresetselection: Towards minimal coreset size under model performance constraints. In Forty-first InternationalConference on Machine Learning, 2024c.",
  "Gregory Yauney, Emily Reif, and David Mimno. Data similarity is not enough to explain language modelperformance. arXiv preprint arXiv:2311.09006, 2023": "Jingwen Ye, Ruonan Yu, Songhua Liu, and Xinchao Wang. Distilled datamodel with reverse gradientmatching. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.1195411963, 2024. aatay Yldz, Nishaanth Kanna Ravichandran, Prishruit Punia, Matthias Bethge, and Beyza Ermis.Investigating continual pretraining in large language models: Insights and implications. arXiv preprintarXiv:2402.17400, 2024. Julio Christian Young and Makoto Shishido. Investigating openais chatgpt potentials in generating chatbotsdialogue for english as a foreign language learning. International journal of advanced computer science andapplications, 14(6), 2023.",
  "Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deeplearning (still) requires rethinking generalization. Communications of the ACM, 64(3):107115, 2021": "Chiyuan Zhang, Daphne Ippolito, Katherine Lee, Matthew Jagielski, Florian Tramr, and Nicholas Carlini.Counterfactual memorization in neural language models. Advances in Neural Information ProcessingSystems, 36:3932139362, 2023a. Jipeng Zhang, Yaxuan Qin, Renjie Pi, Weizhong Zhang, Rui Pan, and Tong Zhang. Tagcos: Task-agnosticgradient clustered coreset selection for instruction tuning data. arXiv preprint arXiv:2407.15235, 2024a.",
  "Lei Zhang. Bilevel optimization in the deep learning era: Methods and applications. 2024": "Shaokun Zhang, Xiaobo Xia, Zhaoqing Wang, Ling-Hao Chen, Jiale Liu, Qingyun Wu, and Tongliang Liu.Ideal: Influence-driven selective annotations empower in-context learners in large language models. arXivpreprint arXiv:2310.10873, 2023c. Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu,Tianwei Zhang, Fei Wu, et al. Instruction tuning for large language models: A survey. arXiv preprintarXiv:2308.10792, 2023d.",
  "Xiao Zhang and Ji Wu. Dissecting learning and forgetting in language model finetuning. In The TwelfthInternational Conference on Learning Representations, 2024": "Xiaoying Zhang, Baolin Peng, Ye Tian, Jingyan Zhou, Yipeng Zhang, Haitao Mi, and Helen Meng. Self-tuning:Instructing llms to effectively acquire new knowledge through self-teaching. arXiv preprint arXiv:2406.06326,2024b. Yifan Zhang, Yifan Luo, Yang Yuan, and Andrew C Yao. Autonomous data selection with language models formathematical texts. In ICLR 2024 Workshop on Navigating and Addressing Data Problems for FoundationModels, 2024c. Yihua Zhang, Yuguang Yao, Parikshit Ram, Pu Zhao, Tianlong Chen, Mingyi Hong, Yanzhi Wang, andSijia Liu. Advancing model pruning via bi-level optimization. Advances in Neural Information ProcessingSystems, 35:1830918326, 2022.",
  "Lianghui Zhu, Xinggang Wang, and Xinlong Wang. Judgelm: Fine-tuned large language models are scalablejudges. arXiv preprint arXiv:2310.17631, 2023": "Yaoming Zhu, Sidi Lu, Lei Zheng, Jiaxian Guo, Weinan Zhang, Jun Wang, and Yong Yu. Texygen: Abenchmarking platform for text generation models. In The 41st international ACM SIGIR conference onresearch & development in information retrieval, pp. 10971100, 2018. Xinlin Zhuang, Xin Mao, Yuan-Hao Jiang, Hongyi Wu, Shangqing Zhao, Li Cai, Shu Liu, Yang Chen, YuxiangSong, Chenghao Jia, et al. Bread: A hybrid approach for instruction data mining through balancedretrieval and dynamic data sampling. In CCF International Conference on Natural Language Processingand Chinese Computing, pp. 229240. Springer, 2024."
}