{
  "Abstract": "Most current domain adaptation methods address either covariate shift or label shift, butare not applicable where they occur simultaneously and are confounded with each other.Domain adaptation approaches which do account for such confounding are designed to adaptcovariates to optimally predict a particular label whose shift is confounded with covariateshift. In this paper, we instead seek to achieve general-purpose data backwards compatibility.This would allow the adapted covariates to be used for a variety of downstream problems,including on pre-existing prediction models and on data analytics tasks. To do this weconsider a modification of generalized label shift (GLS), which we call confounded shift. Wepresent a novel framework for this problem, based on minimizing the expected divergencebetween the source and target conditional distributions, conditioning on possible confounders.Within this framework, we provide concrete implementations using the Gaussian reverseKullback-Leibler divergence and the maximum mean discrepancy. Finally, we demonstrateour approach on synthetic and real datasets.",
  "Introduction": "The heterogeneity of scientific data is a major obstacle to training useful AI models for scientific research(Bronstein & Naef, 2024). The recent success of AlphaFold (Jumper et al., 2021) for protein structureprediction is in large part due to the availability of a large-scale standardized dataset in the form of theProtein Data Bank (Burley et al., 2019). In contrast, for many other biological problems, data is obtainedfrom a variety of settings which vary due to both scientifically-relevant differences and also due to differencesin experimental conditions which give rise to batch effects (Leek et al., 2010). And unlike protein structuredata, for which ground-truth quantities are absolute distances, many other scientific data modalities reportrelative quantities (Bronstein & Naef, 2024). These reported quantities that are relative to some detectionthreshold or noise level are particularly subject to batch effects caused by technical differences in experimentalsetups, assays, and even computational techniques for processing raw sensor data (Cai et al., 2018). Within various scientific fields, such as genomics (Johnson et al., 2007; Sprang et al., 2022), proteomics(Gregori et al., 2012; Pelletier et al., 2024), and neuroscience (Yamashita et al., 2019; Torbati et al., 2021),there exist separate preexisting literatures on domain adaptation methods for combining multiple datasetsaffected by technical artifacts. We cannot hope to discuss all of them in detail, but generally speaking, thesemethods seek to align datasets so that they have similar feature distributions (Johnson et al., 2007; Shahamet al., 2017) or similar intra-dataset sample-sample relationships (Haghverdi et al., 2018). This objective isdangerous when the to-be-combined datasets not only differ for technical reasons, but also due to variablesthat are to be scientifically investigated or predicted (Hicks et al., 2018; Zindler et al., 2020; Antonsson& Melsted, 2024). In this case, the feature distributions of the two datasets should not be aligned to beequal, because the difference in feature distributions is confounded with a difference in distributions for somescientifically-relevant variable.",
  "Published in Transactions on Machine Learning Research (11/2024)": "consists of the MMD loss over paired 20-pixel datasets. Each epoch is defined as a single pass-through overrandomly generated 0.01 min(669 1000, 750 1000) unique paired 20-pixel datasets (for the day-sunsettask, and analogously for the other task). This randomly-generated dataset of datasets is reused acrossepochs. In Figure S14 we see that the ConDo methods do better on Sunset Beach, producing a brighter image,particularly for the clouds and the horizon. We see that both MMD and ConDo MMD fail to produce goodinverse mappings.",
  "A motivating example": "Suppose you have patient health outcome data and EEG data from the first version of an EEG device,depicted as V1 training data in . Using this dataset, you have already conducted various statisticalanalyses and trained health outcome prediction models, including for seizure and depression. But then theEEG machine gets updated to V2, and we obtain a small amount of EEG data collected from the V2 machine,along with patient seizure status, depicted as V2 training data. The V2 data distribution appears shiftedrelative to that from the V1 machine. At this point, it seems appropriate to perform covariate shift domainadaptation, to learn an explicit feature-space transformation that adapts the V1 and V2 data to look alike.",
  "X Z Y": ": Diagram depicting our motivating scenario. There is confounded shift between source (V1) andtarget (V2) domains. The different shades of blue of the EEG data portray the covariate shift betweenV1 features and V2 features. The seizure confounding variable differs in distribution between source andtarget; we portray this label shift in seizure status proportions ( vs +) between V1 and V2. With V1training data, we had previously learned prediction models for seizure and depression given V1 features.With V2 training data, we learn a mapping from V2 features to V1 features via ConDo. At V2 test time,neither seizure nor depression status are provided, but we combine the ConDo V2-to-V1 mapping with thepreviously-learned prediction models. The EEG data correspond to the features variable X, seizure statuscorresponds to the confounding variable Z, and depression status corresponds to the downstream predictionvariable Y . Yet additionally, while the V1 dataset comes from a large number of low-risk and high-risk patients, the V2dataset thus far is mostly comprised of seizure-free individuals. Ignoring the aforementioned covariate shiftproblem, this latter problem in isolation would fall into the label shift domain adaptation problem setting.Our hypothetical scenario combines these two problems: it has both covariate shift and label shift which areconfounded with each other. Our goal is to learn a feature-space transformation correcting for the technical effects between source (V1machine) and target (V2 machine) domains, while being aware of the actual neurological differences betweenthe two datasets. We would like to learn a single feature-space transformation (and transformed dataset)that can be used not only for a single prediction task, but for multiple downstream tasks and statisticalanalyses. For example, we might want to combine the V1 and (adapted) V2 data and then assess for statisticalcorrelation between EEG features and patient age. The key restriction will be that, at inference time, we will not observe these confounding biological variables.To see why the feature transformation may take in only the EEG data as input, consider the fact that wemay want to predict seizure and depression risk on incoming V2 test samples. Even though we could useour confounder (seizure) in the training data to learn a transformation, on test samples we do not yet knowhealth outcomes: indeed, it is what we want to predict! However, we will have access to seizure status whilelearning the feature transformation, which we will assume fully accounts for the confounding between V1 andV2, as will be formalized in . The aforementioned example also illustrates why standard domain adaptation approaches may not beapplicable. A new EEG embedding space invariant to the V1-vs-V2 domain shift would not be a general-purpose solution for a variety of downstream prediction and statistical inference tasks. In contrast, we wouldlike to preserve as much information as possible, and not only the principal components of variation, or only",
  "Notation": "X and Z respectively denote the feature (covariate) space and the confounder space. X and Z denoterandom variables which take values in X and Z, respectively. A joint distribution over covariate space X andconfounder space Z is called a domain D. In our setting, there is a source domain DS and a target domainDT ; we assume NS and NT samples from the source and target domain, respectively. DXS , DXT denote themarginal distributions of covariates under the source and target domains, respectively; DZS , DZT denote thecorresponding marginal distributions of confounders. We assume that feature variables are real-valued vectors, so denote samples as xS RMS, xT RMT . Wemake no such assumption for the confounders, denoting each observation as z, but we assume the existenceof a user-specified confounder-space kernel function kZ(z(n1), z(n2)). We will seek to optimize the mappingg : XT XS from target feature-space to source feature-space. We will also sometimes consider a targetvariable (output label) Y Y, for which we have a classification hypothesis h : X Y that has been trainedon data from the source domain, corresponding to the standard unsupervised domain adaptation (UDA)setting. For arbitrary distributions P and Q, we assume we have been given a distance or divergence function denotedby d(P, Q). By N(, ) we denote the Gaussian distribution with mean and covariance . By || we denotethe absolute value; by det() we denote the matrix determinant. By A we denote the matrix transpose.",
  "Affine domain adaptation based on Gaussian Optimal Transport": "Domain adaptation has a closed-form affine solution in the special case of two multivariate Gaussiandistributions. The optimal transport (OT) map under the type-2 Wasserstein metric for x N(S, S) to adifferent Gaussian distribution N(T , T ) has been shown (Dowson & Landau, 1982; Knott & Smith, 1984)to be the following:",
  "Affine domain adaptation minimizing the maximum mean discrepancy": "An alternative approach can be derived from representing the distance between target and source distributionsas the distance between mean embeddings. This leads to minimizing the (squared) maximum mean discrepancy(MMD), where the MMD is defined by a feature map mapping features x X to a reproducing kernel Hilbertspace H. We denote the feature-space kernel corresponding to as kX (x(n1), x(n2)) = (x(n1)), (x(n2)).Because the feature-space vectors are assumed to be real, MMD-based adaptation methods typically use theradial basis function (RBF) kernel, which leads to the MMD being zero if and only if the distributions areidentical.",
  "+ Ex(n2),x(n2)DSkX (Ax(n2) + b, Ax(n2) + b).(3)": "Prior work has sometimes instead assumed a location-scale transformation (Zhang et al., 2013), or a nonlineartransformation (Liu et al., 2019a). Notably, while previous MMD-based domain adaptation methods havematched feature distributions (Zhang et al., 2013; Liu et al., 2019a; Singh et al., 2020; Yan et al., 2017),joint distributions of features and label (Long et al., 2013), or the conditional distribution of label givenfeatures (Long et al., 2013), they have generally not considered matching the conditional distribution offeatures given labels. One exception to this is IWCDAN (Tachet des Combes et al., 2020), which howeveraligns datasets via sample importance weighting rather than a feature-space transformation.",
  "Background on Covariate Shift, Label Shift, and Generalized Label Shift": "Domain adaptation methods typically assume either covariate shift or label shift. With covariate shift, themarginal distribution over covariates differs between source and target domains. However, for any particularcovariate, the conditional distribution of the label given the covariate is identical between source and target.With label shift, the marginal distribution over labels differs between source and target domains. However,for any particular label, the conditional distribution of the covariates given the label is identical betweensource and target domain. More recently, generalized label shift was introduced to allow covariate distributions to differ between sourceand target domains (Tachet des Combes et al., 2020). Generalized label shift (GLS) instead assumes that,given a transformation function X := g(X) applied to inputs from both source and target domains, theconditional distributions of X given Y = y are identical for all y. This is a weak assumption, and it appliesto our problem setting as well. However, it is designed for the scenario where we simply need g to preserveinformation only for predicting Y given X DXS .",
  "Confounded Domain Adaptation": "Consider our motivating scenario in which our ultimate goal is to reuse a classification hypothesis h : X Yin a new deployment setting. We treat the deployment setting as the target domain. And instead of learningan end-to-end predictor for the deployment domain, we learn an adaptation g from it to the source domain forwhich we have a large number of labeled examples. Then, to perform predictions on the deployment (target)domain, we first adapt them to the source domain, and then we apply the prediction model trained on thesource domain. In other words, we do not need to retrain h, and instead apply h g to incoming unlabeledtarget samples. Similarly, other prediction tasks and statistical analyses can be applied after combiningadapted-target feature data and source feature data. In many real-world structured data applications, new data sources are designed with backwards-compatibilityin mind, with the goal that updated sensor and assays provide at least as much information as the earlierversions. We therefore assume the existence of a true noise-free mapping g from the deployment (target)domain to the large labeled dataset (source) domain. The algorithms developed under our framework couldinstead be applied when treating the deployment setting as the target domain and the labeled dataset as thesource domain. However, this easier setting would allow retraining h on adapted data, and is thus not thefocus of this paper.",
  "Our Assumption: Confounded Shift": "In our case, given X DXT , we instead want to recover what it would have been had we observed the sameobject from the data generating process corresponding to the source domain X DXS . In other words, themapping g(X) should not only preserve information in X useful for predicting Y , but ideally all informationin X DXT that is contained in X DXS .",
  "NameShiftAssumed Invariant": "Covariate ShiftDXS = DXTx X, DS(Y |X = x) = DT (Y |X = x)Label ShiftDYS = DYTy Y, DS(X|Y = y) = DT (X|Y = y)Generalized Label ShiftDYS = DYTy Y, DS(g(X)|Y = y) = DT (g(X)|Y = y)Confounded ShiftDYS = DYTy Y, DS(X|Y = y) = DT (g(X)|Y = y) GraphicalrepresentationWemayformulateoursettingwithlatentvariablesXcorre-spondingtofeaturesbeforethetarget-to-sourcemapping.Webeginbydefininganin-dicatorvariableDwhichspecifieswhetherasampleistakenfromthetargetorthesourcedomain.Wemaythendepictourassumptionwiththefollowinggraphicalmodel",
  "x xD = Tx g(x)D = S(4)": "where is the Dirac delta. By inspection of the graphical model, our setting is a combination of priorprobability shift and covariate observation shift as defined in (Kull & Flach, 2014). Note that latent featuresX are generated from confounders Z, which motivates using a generative model for domain adaptation. Relation to Generalized Label ShiftSuppose GLS intermediate representation g(X) were extended to bea function of both X and the source-vs-target indicator variable D. Then, given this extended representation{X, D}, we restrict g({X, D}) as follows,",
  "g(X)D = TXD = S(5)": "so that samples from the target distribution are adapted by g(), while those from the source distribution passthrough unchanged. For comparability, we let Y = Z, i.e. we consider the scenario where our downstreamprediction task is to predict the confounding variable which is unavailable at test time. With this extendedrepresentation, as well as the restriction on g, confounded shift and GLS coincide. The previous assumptionsas well as our confounded shift assumption are summarized in . Note that while confounded shiftis stronger than GLS, both allow DXS = DXT ; and just as GLS allows Dg(X)S= Dg(X)T, we analogously allowDg(X)S= DXT . Application to downstream prediction tasksFor scenarios where Y = Z (where our downstream taskis to predict a variable other than the confounder), we do not require dependence or correlation betweenthe confounding variable Z and downstream prediction variable Y . Rather, we assume that the conditionaldistribution of Y given features has not changed from source to target, i.e. pDS(Y |X) = pDT (Y |g(X)). Whatis necessary is that we correctly map features from target to source; then any downstream prediction modeltrained on source data can be reused via the composition h g, as long as the downstream conditionaldistribution (estimated by h) has not changed.",
  "Feature-space transformation function": "Our goal is to find the optimal linear transformation g(x) = Ax+b of the target to source. In certain scenarios,particularly scientific analyses, it is important for explainability that each ith adapted feature [Ax(n) + b]ibe derived only from the original feature [x(n)]i. So we will examine both full affine transformations and alsotransformations where A is restricted to be diagonal A = diag(a). The latter is sometimes referred to as alocation-scale adaptation (Zhang et al., 2013); this further requires that the source and target features havethe same dimension.",
  "The product prior over confounding variable(s)": "Our approach is motivated by the desire to minimize the distance between the conditional distributionsDS(x|Z = z) and DT (x|Z = z) only where we can estimate them both with high accuracy. These conditionaldistribution estimators may be poor extrapolators, with noisy estimates in low-density regions of DZS and DZT .This suggests choosing to perform minimization over confounder values that are likely under both source DZSand target DZT distributions, which motivates using the product of the two distributions. We estimate the product of DZS and DZT as follows. For efficient sampling, we use a non-parametric estimatorof the product prior, with non-negative support over the union of confounder values in the source andtarget datasets, so that it can be represented as probabilistic weights attached to each sample. We notethat empirical distributions may have non-intersecting support, such as if Z is a continuous variable. Thismotivates smoothing the estimators of DZS and DZT before taking their product; this avoids taking the productof two Dirac delta functions, which is undefined.",
  "Sampling from the conditional distributions": "When the confounding variable is discrete, and each value occurs in multiple examples in our dataset, thenwe merely sample from these datapoints with replacement. Otherwise (i.e. the confounding variable iscontinuous), we instead need to sample from conditional generative models for DS(x|Z = z) and DS(x|Z = z).While diffusion modeling (Sohl-Dickstein et al., 2015) is possible, we found that multiple imputation withchained equations (MICE) (Van Buuren et al., 1999), as implemented in MICE-Forest (Wilson et al., 2022)with LightGBM (Ke et al., 2017), provided better conditional generation results, replicating the experimentalresults in (Jolicoeur-Martineau et al., 2024). To do this, we concatenate the original dataset and a secondcopy with all features masked and all confounder(s) unmasked. We then request KX multiple imputations,which are then used as conditionally-generated features per each observed Z = z. This process is performedseparately for source and target.",
  "Conditional distribution distance/divergence function": "Below, we propose using the reverse-KLD and the MMD in our loss function. Both yield simple, efficientalgorithms, including a closed-form solution for the reverse KLD with location-scale adaptation. Note that,as we discuss in Future Work, other divergences are possible within our framework. In particular, optimaltransport (OT)-based distances are likely to offer higher accuracy at greater computational expense. However,we limit ourselves to these two divergences for their low computational cost, and to focus on the overallproposed framework rather than the computational challenges and opportunities that arise from combiningConDo with OT.",
  "Reverse Kullback-Leibler divergence under Gaussianity": "It can be straightforwardly shown that the linear map Eq. (1) derived from OT leads to adapted data beingdistributed according to the target distribution. That is, P + A(x Q) N(P , P ). Therefore, theGaussian KLD from the target distribution to the adapted source data distribution is minimized to 0, andsimilarly for the KLD from the adapted source data distribution to the target distribution. This motivatesusing the Gaussian KLD as a loss function, with either the forward KLD d(P, Q) := dKL(P||Q) or reverseKLD d(P, Q) := dKL(Q||P). Note that we do not model the features as being Gaussian distributed, butrather that the conditional distribution of each samples features given confounders as having Gaussian noise. Whether forward or reverse KLD, minimizing Eq.(7) requires estimating the conditional means andconditional covariances, according to both the source and target domain estimators, evaluated at eachz DZ. (If the transformation is location-scale rather than full affine, KLD minimization requires only theconditional variances for each feature.) Given N samples in the prior distribution, each with weight givenby wn, 1 n N, let the source and target estimated conditional means be given by (n)S , (n)T , and theconditional covariances be given by (n)S , (n)T , respectively.",
  ".(10)": "While the forward KLD from target to adapted-source appears to be the natural choice, we instead proposeto use the reverse KLD. Due to its computational tractability and well-conditioned behavior, the reverse KLDhas found wide use in variational inference (Blei et al., 2017), knowledge distillation (Agarwal et al., 2023),and reinforcement learning (Kappen et al., 2012; Levine, 2018). We will see that it also confers benefits in",
  ".(11)": "This is more efficient to optimize, requiring a single matrix inversion per sample, rather than once per sampleafter each optimization update to A. This also minimizes the negative log-abs-determinant of A, whichfunctions as a log-barrier away from 0, maintaining the same sign of the determinant across optimizationiterations. This is useful, because the linear mapping between two Gaussians is not unique. The reverse-KLD,combined with an initial iterate (e.g. the identity matrix) with a positive determinant, chooses the mappingwhich preserves rather than reverses the orientation. In contrast, the forward-KLD objective is liable toproduce iterates with oscillating signs of det(A). That the ( log | det(A)|) term arises naturally out of the reverse-KLD is of potential independent interest.Preventing collapse into trivial solutions is a known problem with MMD-based domain adaptation (Singhet al., 2020; Wu et al., 2021). The reverse-KLD objective may inspire a new regularization penalty for thisproblem. The log-det heuristic was previously proposed (Fazel et al., 2003) as a smooth concave surrogate formatrix rank minimization, while here it prevents rank collapse.",
  "Furthermore, in the case of a location-scale adaptation, the reverse-KLD can be obtained via a fast exactclosed-form solution. Further details are given in Appendix A": "Using the Gaussian KLD requires an estimate of the mean and covariance of the features given each valuez of the confounder. We obtain this by generating KX samples of x DS(|Z = z) and x DT (|Z = z)per each z, then computing the empirical means and covariances. We optimize the ConDo-KLD objectivewith the PyTorch-minimize (Feinman, 2021) implementation of the trust-region Newton conjugate gradientmethod (Lin & Jorge, 1999).",
  "+ Ex(n2),x(n2)DS(|Z=z)kX (Ax(n2) + b, Ax(n2) + b).(13)": "For the feature-space kernel kX , we use the RBF kernel by default. We minimize this objective with stochasticoptimization. For each minibatch, we sample KZ values from the confounder prior; for each confounder valuewe sample KX feature-vectors from each of DT (x|Z = z) and DS(x|Z = z). Additional implementation detailsFurther implementation details and computational complexityanalysis are provided in Appendix B. Our software, with a Scikit-learn (Pedregosa et al., 2011) compatibleAPI and with experimental scripts, is available at",
  "Synthetic 1d features with 1d continuous confounder": "We first examine confounded domain adaptation in the context of a single-dimensional feature confounded bya 1d continuous confounder. We analyze the performance of vanilla and ConDo adaptations, when the effectof the continuous confounder is linear homoscedastic (left column), linear heteroscedastic (middle column),and nonlinear heteroscedastic (right column). In each case there is confounded shift, because confounder isdistributed as Uniform in the target domain, and as Uniform in the source domain. This setting isillustrated in the top row of subplots in . For each of the the three settings, we apply the baseline linear domain adaptation methods, Gaussian OTand MMD, and our ConDo versions of these methods, ConDo Gaussian KLD and ConDo MMD. We thenevaluate the different methods via root-mean-squared-error (rMSE) on a heldout dataset of 100 samples,comparing the known ground-truth to the estimates from each adaptation method. Note that in this and allfollowing settings in the rest of the paper, neither the baseline nor the ConDo methods have access to theconfounding variable values of the heldout test samples. The results are shown on each of the remaining rowsof subplots in . We repeat the above experimental setup, but with modifications to verify whether our approach can beaccurate even when its assumptions no longer apply. We run experiments with-and-without label shift (i.e.different distributions over the confounder between source and target), with-and-without feature shift (i.e.with and without batch effect), and with-and-without additional iid N(0, 1) noise, for a total of 8 settings.Heldout test errors are shown in . ConDo strongly outperforms vanilla adaptation whenever there istarget shift, and is non-inferior otherwise. In Figure S1, we also show that ConDo offers improved estimationof the feature-space mapping parameters whenever there is target shift, and is non-inferior otherwise.",
  "Synthetic 1d features with multi-dimensional continuous confounders": "We extend the previous experiment to consider the scalability of ConDo to multidimensional confounders. Forthe noise-free, label-shift, feature-shift setting, keeping all other experimental settings and hyperparametersidentical, we vary the number of confounders from 1 to 32. We first augment the number of confounders byappending additional irrelevant confounders, sampled from N(0, 1), to our inputs to the ConDo method.The results, shown in (A), indicate that ConDo is very robust to the inclusion of a moderate numberof non-confounders. We next augment the number of confounders by generating a noisy additive decomposition of our originalconfounder. We first uniformly sample the true confounder as before, and generate the feature from it asbefore. We then generate a random multidimensional confounder summing to the true confounder of thedesired dimensionality (Dickinson, 2010), and provide this to the ConDo methods. The results, provided in(B), show that ConDo is still effective in spite of this subtle relationship between the feature and themulti-dimensional confounders. We furthermore use this latter setting to examine the behavior of ConDo when confounders are partiallyobserved. Results, shown in Figure S4, indicate that typically ConDo fails to take advantage of partially-observed confounders while remaining non-inferior to baselines.",
  "Nonlinear": "0.0 2.5 5.0 7.5 10.0 rMSE NoiseFree TargetShift FeatureShiftNoisy TargetShift FeatureShift rMSE NoiseFree TargetShift NoFeatureShiftNoisy TargetShift NoFeatureShift rMSE NoiseFree NoTargetShift FeatureShiftNoisy NoTargetShift FeatureShift rMSE NoiseFree NoTargetShift NoFeatureShiftNoisy NoTargetShift NoFeatureShift Gaussian OTMMDConDo Gaussian KLDConDo MMD Figure S2:Train errors for experiment with synthetic 1d features with 1d continuous confounder. For eachadaptation method, we compute the rMSE of true target feature values vs inferred target feature values afteradaptation, averaged over 10 simulations.",
  "(B)": "# confounders rMSE # confounders # confounders source - Gaussian OTsource - MMDsource - ConDo Gaussian KLDsource - ConDo MMD Figure S3:Results on training data for transforming 1d data with multiple continuous confounders, withextra irrelevant N(0, 1) confounders, shown in (A), and with noisy additive decomposition, shown in (B). TherMSEs are averaged over 10 random simulations are shown for training data (200 samples per simulation).The columns, in order, correspond to a confounder with a linear homoscedastic effect, a confounder with alinear heteroscedastic effect, and a confounder with a nonlinear heteroscedastic effect.",
  "Synthetic 1d and 2d features with 1d categorical confounder": "Here, we generate 1d features based on the value of a 1d binary confounder. The source distribution isa mixture of Gaussians 0.25N(5, 12) + 0.75N(0, 22), while the target is 0.75N(5, 12) + 0.25N(0, 22), andthe confounder variable indicates the true mixture component. We also use this setting to analyze theperformance of ConDo for a variety of sample sizes. For each sample size under consideration, we run 10random simulations, and report the rMSE compared to the actual pre-feature shift values. Results are shown in . In (A) we see that the ConDo methods outperform the baselines, andthat they are robust to small sample sizes. As the number of samples increases, ConDo MMD converges tothe correct transformation. We see in (B) that ConDo Gaussian KLD is not as fast as the closedform Gaussian OT solution, but still scales nicely with sample size; meanwhile MMD and ConDo MMD havecomparable runtimes. The raw data and results are plotted in Figure S5 in the Appendix. In Figure S6, we also show that ConDoleads to lower-error estimates of the true feature-space mapping. We extend the previous experiment to 2dfeatures, showing reduced rMSE and improved classification accuracy in Figure S7, and improved featureshift parameter estimation in Figure S8.",
  "(A)(B)": "b b Gaussian OT MMD ConDo Gaussian KLD ConDo MMD A A b b Gaussian OT MMD ConDo Gaussian KLD ConDo MMD A A Figure S10:Feature mapping parameter estimation performance on ANSUR II, for affine (A) and location-scale (B) transforms. We depict the error in estimating b and A, respectively, with the L2 vector norm andinduced L2 matrix norm (spectral norm). The error bars in the barplots depict standard deviations over the10 random simulations.",
  "Accuracy": "Number of pairs 0.4 0.5 0.6 F1-score MMDConDo MMD Figure S16: SNAREseq cell type classifier performance on test set. Classifier performance (accuracy andF1-score) is shown as a function of the number of paired samples available to the domain adaptation methods.The error bars depict the standard error computed over 10 random simulations.",
  "Image color adaptation": "We here apply domain adaptation to the problem of image color adaptation, treating each image as a datasetwith 3 features (for the RGB components). We start by adapting back and forth between two ocean picturestaken during the daytime and sunset (the Python Optimal Transport library (Flamary et al., 2021) GaussianOT example), depicted in the top row of (A). In this scenario, there is no confounding, since theimages contains water and sky in equal proportions. Thus, conditioning on each pixel label (a categoricalconfounder, taking on a value of either water or sky), is expected to be unnecessary. Below the top row,we show the results of affine adaptation for each of the different methods. On the left column of (A), we show the result of applying the learned source-to-target transform. On the right column of (A), compute the inverse of the aforementioned source-to-target transform, and treat it as a target-to-sourcetransform. We see that both Gaussian OT and Condo Gaussian KLD learn adaptations that work properlyon both source-to-target and target-to-source. MMD produces images with appropriate color balances, butwith colors applied to inappropriate parts of the image. ConDo MMD produces a good source-to-targetimage, but its inverse mapping is bad. Next, we attempted color adaptation between the ocean daytime photo and another sunset photo includingbeach, water, and sky, shown in (B). Here, there is confounded shift, so ConDo utilizes pixels labeledas sky, water, or sand. Now the source-to-target adapted images have poor (grayish) color balancesfor the baseline methods, while they have good color balances for the ConDo methods. For the inversetarget-to-source mapping, both MMD and ConDo MMD struggle.",
  "California housing price prediction": "Here, we apply ConDo to an unsupervised domain adaptation setting derived from the California housingdataset (Pace & Barry, 1997). We split the data into source and target domains based on the first feature,Median Income, defining the source domain as being the housing districts with income less than or equal tothe median. The geographical results of this source-target split are shown in (A), with examplesplotted according to their LatLon features. We see that higher-income districts are geographically clustered,and thus use LatLon coordinates as the two confounding variables. As our classification task, we predictwhether the mean house value in each district exceeds the median over the entire dataset, 1.797 (in $100k).As features, we use all remaining features aside from Median Income. We train a TabPFN classifier on the source domain, and evaluate it on target domain, repeated over 10random simulations, each with 500 source training samples and 500 target test samples in each simulation.",
  "Day Beach": "ConDo MMD : Image color adaptation results without (A) and with (B) confounded shift. We see that ConDois non-inferior in (A). In (B), we see that non-ConDo methods produce gray-ish Day Beach images.Meanwhile, ConDo methods produces light blue sky and yellow clouds in Day Beach images. ConDoGaussian KLD is the only method to produce normal-looking images for all four tasks. We perform location-scale domain adaptation to make the target features resemble the source features beforeapplying the classifier. As shown in (B-C), vanilla Gaussian OT and MMD methods make theperformance even worse than no adaptation. We also see that the ConDo methods better than the vanillamethods, and that ConDo Gaussian KLD improves upon no adaptation. In the Appendix Section C.6, weshow similar results for training a classifier on adapted-source-to-target data, with ConDo outperforming thebaselines.",
  "SNAREseq single-cell multi-omics dataset": "We apply domain adaptation to a SNAREseq single-cell dual-omics dataset (Demetci et al., 2022). In thedataset, 1047 cells are co-assayed with RNA-seq for gene expression and ATAC-seq for chromatin accessibility.Each sequenced cell is associated with one of four cell types; unique cell identities are also given, so matchedsample pairs across the two modalities are known. We treat ATAC-seq assay features as source domain andRNA-seq assay features as target domain. For ConDo, we perform adaptation controlling for cell type, theconfounding variable. We first simulate unsupervised domain adaptation in which we perform cell type classification with TabPFN(Hollmann et al., 2022). In each simulation, we have a large source domain training set of 500 source cells.We also simulate having a small set of C cells for which we have both source and target domain data availableto use for adaptation; we simulate C {5, 10, 20, 50, 100}. We then evaluate the classifier on the remainingunseen cells using target domain data (ATAC-seq features). Because the feature dimensionality differs forthe two assays, we only perform MMD and ConDo MMD adaptation with affine transforms. Results for 10independent random simulations, depicted in , show that ConDo MMD outperforms MMD. Similaradvantage for ConDo is shown when classifiers were instead trained on adapted source-to-target features inFigure S16 in Appendix Section C.7. Number of pairs 0.50 0.55 0.60 0.65",
  "Gene expression microarray batch effect correction": "We analyze performance on the bladderbatch gene expression dataset commonly used to benchmark batchcorrection methods (Dyrskjt et al., 2004; Leek, 2016). We use all 22,283 gene expressions (i.e. features)from this bladder tissue Affymetrix microarray dataset; bladderbatch was preprocessed so that each feature isapproximately Gaussian. We attempt a location-scale transform, as is typical with gene expression batcheffect correction. We choose the second largest batch (batch 2, with 4 cancer samples out of 18 total) as thesource domain, and the largest batch (batch 5, with 5 cancer samples out of 19 total) as the target domain.The confounder is 1d categorical (cancer or non-cancer). For each method, we compute the silhouette scores of the adapted datasets, with respect to the batch variable(and, in parentheses, the test result variable). We desire the silhouette score to be large for the cancer status,and to be close to 0 for the batch label. Because the cancer fractions are roughly the same (4/18 vs 5/19) forbatches 2 and 5, we do not expect to need to account for confounding. Results are shown in . We seethat all domain adaptation methods improve upon no adaptation. For cancer status, MMD has the largestsilhouette score as desired. For the batch variable, all adaptations produced slightly negative scores. Whilethis is less concerning than the larger magnitude positive value (0.0884) for no adaptation, it may suggestoverfitting; the silhouette score is closest to zero for ConDo MMD. We repeat the experiment after inducing confounding by removing samples. For 10 random simulations, weremove half (7) of the non-cancer samples in batch 2, so that batch 2 is 4/11 non-cancerous, while batch 5remains 5/19 non-cancerous. Results are shown in . We see that ConDo methods outperform theirbaseline counterparts, and that ConDo Gaussian KLD in particular produces gene expression data in whichvariation concords with cancer status rather than batch.",
  "Related Work": "As far as we are aware, previous work on domain adaptation does not describe or address our exact problem.There is a large body of research in domain adaptation which maps both source and target distributions to anew latent representation where they match (Baktashmotlagh et al., 2013; Yan et al., 2017; Ganin et al.,2016; Gong et al., 2016). These however cannot achieve data backwards-compatibility, because they create anew latent domain. Other domain adaptation methods are also inapplicable to our setting since they matchdistributions via reweighting samples (Cortes & Mohri, 2011; Tachet des Combes et al., 2020) or droppingfeatures (Kouw et al., 2016). Prior research exists for performing domain adaptation when both features and label are shifted, includingthe generalized label shift (GLS) / generalized target shift (GeTarS) (Zhang et al., 2013; Rakotomamonjyet al., 2020; Tachet des Combes et al., 2020). However, these methods assume the specific prediction settingwhere the label is the confounder, and optimize composite objectives that combine distribution matching andprediction accuracy. In our case, the confounder may not be the label of our prediction model of interest,and indeed we may not even be mapping covariates for the purpose of any downstream prediction task.Furthermore, by conditioning on confounders, our framework can handle multivariate confounders or evencomplex objects which are accessed only via kernels. Landeiro et al. introduced the term confoundingshift to describe a form of GLS/GeTarS, but it does not match our confounded shift assumption, since theconfounding variables are unobserved. Their method, which comprises confounder detection and adversarialconfounder-robust classification, is substantially different from our approach. The most relevant domain adaptation methods for our context perform asymmetric feature transformation,in which source features are adapted to target features, and are thus compatible with general-purposebackwards compatibility. EasyAdapt (Daum III, 2007) and EasyAdapt++ (Daum III et al., 2010) arenotably successful approaches performing supervised learning with data from multiple domains, but they donot provide transformed features for data analysis. Furthermore, EasyAdapt relies on feature concatenation;we expect to not have confounders available at inference time, which means that we cannot utilize them inthe concatenated features at training time either. Previous work which explicitly matches conditional distributions (Long et al., 2013) instead uses the conditionaldistribution of the label given the features, rather than our approach of matching the features conditioned onthe labels. It also constructs a new latent space, rather than mapping from source to target for backwardscompatibility. Optimal transport has also been proposed as a framework for domain adaptation (Courty et al., 2014), andwithin this framework, unbalanced optimal transport has been proposed to address distribution shift (Chizatet al., 2018). This relaxes the OT constraint for conservation of mass to a penalty on deviations from massconservation, yielding improvements in label shift scenarios (Yang & Uhler, 2018). However, this approachrequires challenging adversarial training (Yang & Uhler, 2018), and fails to utilize potentially available sideinformation from confounding variables. Our work is more similar in spirit with optimal transport withsubset correspondence (OT-SI) (Liu et al., 2019b), which implicitly conditions on a categorical confounder(the samples subset) to learn an optimal transport map. Our framework explicitly conditions on confounders",
  "Limitations": "The first main limitation of our framework is that we assume access to all confounders at training time. Whileour experiments suggest that our approach works well given a superset of the true confounding variables, ourexperiments suggest that ConDo fails to benefit from receiving a subset of the true confounders. Controllingfor partially-latent or fully-latent confounding remains as future work. A second limitation is that weassume a deterministic mapping (and in our concrete implementations, a linear mapping) between featurespaces. It would be nontrivial to extend our approach to non-deterministic mappings with distributionalregression or conditional generative modeling. A third limitation is that, even if we are able to learn theground-truth feature adaptation, we may only use the adapted features for downstream prediction taskswhere the conditional distribution of the target variable given features is the same for source and target. Furthermore, despite our limiting assumptions, both our proposed divergences, the reverse KLD and theMMD, suffer from non-identifiability. Multiple transformations may match the source distribution to thetarget distribution, and both our objective functions are indifferent among such transforms. We currentlyrely on gradient flow from the initial parameters to make a sensible choice, but this offers no guarantees.",
  "Future work": "Optimal transport (OT) offers a principled criteria, minimal transport cost, to choose among transformationswhich provide equal fit to the data. This suggests replacing the reverse KLD and MMD with an OT-basedalternative, such as the Wasserstein distance. Yet, while minimal transport cost is an excellent prior, itis not the only defensible choice. For example, Lp regularization, empirical Bayes weight sharing such asused by ComBat (Johnson et al., 2007), and constraints (e.g. non-negativity or zero-off-diagonal for linearmappings) may instead be preferred, and may be fruitfully combined with KLD and/or MMD. Due to the affine restriction on the transformation, our proposed approach is more appropriate for adaptationsettings where source and target correspond to different versions of experimental assays and similar settingswhere the required adaptation is affine (or even location-scale). It would be useful to examine whether ourframework extends gracefully to nonlinear adaptations parameterized by neural networks.",
  "Conclusion": "We have shown that minimizing expected divergences / distances after conditioning on confounders is apromising avenue for domain adaptation in the presence of confounded shift. Our proposed use of the reverseKLD are (to our knowledge) new in the field of domain adaptation, and may be more broadly useful. Focusingon settings where the effect of the confounder is possibly complex, yet where the source-target domains can belinearly adapted, we demonstrated the usefulness of algorithms based on our framework. These experimentsshow that conditioning on confounders via our ConDo framework improves the quality of learned adaptationsfor a variety of domains and tasks.",
  "Bronstein, Gordon, & Naef, Luca. 2024. The Road to Biology 2.0 Will Pass Through Black-Box Data.Towards Data Science. 1": "Burley, Stephen K, Berman, Helen M, Chen, Li, Bhikadiya, Charmi, Bi, Chunxiao, Dutta, Shuchismita, Feng,Zukang, Di Costanzo, Luigi, Ghosh, Sutapa, Christie, Cole, et al. 2019. Protein Data Bank: the singleglobal archive for 3D macromolecular structure data. Nucleic acids research, 47(D1), D520D528. 1 Cai, Hao, Li, Xiangyu, Li, Jing, Liang, Qirui, Zheng, Weicheng, Guan, Qingzhou, Guo, Zheng, & Wang,Xianlong. 2018. Identifying differentially expressed genes from cross-site integrated data based on relativeexpression orderings. International Journal of Biological Sciences, 14(8), 892. 1",
  "Daume III, Hal, & Marcu, Daniel. 2006. Domain adaptation for statistical classifiers. Journal of artificialIntelligence research, 26, 101126. 4": "Daum III, Hal, Kumar, Abhishek, & Saha, Avishek. 2010. Frustratingly easy semi-supervised domainadaptation.Pages 5359 of: Proceedings of the 2010 Workshop on Domain Adaptation for NaturalLanguage Processing. 19 Demetci, Pinar, Santorella, Rebecca, Sandstede, Bjrn, Noble, William Stafford, & Singh, Ritambhara. 2022.SCOT: single-cell multi-omics alignment with optimal transport. Journal of Computational Biology, 29(1),318. 17, 29",
  "Flamary, Rmi, Lounici, Karim, & Ferrari, Andr. 2019. Concentration bounds for linear monge mappingestimation and optimal transport domain adaptation. arXiv preprint arXiv:1905.10155. 4": "Flamary, Rmi, Courty, Nicolas, Gramfort, Alexandre, Alaya, Mokhtar Z., Boisbunon, Aurlie, Chambon,Stanislas, Chapel, Laetitia, Corenflos, Adrien, Fatras, Kilian, Fournier, Nemo, Gautheron, Lo, Gayraud,Nathalie T.H., Janati, Hicham, Rakotomamonjy, Alain, Redko, Ievgen, Rolet, Antoine, Schutz, Antony,Seguy, Vivien, Sutherland, Danica J., Tavenard, Romain, Tong, Alexander, & Vayer, Titouan. 2021. POT:Python Optimal Transport. Journal of Machine Learning Research, 22(78), 18. 15, 27, 28 Ganin, Yaroslav, Ustinova, Evgeniya, Ajakan, Hana, Germain, Pascal, Larochelle, Hugo, Laviolette, Franois,Marchand, Mario, & Lempitsky, Victor. 2016. Domain-adversarial training of neural networks. The journalof machine learning research, 17(1), 20962030. 19 Girshick, Ross, Donahue, Jeff, Darrell, Trevor, & Malik, Jitendra. 2014. Rich feature hierarchies for accurateobject detection and semantic segmentation. Pages 580587 of: Proceedings of the IEEE conference oncomputer vision and pattern recognition. 4 Gong, Mingming, Zhang, Kun, Liu, Tongliang, Tao, Dacheng, Glymour, Clark, & Schlkopf, Bernhard.2016. Domain adaptation with conditional transferable components. Pages 28392848 of: Internationalconference on machine learning. PMLR. 19 Gordon, Claire C, Blackwell, Cynthia L, Bradtmiller, Bruce, Parham, Joseph L, Barrientos, Patricia, Paquette,Stephen P, Corner, Brian D, Carson, Jeremy M, Venezia, Joseph C, Rockwell, Belva M, et al. 2014. 2012anthropometric survey of us army personnel: Methods and summary statistics. Army Natick SoldierResearch Development and Engineering Center MA, Tech. Rep. 13 Gregori, Josep, Villarreal, Laura, Mndez, Olga, Snchez, Alex, Baselga, Jos, & Villanueva, Josep. 2012.Batch effects correction improves the sensitivity of significance tests in spectral counting-based comparativediscovery proteomics. Journal of Proteomics, 75(13), 39383951. 1 Haghverdi, Laleh, Lun, Aaron TL, Morgan, Michael D, & Marioni, John C. 2018. Batch effects in single-cellRNA-sequencing data are corrected by matching mutual nearest neighbors. Nature biotechnology, 36(5),421427. 1",
  "Hicks, Stephanie C, Townes, F William, Teng, Mingxiang, & Irizarry, Rafael A. 2018. Missing data andtechnical variability in single-cell RNA-sequencing experiments. Biostatistics, 19(4), 562578. 1": "Hollmann, Noah, Mller, Samuel, Eggensperger, Katharina, & Hutter, Frank. 2022. Tabpfn: A transformerthat solves small tabular classification problems in a second. arXiv preprint arXiv:2207.01848. 10, 15, 17 Huang, Jiayuan, Gretton, Arthur, Borgwardt, Karsten, Schlkopf, Bernhard, & Smola, Alex. 2006. Correctingsample selection bias by unlabeled data. Advances in neural information processing systems, 19. 2",
  "Kull, Meelis, & Flach, Peter. 2014. Patterns of dataset shift. In: First International Workshop on Learningover Multiple Contexts (LMCE) at ECML-PKDD. 6": "Landeiro, Virgile, Tran, Tuan, & Culotta, Aron. 2019. Discovering and controlling for latent confounds intext classification using adversarial domain adaptation. Pages 298305 of: Proceedings of the 2019 SIAMInternational Conference on Data Mining. SIAM. 19 Leek, Jeffrey T, Scharpf, Robert B, Bravo, Hctor Corrada, Simcha, David, Langmead, Benjamin, Johnson,W Evan, Geman, Donald, Baggerly, Keith, & Irizarry, Rafael A. 2010. Tackling the widespread and criticalimpact of batch effects in high-throughput data. Nature Reviews Genetics, 11(10), 733739. 1",
  "Pan, Sinno Jialin, Tsang, Ivor W, Kwok, James T, & Yang, Qiang. 2010. Domain adaptation via transfercomponent analysis. IEEE transactions on neural networks, 22(2), 199210. 2": "Pedregosa, Fabian, Varoquaux, Gal, Gramfort, Alexandre, Michel, Vincent, Thirion, Bertrand, Grisel,Olivier, Blondel, Mathieu, Prettenhofer, Peter, Weiss, Ron, Dubourg, Vincent, et al. 2011. Scikit-learn:Machine learning in Python. the Journal of machine Learning research, 12, 28252830. 9 Pelletier, Simon J, Leclercq, Mickal, Roux-Dalvai, Florence, de Geus, Matthijs B, Leslie, Shannon, Wang,Weiwei, Lam, TuKiet T, Nairn, Angus C, Arnold, Steven E, Carlyle, Becky C, et al. 2024. BERNN:Enhancing classification of Liquid Chromatography Mass Spectrometry data with batch effect removalneural networks. Nature Communications, 15(1), 3777. 1",
  "Peyr, Gabriel, Cuturi, Marco, et al. 2019. Computational optimal transport: With applications to datascience. Foundations and Trends in Machine Learning, 11(5-6), 355607. 4": "Radua, Joaquim, Vieta, Eduard, Shinohara, Russell, Kochunov, Peter, Quid, Yann, Green, Melissa J,Weickert, Cynthia S, Weickert, Thomas, Bruggemann, Jason, Kircher, Tilo, et al. 2020. Increased powerby harmonizing structural MRI site differences with the ComBat batch adjustment method in ENIGMA.Neuroimage, 218, 116956. 2 Rakotomamonjy, Alain, Flamary, Rmi, Gasso, Gilles, Alaya, Mokhtar Z, Berar, Maxime, & Courty, Nicolas.2020. Match and reweight strategy for generalized target shift. arXiv preprint arXiv:2006.08161. 19 Shafieezadeh Abadeh, Soroosh, Nguyen, Viet Anh, Kuhn, Daniel, & Mohajerin Esfahani, Peyman M. 2018.Wasserstein distributionally robust Kalman filtering. Advances in Neural Information Processing Systems,31. 4 Shaham, Uri, Stanton, Kelly P, Zhao, Jun, Li, Huamin, Raddassi, Khadir, Montgomery, Ruth, & Kluger,Yuval. 2017. Removal of batch effects using distribution-matching residual networks. Bioinformatics,33(16), 25392546. 1 Singh, Ritambhara, Demetci, Pinar, Bonora, Giancarlo, Ramani, Vijay, Lee, Choli, Fang, He, Duan, Zhijun,Deng, Xinxian, Shendure, Jay, Disteche, Christine, et al. 2020. Unsupervised manifold alignment forsingle-cell multi-omics data. Pages 110 of: Proceedings of the 11th ACM International Conference onBioinformatics, Computational Biology and Health Informatics. 5, 9 Sohl-Dickstein, Jascha, Weiss, Eric, Maheswaranathan, Niru, & Ganguli, Surya. 2015. Deep unsupervisedlearning using nonequilibrium thermodynamics. Pages 22562265 of: International conference on machinelearning. PMLR. 8 Sprang, Maximilian, Andrade-Navarro, Miguel A, & Fontaine, Jean-Fred. 2022. Batch effect detectionand correction in RNA-seq data using machine-learning-based automated assessment of quality. BMCbioinformatics, 23(Suppl 6), 279. 1 Tachet des Combes, Remi, Zhao, Han, Wang, Yu-Xiang, & Gordon, Geoffrey J. 2020. Domain adaptation withconditional distribution matching and generalized label shift. Advances in Neural Information ProcessingSystems, 33, 1927619289. 2, 5, 19 Torbati, Mahbaneh Eshaghzadeh, Minhas, Davneet S, Ahmad, Ghasan, OConnor, Erin E, Muschelli, John,Laymon, Charles M, Yang, Zixi, Cohen, Ann D, Aizenstein, Howard J, Klunk, William E, et al. 2021. Amulti-scanner neuroimaging data harmonization using RAVEL and ComBat. Neuroimage, 245, 118703. 1,2 Tzeng, Eric, Hoffman, Judy, Saenko, Kate, & Darrell, Trevor. 2017. Adversarial discriminative domainadaptation. Pages 71677176 of: Proceedings of the IEEE conference on computer vision and patternrecognition. 2",
  "Wilson, Samuel Von, Cebere, Bogdan, Myatt, James, & Wilson, Samuel. 2022 (Dec.). AnotherSamWil-son/miceforest: Release for Zenodo DOI. 8, 26": "Wu, Xiaofu, Zhang, Suofei, Zhou, Quan, Yang, Zhen, Zhao, Chunming, & Latecki, Longin Jan. 2021. EntropyMinimization Versus Diversity Maximization for Domain Adaptation. IEEE Transactions on NeuralNetworks and Learning Systems. 9 Yamashita, Ayumu, Yahata, Noriaki, Itahashi, Takashi, Lisi, Giuseppe, Yamada, Takashi, Ichikawa, Naho,Takamura, Masahiro, Yoshihara, Yujiro, Kunimatsu, Akira, Okada, Naohiro, et al. 2019. Harmonization ofresting-state functional MRI data across multiple imaging sites via the separation of site differences intosampling bias and measurement bias. PLoS biology, 17(4), e3000042. 1 Yan, Hongliang, Ding, Yukang, Li, Peihua, Wang, Qilong, Xu, Yong, & Zuo, Wangmeng. 2017. Mind the classweight bias: Weighted maximum mean discrepancy for unsupervised domain adaptation. Pages 22722281of: Proceedings of the IEEE conference on computer vision and pattern recognition. 5, 19",
  "BImplementation details and computational complexity analysis": "Recall that we have NS and NT source and target samples, respectively; let us use NS and NT to denote theunique values of the source and target confounders. Let N = max(NS, NT ) and N = max( NS, NT ). We useM = max(MS, MT ) as the feature dimension, and let the complexity of evaluating kZ(z(n1), z(n2)) be O(C).Recall that KX is the number of multiple imputations per Z value. Also recall that KZ is the number ofsampled Z values used per ConDo-MMD minibatch. The complexity of computing the product prior weights is O(C N 2). We achieve this speedup by, ratherthan assigning weights to each sample, instead assigning weights to each unique value of the confounder andmultiplying the weights from Eq. (9) with the number of observations of each unique value. When confounding variables are discrete, conditional sampling has negligible runtime cost. When confoundersare continuous, the runtime is dominated by MICE-Forest (Wilson et al., 2022) training LightGBM (Ke et al.,2017) models for each feature given all the other features. The runtime of each LightGBM model is O(NM),we run the MICE imputation algorithm for a fixed number (2) of iterations, and generate KX imputations,so the total runtime complexity for this step is O(NM 2KX ). The space complexity of conditional sampling is potentially burdensome. Compared to input datasets ofsize NS MS and NT MT , we generate datasets of size KX N MS and KX NS MS, for source andtarget, respectively. In our provided software, we implement an option to subsample a fraction F 1 of Nsamples at each epoch before performing conditional sampling, reducing datasets to size FKX N MS andFKX NS MS. For ConDo-KLD, we compute means and (inverse) covariances for each confounder value. Computing thesestatistics from the conditionally sampled data, for continuous confounders, has complexity O( NKX M 2+ NM 3).",
  "In Figure S3, we show the rMSE on the training set for multi-dimension contiguous confounder experiments": "In Figure S4, we show the effect of running ConDo with partially-observed confounding variables. We see that,on the one hand, ConDo is typically non-inferior to the baselines when confounders are partially observed.On the other hand, ConDo performance quickly degrades as the number of observed confounders is reduced,so that when half of the confounding variables are observed, ConDo is not better than the baselines.",
  "C.3.1Synthetic 1d feature with 1d categorical confounder": "Figure S5 provides additional results for our experiments with a 1d feature confounded by a 1d categoricalconfounding variable. We see that both ConDo methods match the true distribution better than the baselinemethods. In particular, as sample size increases, ConDo MMD converges towards the true target distribution.We confirm this in Figure S6, showing that ConDo leads to lower-error estimates of the true mapping, andthat ConDo MMD in particular has decreasing error as a function of sample size.",
  "circles centered at (0, 0) and (0, 2); the points are distributed with angle distributed iid around the circlefrom U, and with radius sampled iid from N(0, 1)": "Next, we analyze the performance of our approach on 2d features requiring an affine transformation. We alsouse this setting to assess the downstream performance of classifiers which are fed the adapted source-to-targetfeatures. The synthetic 2d features, before the batch effect, form a slanted 8 shape, shown in blue/green inFigure S7. A linear classifier separating the upper and lower loops is depicted in cyan, for both the sourceand target domains. Our results are shown in Figure S7. On the left column, we compare methods in the case where thereis no confounded shift. (This setting is from Python Optimal Transport (Flamary et al., 2021).) In themiddle column, we have induced a confounded shift: One-fourth of the source domain samples come fromthe upper loop of the 8, while half of the target domain samples come from the upper loop. This allowsus to assess the affects of confounded shift on downstream prediction of the confounder (up-vs-down), aswell as a non-confounder (left-vs-right). In the right column, we have induced a confounded shift as before,while making the true source-target transform more challenging, by using a randomly-generated true affinetransform. The affine transform matrix is set to be random yet positive-determinant via",
  "C.4ANSUR II anthropometric survey data": "In all ANSUR II experiments, we used the hyperparameters given in the main text. We used the defaultparameters for TabPFN. In Figure S9, we show results for the same experimental setup as , exceptthat TabPFN prediction models are instead trained on adapted source-to-target features, then applied ontarget features. We see the same pattern, with ConDo methods outperforming their baseline counterparts,especially for the affine transformation scenario. In Figure S10, we show feature-space mapping parameter estimation performance. In this setting, MMD,ConDo Gaussian KLD, and ConDo MMD perform equally well. Meanwhile, Gaussian OT performs poorlyon the affine transform scenario, while outcompeting the others on the location-scale transform scenario. In Figure S12, we show results for target feature data simulated with no feature shift (A = Id, b = 0), butwith a male-female split of 75%-25% in source and 25%-75% in target. As before, we show domain adaptationmethods attempting to learn affine and location-scale feature transformations. In Figure S11, we show resultsfor target feature data simulated with no shift in the confounder distribution (probability of sampling maleand female each set to 0.5 for both source and target), but with domain adaptation methods attemptingto learn affine and location-scale feature transformations. In Figure S13, we show results for target featuredata simulated with no change in the shift in the confounder distribution and with no feature shift, but withdomain adaptation methods attempting to learn affine and location-scale feature transformations.",
  "C.6California housing price prediction": "The California Housing dataset has 20,640 samples (before subsampling as described in the main text),and the classifier receives 7 input features (with median income removed). In all experiments, we used thehyperparameters given in the main text. We used the default parameters for the TabPFN classifier. In Figure S15, we repeat the previous experiment, except that TabPFN prediction models are instead trainedon adapted source-to-target features on the train-set; then the model is provided target test-set featuresdirectly. We see the same pattern as before, with ConDo methods outperforming their baseline counterparts.",
  "C.7SNAREseq single-cell multi-omics dataset": "For our experiment with SNAREseq data (Demetci et al., 2022), the ATAC-seq dataset has 19 features, andthe RNA-seq dataset has 10 features. The data was preprocessed in (Demetci et al., 2022), including unitnormalization as in (Demetci et al., 2022). In all SNAREseq experiments, we used the hyperparameters givenin the main text. In Figure S16, we show results for the setting where classifiers were instead trained on adapted source-to-targetfeatures. In other words, using the paired samples, we learned a transformation from source domain to targetdomain. We then applied this transformation to the 500 source domain training samples, and fitted theclassifier on these transformed features. Finally, we applied the classifier to (untransformed) target domainfeatures, and then evaluated predictions.",
  "Heteroscedastic": "LinearNonlinear 0.0 0.2 0.4 0.6 A A Noisy-NoTargetShift-NoFeatureShift Gaussian OTMMDConDo Gaussian KLDConDo MMD Figure S1:Parameter estimation for transform of 1d data with a continuous confounder. We depict theerror in estimating b and A, respectively, with the L2 vector norm and induced L2 matrix norm (spectralnorm). The error bars in the barplots depict standard deviations over the 10 random simulations.",
  "(B) 32 confounders": "# observed confounders rMSE # observed confounders # observed confounders source - Gaussian OTsource - MMDsource - ConDo Gaussian KLDsource - ConDo MMD Figure S4:Results for transforming 1d data with multiple continuous confounders with noisy additivedecomposition, while varying both the number of actual confounders and the number provided to ConDomethods. In each row, we show results for a given number of actual confounders; within each plot, we showConDo performance as a function of how many were provided to ConDo. The rMSEs are averaged over 10random simulations are shown for heldout test data (100 samples per simulation). The columns, in order,correspond to a confounder with a linear homoscedastic effect, a confounder with a linear heteroscedasticeffect, and a confounder with a nonlinear heteroscedastic effect.",
  "(A)(B)(C)": "hotdognot_hotdog confounder feature targetsource - true (unobserved)source - batch-effectedsource - Gaussian OTsource - MMDsource - ConDo Gaussian KLDsource - ConDo MMD hotdognot_hotdog confounder feature targetsource - true (unobserved)source - batch-effectedsource - Gaussian OTsource - MMDsource - ConDo Gaussian KLDsource - ConDo MMD hotdognot_hotdog confounder feature targetsource - true (unobserved)source - batch-effectedsource - Gaussian OTsource - MMDsource - ConDo Gaussian KLDsource - ConDo MMD",
  "(D)(E)(F)": "hotdognot_hotdog confounder feature targetsource - true (unobserved)source - batch-effectedsource - Gaussian OTsource - MMDsource - ConDo Gaussian KLDsource - ConDo MMD hotdognot_hotdog confounder feature targetsource - true (unobserved)source - batch-effectedsource - Gaussian OTsource - MMDsource - ConDo Gaussian KLDsource - ConDo MMD hotdognot_hotdog confounder feature targetsource - true (unobserved)source - batch-effectedsource - Gaussian OTsource - MMDsource - ConDo Gaussian KLDsource - ConDo MMD Figure S5:Additional results for 1d feature confounded by 1d category. Results are shown (A) N = 10, (B)N = 20, (C) N = 50, and (D) N = 100, (E) N = 200, and (F) N = 500 samples in each of the source andtarget training sets. We depict the original, latent, shifted, and adapted data, for each value of the categoricalconfounder. bb Gaussian OT MMD ConDo Gaussian KLD ConDo MMD N=10 bb N=20 bb N=50 bb N=100 bb N=200 bb N=500 00.2 AA Gaussian OT MMD ConDo Gaussian KLD ConDo MMD 00.2 AA 00.2 AA 00.2 AA 00.2 AA 00.2 AA Figure S6:Parameter estimation performance for transform of 1d data with a categorical confounder. Wedepict the error in estimating b and A, respectively, with the L2 vector norm and induced L2 matrix norm(spectral norm). The error bars in the barplots depict standard deviations over the 10 random simulations.Columns correspond to varying sample sizes.",
  "Confounded": "0.00.51.0 bb Confounded - Random Transform AA Gaussian OT MMD ConDo Gaussian KLD ConDo MMD 0.00.51.01.5 AA 0.00.51.01.5 AA Figure S8:Parameter estimation for affine transform of 2d data with a categorical confounder. We depictthe error in estimating b and A, respectively, with the L2 vector norm and induced L2 matrix norm (spectralnorm). The error bars in the barplots depict standard deviations over the 10 random simulations.",
  "No Adaptation": "Gaussian OT MMD ConDo Gaussian KLD ConDo MMD Figure S15: Results on the California Housing dataset, for the setting where TabPFN prediction modelsare trained on adapted source-to-target features and applied to target features. We show accuracy (A) andF1-score (B) on target test data, over 10 random simulations, with error bars depicting the standard deviationover simulations. Number of pairs 0.5 0.6 0.7"
}