{
  "Abstract": "There has been a huge effort to tackle the Domain Generalization (DG) problem with afocus on developing new loss functions. Inspired by the image generation capabilities ofthe diffusion models, we pose a pivotal question: Can diffusion models function as dataaugmentation tools to address DG from a data-centric perspective, rather than relying onthe loss functions? Our findings reveal that trivial cross-domain data augmentation (CDGA)along with the vanilla ERM using readily available diffusion models without additionalfinetuning outperforms state-of-the-art (SOTA) training algorithms. This paper delves into the exploration of why and how this rudimentary data generation canoutperform complicated DG algorithms. With the help of domain shift quantification tools,We empirically show that CDGA reduces the domain shift between domains. We empiricallyreveal connections between the loss landscape, adversarial robustness, and data generation,illustrating that CDGA reduces loss sharpness and improves robustness against adversarialshifts in data. Additionally, we discuss our intuitions that CDGA along with ERM can beconsidered as a way to replace the pointwise kernel estimates in ERM with new densityestimates in the vicinity of domain pairs which can diminish the true data estimation errorof ERM under domain shift scenario. These insights advocate for further investigation intothe potential of data-centric approaches in DG.",
  "Introduction": "Out-of-distribution (OOD) generalization stands as a crucial capability for deep learning models in real-worldscenarios. The prevalent setting for investigating OOD generalization is termed domain generalization (DG)Blanchard et al. (2011), involving multiple source domains to generalize to an unseen target domain. InDG problems, there is a shift between the training domains and the target domain which makes the modelstrained using Empirical Risk Minimization (ERM) Vapnik (1999b) struggle to maintain their performance inthe target domain. To enhance OOD generalization within the DG framework, researchers have proposed innovative lossfunctionstypically achieved by introducing regularizers to ERMto facilitate the learning of domain-invariant mechanisms across domains. Nevertheless, none of these approaches consistently outperforms othersacross all datasets, as illustrated by results from the DomainBed benchmark (Gulrajani & Lopez-Paz, 2020).This observation suggests that a singular regularizer capable of capturing all invariances might not exist. Weposit that the absence of such a universal regularizer arises from the diverse shifts present in each dataset,encompassing correlation shift, diversity shift, label shift, etc. Consequently, a rigid, data-independentregularizer may fall short in eliminating all types of spurious correlations and shifts. Additionally, theincorporation of sub-optimal regularizers can exacerbate optimization challenges, introducing excessive risk(Sener & Koltun, 2022), additional hyperparameters, and computational bottlenecks in ERM. Rather than relying solely on traditional loss functions, recent advancements in generative foundationmodels open up new avenues for addressing the DG problem from a data-centric standpoint. Specifically,the capability of Denoising Diffusion Models (Ho et al., 2020; Song et al., 2020; Rombach et al., 2022) ingenerating high-fidelity synthetic images offers an innovative approach for advanced data augmentation,",
  "Fish": "ERM + CDGAState-of-the-art : Improvements of CDGA + ERM and state-of-the-art (SOTA) DG training methods over ERMacross four dataset using DomainBed benchmark. See for experimental details. CDGA + ERMoutperforms SOTA algorithms across all datasets, highlighting the absence of a singular algorithm achievingbest results across datasets. enabling the creation of domain-invariant images to enhance OOD generalization. To examine this hypothesis,we employ a straightforward Cross Domain Generative Augmentation (CDGA) method. In CDGA, syntheticimages are generated conditioned on images or text descriptions from all possible combinations of the trainingdomains using a pre-trained latent diffusion model (LDM) (Rombach et al., 2022). In , we showthat applying vanilla ERM to combined generated and real images outperforms the previous state-of-the-artalgorithms across all datasets. This paper delves into an exploration of the reasons and mechanisms behind the superior performance ofCDGAs simplistic data generation strategy compared to complex DG training algorithms. Our investigationinvolves quantifying and visualizing domain shifts across domains of generated synthetic images, validatingthat cross-domain data generation mitigates the gap between domains. We also discuss our intuitions thatCDGA can be seen as a way to replace pointwise kernel estimates in ERM with new density estimates inthe proximity of domain pairs. This modification to ERM can the inherent data estimation error in thepresence of domain shift, subsequently enhancing its out-of-distribution (OOD) performance. Furthermore,our empirical results establish connections between the loss landscape, adversarial robustness, and datageneration, revealing that cross-domain data generation lessens loss sharpness and improves robustness againstadversarial shifts in data. To the best of our knowledge, we are the first to utilize latent diffusion models as adata-centric approach for DG.",
  "Our primary contributions are as follows:": "1. Demonstrating superior performance, our study reveals that combining CDGA with vanilla ERMoutperforms state-of-the-art DG training algorithms. We validate this across four datasets usingthree model selection strategies. 2. Employing various metrics such as transfer measure, diversity shift, and near-duplicate quantification,we empirically demonstrate that CDGA reduces distribution shift among domains, attributing to itssuperior performance. 3. Providing possible intuitions that CDGA can be seen as a way to replace pointwise kernel estimatesin ERM with new density estimates in the proximity of domain pairs similar to the Vicinal RiskMinimization principle (VRM) (Chapelle et al., 2000) within DG setup.",
  "Under review as submission to TMLR": "Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towardsdeep learning models resistant to adversarial attacks.In 6th International Conference on LearningRepresentations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings.OpenReview.net, 2018. URL Maxime Oquab, Timothe Darcet, Tho Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, PierreFernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visualfeatures without supervision. arXiv preprint arXiv:2304.07193, 2023.",
  "Problem Settings and Related Work": "We denote our prediction model as f and its parameters as . In DG, the goal is to learn a shared modelfrom n source (train) population domains (environments) {E1, . . . , En}, to generalize to an unseen targetdomain T . For a given domain E, the classification loss is:",
  "k=1(f(xik; ), yik)(2)": "where (xik, yik) Ei is an i.i.d. sample from the training set Si of domain i with |Si| samples, xik is the k-thdata point in Si and yik is its associated label. However, in the case of domain shift between domains, thepointwise kernel estimation of true data distribution proposed by ERM in Eq. 2, becomes less accurate whichresults in a lack of OOD generalization of ERM. Improving the Loss Function: In the pursuit of improving OOD performance within ERM, diverseavenues have been explored. Notable efforts include robust optimization (Sagawa et al., 2020; Hu et al., 2018),invariant representation learning on the feature level (Sun & Saenko, 2016; Ganin et al., 2016; Li et al., 2018;Tzeng et al., 2014), and classifier head adjustments (Arjovsky et al., 2019). Additionally, advancements inloss functions, such as those discussed by Krueger et al. (2021), reveal their efficacy in enhancing domaingeneralization. Further insights stem from investigations into loss gradients/Hessians (Parascandolo et al.,2020; Shahtalebi et al., 2021; Koyama & Yamaguchi, 2020; Shi et al., 2021; Hemati et al., 2023), showcasingthe multifaceted approaches taken to fortify ERM against domain shifts. Data Augmentation for DG: Various strategies enhance OOD performance in Empirical Risk Minimization(ERM). Classic data augmentation, as explored by Gulrajani & Lopez-Paz (2020), demonstrates improvedresults under the DomainBed evaluation protocol. Ilse et al. (2021) introduces Select Data Augmentation,a method that identifies transformations detrimentally affecting validation accuracy. Mixup (Zhang et al.,2017) generates mixed data points and soft labels through linear convex combinations from two classes, whileMixStyle (Zhou et al., 2021) combines per-sample feature statistics (mean and variance) across domains.Somavarapu et al. (2020) introduces a stylization transformation based on in-domain data. To the best ofour knowledge, we are the first to utilize latent diffusion models as a data-centric approach for DG. Denoising diffusion models and their applications. Recent advances in diffusion-based generativemodels (Ho et al., 2020; Song et al., 2020; Rombach et al., 2022; Zhang & Agrawala, 2023) demonstrate theircapability to achieve SOTA image quality. Additionally, works such as Unclip (Ramesh et al., 2022) havesuccessfully integrated Foundation models like CLIP (Radford et al., 2021) with stable diffusion, introducingnew generative functionalities such as image-to-image, text-to-image, image variation, and image mixing todiffusion-based models. The application of diffusion models in representation learning has also been explored,as exemplified by StableRep proposed by Tian et al. (2023). In a setup akin to SimCLR (Chen et al., 2020),they demonstrated that synthetic images generated by stable diffusion models can enhance self-supervisedlearning.",
  ": Illustration of the implementation structure of ERM, CDGA, and CDGA on PACS dataset whenusing P and A domains as training and S as target domain": "In CDGA, each data point in domain Si undergoes transformation to all n domains, including its own domain.This augmentation increases the number of samples for domain Si from |Si| to (b n + 1) |Si|, where n isthe number of training domains, |Si| is the number of data points in Si, and b is the generation batch size.Furthermore, we introduce CDGA, where we assume access to a guidance attribute of the target domain. Inthis scenario, the size of domain Si increases from |Si| to (b (n + 1) + 1) |Si|. For implementing CDGA,we use offline augmentation where we first generate images between each pair of training domains and thenstart the training process. As an example, the folder structure of our implementation for the PACS datasetwhen using P and A domains as train domains and S domain for test domain is illustrated in . Forall the methods, we set generation batch size b = 1 unless stated otherwise. CDGA with Prompt Guidance (CDGA-PG): In CDGA-PG, given the k-th image in Si, i.e., xki , theguidance attribute guidej is a domain description text prompt that represents the same class in Sj. Havingthe image and the prompt guidance, we use the LDM to generate b synthetic images which we expect to",
  "CDGA Outperforms SOTA": "In this section, we compare CDGA + ERM with SOTA DG training methods, demonstrating its superiorperformance. We assess CDGA and CDGA on for datasets, namely VLCS (Fang et al., 2013), PACS(Li et al., 2017), OfficeHome (Venkateswara et al., 2017), and DomainNet (Peng et al., 2019), using theDomainBed benchmark (Gulrajani & Lopez-Paz, 2020). This benchmark has gained popularity as a fairand standard evaluation platform for domain generalization algorithms. The evaluation process involvescomparing DG algorithms across 20 hyperparameter choices and 3 trials, utilizing three distinct modelselection techniques. To demonstrate CDGAs effectiveness, we present its evaluation results using theDomainBed benchmark in Tables 1-4. The tables follow a format of presenting the first and second results.For brevity, we report only the top five performing algorithms for each model selection, with full resultsavailable in the appendix. Examining Tables 1-4, CDGA consistently achieves SOTA performance across alldatasets and model selection techniques. Specifically, we applied prompt guidance for PACS, OfficeHome,and DomainNet, while using image guidance (i.e., image mixer) for VLCS. The code implementation fordeploying CDGA-generated data within the DomainBed scheme is detailed in Appendix H.",
  "Domain shift Visualization": "To visualize domain shifts in CDGA-based data for the class \"dog\" across all domains (P, A, C, and S), wegenerate synthetic images for A A, A P, A C, and A S. Subsequently, we utilize the pretrainedCLIP ViT-B/32 image encoder (Radford et al., 2021) to extract features from both real and synthetic images.These features are then projected onto a two-dimensional space using t-SNE and presented in .Notably, the cross-domain synthetic images effectively interpolate between different domains, addressing thedesired distribution shift. In , examining domains A (in red) and S (in pink) reveals a significantdistribution shift in their two-dimensional representations, despite all images belonging to the dog class.However, A S synthetic images seamlessly bridge the gap between A and S representations. Refer to in the appendix for t-SNE plots of other classes.",
  "Transferability Measurement": "Transferability is another recent approach proposed by Zhang et al. (2021) to quantify the domain shift.The transferability measure is an upper bound of the difference between the source and the target domainexcess risks. Subsequently, Hemati et al. (2023) showed that an upper bound for transferability measure is122HT HS2 + o(2) where is a constant, HT and HS are target and source classifier heads Hessians.Following these findings, to quantify the dynamics of domain shift, we monitor classifier heads Hessiandistances between all possible domain pairs through the training steps for ERM and CDGA, where we setdomains P, A, and C as train (source) and domain S as target. shows the difference between theclassifier heads Hessians given data from domains A and S during the steps. We see that CDGA and CDGA",
  "Diversity Shift": "Ye et al. (2022) proposed a numerical method to measure diversity shift which is equivalent to total variation(Zhang et al., 2021) to quantify domain shift. Diversity shift is usually due to the novel domain-specificfeatures in the data. We employ the proposed algorithm by Ye et al. (2022) to quantify and compare diversityshift between training domains and the target domain in a leave-one domain out scheme for PACS real data,CDGA-PACA, and CDGA-PACS datasets. shows both CDGA and CDGA reduce the diversityshift between training domains and the target domain.",
  "Near-duplicate Analysis": "We employ near-duplicate image detection on images generated using CDGA to quantify the similaritybetween the generated and original images in each domain. Following the self-supervised image retrievaltechnique outlined in (Oquab et al., 2023), we utilize the pretrained CLIP ViT-B/32 image encoder (Radfordet al., 2021) to extract embeddings and calculate cosine similarity between original and generated images.For each original image, if at least one image in a generated domain exhibits a cosine similarity above 0.95,",
  ": Examples of near-duplicates (right-most column) found for the dog image in Sketch domain(left-most column) that are generated using CDGA from the original images (middle column)": "we categorize the original domain as having a near-duplicate. provides a summarized view of thisexperiment for the case of generated images from domain C, while the complete results are available in in the appendix. In , we report, for each original domain, the percentage of near-duplicatesrelative to the original domain size. Clearly, generating synthetic images within the manifold between trainingdomains allows us to obtain examples that are near-duplicates of the target domain. showcasessome of the near-duplicates identified for a sample image in the S domain using this technique. Additionalexamples can be found in in the appendix.",
  "Intuitive Discussion: CDGA with ERM an approximate Extention of Vicinal RiskMinimization Principle to DG Setup": "In this section, we attempt to provide an intuitive justification for the reasons behind the success of CDGA.Our justification relies on the Vicinal Risk Minimization principle (VRM) (Chapelle et al., 2000) initiallyintroduced by Vapnik (1999a). The motivation behind VRM is to improve true data distribution estimationin ERM. First note that we can rewrite ERM (Vapnik, 1999a) loss in Eq. 2 as",
  "k(x; xik)(y; yik).(5)": "To improve the true data estimation error of ERM, this is necessary to improve the sample distribution Ei.To this end, the VRM principle suggests replacing the point-wise estimate in ERM i.e., (x; xik) in eq. 5 withsome better kernel estimate of the density in the vicinity of the data point k within Si which we call K(x; xik).An example for K(x; xik) can be Gaussian kernel functions that act as smooth (x; xik) functions. In this case,the VRM sample distribution represented by Ei is written as",
  "kK(x; xik)(y; yik),(6)": "and the overall loss is obtained by replacing Ei(x) in Equation 4 by Ei(x). In practice, VRM is implementedby employing data augmentation along with ERM. VRM suggests that data augmentation can improveERM estimation error by adding additional synthetic examples from the vicinity distribution around eachobservation within each domain in the data. We argue in the DG setting, if we only employ classic dataaugmentation the estimation error of VRM is still high. This is because, in the DG setting, the estimationerror of true data distribution by ERM is mainly caused by the distribution shift between domains whichcannot be fully addressed by simple data augmentation techniques within each domain. Extending the VRM (Chapelle et al., 2000) principle to the DG setup, a possible intuitive justification onthe success of CDGA is that employing CDGA replaces the pointwise estimates in ERM with new densityestimates that can be in the vicinity of domain pairs so that the distribution shift between domains is furtherreduced. We believe this step can potentially reduce the estimation error induced by ERM under the domainshift scenario. To see the difference between classic augmentation and CDGA more clearly, first, we need todefine the projection operator:Definition 6.1 (domain projection operator). Given a metric d defined on supp(Ei) supp(Ej), thedomain projection operator Pij() that projects a data point x in Ei (i.e., x supp(Ei)) onto domain j isdefined asPij(x) =argminz supp(Ej)d(z, x).",
  "j Ei,j(x) in equation 4": "In practice, thanks to image manipulation of diffusion models, such projection to other domains and samplingfrom the vicinity of projected data points has become feasible (approximately). To this end, in CDGA, weemploy LDM, denoted by M() which takes two arguments, one is a data point in a domain and the secondargument is a guidance attribute in another domain from the same class i.e., xi,jk = M(xik, guidej) P()",
  "Mitigating Class Imbalance": "CDGA can also be utilized to mitigate the class imbalance problem in datasets where the number of instancesin each class of each domain is not equal. In such scenarios, one can use a different b for each class of thedata such that after generating samples, the number of instances in each class of generated domains becomesequal. We test the effectiveness of CDGA method in balancing the OfficeHome dataset (which is highlyimbalanced) through the DomainBed benchmark. More specifically, for every class c and domain Sj, we findthe number of samples n(Sj, c) and then we find m = maxc,j n(Sj, c) which is 100 for OfficeHome. Then forevery domain Sj and class c we set b =m n(Sj,c) which leads to larger batch size for domains and classes withfewer data points and subsequently balances the dataset. The results of this experiment are presented in. Clearly, by choosing b in a way that the dataset is more balanced, the OOD generalization has beenfurther improved.",
  "Single Domain Generative Augmentation (SDGA) vs CDGA": "To show the advantage of employing cross-domain data to mitigate domain shift, We also explore the SDGAmethod, where, unlike CDGA, the image from Si is augmented only from the guidance of the same domain i.e.,M(guidei), where guidance can either be an image (SDGA-IG) or a prompt (SDGA-PG). In SDGA-PG, foreach image in Si, i.e., xik we create prompt guidance guidei that can contain label and/or domain informationfrom Sj and feed guidei to the LDM. In SDGA-IG, for each image from Si, i.e., xik we construct guidancethat contains both xik and label information. To compare variations of CDGA and SDGA, we evaluatethem on the PACS dataset using the DomainBed benchmark with twenty different hyperparameters andone trial. The results of this experiment are presented in . Clearly, CDGA consistently outperformsall variations of SDGA. As some suggestions for use cases, we believe as long as the objective is maximumOOD performance, either textual or visual descriptions of different domains are accessible, and there is nocomputational bottleneck, the CDGA is a better choice compared with SDGA. On the other hand, if we donot have access to the domain descriptions, or we aim to achieve OOD improvement as fast as possible withfewer training examples, SDGA can be a better option.",
  "Scaling Law of Data Size": "In this experiment, we aim to measure the effect of generation batch size b which determines the finalaugmented dataset size on the OOD generalization of models trained with CDGA. To this end, on the PACSdataset, using DomainBed benchmark with 20 choices of hyperparameters and 1 trial, we apply CDGA-PGwith b equal to 1, 2, 3, and 4. We conduct this experiment for two models: ResNet-18 model pretrained onImageNet and ResNet-18 model with random initialization. As we see from , for both pretrainedand random initializations, increasing b, i.e., increasing the data size, further improves OOD generalization.However, for the pretrained model, the performance gets saturated which can be due to the capacity of themodel.",
  "Does superior performance of the CDGA method comes from cross-domaintransfer or the improvement is just because of extra synthetic images from apre-trained diffusion model": "A critical question that may be raised is whether the benefit of the CDGA method comes from cross-domaintransfer and not from generating extra synthetic data from a pre-trained diffusion model. We do believe thesuperior OOD performance of CDGA is due to cross-domain transfer which removes the distribution shiftbetween domains and not just because we are using a model that is pre-trained and generates synthetic data.Here we provide our arguments to support this claim.",
  "Conclusions": "In this paper, we showed that a simple cross domain generative augmentation (i.e., CDGA) alongside ERMsurpasses SOTA DG algorithms in the standard DomainBed benchmark. Additionally, empirical resultsshow by employing various distribution shift quantification methods, we observe a significant reduction indistribution shift between training domains after applying CDGA. Furthermore, we conduct comprehensiveablation studies, particularly focusing on adversarial robustness, loss landscape analysis, and data scalinglaws. Notably, the use of CDGA enhances adversarial robustness and reduces the sharpness of the losslandscape, both contributing to improved model generalization. Finally, intuitively, we establish that CDGAwith ERM approximately could be considered as an extension of the VRM principle to the DG setup. Ourwork provides a novel data-centric point of view for domain generalization, in the era when AI GeneratedContent (AIGC) becomes more and more popular.",
  "Olivier Chapelle, Jason Weston, Lon Bottou, and Vladimir Vapnik. Vicinal risk minimization. Advances inneural information processing systems, 13, 2000": "Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastivelearning of visual representations. In International conference on machine learning, pp. 15971607. PMLR,2020. Chen Fang, Ye Xu, and Daniel N Rockmore. Unbiased metric learning: On the utilization of multiple datasetsand web images for softening bias. In Proceedings of the IEEE International Conference on ComputerVision, pp. 16571664, 2013.",
  "Pierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur. Sharpness-aware minimization forefficiently improving generalization. arXiv preprint arXiv:2010.01412, 2020": "Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Franois Laviolette,Mario Marchand, and Victor Lempitsky. Domain-adversarial training of neural networks. The journal ofmachine learning research, 17(1):20962030, 2016. Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples.In Yoshua Bengio and Yann LeCun (eds.), 3rd International Conference on Learning Representations,ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL",
  "Justin Pinkney. Image mixer. 2023": "Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from naturallanguage supervision. In International conference on machine learning, pp. 87488763. PMLR, 2021. Alexandre Rame, Corentin Dancette, and Matthieu Cord. Fishr: Invariant gradient variances for out-of-distribution generalization. In International Conference on Machine Learning, pp. 1834718377. PMLR,2022.",
  "Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditionalimage generation with clip latents. arXiv preprint arXiv:2204.06125, 2022": "Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjrn Ommer. High-resolutionimage synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computervision and pattern recognition, pp. 1068410695, 2022. Shiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto, and Percy Liang. Distributionally robust neuralnetworks for group shifts: On the importance of regularization for worst-case generalization. In InternationalConference on Learning Representations, 2020.",
  "Vladimir N Vapnik. An overview of statistical learning theory. IEEE transactions on neural networks, 10(5):988999, 1999b": "Hemanth Venkateswara, Jose Eusebio, Shayok Chakraborty, and Sethuraman Panchanathan. Deep hashingnetwork for unsupervised domain adaptation. In Proceedings of the IEEE conference on computer visionand pattern recognition, pp. 50185027, 2017. Nanyang Ye, Kaican Li, Haoyue Bai, Runpeng Yu, Lanqing Hong, Fengwei Zhou, Zhenguo Li, and JunZhu. Ood-bench: Quantifying and understanding two dimensions of out-of-distribution generalization. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 79477958,2022."
}