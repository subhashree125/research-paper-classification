{
  "Abstract": "Besides natural language processing, transformers exhibit extraordinary performance insolving broader applications, including scientific computing and computer vision. Previ-ous works try to explain this from the expressive power and capability perspectives thatstandard transformers are capable of performing some algorithms. To empower transform-ers with algorithmic capabilities and motivated by the recently proposed looped transformer(Yang et al., 2024; Giannou et al., 2023), we design a novel transformer framework, dubbedAlgorithm Transformer (abbreviated as AlgoFormer). We provide an insight that efficienttransformer architectures can be designed by leveraging prior knowledge of tasks and theunderlying structure of potential algorithms. Compared with the standard transformer andvanilla looped transformer, the proposed AlgoFormer can perform efficiently in algorithmrepresentation in some specific tasks. In particular, inspired by the structure of human-designed learning algorithms, our transformer framework consists of a pre-transformer thatis responsible for task preprocessing, a looped transformer for iterative optimization al-gorithms, and a post-transformer for producing the desired results after post-processing.We provide theoretical evidence of the expressive power of the AlgoFormer in solving somechallenging problems, mirroring human-designed algorithms. Furthermore, some theoreticaland empirical results are presented to show that the designed transformer has the poten-tial to perform algorithm representation and learning. Experimental results demonstratethe empirical superiority of the proposed transformer in that it outperforms the standardtransformer and vanilla looped transformer in some specific tasks. An extensive experimenton real language tasks (e.g., neural machine translation of German and English, and textclassification) further validates the expressiveness and effectiveness of AlgoFormer.",
  "Introduction": "The emergence of the transformer architecture (Vaswani et al., 2017) marks the onset of a new era in naturallanguage processing. Transformer-based large language models (LLMs), such as BERT (Devlin et al., 2019)and GPT-3 (Brown et al., 2020), revolutionized impactful language-centric applications, including languagetranslation (Vaswani et al., 2017; Raffel et al., 2020), text completion/generation (Radford et al., 2019;Brown et al., 2020), sentiment analysis (Devlin et al., 2019), and mathematical reasoning (Imani et al., 2023;Yu et al., 2024). Beyond the initial surge in LLMs, these transformer-based models have found extensiveapplications in diverse domains such as computer vision (Dosovitskiy et al., 2021), time series (Li et al., 2019),",
  "Published in Transactions on Machine Learning Research (01/2025)": "Johannes Von Oswald, Eyvind Niklasson, Ettore Randazzo, Joo Sacramento, Alexander Mordvintsev, An-drey Zhmoginov, and Max Vladymyrov. Transformers learn in-context by gradient descent. In Interna-tional Conference on Machine Learning, pp. 3515135174. PMLR, 2023. URL Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al.Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural InformationProcessing Systems, 35:2482424837, 2022. URL Mengzhou Xia, Tianyu Gao, Zhiyuan Zeng, and Danqi Chen. Sheared LLaMA: Accelerating language modelpre-training via structured pruning. In The Twelfth International Conference on Learning Representations,2024. URL",
  "(1)": "where X RDN is the input tokens; h is the number of heads; {W (i)V , W (i)K , W (i)Q } denote value, key andquery matrices at i-th head, respectively; {W2, W1, b2, b1} are parameters of the shallow feed-forward ReLUneural network. The attention layer with softmax activation function mostly exchanges information betweendifferent tokens by the attention mechanism. Subsequently, the feed-forward ReLU neural network appliesnonlinear transformations to each token vector and extracts more complicated and versatile representations.",
  "Algorithmic Structures of Transformers": "As discussed in the introduction, rather than simply interpreting it as an implicit function approximator,the transformer may in-context execute some implicit algorithms learned from training data. However, it isstill unverified whether the standard multi-layer transformer is exactly performing algorithms. AlgoFormer.As shown in the green part of , vanilla looped transformers (Yang et al., 2024;Giannou et al., 2023) admit the same structure as iterative algorithms. However, real applications are usually",
  ": Algorithmic structure of the AlgoFormer. Here, TFpre, TFloop, and TFpost are multi-layer trans-formers; statements represent some fundamental operations in classical algorithms": "much more complicated. For example, given a task with data pairs, a well-trained researcher may first pre-process the data under some prior knowledge, and then formulate a mathematical (optimization) problem.Following that, some designed solvers, usually iterative algorithms, are performed. Finally, the desired resultsare obtained after further post-processing. The designed AlgoFormer (Algorithm Transformer), visualized in, enjoys the same structure as wide classes of algorithms. Specifically, we separate the transformerframework into three parts, i.e., pre-transformer TFpre, looped transformer TFloop, and post-transformerTFpost. Here, those three sub-transformers are standard multi-layer transformers in Equation (1). Giventhe input token vectors X and the number of iteration steps T, the output admits:",
  "(TFpre(X))) ).(2)": "The looped transformer layers (TFloop) share the same set of weights, as they perform identical computa-tions during each iteration. In contrast, the pre-transformer (TFpre) and post-transformer (TFpost) utilizedistinct weights and architectures to handle their specific computational roles. The hyperparameters foreach transformer module (e.g., number of heads, layers, and hidden dimensions) are configured based onprior knowledge of the computational complexity of the target algorithms. Although we present AlgoFormeras in Equation (2), which consists of three modules, the key insight here is that the AlgoFormer can bedesigned flexibly based on the prior knowledge of the algorithm structure for the given task. Importantly,we do not restrict AlgoFormer to the specific form outlined in Equation (2). Compared with standard trans-formers, the AlgoFormer acts more as the algorithm learner, by strictly regularizing the loop structure. Incontrast to Giannou et al. (2023) and Yang et al. (2024), which primarily focus on tasks solvable by iterativealgorithms, our approach introduces additional transformer modules (e.g., pre- and post-transformers) forprocessing. These components are crucial for addressing the processing needs of real-world applications.This design makes the AlgoFormer capable of representing more complex algorithms and solving challengingtasks more efficiently. Additionally, one of the core insights of our work is that transformer architecturescan be designed more efficiently, flexibly, and diversely by leveraging prior knowledge and the pre-definedstructure of potential algorithms. This approach enables AlgoFormer to generalize across a broader rangeof applications while maintaining high efficiency and adaptability, such as representing algorithms involvingnested loops, multiple loops, or multi-processing.",
  "Training Strategy": "Our training strategy builds upon the methodology introduced in Yang et al. (2024).Let P i=[x1, f(x1), , xi1, f(xi1), xi] represents the input prompt for 1 i N. We denote the AlgoFormeras TFtAlgo(; ), where f() is a task-specific function that varies across different sequences and t indicatesthe number of loops (iterations) in Equation 2, and represent the transformer parameters. Instead ofevaluating the loss solely on TFTAlgo(; ) with T iterations, we minimize the expected loss over averagediteration numbers:",
  "Expressive Power": "In this section, we theoretically show by construction that AlgoFormer is capable of solving some challengingtasks, akin to human-designed algorithms. The core idea is as follows. Initially, the pre-transformer under-takes the crucial task of preprocessing the input data, such as representation transformation. The loopedtransformer is responsible for iterative algorithms in optimization problems. Finally, it is ready to output thedesired result by the post-transformer. Through the analysis of AlgoFormers expressive power in addressingthese tasks, we expect its potential to make contributions to the communities of scientific computing andmachine learning. Throughout the section, we assume that the maximal number of data samples is N andall sample observations (e.g., xi and yi) are bounded.",
  "Regression with Representation": "We consider regression problems with representation, where the output behaves as a linear function of theinput with a fixed representation function. Here, we adopt the L-layer MLPs with (leaky) ReLU activationfunction as the representation function ().Specifically, we generate each in-context sample by firstsampling the linear weight A from the prior PA, and then generating the input-label pair {(xi, yi)} withxi Rd Px, yi = A(xi) + i and i N0, 2I. We aim to find the test label ytest := A(xtest),given the in-context samples and test data {x1, y1, , xN, yN, xtest}. Here, the weight matrix A variesacross different sequences of in-context samples but remains constant within a single sequence, and is learnedin-context. The representation function , on the other hand, is fixed across all samples of sequences andis learned during training. A reliable solver is expected first to identify the representation function andtransform the input data x to its representation (x). Then it reduces to a regression problem, and someoptimization algorithms are performed to find the weight matrix from in-context samples. Finally, it outputsthe desired result ytest by applying transformations on the test data. We prove by construction that thereexists an AlgoFormer that solves the task, akin to the human-designed reliable solver. Theorem 3.1. There exists a designed AlgoFormer with TFpre (an (L + 1)-layer two-head transformer),TFloop (a one-layer two-head transformer), and TFpost (a one-layer one-head transformer), that outputsA (xtest) from the input-label pairs {x1, y1, , xN, yN, xtest} by fitting the representation function andapplying gradient descent for multi-variate regression. The emulation of each step is not exact, as there issome error introduced in each step. However, the error can be made arbitrarily close to zero by increasingthe temperature of the softmax and adjusting another free parameter, neither of which affects the size of thenetwork. Remarks. The detailed proof is available in Appendix A.1. Our construction of the transformer frameworkinvolves three distinct sub-transformers, each assigned specific responsibilities. The pre-transformer, char-acterized by identity attention, is dedicated to representation transformation through feed-forward neuralnetworks. This stage reduces the task to a multivariate regression problem. Subsequently, the looped trans-former operates in-context to determine the optimal weight, effectively acting as an iterative solver. Finally,the post-transformer is responsible for the post-processing and generate the desired result A (xtest). Here,",
  "AR(q) with Representation": "We consider the autoregressive model with representation. The dynamical (time series) system is generatedby xt+1 = A ([xt+1q, , xt]) + t, where () is a fixed representation function (e.g., we take the L-layer MLPs), and the weight A varies from different sequences but remains constant within a single sequence.Our goal is to learn () during training and to perform in-context learning of the weight matrix A. Instandard AR(q) (multivariate autoregressive) models, the representation function () is identity. Here, weinvestigate a more challenging situation in which the representation function is fixed but unknown. A well-behaved solver should first find the representation function and then translate it into a modified autoregressivemodel. With standard Gaussian priors on the white noise t, the Bayesian estimator of the AR(q) modelparameters admits arg maxANt=1 f(xt|xt1, , xtq) = arg minANt=1 xt A ([xtq, , xt1])22,where f(xt|xt1, , xtq) is the conditional density function of xt, given previous q observations.Apractical solver initially identifies the representation function and transforms the input time series into itsrepresentation, denoted as (xt).Then the problem is reduced to an autoregressive form.Similar tothe previous subsection, we prove by construction that there exists a AlgoFormer, akin to human-designedalgorithms, capable of effectively solving the given task. Theorem 3.2. There exists a designed AlgoFormer with TFpre (a one-layer q-head transformer with an(L+1)-layer one-head transformer), TFloop (a one-layer two-head transformer), and TFpost (a one-layer one-head transformer), that predicts xN+1 from the data sequence {x1, x2, , xN} by copying, transformationof the representation function and applying gradient descent for multi-variate regression. The emulationof each step is not exact, as there is some error introduced in each step. However, the error can be madearbitrarily close to zero by increasing the temperature of the softmax and adjusting another free parameter,neither of which affects the size of the network.",
  "Chain-of-Thought with MLPs": "Chain-of-Thought (CoT) demonstrates exceptional performances in mathematical reasoning and text genera-tion (Wei et al., 2022). The success of CoT has been theoretically explored, shedding light on its effectivenessin toy cases (Li et al., 2023) and on its computational complexity (Feng et al., 2023). In this subsection, werevisit the intriguing toy examples of CoT generated by leaky ReLU MLPs, denoted as CoT with MLPs, asdiscussed in Li et al. (2023). We begin by constructing an L-layer MLP with leaky ReLU activation. For aninitial data point x Px, the CoT point s represents the output of the -th layer of the MLP. Consequently,the CoT sequence {x, s1, , sL} is exactly generated as the output of each (hidden) layer of the MLP. Theimplicit L-layer MLP remains the same within a single sequence but varies across different sequences, andit is learned in-context. The target of CoT with MLPs problem is to find the next state s+1 based onthe CoT samples {x1, s11, , sL1 , x2, , xN, s1N, , sLN, xtest, s1, , s}, where {s1, , s} denotes theCoT prompting of xtest. We establish by construction in Theorem 3.3 that the AlgoFormer adeptly solvesthe CoT with MLPs problem, exhibiting a capability akin to human-designed algorithms.",
  "Extensions and Further Analysis": "In this section, we provide complementary insights to the results discussed in . Firstly, as discussedin the remark following Theorem 3.1, we construct the looped transformer that employs gradient descentto solve (regularized) multi-variate regression problems. However, in practical scenarios, the adoption ofmore efficient optimization algorithms is often preferred. Investigating the expressive power of transformersbeyond gradient descent is both intriguing and appealing. As stated in Theorem 4.1, we demonstrate that theAlgoFormer can proficiently implement Newtons method for solving linear regression problems. Secondly,the definition in Equation 1 implies the encoder-based transformer. In practical applications, a decoder-based transformer with causal attention, as seen in models like GPT-2 (Radford et al., 2019), may also befavored. For completeness, it is also compelling to examine the behavior of decoder-based transformers inalgorithmic learning. Our findings, presented in Theorem 4.2, reveal that the decoder-based AlgoFormercan also implement gradient descent in linear regression problems. The primary distinction lies in the factthat the decoder-based transformer utilizes previously observed data to evaluate the gradient, while theencoder-based transformer calculates the gradient based on the full data samples.",
  ", Mk+1 = 2Mk MkSMk, wNewtonk= MkXy.(5)": "As described in Sderstrm & Stewart (1974); Pan & Schreiber (1991), the above update scheme (Newtonsmethod) enjoys superlinear convergence, in contrast to the linear convergence of gradient descent.Thefollowing theorem states that Newtons method in Equation 5 can be realized by the AlgoFormer. Theorem 4.1. There exists a designed AlgoFormer with TFpre (a one-layer two-head transformer), TFloop (aone-layer two-head transformer), and TFpost (a two-layer two-head transformer), that implements Newtonsmethod described by Equation 5 in solving regression problems. The emulation of each step is not exact,",
  "Decoder-based Transformer": "In the preceding analysis, the encoder-based AlgoFormer (with full attention) demonstrates its capability tosolve problems by performing algorithms. Previous studies (Giannou et al., 2023; Bai et al., 2023; Zhanget al., 2023a; Huang et al., 2023; Ahn et al., 2023) also focus on the encoder-based models.We optedfor an encoder-based transformer because full-batch data is available for estimating gradient and Hessianinformation. However, in practical applications, decoder-based models, like GPT-2, are sometimes moreprevalent. In this subsection, we delve into the performance of the decoder-based model when executingiterative optimization algorithms, such as gradient descent, to solve regression problems. We consider the linear regression problem in Equation 4.Due to the limitations of the decoder-basedtransformer, which can only access previous tokens, implementing iterative algorithms based on the entirebatch data is not feasible.However, it is important to note that the current token in a decoder-basedtransformer can access data from all previous tokens. To predict the label yi based on the input promptP i = [x1, y1, , xi], the empirical loss for the linear weight at xi is given by",
  "wxj yj2 .(6)": "In essence, the linear weight is estimated using accessible data from the previous tokens, reflecting therestricted information available in the decoder-based transformer.Theorem 4.2. There exists a designed AlgoFormer with TFpre (a one-layer two-head transformer), TFloop(a one-layer two-head transformer), and TFpost (a two-layer two-head transformer), that outputs wiT xifor each input data xi, where wiT comes from arg minw Lw; P iafter T steps of gradient descent. Theemulation of each step is not exact, as there is some error introduced in each step. However, the error canbe made arbitrarily close to zero by increasing the temperature of the softmax and adjusting another freeparameter, neither of which affects the size of the network. Remarks. The detailed proof is available in Appendix A.5. The technical details closely resemble those inTheorem 3.1, with the key distinction being that the decoder-based transformer can solely leverage data fromprevious tokens to determine the corresponding weight wi. Our findings align with those in Guo et al. (2024),although our transformer architectures and attention mechanisms differ. In a related study by Akyrek et al.(2023), similar topics are explored, demonstrating that the decoder-based transformer performs single-samplestochastic gradient descent, while our results exhibit greater strength with wi arg minw Lw; P i. Theconstruction of a decoder-based transformer for representing Newtons method is more challenging. We leftit as a potential topic for future investigation.",
  "Experiments": "In this section, we conduct a comprehensive empirical evaluation of the performance of AlgoFormer intackling challenging tasks, specifically addressing regression with representation, AR(q) with representation,and CoT with MLPs, as outlined in . Additionally, we also implement AlgoFormer on the neuralmachine translation of German and English and AG News classification, demonstrating its expressivenessand effectiveness in real-world language tasks.",
  "(c) CoT with MLPs": ": The validation error of trained models (the standard transformer, the vanilla looped transformer,and the AlgoFormer), assessed on regression with representation, AR(q) with representation, and CoT withMLPs tasks. By choosing suitable hyperparameters (i.e., we set (T, T) = (20, 15)), the AlgoFormer hassignificantly better performance than the standard transformer and the vanilla looped transformer on thosetasks.",
  "Experimental Settings and Hyperparameters": "Experimental settings. In all experiments, we adopt the decoder-based AlgoFormer, standard transformer(GPT-2), and vanilla looped transformer Yang et al. (2024). For synthetic tasks, we utilize N = 40 in-contextsamples as input prompts and d = 20 dimensional vectors with D = 256 dimensional positional embeddingsfor all experiments. To ensure fairness in comparisons, all models are trained using the Adam optimizer, witha learning rate = 1e 4 and a total of 500K iterations to ensure convergence. The standard transformeris designed to have L = 12 layers while pre-, looped and post-transformers are all implemented in one-layer. The default setting for the AlgoFormer, as well as the vanilla looped transformer, involves setting(T, T) = (20, 15). Here, the prompt formulation and the training loss in Equation (3) may slightly differ fordifferent tasks. For example, in the AR(q) task, the input prompt is reformulated as P i = [x1, , xi1, xi]and xi+1 = f([xi+1q, , xi]) is the target for prediction. However, the training strategy can be easilytransmitted to other tasks. Here, both the iteration numbers T0 and T are hyperparameters, which will beanalyzed in the next subsection. Regression with representation. In this task, we instantiate a 3-layer leaky ReLU MLPs, denoted as(), which remains fixed across all tasks. The data generation process involves sampling a weight matrixA R120. Subsequently, input-label pairs {(xi, yi)}40i=1 are generated, where xi N (0, I20), i N0, 2 and yi = A(xi)+i. In a, we specifically set = 0. Additionally, we explore the impact of differentnoise levels by considering = 0.1 and = 1. Here, our target is to learn the representation function ()during training and to perform in-context learning of the weight matrix A. AR(q) with representation.For this task, we set q = 3 and employ a 3-layer leaky ReLU MLPdenoted as (), consistent across all instances.The representation function accepts a 60-dimensionalvector as input and produces 20-dimensional feature vectors. The time series sequence {xt}Nt=1 is gener-ated by initially sampling A N (0, I2020).Then the sequence is auto-regressively determined, withxt+1 = A ([xt+1q, , xt]) + t, where t N (0, I20). Here, our target is to learn the representationfunction () during training and to perform in-context learning of the weight matrix A. CoT with MLPs. In this example, we generate a 6-layer leaky ReLU MLP to serve as a CoT sequencegenerator, determining the length of CoT steps for each sample to be six. The CoT sequence, denoted as{x, s1, , sL} is generated by first sampling x N (0, I20), where s R20 represents the intermediatestate output from the -th layer of the MLP. Here, our target is to perform in-context learning of the generatorfunction (i.e., the 6-layer leaky ReLU MLP).",
  "(c) T = 15": ": The validation error of trained models, evaluated on regression with representation task, withvarying hyperparameters T and T. The AlgoFormers are trained for T loops, defined in Equation 3, andthe evaluation focuses on square loss at longer iterations, where the number of loop iterations far exceeds T. in-context samples square error Standard L=3Standard L=12Standard L=14Standard L=20Standard L=22AlgoFormer L=1AlgoFormer L=2",
  "(b) Varying numbers of heads": ": The validation error of trained models, evaluated on regression with representation task, withvarying numbers of layers (denoted as L) and heads (denoted as h). In the context of AlgoFormer, thenumber of layers L corresponds to the layers in the pre-, looped, and post-transformers, all of which areL-layer transformers. The AlgoFormers are trained with (T, T) = (20, 15), defined in Equation 3. in-context samples square error Newton Iter=5Newton Iter=10Newton Iter=15Newton Iter=20GD Iter=5GD Iter=10GD Iter=15GD Iter=20AlgoFormer T=10AlgoFormer T=20",
  "(c) Linear regression, = 1.0": ": The validation error of trained AlgoFormer models and the linear regression models optimizedby gradient descent and Newtons method.The AlgoFormers are trained with (T, T) = (20, 15) and(T, T) = (10, 10), defined in Equation 3.Loop iterations. We conduct comprehensive experiments on the AlgoFormer with varying loop numbers,on solving the regression with representation task. The results highlight the crucial role of both T and T",
  "AlgoFormer and Human-Designed Algorithms": "In this subsection, we compare the AlgoFormer with Newtons method and gradient descent in solvinglinear regression problems. We adopt the same default hyperparameters, with their selection grounded in acomprehensive grid search. As illustrated in , we observe that in the noiseless case, the AlgoFormer outperforms both Newtonsmethod and gradient descent in the beginning stages. However, Newtons method suddenly achieves nearlyzero loss ( machine precision) later on, benefiting from its superlinear convergence. In contrast, our methodmaintains an error level around 1e 3. With increasing noise levels, both Newtons method and gradientdescent converge slowly, while our method exhibits better performance. Several aspects contribute to this phenomenon. Firstly, in the noiseless case, Newtons method can preciselyrecover the weights through the linear regression objective in Equation 6, capitalizing on its superlinearconvergence. On the other hand, the AlgoFormer operates as a black-box, trained from finite data. Whilewe demonstrate good model expressiveness, the final generalization error of the trained transformer resultsfrom the models expressiveness, the finite number of training samples, and the optimization error. Despiteexhibiting high expressiveness, the trained AlgoFormer cannot eliminate the last two errors entirely. Thisobservation resonates with similar findings in solving partial differential equations (Raissi et al., 2019) andlarge linear systems (Gu & Ng, 2023) using deep learning models. Secondly, with larger noise levels, Newtons method shows suboptimal results. This is partly due to theinclusion of noise, which slows down the convergence rate, and Newtons method experiences convergencechallenges when moving away from the local solution.In terms of global convergence, the AlgoFormerdemonstrates superior performance compared to Newtons method. Human-designed algorithms, backed by problem priors and precise computation, achieve irreplaceable per-formance. Its important to note that deep learning models, including transformers, are specifically designedfor solving black-box tasks where there is limited prior knowledge but sufficient observation samples. We",
  "Applications to Language Tasks": "In this subsection, we extend the evaluation of the proposed AlgoFormer to real-world language tasks, com-plementing its performance in real applications. Specifically, we focus on Neural Machine Translation usingthe IWSLT 2015 German-English dataset. The experimental setup includes a standard Transformer with12 layers, 8 attention heads, a feature dimension of 256, and a learning rate of 5e-5. The pre-, looped, andpost-transformers are all implemented as single layers, with T set to 10 and T to 10 (see the hyperpa-rameters in Equation 3 for training). The input German text is treated as a prefix, and the output Englishtext is generated autoregressively using decoder-based transformers for all three models. The translationperformance is evaluated using cross-entropy loss, where lower values indicate better results. Additionally,BLEU (Bilingual Evaluation Understudy) is a widely used metric for measuring the quality of translationtasks, with higher scores indicating better performance. The results are presented in the table below:",
  ": Quality of Transformer models on machine translation": "We also implement the proposed AlgoFormer on the text classification task using various datasets (AG News,IMDB, DBPedia, Yelp Review, and Yahoo News) to evaluate its performance on a real-world languageapplication.The experimental setup includes a standard Transformer with 12 layers, 1 attention head,a feature dimension of 32, and a learning rate of 1e-3.The pre-, looped, and post-transformers are allimplemented as single layers with one attention head, with (T, T) = (10, 10). The input news text datais processed using encoder-based transformers, and the output classification is generated.Classificationaccuracy, where higher accuracy indicates better performance, is used as the evaluation metric. The resultsare summarized in the table below:",
  ": Accuracy (%) of Transformer models on text classification across different datasets": "As shown in the results, AlgoFormer outperforms and performs comparably with the standard Transformerand the vanilla looped Transformer in the Neural Machine Translation and Text Classification tasks, sug-gesting that conventional models may have inherent redundancies. This aligns with recent findings on theredundancy in large language models (Chen et al., 2024; Frantar & Alistarh, 2023; Xia et al., 2024). Thesepreliminary results indicate the potential of AlgoFormer, which is designed as an algorithmic conductor, inreal-world language tasks.",
  "Computation Comparison": "The additional computational costs and complexity introduced by AlgoFormer over the standard transformerand the vanilla looped transformer depend on the specific structure of the AlgoFormer framework.Inour experimental settings, these costs are negligible.Compared to the standard transformer, the totalcomputation overhead is primarily determined by the loop iterations T and the number of loops T includedin the training loss. Since T in our experiments is comparable to the number of layers in the standardtransformer, the dominant additional computation comes from T. This overhead can be mitigated byusing smaller batch sizes, ensuring that the proposed method does not introduce significant computational",
  "overhead compared to the standard transformer. The main complexity lies in the training strategy outlinedin Equation 3, which ensures stable training": "When compared to the vanilla looped transformer, the computational overhead comes from the pre- andpost-transformer modules.However, if these modules are of similar size to the looped transformer, theadditional computational cost is negligible, especially when the loop iteration T is much greater than thenumber of layers in these modules. For instance, in our experiments, we set the pre-, looped-, and post-transformer layers to 1, with T = 20 during training. In this scenario, the additional computation introducedby the pre- and post-transformer modules constitutes approximately 1/10 of the total computation. Duringinference, where T is typically much larger than during training (e.g., T=200 loop iterations in ),the additional computational overhead is further reduced to about 1/100.",
  "Conclusion and Discussion": "In this paper, we introduce a novel AlgoFormer, an algorithm learner designed from the looped trans-former, distinguished by its algorithmic structures. We provide an insight that the efficient transformerframework can be designed by considering the prior knowledge of the task and the structure of potentialalgorithms. Comprising three sub-transformers, each playing a distinct role in algorithm learning, the Al-goFormer demonstrates expressiveness and efficiency while maintaining a low parameter size. Theoreticalanalysis establishes that the AlgoFormer can tackle challenging in-context learning tasks, mirroring human-designed algorithms. Our experiments further validate our claim, showing that the proposed transformeroutperforms both the standard transformer and the vanilla looped transformer in specific algorithm learningand real-world language tasks.",
  "While the proposed AlgoFormer framework offers flexibility and efficiency, several potential limitations andchallenges warrant discussion": "First, the dependence on prior knowledge for designing concrete architecture. The design of the AlgoFormerframework relies heavily on prior knowledge of tasks and the structure of potential algorithms. In scientificcomputing tasks, where problems are often well-formulated and informed by domain knowledge, this reliancecan be an advantage, enabling the design of task-specific, efficient architectures. However, for real-worldlanguage tasks, the abstract and unstructured nature of the problems often limits the availability of suchprior knowledge. This can make it challenging to design an optimal AlgoFormer architecture for these tasks.While the insights of AlgoFormer can significantly benefit the scientific computing community, extending itsdesign principles to language tasks may require more advanced techniques, such as automated architecturesearch or meta-learning, to determine suitable architectures without extensive prior knowledge. Second, the training complexity of AlgoFormer with more complicated algorithmic structures. Althoughincorporating more complex algorithmic structures into AlgoFormer, such as nested looped transformers,enhances its expressiveness, training AlgoFormer can become more challenging. It inevitably introducesadditional computational costs and requires adjustments to the training strategy. These challenges couldlimit the usability of AlgoFormer in scenarios where algorithmic structures are complicated to model. Third, the scalability of AlgoFormer to large-scale applications. The scaling behavior of AlgoFormer hasnot been thoroughly investigated in this paper. The tasks studied are primarily hand-generated scientificproblems or toy cases, and the language task considered is of medium scale. Similarly, the model sizes usedin this study are relatively small compared to state-of-the-art large language models (LLMs). To fully assessthe potential of AlgoFormer, future research should explore its scalability to larger, more challenging tasksand datasets, as well as the associated computational and memory requirements when deploying AlgoFormerat scale.",
  "Xiaodong Chen, Yuxuan Hu, and Jing Zhang.Compressing large language models by streamlining theunimportant layer. arXiv preprint arXiv:2403.19135, 2024": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectionaltransformers for language understanding.Proceedings of the 2019 Conference of the North AmericanChapter of the Association for Computational Linguistics: Human Language Technologies, 1:41714186,2019. URL Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Un-terthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and NeilHoulsby. An image is worth 16x16 words: Transformers for image recognition at scale. In InternationalConference on Learning Representations, 2021. URL Guhao Feng, Yuntian Gu, Bohang Zhang, Haotian Ye, Di He, and Liwei Wang.Towards revealing themystery behind chain of thought: a theoretical perspective. Advances in Neural Information ProcessingSystems, 2023. URL",
  "Deqing Fu, Tian-Qi Chen, Robin Jia, and Vatsal Sharan.Transformers learn higher-order optimizationmethods for in-context learning: A study with linear models. arXiv preprint arXiv:2310.17086, 2023": "Shivam Garg, Dimitris Tsipras, Percy S Liang, and Gregory Valiant.What can transformers learn in-context? a case study of simple function classes. Advances in Neural Information Processing Systems, 35:3058330598, 2022. URL Angeliki Giannou, Shashank Rajput, Jy-Yong Sohn, Kangwook Lee, Jason D. Lee, and Dimitris Papailiopou-los. Looped transformers as programmable computers. In International Conference on Machine Learning,volume 202, pp. 1139811442. PMLR, 2023. URL",
  "Yu Huang, Yuan Cheng, and Yingbin Liang.In-context convergence of transformers.arXiv preprintarXiv:2310.05249, 2023": "Shima Imani, Liang Du, and Harsh Shrivastava. MathPrompter: Mathematical reasoning using large lan-guage models.In Proceedings of the 61st Annual Meeting of the Association for Computational Lin-guistics (Volume 5: Industry Track), pp. 3742. Association for Computational Linguistics, 2023. URL Amirhossein Kazemnejad, Inkit Padhi, Karthikeyan Natesan Ramamurthy, Payel Das, and Siva Reddy. Theimpact of positional encoding on length generalization in transformers. Advances in Neural InformationProcessing Systems, 2023. URL Shiyang Li, Xiaoyong Jin, Yao Xuan, Xiyou Zhou, Wenhu Chen, Yu-Xiang Wang, and Xifeng Yan. Enhancingthe locality and breaking the memory bottleneck of transformer on time series forecasting. Advances inNeural Information Processing Systems, 32, 2019.URL Yingcong Li, Kartik Sreenivasan, Angeliki Giannou, Dimitris Papailiopoulos, and Samet Oymak. Dissectingchain-of-thought: Compositionality through in-context filtering and learning.In Advances in NeuralInformation Processing Systems, 2023. URL Arvind Mahankali, Tatsunori B Hashimoto, and Tengyu Ma. One step of gradient descent is provably theoptimal in-context learner with one layer of linear self-attention. International Conference on LearningRepresentations, 2024. URL Santiago Ontanon, Joshua Ainslie, Zachary Fisher, and Vaclav Cvicek. Making transformers solve composi-tional tasks. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics(Volume 1: Long Papers), pp. 35913607, 2022. URL",
  "Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language modelsare unsupervised multitask learners. OpenAI blog, 1(8):9, 2019": "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou,Wei Li, and Peter J Liu.Exploring the limits of transfer learning with a unified text-to-text trans-former. Journal of Machine Learning Research, 21(1):54855551, 2020. URL Maziar Raissi, Paris Perdikaris, and George E Karniadakis. Physics-informed neural networks: A deep learn-ing framework for solving forward and inverse problems involving nonlinear partial differential equations.Journal of Computational Physics, 378:686707, 2019.",
  "In this section, we provide comprehensive proofs for all theorems stated in the main content": "Notation. We use boldface capital and lowercase letters to denote matrices and vectors respectively. Non-bold letters represent the elements of matrices or vectors, or scalars. For example, Ai,j denotes the (i, j)-thelement of the matrix A. We use 2 to denote the 2-norm (or the maximal singular value) of a matrix.",
  "A.1Proof for Theorem 3.1": "Positional embedding. The role of positional embedding is pivotal in the performance of transformers.Several studies have investigated its impact on natural language processing tasks, as referenced in (Kazem-nejad et al., 2023; Ontanon et al., 2022; Press et al., 2022). In our theoretical construction, we deviate fromempirical settings by using quasi-orthogonal vectors as positional embedding in each token vector. Thischoice, also employed by Li et al. (2023); Giannou et al. (2023), is made for theoretical convenience.",
  "A(Ak) and a positive stepsize > 0": "Proof for Theorem 3.1. We start by showing that L-layer transformer can represent L-layer MLPs. Itis observed that the identity operation (i.e., Attn (X) = X) can be achieved by setting WV = 0 due tothe residual connection in the attention layer.Each feed-forward neural network in a transformer layercan represent a one-layer MLP. Consequently, the representation function () can be realized by L-layertransformers. At the output layer of the L-th layer transformer, let A0 be an initial guess for the weight.The current output token vectors are then given by:",
  "thenWV P softmaxP W K WQP12y112y1 12yN12yN00,": "where the two sides of the can be arbitrarily close if the temperature of the softmax function is sufficientlylarge, due to the nearly orthogonality of positional embedding vectors. Its important to note that the feed-forward neural network is capable of approximating nonlinear functions, such as multiplication. Here, weconstruct a shallow neural network that calculates the multiplication between the first d elements and thevalue1N in each token. Passing through the feed-forward neural network together with the indicators, weobtain the final output of the first (L + 1)-layer transformer TFpre:",
  ".(7)": "Specifically, it identifies the positional index 1 of the last token s1, retains only s1iand si for 1 i N,and filters out all other irrelevant tokens. In this context, the representation function () corresponds toL-layer leaky ReLU MLPs. Notably, the transformation si = W si1is expressed, where W denotesthe weight matrix at the -th layer, and () represents the leaky ReLU activation function. Given thereversibility and piecewise linearity of the leaky ReLU activation, we can assume, without loss of generality,that si = W s1iin Equation 7.Consequently, the problem is reduced to a multi-variate regression,and a one-layer two-head transformer TFloop is demonstrated to effectively implement gradient descent fordetermining the weight matrix W , as shown in Lemma A.2. Subsequently, the post-transformer TFpostproduces the desired result W si1.",
  "A.4Proof for Theorem 4.1": "In this section, we first show that the one-layer two-head transformer can implement a single step of Newtonsmethod in Equation 5, with the special form of input token vectors. Then, we introduce the pre-transformer,designed to convert general input tokens into the prescribed format conducive to the transformers operation.Finally, the post-transformer facilitates the extraction of the desired results through additional computations,given that the output from the looped-transformer corresponds to an intermediate product.",
  "A.5Proof for Theorem 4.2": "In this section, we extend the realization of gradient descent, as demonstrated in Lemma A.2 for encoder-based transformers, to decoder-based transformers. Although the construction is similar, the key distinctionlies in the decoder-based transformers utilization of previously viewed data for regression, consistent withour intuitive understanding. The following lemma is enough to conclude the proof for Theorem 4.2.Lemma A.5. The one-layer two-head decoder-based transformer can implement one step of gradient descentin linear regression problems in Equation 6."
}