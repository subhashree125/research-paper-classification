{
  "Abstract": "Algorithmic recourse is a process that leverages counterfactual explanations, going beyondunderstanding why a system produced a given classication, to providing a user with actionsthey can take to change their predicted outcome. Existing approaches to compute suchinterventionsknown as recourseidentify a set of points that satisfy some desideratae.g.an intervention in the underlying causal graph, minimizing a cost function, etc. Satisfyingthese criteria, however, requires extensive knowledge of the underlying model structure, anoften unrealistic amount of information in several domains. We propose a data-driven andmodel-agnostic framework to compute counterfactual explanations. We introduce StEP,a computationally ecient method that oers incremental steps along the data manifoldthat directs users towards their desired outcome. We show that StEP uniquely satises adesirable set of axioms. Furthermore, via a thorough empirical and theoretical investigation,we show that StEP oers provable robustness and privacy guarantees while outperformingpopular methods along important metrics.",
  "Introduction": "An automatic decision maker produces a negative prediction for some usere.g. denies their grad schoolapplication, oers them bad loan terms or an overly strict criminal sentence; what can the user do tochange this outcome? Counterfactual explanations (Wachter et al., 2017) recommend actions that changealgorithmic predictions on a given point. This is usually modeled as a constrained optimization problemthat outputs specic points which satisfy certain desirable properties: actionability, validity, data manifoldcloseness and causality to name a few (Verma et al., 2020). Achieving these desiderata often requiresboth signicant compute power and user/model information. We propose a lightweight algorithm forproducing counterfactual explanations; rather than searching for good interventions for users, we searchfor good directions that users can take. We use these directions to create an iterative recourse mechanismwhere stakeholders can repeatedly request new directions after carrying out the recommended changes. We",
  "Our Contributions": "We propose a recourse algorithm called Stepwise Explainable Paths (StEP). Our key theoretical insight is thatStEP is the only method for generating recourse directions (counterfactual explanations) which satises a setof natural properties. StEP directions are model-agnostic and easy to compute, requiring only the trainingdataset and the output of the model of interest on points in the training dataset. That is, our method doesnot require prior knowledge of the underlying model architecture and instead takes a strongly data-dependentapproach. In addition to introducing StEP, a novel step-based recourse method, we present its provablequality (.1), diversity (.2), and privacy (.3) guarantees. We also provide anextensive experimental evaluation of StEP, including a holistic cross-comparison with three popular recoursemethods (DiCE (Mothilal et al., 2020a), FACE (Poyiadzi et al., 2020), and C-CHVAE (Pawelczyk et al.,2020)) on three widely-used nancial datasetsCredit Card Default (Yeh & Lien, 2009), Give Me SomeCredit (Credit Fusion, 2011) and UCI Adult (Kohavi, 1996) datasets. We also investigate StEPs robustnessto noise (.3).",
  "Related Work": "Counterfactual explanations are a fundamental concept in the model explanation literature (Verma et al.,2020). First proposed by Wachter et al. (2017), they are founded in legal interpretations of explainability(Wachter et al., 2018) and are distinct from feature highlighting methods (Barocas et al., 2020), such asShapley value-based methods (Datta et al., 2016; Lundberg & Lee, 2017), Local Interpretable Model-AgnosticExplanations (Ribeiro et al., 2016) and saliency maps (Simonyan et al., 2013). Recent policy eorts describealgorithmic recourse as a means for fostering trust in AI systems (Wachter et al., 2018; NTIA, 2023; Biden Jr.,2023). Whereas feature highlighting methods indicate important features (or feature interactions (Patel et al.,2021)), counterfactual explanations identify changes to features that are likely to change the outcome(Karimiet al., 2022; Verma et al., 2020). These changes are usually solutions to an optimization problem, ensuringthat the explanations are valid, actionable, sparse and diverse (Mothilal et al., 2020a; Karimi et al., 2020a;Ustun et al., 2019a). These solutions are computed using integer linear programs (Ustun et al., 2019a;Kanamori et al., 2020), SAT solvers (Karimi et al., 2020a), or gradient descent (Mothilal et al., 2020a;Wachter et al., 2017). Other works provide a sequence of steps from the point of interest to a point with thedesirable outcome along the data manifold (Poyiadzi et al., 2020). While these prior methods oer reasonableapproaches to recourse generation, none of them axiomatically characterize their approach. Another approach in the literature is to solve the causal problem of nding the best intervention (Karimiet al., 2021). Karimi et al. (2021) argue that any recourse recommendation must be consistent with theunderlying causal relations between variables. However, this requires complete (or, as in Karimi et al. (2020b),imperfect) knowledge of the underlying causal model, which is often practically or computationally infeasible. We address this issue by providing users with promising directional actions, allowing them more exibility andagency in enacting recourse recommendations. Our axiomatic characterization is similar to that of MonotoneInuence Measures (MIM) (Sliwinski et al., 2019). However, Sliwinski et al.s approach is not iterative innature, nor is their recommended direction guaranteed to change the prediction outcome.",
  "We introduce some general notation and denitions used throughout this work. We denote by x Rm": "a point of interest (PoI) corresponding to an individual or their current state, and dene a dataset X ={x1, x2, . . . , xn} Rm with m features and n datapoints. We use xi to denote the i-th index of the vector x. We focus on binary classication using a model of interest f : Rm 1 trained on the dataset X; our goal isto produce a counterfactual explanation for x / X where f(x) = 1. The counterfactual explanation for apoint of interest x is a direction (or a set of directions) d Rm that moves the point x towards the positive",
  ": (Left) The training dataset that the loan approval algorithm uses, (Middle) Plotting possiblerecourse directions for Example 2.1 and (Right) plotting a stakeholder path toward loan acceptance": "class, i.e. f(x + c d) = 1 for some positive value c > 0. We go beyond simply providing an explanation forthe the PoIs outcome by framing our output as recommended recourse which can be actioned by the user.Upon applying recourse to a PoI x, we refer to x + c d as a counterfactual (CF) of x denoted by xCF. Oering a single explanation may not be helpful in real-world settings; users may be unable to make a singledramatic change to their features or may not exactly follow the suggested recourse. We allow the stakeholderto repeatedly request new recourse directions as they change their values until they are positively classied.We call such an approach direction-based recourse. To illustrate this approach, consider the following exampleof algorithmic loan approval. Example 2.1. Consider a loan applicant whose application is rejected by an algorithm utilizing two factors:bank account balance and credit score. The loan applicant, i.e., the PoI, the algorithm, and the trainingdataset are described in a. Several directions can change the PoIs value from (red points withlabel 1) to (blue points with label +1). For example, increasing the credit score while leaving the bankaccount balance unchanged eventually changes the applicants label; increasing both bank balance and creditscore (b) results in a positive outcome. Suppose that we provide the applicant with both alternatives;the applicant makes some modications and reapplies for a loan. If the application is rejected, we providethem with new directions, based on their current state, and repeat until the application is accepted. Thisresults in a recourse path, rather than a single direction (c). We wish to ensure that our directions are robust; thus, even if the stakeholder somewhat deviates from oursuggested path, they are still likely to secure a positive outcome. This also ooads some of the computationalcosts onto the stakeholder, which guarantees certain recourse properties that would otherwise require asignicant amount of information and computational cost. Consider Example 2.1: after providing two dierentdirections, the change that the applicant makes to their datapoint will likely be a minimum cost changethat approximately follows one of the directions we propose in b. Even if the resulting change doesnot facilitate a change in outcome, or is incorrectly executed, we can still oer additional directions until adesirable outcome is obtained.",
  "Data-driven Recourse": "Whether explanations should be computed based on the underlying model or the observed data is a highlydebated topic (Chen et al., 2020; Barocas et al., 2020; Janzing et al., 2020). While data-driven recoursemethods exist (Poyiadzi et al., 2020), most work solely with the model of interest f (Mothilal et al., 2020a;Wachter et al., 2017; Karimi et al., 2020a). Recent work shows that explanations can perform poorly when they are inconsistent with the data manifoldi.e. the underlying distribution from which the data is drawn (Frye et al., 2021; Aas et al., 2021) andare vulnerable to manipulation (Slack et al., 2020; 2021). Ignoring the data manifold when computingcounterfactuals can be even more pernicious, resulting in recourse recommendations that may not improvethe outcome in any way, but simply move it outside the data manifold. To see why this is the case, consider",
  ": end while": "a simplied, two dimensional instance as shown in Example 2.1 but with a dierent point of interest (givenin a). The shortest distance perturbation which crosses the decision boundary corresponds to thedirection d1 (as given in b). This direction suggests that the stakeholder decrease their credit scoreand marginally increase their bank account balance, which actively moves the stakeholder away from thepositively classied points. An explanation algorithm that recognizes the data manifold will oer a directionsimilar to d2, arguably making the stakeholder a better candidate for loan approval.",
  "Stepwise Explainable Paths (StEP)": "We now present our approach for computing recourse directions. The overall method is summarized inAlgorithm 1. We rst partition the dataset X into k clusters {X1, . . . , Xk} (using a standard clusteringalgorithm). For a point of interest x, we generate a direction dc towards each cluster Xc using the expression",
  "xXc(x x)(x x)1(f(x) = 1)(1)": "where : R+ R+ is some non-negative function and . is a rotation invariant distance metric. We selectdirections using Equation (1) (a similar formula is proposed by Sliwinski et al. (2019)). The intuition behindthis equation is as follows: for each point x in the cluster Xc, if f(x) = 1, we move on the line (x x) adistance of (x x), where is a decreasing function of x x. Thus, points closer to x have a greatereect on dc; similarly, positively classied points that are close to each other will have a greater eect ondc. We oer these k directions to the stakeholder, who then returns with a new point x after following therecourse recommendations. The process is repeated until we produce a positively classied point (i.e. acounterfactual) or reach a user-specied maximum number of iterations. StEPs computational complexity islinear in the size of the dataset and is bounded by the maximum number of iterations.",
  "An Axiomatically Justied Direction": "We axiomatically derive our choice of direction in equation 1: we identify a direction that uniquely satises aset of desirable properties. More specically, for a given point of interest x / Xc, we believe any reasonablerecourse direction, denoted by d(x, Xc, f) should satisfy the following axioms: Shift Invariance (SI) Let Xc+b denote the dataset resulting from adding the vectorb to each point in X andlet fb be a shifted model of interest such that fb(z) = f(z b) for all z. Then, d(x, Xc, f) = d(x+b, Xc +b, fb). Rotation/Reection Faithfulness (RRF) Let A be any matrix with det(A) {1, +1} and let AXcdenote the dataset resulting from replacing every point xj in Xc with Axj. Let fA denote a rotated model ofinterest such that fA(z) = f(A1z) for all z. Then, Ad(x, Xc, f) = d(Ax, AXc, fA). Continuity (C) d is a continuous function of the dataset Xc. One can think of the continuity axiom asa notion of recourse robustness: small changes to the input PoI will not cause signicant changes to theresulting recourse.",
  "Positive Classication Monotonicity (PCM) Let x / Xc be a point with f(x) = 1 and xi > xi, thend(x, Xc, f) di(x, Xc {x}, f). Similarly, if xi < xi, then di(x, Xc, f) di(x, Xc {x}, f)": "Our axioms are inspired by Sliwinski et al. (2019), who use a similar approach to characterize a familyof direction-based explanations (Monotone Inuence Measures). Unlike the MIM framework, we removeany dependency on negatively classied points (the Negative Classication Indierence axiom). Withoutthis change, a naive adaptation of MIM may output directions pointing away from all positively classiedpoints. Intuitively, a cluster of negative points near positive points may make it impossible to recommendany recourse, as the MIM framework shies away from negative point clusters (see details in Appendix A). The ve remaining axioms are fundamental. Shift Invariance and Rotation/Reection Faithfulness ensurethe directions depend on the relative locations of the points rather than their absolute values. The RRFaxiom also generalizes the feature symmetry axiom: swapping the coordinates of features i and j does notchange the value assigned to them; this is commonly used in the characterization of other model explanationframeworks (Datta et al., 2015; 2016; Patel et al., 2021; Lundberg & Lee, 2017; Sliwinski et al., 2019). Inaddition, these properties ensure the units in which we measure feature values have no eect on the outcome,e.g., measuring income in dollars rather than cents has no eect on the importance of income. Continuityensures that the direction we pick is robust to small changes in the cluster Xc. Data Manifold Symmetry(DMS) ensures that the direction depends on the model of interest only through points in the dataset. Inother words, DMS ensures that the models output on points outside the data manifold do not aect theoutput direction a desirable property in model explanations (Frye et al., 2021; Lundberg & Lee, 2017;Chen et al., 2020). Positive Classication Monotonicity (PCM) ensures that the direction will point towardsregions with a large number of positively classied points. In other words, if there is a large number ofpositively classied points with a high value in some feature i N (e.g. bank balance), PCM ensures thatthe output direction will ask the stakeholder to increase their bank balance. Any direction which satises the above axioms is uniquely given by Equation (1): i.e. it is a weightedcombination of the directions from the point of interest to every positively classied point in the dataset.The weight given to every point is a function of their distance x x. Theorem 3.1. A recourse direction for a point of interest x given a dataset Xc, a model of interest f and arotation invariant distance metric . satises SI, RRF, C, DMS, NCI and PCM if and only if it is given byequation 1.",
  "Proof. For readability, we replace Xc with X. It is easy to see that equation 1 satises all ve axioms so weonly show uniqueness": "We assume without loss of generality that X contains no negatively classied points. If X does contain anynegatively classied points, we can simply remove them without changing the recourse direction because ofNegative Classication Indierence (NCI). Therefore, our goal is to show that any direction which satises(SI), (RRF), (C), (DMS), (NCI) and (PCM) is of the form",
  "a 0 : d(x, X {y}, f) d(x, X, f) = a(y x)": "Let l = d(x, X {y}, f) d(x, X, f). Let A be a rotation matrix such that (Al)1 < 0 and [A(y x)]1 > 0;such a matrix exists since the two vectors are either linearly independent or l = b(y x) where b R+.Since d satises Rotation and Reection Faithfulness (RRF), we have from (Al)1 < 0",
  "This contradicts the Positive Classication Monotonicity (PCM) property since (Ay)1 > (Ax)1 and fA(Ay) =f(y) = 1": "Consider a direction d that satises the six desired axioms. We go ahead and show uniqueness via inductionon |X|. Let k = 0, X = {}. By Shift Invariance (SI), d(x, {}, f) = d(0, {}, fx). The vector 0 and an emptyX are invariant under rotation. Therefore, since d satises Rotation and Reection Faithfulness (RRF) andData Manifold Symmetry (DMS), we must have d(0, {}, fx) = 0, the only vector invariant under rotation. Let k = 1, X = {y} where y = x. Note that any pair of (x, y) can be translated by shift and rotation to anyother pair (x, y) with the same distance (y x) between them. This is because the distance metric (.)is rotation invariant and any distance metric is shift invariant when computing the distance between twopoints; the shifts cancel each other out. Note that after rotation by some matrix A, we have fA(Ay) = f(y)and similarly, after shift by some vector b, we have fb(y +b) = f(y). Therefore, the label of the point y doesnot change after applying Shift Invariance (SI) or Rotation and Reection Faithfulness (RRF). Therefore, by(SI), (RRF) and Lemma 3.2, we have",
  "where is a non-negative valued function": "Suppose the hypothesis holds when |X| k. Consider a dataset Y of size k + 1. This means Y contains atleast two distinct points y,z = x. We prove our hypothesis for the case where y and z are linearly independent.The case where they are linearly dependent follows from Continuity (C): we can peturb the vectors slightlyto make them linearly independent. By Lemma 3.2, we have",
  "This completes the induction": "Reducing stakeholder travel distance to reach a positive classication is a common objective used inalgorithmic recourse (Mothilal et al., 2020a; Karimi et al., 2020a; Mahajan et al., 2019). This distance can beseen as a measure of the eort required from the stakeholder to change their outcome. We can incorporatethis in StEP using the choice of . Any function of the form (z) =1zk with k 2 ensures that nearbypoints are assigned a greater weight than far-o points. Dierent choices of result in dierent directions.",
  "Despite its theoretical soundness, utilizing StEP without clustering presents two drawbacks": "Lack of guaranteed diversity In many cases, changing the function does not result in a diverse set ofdirections even when many directions are possible. Consider a slight modication to Example 2.1 (givenby ) where the class of positive points is separated into two clusters. A recourse algorithm couldreasonably output a direction towards either cluster as a potential recourse. However, Equation (1) outputs alinear combination of these two clusters. Furthermore, it is easy to see that changing the function will notsignicantly change the direction.",
  "O-manifold directionsSince Equation equation 1 aggregates directions, it is possible to obtain o-manifold directions (refer to a); o-manifold regions may have high prediction error": "Without clustering, Equation equation 1 aggregates directions obtained for dierent parts of the dataset,rather than treating dierent data regions dierently. We resolve both aforementioned concerns by clusteringthe positively classied data points and compute directions per individual cluster. This ensures that weidentify dierent clusters of points and aggregate each cluster in a theoretically sound manner. The impactsare demonstrated in .",
  "Published in Transactions on Machine Learning Research (10/2024)": "For each base ML model, we sweep over a condence cuto between {0.50, 0.55, 0.60, 0.65, 0.7} and evaluatethe performance of each recourse method. We nd that 0.7 provides reasonable results across datasets,base models, and recourse methods while remaining a reasonable value a practitioner may use. We varyk {1, 2, 3, 4, 5}, the number of paths to produce for each PoI (and for StEP, the number of clusters toproduce), and x k = 3 for our comparative analysis and user-interference experiments. For StEP, we consider step sizes in {0.10, 0.25, 0.50, 0.75, 1.00} and x a value of 1 across all experiments.FACE has two main hyperparameters: graph type in {k-NN, , KDE} and distance threshold (a oat). InB.4, we provide substantial discussion on the distance threshold parameter. For all our experiments usingFACE, we used the graph following a suggestion from the authors of the method. For C-CHVAE we setthe step distance hyperparameter to 1. We detail this hyperparameter in B.1. For all applicable recoursemethods, we allow a maximum of 50 iterations to produce k counterfactual(s).",
  "Pr[M(X) S] e Pr[M(X ) S] +": "where X and X are any two datasets that dier by at most one datapoint. Dierential privacy states thatthe output of M does not vary much by the removal of any data point. A simple method to prove thata function is dierentially private is to upper bound its sensitivity, i.e. the change to the function whenadding a datapoint. Dwork & Roth (2014) show that adding a nite amount of noise to a bounded sensitivityfunction guarantees dierential privacy. A similar guarantee is oered for Shapley-based (Datta et al., 2016),and gradient-based (Patel et al., 2022) explanations.",
  "d = maxx,X,X d(x, X, f) d(x, X , f)2(3)": "where X is X with one additional (or one less) datapoint (Dwork & Roth, 2014). We can assume without lossof generality that X contains one additional positively classied datapoint x. If the point is not positivelyclassied, then none of the directions change and the sensitivity is 0. The global sensitivity dened inequation 3 reduces to",
  ") to all the features": "The proof of Theorem 3.4 is a direct application of Dwork & Roth (2014, Theorem 3.22) and is omitted.Oering multiple recourse directions results in an additive increase in privacy cost. More specically, ifwe provide k directions, and each direction is (, ) dierentially private, then our mechanism is (k, k)dierentially private (Dwork & Roth, 2014).",
  "Empirical Evaluation & Analysis": "We compare the performance of StEP and three popular recourse methodsDiCE (Mothilal et al., 2020a),FACE (Poyiadzi et al., 2020) and C-CHVAE (Pawelczyk et al., 2020)on three widely used datasets withincounterfactual research using three base models. We also examine StEPs robustness to noise or user-interference when following recourse directions. We provide an overview of our experimental setup here andinclude full details to support reproducibility in Appendix C. Recall that a counterfactual point for x is aterminal point of a recourse path, xCF, such that f(xCF) = 1. Recourse BaselinesGiven a negatively classied PoI x, DiCE solves an optimization problem thatoutputs a diverse set of counterfactuals. For each of these counterfactual points xCF, (xCF x) can beinterpreted as a direction recommendation for the x. FACE constructs an undirected graph over the set ofdatapoints and nds a path from the point of interest x to a set of positively classied candidate points usingDjikstras algorithm (Dijkstra, 1959). Each edge in the path that connects x to xCF can be thought of as adirection recommendation from x to xCF. The backend of C-CHVAE (Pawelczyk et al., 2020) is a variationalautoencoder (VAE): distances within the latent space surrounding the PoI is used to identify counterfactualpoints. We use the authors implementation for DiCE and adapt FACEs and C-CHVAEs implementationsfrom Poyiadzi et al. (2020) and Pawelczyk et al. (2021). Given a negatively classied PoI x, StEP and FACE produce a sequence of points (x0, x1, . . . , x) where x0 isthe original PoI, x1 is the point after following the rst direction recommendation by the recourse method,and so on. DiCE and C-CHVAE produce two point sequences (x0, x). We refer to this sequence of points asa recourse path, and each of the directions can be referred to as a step. Datasets and ModelsWe employ three real-world datasets in our cross-comparison analysis: CreditCard Default (Yeh & Lien, 2009), Give Me Some Credit (Credit Fusion, 2011), and UCI Adult/CensusIncome (Kohavi, 1996), described in . For each dataset, we train and validate logistic regression,random forest, and two-layer DNN model instances following a 70/15/15 training, validation, and test(recourse-time) data splits. Based on a balanced hyperparameter tuning across all recourse methods, wespecify a condence threshold of 0.7 at test time for each base model to determine whether a PoI is positivelyclassied. Implementation and hyperparameter tuning details are described in Appendix C.2. Metrics & Properties In alignment with counterfactual and recourse literature, we use the followingwell-established metrics to evaluate the performance of StEP, DiCE, FACE, and C-CHVAE on each base modeland dataset (Verma et al., 2020). In the following denitions, we are given a recourse path (x0, x1, . . . , x),where x0 is the PoI x. A recourse path is successful if f(x) = 1, i.e. the recommended path ultimatelyproduced a counterfactual x = xCF. Our results report the average of these metrics over all PoIs.",
  "Categorical Variables and Actionability": "We provide rigorous support for categorical variables, including immutable, semi-mutable, ordinal andunordered. Immutable featuresones that cannot be changed, e.g. raceare ignored during recourse.Semi-mutable features can only be changed in one direction, e.g., education level can only increase. Ordinalfeatures, e.g. an ordered scale like ratings, are encoded features using a Borda scale (from 1 to k). Theremaining categorical features are encoded via one-hot encoding. For recourse to be practically relevant, methods should oer actionable directions (Ustun et al., 2019b):those which do not suggest infeasible changes or changes to immutable features, i.e. that the user canactually execute. Constraints are a natural way of preventing recommendations that ask a user to performan infeasible action, e.g. becoming younger or less educated to secure a loan. StEP encodes such constraintswithin its distance metric, in a manner similar to DiCE (Mothilal et al., 2020a). For each dataset and task,we implement all appropriate encodings and constraints so that the directions produced by StEP satisfyactionability desiderata and requirements. Refer to Appendix C.1 for encodings by dataset.",
  "Comparative Analysis of StEP, DiCE, FACE, and C-CHVAE": "For each PoI, we generate k = 3 recourse paths, repeat this over 10 trials, and compute metrics and statisticsbased on the resulting counterfactuals. The comparison of StEP, DiCE, FACE, and C-CHVAE on all basemodels are presented in . We introduce additional metrics and further discussion in Appendix B. StEP For all tasks, StEP oers a balance between minimizing distance and maximizing diversity performance.StEPs lower distance recourse supports actionabilitymore proximal counterfactuals may be more actionableto the userwhile providing variety in the suggested directions. We observe dierences between distanceand diversitywhile maintaining a desirable trade-o between the two metricsacross base models whileconsistently performing well on success. On UCI Adult, StEP exhibits signicantly dierent success comparedto average success. Given that StEP produces a recourse path for each of its underlying k clusters, averagesuccess may be reduced when one or more of these clusters contains many outliers. However, even in thissetting, StEP produces successful counterfactuals for the remaining clusters. This supports StEPs robustnessto outliers and generalization to the tail of the underlying data distribution.",
  "Practical Robustness Considerations for StEP": "Clustering.We evaluate StEPs sensitivity to the choice of underlying clusters across all tasks andbase models. Firstly, using o-the-shelf k-means, we vary k between {1, . . . , 6}. Our empirical results inAppendix B.6 show that StEP is robust to both the number and relative size of clusters when using k-means.We also evaluate StEP under random clustering, by assigning each point to a random cluster value from{1, . . . , k}. Our metrics of interest slightly improve as k increases. We hypothesize that this improvementstems from the ner-grained dataset partitioning, which allows StEP to produce a path (within the givenmaximum number of iterations) from the PoI to at least one cluster. Even under random clustering, StEPperforms well on most metrics. Given the strict categorical constraints in the UCI Adult task, the centerof each random cluster being distributed uniformly across the entire dataset results in some clusters beinginherently less reachable for many PoIs. User interference.Algorithmic recourse models commonly assume that users exactly follow suggestedactions. In Appendix B.7, we relax this assumption by introducing a noise parameter as a proxy for howmuch a user deviates from the prescribed direction. We construct a noise vector where each dimension thatrepresents a continuous feature in the dataset is independently sampled from the standard normal distribution",
  "and the remaining dimensions are zero. We scale this noise vector to magnitude d, where d is theoriginal recourse vector and Rm0. The noise vector is then added to d as the next suggested action": "The results presented in demonstrate StEPs robustness to user-interference and other noise. Theimprovements in performance w.r.t. success, avg success, and 2 distance in many cases can be interpreted asnoise providing a benet similar to small amounts of stochasticity in gradient methods (e.g. SGD, momentum),helping StEP move out of a local minima. We include additional results and discussion on this task inAppendix B.7. This case study provides considerable empirical evidence to StEPs robustness to deviationfrom the suggested recourse.",
  "Discussion and Conclusion": "We introduce StEP, a data-driven method for direction-based algorithmic recourse that does not depend onthe underlying model or knowledge of its underlying causal relations. We show that the directions computedby StEP uniquely satisfy a set of desirable properties, which can be made dierentially private under somemild assumptions. We empirically demonstrate StEPs ability to produce actionable and diverse recourse, itsrobustness to user-interference, and its practical utility. In .2, we discuss the limitations of StEP without clustering and highlight the necessity of a clustering-based approach. Modulating k controls the number of clusters (and, in turn, number of recourse directionsproduced); a clustering with too ne of a granularity may result in unsuccessful recourse paths for some ofthe clusters. While standard clustering methods perform well in practice, a thorough analysis of their eectson StEP could provide useful insights in practical deployment settings. An additional limitation of StEP is its dependence on how we measure distances between datapoints, aswith DiCE, FACE and Wachters method. Dening new encoding schemas for categorical variables andinvestigating dierent notions of distance in the use of StEP and other recourse methods is an interestingarea of future work. Finally, StEP and other recourse methods raise questions regarding group fairness, e.g.whether StEP oers similar performance for users from dierent protected groups.",
  "Will Cukierski Credit Fusion. Give me some credit, 2011. URL": "Amit Datta, Anupam Datta, Ariel D. Procaccia, and Yair Zick. Inuence in Classication via CooperativeGame Theory. In Proceedings of the 24th International Joint Conference on Articial Intelligence (IJCAI),pp. 511517, 2015. Anupam Datta, Shayak Sen, and Yair Zick. Algorithmic Transparency via Quantitative Input Inuence:Theory and Experiments with Learning Systems. In Proceedings of the 37th IEEE Conference on Securityand Privacy (Oakland), pp. 598617, 2016.",
  "Cynthia Dwork and Aaron Roth. The algorithmic foundations of dierential privacy. Foundations and Trendsin Theoretical Computer Science, 9(34):211407, 2014": "Christopher Frye, Damien de Mijolla, Tom Begley, Laurence Cowton, Megan Stanley, and Ilya Feige. Shapleyexplainability on the data manifold. In Proceedings of the 9th International Conference on LearningRepresentations (ICLR), 2021. Dominik Janzing, Lenon Minorics, and Patrick Blbaum. Feature relevance quantication in explainableAI: A causal problem. In Proceedings of the 23rd International Conference on Articial Intelligence andStatistics (AISTATS), pp. 29072916, 2020. Kentaro Kanamori, Takuya Takagi, Ken Kobayashi, and Hiroki Arimura. Dace: Distribution-aware counter-factual explanation by mixed-integer linear optimization. In Proceedings of the 29th International JointConference on Articial Intelligence (IJCAI), pp. 28552862, 2020. Amir-Hossein Karimi, Gilles Barthe, Borja Balle, and Isabel Valera. Model-agnostic counterfactual expla-nations for consequential decisions. In Proceedings of the 23rd International Conference on ArticialIntelligence and Statistics (AISTATS), pp. 895905, 2020a. Amir-Hossein Karimi, Julius von Kgelgen, Bernard Schlkopf, and Isabel Valera. Algorithmic recourse underimperfect causal knowledge: a probabilistic approach. In Proceedings of the 33rd Annual Conference onNeural Information Processing Systems (NeurIPS), pp. 265277, 2020b. Amir-Hossein Karimi, Bernhard Schlkopf, and Isabel Valera. Algorithmic recourse: From counterfactualexplanations to interventions. In Proceedings of the 4th ACM Conference on Fairness, Accountability andTransparency (FAccT), pp. 353-362, 2021. Amir-Hossein Karimi, Gilles Barthe, Bernhard Schlkopf, and Isabel Valera. A survey of algorithmic recourse:Contrastive explanations and consequential recommendations. ACM Computing Surveys, 55(5), dec 2022.",
  "Divyat Mahajan, Chenhao Tan, and Amit Sharma. Preserving causal constraints in counterfactual explanationsfor machine learning classiers. ArXiv, abs/1912.03277, 2019": "Smitha Milli, Ludwig Schmidt, Anca D. Dragan, and Moritz Hardt. Model Reconstruction from ModelExplanations. In Proceedings of the 1st ACM Conference on Fairness, Accountability and Transparency(FAT*), pp. 19, 2019. Ramaravind K. Mothilal, Amit Sharma, and Chenhao Tan. Explaining machine learning classiers throughdiverse counterfactual explanations. In Proceedings of the 3rd ACM Conference on Fairness, Accountabilityand Transparency (FAccT), pp. 607617, 2020a. Ramaravind K. Mothilal, Amit Sharma, and Chenhao Tan. Explaining machine learning classiers throughdiverse counterfactual examples. In Proceedings of the 3rd ACM Conference on Fairness, Accountabilityand Transparency (FAccT), 2020b.",
  "Neel Patel, Reza Shokri, and Yair Zick. Model explanations with dierential privacy. In Proceedings of the2022 ACM Conference on Fairness, Accountability and Transparency (FAccT), pp. 18951904, 2022": "Martin Pawelczyk, Klaus Broelemann, and Gjergji Kasneci. Learning model-agnostic counterfactual explana-tions for tabular data. In Proceedings of the 26th International World Wide Web Conference (WWW), pp.31263132, 2020. Martin Pawelczyk, Sascha Bielawski, Johan Van den Heuvel, Tobias Richter, and Gjergji. Kasneci. Carla: Apython library to benchmark algorithmic recourse and counterfactual explanation algorithms. In Proceedingsof the 34th Annual Conference on Neural Information Processing Systems (NeurIPS)Track on Datasetsand Benchmarks, 2021. Rafael Poyiadzi, Kacper Sokol, Raul Santos-Rodriguez, Tijl De Bie, and Peter Flach. FACE: Feasible andactionable counterfactual explanations. In Proceedings of the 3rd AAAI/ACM Conference on ArticialIntelligence, Ethics and Society (AIES), pp. 344350, 2020. Marco T. Ribeiro, Sameer Singh, and Carlos Guestrin. Why Should I Trust You?: Explaining the Predictionsof Any Classier. In Proceedings of the 22nd International Conference on Knowledge Discovery and DataMining (KDD), pp. 11351144, 2016.",
  "Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Deep inside convolutional networks: Visualisingimage classication models and saliency maps. arXiv preprint arXiv:1312.6034, 2013": "Dylan Slack, Sophie Hilgard, Emily Jia, Sameer Singh, and Himabindu Lakkaraju. Fooling LIME and SHAP:Adversarial attacks on post hoc explanation methods. In Proceedings of the 3rd AAAI/ACM Conferenceon Articial Intelligence, Ethics and Society (AIES), pp. 180186, 2020. Dylan Slack, Anna Hilgard, Himabindu Lakkaraju, and Sameer Singh. Counterfactual explanations canbe manipulated. Proceedings of the 34th Annual Conference on Neural Information Processing Systems(NeurIPS), pp. 6275, 2021.",
  "xX(x x)(x x)1(f(x) = f(x))": "This direction, unfortunately, can lead to bad recourse recommendations. To see why, consider Example 2.1with a dierent dataset and point of interest (given in ). When nearby points are given a higherweight than far away points, the direction output by MIM could be poor, pointing away from all the positivelyclassied points. This is because when nearby points are given a higher weight than far away points, thepush away from negatively classied points can be stronger than the pull towards positively classied points.This is indeed what happens in .",
  "We compare StEP with three methods, DiCE, FACE, and C-CHVAE by generating k = 3 recourse instructionsfor each negatively classied datapoint in the test set for each dataset": "With StEP, we rst partition the positively labeled training data into 3 clusters, and then for each PoI in thetest set, we produce a direction for each of the 3 clusters. In these comparative experiments, we use sci-kitlearns default k-means implementation without any tuning, and we assume the the stakeholder follows theprovided direction exactly.",
  "B.1Baseline Recourse Methods": "We compare StEP to DiCE (Mothilal et al., 2020a), FACE (Poyiadzi et al., 2020), and C-CHVAE Pawelczyket al. (2020). DiCE outputs a diverse set of points, FACE outputs a set of paths that terminate at a positivelyclassied point, and C-CHVAE outputs points in the latent neighborhood of the PoI using an autoencoder.",
  "2dpp_diversity(c1,c2, . . . ,cm)": "Each of the output points cj can be seen as a recommendation for the direction cj x. Due to the structureof the optimization problem, none of these output points are guaranteed to be positively classied. We runDiCE with the default hyperparameter settings. FACE constructs an undirected graph over the set of data points and nds a path from the point of interestto a set of positively classied candidate points using Djikstras algorithm (Dijkstra, 1959). All the resultsgenerated in this paper use a distance threshold of 3 and the max length of a path is capped at 50 datapoints. C-CHVAE utilizes a variational autoencoder to construct a low-dimensional neighborhood within a radiusaround the PoI and searches for counterfactuals within it. At each iteration the radius to search around getsexpanded by a given step distance hyperparameter. We run C-CHVAE with a step distance of 1 and set themax number of iteration to 50. For the remaining hyperparameters, we use the default given by Pawelczyket al. (2021). We modify Pawelczyk et al.s implementation for C-CHVAE to support non-binary categoricalfeatures and k > 1 counterfactuals.",
  "j=ixi xj2": "To provide diverse recourse one can simply provide counterfactual points signicantly distant from eachother and the PoI. This makes recourse less actionable and the metric value less interpretable, thereforewe normalize by max distance (proximity) between the PoI and its counterfactuals to scale this. Proximaldiversity is only computed for PoIs where at least two recourse paths are successful. The results for these new metrics are presented in . Proximal diversity reects our observations ondistance and diversity in .2. That is, StEP has diverse and close in distance recourse and DiCE hashighly diverse and far in distance recourse. Proximal diversity weighs these two aspects, resulting in similarperformance between the two methods. Given that DiCE and C-CHVAE do not output points between thePoI and counterfactual, path steps was always one step.",
  "B.4Discussion of FACE": "FACE is a direction-based recourse method which nds the shortest path from each PoI x to k positivelyclassied candidate points that exist within the graph (i.e. counterfactuals). The existence of edges betweenpoints in a graph are determined using a distance threshold parameter. If two points have a distance lessthan the distance threshold, an edge is generated; otherwise, no edge spans the two points. Intuitively, thedistance threshold determines the size of the step users are required to take when following FACEs recourse. Finding an appropriate distance threshold for FACE is a challenging and highly task-dependent requirement.If the distance threshold is too small, the graph becomes sparse and typically produces few to no successfulrecourse paths. On the other hand, if the distance threshold is very large, the graph becomes dense andrecourse generates trivial paths between PoIs and candidate points consisting of a single edge. In this case,FACE produces a brittle recourse. One possible option for the distance threshold is to set it equal to the step-size of StEP (equal to 1) to ensurea fair comparison of the two methods, which results in very few successful paths being generated (in sometasks, with a success rate as low as 0). When increasing the distance threshold between 2 to 3, FACE beginsnon-trivial paths across our tasks. All the results generated in this paper are using the distance threshold 3.",
  "B.5Discussion of C-CHVAE": "C-CHVAE uses both the encoder and decoder from a variational autoencoder trained on the given dataset.The encoder transforms data points into a low-dimensional representation and the decoder reconstructslatent representations to the original dimension. During its search, C-CHVAE encodes the PoI and samplespoints around the hyper-sphere around it. These points are then decoded and tested on the base model todetermine if they are counterfactuals. In each iteration, C-CHVAE expands the hyper-sphere it samplesaround by a given step distance hyperparamter. To nd k counterfactuals, we continue the algorithm until kcounterfactuals are found via sampling.",
  "Max Error %1.0615.4215.425.201.6411.2011.202.350.729.419.416.88": ": User-interference analysis results on all datasets and base models. Metrics are computed on scaleddata and averaged over 10 trials. The maximum standard error bounds for each metric by task (row) andacross tasks (column) are included. Prox. Diver. is shorthand for proximal diversity. In most tasks, this iterative process of expanding the search radius step-by-step aids C-CHVAE in producinglow distance counterfactuals with high success. However, the encoding and decoding steps lead C-CHVAE tobe sensitive to strong relationships between mutable and immutable features, leading to failures in certain tasksas described in .2. The choice of number of iterations, step distance, or number of k counterfactualsdid not aect the poor success on Give Me Some Credit under logistic regression. We recommend carefuldataset selection when using C-CHVAE, as the relation between immutable and mutable features can be andis often discriminatory.",
  "B.6The Eects of Clustering on StEP": "StEP supports base clustering methods which allow the user to specify the number of clusters (e.g. k-means)and those which estimate a number of clusters (e.g. anity propagation). With the latter, the clusteringmethod determines k, the number of potential recourse paths StEP can produce. We suggest using clustering approaches where the user can specify the number of clusters (number of recoursepaths), aligning with more traditional recourse methods. Methods that empirically determine a number ofclusters to satisfy some objective function can result in a large kwhile having multiple recourse paths isdesirable, past a certain threshold loses its marginal utility, can overload users, and become challenging tointerpret if one is considering all of those paths without additional post-processing. The consideration around number of clusters has more to do with how the clusters are distributed in high-dimensional space relative to the learned decision boundary of each classier, rather than as a function ofcluster goodness or quality. From these results, we show that StEP is robust to the number of clusters fork-means, and therefore also robust to the relative size of each cluster. We expect that clustering can negatively impact recourse when the clustering method and base modelleverages immutable features. Intuitively, as an example within the Give Me Some Credit, one of the kclusters reects an older/retired population unlikely to experience nancial distress. Additionally, as observedin the C-CHVAE analysis in .2, only 8% of positively classied training examples had a featurevalue age 59 under logistic regression. Therefore, StEP consistently fails to nd a path to this cluster forPoIs younger than 59 years old. This is reected in StEP s Avg Success under logistic regression and Give",
  "Me Some Credit in . However, the other clusters were reachable for most PoIs, meaning StEP almostalways had a successful recourse path for every PoI": "From our empirical results, the consideration around number of clusters has more to do with how the clustersare distributed in high-dimensional space relative to the learned decision boundary of each classier, ratherthan as a function of cluster goodness or quality. From these results, we show that StEP is robust to thenumber of clusters for k-means, and therefore also robust to the relative size of each cluster (). We also generated results with uniformly random cluster assignments, i.e., each point is randomly assigned acluster value from {1, ..., k} (). The k = 1 case for both k-means and random clustering produce thesame results (as expected). For random clustering, as k increases, we observe small improvements across ourmetrics of interest and that StEP demonstrates reasonable performance on most tasks. We note that, giventhe strict categorical constraints in the UCI Adult task, some clusters are inherently less reachable for manyPoIs.",
  "B.7Additional StEP User-Interference Results": "In present results on the path length, path steps, and proximal diversity metrics (dened in SectionB.2). The noise we introduce oers insights into StEPs behavior. Interestingly, introducing noise producesimprovements in performance on the Give Me Some Credit Dataset () by decreasin path length andpath steps while providing a small boost in proximal diversity. Recall that StEP clusters the positively labeledevaluation data which introduces additional hyperparameters: the number of clusters, k, and the clusteringmethod. We cluster the evaluation data in full without removing any outliers via sci-kit learns o-the-shelfk-means algorithm. In particular, one cluster in the Give Me Some Credit dataset contained several outliers.",
  "Max Error %2,912.521.1915.893.213.218.9932.443.302.954.7622.89": ": Number of clusters for random cluster assignment results. Metrics are computed on scaled dataand averaged over 10 trials. Maximum standard error bounds for each metric by task and across tasks areincluded. When generating recourse for a PoI x, StEP produces a direction towards the centers of each of the k clusters.Any outliers in the evaluation data can cause dramatic shifts in the directions, due to the sparse topology ofthe outlier clusters. The Give Me Some Credit dataset exhibits signicant variance between the mean path length across theclusters. Beyond removing outliers from the dataset, we suggest the following to correct for the varianceinuenced by the presence of outliers: (a) increase the number of clusters used and ignore the paths computedfor a cluster which has an average path length of 2 standard deviations higher than the mean path lengthbetween the other clusters; (b) increasing the number of clusters may help disperse the cluster assignments ofoutliers, and the more clusters StEP uses results in more paths produced, which can help support generatinga greater number of viable suggested recourse options for a given stakeholder.",
  "C.1Datasets Details": "Credit Card Default: we produce random train, validation, and test sets from the 30, 000 instances usinga 70/15/15 split, resulting in sets with approximately 21k/4.5k/4.5k datapoints respectively. This datasetcontains 24 features, 3 of which are categorical features. We also convert the columns denoted by PAY tocontinuous values by replacing instances of 1, 2 with 0. We set MARRIAGE, AGE, and SEX asimmutable features. For StEP we set EDUCATION as an ordinal feature that can only increase. Give Me Some Credit: we produce randomly assigned train, validation, and test splits from the instancesin the same manner as with Credit Card Default. Give Me Some Credit consists of 10 features. We remove",
  "points in this dataset with missing values prior to splitting into train, validation and test sets. We set ageas an immutable feature": "UCI Adult/Census Income: we produce randomly assigned train, validation, and test splits from theinstances in the same manner as with Credit Card Default. UCI Adult/Census Income consists of 15 features,6 of which are categorical. We drop the education feature as it is equivalent to the education-numfeature. For StEP we set education-num as a feature that can only increase when giving recourse. Weset the following features as immutable features: age, marital-status, relationship, race, sex,native-country.",
  "C.2Base Model Details": "Each method relies on a base machine learning model; in our experiments, we use sci-kit learns implementationof logistic regression and random forest replicating practitioners preferences for simpler and more explainablemodels. To evaluate more complex models, we use the PyTorch library implement a non-linear neural networkmodel with two hidden layers of 16 and 32 neurons. We use simple holdout set validation to determine thehyperparameters used, outlined in C.4.",
  "C.3StEP Parameters Discussion": "Choosing a Distance Metric:We use the simple rotation invariant 2-norm as our distance metric. Toensure the distance function is not biased towards any feature, we normalize continuous feature values withrespect to their mean and standard deviation (Mothilal et al. (2020b) follow a similar methodology). Moreformally, for a continuous feature i, we transform its value as follows:",
  "where i is the mean value of the feature and i is the standard deviation": "Choosing the functionWe propose two dierent functions the volcano function and the slopedfunction. The volcano function weighs nearby points higher than faraway points, but all points closer thana specic threshold are weighed equally (a similar function is used by Patel et al. (2022)). We denote thisfunction by v and dene it as follows:"
}