{
  "Abstract": "Most existing works on fairness assume the model has full access to demographic information.However, there exist scenarios where demographic information is partially available becausea record was not maintained throughout data collection or for privacy reasons. This settingis known as demographic scarce regime. Prior research has shown that training an attributeclassifier to replace the missing sensitive attributes (proxy) can still improve fairness. However,using proxy-sensitive attributes worsens fairness-accuracy tradeoffs compared to true sensitiveattributes. To address this limitation, we propose a framework to build attribute classifiersthat achieve better fairness-accuracy tradeoffs. Our method introduces uncertainty awarenessin the attribute classifier and enforces fairness on samples with demographic informationinferred with the lowest uncertainty. We show empirically that enforcing fairness constraintson samples with uncertain sensitive attributes can negatively impact the fairness-accuracytradeoff. Our experiments on five datasets showed that the proposed framework yieldsmodels with significantly better fairness-accuracy tradeoffs than classic attribute classifiers.Surprisingly, our framework can outperform models trained with fairness constraints on thetrue sensitive attributes in most benchmarks. We also show that these findings are consistentwith other uncertainty measures such as conformal prediction. The source code is availableat",
  "Introduction": "Mitigating machine learning bias against certain demographic groups becomes challenging when demographicinformation is wholly or partially missing. Demographic information can be missing for various reasons,e.g., due to legal restrictions, prohibiting the collection of sensitive information of individuals, or voluntarydisclosure of such information. As people are more concerned about privacy, reluctant users will not providesensitive information. As such, demographic information is available only for a few users. A demographicscarce regime was the term used by Awasthi et al. (2021) to describe this particular setting. The data in thissetting can be divided into two sets: D1 and D2. The dataset D1 does not contain demographic information,while D2 contains both sensitive and non-sensitive information. The goal is to train a fair classifier withrespect to different (unobserved) demographic groups in D1. Without demographic information in D1, itis more challenging to enforce group fairness notions such as statistical parity (Dwork et al., 2012) andequalized odds (Hardt et al., 2016). Algorithms to enforce these notions require access to sensitive attributesto quantify and mitigate the models disparities across different groups (Hardt et al., 2016; Agarwal et al.,2018). However, having access to another dataset where sensitive attributes are available gives room totrain a sensitive attribute classifier that can serve as a proxy for the missing ones. We are interested inunderstanding what level of fairness/accuracy one can achieve if proxy-sensitive attributes are used to replace",
  "the true sensitive attributes and properties of the sensitive attribute classifier and the data distribution thatinfluences the fairness-accuracy tradeoff": "In their study, Awasthi et al. (2021) demonstrated a counter-intuitive finding: when using proxy-sensitiveattributes, neither the highest accuracy nor an equal error rate of the sensitive attribute classifier has animpact on the accuracy of the bias estimation. Although Gupta et al. (2018) showed that improving fairnessfor the proxy demographic group can improve fairness with respect to the true demographic group; it remainsunclear how existing fairness mechanisms would perform in the presence of proxy-sensitive attributes andhow the fairness-accuracy tradeoff is impacted. We show that existing fairness-enhancing methods (Hardtet al., 2016; Agarwal et al., 2018) can be robust to noise introduced in the sensitive attribute space by theproxy attribute classifier, i.e., unfairness can be mitigated when proxy attributes are used instead of thesensitive attribute. However, the fairness-accuracy tradeoff worsens when fairness constraints are imposedon these proxy-sensitive attributes. We aim to provide insights into the distribution of sensitive attributesthat can yield better fairness and accuracy performances. We hypothesize that the uncertainty of thesensitive attribute classifier plays a critical role in improving fairness-accuracy tradeoffs on downstream tasks.Specifically, we argue that samples with reliable demographic information should be used to fit fairnessconstraints, backed up by the intuition that these samples are easier to discriminate against, while sampleswith uncertain demographic information are already hard to discriminate against. Validating this hypothesisrequires effective uncertainty estimation in the sensitive attribute space. In this paper, we propose a framework to handle Fairness under Demographic Scarce Regime (FairDSR).FairDSR consists of two phases. In the first phase, we construct an uncertainty-aware Deep Neural Network(DNN) model to predict demographic information. The training is semi-supervised using self-ensembling (Tar-vainen & Valpola, 2017), and the uncertainty of the predictions is measured and improved during the trainingusing Monte Carlo dropout (Gal & Ghahramani, 2016). The first phase outputs for each data point, thepredicted sensitive attribute, and the uncertainty of the prediction. In the second phase, the classifier for thetarget task is trained with fairness constraints w.r.t to the predicted sensitive attributes. However, fairnessconstraints are imposed only on samples whose sensitive attribute values are predicted with low uncertainty.Our results rely heavily on dropout regularization and the effectiveness of the uncertainty measure. Existinguncertainty measures compute prediction uncertainty using the model in a certain epoch, e.g., in the lasttraining epoch. However, the uncertainty measure can vary for different models in different training epochsdue to different perturbations, such as network dropout, input noise, and data ordering. Hence, we adopt self-ensembling to compute more consistent uncertainty by considering the training dynamic. In self-ensemblinglearning, the model ensemble is obtained using an exponential running average of model snapshots. In thesemi-supervised setup, an unsupervised loss term ensures the models output remains consistent with differentperturbations. For example, given a set of labeled or unlabeled samples, the unsupervised loss enforces themodels output at the current epoch to be similar to the output aggregated from previous training epochsunder different perturbations. The aggregated outputs can be obtained using the moving average of themodels output Laine & Aila (2017) or the models weights Tarvainen & Valpola (2017) throughout thetraining. In the first phase of FairDSR, we use a similar approach to train the attribute classifier with areliable uncertainty measure in the data without demographic information. In addition to supervised loss, weadd an unsupervised loss term that ensures the models uncertainty remains consistent within epochs. On the other hand, considering the other aspect of our hypothesis, the uncertainty estimated by our methodis used to derive a subset of the dataset having higher uncertainty in the demographic information prediction.We demonstrate that using this subset to train a model without fairness constraints can yield fairer outcomes.Furthermore, we show how other uncertainty measures such as conformal prediction (Angelopoulos et al.,2023) can be integrated with our framework while achieving similar performance gains regarding tradeoffs infairness and accuracy. Our main contributions are summarized as follows: We show that the fairness-accuracy tradeoff is suboptimal when using proxy-sensitive attributesinstead of true-sensitive attributes. Specifically, when the sensitive attribute is partially available,replacing missing values using data imputation techniques based on k-nearest neighbor (KNN) orDNN models can yield a reasonably fair model but a worse fairness-accuracy tradeoff.",
  "Published in Transactions on Machine Learning Research (09/2024)": "confident about samples from well-represented groups than samples from under-represented groups. Whilethis gap between demographic groups can increase, our results show there are still enough samples from thedisadvantaged group with reliable sensitive attributes. Thus, tuning the uncertainty threshold can result in amodel that achieves a better trade-off between accuracy and various fairness metrics. Note that we observedthe same trend for the LSAC dataset. The average uncertainty is 0.66, and the minimum uncertainty is0.62. We also observed that group representation remains consistent (35% difference) when using the averageuncertainty value.",
  "Related Work": "Various metrics have been proposed in the literature to measure unfairness in classification, as well as numerousmethods to enforce fairness as per these metrics. The most popular fairness metrics include demographicparity (Dwork et al., 2012), equalized odds, and equal opportunity (Hardt et al., 2016). Demographic parityenforces the models positive outcome to be independent of the sensitive attributes, while equalized odds aimat equalizing models true positive and false positive rates across different demographic groups. Fairness-enhancing methods are categorized into three groups: pre-processing (Zemel et al., 2013; Kamiran & Calders,2012), in-processing (Agarwal et al., 2018; Zhang et al., 2018), and post-processing (Hardt et al., 2016),depending on whether the fairness constraint is enforced before, during, or after model training respectively.However, enforcing these fairness notions often requires access to demographic information. There are fairnessnotions that do not require demographic information to be achieved, such as the Rawlsian Max-Min fairnessnotion (Rawls, 2020), which aims at maximizing the utility of the worst-case (unknown) group (Hashimotoet al., 2018; Lahoti et al., 2020; Liu et al., 2021; Levy et al., 2020). Specifically, these methods focus onmaximizing the accuracy of the unknown worst-case group. However, they often fall short in effectivelytargeting the specific disadvantaged demographic groups or improving group fairness metrics (Franke, 2021;Lahoti et al., 2020). In contrast, we aim to achieve group fairness through proxy-attributes using limiteddemographic information. Recent efforts have explored bias mitigation when demographic information isnoisy (Wang et al., 2020; Chen et al., 2022a). Noise can be introduced in the sensitive feature space due tohuman annotation, privacy mechanisms, or inference (Chen et al., 2022b). Chen et al. (2022a) aims to correctthe noise in the sensitive attribute space before using them in fairness-enhancing algorithms. Another line ofwork focuses on alleviating privacy issues in collecting and using sensitive attributes. This group of methodsaims to train fair models under privacy-preservation of the sensitive attributes. They design fair modelsusing privacy-preserving mechanisms such as trusted third party (Veale & Binns, 2017), secure multipartycomputation (Kilbertus et al., 2018), and differential privacy (Jagielski et al., 2019). The most related work includes methods relying on proxy-sensitive attributes to enforce fairness whenpartial demographic information is available. Gupta et al. (2018) used non-protected features to infer proxydemographic information to replace the unobserved real ones. They showed empirically that enforcing fairnesswith respect to proxy groups generalizes well to the real protected groups and can be effective in practice.While they focus on post-processing techniques, we are interested in in-processing methods. Coston et al.(2019); Liang et al. (2023) assumed sensitive attribute is available either in a source domain or the targetdomain and used domain adaptation-like techniques to enforce fairness in the domain with missing sensitiveattributes. Moreover, while these methods can improve fairness regarding true sensitive attributes, theyresult in a worse trade-off fairness accuracy than the method using the true sensitive attributes. Diana et al.(2022) showed that training proxy classifier under multi-accuracy constraints can be a good substitute for theground truth sensitive attributes when the latter is missing; however, the resulting proxy sensitive attributesalso yield worse fairness-accuracy tradeoff. Liang et al. (2023) also considers the uncertainty in sensitivespace and proposes to replace uncertain predictions with random sampling from the empirical conditionaldistribution of the sensitive attributes given the non-sensitive attributes P(A|X, Y ). However, the randomsampling of sensitive attributes is sensitive to noise in the empirical distribution, which can negatively impact",
  "Problem Setting and Preliminaries": "Problem formulation.We consider a dataset D1 = {X, Y} where X = {xi}Mi=1 represents the non-sensitive input feature space and Y = {0, 1} represents the target variable. The goal is to build a classifier,f : X Y, that can predict Y while ensuring fair outcomes for samples from different demographic groups.However, demographic information of samples in D1 is unknown. We assume the existance of another datasetD2 = {X, A} sharing the same input feature space as D1 and for which demographic information is available,i.e., A = {0, 1}. We assume binary demographic groups for simplicity. Therefore, the dataset D1 containslabel information and D2 contains demographic information. Our goal is to leverage D2 to train an attributeclassifier h : X A that can serve as a proxy to the sensitive attributes for samples in D1, for which afairness metric can be enforced in a way to improve fairness with respect to the true sensitive attributes.Attribute classifiers have been used in health (Brown et al., 2016; Fremont et al., 2005) and finance (Zhang,2018; Silva et al., 2019) to infer missing sensitive attributes, in particular when users or patients self-reporttheir protected information. To be able to estimate the true disparities in the label classifier f, we assumethere exists a small set of samples drawn from the joint distribution X Y A, i.e., samples that jointlyhave label and demographic information. If this subset is not available, one can consider using the activesampling technique proposed by Awasthi et al. (2021) to approximate bias with respect to the predictedsensitive attributes. This estimation is beyond the scope of this work. We aim to effectively assess fairnesswithout being overly concerned about bias overestimation or underestimation. Reducing the tradeoff between fairness and accuracy is a significant challenge within the fair machine-learningcommunity (Dutta et al., 2020; Kenfack et al., 2021). Our primary goal is to design a method that effectivelyleverages proxy features to achieve similar or better fairness-accuracy tradeoffs compared to settings wherethe true sensitive attributes are available. To this end, we considered a different range of fairness metrics andfairness-enhancing techniques. Fairness Metrics.In this work, we consider three popular group fairness metrics: demographic par-ity (Dwork et al., 2012), equalized odds, and equal opportunity (Hardt et al., 2016). These metrics aim toequalize the models performance across different demographic groups; we provide more details in Appendix B. Fairness Mechanism.We focus on in-processing techniques to improve the models fairness. Thesemethods introduce constraints in the classification problem to satisfy a given fairness metric. Our studyfocuses on state-of-the-art techniques in this category, i.e., exponentiated gradient (Agarwal et al., 2018) andadversarial debiasing (Zhang et al., 2018). We considered these methods as they allow better control overfairness and accuracy. In general, the optimization problem contains a parameter that controls the balancebetween fairness and accuracy, i.e., a higher value of would force the model to achieve higher fairness(respectively lower accuracy) while a smaller yields higher accuracy (respectively lower fairness). We aim todesign a sensitive attribute predictor that achieves a better fairness-accuracy tradeoff, i.e., for the same valueof build a model that provides higher accuracy and lower unfairness.",
  "Step 1: Uncertainty-aware sensitive attribute prediction": ": Overview of FairDSR. Our framework consists of two steps. In the first step (left), the datasetD2 is used to train the attribute classifier for the student-teacher framework. The first step producesproxy-sensitive attributes (h(X) = A) and the uncertainty of their predictions (U). In the second step (right),the fair model is trained using only samples with reliable proxy-sensitive attributes. These samples areselected based on a defined threshold of their uncertainties.",
  "Method": "This section presents the methodology and all components involved in the framework FairDSR. presents an overview of the stages in our framework and the interactions between and within each stage.The first stage consists of training the attribute classifier with uncertainty awareness. This step outputs foreach sample with missing sensitive attribute, its predicted sensitive attribute (proxy), and the predictionsuncertainty. In the second stage, the label classifier is trained with fairness constraints enforced using thepredicted sensitive attributes. To validate our hypothesis, fairness is enforced only on samples with the lowestuncertainty in the sensitive attribute prediction, i.e., samples with an uncertainty lower than a predefineduncertainty threshold H.",
  "Uncertainty-Aware Attribute Prediction": "We build the sensitive attribute classifier using a semi-supervised learning approach that accounts for theuncertainty of the predictions of samples with missing sensitive attributes, similar to Yu et al. (2019);Tarvainen & Valpola (2017); Laine & Aila (2017). Motivated by the uncertainty estimation in Bayesiannetworks, we estimate the uncertainty with the Monte Carlo Dropout (Gal & Ghahramani, 2016). However,measuring the uncertainty of the predictions using the model in a given training step (e.g., the last epoch)might result in an unreliable uncertainty measure due to different perturbations, such as network dropout,input noise, and data ordering (He et al., 2023). In addition, we are interested in designing an attributeclassifier that minimizes the uncertainty of the predictions in the data with missing sensitive attributes. To overcome this challenge, we adopt self-ensemble learning to ensure consistent uncertainty measurement bymaintaining the moving average of the attribute classifiers weights during the training (Tarvainen & Valpola,2017). More specifically, the model maintains a moving average of itself during the training to provide astable learning signal. We refer to the model at the current training epoch as the student model and itsmoving average (self-ensembling) from previous epochs as the teacher model. Student Model.The student model is implemented as a neural network and is trained on D2 (sampleswith sensitive attributes) to predict sensitive attributes. Under a semi-supervised setup, the attribute classifieris optimized to minimize a double loss: the classification loss (Ls), i.e., the cross-entropy loss, and theconsistency loss (Lc) (Yu et al., 2019). The consistency loss (or unsupervised loss) forces the student modelto focus on samples with low uncertainty in the sensitive attributes guided by the uncertainty estimation",
  "x|uxR f(x) h(x) 2(3)": "The consistency loss is applied only on samples, x, whose uncertainty, ux, is lower than the threshold R.Following Srivastava et al. (2014); Baldi & Sadowski (2013), R and are updated using a Gaussian warmupfunction to prevent the model from diverging at the beginning of the training. The motivation behind theconsistency loss is the focus on our primary goal for the attribute classifier, which is to find the missingsensitive attributes in D1 with low uncertainty. In the experiments (.2.3), we show that training themodel without consistency loss results in worse fairness-accuracy tradeoffs. Teacher Model.The teacher model maintains the moving average of the students weights and is used foruncertainty estimation. Specifically, the teacher weights are updated within each training epoch, t, using theexponential moving average (EMA) of student weights as follows:",
  "t = t1 + (1 ),(4)": "where and denote the respective weights of student and teacher and controls the moving decay. Theuse of EMA to update the teacher model is motivated by previous studies (Laine & Aila, 2017; Yu et al.,2019) that have shown that averaging model parameters at different training epochs can provide more reliablepredictions than using the most recent model weights in the last epoch. The teacher model gets as inputboth labeled and unlabeled samples (D1 and D2) and computes the uncertainty of their predictions usingMonte Carlo (MC) dropout (Gal & Ghahramani, 2016). Uncertainty Estimation.MC dropout is an approximation of a Bayesian neural network widely used tointerpret the parameters of neural networks (Abdar et al., 2021). MC dropout can effectively capture varioussources of uncertainties (Aleatoric and Epistemic uncertainty). It uses dropout at test time to computeprediction uncertainty from different sub-networks that can be derived from the whole original neural network.Dropout is generally used to improve the generalization of DNNs. During training, the dropout layer randomlyremoves a unit with probability p. Therefore, each forward and backpropagation pass is done on a differentmodel (sub-network), forming an ensemble of models that are aggregated together to form a final model withlower variance (Srivastava et al., 2014; Baldi & Sadowski, 2013). The uncertainty of each sample is computedusing T stochastic forward passes on the teacher model to output T independent and identically distributedpredictions, i.e., {h1(x), h2(x), , hT (x)}. The softmax probability of the output set is calculated. Theuncertainty of the prediction (ux) is quantified using the resulting entropy: ux = a pa(x) log(pa(x)), wherepa(x) is the probability that sample x belongs to demographic group a estimated over T stochastic forwardpasses, i.e., pa(x) = 1 TTt=1 hat (x). In the experiments (.2.3), we compare with another uncertaintymeasure based on the confidence interval of the prediction probability and show the limitation of this measurein identifying highly uncertain samples. Furthermore, we also show that using reliable uncertainty measuressuch as conformal prediction (Angelopoulos et al., 2023) can also improve the fairness-accuracy tradeoff.",
  "Enforcing Fairness w.r.t Reliable Proxy Sensitive Attributes": "After the first phase, the attribute classifier can produce for every sample in D1, i.e., samples with missingsensitive attributes, their predicted sensitive attribute (proxy) A = {h(xi)xiD1}, and the uncertainty of theprediction U = {uxi}xiD1. To validate our hypothesis, we define a confidence threshold H for samples usedto train the label classifier with fairness constraints, i.e., the label classifier with fairness constraints is trainedon a subset D1 D1 defined as follows:",
  "D1 = {(x, y, f(x))|ux H}(5)": "Note that the threshold R in the previous step is only used to train the attributes classifier while ensuring thestudent and teacher remain consistent for samples with uncertainty lower than R. The threshold H in thisstep is used at test time to select samples with low uncertainty, and its value can be tuned over a validationset.The hypothesis of enforcing fairness on samples whose sensitive attributes are reliably predicted stemsfrom the fact that the model can confidently distinguish these samples based on their sensitive attributes inthe latent space. In contrast, the label classifier is inherently fairer if an attribute classifier cannot reliablypredict sensitive attributes from training data (Kenfack et al., 2023). We further support this in section 5.2 bycomparing the sensitive attribute uncertainty in the New Adult dataset (Ding et al., 2021) and the old versionof the dataset (Asuncion & Newman, 2007). The fairness constraints on samples with unreliable sensitiveattributes could push the models decision boundary in ways that penalize accuracy and/or fairness. Wesupport these arguments in the experiments. In addition to this method, we considered two other approachesbuilt around our hypothesis: the weighted approach FairDSR (weighted) and the uncertain approachFairDSR (uncertain) FairDSR (weighted). Given that downsampling can reduce accuracy when uncertainty is low, insteadof pruning out samples based on the uncertainty of their sensitive attributes, we considered a trainingprocess in the second step that enforces fairness constraints on all samples but weighted proportionally to theuncertainty of their sensitive attributes, i.e., samples with less uncertainty receive higher weights. We showin the experiments that this weighted approach can better preserve accuracy. FairDSR (uncertain). Considering the ethical risks of inferring missing sensitive information, we alsoconsidered a variant in the second step where the model is trained without fairness constraints, i.e., withoutusing sensitive information. Instead, we train the model using samples with higher uncertainty in theirsensitive attributes for a given uncertainty threshold. We show in the experiments that this variant cansignificantly improve fairness, although no fairness constraints are applied.",
  "Experimental Setup": "Datasets.We validate our method on five real-world benchmarks widely used for bias assessment: AdultIncome (Asuncion & Newman, 2007)1, Compas (Jeff et al., 2016), Law school (LSAC), CelebA (Liu et al.,2018) (Wightman, 1998), and the New Adult (Ding et al., 2021) dataset. We use 20% of each dataset as thegroup-labeled dataset (D2) and 80% as the dataset without sensitive attributes (D1). Supplementary C.1shows that our results hold even with a smaller group-labeled ratio, e.g., 5%. More details about the datasetsappear in Supplementary B.2. Attribute classifier.The student and teacher models were implemented as feed-forward Multi-layerPerceptrons (MLPs) with Pytorch (Paszke et al., 2019), and the loss function 1 is minimized using the Adamoptimizer (Kingma & Ba, 2014) with learning rate 0.001 and batch size 256. Following Yu et al. (2019); Laine& Aila (2017), we used = 0.99 for the EMA parameter for updating the teacher weights using the students",
  "For comparison, we considered methods aiming to improve fairness without (full) demographic information.We compare with the following methods:": "FairRF (Zhao et al., 2022): This method assumes that non-sensitive features correlating with sensitiveattributes are known. It leverages these related features to improve fairness w.r.t the unknownsensitive attributes. FairDA (Liang et al., 2023): Similar to our setting, this method assumes the sensitive information isavailable in a source domain (dataset D2 in our setting). It uses a domain adaptation-based approachto transfer demographic information from the source domain to improve fairness in the target usingan adversarial approach. CGL (Jung et al., 2022): With partial access to sensitive attributes, this method also uses an attributeclassifier and replaces uncertain predictions with random sampling from the empirical conditionaldistribution of the sensitive attributes given the non-sensitive attributes P(A|X, Y ).",
  "KSMOTE (Yan et al., 2020) performs clustering to obtain pseudo groups and use them as substitutesto oversample the minority groups": "For each method considered for comparison, we used the code provided by the authors2 along with therecommended hyperparameters. We considered two vanilla baselines: a baseline where the model is trainedwithout fairness constraints (Vanilla (without fairness)) and a baseline model trained with fairness constraintsover the true sensitive attributes (Vanilla (with fairness)). We also considered baselines trained with fairnessconstraints directly using predicted sensitive attributes obtained by data imputation using our studentnetwork (Proxy-DNN) and imputation using K-nearest neighbor (Proxy-KNN). For comparison, in addition to the accuracy, we consider the three fairness metrics described in the appendix B,i.e., equalized odds (EOD), equal opportunity (EOP), and demographic parity (DP). All the baselines aretrained on 70% of D1, and fairness and accuracy are evaluated on the 30% as the test set. We assume thesensitive attribute is observed in the test set to report the true fairness violation. We trained each baseline 7times and averaged the results. We use logistic regression3 as the base classifier for all the baselines and traineach baseline to achieve minimal fairness violation.",
  "FairDSR (weighted)0.846 0.0030.032 0.0130.050 0.0330.027 0.019FairDSR (uncertain)0.825 0.0130.106 0.0360.065 0.0470.068 0.032FairDSR (certain)0.830 0.0040.007 0.0050.015 0.0100.018 0.016": ": Results on the Adult dataset. Bolded values represent the best-performing baselines amongthe fairness-enhancing methods without (full) demographic information. All the baselines are trained on thedataset without the sensitive attributes (D1). Each experiment is conducted 7 times, and the fairness andaccuracy are averaged. method and different fairness measures of a logistic regression model trained without fairness constraintson the dataset D1. These results show the correlation between the uncertainty of the sensitive attributeprediction and the fairness of the model. For example, we observe that the uncertainty in the Adult datasetis lower than in the New Adult dataset, while the unfairness in the Adult dataset is higher. On the otherhand, the LSAC dataset has the highest uncertainty of the sensitive attribute (0.66), and the model withoutfairness constraints has the smallest fairness violation across all fairness metrics. In sum, we can observethat unfairness is higher for datasets with a lower uncertainty in the sensitive attributes, e.g., the Adultand CelebA datasets. These results support our hypothesis that a model can hardly discriminate againstsamples with uncertain demographic information. Furthermore, we investigate the relation between the levelof uncertainty of the sensitive attributes in the training data and the fairness of downstream classifiers trainedwithout fairness constraints. Our study reveals that a model without fairness constraints tends to be fairer asthe uncertainty of the sensitive attribute in the training data increases. Specifically, we trained different classifiers (Logistic Regression and Random Forest) without fairness con-straints but using training data with higher uncertainty in the sensitive attributes. For different uncertaintythresholds H {0.0, 0.1, ..., 0.6}, we prune out samples whose uncertainty is lower than H and train themodel without fairness constraints using the remaining training data, i.e., {(x, y) D1|ux H}, where ux isthe estimated uncertainty provided by our method. In particular, when H = 0, all the data points are usedfor the training, and in other cases, we only maintain samples with uncertainty higher than H, i.e., the modelis trained on samples with more uncertain sensitive attributes. We train the model seven times with differentrandom seeds for each uncertainty threshold and report fairness and accuracy in the testing set containingsensitive attributes.",
  "Uncertainty Threshold": "Demographic ParityEqual OpportunityEqualized OddsAccuracy : Training Logistic Regression without fairness constraints using samples with high uncertainty ofsensitive attribute predictions. For each uncertainty threshold H, the model is trained on samples withuncertainty H. The training is done seven times, and the average fairness (first row) and accuracy (secondrow) are reported. Shaded represents the standard deviation.",
  "Accuracy": ":Accuracy-fairness trade-offs for various fairness metrics (DP, EOP, EOD) and proxy-sensitiveattributes. Top-left is the best (Highest accuracy with the lowest unfairness). The fairness mechanism is theExponentiated gradient with logistic regression as the base classifier on the Compas (a), Adult (b), LSAC (c),and CelebA (c)datasets. The standard deviation is shaded in the figure. while fairness is improved or maintained. On the LSAC dataset ((d)), we observe that increasingthe uncertainty threshold results in a much higher drop in accuracy. This is explained by the high averageuncertainty (0.66), and using a smaller threshold removes most of the data. Experiments with adversarial debaising shows the trade-offs for adversarial debiasing. Ourmethods achieve a better trade-off on the Adult datasets, while for the Compas dataset, the ground-truthsensitive achieves a better trade-off. It is worth noting that adversarial debiasing is unstable to train.",
  "Ablation experiments": "Impact of the uncertainty threshold. showcases the impact of the uncertainty thresholdon the fairness-accuracy threshold. When the feature space encodes much information about the sensitiveattribute as in the Adult dataset (a) with 85% accuracy of predicting the sensitive attributes, theresults show that the more we enforce fairness w.r.t. samples with the lower uncertainty, the better thefairness-accuracy tradeoffs. In this regime, enforcing unfairness helps the model maintain a better accuracylevel (a). In contrast, in a low bias regime, i.e., when the feature space does not encode enoughinformation about the sensitive attributes, such as on the Compas and the New Adult dataset, the modelachieves better fairness-accuracy tradeoffs when a higher uncertainty threshold is used. In this regime, mostof the samples have higher uncertainty in the sensitive attribute prediction, and fairness violation is smaller(see ), as can be observed in b, the use of a lower uncertainty threshold leads to a decreasein accuracy while fairness is improved. We observe similar results in the New Adult, CelebA, and LSACdatasets (Fig 13 in Supplementary). The accuracy drops since more and more samples were pruned outfrom the datasets, and this suggests that the feature space is more informative for the target task than thedemographic information. In the appendix (D), we show that while under-represented demographic groupscan have higher uncertainty on average than well-represented groups, minority groups are still consistentlyrepresented when a lower threshold is used. Importance of the consistency loss.In the proposed framework, we use trained attribute classifierwith a double loss: the cross-entropy loss (supervised loss) and the consistency loss (unsupervised loss).The parameter controlling the consistency loss is updated using a Gaussian ramp-up function startingfrom zero at the beginning of the training. The consistency loss pushes the student to focus on data pointswith highly certain sensitive attributes. Without this second loss term, the student model minimizes theclassification error without explicit consideration of the uncertainty in the dataset without sensitive attributes.",
  ": Consistency loss study on the Adult dataset. The predicted sensitive attributes are obtainedusing our student model with and without consistency loss": "To demonstrate the importance of consistency loss (and thus the importance of self-ensembling with theteacher), we experiment with the student model trained without the consistency loss, i.e., we set = 0 inEquation 1. We applied the same procedure of the second step of FairDSR (certain) to derive data pointswith sensitive attributes predicted with low uncertainty. showcases the Pareto front of our FairDSR trained without (w/) and without (w/o) the consistencyloss. The base classifier is Random Forest with exponentiated gradient as the fairness mechanism. The Figurealso shows the Pareto front of the model trained with the ground truth sensitive attribute and the predictedsensitive attribute (Proxy-DNN). We observe that training the model with the consistency loss providesPareto dominant points across different fairness metrics. This demonstrates the benefit of the consistencyloss in effectively identifying samples with sensitive attributes predicted with low uncertainty in the datasetwithout demographics. We also observed that the mean uncertainty for the model without the consistencyloss is around 0.19 0.08, while the model that uses the consistency loss is 0.15 0.12. This suggests that theconsistency loss effectively enforces the student model focus on data points with low uncertainty. Moreover,we also observe that the model without the consistency loss still outperforms the model that directly uses thesensitive attributes (Proxy-DNN) and the model that uses true sensitive attributes. This demonstrates thatour hypothesis still holds even if the uncertainty measure is less effective, and a better uncertainty measurecan provide even better results. Experiment with uncertainty measure based on confidence intervals.We adopted Monte Carlodropout because it efficiently captures various sources of uncertainty, both in the data and the model. Anotherclassic approach to measuring uncertainty is the models prediction confidence. In this experiment, weevaluate the effect of using confidence intervals as the uncertainty measure on fairness-accuracy tradeoffs.Given a trained attribute classifier, recall we obtain the predicted group label by thresholding the predictionprobability, i.e., f(x) = 1(P( A = a|X = x) 0.5). We construct the confidence interval using a threshold [0.5, 1], such that samples with higher uncertaintyhave their prediction probability closer to 0.5.A given data point x is in the high uncertainty set if1 < P( A = a|X = x) < , while x is in the low uncertainty set otherwise. More specifically, in thisexperiment, we derive the subset of data points with low uncertainty of the sensitive attribute prediction asfollows :",
  ": Study of uncertainty measure based on conformal predictions": "in , models trained using confidence intervals as uncertainty measures yield worse Pareto points.Specifically, we observe a significant drop in accuracy while fairness performances significantly improve. Wealso observe that a higher confidence threshold tends to provide better fairness for a similar level of accuracy.While this observation also supports our hypothesis, the drop in accuracy suggests that the confidenceinterval is less effective in identifying data points with low uncertainty in sensitive attributes. We suspectthis results from the unreliability of the model probability due to overconfidence or underconfidence in wrongpredictions (Guo et al., 2017). As the confidence interval is obtained using a single model, low-confidenceprediction could be solely caused by poor calibration or randomness in the algorithm. We validate this inthe next ablation study by calibrating the attribute classifier and using conformal prediction to measureuncertainty. Ablation on uncertainty measure based on conformal predictions.In this experiment, we inves-tigate the validity of our hypothesis under different and reliable uncertainty measures, such as conformalprediction (Shafer & Vovk, 2008). Conformal prediction, a machine learning framework, constructs predictionsets containing possible labels. These sets are guaranteed to include the true label with a probability of 1 ,where is a user-defined error rate. For instance, setting = 0.1 ensures the prediction set has atleast a 90% chance of containing the correct label. The size of the set reflects the models confidence in itsprediction. Ideally, the set would contain only one label, indicating high confidence. Larger sets suggestgreater uncertainty about the predicted label. To construct the prediction set of the sensitive attribute, withthe 1 guarantee, we implement split conformal prediction as described by Angelopoulos et al. (2023),which is the widely-used version of conformal prediction. Split conformal prediction takes the pretrainedattribute classifier and calibration set to compute the conformal prediction threshold4 used to include labelsin the prediction set of the test set. We employ the Model Agnostic Prediction Interval Estimator (MAPIE)library to train a conformal classifier and measure the uncertainties in the attribute classier (Cordier et al.,2023). The classifier wraps the original attribute classifier and produces conformal prediction sets with",
  "the guaranteed marginal coverage rate 1 . We used 10% of the dataset with sensitive attributes as thecalibration set and to train the calibrated attribute classifier": "In the second step of our framework, we generate prediction sets of the sensitive attributes in the datasetwhere demographic information is missing (D1). Since the sensitive attributes are binary, there are fourpossible prediction sets for each data point: {0}, {0, 1}, {1}, and the empty set {}. This means eachprediction set for every sample consists of either a single attribute value, both values or no value at all (Guptaet al., 2020). Samples with prediction sets containing exactly one attribute value are grouped into subsets ofsamples with low uncertainty (D1). On the other hand, samples with empty prediction sets or sets containingboth attribute values indicate higher uncertainty regarding the sensitive attributes. As we used a score-basedmethod to compute non-conformity scores, the prediction set for uncertainty predictions might be empty if aset size smaller than 1 is necessary to ensure 1 coverage (Cordier et al., 2023). In our experiments on the Adult dataset, we varied across values {0.05, 0.1, 0.2}. This means for = 0.05, = 0.1, and = 0.2, the prediction sets are guaranteed to include the true sensitive attribute with probabilitiesof 95%, 90% and 80%, respectively. Our model training incorporated fairness constraints specifically ondata points where the prediction set contained a single attribute value, indicating higher confidence in theprediction for smaller values of . Following the previous evaluation, we trained the Random Forest classifierfor each and sweeping range of fairness coefficients , taking the median of 7 runs and computing the Paretofront. showcases the Pareto front of different models with fairness constraints on samples withgreater certainty of the sensitive attributes (samples with single-valued prediction sets) for different values of. As can be seen in Fig 7, all models trained using samples with low uncertainty in the sensitive attributesachieve better Pareto front than the model using all the predicted sensitive values (Proxy-DNN) and evenground truth sensitive attributes (clean). This demonstrates the benefit of applying fairness constraintsover data points with low uncertainty in the sensitive on-fairness accuracy tradeoff. Moreover, the figureshows that smaller values of result in better Pareto dominant points, i.e., better fairness-accuracy tradeoff.This is justified by the fact that smaller values of increase the certainty of samples with a single value intheir prediction set. Specifically for = 0.05, the Pareto front is closer to our baseline method using thedropout-based uncertainty measure. We observed a similar trend on other datasets and with different baseclassifiers. These results also support our hypothesis that imposing fairness constraints on the sample withlow uncertainty in the sensitive attribute can better improve fairness and accuracy. On the other hand, using conformal predictions to measure uncertainty, we also evaluated the variant of ourhypothesis where the model is trained without fairness constraints using samples with greater uncertainty inthe predicted sensitive attributes. This means we trained the model without fairness constraints but usingsamples whose prediction sets are empty or contain two values. We also trained models without fairnessconstraints using samples with greater certainty of the sensitive attributes, i.e., samples with single valuesin their prediction sets. shows the fairness and accuracy of Random Forest models trained withoutfairness constraints using data with different uncertainty levels on the Adult dataset. More specifically, weconsidered three cases:",
  "Certain: The model is trained using only samples with greater certainty of the sensitive attribute.This means the training data contains only samples with a single in the prediction sets": "Uncertain: The model is trained using only samples with greater uncertainty of the sensitive attribute.This means the training data contains only samples whose prediction sets are empty or contain twovalues. The results depicted in show that, without fairness constraints, the models using samples with greatercertainty of sensitive attributes have better accuracy but tend to exacerbate unfairness, while models usingdata with lower uncertainty significantly improve fairness at the expense of accuracy. More specifically,for = 0.05, the model using only data with uncertain sensitive attributes achieves fairness in terms ofdemographic parity of 0.009 compared to 0.12 for the model using data with certain sensitive attributes.This corresponds to a 92% improvement in fairness with a smaller drop of 8% for accuracy. This showsmodels hardly discriminate against samples with uncertain sensitive attributes, and the fairness-accuracytradeoffs can be controlled by uncertainty in sensitive attribute space. This also supports our hypothesisunder different and reliable uncertainty measures.",
  "Conclusion": "In this work, we introduced FairDSR, a framework to improve the fairness-accuracy tradeoff when onlylimited demographic information is available. Our method introduces uncertainty awareness in the sensitiveattributes classifier. We showed that uncertainty in the attribute classifier plays an important role in thefairness-accuracy tradeoffs achieved in the downstream model with fairness constraints. We showed thatenforcing fairness on samples whose sensitive attributes are predicted with low uncertainty can yield modelswith better fairness-accuracy tradeoffs. Subsequently, we considered different variations around our hypothesisand demonstrated the benefit of uncertainty quantification in the sensitive attribute space for designing fairmodels under unknown demographic information.",
  "Impact Statement": "The proposed framework shows evidence that models can hardly discriminate against samples with highuncertainty in sensitive attribute prediction. However, our base model relies on the predicted missing sensitiveinformation. Inferring sensitive information can raise ethical concerns and face legal restrictions, especiallywhen individuals do not choose to disclose their sensitive information. The line of methods relying uponproxy attributes or inferring sensitive attributes faces this limitation (Diana et al., 2022; Awasthi et al.,2021; Coston et al., 2019; Liang et al., 2023). For this reason, we proposed a variant of our model (FairDSR(uncertain), Sec. 4.2) that derives a dataset with higher uncertainty in sensitive attributes without usingpredicted information.In addition, verifying that no group of users is left out when training the model due tothe lower uncertainty of their sensitive attributes is important. While we analyzed group representation andshowed that groups are still relatively well represented even with a higher uncertainty threshold ( inSupplementary), there are still a risks of disparate impact.",
  "Pierre Baldi and Peter J Sadowski. Understanding dropout. Advances in neural information processingsystems, 26, 2013": "Sarah Bird, Miro Dudk, Richard Edgar, Brandon Horn, Roman Lutz, Vanessa Milan, Mehrnoosh Sameki,Hanna Wallach, and Kathleen Walker. Fairlearn: A toolkit for assessing and improving fairness in ai.Microsoft, Tech. Rep. MSR-TR-2020-32, 2020. David P Brown, Caprice Knapp, Kimberly Baker, and Meggen Kaufmann. Using bayesian imputation toassess racial and ethnic disparities in pediatric performance measures. Health services research, 51(3):10951108, 2016.",
  "Canyu Chen, Yueqing Liang, Xiongxiao Xu, Shangyu Xie, Yuan Hong, and Kai Shu. On fair classificationwith mostly private sensitive attributes. arXiv preprint arXiv:2207.08336, 2022a": "Canyu Chen, Yueqing Liang, Xiongxiao Xu, Shangyu Xie, Yuan Hong, and Kai Shu. When fairness meetsprivacy: Fair classification with semi-private sensitive attributes. In Workshop on Trustworthy and SociallyResponsible Machine Learning, NeurIPS 2022, 2022b. Jiahao Chen, Nathan Kallus, Xiaojie Mao, Geoffry Svacha, and Madeleine Udell. Fairness under unawareness:Assessing disparity when protected class is unobserved. In Proceedings of the conference on fairness,accountability, and transparency, pp. 339348, 2019. Thibault Cordier, Vincent Blot, Louis Lacombe, Thomas Morzadec, Arnaud Capitaine, and Nicolas Brunel.Flexible and systematic uncertainty estimation with conformal prediction via the mapie library.InConformal and Probabilistic Prediction with Applications, pp. 549581. PMLR, 2023. Amanda Coston, Karthikeyan Natesan Ramamurthy, Dennis Wei, Kush R Varshney, Skyler Speakman,Zairah Mustahsan, and Supriyo Chakraborty. Fair transfer learning with missing protected attributes. InProceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society, pp. 9198, 2019. Emily Diana, Wesley Gill, Michael Kearns, Krishnaram Kenthapadi, Aaron Roth, and Saeed Sharifi-Malvajerdi.Multiaccurate proxies for downstream fairness. In 2022 ACM Conference on Fairness, Accountability, andTransparency, pp. 12071239, 2022.",
  "Faisal Kamiran and Toon Calders. Data preprocessing techniques for classification without discrimination.Knowledge and information systems, 33(1):133, 2012": "Patrik Joslin Kenfack, Adil Mehmood Khan, SM Ahsan Kazmi, Rasheed Hussain, Alma Oracevic, andAsad Masood Khattak. Impact of model ensemble on the fairness of classifiers in machine learning. In2021 International conference on applied artificial intelligence (ICAPAI), pp. 16. IEEE, 2021. Patrik Joslin Kenfack, Adn Ramrez Rivera, Adil Khan, and Manuel Mazzara. Learning fair representationsthrough uniformly distributed sensitive attributes. In First IEEE Conference on Secure and TrustworthyMachine Learning, 2023.",
  "Glenn Shafer and Vladimir Vovk. A tutorial on conformal prediction. Journal of Machine Learning Research,9(3), 2008": "Gabriella C Silva, Amal N Trivedi, and Roee Gutman. Developing and evaluating methods to imputerace/ethnicity in an incomplete dataset. Health Services and Outcomes Research Methodology, 19(2-3):175195, 2019. Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: asimple way to prevent neural networks from overfitting. The journal of machine learning research, 15(1):19291958, 2014. Antti Tarvainen and Harri Valpola. Mean teachers are better role models: Weight-averaged consistencytargets improve semi-supervised deep learning results. Advances in neural information processing systems,30, 2017.",
  "ALimitations": "Our method is evaluated mainly on one fairness-enhancing algorithm (i.e., Exponentiated Gradient). It willbe interesting to explore if our hypothesis applies to pre-processing and post-processing techniques and withdifferent fairness-enhancing algorithms. Finally, our assumption that the true sensitive attributes are availablein the test dataset for fairness evaluation might not be true in several practical scenarios. This might requireevaluation using proxy-sensitive attributes. These proxies are likely noisy and might require evaluations usingbias assessment methods that effectively quantify fairness violation w.r.t to true sensitive attribute (Chenet al., 2019; Awasthi et al., 2021). On the other hand, it will be interesting to back our empirical findingswith theoretical results by characterizing fairness violation bounds based on the uncertainty quantification ofthe sensitive attribute predictions.",
  "B.1Fairness Metrics": "Demographic ParityAlso known as statistical parity, this measure requires that the positive prediction(f(X) = 1) of the model be the same regardless of the demographic group to which the user belongs (Dworket al., 2012). More formally the classifier f achieves demographic parity if P(f(X) = 1|A = a) = P(f(X) = 1)In other words, the models outcome should be independent of sensitive attributes. In practice, this metric ismeasured as follows:",
  "B.2Datasets": "In the Adult dataset, the task is to predict if an individuals annual income is greater or less than $50k peryear. We also considered the recent version of the Adult dataset (New Adult) for 2018 across different statesin US (Ding et al., 2021). For the Compas dataset, the task is to predict whether a defendant will recidivatewithin two years. The LSAC dataset contains admission records to law school. The task is to predict whether",
  ": Datasets": "a candidate will pass the bar exam. The CelebA5 dataset contains 40 facial attributes of humaine annotatedimages. We consider the task of predicting attractiveness using gender as a sensitive attribute. We randomlysample 20% of the data to train the sensitive attribute classifier (D2). All the input features are used to trainthe attribute classifier except for the target variable. provides more details about each dataset andsensitive attributes used. 0.10 0.20 0.30",
  "C.1Impact of the group-labeled ratio": "In all the previous experiments, we considered the ratio of the group-labeled dataset (D2) as 20% of theoriginal dataset. In many real-world scenarios, data labeling is costly, especially given the rising concernssurrounding privacy. As these concerns intensify, fewer users may be inclined to disclose their sensitiveattributes. It is, therefore, important to study the impact of the group-labeled ratio on the performance ofour method. In this ablation study, we consider the attribute classifier trained with different ratios of D2, i.e.,3%, 5%, 10%, 15%, and 20% of the original dataset. We experiment on the Adult, wherein 3% representsaround 1,465 data points among the 48,842 data points available.",
  ": Study of group-labeled ratio. The sensitive attribute classifier is trained using different ratiosof the dataset with sensitive attributes (D2)": "shows the Pareto front for different group-labeled ratios. Our findings remain consistent even inthe most demographic scare regime. While we observe a slight drop in accuracy for smaller group-labeledratios, the fairness performances are maintained. The Pareto fronts still dominate the model directly usingthe predicted sensitive attributes. These results suggest that our hypothesis holds even when very limiteddemographic information is available, as long as the dataset is predictive for sensitive attributes. On the otherhand, even if the dataset is not predictive enough for the sensitive attributes, our results show that training amodel without fairness constraints will exhibit less disparities across the unknown demographic groups.",
  "C.2Results on Nonlinear Classifier": "compares with other baselines on the CelebA dataset with logistic regression as the base classifier. Inthe main paper, we compared existing methods using Logistic Regression as the base classifier. We performedexperiments with a more complex non-linear model to analyze its impact on the performance of differentmethods. We considered a Multi-Layer Perceptron (MLP) with one hidden layer of 32 units and Relu asthe activation function for all the baselines. On the Adult dataset, shows that when using a morecomplex model, our method (FairDSR (certain)) still provides Pareto dominants points in terms of fairnessand accuracy compared to other baselines. At the same time, we observed an improvement in the accuracy ofother methods due to the increased model capacity.",
  "C.3Comparison with Other Baselines": "To assess the effect of the attribute classifier over the performances of downstream classifiers with fairnessconstraints w.r.t the proxy, we also performed extensive comparisons with different methods of obtaining themissing sensitive attributes: Ground truth sensitive attribute. We considered the case where the sensitive attribute is fullyavailable and trained the model with fairness constraints w.r.t the ground truth. This representsthe ideal situation where all the assumptions about the availability of demographic information aresatisfied. This baseline is expected to achieve the best trade-offs.",
  "DUncertainty Estimation of Different Demographic Groups": "In the main paper, we showed that when the dataset does not encode enough information about the sensitiveattributes, the attribute classifier has, on average, greater uncertainty in the predictions of sensitive attributes.This encourages a choice of a higher uncertainty threshold to keep enough samples to maintain the accuracy,i.e., to prune out only the most uncertain samples. shows that the gap between demographic groupscan increase as a smaller uncertainty threshold is used. This is explained by the fact that the model is more",
  ": Adversarial debiasing. Accuracy-fairness trade-offs for various fairness metrics (DP, EOP) andproxy-sensitive attributes": "0.10.20.30.40.50.60.7 Uncertainty threshold Disparity between demographic groups (%) New AdultAdultCompasCeleba : Demographic group representation in each dataset for different uncertainty thresholds. The gapbetween groups increases as the threshold becomes smaller. The plot reveals there are samples from theminority group that exhibit lower uncertainty in the prediction of their sensitive attributes."
}