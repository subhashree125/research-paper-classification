{
  "Abstract": "Contrastive learning demonstrates great promise for representation learning. Data augmen-tations play a critical role in contrastive learning by providing informative views of thedata without necessitating explicit labels. Nonetheless, the efficacy of current methodolo-gies heavily hinges on the quality of employed data augmentation (DA) functions, oftenchosen manually from a limited set of options. While exploiting diverse data augmentationsis appealing, the complexities inherent in both DAs and representation learning can leadto performance deterioration. Addressing this challenge and facilitating the systematic in-corporation of diverse data augmentations, this paper proposes Contrastive Learning withConsistent Representations (CoCor). At the heart of CoCor is a novel consistency metrictermed DA consistency. This metric governs the mapping of augmented input data to therepresentation space. Moreover, we propose to learn the optimal mapping locations as afunction of DA. Experimental results demonstrate that CoCor notably enhances the gener-alizability and transferability of learned representations in comparison to baseline methods.The implementation of CoCor can be found at",
  "Introduction": "Data augmentation (DA) is widely used in supervised learning in computer vision Ho et al. (2019); Lim et al.(2019); Cubuk et al. (2019; 2020); Li & Li (2023), achieving excellent results on popular datasets Cireganet al. (2012); Sato et al. (2015); Wan et al. (2013); Krizhevsky et al. (2017). DA is also a key component inrecent contrastive learning techniques Chen et al. (2020a); Tian et al. (2020b); He et al. (2020); Chen & He(2021); Xiao et al. (2020); Lee & Shin (2023). An encoder that learns good visual representations of the inputdata is trained with a contrastive loss. The contrastive loss is characterized by the following principle: in thefeature space, two views of a given data example, transformed by distinct DA functions, exhibit correlation(similarity), whereas transformed views of different input examples manifest dissimilarity. The effectiveness",
  "MoCo + CoCor": ": (a) Left: An encoder trained with the standard contrastive loss can exhibit inconsistency, asdifferent views of an instance are encouraged to be represented similarly in the feature space, irrespectiveof the actual difference between them. Right: A consistent encoder positions the vector of more stronglyaugmented data further away from that of the raw data.Here the rings represent points with varyingsimilarities to the central representation vector. (b) Nearest-neighbor retrieval in the feature space on CUB-200 Wah et al. (2011) and Flowers102 Nilsback & Zisserman (2008) using pre-trained encoders. Existingcontrastive methods He et al. (2020); Chen & He (2021), which enforce invariance to all data augmentations,may inadvertently cluster dissimilar data closely in the feature space. However, by applying consistency,CoCor ensures that only data sharing similar latent semantics are distributed closely in the latent space.",
  "of the encoder, trained on unlabeled data, is pivotal to the overall performance of contrastive learning andis contingent upon the choice of employed DAs": "To learn effective and transferable representations, numerous studies have focused on enhancing contrastivelearning by selecting suitable DAs. Chen et al. (2020a); Tian et al. (2020b); He et al. (2020); Li et al.(2023); Wang et al. (2023); Van der Sluijs et al. (2024) have shown that combining different augmentationswith appropriate intensities can improve performance. However, these works confine augmentations to arandom composition of specific types within a limited intensity range. Contrastingly, research has indicatedthat employing diverse DAs effectively enhances the models ability to capture the invariance of the trainingdata, thereby boosting the models performance Cubuk et al. (2020), transferability Lee et al. (2021), and ro-bustness Lopes et al. (2019). Consequently, the exploration of diverse DAs has garnered increased attentionrecently. PIRL Misra & Maaten (2020) adopts additional augmentations, SwAV Caron et al. (2020) intro-duces multiple random resized-crops to provide the encoder with a broader range of data reviews. Moreover,some recent works adopt combinations of stronger DAs in contrastive learning Tian et al. (2020b); Wang &Qi (2021); Lee & Shin (2023). While the idea of leveraging diverse data augmentations is appealing, to date, there lacks a systematicapproach for integrating a substantial number of augmentations into contrastive learning.It is crucialto note that the intricacies of data augmentation and representation learning may lead to a performancedegradation if augmentation functions are not judiciously chosen Tian et al. (2020b); Chen et al. (2020a);Wang & Qi (2021). To tackle the aforementioned challenges, this paper introduces Contrastive Learning with Consistent Repre-sentations (CoCor). First, we define a set of composite augmentations, denoted as c, formed by combiningmultiple basic augmentations. Each composite augmentation is represented using a composition vector thatencodes the types and frequencies of the basic augmentations employed. This composite set c facilitatesthe incorporation of diverse DAs, encompassing a wide range of transformation types and overall intensity.",
  "Published in Transactions on Machine Learning Research (09/2024)": "SimSiam Chen & He (2021): A starting learning rate of 5102, a weight decay of 1104, and amomentum of 0.9 are used for the SGD optimizer. Predictor and projector and output dimension areboth 2048. For the consistency loss, representations of the original data are obtained by letting thedata go through the stop-gradient backbone and the projector. The augmented views go throughbackbone, projector, and predictor for the representations of them. All the three models receivegradient while calculating the representations of augmented views. SupCon Khosla et al. (2020): We use a starting learning rate of 3102, a weight decay of 1104,and a momentum of 0.9 for the optimizer. Feature space dimension of the projection head outputis 128. In the consistency loss, the encoder takes both the original data and its augmented view asinput. The encoder is stop-gradient when encoding the original input. The encoder receives gradientwhile calculating representations of the augmented views.",
  "Related Work": "Contrastive Learningdemonstrates significant potential in computer vision tasks and other domainsin recent years Chen et al. (2020a); He et al. (2020); Wang et al. (2023); Caron et al. (2020); Wang et al.(2024). It is a common practice to utilize data augmentations in forming both positive and negative pairsof data for defining the contrastive loss Chen et al. (2020a); Oord et al. (2018). Some methods highlightthe significance of negative pairs in learning distinguishable features from the data Robinson et al. (2020);Awasthi et al. (2022). MoCo introduces a moving-averaged encoder paired with a large negative memoryqueue He et al. (2020); Misra & Maaten (2020). In contrast, Chen & He (2021); Grill et al. (2020) exclusivelylearn features from positive pairs, achieving state-of-the-art performance without incorporating negative",
  "Preliminaries": "The goal of contrastive representation learning is to learn an encoder parameterized by e that maps aninput data x Rn to an 2 normalized feature vector z of dimension m in the feature space Z, i.e.,fe() : Rn Sm1. The encoder is trained by minimizing a contrastive loss. Specifically, it defines twodifferent augmented versions of the same input example as a positive pair, which are expected to havesimilar representations in the feature space. Meanwhile, the encoder shall be trained to discriminate any twoinstances augmented from different input examples, i.e., a negative pair, in the feature space. Minimizingthe contrastive loss pulls together positive pairs and pushes apart negative pairs Oord et al. (2018); Chenet al. (2020a); Li et al. (2020).",
  "The proposed Contrastive Learning with Consistent Representations (CoCor) improves contrastive learningperformance by exploring diverse data augmentations composed from a set of basic augmentations": "Definition 1 (Composite Data Augmentations). A composite augmentation with a length of l,namely, A<l>i, is defined as the composition of l randomly sampled augmentations from a given set ofNa basic augmentation functions a = {a1(), a2(), , aNa()}: A<l>i= ai(1) ai(2) ai(l), wherei(k) [1, Na], k = 1, 2 l. We denote the set of all composite augmentations with a length of l by <l>c, and denote the set of allcomposite augmentations with different lengths by c = <1>c <2>c .Our studies show that theordering of the basic augmentations in a composite augmentation does not have a significant impact onperformance. To simplify the discussion, we assume that composite augmentations are order invariant, e.g.,ai aj = aj ai. We represent a composite augmentation with an unique composition vector as shown in(b).",
  "consistency": ": Overview of the Proposed Method. (a) The proposed property, DA consistency, ensures thatdata augmented with stronger augmentation is positioned farther from the original data compared to aweakly augmented view in the representation space. (b) The Monotonic Mapping Neural Network (MMNN)predicts the optimal latent similarity between an augmented view and the original data, using augmentationcomposition vectors as input. Stronger augmentation results in a smaller predicted latent similarity by theMMNN. Definition 2 (Composition Vector). The composition vector of a length-l composite augmentation A<l>iis defined as a Na-dimensional vector vi = v(A<l>i) NNa0(N0 is the set of all natural numbers), where thejth entry vi[j] is the number of times that the basic augmentation aj is applied in A<l>i, and Naj=1 vi[j] = l.",
  "Data Augmentation Consistency": "Data augmentation plays a crucial role in contrastive learning, requiring careful design to expose the latentstructure of the original data. However, our key observation is that the standard contrastive learning frame-work Chen et al. (2020a); He et al. (2020); Chen & He (2021) does not inherently imply data augmentation(DA) consistency, a fundamental property proposed by this work. In the absence of this consistency, theencoder might simply bring views in positive pairs together in the feature space Z Sm1, irrespective ofthe actual difference between them. However, a weakly augmented view and a strongly augmented view of aninput example may not necessarily share the same latent structure, and it could be advantageous to encodethem into different regions in the feature space, as illustrated in (a). In such cases, an encoderattempting to pull positively paired views together would learn an incorrect representation distribution. To this end, we introduce the concept of DA consistency. With the DA consistency imposed, the encoder istrained not only to cluster positive pairs together but also to map them to their respective optimal locationsin the latent feature space Z based on the strength and types of applied augmentations. Consequently,the encoder preserves critical information introduced by data augmentations, such as variances related tobrightness, sharpness, and rotation, as shown in (a). This characteristic of the pre-trained encoderenhances its performance on various downstream tasks, where such information is crucial for recognition.",
  "Learnable Model for Optimal Latent Similarities": "To train the encoder on the proposed consistency loss in equation 2, we need to provide the optimal latentsimilarity ld(v(A)) for a given composite augmentation A, which is contingent on the strength of A. Nev-ertheless, defining augmentation strength poses a challenge, as two composite augmentations may comprisedifferent basic augmentation types, making it unclear how to define and compare their strength. To tackle this problem, we learn a mapping, i.e., a parameterized neural network gd() that takes a compo-sition vector as input and map it to the optimal latent similarity ld(v(A)). This approach relaxes the needfor directly modeling the strength of different augmentations.",
  "Imposing Monotonicity to the Learnable Model": "Taking it a step further, we not only approximate the optimal latent similarity ld with a neural network butalso enforce an important monotonic property to better learn the desired optimal latent similarities. As the overall strength of a composite augmentation increases, e.g., by incorporating additional basic augmen-tations, the similarity between the augmented data and the raw data is expected to decrease. For instance,although directly comparing the effects of two different augmentations GaussianBlur and Sharpness (whichblurs the image by a Gaussian kernel and adjusts edge contrast, respectively.) may be challenging, applyingSharpness twice in a composite DA would certainly distort the raw input more than applying it once.",
  ":A composite augmentation is consideredstronger than another if and only if it includes all com-ponents of the latter, along with additional basic aug-mentations": "[DA Strength Comparison Operator]Thus, asillustrated in , formally, we define an op-erator > for comparing the strength of compo-sition data augmentations in set c:Ai>Aj iffv(Ai)>v(Aj), where > operates element wise onthe composition vectors: v(Ai)>v(Aj) implies thatv(Ai)[k] v(Aj)[k], k [1, Na], and k [1, Na]such that v(Ai)[k] > v(Aj)[k]. In other words, composite augmentation Ai is con-sidered to be stronger than Aj (i.e., Ai>Aj) if andonly if the number of times any basic augmentationis applied in Ai is at least that of the same basicaugmentation is applied in Aj, and there is at leastone basic augmentation used in Ai more times thanin Aj. Our learnable neural network model gd() takes thecomposition vector of an augmentation Ai as the input, and map it to the optimal latent similarity ofld(v(Ai)) in the feature space: ld(v(Ai)) = gd(v(Ai)). We enforce monotonicity w.r.t. input v(Ai) on gd():gd(v(Ai)) > gd(v(Aj)) if v(Ai)>v(Aj), Ai, Aj c. In other words, stronger data augmentationsresult in a smaller latent similarity. The monotonicity is enforced by incorporating monotonic linearembedding layers You et al. (2017) in gd(), as shown in (b). We refer to gd() as a monotonicmapping neural network (MMNN).",
  "s.t.e(d) = arg mine Lu(e|d)(5)": "The top-level problem in equation 4 optimizes the MMNN and a linear classifier (not explicitly shown inequation 4 based on a cross entropy loss CE on a small amount of labeled data. During each iteration, thebottom-level problem in equation 5 is solved to update the encoder parameter e on the unlabeled data. Wedefer the derivation of es dependency on d and update rule for d to Appendices.",
  "We conclude the algorithm flow of the proposed approach in Algorithm 1": "The design of CoCor makes it compatible with various state-of-the-art self-supervised learning frameworks.CoCor can be integrated into an existing method by replacing Lcontrast in equation 3 with the methodsspecific loss function. The added consistency loss imposes DA consistency and improves the generalizabilityand transferability of the pre-trained encoder within these frameworks.",
  "Experimental Studies": "We demonstrate the performance and generality of CoCor by integrating it into several state-of-the-artcontrastive learning methods: MoCo Chen et al. (2020b); He et al. (2020), a dual-encoder approach with alarge negative memory queue, SimSiam Chen & He (2021), which exclusively leverages positive pairs, andSupCon Khosla et al. (2020), a supervised contrastive learning method that uses labeled data to enhancerepresentation learning. We refer to each of the three methods as a baseline. We compare the performanceof encoders pre-trained using these baselines against those pre-trained using a combination of CoCor withthe baselines across a variety of datasets for linear evaluation and object detection. CoCor is also comparedwith recent works which take augmentation-aware information into consideration Zhang & Ma (2022); Leeet al. (2021); Devillers & Lefort (2023) and works which use stronger data augmentations than conventionalcontrastive learning Lee & Shin (2023); Wang & Qi (2021). To shed more light on the working mechanism ofCoCor, we perform post-hoc analysis on latent similarity and potential dimensional collapse, and a numberof ablation studies.",
  "Experimental settings for encoder pre-training": "The encoders of the baseline methods are pre-trained on the large ImageNet-1K Russakovsky et al. (2015)dataset and its subset ImageNet-100 Tian et al. (2020a), under two different backbone encoder architecturesResNet-50 and ResNet-34 He et al. (2016). The encoders of MoCo and SimSiam based methods are pre-trained for 200 epochs, while SupCons encoder undergoes 100 epochs of pre-training.Batch size of allpre-training experiments is set to 256. We follow Khosla et al. (2020); Chen et al. (2020b); Chen & He(2021) for other settings of these baseline methods. For the sake of fair comparison, pre-training with CoCor incorporated follows the same experiment settingsas their corresponding baselines. The MMNN in CoCor is a 3-layer MLP with monotonic linear embeddinglayers You et al. (2017) and ReLU units. While most of the results are demonstrated when using only 1% ofthe pre-training dataset labels for tuning the MMNN, we also demonstrate the good performance of CoCorin an ablation study where the labeled data usage is significantly further reduced in .4. CoCor utilizesa basic data augmentation set a, consisting of 14 commonly used augmentation functions as detailed inAppendices. We impose the DA consistency constraint to several composite DA sets <l>cof specific lengths.For most pre-training experiments, we take l = 1, 2, 3, i.e., for each example, we sample three compositeDAs of length 1, 2, and 3, and apply them to get three views of data to calculate the consistency loss. Moredetails of data augmentations and pre-training setup are provided in Appendices.",
  "SupCon Khosla et al. (2020)10074.1893.1776.5552.3093.2861.0371.5491.3485.35SupCon + CoCor (Ours)10076.2494.6177.9155.7394.6663.8074.0992.1787.21": "200 Wah et al. (2011), Caltech-101 Fei-Fei et al. (2004), SUN397 Xiao et al. (2010), Food101 Bossard et al.(2014), Flowers102 Nilsback & Zisserman (2008), Oxford-IIIT Pet (Pets) Parkhi et al. (2012), Aircraft Majiet al. (2013), and StanfordCars Krause et al. (2013). The classification accuracies for ImageNet-100 and ImageNet-1K pre-trained encoders are presented in Ta-ble 1 and .Notably, CoCor demonstrates a substantial enhancement in the performance of thepre-trained encoder across diverse classification tasks. This improvement suggests that CoCor contributes toan enhanced generalizability of the encoder, achieved by incorporating a wider range of augmentations andby complementing the semantics within the feature space. Object detectionWe fine-tune the pre-trained encoders on VOC2007+2012 Everingham et al. (2009)and COCO2017 Lin et al. (2014) datasets for object detection downstream tasks. The pre-trained encodersare converted to generalized R-CNN Girshick et al. (2014) detectors with a ResNet50-C4 backbone by usingDetectron2 Wu et al. (2019). The models trained on VOC2007 and COCO2007 are subsequently evaluatedon the corresponding datasets test set. In comparison with the baselines, shows that CoCor substantially improves accuracy under standardmetrics such as AP, AP50, and AP75, suggesting that CoCor is effective in achieving the pre-trained encoderstransferability. Notably, CoCor shows substantial improvements in detecting challenging small and mediumobject detection, as evidenced by the increased APs and APm scores. Post-hoc studyWe visualize the effect of the proposed consistency loss of equation 2 during pre-trainingby comparing the latent similarity of encoders pre-trained with and without the consistency loss in (a),showing the averaged latent similarity of each specific length of composite augmentations on the ImageNet-100 dataset. Clearly, the use of CoCor results in larger latent similarity values across a wide range of DAlengths, demonstrating a tighter clustering of image views with the same identify. This aligns with theanalysis presented in Wang & Isola (2020), which, even without taking into account the dependency onaugmentation strength, suggests that a stronger clustering of views generated by weaker augmentations iscorrelated with improved encoder performance. Additionally, the image retrieval task results in (b)suggest that CoCor learns a more semantically consistent feature space compared to the baseline methods.",
  "This introduced consistency ensures that data sharing similar latent semantics is positioned close, whiledissimilar instances are kept distant in the feature space": "Dimensional collapse evaluationWe illustrate the singular values of the latent representations of pre-trained encoders on ImageNet-100 by conducting principal component analysis (PCA) on the feature vectors.The occurrence of dimensional collapse is typically associated with observed small singular values. However,as depicted in (b), this is less likely to be the case for the encoders pre-trained by CoCor comparedwith the two baselines. While stronger data augmentations Jing et al. (2021) and larger datasets Li et al.(2022a) have been suggested to contribute to dimensional collapse, this issue is not observed in the CoCorencoders. In conventional contrastive learning methods, the encoder is forced to be invariant to featurevariance introduced by augmentations, potentially leading to constant values in some feature space dimen-sions. The dimensional collapse problem is mitigated in CoCor, possibly due to the imposed consistency thatcaptures this variance.",
  "Comparison with the state-of-the-art": "Comparison with augmentation-aware methodsRecently, several works Devillers & Lefort (2023);Lee et al. (2021); Zhang & Ma (2022) have taken into account augmentation-aware information in contrastivelearning. AugSelf Lee et al. (2021) achieves this by training an auxiliary network to predict the differencebetween augmentations applied to generate positive pairs. Zhang & Ma (2022) encodes an augmented imagealong with the DA parameters using two networks, and combines the two embeddings to form a featurevector for contrastive loss. EquiMod Devillers & Lefort (2023) trains a network to predict the representationof an augmented view from the original data and the applied DA. A performance comparison betweenthese methods and CoCor on linear evaluation is presented in . All the encoders have a ResNet-34architecture and are pre-trained on ImageNet-100 for 200 epochs. Compared to Lee et al. (2021); Zhang &Ma (2022); Devillers & Lefort (2023), CoCor introduces much stronger DA and more diverse DA types. CoCorconsistently outperforms them, which may be attributed to its effective utilization of a significantly broaderset of augmentations while maintaining the well-specified monotonic property in augmentation strength. Comparison with methods using stronger data augmentationsSeveral recent contrastive learningapproaches incorporate the use of stronger data augmentations.CLSA Wang & Qi (2021) employs thedistribution divergence between weakly augmented views as supervision for the divergence between a weaklyand a strongly augmented view. RenyiCL Lee & Shin (2023) proposes a new contrastive objective using Renyidivergence to address strongly augmented data. shows the comparison between these methods andours. Here all encoders are ResNet-50 trained for 200 epochs on ImageNet-1K. Unlike RenyiCL and CLSA,which do not explicitly distinguish between different types of data augmentations (DAs), CoCor addressesthis by using the MMNN to differentiate various DA types. Furthermore, CoCor introduces DAs that are notonly stronger but also more diverse, implementing consistency across different lengths of DAs, in contrast toRenyiCL and CLSA, which apply a single length of DA per pre-training process.",
  "Ablation Studies": "Basic Data Augmentationsdefine the augmentation-aware information to be learned by the encoder.To better understand the role of these data augmentations during pre-training, we separate them intotwo groups: color and affine.color includes augmentations that change the color of each pixel whilemaintaining its position, such as Brightness and Contrast. affine comprises affine transformations, suchas Rotate and Shear. Composition of color and affine is provided in the Appendices. Using CoCor, we pre-trained three encoders for 500 epochs on ImageNet-100 with color, affine, and color affine, respectively. presents the linear evaluation results on ImageNet-100 and two fine-grained datasets, CUB-200 andFlowers102. The results indicate that CoCor achieves better overall performance than LooC Xiao et al. (2020)and AugSelf Lee et al. (2021), which have improved transferability to various downstream recognition tasks.Notably, while color affine delivers the best overall results across the tested datasets, refining the basicdata augmentations can enhance CoCors performance on specific downstream tasks. For example, color-aware information is crucial for discriminating flower images in the Flower102 dataset Nilsback & Zisserman(2008), whereas position-related information introduced by affine transformations is less relevant. Thus,applying only color-related augmentations while maintaining invariance to affine transformations improvesperformance on Flower102.",
  "MoCo + CoCor67.0469.5171.66SimSiam + CoCor73.9282.1883.70": "Effectiveness of the MMNNCoCor utilizes an MMNN model to learn the optimal latent similaritiesld of various composite augmentations. To see the benefits of this learnable MMNN model, we replace itby a manually optimized model: we manually search for the best ld value for each composite augmentationlength, determined by extensive trial-and-error, to achieve optimal linear evaluation performance. Moredetailed experimental settings and results on ld selection are included in Appendices. We pre-train ResNet-50 encoders built upon SimSiam and MoCo with the MMNN and the manual modelbased on composite augmentation lengths l = 1, 2, 3, on ImageNet-100 for 200 epochs. The resulting perfor-mances are reported in , which also includes the performances of the MoCo and SimSiam baselines.There is the clear advantages of the encoders trained with the MMNN over the ones without, i.e. with themanual model. Effect of the amount of labeled data in MMNN trainingWe further study the impact of the amountof supervision used to train the MMNN on the encoder performance. We pre-train ResNet-50 CoCor encodersbuilt upon SimSiam on ImageNet-100 for 100 epochs while training the MMNN using 100%, 10%, 1%, and0.1% of the labeled data in the dataset. The pre-trained backbones are evaluated by linear evaluation onImageNet-100. shows that drastically reducing the amount of labeled MMNN training data from100% to 0.1% only results in a 0.94% performance drop, and all four CoCor models outperform the baselineSimSiam by more than 15%. Composite augmentation length in pre-trainingThe composite augmentations introduced in ourwork produce diverse informative views. We test the effect of the strength of composite augmentations byrunning experiments where only one length of composite augmentation is adopted per pre-training experi-ment. We run CoCor on SimSiam and MoCo with single augmentation length 1, 2, 3, 4, and a combinationof lengths 1, 2, and 3. All encoders are ResNet-50 pre-trained on ImageNet-100 for 50 epochs, and are thenevaluated on ImageNet-100. As seen in , CoCor is able to leverage augmentations of varying lengths, and combining DAs at allthese lengths further improves performance. Different from the results presented inChen et al. (2020a);",
  "Top-1 Acc75.9775.8275.7975.4659.20": "Tian et al. (2020b), which show that strong augmentations can degrade performance, we have conductedadditional experiments with much stronger composite augmentations to demonstrate CoCors ability in lever-aging such augmentations. These results, presented in the Appendices, demonstrate CoCors capability tobenefit from even stronger augmentations compared to recent works that introduce strong augmentations intheir methodsLee & Shin (2023); Wang & Qi (2021).",
  "Discussion": "In this paper, we introduce CoCor, a systematic approach to exploring diverse data augmentations in con-trastive learning. Our contribution includes the introduction of DA consistency to quantify the dependencyof the desired latent similarity on the applied data augmentation. We propose a consistency loss to guide theencoder training towards improved DA consistency. To enforce the DA consistency, we employ a data-drivenmethod to learn the optimal dependency and apply it to the encoder training. The effectiveness of CoCor issubstantiated through extensive experimental results on various tasks and datasets. Further details on theMMNN update rule, additional experimental settings, and extended results are provided in the Appendices. As a potential future direction, we suggest exploring the learning of optimal latent similarity with respect todata examples. Specifically, extending the MMNN to take data instances as input could allow for consideringthe variance of latent similarity caused by data variance. This approach has the potential to learn a moreaccurate mapping from data augmentation to latent similarity.",
  "Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum contrastivelearning. arXiv preprint arXiv:2003.04297, 2020b": "Dan Ciregan, Ueli Meier, and Jrgen Schmidhuber. Multi-column deep neural networks for image clas-sification. In 2012 IEEE conference on computer vision and pattern recognition, pp. 36423649. IEEE,2012. Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V Le. Autoaugment: Learningaugmentation strategies from data. In Proceedings of the IEEE/CVF conference on computer vision andpattern recognition, pp. 113123, 2019. Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. Randaugment: Practical automated dataaugmentation with a reduced search space. In Proceedings of the IEEE/CVF conference on computervision and pattern recognition workshops, pp. 702703, 2020.",
  "Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman. The pascalvisual object classes (voc) challenge. International journal of computer vision, 88:303308, 2009": "Li Fei-Fei, Rob Fergus, and Pietro Perona. Learning generative visual models from few training examples:An incremental bayesian approach tested on 101 object categories. In 2004 conference on computer visionand pattern recognition workshop, pp. 178178. IEEE, 2004. Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik.Rich feature hierarchies for accurateobject detection and semantic segmentation. In Proceedings of the IEEE conference on computer visionand pattern recognition, pp. 580587, 2014. Jean-Bastien Grill, Florian Strub, Florent Altch, Corentin Tallec, Pierre Richemond, Elena Buchatskaya,Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap yourown latent-a new approach to self-supervised learning. Advances in neural information processing systems,33:2127121284, 2020.",
  "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. InProceedings of the IEEE conference on computer vision and pattern recognition, pp. 770778, 2016": "Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervisedvisual representation learning. In Proceedings of the IEEE/CVF conference on computer vision and patternrecognition, pp. 97299738, 2020. Daniel Ho, Eric Liang, Xi Chen, Ion Stoica, and Pieter Abbeel. Population based augmentation: Efficientlearning of augmentation policy schedules. In International Conference on Machine Learning, pp. 27312741. PMLR, 2019.",
  "Lujun Li and Anggeng Li. A2-aug: Adaptive automated data augmentation. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition, pp. 22662273, 2023": "Tianhong Li, Peng Cao, Yuan Yuan, Lijie Fan, Yuzhe Yang, Rogerio S Feris, Piotr Indyk, and Dina Katabi.Targeted supervised contrastive learning for long-tailed recognition. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition, pp. 69186928, 2022b. Wei Li, Jiahao Xie, and Chen Change Loy. Correlational image modeling for self-supervised visual pre-training. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp.1510515115, 2023.",
  "Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew Blaschko, and Andrea Vedaldi. Fine-grained visualclassification of aircraft. arXiv preprint arXiv:1306.5151, 2013": "Ishan Misra and Laurens van der Maaten. Self-supervised learning of pretext-invariant representations. InProceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 67076717, 2020. Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large number of classes.In 2008 Sixth Indian conference on computer vision, graphics & image processing, pp. 722729. IEEE, 2008.",
  "Ikuro Sato, Hiroki Nishimura, and Kensuke Yokoi. Apac: Augmented pattern classification with neuralnetworks. arXiv preprint arXiv:1505.03229, 2015": "Yonglong Tian, Dilip Krishnan, and Phillip Isola.Contrastive multiview coding.In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part XI 16, pp.776794. Springer, 2020a. Yonglong Tian, Chen Sun, Ben Poole, Dilip Krishnan, Cordelia Schmid, and Phillip Isola. What makes forgood views for contrastive learning?Advances in neural information processing systems, 33:68276839,2020b. Rogier Van der Sluijs, Nandita Bhaskhar, Daniel Rubin, Curtis Langlotz, and Akshay S Chaudhari. Exploringimage augmentations for siamese representation learning with chest x-rays. In Medical Imaging with DeepLearning, pp. 444467. PMLR, 2024.",
  "Xiao Wang and Guo-Jun Qi.Contrastive learning with stronger augmentations.arXiv preprintarXiv:2104.07713, 2021": "Zihu Wang, Hanbin Hu, Chen He, and Peng Li. Recognizing wafer map patterns using semi-supervisedcontrastive learning with optimized latent representation learning and data augmentation. In 2023 IEEEInternational Test Conference (ITC), pp. 141150. IEEE, 2023. Zihu Wang, Lingqiao Liu, Scott Ricardo Figueroa Weston, Samuel Tian, and Peng Li. On learning discrim-inative features from synthesized data for self-supervised fine-grained visual recognition. arXiv preprintarXiv:2407.14676, 2024.",
  "Yuning You, Tianlong Chen, Yang Shen, and Zhangyang Wang. Graph contrastive learning automated. InInternational Conference on Machine Learning, pp. 1212112132. PMLR, 2021": "Junbo Zhang and Kaisheng Ma. Rethinking the augmentation module in contrastive learning: Learninghierarchical augmentation invariance with expanded views. In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pp. 1665016659, 2022. Tong Zhang, Congpei Qiu, Wei Ke, Sabine Ssstrunk, and Mathieu Salzmann. Leverage your local and globalrepresentations: A new self-supervised learning strategy. In Proceedings of the IEEE/CVF conference oncomputer vision and pattern recognition, pp. 1658016589, 2022.",
  "Lconsistent(e|d) = E<l>cc[softplus[ExX,a<l>c,i <l>c[ld(x; f, a<l>c,i ) gd(vc(a<l>c,i ))]]](7)": ": Images from a same category of a dataset. Target items may be of different sizes and colors, andmay be located at different places in images. Thus same augmentation can have different effects on differentimages. ImageNet is used for illustration. The motivation is to reduce the high variance among images in visual datasets. The exactly same compositeaugmentation hence can lead to different latent similarity in the feature space. For instance, as illustrated in, target items of images may be of different size and shape, and may be located at different places.Therefore, enforcing all augmented views to share the same optimal similarity may not lead to the bestperformance. To this end, equation 7 calculates the averaged latent similarity in a minibatch of each lengthof augmentation composition. Therefore, equation 7 imposes DA consistency constraint on the averagedlatent similarity in a batch instead of on each image, which help better handle the variance. Additionally, data augmentation that can greatly distort an images may not be able to largely distort theidentity of another one. However, the absolute value loss pushes apart those less distorted views from therepresentations of the original data. Thus, softplus is used to replace the absolute value calculation becausesoftplus produce small gradient for negative numbers.",
  "CCandidate Augmentations in CoCor": "The set of basic augmentations a used to form composite augmentation consists of 14 different types dataaugmentations, i.e., a = {AutoContrast, Brightness, Color, Contrast, Rotate, Equalize, Identity,Posterize, Sharpness, ShearX, ShearY, Solarize, TranslateX, TranslateY}. In .4, we conduct ablation studies on the composition of basic data augmentations, as illustrated in. In these experiments, a is divided into two sets: color and affine. The compositions of the two setsare as follows: color = {AutoContrast, Brightness, Color, Contrast, Equalize, Identity, Posterize,Sharpness, Solarize,} and affine = {Rotate, Identity, ShearX, ShearY, TranslateX, TranslateY}.",
  "DExperimental Setup for Pre-training": "All pre-training experiment are conducted using stochastic gradient descent (SGD) as the optimizer. Acosine decay scheduler is adopted for scheduling the learning rate. All pre-training adopt a training databatch size of 256. Detailed setups used for each method are as follows. MoCo He et al. (2020); Chen et al. (2020b): We use a starting learning rate of 3 102, a weightdecay of 1104, and a momentum of 0.9 for the optimizer. For the dual encoders of MoCo, featurespace dimension is 128, memory queue size is 65536, and the momentum of updating the key encoderis 0.999. For the consistency loss, the query encoder takes both the original data and its augmentedview as input. However, the query encoder is stop-gradient when encoding the original input. Theencoder receives gradient while encoding the augmented views.",
  "EExperimental Setup for Linear evaluation": "We follow the linear evaluation protocol of Chen et al. (2020a); Lee et al. (2021); Kornblith et al. (2019).A one-layer linear classifier is trained upon the frozen pre-trained backbone.Resize, CentorCrop, andNormalization are used as the training transformations. An L-BFGS optimizer is adopted for minimizingthe 2-regularized cross-entropy loss. The optimal regularization parameter is selected on the validation setof each dataset. Finally, models are trained with the optimal regularization parameter, and the obtainedtest accuracies are reported. : Linear performance accuracies of CoCor models trained with only one length of composite aug-mentation. CoCor with each length of composite augmentation is run with multiple different constant ldvalues.",
  "FAblation Study on CoCor without the MMNN": "To evaluate the effect of MMNN, we run CoCor where ld is replaced by fixed constants. In practice,a constant is used for each length of composite augmentation in calculating the consistency loss. We runmultiple experiments to select the constant for each length of composite augmentation.All models areResNet-50 pre-trained on ImageNet-100 for 50 epochs. shows the linear evaluation results of CoCorrunning with one length of composite DA and with different constant ld. For each length of compositeaugmentation, there exists an optimal constant which leads to the best result in linear evaluation. These",
  "GCoCor with much stronger data augmentations": "CoCor is a systematic approach of leveraging the information provided by diverse augmentations. In orderto further test the effect of composite augmentations of different strength, we run CoCor on MoCo andSimSiam with single augmentation length of 1, 2, 3, 5, and 10. We train ResNet-50 on ImageNet-100 for 50epochs. Pre-trained backbones are evaluated on ImageNet-100 for linear classification. shows theaccuracies of evaluation.",
  ": Linear evaluation of encoders trained by applying consistency loss on only one length l of compositeaugmentations": "As it is shown in , introducing relatively weaker (shorter) data augmentation in pre-training leads tobetter performance on downstream tasks. When evaluating the pre-trained models on various downstreamtasks, the test data for testing model performance are natural and realistic images. Thus, providing theencoder with less distorted views in pre-training can help the encoder better recognize natural images duringinference. It is also noteworthy that CoCor can leverage very strong augmentations of a length of 10 to improve theencoders performance over baseline, which is even stronger than the data augmentations used in recentworks that include strong augmentations such as CLSA Wang & Qi (2021) and RenyiCL Lee & Shin (2023).Although it has been shown that very strong augmentations which can distort the identity of data too muchmay bring too much noise to the training process. The noise may result in performance degradation Chenet al. (2020a); Tian et al. (2020b), partial dimensional collapse, or even complete dimensional collapse Liet al. (2022a); Jing et al. (2021). However, CoCor is able to capture essential information from the highlydistorted views and enrich the semantics of the feature space using these strong augmentations.",
  "HCoCors compatibility with more baseline contrastive learning methods": "To demonstrate CoCors compatibility with additional baseline methods, e.g. BYOL Grill et al. (2020) andINTL Weng et al. (2023), we implement CoCor on these additional baseline methods. Linear evaluation resultsof these encoders across various datasets are presented in . These results indicate that CoCor notonly enhances the generalizability of learned representations from contrastive methods like MoCo, SimSiam,and SupCon but also extends these benefits to other existing contrastive learning approaches.",
  "JImpact of the Data Augmentation Ordering in CoCor": "To evaluate the impact of the ordering of DAs in CoCor, we conducted experiments with various fixed DAorderings. We trained encoders using MoCo + CoCor with specific DA orderings. Results are shown in. For instance, in configurations CoCor-fixed 1/2, the order of DAs is predetermined. In a fixedorder scenario, such as in CoCor-fixed 1, rotation is consistently applied before brightness whenever bothaugmentations are selected for an image. Results in indicate that in CoCor, the ordering of DAsdoes not significantly affect the encoders performance in downstream tasks. Thus, in final experiments, weuse random DA ordering to explore a wider range of DA variation."
}