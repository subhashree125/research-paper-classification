{
  "Abstract": "Obtaining accurate probabilistic forecasts is an operational challenge in many applications,such as energy management, climate forecasting, supply chain planning, and resourceallocation. Many of these applications present a natural hierarchical structure over theforecasted quantities; and forecasting systems that adhere to this hierarchical structure aresaid to be coherent. Furthermore, operational planning benefits from the accuracy at alllevels of the aggregation hierarchy. However, building accurate and coherent forecastingsystems is challenging: classic multivariate time series tools and neural network methodsare still being adapted for this purpose.In this paper, we augment an MQForecasterneural network architecture with a modified multivariate Gaussian factor model that achievescoherence by construction. The factor model samples can be differentiated with respect to themodel parameters, allowing optimization on arbitrary differentiable learning objectives thatalign with the forecasting systems goals, including quantile loss and the scaled ContinuousRanked Probability Score (CRPS). We call our method the Coherent Learning ObjectiveReparametrization Neural Network (CLOVER). In comparison to state-of-the-art coherentforecasting methods, CLOVER achieves significant improvements in scaled CRPS forecastaccuracy, with average gains of 15%, as measured on six publicly-available datasets.",
  "Introduction": "Obtaining accurate forecasts is an important step for planning in complex and uncertain environments, withapplications ranging from energy to supply chain management (Chen et al., 2022; Wolff et al., 2024), fromtransportation to climate prediction (Hong et al., 2014; Gneiting & Katzfuss, 2014; Makridakis et al., 2022).Going beyond point forecasts such as means and medians, probabilistic forecasting provides a key tool forpredicting uncertain future events. This involves, for example, forecasting that there is a 90% chance ofrain on a certain day or that there is a 99% chance that people will want to buy fewer than 100 items ina certain store in a given week. Providing more detailed predictions of this form permits finer uncertaintyquantification. This, in turn, allows planners to prepare for different scenarios and allocate resources accordingto their anticipated likelihood and cost structure. This can lead to better resource allocation, improveddecision-making, and less waste. In many forecasting applications, there exist natural hierarchies over the quantities one wants to predict,such as energy consumption at various temporal granularities (from monthly to weekly), different geographiclevels (from building-level to city-level to state-level), or retail demand for specific items (in a hierarchicalproduct taxonomy). Typically, most or all levels of the hierarchy are important: the bottom levels are keyfor operational short-term planning, while higher levels of aggregation provide insight into longer-term orbroader trends. Moreover, probabilistic forecasts are often desired to be coherent (or consistent) to ensureefficient decision-making at all levels (Hong et al., 2014; Jeon et al., 2019). Coherence is achieved when theforecast distribution assigns zero probability to forecasts that do not satisfy the hierarchys constraints (BenTaieb et al., 2017; Panagiotelis et al., 2023; Olivares et al., 2023) (see Definition 2.1). Designing an accuratemodel, capable of leveraging information from all hierarchical levels while enforcing coherence is a well-knownand challenging task (Hyndman et al., 2011). The hierarchical forecasting literature has been dominated by two-stage reconciliation approaches, whereunivariate methods are first fitted and later reconciled towards coherence. For many years, most research hasfocused on mean reconciliation (Hyndman et al., 2011; 2024; Vitullo, 2011; Hyndman et al., 2016; Dangerfield& Morris, 1992; Wickramasuriya et al., 2019; Mishchenko et al., 2019). More recent statistical methodsconsider coherent probabilistic forecasts through variants of the bootstrap reconciliation technique (BenTaieb et al., 2017; Panagiotelis et al., 2023) or the clever use of the properties of the Gaussian forecastdistributions (Wickramasuriya, 2023). A detailed hierarchical forecast review is provided by Athanasopouloset al. (2024). Large-scale applications of hierarchical forecasting require practitioners to simplify the two-stagereconciliation process by favoring end-to-end approaches that simultaneously fit all levels of the hierarchy, whilestill achieving coherence. The end-to-end approach refers to training a model constrained to achieve coherenceby directly optimizing for accuracy. End-to-end methods offer advantages such as reduced complexity,improved computational efficiency, and improved adaptability by streamlining the entire forecasting pipelineinto a single, unified model. More importantly, end-to-end models generally achieve better accuracy comparedto two-stage models that are first trained independently for optimized accuracy and then made coherentthrough various reconciliation approaches (Rangapuram et al., 2021; Olivares et al., 2023).",
  "Published in Transactions on Machine Learning Research (12/2024)": ": Empirical evaluation of mean hierarchical forecasts. Relative squared error (relSE) averaged over 5runs, at each aggregation level, the best result is highlighted (lower values are preferred). We report 95%confidence intervals, the methods without standard deviation have deterministic solutions.* The ARIMA-ERM results for Tourism-L differ from Rangapuram et al. (2021), as we improved the numerical stability of their implementation.",
  "(b) Matrix representation": ": A simple time series hierarchical structure with Na = 3 aggregates over Nb = 4 bottom timeseries. a shows the disaggregated bottom variables with blue background. b (right) showsthe corresponding hierarchical aggregation constraints matrix with horizontal lines to separate levels of thehierarchy. We decompose our evaluation throughout the levels.",
  "Hierarchical Forecast Task": "Notation.We introduce the hierarchical forecast task following Olivares et al. (2023). We denote ahierarchical multivariate time series vector by y[i],t =y[a],t | y[b],t RNa+Nb, where [i] = [a] [b], [a],and [b] denote the set of full, aggregate, and bottom indices of the time series, respectively. There are|[i]| = Na + Nb time series in total, with |[a]| = Na aggregates from the |[b]| = Nb bottom time series, atthe finest level of granularity. We use t as a time index. In our notation, we keep track of the shape oftensors using square brackets in subscripts. Since each aggregated time series is a linear transformation ofthe multivariate bottom series, we write the hierarchical aggregation constraint as",
  "y[b],t.(1)": "The aggregation matrix A[a][b] represents the collection of linear transformations for deriving the aggregatesand sums the bottom series to the aggregate levels. The hierarchical aggregation constraints matrix S[i][b] isobtained by stacking A[a][b] and the Nb Nb identity matrix I[b][b]. For a simple example, consider Nb = 4 bottom-series, so [b] = {1, 2, 3, 4} and yT otal,t = 4i=1 yi,t. shows an example of such hierarchical structure, where the multivariate hierarchical time series is defined as:",
  "P[i]Y[i],t+ | x(h)[b][:t], x(f)[b][t+1:t+Nh], x(s)[b]for = 1, , Nh.(3)": "The hierarchical forecasting task augments the forecast probability in Eqn. 3 with coherence constraintsin Eqn. 2.1 (Ben Taieb et al., 2020; Panagiotelis et al., 2023; Olivares et al., 2022c), by restricting theprobabilistic forecast space to assign zero probability to non-coherent forecasts. Definition 2.1 formalizesthe intuition, stating that the distribution of a given aggregate random variable is exactly the distributiondefined as the aggregates of the bottom-series distributions through the summation matrix S[i][b].",
  "for any set B F[b] and its image S[i][b](B) F[i]": "Hierarchical Forecast Scoring Rule.In this work and most of the hierarchical forecast literature, theperformance of probabilistic forecasts is primarily evaluated by the Continuous Ranked Probability Score(CRPS), e.g. Ben Taieb et al. (2017); Rangapuram et al. (2021); Olivares et al. (2023); Panagiotelis et al.(2023); Das et al. (2023); Wickramasuriya (2023). The CRPS between a target y and distributional forecastY is defined asCRPS (y, Y ) = EY [ |Y y| ] 1",
  "Coherent Probabilistic Model": "Our predicted probabilistic forecasts at all hierarchical levels are jointly represented by a Gaussian factormodel. Our neural network maps the known information (past, static and known future) to the location, scale,and shared factor parameters, and the forecasted factor model parameters are designed to model correlationsbetween the bottom-level series, while conditioning on all known information. Our factor model2 combinedwith the coherent aggregation in Eqn. 10 directly estimates the multivariate probability of bottom-level seriesy[b][t+1:t+Nh] conditioning on historical, known-future, and static covariates x(h)[b][:t], x(f)[b][t+1:t+Nh], x(s)[b] , i.e.,",
  "z[b],,t[k],,t": ": The Coherent Learning Objective Reparameterization Neural Network is a Sequence-to-Sequence withContext network that uses dilated temporal convolutions as the primary encoder and multilayer perceptronbased decoders for the creation of the multi-step forecast. CLOVER coherently aggregates the samples of thefactor model y[i],,t = S[i][b]y[b],,t. We mark in red the standard normal samples that are parameter-free, thereparameterization trick allows to apply backpropagation through the factor model outputs. CLOVER extendsupon the univariate MQCNN, through the cross series multi layer perceptron.",
  "([b][h],t, [b][h],t, F[b][k][h],t) = (x(h)[b][:t], x(f)[b][t+1:t+Nh], x(s)[b] ).(16)": "Let Yi,,t() be the random variable parameterized by . In some problems, multi-step coherent forecasts formultiple items are needed (e.g., in retail business, coherent regional demand forecasts are required for eachproduct). Let u be the index of such an item within an index set {1, , Nu} of interest, and let Yu,i,,t()be the coherent forecast random variable for the target yu,i,t+. As we explain in Appendix I, using thereparameterization trick (Kingma & Welling, 2013), within the class of parameters defined by the neuralnetwork architecture, we optimize for either CRPS",
  "E|| Yu,t,[i][h] Yu,t,[i][h]||2. (18)": "We train the model using stochastic gradient descent (Adam (Kingma & Ba, 2014)) with early stopping (Yaoet al., 2007). Appendix C contains details of the networks optimization and hyperparameter selection. 4Temporal exogenous data only aggregates the target signal, other features (e.g., calendar) are maintained without aggregation.5Approach also inspired by regional forecast at Amazons Supply Chain Optimization Technologies (Savorgnan et al., 2024).",
  "Here, we discuss differences between CLOVER (our method) with two coherent end-to-end probabilisticforecasting baselines in HierE2E (Rangapuram et al., 2021) and DPMN (Olivares et al., 2023)": "The HierE2E method of Rangapuram et al. (2021) is too general. It consists of an augmented DeepVAR neuralnetwork model (Flunkert et al., 2017) that produces marginal forecasts for all hierarchical series. HierE2Eclaims to be more general than hierarchical forecasting, since it is designed to enforce any convex constraint sat-isfied by the forecasts; due to the constraining operation in the method, it has to revise the optimized forecasts.It does not leverage the specifics of the hierarchical constraints, which are more structured than a generalconvex constraint. HierE2E produces samples from independent Gaussian distributions for each time-series inthe hierarchy; since the samples are not guaranteed to be hierarchically coherent, HierE2E couples samples byprojecting them onto the space of coherent probabilistic forecasts. Both the sampling operation (Kingma &Welling, 2013) and the projection are differentiable, allowing the method to be trained end-to-end; HierE2Erestricts to optimize likelihood or CRPS. Both HierE2E and CLOVER can allow different distribution choices,since Gaussians can be replaced by any distribution that can be sampled in a differentiable way, i.e., almost anycontinuous distribution (Ruiz et al., 2016; Figurnov et al., 2018; Jankowiak & Obermeyer, 2018). In Rangapu-ram et al. (2021), the projection operator ensures coherence, and correlations between bottom-levels are learnedonly by optimizing the neural network. In contrast, CLOVER produces forecasts for bottom-level series only,while relying on common factors to encode correlations. This removes the need to forecast at all levels simulta-neously, therefore, reducing computational requirements if we are only interested in a subset of the aggregates. On the other hand, the DPMN baseline (Olivares et al., 2023) is too restrictive, in particular, as a Poissonmixture can be prone to distribution misspecification problems. It is known that when a probability model ismisspecified, optimizing log likelihood is equivalent to minimizing KullbackLeibler (KL) divergence withrespect to the true probabilistic distribution, KL divergence measures change in probability space, whileoptimizing CRPS is equivalent to minimizing the Cramer-von Mises criterion (Gneiting & Raftery, 2007),which quantifies the distance with respect to the probability model in the sample space. CLOVERs learningobjective for the probabilistic model is resilient to distributional misspecification (Bellemare et al., 2017).Moreover, CLOVER can be optimized to accommodate other evaluation metrics of interest. Finally, DPMN estimates the covariance among time series, but does not take advantage of the multivariateinput when encoding historical time series. Similarly to other ARIMA-based baselines, on specific hierarchicalbenchmark datasets such as Traffic, DPMN produces suboptimal bottom-series forecasts. We improve theencoder for historical time series by adding a CrossSeriesMLP after the Temporal convolution encoder, whichbridges the accuracy gap between HierE2E and our MQCNN-based approach.",
  "Setting": "Datasets.We analyze six qualitatively different public datasets: Labour, Traffic, Tourism-S, Tourism-L,Wiki, and Favorita, each requiring significant modeling flexibility due to their varied properties. TheFavorita dataset is the largest dataset evaluated which includes count and real-valued regional sales data,with over 340,000 series. The Tourism-S and Tourism-L datasets, report quarterly and monthly visitornumbers to Australian regions, respectively, and they are grouped by region and travel purpose. The Trafficdataset contains daily highway occupancy rates from the San Francisco Bay Area, featuring highly correlatedseries with strong Granger causalities. The smallest Labour dataset tracks monthly Australian employmentby status, gender, and geography, the series in this dataset are highly cointegrated. Lastly, the Wiki datasetsummarizes daily views of online articles by country, topic, and access type. We provide more dataset detailsin Appendix D.",
  ": Summary of publicly-available data used in our empirical evaluation": "Evaluation metrics.Our main evaluation metric is the mean scaled CRPS from Eqn. 5 defined as thescore described in Eqn. 19, divided by the sum of all target values. Let l(g) be a vector of length Na + Nbconsisting of binary indicators for a hierarchical level g, where for each j [i], l(g)j= 1 if aggregated series jis included in the hierarchical level g, and 0 otherwise. Then sCRPS for the hierarchical level g is defined as",
  "Na+Nbi=1||yi,[t+1:t+Nh]||1 l(g)i.(19)": "Baseline Models.We compare our method with the following coherent probabilistic methods: (1) DPMN-GroupBU (Olivares et al., 2023), (2) HierE2E (Rangapuram et al., 2021), (3) ARIMA-PERMBU-MinT (BenTaieb et al., 2017), (4) ARIMA-Bootstrap-BU (Panagiotelis et al., 2023), and (5) an ARIMA. In addition, inAppendix G, we compare our method with the following coherent mean methods: (1) DPMN-GroupBU, (2)ARIMA-ERM (Ben Taieb & Koo, 2019), (3) ARIMA-MinT (Wickramasuriya et al., 2019), (4) ARIMA-BU, (5) anARIMA, and (6) Seasonal Naive. We use the implementation of statistical methods available in the StatsForecastand HierarchicalForecast libraries (Olivares et al., 2022c; Garza et al., 2022).",
  "Forecasting Results": "As mentioned above, we compare the proposed model to the DPMN (Olivares et al., 2023), the HierE2E (Ranga-puram et al., 2021), and two ARIMA-based reconciliation methods (Wickramasuriya et al., 2019; Panagioteliset al., 2023). Following previous work, we report the sCRPS at all levels of the defined hierarchies; see .The reconciliation results for ARIMA are generated using HierarchicalForecast Olivares et al. (2022c), witheach confidence interval calculated based on 10 independent runs. The results for HierE2E are generatedbased on three independent runs using hyperparameters tuned by Olivares et al. (2022c). All metrics forDPMN are quoted from Olivares et al. (2023) with identical experimental settings on all datasets. As the Overall row in shows that CLOVER improves sCRPS compared to the best alternative in five ofsix datasets, with gains of 27.67% on Favorita, 10.24% on Tourism-S, 4.16% on Tourism-L, 4.5% on Wikiand 54.40% in Traffic. There is a degradation of 8% in the reconciliation that performs the best on Labour,the smallest data set. For five out of six datasets, our model achieves the best or second-best sCRPS accuracyacross all the levels of the hierarchy; it is important to consider that aggregate levels are much smaller insample size for which we prefer the bottom-level measurements as an indicator of the methods accuracy. In Traffic, our model achieves remarkably better results than the DPMN and HierE2E baselines, which weexplain by the ability to model VAR relationships accurately because of the smoothing the time series featuresbefore using them as inputs for other series. For the smallest dataset, Australian Labour, all deep learningmodels, including CLOVER, showed a decline in accuracy compared to statistical baselines. We hypothesizethat this performance drop is due to the limited data available, as 57 monthly series may not be sufficientto effectively train complex models like deep learning architectures. Another explanation for the Labourdegradation is the presence of strong trends and cycles, which can lead to artificially high correlations betweenthe series, resulting in spurious relationships that may confuse the VAR modules. We complement the main results with a mean forecast evaluation section in Appendix G. As we see in the sCRPS accuracy gains are mostly mirrored in the relative squared error metric (relSE) . We qualitatively",
  "Learning Objective": "0.025 0.050 0.075 0.100 0.125 0.150 0.175 Validation sCRPS : Ablation studies on the Bay Area Traffic dataset: a) In highly correlated hierarchies, VAR inputsenabled by CLOVER significantly improve over the univariate MQCNNs accuracy. b) The factor model CRPSlearning objective demonstrates clear advantages over classic negative log-likelihood. Full ablation studiesdescribed in Appendix F. show the CLOVER forecast distributions for a hierarchical structure in the Favorita dataset in Appendix E.We also include in Appendix E a probability-probability plot comparing the similarity of the empirical andforecast distributions. As can be seen in , the forecast distributions are qualitatively well calibrated.",
  "Ablation Studies": "To analyze CLOVERs source of improvements, we performed two ablation studies on the Traffic dataset,where we investigated first the effects of the learning objective and then the effects of VAR inputs enabled bythe CrossSeriesMLP. Here we report a summary and refer to the details in Appendix F. In the first study, we explore the effects of a cross-series MLP that mimics the vector autoregressive model. Wecompare the CLOVER architecture with and without the cross-series multi layer perceptron (CrossSeriesMLP)introduced in Eqn. 14. Such a module enables the network to share information from the series in the hierarchywith minimal modifications to the architecture. and show that CrossSeriesMLP improvedTraffic forecast accuracy by 66% compared to variants without it. We attribute the effectiveness of the VARapproach to the presence of Granger-causal relationships in traffic intersections. The VAR-augmented CLOVERimproves upon well-established univariate architectures, including LSTM (Sak et al., 2014), NBEATS (Oreshkinet al., 2020; Olivares et al., 2022a), NHITS (Challu et al., 2023), TFT (Lim et al., 2021), and FCGAGA (Oreshkinet al., 2021), a spatio-temporal specialized architecture. In the second ablation study on learning objectives, we compared CRPS-based optimization, as described inEqn. 17, with the negative log-likelihood estimation for the Gaussian factor model introduced in .1,and other likelihood-estimated distributions. and show that the CRPS-optimized factormodel improves forecast accuracy by nearly 60% when compared to the log-likelihood optimized model.",
  "Conclusion": "In this work, we present a novel multivariate factor forecasting model, integrated with the MQCNN neuralnetwork architecture, resulting in the Coherent Learning Objective Reparameterization Neural Network(CLOVER). Using CRPS as the learning objective, we achieve significant improvements in forecast accuracyover traditional negative log-likelihood objectives. Our experiments on six benchmark datasetsFavorita grocery demand, Australian quarterly and monthlytourism, Australian labour, Wikipedia article visits, and Bay Area Trafficshowed consistent improvementsin CRPS accuracy, averaging over 15 percent. We observed a 4 percent performance drop on the smallerAustralian labour dataset. Ablation studies further confirmed the importance of the CRPS learning objectiveand the enhancements made to support multivariate time series inputs, which were key to the models success.",
  "Acknowledgements": "This work was supported by the Amazon Supply Chain Optimization (SCOT) Forecasting Team. We thankthe TMLR reviewers and the Action Editor for their helpful feedbacks and suggestions. We express ourgratitude to Stefania La Vattiata for her contribution to the illustrations, which effectively captured theessence of our methods. Special thanks to David Luo for his work on hierarchical forecast baselines. Thanksto Boris Oreshkin for his pointers to latest developments in graph neural networks, and recommendations toenhance our ablation studies. We also appreciate Riccardo Savorgnan and the regional team for enrichingdiscussions on alternatives for adapting the MQCNN architecture to multivariate time series. The authorsthank Utkarsh for his suggestion on optimizing the closed-form CRPS for truncated normal variables. Wethank Youxin Zhang for his ideas to improve the computational complexity of the Energy Score and theCRPS estimators. Thanks to Lee Dicker and Medha Agarwal for their input on the energy score part of thepaper and qualitative evaluation of the forecast calibration.",
  "Australian Bureau of Statistics. Labour Force, Australia. Accessed Online, 2019. URL": "Marc G Bellemare, Ivo Danihelka, Will Dabney, Shakir Mohamed, Balaji Lakshminarayanan, Stephan Hoyer,and Rmi Munos. The cramer distance as a solution to biased wasserstein gradients. arXiv preprintarXiv:1705.10743, 2017. Souhaib Ben Taieb and Bonsoo Koo. Regularized regression for hierarchical forecasting without unbiasednessconditions. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &Data Mining, KDD 19, pp. 13371347, New York, NY, USA, 2019. Association for Computing Machinery.ISBN 9781450362016. doi: 10.1145/3292500.3330976. URL",
  "Martin Jankowiak and Fritz Obermeyer. Pathwise derivatives beyond the reparameterization trick. ArXiv,abs/1806.01851, 2018": "Jooyoung Jeon, Anastasios Panagiotelis, and Fotios Petropoulos. Probabilistic forecast reconciliation withapplications to wind power and electric load. European Journal of Operational Research, 279(2):364379,2019. Harshavardhan Kamarthi, Lingkai Kong, Alexander Rodrguez, Chao Zhang, and B Aditya Prakash. Profhit:Probabilistic robust forecasting for hierarchical time-series. arXiv preprint arXiv:2206.07940, 2022. Harshavardhan Kamarthi, Aditya B. Sasanur, Xinjie Tong, Xingyu Zhou, James Peters, Joe Czyzyk, andB. Aditya Prakash. Large scale hierarchical industrial demand time-series forecasting incorporating sparsity.In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD 24,pp. 52305239, New York, NY, USA, 2024. Association for Computing Machinery. ISBN 9798400704901.doi: 10.1145/3637528.3671632. URL",
  "Francesco Laio and Stefania Tamea. Verification tools for probabilistic forecasts of continuous hydrologicalvariables. Hydrology and Earth System Sciences, 11(4):12671277, 2007": "Bryan Lim, Sercan . Ark, Nicolas Loeff, and Tomas Pfister. Temporal fusion transformers for interpretablemulti-horizon time series forecasting. International Journal of Forecasting, 37(4):17481764, 2021. ISSN0169-2070. doi: URL Spyros Makridakis, Evangelos Spiliotis, and Vassilios Assimakopoulos. M5 accuracy competition: Results,findings, and conclusions. International Journal of Forecasting, 38(4):13461364, 2022. URL Special Issue: M5 competition.",
  "Konstantin Mishchenko, Mallory Montgomery, and Federico Vaggi. A self-supervised approach to hierarchicalforecasting with applications to groupwise synthetic controls. ArXiv, abs/1906.10586, 2019": "Kin G. Olivares, Cristian Challu, Grzegorz Marcjasz, Rafa Weron, and Artur Dubrawski. Neural basisexpansion analysis with exogenous variables: Forecasting electricity prices with NBEATSx. InternationalJournal of Forecasting, 2022a. ISSN 0169-2070. doi: URL Kin G. Olivares, Cristian Chall, Federico Garza, Max Mergenthaler Canseco, and Artur Dubrawski.NeuralForecast: User friendly state-of-the-art neural forecasting models. PyCon Salt Lake City, Utah, US2022, 2022b. URL",
  "Francisco J. R. Ruiz, Michalis K. Titsias, and David M. Blei. The generalized reparameterization gradient.In NIPS, 2016": "Hasim Sak, Andrew W. Senior, and Franoise Beaufays. Long short-term memory based recurrent neural net-work architectures for large vocabulary speech recognition. Computing Research Repository, abs/1402.1128,2014. URL Riccardo Savorgnan, Chao Guo, Quenneville-Belair Vincent, Rahul Gopalsamy, and Kenny Shirley. Crossregion attention for regional forecasting. Demand Forecasting Team, Supply Chain Optimization Tech-nologies (SCOT). Amazon Machine Learning Conference, 2024. URL Thordis L. Thorarinsdottir and Tilmann Gneiting. Probabilistic Forecasts of Wind Speed: Ensemble ModelOutput Statistics by using Heteroscedastic Censored Regression. Journal of the Royal Statistical SocietySeries A: Statistics in Society, 173(2):371388, 11 2009. ISSN 0964-1998. doi: 10.1111/j.1467-985X.2009.00616.x. URL",
  "Steven R. Vitullo. Disaggregating time series data for energy consumption by aggregate and individualcustomer. Department of Electrical and Computer Engineering, Ph. D. Dissertation., 2011": "Yuyang Wang, Alex Smola, Danielle Maddix, Jan Gasthaus, Dean Foster, and Tim Januschowski. Deepfactors for forecasting. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36thInternational Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research,pp. 66076617. PMLR, 0915 Jun 2019. URL Ruofeng Wen, Kari Torkkola, Balakrishnan Narayanaswamy, and Dhruv Madeka. A Multi-horizon QuantileRecurrent Forecaster. In 31st Conference on Neural Information Processing Systems NIPS 2017, TimeSeries Workshop, 2017. URL",
  "Shanika L. Wickramasuriya. Probabilistic forecast reconciliation under the Gaussian framework. Accepted atJournal of Business and Economic Statistics, 2023": "Shanika L. Wickramasuriya, George Athanasopoulos, and Rob J. Hyndman. Optimal forecast reconciliationfor hierarchical and grouped time series through trace minimization. Journal of the American StatisticalAssociation, 114(526):804819, 2019. Malcolm Wolff, Kin G. Olivares, Boris Oreshkin, Sunny Ruan, Sitan Yang, Abhinav Katoch, ShankarRamasubramanian, Youxin Zhang, Michael W. Mahoney, Dmitry Efimov, and Vincent Quenneville-Blair. SPADE : Split peak attention decomposition. In Thirty-Eighth Annual Conference on NeuralInformation Processing Systems NeurIPS 2024, volume Time Series in the Age of Large Models Workshop,Vancouver, Canada, 2024. NeurIPS 2024. URL",
  "= P[a]A[a][b](B) | BP[b] (B) = (1 P[a]y[a] / A[a][b](B) | B) P[b] (B) = P[b] (B)": "The first equality is the image of a set B [b] corresponding to the constraints matrix transformation, thesecond equality defines the spanned space as a subspace intersection of the aggregate series and the bottomseries, the third equality uses the conditional probability multiplication rule, and the final equality uses thezero probability assumption.",
  "Here, we complement and extend the description of our method in": "To avoid information leakage, we perform ablation studies in the validation set preceding the test set, wherewe explored variants of the probabilistic method, as well as its optimization. We report these ablation studiesin the Appendix F. For each dataset, given the prediction horizon h, the test set is composed of the last htime steps. The validation set is composed of the h time-steps preceding the test set time range. The trainingset is composed of all dates previous to the validation time-range. When reporting the final accuracy resultsof our model in the test set, we used the settings that perform the best in the validation set. We minimally tune the architecture and its parameters varying only its size and the convolution kernelfilters to match the seasonalities present in each dataset. For the data set Favorita, we use dilations of to match the weekly and monthly seasonalities. For the Tourism-L and Labour datasets weuse dilations of to match the monthly and yearly seasonalities. For the Traffic dataset we usedilations of as multiples of 7 to match the weekly seasonalities. For the Tourism-S dataset weuse dilations of as multiples of 4 to match the quarterly seasonalities. For the Wiki dataset we useminimal dilations . The selection of the number of factors mostly follows the memory constraints of the GPU, as the effectivebatch size implied by our probabilistic model grows rapidly as a function of the multivariate series. Inthe Favorita dataset, more factors are likely to continue to improve accuracy but with the tradeoff of thecomputational speed. Similarly, the Cross-series MLP hidden size is selected following the GPU memoryconstraints. We share a learning rate of 5e-4 constant across the three datasets, which shows that the method is reasonablyrobust across different forecasting tasks. During the optimization of the networks we use adaptive momentstochastic gradient descent (Kingma & Ba, 2014) with early stopping (Yao et al., 2007) guided by the sCRPSsignal measured in the validation set. We use a learning rate scheduler that decimates the learning rate fourtimes during optimization (SGD Maxsteps / 4), to ensure the convergence of the optimization.",
  "The CLOVER model is implemented using PyTorch (Paszke et al., 2019), with the NeuralForecast libraryframework (Olivares et al., 2022b). We run all experiments using a single NVIDIA V100 GPU": "As mentioned earlier in .1, statistical methods available in StatsForecast and HierarchicalForecastlibraries (Olivares et al., 2022c; Garza et al., 2022). In particular, we use available code on the hierarchicalbaselines repository and hierarchical datasets repository. As described in , we deviate slightly fromthe experimental setting in Rangapuram et al. (2021) to ensure the reproducibility of the results of thispaper. For we rerun the HierE2E (Rangapuram et al., 2021) baseline on Labour, Traffic, Wiki andTourism-S using their best reported hyperparameters.",
  "(f) Favorita": ": Estimated correlation matrices produced by CLOVER. The color scale indicates the strength ofcorrelation between pairs of series in the hierarchy. Green represents negative correlations, red indicatespositive correlations, and black highlights the most highly correlated series. The estimated covariances aresparse, with many correlations close to zero. Aggregated levels, seen in the top-left corner, show strongerpositive correlations. The plot uses a single run from the models in .",
  "DDataset Details": "Labour:The Labour dataset (Australian Bureau of Statistics, 2019) tracks monthly Australian employmentfrom February 1978 to December 2019, reporting total employees by part-time/full-time status, gender, andgeography. It includes N = 57 series in total, with Na = 25 aggregate series and Nb = 32 bottom-level series.We use 8 months from May 2019 to December 2019 as test, and the rest of the data as training and validation. Traffic:The Traffic dataset (Ben Taieb & Koo, 2019) contains daily (aggregated from hourly rates) freewayoccupancy rates for 200 car lanes in the San Francisco Bay Area, aggregated from January 2008 to March2009. The data is grouped into three levels: four groups of 50 lanes, two groups of 100 lanes, and oneoverall group of 200 lanes, following the random grouping in Rangapuram et al. (2021); Olivares et al. (2023).Consistent with prior work (Ben Taieb & Koo, 2019; Rangapuram et al., 2021; Olivares et al., 2023), we splitthe dataset into 120 training, 120 validation, and 126 test samples, reporting accuracy for the last test date.We use geographic node dummies, weekend indicators, and proximity to Saturday for exogenous variables.",
  "Tourist Visits": "ValTest TourismLarge 2016-11-262016-12-102016-12-24 Period [November 19, 2016 to December 31, 2016] Frequency [D] ValTest Wiki2 : Tourism-L dataset partition into train, validation, and test sets used in our experiments. Alldatasets use the last horizon window as defined in (marked by the second dotted line), and theprevious window preceding the test set as validation (between the first and second dotted lines). Validationprovides the signal for hyperparameter selection and the ablation studies. Tourism-S:The Tourism-S dataset (Tourism Australia, Canberra, 2005) records quarterly visits to Australiafrom 1996 to 2006. We use data from 2005 for validation, 2006 for testing, and the remaining quarters fortraining. Each series contains 28 quarterly observations and is structured with aggregated visit data bycountry, purpose of travel, state, regions, and urbanization level within regions. The most disaggregated levelincludes 56 regions by urbanization, while the aggregated data consists of 33 series. We use state dummies,and for the future exogenous variables, we use quarterly dummies and seasonal naive 4 and 8 anchors. Tourism-L:The Tourism-L dataset (Wickramasuriya et al., 2019) represents visits to Australia, at amonthly frequency, between January 1998 and December 2016. We use 2015 for validation, and 2016 fortesting, and all previous years for training. The dataset contains 228 monthly observations. For each month,we have the number of visits to each of Australias 78 regions, which are aggregated to the zone, state, andnational level, and for each of four purposes of travel. These two dimensions of aggregation total N = 304leaf entities (a region-purpose pair), with a total of M = 555 series in the hierarchy. We pre-process the data to include static features. We use purpose of travel as well as state dummies. Forthe historical information, we use month dummies, and for the future exogenous variables, we use monthdummies and a seasonal naive anchor forecast that helps greatly to account for the series seasonality. Favorita:The Favorita dataset (Favorita et al., 2017) contains grocery sales of the Ecuadorian CorporacinFavorita in N = 54 stores. We perform geographical aggregation of the sales at the store, city, state, andnational levels, following (Olivares et al., 2023). This yields a total of M = 94 aggregates. Concerning features,we use past unit sales and number of transactions as historical data. In the Favorita dataset, we include itemperishability static information, geographic state dummy variables, and for the historic exogenous featuresand future exogenous features, we use promotions and day of the week. During the models optimization weconsider a balanced dataset of items and stores, for 217,944 bottom level series (4,036 items * 54 stores),along with aggregate levels for a total of 371,312 time series. The dataset is at the daily level and startsfrom 2013-01-01 and ends on 2017-08-15, comprehending 1688 days. We keep 34 days (1654 to 1988 days) ashold-out test and 34 days (1620 to 1654 days) as validation. Wiki:The Wikipedia dataset (Anava et al., 2018) contains daily views of 145,000 online articles from July2015 to December 2016. The dataset is processed into Nb = 150 bottom series and Na = 49 aggregate seriesbased on country, access, agent and article categories, following Ben Taieb & Koo (2019); Rangapuram et al.(2021) processing. The last week of December 2016 is used as test, and the remaining data as training andvalidation. We use day of the week dummies to capture seasonalities, country, access, agent dummy staticfeatures, and a seasonal naive 7 anchor.",
  "Ecuador": "state_[Pastaza] city_[Puyo] Predictions {t + 1, . . . , t + Nh} store_ : CLOVER forecast distributions on the Favorita dataset. We show the forecasted demand for agrocery item on a store of the Puyo City, in the State of Pastaza and the whole country demand in the toprow. Forecast distributions show the 90% forecast intervals in light blue, and the forecasted median in darkblue. The clipped Normal distribution achieves non-negative predictions and a point mass at zero.",
  "Lanes0.09050.00840.11450.01740.16860.01590.14420.02130.14090.01820.14040.00710.15830.0923": ": Ablation study on the Traffic dataset. empirical evaluation of probabilistic coherent forecasts.Mean scaled continuous ranked probability score (sCRPS) averaged over 5 runs, at each aggregation level,the best result is highlighted (lower measurements are preferred). We report 95% confidence intervals.Comparison of MQCNN-based architecture trained with different learning objectives.* The Normal and StudentT are non coherent forecast distributions, in contrast to the Factor Model and the Poisson Mixture.",
  "FAblation Studies Details": "To analyze the sources of improvements in our model, we conducted ablation studies on variants of theCLOVER/MQCNN/DPMN (Wen et al., 2017; Olivares et al., 2022c). We used a simplified setup on the Trafficdataset, focusing on the same forecasting task as in the main experiment. We evaluated sCRPS fromEqn. 19 in the validation set across five randomly initialized neural networks. The experiments hyperparameters, and vary a single characteristic of the network, and measure its validation effects. In our first ablation study, we explore the effects of the impact of including vector autoregressive relationshipsof the hierarchy through the CrossSeriesMLP module described in Eqn. 14. In the experiment, we traina CLOVER with and without the module in the Traffic dataset. Additionally we compare different well-performing neural forecasting architectures6 augmented with the CrossSeriesMLP including (1) LSTM (Saket al., 2014), (2) NBEATS (Oreshkin et al., 2020; Olivares et al., 2022a), (3) NHITS (Challu et al., 2023), (4)TFT (Lim et al., 2021), (5) FCGAGA (Oreshkin et al., 2021) a network specialized in spatio-temporal forecasting.We use the default implementations available in the NeuralForecast library (Olivares et al., 2022b) 7. shows that convolution-based architectures continue to deliver state-of-the-art results., more importantly usingthe CrossSeriesMLP improves sCRPS upon the alternative (without) by 66 percent. The technique bridgesthe gap to the HierE2E (Rangapuram et al., 2021), which previously outperformed all alternative methodsby over 50 percent. We attribute the improvements to the heavy presence of Granger causal relationshipsbetween the traffic lanes, as they carry lag historical information that influences each other. In our second ablation study, we explore learning objective alternatives to Eqn. 17. For this purpose, wereplace the last layer of CLOVER with different distribution outputs, including the normal, Student-t, andPoisson mixture distributions (Olivares et al., 2023). In addition we also compare with our own Factor modelapproach, as we can see in and , the CRPS optimization of the Factor Model improvesupon the negative log likelihood by 60 percent in the mean sCRPS in the validation set. The difference ishighly driven by outlier runs, but it is expected as the CRPS objective has much more convenient numericalproperties, starting with its bounded gradients. 6The Factor Model can augment models from the Neural Forecast library (Olivares et al., 2022b). Because of this, itcan readily augment AutoFormer, BitCN, DeepAR, DeepNPTS, DilatedRNN, DLinear, FedFormer, FCGaga, GRU, Informer,ITransformer, KAN, LSTM, MLP, NBEATS, NBEATSx, NHITS, NLinear, PatchTST, RMOK, RNN, SOFTS, TCN, TFT, Tide,TimeLLM, TimeMixer, and TimesNet. For this ablation study, we focus on NBEATS, NHITS, LSTM, FCGaga, and TFT.7Optimization of the neural forecasting networks follows details from Appendix C, we increased four times the early stoppingpatience on NHITS/NBEATS/FCGAGA to account for the gradient variance compared to MQCNNs gradient based on forking sequences.",
  "GMean Forecast Accuracy Evaluation": "To complement the probabilistic and L1-based results in .1. We also evaluate mean forecasts denotedby y[i][h],t := (y[i],1,t, , y[i],Nh,t) through the relative squared error relSE (Hyndman & Koehler, 2006),that considers the ratio between squared error across forecasts in all levels over squared error of the Naiveforecast (i.e., a point forecast using the last observation y[i],t) as described by",
  "relSEy[i][h],t, y[i][t+1:t+Nh] | l(g)=Na+Nbi=1yi,[t+1:t+Nh] yi,[h],t22 l(g)iNa+Nbi=1yi,[t+1:t+Nh] yi,t 1[h]22 l(g)i.(25)": "As discussed in .1, our factor model naturally defines a probabilistic coherent system, where hierarchi-cal coherence emerges as a consequence. In this section, we compare our method with the following coherentmean approaches: (1) DPMN-GroupBU, (2) ARIMA-ERM (Ben Taieb & Koo, 2019), (3) ARIMA-MinT (Wickrama-suriya et al., 2019), (4) ARIMA-BU, (5) ARIMA, and (6) Seasonal Naive. The statistical methods are implementedusing the StatsForecast and HierarchicalForecast libraries (Olivares et al., 2022c; Garza et al., 2022). Inparticular we use available code on the hierarchical baselines repository, and hierarchical datasets repository. The CLOVER model improves relSE accuracy in five out of six datasets by an average of 32%, while degradingaccuracy on the smallest dataset, Labour, by 32%. These changes can be attributed to squared error metricssensitivity to outliers, though the results are generally consistent with sCRPS outcomes.",
  "Where yi, yji, j = 1, ..., N are independent samples of the random variable Y (), and y a possible observation": "To assess the effect of sample size on the CRPS Monte Carlo estimator, we ran diagnostics throughout themodel training process. shows the training and validation loss trajectories for varying Monte Carlosample sizes, using model configurations from . For reference, we include the analytical CRPS oftruncated normals (Thorarinsdottir & Gneiting, 2009), despite minor distributional differences compared toEqn. 10. For a comprehensive review of alternative CRPS estimators, we refer to Zamo & Naveau (2018). We found that reducing the number of Monte Carlo samples had minimal impact on both training andvalidation scores. Given GPU memory constraints, decreasing the sample size effectively reduces computationaloverhead without significantly affecting model performance. We observed similar phenomena with the energyscore learning objective and its relationship to Monte Carlo samples.",
  "(29)": "Note that the expectations gradient is not the expectation of the gradient due to the second term thatcaptures the dependence of the probability of Y () on the parameters of the network. The second termmay face challenges when the probability of Y () does not have an analytical form or its gradient P(Y |)poses challenges for automatic differentiation tools. The solution proposed by Kingma & Welling (2013)reparameterizes the target variable to recover the differentiability of the gradient loss in Eqn. 29.",
  "(30)": "As highlighted in , the key insight from Eqn. 30 is that the reparameterization trick provides ahighly flexible framework. It is compatible with any differentiable learning objective and can accommodatedifferentiable constraints that extend beyond traditional aggregation constraints, such as those in Eqn. 28.Furthermore, the techniques explored in this paper can be generalized to continuous distributions (Figurnovet al., 2018; Ruiz et al., 2016; Jankowiak & Obermeyer, 2018) beyond Gaussian random variables, opening upexciting avenues for future research."
}