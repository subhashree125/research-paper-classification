{
  "Abstract": "Masked transformer models for class-conditional image generation have become a compellingalternative to diffusion models. Typically comprising two stages an initial VQGAN modelfor transitioning between latent space and image space, and a subsequent Transformer modelfor image generation within latent space these frameworks offer promising avenues for im-age synthesis. In this study, we present two primary contributions: Firstly, an empirical andsystematic examination of VQGANs, leading to a modernized VQGAN. Secondly, a novelembedding-free generation network operating directly on bit tokens a binary quantizedrepresentation of tokens with rich semantics. The first contribution furnishes a transparent,reproducible, and high-performing VQGAN model, enhancing accessibility and matchingthe performance of current state-of-the-art methods while revealing previously undiscloseddetails. The second contribution demonstrates that embedding-free image generation usingbit tokens achieves a new state-of-the-art FID of 1.52 on the ImageNet 256 256 bench-mark, with a compact generator model of mere 305M parameters. The code for this projectis available on",
  "Motivation": "Masked transformer models for class-conditioned image (Chang et al., 2022; Yu et al., 2024a) or text-to-image (Chang et al., 2023) generation have emerged as strong alternatives to auto-regressive models (Esseret al., 2021; Yu et al., 2022a; Lee et al., 2022a) and diffusion models (Ho et al., 2020; Rombach et al.,2022). These masked transformer methods typically employ a two-stage framework: a discrete tokenizer(e.g., VQGAN (Esser et al., 2021)) projects the input from image space to a discrete, compact latent space,while a transformer model (Vaswani et al., 2017; Devlin et al., 2018) serves as the generator to create imagesin latent space from a masked token sequence. These methods achieve performance comparable to state-of-the-art auto-regressive and diffusion models, but with the advantage of requiring significantly fewer samplingsteps and smaller model sizes, resulting in substantial speed-ups. Despite the success of masked transformer frameworks, the development details of a strong tokenizer havebeen largely overlooked. Moreover, one key aspect of training modern VQGAN-based tokenizers (Changet al., 2022; Yu et al., 2023; 2024a) the perceptual loss remains undiscussed. Since latent space-basedgeneration relies on encoding and decoding the input image with the VQGAN tokenizer, the final imagequality is influenced by both the generator network and the VQGAN. The importance of a publicly available,high-performance VQGAN model cannot be overstated, yet the most widely-used model and code-base stilloriginate from the original work (Esser et al., 2021) developed over three years ago. Although stronger,closed-source VQGAN variants exist (Chang et al., 2022; Yu et al., 2023; 2024a), their details are notfully shared in the literature, creating a significant performance gap for researchers without access to theseadvanced tokenizers. While the community has made attempts (Besnier & Chen, 2023; Luo et al., 2024)to reproduce these works, none have matched the performance (for both reconstruction and generation)reported in (Chang et al., 2022; Yu et al., 2023; 2024a). In this paper, we undertake a systematic step-by-step study to elucidate the architectural design and trainingprocess necessary to create a modernized VQGAN model, referred to as VQGAN+. We provide a detailedablation of key components in the VQGAN design, and propose several changes to them, including modeland discriminator architecture, perceptual loss, and training recipe. As a result, we significantly enhancethe VQGAN model, reducing the reconstruction FID from 7.94 (Esser et al., 2021) to 1.66, marking animpressive improvement of 6.28. Moreover, our modernized VQGAN+ improves the generation performanceand establishes a strong, reproducible foundation for future generation frameworks based on VQGAN. Furthermore, we delve into embedding-free tokenization (Yu et al., 2024a; Mentzer et al., 2024), specificallyimplementing Lookup-Free Quantization (LFQ) (Yu et al., 2024a) within our improved tokenizer framework.",
  "Published in Transactions on Machine Learning Research (12/2024)": "Image Generation. Although various types of image generation models exist, mainstream state-of-the-artmethods are usually either diffusion-based (Dhariwal & Nichol, 2021; Chen et al., 2023; Rombach et al.,2022; Peebles & Xie, 2023; Li et al., 2023; Hoogeboom et al., 2023), auto-regressive-based (Chen et al., 2020;Esser et al., 2021; Yu et al., 2022a; Gafni et al., 2022; Tschannen et al., 2024; Yu et al., 2024b), or masked-transformer-based (Chang et al., 2022; Lee et al., 2022b; Yu et al., 2024a; Lezama et al., 2022; 2023). Amongthem, the masked-transformer-based methods exhibit competitive performance, while bringing a substantialspeed-up, thanks to a much fewer sampling steps. This research line begins with MaskGIT (Chang et al.,2022), which generates images from a masked sequence in non-autoregressive manner where at each stepmultiple token can be predicted (Devlin et al., 2018). Follow-up work has successfully extend the idea to thevideo generation domain (Yu et al., 2023; Gupta et al., 2023), large-scale text-to-image generation (Changet al., 2023), along with competitive performance to diffusion models (Yu et al., 2024a) for image generation.Still, these works only utilize the tokens from the tokenizer to learn new embeddings in the transformernetwork, while we aim at an embedding-free generator framework, making better use of the learned semanticstructure from the tokenizer.",
  "Our contribution can be summarized as follows:": "1. We study the key ingredients of modern closed-source VQGAN tokenizers, and develop an open-source and high-performing VQGAN model, called VQGAN+, achieving a significant improvementof 6.28 rFID over the original VQGAN developed three years ago. 2. Building on our improved tokenizer framework, we leverage Lookup-Free Quantization (LFQ). Weobserve that embedding-free bit token representation exhibits highly structured semantics, importantfor generation.",
  "Revisiting VQGAN": "Image generation methods (Esser et al., 2021; Rombach et al., 2022) that operate in a latent space ratherthan in the pixel space require networks to project images into this latent space and then back to the pixelspace. Such networks are usually trained with a reconstruction objective, as is common in VAE-inspiredmethods (Kingma & Welling, 2014; Oord et al., 2017; Esser et al., 2021). Over time, the training frameworksfor these networks have become increasingly complex (Esser et al., 2021; Yu et al., 2022a; Chang et al., 2022;Lee et al., 2022a; Yu et al., 2023; 2024a).Despite this complexity, the exploration and optimization ofthese frameworks remain significantly underrepresented in the scientific literature. In particular, issues inreproduction arise when crucial details are not transparently discussed, making a comparison under fairand consistent conditions hard. Moreover, fully disclosing all details enables the community to scrutinizeand validate the true advancements brought by new designs. In response to this challenge, we conduct asystematic study on the essential modifications to the popular baseline Taming-VQGAN (Esser et al., 2021).For this study, we use the term VQGAN to refer to any network of this type and Taming-VQGANspecifically to refer to the network published by (Esser et al., 2021). In the following, we provide an empiricalroadmap for developing a modernized VQGAN, named VQGAN+, aiming to bridge the performance gapand make advanced image generation techniques more accessible.",
  "Preliminaries": "Variational Autoencoders (VAEs) (Kingma & Welling, 2014) compress high-dimensional data into low-dimensional representations using an encoder network and reverse this process with a decoder network.VAEs are typically trained with a reconstruction loss, such as the L2 loss. However, minimizing the L2 dis-tance alone is insufficient for achieving visually satisfying results. Therefore, a perceptual loss term (Johnsonet al., 2016) based on the LPIPS metric (Zhang et al., 2018) is used to reduce noise and improve image qual-ity. Vector-Quantized VAEs (VQ-VAEs) (Oord et al., 2017) incorporate a learnable codebook that acts as alookup table between the encoder and decoder. The codebook is trained with two L2 losses: the commitmentloss, which reduces the distance of the encoder output to the codebook entries, and the codebook loss, whichminimizes the distance from the codebook entries to the encoder output. VQGANs (Esser et al., 2021) buildon top of VQ-VAEs by incorporating a discriminator network to add an adversarial objective to the training",
  "update #2": ": Detailed roadmap to build a modern VQGAN+. This overview summarizes the performancegains achieved by each proposed change to the architecture and training recipe. The reconstruction FID(rFID) is computed against the validation split of ImageNet at a resolution of 256. The popular and open-source Taming-VQGAN (Esser et al., 2021) serves as the baseline and starting point.",
  "The Experimental Setup": "We follow standard practices to train and evaluate the network on ImageNet (Deng et al., 2009). Unlessspecified otherwise, all networks are trained with a batch size of 256 for 300,000 iterations. Evaluation isperformed using the reconstruction Frchet Inception Distance (rFID) (Heusel et al., 2017), where lowervalues indicate better performance. We use Taming-VQGAN (Esser et al., 2021) as our baseline and startingpoint, due to its thorough study and open-source availability, ensuring reproducibility of results. Taming-VQGAN is an encoder-decoder network that uses residual blocks (He et al., 2016) with convolutions forlow-level feature processing and interleaved blocks of convolutions and self-attention (Vaswani et al., 2017)for high-level feature processing. In our experiments, we focus on encoder networks that produce latentembeddings with an output stride of 16, resulting in 1616 latent embeddings for input images of 256256pixels. The learned vocabulary consists of 1,024 entries, each being a 256-dimensional vector.",
  "VQGAN+: A Modern VQGAN": "Starting with the baseline Taming-VQGAN (Esser et al., 2021) that attains 7.94 rFID on ImageNet-1K256 256 (Deng et al., 2009), we provide a step-by-step walk-through and ablation of all changes in thefollowing paragraphs. A summary of the changes and their effects is given in . Basic Training Recipe and Architecture.The initial modifications to the Taming-VQGAN baselineare as follows: (1) removing attention blocks for a purely convolutional design, (2) adding symmetry to thegenerator and discriminator, and (3) updating the learning rate scheduler. Removing the attention layers,as adopted in recent methods (Chang et al., 2022; Yu et al., 2023; 2024a), reduces computational complexitywithout sacrificing performance. We also observed discrepancies between the open-source implementationof Taming-VQGAN (Esser et al., 2021) and its description in the publication. The implementation uses tworesidual blocks per stage in the encoder but three per stage in the decoder. For our experiments, we adopta symmetric architecture with two residual blocks per stage in both the encoder and decoder, as describedin the publication. Additionally, we align the number of base channels in the generator and discriminator.",
  "Result: 7.15 rFID (an improvement of 0.79 rFID)": "Increasing Model Capacity.Next, we increase the number of base channels from 64 to 128 for bothgenerator and discriminator.This adjustment aligns the generators capacity with that of the Taming-VQGAN baseline, except for the absence of attention blocks. As a result, the generator uses 17 millionfewer parameters overall compared to the Taming-VQGAN baseline, while the number of parameters in thediscriminator increases by only 8.3 million.",
  "Result: 4.02 rFID (an improvement of 3.13 rFID)": "Perceptual Loss.The perceptual loss (Johnson et al., 2016; Zhang et al., 2018) plays a crucial role infurther reducing the rFID score. In Taming-VQGAN, the LPIPS (Zhang et al., 2018) score obtained by theLPIPS VGG (Simonyan & Zisserman, 2015) network is minimized to improve image decoding. However,we noticed in the partially open-source code of recent work on image generation (Yu et al., 2023)1, theusage of a ResNet50 (He et al., 2016) network instead to compute the perceptual loss. Although confirmedby the authors of (Yu et al., 2023; 2024a), this change has so far not been discussed in the literature.We believe this change was already introduced in earlier publications of the same group (Chang et al.,2022), but never transparently documented. Following the open-source code of (Yu et al., 2023), we applyan L2 loss on the logits of a pretrained ResNet50 using the original and reconstructed images.Whileincorporating intermediate features into this loss can improve reconstruction further, it negatively affectsgeneration performance in our experiments. Therefore, we compute the loss solely on the logits.",
  "Result: 2.40 rFID (an improvement of 1.62 rFID)": "Discriminator Update.The original PatchGAN discriminator (Esser et al., 2021) employs 4 4 convo-lutions and batch normalization (Ioffe & Szegedy, 2015), resulting in an output resolution of 30 30 froma 256 256 input and utilizing 11 million parameters (with 128 base channels). We propose to replace the4 4 convolution kernels with 3 3 kernels and switch to group normalization (Wu & He, 2018), producinga 16 16 output resolution to align the output stride between the generator and discriminator. These ad-justments reduce the discriminators parameter count to 7.4 million. In the second update, following priorwork (Yu et al., 2024a), we replace average pooling for downsampling with a precomputed 4 4 Gaussianblur kernel using a stride of 2 (Zhang, 2019). Additionally, we incorporate LeCAM loss (Tseng et al., 2021)to stabilize adversarial training.",
  "Result: 2.00 rFID (an improvement of 0.4 rFID)": "The Final Changes.Our last addition to the training framework is the use of Exponential MovingAverage (EMA) (Polyak & Juditsky, 1992). We found that EMA significantly stabilizes the training andimproves convergence, while also providing a small performance boost. Orthogonal to the reconstructionperformance, we also add an entropy loss (Yu et al., 2024a) to ease the learning in the generation phase. Toachieve the final performance of VQGAN+, we increase the number of training iterations from 300,000 to1.35 million iterations, following common practices (Yu et al., 2024a; Gao et al., 2023).",
  "Result: 1.66 rFID (an improvement of 0.34 rFID)": "An Embedding-Free Variant.Recent works (Yu et al., 2024a; Mentzer et al., 2024) have shown thatremoving the lookup table from the quantization Oord et al. (2017) improves scalability.Following theLookup-Free Quantization (LFQ) approach (Yu et al., 2024a), we implement a binary quantization processby projecting the latent embeddings to K dimensions (K = 12 in this experiment) and then quantizing thembased on their sign values. Due to this binary quantization, we can directly interpret the obtained quantized",
  "7th8th9th10th11th12th": ": Bit tokens exhibit structured semantic representations. We visualize images obtainedafter corrupting tokens via bit flipping. Specifically, we utilize our method to encode images into bit tokens,where each token is represented by K-bits (K = 12 in this example). We then flip the i-th bit for all the bittokens and reconstruct the images as usual. Interestingly, the reconstructed images from these bit-flippedtokens remain semantically consistent to the original image, exhibiting only minor visual modifications suchas changes in texture, exposure, smoothness, color palette, or painterly quality. representations as their token numbers. We use the standard conversion between base-2 and base-10 integers(e.g., 10012 = 910). In VQGAN+, the token is merely the index number of the embedding table; however,in this case, the token itself contains its corresponding representation, eliminating the need for an additionalembedding.Implicitly, this defines a non-learnable codebook of size 2K.Given the compact nature ofbinary quantization with only K feature dimensions, we do not anticipate significant gains in reconstruction.However, we will empirically demonstrate the clear advantages of this representation in the generation stage.",
  "MaskBit: A New Embedding-free Image Generation Model": "Since the typical pipeline of VQGAN-based image generation models (Chang et al., 2022; Yu et al., 2024a)includes two stages with Stage-I already discussed in the previous section, we focus on the Stage-II in thissection. Stage-II refers to a transformer network that learns to generate images in the VQGANs latentspace. During Stage-II training, the tokenizer produces latent representations of images, which are thenmasked according to a predefined schedule (Chang et al., 2022). The masked tokens are fed into the Stage-IItransformer network, which is trained with a categorical reconstruction objective for the masked tokens.This process resembles masked language modeling, making language models such as BERT (Devlin et al.,2018) suitable for this stage. These transformer models relearn a new embedding for each token, as theinput tokens themselves are just arbitrary index numbers. Bit Tokens Are Semantically Structured.Following our study of embedding-free tokenization, whicheliminates the lookup table from Stage-I (i.e., removing the need for embedding tables in VQ-VAE (Oordet al., 2017)) by directly quantizing the tokens into K-bits, we study the achieved representation morecarefully. As shown in (Yu et al., 2024a), this representation yields high-fidelity image reconstruction results,indicating that images can be well represented and reconstructed using bit tokens. However, prior arts (Yuet al., 2024a) have used different token spaces for Stage-I and Stage-II, intuitively allowing the Stage-II tokento focus more on semantics, while enabling the Stage-I token space to capture all visual details necessary forfaithful reconstruction. This raises the question:",
  "(ours)": ": High-level comparison of the architectures. Our training framework comprises two stagesfor image generation. In Stage-I, an encoder-decoder network compresses images into a latent representationand decodes them back. Stage-II masks the tokens, feeds them into a transformer and predicts the maskedtokens. Most prior art uses VQGAN-based methods (top) that learn independent embedding tables in bothstages. In VQGAN-based methods, only indices of embedding tables are shared across stages, but not theembeddings.In MaskBit, however, neither Stage-I nor Stage-II utilizes embedding tables.The Stage-Ipredicts bit tokens by using binary quantization on the encoder output directly. The Stage-II partitions theshared bit tokens into groups, masks and feeds them into a transformer to predict the masked bit tokens.An illustration of grouping and masking of bit tokens can be found in the Appendix C.",
  "Are bit tokens also a good representation for generation, and can bit tokens eliminate the need forre-learning new embeddings in the typical Stage-II network?": "To address this, we first analyze the bit tokens learned representations for image reconstruction. Specifically,given a latent representation with 256 tokens from our model (where each token is represented by K-bits,and each bit by either 1 or 1), we conduct a a bit flipping experiment to show how the structure ofthe token space relates to the content. This test involves flipping the i-th bit (i.e., swapping 1 and 1)for all 256 bit tokens and decoding them as usual to produce images. The bit-flipped tokens each have aHamming distance of 1 from the original bit tokens. Surprisingly, we find that the reconstructed imagesfrom these bit-flipped tokens are still visually and semantically similar to the original images. This indicatesthat the representation of bit tokens has learned structured semantics, where neighboring tokens (within aHamming distance of 1) are semantically similar to each other. We visualize an example of this in and provide more examples in Sec. E. We note that the structural differences in the latent space between bittokens and learnable embeddings also have implications on the generators representation, which we describein Appendix D. Since the generation model is usually semantically conditioned on external input such asclasses, the model needs to learn the semantic relationships of tokens. Given this evidence of the inherentstructured semantic representation in bit tokens, we believe that the learned rich semantic structure in thebit token representation can thus be a good representation for the Stage-II generation model. Proposing MaskBit.This motivates us to propose MaskBit for Stage-II, a transformer-based generativemodel that generates images directly using bit tokens, removing the need for the embedding lookup table as",
  "Experimental Results for Image Generation": "Experimental Setup.We evaluate the proposed MaskBit on class-conditional image generation. Specifi-cally, the network generates a total of 50,000 samples for the 1,000 ImageNet (Deng et al., 2009) classes. AllStage-II generative networks are trained and evaluated on ImageNet at resolutions of 256 256. We reportthe main metric Frchet Inception Distance for generation (gFID) (Heusel et al., 2017), along with the sidemetric Inception Score (IS) (Salimans et al., 2016). All scores are obtained using the official evaluation suiteand reference statistics by ADM (Dhariwal & Nichol, 2021). We train all Stage-II networks for 1,080 epochsand report results with classifier-free guidance (Ho & Salimans, 2022). More training and inference detailscan be found in Sec. B.",
  "Tab. 1 summarizes the generation results on ImageNet 256 256. We make the following observations": "VQGAN+ as a New Strong Baseline for Image Tokenizers.By carefully revisiting the VQGANdesign in Sec. 2, we develop VQGAN+, a solid new baseline for image tokenizers that achieves an rFID of1.66, marking a significant improvement of 6.28 rFID over the original Taming-VQGAN model (Esser et al.,2021). Using VQGAN+ in the generative framework of MaskGIT (Chang et al., 2022) results in a gFID of2.12, outperforming several recent advanced VQGAN-based methods, such as ViT-VQGAN (Yu et al., 2022a)(by 0.92 gFID), RQ-Transformer (Lee et al., 2022a) (by 1.68 gFID), and even the original MaskGIT (Changet al., 2022) (by 1.90 gFID). This result shows that the improvements we propose to the Stage-I model,indeed transfer to the generation model. Furthermore, our result surpasses several recent diffusion-basedgenerative models that utilize the heavily optimized VAE (Kingma & Welling, 2014) by Stable Diffusion (AI,",
  "), including LDM (Rombach et al., 2022), U-ViT (Bao et al., 2023), and DiT (Peebles & Xie, 2023).We believe these results with VQGAN+ establish a new solid baseline for the community": "Bit Tokens as a Strong Representation for Both Reconstruction and Generation.As discussedin previous sections, bit tokens encapsulate a structured semantic representation that retains essential infor-mation for image generation, forming the foundation of MaskBit. Consequently, MaskBit delivers superiorperformance, achieving a gFID of 1.65 with 12 bits and 1.62 with 14 bits, surpassing prior works.No-tably, MaskBit outperforms the recent state-of-the-art MAGVIT-v2 (Yu et al., 2024a) while utilizing animplicit codebook that is 16 to 64 smaller. When the number of sampling steps is increased from 64 to256matching the steps used by autoregressive modelsMaskBit sets a new state-of-the-art with a gFIDof 1.52, outperforming concurrent works (Gao et al., 2024; Li et al., 2024; Liu et al., 2024; Sun et al., 2024;Tian et al., 2024; Yu et al., 2024c), while using a compact generator model of just 305M parameters. Re-markably, MaskBit, which operates with discrete tokens, surpasses methods relying on continuous tokens,such as MDTv2 (Gao et al., 2024) and MAR-H (Li et al., 2024). Comparison under Similar Generator Parameters. In Tab. 2, we present a performance comparisonacross models of similar sizes. Previous works have demonstrated that increasing model size generally leadsto superior performance (Li et al., 2024; Sun et al., 2024; Tian et al., 2024).Therefore, to provide amore comprehensive comparison, we only compare models using a similar number of parameters, but varycodebook size and sampling steps. As shown in the table, MaskBit not only achieves state-of-the-art resultsoverall but also outperforms all other models under similar conditions. It is worth noting that all models inthe comparison use a larger Stage-I (reconstruction) tokenizer than ours.",
  "Unless stated otherwise, we use fewer training iterations for the ablation studies due to limited resourcesand computational costs": "Embedding-free Stages Bring Improvements.In Tab. 3a, we investigate embedding-free designs inboth Stage-I (tokenizer) and Stage-II (generator). To ensure a fair comparison, all models use the same num-ber of tokens, codebook size, and model capacity. In the first row, we employ VQGAN+ with a traditionalStage-II model, as in MaskGIT (Chang et al., 2022), achieving a gFID of 2.12. Using bit tokens in Stage-Iimproves performance to a gFID of 1.95, highlighting the effectiveness of structured semantic representationof bit tokens.Further enabling our Stage-II model MaskBit (i.e., both stages now are embedding-free)achieves the best gFID of 1.82, demonstrating the advantages of embedding-free image generation from bittokens. For completeness, we also explore VQGAN+ with an embedding-free Stage-II model, yielding agFID of 2.83. This variant performs worse due to the lack of the semantic structure in the tokens. Two Groups of Bits Lead to Better Performance.Tab. 3b presents the performance of MaskBitusing a different numbers of groups. As shown in the table, when using two groups, each consisting of sixconsecutive bits, MaskBit achieves the best performance. This configuration outperforms a single group",
  "Related Work": "Image Tokenization. Images represented by raw pixels are highly redundant and commonly compressed toa latent space with autoencoders (Hinton & Salakhutdinov, 2006; Vincent et al., 2008), since the early daysof deep learning. Nowadays, image tokenization still plays a crucial role in mapping pixels into discrete (Oordet al., 2017) or continuous (Kingma & Welling, 2014; Higgins et al., 2017) representation suitable for gener-ative modeling. We focus on the stream of vector quantization (VQ) tokenizers, which proves successful instate-of-the-art transformer generation frameworks (Esser et al., 2021; Chang et al., 2022; Yu et al., 2022b;2024a; Lezama et al., 2022; 2023; Lee et al., 2022a;b). Starting from the seminal work VQ-VAE (Oord et al.,2017), several key advancements have been proposed, such as the perceptual loss (Zhang et al., 2018) anddiscriminator loss (Karras et al., 2019) from (Esser et al., 2021) or better quantization methods (Zheng et al.,2022; Lee et al., 2022a; Zheng & Vedaldi, 2023; Mentzer et al., 2024; Yu et al., 2024a), which significantlyimprove the reconstruction quality. Nonetheless, modern transformer generation models (Chang et al., 2022;Yu et al., 2023; 2024a) utilize an even stronger VQ model, with details not fully discussed in the literature,resulting in a performance gap to the wider research community. In this work, we conducted a systematicstudy to modernize the VQGAN model, aiming at demystifying the process to obtain a strong tokenizer.",
  "Conclusion": "We conducted systematic experiments and provided a thorough, step-by-step study towards a modernizedVQGAN+.Together with bit tokens, we provide two strong tokenizers with fully reproducible trainingrecipes. Furthermore, we propose an embedding-free generation model MaskBit, which makes full use ofthe rich semantic information from bit tokens. Our method demonstrates state-of-the-art results on thechallenging 256 256 class-conditional ImageNet benchmark.",
  "GPU: A100": "We use 32 A100 GPUs for training Stage-I models. Training time varies with respect to the number oftraining iterations between 1 (300K iterations) and 4.5 (1.35M iterations) days. We note that the Stage-Imodel can also be trained with fewer GPUs without any issues, but this will accordingly increase the trainingduration. Stage-II models are trained with 64 A100 GPUs and take 4.2 days for the longest schedule (1.35Miterations).",
  "B.3Sampling Procedure": "Our sampling procedure closely follows prior works for non-autoregressive image generation (Gao et al., 2023;Chang et al., 2022; Yu et al., 2024a; 2023; Chang et al., 2023). By default, the Stage-II network generatesimages in 64 steps. Each step, a number of tokens is unmasked and kept. The number of tokens to keepis determined by an arccos scheduler (Besnier & Chen, 2023; Chang et al., 2022). Additionally, we useclassifier-free guidance and a guidance scheduler (Gao et al., 2023; Yu et al., 2023). We refer to the codebaseavailable on for the detailed sampling parameters for eachmodel.",
  "B.4Sensitivity to Sampling Parameters": "In this subsection, we detail the specific values for the sampling hyper-parameters to ensure reproducibility.However, the reported FID scores can be obtained by a large range of hyper-parameters. Due to the inherentrandomness in the sampling procedure, results might vary by 0.02 FID. For the resolution of 256 256,we verified several choices of parameters obtaining similar results: temperature t [7.3, 8.3], guidances [5.8, 7.8] and scale_pow m [2.5, 3.5].",
  "CVisualization of Masking Groups of Bit Tokens": "provides a visualization of the approach, masking groups of bit tokens, presented in this paper. Whenusing a single group, a bit token will either be masked completely or remain unchanged after the masking isapplied. When using more than a single group, the bit token is split into groups of consecutive bits. Each ofthese groups can be independently masked during the masking procedure, leading to some cases where thebit tokens are partially masked as shown in the illustration. Notably, within each group of bits, the maskingcan only be applied to all bits in the group.",
  "D.1Connection of Semantic Structure and Generator Performance": "Prior work (Gu et al., 2024) studying the tokenizer objective has found that a strong semantic representationin the latent space is important for good generation performance. This observation was made by usingdifferent variations of the LPIPS perceptual loss. The experiments presented in this work in Sec. 2 supportthe same observation. Specifically, VQGAN+ uses a ResNet logits-based perceptual loss which we found towork well for generation. However, as mentioned in the paragraph Perceptual Loss, adding intermediateactivations of ResNet to the perceptual loss decreased the generation performance even though it helps thereconstruction performance. Given the insights on the importance of semantics, this work studies the latent space of bit tokens andthe connection to the image content. The key finding is that bit tokens exhibit this uniquely constrainedstructure by design, which leads to a structured semantic representation. In the next paragraph, we discusshow this structured representation affects the internal generation model feature representation per token.",
  ": 0 (masked)": ": Visualization of the process for masking of groups of bit tokens. The figure visualizesthe masking procedure of bit tokens (12 bits) when using either one group (top row) or two groups (bottomrow). When using a single group, a bit token can either be fully masked or fully non-masked. When usingtwo groups, each bit token is split into two consecutive groups (i.e., the first group contains the first 6 bitsand the second group has the last 6 bits). These two groups can be independently masked. This leads tothe case where a bit token can be partially masked, as shown in the illustration.",
  "D.2Implications of Different Token Spaces on the Generator": "In prior work, only the token indices were shared between Stage-I and Stage-II models. In all cases, theStage-II model relearns an embedding table. Considering our setting of 12 bits, a MaskGIT Stage-II modelwould learn a new embedding table with 212 = 4096 entries. Each of these entries is an (arbitrary) vectorin R1024 with 1024 being the hidden dimension. We note that this approach is equivalent to replacing theembedding table with a linear layer processing one-hot encoded token indices. The linear layer would in thiscase do the same mapping as the embedding table and have the same number of parameters, i.e., 40961024,omitting the bias for simplicity. In MaskBit, instead of token indices, the bit tokens are shared and processed directly. MaskBit also uses alinear layer to map to the models internal representation, but this mapping is from 12 dimensions (whenK = 12) to 1024, which corresponds to learning just 12 embedding vectors. Thus, the learnable mappingis reduced by a factor of 341. This is possible as MaskBit does not use a one-hot encoded representation,but a bit token representation with 12 bits. The linear layer therefore adds/subtracts the learnable weightsdepending on the bit coefficients 1. It follows that the internal representation per token in MaskBit isnot arbitrary in R1024, but significantly constrained. These constraints require a semantically meaningfulstructure in the bit tokens, while in MaskGIT the representation for each token index is independent andarbitrarily learnable.",
  "+1...1+1+1+1": "In MaskBit, the only difference when processing the tokens in the first linear layer is that the last weightvector, corresponding to the last bit in the tokens, is either added (for t2) or subtracted (for t1). Thus, theobtained results are not independent of each other and not arbitrary, which is different from prior work.",
  "EMore Reconstruction Results of Flipping Bit Tokens": "We present additional reconstruction results from the bit flipping analysis. As shown in , the recon-structed images from these bit-flipped tokens remain visually and semantically similar to the original images,consistent with the findings in the main paper. Furthermore, we repeat this experiment in a zero-shot manner on COCO (Lin et al., 2014) with the samemodel only trained on ImageNet. The results are visualized in , demonstrating that the bit tokensstructured semantic representation also holds for COCO images.",
  "FZero-Shot Reconstruction Results on COCO": "In Tab. 5, we provide zero-shot evaluation of the presented tokenizers of Sec. 2 on COCO (Lin et al., 2014)to show how the modifications made to the orginal VQGAN (Esser et al., 2021) impact the performance onmore complex datasets. For this experiment, we use the same models as for the experiments in Sec. 2 thatwere only trained on ImageNet (Deng et al., 2009). All images are resized with their shorter edge to 256and then center cropped before feeding them into the models. The results show that the modifications madeto obtain VQGAN+ and its embedding-free variant also achieve improvements on more complex datasets inthe zero-shot evaluation setting.",
  "GNearest Neighbor Analysis of Generated Images": "Following the idea presented in Esser et al. (2021), we conduct a nearest neighbor analysis. In this analysis,a generated image is used and compared to all images in the training set. For this comparison, we usetwo distance measures: LPIPS and Hamming distance. The LPIPS metric is used to measure perceptualsimilarity of two images, while the Hamming distance takes the bit tokens of the encoded images andcomputes the bit-wise distance.In and , we visualize the 10 nearest neighbors for eachgenerated image and distance measure. The results show that the model is not just memorizing samplesfrom the training set, but generates indeed new images. Since LPIPS and Hamming distance are two differentmeasures, the obtained nearest neighbors also differ. For example, considering the neighbors to the generatedimages of the parrot or the space rocket in , we observe that the nearest samples according to LPIPSfocus more on staying true to the reference color palette, while this is less the case for the closest samplesaccording to the Hamming distance. This finding aligns with the observations in the bit-flipping experiment,",
  "HLimitations": "In this study, the focus has been on non-autoregressive, token-based transformer models for generation.Due to the computational costs, we did not explore using the modernized VQGAN model (VQGAN+)in conjunction with diffusion models. For similar reasons, MaskBit was only trained on ImageNet, thusinheriting the dataset bias, i.e., focus on a single foreground object. This makes it difficult to do have aqualitative comparison to methods trained on large-scale datasets such as LAION (Schuhmann et al., 2022).We also note, that the standard benchmark metric FID has received criticism due to it measuring onlydistribution similarities. However, due to the lack of better and established alternatives, this study uses FIDfor its quantitative evaluation and comparison.",
  "IBroader Impacts": "Generative image models have diverse applications with significant social impacts. Positively, they enhancecreativity in digital art, entertainment, and design, enabling easier and potentially new forms of expression,art and innovation. However, these models pose risks, including the potential for misuse such as deepfakes,which can spread misinformation and facilitate harassment. Additionally, generative models can suffer fromsocial and cultural biases present in their training datasets, reinforcing negative stereotypes. We noticethat these risks have become a dilemma for open-source and reproducible research. The scope of this workhowever is purely limited to class-conditional image generation. In class-conditional generation, the modelwill only be able to generate images for a predefined, public and controlled set of classes. In this case, theimages and class set are coming from ImageNet. As a result, we believe that this specific work does not posethe aforementioned risks and is therefore safe to share."
}