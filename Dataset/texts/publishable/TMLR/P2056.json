{
  "Abstract": "Learning causal representations without assumptions is known to be fundamentally impossible,thus establishing the need for suitable inductive biases. At the same time, the invarianceof causal mechanisms has emerged as a promising principle to address the challenge ofout-of-distribution prediction which machine learning models face. In this work, we explorethis invariance principle as a candidate assumption to achieve identifiability of causalrepresentations.While invariance has been utilized for inference in settings where thecausal variables are observed, theoretical insights of this principle in the context of causalrepresentation learning are largely missing. We assay the connection between invariance andcausal representation learning by establishing impossibility results which show that invariancealone is insufficient to identify latent causal variables. Together with practical considerations,we use our results to reflect generally on the commonly used notion of identifiability in causalrepresentation learning and potential adaptations of this goal moving forward.",
  "Introduction": "Inferring high-level causal variables from low-level measurements is a problem garnering increased attentionin fields interested in understanding epiphenomena that cannot be directly measured and where controlledexperiments are not possible due to practical, economical or ethical considerations, for instance in healthcare(Johansson et al., 2022), biology (Lopez et al., 2023) or climate science (Tibau et al., 2022). This problem ofcausal representation learning (Schlkopf et al., 2021) has been shown to be fundamentally underconstrained(Locatello et al., 2019), leading to various approaches exploring which assumptions lead to algorithms thatidentify the latent causal variables.",
  "Published in Transactions on Machine Learning Research (09/2024)": "Another approach that exploits a specific sparsity assumption is given by Lachapelle et al. (2023). Similarto our setting, the synergy of representation learning with a prediction problem is explored. Instead ofconsidering interventions that induce heterogeneity in the data, this framework assumes multiple predictiontasks where a single, underlying representation contains the potential predictors for each individual task. Thissetting alone only suffices to recover the latent variables up to linear mixing, but by imposing an additionalsparsity constraint, the latents are shown to be causally disentangled (cf. Definition 1). Works considering latent DAG structure learning are also in spirit related to our setting, as they commonlyassume to observe at least some nodes within the graph they aim to learn, similar to how we assumeobservations of the target Y . The main assumption for most works in this setting is the pure childrenassumption (Silva et al., 2006), or some variation thereof. This assumption postulates that all observedvariables have only a single latent variable as a parent. Cai et al. (2019) deal with linearly transformed latentvariables that have two pure children, subsequently generalized by Xie et al. (2020) to permit more than twopure children per latent in the form of the so-called Generalized Independent Noise condition. The sameauthors later work further generalizes this setting to facilitate the learning of hierarchical latent variablemodels (Xie et al., 2022). While our approach also assumes the observation of an effect of the underlyingSCM in the form of Y , the main difference to the aforementioned works lies in the modelling choice of theremaining observed variables, X. In our case, we consider X to come from an injective transformation ofthe underlying variables Z, and therefore not to be causal variables of any SCM directly, while the methodsmentioned above consider all observed variables X to be part of the underlying, latent SCM and consequentlymodel their relation to the latent variables Z in terms of surjective causal mechanisms. Given this difference,we do not require assumptions on the mechanism of Y similar to those in (Xie et al., 2020) and related workspertaining to the number of observed children of latents Z or the linearity of the transform that yields X.",
  "gcausal": ": We consider an SCM with variables(Z1, . . . , Zd) and Y .Observed variables arerepresented by shaded nodes, indicating thatwe do not observe (Z1, . . . , Zd), but only theirtransformation X = gcausal(Z). Intuitively, our problem setting can be motivated by consider-ing the prediction problem with a target Y and observationsX in multiple environments. We assume that there is an un-derlying causal representation of the observationsdenotedby Zwhose constituents are causes of the prediction targetY and interact with each other through a structural causalmodel (SCM). This representation Z is what we are interestedin finding. Notation.We denote scalar variables in normal face (x)and use bold face for vector-valued variables (x). We cap-italize random variables (Y ), and write the values they takein lower case (y). Matrices are denoted capitalized and bold(M) and are explicitly introduced to avoid confusion withvector-valued random variables. The sequence of integersfrom 1 to n is denoted with [n]. Data generating process.Let (Z1, . . . , Zd+1) denote a set of random variables. W.l.o.g. we call Zd+1the target variable and rename it Y , denoting the remaining d variables with Z = (Z1, . . . , Zd). Assume anSCM defined over the random vector (Z, Y ) inducing the joint distribution P(Z, Y ) over (Z, Y ). The graph Ginduced by this SCM is assumed to be acyclic. Assume Y is not a parent of any Z. Since P(Z, Y ) is inducedby an SCM, it factorizes as",
  "where Pai [d + 1] \\ i denotes the parents of variable Zi. We refer to (Pearl, 2009) for an in-depth definitionof SCMs": "Beyond the additive noise assumption we do not place any parametric constraints on the causal mechanismof each variable, i.e. each structural equation can be written as Zi := fi(ZPai) + i, where i with i [d + 1]denote the exogenous, independent noise terms, which are assumed to have zero mean. Since the causalmechanism of the target Y is of particular interest in a predictive setting, we denote it with fcausal := fd+1. We do not directly measure Z and only assume to observe X Rp, where X = gcausal(Z) is a transformation ofthe causal variables Z by the injective, deterministic (and potentially nonlinear) function gcausal. We assumep d. Notice that Y is not transformed by gcausal; we assume that the target variable is directly observed.The set of observed variables is therefore denoted by (X, Y ). depicts a graphical representation of thisdata generating process. We assume to observe (X, Y ) across multiple environments, where subsets S [d] of the underlying latentvariables Z have undergone an intervention in each environment. We model interventions with do-interventions(Pearl, 2009) (also called hard interventions), which set the structural equations of the variables that aretargeted by an intervention to constant values, allowing us to write",
  "Zj := ajfor j S,(2)": "where a R|S|. Each intervention on the subset of variables S with value a induces a new distribution over(Z, Y ) which, following the notation introduced by Meinshausen (2018), is denoted by P (do)a,S . We assumeY is never among the set of intervention targets and the mixing function gcausal does not change betweenenvironments. Objective.Although we explicitly define a target variable, we stress that our main goal is not to learn agood predictive model of Y , but rather to recover the latent variables Z, i.e. we focus on the representationlearning aspect of the setting described above. Our aim is to probe how the auxiliary task of predicting Yfrom transformations of latent variables in multiple environments can help in recovering the unobserved causalvariables. Our formal objective is to invert the mixing function gcausal in order to recover the latent variablesZ from observations X. Since latent variables that are equal to the ground truth up to permutation andelement-wise rescaling can give rise to the same observations X (Zhang et al., 2023), we define an equivalencerelation over this class of latents. Consequently, we define recovering the latent causal variables up to thisequivalence as our notion of identifiability. Equivalent representations are referred to as causally disentangled. Definition 1 (Causally Disentangled Representations, Khemakhem et al. (2020); Lachapelle et al. (2022)).A learned representation Z is causally disentangled w.r.t. the ground truth representation Z if there exists aninvertible diagonal matrix D and a permutation matrix P s.t. Z = DPZ almost surely.",
  "Invariance for causal structure learning": "The connection between (predictive) invariance and causality is long established: Haavelmo (1944) was thefirst to formalize that a model which predicts a target from its direct causes is invariant under interventionson any other covariates of the system. In the language of SCMs this means that the conditional distributionP(Y |ZPaY ) remains invariant under any interventions on Z. This principle is also referred to as autonomy(Aldrich, 1989), modularity (Pearl, 2009) and independence of cause and mechanism (Peters et al., 2017). More recently, the opposite direction has been explored, namely how invariance can be leveraged as a signal toinfer causal structures and mechanisms. Peters et al. (2016) pioneered this approach by exploiting the principleof invariance of causal mechanisms to infer the direct causes of a target Y , assuming direct observations ofthe causal variables Z. A particularly interesting line of works draws a connection between distributionalrobustness and causality (Meinshausen, 2018; Bhlmann, 2018; Rojas-Carulla et al., 2018) by considering theproblem",
  "which thus also holds for the supremum over Q(do) and the supremum over Q(do), and we conclude thatfcausal is the unique optimizer of Eq. (3)": "As we are interested in establishing identifiability results, this uniqueness result provides a potentially fruitfulstarting point: if the unique solution to this optimization problem is related to the underlying causal model,perhaps we can introduce an adaptation of it that yields the causal representation as the unique solution.Notice that Lemma 1 assumes direct access to the variables Z, while our problem setting of interest ischaracterized by the central assumption of only observing transformations X = gcausal(Z) of the underlyingcausal variables. We investigate the implications of this transformation in the next section.",
  "Causal mechanism and representation are jointly unique": "As a first uniqueness result, we show that the joint function composed of the causal mechanism fcausal andthe inverse of the ground-truth representation function g1causal is the unique solution to Eq. (5). Lemma 2. Assume the data generating process in and consider the optimization problem describedin Eq. (5). Let Q(do) :=P (do)a,[d]; a Rd, i.e. the set of do-interventions on all underlying variables Z, exceptY , with arbitrary strength. Define h := f g1 : Rp R and let Im() denote the image of a function. Then,the composed function hcausal := (fcausal g1causal) is the unique optimizer of Eq. (5) on Im(gcausal), i.e.",
  "The same results holds true if one replaces Q(do) with the set of all do-interventions Q(do) on all possiblesubsets of variables, except Y , with arbitrary strength": "To prove Lemma 2, we need to find a b Rp s.t. h(b) = hcausal(b). Notice that hcausal takes as argumentX = gcausal(Z) while we can only intervene directly on Z. Thus, we can only find b Im(gcausal) andconsequently the statement h(b) = hcausal(b) only holds on the image of gcausal. Taking this into account,the proof (presented in Appendix A) follows the same structure as the proof of Lemma 1.",
  "Overparametrization of the unconstrained setting": "While the uniqueness result of Lemma 2 regarding the composed function (fcausal g1causal) is a first promisingdirection in relating the solution of a distributional robustness problem to inverting the representation functiongcausal, we quickly see that this uniqueness result does not yet constrain the individual components f and gof the solution enough. Theorem 1. Consider the data generation process presented in . Without additional assumptions,the distributional robustness problem described by Eq. (5) does not suffice to identify the underlying causalvariables up to the equivalence class detailed in Definition 1. Proof. While (fcausal g1causal) has been shown to be the unique solution to the optimization problem describedby Eq. (5), this does not directly imply the respective uniqueness of its components f and g. To see this,consider any invertible map : Rd Rd and write",
  "Necessity of additional assumptions": "So far, we have not imposed any parametric constraints on fcausal or gcausal. The impossibility result describedin Theorem 1 however implies that we require further assumptions to make progress towards identifyingthe latent causal variables. Since we want our results to hold for general gcausal, we refrain from beginningwith assumptions on the representation function. Rather, we will investigate how parametric assumptions onfcausal may be used to constrain class of functions g that solve Eq. (5). Notice that the functions that solve Eq. (5), f = fcausal 1 and g1 = g1causal, cannot be chosenindependently of each other, but are connected via the map . This connection motivates our reasoningbehind constraining fcausal: for certain parametric choices of fcausal (and accordingly f) perhaps only aconstrained set of maps admits a solution to Eq. (5). If this is the case and we effectively constrain , wemight be able to find a constraint on fcausal such that only transformations of the form = DP, where D isa diagonal matrix and P is a permutation matrix, which would result in recovering the causally disentangledg1 = DPg1causal, according to Definition 1. Linear causal mechanism.A first natural assumption to impose is linearity of fcausal, and correspondinglyof f, in the hopes of constraining to be a linear invertible map. This would allow us to make substantialprogress towards recovering the causal variables up to permutation and rescaling, by first recovering theground truth representation up to linear equivalence and then employing tactics to undo this linear mixing,following a common approach in causal representation learning (Ahuja et al., 2023; Zhang et al., 2023;Lachapelle et al., 2023). We explore the implications of assuming linearity of fcausal with an illustrative example. Assume Y :=1Z1 + 2Z2 + Y , i.e. the two variable case where d = 2. Since we assume fcausal to be linear, we alsoconstrain the search space of f to the class of linear functions. Recall that f := fcausal 1. Does theassumed linearity of f and fcausal now constrain to also be a linear map? Writing out",
  "f = fcausal 1 = 111+ 212 ,": "where 1idenotes the ith component of 1, we see that e.g. by choosing 1 = 1, 2 = 0 only 11isconstrained to be linear, while 12remains wholly unconstrained. We generalize this result to arbitrarychoices of in the following theorem. Theorem 2. Assume that Z, X, Y, fcausal, gcausal follow the definitions in with d 2 and thatadditionally 0 = fcausal : Rd R is a linear function. Then, there exists an invertible nonlinear function : Rd Rd such that f = fcausal is linear. Moreover, this function is not unique and can be chosenarbitrarily (that is, only constrained to be invertible) on a d 1-dimensional subspace of Rd. In particular,the tuple ( f, g) with g := gcausal gives rise to the solution hcausal of Eq. (5) in that hcausal = f g1. Proof. We begin with an example which will later be generalized. Our goal is to elucidate if assuming fand fcausal to be linear functions necessarily constrains to be a linear map, where f := fcausal 1. We",
  "f = fcausal 1 = 111+ + d1d= 11": "For this particular choice of , constraining f to be linear amounts to constraining 11to be linear, whilethe other components 1i; i [d] \\ {1} are not constrained at all and can be chosen arbitratrily as long as remains invertible. For example, the map (z) = (z1, z32, . . . , z3d) does the job. For a general choice of we can always find an orthonormal transformation A : Rd Rd that maps(1, . . . , d)T (1, 0, . . . , 0)T . Consider a nonlinear map 0 for the initial case = (1, 0, . . . , 0)T , whose firstcomponent is linear. Define := AT 0 which remains nonlinear. We can now write",
  "Thus f is linear while was only constrained to be linear in its first component": "Through the generalized counterexample presented in the proof of Theorem 2, we see that the linearityrequirement on fcausal 1 only constrains one dimension of , namely the one orthogonal to the kernel offcausal. We would need one such constraint for each dimension of possibly by assuming dim(Y ) d as in(Lachapelle et al., 2023)in order to draw the desired linearity conclusion. Unfortunately, thearguably strongassumptions presented in this section still do not suffice to recovergcausal up to the desired equivalence class. In the following sections, we delineate possible steps forward, bothin light of the presented impossibility results, as well as practical considerations.",
  "Practical considerations": "In the preceding sections we have shown the impossibility of learning causal representations by means ofexploiting invariance, even if we consider strong parametric assumptions and an idealized setting with infinitedata and perfect optimization. While these assumptions are commonplace in theoretical studies, we wouldlike to draw particular focus to the assumption of observing all possible interventions. The minimax problem in Lemma 1, which is the starting point of our analysis, considers the supremum overall possible interventions, i.e. all possible targets with arbitrary strength. This amounts to observing infinitelymany interventional environments, which is clearly not possible in any practical scenario. Rather, one wouldhave access to a finite number of environments during training, over which we formulate our invariancecondition in hopes of generalizing to unseen environments. Training a model on finite support, i.e. on a finite number of environments, and generalizing outside of thissupport amounts to extrapolation. As Christiansen et al. (2022) show, learning such extrapolating nonlinearfunctions from data with bounded support necessarily requires strong assumptions on the function class. Ifwe do not constrain gcausal, even if fcausal is linear, the function fcausal g1causal, that should generalize in theaforementioned sense, is still generally nonlinear. If, for the sake of argument, we assume invariance as an inductive bias leads to identifiability of the underlyingcausal variables, we would still remain with the fundamental issue of learning such representations from finitedatasets in practice. Beyond the theoretical limitations of exploiting invariance as a learning signal for causalrepresentation learning, we are also left with the aforementioned practical ones. This gives additional weightto the conclusion that the problem setting considered in this work is highly challenging. We reflect thesechallenges in light of potential steps forward in the following, final section.",
  "Peter Bhlmann. Invariance, Causality and Robustness. arXiv preprint arXiv:1812.08233, 2018": "Ruichu Cai, Feng Xie, Clark Glymour, Zhifeng Hao, and Kun Zhang. Triad Constraints for Learning CausalStructure of Latent Variables. In Advances in Neural Information Processing Systems, 32, pp. 1288312892,2019. Rune Christiansen, Niklas Pfister, Martin Emil Jakobsen, Nicola Gnecco, and Jonas Peters. A Causal Frame-work for Distribution Generalization. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(10):66146630, 2022. Cian Eastwood, Alexander Robey, Shashank Singh, Julius Von Kgelgen, Hamed Hassani, George J. Pappas,and Bernhard Schlkopf. Probable Domain Generalization via Quantile Risk Minimization. In Advances inNeural Information Processing Systems, 35, pp. 1734017358, 2022. Luigi Gresele, Julius Von Kgelgen, Vincent Stimper, Bernhard Schlkopf, and Michel Besserve. Independentmechanism analysis, a new concept?In Advances in Neural Information Processing Systems, 34, pp.2823328248, 2021.",
  "Ricardo Silva, Richard Scheine, Clark Glymour, and Peter Spirtes. Learning the Structure of Linear LatentVariable Models. Journal of Machine Learning Research, 7(8):191246, 2006": "Chandler Squires, Anna Seigal, Salil S. Bhate, and Caroline Uhler. Linear Causal Disentanglement viaInterventions. In Proceedings of the 40th International Conference on Machine Learning, pp. 3254032560,2023. Xavier-Andoni Tibau, Christian Reimers, Andreas Gerhardus, Joachim Denzler, Veronika Eyring, andJakob Runge. A spatiotemporal stochastic climate model for benchmarking causal discovery methods forteleconnections. Environmental Data Science, 1:e12, 2022.",
  "AProof of Lemma 2": "Lemma 2. Assume the data generating process in and consider the optimization problem describedin Eq. (5). Let Q(do) :=P (do)a,[d]; a Rd, i.e. the set of do-interventions on all underlying variables Z, exceptY , with arbitrary strength. Define h := f g1 : Rp R and let Im() denote the image of a function. Then,the composed function hcausal := (fcausal g1causal) is the unique optimizer of Eq. (5) on Im(gcausal), i.e.",
  "= 0,": "where we use the fact that Y has mean zero. Hence, for any choice of h the supremum of the l.h.s. of Eq. (6)is always larger or equal to EQ[2Y ] = V ar(Y ). Since Q(do) Q(do), the supremum over Q(do) of the l.h.s.of Eq. (6) is also larger or equal to V ar(Y ). Focusing our attention on the first term of the decomposition presented above, suppose h = hcausal, i.e. thereexists a choice of b s.t. h(b) = hcausal(b). Recall that h takes X as its argument, but we cannot directlyintervene on X = gcausal(Z), only on Z."
}