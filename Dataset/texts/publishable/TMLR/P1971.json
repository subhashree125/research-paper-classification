{
  "Abstract": "This paper studies a natural generalization of the problem of minimizing a convex function fby querying its values sequentially. At each time-step t, the optimizer selects a query pointXt and invests a budget bt (chosen by the environment) to obtain a fuzzy evaluation of fat Xt whose accuracy depends on the amount of budget invested in Xt across times. Thissetting is motivated by the minimization of objectives whose values can only be determinedapproximately through lengthy or expensive computations, where it is paramount to recyclepast information. In the univariate case, we design ReSearch, an anytime parameter-freealgorithm for which we prove near-optimal optimization-error guarantees. Then, we presenttwo applications of our univariate analysis. First, we show how to use ReSearch for stochasticconvex optimization, obtaining theoretical and empirical improvements on state-of-the-artbenchmarks. Second, we handle the d-dimensional budget problem by combining ReSearchwith a coordinate descent method, presenting theoretical guarantees and experiments.",
  "Introduction": "Consider the following fundamental question: given a convex real-valued function f, how can we efficientlyand sequentially select oracle queries of it in order to recommend a point x such that f(x) is as close aspossible to the infimum of f? This problem is known as zeroth order convex optimization and has beenstudied for more than half a century (Rosenbrock, 1960). The field has also recently attracted the interest ofthe machine learning and statistical community because computing the gradient of a function that dependson a large dataset (e.g., the empirical risk) can be very expensive if not unfeasible (see for example Bubecket al. 2021 and references therein). Another significant application arises in simulation-based optimization,where the goal is to optimally tune the parameters of a system by only observing its output (Conn et al.,2009; Spall, 2005).",
  "Published in Transactions on Machine Learning Research (10/2024)": "holds trivially when xi1 {li,ci,ri}. When this is not the case, since ReSearch discarded xi1 at the endof some previous epoch, it either holds that Ii (,xi1] or Ii [xi1,). If x xi1 (meaning that=), it follows from x Ii that Ii (,xi1], which implies x xi < xi1. Analogously, if x > xi1(meaning that =>), it follows from x Ii that Ii [xi1,), which implies x > xi > xi1. This provesequation 6. Moreover, as a direct consequence of equation 4 and equation 6, we obtain equation 7.",
  "Contributions.We make the following contributions:": "We design a novel zeroth-order budget optimization setting where the oracle answers each query xwith an interval that is guaranteed to contain f(x) and whose length decreases with the amount ofbudget invested on x so far. In addition to generalizing the deterministic and stochastic settings, ourmodel also captures the aforementioned problems not covered by them. (.)",
  "Related Work.Zeroth-order convex optimization is a massive field with vast literature. We limit ourdiscussion to references more closely aligned with the scope of this paper": "The deterministic case is the simplest setting in zeroth-order optimization, where the oracle answers eachquery x with the exact value of the objective f(x) (see Nesterov et al. 2018 and references therein). Althoughnot the core of our work, we highlight that in this setting, our one-dimensional algorithm ReSearch achievesthe well-known optimal geometric decay on the optimization error while not requiring the objective to beglobally Lipschitz. To the best of our knowledge, our flexible budget setting with errors decaying as functions of the budget isnot addressed theoretically in the convex optimization literature. The more specific stochastic setting, wherethe oracle answers queries with random independent estimates of the objective, is studied in particular byAgarwal et al. (2013); Jamieson et al. (2012); Shamir (2013); Belloni et al. (2015). Our budget setting recoversit as a special case when errors decay as O(1/budget). In the one-dimensional case, an optimization error of(1/ T) is unavoidable in the stochastic setting, even knowing the time-horizon T in advance and under theadditional assumptions of smoothness and strong-convexity (see Shamir 2013, Theorem 3). This rate is alsoachieved by Belloni et al. (2015) and Lattimore (2020), with high-probability and in-expectation respectively,up to extra log terms; however, these algorithms are quite involved. Agarwal et al. (2013) and Jamiesonet al. (2012) propose simpler and more practical trisection-based algorithms with similar optimization errorguarantees. While these algorithms share some features with ReSearch (e.g., they monitor confidence-intervalseparation to discard domain portions), our analysis departs substantially from those of Agarwal et al. (2013)and Jamieson et al. (2012), leading to additional theoretical benefits (in particular, a negligible dependenceon the Lipschitz constant and an improved logarithmic dependence on the time horizon T). Finally, in",
  "Setting": "Given a bounded convex set I Rd, our goal is to minimize an unknown convex function fI R pickedby a possibly adversarial and adaptive environment by only requesting fuzzy evaluations of f. At everyinteraction t, the optimizer selects a query point Xt and the environment selects and reveals a budget bt.This budget is then used to reduce the fuzziness on the value of f(Xt), modeled by an interval Jt f(Xt). Inother words, the reader might think of the budget as a perishable (must be spent in full at every interaction)and non-divisible (all must be spent in a single query point) amount of resources made available by theenvironment to reduce the fuzziness of the value of the unknown objective at the current query point.",
  ":The optimizer recommends a point Rt I": "We stress that the environment is adaptive. Indeed, the intervals Jt that are given as answers to the queriesXt can be chosen by the environment as an arbitrary function of the past history, as long as they representfuzzy evaluations of the convex objective f (i.e., as long as f(Xt) Jt for all t). Note that optimization would be impossible without further restrictions on the behavior of the environment,since an adversarial convex environment could return Jt = R for all t N, making it impossible to gather anymeaningful information. We limit the power of the environment by relating the amount of budget invested ina query point Xt with the length of the corresponding fuzzy representation Jt of f(Xt), as quantified by thefollowing assumption.",
  "An optimal algorithm for the univariate case": "In this section we study the univariate budget convex optimization problem, i.e., the case when the underlyingconvex set I R is a bounded interval. To solve this problem we propose ReSearch, an algorithm exploitingthe 1-dimensional nature of the problem by following a query strategy that allows the learner to recycle mostof the past queries (Algorithm 2).",
  ": The uniform (u) and non-uniform (/u) partition functions applied to the interval I =": "will delete. Additionally, we will consider the element representing the case where no parts of theactive interval will be deleted. Let J be the set of all intervals, and I J that of all bounded intervals.Furthermore, for any interval J J , let J = inf(J) and J+ = sup(J). ReSearch relies on four auxiliaryfunctions: the delete function, the uniform partition function u, the non-uniform partition function /u, andthe update function. The delete function (see )",
  "if Jc J+r , elseif Jc J+l , elseif Jl min(J+c ,J+r ) & Jr min(J+l ,J+c ), elseif Jl min(J+c ,J+r ), elseif Jr min(J+l ,J+c ), else": "In words, the intervals Jl,Jc,Jr will represent the fuzzy evaluations of three points l < c < r in the domain ofthe unknown objective (left, center, and right). Since we are assuming that the objective is convex, notethat whenever an upper bound on the value of the objective at a point x is lower than the lower bound atanother point y that is left (resp., right) of x, then, all points that are left (resp., right) of y (y included) areno better than x. Therefore, the function delete returns which part of an interval containing three distinctpoints l < c < r should be deleted given the fuzzy evaluations Jl,Jc,Jr. (E.g., represents the deletion ofall points of the active interval left of c, represents the deletion of all points of the active interval rightof r, is returned when the fuzzy evaluations are not sufficient to delete anything, etc.)",
  "I+], u)": "In words, when applied to an interval I, a type of partition , and a symbol del (representing the subset of Ito be deleted), the update function returns as the first component the interval I pruned of the subset of Ispecified by and del, and, as the second component, how the new interval will be partitioned. It can beseen that the types of partitions returned by update are chosen so that our ReSearch algorithm will onlyquery points on a (rescaled) dyadic mesh (e.g., if I = , ReSearch will only query points of the form k/2h,for k,h N). For all t N, if the sequence of budgets picked by the environment up to time t is b1,...,bt, the se-quence of query points selected by the optimizer is X1,...,Xt, the corresponding feedback is J1,...,Jt (seeOptimization Protocol 1), then, for each x R, we define the quantities",
  "Bx,t =ts=1bsI{Xs = x}andJx,t =s[t],Xs=xJs": "with the understanding that Jx,t = R whenever Xs x for all s [t]. Furthermore, define Bx,0 = 0 for allx R. In words, Bx,t is the total budget that has been invested in x by the optimizer up to and includingtime t, while Jx,t is the best fuzzy evaluation of the unknown objective at x that is available at the end oftime t. The pseudocode of ReSearch is provided in Algorithm 2. ReSearch proceeds in epochs where it maintainsan active interval I and three query points l,c,r I. During each epoch , it repeatedly queries a pointin {l,c,r} where it invested the least amount of budget until the function delete has gathered enoughinformation to prune the current active interval. When this happens, first it updates the active interval andthe type of partition using the update function. Then, it computes the three query points l+1,c+1,r+1 ofthe next epoch + 1. Notably, among l+1,c+1,r+1 there will be the point among l,c,r that has thesmallest value of f (or one of them, if there are more than one). Afterwards, the algorithm recommends apoint x {l+1,c+1,r+1} with the best known upper bound J+x,t on the value of f(x) available at the presenttime t,1 and concludes the current epoch. In all rounds in which function delete has not yet gathered enoughinformation to prune the current active interval, the algorithm makes different recommendations dependingon whether or not the amount of budget invested in the current epoch is higher than the amount of budgetspent in all past epochs combined. See for an illustration of how ReSearch works. We stress that ReSearch is any-time (it does not need to know the time horizon T a priori), any-budget(it does not need to know the total budget B = Tt=1 bt) and does not require the unknown objective to beLipschitz. Nevertheless, we will show in Theorems 3.1 and 3.2 that its performance is guaranteed to benear-optimal even when compared to algorithms with full knowledge of T and B, and run on convex Lipschitzfunctions with known Lipschitz constant.",
  "We now provide theoretical guarantees for ReSearch": "1Under Assumption 2.1, this corresponds to recommending a point x {l, c, r} with the best known upper bound J+x,t onthe value of f(x) that will survive as a query point of the next epoch. Indeed, for x {l+1, c+1, r+1} {l, c, r}, we haveJ+x,t = +, since x has never been evaluated. On the other hand, any x {l, c, r} has already been evaluated, hence J+x,t < .",
  "x": ": A run of ReSearch. Here, the function is piece-wise linear and its graph is in thick black. Thehorizontal segments are the active intervals I of consecutive epochs . The short vertical segments are thecurrent query points l,c,r of epoch , and the dots (prolonged down vertically) are the recommendationsat the end of each epoch, that converge towards x. Note that, from one epoch to the next, two out of threepoints are kept (together with their guarantees), maximizing the recycling of past information.",
  "where L is the local Lipschitz constant of f on [lT ,rT ]": "The full proof of this result can be found in Appendix A. Before presenting a sketch of it here, we make a fewremarks. First, note that the bound is non-trivial even when the function is not globally Lipschitz (as it isthe case, e.g., for the function f(x) = 1 x2 defined on the interval I = ), since it depends on a localLipschitz constant L (which is always finite) that, informally, as the epochs go by, captures better and betterhow much the function varies around the points that are close to the minimum.2 Second, note that (up tothe constants c1,c2,c3) the bound consists of two terms. The first term c/(Tt=1 bt) is a consequence of the fuzziness of the evaluations, that is regulated by Assumption2.1: when Tt=1 bt 1, it decreases when increases or c decreases. Moreover, when c = 0 and bt = 1 forall t [T], our problem reduces to deterministic convex optimization. In this case, the first term vanishescompletely, leaving behind only the known optimal exponentially-decaying rate LIe(T ) for deterministicconvex optimization. The second term LIe(Tt=1 bt/ maxt[T ] bt) is a consequence of the discrete nature of our setting. Notably, ifthe optimizer could choose to invest infinitesimally small budgets bt (i.e., if the discrete optimization protocolbecame a continuous one), the term would vanish completely. Strikingly, when this is the case, the boundbecomes completely independent of the Lipschitz constant L. To the best of our knowledge, this is the firstresult in convex optimization that shows how the dependence on L could be entirely lifted if we transitionedfrom a discrete to a continuous setting. In other words, our bound gives a parameterization of the dependenceon the Lipschitz constant in terms of how close our setting is to a continuous one. The high-level reason forthis behavior is that, in a discrete setting, the optimizer might be forced to spend a large amount of budgetbt on a point Xt where a significantly smaller investment would have been sufficient to determine whether ornot that point was suboptimal. In this case, if the function is varying significantly, the number of queriescould not be sufficient to get close to a minimizer, and this would yield an optimization error that scales withL. Finally, we note that, naturally, the Lipschitz constant L and the domain length I appear as a product.Indeed, shrinking (resp., dilating) the domain of a function fI R corresponds (inversely-proportionally) toan increase (resp., decrease) of the Lipschitz constant.",
  "Proof sketch. We divide the analysis in the 3 cases sketched below, depending on how ReSearch selects RT": "1. delT . In this case, we partition the number of epochs in several classes and focus our attentionon the class where we invested the highest fraction of the total budget Tt=1 bt. Say that this classcontains n epochs. If n is small, we show that in the last epoch of this class there exist two querypoints that are near-optimal and that the recommendation RT of ReSearch has guarantees that areclose to those of these two near-optimal points. If, on the other hand, n is large, the result follows bythe local Lipschitzness of f. 2. delT = and the majority of the budget was invested in the last epoch. In this case, we splitagain the analysis in two further cases. If the maximum budget maxt[T ] bt is small, we show that allthree query points of the last epoch are near-optimal, therefore so is the recommendation RT . If, onthe other hand, the maximum budget maxt[T ] bt is large, we fall back again to the local Lipschitznessof the objective. 3. delT = and the majority of the budget was invested before the last epoch. Since in this case therecommendation RT is the same as the recommendation that ended the previous epoch, the resultfollows by Item 1, using half of the total budget.",
  "We gave the optimizer the freedom to select the time horizon T and total budget B ahead of time. We restricted the result to convex Lipschitz functions": "Note that both these changes make our results stronger, since ReSearch is able to match the lower bounddespite lacking the freedom to select T,B (in fact, being totally oblivious to a possibly adversarial choice ofboth) and Theorem 3.1 holds even for non-Lipschitz functions. Theorem 3.2. For any nondegenerate bounded interval I R, if the environment satisfies Assumption 2.1for some c 0 and > 0, then, there exist c1 1/4,c2 1/32e,c3 1 such that, for any time T N, every totalbudget B > 0, every Lipschitz constant L > 0, and every deterministic algorithm run by the optimizer, thereexists a sequence of budgets b1,...,bT such that Tt=1 bt = B and there exists a max (c",
  "Applications": "We present two notable applications of our method. First, we show how to apply ReSearch to the case ofunivariate stochastic convex optimization, improving on state-of-art bounds; remarkably and in contrast withprevious works, the algorithm does not require the Lipschitzness of the objective. Second, we illustrate howto address the multivariate budget case by combining ReSearch with a classic coordinate descent method (seeTseng 2001 and references therein) when the objective is smooth and strongly convex.",
  "Univariate Stochastic Convex Optimization": "In this section, we show how to apply ReSearch to the related problem of univariate stochastic convexoptimization (SCO). Typically, in this problem, one assumes that querying a point x returns an i.i.d.subgaussian (noisy) evaluation of the unknown objective f(x). Instead, we will introduce a more generalsetting where the key property is the concentration of the (averages of the) queried evaluations. This way, wecan recover the classic SCO but also obtain results for more general non-i.i.d. settings (see below).",
  "Note that, in classic SCO, where for each x I, the sequence (Yx,t)tN is i.i.d. and -subgaussian, Assumption4.1 is implied by Hoeffdings inequality for = 1/2, c() =": "82 ln(2/) (for all (0,1)) and m asthe empirical average. The case < 1/2 in Assumption 4.1 is relevant to model errors with long-rangedependence (Lahiri, 2003; Beran, 2017). To give a simple example, consider that (Yx,t)tN are Gaussianwith Cov(Yx,t,Yx,s) = 2(1 + t s) for t,s N and for a fixed (0,1). Then, it can be checked thatAssumption 4.1 holds with m as the empirical average, with = /2 and with c() =",
  ": The optimizer recommends a point RT I": "By running ReSearch with feedback Jt equal to a suitable confidence interval for f(Xt) (at each timet), we extend the state-of-the-art for stochastic convex optimization (Agarwal et al., 2013) beyond theglobally-Lipschitz case and, even where the previous guarantees held, we improve them in two ways: weremove a logarithmic factor from the bound and we only pay the Lipschitz constant in the non-dominatingterm, which decreases exponentially with T.",
  "Multivariate Budget Convex Optimization": "We now illustrate how to use ReSearch to address the multivariate budget setting (see Algorithm 5), wherethe objective f is defined on a convex bounded subset I Rd. We use an adaptation to the budget setting ofa coordinate descent method in the spirit of Jamieson et al. (2012). Coordinate descent is typically analyzedassuming that the objective f is strongly convex and smooth, which we also assume until the end of thesection. Algorithm 5 proceeds by performing sequential line searches. During line search k, it uses ReSearch along asegment determined by a base point xk and a randomly drawn axis i(k), recommending points based onthe recommendations of ReSearch. A line search is concluded as soon as the length of the active interval ofReSearch is . At the end of each line search, the base point xk+1 is updated using the best point found byReSearch. 3Because ReSearch recycles past observations multiple times, even after they have been used to decide which part of thecurrent interval to discard. (At a high level, the estimates of the values of the function at the points that we keep have thetendency to look better.) This recycling of information is a feature, not a flaw of the algorithm, as it allows reaching anapproximate minimizer with a smaller number of queries (see .2).",
  "(d) Multivariate budget: runtime": ": (a) The optimization error of ReSearch (black) is compared with the corresponding upper (red)and lower (blue) bounds. (b) The median optimization error of ReSearch for SCO (black) is compared withJamieson et al. 2012 (green) and Agarwal et al. 2013 (orange); shaded areas are Inter-Quartile Ranges. (c)The average optimization error of Algorithm 5 tuned as in Theorems 4.3 and D.1 (black) is compared withits corresponding upper bound (red) and alternative choices of : 1 (blue), 0.1 (purple), 0.01 (orange) see.3. Plots are in log scale. (d) The average runtime of Algorithm 5 is plotted against the dimensionof the problem see .3. All, but (d), plots are in log scale.",
  "We now make the following comments": "First, to put things into perspective, when = 1/2, we note that our rate d(d/T)1/2 for the budget settingis in line with state-of-the-art bounds in the related classic field of (i.i.d. subgaussian) stochastic convexoptimization (Jamieson et al., 2012), where confidence intervals shrink at the rate in Assumption 2.1, with = 1/2. Second, in convex optimization, strong convexity usually allows for unconstrained optimization (i.e., I = Rd);our method can be extended to this case by adding a pre-processing step before each line search that worksby doubling the search space sequentially on the given line until we can guarantee to contain the minimizer.For the sake of conciseness, we leave this standard step out of our presentation. Third, if one sets a target optimization error of , it is a consequence of Theorem D.1 that the runtime ofAlgorithm 5 is of the order of (d1+/)1/. This is due to the fact that Line 3 takes constant time for eachfunction query and that after T iterations of Algorithm 5, the optimization error is of the order of d1+/T (ignoring logarithmic factors). Furthermore, the algorithm, at each iteration, only needs to store three pointsfor ReSearch, the d-dimensional vector with the current direction, and finally the d pairs of interval limitsdelimiting the box-constraint. Thus, the overall space complexity is of the order of d. We believe that itis unlikely that in this setting one can obtains bounds that are independent from d. Indeed, in the relatedstochastic case, (Shamir, 2013) shows that at least a linear dependence on d is unavoidable. Finally, we highlight a remarkable benefit of coordinate descent frameworks (in particular, Algorithm 5): theytrade off some generality (by requiring strong convexity and smoothness) to gain an easy implementation andobtain efficiency on the two separate fronts of query (by featuring state-of-the-art optimization error bounds)and computational complexity (having low memory requirements and fast execution, irrespectively of thedimension).",
  "Experiments": "We present a preliminary experimental evaluation in support of our theoretical findings, illustrating thefollowing. First, the performance of ReSearch is in line with the theoretical guarantees and, in practice,closer to the lower (Theorem 3.2) than to the upper bound (Theorem 3.1). Second, Algorithm 4 significantlyoutperforms its natural competitors. Third, the role played by the threshold parameter in Algorithm 5,and its performance compared to the upper bound.",
  "Comparison with upper and lower bounds": "This experiment aims at comparing the performance of ReSearch with the corresponding theoretical upper(Theorem 3.1) and lower (Theorem 3.2) bounds. The oracle is based on the setting described in the firstpart of the proof of Theorem 3.2 with parameters c = 0.1, = 1/2,bt = 1 for all t. We test the performance ofReSearch on a grid of time horizons T {102,103,...,106} against the objective f constructed in the lowerbound. (a) shows that ReSearch performs well, appearing significantly (note the log scale) closer tothe lower than the upper bound.",
  "Stochastic Case": "This experiment ((b)) aims at showing the effectiveness of Algorithm 4 when compared to theuni-dimensional algorithms proposed in Agarwal et al. (2013) and Jamieson et al. (2012), which are state-of-the-art for this setting. To this end, we optimize the function f(x) = 1 2x2 over I = ; notice thatf is 1-Lipschitz over I and its minimum value is 0. The stochastic oracle is implemented according toStochastic Optimization Protocol 3 with independent Gaussian noise with mean 0 and variance 2 = 0.1.This corresponds to having c() =",
  "These experiments illustrate the performance of Assumption 5 in the multivariate budget case. We minimizethe function f(x) = 1": "2x22 over [1,3d]d for d = 10, which has the minimum close to a corner, but notexactly there. We implemented the oracle according to Optimization Protocol 1 and Assumption 2.1 withc = 1, = 1 and bt = 1 for all t. In addition to using the threshold T recommended by the tuning inappendix D and the corresponding theoretical upper bound, we also run algorithm 5 using {1,101,102}for T {102,103,...,106}. We repeat each run 10 times and report the average optimization error and thestandard deviation (note the high concentration: paths have little to no variance). (c) shows theresults (in logarithmic scale), highlighting that the tuning of the threshold T dictated by the theory can bea little more conservative in some cases than some ad hoc choices of , but it is still far better than the upperbound (which is on par with state-of-the-art bounds for analogous settings) and a random guess of . In a second experiment, we considered the same problem setting and measured the runtime of the algorithmagainst the dimension, varying the dimension d in the interval with a step of 10. We set a targetoptimization error 0.1 and run the algorithm until it reaches it. In (d) we reported the averageruntime across 10 repetitions. As predicted by the theory, the runtime follows the predicted d2 behavior andremarkably the execution for d = 100 takes less than a second.",
  "Conclusions": "We designed and studied a flexible zeroth-order convex optimization setting, where the accuracy of the queriesimproves with the invested budget (Assumption 2.1). This framework grants additional modeling power (see) compared to standard stochastic settings. In dimension one, we designed an any-time, any-budget,parameter-free algorithm, called ReSearch (Algorithm 2), that does not require the a priori knowledge ofthe (local) Lipschitz constant of the objective and works even in an adversarial and adaptive environment.We provided upper and lower bounds on the optimization error that match up to constants. We provided anatural adaptation of ReSearch to the stochastic setting together with a corresponding upper bound featuringthe same mild dependence on the Lipschitz constant, and further improved the state-of-the-art for thissetting by a logarithmic term. Finally, in the multivariate budget setting, we illustrated the benefits of usingReSearch as a line search procedure in a coordinate descent method: this results in a numerically efficientand practicable budget optimizer that can run even in high-dimensional spaces.",
  "In this section, we give a detailed proof of Theorem 3.1": "Proof of Theorem 3.1. Fix any bounded interval I R, a time T N, and a convex function fI R. Withoutloss of generality, we can (and do!) assume that I contains at least two distinct points.4 Moreover, withoutloss of generality, we can also (and do!) assume that f attains its minimum in I.5 Then, note that the activeinterval I of any epoch [T ] (defined in the initialization and updated at Line 8) always contains at leasta minimizer, because the first active interval I1 is the entire domain I and, by the unimodality of f andthe definition of the delete and update functions, ReSearch deletes a fraction of the active interval (Line 8)only if it is certain that the value of f at one of the remaining points is no bigger than all of the deletedpoints. Thus, there exists (and we fix for the rest of the proof) an x I such that x IT ... I1 andf(x) = min(f). Recall that, for all t N, t is the epoch of round t (Line 5 of Algorithm 2). Also, for thesake of convenience, we define t0 = 0 and, if the last epoch is not concluded exactly at the end of time T, weredefine tT = T and BT = BT ,T .",
  "To prove the result, we analyze separately the performance of the recommendation RT of ReSearch in thethree cases of Lines 10, 13 and 15 in Algorithm 2": "Assume at first that delT (i.e., the condition on Line 6 is true and we recommend RT as in Line10). We partition the epochs [T ] into four sets, depending on whether or not the epoch is uniform andwhether or not x c. More precisely, for any {u, /u} and {,>}, we let A, be the set of all epochs [T ] such that = and x c. (or, equivalently stated, that there exist at least two distinct elementsx,y {l,c,r} such that x x y). Now fix A = A,, where",
  "yi x yi1 xif 2 i n .(7)": "Here, equation 3 follows directly by the definition of the update function, noting that there are never twouniform or two non-uniform epochs in a row, unless half of the current interval is eliminated in one single callof the update function. Inequality equation 4 follows directly from equation 3. Inequality equation 5 is aconsequence of the definitions of the partition functions u and /u. To prove equation 6, note first that the claim 4Otherwise, the optimization error is trivially zero.5Indeed, if it does not, then there exists x {I, I+} such that limxx,xI f(x) = infxI f(x) (this can only happen if I isnot closed or f is discontinuous at x; in the latter case, note that by convexity, f(x) > limxx,xI f(x) = infxI f(x)). Thus,noting that ReSearch never queries nor recommends the endpoints {I, I+} of I, one can replace f with f, where f(x) = f(x)for all x I and f(x) = infxI f(x). This way, up to extending (or redefining) f at x, we are left with a convex function f suchthat f(X1) J1, . . . , f(XT ) JT , attains its minimum at x (in its domain), and satisfies f(RT ) infxI f(x) = f(RT ) f(x).",
  ",(9)": "i.e., that the total budget Bx, t k spent on any query point x {l,c,r} up to time t k is no smaller (up to(k)maxt[T ] bt) than the budget B spent (on all query points) only during epoch . Indeed, for any [T ]and k {0,...,t}, letting xmin argminx{l ,c ,r } Bx, t k be a query point where the algorithm spent theleast amount of budget up to time t k and M = {x {l,c,r} Bx,t k Bxmin,t k maxt[T ] bt} be theset of all query points in which the algorithm spent a budget close to that of xmin, we have",
  "B (2 + k)maxt[T ] bt": "with the understanding that any sum js=i zs is equal to zero whenever i > j, and where in () we used thefact that, if x {l,c,r} is such that Bx,t k Bxmin,t k > maxt[T ] bt, then ReSearch never queried x inepoch up to time t k, i.e.,t kt=t1+1bt = 0.",
  "(Tt=1 bt) .(16)": "Recall that the derivations for equation 15 and equation 16 were carried out under the assumption thatf(yn) f(xn) > 0. If this assumption does not hold, by convexity, f(yn) = f(xn) = f(x), thereforeequation 15 and equation 16 are still (trivially) true. Now, we will prove that the recommendation RT is approximately at least as good as xn and yn. Sinceunder the assumption that Tt=1 bt 24nmaxt[T ] bt we showed in equation 15 and equation 16 that xn andyn are both near-minimizers, this will yield under the same assumption that RT is also a near-minimizer.",
  ". If RT {xn,yn}, and {xn,yn} {lT ,cT ,rT }, then there exists x {lT ,cT ,rT } {RT } ={xn,yn} such that f(RT ) J+RT ,T Jx,T f(x); therefore, equation 1 follows by equation 15 andequation 16": "3. If RT {xn,yn}, and {xn,yn} {lT ,cT ,rT }, then, since at least one between xn and yndoes not belong to the set of active query points {lT ,cT ,rT } at time T, there exist a past timet [T] and a past query point x {lt,ct,rt} such that J+x,t max(Jxn,t, Jyn,t); therefore, notingthat the sequence s minx{ls,cs,rs} J+x,s is non-increasing, we have f(RT ) J+RT ,T J+x,t max(Jxn,t, Jyn,t) max(f(xn), f(yn)) and equation 1 follows by equation 15 and equation 16.",
  "maxt[T ] bt) ,": "where L is the smallest between the Lipschitz constants of f on [lT ,rT ] and [lT +1,rT +1] indeed, byconvexity, L is a Lipschitz constant for f on the convex hull of {x,lT ,rT } (resp., {x,lT +1,rT +1}) ifand only if it is a Lipschitz constant on [lT ,rT ] (resp., [lT +1,rT +1]). Putting everything together yieldsequation 1 when delT . Assume now that delT = and BT T 1 =0 B (i.e., the condition on Line 12 is true and we recommendRT as in Line 13). This implies that the three intervals JlT ,T ,JcT ,T ,JrT ,T have non-empty intersection,which in turn implies that",
  "maxx{lT ,cT ,rT }J+x,T minx{lT ,cT ,rT }Jx,T 2maxx{lT ,cT ,rT }Jx,T .(17)": "Now, we define f1,f2,f3,f4 as the four functions whose graphs are straight lines such that f1 passes throughthe points (lT ,JlT ,T ) and (rT ,J+rT ,T ), f2 passes through the points (cT ,Jc ,T ) and (rT ,J+rT ,T ), f3passes through the points (lT ,J+lT ,T ) and (cT ,JcT ,T ), and f4 passes through the points (lT ,J+lT ,T ) and(rT ,JrT ,T ) ().",
  "f": ": A representation of the four lines f1,...,f4. By convexity, f is lower bounded by the blue solidsegments. Note that, since JlT ,T JcT ,T JrT ,T , then f1,f2 are nondecreasing and f3,f4 nonincreasing.Therefore, the minimum of the y coordinates of the red dots is a lower bound on the value of the function,which in turn implies equation 18. By the convexity of f, for each x [IT ,lT ] we have f(x) f1(x), for each x [lT ,cT ] we have f(x) f2(x),for each x [cT ,rT ] we have f(x) f3(x), and for each x [rT ,I+T ] we have f(x) f4(x). Writing downexplicitly these four inequalities and upper bounding, we conclude that",
  "In this section, we give a detailed proof of Theorem 3.2": "Proof. Fix a nondegenerate bounded interval I R. Fix also an horizon T N, a total budget B > 0, and aLipschitz constant L > 0. For each t [T], define bt = B/T. We divide the proof in two cases, depending onwhich of the two addends in equation 2 is the dominant term.",
  "B": "At each time t [T], if the algorithm chosen by the optimizer queried X1,...,Xt, the environment returnsthe fuzzy evaluation Jt = J(BXt,t), where we recall that Bx,t was defined, for any x I, by ts=1 bsI{Xs = x}.Note that the environment satisfies Assumption 2.1 and that both functions f arec IB -Lipschitz. Moreover,the algorithm provides the same queries and recommendations for both f and f +, as it receives the sameJ1,...,JT . Furthermore, if the algorithm recommends RT (I +I+)/2 then f(RT )infxI f(x) c/(2B),while if the algorithm recommends RT < (I + I+)/2 then f+(RT ) infxI f+(x) c/(2B). Thus, in bothcases there exists ac",
  "t }": "and note that t[T ]{f(Xt) Jt} E. By De Morgans laws, a union bound, and Assumption 4.1, we haveP[Ec] 10T 2. Now, if we are in the good event E, then Assumption 2.1 holds for all t [T], with c = c()and b1 = = bT = 1, since ReSearch queries points only in D. To prove the last claim, consider the budgetversion of ReSearch. Recall the observation (at the beginning of the proof in Appendix A) that the minimizerx of f belongs to all active intervals of ReSearch at all epochs. Assume by contradiction that there existsan epoch [T ] such that {l,c,r} is not included in D. Then, the set of query points {l,c,r} is notincluded in Dn (0,1) when n = log2((r l)/2). Consider the case where r Dn (0,1) and r > xn,10(the other cases can be analyzed analogously). Since the leftmost point I of the active interval of epoch isalways bigger than or equal to r 4 2n, then",
  "DMissing details on .2": "We now consider the minimization of a multivariate objective over a convex bounded subset I Rd, whereOptimization Protocol 1 has bt = 1 for each t and Assumption 2.1 holds. In this section, we refer to thissetting as Multivariate Budget Convex Optimization (MBCO) for the sake of emphasis. We assume theobjective to be strongly convex and -smooth with 0 < i.e., x,x0 Rd",
  "x x02": "where the first equation corresponds to the strong convexity and the second to the smoothness. For simplicity,we further assume the existence of a unique minimizer6 x int(I). As it will be apparent later, it is notnecessary to assume that x is in the interior of I: the entire analysis that follows holds if the objective isLipschitz, a condition that is implied by the strong convexity and smoothness. This cosmetic choice allows usto replace the local Lipschitz constant appearing in equation 1 with . Before providing the detailed version of Theorem 4.3, let us provide some intuition on the bounds appearingthere. Given some budget T for a line search subroutine, the bound of equation 1 scales roughly as 1/ T ,which implies that the point recommended by ReSearch is at distance at most 1/ T /2 from x due to strong",
  ".(20)": "Proof of Theorem D.1. Take ,,, and as defined in Theorem D.1. Let g be a univariate functionobtained by considering f on a segment I of I. We can upper bound the Lipschitz constant of the function gusing the global Lipschitz constant L of f, which, given that we are assuming that x I, can be furtherupper bounded by diam(I). We start noticing that, assuming bt = 1 for all t and c = 1, the bound ofequation 1 can be bounded from above by 2c1/T whenever",
  ")": "1 , then itfollows that the point xT found by ReSearch satisfies xT x . Now let f be a multi-variate functionsatisfying the assumption of Theorem D.2. Notice that any restriction fk of f along a coordinate line k, willalso satisfies the same assumptions over the interval Ik (see line 3 of Algorithm 5).",
  "k0 + 2d2": "Recalling that B() is the worst-case number budget required by ReSearch to find an -optimizer, andif we are given a total budget of BT , the number of epochs made by Line 1 of Algorithm 5 is at leastK(T,) = BT /B(). From this we get the bound of equation 23. Now notice that",
  "we obtain equation 20": "In the following we also present a generalization of the above theorem to arbitrary budgets bt, > 0 and c.In the context of the next theorem B() (defined in the proof) is meant to be the worst case total budgetrequired to find an -optimizer at a given epoch, BT is the total budget and K(T,) = BT /B().",
  ".(24)": "Proof of Theorem D.2. Take ,,, and as defined in Theorem D.2. Let g be a univariate functionobtained by considering f on a segment I of I. We can upper bound the Lipschitz constant of the function gusing the global Lipschitz constant L of f, which, given that we are assuming that x I, can be furtherupper bounded by diam(I). We start noticing that, using bT = maxt[T ] bt, the bound of equation 1 canbe bounded from above by 2c1c/(Tt=1 bt) whenever",
  "EAdaptive Lipschitz constants": "This experiment aims to show the advantages of featuring the local Lipschitz constant, as given in Theorem3.1, over a bound that uses the global constant. To this end, we optimize the function f(x) = x + 1 overthe interval (a,1) with a > 0. The Lipschitz constant of f grows roughly as 1/a. Moreover, f(x) = 0 andthe maximum value is smaller than 1, thus any meaningful upper bound on the optimization error should besmaller than 1. In the experiment we set a = 0.001, c = 0.1, = 1 (Assumption 2.1), bt = 1 (for all t), and welet the intervals Jt to be symmetric around f(x) for a query at x at time t. We run ReSearch for T = 1000iterations. shows that the upper bound featuring the local Lipschitz constants (local) is much tighter than thatfeaturing the global constant (global). In particular, as denoted by the vertical lines, the former falls below 1after 84 iterations, while the latter becomes non-trivial only at iteration 339."
}