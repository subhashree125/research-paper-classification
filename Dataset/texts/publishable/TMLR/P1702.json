{
  "Abstract": "One-step text-to-image generator models offer advantages such as swift inference efficiency,flexible architectures, and state-of-the-art generation performance. In this paper, we studythe problem of aligning one-step generator models with human preferences for the firsttime.Inspired by the success of reinforcement learning using human feedback (RLHF),we formulate the alignment problem as maximizing expected human reward functions whileadding an Integral Kullback-Leibler divergence term to prevent the generator from diverging.By overcoming technical challenges, we introduce Diff-Instruct++ (DI++), the first, fast-converging and image data-free human preference alignment method for one-step text-to-image generators. We also introduce novel theoretical insights, showing that using CFGfor diffusion distillation is secretly doing RLHF with DI++. Such an interesting findingbrings understanding and potential contributions to future research involving CFG. In theexperiment sections, we align both UNet-based and DiT-based one-step generators usingDI++, which use the Stable Diffusion 1.5 and the PixelArt- as the reference diffusionprocesses. The resulting DiT-based one-step text-to-image model achieves a strong AestheticScore of 6.19 and an Image Reward of 1.24 on the COCO validation prompt dataset. Italso achieves a leading Human preference Score (HPSv2.0) of 28.48, outperforming otheropen-sourced models such as Stable Diffusion XL, DMD2, SD-Turbo, as well as PixelArt-. Both theoretical contributions and empirical evidence indicate that DI++ is a stronghuman-preference alignment approach for one-step text-to-image models. The homepage ofthe paper is:",
  "Introductions": "In recent years, deep generative models have achieved remarkable successes across various data generationand manipulation applications (Karras et al., 2020; 2022; Nichol & Dhariwal, 2021; Oord et al., 2016; Hoet al., 2022; Poole et al., 2022; Hoogeboom et al., 2022; Kim et al., 2022; Tashiro et al., 2021; Kingma &Dhariwal, 2018; Chen et al., 2019; Meng et al., 2021; Couairon et al., 2022; Zhang et al., 2023a; Luo &Zhang, 2024; Xue et al., 2023; Luo et al., 2023c; Zhang et al., 2023b; Feng et al., 2023; Deng et al., 2024; Luoet al., 2024c; Geng et al., 2024; Wang et al., 2024; Pokle et al., 2022). These models have notably excelled inproducing high-resolution, text-conditional models such as images (Rombach et al., 2022; Saharia et al., 2022;Ramesh et al., 2022; 2021; Luo et al., 2024b) and other modalities with different applications(Brooks et al.,2024; Kondratyuk et al., 2023; Evans et al., 2024; Luo et al., 2024c), pushing the boundaries of ArtificialIntelligence Generated Content. Among the spectrum of deep generative models, one-step generators have emerged as a particularly efficientand possibly best performing (Zheng & Yang, 2024; Kim et al., 2024; Kang et al., 2023; Sauer et al., 2023a)generative model. Briefly speaking, a one-step generator uses a neural network to directly transport somelatent variable to generate an output sample. Recently, there have been many fruitful successes in trainingone-step generator models by distilling from pre-trained diffusion models (aka, Diffusion Distillation (Luo,",
  ": Images generated by a one-step text-to-image generator that has been aligned with human prefer-ences using Diff-Instruct++. We put the prompt in Appendix D.1": "2023)) in domains of image generations (Salimans & Ho, 2022; Luo et al., 2024a; Geng et al., 2023; Songet al., 2023; Kim et al., 2023; Song & Dhariwal, 2023; Zhou et al., 2024b), text-to-image synthesis (Menget al., 2022; Gu et al., 2023; Nguyen & Tran, 2023; Luo et al., 2024b; 2023a; Song et al., 2024; Liu et al.,2024b; Yin et al., 2023), data manipulation (Parmar et al., 2024), etc. However, current one-step text-to-image generators face several limitations, including insufficient adherenceto user prompts, suboptimal aesthetic quality, and even the generation of toxic content. These issues arisebecause the generator models are not aligned with human preferences. In this paper, we study the problemof aligning one-step generator models with human preferences for the first time. We achieve substantialprogress by training generator models to maximize human preference reward. Inspired by the success ofreinforcement learning using human feedback (RLHF) (Ouyang et al., 2022) in aligning large languagemodels, we formulate the alignment problem as a maximization of the expected human reward function withan additional regularization term to some reference diffusion model. By addressing technical challenges, weobtain effective loss functions and formally introduce Diff-Instruct++, an image data-free method to trainone-step text-to-image generators to follow human preferences. In the experiment part, we demonstrate the strong compatibility of Diff-Instruct++ on both the UNet-baseddiffusion model and one-step generator such as Stable Diffusion 1.5, and the DiT-based (Peebles & Xie, 2022)diffusion and generator such as PixelArt- (Chen et al., 2023). We first pre-train a one-step generator modelusing Diff-Instruct (Luo et al., 2024a), initialized with Stable Diffusion 1.5 and PixelArt-. We name thismodel an unaligned base generator model (base model for short). Next, we align the base model usingDI++ with an off-the-shelf Image Reward (Xu et al., 2023a) model using different configurations, resultingin various human-preferred one-step text-to-image models. The alignment process with DI++ significantly improves the generation quality of the one-step generatormodel with minimum computational cost. To evaluate our models from different perspectives, we conductboth qualitative and quantitative evaluations of aligned models with different alignment settings. In thequantitative evaluation, we evaluate the model with several commonly used quality metrics such as Huma-preference Score (HPSv2.0)(Wu et al., 2023), the Aesthetic score (Schuhmann, 2022), the Image Reward (Xuet al., 2023a), and the PickScore (Kirstain et al., 2023) and the CLIP score on the same 1k prompts fromMSCOCO-2017 validation datasets. Our main findings are: 1) the best DiT-based one-step text-to-imagemodel achieves a leading HPSv2.0 score of 28.48; 2) The aligned models outperform the unaligned ones",
  "Published in Transactions on Machine Learning Research (11/2024)": "The equation (B.19) recovers the so-called classifier-free guided score function.The final result is justan integral of the (B.19). Our results show that when using the classifier-free guided score for diffusiondistillation using Diff-Instruct (i.e. the equation (3.6)) is secretly doing RLHF (i.e. the Diff-Instruct++) byusing the reward (B.14). Besides, our results also bring a new perspective: when sampling from the diffusionmodel using CFG, the user is secretly doing an inference-time RLHF, and the so-called CFG scale is theRLFH strength.",
  "with significant margins in terms of human preference metrics; 3) The alignment cost is cheap, and theconvergence is fast; 4) The aligned model still makes simple mistakes": "We will discuss these findings in .3 in detail. We also establish the theoretical connections of DI++with diffusion distillation methods, as well as the classifier-free guidance (Ho & Salimans, 2022). In Theorem3.4 in .1, we show an interesting finding that the well-known classifier-free guidance is secretly doingRLHF according to an implicitly-defined reward function. Therefore, diffusion distillation with CFG can beunified within DI++. These theoretical findings not only help understand the behavior of classifier-freeguidance but also bring new tools for human preference alignment for text-to-image models. In .4, we discuss the potential shortcomings of DI++, showing that even aligned with humanreward, the one-step model still makes simple mistakes sometimes. Imperfect human reward and insufficienthyperparameter tunings possibly cause this issue.",
  "Preliminary": "Diffusion Models.In this section, we introduce preliminary knowledge and notations about diffusionmodels. Assume we observe data from the underlying distribution qd(x). The goal of generative modelingis to train models to generate new samples x qd(x). Under mild conditions, the forward diffusion processof a diffusion model can transform any initial distribution q0 = qd towards some simple noise distribution,",
  "t=0(t)Ex0q0,xt|x0qt(xt|x0) s(xt, t) xt log qt(xt|x0)22dt.(2.2)": "Here the weighting function (t) controls the importance of the learning at different time levels and qt(xt|x0)denotes the conditional transition of the forward diffusion (2.1). After training, the score network s(xt, t) xt log qt(xt) is a good approximation of the marginal score function of the diffused data distribution. Reinforcement Learning using Human Feedback.Reinforcement learning using human feedback(Christiano et al., 2017; Ouyang et al., 2022) (RLHF) is originally proposed to incorporate human feedbackknowledge to improve large language models (LLMs).Let p(x|c) be a large language models outputdistribution, where c is the input prompt that is randomly sampled from a prompt dataset C, and the x isthe generated responses. Let r(x, c) be a scalar reward model that probably has been trained with humanfeedback data and thus can measure the human preference on an image-prompt pair (x, c). Let pref(x|c)be some reference LLM model. The RLHF method trains the LLM to maximize the human reward with aKullback-Leibler divergence regularization, which is equivalent to minimizing:",
  "L() = EcC,xp(x|c) r(x, c)+ DKL(p(x|c), pref(x|c))(2.3)": "The KL divergence regularization term lets the model be close to the reference model thus preventing itfrom diverging, while the reward term encourages the model to generate outputs with high rewards. Afterthe RLHF, the model will be aligned with human preference. One-step Text-to-image Generator Model.A one-step (Goodfellow et al., 2014; Luo et al., 2024a; Yinet al., 2023; Zhou et al., 2024b; Luo et al., 2024b) text-to-image generator model is a neural network g(|),that can turn an input latent variable z pz(z) and an input prompt c to a generate image x (or somelatent vector before decoding as the case of latent diffusion models (Rombach et al., 2022)) with a singleneural network forward inference: x = g(z|c). Compared with diffusion models, the one-step generatormodel has advantages such as fast inference speed and flexible neural architectures.",
  "Human-preference Alignment of One-step Text-to-image Models": "In this section, we introduce how to align one-step text-to-image generator models and human preferencesusing Diff-Instruct++. In .1, we introduce the formulation of the alignment problem and thenidentify the alignment objective. After that, we address technical challenges and propose the Theorem 3.1and Theorem 3.2, which set the theoretical foundation of DI++ through the lens of reward maximization.Interestingly, we also find that the diffusion distillation using classifier-free guidance is secretly doing RLHFwith DI++. Based on the theoretical arguments in .1, we formally introduce the practical algorithmof DI++ in .3, and give an intuitive understanding of the algorithm as an education process thatincludes a teacher diffusion model, a teaching assistant diffusion model, and a student one-step generation.",
  "The Alignment Objective": "The Problem Formulation.We consider the text-to-image generation task. Other conditional generationapplications share a similar spirit. Assume x is an image and c is a text prompt that is sampled from someprompt dataset C. The basic setting is that we have a one-step generator g(|), which can transport a priorlatent vector z pz to generate an image based on input text prompt c: x = g(z|c). We use the notationp(x|c) to denote the distribution induced by the generator. We also have a reward model r(x, c) whichrepresents the human preference on a given image-prompt pair (x, c). Inspired by the success of reinforcement learning using human feedback (Ouyang et al., 2022) in fine-tuninglarge language models such as ChatGPT (Achiam et al., 2023), we first set our alignment objective tomaximize the expected human reward function with an additional Kullback-Leibler divergence regularization",
  "(3.2)": "We will give the proof in Appendix B.1. For gradient formula (3.2), we can see that the x gradient of r(x, c)is easy to obtain. If we can approximate the score function of both generator and reference distribution,i.e. x log p(x|c) and x log pref(x|c), we can directly compute the gradient and use gradient descentalgorithms to update the parameters . However, since the generator distribution is defined directly in imagespace, where the distributions are assumed to lie on some low dimensional manifold (Song & Ermon, 2019).Therefore, approximating the score function and minimizing KL divergence is difficult in practice. Diffusion Models are Reference Processes.Instead of minimizing the negative reward with KL regu-larization in (3.1), we turn to (3.3) by generalizing the KL divergence regularization to the Integral Kullback-Leibler divergence proposed in Luo et al. (2024a) w.r.t to some reference diffusion process pref(xt|t, c). Thisnovel change of regularization divergence distinguishes our approach from RLHF methods for large languagemodel alignments. Besides, such a change from KL divergence to IKL divergence makes it possible to usepre-trained diffusion models as reference processes, as we will show in the following paragraphs. Let xt be noisy data that is diffused by the forward diffusion (2.1) starting from x0. We use pref(xt|t, c)and sref(xt|t, c) to denote the densities and score functions of the reference diffusion process (the scorefunctions can be replaced with pre-trained off-the-shelf diffusion models). Let p(x|t, c) and s(xt|t, c) bethe marginal distribution and score functions of the generator output after forward diffusion process (2.1).We propose to minimize the negative reward function with an Integral KL divergence regularization:",
  "Classifier-free Guidance is Secretly Doing RLHF": "In previous sections, we have shown in theory that with available reward models, we can readily do RLHFfor one-step generators. However, in this part, we additionally find that the classifier-free guidance is secretlydoing RLHF, and therefore we will show that using CFG for reference diffusion models when distilling themusing Diff-Instruct is secretly doing Diff-Instruct++ with an implicitly defined reward.",
  "pref(xt|t) dt.(3.5)": "This reward function will put a higher reward to those samples that have higher conditional probability thanunconditional probability. Therefore, it can encourage samples with high conditional probability. We showthat the gradient formula (3.4) in Theorem 3.2 will have an explicit solution:Theorem 3.4. Under mild conditions, if we set an implicit reward function as (3.5), the gradient formula(3.4) in Theorem 3.2 with have an explicit expression:",
  ")sref(xt|t, c) sref(xt|t)": "We will give the proof in Appendix B.4. This gradient formula (3.6) recovers the case that uses the CFG fordiffusion distillation using the Diff-Instruct algorithm to train a one-step generator. The parameter (1+ 1 ) isthe so-called classifier-free guidance scale. In our Algorithm 1 and 2, we use the coefficient cfg to representthe CFG scale. In the following section, we formally introduce the practical algorithm of Diff-Instruct++.Remark 3.5. Theorem 3.4 has revealed a new perspective that understands classifier-free guidance as atraining-free and inference-time RLHF. This helps to understand why humans prefer samples generatedby using CFG. Besides, Theorem 3.4 also shows that using Diff-Instruct with CFG to distill text-to-imagediffusion models is secretly doing DI++. Therefore we can use both CFG and the human reward to strengthenthe one-step generator models.",
  "estimations for executing the algorithm. To address such an issue, we present a pseudo loss function definedas formula (B.13), which we show has the same gradient as (3.4) in Appendix": "With the pseudo loss function (B.13), we formally introduce the DI++ algorithm which is presented inAlgorithm 1 (and a more executable version in Algorithm 2). As in Algorithm 1, the overall algorithmsconsist of two alternative updating steps. The first step update of the TA diffusion model by fine-tuningit with student-generated data. Therefore the TA diffusion s(xt|t, c) can approximate the score functionof student generator distribution. This step means that theTA needs to communicate with the studentto know the students status. The second step estimates the parameter gradient of equation (3.2) and usesthis parameter gradient for gradient descent optimization algorithms such as Adam (Kingma & Ba, 2014)to update the generator parameter . This step means that the teacher and the TA discuss and incorporatethe students interests to instruct the student generator. Due to page limitations, we put a discussion aboutthe meanings of the hyper-parameters in Appendix A.2.",
  "Related Works": "RLHF for Large Language Models.Reinforcement learning using human feedback (RLHF) has wongreat success in aligning large language models (LLMs). Ouyang et al. (2022) formulates the LLM alignmentproblem as maximization of human reward with a KL regularization to some reference LLM, resulting inthe InstructGPT model. The Diff-Instruct++ draws inspiration from Ouyang et al. (2022). However, DI++differs from RLHF for LLMs in several aspects: the introduction of IKL regularization, the novel gradientformula, and the overall algorithms. Many variants of RLHF for LLMs have also been intensively studied inTian et al. (2023); Christiano et al. (2017); Rafailov et al. (2024); Ethayarajh et al. (2024), etc. Diffusion Distillation Through Divergence Minimization.Diffusion distillation (Luo, 2023) is aresearch area that aims to reduce generation costs using teacher diffusion models.Among all existingmethods, one important line is to distill a one-step generator model by minimizing certain divergencesbetween one-step generator models and some pre-trained diffusion models. (Luo et al., 2024a) first studythe diffusion distillation by minimizing the Integral KL divergence.Yin et al. (2023) generalize such aconcept and add a data regression loss to distill pre-trained Stable Diffusion Models. Many other workshave introduced additional techniques and improved the distillation performance (Geng et al., 2023; Kimet al., 2023; Song et al., 2023; Song & Dhariwal; Nguyen & Tran, 2023; Song et al., 2024; Yin et al., 2024;",
  "Experiments": "In previous sections, we have established the theoretical foundations for DI++. In this section, we considerone-step text-to-image models with two kinds of neural network architectures: the UNet-based one-steptext-to-image model with the well-known Stable Diffusion 1.5 (SD1.5)(Rombach et al., 2022) as the referencediffusion model, and the diffusion-transformer-based one-step model with the PixelArt- as the referencediffusion.For both models, we use DI++ to align the one-step models with human preferences usingImageReward(Xu et al., 2023a).",
  "As we have shown in , our experiment workflow consists of three modeling stages: the pre-trainingstage, the reward modeling stage (not necessary), and the alignment stage": "The Pre-training Stage.As the leftmost column of shows, the pre-training stage pre-trainsthe reference diffusion model, and a base one-step generator that is not necessarily aligned with humanpreference. The researcher can either train the diffusion model in-house or just use publicly available off-the-shelf diffusion models. With the pre-trained teacher diffusion model, we can pre-train our one-step generatormodel using diffusion distillation methods (Luo, 2023), generative adversarial training (Sauer et al., 2023a;Zheng & Yang, 2024), or a combination of them (Kim et al., 2023; Yin et al., 2024; Xu et al., 2024). Noticethat in the pre-training stage, we do not necessarily use human reward to instruct the one-step model,therefore the generator can only learn to match the reference distribution, leading to decent generationresults without strictly following human preference. The Reward Modeling Stage.As the middle column of shows, in the second stage, the re-searcher is supposed to collect human feedback data and train a reward model that reflects human preferencesfor images and corresponding captions. Notice that the image and caption data for this stage can either bereal image data or those images and prompts generated by users using the one-step generators in the firststage. For instance, if researchers want to enhance the generation quality of their users commonly usedprompts, they can collect the most used prompts from their users activity and generate images using theone-step generator pre-trained in the first stage. They can send the image-prompt pair to users for feedbackon their preferences. The reward modeling method is quite flexible. Researchers can either train the reward",
  "In this section, we quantitatively evaluate one-step models aligned with DI++ together with other text-to-image generative models": "Evaluation Metrics.We evaluate five widely used human preference scores: the human preference score(HPSv2.0) (Wu et al., 2023), the ImageReward(Xu et al., 2023a), the Aesthetic Score (Schuhmann, 2022),the PickScore(Kirstain et al., 2023), and the CLIPScore(Radford et al., 2015). Among them, the HPSv2.0has a standard prompt dataset for evaluations, therefore we follow its tradition to evaluate model scores.For the other four scores, we pick 1k text prompts from the MSCOCO-2017 validation dataset and evaluateall models on these prompts. DiT-based DI++ Model Shows the Best Performance.As shows, the DiT-DI++ modelshows a leading HPSv2.0 of 28.48 with only 0.6B parameters, outperforming other open-sourced modelswhich have sizes varying from 0.6B to 10+B. In , the DiT-DI++ model achieves an ImageRewardof 1.24 and an Aesthetic score of 6.19. Besides, in both and 2, the SD1.5-DI++ models showimproved scores than SD1.5 diffusion model, Hyper-SD, and SD1.5 diffusion DPO(Wallace et al., 2024). The Zero-shot Generalization Ability of Aligned Models.Another interesting finding of the DI++algorithm, as well as the human preference alignment of the one-step generator model, is its zero-shotgeneralization ability. As and 2 show, models aligned with Image Reward not only show a dominatingImage Reward metric but also show strong aesthetic scores and HPSv2.0. Such a zero-shot generalizationability indicates that if the models are aligned with a sufficiently good reward function with DI++, they canreadily generalize well to other human preference scores. Comparing with Other Few-step Generator Models. and 2 show the superior advantageof DI++ aligned one-step model over other few-step models. In this paragraph, we give for aqualitative comparison of our models with other few-step models in . When compared with otherfew-step generative models, the aligned model shows better aesthetic quality. Ablation Comparison shows an ablation comparison of the effects of Diff-Instruct++ withdifferent explicit reward scales r and classifier-free guidance reward scales c. For instance, for the SD1.5model, we find that a larger explicit reward scale always leads to higher Image Reward and AestheticScore, while it may trade off the CLIPScore which represents the image-text semantic alignment. Such anobservation indicates a trade-off between the image-text alignment and human-preference alignment if wesolely use human-preference reward as the implicit reward. One possible solution is to also add the image-textalignment scores such as the CLIPScore as an explicit reward. Besides, we also find that a CFG reward scalethat is too large might hurt the human-preference alignment. For instance, the SD1.5-DI++ one-step modelwith (r = 100, c = 1.5) shows better score than models with (r = 100, c = 4.5). This indicates thatthough CFG is practically useful in diffusion sampling, it is not perfectly aligned with human preferences.Therefore, finding a proper CFG reward scale is important for optimal human-preference alignment withDiff-Instruct++.",
  "ModelAnimationConcept-artPaintingPhotoAverage": "GLIDE Nichol et al. (2021)23.3423.0823.2724.5023.55LAFITE Zhou et al. (2022)24.6324.3824.4325.8124.81VQ-Diffusion Gu et al. (2022)24.9724.7025.0125.7125.10FuseDream Liu et al. (2021)25.2625.1525.1325.5725.28Latent Diffusion Rombach et al. (2022)25.7325.1525.2526.9725.78CogView2 Ding et al. (2022)26.5026.5926.3326.4426.47DALLE mini26.1025.5625.5626.1225.83Versatile Diffusion Xu et al. (2023b)26.5926.2826.4327.0526.59VQGAN + CLIP Esser et al. (2021)26.4426.5326.4726.1226.39DALLE 2 Ramesh et al. (2022)27.3426.5426.6827.2426.95Stable Diffusion v1.4 Rombach et al. (2022)27.2626.6126.6627.2726.95Stable Diffusion v2.0 Rombach et al. (2022)27.4826.8926.8627.4627.17Epic Diffusion27.5726.9627.0327.4927.26DeepFloyd-XL27.6426.8326.8627.7527.27Openjourney27.8527.1827.2527.5327.45MajicMix Realistic27.8827.1927.2227.6427.48ChilloutMix27.9227.2927.3227.6127.54Deliberate28.1327.4627.4527.6227.67Realistic Vision28.2227.5327.5627.7527.77SDXL-base(Podell et al., 2023)28.4227.6327.6027.2927.73SDXL-Refiner(Podell et al., 2023)28.4527.6627.6727.4627.80Dreamlike Photoreal 2.028.2427.6027.5927.9927.86 SD15-15Step(Rombach et al., 2022)26.7626.3726.4127.1226.66SD15-25Step(Rombach et al., 2022)27.0426.5726.6127.3026.88SD15-DPO-15Step(Wallace et al., 2024)27.1126.7526.7027.3026.97SD15-DPO-25Step(Wallace et al., 2024)27.5426.9726.9927.4927.25SD15-LCM-1Step(Luo et al., 2023a)23.3523.4123.5323.8123.52SD15-LCM-4Step(Luo et al., 2023a)26.4225.7925.9526.9126.27SD15-TCD-1Step(Zheng et al., 2024)23.3723.1623.2623.8823.42SD15-TCD-4Step(Zheng et al., 2024)26.6726.2526.2627.1926.59SD15-Hyper-1Step(Ren et al., 2024)27.7627.3627.4127.6327.54SD15-Hyper-4Step(Ren et al., 2024)28.0427.3927.4227.8927.69SD15-Instaflow-1Step(Liu et al., 2023)26.0725.8025.8926.3226.02SD15-PeReflow-1Step(Yan et al., 2024)25.7025.4525.5725.9625.67SD15-BOOT-1Step(Gu et al., 2023)25.2924.4024.6125.1624.86SD21-SwiftBrush-1Step(Nguyen & Tran, 2023)26.9126.3226.3727.2126.70SD15-SiDLSG-1Step(Zhou et al., 2024a)27.3926.6526.5827.3026.98SD21-SiDLSG-1Step(Zhou et al., 2024a)27.4226.8126.7927.3127.08SD21-TURBO-1Step(Sauer et al., 2023b)27.4826.8627.4626.8927.71SDXL-DMD2-1Step-1024(Yin et al., 2024)27.6727.0227.0126.9427.16SDXL-DMD2-4Step-1024(Yin et al., 2024)28.9727.9927.9028.2828.29SDXL-DMD2-1Step-512(Yin et al., 2024)27.7027.0727.0226.9427.18SDXL-DMD2-4Step-512(Yin et al., 2024)27.2226.6526.6226.5726.76SD15-DMD2-1Step-512(Yin et al., 2024)26.3125.7525.7826.5926.11PixelArt--25Step-512(Chen et al., 2023)28.7727.9227.9628.3728.25PixelArt--15Step-512(Chen et al., 2023)28.6827.8527.8728.2928.17SD15-DI++-1Step(r = 100, c = 1.5)28.4227.8428.0128.1928.12DiT-DI++-1Step(r = 10, c = 4.5)28.9128.2528.2828.5028.48",
  "ModelStepsType ParamsImageRewardAesScorePickScoreCLIPScore": "SD15-Base(Rombach et al., 2022)15UNet0.86B0.085.250.21230.99SD15-Base(Rombach et al., 2022)25UNet0.86B0.225.320.21631.13SD15-DPO(Wallace et al., 2024)15UNet0.86B0.205.290.21431.07SD15-DPO(Wallace et al., 2024)25UNet0.86B0.285.370.21831.25SD15-LCM(Luo et al., 2023a)1UNet0.86B-1.585.040.19427.20SD15-LCM(Luo et al., 2023a)4UNet0.86B-0.235.400.21430.11SD15-TCD(Zheng et al., 2024)1UNet0.86B-1.495.100.19628.30SD15-TCD(Zheng et al., 2024)4UNet0.86B-0.045.280.21230.43PeRFlow(Yan et al., 2024)4UNet0.86B-0.205.510.21129.54SD15-Hyper(Ren et al., 2024)1UNet0.86B0.285.490.21430.82SD15-Hyper(Ren et al., 2024)4UNet0.86B0.425.410.21731.03SD15-InstaFlow(Liu et al., 2023)1UNet0.86B-0.165.030.20730.68SD15-SiDLSG(Zhou et al., 2024a)1UNet0.86B-0.185.160.21030.04SDXL-Base(Rombach et al., 2022)25UNet2.6B0.745.570.226 31.83SDXL-DMD2-1024(Yin et al., 2024)1UNet2.6B0.825.450.22431.78SDXL-DMD2-1024(Yin et al., 2024)4UNet2.6B0.875.520.231 31.50SDXL-DMD2-512(Yin et al., 2024)1UNet2.6B0.365.030.21531.54SDXL-DMD2-512(Yin et al., 2024)4UNet2.6B-0.185.170.20629.28SD15-DMD2-512(Yin et al., 2024)1UNet2.6B-0.125.240.21130.00SD21-Turbo(Sauer et al., 2023b)1UNet0.86B0.565.470.22531.50PixelArt--512(Chen et al., 2023)25DiT0.6B0.826.010.22731.20PixelArt--512(Chen et al., 2023)15DiT0.6B0.826.030.22631.16SD15-DI++ (r=0, c=1.5)1UNet0.86B0.295.260.21630.64SD15-DI++ (r=100, c=1.5)1UNet0.86B0.465.440.21830.33SD15-DI++ (r=100, c=4.5)1UNet0.86B0.455.300.21731.00SD15-DI++ (r=1000, c=1.5)1UNet0.86B0.825.780.21930.30DiT-DI++ (r=0, c=4.5)1DiT0.6B0.745.910.22531.04DiT-DI++ (r=1, c=4.5)1DiT0.6B0.856.030.22430.76DiT-DI++ (r=10, c=4.5)1DiT0.6B1.246.190.22530.80",
  "Qualitative Evaluations": "In this section, we evaluate all one-step text-to-image generator models, showing that the DI++-alignedmodel shows improved human preference performances. Before the quantitative evaluations, we first give aqualitative comparison of the models with and without alignment. The Effects of Human Reward and CFG Reward. shows a visualization comparison ofDiT-based one-step models trained with DI++ with different image reward scales and CFG reward scales.We will give some intuition to better understand the effects of two rewards. 1. From the bottom to the top, the first row of * is the generated results from models trainedwith no image reward and no CFG reward (i.e., scales are set to 0). These images are pretty realistic(such as the coast image), but are of poor details and aesthetic appearance;",
  ": Bad generation cases by aligned one-step generator model (4.5 CFG + 1.0 reward)": "ignore the concept of pear earrings, the battling in a coffee cup, and the car playing football. However, wefind such a mistake happens only occasionally. (2) The Generator Sometimes Generates Bad Human Facesand Hands. Please see the fourth and fifth images of . The face of the generated lady in the fourthimage is not satisfying with blurred eyes and mouth. In the fifth image, the generated Iron Man characterhas multiple hands. (3) Sometimes The aligned model Still Can not Count Correctly. For instance, in therightmost image, the prompt asks the model to generate a birthday cake, however, the model generates twocakes with one near and another lying far away. Besides, as shows, very strong explicit reward scaleslead the image to be very colorful with vivid details. This may cause generated images to look more likepaintings instead of realistic photos. Therefore, we suggest researchers carefully choose the explicit rewardscales according to their alignment purpose when using Diff-Instruct++.",
  "Conclusion and Future Works": "In this paper, we have presented the Diff-Instruct++ method, the first attempt to align one-step text-to-image generator models with human preference. By formulating the problem as a maximization of expectedhuman reward functions with an IKL divergence regularization, we have developed practical loss functionsand a fast-converging yet image data-free alignment algorithm. We also establish theoretical connectionsof Diff-Instruct++ with previous methods, pointing out that the commonly used classifier-free guidanceis secretly doing Diff-Instruct++. Besides, we also introduce a three-stage workflow to develop one-steptext-to-image generator models: the pre-training, the reward modeling, and the alignment stage. We trainone-step generator models with different alignment configurations and demonstrate the superior advantageof Diff-Instruct++ with a human reward that improves the sample quality and better prompt alignment. While Diff-Instruct++ does not completely eliminate the occurrence of simple mistakes in image genera-tion, our findings strongly suggest that this approach represents a promising direction for aligning one-stepgenerators with human preferences. We think our work can shed light on future research in improving theresponsiveness and accuracy of text-to-image generation models, bringing us closer to AGI systems that canmore faithfully interpret and execute human intentions in visual content creation. First, we would like to acknowledge Zhengyang Geng for helpful discussions on experiment settings and theoverall writing of the paper. We also appreciate the authors of Diff-Instruct(Luo et al., 2023b), Score-implicitMatching(Luo et al., 2024b) and Long-short Score-identity Distillation(Zhou et al., 2024a) for their high-quality codebases on diffusion distillation. Second, we would also like to acknowledge Xiaohongshu Inc. forits support of computing on ablation experiments. Finally, we would like to thank the reviewers and editorsof the Diff-Instruct++ paper for their professional opinions and wonderful efforts in organizing the reviewingprocess.",
  "Wei Deng, Weijian Luo, Yixin Tan, Marin Bilo, Yu Chen, Yuriy Nevmyvaka, and Ricky TQ Chen. Varia-tional schr\\\" odinger diffusion models. arXiv preprint arXiv:2405.04795, 2024": "Ming Ding, Wendi Zheng, Wenyi Hong, and Jie Tang. Cogview2: Faster and better text-to-image generationvia hierarchical transformers. Advances in Neural Information Processing Systems, 35:1689016902, 2022. Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis.In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1287312883,2021.",
  "Emiel Hoogeboom, Vctor Garcia Satorras, Clment Vignac, and Max Welling. Equivariant diffusion formolecule generation in 3d. In International Conference on Machine Learning, pp. 88678887. PMLR,2022": "Minguk Kang, Jun-Yan Zhu, Richard Zhang, Jaesik Park, Eli Shechtman, Sylvain Paris, and Taesung Park.Scaling up gans for text-to-image synthesis. In Proceedings of the IEEE Conference on Computer Visionand Pattern Recognition (CVPR), 2023. Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing andimproving the image quality of stylegan. In Proceedings of the IEEE/CVF conference on computer visionand pattern recognition, pp. 81108119, 2020.",
  "Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-basedgenerative models. In Proc. NeurIPS, 2022": "Dongjun Kim, Chieh-Hsin Lai, Wei-Hsiang Liao, Naoki Murata, Yuhta Takida, Toshimitsu Uesaka, YutongHe, Yuki Mitsufuji, and Stefano Ermon. Consistency trajectory models: Learning probability flow odetrajectory of diffusion. arXiv preprint arXiv:2310.02279, 2023. Dongjun Kim, Chieh-Hsin Lai, Wei-Hsiang Liao, Yuhta Takida, Naoki Murata, Toshimitsu Uesaka, YukiMitsufuji, and Stefano Ermon. Pagoda: Progressive growing of a one-step generator from a low-resolutiondiffusion teacher. arXiv preprint arXiv:2405.14822, 2024.",
  "Diederik P Kingma and Jimmy Ba.Adam:A method for stochastic optimization.arXiv preprintarXiv:1412.6980, 2014": "Durk P Kingma and Prafulla Dhariwal. Glow: Generative flow with invertible 1x1 convolutions. In S. Bengio,H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (eds.), Advances in NeuralInformation Processing Systems 31, pp. 1021510224. 2018. Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao,Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. In Proceedings of theIEEE/CVF International Conference on Computer Vision, pp. 40154026, 2023.",
  "Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neuralinformation processing systems, 36, 2024a": "Hongjian Liu, Qingsong Xie, Zhijie Deng, Chen Chen, Shixiang Tang, Fueyang Fu, Zheng-jun Zha, andHaonan Lu. Scott: Accelerating diffusion models with stochastic consistency distillation. arXiv preprintarXiv:2403.01505, 2024b. Xingchao Liu, Chengyue Gong, Lemeng Wu, Shujian Zhang, Hao Su, and Qiang Liu.Fusedream:Training-free text-to-image generation with improved clip+ gan space optimization.arXiv preprintarXiv:2112.01573, 2021. Xingchao Liu, Xiwen Zhang, Jianzhu Ma, Jian Peng, et al. Instaflow: One step is enough for high-qualitydiffusion-based text-to-image generation. In The Twelfth International Conference on Learning Represen-tations, 2023.",
  "Weijian Luo, Hao Jiang, Tianyang Hu, Jiacheng Sun, Zhenguo Li, and Zhihua Zhang. Training energy-basedmodels with diffusion contrastive divergences. arXiv preprint arXiv:2307.01668, 2023c": "Weijian Luo, Tianyang Hu, Shifeng Zhang, Jiacheng Sun, Zhenguo Li, and Zhihua Zhang. Diff-instruct:A universal approach for transferring knowledge from pre-trained diffusion models. Advances in NeuralInformation Processing Systems, 36, 2024a. Weijian Luo, Zemin Huang, Zhengyang Geng, J Zico Kolter, and Guo-Jun Qi. One-step diffusion distil-lation through score implicit matching. In The Thirty-eighth Annual Conference on Neural InformationProcessing Systems, 2024b. URL",
  "Alex Nichol and Prafulla Dhariwal.Improved denoising diffusion probabilistic models.arXiv preprintarXiv:2102.09672, 2021": "Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, IlyaSutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guideddiffusion models. arXiv preprint arXiv:2112.10741, 2021. Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, NalKalchbrenner, Andrew Senior, and Koray Kavukcuoglu. Wavenet: A generative model for raw audio.arXiv preprint arXiv:1609.03499, 2016. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang,Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions withhuman feedback. Advances in neural information processing systems, 35:2773027744, 2022.",
  "Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep convolu-tional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015": "Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn.Direct preference optimization: Your language model is secretly a reward model. Advances in NeuralInformation Processing Systems, 36, 2024. Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, andIlya Sutskever. Zero-shot text-to-image generation. In International Conference on Machine Learning, pp.88218831. PMLR, 2021.",
  "Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditionalimage generation with clip latents. arXiv preprint arXiv:2204.06125, 2022": "Yuxi Ren, Xin Xia, Yanzuo Lu, Jiacheng Zhang, Jie Wu, Pan Xie, Xing Wang, and Xuefeng Xiao. Hyper-sd:Trajectory segmented consistency model for efficient image synthesis. arXiv preprint arXiv:2404.13686,2024. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjrn Ommer. High-resolutionimage synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition, pp. 1068410695, 2022.",
  "Pascal Vincent. A Connection Between Score Matching and Denoising Autoencoders. Neural Computation,23(7):16611674, 2011": "Bram Wallace, Meihua Dang, Rafael Rafailov, Linqi Zhou, Aaron Lou, Senthil Purushwalkam, StefanoErmon, Caiming Xiong, Shafiq Joty, and Nikhil Naik. Diffusion model alignment using direct preferenceoptimization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,pp. 82288238, 2024. Yifei Wang, Weimin Bai, Weijian Luo, Wenzheng Chen, and He Sun. Integrating amortized inference withdiffusion models for learning clean distribution from corrupted images. arXiv preprint arXiv:2407.11162,2024. Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng Zhu, Rui Zhao, and Hongsheng Li. Humanpreference score v2: A solid benchmark for evaluating human preferences of text-to-image synthesis. arXivpreprint arXiv:2306.09341, 2023.",
  "Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong.Imagereward: Learning and evaluating human preferences for text-to-image generation, 2023a": "Xingqian Xu, Zhangyang Wang, Gong Zhang, Kai Wang, and Humphrey Shi. Versatile diffusion: Text,images and variations all in one diffusion model. In Proceedings of the IEEE/CVF International Conferenceon Computer Vision, pp. 77547765, 2023b. Yanwu Xu, Yang Zhao, Zhisheng Xiao, and Tingbo Hou. Ufogen: You forward once large scale text-to-image generation via diffusion gans. In Proceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition, pp. 81968206, 2024. Shuchen Xue, Mingyang Yi, Weijian Luo, Shifeng Zhang, Jiacheng Sun, Zhenguo Li, and Zhi-Ming Ma.SA-solver: Stochastic adams solver for fast sampling of diffusion models. In Thirty-seventh Conference onNeural Information Processing Systems, 2023. URL",
  "Hanshu Yan, Xingchao Liu, Jiachun Pan, Jun Hao Liew, Qiang Liu, and Jiashi Feng. Perflow: Piecewiserectified flow as universal plug-and-play accelerator. arXiv preprint arXiv:2405.07510, 2024": "Kai Yang, Jian Tao, Jiafei Lyu, Chunjiang Ge, Jiaxin Chen, Weihan Shen, Xiaolong Zhu, and Xiu Li. Usinghuman feedback to fine-tune diffusion models without any reward model. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition, pp. 89418951, 2024. Tianwei Yin, Michal Gharbi, Richard Zhang, Eli Shechtman, Fredo Durand, William T Freeman, andTaesung Park. One-step diffusion with distribution matching distillation. arXiv preprint arXiv:2311.18828,2023. Tianwei Yin, Michal Gharbi, Taesung Park, Richard Zhang, Eli Shechtman, Fredo Durand, andWilliam T Freeman. Improved distribution matching distillation for fast image synthesis. arXiv preprintarXiv:2405.14867, 2024.",
  "Mingyuan Zhou, Zhendong Wang, Huangjie Zheng, and Hai Huang.Long and short guidance in scoreidentity distillation for one-step text-to-image generation. arXiv preprint arXiv:2406.01561, 2024a": "Mingyuan Zhou, Huangjie Zheng, Zhendong Wang, Mingzhang Yin, and Hai Huang. Score identity distilla-tion: Exponentially fast distillation of pretrained diffusion models for one-step generation. arXiv preprintarXiv:2404.04057, 2024b. Yufan Zhou, Ruiyi Zhang, Changyou Chen, Chunyuan Li, Chris Tensmeyer, Tong Yu, Jiuxiang Gu, JinhuiXu, and Tong Sun. Towards language-free training for text-to-image generation. In Proceedings of theIEEE/CVF conference on computer vision and pattern recognition, pp. 1790717917, 2022.",
  "Broader Impact Statement": "This work is motivated by our aim to increase the positive impact of one-step text-to-image generativemodels by training them to follow human preferences. By default, one-step generators are either trainedover large-scale image-caption pair datasets or distilled from pre-trained diffusion models, which convey onlysubjective knowledge without human instructions. Our results indicate that the proposed approach is promising for making one-step generative models moreaesthetic, and more preferred by human users. In the longer term, alignment failures could lead to moresevere consequences, particularly if these models are deployed in safety-critical situations. For instance,if alignment failures occur, the one-step text-to-image model may generate toxic images with misleadinginformation, and horrible images that can potentially be scary to users. We strongly recommend using ourhuman preference alignment techniques together with AI safety checkers for text-to-image generation toprevent undesirable negative impacts.",
  "A.2Meanings of Hyper-parameters": "Meanings of Hyper-parameters.As we can see in Algorithm 1 (as well as Algorithm 2). Each hy-perparameter has its intuitive meaning. The reward scale parameter rew controls the strength of humanpreference alignment. The larger the rew is, the stronger the generator is aligned with human preferences.However, the drawback for a too large rew might be the loss of diversity and reality. Besides, we empiri-cally find that larger rew leads to richer generation details and better generation layouts. The CFG scalecontrols the strength of CFG when computing score functions for the reference diffusion model. As we haveshown in Theorem 3.4, the cfg represents the strength of the implicit reward function (3.5). We empirically",
  "A.3Experiment Details for Pre-training and Alignment": "Experiment Details for Pre-training and AlignmentWe follow the setting of Diff-Instruct (Luoet al., 2024a) to use the same neural network architecture as the reference diffusion model for the one-stepgenerator. The PixelArt- model is trained using so-called VP diffusion(Song et al., 2020), which first scalesthe data in the latent space, then adds noise to the scaled latent data. We reformulate the VP diffusion asthe form of so-called data-prediction proposed in EDM paper (Karras et al., 2022) by re-scaling the noisydata with the inverse of the scale that has been applied to data with VP diffusion. Under the data-predictionformulation, we select a fixed noise init level to be init = 2.5 following the Diff-Instruct and SiD (Zhouet al., 2024b). For generation, we first generate a Gaussian vector z pz = N(0, 2initI). Then we inputz into the generator to generate the latent. The latent vector can then be decoded by the VAE decoder toturn into an image if needed. The Training Setup and Costs.We train the model with the PyTorch framework. In the pre-trainingstage, we use the official checkpoint of off-the-shelf PixelArt--512512 model 2 as weights of the referencediffusion.We initialize the TA diffusion model with the same weights as the reference diffusion model.We use Diff-Instruct to pre-train the generator. We use the Adam optimizer for both TA diffusion andgeneration at all stages. For the reference diffusion model, we use a fixed classifier-free guidance scale of 4.5,while for TA diffusion, we do not use classifier-free guidance (i.e., the CFG scale is set to 1.0). We set theAdam optimizers beta parameters to be 1 = 0.0 and 2 = 0.999 for both the pre-training and alignmentstages. We use a learning rate of 5e 6 for both TA diffusion and the student one-step generator. For theone-step generator model, we use the adaptive exponential moving average technique by referring to theimplementation of the EDM (Karras et al., 2022). We pre-train the one-step model on 4 Nvidia A100 GPUsfor two days (4 48 = 192 GPU hours), with a batch size of 1024. We find that the Diff-Instruct algorithmconverges fast, and after the pre-training stage, the generator can generate images with decent quality. In the alignment stage, we aim to inspect the one-step generators behavior with different alignment con-figurations. Notice that as we have shown in Theorem 3.4, using classifier-free guidance is secretly doingRLHF with Diff-Instruct++, therefore we add both CFG and human reward with different scales to thor-oughly study the human preference alignment. More specifically, we align the generator model with fiveconfigurations with different CFG scales and reward scales:",
  ". strong CFG and strong reward: 4.5 CFG scale and 10.0 reward scale": "For all alignment training, we initialize the generator with the same weights that we obtained in the pre-training stage. We put the details of how to construct the one-step generator in Appendix C.1. We alsoinitialize the TA diffusion model with the same weight as the reference diffusion. We use the Image Rewardas the human preference reward and use the Diff-Instruct++ algorithm 1 (or equivalently the algorithm 2)",
  "A.4More Discussions on Findings of Qualitative Evaluations": "There are some other interesting findings when qualitatively evaluate different models. First, we find that theimages generated by the aligned model show a better composition when organizing the contents presentedin the image. For instance, the main objects of the generated image are smaller and show a more naturallayout than other models, with the objects and the background iterating aesthetically. This in turn revealsthe human presence: human beings would prefer that the object of an image does not take up all spaces ofan image. Second, we find that the aligned model has richer details than the unaligned model. The strongerwe align the model, the richer details the model will generate. Sometimes these rich details come as a hintto the readers about the input prompts. Sometimes they just come to improve the aesthetic performance.We think this phenomenon may be caused by the fact that human prefers images with rich details. Anotherfinding is that as the reward scale for alignment becomes stronger, the generated image from the alignmentmodel becomes more colorful and more similar to paintings. Sometimes this leads to a loss of reality tosome degree. Therefore, we think that users should choose different aligned one-step models with a trade-offbetween aesthetic performance and image reality according to the use case.",
  "B.1Proof of Theorem 3.1": "Proof. Recall that p() is induced by the generator g(), therefore the sample is obtained by x = g(z|c), z pz. The term x contains parameter through x = g(z|c), z pz. To demonstrate the parameter dependence,we use the notation p(). pref() is the reference distribution. The alignment objective writes",
  "(B.6)": "The equality (B.4) holds if function p(x|c) satisfies the conditions (1). p(x|c) is LebeStaue integrable forx with each ; (2). For almost all x RD, the partial derivative p(x|c)/ exists for all . (3) thereexists an integrable function h(.) : RD R, such that p(x|c) h(x) for all x in its domain. Then thederivative w.r.t can be exchanged with the integral over x, i.e.",
  "x0r(x0, c) + w(t)s(xt|t, c) sref(xt|t, c)xt": "Recall the definition of p(|t, c), the sample is obtained by x0 = g(z|c), z pz, and xt|x0 pt(xt|x0)according to forward SDE (2.1). Since the solution of forward, SDE is uniquely determined by the initial pointx0 and a trajectory of Wiener process wt[0,T ], we slightly abuse the notation and let xt = F(g(z|c), w, t) torepresent the solution of xt generated by x0 and w. We let w Pw to demonstrate a trajectory from theWiener process where Pw represents the path measure of Weiner process on t [0, T]. There are two termsthat contain the generators parameter . The term xt contains parameter through x0 = g(z|c), z pz.The marginal density p(|t, c) also contains parameter implicitly since p(|t, c) is initialized with thegenerator output distribution p(|t = 0, c).",
  "(B.12)": "The equality (B.10) holds if function p(xt|t, c) satisfies the conditions (1). p(xt|t, c) is LebeStaue integrablefor x with each ; (2). For almost all xt RD, the partial derivative p(xt|t, c)/ exists for all .(3) there exists an integrable function h(.) : RD R, such that p(xt|t, c) h(xt) for all xt in its domain.Then the derivative w.r.t can be exchanged with the integral over xt, i.e.p(xt|t, c)dx =",
  "D(x; ) = x F(C.10)": "Here, following the iDDPM+DDIM preconditioning in EDM, PixelArt- is denoted by F, x is the imagedata plus noise with a standard deviation of , for the remaining parameters such as C1 and C2, we keptthem unchanged to match those defined in EDM. Unlike the original model, we only retained the imagechannels for the output of this model. Since we employed the preconditioning of iDDPM+DDIM in theEDM, each value is rounded to the nearest 1000 bins after being passed into the model. For the actualvalues used in PixelArt-, beta_start is set to 0.0001, and beta_end is set to 0.02. Therefore, accordingto the formulation of EDM, the range of our noise distribution is [0.01, 156.6155], which will be used totruncate our sampled . For our one-step generator, it is formulated as:",
  "g(x; init) = x initF(C.11)": "Here following Diff-Instruct to use init = 2.5 and x N(0, initI), we observed in practice that larger valuesof init lead to faster convergence of the model, but the difference in convergence speed is negligible for thecomplete model training process and has minimal impact on the final results. We utilized the SAM-LLaVA-Caption10M dataset, which comprises prompts generated by the LLaVA modelon the SAM dataset. These prompts provide detailed descriptions for the images, thereby offering us achallenging set of samples for our distillation experiments. All experiments in this section were conducted with bfloat16 precision, using the PixelArt-XL-2-512x512model version, employing the same hyperparameters. For both optimizers, we utilized Adam with a learningrate of 5e-6 and betas=[0, 0.999]. Finally, regarding the training noise distribution, instead of adhering tothe original iDDPM schedule, we sample the from a log-normal distribution with a mean of -2.0 and astandard deviation of 2.0, we use the same noise distribution for both optimization steps and set the two lossweighting to constant 1. Our best model was trained on the SAM Caption dataset for approximately 16kiterations, which is equivalent to less than 2 epochs. This training process took about 2 days on 4 A100-40GGPUs.",
  "C.2More Discussions on Experiment Results": "Low Alignment Costs.Besides the top performance, the training cost with DI++ is surprisingly cheap.Our best model is pre-trained with 4 A100-80G GPUs for 2 days and aligned using the same computationcosts. while other industry models in require hundreds of A100 GPU days. We summarize the dis-tillation costs in , marking that DI++ is an efficient yet powerful alignment method with astonishingscaling ability. We believe such efficiency comes from the image-data-free property of DI++. The DI++does not require image data when aligning, this distinguishes the DI++ from other methods that fine-tunemodels on highly curated image datasets, which potentially is inefficient.",
  "DM sGenerator gDM sGenerator g": "Learning rate5e-65e-65e-65e-6Batch size10241024256256(t)2.52.52.52.5Adam 00.00.00.00.0Adam 10.9990.9990.9990.999Time DistributionpEDM(t)(C.5)pEDM(t)(C.5)pEDM(t)(C.5)pEDM(t)(C.5)WeightingEDM(t)(C.7)1EDM(t)(C.7)1Number of GPUs4A100-40G4A100-40G4H800-80G4H800-80G Algorithm 3: Diff-Instruct++ Pseudo Code under EDM formulation.Input: prompt dataset C, generator g(x0|z, c), prior distribution pz, reward model r(x, c), rewardscale rew, CFG scale cfg, reference EDM denoiser model dref(xt|c, c), TA EDM denoiserd(xt|t, c), forward diffusion p(xt|x0) (2.1), TA EDM denoiser updates rounds KT A, timedistribution (t), diffusion model weighting (t), generator IKL loss weighting w(t).while not converge do",
  "A dog is reading a thick book": "A delicate apple(universe of stars inside the apple) made of opal hung on a branch in the earlymorning light, adorned with glistening dewdrops. in the background beautiful valleys, divine iridescentglowing, opalescent textures, volumetric light, ethereal, sparkling, light inside body, bioluminescence,studio photo, highly detailed, sharp focus, photorealism, photorealism, 8k, best quality, ultra detail,hyper detail, hdr, hyper detail. Drone view of waves crashing against the rugged cliffs along Big Surs Garay Point beach. Thecrashing blue waters create white-tipped waves, while the golden light of the setting sun illuminatesthe rocky shore. A small island with a lighthouse sits in the distance, and green shrubbery coversthe cliffs edge. The steep drop from the road down to the beach is a dramatic feat, with the cliffsedges jutting out over the sea. This is a view that captures the raw beauty of the coast and the ruggedlandscape of the Pacific Coast Highway. Image of a jade green and gold coloured Faberg egg, 16k resolution, highly detailed, product pho-tography, trending on artstation, sharp focus, studio photo, intricate details, fairly dark background,perfect lighting, perfect composition, sharp features, Miki Asai Macro photography, close-up, hyperdetailed, trending on artstation, sharp focus, studio photo, intricate details, highly detailed, by gregrutkowski.",
  "A small cactus with a happy face in the Sahara desert;": "A delicate apple(universe of stars inside the apple) made of opal hung on a branch in the earlymorning light, adorned with glistening dewdrops. in the background beautiful valleys, divine iridescentglowing, opalescent textures, volumetric light, ethereal, sparkling, light inside body, bioluminescence,studio photo, highly detailed, sharp focus, photorealism, photorealism, 8k, best quality, ultra detail,hyper detail, hdr, hyper detail; Drone view of waves crashing against the rugged cliffs along Big Surs Garay Point beach. Thecrashing blue waters create white-tipped waves, while the golden light of the setting sun illuminatesthe rocky shore. A small island with a lighthouse sits in the distance, and green shrubbery coversthe cliffs edge. The steep drop from the road down to the beach is a dramatic feat, with the cliffsedges jutting out over the sea. This is a view that captures the raw beauty of the coast and the ruggedlandscape of the Pacific Coast Highway;",
  "prompt for first row of : A small cactus with a happy face in the Sahara desert": "prompt for second row of : An image of a jade green and gold coloured Faberg egg, 16kresolution, highly detailed, product photography, trending on artstation, sharp focus, studio photo,intricate details, fairly dark background, perfect lighting, perfect composition, sharp features, MikiAsai Macro photography, close-up, hyper detailed, trending on artstation, sharp focus, studio photo,intricate details, highly detailed, by greg rutkowski.",
  "the first row from left to right is the one-step model (4.5 CFG and 1.0 reward); the PixelArt-diffusion with 30 generation steps; the one-step model (4.5 CFG and 10.0 reward);": "the PixelArt- diffusion with 30 generation steps; the PixelArt- diffusion with 30 generation steps;the one-step model (4.5 CFG and 10.0 reward); the first row from left to right is the one-step model(4.5 CFG and 1.0 reward); the PixelArt- diffusion with 30 generation steps; the first row from left to right is the one-stepmodel (4.5 CFG and 1.0 reward); the first row from left to right is the one-step model (4.5 CFG and10.0 reward);",
  "A dog that has been meditating all the time;": "Drone view of waves crashing against the rugged cliffs along Big Surs Garay Point beach. Thecrashing blue waters create white-tipped waves, while the golden light of the setting sun illuminatesthe rocky shore. A small island with a lighthouse sits in the distance, and green shrubbery coversthe cliffs edge. The steep drop from the road down to the beach is a dramatic feat, with the cliffsedges jutting out over the sea. This is a view that captures the raw beauty of the coast and the ruggedlandscape of the Pacific Coast Highway; A delicate apple(universe of stars inside the apple) made of opal hung on a branch in the early morn-ing light, adorned with glistening dewdrops. in the background beautiful valleys, divine iridescentglowing, opalescent textures, volumetric light, ethereal, sparkling, light inside the body, biolumines-cence, studio photo, highly detailed, sharp focus, photorealism, photorealism, 8k, best quality, ultradetail, hyper detail, hdr, hyper detail."
}