{
  "Abstract": "State-of-the-art self-supervised representation learning methods for Graphs are typicallybased on contrastive learning (CL) principles. These CL objective functions can be posedas a supervised discriminative task using hard labels that consider any minor augmentedpairs of graphs as equally positive. However, such a notion of equal pairs is incorrectfor graphs as even a smaller discrete perturbation may lead to large semantic changesthat should be carefully encapsulated within the learned representations. This paper pro-poses a novel CL framework for GNNs, called Teacher-guided Graph Contrastive Learning(TGCL), that incorporates soft pseudo-labels to facilitate a more regularized discrimi-nation. In particular, we propose a teacher-student framework where the student learnsthe representation by distilling the teachers perception.Our TGCL framework can beadapted to existing CL methods to enhance their performance. Our empirical findings val-idate these claims on both inductive and transductive settings across diverse downstreamtasks, including molecular graphs and social networks.Our experiments on benchmarkdatasets demonstrate that our framework consistently improves the average AUROC scoresfor molecules property prediction and social network link prediction. Our code is availableat",
  "Introduction": "Graphs are versatile data structures representing relationships between entities in various domains, such associal networks (Ohtsuki et al., 2006; Fan et al., 2019), bio-informatics (Muzio et al., 2021), and knowledgegraphs (Wang et al., 2014; Baek et al., 2020). Analyzing and understanding graph data is crucial in manyreal-world applications, including community detection (Fortunato, 2010), node classification (Bhagat et al.,2011), link prediction (Zhang & Chen, 2018; Rossi et al., 2021), recommendation (Wu et al., 2019), continuallearning (Mondal et al., 2024), and time-series analysis (Chauhan et al., 2022). Graph representation learningcan potentially make significant leaps in graph-based analysis and prediction. Self-supervised Learning (SSL) for graphs has emerged as an important research area that leverages theinherent structure or content of inputs to learn informative representations without relying on explicit la-bels (Hu et al., 2020a; Hwang et al., 2020). Existing graph-SSL methods can be broadly categorized as:",
  "(a)(b)(c)": ": Illustrating the shortcomings of existing CL methods: (a) Even a minor change, i.e., removalof one edge can significantly change a graphs semantics, leading to disconnected components that arenot captured using edit-distance-based discrepancy (Kim et al., 2022). (b & c) A more specific exampleof correlated-structured molecules that either actively bind to a set of human -secretase inhibitors orinactive (Wu et al., 2018). (b) Molecules having dissimilar properties can have smaller edit distances, while(c) molecules from the same class can have larger edit distances. In other words, edit distance remainsineffective in capturing chemical semantics. Our proposed distilled perception distance form a pre-trainedteacher incorporates soft semantic distances for any arbitrary graphs to train a better student representationmodel. (a) local similarity-based predictive learning & (b) global similarity-based contrastive learning. Predictivelearning-based methods (Hu et al., 2020a; Kim & Oh, 2021; Rong et al., 2020) produces artificial labels bycapturing specific local contextual information of neighborhood sub-graphical features to produce the repre-sentations. However, it restricts them to capturing only the local graph semantics. Alternatively, contrastivelearning (CL)-based models for graphs aim to maximize the agreements between instances perturbed bysemantic-invariant augmentations (positive views) while repelling the others (negative views) to captureglobal semantics.CL-based SSL models are extremely popular in the computer-vision community.Forsuch applications, we can easily generate such semantic-invariant perturbations using simple techniques e.g.,rotation, flipping, and color jittering (Chen et al., 2020a; Grill et al., 2020b). Several graph contrastive learning methods are also proposed where the positive pairs are produced usingtransformations e.g., edge perturbation, attribute masking, and subgraph sampling. However, unlike con-tinuous computer vision domains, even minor modifications in the graph structures, such as removing oneedge or node, can significantly change the properties of graphs due to their discrete nature (a & 1b).Recently, (Kim et al., 2022) introduced discrepancy-based self-supervised learning (D-SLA) by incorporatingedit distance-based discrepancy measures between two graphs to address these limitations. However, com-puting the edit distance between two arbitrary graphs is NP-hard (Sanfeliu & Fu, 1983; Zeng et al., 2009).Further, it can only provide high-level structural information without capturing any semantic differences(a & 1b). In this paper, we propose a graph representation learning framework by incorporatingmore semantically-rich soft-discriminative features using such an imperfect pre-trained teacher to regularizethe learning.",
  "Motivation & Contributions": "The existing CL methods for graphs can be viewed under the same umbrella where these techniques learnrepresentations by contrasting different views of the input graphs. In principle, their loss functions can beconsidered as supervised classification objectives by creating pseudo-labels among different views of inputgraphs (Oord et al., 2018; Gutmann & Hyvrinen, 2010). In contrast, in the supervised learning literature, ithas been observed that incorporating soft labels, even from an imperfect teacher, in the form of KnowledgeDistillation (KD) leads to better generalization (Hinton et al., 2015; Menon et al., 2021; Kaplun et al., 2022).Given these prior results, we explore the following question: Can soft guidance from an imperfect teacherlead to a better CL framework for graphs? The fundamental idea of KD is to use softened labels via a teacher network while minimizing the supervisedrisk of a student network by reducing the divergence between their logits (Hinton et al., 2015). Prior workshave shown that Bayes-distilled risk has lower variance compared to naive undistilled counterpart, which",
  "(c) Proposed TGCL": ": Comparing our proposed TGCL framework with the existing contrastive learning You et al.(2020); Xu et al. (2021), and D-SLA Kim et al. (2022). From the classification point of view, the standardCL methods consider the similarity between the anchor and the perturbed graphs as hard positive pairswhile the other graphs as hard negative pairs. D-SLA introduces hard discrepancies using edit distancebetween the anchor and the perturbed graphs, while the other graphs as hard negative pairs. Our proposedTGCL introduces a novel distilled perception distance for smooth discrimination between arbitrary graphs. leads to better generalization (Menon et al., 2021). Motivated by these results, we propose a novel Teacher-guided Graph Contrastive Learning (TGCL) framework. We design a distilled perception distance (or distilleddistance) between two arbitrary input graphs using their deep features obtained from a pre-trained teacher\"to define a softer notion of positive/negative pairs. We train the student network by incorporating suchsoft labels for each pair of graphs. We argue that by introducing distilled distance, we can introduce theregularized semantic difference between two arbitrary graphs, addressing the shortcomings of the existingCL frameworks for graphs. For example, c demonstrates that our distilled distance obtained fromthe teacher can significantly differ among molecular graphs with correlated structures towards capturingthe chemical semantic differences for graphs.b shows that the distilled distance captures thechemical semantic difference of molecules with different chemical properties, however, with a minor structuraldifference. The contributions of our work can be summarized as follows: 1. we propose to obtain distilled perceptual distances by comparing the deep features from a pre-trainedteacher, followed by injecting them as soft pseudo-labels into the contrastive loss objective to appropri-ately capture the semantic differences between two arbitrary graphs in the students representation space.Theoretically, by viewing the contrastive loss objective for graphs from a supervised loss, incorporating suchdistilled perceptual distances acts as soft pseudo-labels that reduce the variance of Bayes-distilled risk toprovide better graph representations.To the best of our knowledge, we are the first to propose such ateacher-guided soft-discrimination-based contrastive learning framework for the discrete domain of graphs. 2. Our proposed concept of soft-labeled pairs of graphs can be adapted to any contrastive learning frame-work. We demonstrated two variations of TGCL frameworks by modifying the well-known NT-Xent lossChen et al. (2020a); You et al. (2020) and D-SLA method Kim et al. (2022) to incorporate smooth perceptionfrom a teacher network for training the student network. Notably, TGCL is specifically designed for graphs toappropriately incorporate the representational distance even when minor perturbations significantly changethe input semantics. 3. Experiments on graph classification for molecular datasets and link prediction on social network datasetswhere our proposed framework consistently outperforms the existing methods by improving upon the teacher.we improve the average area under receiver operating curve (AUROC) score by 2.23% and 6%.formolecules property prediction and social network link prediction tasks respectively.",
  "Published in Transactions on Machine Learning Research (11/2024)": "for graph classification tasks. In contrast, TGCL-DSLA also effective in capturing the local structural in-formation by explicitly learning to distinguish the anchor and augmented samples (using LT soft [Eq. 7]and LT percept [Eq. 10]). Hence, it leads to better performance for the link prediction task in transduc-tive settings. In summary, we should choose TGCL-GraphCL for inductive settings and TGCL-DSLA fortransductive settings.",
  "Knowledge Distillation (KD)": "KD (Hinton et al., 2015) was originally introduced to transfer knowledge from a complex teacher modelwith large capacity to an efficient student model with lower capacity while performing similarly to theteacher. Several works also focus on improving the students performance on a wide range of applications(Heo et al., 2019; Furlanello et al., 2018; Lopes et al., 2017; Li et al., 2021; Lee et al., 2018; Bhat et al.,2021). Surpassing the Teachers performance. KD allows the student to learn from both the raw data anddistilled knowledge of the teacher, improving their generalized performance (Menon et al., 2021). Therefore,recent works successfully demonstrated that a student with a larger or the same capacity can consistentlyexceed the teachers performance to produce a more generalized model. Existing Distillation-based SSL. Existing distillation-based SSL methods were mainly explored in thecontinuous domain (e.g., images, video) to remove the requirement of negative sampling for contrastivelearning frameworks Grill et al. (2020a); Caron et al. (2021). They were also explored to reduce the size ofthe representation learning models (Abbasi Koohpayegani et al., 2020; Chen et al., 2020b). Many of theseapproaches combined KD with CL methods (Fang et al., 2021; Gao et al., 2022). SimCLR-v2 (Chen et al.,2020b) applied a larger teacher model, first trained using contrastive loss followed by supervised fine-tuningto distill a smaller model using the teacher.(Xu et al., 2020) incorporates auxiliary contrastive loss toobtain richer knowledge from the teacher network. A few other approaches also explored transferring thefinal embeddings of a self-supervised pre-trained teacher (Navaneet et al., 2022; Song et al., 2023).",
  "Limitation of Existing Distillation-based SSL methods for graphs": "Since a minor perturbation does not change the input semantics in the continuous domain, existing KD-based SSL methods did not focus on learning any semantic distance in the representation space for a pair ofpositive samples. In contrast, we should design a novel representation learning framework for graphs thatappropriately incorporate semantic differences due to minor discrete perturbations. Our proposed TGCL first aims to obtain the teachers distilled perception to calculate the semantic differencefor any pairs, followed by formulating soft self-supervised losses to train the student. The notion of distilled",
  "Preliminaries": "Graph Neural Network (GNN). Let G = (V, E, XV , XE) be an undirected graph in the space of graphsG, where V, E, XV , XE denote the set of nodes, edges, node attributes, and edge attributes respectively.GNN encodes a graph G G to a d-dimensional embedding vector: f : G Rd. f is often composedby stacking multiple message-passing layers. Let h(l)vdenote the representation of a node v V having aneighborhood Nv in the lth layer. h(l1)vurepresents the attributes of edge (v, u) E in the (l 1)th layer.Then, h(l)vcan be expressed as follows:",
  "where (l1)U, (l1)Mare the update and the message function of (l1)th layer respectively. is a permutationinvariant aggregator": "Global Representations using Contrastive Learning. Contrastive learning (CL) aims to learn mean-ingful representations by attracting the positive pairs (i.e., similar instances, such as two different perturba-tions of the same graph) while repelling negative pairs (i.e., dissimilar instances, such as two different inputgraphs) in an unsupervised manner, as shown in a. Formally, let G0 denote the original graph andGp denote a perturbed version (i.e., positive sample), and {Gnj}j are other input graphs that are treatedas negative sample. Then, the NT-Xent (normalized temperature-scaled cross-entropy) loss for CL objectivefor G0 can be constructed as follows (Chen et al., 2020a; You et al., 2020):",
  "where f is a GNN and sim(, ) is a similarity measure for embeddings with temperature-scaling": "Minimization of Equation 2 brings positive pairs closer and pushes negative pairs further apart in theembedding space. However, unlike image augmentation schemes (e.g., scaling, rotation, color jitter), graphaugmentation schemes (e.g., node/edge perturbations, subgraph sampling) may fail to preserve the graphsemantics. For example, (a) illustrates that removing one edge leads to two disconnected graphs,significantly changing the original semantics.Recently, D-SLA incorporates edit distances to introducerepresentational discrepancy even between graphs with minor perturbations (Kim et al., 2022). However, itonly partly solves the issue.",
  "Proposed TGCL Framework": "This section presents our Teacher-guided Graph Contrastive Learning (TGCL) framework. Our proposedTGCL is fundamentally based on the following theoretical propositions. While these two propositions wereproposed in two different literature of unsupervised contrastive learning and supervised distillation frame-works, they provide the perfect motivation to propose our teachers distilled perception-guided contrastivelearning framework for discrete domains of graphs.",
  "Teacher": "GNN : Block diagram of our proposed TGCL framework. Weobtain the representations from a pre-trained teacher model andcompute the distilled distance for each pair of inputs.Thesepairwise distances are employed to soften\" the loss functions totrain the student. Proposition 1 indicates that a CL losscan be framed as a supervised classi-fication loss where the network learnsto separate positive pairs from negativepairsbyartificiallyassigninghardpseudo-labels for each pair. Specifically,in Eq. 2, for LCL formulation, we com-pute the probability for any arbitrarygraph, G Gp {Gnj}j constructing apositive pair with the original graph G0as a softmax function i.e., Pr(G0, G) :=",
  "Gnj y(G0,Gnj ) log Pr(G0, Gnj)],whichis precisely the cross-entropy loss forclassification": "Alternatively, we can construct a CL objective by directly distinguishing scores for positive pairs from negativepairs without computing explicit probabilities (You et al., 2020). Similarly, in D-SLA (Kim et al., 2022),both the graph discrimination loss and the edit-distance-based loss approach the task of distinguishing ananchor graph from a perturbed graph as a hard binary classification problem. Furthermore, the marginloss in D-SLA functions similarly to Lcl, with a primary focus on separating positive and negative pairs. Proposition 2. In supervised learning, the variance of Bayesian-distilled risk, obtained using the soft labels,weighted by the likelihood from the Bayes teacher in the form of knowledge distillation (KD), remains lowerthan the variance of empirical risk, obtained using the hard class-labels [Menon et al. (2021)]. That is,",
  "labels for input graph Gi is the empirical loss on f (e.g., softmax cross-entropy).The equality holdsiff G G, the loss values (f(G)) are constant on the support of p(G)": "Proposition 2 suggests that irrespective of the size/capacity of a student model, it statistically produces abetter generalization. In particular, it achieves a better generalization by using soft and distilled labelsfor an existing CL method as the Bayes-distilled risk has lower variance compared to the naive un-distilledcounterpart (Menon et al., 2021). Notably, this result does not depend on the capacity of the teacher or thestudent models. Motivated by these results, we propose to obtain distilled perceptual distances, Ddp by comparing the deepfeatures from a pre-trained teacher, followed by injecting them as soft pseudo-labels\" into the contrastiveloss objective to learn the semantic differences between two arbitrary graphs in students representationspace.",
  "Distilled Perceptual Distance": "Let Ga and Gb be two arbitrary graphs. Consider a representation learning model with L message passinglayers as the teacher. At each layer, l, we obtain the node-embedding {h(l)v }vV for a graph G and apply apooling operation (e.g., max-pool) to obtain a fixed-length vector, h(l)G . We extract such fixed-length featuresfrom each layer and concatenate them, i.e., hGa = [{h(l)Ga}l] and hGb = [{h(l)Gb}l] for Ga and Gb respectively.The distilled perception distance (or distilled distance) Ddp is then defined as the L2 distance between theseconcatenated features, as:",
  "TGCL-GraphCL: TGCL using GraphCL Loss": "NT-Xent (normalized temperature-scaled cross-entropy) is a well-known loss function for contrastive learningmodels that have been widely explored for different domains, including graphs (Chen et al., 2020a; You et al.,2020). Loss Objective. For GraphCL, the contrastive loss is obtained by applying the similarity function, sim(Eq. 2) using exponential of temperature scaled dot product of representations. In order to incorporatethe soft distilled perception for a pair of graphs, when the distilled perceptual distance, Ddp(G0, Gi) issmaller, we want the dot product of their representations, fs(G0) fs(Gi) to be higher. Therefore, we canbalance their similarities by multiplying the teachers distilled distance, Ddp(G0, Gi) with the normalizeddot product,fs(G0)fs(Gi)",
  "||fs(G0)||||fs(Gpi)|| (at a smaller rate) to minimize the overall loss. Similarly, we can analyze thenegative pairs in the denominator": "Note that our LT GCLGraphCL loss learns to discriminate the global representations of the whole graphwithout capturing local structural changes. Therefore, the TGCL-GraphCL framework is more appropriatefor tasks related to global representations such as graph classification.In the following, we present theTGCL-DSLA framework for other tasks (such as link prediction) where the representations should alsocapture the local structural changes.",
  "Next, we present TGCL-DSLA by introducing teacher-guided distilled perception distance for D-SLA. Itconsists of three components as follows:": "(a) Teacher-guided Soft Discrimination: We first discriminate the perturbed graphs from the originalanchor by introducing LT Soft: It consists of two terms: The first one is a KD-based loss, LKD, while thesecond component is a weighted graph discrimination loss (LwGD). We first obtain the distilled distances:[Ddp(G0, G0), {Ddp(G0, Gpi)}i] between the anchor, G0, with itself and the ith perturbed variations, Gpi.We obtain the similarities by taking reciprocals of the normalized distilled distance, followed by clipping toensure numerical stability:",
  "i > 0(6)": "Next, we compute a probability distribution (soft labels) using the softmax-activation with temperature,, i.e., softmax(s0, s1, ; T = ). Similarly, we obtain a score for each graph and compute a probabilitydistribution using temperature-scaled softmax: softmaxfs(Gpi); T = . Now, we obtain the distillationloss, LKD by minimizing the entropy between these probability distributions:",
  "where, H(y, y) =": "y y log y is the cross-entropy function. is the scoring layer and fs is the studentnetwork. fs is the composition of , and fs. The representations obtained from fs are fed into the layer to obtain the scores. Therefore, we incorporate the smoothened perception of the teacher in the scorefunctions to learn the students representations.",
  "Experiments": "In this section, we investigate the performance of TGCL for both TGCL-GraphCL and TGCL-DSLA frame-works on two diverse sets of experiments: (i) Graph Classification task in the chemical and biological domainand (ii) Link prediction on social network datasets. The graph classification task needs to capture the globalstructural representation of the graphs to improve the performance. In contrast, the link prediction taskrelies on the quality of capturing the local structural information. Therefore, it allows us to empiricallycompare our proposed TGCL-GraphCL and TGCL-DSLA frameworks and understand their effectiveness fordifferent scenarios.",
  "Graph Classification": "Datasets.Following the prior works (You et al., 2021; Xu et al., 2021; Kim et al., 2022), we utilize ZINC15(Sterling & Irwin, 2015) to train the self-supervised representation learning models. Next, we finetune themodels on eight different molecular benchmarks from MoleculeNet (Wu et al., 2018). We divide the datasetsbased on the constituting molecules scaffold (molecular substructure).In table 1, we evaluate modelsgeneralization ability on out-of-distribution test data samples (Wu et al., 2018). We also present results from biological domains where the datasets are produced by the sampled ego networksfrom the PPI networks Zitnik et al. (2019). We use the same experimental setup as You et al. (2021) forpredicting proteins biological functions where we pre-train and fine-tune the model using the PPI networkdataset Zitnik et al. (2019). In , we provide the dataset statistics. Evaluation Metric.We use the Area Under Receiver Operating Characteristic curve (AUROC) forbenchmarking (Davis & Goadrich, 2006). AUROC quantifies the overall discriminative power of the classifieracross all possible classification thresholds where higher values indicating better discrimination ability of themodel. We report mean std with 5 independent runs.",
  "No Pretrain65.8 4.558.0 4.471.8 2.575.3 1.970.1 5.457.3 1.674.0 0.863.4 0.666.96": "PredictiveEdgepred (Hamilton et al., 2017)67.3 2.464.1 3.774.1 2.176.3 1.079.9 0.960.4 0.776.0 0.664.1 0.670.28AttrMasking (Hu et al., 2020a)64.3 2.871.8 4.174.7 1.477.2 1.179.3 1.661.0 0.776.7 0.464.2 0.571.15ContextPred (Hu et al., 2020a)68.0 2.065.9 3.875.8 1.777.3 1.079.6 1.260.9 0.675.7 0.763.9 0.670.89GraphMAE Hou et al. (2022)70.1 0.680.8 1.274.5 2.377.0 0.481.4 0.959.0 0.774.4 0.563.9 0.472.64 ContrastiveInfomax (Velikovi et al., 2019)68.8 0.869.9 3.075.3 2.576.0 0.775.9 1.658.4 0.875.3 0.562.7 0.470.29GraphCL (You et al., 2020)69.7 0.776.0 2.769.8 2.778.5 1.275.4 1.460.5 0.973.9 0.762.4 0.670.78JOAO (You et al., 2021)70.2 1.081.3 2.571.7 1.476.7 1.277.3 0.560.0 0.875.0 0.362.9 0.571.89JOAOv2 (You et al., 2021)71.4 0.981.0 1.673.7 1.077.7 1.275.5 1.360.5 0.774.3 0.663.2 0.572.16GraphLoG (Xu et al., 2021)72.5 0.876.7 3.376.0 1.177.8 0.883.5 1.261.2 1.175.7 0.563.5 0.773.36BGRL (Thakoor et al., 2022)66.7 1.764.7 6.569.4 2.775.5 1.971.3 5.560.4 1.474.8 0.763.2 0.868.25SimGCL (Yu et al., 2022)67.4 1.255.7 4.771.2 1.875.0 0.974.1 2.757.4 1.774.4 0.562.3 0.467.19SimGRACE (Xia et al., 2022)71.3 0.964.2 4.571.2 3.474.5 1.173.8 1.460.59 0.974.2 0.663.4 0.569.13D-SLA (Kim et al., 2022)72.6 0.880.2 1.576.6 0.978.6 0.483.8 1.060.2 1.176.8 0.564.2 0.574.13 + Additional3D-InfoMax Strk et al. (2022)67.9 1.289.7 0.576.7 0.673.4 1.279.9 0.959.6 0.775.3 0.364.6 0.473.4Data (*)GraphMVP-G Liu et al. (2022)70.1 0.789.4 1.577.7 1.675.3 0.880.2 1.561.0 0.575.3 0.964.2 0.974.1FragCL Kim et al. (2023)71.4 0.495.2 1.077.6 1.076.3 0.482.3 1.661.0 0.675.2 0.765.1 0.875.5 OursTGCL-GraphCL (w/ GraphLoG)74.9 0.985.3 2.278.9 1.079.1 0.583.7 1.463.6 0.676.7 0.464.1 0.475.79TGCL-GraphCL (w/ D-SLA)74.0 0.482.8 2.277.0 0.977.9 0.384.3 1.064.2 0.376.6 0.164.7 0.475.19TGCL-DSLA (w/ GraphLoG)74.8 0.380.6 0.577.4 0.178.6 0.283.0 1.161.4 0.476.1 0.164.0 0.374.49TGCL-DSLA (w/ D-SLA)73.5 0.984.9 1.379.4 0.978.8 0.585.2 0.461.2 1.076.9 0.164.9 0.275.60 (a) Predictive vs. Contrastive Models. While predictive pretraining improves upon the no pertainingmodel, their performance remains worse than the CL models. This is because predictive methods primarilyfocus on the local structure, while molecular properties depend on the global structure. In contrast, CLmethods focus on the global structure by contrasting between original and perturbed graphs to achievebetter performance.While a few augmentation-free CL (Yu et al., 2022; Xia et al., 2022) methods areproposed, their performance remains significantly lower than the state-of-the-art. GraphLoG achieves higherAUROC scores by exploring both global semantics and local substructures. However, D-SLA achieves thebest performance among the existing models by exploring the local discrete graph structures.",
  "TGCL-GraphCL (w/ GraphLoG)71.96 0.77TGCL-DSLA (w/ D-SLA)71.63 0.96": "(b) Performace of Proposed TGCL. For ourTGCL, we report the results by using GraphLoGand D-SLA as teacher modules for both TGCL-GraphCL and TGCL-DSLA models, demonstratingthe generalizability of our framework. We observethat our proposed method consistently boosts theperformance of teachers irrespective of the teacherstraining methodology for both TGCL models. Fur-thermore, we also outperformed the existing molec-ular graph-specific representation learning modelsthat incorporate additional 3D molecular graphs of GEOM for training their representation models. Next, we observe that TGCL-GraphCL (w/ GraphLoG) performs best. Also, TGCL-DSLA (w/ D-SLA)achieves comparable performance as TGCL-GraphCL (w/ GraphLoG). In particular, we do not observe anadditional advantage of using more graph-specific D-SLA-based loss functions while learning global graph-level representations for molecular property prediction tasks. Performance on PPI network. In table 2, we compare with GraphLoG and D-SLA and the correspondingstudent i.e., TGCL-GraphCL (w/ GraphLoG) and TGCL-DSLA (w/ D-SLA). The results show that thestudent models consistently outperform their corresponding teachers. However, the observed improvementsare not substantial, and the teacher models have already approached their performance saturation pointFurlanello et al. (2018). This phenomenon, known as knowledge saturation, is further explored in AppendixA.3.2. Visualizing the learned latent representation space.In , we visualize the learned latentrepresentation space of D-SLA and our TGCL-DSLA (w/ DSLA) utilizing t-SNE (Van der Maaten & Hinton,2008). We observe that TGCL-DSLA segregates the positive and negative samples more successfully than",
  ":t-SNE visualization of representations for BBBPdataset using (a) D-SLA teacher and (b) TGCL-DSLA": "Datasets.WeconsiderCOLLAB,IMDB-Binary, and IMDB-Multi from TUBenchmarks (Morris et al., 2020).Weseparate the dataset into four parts: pre-training, training, validation, and testsets in the ratio of 5:1:1:3, as in Kim et al.(2022). Additional details are provided in (Appendix). Evaluation Metric. presentscomparative results with several existingmodels and No Pretrain\" baselines. Wecompare average precision (as in (Kimet al., 2022)) where a higher value in-dicates better performance.We reportmeanstd for 5 independent runs.",
  "Performance Analysis": "(a) Predictive vs. Contrastive Models Unlike graph classification tasks, local context plays a crucial rolein link prediction. Therefore, the predictive models typically outperformed most of the CL methods. Amongthe existing CL methods, GraphLog performs similarly to ContextPred as it focuses on both local and globalstructures. D-SLA performs better by capturing local structures using edit-distance-based discriminationsthat standard CL models fail to distinguish.",
  "AttrMasking (Hu et al., 2020a)81.43 0.8070.62 3.6863.37 2.1571.81ContextPred (Hu et al., 2020a)83.96 0.7570.47 2.2466.09 2.7473.51": "Infomax (Velikovi et al., 2019)80.83 0.6267.25 1.8764.98 2.4771.02GraphCL (You et al., 2020)76.04 1.0463.71 2.9862.40 3.0467.38JOAO (You et al., 2021)76.57 1.5465.37 3.2362.76 1.5268.23GraphLoG (Xu et al., 2021)82.95 0.9869.71 3.1864.88 1.8772.51BGRL (Thakoor et al., 2022)76.79 1.1367.97 4.1463.71 2.0969.49SimGCL (Yu et al., 2022)77.46 0.8664.91 2.6063.78 2.2868.72SimGRACE (Xia et al., 2022)74.51 1.5464.49 2.7962.81 2.3267.27D-SLA (Kim et al., 2022)86.21 0.3878.54 2.7969.45 2.2978.07 TGCL-GraphCL (w/ GraphLoG)87.23 0.1475.09 1.8867.11 3.7376.48TGCL-GraphCL (w/ D-SLA)87.51 1.2477.95 3.8967.88 2.2077.78TGCL-DSLA (w/ GraphLoG)91.09 0.3383.15 0.8974.11 1.4482.78TGCL-DSLA (w/ D-SLA)87.51 0.5980.03 4.1370.97 2.4279.50 (b)PerformaceofProposedTGCL. In comparison, our pro-posed distilled distance from theteacher network integrates a regu-larized approach to both local andglobal semantics.Local semanticsare captured from the initial la-tent features, while global seman-tics are derived from deeper globalfeatures.Therefore, we can sur-pass existing local and global rep-resentation learning-based modelsby visible margins for all threedatasets. Interestingly, our TGCL-DSLA (w/GraphLog) performs bet-ter than TGCL-DLSA (w/D-SLA)even though D-SLA outperformedGraphLog.Therefore, a better teacher does not necessarily produce better distillation for the student,as previously observed and analyzed in supervised learning Menon et al. (2021); Kaplun et al. (2022); Zonget al. (2023).",
  "TGCL-GraphCL vs. TGCL-DSLA: Choosing the correct framework for downstream tasks": "As we can see for the graph classification task (), our TGCL-GraphCL framework achieves betterperformance while for link prediction tasks (), TGCL-DSLA produces better results. Therefore, theseempirical results indicate that TGCL-GraphCL produces better global representations for graphs, allowingus to easily distinguish two arbitrary graphs in inductive settings. Hence, this framework is better suited",
  "Conclusion & Discussion": "We utilize Knowledge distillation (KD) for graph representation learning, where a self-supervised pre-trainedteacher is used to guide a student model to produce more generalized representations. Extensive experi-mentation demonstrates the effectiveness of the proposed method in improving the performance of graphclassification and link prediction tasks. However, there are still many open challenges in graph representationlearning, such as the efficient handling of large-scale graphs, the ability to handle heterogeneity and multi-modality, and the development of robust methods for noisy or incomplete data. Probing these challengesfurther and developing new graph representation learning techniques are in the scope of future research. Discussion. Potential Advantages and Limitations of KD for CLAdvantages.KD typically enhances a CL framework to produce improved feature representations bytransferring insights from a well-trained teacher, improving the downstream performance (Hinton et al.,2015; Furlanello et al., 2018; Yim et al., 2017). Further, by utilizing the teachers knowledge, we can speedup the training time efficiency and reduce the sample requirement to learn the student model (Tian et al.,2019).A well-trained teacher can also improve the student models robustness, leading to even betterrepresentations compared to the teacher. Finally, regularizing the student model with softer probabilityscores reduces the variance of Bayes-distilled risk, therefore, making the model more generalizable and lessprone to overfitting (Menon et al., 2021). Limitations.The teacher-student framework for knowledge distillation may face several potentialchallenges (a) KD with CL adds both computation overhead and training complexity (Hinton et al., 2015;Tian et al., 2019). (a) If the teacher model is flawed, the student may inherit its errors, leading to suboptimalperformance (Hinton et al., 2015). (b) Compatibility issues between teacher and student models and traininginstability can hinder effective knowledge transfer.It requires intricate tuning of hyperparameters anddesigning the loss functions. (Mirzadeh et al., 2020). (c) Further, the generalization may be restricted dueto limited knowledge of the teacher with respect to the downstream tasks Yim et al. (2017). (d) Finally,there is a risk of knowledge saturation, where the benefits of additional knowledge transfer may diminishafter a certain saturation point (Furlanello et al., 2018).",
  "A.3 and A.4 provide additional ablation studies to empirically investigate these challenges": "Broader Impact.KD can significantly impact graph representations, with broader implications forvarious fields, including bioinformatics, drug discovery, social network analysis, recommendation systems,etc. A few potential impacts of our work are as follows: (a) Improves the efficiency and scalability of graphrepresentation learning by enabling soft knowledge transfer from a pre-trained teacher model to a smaller,more efficient student network. (b) Improves the generalization performance of graph representation learningby leveraging the dark knowledge encoded in a pre-trained teacher models representations.",
  "K.M. Borgwardt and H.P. Kriegel. Shortest-path kernels on graphs. In ICDM, 2005": "Mathilde Caron, Hugo Touvron, Ishan Misra, Herv Jgou, Julien Mairal, Piotr Bojanowski, and ArmandJoulin.Emerging properties in self-supervised vision transformers.In Proceedings of the IEEE/CVFinternational conference on computer vision, pp. 96509660, 2021. Jatin Chauhan, Aravindan Raghuveer, Rishi Saket, Jay Nandy, and Balaraman Ravindran. Multi-variatetime series forecasting on variable subsets.In Proceedings of the 28th ACM SIGKDD Conference onKnowledge Discovery and Data Mining, pp. 7686, 2022.",
  "A.1Molecular graph classification": "This section presents the implementation details and dataset descriptions of our experiments on moleculargraph classification and link prediction tasks. For all experiments, we use PyTorch (Paszke et al., 2019) andPyTorch Geometric libraries (Fey & Lenssen, 2019) with a single NVIDIA A30 Tensor Core GPU for all ofour experiments. Datasets.For our first experiments on molecular graph classification, we use the ZINC dataset (Sterling& Irwin, 2015), a large corpus of2 million unlabelled molecules for pretraining the teacher and studentnetwork. For the downstream tasks, we experimented with 8 labeled molecular datasets from MolecularNet(Wu et al., 2018). The molecule classes are determined using the biophysical and physiological properties. We also present results from biological domains where the datasets are produced by the sampled ego networksfrom the PPI networks Zitnik et al. (2019). We use the same experimental setup as You et al. (2021) forpredicting proteins biological functions where we pre-train and fine-tune the model using the PPI networkdataset Zitnik et al. (2019). In , we provide the statistics of these datasets.",
  "PPI (Pre-training)306,92539.83364.82-PPI (Finetune)88,00049.35445.3940": "Implementation details.For our proposed framework, we use the same network architecture for boththe teacher and the student model. In particular, we use Graph Isomorphism Networks (GINs) (Xu et al.,2019) as applied in the previous works Hu et al. (2020a); Xu et al. (2021); Kim et al. (2022). These networksconsist of 5 layers with 300 dimensional embeddings for nodes and edges along with average pooling strategiesfor obtaining the graph representations. To obtain distilled perception distance from the teacher network,we use global average pooling to extract the fixed-length features from each layer. We use the official D-SLA codes1 provided by Kim et al. (2022) as the backbone for our experiments andapply the same perturbation strategies as used in (Kim et al., 2022).In particular, their perturbationstrategy aims to minimize the risk of creating unreasonable cycles, reducing the chance of significant changein the chemical properties. For our experiments, we use three perturbations for each input sample. We report results using two different teacher modules, trained using existing GraphLog (Xu et al., 2021) andD-SLA (Kim et al., 2022) while training the following student network using the loss functions as proposedin .3. We divide the perceptual distances by 4 and 1 as we use GraphLog (Xu et al., 2021) andD-SLA (Kim et al., 2022) as the teacher, respectively. For TGCL-GraphCL, we use = 10 in Equation 5.For TGCL-DSLA, we use 1 and 2 to 1.0 and 0.5 respectively for the student model. For LT soft loss, weset the temperature, = 10 (Equation 7) and = 0.95 (Equation 9). For LT margin, we set = 5. Bothteacher and student models are trained using batch-size of 256 and for 25 epochs with learning rate 1e 3and Adam optimizer (Kingma & Ba, 2014).",
  "COLLAB432076.122331.4IMDB-B203920.1385.5IMDB-M147816.6477.9": "COLLAB is a dataset of scientific collaboration networks. It contains 4, 320 graphs where the researcher andtheir collaborators are nodes and an edge indicates collaboration between two researchers. A researchersego network has three possible labels: High Energy Physics, Condensed Matter Physics, and Astro Physics,representing the field of the researchers. IMDB-Binary is a movie collaboration dataset. It consists of the ego networks of actors/actresses from themovies in IMDB. It consists of 2, 039 graphs. For each graph, the nodes are actors/actresses, with an edgebetween them if they appear in the same movie. These graphs are derived from the Action and Romancegenres. IMDB-Multi is a relational dataset that consists of a network of actors or actresses, played in movies inIMDB. It contains 1,478 graphs. As before, a node represents an actor or actress, and an edge connects twonodes when they appear in the same movie. The edges are collected from three different genres: Comedy,Romance, and Sci-Fi. Implementation details.For our experiments, we use Graph Convolutional Network (GCN) Kipf& Welling (2017) for both teacher and student models. These networks consist of three layers with 300dimensions for embeddings.As before, we use the same perturbation strategy as applied in Kim et al.(2022). We have also experimented with two different teachers, i.e., D-SLA Kim et al. (2022) and GraphLogXu et al. (2021). We use a batch size of 32 and a learning rate of 0.001 for training the student representationlearning models. For TGCL-GraphCL, we set = 10 (Eq. 5). For TGCL-DSLA, we use 1 and 2 to 0.7 and0.0, respectively. For LT soft loss, we select the temperature, from three different values i.e., {5, 10, 20}(Equation 7) and set = 0.95 (Eq. 9).",
  "A.3.1Student Network with reduced Capacity": "The original goal of Knowledge Distillation (KD) (Hinton et al., 2015) was to transfer knowledge froma complex teacher model to a simpler student model, achieving comparable performance with reducedcomputational cost. However, subsequent studies demonstrate that using a complex student model, similarto the teacher, can also enhance model robustness, efficiency, and generalization across various applications(Furlanello et al., 2018; Tian et al., 2019). Although our main experiments utilize identical architectures forboth teacher and student models, this section explores the impact of employing a smaller student network. demonstrates the performance of a student TGCL-DSLA model on the downstream molecularproperty prediction task. We can see that, with the same capacity (i.e., 5 layers of GNN) as the teachermodule of D-SLA, our proposed student network consistently outperformed the teacher. As we decreasethe capacity of our student network by reducing the number of layers, the overall performance reduces.However, we observe that even with 3 layers of GNN, our student module outperforms the teacher D-SLAmodel. Therefore, these results demonstrate that our proposed TGCL framework can compress the student",
  "A.3.2TGCL with multi-level teachers & Knowledge Saturation": "Proposition 2 suggests that irrespective of the size/capacity of a student model, it statistically producesa better generalization. Hence, it raises a natural question: can we further improve the performance byiteratively using the student models as the teacher to train another follow-up student network? In ,we investigate this by training a 2-level iterative teacher model for our TGCL-DSLA framework. In otherwords, we use the TGCL-DSLA (w/ D-SLA) student model to train another 2nd-level student, denoted asTGCL2-DSLA (w/ D-SLA). We can see that the performance of TGCL2-DSLA (w/ D-SLA) saturates and does not improve the overallperformance than the original TGCL-DSLA (w/ D-SLA). These results indicate that the TGCL-DSLA al-ready receives sufficient probability calibrations from the first-level teacher model. Hence, their performanceimprovement converges after the first-level teacher. This phenomenon is known as knowledge saturation\"in the KD literature Furlanello et al. (2018).",
  "A.3.3Choice of teacher models: Computational overhead and convergence": "KD enhances a CL framework by improving feature representations but introduces additional computationaloverhead and training complexity. In , we examine the performance of the TGCL student modelusing an undertrained teacher model with early stopping. We observe that the student model consistently outperforms the teacher models, even when the teachermodels are undertrained. For instance, our TGCL-GraphCL model with D-SLA (trained for 20 epochs)achieves an average AUROC score of 74.4%, surpassing the performance of D-SLA model trained for 100epochs. These results suggest that the TGCL student model can achieve comparable performance withoutrequiring a fully trained teacher model. Consequently, it is possible to enhance downstream performancewith only a modest increase in computational overhead.",
  "A.4.1Impact of different loss components": "In , we first demonstrate the impact of different loss components. The first three rows demonstratethe performance of individual loss components. We observe that Lsoft is the most essential component,providing the maximum performance boost for the downstream molecular prediction tasks. The other twoloss components, i.e. LT percept and LT margin act as regularizer. While, individually, they do not performwell, incorporating them with Lsoft in Loverall, we observe a significant boost in the overall performance.",
  "A.4.2Impact of hyper-parameters of LT soft": "Since LT soft is the most important loss component, we further analyze hyper-parameters associated withit. We can see in Eq. 9, Lsoft is similar to the distillation loss for classification tasks, consisting of two losscomponents, i.e. LKD (Eq. 7) and LwGD (Eq. 8). Here, we analyze the temperature term, for LKD,followed by the weights of these components, .",
  "BBBPClinToxMUVHIVBACESIDERTox21ToxCastAvg": "073.2 0.780.2 1.876.4 0.778.1 0.684.1 0.962.3 0.575.5 0.363.8 0.374.200.573.6 0.782.5 1.275.0 1.578.4 0.685.6 0.562.4 0.175.8 0.164.5 0.374.730.9573.5 0.984.9 1.379.4 0.978.8 0.585.2 0.461.2 1.076.9 0.164.9 0.275.601.072.8 0.683.6 1.277.6 1.678.8 0.483.4 1.361.3 0.676.6 0.364.1 0.274.78 Next, in , we analyze the impact of with fixed = 10. A larger value of provides more weight toLKD. We can see that increasing to a non-zero value improves the models overall performance. However,performance tends to reduce as we choose = 1 to remove LwGD entirely. We achieve the best performanceat = 0.95.",
  "A.4.3Sensitivity of 1 and 2": "In , we present the performance of TGCL DSLA(w/DSLA) model as we vary 1 and 2 in Eq.11. We first vary 1 to {0.3, 0.5, 0.7, 1.0} as we fix 2 = 0.5. We observe that the performance improves aswe choose larger values, i.e., when we set 1 to 0.7 or 1.0. Next, we vary 2 to {0.3, 0.5, 0.7, 1.0} as we fix1 = 1.0. Here, we observe that we achieve the average performance as we set 2 = 0.5"
}