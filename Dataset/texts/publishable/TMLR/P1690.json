{
  "Abstract": "Diffusion models have become the de-facto approach for generating visual data, which aretrained to match the distribution of the training dataset. In addition, we also want to controlgeneration to fulfill desired properties such as alignment to a text description, which canbe specified with a black-box reward function. Prior works fine-tune pretrained diffusionmodels to achieve this goal through reinforcement learning-based algorithms. Nonetheless,they suffer from issues including slow credit assignment as well as low quality in theirgenerated samples. In this work, we explore techniques that do not directly maximizethe reward but rather generate high-reward images with relatively high probability anatural scenario for the framework of generative flow networks (GFlowNets). To this end, wepropose the Diffusion Alignment with GFlowNet (DAG) algorithm to post-train diffusionmodels with black-box property functions. Extensive experiments on Stable Diffusion andvarious reward specifications corroborate that our method could effectively align large-scaletext-to-image diffusion models with given reward information. Our code is public availableat",
  ": Generated samples before (top) and after(bottom) the proposed training with Aesthetic reward": "Diffusion models (Sohl-Dickstein et al., 2015; Hoet al., 2020) have drawn significant attention in ma-chine learning due to their impressive capability togenerate high-quality visual data and applicabilityacross a diverse range of domains, including text-to-image synthesis (Rombach et al., 2021), 3D genera-tion (Poole et al., 2022), material design (Yang et al.,2023), protein conformation modeling (Abramsonet al., 2024), and continuous control (Janner et al.,2022). These models, through a process of graduallydenoising a random distribution, learn to replicatecomplex data distributions, showcasing their robust-ness and flexibility. The traditional training of diffu-sion models typically relies on large datasets, from which the models learn to generate new samples thatmimic and interpolate the observed examples. However, such a dataset-dependent approach often overlooks the opportunity to control and direct thegeneration process towards outputs that not only resemble the training data but also possess specific, desirableproperties (Lee et al., 2023). These properties are often defined through explicit reward functions that assesscertain properties, such as the aesthetic quality of images. Such a requirement is crucial in fields whereadherence to particular characteristics is necessary, such as alignment or drug discovery. The need to integrateexplicit guidance without relying solely on datasets presents a unique challenge for training methodologies.",
  "Published in Transactions on Machine Learning Research (12/2024)": "Kimin Lee, Hao Liu, Moonkyung Ryu, Olivia Watkins, Yuqing Du, Craig Boutilier, P. Abbeel, MohammadGhavamzadeh, and Shixiang Shane Gu. Aligning text-to-image models using human feedback. ArXiv,abs/2302.12192, 2023. Dianbo Liu, Moksh Jain, Bonaventure F. P. Dossou, Qianli Shen, Salem Lahlou, Anirudh Goyal, NikolayMalkin, Chris C. Emezue, Dinghuai Zhang, Nadhir Hassen, Xu Ji, Kenji Kawaguchi, and Yoshua Bengio.Gflowout: Dropout with generative flow networks. In International Conference on Machine Learning, 2022.",
  "Diffusion models": "Denoising diffusion model (Vincent, 2011; Sohl-Dickstein et al., 2015; Ho et al., 2020; Song et al., 2020) is aclass of hierarchical latent variable models. The latent variables are initialized from a white noise xT N(0, I)and then go through a sequential denoising (reverse) process p(xt1|xt). Therefore, the resulting generateddistribution takes the form of",
  "t=1p(xt1|xt) dx1:T .(1)": "On the other hand, the variational posterior q(x1:T |x0), also called a diffusion or forward process, can befactorized as a Markov chain Tt=1 q(xt|xt1) composed by a series of conditional Gaussian distributionsq(xt|xt1) = N(xt; t/t1xt1, (1 2t/2t1)I), where {t, t}t is a set of pre-defined signal-noise schedule.Specifically, in Ho et al. (2020) we have 2t + 2t = 1. The benefit of such a noising process is that its marginalhas a simple close form: q(xt|x0) =q(x1:t|x0) dx1:t1 = N(xt; tx0, 2t I).",
  "Ldenoising() = Et,x0pdata,N(0,I)x0 x(tx0 + t, t)2,(2)": "where x(xt, t) is a deep neural network to predict the original clean data x0 given the noisy in-put xt=tx0 + t, which can be used to parameterize the denoising process p(xt1|xt)=N(xt1;2t1txt + (2t1 2t)x(xt, t)/2t t1, (1 2t/2t1)I). In practice, the network can also beparameterized with noise prediction or v-prediction (Salimans & Ho, 2022). The network architecture usuallyhas a U-Net (Ronneberger et al., 2015) structure. In multimodal applications such as text-to-image tasks, the denoising diffusion model would have a conditioningc in the sense of p(x0; c) =p(xT ) Tt=1 p(xt1|xt; c) dx1:T . The data prediction network, x(xt, t, c) inthis case, will also take c as a conditioning input. We ignore the notation of c without loss of generality.",
  "GFlowNets": "Generative flow network (Bengio et al., 2021, GFlowNet) is a high-level algorithmic framework of amortizedinference, also known as training generative models with a given unnormalized target density function. LetG = (S, A) be a directed acyclic graph, where S is the set of states and A S S are the set of actions.We assume the environmental transition is deterministic, i.e., one action would only lead to one next state.There is a unique initial state s0 S which has no incoming edges and a set of terminal states sN withoutoutgoing edges. A GFlowNet has a stochastic forward policy PF (s|s) for transition (s s) as a conditionaldistribution over the children of a given state s, which can be used to induce a distribution over trajectories viaP() = N1n=0 PF (sn+1|sn), where = (s0, s1, . . . , sN). On the other hand, the backward policy PB(s|s) is adistribution over the parents of a given state s. The terminating distribution defined by PT (x) = x PF ()is the ultimate terminal state distribution generated by the GFlowNet. The goal of training GFlowNet is toobtain a forward policy such that PT () R(), where R() is a black-box reward function or unnormalizeddensity that takes only non-negative values. Notice that we do not know the normalizing factor Z =",
  "F(s)PF (s|s) = F(s)PB(s|s),(s s) A.(3)": "Furthermore, for any terminating state x, we require F(x) = R(x). In practice, these constraints can betransformed into tractable training objectives, as will be shown in . Based on GFlowNet theories inBengio et al. (2023), if the DB criterion is satisfied for any transition, then the terminating distribution PT ()will be the same desired target distribution whose density is proportional to R().",
  "at = xT t1,r(st, at) = R(st+1, c) only if t = T 1,p(st+1|st, at) = at c.(5)": "Here st, at is the state and action at time step t under the context of MDP. The state space is defined to bethe product space (denoted by ) of x in reverse time ordering and conditional prompt c. The RL policy is just the denoising conditional distribution. In this MDP, when time t has not reached the terminal step,we define the reward r(st, at) to be 0. here denotes the Dirac distribution. Remark 1 (diffusion model as GFlowNet). This formulation has a direct connection to the GFlowNetMDP definition in .2, which has been pointed out by Zhang et al. (2022a) and developed inLahlou et al. (2023); Zhang et al. (2023b); Venkatraman* et al. (2024). To be specific, the action transition(st, at) st+1 is a Dirac distribution and can be directly linked with the (st st+1) edge transition in theGFlowNet language. More importantly, the conditional distribution of the denoising process p(xT t1|xT t)corresponds to the GFlowNet forward policy PF (st+1|st), while the conditional distribution of the diffusionprocess q(xT t|xT t1) corresponds to the GFlowNet backward policy PB(st|st+1). Besides, xt is a GFlowNetterminal state if and only if t = 0.",
  "Diffusion alignment with GFlowNets": "In this section, we describe our proposed algorithm, diffusion alignment with GFlowNets (DAG). Ratherthan directly optimizing the reward targets as in RL, we aim to train the generative models so that in theend they could generate objects with a probability proportional to the reward function: p(x0) R(x0). Toachieve this, we construct the following DB-based training objective based on Equation 3, by regressing itsone side to another in the logarithm scale for any diffusion step transition (xt, xt1).",
  "DB(xt, xt1) = (log F(xt, t) + log p(xt1|xt, t) log F(xt1, t 1) log q(xt|xt1))2(6)": "We additionally force F(xt, t = 0) = R(x0) to introduce the reward signal. Here , are the parametersof the diffusion U-Net model and the GFlowNet state flow function (which is another neural network),respectively. One can prove that if the optimization is perfect, the resulting model will generate a distributionwhose density value is proportional to the reward function R() (Bengio et al., 2023; Zhang et al., 2023b).",
  ": until some convergence condition": "One way to parameterize the state flow function F is throughthe so-called forward-looking (Pan et al., 2023b, FL) tech-nique in the way of F(xt, t) = F(xt, t)R(xt), where F isthe actual neural network to be learned. Intuitively, this isequivalent to initializing the state flow function to be thereward function in a functional way; therefore, learning ofthe state flow would become an easier task. Note that toensure F(x0, 0) = R(x0), we need to force F(x0, 0) = 1for all x0 at the terminal step. Incorporating denoising diffusion-specific structureHowever, the intermediate state xt is noisy under our context,and thus not appropriate for being evaluated by the givenreward function, which would give noisy result. Whatsmore, what we are interested here is to foresee the rewardof the terminal state x0 taken from the (partial) trajectoryxt:0 starting from given xt. As a result, we can do the FLtechnique utilizing the particular structure of diffusion modelas in F(xt, t) = F(xt, t)R(x(xt, t)), where x is the dataprediction network. We notice that a similar technique hasbeen used to improve classifier guidance (Bansal et al., 2023). In short, our innovation in FL technique is",
  ".(8)": "Since in this work the reward function is a black-box, the gradient flow would not go through x(xt, t) whenwe take the gradient of . We summarize the algorithm in Algorithm 1 and refer to it as DAG-DB. Remark 2 (GPU memory and the choice of GFlowNet objectives). Similar to the temporal difference- inRL (Sutton, 1988), it is possible to use multiple connected transition steps rather than a single transition stepto construct the learning objective. Other GFlowNet objectives such as Malkin et al. (2022); Madan et al.(2022) use partial trajectories with a series of transition steps to construct the training loss and provide adifferent trade-off between variance and bias in credit assignment. However, for large-scale setups, this is noteasy to implement, as computing policy probabilities for multiple transitions would correspondingly increasethe GPU memory and computation multiple times. For example, in the Stable Diffusion setting, we couldonly use a batch size of 8 on each GPU for single transition computation. If we want to use a two transitionbased training loss, we would need to decrease the batch size by half to 4. Similarly, we will have to shortenthe trajectory length by a large margin if we want to use trajectory balance. This may influence the image",
  "We defer its proof to Section A.1 and make the following remarks:": "Remark 5 (gradient equivalence to detailed balance). Recalling Equation 6, since we have DB(xt, xt1) =b(xt, xt1) log p(xt1|xt), it is clear that this KL-based objective would lead to the same expected gradienton with Equation 6, if xt1 p(|xt) (i.e., samples being on-policy). Nonetheless, this on-policy propertymay not be true in practice since the current model is usually not the same as the model used for rollouttrajectories after a few optimization steps. Remark 6 (analysis of b(xt, xt1)). The term b(xt, xt1) seems to serve as the traditional reward in theRL framework. Previous works (Tiapkin et al., 2024; Mohammadpour et al., 2024; Deleu et al., 2024) haveshown that GFlowNet could be interpreted as solving a maximum entropy RL problem in a modified MDP,where any intermediate transition is modified to have an extra non-zero reward that equals the logarithmof GFlowNet backward policy: rmod(xt, xt1) = log q(xt|xt1), t > 0. Whats more, the logarithm of thetransition flow function (i.e., either side of the detailed balance constraint of Equation 3) could be interpretedas the Q-function of this maximum entropy RL problem on the modified MDP. Therefore, we could take amore in-depth analysis of b(xt, xt1):",
  ".(12)": "The last term F(xt, t) is a constant for the xt1 p(|xt) process. Consequently, we can see that min-imizing this KL is effectively equivalent to having a REINFORCE gradient with the modified Q-functionplus an entropy regularization (Haarnoja et al., 2018).Whats more, the entropy of forward policyExt1p(|xt) [ log p(xt1|xt)] is actually also a constant (the diffusion models noise schedule is fixed),which does not affect the learning. Note that this REINFORCE style objective in Equation 11 is on-policy; the data has to come from the samedistribution as the current model. In practice, the model would become not exactly on-policy after a few",
  ".(15)": "We use this to update the policy parameter and use FL-DB to only update . We call this diffusionalignment with GFlowNet and REINFORCE gradient method to be DAG-KL. Note that when calculatingb(xt, xt1), we also adopt the diffusion-specific FL technique developed in .2. We also put thealgorithmic pipeline of DAG-KL in Algorithm 1.",
  "Related Works": "Diffusion alignmentPeople have been modeling human values to a reward function in areas such asgame (Ibarz et al., 2018) and language modeling (Bai et al., 2022) to make the model more aligned. Indiffusion models, early researchers used various kinds of guidance (Dhariwal & Nichol, 2021; Ho & Salimans,2022; Kong et al., 2024) to achieve the goal of steerable generation under the reward. This approach isas simple as plug-and-play but requires querying the reward function during inference time. Another wayis to post-train the model to incorporate the information from the reward function, which has a differentsetup from guidance methods; there is also work showing that this outperforms guidance methods (Ueharaet al., 2024). Lee et al. (2023); Dong et al. (2023) achieve this through maximum likelihood estimation onmodel-generated samples, which are reweighted by the reward function. These works could be thought ofas doing RL in one-step MDPs. Black et al. (2023); Fan et al. (2023) design RL algorithm by taking the",
  "Method": "DAG-DBDAG-KLDDPO : Sample efficiency results of our proposed methods and our RL baseline (DDPO). The number oftraining steps is proportional to the number of sampled trajectories. The experiments are conducted onreward functions including aesthetic score, ImageReward, and HPSv2. diffusion generation process as a MDP (.1). In this work, we focus on black-box rewards where it isappropriate to use RL or GFlowNet methods. Furthermore, there are methods developed specifically fordifferentiable rewards setting (Clark et al., 2023; Wallace et al., 2023b; Prabhudesai et al., 2023; Wu et al.,2024; Xu et al., 2023; Uehara et al., 2024; Marion et al., 2024). Besides, Chen et al. (2023) study the effect offinetuning text encoder rather than diffusion U-Net. There is also work that relies on preference data ratherthan an explicit reward function (Wallace et al., 2023a; Yang et al., 2024). Kim et al. (2024a) investigate howto obtain a robust reward based on multiple different reward functions. GFlowNetsGFlowNet is a family of generalized variational inference algorithms that treats the datasampling process as a sequential decision-making one. It is useful for generating diverse and high-qualitysamples in structured scientific domains (Jain et al., 2022; 2023b; Liu et al., 2022; Jain et al., 2023a; Shenet al., 2023; Zhang et al., 2023e; Pan et al., 2023a; Kim et al., 2023; 2024b). A series of works have studiedthe connection between GFlowNets and probabilistic modeling methods (Zhang et al., 2022b; Zimmermannet al., 2022; Malkin et al., 2023; Zhang et al., 2022a; Ma et al.; Zhang et al., 2024a), and between GFlowNetsand control methods (Pan et al., 2023c;d;b; Zhang et al., 2024b; Tiapkin et al., 2024). GFlowNets also havewide application in causal discovery Deleu et al. (2022), phylogenetic inference (Zhou et al., 2024), andcombinatorial optimization (Zhang et al., 2023a;d). A concurrent work (Venkatraman* et al., 2024) alsostudies GFlowNet on diffusion alignment which is similar to this work but has different scope and differentdeveloped algorithm. Specifically, this work is aiming for posterior approximate inference that the rewardfunction is treated as likelihood information, and develops a trajectory balance (Malkin et al., 2022) basedalgorithm on length modified trajectories.",
  ": Sample efficiency results of our proposed methodsand our RL baseline (DDPO) on learning from compressibil-ity and incompressibility rewards": "Experimental setupsWe choose Stable Dif-fusion v1.5 (Rombach et al., 2021) as our basegenerative model. For training, we use low-rank adaptation (Hu et al., 2021, LoRA) forparameter efficient computation. As for thereward functions, we do experiments with theLAION Aesthetics predictor, a neural aestheticscore trained from human feedback to givean input image an aesthetic rating. For text-image alignment rewards, we choose ImageRe-ward (Xu et al., 2023) and human preferencescore (HPSv2) (Wu et al., 2023). They are both CLIP (Radford et al., 2021)-type models, taking a text-imagepair as input and output a scalar score about to what extent the image follows the text description. Wealso test with the (in)compressibility reward, which computes the file size if the input image is stored inhardware storage. As for the prompt distribution, we use a set of 45 simple animal prompts from Black et al.",
  "Several cars drive down theroad on a cloudy day": ": Text-image alignment results. We display four prompts and the corresponding generation visualiza-tion from the original Stable Diffusion (1st row), DDPO (2nd row), DAG-DB (3rd row), and DAG-KL (4throw) models to compare their alignment abilities. See for more results. (2023) for the Aesthetics task; we use the whole imagenet classes for the (in)compressibility task; we usethe DrawBench (Saharia et al., 2022) prompt set for the ImageReward task; we use the photo and paintingprompts from the human preference dataset (HPDv2) (Wu et al., 2023) for the HPSv2 task. We notice thatin our experiments, we use prompt set containing hundreds of prompts which is more than some previouswork such as Black et al. (2023). Effectiveness of the proposed methodsWe first demonstrate that our proposed methods could generatedimages that have meaningful improvements corresponding to the rewards being used. In , we comparethe images from the original Stable Diffusion pretrained model and our proposed method. After our post-training, the generated images become more vibrant and vivid; we also notice that these images have slightlyhigher saturation, which we believe is aligned with the human preference on good-looking pictures. Wealso visualize the experiment results on compressibility and incompressibility tasks in . The secondrow shows the generated images from the model trained with the compressibility reward, which have lowdetails and smooth textures, and also have very limited colors. On the other hand, the model trained withincompressibility reward would generate images with high frequency texture, as shown in the third row. Theseresults indicate that our method could effectively incorporate the reward characteristics into the generativemodels. We defer more experimental details to Section B.2. Algorithmic comparisonsThe main baseline we compare with is denoising diffusion policy optimiza-tion (Black et al., 2023, DDPO), an RL algorithm that is specifically designed for denoising diffusion alignmentand has been shown to outperform other align-from-black-box-reward methods including (Lee et al., 2023;Fan et al., 2023). We show the reward curves w.r.t. the training steps of the aesthetic, ImageReward,and HPSv2 rewards in . Here, the number of training steps corresponds proportionally to thenumber of trajectories collected (see appendix for more details). Both our proposed methods, DAG-DB and",
  "DDPO samples": ": Visualization of alignment with regard to training progress. Left: the generated images from theproposed method become more aligned to the text prompt over the course of training. Right: samples fromthe DDPO baseline. DAG-KL, achieve faster credit assignment than the DDPO baseline by a large margin. We additionallyput corresponding curve plots for compressibility and incompressibility rewards in , which alsodemonstrate the advantages of our proposed methods. Whats more, we provide a diversity comparison in . For the RL baseline, we take the last checkpoint;for our proposed methods, we take the earliest checkpoint with a larger reward value than the chosen RLalgorithm checkpoint. This is to show that our methods could achieve results with both better rewardand diversity performance. For diversity measurement, we calculate the FID score between two batches ofindependently generated samples from that model. We use this as a diversity metric, so the larger the better.The results indicate that GFlowNet-based methods achieve a better reward-diversity trade-off due to theirdistribution matching (rather than reward maximizing) formulation. We defer related details to Section B.2. Apart from quantitative comparisons, we also visualize the alignment improvement for models trained inthe HPSv2 task. In and in Appendix, we exhibit generation results for different promptsacross the original Stable Diffusion, DDPO, DAG-DB, and DAG-KL models. For example, in the first acounter top with food sitting on some towels example, images from the original Stable Diffusion either do nothave food or the food is not on towels, which is also the case for DDPO generation. This is improved for bothDAG-DB and DAG-KL generation in that they capture the location relationship correctly. In the personalcomputer desk room with large glass double doors example, both the original and DDPO models cannotgenerate any double doors in the image, and DAG-DB model sometimes also fails. In contrast, the DAG-KLmodel seems to understand the concept well. Generation with other prompts also has similar results. In , we visualize the gradual alignment improvement of our DAG-KL method with regard to thetraining progress for the HPSv2 task. We show the images of our methods at 0%, 25%, 50%, 75%, and 100%training progress. In the example of a helmet-wearing monkey skating, the DDPO baseline could generate askating monkey but seems to fail to generate a helmet. For the proposed method, the model gradually learns",
  "Conclusion": "We propose Diffusion Alignment GFlowNet (DAG), a family of algorithms designed to fine-tune pretraineddiffusion models based on external reward functions. Our approach advances the GFlowNet framework toaccommodate the specific properties of text-to-image diffusion models. We introduce two variants, DAG-DBand DAG-KL, each tailored to optimize the models performance with respect to different objectives. Throughextensive experiments on Stable Diffusion models, we demonstrate that DAG achieves a more effective balancebetween maximizing reward values and maintaining output diversity than previous methods. Josh Abramson, Jonas Adler, Jack Dunger, Richard Evans, Tim Green, Alexander Pritzel, Olaf Ronneberger,Lindsay Willmore, Andrew J Ballard, Joshua Bambrick, et al. Accurate structure prediction of biomolecularinteractions with alphafold 3. Nature, pp. 13, 2024. Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain,Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion, TomConerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, ScottJohnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown, JackClark, Sam McCandlish, Chris Olah, Ben Mann, and Jared Kaplan. Training a helpful and harmlessassistant with reinforcement learning from human feedback, 2022. Arpit Bansal, Hong-Min Chu, Avi Schwarzschild, Soumyadip Sengupta, Micah Goldblum, Jonas Geiping, andTom Goldstein. Universal guidance for diffusion models. 2023 IEEE/CVF Conference on Computer Visionand Pattern Recognition Workshops (CVPRW), pp. 843852, 2023. Emmanuel Bengio, Moksh Jain, Maksym Korablyov, Doina Precup, and Yoshua Bengio. Flow network basedgenerative models for non-iterative diverse candidate generation. Neural Information Processing Systems(NeurIPS), 2021.",
  "Minsu Kim, Joohwan Ko, Taeyoung Yun, Dinghuai Zhang, Ling Pan, Woochang Kim, Jinkyoo Park,Emmanuel Bengio, and Yoshua Bengio. Learning to scale logits for temperature-conditional gflownets,2024b": "Lingkai Kong, Yuanqi Du, Wenhao Mu, Kirill Neklyudov, Valentin De Bortol, Haorui Wang, Dongxia Wu,Aaron Ferber, Yi-An Ma, Carla P Gomes, et al. Diffusion models as constrained samplers for optimizationwith unknown constraints. arXiv preprint arXiv:2402.18012, 2024. Salem Lahlou, Tristan Deleu, Pablo Lemos, Dinghuai Zhang, Alexandra Volokhova, Alex Hernndez-Garca,Lna Nhale Ezzine, Yoshua Bengio, and Nikolay Malkin. A theory of continuous generative flow networks.International Conference on Machine Learning (ICML), 2023.",
  "Nikolay Malkin, Moksh Jain, Emmanuel Bengio, Chen Sun, and Yoshua Bengio. Trajectory balance: Improvedcredit assignment in GFlowNets. Neural Information Processing Systems (NeurIPS), 2022": "Nikolay Malkin, Salem Lahlou, Tristan Deleu, Xu Ji, Edward Hu, Katie Everett, Dinghuai Zhang, and YoshuaBengio. GFlowNets and variational inference. International Conference on Learning Representations(ICLR), 2023. Pierre Marion, Anna Korba, Peter Bartlett, Mathieu Blondel, Valentin De Bortoli, Arnaud Doucet, FelipeLlinares-Lpez, Courtney Paquette, and Quentin Berthet. Implicit diffusion: Efficient optimization throughstochastic sampling, 2024. Sobhan Mohammadpour, Emmanuel Bengio, Emma Frejinger, and Pierre-Luc Bacon. Maximum entropygflownets with soft q-learning. In International Conference on Artificial Intelligence and Statistics, pp.25932601. PMLR, 2024.",
  "Mihir Prabhudesai, Anirudh Goyal, Deepak Pathak, and Katerina Fragkiadaki. Aligning text-to-imagediffusion models with reward backpropagation, 2023": "Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferablevisual models from natural language supervision. In International Conference on Machine Learning, 2021. Robin Rombach, A. Blattmann, Dominik Lorenz, Patrick Esser, and Bjrn Ommer. High-resolution imagesynthesis with latent diffusion models. 2022 IEEE/CVF Conference on Computer Vision and PatternRecognition (CVPR), pp. 1067410685, 2021.",
  "Richard S Sutton. Learning to predict by the methods of temporal differences. Machine Learning, 3(1):944,1988": "Daniil Tiapkin, Nikita Morozov, Alexey Naumov, and Dmitry P Vetrov. Generative flow networks asentropy-regularized rl. In International Conference on Artificial Intelligence and Statistics, pp. 42134221.PMLR, 2024. Masatoshi Uehara, Yulai Zhao, Kevin Black, Ehsan Hajiramezanali, Gabriele Scalia, Nathaniel Lee Diamant,Alex M Tseng, Tommaso Biancalani, and Sergey Levine. Fine-tuning of continuous-time diffusion modelsas entropy-regularized control, 2024. Siddarth Venkatraman*, Moksh Jain*, Luca Scimeca*, Minsu Kim*, Marcin Sendera*, Mohsin Hasan, LukeRowe, Sarthak Mittal, Pablo Lemos, Emmanuel Bengio, Alexandre Adam, Jarrid Rector-Brooks, YoshuaBengio, Glen Berseth, and Nikolay Malkin. Amortizing intractable inference in diffusion models for vision,language, and control. 2024.",
  "Pascal Vincent. A connection between score matching and denoising autoencoders. Neural Computation, 23:16611674, 2011. URL": "Bram Wallace, Meihua Dang, Rafael Rafailov, Linqi Zhou, Aaron Lou, Senthil Purushwalkam, StefanoErmon, Caiming Xiong, Shafiq Joty, and Nikhil Naik. Diffusion model alignment using direct preferenceoptimization, 2023a. Bram Wallace, Akash Gokul, Stefano Ermon, and Nikhil Vijay Naik. End-to-end diffusion latent optimizationimproves classifier guidance. 2023 IEEE/CVF International Conference on Computer Vision (ICCV), pp.72467256, 2023b. Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng Zhu, Rui Zhao, and Hongsheng Li. Humanpreference score v2: A solid benchmark for evaluating human preferences of text-to-image synthesis. ArXiv,abs/2306.09341, 2023.",
  "Dinghuai Zhang, Ricky T. Q. Chen, Nikolay Malkin, and Yoshua Bengio. Unifying generative models withGFlowNets and beyond. arXiv preprint arXiv:2209.02606v2, 2022a": "Dinghuai Zhang, Nikolay Malkin, Zhen Liu, Alexandra Volokhova, Aaron Courville, and Yoshua Bengio.Generative flow networks for discrete probabilistic modeling. International Conference on Machine Learning(ICML), 2022b. Dinghuai Zhang, Ricky Tian Qi Chen, Cheng-Hao Liu, Aaron C. Courville, and Yoshua Bengio. Diffusiongenerative flow samplers: Improving learning signals through partial trajectory optimization. ArXiv,abs/2310.02679, 2023b.",
  "B.1RL optimal solutions of the denoising MDP": "Training a standard RL algorithm within this diffusion MDP in .1 to perfection means the modelwould only generate a single trajectory with the largest reward value. This usually comes with the disadvantageof mode collapse in generated samples in practice. One direct solution is soft / maximum entropy RL (Ziebartet al., 2008; Fox et al., 2017; Haarnoja et al., 2017; Zhang et al., 2023c), whose optimal solution is a trajectory-level distribution and the probability of generating each trajectory is proportional to its trajectory cumulativereward, i.e., p(x0:T ) t Rt(xt) = R(x0). However, in theory this means p(x0) =p(x0:T ) dx1:T R(x0) dx1:T = R(x0) 1 dx1:T , which is not a well-defined finite term for unbounded continuous spaces.In contrast, the optimal solution of GFlowNet is p(x0) R(x0).",
  "B.2Experimental details": "Regarding training hyperparameters, we follow the DDPO github repository implementation and describethem below for completeness. We use classifier-free guidance (Ho & Salimans, 2022, CFG) with guidanceweight being 5. We use a 50-step DDIM schedule. We use NVIDIA 8A100 80GB GPUs for each task,and use a batch size of 8 per single GPU. We do 4 step gradient accumulation, which makes the essentialbatch size to be 256. For each epoch, we sample 512 trajectories during the rollout phase and perform 8optimization steps during the training phase. We train for 100 epochs. We use a 3 104 learning rate forboth the diffusion model and the flow function model without further tuning. We use the AdamW optimizerand gradient clip with the norm being 1. We set = 1 104 in Equation 14. We use bfloat16 precision. The GFlowNet framework requires the reward function to be always non-negative, so we just take theexponential of the reward to be used as the GFlowNet reward. We also set the reward exponential to = 100(i.e., setting the distribution temperature to be 1/100). Therefore, log R() = Roriginal(). Note that inGFlowNet training practice, we only need to use the logarithm of the reward rather than the original rewardvalue. We linearly anneal from 0 to its maximal value in the first half of the training. We found that thisalmost does not change the final result but is helpful for training stability.For DAG-KL, we put the final coefficient on the KL gradient term. We also find using a KL regularization DKL (p(xt1|xt)pold(xt1|xt))to be helpful for stability (this is also mentioned in Fan et al. (2023)). In practice, it is essentially adding a2 regularization term on the output of the U-Net after CFG between the current model and previous rolloutmodel. We simply use a coefficient 1 on this term without further tuning.",
  "DAG": ": Samples on CIFAR-10 diffusion alignment experiments.The reward function is the probability of the generated imagefalling into the categories of car, truck, ship, and plane calculatedby a pretrained classifier. The RL baseline shows mode collapsebehaviors while the target distribution is actually multimodal. Training step 4.5 5.0 5.5 6.0 6.5 7.0 7.5",
  ":Ablation study for theforward-looking (FL) usage in Sec-tion 3.2 on the Aesthetic reward task": "We use Stable Diffusion v1.5 as base model and use LoRA for post-training, following Black et al. (2023). Forthe architecture of the state flow function, we take a similar structure to the downsample part of the U-Net.The implementation is based on the hugging face diffusers package. We use 3 CrossAttnDownBlock2Dblocks and 1 DownBlock2D and do downsampling on all of them. We set the layers per block to be 1, andset their block out channels to be 64, 128, 256, 256. We use a final average pooling layer with kernel and stridesize 4 to output a scalar given inputs including latent image, time step, and prompt embedding. We do notreport diversity metric as in previous GFlowNet literature, as the average pairwise Euclidean distance in highdimensional space (64 64 4 > 10, 000 dim.) is not a meaningful metric. For computing diversity in , we take a trained model and independently generate two batches ofimages based on corresponding prompts, with the batch size being 2048. For our proposed methods, wechoose the earliest checkpoint with a reward larger than the DDPO final rewards. We save our checkpointsfor every 10 epochs. We use an Inception network to compute the mean and covariance features and calculatethe Frechet distance; then we use the resulting FID as the diversity metric. Additionally, we show in that DAG-KL achieves a better Pareto frontier on the reward-diversity trade-off on the HPSv2 task.",
  "Reward": "15.0 17.5 20.0 22.5 25.0 27.5 30.0 32.5 35.0 FID DAG-DBDAG-KLDDPO : Scatter plot about model diversityand reward when they have been trained fordifferent number of epochs (shown besideeach of the points).It can be seen thatour method (DAG-KL) achieves a betterreward-diversity trade-off. We also include a toy experiment on a CIFAR-10 pretrainedDDPM1. We train a ResNet18 classifier and set the rewardfunction to be the probability of the generated image fallinginto the categories of car, truck, ship, and plane. We use thesame hyperparameters with the Stable Diffusion setting, exceptwe only use 1 GPU with a 256 batch size for each run withoutgradient accumulation. We illustrate the generation results in. We use DAG-DB here, and the DAG-KL generationis similar and non-distinguishable with it. We can see thatin this relative toy task, the RL baseline easily optimizes theproblem to extremes and behaves mode collapse to some extent(only generating samples of a particular plane). While for ourmethods, the generation results are diverse and cover differentclasses of vehicles. Both methods achieve average log probabilitylarger than 0.01, which means the probability of falling intotarget categories are very close to 1.",
  "B.4More results": "In , we perform an ablation study on the proposed denoising diffusion-specific way of forward-lookingtechnique for the Aesthetic score task. Specifically, we compare the left and right hand sides of Equation 7,where we use naive FL to refer to the left hand side, and ours for the right hand side, as it is just theDAG-DB method in the main text. We also show the performance of not using the forward-looking techniqueand call it without FL in the figure. We can see that both the variants cannot achieve effective learningcompared to our proposed method.",
  "In , we put more visualization comparisons about the text-image alignment performance from themodels trained on the HPSv2 reward, with a similar form to": "In of our paper, we have discussed the reason to use the DDPO baseline as it outperforms otheralign-from-black-box methods including DPOK (Fan et al., 2023). For completeness, we conduct an experimentof DPOK on the Aesthetic score experiment. We keep the same experimental setup as the ones in ;while DAG-DB, DAG-KL, and DDPO achives a score of 7.6587, 7.6596, and 6.3178 respectively, DPOKobtains a result of 6.1876. This result shows that our method is superior to other baselines."
}