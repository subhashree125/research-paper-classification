{
  "Abstract": "Stochastic processes defined on integer valued state spaces are popular within the physicaland biological sciences. These models are necessary for capturing the dynamics of smallsystems where the individual nature of the populations cannot be ignored and stochasticeffects are important. The inference of the parameters of such models, from time series data,is challenging due to intractability of the likelihood. To work at all, current simulation basedinference methods require the generation of realisations of the model conditional on the data,which can be both tricky to implement and computationally expensive. In this paper weinstead construct a neural likelihood approximation that can be trained using unconditionalsimulation of the underlying model, which is much simpler. We demonstrate our method byperforming inference on a number of ecological and epidemiological models, showing thatwe can accurately approximate the true posterior while achieving significant computationalspeed ups compared to current best methods.",
  "Introduction": "Mechanistic models where the state consists of integer values are ubiquitous across all scientific domains,but particularly in the physical and biological sciences (van Kampen, 1992; Wilkinson, 2018). Integer valuedstates are important when the population being modelled is small and so the individual nature of thepopulation cannot be ignored. Models of these types are used extensively for modelling in epidemiology(Allen, 2017), ecology (McKane & Newman, 2005), chemical reactions (Wilkinson, 2018), queuing networks(Breuer & Baum, 2005) and gene regulatory networks (Shea & Ackers, 1985), to name but a few examples. Typically the main interest is in learning or inferring the model parameters, rather than unobserved latentstates, as the parameters are directly related to the individual level mechanisms that drive the overalldynamics.In spite of this, most methods for actually performing such inference typically still involvegenerating realisations of the full hidden process, commonly known as simulation based inference (Cranmeret al., 2020). Particle marginal Metropolis Hastings (Doucet et al., 2015; Chopin & Papaspiliopoulos, 2020)and approximate Bayesian computation (Sisson et al., 2019) are two well known methods in this class. As with most state-space models, the underlying state is only partially observed, but a particular feature ofmany integer valued models is that the subset of the state that is observed is done so accurately. The typicalsituation includes highly informative observations of parts of the state, while other parts are completely",
  "Published in Transactions on Machine Learning Research (10/2024)": "should correspond to the model learning the dynamics of the individual species. If we centred the data,then extinctions would not correspond to zeroing the weights. We also attempted to order the prey beforethe predators in the autoregressive model, but found that there was not a large difference between the twoorderings. For PMMH, we used a bootstrap particle filter (Kitagawa, 1996), using 100, 200 and 250 particles forr = 0.5, 0.7 and 0.9 respectively. We used 180,000 MCMC iterations for each experiment, which producedan ESS of greater than 500 for all parameters. Generating a large ESS for this data is difficult due to theextremely high posterior correlation between d1 and p1, and the computational time per MCMC iterationis quite high for these experiments. We estimated the covariance matrix for the MCMC kernel from a priorSNL run, and tuned further based on a pilot run. Without the SNL run to inform the initial covariancematrix, tuning the MCMC proposal would likely take huge amounts of computational time, since a proposalwith diagonal covariance would mix extremely slowly with the highly correlated parameters in the posterior.",
  "Problem Setup": "The models we consider are continuous-time Markov chains (CTMC), {X(t), t 0}, also known as a Markovjump process (Ross, 2000; Wilkinson, 2018). The state of the system, X(t), is a vector of positive integers,which typically represent populations sizes or counts of events. The dynamics of such models are specifiedby describing the possible transitions in the state and the rates at which these occur. The possible statetransitions are fixed for a given model, but the rates depend on small number of parameters, , some or allof which we wish to infer from partial time-series data, y1:n, generated from the model. Our observationmodel is that parts of the state are observed exactly, but other parts not at all. Formally the observationlikelihood can be written as a product of delta functions",
  "where xj represents the jth component of the state and B is the set of components that are assumed observed": "Although the models evolve in continuous time, we assume that observations are made at discrete timeintervals and only depend on the state of the system at that time, so the inference problem fits within thea state-space model (SSM) framework (Del Moral, 2004; Murphy, 2023). Denoting the latent state and theobserved data at time step i as xi and yi respectively, then the joint likelihood is given by",
  "Methods": "In this section, we construct an autoregressive model to approximate p(y1:n|). The simplest version ofthe model works with univariate time series data y1:n and is described in .1. We break up theconstruction into two parts: First, in .1.1 we describe a CNN architecture using causal convolutions(van den Oord et al., 2016), which is used to map the input sequence y1:n to an output sequence o1:n such thatoi does not look into the future (it is only dependent on y1:i and ). Second, in .1.2 we describe away to map each oi onto a probability distribution supported on the set of integers Z (or a subset thereof), sothat we can use oi to approximate p(yi+1|y1:i, ). In .2, we describe how the autoregressive modelcan be extended to multidimensional integer valued time series y1:n. In particular, this extension allows theautoregressive model to model correlations between the features of yi. Finally, in .3 we providean overview of sequential neural likelihood (SNL), which is an algorithm for simultaneously training thesurrogate likelihood and performing approximate inference (Papamakarios et al., 2019). We also discussingsome modifications to SNL to help increase consistency in the approximate posterior.",
  "Autoregressive Model": "Autoregressive models approximate the density of a time series y1:n by modelling a sequence of conditionals,namely p(yi|y1:i1), exploiting causality in the time series (Shumway & Stoffer, 2017).We will denotethe autoregressive model conditionals by q(yi|y1:i1), where are the autoregressive model parameters1,e.g. neural network weights. Suppressing the dependence on , the joint density for the likelihood can befactorised as",
  "j=1log qy(j)1:n+ C,": "where C is a constant with respect to . Since the KL divergence is non-negative and equal to 0 if and onlyif q(y1:n) = p(y1:n) (Bishop, 2006), it follows that a sufficiently flexible autoregressive model will be able toapproximate the true density arbitrarily well in the large dataset limit. When conditioning on , the NLLloss is still appropriate, as if p() for some proposal p(), then as pointed out by Papamakarios et al.(2019), in the large N limit",
  "The right hand side is non-negative and equal to zero if and only if p(y1:n|) = q(y1:n|) almost everywhereon the support of p()": "Since we are interested in using q(y1:n|) as a surrogate within an MCMC scheme, where it is repeatedlyevaluated, it is preferable to use a model architecture which allows for efficient evaluation of Equation 3.For univariate time series, CNN based architectures have been proposed which allow for parallel evaluationof all of the terms in Equation 3. These CNNs, called causal CNNs, map the univariate input sequence y1:nto the sequence of log-conditionals (log q(yi|y1:i1))1:n. A typical example is the model WaveNet (van denOord et al., 2016), which uses a causal CNN2 to synthesise audio data. Due to the efficiency of convolutionsin deep learning, we construct our autoregressive model using causal convolutions. Additionally, automaticdifferentiation allows for computation of the gradient of the causal CNN surrogate (with respect to the modelparameters ), and this has the same complexity as the evaluation (Baydin et al., 2017), so efficient MCMCsamplers such as NUTS (Hoffman & Gelman, 2014) can be employed to sample from the posterior ratherthan basic random walk Metropolis Hastings samplers, which are much less efficient.",
  "j=0wkjyij1j<i,": "where we have used to denote the standard convolution operation. It is clear that ck(wkj, y1:n)i dependsonly on the input sequence only through the elements yl(i):i, where l(i) = max{1, i (k 1)}, hence thecausal structure in the input sequence is preserved by ck. By constructing CNN layers using the operationck (assuming that the kernel length is k in every layer), then it is clear that composing these layers will yielda function which also preserves the causal structure of the input sequence. Moreover, denoting the output",
  "Discretised Mixture of Logistics": "To calculate the autoregressive model conditionals from o1:n, we use the components of each oi to compute theparameters of a discretised mixture of logistic distributions (DMoL) (Salimans et al., 2017). The discretisedlogistic (DL) is family of distributions supported on Z, characterised by a shift parameter R and a scaleparameter s > 0. A DL(, s) random variable Y is sampled by first sampling X Logistic(, s), and thencalculating Y = X, where denotes rounding down to the nearest integer. The logistic distributionhas similar properties to the Gaussian distribution, however it has an analytically expressible cumulativedistribution function (CDF), which makes it possible to calculate the probability mass function",
  "i=1i pDL(y; i, si),(4)": "where the i terms are weights satisfying i 1 and ci=1 i = 1. The DMoL serves as an analyticallytractable family of distributions that is flexible enough to approximate a wide class of distributions supportedon Z, analogous to a mixture of Gaussians in the continuous case (Salimans et al., 2017). It can also helpfulto truncate the discretised logistics when a problem involves data supported on a subset {a n b | n Z}.This is easy, as it only involves renormalising the PMF of each DL component in Equation 4, which can bedone analytically, noting that the DL CDF is the same as the logistic CDF restricted to Z.",
  "j ji = 1 for all i, so the conditionalsq(yi|y1:i1, ) are well defined distributions": "For some models we alter the default expressions in Equation 5 to better reflect the particular structure of amodel. For example, in Sections 4.1 and 4.2 the epidemic models we investigate produce count data whichis bounded above by the total population size N, hence we scale sji by (N yi1)/N to reflect the shrinkingsupport of the conditionals over time.",
  "Multivariate Data": "When the time series consists of multivariate observations yi, then constructing the autoregressive modelis less straightforward. The main hurdle is to replace the DMoL distribution used in the univariate casewith a suitable parametric family of multivariate discrete distributions. For yi Zd, d > 1, the conditionalsp(yi|y1:i1, ) can be factorised as follows:",
  "j=2p(yji |y1:j1i, y1:i1, ),(6)": "where yji is the jth component of yi and y1:j1iis a vector of the first j 1 components of yi. It follows thatthe problem of modelling p(yi|y1:i1, ) can be approached by modelling the d univariate conditionals on theright hand side of Equation 6. We will denote the univariate conditionals approximated by the autoregressivemodel by q(yji |y1:j1i, y1:i1, ), and define the full autoregressive model conditionals by",
  "The CNN architecture described in .1.1 is not immediately suitable for calculation ofq(yji |y1:j1i, y1:i1, ). This is because to construct a suitable model for q(yji |y1:j1i, y1:i1, ), the ith": "output element of the CNN oi must decompose in a way such that there is a group of elements which onlydepend on yi through y1:j1i, which can then be used to define q(yji |y1:j1i, y1:i1, ). However using theCNN from .1.1 yields an output oi that is dependent on yi in an arbitrary way, hence oi cannotbe split like this. A simple solution to this issue is to notice that we may unroll the time series y1:n into along univariate time seriesz1:dn =y11, y21, . . . , yd1, y12, . . . , yd1n, ydn,",
  "q(zid+j|z1:id+j1, ) = q(yji |y1:j1i, y1:i1, )": "In practice, we do not unroll the time series, and instead force certain weight matrices in the convolutionkernels of the CNN to have a block lower triangular structure. The block lower triangular structure of theweight matrices is achieved with binary masking as in the density model MADE (Germain et al., 2015). The",
  "Sequential Neural Likelihood": "Sequential neural likelihood (SNL) is a SBI algorithm which uses a NCDE as a surrogate for the truelikelihood within a MCMC scheme (Papamakarios et al., 2019). The main issue that is addressed by SNL ishow to choose the proposal distribution over parameters in the training dataset p(). Ideally, the proposalwould be the posterior over the model parameters so that the surrogate likelihood is well trained in regionswhere the MCMC sampler is likely to explore, without wasting time learning the likelihood in other regions.Obviously the posterior is not available, so instead SNL iteratively refines q by sampling additional trainingdata over a series of rounds. The initial proposal is taken to be the prior, and subsequent proposals aretaken to be an approximation of the posterior, which is sampled from using MCMC with the surrogate fromthe previous round as an approximation of the likelihood. We refer the reader to Papamakarios et al. (2019)for more details on SNL. We use our autoregressive model q(y1:n|) within SNL to perform inference for the experiments describedin . We make a modification to how we represent the SNL posterior, in that we run SNL overr rounds, and keep a mixture of MCMC samples over the final r < r rounds. The resulting approximateposterior is therefore an equally weighted mixture of approximations over successive SNL rounds. We dothis because sometimes the posterior statistics exhibit variability which does not settle down over the SNLrounds, so it can be difficult to assess whether the final round is truly the most accurate. Instead we use anensemble to average out errors present in any given round, which worked well in practice. Variability acrossthe SNL rounds occurs mostly when the length of time series is random, presumably because the randomlengths inflate variability in the training dataset.",
  "Experiments": "We evaluate our methods on simulated data from three different models, comparing the results to the exactsampling method PMMH (Andrieu et al., 2010). The first two models we consider are epidemic models,namely the SIR and SEIAR model, the former being a simple but ubiquitous model in epidemiology (Allen,2017), and the latter being a more complex model which admits symptomatic phases (Black, 2019). Thethird example is the predator-prey model described in McKane & Newman (2005). We perform two sets of experiments using the SIR model; one set considers a single simulated outbreak ina population of size 50, and the other considers multiple independent observations from outbreaks withinhouseholds (small closed populations). The experiments on the single outbreak serve as a proof of conceptfor our method, as PMMH can target the posterior for this data efficiently (Black, 2019). The experimentson household data allow us to assess the scalability of our method with an increasing number of independenttime series, where PMMH does not scale well due to the exponential increase in the number of particlesneeded to keep the variance of the likelihood estimate low enough (Sherlock et al., 2015).",
  "PMMH 1": "We also report a classifier 2-sample t-test (C2ST) score (Lueckmann et al., 2021) for the SEIAR modelexperiments, which measures the similarity of two distributions by evaluating the average score of a classifiertrained to distinguish between samples from the two distributions. We do this because the 5-dimensionalposteriors have complex geometries that make mean and variance comparisons insufficient in assessing thediscrepancies between the two posteriors. In the case of the predator-prey experiments, achieving a largeenough ESS from the PMMH posterior to train a classifier becomes far more expensive, which is why we omitthe C2ST score in this case. All posterior pairs plots can be found in Appendix D for a qualitative comparisonbetween the results. We assess the runtime performance of our method by calculating the effective samplesize (of the MCMC samples) per second (ESS/s) averaged over the parameters, where we take the ESS forSNL to be an average of the ESS values obtained from the MCMC chains over the final 5 rounds. For allexperiments, the chosen simulation budget ensured that we generated an ESS of approximately 1,000-3,000per SNL round. More detailed descriptions of the models and additional details for the experiments such ashyperparameters can be found in Appendix B.",
  "SIR model": "We first consider outbreak data from the SIR model (Allen, 2017). This model is a CTMC whose state attime t counts the number of susceptible and infected individuals, St and It respectively. Equivalently, thestate can be given in terms of two different counting processes: the total number of infections Z1,t = N Stand removals Z2,t = N (St +It), where N is the population size. Our observed data is a time series of totalcase numbers y1:n, where yi = Z1,i, equivalent to recording the number of new cases every day without error.Additionally, we assume that the outbreak has ended by day n so the final size of the outbreak is equal to yn.Since the outbreak never grows beyond yn cases, we can replace the full time series by y1:, where is random,and defined as the smallest i with yi = yn. The task is to infer the two parameters of the model, R0 > 0and 1/ > 0, using the data y1:. To model the randomness in , we augment the autoregressive model withthe binary sequence f1:, where fi = i,. The autoregressive model for this data is constructed by using",
  "SEIAR model": "This more sophisticated outbreak model includes a latent exposure period (E), symptomatic phases (I)and asymptomatic (A) phases (Black, 2019). The state of the model can be given in terms of 5 countingprocesses Z1,t, . . . , Z5,t, which count the total number of exposures, pre-symptomatic cases, symptomaticcases, removals and asymptomatic cases respectively. In addition to R0 and 1/, the model has three extraparameters: 1/ 0, (0, 1) and q (0, 1). The observations are of the same form as the SIR model,where the case numbers at day i are given by yi = Z3,i, corresponding to counting the number of new",
  "SNLPMMH": ": Comparison of ESS/s for SNL and PMMH for all three models. The lines plotted for SNL arethe averages over the runs, and the error bars correspond to the maximum and minimum. All plots areon a log-log scale. The horizontal axis of the left panel scales the observed dataset size, showing that ourmethod has improved scaling compared to the gold standard PMMH. The horizontal axis of the middle panelscales the system size and similarly shows improved scaling. The horizontal axis of the right panel scalesthe observation accuracy, as discussed in .3. We observe the well-known phenomenon that PMMHruntime degrades with more accurate data, but SNL improves. symptomatic cases each day.The likelihood has a complicated relationship between the parameters R0and q, which manifests as a banana shaped joint posterior. We assess our method on observations frompopulations of size N = 350, 500, 1000 and 2000. R0 0.70 0.75 0.80 0.85 0.90 0.95 1.00 q",
  ": (R0, q) joint posterior samples for SEIAR experiment with N = 500": "shows a comparison of the posterior statistics for the parameters R0 and . The metrics for theother parameters are deferred to Appendix C. The bias in the mean value of R0 is consistently small, butthe standard deviations are slightly inflated, more so with larger population sizes. The R0 posterior forthese experiments is quite right skewed, and SNL produces posteriors which are slightly too heavy in thetail, which explains the inflated variance estimates.For , the posterior means do exhibit a noticeablebias, but the posterior standard deviations do not on average. The bias in is lowest for the N = 500",
  "Predator-Prey model": "Here, we consider data generated from the predator-prey model described in McKane & Newman (2005),which exhibits resonant oscillations in the predator and prey populations. The model is a CTMC with state(Pt, Qt) where Pt and Qt are the numbers of predators and prey at time t respectively. The model has 5parameters, b, d1, d2, p1, p2 > 0, which are the prey birth rate, predator death rate, prey death rate, andtwo predation rates respectively. The task is to infer these 5 parameters from predator and prey countscollected at discrete time steps, when any individual predator/prey has a probability r (assumed to be fixedand known) of being counted.We use the values 0.5, 0.7 and 0.9 for r, and all noisy observations aregenerated from a single underlying realisation, where we simulate the predator and prey populations over200 time units, recording the population every 2 time units; see Appendix C for a plot of the data withr = 0.9. We order the predators before the prey in the autoregressive model conditionals in Equation 6. Forthese experiments, we only use a simulation budget of 25, 000, because the observations are longer and moreinformative than in previous sets of experiments. shows the comparison between the posterior statistics for the experiments with r = 0.9, which sug-gests negligible differences between the posterior mean and standard deviations. It is also worth noting thatSNL correctly reproduces a highly correlated posterior between d1 and p1. Furthermore, the d2 posterior",
  "Discussion": "In this paper we have constructed a surrogate likelihood model for integer valued time series data. Such dataoften arises from counting processes and models of small populations and presents particular challenges forperforming inference due to the low noise in the observation process. Low noise leads to highly informativeobservations of parts of the state, which means that a naive application of simulation based inference methodsusually fails; more sophisticated approaches, typically utilising bespoke importance sampling simulations, dowork, but are computationally expensive and difficult to implement. The main feature of our neural modelis that it can be trained using simple unconditional simulations of the dynamical model. Unconditionalsimulation is both much easier to implement and faster to run and our experiments demonstrated significantreduction in the overall computational expense of performing inference in scenarios where exact samplingmethods struggle. Our method produces good approximations of the parameter posterior, accurately reproducing many featuresof the posteriors obtained from exact sampling methods. There are negligible biases in the posterior meansacross experiments, with the exception of the parameter in the SEIAR model. This parameter has a veryweak relationship with the data except near = 0 or 1, so it is unsurprising that it is difficult to learn.Our method always yielded posterior standard deviations which were either very close to that of PMMH,or slightly too large, suggesting that we generally avoid overconfident estimates, which can be a problem insimulation based inference (Hermans et al., 2022). Furthermore, an approximate posterior that has slightlyinflated variance can still be used as an importance sampling distribution in a more computationally expensiveexact method, which can significantly improve efficiency (Papamakiros & Murray, 2015) Our method is alsorelatively robust to randomness within the algorithm, not showing any major differences over independentruns of SNL. In the right-skewed R0 posteriors in the SEIAR model experiments, we saw that the SNL posteriors wereslightly heavy in the tail. Improved training of the likelihood in these tail regions requires a scheme forgenerating SNL proposals that better cover these, e.g. by jittering the proposed samples. A particularlysimple alteration would be to use the geometric mean of the SNL posterior and the prior, which leads tomore sampling in the tails and better performance in sequential ABC methods (Alsing et al., 2018). Bayesianactive learning has also been employed in SNL before (Lueckmann et al., 2018), and this kind of approach hasthe advantage that it allows for assessing the uncertainty in the surrogate likelihood, so it might be possibleto derive an uncertainty score for the approximate posterior samples based on the surrogate uncertainty. Ingeneral, more robust SNL training routines are an important topic of current research (Kelly et al., 2024).",
  "Linda J.S. Allen. A primer on stochastic epidemic models: Formulation, numerical simulation, and analysis.Infectious Disease Modelling, 2:128142, 2017. doi: 10.1016/j.idm.2017.03.001": "Justin Alsing, Benjamin D. Wandelt, and Stephen M. Feeny. Alsing, j., wandelt, b.d. and feeney, s.m., 2018.optimal proposals for approximate bayesian computation. arXiv preprint arXiv:1808.06040, 2018. Christophe Andrieu, Arnaud Doucet, and Roman Holenstein. Particle Markov chain Monte Carlo methods.Journal of the Royal Statistical Society: Series B (Statistical Methodology), 72(3):269342, 2010.doi:10.1111/j.1467-9868.2009.00736.x.",
  "R. Bardenet, A. Doucet, and C. Holmes. On Markov chain Monte Carlo methods for tall data. Journal ofMachine Learning Research, 18:143, 2017": "Atlm Gnes Baydin, Barak A. Pearlmutter, Alexey Andreyevich Radul, and Jeffrey Mark Siskind. Au-tomatic Differentiation in Machine Learning: A Survey. Journal of Machine Learning Research, 18(1):55955637, January 2017. Peter Bickel, Bo Li, and Thomas Bengtsson. Sharp failure rates for the bootstrap particle filter in highdimensions. In Institute of Mathematical Statistics Collections, pp. 318329. Institute of MathematicalStatistics, Beachwood, Ohio, 2008. doi: 10.1214/074921708000000228.",
  "Mathieu Germain, Karol Gregor, Iain Murray, and Hugo Larochelle.MADE: Masked Autoencoder forDistribution Estimation.In International Conference on Machine Learning, volume 37, pp. 881889,2015": "Daniel T Gillespie. A general method for numerically simulating the stochastic time evolution of coupledchemical reactions. Journal of Computational Physics, 22(4):403434, 1976. doi: 10.1016/0021-9991(76)90041-3. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep Residual Learning for Image Recognition.In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 770778, 2016.doi:10.1109/CVPR.2016.90.",
  "S. A. Sisson, Y. Fan, and M. Beaumont. Handbook of Approximate Bayesian Computation. Chapman andHall/CRC, 2019": "Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, NalKalchbrenner, Andrew Senior, and Koray Kavukcuoglu. WaveNet: A Generative Model for Raw Audio.arXiv preprint arXiv:1609.03499, 2016. Aaron van den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel Recurrent Neural Networks. InMaria Florina Balcan and Kilian Q. Weinberger (eds.), 33rd International Conference on Machine Learn-ing, volume 48 of Proceedings of Machine Learning Research, pp. 17471756, June 2016.",
  "where (1) and (2) are of the form in Equation 8, both having the same number of input/output channelsfor simplicity": "From Equation 8, it can be seen that (z1:n, c())i is dependent on zi(k1):i. Defining f(y1:n, ) as thecomposition of h causal convolutions4, it follows that f(y1:n, c())i is dependent on yih(k1):i. Evaluationof the CNN N(y1:n, ) and calculation of the logistic parameters from the output is given in Algorithm1. Note that we assume that the initial observation y0 is known, so that the shifts of q(y1|, y0) can becalculated in the same form as the other shifts.",
  ",": "where 0mn/1mn are m n matrices of ones/zeros and d1 = 1, d2 = p. In other words, Mj is a block lowertriangular matrix where all entries on and below the block diagonal are 1. The masked causal convolutionappropriate for the hidden layers of the CNN is then defined by",
  ": Return 2:n, s2:n, w2:n": "second block is dependent ony1i , y2i, etc. In other words, if z1:n is the output of any hidden layer, then theelements z1+(j1)pi, . . . , zjpiare only dependent on yi through the elements y1i , . . . , yji . The CNN will output3cd output channels, where each block of d channels corresponds to 3c mixture of logistic parameters. Toremove dependence of the jth block of output channels on yji , we apply a d d block lower triangular maskmatrix similar to Mj to the final layer, but where the block diagonal set to zeros:",
  "B.1Hyperparameters and Implementation Details": "All experiments were performed using a single NVIDIA RTX 3060TI GPU, which provides a significantspeed up over a CPU due to the use of convolutions. All instances of PMMH were written in the Juliaprogramming language, and the particle filters were run on 16 threads to improve performance. We used a kernel length of 5 for all experiments, and since all observations have relatively low to no noise,this was sufficient to ensure that p(yi|yim:i1, ) p(yi|y1:i1, ) for all experiments. We used 5 mixturecomponents in all experiments. We found that more mixture components resulted in slower training withouta significant increase in performance, and less would slightly worsen accuracy without much increase inspeed. For the SIR/SEIAR model experiments, we used 64 hidden channels and 2/3 residual blocks. Forthe predator-prey model experiments, we used 100 hidden channels and 3 residual blocks.All of thesehyperparameters were chosen initially to be relatively small, and were increased when necessary based onan pilot run of training, adding depth to the network over increasing hidden channnels, since computationalcost grows quadratically with the number of hidden channels. We also used the same activation function forall experiments, namely the Gaussian linear error unit (GeLU) (Hendrycks & Gimpel, 2016).",
  "B.2.2Autoregressive Model Tweaks": "We assume that there is one initial infection, and that we subsequently observe the number of new casesevery day (without any noise), equivalent to observing Z1 at a frequency of once per day. We also assumethat the outbreak has reached completion, hence the final size of the outbreak NF can be measured. Usingthe data (y1:n, nF ), we can calculate the day at which the final size is reached:",
  "= p(fi|y1:i, fi1, )p(yi|y1:i1, fi1, ),": "since fi1 = 0 implies that fk = 0 for all k < i 1. Furthermore, since fi = 1 if and only if i = , then itfollows that fi1 = 0 for all i . It would therefore be redundant to make q(yi, fi|y1:i1, fi1, ) explicitlydependent on fi1, since fi1 = 0 always by construction. As a result, we only need to use y1: as an inputto the CNN. We do require a tweak to the output layer of the CNN, so that it outputs 16 parameters perconditional, 15 for the DMoL parameters, and one for q(fi|y1:i, fi1, ). Since fi should be conditionedon yi, we use a mask on the final causal convolution layer which has a similar structure to Equation 11 inAppendix A. Explicitly, the mask matrix has 15 rows of zeros, followed by a single row of ones, ensuringthat only the parameter corresponding to q(fi|y1:i, fi1, ) is dependent on yi, and the other 15 are not.",
  "We therefore train the autoregressive model on samples conditioned on NF > 1, and define the surrogatelikelihood to beq(y1:n, nF |) p(f0|)q(y1:, f1:|, f0 = 0)": "This prevents us from wasting simulations on time series with 0 length, which are likely when R0 is small.In principle, we could also calculate p(NF > t|) for some threshold size t, and only train on simulationswhich cross this threshold, which would further reduce the number of short time series in the training data.This is possible largely due to the simplicity of the SIR model, and such a procedure generally becomes moredifficult for CTMCs with a higher dimensional state space. The final augmentation that we make to the autoregressive model concerns masking out certain terms ofthe form log q(fi|y1:i, fi1, ), which forces q(fi|y1:i, fi1, ) = p(fi|y1:i, fi1, ) = 1. There are threescenarios where this is appropriate:",
  ". If yi = N, since this corresponds to the whole population becoming infected, therefore we haveNF = N and fi = 1 with probability 1": "We calculate the context vector c((, )) using a shallow neural network with 64 hidden units and 12 outputunits, using a GeLU non-linearity. For the household experiments, we additionally concatenate a one-hotencoding of the household size to (, ) before calculating c, so that the autoregressive model can distinguishbetween household sizes. To obtain the logistic parameters from the output of the neural network o1:, wecalculate5",
  "B.2.3Training and Inference Details": "For inference, we used a Uniform(0.1, 10) prior on R0 and a Gamma(10, 2) prior on 1/, where we truncatethe prior of 1/ so that it is supported on [1, ). For the single observation experiment, we simulate thedata using the Gillespie (1976) algorithm, running for a maximum length of 100 days, which ensures thatapproximately 99.9% of samples (estimated based on simulation) from the prior predictive reach their finalsize. We construct a binary mask for the simulations where < 100 so that values following do notcontribute to the loss during training. For the household experiments, we did not need to generate training data from households of size 2, as thelikelihood of such observations can be calculated analytically. The case of NF = 1 is handled by Equation 16,and if NF = 2 it can be shown that",
  "p(y1:, nF = 2|, NF > 1) = e(+) 1 e(+)": "The training data was generated from five realisations for a single value of , using a unique householdsize (between 3 and 7) for each of the realisations. We found that it was useful to increase the varianceof the proposals generated from MCMC, as the proposals had small variance and sometimes this seemedto reinforce biases over the SNL rounds. We inflated the variance of the proposals by fitting a diagonalcovariance Gaussian to the proposed samples (centred to have mean 0), and added noise from the Gaussianto the proposals, effectively doubling the variance of the proposals over the individual parameters. For all experiments on the SIR model we, we used the particle filter given in Black (2019) within PMMH.For the single observation experiment, we used 20 particles, and the MCMC chain was run for 30,000 steps,which was sufficient for this experiment since the particle filter works particularly well on the SIR model. Forthe household experiments, we used 20, 40 and 60 particles for 100, 200 and 500 households respectively. Weran the MCMC chain for long enough as necessary to obtain an ESS of approximately 1000 or above for bothparameters. Mixing of the MCMC chain slows down with increasing household numbers due to increasingvariance in the log-likelihood estimate that must be offset by also increasing the number of particles. Thecovariance matrix for the Metropolis-Hastings proposal was tuned based on a pilot run for all experiments.",
  "(Z1, Z2, Z3, Z4, Z5) (Z1, Z2, Z3, Z4, Z5 + 1)at rate (1 q)(Z1 Z2 Z5).(21)": "The event Equation 17 represents a new exposure, Equation 18 represents an exposure becoming infectious(but pre-symptomatic), Equation 19 represents a pre-symptomtic individual becoming symptomatic, Equa-tion 20 represents a recovery, and Equation 21 represents an exposure who becomes asymptomatic (andnot infectious). The parameters of the model are the transmissibilities attributed to the pre-symptomaticand symptomatic population p, s > 0, the rate of leaving the exposed class > 0, the rate of leavingthe (pre)symptomatic infection class > 0, and the probability of becoming infectious q (0, 1). We fol-low Black (2019) and reparameterise the model in terms of the parameters R0 = (p + s)/q, 1/, 1/, = p/(p + s) and q. We use the same parameters to generate the observations and priors that were usedin Black (2019): R0 = 2.2, 1/ = 1, 1/ = 1, = 0.7 and q = 0.9, and priors",
  "truncating the priors for 1/ and 1/ to have support [0.1, ) and [0.5, ) respectively": "The observations are given by the values of Z3 at discrete time steps, corresponding to the number of newsymptomatic individuals per day, as well as the final size NF . We handle the final size data in the same way asdiscussed for the SIR model, but we include f0 for this model, as p(NF = 1|) is more difficult to calculatedue to the asymptomatic individuals. The initial state is taken to be (Z1, Z2, Z3, Z4, Z5) = (1, 1, 0, 0, 0),which we simulate forward using the Gillespie algorithm until Z3 = 1, after which we begin recording thevalues of yi at integer time steps. For population sizes of 350, 500, 1000 and 2000, we generate time series ofmaximum length 70, 80, 100 and 110 respectively. This ensured that at least 99% of samples from the priorpredictive reached their final size (estimated by simulation). The autoregressive model follows the exactsame construction as given for the SIR model experiments, except that we rescale the time series by thepopulation size before inputting to the CNN. We also divide y1: by N before inputting to the CNN, whichhelps to stabilise training. For PMMH, we used the particle filter given in Black (2019), using 70, 80, 100 and 120 particles for populationsizes of 350, 500, 1000 and 2000 respectively. The MCMC chain was run until all parameters had an ESS ofapproximately 1000 or above, and the proposal covariance matrix was estimated based on a pilot run.",
  "B.4Predator-Prey Model": "The predator-prey model that we use is described in McKane & Newman (2005). For large populationsin predator-prey models, it is often sufficient to consider a deterministic approximation via an ODE. Forthis model, the corresponding ODE has a stable fixed point, which makes it incapable of exhibiting thecyclic behaviour seen in e.g. the Lotka-Volterra model (Wilkinson, 2018). In the stochastic model however,there is a resonance effect which causes oscillatory behaviour. Unlike predator-prey models with limit cyclebehaviour, in this model the dominant frequencies in the oscillations have a random contribution to thetrajectory, which causes greater variability among the realisations for a fixed set of parameters.",
  "(P, Q) (P + 1, Q 1)at rate 2p1PQK ,(25)": "where b, d1, d2, p1, p2 > 0 are the parameters of the model. To simulate the data, we use parameters b = 0.26,d1 = 0.1, d2 = 0.01, p1 = 0.13, p2 = 0.05. We use informative priors on b, d1 and d2, assuming that reasonableestimates can be obtained based on observing the two individual species. In particular we use",
  "p2 Exponential(0.05),": "where we truncate the prior of p1 so that it is supported on [0.01, ). This truncation, which removes only5.4% of the mass from p1, rules out a population explosion in the prey due to unphysically low predation.Furthermore, when p1 is close to 0, (25) becomes negligible and the resulting model does not behave like atrue predator-prey model, so truncating the prior rules out this possibility. To simulate the realisation forthe observations, we use a carrying capacity of K = 800 and initial condition (250, 250). We then run theGillespie algorithm over 200 time steps, taking yi = (P2i, Q2i), and apply observation error to generate theobservations, namely",
  "for r = 0.5, 0.7, 0.9": "To generate the training data, we firstly infer the initial condition from the noisy initial observation y0. Inparticular, we assume p(P0) 1 and p(Q0) 1, from which it is easy to derive the posterior p((P0, Q0)|y0):P0 y10|y0 NegativeBinomial(y0,1 + 1, r),Q0 y20|y0 NegativeBinomial(y0,2 + 1, r). We then generate the training data in the same way we produced the observations. No tweaks are madeto the autoregressive model, though we do divide y1:100 by 60 before inputting to the CNN, and multiplythe shifts (before adding yi1) and scales by 60. We do this because the standard deviation across the timeaxis for samples from the prior predictive was estimated to be approximately 60. We also tried rescalingthe input/output by larger values, but this generally resulted in less stable training and slower MCMCiterations.We do not centre the inputs, because if yji = 0 for multiple time steps in a row, then thisimplies that an extinction has likely occurred, hence zeroing the weights associated with yji in the first layer"
}