{
  "Abstract": "We present a bag of tricks framework for few-shot class-incremental learning (FSCIL), which isa challenging form of continual learning that involves continuous adaptation to new tasks withlimited samples. FSCIL requires both stability and adaptability, i.e., preserving proficiencyin previously learned tasks while learning new ones. Our proposed bag of tricks bringstogether six key and highly influential techniques that improve stability, adaptability, andoverall performance under a unified framework for FSCIL. We organize these tricks into threecategories: stability tricks, adaptability tricks, and training tricks. Stability tricks aim tomitigate the forgetting of previously learned classes by enhancing the separation between theembeddings of learned classes and minimizing interference when learning new ones. On theother hand, adaptability tricks focus on the effective learning of new classes. Finally, trainingtricks improve the overall performance without compromising stability or adaptability. Weperform extensive experiments on three benchmark datasets, CIFAR-100, CUB-200, andminiIMageNet, to evaluate the impact of our proposed framework. Our detailed analysisshows that our approach substantially improves both stability and adaptability, establishinga new state-of-the-art by outperforming prior works in the area. We believe our methodprovides a go-to solution and establishes a robust baseline for future research in this area.",
  "Introduction": "Continual learning is a machine learning paradigm that focuses on the ability of a model to learn newknowledge, without forgetting what it previously learned. In real-world applications, machine learning modelsoften encounter scenarios where they must adapt to novel classes with only a limited number of samplesavailable for learning. This scenario has inspired the introduction of an exciting paradigm called Few-ShotClass Incremental Learning (FSCIL) (Tao et al., 2020). Existing literature (Tao et al., 2020; Zhou et al.,2022a) has demonstrated that traditional continual learning approaches are ineffective in FSCIL, primarilydue to the scarcity of labelled data during incremental learning sessions. This data limitation often leads tooverfitting on the novel classes, resulting in a well-known catastrophic forgetting issue. Some prior works have linked the problem of catastrophic forgetting with high adaptability (or plasticity)during incremental training (Chi et al., 2022; Zhao et al., 2021). Consequently, these approaches reduce theadaptability by utilizing an incremental-frozen framework, where the encoder is trained only in the basesession and remains frozen during the incremental sessions (Zhao et al., 2021; Tao et al., 2020; Chi et al.,2022; Cheraghian et al., 2021a). The resulting methods provide very high stability, but very little adaptability.This phenomenon is often referred to as the stability-adaptability dilemma (Peng et al., 2022b; Zhao et al.,2023) in the FSCIL, where high stability causes reduced adaptability and vice-versa. In this work, we combine a collection of techniques under a bag of tricks framework with the goal ofconcurrently enhancing both the stability and adaptability of FSCIL. These tricks have never been explored",
  "Published in Transactions on Machine Learning Research (09/2024)": "are Lion-Tigar, Bus-Streetcar, and Shark-Dolphin, respectively. To understand the effectiveness ofthe stability on closely related classes, we report the average accuracy on the above-mentioned base andincremental classes with and without our tricks below in . As we find from this experiment, even forsemantically similar incremental classes, our proposed framework is capable of improving the performance ofboth the base classes (improved stability) and the incremental ones (improved adaptability).",
  "Class Incremental Learning": "Class-incremental learning (CIL) is a continual learning paradigm that involves continuously learning novelclasses while preserving the knowledge of previously learned ones (Masana et al., 2022). Existing literatureon CIL can be broadly categorized into three main groups. The first group, commonly referred to as thereplay-based method, stores past samples in a memory bank for rehearsal during incremental sessions to ensureretention of old knowledge (Rebuffi et al., 2017; Rolnick et al., 2019). The second group is regularization-based,focusing on preventing significant changes in the parameters to prevent forgetting (Li & Hoiem, 2017; Liuet al., 2018). In contrast, the third group dynamically expands the network architecture to accommodate the",
  "Few-Shot Class-Incremental Learning": "In real-world applications, assuming that an incremental session contains a large number of samples foreach novel class is impractical (Tao et al., 2020). FSCIL (Tao et al., 2020; Hersche et al., 2022; Cheraghianet al., 2021b; Liu et al., 2022) tackles this challenging scenario where the model needs to incrementally learnnovel classes with only a few samples per class. It also assumes that no samples from previous sessionsare available, which causes privacy concerns in many domains. Consequently, none of the traditional CILmethods, mentioned in the previous section, perform well in the FSCIL setting (Zhou et al., 2022a; Tao et al.,2020). However, there have been developments in FSCIL literature, which can be discussed in two broadcategories. The first category trains the model only on the base session, keeping the model frozen in theincremental session (Zhu et al., 2021b; Shi et al., 2021; Zhang et al., 2021); while the second group tunes themodel in the incremental sessions (Tao et al., 2020; Cheraghian et al., 2021a; Dong et al., 2021; Zhao et al.,2021). The main idea of the first category (frozen encoder-based methods) is to ensure greater separability of baseclasses in the embedding space so that novel classes in the incremental step can easily fit in this space withminimal interference (Zhu et al., 2021b; Shi et al., 2021; Zhang et al., 2021). This is often referred to asforward compatibility (Song et al., 2023). Prior works in the literature proposed different techniques aimingto enable forward compatibility. For instance, FACT (Zhou et al., 2022a) proposed to use virtual prototypesto force the embedding of different classes to be maximally separated while respecting the relative positioningof classes in the embedding space. SAVC (Song et al., 2023) created virtual classes during the base session,and (Yang et al., 2022) assigned random prototypes that are maximally separated from one another. Thesemethods offer relatively high stability, but little adaptability for incremental training. The second group of methods tunes the encoder in the incremental session to provide better flexibility forlearning new tasks, thus providing better adaptability. For example, MgSvF (Zhao et al., 2021) strategicallyupdated different components at different rates, effectively balancing the adaptation to new knowledge andthe preservation of old ones. The exemplar relation distillation framework (Dong et al., 2021) constructedand updated an exemplar relation graph to facilitate the learning of novel classes. SoftNet (Kang et al., 2023)proposed to utilize the concept of the lottery-ticket hypothesis to find a sub-network of important parametersfrom the previous session, which is left frozen during incremental tuning with the rest of the parameters. Acommon problem of this second group of methods is that adaptability comes at the cost of stability. That is,the models performance in the old classes deteriorates as it learns novel classes. Overall, there is a lack ofbalance between the stability and adaptability in existing FSCIL methods, which we aim to improve with ourbag of tricks.",
  "Overview": "In FSCIL, a model is trained across T consecutive sessions, with each session introducing novel classes forthe model to learn. Training data for each session t T is labelled, Dttrain = {(xi, yi)}Nti=0, where xi andyi are the i-th sample and the corresponding label. In FSCIL, only the base session (first session) containsa sufficient amount of samples for effective training. Subsequent incremental sessions contain only a fewsamples per class, typically organized in an N-way K-shot format, containing K training samples for each ofthe N classes. By definition, each session exclusively contains samples from novel classes, meaning that thelabel space for each session (Ct) is mutually exclusive with others. The performance of a method is evaluatedafter each session on the test set Dttest, which contains samples from all classes encountered so far. In our",
  "Baseline": "We consider the incremental frozen framework as our baseline due to its proven effectiveness (Zhang et al.,2021; Shi et al., 2021; Song et al., 2023) in addressing the data-scarce incremental learning scenario of FSCIL.In this framework, the model (x) is trained only on the base session (D0train) using a standard cross-entropyloss, Lce = ((x), y). The model (x) consists of an encoder, f(x) Rd1 and a classifier head W Rd|C0|.The prediction can therefore be expressed as (x) = W T f(x). After training on the base session, the encoderf(x) remains frozen during the incremental sessions. To classify novel classes, the classifier W is expandedwith the classifier weight for novel classes parameterized by the prototype of each class. A prototype is theaverage of the embeddings of all samples belonging to that class, wtc =1ntcntci=1 f(xc,i).",
  ": The intuition behind stability tricks. Better separation of baseclasses ensures stability in incremental learning": "Stability tricks in our frame-work revolve around the ideathat better separation of baseclasses ensures improved stabil-ity in learning new classes inthe incremental sessions (Songet al., 2023). As illustrated in, better separation ofbase classes in the embeddingspace allows the novel classesto be placed in the embeddingspace without interfering with existing classes. This involves increasing the distance between classes (inter-class distance) while reducing the distance between samples within the same class (intra-class distance).This approach is also known as forward compatibility in FSCIL literature. Accordingly, we incorporatethree techniques that can effectively promote better stability: training with a supervised contrastive loss,pre-assigning prototypes, and including pseudo-classes. Supervised Contrastive Loss. While most of the existing literature on FSCIL use the standard cross-entropy loss for learning during the base session, some prior works (Song et al., 2023) have demonstrated thatcross-entropy does not effectively separate classes in the embedding space. Some studies have indicated that thesupervised contrastive loss (SupCon) (Khosla et al., 2020) exhibits better separability in the embedding space.SupCon is a variation of the popular contrastive loss (Chen et al., 2020), which additionally includes classlabels to guide representation learning in a supervised manner. Specifically, SupCon learns representations bypulling the samples (and their augmentations) of the same class closer in the embedding space while pushingthe samples of different classes apart, resulting in a more separable embedding space than cross-entropy. Inother words, SupCon forces the representation of each sample to be close to its corresponding class prototype(center of embedding), while pushing the prototypes away from one another. For a batch of labelled samples{(xi, yi)}Ni=0, the SupCon loss can be represented as:",
  ",(1)": "where z = f(x), N is the batch-size, and Nyi is the number of positive samples from class yi. The indicatorfunction denoted by 1[yi=yj] yields a value of 1 when indices i and j correspond to instances of the sameclass, is a temperature parameter. Pre-assigning Prototypes. In SupCon, prototypes are learned along with the optimization of the model.However, it was shown in Yang et al. (2022) that pre-assigning prototypes that are maximally separated",
  "where, Pc and wc are the assigned and learned prototypes of class c, and C0 is the total number of classes inthe base session": "Including Pseudo-classes. Previous studies such as FACT (Zhou et al., 2022a) and SAVC (Song et al.,2023) introduced the concept of integrating pseudo-classes during the base session to serve as placeholdersin the embedding space for novel classes. In SAVC, the pseudo-classes were generated by a pre-definedtransformation, which was considered a more fine-grained variant of the original classes. In our work, we takea similar approach of including pseudo-classes from (Lee et al., 2020), which employs hard augmentationsto transform the semantics of the sample and consider it as a pseudo-class. Let, F be a set of pre-defined(hard) augmentations for pseudo-class formation, and xc,i be a sample of class c. With the pseudo-classtrick, we consider the transformation of the image F(xc,i) as an instance of a pseudo-class (C0 M + c),where C0 is the total number of classes in base session, and M is the pseudo-class multiplication factor. Suchpseudo-classes can be seen as a fine-grained class derived from the original class. In our work, we use M = 2,which doubles the total number of classes (including pseudo-classes) during base training. This trick does notinclude any new loss function.",
  "Adaptability Tricks": "While stability tricks help the model retain the knowledge of base classes, they offer limited adaptability forthe model to learn novel classes effectively. Independent performance evaluation of base and novel classes atthe end of training demonstrates that the performance of novel classes is substantially inferior to that of baseclasses (Song et al., 2023). Consequently, the overall performance at the end of training is predominantlyinfluenced by the performance of the base classes. This underscores the need for FSCIL methods to enhancethe models adaptability on incremental sessions to improve the models performance on novel classes. Inthis section, we discuss two tricks that provide more adaptability for the model: incremental fine-tuning andSubNet tuning, which we combinedly denote as Incremental SubNet Tuning. Incremental SubNet Tuning. Fine-tuning is a common practice in machine learning literature to tune apre-trained model for a new task or setting. It is also a widely used technique across conventional continuallearning literature, predominantly with rehearsal-based techniques. However, fine-tuning in the context ofFSCIL requires careful consideration of the training setup, since no data from the previous session is availablefor rehearsal during the current session. Consequently, tuning can cause catastrophic forgetting of alreadylearned knowledge. In our framework, we adopt the fine-tuning concept of Song et al. (2023), which utilizes asmall learning rate to tune certain portions of the pre-trained encoder, while keeping the rest of the encoderfrozen. Specifically, we freeze the shallow layers of the network since shallower layers are known to capturedomain-invariant features, whereas the deeper layers learn more fine-grained features. While such an incremental fine-tuning approach provides more adaptability for learning novel classes, it mayresult in decreased stability. To deal with this issue, we combine this adaptability trick with the concept of",
  "i=1Lf(m)(xi), yi Lf(xi), yi,(3)": "where is an element-size dot-product operation. Here, L is the loss function for training the model on thebase session, and m is the optimal binary mask with the same size as the network, for which the performanceof the masked SubNet is comparable to that of the original network. During the incremental session, wefreeze the sub-network to ensure the performance of the base session and fine-tune the remaining parametersfor learning the novel classes.",
  "Training Tricks": "Building upon the principles of stability and adaptability tricks discussed earlier, we introduce a set oftraining techniques that can further enhance overall performance without compromising either stability oradaptability. These tricks include adding a pre-training step and including an additional learning signal. Additional Pre-training Step. Existing research in this field indicates that self-supervised pre-training,followed by supervised fine-tuning, consistently outperforms fully supervised training, particularly in scenarioswith limited training data. In the context of FSCIL where data scarcity is a significant challenge, leveraginga self-supervised pre-training step has the potential to provide substantial benefits. As a result, we introducea contrastive self-supervised pre-training step before the training of the base session.The contrastiveself-supervised loss is similar to the SupCon loss introduced earlier, except no label information is utilized.Accordingly, the contrastive pre-training loss can be represented as:",
  "2bk=1 1[k=i]exp(zi, zk/),(4)": "where, (i) is the index of the second augmented sample, and 1[k=i] is an indicator function that returns 1when k is not equal to i, and 0 otherwise. is a temperature parameter, and b is the batch size. Including Additional Learning Signal. Following the intuition from the previous trick, we includeanother self-supervised learning signal, but this time while training on the base session rather than as aseparate step. Existing literature on other data-scarce scenarios, such as semi-supervised learning, has shownthat adding a pre-text task along with the supervised learning helps the model learn better representationwithout overfitting to the small labelled set (Berthelot et al., 2019; Roy & Etemad, 2023). To this end, weinclude a rotation prediction task (Gidaris et al., 2018) that is shown to perform well with supervised learning(Berthelot et al., 2019). Here, the basic idea is to apply a rotation operation on the input image, and thetask is to predict the amount of rotation applied to the image. In practice, a rotation module randomlysamples one of the following rotations and applies it to the image: 0, 90, 180, 270. As a result, the rotationprediction task can be viewed as a four-way classification task, represented as:",
  "Datasets and Implementation Details": "Following the established protocol in the FSCIL literature, we conduct our experiments on three populardatasets: CIFAR100 (Krizhevsky et al., 2009), miniImageNet (Russakovsky et al., 2015) and CUB200 (Wahet al., 2011). To ensure a fair comparison with prior works (Tao et al., 2020; Chi et al., 2022) on FSCIL, weuse the same encoder (ResNet-18), and data split across the training sessions. Specifically, for CIFAR-100and miniImageNet, we use 60 classes for the base session and 40 classes for the incremental sessions. Theincremental learning experiments are conducted on a 5-way, 5-shot setting. In the case of CUB-200, weallocate 100 classes for the base session and another 100 classes for the incremental sessions, each containingten classes (10-way, 5-shot). Further details on implementation and hyper-parameters are presented inAppendix S1.",
  "Evaluation Protocols": "Following the standard evaluation protocol in FSCIL literature (Tao et al., 2020; Song et al., 2023; Kanget al., 2023), we report the models accuracy after each incremental session. To further understand theproperties of the learned representations, we investigate inter-class distance, intra-class distance, and classseparation. Following, we define these properties. Inter-class distance is the distance between the prototypes of any two classes in the embedding space.Given, two class prototypes wi and wj for classes i and j, we compute their inter-class distance as:",
  "where nk is the number of samples belonging to class k, and zi is the embedding of the ith sample": "Class-separation determines how well the samples from one class are separated from other classes in theembedding space. For a dataset with C classes, class separation can be represented as: 1 dwithin/dtotal.Here, dwithin is the average distance between samples of the same classes, while dtotal is the average distancebetween samples in the embedding space. Accordingly, they are formulated as:",
  "Main Results": "In this section, we present the main results of our tricks by cumulatively adding them to the baseline. Theresults are presented in , which include the category of the tricks, along with the stages at which thetricks are applied and the performance on CIFAR-100, CUB-200 and miniIN. As outlined in .2, weadopt the incremental-frozen framework as the baseline for our study. As shown in , the accuracyof this baseline for CIFAR-100, CUB-200, and miniImageNet datasets are 43.77%, 59.88%, and 45.08%,respectively. To ensure the best performance for the baseline, we perform an extensive hyper-parameter study",
  "(d)": ": Properties of adaptability tricks on CIFAR-100. (a) Presents accuracy of Base, Novel, and Totalclasses; (b) presents the total accuracies after eachsession, which we aim to maximize; (c) and (d) depict t-SNE visualizations for stability and adaptability tricks,where incorporating adaptability tricks shows moreseparation. Here, 0-4 are base classes, and 5-6 arenovel classes. Next, we include our second stability trick of pre-assigning ETF vectors as prototypes. As we see ina, this trick further increases inter-class dis-tance (a) over the previous trick. Althoughthis trick does not further reduce the intra-class dis-tance (b), it increases the class separation ofnovel classes (c), resulting in a 2.1% improve-ment in novel classes. Overall, including this trickincreases the final performance to 51.10%, 60.74%,and 49.73%, respectively. Finally, adding pseudo-classes causes a further reduc-tion in the intra-class distance since it requires fittingtwice the number of classes in the same amount ofspace. Although this trick reduces the inter-classdistance, it provides an increase in the novel classseparation. This results in a 1.20% improvementin novel classes and 0.6% in the final performance.Overall, we find an improved accuracy of 51.21%,62.27%, and 55.82% for CIFAR-100, CUB-200, andminiImageNet, respectively. Here, we find the largestimprovement of 5.06% on the miniImageNet dataset. Adaptability Tricks As discussed earlier, stabilitytricks do not provide sufficient adaptability for themodel to perform well on novel classes.This isevident from the accuracy on base and novel classesin d, where we see a 49.6% difference in theperformance on base and novel classes. Specifically,the performance on base and novel classes are 71.5%and 21.9%, respectively.This indicates that themodel struggles to effectively learn the novel classes, demonstrating the need for adaptability tricks. Predicted label True label",
  "presented in Appendix S3.1. As we later compare our results with prior works (.5), we observe thatthis baseline outperforms many of the prior studies, showing the robustness of our baseline": "Stability Tricks. Next, we discuss the results of including stability tricks into the baseline, beginning withthe addition of SupCon loss. The results from this study show substantial improvement for all datasets(), obtaining accuracies of 50.16%, 60.38%, and 48.90%, for CIFAR-100, CUB-200, and miniImageNetrespectively. As discussed earlier, a key element for ensuring high stability in FSCIL is to ensure increasedseparability in the embedding space (increased inter-class distance and reduced intra-class dispersion). InFigures 2a and 2b, we plot the cumulative probability of inter-class and intra-class distances (defined in.2) for different tricks. We plot the cumulative probability of inter-class and intra-class distance (Songet al., 2023) instead of the average since the average distance can be misleading due to the presence of outliers,whereas the cumulative probability provides a more robust and nuanced representation of the distribution ofdistances. As we observe from a, adding SupCon provides a large increase in the inter-class distancecompared to training with cross-entropy loss in the baseline. At the same time, SupCon greatly reducesthe intra-class distance compared to the cross-entropy-based baseline (illustrated in b). However,our findings diverge from those of Song et al. (2023), which indicated that while supervised contrastiveloss effectively reduces intra-class distances, it unexpectedly leads to a reduction in inter-class distances aswell. The divergence in our findings may be attributed to implementation specifics. For example, SAVCutilized multi-crop augmentation, which we do not use. Also, we adopted a different set of hyper-parametersdetermined through our study (Appendix S3.2). We also show the overall class separation degree for different tricks in c. Class separation degreerefers to the degree of distinctiveness or separability between different categories within the embedding space,which can be measured as defined in .2. It typically ranges between 0 and 1, with higher valuesindicating clearer boundaries and better separability between classes. As we observe, adding SupCon providesbetter class separation for both base and novel classes. Overall, the higher separation in the embedding space",
  "(b) bag of tricks": ": Confusion matrices for the baseline and our bag oftricks. The baseline performs well on the base session, butperformance drops for novel classes. Our framework showsimproved performance for both base and novel classes. Our adaptability trick with Incremental Sub-Net Tuning provides adaptability by means offine-tuning a sub-network on the encoder inincremental sessions. As shown in a,this trick improves accuracy on novel classesby 11.1%. We also show the accuracy aftereach session in b. From this figure,we observe comparable accuracies to that ofstability tricks in the earlier sessions. However,Incremental SubNet Tuning shows better per-formance in later sessions when the numberof novel classes increases. This again showsthe importance of the adaptability trick forlearning novel classes. Overall, this trick pro-vides an improved accuracy of 58.12%, 63.1%,and 57.85% on the CIFAR-100, CUB-200, andminiImageNet.",
  "ViT-B/16 (pre-trained)50.1469.72ViT-B/32 (pre-trained)50.0169.58ViT-L/14 (pre-trained)51.5571.09": "Performance with Larger Encoders. For all the experimentsin this work, we followed the same problem setup and encodersused by all the prior works on FSCIL (Song et al., 2023; Kanget al., 2023). Our focus in this paper is to improve the stability,adaptability, and overall performance of FSCIL while followingthe same benchmarks and problem setups, as well as encoderbackbones, to ensure a fair comparison. However, the tricks inour framework are independent of the encoders choice and canbe easily adapted to any encoder, including ViT. To investigatethe performance of our framework on larger encoders, in , we show the results on ResNet-50, ViT-B/16, and pre-trained(on ImageNet) ViT-B/16, ViT-B/32, and ViT-L/14. The resultsfrom this study show that the proposed tricks not only transferto larger encoders and ViTs, but the performance gains areeven larger than those of smaller networks like ResNet-20. Forinstance, ResNet-50 shows a 20.74% improvement compared to 14.78% in ResNet-20. Similarly, ViT-B/16with pre-trained and randomly initialized encoders shows 19.58% and 15.75% improvements over the baseline.Additionally, pre-trained ViT-B/32 and ViT-L/14 show 19.57% and 19.54% improvements over the baseline.This is due to the fact that without the stability and adaptability tricks from our framework, a larger encoderwith more parameters is more prone to overfitting, making it difficult to learn incremental classes in a few-shotlearning setting. With our proposed framework, the larger encoders show a large improvement in performance.",
  "MethodAcc. in each session (%)": "Baseline69.0864.460.2257.0853.850.8848.4246.5445.08iCaRL (Rebuffi et al., 2017)71.7761.8558.1254.6051.4948.4745.9044.1942.71Rebalancing (Hou et al., 2019)72.3066.3761.0056.9353.3149.9346.4744.1342.19TOPIC (Tao et al., 2020)61.3150.0945.1741.1637.4835.5232.1929.4624.42EEIL (Castro et al., 2018)61.3146.5844.0037.2933.1427.1224.1021.5719.58FSLL (Mazumder et al., 2021)66.4861.7558.1654.1651.1048.5346.5444.2042.28FSLL+SS (Mazumder et al., 2021)68.8563.1459.2455.2352.2449.6547.7445.2343.92F2M (Shi et al., 2021)72.0567.4763.1659.7056.7153.7751.1149.2147.84CEC (Zhang et al., 2021)72.0066.8362.9759.4356.7053.7351.1949.2447.63MetaFSCIL (Chi et al., 2022)72.0467.9463.7760.2957.5855.1652.9050.7949.19C-FSCIL (Hersche et al., 2022)76.4071.1466.4663.2960.4257.4654.7853.1151.41FACT (Zhou et al., 2022a)72.5669.6366.3862.7760.6057.3354.3452.1650.49CLOM (Zou et al., 2022)73.0868.0964.1660.4157.4154.2951.5449.3748.00LIMIT (Zhou et al., 2022b)72.3268.4764.3060.7857.9555.0752.7050.7249.19SoftNet (Kang et al., 2023)79.7775.0870.5966.9364.0061.0057.8155.8154.68ALICE (Peng et al., 2022a)80.6070.6067.4064.5062.5060.0057.8056.8055.70SAVC (Song et al., 2023)81.1276.1472.4368.9266.4862.9559.9258.3957.11Ours84.379.5975.4971.468.4565.12962.260.5259.11",
  ": Comparison to prior works across CIFAR-100, CUB-200, and miniImageNet datasets, demonstratingthat our solution outperforms prior works": "results for the base session and the overall performance across incremental sessions. The first row in thistable presents the results for our baseline. Among prior works, SoftNet held the previous state-of-the-art forCIFAR-100 with a final accuracy of 55.33%. Our framework achieves a performance of 58.55%, marking a3.22% improvement over the current state-of-the-art. In , we plot the accuracy over the incrementalsessions for CIFAR-100, CUB-200, and miniImageNet. For CUB-200, the previous state-of-the-art was heldby SAVC with an accuracy of 62.50%, which our framework outperforms with an accuracy of 63.60%. Finally,the state-of-the-art on miniImageNet was also held by SAVC, with an accuracy of 57.11%, which our methodoutperforms by 2.0%, achieving an accuracy of 59.11%. Performance Across Different Shots. Following prior works in FSCIL (Zhou et al., 2022a; Tao et al.,2020), we report the main results in a 5-shot setting. Nonetheless, We also investigate the performanceon 1-shot and 2-shot settings to evaluate how the model performs in data-scarce settings and compare theperformance with two of the previous state-of-the-art methods, SAVC and SoftNet. We also investigate the10-shot setting to determine how an increase in data impacts performance. As seen from , in 1-shotlearning, our proposed framework outperforms prior works by a larger margin than the 5-shot setting. This",
  ": Performance for differentshots.Solid and dotted lines repre-sent 5- and 1-shot performances": "Performance on ImageNet-1K. So far we have report the mainresults of our approach on, CIFAR-100, CUB-200, and miniImageNet,which are commonly used by prior works in this area. However, thesedatasets are relatively small in terms of the number of classes. Tobetter understand the scalability of FSCIL in terms of the number ofclasses, we conduct an experiment on the ImageNet-1K (Russakovskyet al., 2009) dataset, which contains 1000 classes. For this study, weconsider 500 randomly selected classes as the base classes and reportthe results for a 50-way, 10-shot incremental learning setting. Theresults of this study with our method and its comparison to priormethods are presented in . As seen from the results, ourproposed framework outperforms prior works on the ImageNet-1Kdataset by a considerable margin. More specifically, our frameworkachieves 7.04% and 2.13% improvement over SoftNet and SAVC,respectively.",
  "SAVC53.1281095021506.1hOurs (ResNet-18)58.5563093521506.8hOurs (ResNet-50)58.7545071018008.1h": "In , we discuss the time complexity of ourframework using a single Nvidia RTX 2080 GPUin comparison to SAVC (Song et al., 2023). Tothis end, we report the throughput in frames persecond (FPS) and the training time. During thetraining phase on the base session, the through-puts for SAVC and our framework are 630 FPSand 450 FPS, respectively. Nonetheless, duringincremental training and inference, our frame-work with ResNet-18 is as fast as SAVC. Con-sequently, once trained, our framework can be reliably deployed in real-world applications with the sameinference time as previous state-of-the-art while achieving enhanced performance.",
  "Baseline65.3%19.7%Ours74.5%30.4%": "According to the setup adopted in the literature for FSCIL Songet al. (2023); Kang et al. (2023), each of the new classes in the in-cremental session is different from the classes seen so far. Therefore,incremental classes are generally not exactly identical to the baseclasses. However, some of the incremental classes can be similarto the base classes. For instance, in CIFAR-100, Orchid is a classof flower that is part of the base class, while Sun-flower is anotherflower that is part of the incremental class. A few similar pairsof examples of base classes with similarities to incremental classes",
  "Conclusion and Future Work": "We present a bag of tricks framework that combines six effective tricks in three distinct categories to improvethe stability, adaptability, and overall performance of FSCIL. Stability tricks improve the separation amonglearned classes to facilitate the learning of new ones, resulting in large improvements on both the baseand total classes. Adaptability tricks improve the performance on novel classes by providing more learningcapability during incremental sessions. Finally, training tricks provide an additional boost to the finalperformance. While we did not introduce any new tricks, our contribution in this work lies in developinga framework that combines a set of tricks that jointly improve both stability and adaptability, which, asdiscussed in the literature review, is a difficult task since improved stability or adaptability often hampers theother aspect. Additionally, We provide extensive analysis of these tricks for a better understanding of theirimpact beyond the final accuracy, including the impact on stability and adaptability, class separation in thelearned embedding space, and performance improvement on base and novel classes. Furthermore, we presenta detailed study and comparison to prior works on different aspects that are not explored in the existingliterature, including low-shot (1-, 2-shot) performance, results on a dataset with a large number of classes(InageNet-1k), performance on larger encoders (e.g. ResNet-50, ResNet-101), performance on fine-tuningpre-trained foundation models (ViT-B/16, ViT-B/32, and ViT-H/14), performance when new-class hassimilarity to a base classes (included in revised manuscript). We believe the unified framework and theextensive set of experiments will add value to the further development of the challenging setting of FSCIL. Though our framework provides adequate adaptability, the performance of the novel classes is still relativelylower compared to the base classes. While this phenomenon has also been widely reported in prior works, ourframework was successfully able to reduce the gap between the performance of the base and incremental classfrom 55% in the literature to 45.2%. However, the performance of the incremental class is still considerablylower than the base class, which can be a potentially interesting direction of inquiry for future work. David Berthelot, Nicholas Carlini, Ekin D Cubuk, Alex Kurakin, Kihyuk Sohn, Han Zhang, and Colin Raffel.Remixmatch: Semi-supervised learning with distribution matching and augmentation anchoring. In ICLR,2019.",
  "Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastivelearning of visual representations. In ICML, pp. 15971607, 2020": "Ali Cheraghian, Shafin Rahman, Pengfei Fang, Soumava Kumar Roy, Lars Petersson, and Mehrtash Harandi.Semantic-aware knowledge distillation for few-shot class-incremental learning. In CVPR, pp. 25342543,2021a. Ali Cheraghian, Shafin Rahman, Sameera Ramasinghe, Pengfei Fang, Christian Simon, Lars Petersson, andMehrtash Harandi. Synthesized feature based few-shot class-incremental learning on a mixture of subspaces.In ICCV, pp. 86618670, 2021b.",
  "Huan Liu, Li Gu, Zhixiang Chi, Yang Wang, Yuanhao Yu, Jun Chen, and Jin Tang. Few-shot class-incrementallearning via entropy-regularized data-free replay. In ECCV, pp. 146162, 2022": "Xialei Liu, Marc Masana, Luis Herranz, Joost Van de Weijer, Antonio M Lopez, and Andrew D Bagdanov.Rotate your networks: Better weight consolidation and less catastrophic forgetting. In ICPR, pp. 22622268,2018. Marc Masana, Xialei Liu, Bartomiej Twardowski, Mikel Menta, Andrew D Bagdanov, and Joost VanDe Weijer. Class-incremental learning: survey and performance evaluation on image classification. TPAMI,45(5):55135533, 2022.",
  "Guangyuan Shi, Jiaxin Chen, Wenlong Zhang, Li-Ming Zhan, and Xiao-Ming Wu. Overcoming catastrophicforgetting in incremental few-shot learning by finding flat minima. In NeurIPS, pp. 67476761, 2021": "Yujun Shi, Kuangqi Zhou, Jian Liang, Zihang Jiang, Jiashi Feng, Philip HS Torr, Song Bai, and Vincent YFTan. Mimicking the oracle: an initial phase decorrelation approach for class incremental learning. In CVPR,pp. 1672216731, 2022. Zeyin Song, Yifan Zhao, Yujun Shi, Peixi Peng, Li Yuan, and Yonghong Tian. Learning with fantasy:Semantic-aware virtual contrastive constraint for few-shot class-incremental learning.In CVPR, pp.2418324192, 2023.",
  "S1Datasets and Implementation Details": "Following previous works (Tao et al., 2020; Song et al., 2023), we evaluate our framework on three populardatasets: CIFAR100 (Krizhevsky et al., 2009), miniImageNet (Russakovsky et al., 2015), and CUB200 (Wahet al., 2011). CIFAR-100 is a dataset of 100 classes, where we use 60 classes (following (Tao et al., 2020))in the base session and the remaining 40 classes in the incremental sessions. Each incremental session isformulated as a 5-way, 5-shot problem. CUB-200 is a dataset with 200 fine-grained categories. For thisdataset, we use 100 classes (following (Tao et al., 2020)) in the base session and the remaining 100 classes inthe incremental sessions, with 10 classes in each session. Finally, miniImageNet is a subset of the popularImageNet dataset that contains 100 classes. For this dataset, we use 60 classes (following (Tao et al., 2020))for the base session and 5 classes per incremental session over 8 sessions. For the encoder, we use ResNet-18 by default for all datasets, similar to (Tao et al., 2020; Kang et al., 2023).We train the model with an SGD optimizer, a momentum of 0.9, and a batch size of 64. The learning rateis set to 0.1 for CIFAR-100 and miniImageNet and 0.001 for CUB-200. For all experiments, the model istrained on an Nvidia RTX 2080 GPU.",
  "S2Comparison to State-of-the-art": "In this section, we present the results for CUB-200 and miniImageNet datasets and their comparison to priorworks. As we observe from Table S1, our framework outperforms all existing works on CUB-200 and sets astate-of-the-art of 63.60%. Similarly, for miniImageNet in Table S2, our framework performs better than allprior works, showing a new best accuracy of 59.11%.",
  "S3.1Baseline": "In this section we discuss sensitivity studies on some of the key hyper-parameters of the baseline method,including the number of training epochs and learning rate. The sensitivity study on the training epochs ispresented in Figure S1. Our findings from this study show that the best results for CIFAR-100, CUB-200 andminiImageNet datasets are obtained for training 400, 80, and 80 epochs, respectively. The study on learningrate in Table S3 shows that the best performances are achieved for a learning rate of 0.1 for CIFAR-100, andminiImageNet datasets, and 0.001 for the CUB-200 dataset.",
  "S3.2Stability Tricks": "In this section, we discuss additional results on the stability tricks. In Figure S2, we study the sensitivitytowards the number of epochs when training with the SupCon loss. This study shows that optimal performancewith the SupCon loss is observed for a relatively larger number of epochs in comparison to the baseline. Morespecifically, the best results for CIFAR-100, CUB-200, and miniImageNet are observed when trained for 500,100, and 120 epochs, respectively. Next, we discuss the experiments on pre-assigning prototypes. As discussed in .3, we assign theprototype after training for a pre-defined number of epochs. In Table S4, we study the optimal epoch beforeassigning the prototype, defined as a factor of total epochs. For instance, a value of 0.1 means the prototypesare assigned after training for 10% of the total number of epochs. As we find from this table, assigning aprototype at the beginning of the training does not yield the best performance for any dataset. The bestresults for the CIFAR-100, CUB-200, and miniImageNet are obtained for the epoch factor of 0.1, 0.5, and 0.5,respectively.",
  "S3.3Adaptability Tricks": "In this section, we discuss the experiments on the adaptability tricks. As discussed in .5, we onlytune a few layers of the encoder during the incremental fine-tuning, keeping the remaining layers frozen.Table S5 presents the results for fine-tuning different portions of the pre-trained encoder. As seen in thistable, the best results are observed when only the last ResNet block is turned, while the worst results areconsistently observed for tuning the full encoder.",
  "S3.5Performance on Different Shots": "In this section, we present additional results on different shots. More specifically, we present the results for1-, 2-, 5-, and 10-shots on the CIFAR-100 dataset. The results of this study are presented in Table S7. As wesee from this table, our framework outperforms prior works in data-scarce settings. Specifically, in a 1-shotsetting, our framework outperforms the previous state-of-the-art by a significant margin of 6.83%. In the2-shot setting, the difference increases slightly to 7.14%. In a 5-shot setting, the improvement over existingmethods decreases to 3.42%. Finally, we find a boost in performance for all the methods when we increasethe labelled samples to a 10-shot setting, with our framework showing 3.11% improvement over the SoftNet."
}