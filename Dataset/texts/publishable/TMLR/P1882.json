{
  "Abstract": "Capturing the correct tail behavior is difficult, yet essential for a faithful generative model.In this work, we provide an improved framework for training flows-based models with robustcapabilities to capture the tail behavior of mixed-tail data. We propose a combination of atail-flexible base distribution and a robust training algorithm to enable the flow to modelheterogeneous tail behavior in the target distribution. We support our claim with extensiveexperiments on synthetic and real world data.",
  "INTRODUCTION": "Real-world data often show mixed tails - both heavy (representing black swan events, seen in communicationnetworks traffic and actuarial risk (Wang et al., 2006; Afify et al., 2020)) and light (indicating events withinnarrow outcome ranges, seen in certain pricing models and extreme engineering events (Singh & Gor; Jamissenet al., 2022)). Capturing tail behavior accurately is vital in data synthesis, especially in sectors such ashealthcare, where mis-estimation can cause significant inaccuracies in analyses based on the synthesized data,leading to misinferred risks (Ibragimov et al., 2015). Normalizing flows (Papamakarios et al., 2021) present an attractive model for mixed-tailed data synthesis.This is due to the fact that the tail behavior of the base distribution directly and transparently affectsthe tail of the generated data (Jaini et al., 2020). Other classes of deep generative models do not providesuch an explicit mechanism to control their tails. demonstrates the impact of mis-specification ofparameterization of base density on the capabilities of normalizing flows on capturing the tail behavior oftargets with different tail behaviors.",
  "Published in Transactions on Machine Learning Research (10/2024)": "GASThe GAS dataset (Fonollosa et al., 2015) contains measurements from 16 chemical sensors exposed togas mixtures over a duration of 12 hours. Analogous to the POWER dataset, it is a time series that has beenhandled as if each instance were an i.i.d sample from the marginal distribution. Data was exclusively usedfrom the file ethylene_CO.txt, which pertains to an ethylene and carbon monoxide mixture. The removal ofhighly correlated attributes resulted in an eight-dimensional dataset. The training data has 852,174 examplesfeaturing 8 variables. HEPMASSThe HEPMASS dataset (Baldi et al., 2016) contains particle collision measurements in thefield of high-energy physics. Half of the instances denote particle-generating collisions (positive), while theremainder originate from a background source (negative). For this study, positive examples from the \"1000\"dataset were selected. Five features were excluded due to their high frequency of recurring values, as suchrepetition can induce spikes in the density and potentially yield misleading outcomes. The training data has315,123 examples featuring 21 variables. MINIBOONEThe MINIBOONE dataset (Roe et al., 2005), is derived from the MiniBooNE experimentconducted at Fermilab. Like HEPMASS, it contains positive (electron neutrinos) and negative examples(muon neutrinos). For this study, only the positive examples were used. Notable outliers (11 instances)exhibiting a constant value of -1000 across all columns were removed, as well as seven features displayingexcessively high counts for specific values, such as 0.0. The training data has 29,556 examples featuring 43variables.",
  "Characterizing Tail Behavior": "A random variable X is said to follow a heavy-tailed distribution if its tail is not exponentially bounded.This can be formally defined using the moment generating function (MGF): MX(t) = E[etX]. A distributionis heavy-tailed if there exists some t > 0 such that MX(t) is infinite or does not exist. An example of thiscategory is the Pareto distribution with scale parameter > 0, for which the MGF is MX(t) =",
  "t, which isundefined for t": "The tails of a light-tailed random variable X decay sub-exponentially. The Gumbel distribution, with locationparameter and scale parameter > 0, is an example of a light-tailed distribution as its survival functionSX(x) = ee(x)/ indicates a sub-exponential decay for large x. In many real-world scenarios, we encounter data with mixed tail properties. We define such data as havingmixed tails if some of its marginal distributions exhibit heavy tails and others light tails. This underscoresthe multifaceted nature of real-world data, where different variables or features may be governed by distinctdistributional properties. Tail Index EstimationThe tail behavior of a probability distribution is primarily characterized by theirtail index. The tail index, which we denote as , is defined as the exponent in the power-law tail of thedistribution:F(x) = P(X x) 1 x where P(X x) is the probability of the random variable X being greater than or equal to a certain valuex, and F(x) is the cumulative distribution function of X. As approaches zero, the distribution follows apower law to a greater degree. Hills estimator (Hill, 1975) is a widely-used method for estimating the tailindex. It is defined as follows:",
  "Normalizing Flows": "Normalizing flows (NFs) (Tabak & Turner, 2013; Rezende & Mohamed, 2015; Papamakarios et al., 2021) aremodels represented by a sequence of invertible transformations that warp a simple base distribution (such asa standard normal) into a richer target distribution. The transformations are chosen such that their (log)Jacobian determinant is easy to compute, allowing for efficient computation of the likelihood. Let u pu(u)denote the base density, and let the sequence of invertible transformations be denoted T = TN1 . . . T0,where denotes the neural network parameters. We can evaluate the density of observation x by applyingthe change of variables formula:",
  "CHALLENGES OF TRAINING TAIL-ADAPTIVE NORMALIZING FLOWS FORMIXED-TAIL TARGETS": "Reviewing literature indicates that to adapt a flow models base density to the target tail, existing methodseither incorporate assumptions about tail behavior into the model through a specific base density selection, asin Jaini et al. (2020), or directly estimate the targets tail behavior, as in the marginal adaptive base methodby Laszkiewicz et al. (2022). Furthermore, previous research (Behrmann et al., 2021) shows flow-based modelssuffering from the exploding/vanishing gradients problem, which will be exacerbated in presence of tailedmini-batches during training. We explore the limitations of these approaches in modeling mixed-tailed data.",
  "Challenges in Adjusting the Tail Behavior of the Flow Models Base Density": "Pre-Hoc Selection of Base Density Tail Behavior for Mixed-Tail TargetsReal-world data may havetail behavior that deviate from any specific choice of base distribution family such as having a multi-modalmixed-tail distribution. Therefore, choosing a specific heavy-tailed parametric distribution family as the flowbase as suggested by Jaini et al. (2020) a priori will inherently restrict the range of possible tail shapes andasymptotic decay rates that can be represented. The other issue with this method for mixed-tail targets isbased on the fact that normalizing flow transforms on top of the base distribution can either alter its tailbehavior significantly or not converge due to exploding/vanishing gradients. Thus to make the flow robust,the tail modeling capabilities must come more from the flows rather than the base distribution alone. Pre-Hoc Estimation of Base Tail Index for Mixed-Tail TargetsAdapting the tail behavior ofthe flows base density directly on the target distribution is another way to make the base distribution tailadaptive, with a prominent example being Marginal Tail Adaptive Flows (mTAF) by (Laszkiewicz et al.,2022). mTAFs calculate the tail index of each marginal of x. Since tail estimation is notoriously difficult,Laszkiewicz et al. combine the results from three different tail index estimators. The base density for light-tailmarginals is set to a Gaussian and a univariate Students t with a degree of freedom equal to the estimatedtail index for heavy tail marginals. The limitation of these methods for modelling complicated mixed-tail targets is their reliance on estimatedtail index of the target distribution. Tail index estimators generally suffer from high variance and sensitivityto tuning parameters, especially for small sample sizes or multidimensional settings. For example, a majordrawback of Hills estimator is its sensitivity to the choice of k. Mason (Mason, 1982) demonstrated that His inconsistent if k stays fixed as n approaches infinity.",
  "Challenges in Maximum Likelihood Training of Flow Models for Mixed-Tailed Targets": "Regardless of the choice for the base distribution and its tail properties, we argue that it is simply not enoughto enable the flow to be trained properly and capture the tail behavior of a mixed-tail target distribution.The reason is unstable training due to increased variance of the gradients when the target is heavy-tailed. Consider the likelihood term in the maximum likelihood training objective (Equation (1)). Let u = T 1(x; )be the inverse transformation of the observation x. The term log pu(u) in the likelihood involves the logarithmof the base density evaluated at u. In the presence of heavy tails, the density can have slower-than-exponentialdecay, resulting in higher likelihood contributions from extreme values of u (and consequently, x). Similarly,",
  "Tail-Adaptive Base Density for Mixed-Tail Targets": "We propose using a mixture of Generalized Gaussian Distributions (GGDs) as the base density of the flow.The Generalized Gaussian represents a family of continuous probability distributions that extend the conceptof the Gaussian distribution by incorporating a shape parameter, denoted as > 0. Formally, the probabilitydensity function (PDF) of a GGD with mean , scale parameter > 0, and shape parameter > 0 is definedas:",
  "|x |": "where () is the gamma function. This distribution encompasses a wide range of density shapes includingbut not limited to the Gaussian distribution ( = 2) and Laplacian distribution ( = 1). Higher values of give rise to light-tailed distributions, whereas lower values lead to heavy-tailed distributions. shows the flexibility of the tail behavior of GGD with different shape parameters (for a more thoroughdiscussion on GGDs, see (Dytso et al., 2018)).",
  "where m is the component index, m is the weight of the mth component such that m m = 1, andm are the parameters of the mth GGD component parameterized by (, , )": "Our choice for using a mixture for better modelling mixed tail behavior is substantiated by previous workthat successfully used mixtures to model tailed distributions (Feldmann & Whitt, 1998; Okada et al., 2020;Venturini et al., 2008). Hagemann & Neumayer (2021) show that normalizing flow training can be stabilizedin cases (e.g. disjoint support of base and target) by using a mixture for pu(u). We posit that training a flowmodel on mixed-tail targets will similarly introduce optimization difficulties that may only be exacerbated byhaving a flexible base density with wide-ranging tail behavior. Satisfying the Tail Condition of the Base DensityJaini et al. (2020) show that for Lipschitz triangularflows to be able to capture the tail of the target distribution, their base density should be at least as heavytailed. When the tails of the base density pu are fixed, and the determinant of the Jacobiandet JT 1 (x)is bounded due to the Lipschitz property, it follows that p cannot exhibit heavier tails than pu, as thetransformation T cannot increase the rate of decay of pu at its tails. This implies that for u pu(u) withknown tail behavior, and x p(x), the tail properties of p are essentially inherited from pu and limited bythe expressiveness of T, ultimately constraining the models capacity to represent target densities p withtails heavier than those of the base density pu. To comply with this requirement, our choice of a flexible GGD mixture base is motivated by the the abilityof GGD to control its tail behavior through its shape parameter. We argue that the mixture base densityrequires only one component to be at least as heavy-tailed as the target density for the flow model to adhereto the requirement of Jaini et al. (2020), while the flexible nature of the mixture will let the flow capturecomplex mixed-tail behavior of the target density.",
  "Training Flows On Mixed-Tail Targets": "In the previous section we proposed using a mixture of generalized Gaussians base density for modellingheavy and mixed-tail targets. We also argued that the flexibility to model the tail behavior should comefrom the flow itself and not be put entirely upon the base density. In this section, based on the motivationprovided in the .2, we propose our method for stabilizing the training process of the flow models bymaking it tail adaptive.",
  "Our Proposed Robust Monte Carlo Maximum Likelihood-Based Training": "To mitigate the problem described in previous section, we propose using robust estimation methods during themaximum likelihood estimation. Robust gradient estimators have been employed similarly in the literature(e.g. Hsu & Sabato (2016)). However, to the best of our knowledge they have not been applied to the problemof stabilizing MLE in presence of mixed-tail behavior. Specifically, we propose employing the GeometricMedian (GM):",
  "To motivate this, we pay attention to mean estimation in presence of heavy-tailed data. Let X1, X2, . . . , Xnbe i.i.d. random variables with a heavy-tailed distribution. Let Xn = 1": "nni=1 Xi be the sample mean andXmed be the sample geometric median. Then Var( Xn) Var(Xmed). For heavy-tailed distributions, thevariance can be arbitrarily large. In fact, by definition of heavy tails, the tails of the distribution decay slowerthan an exponential distribution. Therefore, for any finite V , there exists some x0 such that:",
  "n": "Where f(x) is the probability density function. This means there is non-negligible probability mass in theheavy tails that can lead to extremely large values of Xi. These extreme values disproportionately increasethe variance of the sample mean Xn (more in depth discussion can be found in literature, e.g. (Sun et al.,2015)). On the other hand, the geometric median only depends on the relative ordering of the Xi values, nottheir magnitudes. Therefore, it has robust variance in the presence of heavy tails. Next we posit that using the geometric median estimator for gradients instead of the sample mean whentraining normalizing flows on mixed-tailed data reduces the variance and leads to more stable optimization.From the training objective, the gradient is:",
  "i=1 log px(f(zi))": "We already established that when px is heavy-tailed, the gradients log px(f(z)) have very high variance.Therefore, using the geometric median instead of the sample mean to aggregate the gradients reduces thevariance while still being a consistent estimator. This results in more stable optimization. Robust estimatorslike the geometric median can mitigate the training instability induced by mixed-tailed target distributionsfor normalizing flows. We demonstrate this phenomenon in , which depicts the instability of thestandard maximum likelihood estimation in presence of heavy tails. As can be seen, regardless of whether themodel is mis-specified or not or the tail behavior of the base and target densities, the loglikelihood estimateis impacted by the heavy-tailed minibatches, resulting in a biased estimation of expectation. This bias ismore prominent when the base density is heavy-tailed, which explains why the proposed method of Jainiet al. (2020) yields mixed results in practice. We employ the Weiszfeld method (Eftelioglu, 2017) to estimatethe geometric median.",
  "EXPERIMENTS": "We experiment using a range of simulated and tabular data. We train Real NVP (RNVP (Dinh et al.,2017) and Masked Autoregressive (MAF (Papamakarios et al., 2017)) flows with GGD mixture base density(100 components, learnable parameters) and robust MLE. The choice of RNVP is motivated by it being aLipschitz-continuous function, which results in the density function having the same tail properties as itsbase, making the dynamics of the model simpler and easier to interpret. The shift and scale operations ofRNVPs are modelled by an feed forward network with one hidden layer the width of 1024. The MAF ispotentially more expressive than RNVP, letting us study the performance of our method for different flowmodels. Each flow model is 12 steps deep, with the MAFs having 12 autoregressive layers which use a stackedtransformation with 1024 hidden units and 2 blocks to model the conditional dependencies of the variables.Minibatch size is 1024 and we perform Adam optimization (lr = 105) All trainings are over 1000 iterations.We perform experiments in this section 5 times - unless explicitly mentioned - and report the mean andstandard error.",
  "Data Quality metrics": "To measure the performance of our proposed method we perform experiments on synthetic and real dataand report the negative log likelihood. Also following Laszkiewicz et al. (2022) we report the tail-specificperformance metrics of Tail Value at Risk (tVaR) and Area Under Log-Log Plot (AULLP). The details of themetrics used can be found in the Appendix C. Average Negative Log LikelihoodIn the context of normalizing flows, the negative log-likelihood (NLL)loss is often used as a performance metric to train the model. The NLL loss measures the models abilityto approximate the true data distribution. The negative log-likelihood of the model given the dataset D isdefined as:",
  "i=1log p(x(i))": "Tail Value at RiskTail Value at Risk (tVaR) is a measure used to estimate expected losses beyond acertain threshold, known as the Value at Risk (VaR). It calculates the average loss above the VaR levelbased on a given quantile. The tVaR is obtained by taking the conditional expectation of the loss variable,given that the loss exceeds the VaR level. The VaR itself is determined as the minimum value of the lossvariable for which the cumulative distribution function exceeds the specified quantile level. We report thetVaR difference between test dataset and synthetic dataset. Area Under Log-Log PlotThe Area Under Log-Log Plot (AULLP) involves integrating the logarithmof the survival function, which represents the probability of a random variable exceeding a given value, inlog-log space beyond a high quantile threshold. We report the The AULLP difference between test datasetand synthetic dataset.",
  ": Performance of RNVP flow with different base distributions for estimation of a mixed-tail target.Our modified Neals funnel with = 6.0 has a heavier-tailed and a lighter-tailed variable": "Results compares the performance of our method for an RNVP flow with baselines with multivariateGaussian base (as is commonly used in practice), Students t and generalized Gaussian base densities. Ourmethod performs well for both variables while the other methods performance is sub-optimal for one or bothvariables. shows performance of an RNVP with difference base densities. Results reported here show theadvantage of our proposed method. We observe a meaningful advantage for our method, especially for theheavier-tailed variable (v2). Of note is the high variance of the flow with Students t base (following Jainiet al. (2020)) which as discussed in section 3 is due to attenuated tail misspecification in the presence ofmixed-tailed targets. The mixture of Gaussian base reports better results for the lighter tailed variable (4.58vs 6.1 for AULLP, 54.5 vs 73.72 for tVaR)which is to be expected since v1 is a standard Gaussian and hasthe same normal tail behavior as the flow base. As can be observed in the next section, for targets with morecomplicated tail behavior, the mixture of Gaussian base loses its advantage.",
  "Ablation Study": "DataTo measure the impact of each aspect of our method, we perform an ablation study. We use two basedistributions, a GGD and a mixture of GGDs. We train an RNVP model on both bases with and withoutrobust MLE. For each base, we train flows on a two-dimensional distribution P(X1, X2) such that X1 followsa generalized Gaussian distribution, and X2 adheres to a Students t-distribution. X is parameterized by .",
  "Performance Comparison with Pre-Hoc Tail Estimation Methods": "In this section we report the performance of our method on a synthetic experiment from Laszkiewicz et al.(2022) to compare the performance of our method to the state of the art in pre-hoc tail estimation-dependentmodels. DataThe target dataset is generated by a 8-dimensional Gaussian copula. The marginals of the copulaconsist of two Gaussians, followed by a 2-mixture and a 3-mixture of Gaussians. The last four marginalsare a mixture of two t-distributions with = 2. Mixtures have randomized means and variances and equalweights and the correlation matrix is randomized.",
  "More details can be found in Appendix C.2 of Laszkiewicz et al. (2022)": "Results compares the performance of our method with that of Laszkiewicz et al. (2022). We reportthe results of training our model on an RNVP - with the same architecture used in previous experiments -with the same setup as per .1 of Laszkiewicz et al. (2022). Results of Laszkiewicz et al. (2022) arecopied from the dh = 4 section of their . The metrics are reported for heavy tailed and light tailedcomponents separately (denoted by h and l subscripts). We observe that our method performs better in caseof heavy-tailed components while Laszkiewicz et al. (2022)s mTAF reports better results for light-tailedcomponents. We attribute our methods slightly lower performance in case of light tails to mis-specificationof tails since Laszkiewicz et al. (2022) use Students t base marginals with the same degree of freedom as thesynthetic example ( = 2) for heavy-tailed target components. This pattern is consistent with results from.2.",
  "Tabular Datasets": "DataTo test our method in a more realistic setting, following and expanding on the empirical evaluationsdone by other methods in this area we train our model on four tabular datasets from the UCI repository,namely Power, Gas, Miniboone and Hepmass. We focus on tabular data for the ease with which one canmeasure and evaluate the tailedness of the training and synthetized data. For other data types such as imagesand language, due to their inherent high-dimensionality and the rich spatial (for images) and contextual(for language) dependencies it is difficult to define, measure and evaluate the tailedness of the data andthe tail-adaptiveness of the model. We follow the same preprocessing steps as Papamakarios et al. (2017)(According to Appendix Section D of their paper). We train RNVP and MAF flows with different basedensities as well as our own method. We report negative log likelihood and mean AULLP difference andtVaR difference over all columns. ResultsIn , we present a comprehensive evaluation of our proposed approach for an RNVPnormalizing flow. The results are contrasted against various base distributions with trainable parameters. Wereport average tVaR and AULLP over all variables. Our methods superiority is evidenced by the lowest NLLand tVar for all datasets. We report lowest AULLPs for all datasets except MINIBOONE (1.19 vs 1.01). Even",
  "RNVP +NLLAULLPtVaRNLLAULLPtVaRNLLAULLPtVaRNLLAULLPtVaR": "Gaussian base8.86 .5616.57 1.320.74 .0510.41 .643.38 .221.5e2 9.937.8 .479.28 .482.02 .1656.29 4.751.34 .1067.29 4.07Mx.Gaussian base8.01 .3616.51 1.060.72 .0410.33 .593.21 .201.21 .087.78 .529.12 .641.79 .1052.97 3.581.01 .0565.91 2.72Students t base9.13 1.5324.6 2.74100.5 10.1410.19 1.3810.07 1.192.1e9 3e810.33 1.0721.45 2.282.7e6 2.4e561.63 6.092.05 .281.6e3 2.5e2Mx.Students t base7.9 .5316.6 1.010.72 .0610.2 .833.34 .273.38 .297.82 .509.21 .751.73 .1754.75 5.021.25 .0966.32 5.36GGD base9.02 .8217.5 1.960.69 .0810.41 .843.38 .391.5e2 11.768.45 .899.64 .562.1 .1365.39 6.674.37 .2772.17 6.45Mx.GGD + R.MLE7.22 .5415.4 1.670.44 .038.17 .703.18 .301.12 .096.02 .688.64 .761.59 .1549.68 3.741.19 .1063.39 6.77 in this case the difference is relatively small. Notably, the stark tVaR discrepancy, especially in HEPMASSwith Students t base (2.1e9 vs 1.12), underscores the challenge of tail estimation which our method effectivelyaddresses. Our method reports superior results across the board except for tVar in HEPMASS dataset (1.08vs 1.12), again with a very small margin. This comparison showcases the robustness of our methodology forcapturing the tail behavior of the mixed-tail targets. Also of note is the inferior performance of the Studentst and GGD bases. Considering these results alongside results from section 5.2 we conclude that while theGaussian base maintains a fixed tail behavior and produces average results dependant on the tail behavior ofthe target distribution and its pathologies, since both Students t and GGDs tail behavior is parameterizedand trainable, the flow either pushes them to underestimate the tail - violating the requirements set by Jainiet al. (2020) - or overestimate the tail, leading to generation of samples with extreme values which will resultsin subpar performance in tail-specific metrics. : Comparison of NLL, AULLP difference and tVaR difference for our proposed method applied toa MAF in comparison with the same model with different base distributions. R.MLE refers to the robustgradient estimation of .2.1.",
  "MAF +NLLAULLPtVaRNLLAULLPtVaRNLLAULLPtVaRNLLAULLPtVaR": "Gaussian base3.21 .2318.07 1.387.8 .7054.25 2.9913.42 1.1517.27 1.422.37 .149.45 .723.39 .2597.12 6.973.86 .267.84 .55Mx.Gaussian base3.23 .1918.05 1.047.77 .4754.25 2.3913.34 .7717.1 1.122.37 .159.38 .613.32 .2197.12 5.073.87 .197.81 .45Students t base1.72 .1724.35 2.7817.03 1.5627.55 4.3316.95 2.8436.23 3.571.29 .1215.8 1.5714.92 2.0636.12 3.8312.9 2.1513.06 1.93Mx.Students t base3.18 .2918.17 1.567.94 .7653.48 4.3613.42 1.1517.27 1.223.81 .379.7 .643.11 .3295.57 10.203.89 .257.11 .70GGD base1.36 .1627.79 2.0426.45 2.0451.13 5.8913.68 .9519.08 1.911.1 .0913.47 1.4411.57 1.2581.53 10.3717.37 1.6918.52 2.40Mx.GGD + R.MLE5.95 .5815.19 1.445.36 .63102.4 7.219.94 1.088.65 .631.57 .146.96 .631.47 .1096.13 6.773.81 .437.93 .95 We also investigate the efficacy of our method on a Masked Autoregressive Flow. Our findings, summarizedin , strongly suggest that integrating the GGD mixture base with robust MLE significantly improvesthe models general performance - in terms of Negative Log Likelihood (NLL) - as well as its capabilities tocapture tail behavior - as evident from metrics AULLP and tVaR, for which we report the difference. Our proposed method achieves the lowest NLL for all datasets except POWER and in this exception themargin of difference is quite small. This improvement is most pronounced in the HEPMASS dataset, wherethe NLL reached a remarkable 102.4. Additionally, our approach also yields the smallest AULLP and tVaRvalues in most datasets, highlighting its capability to more accurately capture tail distributions and overalldensity estimation.",
  "DISCUSSION AND FUTURE WORK": "In this work we demonstrated the practical utility of using generalized Gaussians for base density coupledwith a robust maximum likelihood training algorithm when training normalizing flows to model mixed-taileddata. We proposed a new method of making normalizing flows capable of modelling mixed-tail targetswithout explicitly binding the tail behavior of the base density to that of the target and instead letting thegradient-based optimization shape the tail of the base. We showcased the general capabilities of our proposedmethod in estimating mixed-tail targets. Our method could potentially benefit from being able to set the number of modes in the base mixtureadaptively. We varied the number of mixture modes for different tasks and observed an inflection point inperformance enhancement with the increase of mixture components, beyond which the models effectiveness",
  "APoints of Failure in Tail Estimators": "shows the accuracy and consistency of the bootstrapped Hills estimator H for samples withvarying sizes from a Pareto distribution with different tail indices. We report the difference between true andestimated tail indices. We observe that H overestimates for heavy-tailed samples and underestimates forshort-tailed ones. We also observe increased variance for smaller batch sizes closer to the usual minibatchsizes from extremely heavy-tailed targets.",
  "BSelective Generation by Exploring the Latent Space": "In this section, we introduce an extension on our tail-adaptive normalizing flows method for controlleddata generation, enabling precise sampling from specific regions of complex probability distributions. Byregularizing the training process to encourage the mixture components to spread out in the latent space,different components specialize in generating different regions of the target distribution, from the tails to thehigh-density areas around the mean. This allows for targeted sampling from specific components to generatesamples from desired regions, offering benefits in applications like fairness-aware machine learning whereoversampling minority groups in the tail regions is essential. We posit that, given our use of a tail-adaptive mixture to model the latent space of our flow model, andconsidering that each component of the mixture can exhibit a spectrum of tail behaviors, different individualcomponents of the mixture or subsets thereof will, upon model convergence, be responsible for generatingdifferent parts of the target marginals relative to their tail behavior. In other words, some components willbe responsible for generating the tail of the target distribution while alternate components are assignedto represent the regions of high probability density, typically centered around the distributions mean. Tocapture extreme tails, we find that it is helpful for the components of the mixture base distribution to be",
  "k=1||g k||2(3)": "where g denotes the geometric median of the component locations (k). Since the means could, due toinitialization or the stochastic nature of the optimization, have long tails, we calculate the geometric medianof the means, g and then calculate the sum of the l2 norms of the means and the calculated medians as thetotal spread of the means. We add the R term to the models loss function. The model is thereby rewardedduring optimization by increasing the distance across the locations.",
  "L LendAlgorithm 1: Training Algorithm for tail-adaptive normalizing flows with enhanced selective generation": "The regularization term in our loss function incentivizes the model to push the means of the mixture modesapart, covering a larger part of the latest space - including the tail area - instead of being centered aroundcertain areas. This by design leads to each sample mapped to the latent space having a small subset ofmixture modes as its nearest neighbors. We leverage this aspect of our model for Selective Generation. To generate samples that are on the tail area of the distribution or near its mean, or in case of complexreal world dataset in a specific area of the joint density of a subset of its variables we can map one orfew samples from that area to the latent space, find k nearest mixture modes to these samples k being apredetermined number or the radius of a hypersphere around the geometric median of our subsample andthen generate our desired number of samples from these modes by sampling from them indiscriminately or bynormalizing their weights to form a discrete mixing distribution and forming a separate mixture model. The advantage of this method is especially evident for areas such as fairness where minority groups aredefined in the joint tail section of several features of the dataset. We can upsample any subgroup within ourpopulation which has been shown to help mitigate some of the problems with fairness and more generallywith performance of ML models. The advantage here is that we are not blindly exploring the latent space.Rather, we are systematically choosing the components in our base distribution with highest probability ofbeing responsible for generating samples similar to our target sample or subsample and sample directly fromthose components to be pushed forward through the flow model and be transformed into our target.",
  "B.1Selective Generation in Practice": "illustrates an example of our methods capabilities for selective generation. We train an RNVP usingAlgorithm 1 with a 20-component GGD mixture base and robust MLE on a target dataset sampled from atwo dimensional Students t with =2. We then sample a dataset with the size of 100000 from the Studentst target and evaluate the probabilities of each sample on our target distribution. We bin the probabilitiesinto 15 equal sized bins, with samples belonging to bins with lower probabilities assumed to be further downeither tails of the distribution and vice versa. We then evaluate the samples of each bin on every single of the20 training base mixture components. The component that exhibits the highest average probability valuefor each bin is assigned as the primary generator for that bin. Since the samples have the highest averageprobability value when evaluated on that specific component. Therefore, if we sample from that componentin the latent space and push the samples through the flow, we will predominantly yield instances that arebelong to that specific bin. In , for each bin the component with the highest mean probability values",
  ": Selective generation from an RNVP model with a 20 component GGD mixture base for a 2dStudents t target": "is marked. We note that component 0 is primarily responsible for generating instances for the bin with thelowest average probabilities (i.e., instances on the tail as evaluated on the target distribution itself), whilethe majority of the distribution mass is generated by components 3 and 18. Component 15 generates theremaining instances, notably those that are not extreme on the distributions tail but are situated furtherfrom the distributions mean. It should be noted that our target is a two dimensional target. However, torefrain from over-complicating this analysis we do not discriminate between the dimensions when binningthe samples. If each marginal is considered separately, it will offer a more nuanced control over the samplegeneration process. In a scenario with a real world target dataset, we will not bin the samples, but rather select a representativesample of the type we want to generate from the target dataset. We then map these representative samplesto the latent space. Once we have the mappings in the latent space, we could, for example, choose the topk components with the highest probability values for our selective generation and use a simple weightingscheme (e.g., using the average likelihood values or distance to the component mean), creating a sub-mixturefor that particular subset of samples. This sub-mixture will then enable us to upsample our dataset throughgeneration of samples similar to our representative samples. While not a primary focus of the main paper, the concept of selective generation through targeted samplingwithin the latent space presents a promising avenue for future research. This approach could be particularlyvaluable in applications like fairness-aware machine learning, where oversampling minority groups representedin the tail regions of the data distribution is crucial.",
  "evaluates a generative models tail capture capability. Smaller tVaR-difference implies better model perfor-mance in mimicking the distributions extreme quantiles": "Area Under Log-Log PlotThe Area Under Log-Log Plot (AULLP) involves integrating the logarithmof the survival function, which represents the probability of a random variable exceeding a given value, inlog-log space beyond a high quantile threshold. We report the The AULLP difference between test datasetand synthetic dataset. The Area under the log-log plot (AULLP) measures a models ability to capture the far distribution tail,emphasizing extreme values more than the Tail Value at Risk (tVaR). It involves integrating the survivalfunction (S(x) = 1 F(x)), which represents the probability of a random variable X exceeding a value x, inlog-log space beyond a high quantile threshold x (e.g., 95th or 99th percentile):",
  "Following the steps from Papamakarios et al. (2017), we prepared the data. The next parts of this sectionwill give a short explanation of the preprocessing done for each dataset": "POWERThe POWER dataset (Hebrail & Berard, 2012) includes records of household electricity use over47 months. Although originally a time series, records were treated as separate, identical samples. The timewas changed into an integer representing minutes in the day, and random noise was added. The date andthe global reactive power parameter, which often shows a zero value, were removed to prevent unexpectedchanges in the distribution. Uniform noise was added to each feature. The magnitude of the added noise waslarge enough to avoid duplication but small enough to keep the data values largely the same. The trainingdata has 1,659,917 examples featuring 6 variables."
}