{
  "Abstract": "Much of explainable AI research treats explanations as a means for model inspection. Yet,this neglects findings from human psychology that describe the benefit of self-explanationsin an agents learning process. Motivated by this, we introduce a novel workflow in thecontext of image classification, termed Learning by Self-Explaining (LSX). LSX utilizesaspects of self-refining AI and human-guided explanatory machine learning. The underlyingidea is that a learner model, in addition to optimizing for the original predictive task, isfurther optimized based on explanatory feedback from an internal critic model. Intuitively,a learners explanations are considered useful if the internal critic can perform the sametask given these explanations. We provide an overview of important components of LSXand, based on this, perform extensive experimental evaluations via three different exampleinstantiations. Our results indicate improvements via Learning by Self-Explaining on severallevels: in terms of model generalization, reducing the influence of confounding factors, andproviding more task-relevant and faithful model explanations. Overall, our work providesevidence for the potential of self-explaining within the learning phase of an AI model.",
  "Introduction": "Self-reflection is considered an important building block of human intelligence and a crucial componentin the learning process of humans (Glser-Zikuda, 2012; Ellis et al., 2014).In fact, one aspect of self-reflectionself-explaininghas been identified in several psychological studies as greatly beneficial for theoverall learning, problem-solving and comprehension abilities of human subjects (Chi, 2018; Chi et al., 1981;1994; Chamberland & Mamede, 2015; Belobrovy, 2018; Larsen et al., 2013; Kwon & Jonassen, 2011; Bisraet al., 2018). Accordingly, self-explanations have been suggested to act as a means of making initially implicitknowledge explicit and therefore to represent an important component for critical self-refinement. Indeed, recent works in machine learning (ML) research have picked up on the idea of self-refining. Whilesome are directly inspired by findings from human studies (Madaan et al., 2023), others utilize the potentialof pre-trained large language models (LLMs), e.g., on the topics of self-debiasing (Schick et al., 2021), self-instructing (Wang et al., 2023) and self-rewarding (Yuan et al., 2024). Although these works are quite specific",
  "(I.) Self-refining ML": ": (left) Current (self-)refinement machine learning utilizes (I.) forms of self-supervised model refinement(e.g., self-rewarding). On the other hand, it also relies on (II.) explanatory interactive learning (XIL) which uti-lizes human feedback via explanations for model refinement. (right) In contrast, we introduce Learning by Self-Explaining which integrates ideas from both research fields into one approach as explanation-based self -refinement(I + II.). A model in LSX consists of two submodels, a learner and (internal) critic, and performs refinement via fourmodules (Fit, Explain, Reflect, Revise, cf. Alg. 1). The learner is optimized for a base task in Fit (e.g., imageclassification), after which it provides explanations to its decisions in Explain. In the Reflect module, the criticassesses how useful the explanations are for performing the base task. The resulting feedback from the critic is usedto Revise the learner. in their form of self-refinement (cf. survey by Pan et al. (2023)) and far from the general idea of self-reflectionfrom human psychology, they do provide valuable first steps for more reflective AI models. However, noneof these focus on the value and potential of explanations as the basis and means of such reflective processes. On the other hand, research on interactive machine learning (Teso et al., 2023; Gao et al., 2024), such asexplanatory interactive learning (XIL) (Teso & Kersting, 2019; Schramowski et al., 2020), has long identifiedthe value of explanations as a means of communication between human users and AI models, particularly, asa very successful means for model refinement. However, hereby explanations are only leveraged for refinementthrough human guidance. In this work, we introduce Learning by Self-Explaining (LSX), a novel workflow that combines the ideas ofself-refining ML and XIL (cf. (left)) by leveraging explanations in the learning process of a model priorto any form of human explanatory feedback. In LSX (cf. (right)) a model consists of two submodels,a learner and an internal critic, and undergoes four distinct learning modules (Fit, Explain, Reflect,Revise). In Fit, the learner is trained on a base task (e.g., image classification) upon which it providesexplanations for its predictions (Explain). The critic next assesses the quality of these explanations forperforming the base task (Reflect). Intuitively, a useful explanation should thereby provide importantinformation for the task at hand. Finally, the critics feedback is used to revise the learner (Revise) andthe Explain, Reflect and Revise loop is repeated, if needed. Overall, we consider the two submodelsto constitute a collective model, whereby both work jointly to improve on the same base task. However, atinference time, only the learner provides the final task predictions (cf. (right)). We introduce LSX in the context of image classification and provide experimental evaluations via multipledatasets and evaluation metrics to investigate the benefits of learning by self-explaining. Specifically, theseevalautions focus on the generalization performances of a learner, mitigating confounding behavior, consol-idating explanations and explanation faithfulness. We perform these evaluations on three diverse exampleinstantiations where the learner is based on a convolutional neural network (CNN), a neuro-symbolic (NeSy),and a vision-language model (VLM), respectively. Overall, our contributions are the following: (i) We introduce LSX, a novel learning workflow in the spiritof XIL and self-refining ML in which a model refines itself by assessing its explanations. (ii) In the contextof image classification, we introduce several example instantiations for LSX thereby illustrating different",
  "model and learning configurations. (iii) We provide extensive experimental evidence on various datasets andevaluation metrics, illustrating the potential of LSX for model refinement": "We proceed as follows. In section 2, we formally introduce LSX. For our experimental evaluations in section 3,we introduce several example instantiations for LSX and provide results for multiple datasets, metrics, andablations. We wrap up our work with an extensive discussion of our results as well as related work and leavethe reader with a final conclusion.",
  "Learning by Self-Explaining (LSX)": "In the following, we introduce Learning by Self-Explaining in the context of image classification and presentan overview of its important components and modules. Let us first provide some background notations. In LSX, a model is comprised of two submodels: the learner submodel, denoted as f, and the internal critic,denoted as c (cf. (right)). Further, let x X be an image, with X := {xi}Ni=1, and with correspondingclass label y Y with Y NN K for K classes. Hereby, N N denotes the number of samples. We denoteX := (X, Y ) as the joint set of images and corresponding labels and Xc X as a subset of this that isspecifically provided to the critic (details below). While the learner performs an underlying base task, e.g., image classification, the critics role is to providefeedback on the learners explanations within the task. Overall, the learner represents a trainable predictivemodel (e.g., a CNN). The critic also represents a predictive model which, however, must not necessarily belearnable. Its specific model type should be chosen depending on constraints such as the type of explanationthat the learner provides (will be discussed below). At inference, the learner provides the overall modelsfinal predictions (cf. (right)). The general procedure of LSX can be described via four modules (inspired by Friedrich et al. (2023a)): Fit,Explain, Reflect, and Revise, where the last three modules are the core modules of LSX (cf. ).Let us now describe these four modules in more detail based on the pseudo-code in Alg. 1. Optimize for the base task (Fit). The Fit module describes the underlying, base learning task in whichthe learner is optimized to solve a particular problem, e.g., supervised image classification. Ultimately, Fitreturns a model that has been optimized for the base task, i.e., f Fit(f, X) (cf. Alg. 1). In general,this module represents the vanilla learning approach for the underlying task. The details of this are thusindependent of LSX. E.g., for the task of supervised image classification a CNN-based learner might beoptimized via a standard cross-entropy loss. More formally, f is provided with a sample from the trainingdataset, (x, y) X, and makes predictions y = f(x).Its latent representations are optimized given acorresponding loss function, lB (e.g., cross-entropy: lB = lCE(y, y)). Generally, within Fit, one can employany bells and whistles of modern machine learning setups (e.g., hyperparameter optimization via cross-validation, learning rate schedulers, etc.) and optimize it until it has reached satisfactory performances.",
  "Published in Transactions on Machine Learning Research (09/2024)": "confounder. We observe that both LSX configurations do not put any importance on this confounder forthis sample. The bottom row shows the same setting for altogether 20 randomly selected test set images(two per class). Importantly, we observe a greatly reduced confounder importance in the explanations of theLSX configurations, though this is not fully removed (consistent with the accuracy results of Tab. 3). presents concept-level exemplary explanations (ezi) from the different NeSy configurations on class1 images of the CLEVR-Hans3 dataset. Over four randomly chosen class 1 training images we observe thatthe baseline NeSy model puts great importance on the confounding factor of class 1 (e.g., the gray colorof large cubes, highlighted in red in the figure) the LSX based models both ignore this factor and evenindicate the original groundtruth class rule (a large cube and large cylinder, highlighted in blue in the figure)despite never having received any explicit feedback on this. These qualitative results further indicate theconfounding mitigation results observed in Tab. 3.",
  "VLM-LSXVLMLMImage+ TextLanguageGenerationTextualExplanationExplanationRankingCE-loss onTextual Expl": "The last three modules (Explain, Reflect, Revise) can be repeated, e.g., until an iteration budgetT N is reached (cf. Alg. 1). Overall, the above typology presents the basic building blocks that an LSXinstantiation should contain. As mentioned above, the choices of some components influences the choices ofothers. We further illustrate this via example instantiations in the context of our experimental evaluations.",
  "Experimental Evaluations": "In the following, we investigate the effects of Learning by Self-Explaining across various metrics, datasetsand base models.To provide thorough examinations, we employ three different base models in LSX: aconvolutional neural network (CNN), a neuro-symbolic model (NeSy), and a vision-language model (VLM).Based on these, we investigate the potential benefits of LSX concerning test-set generalization, explanationconsolidation, explanation faithfulness and shortcut learning mitigation. We further provide extensive abla-tion evaluations, discussing the potential limits of LSX. Let us first specify our research questions followedby the model and experimental setup2. The investigated research questions are: (Q1) Can training via LSX lead to competitive predictors? (Q2) Can training via LSX lead to improvedgeneralization? (Q3) Can training via LSX help mitigate shortcut behavior? (Q4) Can training via LSXlead to more task-relevant explanations? (Q5) Can LSX lead to more faithful model explanations? (Q6)Can we observe predictive improvements via LSX in settings that go beyond one modality?",
  "Example LSX Instantiations": "We here provide a brief overview of the three investigated LSX instantiations. These represent examplesof how to integrate a base model into LSX. Tab. 1 provides an overview of the instantiations based on ourintroduced typology. We briefly summarize these in the following and refer to Sec. A.1, Sec. A.2 and Sec. A.3,respectively, for full details. CNN-LSX. For the CNN-based instantiation, both the learner and critic correspond to a CNN. Thisinstantiation is evaluated in the context of supervised image classification wherefore in Fit the learner isoptimized via a standard cross-entropy loss (CE-loss). The explanation method of the learner is an inputattribution approach. Thus, the attribution-based explanation indicates the importance of each input pixelfor a final class prediction. In the Reflect module the critic attempts to classify the learners explanations,i.e., predict the image class of an explanations underlying input sample. The quality of this explanationclassification is measured via a second cross-entropy loss for the critic. Finally, in the Revise module, thelearner is finetuned based on a training signal from the learner classifying the original input images and thecritics (cross-entropy) classification loss over the provided explanations.",
  "Experimental Setup": "Data. To provide evidence for the benefits of LSX, we examine each instantiation via several suited datasets.Particularly, we examine i) CNN-LSX on the MNIST (LeCun et al., 1989) and ChestMNIST (Yang et al.,2023; Wang et al., 2017) datasets, ii) NeSy-LSX on the concept-based datasets CLEVR-Hans3 (Stammeret al., 2021) and a variant of Caltech-UCSD Birds-200-2011 dataset (Wah et al., 2011), CUB-10, and iii)VLM-LSX on the VQA-X dataset (Park et al., 2018). Furthermore, for investigating the effect of confoundingfactors (Q3), we also use the decoy version of CLEVR-Hans3, as well as DecoyMNIST (Ross et al., 2017)and ColorMNIST (Kim et al., 2019; Rieger et al., 2020). These three datasets contain confounded trainingand validation sets and non-confounded test sets. We note that in all CLEVR-Hans3 evaluations that do nottarget shortcut learning, the confounded validation set was used as held-out test set. We refer to Suppl. Bfor details on all datasets. Metrics. We provide evaluations for LSX based on five metrics and briefly describe these here. (1) The firstmetric is the standard classification accuracy on a held-out test set. This is particularly used in the contextof (Q1-3). (2) For investigating the revised explanations via LSX (Q4), we provide the classification accuracyof a linear, ridge regression model. This linear model is optimized to classify a set of explanations (givencorresponding ground-truth class labels) and finally evaluated on a held-out set of explanations. The higherthis models accuracy, the more separable and distinct a learners explanations. (3) For (Q4), we furtherprovide a cluster analysis based metric over all explanations, similar to the Dunn index (Dunn, 1973; 1974).This metric, which we denote as Inter- vs. Intraclass Explanation Similarity (IIES), quantifies how similarexplanations are within one class, but dissimilar between classes (lower values indicate better separability).For investigating whether the learner in fact makes a decision based on the reported explanations (Q5),",
  "conf.deconf.NeSy85.964.2091.231.2NeSy-LSX90.904.3895.642.21": "we analyze the faithfulness (Hooker et al., 2019; Chan et al., 2022) of the learners explanations via twometrics as introduced by DeYoung et al. (2020), namely (4) sufficiency and (5) comprehensiveness. Bothmetrics measure the impact of removing specific parts of the input (based on the models explanations) onthe models predictive performance. Comprehensiveness measures the impact of important input features(as indicated by the models explanations) on the models performance. Sufficiency measures the impact ofunimportant features. Both metrics provide a relative measure, whereby high comprehensiveness and lowsufficiency score is better. We refer to Suppl. C for details of all described metrics. Setup.In all evaluations, we compare the performances of each base model (CNN, NeSy and VLM),i.e., when trained in its vanilla training setup, with the corresponding LSX-trained versions. When comparingthese different training configurations, we use the same setup, i.e., training steps, datasets, hyperparametersetc. We provide results as mean values with standard deviations over five runs with random seeds. ForVLM-LSX we revert to one seed due to the resource demand of large-scale VLMs. We evaluate (Q1-5) inthe context of CNN-LSX and NeSy-LSX and (Q6) in the context of VLM-LSX.",
  "Experimental Results": "Improved (few-shot) generalisation (Q1-2). We start by investigating LSX for improved generalization.To this end, we measure the held-out test set accuracy of CNN-LSX on the MNIST and ChestMNISTdatasets and of NeSy-LSX on the CLEVR-Hans3 and CUB-10 datasets. Further, to evaluate for few-shotgeneralization, we use different-sized subsets of the original training set. The rightmost column of Tab. 2shows test set accuracies when trained on the full training size of each dataset. We observe that on average(last row) there is a substantial improvement in accuracy particularly for ChestMNIST and CUB-10. Thus,our results indicate that integrating explanations in a self-refining approach via LSX can lead to competitive,even improved performance. We therefore answer (Q1) affirmatively. In the remaining columns of Tab. 2, we present the test-set accuracy in smaller-data regimes, i.e., when themodels were trained on different-sized subsets of the original training set. We observe large performancegains with LSX over all model configurations and datasets. Particularly, these improvements are greaterthan those observed on the full training set sizes. We provide important analyses and a discussion on thepotential limits of this effect in the ablation evaluations of Sec. 3.4. Altogether, these results suggest thatlearning via self-explaining leads to improved test-set generalization performances and we therefore answer(Q2) affirmatively.",
  ": Exemplary explanations on MNIST fromCNN baseline vs. CNN-LSX. Four random explana-tions are shown per image class (class ids on sides)": "Self-unconfounding (Q3). In the second set of evaluations, we are interested in how LSX-trained modelsperform in the context of shortcut behavior (Geirhos et al., 2020). We particularly focus on confoundedbehavior as a form of shortcut learning in which a learner picks up on spurious correlations within thetraining dataset that are not present in the test set (Schramowski et al., 2020). We investigate two settings.(i) In the first setting, denoted as deconf., Xc represents an explicitly deconfounded dataset that is with-heldfrom training the learner (i.e., Xc X = ). Thus, the spurious correlation present in X is not present in Xc(cf. Suppl. B for details). (ii) In the second setting, denoted as conf., we investigate the performance of LSXunder the influence of confounding factors. In this case Xc X where X and Xc both contain confoundeddata samples. We note that the baseline models are trained on { X, Xc}. We focus on the Decoy-MNISTdataset for CNN-LSX and the original, confounded CLEVR-Hans3 version for NeSy-LSX. In Tab. 3, we present the held-out test set accuracies of all configurations. We observe a strong improvementin test set performances when training via LSX on the deconfounded critic sets (deconf.). This suggeststhat reflecting on the explanations of an explicitly deconfounded critic set can lead to reduced confoundingbehavior over just adding these to the normal training set (as done in the baseline setup).Even moreinteresting, we observe that the LSX trained models in the conf. setting show greatly reduced confoundingbehavior in comparison to the baselines, despite both models having only ever seen confounded and neverdeconfounded data. We refer to Sec. 3.4 for ablations and a more detailed discussion on the observed effects.We additionally provide example explanations from both instantiations in Sec. C.3 which further supportthe findings of Tab. 3. Overall, our results show a remarkable beneficial effect of Learning by Self-Explainingin mitigating the issue of shortcut learning and we answer (Q3) positively. Consolidation of explanations (Q4). In this evaluation, we wish to analyze how the critics feedbacksignal influences the learners representations, specifically its explanations. Based on the intuition behind theReflect module concerning good explanations, we hypothesize that the explanations of an LSX-trainedmodel highlight distinct information which is more relevant for the underlying task, e.g., wing color fordistinguishing between bird species. We refer to such an effect as explanation consolidation. We base theseevaluations on the Inter- vs. Intraclass Explanation Similarity4 (IIES) and the accuracy of a ridge regressionmodel that was trained to classify a set of explanations and evaluated on a second, held-out set. Both ofthese metrics measure the class-based separability of a models explanations. Tab. 4 shows that trainingvia LSX leads to much more separable and distinct explanations for both metrics. This effect appears less",
  "VQAv2 VQA-Acc. (%)80.6685.0585.73": "pronounced for the NeSy datasets, which is likely due to the sparseness and low dimensionality of theirconcept-level data and therefore also of the explanations. In , we also provide qualitative results of theexplanation consolidation for MNIST, where explanations from four randomly sampled input samples arepresented for each digit class. These visualizations support the quantitative results of Tab. 4 and particularlyillustrate the distinctness of the explanations from an LSX-trained model. Overall, our results suggest thattraining via LSX results in more consistent explanations within a class and yet more distinct explanationsacross classes. Conclusively, we answer (Q4) positively. Explanation faithfulness (Q5).In the next evaluations we investigate whether LSX-trained modelsproduce explanations which are more faithful (Hooker et al., 2019; DeYoung et al., 2020; Schramowskiet al., 2020).In other words, do the learners in fact use their explanations in their decisions.This isa relevant question as models that produce unfaithful explanations are detrimental e.g., to building trustbetween human users and machines and at the same time make potential revisions via these explanationsdifficult (Schramowski et al., 2020; Friedrich et al., 2022; Teso et al., 2023). We investigate the faithfulness of LSX-learned explanations with well-established metrics: sufficiency andcomprehensiveness (DeYoung et al. (2020); cf. Suppl. C for details). In Tab. 5, we present the results ofthese metrics over the CNN and NeSy-based instantiations and the MNIST, ChestMNIST, CLEVR-Hans3and CUB-10 datasets. One can observe a strong improvement via LSX in both metrics across all models anddatasets. Specifically, the results show improved comprehensiveness for LSX-trained models. This indicatesa higher dependency on important features of the explanation. At the same time, the LSX-trained modelsshow improved sufficiency.Thus, unimportant information based on the explanations has a low impacton the learners decisions. Overall, these results suggest that training via LSX also leads to more faithfulexplanations and we answer (Q5) affirmatively. Going beyond one modality (Q6). Lastly, we provide results where we move beyond one input modality,but also beyond the task of supervised image classification. For this reason, we train a vision-languagemodel (VLM) via LSX for the challenging task of visual question answering (VQA). Significantly, in thisinstantiation the model self-refines its vision-language representations only via language explanations. InTab. 6, we provide test-set accuracies on the VQA dataset (Park et al., 2018; Lin et al., 2014) for threedifferent training configurations. VLM0 represents the off-the-shelf, pre-trained VLM (i.e., zero-shot) andVLM (ft.) its VQA finetuned version, i.e., finetuned only via Fit. Lastly, VLM-LSX is finetuned on theVQA task and via explanatory feedback from its language-model based critic.We observe that VLM-LSX leads to VQA performance improvements over the standard finetuned model. This depicts a 15%larger improvement over the zero-shot performance (cf. VLM0) compared to the finetuned model. Thesepreliminary results indicate that LSX potentially represents a beneficial learning approach outside of the",
  "Ablations": "Our main results indicate promising benefits via LSX, however, wrong choices of individual components can,in general, lead to poor overall behavior. In the following, we therefore wish to investigate and discuss thelimits and potential failure cases of LSX. We investigate three distinct cases that influence the performanceof models trained via LSX. Notably, these investigations are not all-inclusive, but provide an importantassessment for future endeavors. The first case focuses on the issue of too small data. The second casefocuses on the issue of the critics capacity and its influence on the learners performance.Lastly, weinvestigate what happens when the explanation method is inadequate in describing discriminative featuresof the data in the context of confounding factors. Too small data. With the first set of evaluations we wish to investigate the learners performance in thecase of more extreme data set sizes, particularly very small data sets. For this we provide further experimentswith the example CNN-LSX and NeSy-LSX instantiations. We provide CNN-LSX with only 300 MNISTsamples and NeSy-LSX with only 30 samples of CUB-10.The results are presented in Tab. 7 (test setprediction accuracies in %) and indeed suggest that the smaller the data set the less pronounced the effectbecomes that we had originally observed in Tab. 2. Although we still observe improvements in the smalldata set regime with CNN-LSX, the results on CUB-10 in Tab. 7 even appear to suggest that given a verysmall data set LSX can have a negative impact on the learners predictive performance. This difference islikely due to explanation selection in NeSy-LSX, i.e., choosing the maximally probable explanation. If a badexplanation is chosen by the critic and enforced on the learner the learners predictive performance will likelybe negatively influenced by this (cf. Friedrich et al. (2023a)). Though it is difficult to adequately compare thetwo data set sizes (300 MNIST vs 30 CUB-10 samples) given the individual nature and complexity of them(grayscale images vs. concept-tagged images). Interestingly, the point on potentially bad feedback from thecritic also hints at a form of overfitting that is quite specific to LSX (and other forms of explanation-guidedlearning): overfitted explanatory feedback can lead to poor predictive performances. Low capacity models. In the second set of evaluations, we investigate the potential issues that arise froma critic submodel that performs poorly due to too low capacity, ultimately leading to random feedback. Weevaluate CNN-LSX and NeSy-LSX where we have replaced their original critic submodels with simulatedcritic models that only perform random guessing on the base task. We denote this setting as rand. c. InTab. 8, we present the test set prediction accuracies of these training configurations on 3000 MNIST trainingsamples for CNN-LSX and 150 CUB-10 samples for NeSy-LSX. Generally, we observe that indeed given atoo-low capacity of the critic the learner falls back to its base predictive performance for both instantiations.These results seem to suggest a lower bound for LSX due to random explanatory feedback. Interplay between explanation method and confounders.The inductive bias of the explanationmethod in combination with the nature of the confounding factor likely plays an important role in ourobserved effects on confounding mitigation in Tab. 3. Hence, we here investigate the influence of an insuf-",
  "Discussion & Limitations": "Overall, our evaluations provide evidence for the benefits of training via LSX on a variety of important tasksand metrics that go beyond standard evaluations of ML research. In the following, we wish to give a generalperspective on LSX and provide a discussion of our findings and potential limitations. Configuration choices.LSX can be instantiated in various ways, each with its own specific moduleand configuration choices. The example instantiations introduced in this work, however, already portraysome interesting characteristics and differences which we wish to highlight here. For example, an importantdifference between CNN-LSX, NeSy-LSX and VLM-LSX lies within their Reflect modules, specificallyhow the feedback of the learners explanations is computed.In CNN-LSX, the feedback representsa differentiable signal, whereas in NeSy-LSX and VLM-LSX this represents a (probabilistic) ranking ofa set of explanations.This second form of critiquing allows the model to weight the explanations andidentify the most useful explanation. The first form of critiquing, on the other hand, allows the modelto perform loss-based explanation fine-tuning. Related to this, but concerning Explain, where CNN-LSXutilizes continuous input-level explanations, the logical explanations in NeSy-LSX and the natural languageexplanations in VLM-LSX are discrete. As an effect of this, the form of revision in the Revise modulediffers. In CNN-LSX one can simply pass the feedback from the critic to the learner via a backpropagatedclassification signal (cf. Tab. 1). In NeSy-LSX and VLM-LSX, however, we enforce the identified, most-likely explanation. Lastly, but also importantly, where in CNN-LSX and NeSy-LSX the critic evaluates thelearners explanations based only on the task information, the critic in VLM-LSX represents a pre-trainedmodel that also utilizes its previously acquired knowledge (Zheng et al., 2023; Zhu et al., 2023; Li et al.,2024; Koo et al., 2024). LSX as explanation-based regularization. Let us now take a different yet intuitive perspective on theeffect of LSX. As previously stated, LSX combines ideas from self-refining AI and explanatory interactivelearning (XIL). For XIL, several works (Ross et al., 2017; Selvaraju et al., 2019) have provided evidence that(human) explanations act as a regularizer during model refinement. We argue that this explanation-based",
  "Related Works": "Explanations and Leveraging Explanations in ML. Receiving explanations to an ML models decisionhas become a heavily advocated and investigated topic in recent years, culminating in the field of explainableAI (XAI) (cf. (Guidotti et al., 2019; Ras et al., 2022; Roy et al., 2022; Saeed & Omlin, 2023)) and interpretableAI (Ruker et al., 2023; Li et al., 2018; Rudin et al., 2022; Rudin, 2019) with the later focusing on explicitly,inherent model explanations.Closely related to these fields are self-explaining models (Alvarez-Melis &Jaakkola, 2018; Lee et al., 2022; Roy et al., 2022; Camburu et al., 2018; Bastings et al., 2019; Majumderet al., 2022). Explanations from any of these approaches are used to evaluate the reasons for a modelsdecision. From backpropagation-based (Sundararajan et al., 2017; Ancona et al., 2018), to model distillation(Ribeiro et al., 2016) or prototype-based (Li et al., 2018) methods, the explanations often highlight importantinput elements (e.g., pixels) for a models prediction. However, several studies have also investigated multi-modal explanations (Rajani et al., 2020), logic-based explanations (Aditya et al., 2018; Rabold et al., 2019),and concept-based explanations (Stammer et al., 2021; Zhou et al., 2018; Ghorbani et al., 2019; Poeta et al.,2023). In all of these approaches explanations are only provided in a one-way communication as a means ofmodel inspection.",
  "Conclusion": "In this work, we have introduced a novel learning approach, Learning by Self-Explaining (LSX), with whichwe argue for a novel perspective on the role of self-explaining in the context of explainability and learning.With this approach, we claim that explanations are important not just for human users to understand orto revise an AI model. Rather, they can also play an important role in a form of self-refinement wherebya model assesses its own learned knowledge via its explanations. Our experimental evaluations highlight",
  "David Alvarez-Melis and Tommi S. Jaakkola. Towards robust interpretability with self-explaining neuralnetworks. In Advances on Neural Information Processing Systems (NeurIPS), pp. 77867795, 2018": "Marco Ancona, Enea Ceolini, Cengiz ztireli, and Markus Gross. Towards better understanding of gradient-based attribution methods for deep neural networks. In International Conference on Learning Represen-tations (ICLR). OpenReview.net, 2018. Alessa Angerschmid, Jianlong Zhou, Kevin Theuermann, Fang Chen, and Andreas Holzinger. Fairness andexplanation in ai-informed decision making. Machine Learning and Knowledge Extraction, 4(2):556579,2022. Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C. Lawrence Zitnick, andDevi Parikh. VQA: visual question answering. In International Conference on Computer Vision (ICCV),pp. 24252433. IEEE Computer Society, 2015. Jasmijn Bastings, Wilker Aziz, and Ivan Titov. Interpretable neural predictions with differentiable binaryvariables. In Conference of the Association for Computational Linguistics (ACL), pp. 29632977. Associ-ation for Computational Linguistics, 2019. Sander Beckers. Causal explanations and XAI. In Bernhard Schlkopf, Caroline Uhler, and Kun Zhang(eds.), Conference on Causal Learning and Reasoning (CLeaR), volume 177 of Proceedings of MachineLearning Research, pp. 90109. PMLR, 2022.",
  "Kiran Bisra, Qing Liu, John C Nesbit, Farimah Salimi, and Philip H Winne. Inducing self-explanation: Ameta-analysis. Educational Psychology Review, 30:703725, 2018": "Grady Booch, Francesco Fabiano, Lior Horesh, Kiran Kate, Jonathan Lenchner, Nick Linck, Andrea Loreg-gia, Keerthiram Murugesan, Nicholas Mattei, Francesca Rossi, and Biplav Srivastava. Thinking fast andslow in AI. In Conference on Artificial Intelligence (AAAI), pp. 1504215046. AAAI Press, 2021. Manuel Brack, Patrick Schramowski, Bjrn Deiseroth, and Kristian Kersting. ILLUME: rationalizing vision-language models through human interactions. In International Conference on Machine Learning (ICML),volume 202 of Proceedings of Machine Learning Research, pp. 30213037. PMLR, 2023. Oana-Maria Camburu, Tim Rocktschel, Thomas Lukasiewicz, and Phil Blunsom. e-snli: Natural languageinference with natural language explanations.In Advances in Neural Information Processing Systems(NeurIPS), pp. 95609572, 2018.",
  "Michelene TH Chi, Nicholas De Leeuw, Mei-Hung Chiu, and Christian LaVancher. Eliciting self-explanationsimproves understanding. Cognitive science, 18(3):439477, 1994": "Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang,Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun,Xinyun Chen, Aakanksha Chowdhery, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Y. Zhao,Yanping Huang, Andrew M. Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, AdamRoberts, Denny Zhou, Quoc V. Le, and Jason Wei. Scaling instruction-finetuned language models. CoRR,abs/2210.11416, 2022. Quentin Delfosse, Sebastian Sztwiertnia, Wolfgang Stammer, Mark Rothermel, and Kristian Kersting. In-terpretable concept bottlenecks to align reinforcement learning agents. CoRR, abs/2401.05821, 2024. Jay DeYoung, Sarthak Jain, Nazneen Fatema Rajani, Eric Lehman, Caiming Xiong, Richard Socher, andByron C. Wallace. ERASER: A benchmark to evaluate rationalized NLP models. In Conference of the As-sociation for Computational Linguistics (ACL), pp. 44434458. Association for Computational Linguistics,2020.",
  "Joseph C Dunn. Well-separated clusters and optimal fuzzy partitions. Journal of cybernetics, 4(1):95104,1974": "Constantin Eichenberg, Sidney Black, Samuel Weinbach, Letitia Parcalabescu, and Anette Frank. MAGMA- multimodal augmentation of generative models through adapter-based finetuning. In Findings of theAssociation for Computational Linguistics: (EMNLP), pp. 24162428. Association for ComputationalLinguistics, 2022. Shmuel Ellis, Bernd Carette, Frederik Anseel, and Filip Lievens. Systematic reflection: Implications forlearning from failures and successes. Current Directions in Psychological Science, 23(1):6772, 2014.",
  "Anirudh Goyal and Yoshua Bengio. Inductive biases for deep learning of higher-level cognition. Proceedingsof the Royal Society A, 2022": "Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the V in VQA mat-ter: Elevating the role of image understanding in visual question answering. In Conference on ComputerVision and Pattern Recognition (CVPR), pp. 63256334, 2017. Riccardo Guidotti, Anna Monreale, Salvatore Ruggieri, Franco Turini, Fosca Giannotti, and Dino Pedreschi.A survey of methods for explaining black box models. ACM Computing Surveys, 51(5):93:193:42, 2019.",
  "Yotam Hechtlinger. Interpretation of prediction models using the input gradient. CoRR, abs/1611.07634,2016": "Tom Heskes, Evi Sijben, Ioan Gabriel Bucur, and Tom Claassen. Causal shapley values: Exploiting causalknowledge to explain individual predictions of complex models. In Advances in Neural Information Pro-cessing Systems (NeurIPS), 2020. Andreas Holzinger. The next frontier: AI we can really trust. In Machine Learning and Principles andPractice of Knowledge Discovery in Databases - International Workshops of ECML PKDD, volume 1524of Communications in Computer and Information Science, pp. 427440. Springer, 2021. Sara Hooker, Dumitru Erhan, Pieter-Jan Kindermans, and Been Kim. A benchmark for interpretabilitymethods in deep neural networks. In Conference on Neural Information Processing Systems (NeurIPS),pp. 97349745, 2019. Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe, Andrea Ges-mundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for NLP. In InternationalConference on Machine Learning (ICML), volume 97 of Proceedings of Machine Learning Research, pp.27902799. PMLR, 2019.",
  "Henry A. Kautz. The third AI summer: AAAI Robert S. Engelmore Memorial Lecture. AI Magagazine, 43(1):93104, 2022": "Isaac Kauvar, Chris Doyle, Linqi Zhou, and Nick Haber. Curious replay for model-based adaptation. InInternational Conference on Machine Learning (ICML, volume 202 of Proceedings of Machine LearningResearch, pp. 1601816048. PMLR, 2023. Byungju Kim, Hyunwoo Kim, Kyungsu Kim, Sungjin Kim, and Junmo Kim. Learning not to learn: Trainingdeep neural networks with biased data.In Conference on Computer Vision and Pattern Recognition(CVPR), pp. 90129020, 2019. Pang Wei Koh, Thao Nguyen, Yew Siang Tang, Stephen Mussmann, Emma Pierson, Been Kim, and PercyLiang. Concept bottleneck models. In International Conference on Machine Learning (ICML), volume119 of Proceedings of Machine Learning Research, pp. 53385348. PMLR, 2020. Ryan Koo, Minhwa Lee, Vipul Raheja, Jong Inn Park, Zae Myung Kim, and Dongyeop Kang. Benchmarkingcognitive biases in large language models as evaluators. In Findings of the Association for ComputationalLinguistics (ACL), pp. 517545. Association for Computational Linguistics, 2024. Satyapriya Krishna, Jiaqi Ma, Dylan Slack, Asma Ghandeharioun, Sameer Singh, and Himabindu Lakkaraju.Post hoc explanations of language models can improve language models. In Advances in Neural InformationProcessing Systems (NeurIPS), 2023.",
  "Kyungbin Kwon and David H Jonassen. The influence of reflective self-explanations on problem-solvingperformance. Journal of Educational Computing Research, 44(3):247263, 2011": "Andrew K. Lampinen, Ishita Dasgupta, Stephanie C. Y. Chan, Kory W. Mathewson, Michael Henry Tessler,Antonia Creswell, James L. McClelland, Jane Wang, and Felix Hill. Can language models learn fromexplanations in context?In Findings of the Association for Computational Linguistics (EMNLP), pp.537563. Association for Computational Linguistics, 2022a. Andrew K. Lampinen, Nicholas A. Roy, Ishita Dasgupta, Stephanie C. Y. Chan, Allison C. Tam, James L.McClelland, Chen Yan, Adam Santoro, Neil C. Rabinowitz, Jane X. Wang, and Felix Hill. Tell me why!explanations support learning relational and causal structure. In International Conference on MachineLearning (ICML), volume 162 of Proceedings of Machine Learning Research, pp. 1186811890. PMLR,2022b.",
  "Douglas P Larsen, Andrew C Butler, and Henry L Roediger III.Comparative effects of test-enhancedlearning and self-explanation on long-term retention. Medical education, 47(7):674682, 2013": "Yann LeCun, Bernhard Boser, John Denker, Donnie Henderson, Richard Howard, Wayne Hubbard, andLawrence Jackel. Handwritten digit recognition with a back-propagation network. Advances on NeuralInformation Processing Systems (NeurIPS), 2, 1989. Seungeon Lee, Xiting Wang, Sungwon Han, Xiaoyuan Yi, Xing Xie, and Meeyoung Cha. Self-explainingdeep models with logic rule reasoning. In Advances on Neural Information Processing Systems (NeurIPS),2022. Tao Lei, Regina Barzilay, and Tommi S. Jaakkola. Rationalizing neural predictions. In Conference on Empir-ical Methods in Natural Language Processing (EMNLP), pp. 107117. The Association for ComputationalLinguistics, 2016.",
  "Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In Text summarization branchesout, pp. 7481, 2004": "Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollr,and C. Lawrence Zitnick.Microsoft COCO: common objects in context.In European Conference onComputer Vision (ECCV), volume 8693 of Lecture Notes in Computer Science, pp. 740755. Springer,2014. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, NouhaDziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, KatherineHermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark.Self-refine: Iterative refinement withself-feedback. In Advances in Neural Information Processing Systems (NeurIPS), 2023. Bodhisattwa Prasad Majumder, Oana Camburu, Thomas Lukasiewicz, and Julian J. McAuley. Knowledge-grounded self-rationalization via extractive and natural language explanations. In International Conferenceon Machine Learning (ICML), volume 162, pp. 1478614801. PMLR, 2022.",
  "Varun Nair, Elliot Schumacher, Geoffrey J. Tso, and Anitha Kannan. DERA: enhancing large languagemodel completions with dialog-enabled resolving agents. CoRR, abs/2303.17071, 2023": "Antonio Norelli, Giorgio Mariani, Luca Moschella, Andrea Santilli, Giambattista Parascandolo, SimoneMelzi, and Emanuele Rodol.Explanatory learning: Beyond empiricism in neural networks.CoRR,abs/2201.10222, 2022. Liangming Pan, Michael Saxon, Wenda Xu, Deepak Nathani, Xinyi Wang, and William Yang Wang. Auto-matically correcting large language models: Surveying the landscape of diverse self-correction strategies.CoRR, abs/2308.03188, 2023. Dong Huk Park, Lisa Anne Hendricks, Zeynep Akata, Anna Rohrbach, Bernt Schiele, Trevor Darrell, andMarcus Rohrbach. Multimodal explanations: Justifying decisions and pointing to the evidence. In Con-ference on Computer Vision and Pattern Recognition (CVPR), pp. 87798788, 2018. Debjit Paul, Mete Ismayilzada, Maxime Peyrard, Beatriz Borges, Antoine Bosselut, Robert West, and BoiFaltings.REFINER: reasoning feedback on intermediate representations.In Conference of the Euro-pean Chapter of the Association for Computational Linguistics (EACL), pp. 11001126. Association forComputational Linguistics, 2024.",
  "Eleonora Poeta, Gabriele Ciravegna, Eliana Pastor, Tania Cerquitelli, and Elena Baralis. Concept-basedexplainable artificial intelligence: A survey. CoRR, abs/2312.12936, 2023": "Danish Pruthi, Rachit Bansal, Bhuwan Dhingra, Livio Baldini Soares, Michael Collins, Zachary C. Lipton,Graham Neubig, and William W. Cohen. Evaluating explanations: How much do explanations from theteacher aid students? Transactions of the Association for Computational Linguistics, 10:359375, 2022. Johannes Rabold, Hannah Deininger, Michael Siebers, and Ute Schmid. Enriching visual with verbal expla-nations for relational conceptscombining lime with aleph. In European Conference on Machine Learningand Knowledge Discovery in Databases (ECML-PKDD), pp. 180192. Springer, 2019. Nazneen Fatema Rajani, Rui Zhang, Yi Chern Tan, Stephan Zheng, Jeremy Weiss, Aadit Vyas, AbhijitGupta, Caiming Xiong, Richard Socher, and Dragomir R. Radev. ESPRIT: explaining solutions to physicalreasoning tasks. In Annual Meeting of the Association for Computational Linguistics (ACL), pp. 79067917. Association for Computational Linguistics, 2020.",
  "Gabrielle Ras, Ning Xie, Marcel van Gerven, and Derek Doran. Explainable deep learning: A field guide forthe uninitiated. Journal of Artificial Intelligence Research, 73:329396, 2022": "Tilman Ruker, Anson Ho, Stephen Casper, and Dylan Hadfield-Menell. Toward transparent ai: A survey oninterpreting the inner structures of deep neural networks. In IEEE Conference on Secure and TrustworthyMachine Learning (SaTML), pp. 464483. IEEE, 2023. Marco Tlio Ribeiro, Sameer Singh, and Carlos Guestrin.\"why should I trust you?\": Explaining thepredictions of any classifier. In International Conference on Knowledge Discovery and Data Mining (ACM-SIGKDD), pp. 11351144. ACM, 2016. Laura Rieger, Chandan Singh, W. James Murdoch, and Bin Yu.Interpretations are useful: Penalizingexplanations to align neural networks with prior knowledge.In International Conference on MachineLearning (ICML), volume 119 of Proceedings of Machine Learning Research, pp. 81168126. PMLR, 2020.",
  "Johannes Schneider and Michalis Vlachos. Reflective-net: Learning from explanations. Data Mining andKnowledge Discovery, pp. 122, 2023": "Patrick Schramowski, Wolfgang Stammer, Stefano Teso, Anna Brugger, Franziska Herbert, Xiaoting Shao,Hans-Georg Luigs, Anne-Katrin Mahlein, and Kristian Kersting. Making deep neural networks right forthe right scientific reasons by interacting with their explanations.Nature Machine Intelligence, 2(8):476486, 2020. Ramprasaath Ramasamy Selvaraju, Stefan Lee, Yilin Shen, Hongxia Jin, Shalini Ghosh, Larry P. Heck,Dhruv Batra, and Devi Parikh. Taking a HINT: leveraging explanations to make vision and languagemodels more grounded. In International Conference on Computer Vision (ICCV), pp. 25912600. IEEE,2019.",
  "Mukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic attribution for deep networks. In InternationalConference on Machine Learning (ICML), volume 70, pp. 33193328. PMLR, 2017": "Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. Rethinking theinception architecture for computer vision. In Conference on Computer Vision and Pattern Recognition(CVPR), pp. 28182826, 2016. Niket Tandon, Aman Madaan, Peter Clark, and Yiming Yang. Learning to repair: Repairing model out-put errors after deployment using a dynamic memory of feedback. In Findings of the Association forComputational Linguistics (NAACL), pp. 339352. Association for Computational Linguistics, 2022.",
  "Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge Belongie. The caltech-ucsd birds-200-2011 dataset. 2011": "Lin Wang and Kuk-Jin Yoon. Knowledge distillation and student-teacher learning for visual intelligence:A review and new outlooks. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(6):30483068, 2022. Xiaosong Wang, Yifan Peng, Le Lu, Zhiyong Lu, Mohammadhadi Bagheri, and Ronald M. Summers. Chestx-ray8: Hospital-scale chest x-ray database and benchmarks on weakly-supervised classification and local-ization of common thorax diseases. In Conference on Computer Vision and Pattern Recognition (CVPR),pp. 34623471, 2017. Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and HannanehHajishirzi. Self-instruct: Aligning language models with self-generated instructions. In Annual Meetingof the Association for Computational Linguistics (ACL), pp. 1348413508. Association for ComputationalLinguistics, 2023. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le,and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. In Advances inNeural Information Processing Systems (NeurIPS), 2022.",
  "Explain": "Explanations in VLM-LSX represent explicitly generated textual sequences. Specifically, we let the learnergenerate a set of Ne N explanations per sample, denoted as Ei for sample triplet (xi, qi, ai) Xc. Herewe follow the approach of Brack et al. (2023) for the explanation sampling process. Briefly this is basedon a combination of top-k and temperature-based sampling and explanation prompt engineering, where anexplanation prompt is the sequence of tokens appended to the image, question, and answer to elicit textualexplanations. We select temperatures T {0.01, 0.1, 0.3, 0.6, 0.9}. This process overall results in Ne = 25different natural language explanations per data sample.",
  "xi.(1)": "This form of explanation method is considered a post-hoc explanation method as it estimates the explanationof a model after its decision processing (in comparison to inherent model explanations). Furthermore, itrepresents an input attribution method, i.e., for each input element (i.e., pixel) it provides an estimate ofits importance for the models final decision. Lastly, as Eq. 1 shows, InputXGradient contains the originalinput multiplied by the attribution information.Therefore, when the critic in CNN-LSX assesses theseexplanations the critic does not need additional information on the original input data, but receives this viathe InputXGradient explanation.",
  "Reflect": "Within the Reflect model of VLM-LSX (and similar to NeSy-LSX) the critic provides preference scores,i Ne, over the generated explanations. This preference scoring is based on calculating the sample-wiseROUGE-L score (Lin, 2004) between the learners generated explanations and the annotated explanationsthat are stored in the critics knowledge base. We further follow Brack et al. (2023) and consider everycandidate with ROUGE-L 0.7 as indicating a good explanation with respect to the ground-truth values.",
  "Revise": "Based on the feedback of the previous Reflect module we next select the explanations from Ei withrespect to the threshold defined above. We denote Emax as the set of these explanations over all sampleswithin Xc. We next add an additional loss to the learners base loss: L = lB +lexpl(f((Ic, Qc, Ac)), Emax) andoptimize the learner via adapter-based finetuning (as in the Fit module) for predicting the corrext questionanswer and the best corresponding explanation.",
  "Implementation details": "In our training setup we sample a total of Ne = 25 explanations, i.e., 5 per temperature value. Furthermore,Xc = X. The number of LSX iterations was set to T = 8, where VLM (ft.) (cf. Tab. 6) was trained for thesame number of overall steps. In the setting of VLM-LSX Xc = X.",
  "Computational details": "This particular instantiation incorporates finetuning the critic on the learners explanations, which representsa computational bottleneck. Specifically, processing one batch over the Explain, Reflect and Revise loopin our implementation takes 0.71 seconds vs. 0.003 seconds for the vanilla setup only via Fit. Theseresults were averaged over ten batches. Notably, the vanilla setup is based on highly optimized pytorchcode and refactoring of our implementations can lead to improvements.However, the major limitationremains, i.e., finetuning the critic within the learners finetuning step. It is thus important for future workto investigate more efficient instantiations, e.g., based on retrieval-based feedback.",
  "feedback": "class1(X):-in(O1,X),color(O1,green).class1(X):-in(O1,X),color(O1,red).class1(X):-in(O1,X),shape(O1,cube).class1(X):-in(O1,X),in(O2,X), color(O1,green),color(O2,red).class1(X):-in(O1,X),in(O2,X), shape(O1,cube),color(O2,red).class1(X):-in(O1,X), shape(O1,cube),color(O1,green).class1(X):-in(O1,X),in(O2,X), color(O1,green),shape(O1,cube), color(O2,red). : NeSy-LSX: Learning by Self-Explaining instantiation for supervised image classification via neuro-symbolicconcept learner. The learner proposes a set of candidate class-specific logical explanations. The critic represents aneuro-symbolic forward reasoner, which computes the validity of these logical statements given visual input. Thefeedback represents a probabilistic ranking of the set of logical explanations with which we identify the most likelyexplanation per image class and revise the learner to only use this explanation for samples of that class.",
  "Learner": "The learner submodel of the VLM-LSX instantiation corresponds to the pretrained vision-language modelMAGMA (Eichenberg et al., 2022).This VLM is built from a GPT-based LM backbone by adding aCLIP-based image prefix to encode images into the LMs embedding space. Subsequently, MAGMA trainsdedicated bottleneck adapters on vision-language tasks to adjust the model to the new domains. Overall, inVLMs image and text can be input to the model as a joint sequence of tokens.",
  "Critic": "We simulate a Language Model (LM) critic that provides preference scores for a set of textual explanationsproposed by the learner. Specifically, we follow the methodology of Brack et al. (2023) and utilize a set ofhuman-generated ground-truth explanations from VQA-X dataset (Park et al., 2018) as a form of knowledgebase for the critic (further details in Reflect below).",
  "Fit": "Similar to the CNN-LSX instantiation the task of supervised image classification in the Fit module in NeSy-LSX is performed by optimizing via a cross-entropy loss lB = lCE(f(xi), yi) with (xi, yi) X. As previouslymentioned, in our evaluations we hereby freeze the parameters of the encoder module of f, thus optimizingonly the parameters of the predictor module of the learner.",
  "zi|zi=zi+(ziz)d": "Hereby, zi represents a baseline value, which in our evaluations corresponds to a zero vector.Next,the resulting attribution map on the latent concept representations, ezi OA, is binarized via ahyperparameter to ezi {0, 1}OA. We next transform these binarized attribution maps into logicalstatements which represent the final explanation form in NeSy-LSX (cf. ). These logical statementsconsist of all subsets of conjunctive combinations of the important attributes and objects that are present inezi. We denote the set of these candidate logical explanations generated from sample xi as Ei. For example,let us assume that for a specific CLEVR-Hans3 sample xi of class 1 we identify two objects to be importantfor the final class prediction. Hereby, the attributes green color and cubical shape are important for the firstobject and red color for the second object. Following the notation of Shindo et al. (2023) the set of generatedcandidate are:",
  "We refer to this step of constructing all potential candidate rules as propositionalizing (i.e., changing therepresentation of relational data)": "Notably, each input sample thereby produces a set of such candidate rules which may potentially containmany duplicates over samples of the same underlying class. Finally, by iterating over all samples in Xc,grouping the resulting candidate rules by image class and removing duplicates we receive a set of candidaterules per class as E = { E1, ..., EK}, where Ek denotes the set of generated candidate logical explanationsgathered over all samples of class k and with duplicates removed.",
  "q=1(eziq eyimaxq)2": "In comparison to CNN-LSX in our evaluations we set T = 1 for NeSy-LSX. This means the critic onlyscores the proposed underlying logical explanations once and passes this back as feedback. Although it isin principle possible to perform multiple steps of this, e.g., by first removing explanations which are mostunprobable from the learners representations and only after several of such iterations choose the most likelyexplanation, we leave this for future investigations.",
  "A: giraffe": ": VLM-LSX: Learning by Self-Explaining instantiation for visual question answering via a vision-languagemodel. The learner proposes a set of candidate explanations via explanation prompting. The critic represents a pre-trained language model, which provides preference scores over the generated explanations. The feedback representsa binary ranking of the set of explanations with which we identify good explanations and revise the learner forpredicting these explanations.",
  "CLEVR-Hans3 we present the critic with 50 samples and the learner 7500 separate training samples for w/conf. and 150 test set samples for w/ deconf": "The setting of NeSy-LSX on CUB-10 for the results in Tab. 2 represented a small variation from the previoussettings in that X Xc, due to the small number of samples per class in CUB-10 in combination with theneuro-symbolic forward reasoner critic (requires sufficient amount of positive and negative class samples).In this case the datasets official validation set was concatenated with the (full) training set as a simpleworkaround. In this way for the results in Tab. 2 for CUB-10 from left column to right column the ratiobetween Xc and X was 29 to 24, 149 to 124 and 300 to 249, respectively. Note, the vanilla-trained modelswere also trained on the concatenated set.",
  "BData": "MNIST. The MNIST dataset (LeCun et al., 1989) contains images of handwritten digits between 0-9. Weutilize this dataset in the context of image classification, i.e., an AI model should predict the underlyingdigit from an image. The number of training images in MNIST corresponds to 60k. ChestMNIST. ChestMNIST (Yang et al., 2023; Wang et al., 2017) originates from the NIH-ChestXray14dataset (Wang et al., 2017), which consists of 112,120 frontal-view X-ray images from 30,805 individualpatients. Each image is originally associated with 14 disease labels extracted from text, making it a taskof multi-label binary classification. In our setting we convert the data to a binary classification problem,i.e., an image depicts an x-ray scan of a normal patient or abnormal. The number of training images inChestMNIST corresponds to 78k. DecoyMNIST DecoyMNIST (Ross et al., 2017) represents a version of MNIST (LeCun et al., 1989) inwhich small boxes are placed randomly in one of the four corners for each sample (cf. ). Importantly,the dataset contains confounders. Within the training set the gray boxes possess a specific grayscale value",
  ": Example training images vs test images from ColorMNIST. Figure from Stammer et al. (2021)": "ColorMNIST The ColorMNIST dataset (Kim et al., 2019; Rieger et al., 2020; Stammer et al., 2021)represents another confounded variation of the MNIST dataset.Hereby, the confounding feature is notspatially separate from the relevant features of the data. Specifically, in the training set each digit classis strongly correlated with a certain color. At test time however each digit is randomly assigned a color.(cf. ). The number of training images in ColorMNIST corresponds to 60k. CUB-10. CUB-10 represents a subset of the original Caltech-UCSD Birds-200-2011 dataset (CUB-200-2011) (Wah et al., 2011) that comprises images of the first 10 classes of the full dataset. Koh et al. (2020)originally perform a preprocessing step for CUB-200-2011 where concept vectors are replaced with the maxvoting vector over all samples of a class. In other words, the resulting concept activations are identicalacross all samples of a class which leads to a one-to-one mapping between concept activations and the classaffiliation. In CUB-10 we simulate a more realistic setting in which the class concept activations of (Koh et al., 2020) areoverlaid with additional random noise, thereby maintaining the underlying class-based concept activation,but producing random variations per class sample. Specifically, we add uniformly distributed noise between0 and 1 onto the class-based concept activations and binarize the resulting activations with a threshold of0.75. The number of training images in CUB-10 corresponds to 300 images. CLEVR-Hans3 presents the data distribution in CLEVR-Hans3 (Stammer et al., 2021). Specifically,CLEVR-Hans3 is based on the graphical environment of the original CLEVR dataset (Johnson et al., 2017),however reframed for image classification rather than visual question answering. Each class is representedby an underlying logical rule consisting of the presence of specific object combinations. Within the originaltraining and validation set specific combinations of object properties are highly correlated, where they arenot within the test set. E.g., for the first class all large cubes are gray within the training set, but any colorin the test set. This gray color thus represents a confounding factor within CLEVR-Hans3. The number oftraining images in CLEVR-Hans3 corresponds to 9k. VQA-X. The VQA-X dataset (Park et al., 2018) extends the COCO (Lin et al., 2014) based VQA-v1 (Zitnicket al., 2016; Antol et al., 2015) and v2 (Goyal et al., 2017) datasets with human-annotated explanations. Itcontains open-ended queries to corresponding images which require a comprehension across vision, language,and common-sense domains for answering.",
  "C.1Explanation consolidation": "Ridge regression classification. For evaluating the separability of learned explanations, we provide theaccuracy of a ridge regression (RR) model that is fitted on a set of tuples consisting of explanations froma trained learner and the corresponding ground-truth (GT) class labels of the underlying image. The RRmodel is fitted on a training set and tested on an additional, held-out set of explanations (and correspondingclass labels). This evaluation acts as a proxy of the separability of learned explanations. The higher the RR accuracyon the test set the better the separability between explanations. For each learning configuration in ourevaluations we train a RR model separately on the explanations from the five differently seeded models.",
  "KKj,j=k d(j, k)": "Essentially, this metric estimates in how far the explanations stemming from samples of one class are close toanother compared to the explanations of samples from other classes. The encoding of a separate, pretrainedmodel, h, provides the encoding space in which this similarity is assessed. The lower the values of IICSthe better separable are the data for h. Specifically, zki corresponds to the encoding of the explanation ofa sample i from class k. This encoding is provided by an additional model, h, via h(ei) = zki , where ei isa provided explanation of sample i from a learner f. h is identical in architecture to the learner of whichthe explanations are being evaluated, however h was separately pretrained only on the original task. Forevaluating explanations from the CNN configurations, h corresponds to an identical CNN that was trainedfor image classification as in the vanilla configurations.For evaluating the NeSy configurations a NeSyconcept learner was pretrained for classification as in the NeSy vanilla setting. In both cases h was providedwith a random seed different from those used in the original training setups. Furthermore, k corresponds to the average encoding over all samples of class k (where for notations sakewe assume M samples in each class, although this can vary in practice). d(x, y) represents a distance metricbetween x and y, where we have used the euclidean distance in our evaluations. We divide the distance",
  "C.2Faithfulness": "For comprehensiveness, parts of the input are removed that correspond to important features as identified bythe explanation. As a result, the model should be less accurate in its predictions. In the case of sufficiency,one removes those input features which were deemed unimportant according to the explanation. Hereby,the model should not lose much accuracy. Notably, the original sufficiency and comprehensiveness metricsof (DeYoung et al., 2020) were introduced in the context of NLP in which input sequences are consideredas discrete inputs. However, removing input features from continuous inputs such as images presents anissue (Hooker et al., 2019) as measured differences due to pixel removal may reflect the influence of themodified, out-of-distribution input rather than faithfulness of the explanation. For this case, we modifiedthe metrics for the CNN configurations (i.e., for explanations that are in a continuous form) to approximatelycompensate for this effect. For evaluating explanation faithfulness we thus provide results for CNN-LSX (andvanilla CNN) via the continuous adaptation of both metrics (denoted as COMPcont. and SUFFcont.) andfor NeSy-LSX (and NeSy vanilla) via the original comprehensiveness and sufficiency definitions (denoted asCOMPdiscr. and SUFFdiscr.). We formalize these in the following. We follow the notation for COMPdiscr. and SUFFdiscr. of Chan et al. (2022). For this, x denotes an inputsample. We denote the predicted class of x as c(x), and the predicted probability corresponding to class jas pj(x). Assuming an explanation is given, we denote denote the input containing only the q% importantelements as x:q%. We denote the modified input sequence from which a token sub-sequence x are removedas x \\ x. Comprehensiveness and sufficiency for discrete explanations are finally defined as:",
  "Where N here represents the number of data samples in the evaluation set.In our evaluations we setB = {1, 5, 10, 20, 50} as in the original work of DeYoung et al. (2020)": "For computing comprehensiveness and sufficiency scores based on continuous explanations we first computethe comprehensiveness and sufficiency when a percentage q of the top input elements (e.g., pixels) are set tothe median value of all input elements of the evaluation set. In comparison to the definition of COMPdiscr.and SUFFdiscr. of DeYoung et al. (2020) for the adaptation to continuous explanations we base the metricson class accuracy rather than class probabilities. We denote these alternative computations as:",
  "Test set samplesExpls. CNN (w/ conf.)Expls. CNN-LSX (w/ conf.)Expls. CNN-LSX (w/ deconf.)": ": Example input attribution (InputXGradient) explanations from the different CNN configurations on De-coyMNIST. The top row show an original test set sample and the corresponding explanations for CNN (w/ conf.),CNN-LSX (w/ conf.) and CNN-LSX (w/ deconf.). The bottom row shows the same setup for 20 randomly selectedtest set samples. In red we have highlighted the confounding factor of the specific example. Note that the CNN-LSXmodels (both trained w/ conf. and w/ deconf.) do not indicate importance of the confounder in their explanations.",
  "C.3Self-unconfounding: Sample Explanations": "presents exemplary explanations from the different CNN configurations on the DecoyMNIST dataset.Specifically, we provide images from original test set samples (left), explanations of the baseline CNN (w/conf.) (second to left), explanations of the CNN-LSX (w/ conf.) (second to right) and explanations of theCNN-LSX (w/ deconf.) (right). The explanations correspond to the InputXGradient importance maps. Thetop row represents images for a single sample, where the red box in the test sample image indicates the",
  "DAdditional Discussion": "Human-machine interactions. Accurate and trustworthy human-machine interactions have been iden-tified as important criteria for the future deployability of AI systems (Friedrich et al., 2023a; Teso et al.,2023; Holzinger, 2021; Angerschmid et al., 2022). Also for LSX-trained models there is no guarantee that itsexplanations are aligned with human requirements and knowledge. This makes conclusive human assessmentand potential human-based revisions necessary also for LSX trained models. However, in contrast to otherlearning frameworks, LSX directly facilitates the development and integration of such mechanisms that allowfor fruitful human-machine interactions. E.g., when integrating a model into LSX one must develop boththe Explain and Revise module. Via the Explain module a human user can directly query the learnersreasons for a prediction and via the Revise module integrate feedback on these explanations. This canpotentially ease the integration of necessary revisory feedback from humans. System 1 and 2 processing.A prominent hypothesis from cognitive psychology (which has gainedrecent interest in AI research(Goyal & Bengio, 2022; Kautz, 2022; Ganapini et al., 2022; Booch et al.,2021)) is that human cognition can be described via two processing systems: an approximate, fast system(system 1) that handles the majority of familiar situations and an embedded, slower, yet more exact system(system 2) that processes unfamiliar settings (Kahneman, 2011). There are interesting parallels between thisframework and that of LSX where Fit can be considered to represent a fast, initial processing phase, andthe triad consisting of Explain, Reflect and Revise to represent a slower, embedded processing phase.An important open question, particularly in AI research on system 1 and 2 processing, is on the form ofcommunication between the two systems (Goyal & Bengio, 2022; Kautz, 2022). Explanations, as utilized inLSX, possess interesting properties for this aspect. Specifically, explaining and reflecting on the learnersexplanations in LSX represents a form of making the implicit knowledge of the learner explicit. At thesame time, system 2 processing can also influence the processing of system 1 as alluded to by our findingson explanation consolidation (cf. Tab. 4). Lastly, our NeSy-LSX example instantiation has many parallelsconcerning the integration of neural and symbolic components to Henry Kautzs Neuro[Symbolic] system 1and 2 approach (Kautz, 2022). Overall however, LSX is still far away from such models of human cognitionand much additional research is needed.",
  "zi": ": Example explanations from the different NeSy configurations for class 1 images of CLEVR-Hans3. Specif-ically, we provide images of the integrated gradients-based explanations, ezi. The first row depicts original imagesof four randomly selected training samples that belong to class 1. The second, fourth and sixth row depicts thesymbolic representation, zi, of these images, as processed by the slot-attention-based perception module, where rowfour and six merely represent row-wise permutations of zi in row two. Row three depicts explanations of baselineNeSy (w/ conf.). Row five depicts explanations from NeSy-LSX (w/ conf.) and the last row depicts explanationsfrom NeSy-LSX (w/ deconf.). In red we highlight the confounding object attribute of class 1. In blue we highlightthe underlying rule of class 1 based on each sample."
}