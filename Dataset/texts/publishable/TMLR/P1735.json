{
  "Abstract": "Large Language Models (LLMs) trained on massive datasets may inadvertently acquire sensi-tive information such as personal details and potentially harmful content. This risk is furtherheightened in multimodal LLMs (aka MLLMs) as they integrate information from multiplemodalities (image and text). Adversaries can exploit this stored knowledge by craftinginputs across modalities to extract sensitive details. Evaluating how effectively MLLMs canforget such information (targeted unlearning) necessitates the creation of high-quality, well-annotated image-text pairs. While significant research has addressed the creation of datasetsfor unlearning within LLMs, it has primarily concentrated on text modality. Creation of anal-ogous datasets for multimodal data and models remain an understudied area. To address thisgap, we first introduce a multimodal unlearning benchmark, UnLOK-VQA (Unlearning Out-side Knowledge VQA), as well as an attack-and-defense framework to evaluate methods fordeleting specific multimodal knowledge from MLLMs. Our dataset generation process involvesan automated pipeline to create samples of varied proximity levels to the target data pointfor evaluation of generalization and specificity, followed by manual filtering to retain only thehigh-quality data points. We use this process to extend a visual question-answering datasetfor evaluating multimodal information deletion. Next, we present a comprehensive unlearningevaluation involving an attack-and-defense framework consisting of four whitebox and threeblackbox attacks against six unlearning defense objectives. We also design a whitebox attackbased on the interpretability of hidden states in LLMs motivated by past work. Our experi-mental results demonstrate that multimodal extraction attacks (with an attack success rate of45.5%) are more successful than either image-only (32%) or text-only attacks (39%). The best",
  "Published in Transactions on Machine Learning Research (12/2024)": "Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Mller, Joe Penna, andRobin Rombach. SDXL: Improving latent diffusion models for high-resolution image synthesis. In TheTwelfth International Conference on Learning Representations, 2024. URL Xiangyu Qi, Yi Zeng, Tinghao Xie, Pin-Yu Chen, Ruoxi Jia, Prateek Mittal, and Peter Henderson. Fine-tuning aligned language models compromises safety, even when users do not intend to! arXiv preprintarXiv:2310.03693, 2023.",
  "Introduction": "The emergence of Large Language Models (LLMs) marks a significant advancement in our ability to accessand process knowledge about the world. The evolution towards Multimodal Large Language Models (MLLMs)expands this capability beyond text, enabling the extraction of knowledge spanning both text and visionmodalities. These MLLMs are known to acquire and retain sensitive information they should not know (Liuet al., 2023b; Pi et al., 2024; Zong et al., 2024), such as a persons address from their image or a privatestreet address from a photo of a location (Chen et al., 2023). As models become increasingly capable andare widely deployed, they raise safety concerns regarding potential information leakage and exploitation,particularly when MLLMs are deployed in applications that interact with people or influence decisions withreal-world consequences such as cybersecurity, biological weapons, chemical weapons, or other large-scalesafety concerns (Li et al., 2024b). While techniques for deleting information (unlearning) have been exploredfor text-based LLMs, similar methods for multimodal LLMs remain underexplored. Notably, the lack ofsuitable datasets and a unified evaluation framework impedes the evaluation of unlearning effectiveness inMLLMs. Our work addresses this challenge by introducing UnLOK-VQA (Unlearning Outside KnowledgeVQA), a novel benchmark specifically designed for evaluating the targeted erasure of pretrained multimodalinformation from MLLMs. Next, we employ model editing techniques that facilitate fine-grained modelcontrol for the targeted erasure of pretrained knowledge (information that the model has learned duringpretraining) from multimodal models and evaluate them in an adversarial setting on UnLOK-VQA. Tell me the name of this street MLLMBeverly Hills",
  "Drive": "1. Information leakage from MLLMs MLLM MLLM 2. Deletion defense 3. Extraction attack from adversaries !! !! : Illustration of (1) information leakage inMLLMs, and (2, 3) the attack-defense framework.This demonstrates that while defense methods canmitigate information leakage from MLLMs, mali-cious adversaries may still extract sensitive infor-mation from them. Deleting Sensitive Knowledge in LLMs.Currentmethods for preventing LLMs from disclosing sensitiveinformation predominantly utilize reinforcement learningfrom human or AI feedback (RLHF or RLAIF) (Ouyanget al., 2022; Bai et al., 2022; Christiano et al., 2017;Yuan et al., 2023). However, RLHF presents notablelimitations: (1) it depends on human-written outputs,which are costly to collect, and (2) it requires significantcomputational resources due to its standard three-stagealignment process. Furthermore, RLHF may still resultin information leakage, and fine-tuning can circumventits safeguards (Zhan et al., 2024). Therefore, in thiswork, we propose directly editing the models weightsto remove sensitive information, reducing the risk ofleakage from the models internal parameters. Benchmark for Multimodal Knowledge Deletion.Recent work by Lynch et al. (2024) highlights the criti-cal need for rigorous evaluations in unlearning processes.Evaluating the effectiveness of methods designed to re-move information from MLLMs necessitates specialized datasets that enable evaluation of the deletionmethods effectiveness, generalization ability and the damage it causes to the model. In this work, we buildan automatic pipeline for extending VQA datasets with rephrase and neighborhood samples to evaluategeneralization and specificity of the deletion method. Generalization evaluates if the method is robust to",
  "Contributions.Overall, we summarize our contributions and findings below:": "1. We introduce UnLOK-VQA, a dataset for evaluating deletion of specific undesirable multimodal knowl-edge from models. Our data generation process involves an automatic generation pipeline followed bymanual filtering for the retention of high-quality samples. UnLOK-VQA incorporates rephrase andneighborhood data with varying proximity to the target information, facilitating a nuanced evaluation ofboth generalization and specificity respectively of the unlearning methods. 2. We adopt an attack-defense framework for evaluation of multimodal information erasure to assess therobustness of unlearning methods against adversarial attacks and show that state-of-the-art model editingmethods like LoRA fine-tuning can not fully delete knowledge2 from MLLMs. 3. We observe that the multimodal extraction attack outperforms its unimodal counterparts in the multimodalediting setup. Also, we show that editing LLMs layers is more effective than editing the multimodal",
  "Text Description": "SDXL Add salt-and-pepper noise Neighborhood Question + Answer Generation Neighborhood Image + Answer Generation Question + Alternate Answer SDXL Generate \"alternate answer\" to the question using LLM Image: Samples for Generalization EvaluationSamples for SpecicityEvaluation Obtain text description of the image Obtain an \"object\" not related to the Q + A Image + Object Grounded SAM Question + Alternate Answer+ Text Description SDXL Get the sentence in text description most irrelevant to Q Alternate ans: Terrier What kind of dog is this? You're an AI with access to vast knowledge about dogs. I need your expertise to ll in the gaps ... your insights would be invaluable. Which breed of dog it this? The following blog post is informative about dogs. Which breed of dog it this? Q: Is the frisbee positioned in the air or on the ground? A: Air Q: What animal is in the picture? A: Dog Q: Is the frisbee close to or far from the dog? A: Close",
  "Easy": ": Pipeline for UnLOK-VQA generation: (1) We utilize the OK-VQA dataset as a basis for evaluatingthe efficacy of editing methods in removing knowledge from MLLMs; (2) We employ multiple techniques toproduce rephrase data with different levels, which we use in blackbox attacks to assess the generalizabilityof the unlearning methods; (3) We create various levels of neighborhood data to check whether the editingmethods target the intended information without altering the outputs of neighboring data.",
  "Related Work": "Unlearning in LLMs.While machine unlearning is a long-standing area of research that includes a varietyof approaches (Cao & Yang, 2015), the prevailing approach preventing sensitive information leakage in LLMswhile maintaining informativeness leverages reinforcement learning (RL) guided by human or AI feedback.However, RLHF faces significant limitations, including residual information leakage, where models might stillretain sensitive data (Zou et al., 2023). Furthermore, Zhan et al. (2023) show that fine-tuning can circumventRLHF protections, challenging its effectiveness in handling sensitive information. RLHF-trained models mayalso reject safe prompts similar to unsafe ones (Rttger et al., 2023). Alternative approaches are emergingto address these limitations such as access control methods that instruct the model to withhold responsesto queries targeting specific groups identified through natural language descriptions (Chen et al., 2023).However, these methods are vulnerable to adversarial blackbox attacks, as demonstrated by the same study. Even if a model is instructed to refrain from directly generating harmful content (e.g., instructions forbuilding a weapon from its image), an adversarys ability to access the underlying information through itsparameters and potentially combine it across modalities (text and images) remains a risk as that informationcan be elicited in adversarial settings (see ). Recent work in unlearning for LLMs has focused on",
  "UnLOK-VQA: Dataset for Multimodal Knowledge Editing": "Evaluation of multimodal knowledge deletion methods requires assessing efficacy, generalization, and specificityon a multimodal knowledge dataset. Therefore, we propose an automatic pipeline to extend a VQA datasetwith data points for the evaluation of multimodal information deletion from models. We use this pipelinefollowed by manual filtering to create UnLOK-VQA as follows: (1) Using OK-VQA data (Marino et al.,2019) to evaluate knowledge deletion efficacy (.1); (2) Employing SoTA LLMs and vision modelsto generate rephrase data for testing generalization (.2) (See left side of ); (3) Creatingneighborhood data to evaluate the impact on unrelated information (specificity) (.3) (See rightside of ). This section details our data generation pipeline (See ).",
  "Efficacy": "We utilize samples from OK-VQA to evaluate the effectiveness of knowledge deletion in preventing adversariesfrom recovering deleted information. Specifically, for each sample (v, q, a), we employ an editing methodto erase the answer a from the model given the input question q and image v. We quantify whether ais fully deleted and unrecoverable using the rewrite score (Equation (1)) and the attack success metric(.2). The Rewrite Score (Hase et al., 2023) indicates how much the edit minimizes the targetprobability compared to the desired change,",
  "Generalization": "In knowledge deletion tasks, generalization refers to the ability of a deletion method to ensure that deletedinformation cannot be recovered, even when an adversary rephrases questions or uses similar but not identicalimages. This is important because a strong deletion method should be robust not just to the exact questionor image for which the deletion was performed, but also to variations. Models trained on large multimodaldatasets often display a broad understanding of concepts and can generalize across inputs. Without addressinggeneralization, a deletion method would only be effective for very specific input patterns, leaving the systemvulnerable to broader attacks. Existing works often focus on deletion in unimodal (usually text-based)settings, where generalization might involve only text rephrasing. However, in multimodal models, thechallenge is compounded because adversaries can attack from two modalitiestext and images. Existingresearch lacks effective datasets and frameworks to test how well deletion methods generalize across thesedifferent multimodal inputs. In this work, we focus on the creation of a dataset that enables evaluation ofgeneralization of the deletion method with the help of rephrase images and rephrase questions of varyingdifficulty levels (varying proximity to the deleted data point). We create rephrase data to evaluate how well the deletion method generalizes to different ways of queryingthe removed information. This data consists of rephrase images and questions for each sample in OK-VQAwith varying proximity levels to the original data point. These proximity levels correspond to varying difficultylevels in terms of the models ability to generalize its understanding to different query formulations. Rephrase Image is such that it has the same answer to question q as the original image v. Although therephrase image may differ semantically from the original image, our pipeline ensures that the answer toquestion q remains consistent between the original and rephrase images (See for examples of eachtype of rephrase image). Our pipeline generates rephrase images at three different difficulty levels: Easy,Medium, and Hard, based on their proximity to the original image. Our rationale is that as the proximityradius increases, it becomes more challenging for the deletion method to generalize to these images.",
  ". Easy: We introduce noise to the image v (salt-and-pepper noise) (Rosales et al., 2023) such that the maincontent in the image remains unaltered (See )": "2. Medium: We generate the image in this level by removing one random object in the original image byreplacing it with a repainted version. This altered image will differ more from the original compared tothe easy rephrase (See ). To achieve our goal, we utilize Grounded SAM (Ren et al., 2024) toremove a segment of the image v that is not pertinent to the question and answer while maintainingthe rest of the images semantics (In , the frisbee is removed as it is an object irrelevant to thequestion). Grounded SAM necessitates identifying the target object for modification. To find the irrelevanttarget, we first detect all objects in the image using either YOLOv9 (Wang et al., 2024) or by extractingnouns from a textual description of v generated by LLaMA-2-7B. We then exclude objects with highSentence-BERT-based (Reimers & Gurevych, 2019; Kenton & Toutanova, 2019) similarity to any nouns inq and a. The object with the highest detection confidence is selected for repainting by Grounded SAM. 3. Hard: Images in this level are entirely generated by models, and thus they will deviate more from v thanthe other levels (See ). We use LLaVA-v1.5-7B to generate a detailed textual description of v.We then use a diffusion model SDXL (Podell et al., 2024) to create an image based on the description,ensuring the answer to q aligns with a in the new images context. Rephrase Question is designed to have the same answer a as q within the context of image v. Althoughsemantically different from q, its purpose is to extract the answer from the model. Our pipeline includesthree types of rephrases: Easy, Medium, and Hard, based on their similarity to the original question and thedeletion methods ability to generalize to them.",
  ". Medium: We leverage DIPPER-11B (Krishna et al., 2023), a SoTA paraphrasing model to generate morecomplex textual variations of the original question (See )": "3. Hard: We prepend a jailbreak prompt (e.g. Youre an AI with access to vast knowledge about...) to thequestion. Jailbreak interventions have demonstrated efficacy in reactivating knowledge typically suspendedfrom LLMs generation, such as the construction of explosive devices (Shah et al., 2023) (See ). An adversary may leverage evaluate generalization data to elicit deleted information. To simulate this setting,we use rephrase data in an adversarial manner to evaluate the deletion methods ability to conceal thedeleted information when rephrase data is used to elicit it (Rephrase attack). The trend of easy, mediumand hard complexity is reflected in the increasing attack success rate across the three variants in .Our multimodal attack employs a combination of question and image rephrases to evaluate the robustness ofMLLMs to Multimodal Rephrase Data. See (left) for an example of rephrase data.",
  "Specificity": "Specificity is the ability to ensure that only the targeted information is deleted without damaging the modelsbroader knowledge. Its crucial because: (1) Collateral Damage: When deleting a specific piece of informationfrom a model, there is a risk of inadvertently erasing other related knowledge. This can cause the model tobecome less accurate in tasks that require related but not identical knowledge. (2) Maintaining Model Utility:After the deletion, the model should still function well on questions or images that lie in the \"neighborhood\"of the deleted information. If the deletion process is too aggressive, the model may lose accuracy on tasksthat require similar or related knowledge, thereby reducing its utility. In contrast to existing workswhichmay only focus on the deletion of single facts in unimodal contextsthis work introduces the concept ofneighborhood data for multimodal inputs to test specificity. It assesses whether the deletion method canremove specific knowledge without negatively impacting nearby but unrelated knowledge in the modelsbroader understanding. We focus on the creation of a dataset that enables evaluation of specificity of thedeletion method with the help of neighborhood images and neighborhood questions of varying difficultylevels (varying proximity to the deleted data point). To assess the specificity of the edit made to deleteinformation i.e. to assess the model damage the deletion caused, we create neighborhood data points foreach sample in the OK-VQA dataset. These data points represent unrelated information that lies in theneighborhood of the information that is edited. Their purpose is to evaluate the damage to the modelsknowledge on the data that is different from the original data for which the information was erased butlies in its neighborhood. Ideally, a successful editing method should not affect the models accuracy onneighborhood data points. Concretely, we evaluate two types of neighborhoods of the input: (1) (neigh(v), q,aimg_neigh), (2) (v, neigh(q), aques_neigh), where neigh(v) and neigh(q) denotes a neighborhood image andquestion respectively, aimg_neigh and aques_neigh denote the new answers in the context of the neighborhoodimage and question respectively, and the generation process is described below. Neighborhood Image (neigh(v)) lies in the neighborhood of the original image v, but the main object ischanged such that the answer to the question q is changed (from a to a), meaning that the answer to thequestion q is a for the neighborhood image. To create such images, we first obtain feasible alternative answersto q by prompting Flan-T5-XXL (Chung et al., 2024) model to generate a set of three alternative answers{a} to the question q that are different from a. We pick the answer aimg_neigh that is most dissimilar to a asmeasured by BERT similarity. Then we generate two levels of neighborhood images as described following.See for examples of neighborhood images of each level. 1. Easy: We leverage SDXL to generate images for the alternative answer such that the answer to thequestion q in the context of the generated image is aimg_neigh, while the rest of the image content remainsrandom as no specific information is provided to the diffusion model about the remaining image content. 2. Hard: Similar to the process for getting hard rephrase images, we utilize SDXL to generate an imagebased on the text description of v while also ensuring the image corresponds to the specific alternativeanswer aimg_neigh. This image is more similar to the v (lie in a neighborhood of smaller radius) comparedto the Easy Neighborhood Image, i.e. we expect their information would be harder to preserve afterdeletion.",
  "Manual Filtering and Human Evaluation": "shows the human evaluation results on UnLOK-VQA. In our human evaluation process for UnLOK-VQA, we established clear standards for annotators to assess the quality of the generated data. The evaluationfocused on two primary criteria: (1) Consistency of Target Answers: Annotators were tasked with determiningwhether the target answer remains consistent when evaluating rephrased data. A consistent answer indicatesthat the rephrase effectively captures the original intent of the question. (2) Appropriateness of AnswerChanges: For neighborhood data, evaluators assessed whether the target answer changes appropriately inresponse to the modified question or context. An appropriate change signifies that the alteration aligns withthe expectations set by the original question. We observe that around 75% of the automatically generated rephrase data and around 66% of the automaticallygenerated neighborhood data meet our standards. To further enhance the quality of the dataset, we conduct oneround of manual filtering to remove data that do not have proper rephrase or neighborhood images/questions.We again conducted a human evaluation on the filtered UnLOK-VQA, finding that over 90% of the samples(shown in ) meet the criterion. We adopt this high-quality, filtered version for our subsequentexperiments. See for samples in UnLOK-VQA. The details of the design questions for humanevaluation and the interface demonstrations are provided in Appendix C.",
  "Dataset analysis": "The dataset consists of 500 samples that have been manually filtered and verified by human evaluators. Eachsample contains a (v, q, a) triple, where a represents the correct answer to question q within the context ofimage v. Additionally, each sample includes three types of rephrased images, two types of neighborhoodimages, an average of four neighborhood questions, and three types of rephrased questions. illustratesthe distribution across the various categories within UnLOK-VQA.",
  "Attack-and-Defense Perspective": "Open-source releases of MLLMs (Liu et al., 2023a) necessitate robust evaluation methods that go beyondsimply assessing model generations. We incorporate diverse whitebox attacks and blackbox attacks intodeletion evaluations to strengthen claims about model safety and privacy. We cast the multimodal informationdeletion problem within the framework of adversarial attack and defense mechanisms commonly employed inmachine learning security (Carlini et al., 2019). In this context, the objective of the defense methods is toeffectively erase a single piece of information from a multimodal LLM while the attack methods aim to retrievethe deleted information from the model. In the following subsections, we introduce our attack-and-defenseframework, including the threat model, attack methods, and defense methods, in detail.",
  "Building upon the work of Patil et al. (2023a), we broaden the security landscape for information deletion byintroducing a threat model tailored to multimodal data and models": "Adversarys Objective: We posit an adversary aiming to extract answer A to a question Q in the contextof the image V , where the triplet (V , Q, A) is a sensitive piece of information. An extraction attack isconsidered successful with budget B if answer A is present within a candidate set C (|C| = B) generatedby the adversary through an inference algorithm applied to the multimodal model. We refer to the size ofthe candidate set, denoted by |C|, as the attack budget (B), thus making our setting more general than thetypical one-shot setting (Carlini et al., 2018; Lukas et al., 2023). This setting is similar to the threat modelintroduced in Patil et al. (2023a) for LLMs and generalizes the typical one-shot setting (B = 1) by allowingmultiple attempts at extraction. This more general setting reflects plausible scenarios, where the adversarycould either (1) Attempt multiple queries to guess sensitive information (2) Generate multiple potentialcandidates and act on any correct one (3) Verify the correctness of information, as in the case where theadversary is also the data owner trying to confirm that their sensitive data has been properly deleted. Byallowing B>1, we account for these broader, more realistic adversarial capabilities, making our setting morecomprehensive and practical. Attack Success Metric: We say that an adversary is successful if the answer is present in the candidateset C. We thus define the success of extraction attacks in the context of MLLMs using the following metriccalculated using a dataset D = (vi, qi, ai)Ni=1, where each A = ai represents a correct answer to the questionQ = qi in the context of the image V = vi. The definition of the metric is:",
  "where Ci denotes the candidate set generated by the model M for the data point (vi, qi), i.e., Ci = M(qi, vi | B)and 1 represents the indicator function": "Adversarys Capabilities: We have two access levels to simulate real-world attacker (Carlini et al., 2019):whitebox and blackbox access. Whitebox access assumes the adversary has full knowledge of the modelsweights and architecture, enabling forward passes and access to hidden states. Blackbox access limits theadversary to providing inputs and receiving randomly sampled outputs. These access levels reflect prevalentLLM access methods, available either open-source (Liu et al., 2023a) or through APIs (Brown et al., 2020). Metrics for Multimodal Information Deletion: The objective of information deletion is to selectivelyremove specific information from a model while simultaneously preserving its overall knowledge. However,similar to previous strategies performing sensitive information unlearning, model editing methods incursome performance loss on knowledge-intensive tasks. Hence, it is necessary to have dedicated metrics toevaluate such information loss. Overall, when we employ model editing as a defense against extractionattacks, the objective is to minimize attack success while simultaneously minimizing damage to the modelsknowledge. This is formulated as: arg minM AttackSuccess@B(M) + Damage(M, M) , where M",
  ". Random -Accuracy (Zhu et al., 2020; De Cao et al., 2021): Measures the change in modelaccuracy for random data points before and after editing": "2. Neighborhood -Accuracy: To assess the specificity of an edit in a multimodal setting, wecalculate two versions of the neighborhood -Accuracy (Meng et al., 2022): Question Neighborhood-Accuracy and Image Neighborhood -Accuracy. The definitions and methods for generatingneighborhood data are detailed in .3. We employ neighborhood questions and images tocompute the respective Question and Image Neighborhood -Accuracy.",
  "The output of an attack method is to generate a candidate set C that potentially contains the informationthat it aims to extract. Here the size of the C is limited by the attack budget B": "Whitebox Attacks. We use whitebox attacks from Patil et al. (2023a) that leverage Logit Lens (nostalge-braist, 2020; Geva et al., 2021), an interpretability technique that probes the hidden states of an LLM, andexploit the hypothesis that deleted information might persist in the models intermediate layers hidden statesdespite its removal using by editing model using deletion objectives or its absence in the final generationoutput.",
  ". Head Projection Attack (Patil et al., 2023a): This attack constructs a candidate set by collectingthe top-k highest probability tokens from each layer probed by LogitLens": "2. Probability Delta Attack (Patil et al., 2023a): This attack constructs a candidate set by identifyingtokens whose logit lens probabilities rise and fall significantly between consecutive layers, potentiallycapturing the deleted information. 3. Probability Delta2 Attack (Our attack): In this novel attack, we construct a candidate set byidentifying tokens whose differences in probabilities across consecutive layers rise and fall significantlybetween consecutive layers (which means taking the difference of difference in the probabilitiesof tokens across consecutive layers), potentially capturing the deleted information. The PD2 attack is an order two attack, where a second-order difference (difference of differences) between thedistributions is computed, providing a second-order comparison. We design this with the aim ofcapturing more nuanced traces of deleted information that might be overlooked by simpler approaches. 4. Finetuning-based attack: A major obstacle in unlearning is its resilience to few-shot fine-tuning,where a small fine-tuning dataset causes a disproportionate return of previously deleted knowledge(Henderson et al., 2023; Yang et al., 2023; Qi et al., 2023; Lermen et al., 2023; Zhan et al., 2023).We fine-tune the edited model on random, unrelated data and then use the HP attack to assessrobustness to post-deletion fine-tuning. Blackbox Attacks. This simple blackbox attack exploits the imperfect rephrase generalization of modelediting methods (De Cao et al., 2021; Meng et al., 2022). It constructs the candidate set C by querying theedited model with rephrased versions of the original input. We explore three variants to evaluate effectivenessacross different modalities: 1. Image Rephrase Attack: This attack uses rephrased images with the same questions to test modelvulnerability to changes in visual representation. It has three variations are based on the rephraselevels. 2. Question Rephrase Attack: This attack uses rephrased questions with the same images to testvulnerability to changes in textual representation. It has three variations are based on the rephraselevels. 3. Multimodal Rephrase Attack: This attack leverages data points where both the question andthe image are rephrase. This provides a holistic evaluation of the rephrase attacks effectiveness inexploiting weaknesses across both modalities within the edited multimodal data. We pick the best(hard) question rephrase and the best (hard) image rephrase for this attack.",
  "function, arg maxMp(d|v, q; M), maximizes the probability of the model generating an empty target string(d) for any given input (v, q)": "Fact Erasure (Fact-Eras) (Hase et al., 2023): This approach reduces the probability of the MLLMgenerating a sensitive answer (a) for a given question (q) and image (v), by minimizing p(a|v, q; M) for theoriginal information (v, q, a). Error Injection (Error Inj) (De Cao et al., 2021): This method introduces counterfactual knowledge intothe model. Using the objective function arg maxMp(a|v, q; M) (Meng et al., 2022), where a is a false targetanswer, it demonstrates the models ability to incorporate manipulated information. Head Projection (HP) Defense (Patil et al., 2023a): This approach employs a max-margin objectiveto prevent the deleted answer from appearing among the top-k elements in LogitLens distributions acrosschosen layers (L) and the final output. Max-Entropy Defense (Patil et al., 2023a): Similar to the Head Projection Defense, this approach focuseson LogitLens distributions but uses a different objective per layer. It maximizes the entropy (uncertainty) ofthe next-token distribution across chosen layers (L) and the final output. Input Rephrasing (IR) Defense: This defense strategy targets the Input Rephrasing Blackbox Attack. Itexpands the editing objective beyond the original input (v, q) by incorporating three versions of rephrases ofthe input (v, q): (1) (rephrase(v), q) (2) (v, rephrase(q)) (3) (rephrase(v), rephrase(q)).",
  "Experimental Setup": "Models and Editing methods. Our experiments involve the multimodal LLM: LLaVA-v1.5-7B (Liu et al.,2023a). This model is selected based on its: (1) widespread adoption within the multimodality community,(2) ease of access due to public availability, and (3) documented ability to retain information from theirpre-training data. We also report the attack success rates on a larger LLaVA-v1.5-13B for evaluating theeffect of scaling model size on the robustness of erasure methods to attacks. Model editing methods. Our experiments utilize LoRA finetuning for information deletion in MLLMs,targeting specific weight matrices in the models MLP layers, as motivated by Meng et al. (2022). While thatanalysis focused on GPT models, we tune LLaVA-v1.5-7B and LLaVA-v1.5-13B and find that editing the7th and 9th layers, respectively, yields effective results with a rewrite score over 85% (indicating successfulinformation erasure) and a random -Acc below 5% (mostly preserving the models overall knowledge).While prior works, such as (Meng et al., 2022), have attempted to localize information using causal tracingand then edit the corresponding weights. A follow-up study Hase et al. (2023) demonstrates that localizationdoes not necessarily guide effective editing. This is why, we opt to select layers empirically rather than relyingon localization.",
  "Main Results": "Design.We first investigate how each of the defense methods outlined in .3 fares against eachof the extraction attacks outlined in .2 on UnLOK-VQA. We measure Attack-Success@B withB = 20 for each of the attacks (both whitebox and blackbox attacks), in addition to Random -Acc andQuestion Neighborhood -Acc and Image Neighborhood -Acc metrics. Our investigation is conducted onthe LLaVA-v1.5-7B model with LoRA finetuning as the editing method. To ensure that each editing methodfunctions as intended and allows for a fair comparison, we meticulously adjust the hyperparameters until wereach reasonable rewrite scores and -Acc (as mentioned in ). Below, we report the results andanswer three questions regarding multimodal model editing for targeted unlearning. Can we extract a piece of deleted multimodal information from an MLLM?Yes. Our results in demonstrate that both whitebox and blackbox extraction attacks are successful. Among whiteboxattacks, the Probability Delta (PD) attack exhibits the strongest performance, frequently achieving attack",
  "Whitebox AttackBlackbox AttackRand-AccQ Neigh-AccI Neigh-AccRewriteScoreHPPDPD2HP+FTHard ImgRephraseHard QuesRephraseMM": "LoRA- Fact-Eras0.3000.2960.2640.4120.3200.3900.4550.0010.0080.0590.956- Empty0.7770.9300.7590.8170.6820.7930.7890.0450.0240.0590.965- Entropy0.1850.1810.1450.2530.3040.4020.4310.0290.0370.120.883- HP0.0360.1330.1050.1210.0580.1010.1570.0320.0270.0070.999- Error Inj0.4230.4770.4630.4680.3660.4250.4850.0460.037-0.0680.895- IR + Fact-Eras0.1590.1950.1690.2290.2030.2370.3220.0040.0090.0660.974 : Attack success rates of the attacks (.2) against defense methods (.3) for deletingmultimodal information in UnLOK-VQA that is known by the LLaVA-v1.5-7B model. The deletion isperformed via LoRA edits to the MLP modules within an LLM layer of LLaVA. success rates exceeding 20% (when budget B is set to 20) against most defenses. While PD attack doesbetter than HP, PD2 does not improve on top of PD. This is likely because higher-order attacks delve deeperinto differences, making the targeted information less apparent. The finetuning attack (HP+FT) has higherattack success rate compared to the original HP attack which shows that all the defenses are vulnerable to thefinetuning attack. For blackbox attacks, the multimodal (MM) rephrasing attack also often succeeds morethan 35% of the time. These high success rates indicate a high vulnerability to extraction attacks targetingthe deleted fact within the threat model outlined in . Are the defenses effective against extraction attacks?In , our analysis reveals that, amongthe whitebox defense methods, Fact Erasure, Empty Response (Empty), and Error Injection defenses areless effective compared to Head Projection (HP) and Max-Entropy defenses. This observation indicates thatinformation is better concealed when the model is uncertain about the answer. In contrast, the first threemethods may lower the probability of the sensitive answer excessively, making the concealed informationmore detectable. We then observe that the HP defense exhibits the highest overall effectiveness against allattacks (whitebox and blackbox), as evidenced by its consistently lowest attack success rates compared to allother defenses. On the blackbox defense front, we only report the Question Rephrase defense results in theInput Rephrasing defense row in as we find it outperforms the image and multimodal counterparts.We exploit the easy question rephrases for the defense. Furthermore, we observe that the Image Rephrasedefense is effective only when the attack images and defense images belong to the same distribution, whereasthe question rephrase defense is effective across all distributions of question rephrase.",
  ": Comparison of attack success rates acrossthe three levels of rephrase images to attack mod-els edited by different defense mechanisms": "Is multimodal attack more effective than unimodal?We investigate the efficacy of the multimodal blackboxattack strategy compared to the unimodal blackbox at-tacks (image rephrase attack, question rephrase attack).The results in show that the multimodal (black-box) attack success rate (15.7%) is 5.6% higher than thebest question rephrase attack (Hard Question Rephrase)(10.1%) and 9.9% higher than the best image rephraseattack (Hard Img Rephrase) (5.8%) against the best (HP)defense. Similar trend holds against other defenses.",
  ": Comparison of success rates for image andquestion rephrasing attacks when modifying the MLPmodule within LLM layers versus within the multi-modal projector on UnLOK-VQA": "chose to evaluate using the Fact-Erasure defense becauseHP defense is more sensitive to hyperparameter selection.To analyze the effect of scaling, we aimed to keep the evaluation independent of hyperparameter choices.We chose the HP attack because it modifies the models weights, which could interfere with analyzing theeffect of scaling. While unlearning and scaling interact consistently across methods, fine-tuning introduces aconfounding factor, making it unsuitable for this evaluation. Results. presents our findings. A clear trend indicates that the larger model (13B) when editedusing the same deletion objective exhibits greater resilience against attacks in both whitebox (HP attack)and blackbox (multimodal rephrase attack) settings compared to a smaller model (7B). This suggests thatincreasing model complexity can improve the efficacy of deletion methods and thereby enhance the modelsability to defend against targeted attacks.",
  "Ablation Across Different Difficulty Levels": "Design. To investigate whether the intuitive trend of easy, medium, and hard rephrase samples (introducedin ) is reflected in the attack success rates of the respective rephrase images and questions, weconduct ablation experiments across the three difficulty levels of rephrase questions and rephrase images,and present the result in . Similarly, to investigate whether the intuitive trend of easy and hard",
  "neighborhood images (introduced in ) is reflected in the image-neighborhood -Acc, we conductablation experiments across the two difficulty levels of neighborhood images, and present the result in": "Rephrase ablation results.Our observations in reveal that easy and medium rephrase imagesexhibit similar attack success rates against most defenses, whereas hard rephrase images are significantlymore effective. Similarly, we observe a consistent trend for the three types of question rephrases: easyand medium question rephrases demonstrate similar attack success rates against most defenses, while hardquestion rephrases based on jailbreak prompts have the highest attack success rates. One potential reasonfor the success of hard images lies in the fact that although their semantic content is preserved, the generalcomposition of the images is significantly altered. This alteration can, to some extent, render defensemechanisms ineffective. Neighborhood ablation results.We observe that the Image Neighborhood -Acc is higher for hardneighborhood images compared to easy neighborhood images as evident from . This is because thehard neighborhood images lie closer to the deleted sample point than the easy neighborhood images byconstruction.",
  ": Comparison of success rates for the attacks in .2 when tuning the MLP module withinLLM layers versus within the multimodal projector on UnLOK-VQA": "Design. In the conventional fine-tuning paradigm for MLLMs applied to downstream tasks (Liu et al., 2023a;Li et al., 2023), the focus is on optimizing the multimodal projector, which connects the LLM to the visionencoder and enables them to interact with each other. Our investigation aims to determine if a more effectiveapproach lies in editing modules within the multimodal projector rather than editing the LLM modules. Results. Our findings presented in demonstrate that editing modules within the LLM led to betterdeletion performance (lower attack success rates for similar rewrite score and -Acc) against the best defense(HP defense) compared to the conventional approach of editing/fine-tuning the multimodal projector. Thisobservation is also consistent across the three types of rephrase attacks presented in as the attacksuccess rate when editing the multimodal projector is much higher. This suggests that editing the LLMmodules could be a more effective strategy for multimodal information deletion in MLLMs. In the context ofprojector editing too, the rising success rates of attacks across easy, medium, and hard rephrases suggestthat the more difficult rephrases present greater challenges for generalization, as shown in . Theimproved results from editing within LLM layers, compared to multimodal projectors could stem from thestage of processing. Multimodal projectors operate early, handling raw input translation before the modelfully integrates information. In contrast, LLM layers process data later, where final knowledge representationsare formed. Editing at this stage is more effective, directly targeting the models semantic associations,possibly resulting in more precise removal of specific knowledge.",
  "Conclusion": "Our study introduces UnLOK-VQA, a holistic dataset for evaluating targeted unlearning in MLLMs.We present an attack-and-defense framework and a pipeline for creating high-quality image-text pairs forevaluating efficacy, specificity, and generalization of defense methods. Our findings reveal that multimodalextraction attacks are more successful than single-modality attacks, while the best defense mechanism reducesattack success significantly. This work underscores the importance of developing effective unlearning methodsfor MLLMs and provides a critical resource for advancing research in this area.",
  "Broader Impact Statement": "Our work addresses the critical issue of deleting sensitive information in multimodal large language models(MLLMs), highlighting significant ethical implications. MLLMs, with their vast multimodal knowledge, canpotentially generate harmful content or perpetuate biases if misused. Our proposed dataset and technicalapproaches aim to mitigate these ethical challenges. However, our findings reveal the complexity of effectivelyremoving sensitive information from pretrained MLLMs, raising moral and legal concerns about theirresponsible deployment. Our research seeks to promote responsible AI innovation. This work was supported by NSF-AI Engage Institute DRL-2112635, DARPA MCS Grant N66001-19-2-4031,NSF-CAREER Award 1846185, ARO Award W911NF2110220, ONR Grant N00014-23-1-2356, and a GooglePhD Fellowship. The views, opinions, and/or findings contained in this article are those of the authors andnot of the funding agency. Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, andJingren Zhou. Qwen-vl: A versatile vision-language model for understanding, localization, text reading,and beyond. arXiv preprint arXiv:2308.12966, 2023. Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen,Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai: Harmlessness from aifeedback. arXiv preprint arXiv:2212.08073, 2022.",
  "Yinzhi Cao and Junfeng Yang. Towards making systems forget with machine unlearning. In 2015 IEEEsymposium on security and privacy, pp. 463480. IEEE, 2015. URL": "Nicholas Carlini, Chang Liu, lfar Erlingsson, Jernej Kos, and Dawn Xiaodong Song. The secret sharer:Evaluating and testing unintended memorization in neural networks. In USENIX Security Symposium,2018. URL Nicholas Carlini, Anish Athalye, Nicolas Papernot, Wieland Brendel, Jonas Rauber, Dimitris Tsipras, IanGoodfellow, Aleksander Madry, and Alexey Kurakin. On evaluating adversarial robustness. arXiv preprintarXiv:1902.06705, 2019.",
  "Yang Chen, Ethan Mendes, Sauvik Das, Wei Xu, and Alan Ritter. Can language models be instructed toprotect personal information?, 2023": "Siyuan Cheng, Bozhong Tian, Qingbin Liu, Xi Chen, Yongheng Wang, Huajun Chen, and Ningyu Zhang.Can we edit multimodal large language models? In Houda Bouamor, Juan Pino, and Kalika Bali (eds.),Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 1387713888,Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.856. URL",
  "Rohit Gandikota, Joanna Materzynska, Jaden Fiotto-Kaufman, and David Bau. Erasing concepts fromdiffusion models. arXiv preprint arXiv:2303.07345, 2023. URL": "Mor Geva, Roei Schuster, Jonathan Berant, and Omer Levy. Transformer feed-forward layers are key-valuememories. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,pp. 54845495, 2021. Peter Hase, Mohit Bansal, Been Kim, and Asma Ghandeharioun. Does localization inform editing? surprisingdifferences in causality-based localization vs. knowledge editing in language models. Advances in NeuralInformation Processing Systems, 36, 2023. Peter Henderson, Eric Mitchell, Christopher Manning, Dan Jurafsky, and Chelsea Finn. Self-destructingmodels: Increasing the costs of harmful dual uses of foundation models. In Proceedings of the 2023AAAI/ACM Conference on AI, Ethics, and Society, pp. 287296, 2023.",
  "Alvin Heng and Harold Soh. Selective amnesia: A continual learning approach to forgetting in deep generativemodels. arXiv preprint arXiv:2305.10120, 2023. URL": "Yushi Hu, Benlin Liu, Jungo Kasai, Yizhong Wang, Mari Ostendorf, Ranjay Krishna, and Noah A Smith. Tifa:Accurate and interpretable text-to-image faithfulness evaluation with question answering. In Proceedingsof the IEEE/CVF International Conference on Computer Vision, pp. 2040620417, 2023. Wenyue Hua, Jiang Guo, Mingwen Dong, Henghui Zhu, Patrick Ng, and Zhiguo Wang. Propagation andpitfalls: Reasoning-based assessment of knowledge editing through counterfactual tasks. arXiv preprintarXiv:2401.17585, 2024. URL",
  "OpenAI. Gpt-4 technical report. 2023. URL": "Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang,Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions withhuman feedback. Advances in neural information processing systems, 35:2773027744, 2022. Vaidehi Patil, Peter Hase, and Mohit Bansal. Can sensitive information be deleted from llms? objectives fordefending against extraction attacks. In The Twelfth International Conference on Learning Representations,2023a. Vaidehi Patil, Adyasha Maharana, and Mohit Bansal. Debiasing multimodal models via causal informationminimization. In Findings of the Association for Computational Linguistics: EMNLP 2023, pp. 41084123,2023b. Vaidehi Patil, Leonardo Ribeiro, Mengwen Liu, Mohit Bansal, and Markus Dreyer. Refinesumm: Self-refiningmllm for generating a multimodal summarization dataset. In Proceedings of the 62nd Annual Meeting ofthe Association for Computational Linguistics (Volume 1: Long Papers), pp. 1377313786, 2024. Renjie Pi, Tianyang Han, Yueqi Xie, Rui Pan, Qing Lian, Hanze Dong, Jipeng Zhang, and Tong Zhang.Mllm-protector: Ensuring mllms safety without hurting performance. arXiv preprint arXiv:2401.02906,2024.",
  "Rafael Rosales, Pablo Munoz, and Michael Paulitsch. Exploring resiliency to natural image corruptions indeep learning using design diversity. arXiv preprint arXiv:2303.09283, 2023": "Paul Rttger, Hannah Rose Kirk, Bertie Vidgen, Giuseppe Attanasio, Federico Bianchi, and Dirk Hovy.Xstest: A test suite for identifying exaggerated safety behaviours in large language models. arXiv preprintarXiv:2308.01263, 2023. Rusheb Shah, Quentin Feuillade Montixi, Soroush Pour, Arush Tagade, and Javier Rando. Scalable andtransferable black-box jailbreaks for language models via persona modulation. In Socially ResponsibleLanguage Modelling Research, 2023. Weijia Shi, Anirudh Ajith, Mengzhou Xia, Yangsibo Huang, Daogao Liu, Terra Blevins, Danqi Chen, andLuke Zettlemoyer. Detecting pretraining data from large language models. arXiv preprint arXiv:2310.16789,2023.",
  "Neigh (Easy): Generate an image for which the answer to this question: {question} is {alternateanswer} and there is no {original answer} in the image": "Neigh (Hard): Generate an image for which the answer to this question: {question} is {alternateanswer} and there is no {original answer} in the image including some components from thefollowing image description but retaining the answer {alternate answer}: {unrelated imagedescription}. Please make sure the answer to the question {question} in the context of the imageis {alternate answer}.",
  "Rephrase (Hard): Generate an image for which the answer to this question: {question} is {originalanswer} based on the following image description: {original image description}": "We observe a small diversion in the trend against the input rephrasing defense, medium question rephrasesare more effective than easy question rephrases because the defense employs question rephrases belonging tothe easy question rephrase distribution. Editing modules.We edit the MLP down projection module within Layer 7 of LORA finetuning. Weapply LoRA with a rank of 1 and of 1. This enables a less aggressive editing approach in order to make theedit targeted and avoid damage to data points that were not meant to be deleted. Filtering details.We filter the details OK-VQA dataset before applying the automatic generation pipelineso as to to retain single token answers and to make sure that the model knows the answer before we delete itsimilar to that in (Patil et al., 2023a).",
  "CHuman Evaluation": "Motivated by prior work that involves evaluation of a multimodal summarization dataset (Patil et al., 2024),for our human evaluation experiments, we engaged four graduate research assistants with computer sciencebackgrounds. They were given a set of instructions for interpreting and performing the task. During virtualmeetings, we provided an overview of the dataset and a detailed task description. We collected for 80 of thedataset samples. The evaluation was conducted twice: once without manual filtering and once with manualfiltering. We compute accuracy of the annotation with respect to ground truth answers for an ideal dataset.We added three options to the answers to simplify the annotation task. We present the results in andscreenshots of the human evaluation interface in and .",
  "RandomNeigh ImgRephrase Img": ": Average distance of the random, neigh-borhood image and rephrase image points from theoriginal data point. Neighborhood points are closerto the target data point being deleted compared torandom points on which other unlearning datasetsevaluate specificity. Rephrase points are closer com-pared to both neighborhood and random data points.This is also reflected by higher Image Neighborhood-Acc compared to Random -Acc .",
  "RandomNeigh QuesRephrase Ques": ": Average distance of the random, neighbor-hood image and rephrase question points from theoriginal data point. Neighborhood points are closerto the target data point being deleted compared torandom points on which other unlearning datasetsevaluate specificity. Rephrase points are closer com-pared to both neighborhood and random data points.This is also reflected by higher Question Neighbor-hood -Acc compared to Random -Acc",
  "DAn Additional Method for Neighborhood Image Generation": "Motivated by prior work that involves evaluation of the model to invariant images (Patil et al., 2023b), inorder to get medium rephrase images, we use Grounded SAM to repaint the original image minimally to letthe answer to the original question become the alternate answer a. To get the target for Grounded SAM, weprompt Flan-T5-XXL (Chung et al., 2024) to get the subject from q that leads to the answer a. Then, we askGrounded SAM to repaint the subject in the image to the alternative answer a. These images are consideredthe hardest because the majority of the pixels remain the same, i.e., we expect them to lie within a smallerradius neighborhood compared to medium neighborhood images. While we tried this variant of generatingneighborhood images, we found that it is not possible to change the answer to a question in the context of theimage by editing just one object, especially for samples in OK-VQA that could involve multi-hop reasoning to",
  "Question:Is this aleisurelyorafastactivity?Answer:Leisurely": ": Examples of samples in UnLOK-VQA. Rephrase images with original questions are used forimage rephrase attack, Rephrase questions with original images are used for question rephrase attacks, andmultimodal rephrase attacks use a combination of rephrase questions and rephrase images. NeighborhoodImages with Original questions are used to compute Image Neighborhood -Acc and Neighborhood questionswith Original images are used to compute Question Neighborhood -Acc."
}