{
  "Abstract": "Grokking has been actively explored to reveal the mystery of delayed generalization andidentifying interpretable representations and algorithms inside the grokked models is asuggestive hint to understanding its mechanism. Grokking on modular addition has beenknown to implement Fourier representation and its calculation circuits with trigonometricidentities in Transformers. Considering the periodicity in modular arithmetic, the naturalquestion is to what extent these explanations and interpretations hold for the grokking onother modular operations beyond addition. For a closer look, we first hypothesize that (1) anymodular operations can be characterized with distinctive Fourier representation or internalcircuits, (2) grokked models obtain common features transferable among similar operations,and (3) mixing datasets with similar operations promotes grokking. Then, we extensivelyexamine them by learning Transformers on complex modular arithmetic tasks, includingpolynomials. Our Fourier analysis and novel progress measure for modular arithmetic,Fourier Frequency Density and Fourier Coefficient Ratio, characterize distinctive internalrepresentations of grokked models per modular operation; for instance, polynomials oftenresult in the superposition of the Fourier components seen in elementary arithmetic, but clearpatterns do not emerge in challenging non-factorizable polynomials. In contrast, our ablationstudy on the pre-grokked models reveals that the transferability among the models grokkedwith each operation can be only limited to specific combinations, such as from elementaryarithmetic to linear expressions. Moreover, some multi-task mixtures may lead to co-grokking where grokking simultaneously happens for all the tasks and accelerate generalization,while others may not find optimal solutions. We empirically provide significant steps towardsthe interpretability of internal circuits learned through modular operations, where analyticalsolutions are not attainable.",
  "Introduction": "Grokking is a late generalization phenomenon when training Transformer (Vaswani et al., 2017) and otherarchitectures with algorithmic data (Power et al., 2022) where training accuracy soon reaches 100% withlow test accuracy (often 0%), and after several iterations, test accuracy gradually reaches 100%. Grokkinghas been actively explored to reveal the mystery of delayed generalization, and identifying interpretablecircuits inside the grokked models should be a suggestive hint to understanding the grokking mechanismand dynamics. The interpretability analysis has mainly shed light on modular addition, where grokkingobtains the calculation with Fourier basis and trigonometric identities (Nanda et al., 2023; Zhong et al., 2023;Gromov, 2023; Rubin et al., 2023). Considering the periodicity in modular arithmetic, the natural question isto what extent these explanations and interpretations hold for the grokking on other modular operationsbeyond addition. For a closer look at the connections among the grokking phenomena in other modular operations, we firsthypothesize that (1) any modular operations can be characterized with unique Fourier representation or",
  "Published in Transactions on Machine Learning Research (11/2024)": "LimitationWe have observed all modular arithmetic operations that can cause grokking have showninterpretable trends with the Fourier basis. However, except for a few cases, we may not derive exactalgorithms. It remains as future works to derive the approximate solutions covering the other modularoperations beyond addition remain as future works. We also have examined a broader range of complexmodular arithmetic than prior works and obtained some implications to bridge the analysis gaps between thesynthetic and practical settings. However, our observations imply that the mechanism of grokking might notalways share the underlying dynamics with common machine learning. Further investigations of internalcircuits in practical models such as LLMs are important future directions.",
  "Related Work": "GrokkingGrokking has been actively studied to answer the questions: (1) when it happens, (2) whyit happens, and (3) what representations are learned. In simple algorithmic tasks like modular addition,grokking would be observed with proper weight decay and the ratio of train-test splits (Power et al., 2022;Lyu et al., 2023). In addition to synthetic data (Liu et al., 2023b), grokking could occur in more generalsettings such as teacher-student (Levi et al., 2023), NLP (Murty et al., 2023), computer vision (Thilaket al., 2022), or molecular graph tasks (Liu et al., 2023a), which could be explained with the dynamic phasetransitions during training (Rubin et al., 2023; Kumar et al., 2023) or mismatch between the train-test losslandscape against weight norm (Liu et al., 2023a). Recent findings have revealed that while grokking hasinitially been observed in neural networks (MLP and Transformer) with weight decay, it may also occur inGaussian processes and linear regression models (Levi et al., 2023; Miller et al., 2023) or even the case withoutweight decay (Kumar et al., 2023). Our work focuses on complex modular arithmetic including subtraction,multiplication, polynomials, and a multi-task mixture, and then empirically analyzes the difference betweengrokked and non-grokked modular operations. Several works have argued that the late generalization dynamics has been driven by the sparsification ofneural network emerging dominant sub-networks (Merrill et al., 2023; Tan & Huang, 2023) and the structuredrepresentations (Liu et al., 2022); the training process could be a phase transition divided into memorization,circuit formation, and cleanup phase (Nanda et al., 2023; Xu et al., 2023; Doshi et al., 2023; Davies et al.,2023; unkovi & Ilievski, 2022), and the formation of generalization circuits produces higher logits withsmall norm parameters than memorization circuits (Varma et al., 2023). The sparse lottery tickets in neural",
  "Preliminaries": "Grokking This paper focuses on grokking on the classification tasks from simple algorithmic data commonlyinvestigated in the literature (Power et al., 2022; Liu et al., 2022; Barak et al., 2022). We have train and testdatasets (Strain, Stest) without overlap, and learn a neural network f(x; ) where input x is a feature vectorof elements in the underlying algorithm space for synthetic data and are weights of neural network. Thesmall-size Transformers (e.g. one or two layers) or MLP are usually adopted as f. Specifically, prior workstrain the network using stochastic gradient decent over the cross-entropy loss L and weight decay:",
  ",": "where y {0, ..., p 1} is a scalar class label (p is a number of classes) correspond to the inputs x {0, ..., p 1} {0, ..., p 1}, and is a hyper-parameter controlling the regularization. The weight decay isone of the factors inducing the grokking phenomenon (Power et al., 2022; Liu et al., 2023a; Varma et al.,2023). We employ AdamW (Loshchilov & Hutter, 2019)) as an optimizer in practice. The fraction of trainingdata from all the combinations is defined as:",
  "Embed": ": Grokking has been inves-tigated with training from scratch.To shed light on the dynamics in-side Transformer, we introduce thenotion of pre-grokked models, whichare pre-trained on a similar task un-til grokking and used to replace ran-domly initialized modules withoutany parameter updates (i.e. frozen).We use pre-grokked embedding andTransformer in the later section. Experimental Setup In this paper, we expand the discussion aboveon modular addition to entire modular arithmetic: a b % p = c where represents arbitrary operations (or polynomials) that take two integers aand b as inputs, such as a b (subtract), a b (multiplication), 2a b, ab +b, a2 +b2, a3 +ab, (a+b)4 (polynomials) 3. Transformer takes three one-hottokens as inputs, a, , b. In addition to p integer tokens, we preparenop special tokens representing the mathematical operations above. Themodels are trained to predict c as an output. Our neural network is composed of a single-layer causal Transformerarchitecture () with learnable embedding and unembedding(demb = 128). We use ReLU for the activation functions and removepositional embedding, layer normalization, and bias terms for all the layers.We initialize each weight from Gaussian distribution, where the mean is0 and the standard deviation is1dout (LeCun et al., 2012). This Trans-former is trained via full batch gradient descent with AdamW (Loshchilov& Hutter, 2019) and weight decay = 1.0. We use p = 97 for all theexperiments. For the dataset faction, we use r = 0.3 unless otherwisementioned. We conduct the experiments with 3 random seeds and reportthe average4. Other hyper-parameters are described in Appendix C.",
  "Pre-Grokked Models and Fourier Metrics": "In contrast to modular addition, the exact analysis of internal circuits across other modular arithmetic wouldbe challenging, since not all the operations have analytical algorithms. To mitigate such interpretability issues,we introduce the notion of pre-grokked models, and propose a pair of novel progress measures for grokkingin modular arithmetic; Fourier Frequency Density (FFD) and Fourier Coefficient Ratio (FCR), which arederived from our empirical observation on sparsity and sinusoidal bias in embedding and neuron-logit map. Pre-Grokked Models To dive into the internal dynamics, we leverage pre-grokked models, which are pre-trained on similar algorithmic tasks until grokking and used for another training to replace randomly initializedmodules without any parameter updates (i.e. frozen). This allows us to consider learning representations andalgorithms separately. We will use pre-grokked embedding (freezing Wemb) and Transformer (freezing all theweights except for WembandWU) in later sections. We also use the same random seed between pre-grokkingand downstream grokking experiments to reduce identifiability issues (Singh et al., 2024). 3We omit the discussion on modular division, since it requires division into cases while we also consider a multi-task mixture.4We optimize the models up to 3e5 gradient steps and then judge whether grokking happens or not.",
  "Accuracy": "a b a b (mod p = 97) ScratchPre-Grokked EmbeddingPre-Grokked Transformerr = 0.3r = 0.5r = 0.7r = 0.9 : Test accuracy in modular elementary arithmetic (addition and multiplication) with pre-grokked models(embedding and Transformer) trained with subtraction. In addition to the plot in , we visualize the standarddeviation (shadow area).",
  "Optimization Steps": "0.00 0.20 0.40 0.60 0.80 1.00a3 + ab 2 + b (mod p = 97) Single-TaskMulti-Taskr = 0.3r = 0.5r = 0.7r = 0.9 : (Left) Test accuracy in grokking with a mixture of modular polynomials ({a + b, ab + b} and {a2 + b2, a2 +ab + b2, (a + b)2}). The multi-task training across similar operations promotes grokking. (Right) Test accuracy ingrokking with a mixture of modular polynomials ({(a + b)3, a3 + ab} and {(a + b)3, a3 + ab2 + b}). The multi-tasktraining across similar operations promotes the improvement of test accuracy.",
  "The low FCR means that Fourier representations of the weights have either cosine- or sine-biased components,which can be often observed in modular multiplication": "The decrease of either FFD or FCR (or both) correlates to the progress of grokking, and the responsibleindicator varies for each modular operation; for instance, FFD is a good measure for addition, and FCRis for multiplication. They are not only aligned with the late improvement in test accuracy but also cancharacterize each Fourier representation of modular operations at a certain layer (.3).",
  "Const": "cos 5 sin 9 cos 14 sin 18 cos 23 sin 27 cos 32 sin 36 cos 41 sin 45 cos 50 sin 54 Constcos 5sin 9 cos 14sin 18cos 23sin 27cos 32sin 36cos 41sin 45cos 50sin 54 a2 b b Component Frequency k 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 ab + a + b Norm of Fourier Component sincos Frequency k ab + a + b Norm of Fourier Component sincos",
  "cos(k(a b c)) = cos(k(a b)) cos(kc) + sin(k(a b)) sin(kc),": "and then we would anticipate similar interpretable representations to addition. However, we observe thatthe grokked models exhibit asymmetric properties for both embedding and Transformer. We transformthe embedding into a Fourier domain along the input dimension and compute the L2 norm along otherdimensions. In , subtraction learns similar embedding to addition with sparse Fourier components( and ). On the other hand, it imposes an asymmetric neuron-logit map and norms of logitswith cosine-biased components ( and ), which may represent alternatings (a b = b a). Such an asymmetry is also observed in grokked Transformers. The pre-grokked Transformer on subtractioncould not be transferred to any downstream elementary arithmetic ([1, :]), even subtraction itself(), and pre-grokked models with addition or multiplication could not learn subtraction as well([:, 4]). This implies that while we could interpret subtraction as a part of addition with negativenumbers, it is possible that the embedding and algorithm inside Transformer are quite different. Lastly, we examine the restricted loss and ablated loss in Appendix E, where the restricted loss is calculatedonly with the Fourier components of significant frequencies, and the ablated loss is calculated by removing acertain frequency from the logits. The analysis emphasizes the subtle dependency on other frequencies thansignificant ones.",
  "Modular Multiplication Leverages All Frequencies": "In contrast to modular addition and subtraction, we do not attempt to fully interpret the learned model in aclosed form or a form of pseudocode. However, following the empirical analysis in modular addition, we canobserve that multiplication also leverages the periodicity in the Fourier domain. reveals that multiplication obtains significantly different Fourier representation from addition orsubtraction ([2, :]); it employs all the frequencies equally with cosine bias for both embedding andneuron-logit map. Surprisingly, multiplication-pre-grokked Transformer accelerates grokking in addition() and addition-pre-grokked Transformer () causes grokking in multiplication. Thisimplies that in contrast to the asymmetry of subtraction, addition, and multiplication leverage their symmetryin the operations. Since the embedding of multiplication is quite different from addition and subtraction,it seems to be reasonable to fail to grok with addition/subtraction-pre-grokked embeddings ([0:2,2] and [2, 0:2]). Moreover, we find that grokking in elementary arithmetic occurs even with frozen randomembedding (see Appendix F; sampled from the same random seed) that does not have biased components norsparsity, which also supports that some unique, non-transferable patterns are learned in grokked models.",
  "Analysis in Polynomials": "It has been known that grokking would be less likely to occur as increasing the complexity of operators ingeneral (Power et al., 2022), but the underlying reasons or conditions are still unclear. In addition to elementaryoperations, we examine the interpretable patterns of grokked models in modular polynomials. We firstinvestigate the case of simple polynomials (.1), quadratic, cubic, and quartic expressions (.2).",
  "Polynomials Discover Superposition of Representations for Elementary Arithmetic": "We here investigate the relatively simple polynomials that induce grokking (univariate terms: a2 + b2, a2 b,a3 2b, the degree-1 with cross term: ab + a + b). In , grokking occurs even in quadratic or cubicexpressions asymmetric with input a and b, and suggests that the existence of symmetry or the cross termmight be a key for occurrence.",
  "High-Degree Factorization Allows Grokking": "Increasing the complexity of operators, we test modular polynomials with quadratic, cubic, and quarticformulas in . Apparently, Transformer fails to generalize in degree-n polynomials with cross term(a2 + ab + b2, a2 + ab + b2 + a, a3 + ab, a3 + ab2 + b). However, if polynomials are factorizable as a product ofaddition or subtraction (in the form of (a b)n), they easily grok, even when they also have cross terms (e.g.,(a + b)2 + a + b). Besides, the sum of powers is also easy to be grokked. Even, cubic ([1, 2:4]) orquartic ([1, 4:]) expressions, grokking occurs if they are factorizable. Comparing a2 + ab + b2 and(a + b)2 or a2 + ab + b2 + a and (a + b)2 + a + b emphasizes the importance of factorizability for the emergenceof grokking. analyzes the frequency components in factorizable polynomials. Non-factorizable operation (a2 +ab+b2) cannot find the sparse embedding representation. In contrast, factorizable operations promote grokking inboth quadratic ((a + b)2) and cubic expression ((a + b)3) obtaining sparsity in embedding. The factorizableoperations find more biased Fourier components than the non-factorizable ones in the neuron-logit map.Moreover, factorizable polynomials exhibit clear logits patterns as shown in elementary arithmetic (),while non-factorizable ones only show significant norms around a constant component.",
  "As shown in , we measure FFD and FCR in embedding layer WE for various modular operations.See Appendix G for the results in neuron-logit map WL": "Elementary ArithmeticAddition (red) and subtraction (blue) decrease FFD and keep a high FCR,whereas multiplication maintains FFD as 1.0 and decreases FCR (green). In all the cases, the saturationof accuracy and inflection point of either FFD or FCR almost match (vertical lines). Interestingly, ab + b(purple) exhibits decreasing both FFD and FCR, which reflects the feature of addition and multiplicationsimultaneously. Sum of Powers In an +bn, FFD and FCR exhibit the same progress as multiplication, while the neuron-logitmap has sparsity the same as addition (Appendix G). We also observe different behaviors depending on theparity of exponent n; FFD decreases more when n is odd (blue) and FCR drops more when n is even (red). Factorizable Polynomials(a + b)n exhibits the same trend as addition: high sparsity and balancedcomponents. In contrast, the neuron-logit map behaves similarly to multiplication (Appendix G). As in thesum of powers, the dynamics would be different depending on the parity of exponent n; FCR significantlydrops when n is even. In the case of non-factorizable a2 + ab + b2, FFD do not change during training, andthe model cannot achieve late generalization.",
  "Analysis in Transferability": "Since all the modular arithmetic has periodicity, we could hypothesize that grokked models obtain commonfeatures among similar operations (transferability). Furthermore, pre-grokked models in a certain taskcould promote grokking in other similar tasks because they already have a useful basis. We first test thetransferability of pre-grokked models from elementary arithmetic to linear expressions (.1), and thenextensively investigate it with higher-order polynomials (.2).",
  "Pre-Grokked Models Accelerate Grokking in Linear Expression": "We test whether frozen pre-grokked modules in elementary arithmetic (a + b, a b) are transferable togrokking in modular linear expression (2a b, 2a 3b, ab b). Those asymmetric expressions are hard togrok from scratch, especially if the fraction is small (r = 0.3) despite their simplicity. shows thatpre-grokked embedding with addition accelerates grokking in 2a b, 2a 3b, and pre-grokked Transformerwith multiplication does in ab b. These support our hypothesis and imply that in complex operations,internal circuits struggle with finding interpretable patterns.",
  "Pre-Grokked Models May not Help Higher-Order Polynomials": "In .1, we demonstrate that pre-grokked models accelerate grokking in linear expressions. We hereextensively test pre-grokked models in higher-order polynomials (quadratic and cubic). shows thatpre-grokked models could not accelerate, and they even prevent grokking in higher-order polynomials, whichimplies that pre-grokked models may not always help grokking accelerations, except for linear expressions.While the learned representation of polynomials seems to be a superposition of that of elementary arithmetic(e.g. .1), their functionalities might differ significantly.",
  "Analysis in Multi-Task Training": "While previous works on grokking have only dealt with a single task during training, the application ofTransformers such as large language models (Brown et al., 2020) is usually trained on a mixture of varioustasks or datasets. Given the periodicity and similarity across entire modular arithmetic, we also hypothesizethat mixing functionally similar operations in the dataset promotes grokking. To fill the gap betweensynthetic tasks and practice, we here investigate grokking on mixed datasets with addition, subtraction, andmultiplication (.1). We also study multi-task training mixing hard and easy polynomial operations(.2). We prepare r = 0.3 datasets and jointly train Transformers on their mixture.",
  "Multi-Task Mixture Discovers Coexisting Solutions": "reveals that co-grokking (i.e. grokking happens for all the tasks) occurs, but it requires a largerfraction of train dataset than a single task; for instance, r = 0.3 could not cause grokking while it does in. The test accuracy of multiplication increases slower than the other two, which implies the conflictamong different Fourier representations may affect the performance and generalization. For the Fourier analysis of grokked models, training with a multi-task mixture seems to discover Pareto-optimal representations for all the operations in embedding and neuron-logit map ([1, :]). We cansee the coexistence of component sparsity in embedding (addition), asymmetric cosine sparsity in neuron-logitmap (subtraction), and cosine-biased components for all the frequencies (multiplication). Furthermore, thenorms of logits in 2D Fourier basis for addition and subtraction exhibit the same patterns. This means thataddition and subtraction can be expressed on the same representation space originally, while they find quitedifferent grokked models after the single-task training.",
  "Proper Multi-Task Mixture also Accelerates Grokking in Polynomials": "We also investigate the multi-task training with the mixture of polynomials; preparing the combinationof easy and hard operations as {a + b, ab + b}, {a2 + b2, a2 + ab + b2, (a + b)2}, {(a + b)3, a3 + ab} and{(a + b)3, a3 + ab2 + b}. As shown in (left), a proper mixture of polynomials, in terms of operationsimilarity, also accelerates grokking in multi-task settings. For instance, a2+b2 and (a+b)2 help generalizationin a2 + ab + b2. This implies that the required representations among {a2 + b2, a2 + ab + b2, (a + b)2} wouldbe the same while original single-task a2 + ab + b2 fails to grok due to the difficulty in non-factorizable crossterm. The test accuracy also improves in the cubic expression (, right). However, it hits a plateaubefore the perfect generalization. The results imply that some multi-task mixtures may lead to co-grokking and accelerate generalization whileothers may not find optimal solutions. It would be an interesting future direction to further reveal thegrokking dynamics and mechanism for multi-task training.",
  "Conclusion": "Our empirical analysis has shed light on significant differences in internal circuits and grokking dynamicsacross modular arithmetic. The learned representations are distinct from each other depending on the typeof mathematical expressions. and despite the periodicity of modular arithmetic itself, the distinctive Fourierrepresentations are only obtained in the operations that cause grokking. While grokking can also happenwith complex synthetic data, we find that not all the insights are related to the nature seen in practicalmodels. For instance, the ablation with frozen pre-grokked modules demonstrates that the transferabilitycould only be limited to the specific combination of modular operations. The functional similarity between themathematical expressions may not help. In addition, some multi-operation mixtures may lead to co-grokkingand even promote generalization while others might not reach optimal solutions. We hope our extensiveempirical analysis encourages the community to further bridge the gap between simple synthetic data andthe data where analytical solutions are not attainable for a better understanding of grokked internal circuits.",
  "We thank Tadashi Kozuno for helpful discussion on this work. HF was supported by JSPS KAKENHI GrantNumber JP22J21582": "Ekin Akyrek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. What learning algorithmis in-context learning?investigations with linear models.In International Conference on LearningRepresentations, 2023. Boaz Barak, Benjamin L. Edelman, Surbhi Goel, Sham M. Kakade, eran malach, and Cyril Zhang. Hiddenprogress in deep learning: SGD learns parities near the computational limit. In Advances in NeuralInformation Processing Systems, 2022. Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, GretchenKrueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter,Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark,Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language modelsare few-shot learners. arXiv preprint arXiv:2005.14165, 2020.",
  "Darshil Doshi, Aritra Das, Tianyu He, and Andrey Gromov.To grok or not to grok: Disentanglinggeneralization and memorization on corrupted algorithmic datasets. arXiv preprint arXiv:2310.13061,2023": "Nouha Dziri, Ximing Lu, Melanie Sclar, Xiang Lorraine Li, Liwei Jiang, Bill Yuchen Lin, Peter West, ChandraBhagavatula, Ronan Le Bras, Jena D. Hwang, Soumya Sanyal, Sean Welleck, Xiang Ren, Allyson Ettinger,Zaid Harchaoui, and Yejin Choi. Faith and fate: Limits of transformers on compositionality. arXiv preprintarxiv:2305.18654, 2023. Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell,Yuntao Bai, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds,Danny Hernandez, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown,Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. A mathematical framework for transformercircuits. Transformer Circuits Thread, 2021. Nelson Elhage, Tristan Hume, Catherine Olsson, Nicholas Schiefer, Tom Henighan, Shauna Kravec, ZacHatfield-Dodds, Robert Lasenby, Dawn Drain, Carol Chen, Roger Grosse, Sam McCandlish, Jared Kaplan,Dario Amodei, Martin Wattenberg, and Christopher Olah. Toy models of superposition, 2022.",
  "Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya Sutskever. Deep doubledescent: Where bigger models and more data hurt. arXiv preprint arXiv:1912.02292, 2019": "Neel Nanda, Lawrence Chan, Tom Lieberum, Jess Smith, and Jacob Steinhardt. Progress measures forgrokking via mechanistic interpretability. In International Conference on Learning Representations, 2023. Pascal Jr. Tikeng Notsawo, Hattie Zhou, Mohammad Pezeshki, Irina Rish, and Guillaume Dumas. Predictinggrokking long before it happens: A look into the loss landscape of models which grok. arXiv preprintarXiv:2306.13253, 2023.",
  "Chris Olah, Nick Cammarata, Ludwig Schubert, Gabriel Goh, Michael Petrov, and Shan Carter.Zoomin:Anintroductiontocircuits.Distill,2020.doi:10.23915/distill.00024.001": "Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann,Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds,Danny Hernandez, Scott Johnston, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, DarioAmodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. In-context learning andinduction heads. Transformer Circuits Thread, 2022.",
  "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser,and Illia Polosukhin. Attention is all you need. arXiv preprint arXiv:1706.03762, 2017": "Jesse Vig, Sebastian Gehrmann, Yonatan Belinkov, Sharon Qian, Daniel Nevo, Yaron Singer, and StuartShieber. Investigating gender bias in language models using causal mediation analysis. In H. Larochelle,M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neural Information ProcessingSystems, 2020. Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama,Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, PercyLiang, Jeff Dean, and William Fedus.Emergent abilities of large language models.arXiv preprintarXiv:2206.08853, 2022.",
  "In this section, we describe the structure of causal Transformer in our work, loosely following the notation ofElhage et al. (2021)": "As defined in , we define embedding matrix as WE, query, key, and value matrices of j-th head in theattention layer as W jQ, W jK, W jV . The input and output layer at the MLP block is denoted as Win, Wout, andthe unembedding matrix is denoted as WU. We use ReLU for the activation functions and remove positionalembedding, layer normalization, and bias terms for all the layers. We also denote the token (one-hot representation of integers) in position i as ti, the initial residual streamon i-th token as x(0)i , causal attention scores from the last tokens (t2, because the context length is 3) toall previous tokens at j-th head as Aj, the attention output at j-th head as W jO, the residual stream afterthe attention layer on the final token as x(1), the neuron activations in the MLP block as MLP, and thefinal residual stream on the final token as x(2). Logits represents the logits on the final token since we onlyconsider the loss from it.",
  "Additiona + bSubtractiona bMultiplicationa bElementary Arithmeticall the above (+, , )": "Polynomialsa2 + b2, a3 + ab, (a + b)4, ... (including all the below)Linear Expression (degree-1)2a b, 2a + 3b, ab + b, ...Cross Termab, ab2, ...Quadratic Expression (degree-2)(a b)2, a2 + ab, a2 bCubic Expression (degree-3)(a b)3, ...Quartic Expression (degree-4)(a b)4, ...Factorizable Polynomials(a b)n, (a b)n (a b)k (n = 2, 3, ..., k < n)Polynomials with Cross Terma2 + ab + b2, a3 + ab2 + b, ...(Non-Factorizable Polynomials)Sum of Powersan + bn (n = 2, 3, ...)",
  "EAnalysis of Restricted Loss in Modular Subtraction": "In , we test the restricted loss and ablated loss, the metrics proposed by Nanda et al. (2023), wherethe restricted loss is calculated only with the Fourier components of key frequencies, and the ablated loss iscalculated by removing a certain frequency from the logits. The results show that modular subtraction hasseveral dependent frequencies, which cause worse restricted loss if ablated, while they are not key frequencies(we set the threshold to L > 1e 9). Those dependent frequencies are not observed in modular addition.Moreover, the restricted loss for modular subtraction significantly gets worse than the original loss, whichalso emphasizes the subtle dependency on other frequency components.",
  "where logits from residuals are estimated by subtracting logits of all the frequencies from the raw logits": "The results are presented in . In modular addition, we find that key frequencies contribute to theprediction and non-key frequencies only have a negligible effect on the loss (e.g. train loss v.s. ablation(d), restricted loss v.s. ablation (c)). The residuals actually hinder prediction accuracy (e.g., train lossv.s. ablation (c)). In modular subtraction, any ablations drop the performance and all the componentscontribute to the predictions, which implies that the grokked models in modular subtraction have informativerepresentations to some degree over all the frequencies, even residuals in the logits.",
  "Original": "k log Loss Loss after Ablating Frequencies (Subtraction) Key FreqDependent FreqOther FreqRestricted LossOriginal Loss : Loss of Transformer when ablating each frequency (k = 1, ..., 48) and everything except for the keyfrequencies (restricted loss). In modular subtraction, we find several dependent frequencies (orange), which causeworse restricted loss if ablated while they are not key frequencies.",
  "FGrokking with Frozen Random Embedding": "We here show that even if the sparsity and non-trivial biases are not realizable in embedding, grokking couldoccur in . In this experiment, we initialize embedding weights from Gaussian distribution, wherethe mean is 0 and the standard deviation is1dout (LeCun et al., 2012), and then freeze them not allowingany parameter updates during training. Even with the restricted capacity, the models exhibit grokking inelementary arithmetic. 010203040500.0 0.2 0.4 0.6 0.8 1.0 1.2 a + b Norm of Fourier Component Fourier Components of Embedding Matrix sincos a + b Norm of Fourier Component Fourier Components of Neuron-Logit Map sincos",
  "GFFD and FCR in Neuron-Logit Map": "presents our progress measures: FFD and FCR in neuron-logit map WL. For elementary arithmeticoperators, the dynamics seem to be the same as seen in embedding (). This might be due to thesimilarity of embedding and neuron-logit map (). For sum of powers (an + bn) and the factorizable((a + b)n) behaves differently from embedding (). The sum of powers decreases FFD while keepingFCR relatively higher. The factorizable polynomials maintain both FFD and FCR relatively higher. Thismight be due to the representation asymmetry between embedding and neuron-logit map in polynomials().",
  "HDetailed Analysis on FFD and FCR": "In this section, we provide experiments to measure the FCR and FFD with different train dataset fractions ror threshold . As shown in (above), if we change the dataset ratio, the needed optimization stepsto be grokked are also changed, and the inflection point of the corresponding progress measure is changed too.For instance, when we change r = 0.3 with r = 0.5 in a + b and a b, the decrease of FFD is also acceleratedalong with the increase of test accuracy. We also observe that if the grokking does not happen, the progressmeasure does not exhibit the change, such as the case of FFD in ab + b with r = 0.3. As for different threshold , demonstrates that lower delays the decrease of the metrics in contrastto the progress of grokking; for instance, the decrease of the metrics starts after test accuracy reaches 1.0 (inthe case of a + b, a b, ab + b). On the other hand, larger results in too fast convergence. Based on thoseobservations, we have set = 0.5 in this paper.",
  "J.1Periodic Patterns in Fourier Components are Common Characteristics": "We here provide the ablation study with different modulo p = 59, 113 (from to ). Theseresults show that, basically, our findings and observed trends in the main text (done with p = 97) can be seenin different modulo p as well. The periodic patterns in Fourier components can be common characteristicsamong different p in most cases. 1001011021031040.00 0.25 0.50 0.75 1.00",
  "J.2Grokking Can be a Function of Modulo p": "In addition to mathematical operation and dataset fraction, grokking can be a function of modulo p. shows that p = 97 causes grokking with a3 + ab, while p = 59 and p = 113 do not. Surprisingly, p = 59 hasfewer combinations than p = 97, but p = 59 does not generalize to the test set even with r = 0.9. The resultssuggest that we might need to care about the choice of p for grokking analysis.",
  "KDataset Distribution Does not Have Significant Effects": "One possible hypothesis why some modular polynomials are hard to generalize is that some polynomials biasthe label distribution in the dataset. To examine this hypothesis, we calculate several statistics on labeldistribution in the dataset. We first randomly split train and test dataset (r = 0.3), and get categoricallabel distributions. We compute the KL divergence between train label distribution dtrain and test labeldistribution dtest, train label entropy, and test label entropy, averaging them with 100 random seeds. shows KL divergence between train and test datasets (top), train dataset entropy (middle), andtest dataset entropy (bottom). While those values slightly differ across the operations, there are no significantdifference between generalizable (e.g. a3 + b3, a2 + b2) and non-generalizable (e.g. a3 + ab, a2 + ab + b2)polynomials despite their similarity. The results do not imply that dataset distribution has significant impactson grokking. 0.00 0.02 12(DKL(dtrain|dtest) + DKL(dtest|dtrain)) 0.00 0.02 0.04 dtrainlogdtrain a + b a b a b a2 + b 2 a2 b 2 a2 + b a2 b a3 + 2b a3 2b ab + a + b ab + b ab b 2a + b 2a b 2a + 3b 2a 3b a2 + ab + b 2 (a + b)2 (a + b)2 + a + b (a b)2 (a + b)3 (a b)3 (a + b)4 (a b)4 a3 + ab a3 + ab 2 + b a3 + b 3 a4 + b 4 a5 + b 5 a6 + b 6 a7 + b 7 (a + b)5 (a + b)6 (a + b)7 0.00 0.02 0.04 dtestlogdtest",
  "LExtended Limitation": "Our work extends the grokking analysis from simple modular addition to complex modular polynomials.However, those tasks are still synthetic and far from LLMs (Brown et al., 2020), the most popular applicationof Transformers. Connecting grokking phenomena or mechanistic interpretability analysis into the emergentcapability (Wei et al., 2022), or limitation in compositional generalization (Dziri et al., 2023; Furuta et al.,2023) and arithmetic (Lee et al., 2023) would be interesting future directions.",
  "MInitialization and Pre-Grokked Models": "To clarify that our experimental observations do not trivially come from random initialization, we providea detailed version of in , which has a visualization of standard deviation among threerandom seeds (shadow area). These results show that the overlap is small and thus our observations areconsistent and do not come from the initialization or variance. 1001011021031040.00 0.25 0.50 0.75 1.00",
  "NIdentifiability and Pre-Grokked Models": "Through the experiments with pre-grokked models, we observe that some pre-grokked models preventgrokking in certain combinations. However, Singh et al. (2024) point out that freezing a subset of modelsmay suffer from identifiability issues, which may not prompt the downstream performance. To mitigate thepotential issues, our experiments use the same random seed between pre-grokking and downstream grokkingexperiments. Moreover, we observe that the combinations that cannot accelerate grokking have quite differentFourier components (for instance, modular addition and multiplication as seen in ) in the weightmatrix, while the rotating operations to the weight matrix do not change the Fourier basis. We think thatour claims are not just due to the identifiability issues."
}