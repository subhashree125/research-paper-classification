{
  "Abstract": "This paper revisits the robust overfitting phenomenon of adversarial training. Observingthat models with better robust generalization performance are less certain in predictingadversarially generated training inputs, we argue that overconfidence in predicting adversar-ial examples is a potential cause. Therefore, we propose a formal definition of adversarialcertainty that captures the variance of the models predicted logits on adversarial examplesand hypothesize that generating adversarial examples after the optimization of decreasingadversarial certainty improves robust generalization. Our theoretical analysis of syntheticdistributions characterizes the connection between adversarial certainty and robust generaliza-tion. Accordingly, built upon the notion of adversarial certainty, we develop a general methodto search for models that can generate training-time adversarial inputs with reduced certainty,while maintaining the models capability in distinguishing adversarial examples. Extensive ex-periments on image benchmarks demonstrate that our method effectively learns models withconsistently improved robustness and mitigates robust overfitting, confirming the importanceof generating less certain adversarial examples for robust generalization. Our implementationsare available as open-source code at:",
  "Introduction": "Deep neural networks (DNNs) have achieved exceptional performance and have been widely adopted in variousapplications, including computer vision He et al. (2016), natural language processing Devlin et al. (2019) andrecommendation systems Covington et al. (2016). However, DNNs have been shown highly vulnerable toclassifying inputs, known as adversarial examples Szegedy et al. (2014); Goodfellow et al. (2015), craftedwith imperceptible perturbations that are designed to trick the model into making wrong predictions. Theprevalence of adversarial examples has raised serious concerns regarding the robustness of DNNs, especiallywhen deployed in security-critical applications such as self-driving cars Chen et al. (2015), biometric facialrecognition Komkov & Petiushko (2021) and medical diagnosis Finlayson et al. (2019); Ma et al. (2021). Toimprove the resilience of deep neural networks against adversarial perturbations, numerous defenses havebeen proposed, such as distillation Papernot et al. (2016), adversarial detection Ma et al. (2018), featuredenoising Xie et al. (2019), randomized smoothing Cohen et al. (2019), and semi-supervised methods Alayracet al. (2019). Among them, adversarial training Madry et al. (2018); Zhang et al. (2019) is by far the mostpopular approach to learn robustness against adversarial perturbations. Nevertheless, even the state-of-the-artadversarial training methods Croce et al. (2020); Rebuffi et al. (2021); Wang et al. (2023) cannot achievesatisfactory robustness performance on simple classification tasks like classifying CIFAR-10 images.",
  "Published in Transactions on Machine Learning Research (10/2024)": "Potential Limitation. depicts that applying DAC earlier can achieve better robustness. Thus,DAC101 and DAC151, decreasing adversarial certainty when robust overfitting happens, cannot achievecomparable robustness on the best epoch with DAC1 that works from scratch. That is, our method requiresa warm-up instead of an immediate functionality. Consequently, our method might not be suitable for thereal-time scenario, where DAC is deployed only when robust overfitting happens, which limits the flexibilityof our method.",
  "Notation. We use lowercase boldfaced letters for vectors, and 1() for the indicator function. For any x Rd": "and i {1, 2, . . . , d}, let xi be the i-th element of x. For any finite-sample set S, let |S| be the cardinalityof S. Let (X, ) be a metric space, where : X X R denotes a distance metric. For any x Xand 0, let B(x; ) = {x X : (x, x) } be the ball centered at x with radius and metric .When is free of context, we simply write B(x) = B(x; ). Let be a probability distribution on X Y,where Y denotes a label space. The empirical distribution of with respect to a sample set S is definedas: S(C) = (x,y)S 1(x, y) C/|S| for any measurable set C X Y. Let N(, 2) be the Gaussiandistribution with mean and standard deviation > 0.",
  "Overconfidence Compromises Robustness": "In this section, we first introduce the most relevant concepts, including adversarial robustness, adversarialtraining and robust overfitting, of which the complete introduction and discussion are detailed in Appendix A.Next, we visualize the label predictions of adversarially trained models by heatmaps, and propose ourhypothesis that model overconfidence is a potential cause of the decreased robust generalization in adversarialtraining, where robust overfitting occurs.",
  "R(f; ) = 1 E(x,y)maxxB(x) 1f(x) = y,": "where f is an arbitrary classifier, denotes the underlying data distribution, and 0 captures the adversarialstrength. In practice, adversarial robustness estimated based on a set of testing examples R(f; Ste) istypically used as the evaluation metric for measuring the robust generalization of f. Adversarial trainingaims to improve model robustness by training on adversarially-perturbed inputs Goodfellow et al. (2015);Madry et al. (2018); Zhang et al. (2019), which can be formulated as a min-max optimization problem:",
  "(x,y)StrmaxxB(x) Lf, x, y,(1)": "where represents the model class, Str is a set of training examples independently and identically sampledfrom , and L denotes some convex surrogate loss such as cross-entropy. Note that PGD attacks Madryet al. (2018) are typically employed in adversarial training to provide approximated solutions to the innermaximization problem in Equation (1). Nevertheless, PGD-based adversarial training and its variants Madry",
  ": Model confidence in pre-dicting training-time adversarial ex-amples conditioned on the ground-truth class label using different met-rics: (a) label-level variance, and (b)adversarial certainty": "et al. (2018); Zhang et al. (2019) suffer from the robust overfitting phenomenon Rice et al. (2020): Thetest-time robustness of intermediate models produced during the training process sharply increases afterthe first learning rate decay but keeps decreasing afterward. As a result, the model produced from the lasttraining epoch cannot achieve a satisfactory robust generalization performance. Heatmap Visualizations. To gain a deeper understanding of robust overfitting, we visualize the heatmapsof the label predictions for adversarially-perturbed CIFAR-10 images. Given that robust overfitting capturesthe gap of robust generalization performance with respect to models produced at the last and best epochs,we first plot Figures 1(b) and 1(d). Since only the training process is accessible in adversarial training,we also depict the corresponding training-time heatmaps in Figures 1(a) and 1(c). Here, the ground-truthlabel represents the underlying class of clean images and the predicted label denotes the class of adversarialexamples predicted by the corresponding model. More experimental details about are providedin Appendix B. Specifically, when comparing Figures 1(a) and 1(c), we find that the predictions of LastModel mainly concentrate on the ground-truth class, which means the model is overconfident in predictingadversarial examples generated by itself. In contrast, the heatmap of Best Model, which achieves betterrobust generalization performance, depicts less overconfidence. Moreover, by comparing the same modelbetween the training and testing time, i.e., (a) versus (b) and (c) versus (d),we discover that the train-test gap is significantly smaller with respect to the Best Model. We note that thisresult is aligned with the classical machine learning theory: If the testing distribution deviates more from thetraining distribution, standard learners will show a decreased generalization performance. According to the above findings, we hypothesize that the overconfidence property is detrimental to robustgeneralization. To be more specific, if the model cannot generate perturbed training inputs with sufficientuncertainty, the model will not be able to well predict the less certain adversarial examples during the",
  "Introducing Adversarial Certainty": "To numerically summarize our findings from the heatmaps, we measure the variance of the class probabilitiesof the predicted labels, denoted as label-level variance, with respect to the training-time adversarial examplesfor each ground-truth category in (a). A lower label-level variance indicates the prediction confidencesof different labels are closer, which corresponds to less certainty. More specifically, we observe that the BestModel with better robust generalization performance exhibits a lower label-level variance than that of theLast Model, which is consistent with the results illustrated in . Even though the label-level variancecan characterize how certain the training-time generated adversarial examples are, such statistics are on aclass level, which is not easy for optimization. Thus, we propose the following logit-level definition, termed asadversarial certainty, to capture the certainty of a model in classifying the adversarial examples generated byitself, where a lower score of adversarial certainty suggests the model has a stronger ability to generate lesscertain adversarial inputs: Definition 1 (Adversarial Certainty). Let X be the input space and Y = {1, 2, . . . , m} be the label space.Suppose is the underlying distribution and S is a set of sampled examples. Let 0, be the perturbationmetric. For any f : X Y, we define the adversarial certainty of f as:",
  "k[m] (uk u)2/m,with uk and u denoting the k-th element and mean of u Rm respectively": "Different from label-level variance, adversarial certainty is an averaged sample-wise metric, which calculates thevariance of the logits returned by the model f for each adversarially-perturbed example A(x; y, f, ). Similarto (a), we visualize the adversarial certainty of the Best Model and the Last Model in (b).Since predicted labels are decided by the class with the highest predicted probabilities, adversarial certaintydepicts a similar pattern to the label-level variance as expected. Based on Definition 1, our hypothesis canthen be specifically formulated as:",
  "Decreasing adversarial certainty during adversarial training can improve robust generalization": "We note that there also exist other alternative metrics, such as confidence and entropy, which can capturea models certainty in predicting adversarial examples and summarize the observations of the heatmapsdepicted in . As will be discussed in .1, we choose logit-level variance as the metric todefine adversarial certainty, mainly because our DAC method illustrated in always achieves thebest robust generalization performance with such a choice. Theoretical Analysis. To better understand the proposed definition of adversarial certainty, we further studyits connection with robust generalization using synthetic data distributions. Following existing works Tsipraset al. (2019); Wei et al. (2023), we assume the following data generating procedure for any example (x, y) :The binary label y is first sampled uniformly from Y = {1, +1}, then the robust feature x1 = y with samplingprobability p and x1 = y otherwise, while the remaining non-robust features x2, , xd+1 are sampled i.i.d.from the Gaussian distribution N(y, 1). Here, p (1/2, 1) and < 1/2 is a small positive number. FollowingWei et al. (2023), we consider linear SVM classifiers: fw(x) = sgn(x1 + x2++xd+1 w) with w > 0, where sgn()denotes the sign operator. Subsequently, we assume all the adversarial examples x are sampled from thefollowing adversarial distribution adv() with > 0: x1 = x1, and x2, , xd+1i.i.d. N( )y, 1. Detaileddiscussions about the configurations of this synthetic robust classification task are provided in Appendix C.",
  "Theorem 1. Consider the aforementioned data distribution and robust classification task. Let te (, 2)and fw be an arbitrary SVM classifier with w > 0. For any [ w": "d , ], AC(fw; , adv()), the adversarialcertainty of fw, is monotonically decreasing with respect to . Suppose we conduct one-step gradient update onw using adversarial examples sampled from adv(): w = w+wR(fw; adv()), where > 0 stands for thelearning rate. Then, R(f w; adv(te)), the robust generalization performance of f w, also increases as increases. Note that, since we consider the adversarial data distribution adv instead of p perturbations, we nowgeneralize the notion of adversarial certainty and robust generalization correspondingly. Theorem 1 suggeststhat if we decrease the certainty of the adversarial examples sampled from adv(), the robustness of theSVM classifier f w will increase after one-step gradient update based on the sampled adversarial examples,confirming the importance of less certain adversarial examples for robust generalization. We remark thatour theoretical analysis can also be extended to the typical setting of -norm bounded perturbations. InAppendix C.2, we show that considering perturbations is equivalent to considering the adversarial datadistribution of x1 = x1 y and x2, , xd+1 i.i.d. sampled from N(( )y, 1) for any w > 0, and derivesimilar results to Theorem 1.",
  "Decreasing Adversarial Certainty Helps Robust Generalization": "Previous sections illustrate why decreasing the certainty of adversarial inputs used for adversarial training isbeneficial for robust generalization. To further validate our hypothesis, this section proposes a novel methodto explicitly Decrease Adversarial Certainty (DAC) based on adversarial training. In particular, DAC isdesigned to find less certain adversarial examples that are used to improve robust generalization, which aimsto solve the following optimization problem:",
  "(x,y)StrmaxxB(x) Lf, x, y, where = argminC()AC(f; Str, A).(2)": "Str is the clean training dataset, A denotes a specific attack method (e.g., PGD attacks Apgd), and C()represents the feasible search region for . We remark that imposing the constraint of C() is necessary,because the goal of DAC is to improve robust generalization of adversarial training, instead of merely obtainingadversarial certainty as low as possible. Without such a constraint, minimizing adversarial certainty willcause to significantly deviate from the initial . This will render the adversarial examples generated withrespect to less useful, thereby inducing a negative impact on robust generalization (see (a) and ourcorrelation analysis for more discussions regarding the design choice of imposing such a constraint set). Directly solving the min-max-min problem introduced in Equation (2) is challenging, due to the non-convexnature of the optimization and the implicit definition of C(). Thus, we resort to gradient-based methods foran approximate solver. To be more specific, we take the t-th iteration of adversarial training as an exampleto illustrate our design of DAC. Given a set of clean training examples Str, a specific attack method A, and aclassification model f, our DAC method can be formulated as a two-step optimization:",
  "t+1 = t+0.5 Lrob(f; Str, A)=t+0.5,(3)": "where > 0 and > 0 represent the step sizes of the two optimization steps, AC(f; Str, A) denotes theadversarial certainty of f with respect to Str and A, and Lrob(f; Str, A) can be roughly understood as therobust loss except that the inner maximization is approximated using some attack method such as Apgd. Thefirst step in Equation (3) optimizes the adversarial certainty, which adjusts the model parameters t in adirection such that the generated training-time adversarial examples are less certain, whereas the second stepin Equation (3) optimizes the models ability in distinguishing adversarial examples generated by the modelitself as in standard adversarial training.",
  "Does decreasing adversarial certainty always induce better robust generalization?": "Recall that in Equation (2), C() defines the feasible region for optimizing adversarial certainty. Therefore,the answer would be affirmative within this region, i.e., decreasing adversarial certainty will increase testrobust accuracy. To support the answer to this question with evidence, we conduct a correlation analysisbetween adversarial certainty and robust generalization. The results are illustrated in (a). Specifically,we use an AT-trained model as the starting point, from which the heatmaps in are derived. Then,we respectively update the model with one more epoch using DAC with different step sizes, ranging from0.1 to 2.0, in the t t+0.5 step of Equation (2) to decrease adversarial certainty. Finally, we measure thetraining-time adversarial certainty (i.e., the blue bars) and robust test accuracy (i.e., the orange curve) ofthe result models. (a) shows that adversarial certainty keeps decreasing as the step size increases.Meanwhile, the models robustness first keeps improving but then decreases when the step size is beyond thevalue of 1.3. This result suggests that if the model parameters lie in the feasible search region with a properlyselected step size, lower adversarial certainty leads to higher test robust accuracy. However, when the model isout of the feasible search region, decreasing adversarial certainty will no longer improve robust generalization. The above investigation suggests a negative answer to the question by performing one-epoch optimization withdifferent step sizes on the last model of AT, i.e., the model of Figures 1(a) and 1(b). Here, we demonstratethat our finding is indeed derived from the changes of adversarial certainty but not from the one-epochsetting. To be more specific, we separate one step of decreasing adversarial certainty with size 0.7 into tsteps, i.e., each step corresponds to the size of 0.7/t, where t = {1, 2, 3, 4, 5, 6, 7, 8, 9, 10}. The results oftraining-time adversarial and testing-time robust generalization are depicted in (b), where we canobserve a consistent pattern with (a) that robust generalization first gains more improvements butthen less with the decrease of adversarial certainty. Moreover, we apply the one-epoch optimization on the best AT model that corresponds to Figures 1(c) and1(d) to derive more observations. From (c), we can find that the changes of robust generalizationwith the increase of adversarial certainty depict a different pattern from (a). First, the robustgeneralization improvements are slighter than those in the last model. Besides, there is no trend of robustgeneralization decreasing even if the step size arrives at 2.0. These results suggest that the feasible regionof the best model is larger than that of the last model, which suffers from more severe robust overfitting.Consequently, when applying our DAC method, an overconfident model is supposed to select the optimizationstep size carefully; otherwise, the selection could be more ambitious.",
  "Experiments": "This section examines the performance of our DAC method under perturbations with = 8/255 on variousmodel architectures, including PreActResNet-18, denoted as PRN18, and WideResNet-34, denoted as WRN34.And we train a model for 200 epochs using SGD with a momentum of 0.9. Besides, the initial learning rateis 0.1, and is divided by 10 at the 100-th epoch and at the 150-th epoch. The adversarial attack used intraining is PGD-10 with a step size of 1/255 for SVHN, and 2/255 for CIFAR-10 and CIFAR-100, while we",
  "+ DAC84.69 (80.09)52.00 (59.31)51.32 (59.26)49.50 (53.02)47.65 (51.48)": "utilize the commonly-used attack benchmarks of PGD-20 Madry et al. (2018), PGD-100 Madry et al. (2018),CW Carlini & Wagner (2017) and AutoAttack Croce & Hein (2020) for evaluation. In addition, we measurethe Clean performance to investigate the influence on clean images. Regarding other hyperparameters, wefollow the settings described in their original papers. In all cases, we evaluate the performance of the last(best) model in terms of testing-time robust accuracy. In .1, we evaluate the effectiveness of our DAC method in improving robust generalization on threewidely-used benchmark datasets: CIFAR-10 Krizhevsky & Hinton (2009), CIFAR-100 Krizhevsky & Hinton(2009) and SVHN Netzer et al. (2011) based on three baseline adversarial training methods: AT Madry et al.(2018), TRADES Zhang et al. (2019) and MART Wang et al. (2020). To study the generalizability of ourmethod, we further conduct experiments under 2 perturbations, where we set = 128/255 with a step sizeof 15/255 for all datasets. In .2, we associate with other robustness-enhancing techniques to furtherinvestigate the effect of adversarial certainty in adversarial training. Finally, we demonstrate the efficacyof DAC under a simplified one-step optimization setting in .3, and improve the DAC efficiency byregularizing decreasing adversarial certainty in a loss term, i.e., DAC_Reg that will be detailed in .4.",
  "Main Results": "We first evaluate the robust generalization performance of our proposed DAC method on the benchmarkCIFAR-10 image dataset. The comparison results are depicted in , showing that DAC significantlyenhances model robustness across different adversarial attacks, such as PGD attacks Madry et al. (2018),CW attacks Carlini & Wagner (2017) and AutoAttack Croce & Hein (2020). These results demonstrate theeffectiveness of DAC, indicating the significance of generating less certain adversarial examples for robustgeneralization. Besides, we observe that although WRN34 suffers from more severe robust overfitting usingbaseline adversarial training methods, it achieves more robustness improvement by our method. This suggeststhat WRN34 is superior to PRN18 in terms of robust generalization with the help of DAC. In addition toadversarial robustness, it is also worth noting the effect of DAC on clean test accuracy, which captures thestandard generalization ability of the model. reveals that DAC consistently improves the clean testaccuracy under all experimental settings. This promotion shows that DAC could also help models gain bettergeneralization performance on unseen clean images even by learning from adversarial examples. The resultsthat include evaluations on more benchmark datasets (i.e., SVHN and CIFAR-100) are depicted in Tables 2and 3, of which the full versions are shown in Tables 7 and 8 (Appendix E) due to space limit, showing asimilar pattern of improvements. Moreover, we empirically study the impact of DAC on the phenomenon of robust overfitting. More specifically,we evaluate the gap of testing-time adversarial robustness between the best and the last models. The results",
  "AT66.4563.2066.0265.1839.2335.68+ DAC69.1167.4469.1067.3740.7536.32": "are shown in (a), where DAC consistently mitigates robust overfitting across different settings. Theseresults indicate that decreasing adversarial certainty can successfully mitigate robust overfitting. Besides, wealso measure the adversarial certainty gap between the best model and the last model produced by AT and AT-DAC in (b). It can be observed that the adversarial certainty gap of AT-DAC is significantly smallerthan that of AT, which is aligned with the closer adversarial robustness of the best model and the last model. Comparison with Other Metrics. Recall our discussions in , we propose the notion of adversarialcertainty based on logit-level variance (Definition 1), which is further used in our design of DAC. Noticing thatconfidence and entropy are also relevant metrics that can capture the models overconfidence in predictingadversarial examples, we conduct a case study to illustrate why we choose to define adversarial certaintybased on variance. For ease of presentation, we only present results on CIFAR-10 and AT as an illustration,where similar trends are observed among other settings. reports the test-time adversarial robustnessof models learned using AT-DAC with different metrics used in the definition of adversarial certainty. Wecan see that the last and best models produced using our method with the variance metric achieve the bestrobustness performance, which empirically supports our design choice.",
  "Effect of Adversarial Certainty on Other Robustness-Enhancing Techniques": "We note that several recent works also focus on understanding robust generalization and developing methodsto improve adversarial training, including adversarial weight perturbation Wu et al. (2020) (AWP), andconsistency regularization Tack et al. (2022) (Consistency). More concretely, Wu et al. discovered thatthe flatness of the weight loss landscape is an important factor related to robust generalization Wu et al.(2020).And the method of Consistency regularizes the adversarial consistency based on various dataaugmentations Tack et al. (2022). However, since these methods focus on different strategies to improverobust generalization, it is unclear whether our proposed adversarial certainty has any connection with them.Therefore, we study the changes in adversarial certainty when involving AWP and Consistency in adversarialtraining, respectively, which are shown in . Surprisingly, we find that AWP and Consistency, whichimprove the robust generalization of AT on both the last and best models, can gain lower adversarial certainty.These findings are consistent with the idea behind our DAC method decreasing adversarial certainty helpsrobust generalization. In other words, AWP and Consistency, which are designed toward their specifieddirections, will implicitly decrease adversarial certainty. Note that, even if AWP and Consistency haveinfluences on adversarial certainty, it does not mean that our work proposes a similar concept to them.Specifically, adversarial certainty is derived by observing an adversarial-training-unique phenomenon robustoverfitting, meanwhile, AWP is inspired by the theory of weight loss landscape from standard learning andConsistency considers the augmentation scope. Consequently, our proposed adversarial certainty is a crucialproperty in adversarial training, which can either explicitly or implicitly affect robust generalization. As AWP and Consistency can implicitly improve adversarial certainty, we then investigate the compatibility ofour DAC method with AWP and Consistency by a naive attempt. To incorporate DAC in AWP, we add a step",
  ": Visualization results for comparing the adversarial certainty and robust generalization of differentadversarial training methods with and without the involvement of DAC": "before weight perturbation to optimize the certainty of adversarial examples. Then the updated intermediatemodel is used to generate new adversarial examples for the following AWP optimization. Similarly, wefirst explicitly update the adversarial certainty on augmented samples, and then follow the Consistencyoptimization. The results are shown in . As expected, since AWP and Consistency have alreadyimplicitly decreased adversarial certainty, even if DAC conducts an explicit optimization, our method can onlygain limited benefit. Nevertheless, our repeated trials demonstrate that the improvements, even slight, areindeed derived from our method rather than randomness. Further, we conduct a significance test, which showsthat the improvements of robust generalization on AWP and Consistency are statistically significant, as fullypresented in Appendix D. The goal of our work is to propose adversarial certainty and clarify its significancein adversarial training, thus better designs of involving adversarial certainty in existing robustness-enhancingstrategies are left as future work.",
  "Further Discussion on DAC": "Based on previous results, we demonstrate the benefits of involving our DAC method in adversarial training.To more intuitively demonstrate the efficacy of our method, we empirically measure the performanceimprovements derived by conducting DAC for a single epoch starting with different models. First, we train asequence of models by AT and TRADES for 200 epochs, and by MART for 120 epochs, respectively. Forevery 20 epochs, we then update the same intermediate model by one further epoch using each of the threeadversarial training methods with and without the help of DAC. Finally, we measure the adversarial certaintyand robust generalization for all the updated models. summarizes the results, where the blue colorrepresents the original method without DAC and orange corresponds to results with our DAC. The bars showadversarial certainty and the curves depict robust generalization. It can be seen from that startingwith different intermediate models, DAC can consistently gain less certain adversarial examples, from whichthe updated model attains better robust generalization performance, which is aligned with our theoretical",
  "Improvement on DAC Efficiency": "To gain a better understanding of our method, we explicitly examine our proposed adversarial certainty byinvolving two steps for each iteration i.e., DAC, as formulated in Equation (3). In this section, we propose amore efficient method, denoted as DAC_Reg, by regularizing the optimization of adversarial certainty as aterm in adversarial training loss. More concretely, the optimization problem with the additional regularizercan be cast as:",
  "(x,y)StrLf, x, y+ AC(f; Str, A),": "where > 0 denotes the trade-off parameter between the regularization of adversarial certainty and therobust loss. Similar to adversarial training, the model parameters are iteratively updated using stochasticgradient descent (SGD) with respect to the regularized robust loss. Benefiting from the regularization design,DAC_Reg requires similar training time to the standard adversarial learning, which is 0.56 of that of DAC.For instance, for a PRN18 model of AT and CIFAR-10 on a single NVIDIA A100 GPU, DAC averagely costs143s for each training epoch while DAC_Reg costs 80s. The comparison results of AT, TRADES and MARTmodels on SVHN, CIFAR-10 and CIFAR-100 datasets are shown in and . We can see thatDAC_Reg achieves comparable performance, due to the additional penalty on adversarial certainty, whichis only a bit inferior to DAC. In a few cases, DAC could bring better and more stable improvements. Forinstance, when a PRN18 model is trained on CIFAR-100 by AT, DAC_Reg can only gain the improvementon the last epoch but not on the best epoch. In addition, we measure the adversarial certainty of a sequenceof models trained by AT, DAC and DAC_Reg, respectively, in (c). We observe that DAC gains thelowest adversarial certainty with a slight advantage over DAC_Reg, again indicating that lower adversarialcertainty corresponds to higher robust generalization.",
  "Conclusion and Future Work": "We revisited the robust overfitting phenomenon of adversarial training and argued that model overconfidencein predicting training-time adversarial examples is a potential cause. Accordingly, we introduced the notion ofadversarial certainty to capture the degree of overconfidence and designed a strategy to decrease adversarialcertainty for models produced during adversarial training. Experiments on image benchmarks demonstratethe effectiveness of our method, which confirms the importance of generating less certain adversarial examplesfor robust generalization. Our work aims to gain a better understanding of robust generalization throughobservations from robust overfitting. We believe our work provides a significant contribution to advancingthe field of adversarial machine learning, which might inspire practitioners to look into the important role ofless certain adversarial examples when building real-world robust systems against adversarial examples. Investigating whether the notion of adversarial certainty connects with the robustness of language modelscould be an interesting future direction of our work. Nevertheless, natural language processing (NLP) is adifferent field from computer vision (CV), as it focuses on the discrete space while that of CV is continuous.Thus, directly transferring our method to the NLP domain is non-trivial. According to the pipeline of vanillaadversarial training for text classification (Morris et al., 2020), the generation of adversarial examples is tosubstitute some selected words. Such a generation scheme is different from that of CV and does not supportthe gradient-based optimization of decreasing adversarial certainty. In that case, we will need to design anadditional objective function to measure how certain an NLP model is about its generated sentences, whichcould be the metric for selecting and substituting words. We regard the exploration of DAC in other domainslike NLP as interesting future work but is beyond the scope of our work. We believe our proposed concept ofadversarial certainty will provide important insights for the development of robust machine learning systems.",
  "Nicholas Carlini and David Wagner. Towards Evaluating the Robustness of Neural Networks. In IEEESymposium on Security and Privacy (S&P), pp. 3957. IEEE, 2017": "Yair Carmon, Aditi Raghunathan, Ludwig Schmidt, John C. Duchi, and Percy Liang. Unlabeled data improvesadversarial robustness. In Annual Conference on Neural Information Processing Systems (NeurIPS), pp.1119011201. NeurIPS, 2019. Chenyi Chen, Ari Seff, Alain Kornhauser, and Jianxiong Xiao. Deepdriving: Learning affordance for directperception in autonomous driving. In IEEE International Conference on Computer Vision (ICCV), pp.27222730. IEEE, 2015. Tianlong Chen, Zhenyu Zhang, Sijia Liu, Shiyu Chang, and Zhangyang Wang. Robust overfitting may bemitigated by properly learned smoothening. In International Conference on Learning Representations,2021.",
  "Paul Covington, Jay Adams, and Emre Sargin. Deep neural networks for youtube recommendations. In ACMConference on Recommender Systems (RecSys), pp. 191198. ACM, 2016": "Francesco Croce and Matthias Hein. Reliable evaluation of adversarial robustness with an ensemble of diverseparameter-free attacks. In International Conference on Machine Learning (ICML), pp. 22062216. PMLR,2020. Francesco Croce, Maksym Andriushchenko, Vikash Sehwag, Edoardo Debenedetti, Nicolas Flammarion,Mung Chiang, Prateek Mittal, and Matthias Hein. Robustbench: a standardized adversarial robustnessbenchmark. arXiv preprint arXiv:2010.09670, 2020. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of Deep Bidirec-tional Transformers for Language Understanding. In Conference of the North American Chapter of theAssociation for Computational Linguistics: Human Language Technologies (NAACL-HLT), pp. 41714186.ACL, 2019.",
  "Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. 2009": "Xingjun Ma, Bo Li, Yisen Wang, Sarah M. Erfani, Sudanthi N. R. Wijewickrema, Grant Schoenebeck, DawnSong, Michael E. Houle, and James Bailey. Characterizing adversarial subspaces using local intrinsicdimensionality. In International Conference on Learning Representations (ICLR), 2018. Xingjun Ma, Yuhao Niu, Lin Gu, Yisen Wang, Yitian Zhao, James Bailey, and Feng Lu. Understandingadversarial attacks on deep learning based medical image analysis systems. Pattern Recognition, 110:107332, 2021. Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards DeepLearning Models Resistant to Adversarial Attacks. In International Conference on Learning Representations(ICLR), 2018. John X. Morris, Eli Lifland, Jack Lanchantin, Yangfeng Ji, and Yanjun Qi. Reevaluating adversarial examplesin natural language. In Conference on Empirical Methods in Natural Language Processing (EMNLP), pp.38293839. ACL, 2020.",
  "Leslie Rice, Eric Wong, and J. Zico Kolter. Overfitting in adversarially robust deep learning. In InternationalConference on Machine Learning (ICML), pp. 80938104. PMLR, 2020": "Amrith Setlur, Benjamin Eysenbach, Virginia Smith, and Sergey Levine. Adversarial unlearning: Reducingconfidence along adversarial directions. In Annual Conference on Neural Information Processing Systems(NeurIPS). NeurIPS, 2022. Ali Shafahi, Mahyar Najibi, Amin Ghiasi, Zheng Xu, John P. Dickerson, Christoph Studer, Larry S. Davis,Gavin Taylor, and Tom Goldstein.Adversarial training for free!In Annual Conference on NeuralInformation Processing Systems (NeurIPS), pp. 33533364. NeurIPS, 2019. Mahmood Sharif, Lujo Bauer, and Michael K Reiter. On the suitability of lp-norms for creating and preventingadversarial examples. In Proceedings of the IEEE Conference on Computer Vision and Pattern RecognitionWorkshops, pp. 16051613, 2018.",
  "David Stutz, Matthias Hein, and Bernt Schiele. Relating adversarially robust generalization to flat minima.In IEEE International Conference on Computer Vision (ICCV), pp. 77877797. IEEE, 2021": "Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and RobFergus. Intriguing Properties of Neural Networks. In International Conference on Learning Representations(ICLR), 2014. Jihoon Tack, Sihyun Yu, Jongheon Jeong, Minseon Kim, Sung Ju Hwang, and Jinwoo Shin. Consistencyregularization for adversarial robustness. In AAAI Conference on Artificial Intelligence (AAAI), pp.84148422. AAAI, 2022. Florian Tramr, Alexey Kurakin, Nicolas Papernot, Ian Goodfellow, Dan Boneh, and Patrick McDaniel.Ensemble Adversarial Training: Attacks and Defenses. In International Conference on Learning Represen-tations (ICLR), 2017. Dimitris Tsipras, Shibani Santurkar, Logan Engstrom, Alexander Turner, and Aleksander Madry. Robustnessmay be at odds with accuracy. In International Conference on Learning Representations (ICLR), 2019.",
  "Yuancheng Xu, Yanchao Sun, Micah Goldblum, Tom Goldstein, and Furong Huang. Exploring and exploitingdecision boundary dynamics for adversarial robustness. CoRR abs/2302.03015, 2023": "Chaojian Yu, Bo Han, Li Shen, Jun Yu, Chen Gong, Mingming Gong, and Tongliang Liu. Understandingrobust overfitting of adversarial training and beyond. In International Conference on Machine Learning(ICML), pp. 2559525610. PMLR, 2022. Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric P. Xing, Laurent El Ghaoui, and Michael I. Jordan.Theoretically Principled Trade-off between Robustness and Accuracy. In International Conference onMachine Learning (ICML), pp. 74727482. PMLR, 2019. Yi Zhang, Orestis Plevrakis, Simon S Du, Xingguo Li, Zhao Song, and Sanjeev Arora. Over-parameterizedadversarial training: An analysis overcoming the curse of dimensionality. Advances in Neural InformationProcessing Systems, 33:679688, 2020.",
  "AComplete Introduction of Preliminaries": "For the sake of completeness, this section presents the detailed definitions and discussions of the preliminaryconcepts introduced in , including adversarial robustness, robust generalization and adversarialtraining. Let (X, ) be a metric space. For any set C X and any x X, let C(x) = argminxC (x, x)be the projection of x onto C.",
  "Adversarial Robustness. Adversarial robustness captures the classifiers resilience to small adversarialperturbations. In particular, we work with the following definition of adversarial robustness:": "Definition 2 (Adversarial Robustness). Let X Rn be input space, Y be label space, and be theunderlying distribution of inputs and labels. Let be a distance metric on X and 0. For any classifierf : X Y, the adversarial robustness of f with respect to , and is defined as:",
  "x B(x) s.t. f(x) = y.(4)": "When = 0, R0(f; ) is equivalent to the clean accuracy of f. In practice, the probability density function ofthe underlying distribution is typically unknown. Instead, we only have access to a set of test examples Stei.i.d. sampled from . Thus, a classifiers adversarial robustness is estimated by replacing in Equation (4)with its empirical counterpart based on Ste. To be more specific, the testing-time adversarial robustness of fwith respect to Ste, and is given by:",
  "(x,y)StemaxxB(x) 1f(x) = y,(5)": "where Ste denotes the empirical measure of based on Ste. We remark that robust generalization, themain subject of this study, captures how well a model can classify adversarially-perturbed inputs that arenot used for training, which is essentially the testing-time adversarial robustness R(f; Ste). And wewrite R(f) = R(f; Ste) in the following discussions when Ste is free of context. In this work, wefocus on the p-norm distances as the perturbation metric , since they are most widely-used in existingliterature on adversarial examples.Although p distances may not best reflect the human-perceptualsimilarity Sharif et al. (2018) and perturbation metrics beyond p-norm such as geometrically transformedadversarial examples Kanbak et al. (2018); Xiao et al. (2018) were also considered in literature, there is still asignificant amount of interest in understanding and improving model robustness against p perturbations.We hope that our insights gained from p perturbations will shed light on how to learn better robust modelsfor more realistic adversaries. Adversarial Training. Among all the existing defenses against adversarial examples, adversarial train-ing Madry et al. (2018); Zhang et al. (2019); Carmon et al. (2019) is most promising in producing robustmodels. Given a set of training examples Str sampled from , adversarial training aims to solve the followingmin-max optimization problem:",
  "(x,y)StrmaxxB(x) Lf, x, y.(6)": "Here, denotes the set of model parameters, and L is typically set as a convex surrogate loss such thatLf, x, yis an upper bound on the 0-1 loss 1f(x) = yfor any (x, y). For instance, L is set as the cross-entropy loss in vanilla adversarial training Madry et al. (2018), whereas the combination of a cross-entropyloss for clean data and a regularization term for robustness is used in TRADES Zhang et al. (2019). Intheory, if Str well captures the underlying distribution and the robust loss LR(f; Str) is sufficiently small,then f is guaranteed to achieve high adversarial robustness R(f; ). However, directly solving the min-max optimization problem (6) for non-convex models such as deep neuralnetworks is challenging. It is typical to resort to some good heuristic algorithm to approximately solve the",
  "xs+1 = B(x)xs + sgn(xsL(f, xs, y)for any (x, y) and s {0, 1, . . . , S 1},(7)": "where x0 = x, > 0 denotes the step size and S denotes the total number of iterations. For the easeof presentation, we use Apgd to denote PGD attacks such that for any example (x, y) and classifier f, itgenerates x = xS = Apgd(x; y, f, ) based on the update rule (7). After generating the perturbed input foreach example in a training batch, the model parameter is then updated by a single SGD step with respectto L(f, x, y) for the outer minimization problem in Equation (6).",
  "BMore Details of Figures in Sections 3 and 4": "This section provides all the experimental details for producing the heatmaps and the histograms illustratedin Sections 3 and 4. Given a model f (e.g., Best Model and Last Model) and a set of examples S sampledfrom the underlying distribution (e.g., CIFAR-10 training and testing datasets), adversarial examples aregenerated by PGD attacks within the perturbation ball B(x) centered at x with radius = 8/255 under the perturbations, which follows the settings of generating training samples considered in , e.g., PGDis iteratively conducted by 10 steps with the step size of 2/255. We record and plot the label predictions ofthe generated adversarial examples with respect to each model as heatmaps in .",
  ",(8)": "where Apgd denotes PGD attacks defined by the update rule (7). More specifically, for any (x, y) S, the PGDattack produces the corresponding adversarial example x = Apgd(x; y, f, ). Then, we measure the predictedlabel y = f(x). In that case, for the given training data, we could construct (ground-truth, predicted) labelpairs, simply denoted by {(y, y)}. Afterward, we first cluster {(y, y)} separately by the ground-truth label,e.g., the subset of ground-truth label j includes all pairs such that y = j (denoted by {(y, y)}j), whichcorresponds to the rows of heatmaps. Further, for each subset, we group it into sub-subsets separately by thepredicted labels, e.g., {(y, y)}j,k contains all pairs in {(y, y)}j such that y = k. Consequently, the number ofadversarial examples of the ground truth label j is calculated as:",
  "|{(y, y)}j,k| =(x, y) S : y = j and fApgd(x; y, f, )= k,": "where y = fApgd(x; y, f, ). Finally, we compute the (j, k)-th entry of the heatmap HMj,k as the ratio of|{(y, y)}j,k| to |{(y, y)}j| (Equation (8)). Following the same settings, we plot the corresponding label-levelvariance and adversarial certainty in . Specifically, we first measure the label-level variance ofthe training-time adversarial examples of the last model ((a)) and the best model ((c))conditioned on the ground-truth label in (a). Taking the ground-truth label j as an example, the",
  "kY(HMj,k HMj)2,": "where HMj averages all HMj,k with different k, and Y = {1, 2, , m} is the label space. According toDefinition 1, we measure the adversarial certainty of the last and the best models, as illustrated in (b),with respect to the predicted logits of all the adversarial examples with respect to each ground-truth labelclass.",
  "CProofs of Theoretical Results in": "To gain a better understanding of the proposed definition of adversarial certainty, we further study itsconnection with robust generalization using theoretical data distributions. Following existing works Tsipraset al. (2019); Wei et al. (2023), we consider a simple binary classification task, but a further step of gradientupdate is considered based on our work. First, we lay out the mathematical formulations of the importantconcepts under the assumed setting that will be used for the proofs. Data Distribution. For this binary classification task, we assume the following procedure of data generationfor any example (x, y) : The binary label y is uniformly sampled, i.e., yu.a.r.{1, +1}, then the robustfeature x1 = y with sampling probability p and x1 = y otherwise, while the remaining non-robust featuresx2, , xd=1 are sampled i.i.d. from the Gaussian distribution N(y, 1). Here, p ( 1",
  "without loss of generality, since x2, , xd+1i.i.d. N(y, 1) tend to share the same sign symbol with y, wefurther assume w > 0": "Adversarial Distribution. As discussed in Tsipras et al. (2019) and Ilyas et al. (2019), x1 is robust toperturbation but not perfect (as p < 1), while x2, , xd+1 are useful for classification but sensitive to smallperturbation. Following the setting of Tsipras et al. (2019), the non-robust features are shifted towards yby an adversarial bias distribution for constructing adversarial examples. More specifically, the adversarialexamples x are sampled from the following adversarial distribution adv() with > 0:",
  "x1 = x1, and x2, , xd+1i.i.d. N( )y, 1.(10)": "Note that, in this task, no perturbation bound is involved, which is different from PGD-Attack. Instead,the distribution bias is used to find/sample adversarial examples, which is independent of the attackersbudget. Besides, the goal of this work is to find less certain adversarial examples in the training time. As can directly decide the distribution of adversarial examples, there is no need to vary adversarial certainty byfinding a new model status. Robust Generalization. Since we do not use PGD-based attacks to find adversarial examples as theempirical parts, instead of Definition 2, we utilize the corresponding version of robust generalization based onthe adversarial distribution adv(). Accordingly, given the model fw, the clean and robust generalizationsare separately denoted by R(fw; ) and R(fw; adv()), which are simply written as R0(fw) and R(fw)when and adv() are free of context:",
  "d).(16)": "Adversarial Certainty. In , we provide our definition of adversarial certainty (Definition 1) byusing the empirical counterpart of adversarial distribution. However, in the theoretical part, adversarialdistribution adv() is accessible. Thus, we use adv() to directly define the adversarial certainty for this binaryclassification task. In general, adversarial certainty measures how certain a model predicts the training-timeadversarial examples, i.e., the variance of different cases of the ground-truth and the predicted labels. Basedon Equation (15), all probable cases of robust generalization are:",
  "C.2Extension of Theorem 1 to Perturbations": "Theorem 1 suggests that if we decrease the certainty of the adversarial examples sampled from adv(),the robustness of the SVM classifier f w will increase after one-step gradient update based on the sampledadversarial examples. In this section, we generalize our theoretical analysis to the typical setting of -normbounded perturbations. First, we prove the following lemma to derive the adversarial data distribution withrespect to worst-case perturbations under our problem setup. Lemma 2. Consider the same data distribution and SVM classifiers as assumed in Theorem 1. For any w > 0and (x, y) sampled from , the distribution of worst-case adversarial example (x, y) under perturbationsby using the distribution bias is equivalent to the following adversarial data distribution:",
  "w+d ), where (0, w+d": "w+d ], AC(fw; , adv()), the adversarial certainty of fw,is monotonically decreasing with respect to . Suppose we conduct one-step gradient update on w usingadversarial examples sampled from adv(): w = w + wR0(fw; adv()), where > 0 stands for thelearning rate. Then, R0(f w; adv(te)), the robust generalization performance of f w, also increases as increases.",
  "DSignificance Test for the Improvements of DAC on AWP and Consistency": "As discussed in .2, our DAC method can only bring slight improvements in robust generalizationfor AWP and Consistency. Although our repeated trials have suggested that the improvements are theconsequence of DAC (see ), it is helpful to provide some statistical support. Therefore, in this section,we conduct a t-test to measure the statistical significance of our DAC method.1 Specifically, we first make anull hypothesis, i.e.,",
  "H0 : Our DAC method does not improve the robust generalization of AWP and Consistency": "We then collect the robust generalization under AutoAttack without and with DAC from , whichare separately denoted as two samples X1 and X2. This decision is because AutoAttack is more powerfuland comprehensive than other adversarial attacks used in our evaluation, and AutoAttack is now the defaultmetric for the leaderboard of adversarial defenses.2 In that case, the null hyperthesis H0 can be informallyunderstood as X1 X2. Next, we calculate the mean of X1 and X2, which can be formulated as:",
  "RGlast/RGbest41.49/45.5145.55/52.2047.96/53.32": "In the previous evaluation, our work focuses on the setting of = 8/255 in both the training and testing timefor -norm perturbations, which is widely used in this field as is usually associated with the assumption ofthe adversarial strength. However, it is interesting to investigate the influence of on our DAC method. Thus,we separately fixed the training- and testing-time to 8/255, and vary the other one in {4/255, 8/255, 12/255},as shown in Tables 9 and 10. Specifically, depicts that the model of a fixed training-time tr is morevulnerable to a larger testing-time te, on both the last and best epochs. On the other hand, , showsthat a stronger adversarial ability in the training time can derive a better robustness when facing the sametesting-time te. Consequently, these results suggest that model robustness will benefit from a larger tr anda smaller te.",
  "DAC145.5552.20DAC10145.8450.49DAC15145.6849.85": "In .4, we proposed DAC_Reg to reduce the training cost. However, it modifies the optimizationflow. Therefore, to enhance DACs efficiency, we conduct an additional attempt by decreasing adversarialcertainty only when robust overfitting occurs. In our evaluation, we start the DAC method from the 101-thand 151-th epochs, respectively, corresponding to the first and second time of learning rate decay. FromTable F.2, we can see that an earlier application of DAC brings better Best robustness, while the Lastone is comparable. Consequently, improving training efficiency by reducing the epochs that consider DACcan only derive limited performance promotion. In other words, the potential direction of developing efficientmethods might be regularizing some specific insights in a loss term, such as the design of DAC_Reg. Still,the effort to guarantee stability is necessary."
}