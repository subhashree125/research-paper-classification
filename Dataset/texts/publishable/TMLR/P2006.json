{
  "Abstract": "Devising deep latent variable models for multi-modal data has been a long-standing theme inmachine learning research. Multi-modal Variational Autoencoders (VAEs) have been a pop-ular generative model class that learns latent representations that jointly explain multiplemodalities. Various objective functions for such models have been suggested, often moti-vated as lower bounds on the multi-modal data log-likelihood or from information-theoreticconsiderations.To encode latent variables from different modality subsets, Product-of-Experts (PoE) or Mixture-of-Experts (MoE) aggregation schemes have been routinely usedand shown to yield different trade-offs, for instance, regarding their generative quality orconsistency across multiple modalities. In this work, we consider a variational objectivethat can tightly approximate the data log-likelihood. We develop more flexible aggregationschemes that avoid the inductive biases in PoE or MoE approaches by combining encodedfeatures from different modalities based on permutation-invariant neural networks. Our nu-merical experiments illustrate trade-offs for multi-modal variational objectives and variousaggregation schemes. We show that our variational objective and more flexible aggregationmodels can become beneficial when one wants to approximate the true joint distributionover observed modalities and latent variables in identifiable models.",
  "(d)": ": Reconstruction or cross-prediction of modalities in 1a and 1b for a mixture-based bound andour objective, respectively. The mixture-based bound resorts to a single latent variable Z q(|xS) thatencodes information from a modality subset xS and is trained to reconstruct the conditioning modalities xS,as well as to predict the masked modalities x\\S. Our objective relies on two latent variables ZS q(|xS)and ZM q(|xS, x\\S), where ZS is learned to reconstruct all its conditioning modalities, with ZM learnedto reconstruct the remaining modalities. KL regularization terms in 1c and 1d for a mixture-based boundand our objective, respectively. The mixture-based bound aims to minimize the KL divergence between theencoding distribution given a modality subset xS and a prior distribution. Our objective additionally aims tominimize the KL divergence between the encoding distribution given all modalities relative to the encodingdistribution of a modality subset xS. more comprehensive understanding of biological systems if multiple modalities are analyzed in an integrativeframework (Argelaguet et al., 2018; Lee and van der Schaar, 2021; Minoura et al., 2021). In neuroscience,multi-modal integration of neural activity and behavioral data can help to learn latent neural dynamics(Zhou and Wei, 2020; Schneider et al., 2023). However, annotations or labels in such data sets are often rare,making unsupervised or semi-supervised generative approaches particularly attractive as such methods can beused in these settings to (i) generate data, such as missing modalities, and (ii) learn latent representationsthat are useful for down-stream analyzes or that are of scientific interest themselves. The availability ofheterogeneous data for different modalities promises to learn generalizable representations that can captureshared content across multiple modalities in addition to modality-specific information. A promising classof weakly-supervised generative models is multi-modal VAEs (Suzuki et al., 2016; Wu and Goodman, 2019;Shi et al., 2019; Sutter et al., 2021) that combine information across modalities in an often-shared low-dimensional latent representation. A common route for learning the parameters of latent variable models isvia maximization of the marginal data likelihood with various lower bounds thereof, as suggested in previouswork. Setup.We consider a set of M random variables {X1, . . . , XM} with empirical density pd, where eachrandom variable Xs, s M = {1, . . . , M}, can be used to model a different data modality taking valuesin Xs. With some abuse of notation, we write X = {X1, . . . , XM} and for any subset S M, we setX = (XS, X\\S) for two partitions of the random variables into XS = {Xs}sS and X\\S = {Xs}sM\\S.We pursue a latent variable model setup, analogous to uni-modal VAEs (Kingma and Ba, 2014; Rezendeet al., 2014).For a latent variable Z Z with prior density p(z), we posit a joint generative model1 p(z, x) = p(z) Ms=1 p(xs|z), where p(xs|z) is commonly referred to as the decoding distribution formodality s. Observe that all modalities are independent given the latent variable z shared across all modal-ities. However, one can introduce modality-specific latent variables by making sparsity assumptions for thedecoding distribution. Intuitively, this conditional independence assumption means that the latent variableZ captures all unobserved factors shared by the modalities. 1We usually denote random variables using upper-case letters, and their realizations by the corresponding lower-case letter.We assume throughout that Z = RD, and that p(z) is a Lebesgue density, although the results can be extended to more generalsettings such as discrete random variables Z with appropriate adjustments, for instance, regarding the gradient estimators.",
  "LMixS(x, , , ) =q(z|xS) [log p(x|z)] dz KL(q(z|xS)|p(z))(1)": "and is some distribution on the power set P(M) of M and > 0. For = 1, one obtains the boundLMixS(x, , , ) log p(x). However, as shown in Daunhawer et al. (2022), there is a gap between thevariational bound and the log-likelihood given by the conditional entropies that cannot be reduced even forflexible encoding distributions. More precisely, it holds thatpd(x) log p(x)dx pd(x)LMix(x, , , 1)dx +(S)H(pd(X\\S|XS))dS, where H(pd(X\\S|XS)) is the entropy of the conditional data distributions. Intuitively, in (1), one tries toreconstruct or predict all modalities from incomplete information using only the modalities S, which leadsto learning an inexact, average prediction (Daunhawer et al., 2022). In particular, it cannot reliably predictmodality-specific information that is not shared with other modality subsets, as measured by the conditionalentropies H(pd(X\\S|XS)). For an illustration, see a, where the latent variable Z encodes informationfrom a text and audio modality and is tasked to both reconstruct the text and audio modalities and to predictan unobserved image. Information that is specific to the image modality only thus cannot be recovered.A related observation has been made for Masked AutoEncoders (He et al., 2022) that correspond to thelimiting case 0, where the cross-reconstruction objective leads to learning features that are invariantto the masking of modalities (Kong and Zhang, 2023) and allows for the recovery of latent variables thatrepresent maximally shared information between the unmasked and masked modality (Kong et al., 2023).",
  "where Iubqand Ilbq are variational upper, respectively, lower bounds of the corresponding mutual informationIq(X, Y ) =q(x, y) logq(x,y)": "q(x)q(y)dxdy of random variables X and Y having marginal and joint densities q.In order to emphasize that the latent variable Z is conditional on XS under the encoding density q, wewrite ZS instead of Z. Variations of (1) have been suggested (Sutter et al., 2020), such as by replacingthe prior density p in the KL-term by a weighted product of the prior density p and the uni-modalencoding distributions q(z|xs), for all s M. Likewise, the multi-view variational information bottleneckapproach developed in Lee and van der Schaar (2021) for predicting X\\S given XS can be interpreted asmaximizing Ilbq(X\\S, ZS) Iubq(XS, ZS).Hwang et al. (2021) suggested a related bound that aims tomaximize the reduction of total correlation of X when conditioned on a latent variable. Similar bounds havebeen suggested in Sutter et al. (2020) and Suzuki et al. (2016) by considering different KL-regularisationterms; see also Suzuki and Matsuo (2022). Shi et al. (2020) add a contrastive estimate Ip of the point-wise mutual information to the maximum likelihood objective and minimize log p(x) Ip(xS, x\\S).Optimizing variational bounds of different mutual information terms such as (2) yield latent representationsthat have different trade-offs in terms of either (i) reconstruction or (ii) cross-prediction of multi-modal datafrom a rate-distortion viewpoint (Alemi et al., 2018). Multi-modal aggregation schemes.To optimize the variational bounds above or to allow for flexibleconditioning at test time, we need to learn encoding distributions q(z|xS) for any S P(M). The typicalaggregation schemes that are scalable to a large number of modalities are based on a choice of uni-modalencoding distributions qs(z|xs) for any s M, which are then used to define the multi-modal encodingdistributions as follows:",
  "(d) Self-Attention": ": Illustration of multi-modal aggregation schemes. All encoding schemes first apply modality-specificencoders to each individual modality. A PoE model (2a) aggregates the outputs from the modality-specificencoders into a single Gaussian distribution that results from a multiplication of the corresponding uni-modal Gaussian densities. An MoE model (2b) assumes an equally weighted Gaussian mixture distributioncomprised of the uni-modal Gaussian densities. Our new aggregation schemes allow for learning permutation-invariant fusion models: A Sum-Pooling or Deep Set model (2c) applies the same function g to the encodedfeatures hs, s {T, A, I}, before summing them up and using a non-linear projection to the parametersof a Gaussian distribution. A Self-Attention model (2d) differs from the Sum-Pooling approach by applyingself-attention layers or transformer layers before summing up the features, thereby accounting for pairwiseinteractions between the encoded modalities.Our newly introduced schemes allow for encoding only amodality subset by using standard masking operations.",
  "sSqs(z|xs)": "a-2b illustrates these previously considered aggregation schemes. While these schemes do not requirelearning the aggregation function, we introduce aggregation schemes that, as illustrated in 2c-2d, involvelearning additional neural network parameters that specify a learnable aggregation function. Contributions.This paper contributes (i) a new variational objective as an approximation of a lowerbound on the multi-modal log-likelihood (LLH). We avoid a limitation of mixture-based bounds (1), whichmay not provide tight lower bounds on the joint LLH if there is considerable modality-specific variation(Daunhawer et al., 2022), even for flexible encoding distributions. The novel variational objective containsa lower bound of the marginal LLH log p(xS) and a term approximating the conditional log p(x\\S|xS) forany choice of S P(M), provided that we can learn a flexible multi-modal encoding distribution. Thispaper then contributes (ii) new multi-modal aggregation schemes that yield more expressive multi-modalencoding distributions when compared to MoEs or PoEs. These schemes are motivated by the flexibility ofpermutation-invariant (PI) architectures such as DeepSets (Zaheer et al., 2017) or attention models (Vaswaniet al., 2017; Lee et al., 2019). We illustrate that these innovations (iii) are beneficial when learning identifiablemodels, aided by using flexible prior and encoding distributions consisting of mixtures, and (iv) yield higherLLH in experiments. Further related work.Canonical Correlation Analysis (Hotelling, 1936; Bach and Jordan, 2005) is a clas-sical approach for multi-modal data that aims to find projections of two modalities by maximally correlating",
  "Published in Transactions on Machine Learning Research (09/2024)": "We will pursue here an encoding approach that does not require modeling the encoding distribution overthe discrete latent variables explicitly, thus avoiding large variances in score-based Monte Carlo estimators(Ranganath et al., 2014) or resorting to advanced variance reduction techniques (Kool et al., 2019) oralternatives such as continuous relaxation approaches (Jang et al., 2016; Maddison et al., 2016).",
  "DS = pd(xS)q(z|xS) log p(xS|z)dzdxS": "given as the negative reconstruction log-likelihood of the modalities in S. While the latent variable ZSthat is encoded via q(z|xS) from XS can be tuned via the choice of > 0 to tradeoff compression andreconstruction of all modalities in S jointly, it does not explicitly optimize for cross-modal prediction ofmodalities not in S.Indeed, the mixture-based variational bound differs from the above decompositionexactly by an additional cross-modal prediction or cross-distortion term",
  "L(x, , , ) =(S)LS(xS, , , ) + L\\S(x, , , )dS,": "with respect to and , which is a generalization of the bound suggested in Wu and Goodman (2019) to anarbitrary number of modalities. This bound can be optimized using standard Monte Carlo techniques, forexample, by computing unbiased pathwise gradients (Kingma and Ba, 2014; Rezende et al., 2014; Titsiasand Lzaro-Gredilla, 2014) using the reparameterization trick. For variational families such as Gaussianmixtures2, one can employ implicit reparameterization (Figurnov et al., 2018). It is straightforward to adaptvariance reduction techniques such as ignoring the scoring term of the multi-modal encoding densities forpathwise gradients (Roeder et al., 2017), see Algorithm 1 in Appendix H for pseudo-code. Nevertheless, ascalable approach requires an encoding technique that allows to condition on any masked modalities witha computational complexity that does not increase exponentially in M. We will analyse scalable modelarchitectures in .",
  "Multi-modal distribution matching": "Likelihood-based learning approaches aim to match the model distribution p(x) to the true data distributionpd(x). Variational approaches achieve this by matching in the latent space the encoding distribution to thetrue posterior as well as maximizing a tight lower bound on log p(x), see Rosca et al. (2018). These types ofanalyses have proved useful for uni-modal VAEs as they can provide some insights as to why VAEs may leadto worse generative sample quality compared to other generative models such as GANs (Goodfellow et al.,2014) or may fail to learn useful latent representations (Zhao et al., 2019; Dieng et al., 2019). We show similarresults for the multi-modal variational objectives. This suggests that limitations from uni-modal VAEs alsoaffect multi-modal VAEs, but also that previous attempts to address these shortcomings in uni-modal VAEsmay benefit multi-modal VAEs. In particular, mismatches between the prior and the aggregated prior foruni-modal VAEs that result in poor unconditional generation have a natural counterpart for cross-modalgenerations with multi-modal VAEs that may potentially be reduced using more flexible conditional priordistributions, see Remark 3, or via adding additional mutual information regularising terms (Zhao et al., 2For MoE aggregation schemes, Shi et al. (2019) considered a stratified ELBO estimator as well as a tighter bound basedon importance sampling, see also Morningstar et al. (2021), that we do not pursue here for consistency with other aggregationschemes that can likewise be optimized based on importance sampling ideas.",
  "Proof This follows from (Xmarginal) and (Xconditional)": "Our approach recovers meta-learning with (latent) Neural processes (Garnelo et al., 2018b) when one op-timizes only L\\S with S determined by context-target splits, cf. Appendix B. Our analysis implies thatLS + L\\S is an approximation of a lower bound on the multi-modal log-likelihood that becomes tight forinfinite-capacity encoders so that q(z|xS) = p(z|xS) and q(z|x) = p(z|x), see Remarks 3 and 5 for details.",
  "dzdx": "arising in Corollary 2 and in (Xconditional) is not necessarily negative. Analogous to other variational ap-proaches for learning conditional distributions such as latent Neural processes, our bound becomes an ap-proximation of a lower bound. Note that LS is maximized when q(z|xS) = p(z|xS), see (Xmarginal), whichimplies a lower bound in Corollary 2 ofpd(x)LS(xS, , , 1) + L\\S(x, , , 1)dx =pd(x) [log p(x) KL(q(z|x)|p(z|x))] dx.",
  "p(x\\S|xS)dx\\S = p(z|xS) = q(z|xS)": "Remark 4 (Prior-hole problem and Bayes or conditional consistency) In the uni-modal setting,the mismatch between the prior and the aggregated prior can be large and can lead to poor uncondi-tional generative performance because this would lead to high-probability regions under the prior that havenot been trained due to their small mass under the aggregated prior (Hoffman and Johnson, 2016; Roscaet al., 2018). Equation (Zmarginal) extends this to the multi-modal case, and we expect that unconditionalgeneration can be poor if this mismatch is large. Moreover, (Zconditional) extends this conditioned on somemodality subset, and we expect that cross-generation for x\\S conditional on xS can be poor if the mismatchbetween qagg,\\S(z|xS) and q(z|xS) is large for xS pd, because high-probability regions under q(z|xS) willnot have been trained - via optimizing L\\S(x) - to model x\\S conditional on xS, due to their small massunder qagg,\\S(z|xS). The mismatch will vanish when the encoders are consistent and correspond to a singleBayesian model where they approximate the true posterior distributions. A potential approach to reducethis mismatch may be to include as a regulariser the divergence between them that can be optimized bylikelihood-free techniques, such as the Maximum-Mean Discrepancy (Gretton et al., 2006), as in Zhao et al.(2019) for uni-modal or unconditional models. For the mixture-based bound, the same distribution mismatchaffects unconditional generation, while both the training and generative sampling distribution is q(z|xS)for cross-generation. Remark 5 (Variational gap for mixture-based bounds) Corollary 2 shows that the variational objec-tive can become a tight bound in the limiting case where the encoding distributions approximate the trueposterior distributions. A similar result does not hold for the mixture-based multi-modal bound. More-over, our bound can be tight for an arbitrary number of modalities in the limiting case of infinite-capacityencoders. In contrast, Daunhawer et al. (2022) show that for mixture-based bounds, this variational gapincreases with each additional modality if the new modality is sufficiently diverse, even for infinite-capacityencoders. Remark 6 (Optimization, multi-task learning and the choice of ) For simplicity, we have chosento sample S in our experiments via the hierarchical construction U(0, 1), mj Bern() iid forall j [M] and setting S = {s [M]: mj = 1}. The distribution for masking the modalities can beadjusted to accommodate various weights for different modality subsets. Indeed, (2) can be seen as a linearscalarization of a multi-task learning problem (Fliege and Svaiter, 2000; Sener and Koltun, 2018). We aim tooptimize a loss vector (LS +L\\S)SM, where the gradients for each S M can point in different directions,making it challenging to minimize the loss for all modalities simultaneously. Consequently, Javaloy et al.(2022) used multi-task learning techniques (e.g., as suggested in Chen et al. (2018); Yu et al. (2020)) foradjusting the gradients in mixture-based VAEs. Such improved optimization routines are orthogonal to ourapproach. Similarly, we do not analyze optimization issues such as initializations and training dynamics thathave been found challenging for multi-modal learning (Wang et al., 2020; Huang et al., 2022).",
  "Information-theoretic perspective": "Beyond generative modeling, -VAEs (Higgins et al., 2017) have been popular for representation learningand data reconstruction. Alemi et al. (2018) suggest learning a latent representation that achieves certainmutual information with the data based on upper and lower variational bounds of the mutual information.A Legendre transformation thereof recovers the -VAE objective and allows a trade-off between informationcontent or rate versus reconstruction quality or distortion. We show that the proposed variational objectivegives rise to an analogous perspective for multiple modalities. Recall that the mutual information on theinference path3 is given by",
  "Remark 9 (Mixture-based variational bound) We show in Appendix C, see also Daunhawer et al.(2022), thatHM DS DcS HM DS DcS + 1 = Iq(XM, ZS),": "where 1 =qagg(z)KL(q(x|z)|p(x|z))dz > 0. Consequently, HM DS DcS is a variational lower bound,while RS is a variational upper bound on Iq(XM, ZS), which establishes (2). Maximizing the mixture-based bound thus corresponds to encoding a single latent variable ZS that maximizes the reconstruction ofall modalities while at the same time being maximally compressive relative to the prior. Remark 10 (Optimal variational distributions) Considertheannealedlikelihoodp,(xS|z)p(xS|z)1/ as well as the adjusted posterior p,(z|xS) p,(xS|z)p(z). The minimum of the boundpd(dx)LS(x) is attained at any xS for the variational density",
  "[log p(xS|z) + log p(z)] p,(z|xS),(8)": "see also Huang et al. (2020) and Remark 18. Similarly, if (8) holds, then it is readily seen that the minimumof the boundpd(dx)L\\S(x) is attained at any x for the variational density q(z|x) = p,(z|x). In contrast,as shown in Appendix 19, the optimal variational density for the mixture-based (1) multi-modal bound isattained at",
  "Permutation-invariant modality encoding": "Optimizing the above multi-modal bounds requires learning variational densities with different conditioningsets. We write hs, : Xs RDE for some modality-specific feature function. We recall the following multi-modal encoding functions suggested in previous work where usually hs,(xs) =s,(xs), vec(s,(xs)) with s, and s, being the mean, respectively the (often diagonal) covariance, of a uni-modal encoder ofmodality s. Accommodating more complex variational families, such as mixture distributions for the uni-modal encoding distributions, can be more challenging for these approaches.",
  "Learnable permutation-invariant aggregation schemes": "We aim to learn a more flexible aggregation scheme under the constraint that the encoding distribution isinvariant (Bloem-Reddy and Teh, 2020) with respect to the ordering of encoded features of each modality.Put differently, for all (Hs)sS R|S|DE and all permutations SS of S, we assume that the conditionaldistribution is SS-invariant, i.e. q(z|h) = q(z|h) for all z RD, where acts on H = (Hs)sS via H =(H(s))sS. We set q(z|xS) = q(z|hs,(xs)sS), = (, ) and remark that the encoding distribution isnot invariant with respect to the modalities, but becomes only invariant after applying modality-specificencoder functions hs,. Observe that such a constraint is satisfied by the aggregation schemes above for hs,being the uni-modal encoders. A variety of invariant (or equivariant) functions along with their approximation properties have been consid-ered previously, see for instance Santoro et al. (2017); Zaheer et al. (2017); Qi et al. (2017); Lee et al. (2019);Segol and Lipman (2019); Murphy et al. (2019); Maron et al. (2019); Sannai et al. (2019); Yun et al. (2019);Bruno et al. (2021); Wagstaff et al. (2022); Zhang et al. (2022b); Li et al. (2022); Bartunov et al. (2022), andapplied in different contexts such as meta-learning (Edwards and Storkey, 2016; Garnelo et al., 2018b; Kimet al., 2018; Hewitt et al., 2018; Giannone and Winther, 2022), reinforcement learning (Tang and Ha, 2021;Zhang et al., 2022a) or generative modeling of (uni-modal) sets (Li et al., 2018; 2020; Kim et al., 2021; Biloand Gnnemann, 2021; Li and Oliva, 2021). We can use such constructions to parameterize more flexibleencoding distributions. Indeed, the results from Bloem-Reddy and Teh (2020) imply that for an exchangablesequence HS = (Hs)sS R|S|DE and random variable Z, the distribution q(z|hS) is SS-invariant if andonly if there is a measurable function4 f : M(RDE) RD such that",
  "(HS, Z)a.s.= (HS, f (, MHS)), where U and HS": "with MHS() = sS Hs() being the empirical measure of hS, which retains the values of hS, but discardstheir order. For variational densities from a location-scale family such as a Gaussian or Laplace distribution,we find it more practical to consider a different reparameterization in the form Z = (hS) + (hS) ,where is a sample from a parameter-free density p such as a standard Gaussian and Laplace distribution,while(hS), log (hS)= f(hS) for a PI function f : R|S|DE R2D. Likewise, for mixture distributionsthereof, assume that for a PI function f,1(hS), log 1(hS), . . . , K(hS), log K(hS), log (hS)= f(hS) R2DK+K and Z = L(hS) + L(hS) with L Cat((hS)) denoting the sampled mixture component out of Kmixtures. For simplicity, we consider here only two examples of PI functions f that have representationswith parameter in the form",
  "for a function : RDP RDO and permutation-equivariant function g : RNDE RNDP": "Example 1 (Sum Pooling Encoders) The Deep Set (Zaheer et al., 2017) construction f(hS)=sS (hs)applies the same neural network : RDE RDP to each encoded feature hs.Weassume that is a feed-forward neural network and remark that pre-activation ResNets (He et al., 2016)have been advocated for deeper .For exponential family models, the optimal natural parameters ofthe posterior solve an optimization problem where the dependence on the generative parameters from thedifferent modalities decomposes as a sum, see Appendix F. Example 2 (Set Transformer Encoders) Let MTB be a multi-head pre-layer-norm transformer block(Wang et al., 2019a; Xiong et al., 2020), see Appendix D for precise definitions. For some neural network : RDE RDP , set g0S = (hS) and for k {1, . . . , L}, set gkS = MTB(gk1S).We then considerf(hS) =",
  "xSP(xM)qPoE(z|xS)": "This can also be seen as another PI model. While it does not require learning separate encoding models forall modality subsets, it, however, becomes computationally expensive to evaluate for large M. Our mixturemodels using components with a SumPooling or SelfAttention aggregation can be seen as an alternativethat allows one to choose the number of mixture components K to be smaller than 2M, with non-uniformweights, while the individual mixture components are not constrained to have a PoE form. Remark 12 (Pooling expert opinions) Combining expert distributions has a long tradition in decisiontheory and Bayesian inference; see Genest and Zidek (1986) for early works, with popular schemes being linearpooling (i.e., MoE) or log-linear pooling (i.e., PoE with tempered densities). These are optimal schemes forminimizing different objectives, namely a weighted (forward or reverse) KL-divergence between the pooleddistribution and the individual experts (Abbas, 2009). Log-linear pooling operators are externally Bayesian,allowing for consistent Bayesian belief updates when each expert updates her belief with the same likelihoodfunction (Genest et al., 1986).",
  "p(z, zS, z\\S|xS) = p(z, zS, |xS)p(z\\S|z, zS, xS) = p(z, zS, |xS)p(z\\S|z, zS).(11)": "An encoding distribution q(z|xS) that approximates p(z|xS) should thus be unaffected by the inputs xSwhen encoding zs for s / S, provided that, a priori, all private and shared latent variables are independent.Observe that for f with the representation (3.1) where has aggregated inputs y, and that parameterizesthe encoding distribution of z = (z, zS, z\\S), the gradients of its i-th dimension with respect to the modalityvalues xs isxs[f(hS(xS))i] = ,i",
  "sS (hs(xs)) if some other components have a non-zero gradientwith respect to Xs. It it thus very likely that inputs Xs for s S can impact the distribution of the privatelatent variables z\\S": "However, the specific generative model also lends itself to an alternative parameterization that guaranteesthat cross-modal likelihoods from X\\S do not affect the encoding distribution of ZS under our new variationalobjective. The assumption of private latent variables suggests an additional permutation-equivariance intothe encoding distribution that approximates the posterior in (11), in the sense that for any permutation SS, it holds thatq(zS| h(xS), z) = q( zS|h(xS), z), assuming that all private latent variables are of the same dimension D.5 Indeed, suppose we have modality-specific feature functions h,s such that {Hs = h,s(Xs)}sS is exchangeable. Clearly, (10) implies for anys = t thath,s(Xs) Zt | Z, Zs.",
  "(HS, Zs) = (HS, f (s, Z, Hs, MHS)), where s U iid and s HS.(12)": "This fact suggests an alternative route to approximate the posterior distribution in (11): First, p(z\\S|z, zS)can often be computed analytically based on the learned or fixed prior distribution. Second, a permutation-invariant scheme can be used to approximate p(z|xS).Finally, a permutation-equivariant scheme canbe employed to approximate p(zS|xS, z) with a reparameterization in the form of (12). The variationalobjective that explicitly uses private latent variables is detailed in Appendix E. Three examples of suchpermutation-equivariant schemes are given below with pseudocode for optimizing the variational objectivegiven in Algorithm 2. Note that the assumption q(zS|z, z\\S, xS) = q(zS|z, xS) is an inductive bias thatgenerally decreases the variational objective as it imposes a restriction on the encoding distribution thatonly approximates the posterior where this independence assumption holds. However, this independenceassumption allows us to respect the modality-specific nature of the private latent variables during encod-ing. In particular, for some permutation-invariant encoder q(z|xS) for the private latent variables andpermutation-equivariant encoder q(zS|z, xS) for the private latent variables of the observed modalities, wecan encode viaq(z, zM|xS) = q(z|xS)p(z\\S|z)q(zS|z, xS)",
  "sSqN (z|s,(xs), s,(xs))": "is a (permutation-invariant) PoE aggregation, and we assumed that the prior density factorizes over theshared and different private variables. For each modality s, we encode different features hs, = (s,, s,)and hs, = (s,, s,) for the shared, respectively, private, latent variables. We followed previous works(Tsai et al., 2019b; Lee and Pavlovic, 2021; Sutter et al., 2020) in that the encodings and prior distributionsfor the modality-specific latent variables are independent of the shared latent variables.However, thisassumption can be relaxed, as long as the distributions remain Gaussian.",
  "Identifiability": "Identifiability of parameters and latent variables in latent structure models is a classic problem (Koopmansand Reiersol, 1950; Kruskal, 1976; Allman et al., 2009), that has been studied increasingly for non-linearlatent variable models, e.g., for ICA (Hyvarinen and Morioka, 2016; Hlv and Hyvarinen, 2020; Hlv et al.,2021), VAEs (Khemakhem et al., 2020a; Zhou and Wei, 2020; Wang et al., 2021; Moran et al., 2021; Lu et al.,2022; Kim et al., 2023), EBMs (Khemakhem et al., 2020b), flow-based (Sorrenson et al., 2020) or mixturemodels (Kivva et al., 2022). Non-linear generative models are generally unidentifiable without imposing some structure (Hyvrinen andPajunen, 1999; Xi and Bloem-Reddy, 2022). Yet, identifiability up to some ambiguity can be achieved insome conditional models based on observed auxiliary variables and injective decoder functions wherein theprior density is conditional on auxiliary variables. Observations from different modalities can act as auxiliaryvariables to obtain identifiability of conditional distributions given some modality subset under analogousassumptions. Example 6 (Auxiliary variable as a modality) In the iVAE model (Khemakhem et al., 2020a), thelatent variable distribution p(z|x1) is independently modulated via an auxiliary variable X1 = U. Insteadof interpreting this distribution as a (conditional) prior density, we view it as a posterior density giventhe first modality X1.Khemakhem et al. (2020a) estimate a model for another modality X2 by lowerbounding log p(x2|x1) via L\\{1} under the assumption that q(z|x1) is given by the prior density p(z|x1).Similarly, Mita et al. (2021) optimize log p(x1, x2) by a double VAE bound that reduces to L for a masking",
  "Proposition 13 (Weak identifiability) Considerthedatagenerationmechanismp(z, x)=p(z)": "sM p(xs|z) where the observation model satisfies (14) for an injective f\\S.Suppose furtherthat p(z|xS) is strongly exponential and (13) holds. Assume that the set {x\\S X\\S|\\S,(x\\S) = 0} hasmeasure zero, where \\S, is the characteristic function of the density p\\S,.Furthermore, suppose thatthere exist k + 1 points x0S, . . . , xkS XS such that",
  "is invertible. Then pS(x\\S|xS) = pS(x\\S|xS) for all x X implies AS": "This result follows from Theorem 4 in Lu et al. (2022).Note that pS(x\\S|xS) = pS(x\\S|xS) for allx X implies with the regularity assumption on \\S, that the transformed variables Z = f 1\\S (X\\S) andZ = f 1\\S (X\\S) have the same density function conditional on XS. Remark 14 (Conditional identifiability) The identifiability result above is about conditional modelsand does not contradict the un-identifiability of VAEs: When S = and we view x = xM as one modality,then the parameters of p(x) characterized by the parameters V and of the prior p(z|x) and theencoders fM will not be identifiable as the invertibility condition will not be satisfied.",
  "Mixture models": "An alternative to the choice of uni-modal prior densities p has been to use Gaussian mixture priors (Johnsonet al., 2016; Jiang et al., 2017; Dilokthanakul et al., 2016) or more flexible mixture models (Falck et al.,2021). Following previous work, we include a latent cluster indicator variable c [K] that indicates themixture component out of K possible mixtures with augmented prior p(c, z) = p(c)p(z|c). The classicexample is p(c) being a categorical distribution and p(z|c) a Gaussian with mean c and covariance matrixc. Similar to Falck et al. (2021) that use an optimal variational factor in a mean-field model, we use anoptimal factor of the cluster indicator in a structured variational density q(c, z|xS) = q(z|xS)q(c|z, xS)with q(c|z, xS) = p(c|z). Appendix G details how one can optimize an augmented multi-modal bound.Concurrent work (Palumbo et al., 2024) considered a similar optimal variational factor for a discrete mixturemodel under a MoE aggregation.",
  "Missing modalities": "In practical applications, modalities can be missing for different data points.We describe this missing-ness pattern by missingness mask variables ms {0, 1} where ms = 1 indicates that observe modal-ity s, while ms = 0 means it is missing. The joint generative model with missing modalities will be ofthe form p(z, x, m) = p(z) sM p(xs|z)p(m|x) for some distribution p(m|x) over the mask variablesm = (ms)sM. For S M, we denote by xoS = {xs : ms = 1, s S} and xmS = {xs : ms = 0, s S} the set ofobserved, respectively missing, modalities. The full likelihood of the observed and missingness masks becomesthen p(xoS, m) =p(z) sS p(xs|z)p(m|x)dxms dz. If p(m|x) does not depend on the observations, thatis, observations are missing completely at random (Rubin, 1976), then the missingness mechanisms p(m|x)for inference approaches maximizing p(xo, m) can be ignored. Consequently, one can instead concentrateon maximizing log p(xo) only, based on the joint generative model p(z, xo) = p(z) {sM: ms=1} p(xs|z).In particular, one can employ the variational objectives above by considering only the observed modalities.Since masking operations are readily supported for the considered permutation-invariant models, appropri-ate imputation strategies (Nazabal et al., 2020; Ma et al., 2019) for the encoded features of the missingmodalities are not necessarily required. Settings allowing for not (completely) at random missingness havebeen considered in the uni-modal case, for instance, in Ipsen et al. (2021); Ghalebikesabi et al. (2021); Gonget al. (2021), and we leave multi-modal extensions thereof for future work for a given aggregation approach.",
  "Experiments": "We conduct a series of numerical experiments to illustrate the effects of different variational objectives andaggregation schemes. Recall that the full reconstruction log-likelihood is the negative full distortion DMbased on all modalities, while the full rate RM is the averaged KL between the encoding distribution of allmodalities and the prior. Note that mixture-based bounds maximize directly the cross-modal log-likelihoodDc\\S, see (4), and do not contain a cross-rate term R\\S, i.e. the KL between the encoding distributionfor all modalities relative to a modality subset, as a regulariser, in contrast to our objective (Lemma 7 andCorollary 8). The log-likelihood should be higher if a generative model is able to capture modality-specificinformation for models trained with = 1.For arbitrary , we can take a rate-distortion perspectiveand look at how different generative models self-reconstruct all modalities, i.e., the full reconstruction termDM, relative to the KL-divergence between the multi-modal encoding distribution and the prior, i.e. RM.This corresponds to a rate-distortion analysis of a VAE that merges all modalities into a single modality.A high full-reconstruction term is thus indicative of the encoder and decoder being able to reconstruct allmodalities precisely so that they do not produce an average prediction. Note that neither our objective northe mixture-based bound optimize for the full-reconstruction term directly.",
  "Linear multi-modal VAEs": "The relationship between uni-modal VAEs and probabilistic PCA (Tipping and Bishop, 1999) has beenstudied in previous work (Dai et al., 2018; Lucas et al., 2019; Rolinek et al., 2019; Huang et al., 2020;Mathieu et al., 2019). We analyze how different multi-modal fusion schemes and multi-modal variationalobjectives affect (a) the learned generative model in terms of its true marginal log-likelihood (LLH) and (b)the latent representations in terms of information-theoretic quantities and identifiability. To evaluate the(weak) identifiability of the method, we follow Khemakhem et al. (2020a;b) to compute the mean correlationcoefficient (MCC) between the true latent variables Z and samples from the variational distribution q(|xM)after an affine transformation using CCA. Generative model.Suppose that a latent variable Z taking values in RD is sampled from a standardGaussian prior p(z) = N(0, I) generates M data modalities Xs RDs, D Ds, based on a linear decodingmodel p(xs|z) = N(Wsz + bs, 2 I) for a factor loading matrix Ws RDsD, bias bs RDs and observationscale > 0. Note that the annealed likelihood function p,(xs|z) = N(Wsz + bs, 2 I) corresponds to ascaling of the observation noise, so that we consider only the choice = 1, set = 1/2 and vary > 0.It is obvious that for any S M, it holds that p,(xS|z) = N(WSz + bS, 2 IS), where WS and bS aregiven by concatenating row-wise the emission or bias matrices for modalities in S, while 2 IS is the diagonalmatrix of the variances of the corresponding observations. By standard properties of Gaussian distributions,it follows that p,(xS) = N(bS, CS) where CS = WSW S +2 IS is the data covariance matrix. Furthermore,with KS = W S WS + 2 Id, the adjusted posterior is p,(z|xS) = N(K1S W S (xS bS), 2 Id K1S ). If wesample orthogonal rows of W, the posterior covariance becomes diagonal so that it can - in principle - bewell approximated by an encoding distribution with a diagonal covariance matrix. Indeed, the inverse ofthe posterior covariance matrix is only a function of the generative parameters of the modalities within Sand can be written as the sum 2 I +W S WS = 2 I +",
  "MoE0.110.6-32.07-30.091.022.84-33.27-28.522.3719.33SumPooling3.6 1050.06-2.84-3.232.882.82-52.58-27.261.4227.35SelfAttention3.4 1050.06-2.85-3.232.872.82-52.59-27.251.4227.41": ": Gaussian model with five modalities: Relative difference of true LLH to the learned LLH. MCC totrue latent. The generative model for the invariant aggregation schemes uses dense decoders, whereas theground truth model for the permutation-equivariant encoders uses sparse decoders to account for privatelatent variables. We report mean values with standard deviations in parentheses over five independent runs.",
  "AggregationLLH GapMCCLLH GapMCC": "PoE0.03 (0.058)0.75 (0.20)0.04 (0.074)0.77 (0.21)0.00 (0.000)0.91 (0.016)0.01 (0.001)0.88 (0.011)MoE0.01 (0.005)0.82 (0.04)0.02 (0.006)0.67 (0.03)SumPooling0.00 (0.000)0.84 (0.00)0.00 (0.002)0.84 (0.02)0.00 (0.000)0.85 (0.004)0.00 (0.000)0.82 (0.003)SelfAttention0.00 (0.003)0.84 (0.00)0.02 (0.007)0.83 (0.00)0.00 (0.000)0.83 (0.006)0.00 (0.000)0.83 (0.003) values show first that learnable aggregation models yield higher log-likelihoods6, and second that our boundyields higher log-likelihood values compared to mixture-based bounds for any given fixed aggregation model.We also compute various information theoretic quantities, confirming that our bound leads to higher fullreconstructions at higher full rates and lower cross predictions at lower cross rates compared to mixture-based bounds. More flexible aggregation schemes increase the full and cross predictions for any given boundwhile not necessarily increasing the full or cross rates, i.e., they can result in an improved point within arate-distortion curve for some configurations. Simulation study.We consider M = 5 modalities following multi-variate Gaussian laws. We considergenerative models, where all latent variables are shared across all modalities, as well as generative models,where only parts of the latent variables are shared across all modalities, while the remaining latent vari-ables are modality-specific. The setting of private latent variables can be incorporated by imposing sparsitystructures on the decoding matrices and allows us to analyze scenarios with considerable modality-specificvariation described through private latent variables. We provide more details about the data generationmechanisms in Appendix J. For illustration, we use multi-modal encoders with shared latent variables usinginvariant aggregations in the first case and multi-modal encoders that utilize additional equivariant aggre-gations for the private latent variables in the second case. Results in suggest that more flexibleaggregation schemes improve the LLH and the identifiability for both variational objectives. Furthermore,our new bound yields higher LLH for a given aggregation scheme.",
  "Non-linear identifiable models": "Auxiliary labels as modalities.We construct artificial data following Khemakhem et al. (2020a), withthe latent variables Z RD being conditionally Gaussian having means and variances that depend onan observed index value X2 [K]. More precisely, p(z|x2) = N(x2, x2), where c U(5, 5) andc = diag(c), c U(0.5, 3) iid for c [K]. The marginal distribution over the labels is uniformU([K]) so that the prior density p(z) = [K] p(z|x2)p(x2)dx2 becomes a Gaussian mixture. We choosean injective decoding function f1 : RD RD1, D D1, as a composition of MLPs with LeakyReLUs andfull rank weight matrices having monotonically increasing row dimensions (Khemakhem et al., 2020b), withiid randomly sampled entries. We assume X1|Z N(f1(Z), 2 I) and set = 0.1, D = D1 = 2. f1 has a",
  "(j)Mixturebound+PoE": ": Continuous data modality in (a) and reconstructions using different bounds and fusion models in(b)-(e). The true latent variables are shown in (f), with the inferred latent variables in (g)-(j) with a lineartransformation indeterminacy. Labels are color-coded. single hidden layer of size D1 = 2. One realization of bi-modal data X, the true latent variable Z, as well asinferred latent variables and reconstructed data for a selection of different bounds and aggregation schemes,are shown in , with more examples given in Figures 6 and 7. We find that learning the aggregationmodel through a SumPooling model improves the data reconstruction and better recovers the ground truthlatents, up to rotations, in contrast to a PoE model. Simulating five different such datasets, the resultsin indicate first that our bound obtains better log-likelihood estimates for different fusion schemes.Second, it demonstrates the advantages of our new fusion schemes that achieve better log-likelihoods for bothbounds. Third, it shows the benefit of using aggregation schemes that have the capacity to accommodateprior distributions different from a single Gaussian. Also, MoE schemes lead to low MCC values, while PoEschemes have high MCC values. : Non-linear identifiable model with one real-valued modality and an auxiliary label acting as a secondmodality: The first four rows use a fixed standard Gaussian prior, while the last four rows use a Gaussianmixture prior with 5 components. Mean and standard deviation over 4 repetitions. Log-likelihoods areestimated using importance sampling with 64 particles.",
  "AggregationLLH ( = 1)MCC ( = 1)MCC ( = 0.1)LLH ( = 1)MCC ( = 1)MCC ( = 0.1)": "PoE-43.4 (10.74)0.98 (0.006)0.99 (0.003)-318 (361.2)0.97 (0.012)0.98 (0.007)MoE-20.5 (6.18)0.94 (0.013)0.93 (0.022)-57.9 (6.23)0.93 (0.017)0.93 (0.025)SumPooling-17.9 (3.92)0.99 (0.004)0.99 (0.002)-18.9 (4.09)0.99 (0.005)0.99 (0.008)SelfAttention-18.2 (4.17)0.99 (0.004)0.99 (0.003)-18.6 (3.73)0.99 (0.004)0.99 (0.007) SumPooling-15.4 (2.12)1.00 (0.001)0.99 (0.004)-18.6 (2.36)0.98 (0.008)0.99 (0.006)SelfAttention-15.2 (2.05)1.00 (0.001)1.00 (0.004)-18.6 (2.27)0.98 (0.014)0.98 (0.006)SumPoolingMixture-15.1 (2.15)1.00 (0.001)0.99 (0.012)-18.2 (2.80)0.98 (0.010)0.99 (0.005)SelfAttentionMixture-15.3 (2.35)0.99 (0.005)0.99 (0.004)-18.4 (2.63)0.99 (0.007)0.99 (0.007) Multiple modalities.Considering the same generative model for Z with a Gaussian mixture prior,suppose now that instead of observing the auxiliary label, we observe multiple modalities Xs RDs,Xs|Z N(fs(Z), 2 I), for injective MLPs fs constructed as above, with D = 10, Ds = 25, = 0.5",
  "AggregationLLHMCCLLHMCCLLHMCCLLHMCC": "PoE-250.9 (5.19)0.94 (0.015)-288.4 (8.53)0.93 (0.018)-473.6 (9.04)0.98 (0.005)-497.7 (11.26)0.97 (0.008)MoE-250.1 (4.77)0.92 (0.022)-286.2 (7.63)0.90 (0.019)-477.9 (8.50)0.91 (0.014)-494.6 (9.20)0.92 (0.004)SumPooling-249.6 (4.85)0.95 (0.016)-275.6 (7.35)0.92 (0.031)-471.4 (8.29)0.99 (0.004)-480.5 (8.84)0.98 (0.005)SelfAttention-249.7 (4.83)0.95 (0.014)-275.5 (7.45)0.93 (0.022)-471.4 (8.97)0.99 (0.002)-482.8 (10.51)0.98 (0.004) SumPooling-247.3 (4.23)0.95 (0.009)-269.6 (7.42)0.94 (0.018)-465.4 (8.16)0.98 (0.002)-475.1 (7.54)0.98 (0.003)SelfAttention-247.5 (4.22)0.95 (0.013)-269.9 (6.06)0.93 (0.022)-469.3 (4.76)0.98 (0.003)-474.7 (8.20)0.98 (0.002)SumPoolingMixture-244.8 (4.44)0.95 (0.011)-271.9 (6.54)0.93 (0.021)-464.5 (8.16)0.99 (0.003)-474.2 (7.61)0.98 (0.004)SelfAttentionMixture-245.4 (4.55)0.96 (0.010)-270.3 (5.96)0.94 (0.016)-464.4 (8.50)0.99 (0.003)-473.6 (8.24)0.98 (0.002) : Test LLH estimates for the joint data (M+S+T) and marginal data (importance sampling with512 particles). The first part of the table is based on the same generative model with shared latent variableZ R40, while the second part of the table is based on a restrictive generative model with a shared latentvariable Z R10 and modality-specific latent variables Zs R10.",
  "AggregationM+S+TMSTM+S+TMST": "PoE0.988 (0.000)0.940 (0.009)0.649 (0.039)0.998 (0.001)0.991 (0.004)0.977 (0.002)0.845 (0.000)1.000 (0.000)PoE+0.978 (0.002)0.934 (0.001)0.624 (0.040)0.999 (0.001)0.998 (0.000)0.981 (0.000)0.851 (0.000)1.000 (0.000)MoE0.841 (0.008)0.974 (0.000)0.609 (0.032)1.000 (0.000)0.940 (0.001)0.980 (0.001)0.843 (0.001)1.000 (0.000)MoE+0.850 (0.039)0.967 (0.014)0.708 (0.167)0.983 (0.023)0.928 (0.017)0.983 (0.002)0.846 (0.001)1.000 (0.000)SelfAttention0.985 (0.001)0.954 (0.002)0.693 (0.037)0.986 (0.006)0.991 (0.000)0.981 (0.001)0.864 (0.003)1.000 (0.000)SumPooling0.981 (0.000)0.962 (0.000)0.704 (0.014)0.992 (0.008)0.994 (0.000)0.983 (0.000)0.866 (0.002)1.000 (0.000) PoE+0.979 (0.009)0.944 (0.000)0.538 (0.032)0.887 (0.07)0.995 (0.002)0.980 (0.002)0.848 (0.006)1.000 (0.000)SumPooling0.987 (0.004)0.966 (0.004)0.370 (0.348)0.992 (0.002)0.994 (0.001)0.982 (0.000)0.870 (0.001)1.000 (0.000)SelfAttention0.990 (0.003)0.968 (0.002)0.744 (0.008)0.985 (0.000)0.997 (0.001)0.974 (0.000)0.681 (0.031)1.000 (0.000)",
  "MNIST-SVHN-Text": "Following previous work (Sutter et al., 2020; 2021; Javaloy et al., 2022), we consider a tri-modal dataset basedon augmenting the MNIST-SVHN dataset (Shi et al., 2019) with a text-based modality. Herein, SVHN con-sists of relatively noisy images, whilst MNIST and text are clearer modalities. Multi-modal VAEs havebeen shown to exhibit differing performances relative to their multi-modal coherence, latent classificationaccuracy or test LLH, see Appendix I for definitions. Previous works often differ in their hyperparameters,from neural network architectures, latent space dimensions, priors and likelihood families, likelihood weight-ings, decoder variances, etc. We have chosen the same hyperparameters for all models, thereby providinga clearer disentanglement of how either the variational objective or the aggregation scheme affects differentmulti-modal evaluation measures. In particular, we consider multi-modal generative models with (i) sharedlatent variables and (ii) private and shared latent variables. We also consider PoE or MoE schemes (denotedPoE+, resp., MoE+) with additional neural network layers in their modality-specific encoding functions sothat the number of parameters matches or exceeds those of the introduced PI models, see Appendix M.5for details. For models without private latent variables, estimates of the test LLHs in suggest thatour bound improves the LLH across different aggregation schemes for all modalities and different s (), with similar results for PE schemes, except for a Self-Attention model. More flexible fusion schemesyield higher LLHs for both bounds. Qualitative results for the reconstructed modalities are given in Figures",
  "Summary of experimental results": "We presented a series of numerical experiments that illustrate the benefits of learning more flexible aggre-gation models and that optimizing our variational objective leads to higher log-likelihood values. Overall,we find that for a given choice of aggregation scheme, our objective achieves a higher log-likelihood acrossthe different experiments. Likewise, fixing the variational objective, we observe that Sum-Pooling or Self-Attention encoders achieve higher multi-modal log-likelihoods compared to MoE or PoE schemes. Moreover,we demonstrate that our variational objective results in models that differ in their information theoreticquantities compared to those models trained with a mixture-based bound. In particular, our variational ob-jective achieves higher full-reconstruction terms with higher full rates across different data sets, aggregationschemes, and beta values. Conversely, the mixture-based bound improves the cross-prediction while havinghigher cross-rate terms.",
  "tationally expensive compared to fixed schemes. Mixture-based bounds might be preferred if one is interestedprimarily in cross-modal reconstructions": "Outlook.Using modality-specific encoders to learn features and aggregating them with a PI function isclearly not the only choice for building multi-modal encoding distributions. However, it allows us to utilizemodality-specific architectures for the encoding functions. Alternatively, our bounds could also be used, e.g.,when multi-modal transformer architectures (Xu et al., 2022) encode multiple modalities with modality-specific tokenization and embeddings onto a shared latent space. Our approach applies to general priordensities if we can compute its cross-entropy relative to the multi-modal encoding distributions. An examplewould be to apply it with more flexible prior distributions, e.g., as specified via score-based diffusion models(Vahdat et al., 2021). Likewise, diffusion models could be utilized to specify PI conditional prior distributionin the conditional bound by utilizing permutation-equivariant score models (Dutordoir et al., 2023; Yimet al., 2023; Mathieu et al., 2023). This work is supported by funding from the Wellcome Leap 1kD Program and by the RIE2025 Human Po-tential Programme Prenatal/Early Childhood Grant (H22P0M0002), administered by A*STAR. The com-putational work for this article was partially performed on resources of the National Supercomputing Centre,Singapore (",
  "S. Chatterjee, P. Diaconis, et al. The sample size required in importance sampling. The Annals of AppliedProbability, 28(2):10991135, 2018": "Z. Chen, V. Badrinarayanan, C.-Y. Lee, and A. Rabinovich. Gradnorm: Gradient normalization for adaptiveloss balancing in deep multitask networks. In International conference on machine learning, pages 794803.PMLR, 2018. J. Chung, K. Kastner, L. Dinh, K. Goel, A. C. Courville, and Y. Bengio. A recurrent latent variable modelfor sequential data. In Advances in neural information processing systems, pages 29802988, 2015.",
  "P. Diaconis and D. Freedman. Finite exchangeable sequences. The Annals of Probability, pages 745764,1980": "A. B. Dieng, Y. Kim, A. M. Rush, and D. M. Blei. Avoiding latent variable collapse with generative skipmodels. In The 22nd International Conference on Artificial Intelligence and Statistics, pages 23972405.PMLR, 2019. N. Dilokthanakul, P. A. Mediano, M. Garnelo, M. C. Lee, H. Salimbeni, K. Arulkumaran, and M. Shana-han.Deep unsupervised clustering with Gaussian Mixture Variational Autoencoders.arXiv preprintarXiv:1611.02648, 2016.",
  "J. Fliege and B. F. Svaiter. Steepest descent methods for multicriteria optimization. Mathematical methodsof operations research, 51:479494, 2000": "A. Foong, W. Bruinsma, J. Gordon, Y. Dubois, J. Requeima, and R. Turner. Meta-learning stationarystochastic process prediction with convolutional neural processes. Advances in Neural Information Pro-cessing Systems, 33:82848295, 2020. S. Gao, R. Brekelmans, G. Ver Steeg, and A. Galstyan. Auto-encoding total correlation explanation. In The22nd International Conference on Artificial Intelligence and Statistics, pages 11571166. PMLR, 2019. M. Garnelo, D. Rosenbaum, C. Maddison, T. Ramalho, D. Saxton, M. Shanahan, Y. W. Teh, D. Rezende,and S. A. Eslami. Conditional neural processes. In International conference on machine learning, pages17041713. PMLR, 2018a.",
  "G. Giannone and O. Winther.Scha-vae: Hierarchical context aggregation for few-shot generation.InInternational Conference on Machine Learning, pages 75507569. PMLR, 2022": "Y. Gong, H. Hajimirsadeghi, J. He, T. Durand, and G. Mori. Variational selective autoencoder: Learningfrom partially-observed heterogeneous data. In International Conference on Artificial Intelligence andStatistics, pages 23772385. PMLR, 2021. I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio.Generative adversarial nets. In Advances in neural information processing systems, pages 26722680, 2014.",
  "J. Heek, A. Levskaya, A. Oliver, M. Ritter, B. Rondepierre, A. Steiner, and M. van Zee. Flax: A neuralnetwork library and ecosystem for JAX, 2023. URL": "L. B. Hewitt, M. I. Nye, A. Gane, T. Jaakkola, and J. B. Tenenbaum. The variational homoencoder: Learningto learn high capacity generative models from few examples. arXiv preprint arXiv:1807.08919, 2018. I. Higgins, L. Matthey, A. Pal, C. Burgess, X. Glorot, M. Botvinick, S. Mohamed, and A. Lerchner. -VAE:Learning basic visual concepts with a constrained variational framework. In International conference onlearning representations, 2017.",
  "A. Javaloy, M. Meghdadi, and I. Valera. Mitigating Modality Collapse in Multimodal VAEs via ImpartialOptimization. arXiv preprint arXiv:2206.04496, 2022": "Z. Jiang, Y. Zheng, H. Tan, B. Tang, and H. Zhou. Variational deep embedding: an unsupervised andgenerative approach to clustering. In Proceedings of the 26th International Joint Conference on ArtificialIntelligence, pages 19651972, 2017. M. J. Johnson, D. Duvenaud, A. B. Wiltschko, S. R. Datta, and R. P. Adams. Structured vaes: Composingprobabilistic graphical models and variational autoencoders. arXiv preprint arXiv:1603.06277, 2016.",
  "M. Karami and D. Schuurmans. Deep probabilistic canonical correlation analysis. In Proceedings of theAAAI Conference on Artificial Intelligence, volume 35, pages 80558063, 2021": "I. Khemakhem, D. Kingma, R. Monti, and A. Hyvarinen. Variational Autoencoders and nonlinear ICA: Aunifying framework. In International Conference on Artificial Intelligence and Statistics, pages 22072217.PMLR, 2020a. I. Khemakhem, R. Monti, D. Kingma, and A. Hyvarinen.ICE-BeeM: Identifiable Conditional Energy-Based Deep Models Based on Nonlinear ICA. Advances in Neural Information Processing Systems, 33:1276812778, 2020b.",
  "H. Kim, A. Mnih, J. Schwarz, M. Garnelo, A. Eslami, D. Rosenbaum, O. Vinyals, and Y. W. Teh. Attentiveneural processes. In International Conference on Learning Representations, 2018": "J. Kim, J. Yoo, J. Lee, and S. Hong. Setvae: Learning hierarchical composition for generative modelingof set-structured data. In Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition, pages 1505915068, 2021. Y.-g. Kim, Y. Liu, and X.-X. Wei. Covariate-informed Representation Learning to Prevent Posterior Collapseof iVAE. In International Conference on Artificial Intelligence and Statistics, pages 26412660. PMLR,2023.",
  "A. Klami, S. Virtanen, and S. Kaski. Bayesian canonical correlation analysis. Journal of Machine LearningResearch, 14(4), 2013": "L. Kong, M. Q. Ma, G. Chen, E. P. Xing, Y. Chi, L.-P. Morency, and K. Zhang. Understanding maskedautoencoders via hierarchical latent variable models. In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition, pages 79187928, 2023. X. Kong and X. Zhang. Understanding masked image modeling via learning occlusion invariant feature. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 62416251,2023.",
  "G. E. Moran, D. Sridhar, Y. Wang, and D. M. Blei. Identifiable deep generative models via sparse decoding.arXiv preprint arXiv:2110.10804, 2021": "W. Morningstar, S. Vikram, C. Ham, A. Gallagher, and J. Dillon. Automatic differentiation variationalinference with mixtures. In International Conference on Artificial Intelligence and Statistics, pages 32503258. PMLR, 2021. R. Murphy, B. Srinivasan, V. Rao, and B. Riberio. Janossy pooling: Learning deep permutation-invariantfunctions for variable-size inputs. In International Conference on Learning Representations (ICLR 2019),2019.",
  "Q. Wang and H. Van Hoof. Doubly stochastic variational inference for neural processes with hierarchicallatent variables. In International Conference on Machine Learning, pages 1001810028. PMLR, 2020": "Q. Wang, B. Li, T. Xiao, J. Zhu, C. Li, D. F. Wong, and L. S. Chao. Learning deep transformer modelsfor machine translation. In Proceedings of the 57th Annual Meeting of the Association for ComputationalLinguistics, pages 18101822, 2019a. T. Wang and P. Isola. Understanding contrastive representation learning through alignment and uniformityon the hypersphere. In International Conference on Machine Learning, pages 99299939. PMLR, 2020.",
  "F. Zhang, B. Liu, K. Wang, V. Y. Tan, Z. Yang, and Z. Wang. Relational Reasoning via Set Transformers:Provable Efficiency and Applications to MARL. arXiv preprint arXiv:2209.09845, 2022a": "L. Zhang, V. Tozzo, J. Higgins, and R. Ranganath. Set Norm and Equivariant Skip Connections: Puttingthe Deep in Deep Sets. In International Conference on Machine Learning, pages 2655926574. PMLR,2022b. Y. Zhang, H. Jiang, Y. Miura, C. D. Manning, and C. P. Langlotz. Contrastive learning of medical visualrepresentations from paired images and text. In Machine Learning for Healthcare Conference, pages 225.PMLR, 2022c.",
  "AMulti-modal distribution matching": "Proof [Proof of Proposition 1] The equations for LS(xS) are well known for uni-modal VAEs, see for exampleZhao et al. (2019). To derive similar representations for the conditional bound, note that the first equation(ZXconditional) for matching the joint distribution of the latent and the missing modalities conditional on amodality subset follows from the definition of L\\S,pd(x\\S|xS)L\\S(x, , )dx\\S",
  "BMeta-learning and Neural processes": "Meta-learning.We consider a standard meta-learning setup but use slightly non-standard notations toremain consistent with notations used in other parts of this work. We consider a compact input or covariatespace A and output space X. Let D = M=1(A X)M be the collection of all input-output pairs. Inmeta-learning, we are given a meta-dataset, i.e., a collection of elements from D. Each individual dataset D = (a, x) = Dc Dt D is called a task and split into a context set Dc = (ac, xc), and target setDt = (at, xt). We aim to predict the target set from the context set. Consider, therefore, the prediction map",
  "LLNP\\C(x, a) =q(z|x, a) log p(xt|at, z)dz KL(q(z|a, x)|q(z|ac, xc))": "Note that this objective coincides with L\\C conditioned on the covariate values a and where C comprisesthe indices of the data points that are part of the context set. Using the variational lower bound LLNP\\Ccanyield subpar performance compared to another biased log-likelihood objective (Kim et al., 2018; Foong et al.,2020),",
  "(xt,at)Dtlog p(xt|at, zlc)": "for L importance samples zlc q(zc|xc, ac) drawn from the conditional prior as the proposal distribution.The required number of importance samples L for accurate estimation scales exponentially in the forwardKL(q(z|x, a)|q(z|xc, ac)), see Chatterjee et al. (2018). Unlike a variational approach, such an estimatordoes not enforce a Bayes-consistency term for the encoders and may be beneficial in the setting of finitedata and model capacity. Note that the Bayes consistency term for including the target set (xt, at) into thecontext set (xc, ac) writes as",
  "with 1=qagg(z)KL(q(xS|z)|p(xS|z))dz>0, 2=KL(qagg,S(z)|p(z))>0 and q(xS|z)=q(xS, z)/qagg(z)": "Moreover, if the bounds in (7) become tight with 1 = 2 = 0 in the hypothetical scenario of infinite-capacity decoders and encoders, one obtainspdLS = (1 ) Iq(XS, ZS) + HS. For > 1, maximizing LSyields an auto-decoding limit that minimizes Iq(xS, z) for which the latent representations do not encodeany information about the data, whilst < 1 yields an auto-encoding limit that maximizes Iq(XS, ZS) andfor which the data is perfectly encoded and decoded. The mixture-based bound can be interpreted as the maximization of a variational lower bound of Iq(XM, ZS)and the minimization of a variational upper bound of Iq(XS, ZS). Indeed, see also Daunhawer et al. (2022),",
  "Recalling thatpd(dx)LMixS(x) = DS Dc\\S RS,": "one can see that maximizing the first part of the mixture-based variational bound corresponds to maximizingDS Dc\\S as a variational lower bound of Iq(XM, ZS), when ignoring the fixed entropy of the multi-modaldata. Maximizing the second part of the mixture-based variational bound corresponds to minimizing RS asa variational upper bound of Iq(XS, ZS), see (15).",
  "=R\\S \\S,2": "Remark 17 (Total correlation based objectives) The objective suggested in Hwang et al. (2021) ismotivated by a conditional variational bottleneck perspective that aims to maximize the reduction of totalcorrelation of X when conditioned on Z, as measured by the conditional total correlation, see Watanabe(1960); Ver Steeg and Galstyan (2015); Gao et al. (2019), i.e.,",
  "(h) +": "where and standardize the input h by computing the mean, and the variance, respectively, over someaxis of h, whilst and define a transformation. LayerNorm (Ba et al., 2016) standardises inputs over thelast axis, e.g., (h) =1DDd=1 ,d, i.e., separately for each element. In contrast, SetNorm (Zhang et al.,2022b) standardises inputs over both axes, e.g., (h) =1",
  "D": "can be seen as a learnable non-symmetric kernel (Wright and Gonzalez, 2021; Cao, 2021). Conceptually, theattention encoder pools a learnable D-dimensional function v using a learnable context-dependent weightingfunction. While such attention models directly account for the interaction between the different encodings,a DeepSet aggregation approach may require a sufficiently high-dimensional latent space DP to achieveuniversal approximation properties (Wagstaff et al., 2022). Remark 21 (Multi-modal time series models) We have introduced a multi-modal generative modelin a general form that also applies to the time-series setup, such as when a latent Markov processdrives multiple time series.For example, consider a latent Markov process Z = (Zt)tN with prior dy-namics p(z1, . . . , zT ) = p(z1) Tt=2 p(zt|zt1) for an initial density p(z1) and homogeneous Markovkernels p(zt|zt1).Conditional on Z, suppose that the time-series (Xs,t)tN follows the dynamicsp(xs,1, . . . , xs,T |z1, . . . , zT ) = Tt=2 p(xs,t|zt) for decoding densities p(xs,t|zt). A common choice (Chunget al., 2015) for modeling the encoding distribution for such sequential (uni-modal) VAEs is to assume thefactorization q(z1, . . . zT |x1, . . . xT ) = q(z1|x1) Tt=2 q(zt|zt1, xt) for xt = (xs,t)sM, with initial encod-ing densities q(z1|x1) and encoding Markov kernels q(zt|zt1, xt). One can again consider modality-specificencodings hs = (hs,1, . . . , hs,T ), hs,t = hs,(xs,t), now applied separately at each time step that are thenused to construct Markov kernels that are permutation-invariant in the form of q(zt|zt1, h(xt,S)) =",
  "KLq(z, zS, z\\S|xM)q(z, zS, z\\S|xS)": "Remark 24 (Comparison with MMVAE+ variational bound) It is instructive to compare ourbound with the MMVAE+ approach suggested in Palumbo et al. (2023). Assuming a uniform maskingdistribution restricted to uni-modal sets so that S = {s} for some s M, we can write the bound fromPalumbo et al. (2023) as1MMs=1 LMMVAE+{s}(x) with",
  "sA r(zs) are additional trainable prior distributions": "Remark 25 (Cross-modal context variables and permutation-equivariant models) In contrast tothe PoE model, where the private encodings are independent, the private encodings are dependent in the Sum-Pooling model by conditioning on a sample from the shared latent space. The shared latent variable Z can beseen as a shared cross-modal context variable, and similar probabilistic constructions to encode such contextvariables via permutation-invariant models have been suggested in few-shot learning algorithms (Edwardsand Storkey, 2016; Giannone and Winther, 2022) or, particularly, for neural process models (Garnelo et al.,2018b;a; Kim et al., 2018). Permutation-equivariant models have been studied for stochastic processes whereinvariant priors correspond to equivariant posteriors (Holderrieth et al., 2021), such as Gaussian processesor Neural processes with private latent variables, wherein dependencies in the private latent variables canbe constructed hierarchically (Wang and Van Hoof, 2020; Xu et al., 2023).",
  "q(z|xS) = (z) exp [V (z), ,S(xS) log S(,S(xS))]": "where s and are base measures, Ts(xs) and V (z) are sufficient statistics, while the natural parameters,S(xS) and fs,(z) are parameterized by the decoder or encoder networks, respectively, with Zs and Sbeing normalizing functions. Note that we made a standard assumption that the multi-modal encodingdistribution has a fixed base measure and sufficient statistics for any modality subset. For fixed generativeparameters , we want to learn a multi-modal encoding distribution that minimizes over xS pd,",
  "HAlgorithm and STL-gradient estimators": "We consider a multi-modal extension of the sticking-the-landing (STL) gradient estimator (Roeder et al.,2017) that has also been used in previous multi-modal bounds (Shi et al., 2019).The gradient estima-tor ignores the score function terms when sampling q(z|xS) for variance reduction purposes because ithas a zero expectation.For the bounds (2) that involves sampling from q(z|xS) and q(z|xM), wethus ignore the score terms for both integrals. Consider the reparameterization with noise variables S,M p and transformations zS = tS(, S, xS) = finvariant-agg(, S, S, hS), for hS = h,s(xs)sS andzM = tM(, M, xM) = finvariant-agg(, M, M, hM), for hM = h,s(xs)sM . We need to learn only asingle aggregation function that applies and masks the modalities appropriately. Pseudo-code for computingthe gradients are given in Algorithm 1. If the encoding distribution is a mixture distribution, we applythe stop-gradient operation also to the mixture weights. Notice that in the case of a mixture prior and anencoding distribution that includes the mixture component, the optimal encoding density over the mixturevariable has no variational parameters and is given as the posterior density of the mixture component underthe generative parameters of the prior. Algorithm 1 Single training step for computing unbiased gradients of L(x).Input: Multi-modal data point x, generative parameter , variational parameters = (, ).Sample S .Sample S, M p.Set zS = tS(, S, xM) and zM = tM(, M, xM).Stop gradients of variational parameters = stop_grad().Set LS(, ) = log p(xS|zS) + log p(zS) log q(zS|xS).Set L\\S(, ) = log p(x\\S|zM) + log q(zM|xS) log q(zM|xM).",
  "Output: ,LS(, ) + L\\S(, )": "In the case of private latent variables, we proceed analogously and rely on reparameterizations zS =tS(, S, xS) for the shared latent variable zS q(z|xS) as above and zS= tS(, z, S, xS) =fequivariant-agg(, S, z, S, hS) for the private latent variables zS q(zS|z, xS). Moreover, we write PSfor a projection on the S-coordinates. Pseudo-code for computing unbiased gradient estimates for our boundis given in Algorithm 2. Algorithm 2 Single training step for computing unbiased gradients of L(x) with private latent variables.Input: Multi-modal data point x, generative parameter , variational parameters = (, ).Sample S .Sample S, S, \\S, M, M, \\M p.Set zS = tS(, S, xS), zS = tS(, zS, S, xS).Set zM = tM(, M, xM), zM = tM(, zM, M, xM).Stop gradients of variational parameters = stop_grad().Set LS(, ) = log p(xS|zS, zS) + log p(zS) log q(zS|xS) + log p(zS|zS) log q(zS|zS, xS).Set L\\S(, ) = log p(x\\S|zM) + log q(zM|xS) log q(zM|zM, xM) + log q(PS(zM)|zM, xS) + log p(P\\S(zM)|zM, zM) log q(zM|zM, xM).",
  "for zk q(|xS). This also allows to approximate the joint log-likelihood log p(x), and consequently alsothe conditional log p(x\\S|xS) = log p(x) log p(xS)": "Generative coherence with joint auxiliary labels.Following previous work (Shi et al., 2019; Sutteret al., 2021; Daunhawer et al., 2022; Javaloy et al., 2022), we assess whether the generated data share thesame information in the form of the class labels across different modalities. To do so, we use pre-trainedclassifiers clfs : Xs [K] that classify values from modality s to K possible classes. More precisely, forS M and m M, we compute the self- (m S) or cross- (m / S) coherence CSm as the empiricalaverage of1{clfm(xm)=y},",
  "over test samples x with label y where zS q(z|xS) and xm p(xm|zS).The case S = M \\ {m}corresponds to a leave-one-out conditional coherence": "Linear classification accuracy of latent representations.To evaluate how the latent representationcan be used to predict the shared information contained in the modality subset S based on a linear model,we consider the accuracy AccS of a linear classifier clfz : Z [K] that is trained to predict the labelbased on latent samples zS q(zS|xtrainS) from the training values xtrainSand evaluated on latent sampleszS q(z|xtestS) from the test values xtestS.",
  "JLinear models": "Data generation.We generate 5 data sets of N = 5000 samples, each with M = 5 modalities. We setthe latent dimension to D = 30, while the dimension Ds of modality s is drawn from U(30, 60). We set theobservation noise to = 1, shared across all modalities, as is standard for a PCA model. We sample thecomponents of bs independently from N(0, 1). For the setting without modality-specific latent variables, Wsis the orthonormal matrix from a QR algorithm applied to a matrix with elements sampled iid from U(1, 1).The bias coefficients Wb are sampled independently from N(0, 1/d). Conversely, the setting with privatelatent variables in the ground truth model allows us to describe modality-specific variation by consideringthe sparse loading matrix",
  "NDNj=D+1 j andWML = UD(D 2ML I)1/2 with the loading matrix identifiable up to rotations": "Model architectures.We estimate the observation noise scale based on the maximum likelihood esti-mate ML. We assume linear decoder functions p(xs|z) = N(W s z +b, 2ML), fixed standard Gaussian priorp(z) = N(0, I) and generative parameters = (W 1 , b1, . . . , W M, bM). Details about the various encodingarchitectures are given in . The modality-specific encoding functions for the PoE and MoE schemeshave a hidden size of 512, whilst they are of size 256 for the learnable aggregation schemes having additionalaggregation parameters .",
  "L.3Log-likelihood estimates": ": Test log-likelihood estimates for varying choices for the joint data (M+S+T) as well as forthe marginal data of each modality based on importance sampling (512 particles). Multi-modal generativemodel with a 40-dimensional shared latent variable. The second part of the Table contains reported log-likelihood values from baseline methods that, however, impose more restrictive assumptions on the decodervariances, which likely contributes to much lower log-likelihood values reported in previous works, irrespectiveof variational objectives and aggregation schemes.",
  "(, Aggregation)M+S+TMSTM+S+TMST": "(0.1, PoE+)0.983 (0.006)0.919 (0.001)0.561 (0.048)0.988 (0.014)0.992 (0.002)0.979 (0.002)0.846 (0.004)1.000 (0.000)(0.1, SumPooling)0.982 (0.004)0.965 (0.002)0.692 (0.047)0.999 (0.001)0.994 (0.000)0.981 (0.002)0.863 (0.005)1.000 (0.000)(1.0, PoE+)0.978 (0.002)0.934 (0.001)0.624 (0.040)0.999 (0.001)0.998 (0.000)0.981 (0.000)0.851 (0.000)1.000 (0.000)(1.0, SumPooling)0.981 (0.000)0.962 (0.000)0.704 (0.014)0.992 (0.008)0.994 (0.000)0.983 (0.000)0.866 (0.002)1.000 (0.000)(4.0, PoE+)0.981 (0.006)0.943 (0.007)0.630 (0.008)0.993 (0.001)0.998 (0.000)0.981 (0.000)0.846 (0.001)1.000 (0.000)(4.0, SumPooling)0.984 (0.004)0.963 (0.001)0.681 (0.009)0.995 (0.000)0.992 (0.002)0.980 (0.001)0.856 (0.001)1.000 (0.000)",
  "Results from Sutter et al. (2021), Sutter et al. (2020) and Hwang et al. (2021)": "MVAE0.96 (0.02)0.90 (0.01)0.44 (0.01)0.85 (0.10)MMVAE0.86 (0.03)0.95 (0.01)0.79 (0.05)0.99 (0.01)MoPoE0.98 (0.01)0.95 (0.01)0.80 (0.03)0.99 (0.01)MMJSD0.98 (NA)0.97 (NA)0.82 (NA)0.99 (NA)MVTCAE (w/o T)NA0.93 (NA)0.78 (NA)NA : Unsupervised latent classification for different s and models with shared latent variables only.Accuracy is computed with a linear classifier (logistic regression) trained on multi-modal inputs (M+S+T)or uni-modal inputs (M, S or T).",
  "NMNIST-SVHN-Text Decoder Model architectures": "For models with private latent variables, we concatenate the shared and private latent variables. We use aLaplace likelihood as the decoding distribution for MNIST and SVHN, where the decoder function learnsboth its mean as a function of the latent and a constant log-standard deviation at each pixel. Followingprevious works (Shi et al., 2019; Sutter et al., 2021), we re-weight the log-likelihoods for different modalitiesrelative to their dimensions.",
  "OCompute resources and existing assets": "A reference implementation is available at were performed on shared HPC systems. All experiments except .3 were run on aCPU server using one or two CPU cores. The experiments in .3 were run on a GPU server usingone NVIDIA A100. Our implementation is based on JAX (Bradbury et al., 2018) and Flax (Heek et al., 2023).We com-pute the mean correlation coefficient (MCC) between true and inferred latent variables following Khe-makhem et al. (2020b), as in and follow the data and modelgeneration from Khemakhem et al. (2020a), in .2, as wellas from Zhang et al. (2019) for generating the missingnessmechanism.In our MNIST-SVHN-Text experiments, we use code from Sutter et al. (2021),"
}