{
  "Abstract": "Interest in automatically searching for Transformer neural architectures for machine trans-lation (MT) has been increasing. Current methods show promising results in in-domain set-tings, where training and test data share the same distribution. However, in real-world MTapplications, it is common that the test data has a different distribution than the trainingdata. In these out-of-domain (OOD) situations, Transformer architectures optimized for thelinguistic characteristics of the training sentences struggle to produce accurate translationsfor OOD sentences during testing. To tackle this issue, we propose a multi-level optimiza-tion based method to automatically search for neural architectures that possess robust OODgeneralization capabilities. During the architecture search process, our method automati-cally synthesizes approximated OOD MT data, which is used to evaluate and improve thearchitectures ability of generalizing to OOD scenarios. The generation of approximatedOOD data and the search for optimal architectures are executed in an integrated, end-to-end manner. Evaluated across multiple datasets, our method demonstrates strong OODgeneralization performance, surpassing state-of-the-art approaches.Our code is publiclyavailable at",
  "Introduction": "Machine Translation (MT) is a pivotal area within natural language processing (NLP), enabling the au-tomatic translation of texts from one language to another. The emergence of Neural Machine Translation(NMT) (Bahdanau et al., 2014) has revolutionized the field by eliminating the need for labor-intensive featureengineering through an end-to-end training mechanism. Many deep neural networks, such as Recurrent Neu-ral Networks (RNNs) Bahdanau et al. (2014) and Long Short-Term Memory (LSTM) Hochreiter & Schmid-huber (1997) networks, have propelled advancements in NMT. Notably, the Transformer model Vaswaniet al. (2017), which leverages self-attention to capture long-range dependency between tokens, has signifi-cantly enhanced MT performance. Despite these advancements, current NMT models are manually designed,",
  "Published in Transactions on Machine Learning Research (12/2024)": ": Architectures searched by darts on the WMT (En-De) dataset. The architecture consists of encoderlayers and decoder layers. Multiple identical layers with searched architecture are stacked to construct thefinal model. Green boxes represent search nodes that are optimized during the search phase, while yellowboxes indicate predefined layers which are fixed during the search process.",
  "Stage III": ": Overview of our method. It consists of three learning stages which are performed end-to-end.In the first stage, we train the Transformers weight parameters with its architecture tentatively fixed. Inthe second stage, we generate a synthetic OOD MT dataset by adding learnable perturbations to real MTdata. In the third stage, we evaluate the trained Transformers performance on the OOD data and updateits architecture by maximizing this performance.",
  "Neural Architecture Search (NAS)": "The objective of NAS is to autonomously discover neural network architectures that have the potentialto outperform those created by humans.In recent years, NAS has witnessed significant advancements.NAS methods are categorized into three primary types: reinforcement learning (RL)-based, evolutionaryalgorithms (EA)-based, and gradient-based. A detailed discussion of RL-based and EA-based NAS methodsis provided in Appendix A. To address the issue of high computational cost of RL-based and EA-based NASapproaches, researchers have proposed differentiable search techniques Cai et al. (2019); Liu et al. (2018);Xie et al. (2019). These methods conceptualize each potential architecture as a configuration of numerousbuilding blocks, with each block assigned a specific importance weight. The process of architecture search isthen simplified to optimizing these weights, a task achievable through differentiable optimization techniqueslike gradient descent.Specifically, many gradient-based NAS methods use one-shot architecture search,in which architecture weights are learned by minimizing loss on a validation dataset while model weightsare learned simultaneously by minimizing loss on a training dataset. This approach significantly enhancescomputational efficiency compared to RL-based methods. Recently, differentiable NAS techniques have beenused to search for optimal Transformer architectures as well Zhao et al. (2021). Our method also belongs tothe category of gradient-based NAS, but unlike previous approaches, it specifically focuses on searching forarchitectures that optimize out-of-domain generalization.",
  "Out-of-Domain Generalizable Learning": "Existing methods for improving the OOD generalizability of ML models can be roughly categorized intotwo paradigms. The first paradigm learns domain-invariant latent representations so that the discrepancyof different domains is mitigated in the latent space. Domain Invariant Component Analysis Muandet et al.(2013) is a kernel-based algorithm that learns a transformation to minimize the difference between marginaldistributions of different domains. Multi-task Autoencoder (MTAE)Ghifary et al. (2015) jointly trainsan encoder and several domain specific decoders to learn invariant representations with multi-task learning.MultiTCA Grubinger et al. (2017) learns domain-invariant representations by minimizing the discrepancybetween domains while maximizing the variance within each domain. EISNet Wang et al. (2020a) learns togeneralize across domains by solving a momentum metric learning task and a self-supervised auxiliary task. The second paradigm of methods focuses on generating additional data and leveraging it to improve OODgeneralizability. Domain Randomization Tobin et al. (2017) creates synthetic OOD images by randomlycombining elements from different environment simulators. GUD Volpi et al. (2018) employs adversarialdomain augmentation to iteratively generate fictitious worst-case target distributions. Jigen Carlucci et al.(2019) augments data by transforming it into jigsaw puzzles to simulate OOD scenarios. CrossGrad Shankar",
  "Noisy Embeddings": "It is worth noting that noisy embedding based data augmentation methods not only benefit OOD gener-alization tasks, as mentioned in the previous section, but enhance a models in-domain capability as well.For instance, FreeLB Zhu et al. (2020) adds adversarial perturbations to word embeddings, enhancing theperformance of Transformer-based models on natural language understanding and commonsense reasoningtasks. WaffleCLIP Roth et al. (2023) adds random words to image captions during the fine-tuning of vision-language models, yielding performance gains. NAYER Tran et al. (2023) introduces noisy layers to generateperturbed embeddings for image text captions, improving performance on data-free knowledge distillationtasks. BFTSS Somayajula et al. (2023) learns semantically consistent perturbations for embedding vectors,effectively enhancing performance on NLU tasks Wang (2018). NEFTune Jain et al. (2024) introduces ran-dom noise to the embedding vectors of the training data during the forward pass of fine-tuning, improvingthe outcome of instruction fine-tuning. Our method employs a novel strategy to generate noisy embed-dings under a multi-level optimization framework, which enhances performance in both out-of-domain andin-domain scenarios.",
  "Bi-level and Multi-level Optimization": "A wide range of machine learning applications leverage bi-level optimization (BLO), which represents thesimplest form of multi-level optimization, characterized by a two-tier hierarchy. Specifically, BLO consists oftwo nested optimization problems with a mutual dependency. These applications include, but are not limitedto, neural architecture search Liu et al. (2018); Zhang et al. (2021), hyperparameter optimization Baydinet al. (2017); Feurer et al. (2015); Franceschi et al. (2017; 2018); Lorraine et al. (2020); Maclaurin et al.(2015), reinforcement learning Hong et al. (2020); Konda & Tsitsiklis (1999); Rajeswaran et al. (2020),data valuation Ren et al. (2020); Shu et al. (2019); Wang et al. (2020b), meta learning Finn et al. (2017);Rajeswaran et al. (2019), and label correction Zheng et al. (2019). As BLO gains popularity, the focus has also broadened to multi-level optimization (MLO) involving morecomplex hierarchical structures Garg et al. (2022); He et al. (2021); Raghu et al. (2021); Somayajula et al.(2022); Such et al. (2020); Xie & Du (2022). MLO consists of more than two nested optimization problemswith more complicated dependencies. Recent research in this area has shown a growing interest in con-structing and optimizing multi-stage machine learning pipelines end-to-end using MLO. However, solvingMLO problems poses computational challenges due to its complex structure. Sato et al. (2021) proposeda gradient-based solver. Choe et al. (2022) developed a software which enables users to compute hyper-gradients within MLO problems with multiple approximation methods easily and efficiently. To cope withhigh memory and computation costs associated with large-scale multi-level optimization, Choe et al. (2023)developed a distributed framework. In this paper, we propose a framework with three nested optimizationproblems, and solve this MLO problem with gradient-based algorithm.",
  "Method": "Let W and A denote the weight parameters and learnable architecture of a Transformer model used formachine translation (MT). This model takes a source sentence as input and generates a target sentence.Let Dtr = {(xi, yi)}Ni=1 denote an MT training dataset where xi is an input source sentence and yi is",
  "Stage I": "In the first stage, we train the weight parameters W of the Transformer with its architecture A tentativelyfixed. Given a source sentence xi from Dtr, it is fed into the Transformer which generates a target sentencef(E(xi); W, A), where E is an embedding module for sentences. A teacher-forcing based negative log likeli-hood (NLL) loss l is used to measure the discrepancy between f(E(xi); W, A) and the ground-truth targetsentence yi. We learn W by minimizing l defined on each training example. This stage amounts to solvingthe following optimization problem:",
  "i=1l(f(E(xi); W, A), yi)(1)": "where W (A) denotes that the optimal solution W depends on A since W depends on the loss function,which depends on A. Note that A cannot be learned at this stage. Otherwise, the result will be an overfittedsolution where A perfectly fits the training data but performs poorly on the test data.",
  "Stage II": "In the second stage, we generate an additional MT dataset to learn an architecture with robust OODgeneralization capabilities. Since real OOD data is unavailable, we create a close approximation of OODdata for use in architecture search in the next stage (see below). For each real training example (xi, yi) Dtr,we first map an input sentence xi to a sentence embedding with the embedding module E. We add a smalllearnable perturbation i to E(xi) in a way that the perturbed embedding E(xi) + i satisfies the followingtwo conditions. First, the translation yi for E(xi) can still be used as a translation for E(xi)+i. Second, theperturbed source sentence embeddings S({i}Ni=1) = {E(xi)+i}Ni=1 should have a large domain discrepancywith the original source sentence embeddings S = {E(xi)}Ni=1. To satisfy the first condition, we use thetrained MT model with weights W (A) to generate a translation f(E(xi) + i; W (A), A) and learn i tominimize the discrepancy (measured using the NLL l) between f(E(xi) + i; W (A), A) and yi. To satisfythe second condition, we use the Maximum Mean Discrepancy (MMD) Gretton et al. (2012) M to measurethe domain difference between S({i}Ni=1) and S, and learn {i}Ni=1 to maximize M(S({i}Ni=1), S). Let k(, )be a kernel function. Given two distributions p and q, their MMD is defined as:",
  "(6)": "The three stages are mutually dependent on each other. The output W (A) of the first stage is the input ofthe loss function in the second stage. The outputs of the first two stages, including W (A) and {i (A)}Ni=1,are the inputs to the loss function in the third stage. The optimization variable A in the third stage is usedto define the loss functions in the first and second stage. By solving the three interdependent optimizationproblems together, we can perform the three learning stages end-to-end.",
  "Search Space and Search Method": "We set the the candidate operation set in our experiments following So et al. (2019); Zhao et al. (2021),which is detailed in Appendix B. Finally, we apply dropout Srivastava et al. (2014) to the output of anoperation, then add this result to the original input to form a residual connection He et al. (2016). Afterthis, layer normalization Ba et al. (2016) is applied to the combined output. We adopt a differentiable search method Liu et al. (2018), where each candidate operation is associated witha selection variable representing how likely this operation is going to be selected into the final architecture.The architecture A is represented by the collection of selection variables and architecture search amountsto learning these variables. After learning, operations associated with the highest-valued selection variablesare preserved to construct the final architecture.",
  "i=1l(f(E(xi) + i; W , A), yi)(9)": "where a is a learning rate. After A is updated, it is plugged into Eq.(7) and Eq.(8), yielding a new W anda new i. These update steps iterate until convergence, yielding the final learned A A, as summarizedin Algorithm 1. We provide additional details about our optimization algorithm in Appendix C, includingthe detailed computation of the gradient in Eq. (9) and a discussion on the convergence properties of themethod.",
  "Datasets": "We evaluate OOD generalization performance on low-resource languages, including English-Igbo (En-Ig),English-Hausa (En-Ha), and English-Irish (En-Ga) language pairs. Igbo, part of the Niger-Congo languagefamily, incorporates diacritics, presenting challenges for MT tasks Orife (2018); Dossou & Emezue (2021).Hausa, a Chadic language within the Afroasiatic phylum, presents unique linguistic features. Irish, a Goideliclanguage of the Celtic family, employs an array of grammatical mutations and a VSO (verb-subject-object)word order, which pose challenges for machine translation tasks Dhonnchadha et al. (2003). Following Ahiaet al. (2021), we obtain training data for En-Ig, En-Ha and En-Ga from the CCMatrix parallel corpusSchwenk et al. (2019), which offers the largest collection of high-quality, web-based bitexts for machinetranslation. The test data for En-Ig and En-Ha is from the Gnome and Ubuntu datasets, and the test datafor En-Ga is from the Flores dataset, all considered OOD for CCMatrix. We include detailed description ofthese datasets in Appendix D. Furthermore, we conduct experiments on high-resource languages, following So et al. (2019) and Zhao et al.(2021), using the following training datasets: 1) WMT18 English-German (En-De) without ParaCrawl,consisting of 4.5 million sentence pairs; 2) WMT14 English-French (En-Fr), comprising 36 million sentencepairs; and 3) WMT18 English-Czech (En-Cs) without ParaCrawl, with 15.8 million sentence pairs. The testdata for these language pairs is from the WMT-Chat and WMT-Biomedical datasets, both considered OODfor WMT14 and WMT18. Detailed descriptions can be found in Appendix D.",
  "Baselines": "Our method is evaluated against the vanilla Transformer architecture Vaswani et al. (2017). We also compareour method with differentiable architecture search baselines including DARTS Liu et al. (2018) and PDARTSChen et al. (2019), that balance performance and computational efficiency.We did not compare withevolutionary algorithm based methods So et al. (2019) since they are computationally very expensive. Weinclude detailed description of baseline methods in Appendix E. Our method represents a general framework that can be integrated with various differentiable NAS methods.For example, the search space and search strategy in our method can be set to those in either DARTS orPDARTS. We use Ours-darts and Ours-pdarts to denote that our method is integrated with DARTS andPDARTS respectively.",
  "Experimental Settings": "Search configuration.We follow the settings in Zhao et al. (2021) for architecture search. Both thevanilla DARTS and Ours-darts utilize two identical encoder and two identical decoder layers during thesearch phase. Each encoder layer consists of Self Attention Search Node Search Node, whereaseach decoder layer consists of Self Attention Cross Attention Search Node Search Node. Onlythe Search Node is searched, while the architectures of Self Attention and Cross Attention are fixed.Zhao et al. (2021) empirically shows that architecture search with this configuration yields better results.The layers with searched architecture are stacked to construct the final model with six encoder layers and",
  "Transformer 2.53(0.07) 2.04(0.07) 0.99(0.12) 0.52(0.12)30.79(0.20)28.71(0.48)54.08(0.11)26.90(0.58)19.84(0.26)": "DARTS1.22(0.26) 1.05(0.39) 0.75(0.04) 0.38(0.02)26.63(0.08)29.03(0.25)55.39(0.94)18.30(0.26)22.62(0.45)Ours-darts2.97(0.01) 2.39(0.07) 1.41(0.07) 0.83(0.08) 33.19(0.20) 30.21(0.57) 58.71(0.46) 28.06(0.20) 25.08(0.38) PDARTS1.28(0.25) 1.43(0.51) 0.96(0.07) 0.49(0.14)29.56(0.25)29.60(1.27)58.24(0.64)27.41(0.22)23.53(0.27)Ours-pdarts 3.27(0.20) 2.81(0.21) 1.58(0.08) 1.01(0.07) 33.33(0.24) 31.29(0.80) 59.73(0.94) 28.09(0.42) 24.91(0.28) :BLEU scores (mean and standard deviation across three runs) in nine out-of-domain (OOD)generalization experiments, where each column corresponds to an experimental setting: (1-2) The trainingdataset was En-Ig (CCMatrix) and the test datasets included En-Ig (Gnome) and En-Ig (Ubuntu); (3-4)The training dataset was En-Ha (CCMatrix) and the test datasets included En-Ha (Gnome) and En-Ha(Ubuntu); (5) The training dataset was En-Ga (CCMatrix) and the test dataset was En-Ga (Flores); (6-7)The training dataset was En-De (CCMatrix) and the test datasets included En-De (WMT-Chat) and En-De (WMT-Biomedical); (8) The training dataset was En-Fr (CCMatrix) and the test dataset was En-Fr(WMT-Chat); and (9) The training dataset was En-Cs (CCMatrix) and the test dataset was En-Cs (WMT-Biomedical). Double-sided t-tests were conducted between our methods and baselines, with p-values lessthan 0.05, indicating statistically significant improvements achieved by our methods over baselines. six decoder layers. PDARTS and Ours-pdarts adopt a progressive learning approach, where the number ofencoder and decoder layers increases from 2 to 4 to 6 through the search process. Simultaneously, the sizeof the operation set in the encoder layer is reduced from 15 to 10 to 5, and in the decoder layer from 16 to11 to 6. Hyperparameter settings.Following Vaswani et al. (2017), we utilize 6 encoder and decoder layers, ahidden size of 512, a filter size of 2048, and 8 attention heads for vanilla Transformers, DARTS, PDARTS,Ours-darts and Ours-pdarts. For Ours-darts and Ours-pdarts, the radial basis function (RBF) kernel is usedto compute maximum mean discrepancy (MMD) used in Stage II. The tradeoff parameter is set to 1.5. For the optimization of W in our methods and baselines, the same hyperparameter settings as Vaswaniet al. (2017) are used, including the learning rate and its scheduler, with warm-up steps set to 4000. TheAdam Kingma & Ba (2014) optimizer with 1 = 0.9, 2 = 0.98, and = 109 is used. We increase thelearning rate linearly for the first warmup training steps, and decrease it thereafter proportionally to theinverse square root of the step number. For the optimization of architecture weights A, both our methodsand the baselines use the same hyperparameters following Liu et al. (2018), employing a constant learningrate of 3 104 and a weight decay of 103, with the Adam optimizer (1 = 0.9, 2 = 0.98, and = 109)being used. The perturbation , in our framework, is optimized using an Adam optimizer with 1 = 0.9,2 = 0.98, and = 109 and a constant learning rate of 103. We set the dropout rate to 0.1 and employ labelsmoothing with a value of 0.1 during architecture search. We set the batch size to 4096 for all experiments. The maximum sentence length is set to 256 for all experiments. Tokenization is performed using Moses1,which is a rule-based tokenizer. BLEU Papineni et al. (2002) is used as the evaluation metric. We employbeam search during inference with a beam size of 4 and a length penalty = 0.6. All the experiments wereconducted on Nvidia A100 GPU.",
  "Results on Out-of-Domain Generalization": "We evaluate the OOD generalization performance of our method in nine experiments.In experiment 1and 2, the training dataset was En-Ig (CCMatrix) and the test datasets included En-Ig (Gnome) and En-Ig(Ubuntu). In experiment 3 and 4, the training dataset was En-Ha (CCMatrix) and the test datasets includedEn-Ha (Gnome) and En-Ha (Ubuntu). In experiment 5, the training dataset was En-Ga (CCMatrix) and",
  "PDARTS45.38 1.0140.34 0.6228.11Ours-pdarts59.23 2.0957.41 1.9028.24": ":BLEU scores of in-domain generalization experiments, where training and test data are fromthe same dataset. Each of the three datasets, including En-Ig (CCmatrix), En-Ha (CCmatrix), and En-De(WMT), is divided into training, validation, and test splits. Models are trained on the training split, evaluatedon the test split (results shown in this table), and the validation split is used for tuning hyperparameters. the test dataset was En-Ga (Flores). In experiments 6 and 7, the training dataset was En-De (CCMatrix)and the test datasets included En-De (WMT-Chat) and En-De (WMT-Biomedical). In experiment 8, thetraining dataset was En-Fr (CCMatrix) and the test dataset was En-Fr (WMT-Chat). In experiment 9, thetraining dataset was En-Cs (CCMatrix) and the test dataset was En-Cs (WMT-Biomedical). Additionally,we evaluate the OOD generalization performance of our method across language pairs. Specifically, thedataset for architecture search was En-De (WMT) and the datasets for retraining and testing includedEn-Fr (WMT) and En-Cs (WMT). and 2 show the results. As can be seen, Ours-darts and Ours-pdarts demonstrate superior per-formance compared to DARTS and PDARTS, respectively. In particular, Ours-pdarts achieves the bestperformance on 8 out of 9 datasets across all methods. This is because our method automatically generatesapproximated OOD data (in stage II) and optimizes the architecture by minimizing the MT loss on thisgenerated data (in stage III). By doing so, our method aims to enhance the architectures ability to generalizeto real OOD data. Such a mechanism is lacking in DARTS and PDARTS. Furthermore, Ours-darts andOurs-pdarts surpass the performance of the vanilla Transformer architecture, which further demonstratesthe superior OOD generalization capability of our methods, due to its mechanism of generating additionaldata to improve the OOD generalization performance of architectures. In contrast, the manual designedTransformer architecture does not take OOD generalization into account.",
  "Results on In-Domain Generalization": "In addition to the OOD generalization performance, we also evaluated the in-domain generalization perfor-mance of searched architectures, where the training and test data are disjoint splits of the same dataset.Datasets used for this evaluation include WMT18 English-German (En-De), WMT14 English-French (En-Fr), WMT18 English-Czech (En-Cs), CCMatrix English-Igbo (En-Ig) and CCMatrix English-Hausa (En-Ha). shows the results. As can be seen, Ours-darts outperforms DARTS; Ours-pdarts outperformsPDARTS; and both of our methods outperform vanilla Transformer. This demonstrates the superior per-",
  "To better evaluate the effectiveness of individual components in our framework, we perform several ablationstudies": "Sensitivity to the tradeoff parameter .We investigate how the trade-off parameter in our frameworkaffects downstream performance. is varied within the set {0.5, 1.0, 1.5, 2.0}. The WMT18 English-Germandataset was used as the training data and WMT14 English-French was used as test data. The study wasconducted using Ours-darts. shows how the BLEU score on the WMT14 English-French test setvaries as increases.As can be seen, a value in the middle ground yields the best performance.Avery small such as 0.5 reduces the contribution from the MMD, making the domain difference betweengenerated data and real training data small. As a result, the generated data cannot effectively improve theOOD generalization performance of the architecture. Conversely, a higher increases the contribution fromthe MMD, encouraging a larger domain difference but with the risk of deviating too much such that thetranslation of E(xi) + i no longer corresponds to yi. Thus, achieving a balance is essential by choosing anoptimal . From the results, we observe that = 1.5 is optimal.",
  "PDARTS39.3925.33No-MMD-pdarts37.8525.09L2-pdarts39.4925.44No-Stage-II-pdarts39.5325.49Ours-pdarts39.7325.89": ":BLEU scores on the test set of WMT English-French(En-Fr) and English-Czech (En-Cs) datasets, under various abla-tion study settings: 1) removing the MMD loss term (No-MMD),2) replacing MMD loss with L2 loss (L2), and 3) removing thesecond stage from our framework (No-Stage-II). Impact of the MMD loss term.We study the impact of the MMD loss term in our framework andits effect on downstream performance. This experiment is carried out by 1) omitting the MMD loss term(denoted as No-MMD) and 2) replacing the MMD loss term with an L2 regularization on (denoted as L2).The training data was WMT14 English-German. The test data was WMT14 English-French and WMT18English-Czech. shows the results. We observe that No-MMD-darts and No-MMD-pdarts underperform compared to Ours-darts and Ours-pdarts, respectively, due to the absence of the MMD loss term. This absence leads to a degenerate solutionof = 0, resulting in the generated additional data identical to the real training data Dtr and increasing therisk of overfitting as both model weights and architecture parameters are optimized on Dtr. This is evidentfrom the inferior performance of No-MMD-darts and No-MMD-pdarts compared to DARTS and PDARTS,which learn model weights and architecture parameters on disjoint splits of the training dataset to preventoverfitting. The results underscore the critical role of the MMD loss in creating a significant domain gap",
  "between the generated MT data and Dtr, which is crucial for improving the OOD generalization performanceof searched architectures subsequently": "Furthermore, we observe that Ours-darts and Ours-pdarts outperform L2-darts and L2-pdarts, respectively,on both language pairs. This indicates that MMD outperforms the L2 distance in quantifying and amplifyingthe differences between domains. The calculation of MMD involves all data examples from both datasets,thereby more accurately reflecting dataset differences at a distributional level. It achieves this by calculat-ing the kernel-based dissimilarity for each pair of data examples, both within and between datasets, thencombining these dissimilarity measures to form a comprehensive MMD score. In contrast, the L2 distancemeasures are limited to comparing pairs comprising a real sentence and its perturbed counterpart, focusingsolely on individual data example disparities rather than assessing the dataset level difference. On the otherhand, L2-darts and L2-pdarts outperform vanilla DARTS and PDARTS. Although L2 distance is not aseffective as MMD, it can still enlarge the domain difference between generated data and real data, therebyimproving architectures OOD generalization performance. Impact of removing Stage II.In this study, we explore the significance of Stage II within our frameworkby removing it (denoted as No-Stage-II). Rather than acquiring the perturbations through learning, werandomly generate Gaussian noise and employ this as the perturbations.These are then added to theembeddings of real MT sentences to create the synthetic OOD data for neural architecture search. Afterremoving Stage II, the original three-level optimization framework is reduced to a bi-level formulation:",
  "where i is drawn from a Gaussian distribution with a mean of 2.7689 105 and a standard deviation of0.0026). These parameters were derived from the statistics of the learned in Ours-darts": "The results, presented in , reveal that both No-Stage-II-darts and No-Stage-II-pdarts exhibit de-creased performance compared to their counterparts Ours-darts and Ours-pdarts. This outcome stronglyunderscores the importance of Stage II, which learns perturbations instead of setting them randomly. InOurs-darts and Ours-pdarts, the perturbations are learned to create the optimal synthetic OOD data that isbest suitable for evaluating and improving the OOD generalization performance of architectures. In contrast,the generated approximated OOD data in No-Stage-II-darts and No-Stage-II-pdarts is randomly generated,which is sub-optimal. Notably, No-Stage-II-darts and No-Stage-II-pdarts still outperform vanilla DARTSand PDARTS, suggesting that even randomly generated perturbations to create additional data can improvethe OOD generalization performance of architectures, compared to not using OOD data at all. Impact of generating out-of-domain validation data.We investigate the effect of generating anadditional dataset to approximate OOD data for architecture search within our framework, rather thanrelying on pre-existing OOD datasets. For this ablation study, we conducted experiments on the English-Igbo language pair. We used CCMatrix as the training set and Gnome as the OOD validation set to performarchitecture search using DARTS. Ubuntu was used as the test set. The results, presented in , showthat our method outperforms this ablation setting, highlighting the importance of generating synthetic OODdataset within our framework. Compared with a fixed OOD dataset, the generated dataset can be morediverse since it is explicitly optimized to be OOD. This increased diversity can lead to more robust OODgeneralization performance.",
  "Attention": ": Architectures searched by Ours-darts and Ours-pdarts on the WMT (En-De) dataset.Eacharchitecture consists of encoder layers and decoder layers. Multiple identical layers with searched architectureare stacked to construct the final model. Green boxes represent search nodes that are optimized during thesearch phase, while yellow boxes indicate predefined layers which are fixed during the search process.",
  "Ours-pdarts28.2439.7325.89Ours-pdartsL29.2942.9727.26": ":BLEU score comparison between Ours-darts, Ours-pdarts and their larger architecture versionsOurs-dartsL and Ours-pdartsL, across En-De, En-Fr, and En-Cs language pairs, demonstrating the benefitsof model scaling. Impact of model size.To investigate the impact of final model size on machine translation performance,we conducted search to develop larger architectures, denoted as Ours-dartsL and Ours-pdartsL. Followingthe Transformer-big architecture design in Vaswani et al. (2017), our large model architecture consists of 6encoder and decoder layers, a hidden size of 1024, a filter size of 4096, and 16 attention heads. Evaluationswere conducted across three language pairs: English-German, English-French, and English-Czech, as detailedin . The results demonstrate that increasing the final model size significantly improves translationperformance on these language pairs, underscoring the importance of model scaling in achieving highertranslation accuracy.",
  "Discussion": "In-domain generalization performance.The superior in-domain generalization performance of ourmethods can be attributed to several interrelated factors. Firstly, synthesizing approximated OOD dataduring the architecture search phase exposes the model to a wider array of linguistic features, enhancing itsability to generalize across diverse scenarios. This exposure likely leads to the selection of more robust neuralarchitectures that are inherently better at handling not only OOD but also in-domain data. Additionally,training with OOD data may serve as a form of regularization, preventing overfitting and promoting a moregeneralized understanding of the language, which in turn facilitates in-domain generalization. Model size and computational costsWe compare the total number of parameters for our methodand all baselines in . Ours-darts has fewer parameters than DARTS and Transformer. Ours-pdartshas fewer parameters than PDARTS. This indicates that our method significantly improves the OOD MTperformance without increasing model size. The computational costs of conducting architecture search withOurs-darts, DARTS, Ours-pdarts, and PDARTS are presented in , in terms of GPU days.Ourmethods, including Ours-darts and Ours-pdarts, do not incur significantly higher costs (less than 10%)compared to baseline methods. This marginal increase in computational expense is notably outweighed bythe significant performance enhancements observed across various OOD machine translation tasks.",
  ":Comparison of search costs in GPU days for baseline methods DARTS, PDARTS, and our methodsOurs-darts and Ours-pdarts": "Analysis of searched architecturesWe analyze the architectures searched by our methods, shownin , with green boxes indicating the search nodes optimized during the search phase, and yellowboxes representing nodes with predefined architectures.The architecture learned by Ours-darts utilizesself-attention for input processing and GLUs for selective feature enhancement in the encoder, while itsdecoder refines the output iteratively with multiple cross-attention layers.In contrast, the architecturesearched by Ours-pdarts, which yields the best results, features a streamlined encoder that employs self-attention for feature extraction. The extracted features are further maintained through identity layers. Thedecoder is more complex, incorporating both self-attention for refining its own output and cross-attention forintegrating encoder information, further enriched by two FFNs for improved feature processing. Interestingly,the searched architectures opt to exclude convolution operations, implying that the self-attention and cross-attention mechanisms adequately capture the relevant features for the MT task. This effectiveness is likelyaugmented by operations such as GLUs and FFNs, which improve feature representation without the needfor convolutions. This observation is in line with the architecture choices of popular Transformer modelsthat also omit convolution operations Devlin et al. (2018); Liu et al. (2019); Lewis et al. (2019); He et al.(2020); Yang et al. (2019); Brown et al. (2020). Further comparisons between the architectures searched byour method and those found by baseline methods are provided in Appendix F.",
  "Conclusions and Future Works": "In this paper, we propose a novel multi-level optimization framework for searching OOD-generalizable Trans-former architectures for MT tasks. Our method automatically generates additional data from the availabletraining data that approximates OOD data, which is subsequently used to enhance the OOD generalizationperformance of the searched architectures. Our framework consists of three optimization stages performedend-to-end: 1) Training Transformer model weights on original real MT training data, 2) Generating addi-tional synthetic OOD MT data that diverges in domain from the real training dataset, 3) Searching for theTransformer architecture that minimizes loss on this generated approximated OOD MT data. We demon-strate the effectiveness of our method across a variety of OOD machine translation tasks. Furthermore, oursearched architectures also achieve high performance on various in-domain MT tasks. Moreover, throughvarious ablation studies, we further highlight the importance of learning adversarial perturbations throughour formulation to generate additional synthetic OOD data for OOD generalizable Transformer architecturesearch. In this work, our focus has been on applying our method to MT tasks, with plans to extend it to a variety ofNLP tasks such as text classification, named entity recognition, and text summarization. Additionally, weare exploring its applicability to other modalities, including image and audio data. This broadened scopeaims to enhance our methods versatility and robustness in addressing OOD generalizable architecture searchacross diverse tasks and modalities.",
  "Atilim Gunes Baydin, Robert Cornish, David Martnez-Rubio, Mark Schmidt, and Frank D. Wood. Onlinelearning rate adaptation with hypergradient descent. CoRR, abs/1703.04782, 2017. URL": "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss,Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, ClemensWinter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, JackClark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei.Lan-guage models are few-shot learners.In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, andH. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 18771901. Cur-ran Associates, Inc., 2020. URL",
  "Bonaventure F. P. Dossou and Chris C. Emezue. Okwugb: End-to-end speech recognition for fon and igbo.ArXiv, abs/2103.07762, 2021. URL": "Matthias Feurer, Jost Springenberg, and Frank Hutter. Initializing bayesian hyperparameter optimizationvia meta-learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 29, 2015. Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deepnetworks. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pp. 11261135. JMLR. org, 2017. Luca Franceschi, Michele Donini, Paolo Frasconi, and Massimiliano Pontil. Forward and reverse gradient-based hyperparameter optimization. In International Conference on Machine Learning, pp. 11651173.PMLR, 2017. Luca Franceschi, Paolo Frasconi, Saverio Salzo, Riccardo Grazzi, and Massimiliano Pontil.Bilevel pro-gramming for hyperparameter optimization and meta-learning. In International Conference on MachineLearning, pp. 15681577. PMLR, 2018. Bhanu Garg, Li Zhang, Pradyumna Sridhara, Ramtin Hosseini, Eric Xing, and Pengtao Xie. Learning frommistakesa framework for neural architecture search. Proceedings of the AAAI Conference on ArtificialIntelligence, 2022. Muhammad Ghifary, W. Kleijn, Mengjie Zhang, and David Balduzzi. Domain generalization for object recog-nition with multi-task autoencoders. 2015 IEEE International Conference on Computer Vision (ICCV),pp. 25512559, 2015. Riccardo Grazzi, Luca Franceschi, Massimiliano Pontil, and Saverio Salzo. On the iteration complexity ofhypergradient computation. In International Conference on Machine Learning, pp. 37483758. PMLR,2020.",
  "Hanxiao Liu, Karen Simonyan, and Yiming Yang.Darts:Differentiable architecture search.ArXiv,abs/1806.09055, 2018": "Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, LukeZettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXivpreprint arXiv:1907.11692, 2019. Jonathan Lorraine, Paul Vicol, and David Duvenaud. Optimizing millions of hyperparameters by implicitdifferentiation. In International Conference on Artificial Intelligence and Statistics, pp. 15401552. PMLR,2020.",
  "David R. So, Chen Liang, and Quoc V. Le. The evolved transformer. In International Conference on MachineLearning, 2019": "Sai Ashish Somayajula, Linfeng Song, and Pengtao Xie. A multi-level optimization framework for end-to-endtext augmentation. Transactions of the Association for Computational Linguistics, 10:343358, 2022. Sai Ashish Somayajula, Lifeng Jin, Linfeng Song, Haitao Mi, and Dong Yu. Bi-level finetuning with task-dependent similarity structure for low-resource training. In Findings of the Association for ComputationalLinguistics: ACL 2023, pp. 85698588, 2023. Nitish Srivastava, Geoffrey E. Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout:a simple way to prevent neural networks from overfitting. J. Mach. Learn. Res., 15:19291958, 2014. Felipe Petroski Such, Aditya Rawal, Joel Lehman, Kenneth Stanley, and Jeffrey Clune. Generative teachingnetworks: Accelerating neural architecture search by learning to generate synthetic training data.InInternational Conference on Machine Learning, pp. 92069216. PMLR, 2020. Joshua Tobin, Rachel Fong, Alex Ray, Jonas Schneider, Wojciech Zaremba, and P. Abbeel. Domain ran-domization for transferring deep neural networks from simulation to the real world. 2017 IEEE/RSJInternational Conference on Intelligent Robots and Systems (IROS), pp. 2330, 2017. Minh-Tuan Tran, Trung Le, Xuan-May Le, Mehrtash Harandi, Quan Hung Tran, and Dinh Q. Phung. Nayer:Noisy layer data generation for efficient and effective data-free knowledge distillation. 2024 IEEE/CVFConference on Computer Vision and Pattern Recognition (CVPR), pp. 2386023869, 2023. URL",
  "Sirui Xie, Hehui Zheng, Chunxiao Liu, and Liang Lin. SNAS: stochastic neural architecture search. In ICLR,2019": "Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le. Xlnet: Gen-eralized autoregressive pretraining for language understanding. Advances in neural information processingsystems, 32, 2019. Li Zhang, Youwei Liang, Ruiyi Zhang, Amirhosein Javadi, and Pengtao Xie. Blo-sam: Bi-level optimizationbased finetuning of the segment anything model for overfitting-preventing semantic segmentation.InICML, 2024. URL Miao Zhang, Steven W Su, Shirui Pan, Xiaojun Chang, Ehsan M Abbasnejad, and Reza Haffari. idarts: Dif-ferentiable architecture search with stochastic implicit gradients. In International Conference on MachineLearning, pp. 1255712566. PMLR, 2021.",
  "ARL-based and EA-based Neural Architecture Search Methods": "Early NAS methodologies Zoph & Le (2016); Pham et al. (2018); Zoph et al. (2018) are based on reinforcementlearning (RL). They train a policy network to generate optimal architectures by maximizing validationaccuracy, which serves as the reward. These strategies are straightforward in principle and offer the flexibilityto explore various search spaces. Nonetheless, they require substantial computational resources. Specifically,to evaluate the performance of a proposed architecture, it necessitates training on a dataset, a process thatis notably resource-intensive and time-consuming. Another category of NAS methods Liu et al. (2017); Real et al. (2019) employ evolutionary algorithms(EA). In these methods, architectures are treated as individuals within a population, each rated by a fitnessscore that reflects its performance. Architectures that score higher are more likely to produce offspring(i.e., new architectures), which then supplant those with lower fitness scores. Similar to RL-based methods,EA-based methods are also marked by high computational demands, as they require the training of eacharchitecture to evaluate its fitness score. Recently, EA-based NAS methods have been applied to search foroptimal architectures of Transformers So et al. (2019), and achieved state-of-the-art performance on multiplemachine translation tasks.",
  ":Number of sentences in the CCMatrix, Gnome, and Ubuntu datasets for the English-Igbo (En-Ig)and English-Hausa (En-Ha) language pairs": "We now discuss key properties of our proposed optimization algorithm. Previous research has extensivelyanalyzed the convergence properties of optimization algorithms for bi-level optimization (BLO) problems.For instance, Franceschi et al. (2018) established sufficient conditions under which solutions of approximateBLO problems converge to those of exact problems. Similarly, Shaban et al. (2018) provided sufficient condi-tions for the convergence of approximate gradients obtained via truncated back-propagation during iterativeoptimization. Further, convergence analyses for implicit gradient-based methods have been explored in workssuch as Grazzi et al. (2020) and Zhang et al. (2021). Experimental results in this paper demonstrate that ourproposed algorithm consistently converges and achieves superior performance compared to baseline meth-ods. However, our algorithm diverges from traditional BLO frameworks due to its multi-level optimization(MLO) structure, introducing unique challenges and opportunities for future exploration of its convergenceproperties. Although approximated optimization algorithms for MLO problems, including our method, are theoreticallyand empirically likely to converge to stationary points, it remains unclear whether they achieve global optimarather than sub-optimal solutions. Nevertheless, prior research has demonstrated that sub-optimal solutionscan still yield superior generalization performance (Dinh et al., 2017; Li et al., 2017), a finding corroboratedby our experimental results. Furthermore, existing studies suggest that MLO algorithms enhance generaliz-ability and mitigate overfitting, attributed to their multi-stage optimization process and the use of distinctdata splits (Somayajula et al., 2022; Bao et al., 2021). Specifically, by optimizing different sets of parameterson distinct data subsets, MLO-based methods effectively mitigate overfitting to a single dataset (Zhanget al., 2024).",
  "DDatasets": "The Gnome dataset comprises 187 language pairs derived from the translation of Gnome documentation 2,while the Ubuntu dataset includes 42 language pairs generated from the translation of Ubuntu OS localizationfiles 3. Each sentence in the Gnome and Ubuntu datasets has an average length of 6-9 tokens. summarizes the dataset statistics. WMT-Chat dataset includes data for translating conversational text, inparticular customer support chats 4. WMT-Biomedical dataset includes data for the translation of documentsfrom the biomedical domain 5. Flores dataset is a multilingual machine translation benchmark, which consistsof translations primarily from English into around 200 language varieties 6. Here, we further discuss the discrepancies among the CCMatrix, Gnome, and Ubuntu datasets. Gnomecontains technical and UI-related text specific to the Gnome environment, while Ubuntu contains systemmessages and interface text in Ubuntu OS, representing a software domain.The text in this domain includesspecific terms (\"permissions\", \"kernel\", \"GUI\", \"repository\", \"configuration file\"), technicalverbs (\"execute,\" \"compile,\" \"deploy.\") and abbreviations (\"sudo,\" \"bash,\" \"root\").Sentencesare typically task-oriented or informational, dealing with instructions, warnings, and descriptions of softwarefunctionality. In contrast, CCMatrix is derived from multilingual web pages which cover a wide range of topics, representinga general domain. For text in the general domain, it includes topics such as politics, sports, entertainment,",
  "EBaseline Methods": "In the DARTS framework Liu et al. (2018), each search layer comprises multiple search nodes and theobjective is to determine the most suitable operation for each node from a predefined candidate set, denotedas O. This set includes operations such as FFNs, self-attention mechanisms, and so on (see Appendix B).Each operation is applied to either the input of the layer or the outputs from intermediate nodes, generatingnew outputs for subsequent processing. DARTS represents the output of each search node as a weightedsum of outputs of all operations in O. The weights for each operation in this sum, pertinent to a searchnode, are derived from the softmax of learnable parameters, referred to as architecture weights A. They actas selection variables, representing the likelihood of this operation being selected into the final architecture.Specifically, for the output of a search node f() and input x, the operation is defined as,",
  "oO exp(o)o(x)(14)": "where O is the candidate operation set, o O is an operation within this candidate set and {o} arethe architecture weights for the search node. DARTS utilizes a bi-level optimization framework to learnarchitecture weights A and model parameters W on two disjoint splits D1 and D2 of the training dataset.W is optimized by minimizing a loss L on data split D1 in the lower level, and A is learned on data split D2in the upper level:",
  "s.t.W (A) =argminW L(W, A, D1)(16)": "This optimization problem is solved using one-step gradient descent and finite-difference approximationsimilar to the one described in .6.After convergence, the final architecture is determined byselecting the operation with the highest architecture weight for each search node. For computational andmemory efficiency, DARTS initially searches for the architecture with only a few search layers. After thesearching phase, there is a model retraining phase: a larger model (called final model) with more layers iscomposed by stacking the searched layer multiple times; then the final model is retrained on the combinationof data splits D1 and D2. However, this approach introduces a discrepancy between the search phrase andretraining phrase, in terms of the number of layers in the models. PDARTS Chen et al. (2019) addresses this limitation by progressively increasing the architectures depthduring the search phase. Initially, PDARTS trains the architecture with a few layers for several epochs, thenrefines the operation set to only include those with high architecture weights. This procedure is repeated,gradually increasing the number of layers to match the final models layer number. However, this approachleads to increased computational costs.",
  "FFurther Analysis of Searched Architectures": "In this section, we present the architecture searched by baseline method, DARTS, as shown in , forfurther comparison with the architecture searched by our method, Ours-darts. We observe that both DARTSand Ours-darts result in cross attention modules, highlighting their effectiveness in the machine translationtask. However, DARTS method leads to multiple feed-forward network (FFN) layers, which are absent in",
  "GAnalysis of Generated Samples in Stage II": "In this section, we analyze some characters of the generated approximated OOD samples in the Stage II ofour framework for improving the OOD performance of searched architecture. For each generated sample, weselect 3 of the data samples in the real training dataset that are closest to this generated sample. These 3samples determines a plane, and we compute the angle between the generated sample and this plane. Thedistribution of angles are presented in , with an average degree of 89.66. Results show that the localangles between generated samples are close to 90 degree, which is almost vertical to the approximate plane ofthe training data. This provide a potential underlying reason for our method to achieve good out-of-domaingeneralization performance.",
  "HImpact of Neural Architecture Search": "In this section, we conduct additional experiments to investigate the impact of neural architecture search inour method. We introduce an additional baseline by replacing the optimization of the trainable architecturein Stage III with optimizing model weights of a Transformer model instead (denoted as No-NAS). We presentthe results of this ablation study in . We observe that Ours-darts significantly outperforms the No-NAS baseline, highlighting the necessity of using neural architecture search. It also demonstrates that theout-of-domain samples generated in the second stage of our method do benefit the NAS process, instead ofonly being beneficial to model weights in general deep learning process."
}