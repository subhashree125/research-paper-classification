{
  "Abstract": "Previous research on PAC-Bayes learning theory has focused extensively on establishingtight upper bounds for test errors. A recently proposed training procedure called PAC-Bayestraining, updates the model toward minimizing these bounds. Although this approach istheoretically sound, in practice, it has not achieved a test error as low as those obtainedby empirical risk minimization (ERM) with carefully tuned regularization hyperparameters.Additionally, existing PAC-Bayes training algorithms (e.g., Prez-Ortiz et al. (2021)) oftenrequire bounded loss functions and may need a search over priors with additional datasets,which limits their broader applicability. In this paper, we introduce a new PAC-Bayes trainingalgorithm with improved performance and reduced reliance on prior tuning. This is achievedby establishing a new PAC-Bayes bound for unbounded loss and a theoretically groundedapproach that involves jointly training the prior and posterior using the same dataset. Ourcomprehensive evaluations across various classification tasks and neural network architecturesdemonstrate that the proposed method not only outperforms existing PAC-Bayes trainingalgorithms but also approximately matches the test accuracy of ERM that is optimized bySGD/Adam using various regularization methods with optimal hyperparameters.",
  "Introduction": "The PAC-Bayes bound plays a vital role in assessing generalization by estimating the upper limits of testerrors without using validation data. It provides essential insights into the generalization ability of trainedmodels and offers theoretical backing for practical training algorithms (Shawe-Taylor & Williamson, 1997).For example, PAC-Bayes bounds highlight the discrepancy between the training and generalization errors,indicating the need to incorporate regularizers in empirical risk minimization and explaining how largerdatasets contribute to improved generalization. Furthermore, the effectiveness of the PAC-Bayes boundsin estimating the generalization capabilities of machine learning models has been supported by extensiveexperiments across different generalization metrics (Jiang et al., 2019).",
  "Published in Transactions on Machine Learning Research (09/2024)": "update. To simulate a few-shot learning scenario, we randomly sample 100 instances from the original trainingset and take the whole development set to evaluate the classification performance. We split the training setinto 5 splits, taking one split as the validation data and the rest as the training set. Each experiment wasconducted five times, and we report the average performance. We used the PAC-Bayes training with thescalar prior in this experiment. According to , our method is competitive to the baseline method onthe SST task, and the performance gap is only 0.4 points. On the QNLI task, our method outperforms thebaseline by a large margin, and the variance of our proposed method is less than that of the baseline method.",
  "Preliminaries": "This section outlines the PAC-Bayes framework. For any supervised learning problem, the goal is to find aproper model h from some hypothesis space H, with the help of the training data S {zi}mi=1, where zi isthe training pair with sample xi and its label yi. Given the loss function (h; zi) : h R, which measuresthe misfit between the true label yi and the predicted label by h, the empirical and population/generalizationerrors are defined as:",
  "holds with probability at least 1 . Here, KL stands for the Kullback-Leibler divergence": "A PAC-Bayes bound measures the gap between the expected empirical and generalization errors. Its worthnoting that this bound holds for all posterior Q for any given data-independent prior P and, which enablesoptimization of the bound by searching for the best posterior. In practice, the posterior mean corresponds tothe trained model, and the prior mean can be set to the initial model. In this paper, we will use || || todenote a generic norm, and || ||2 to denote the L2 norm. The focus on Neural Networks: PAC-Bayes training algorithms, including the one proposed here, canbe applied to a wide range of supervised learning problems. However, in this paper, we will focus on thetraining of deep neural networks for the following reasons: Over-parameterized deep neural networks present a significant challenge for PAC-Bayes training, as theKullback-Leibler (KL) term in the PAC-Bayes bounds is believed to grow rapidly with the number ofparameters, quickly leading to vacuous bounds. According to the current literature, deep networks are indeedone of the most critical models where existing PAC-Bayes training algorithms encounter difficulties. Intuitively, minimizing the generalization bound does not require additional implicit or explicit regularization,thus reducing the need for tuning. This advantage of reduced tuning is particularly significant in the trainingof neural networks, where extensive tuning is typically required for ERM. However, for classical problemssuch as Lasso, which involve only one or two hyperparameters, this advantage of PAC-Bayes training may beless pronounced.",
  "Related Work": "PAC-Bayes bounds were first used to train neural networks in Dziugaite & Roy (2017). Specifically, thebound McAllester (1999) has been employed for training shallow stochastic neural networks on binary MNISTclassification with bounded 0-1 loss and has proven to be non-vacuous. Following this work, many recentstudies (Zhou et al., 2018; Letarte et al., 2019; Rivasplata et al., 2019; Prez-Ortiz et al., 2021; Biggs &Guedj, 2021; Perez-Ortiz et al., 2021; Viallard et al., 2023) expanded the applicability of PAC-Bayes boundsto a wider range of neural network architectures and datasets. However, most studies are limited to trainingshallow networks with binary labels using bounded loss, which restricts their broader application to deepnetwork training. Although PAC-Bayes bounds for unbounded loss have been established (Audibert &Catoni, 2011; Alquier & Guedj, 2018; Holland, 2019; Kuzborskij & Szepesvri, 2019; Haddouche et al., 2021;Rivasplata et al., 2020; Rodrguez-Glvez et al., 2023; Casado et al., 2024), it remains unclear whether thesebounds can lead to enhanced test performance in training neural networks. This uncertainty arises partlybecause they usually include assumptions that are difficult to validate or terms that are hard to computein real applications. For example, Kuzborskij & Szepesvri (2019) derived a PAC-Bayes bound under thesecond-order moment condition of the unbounded loss. However, as mentioned in the paper, that bound issemi-empirical, in the sense that it contains the population second order moment of the loss with respect toboth the posterior and the data distributions. Since conditional on the posterior, the samples are no longeri.i.d., this type of bound is difficult to estimate. To the best of our knowledge, existing PAC-Bayes boundsbuilt under the second-order moment condition all suffer from this issue. Recently, Dziugaite et al. (2021) suggested that a tighter PAC-Bayes bound could be achieved with adata-dependent prior. They divide the data into two sets, using one to train the prior and the other totrain the posterior with the optimized prior, thus making the prior independent from the training dataset",
  "Motivation": "To help readers understand the challenges involved in designing practical PAC-Bayes training algorithmsfor deep neural networks, we begin by offering a detailed examination of the limitations inherent in currentPAC-training algorithms and PAC-Bayes bounds. Most popular PAC-Bayes training algorithms (Dziugaite& Roy, 2017; 2018; Prez-Ortiz et al., 2021) are designed for learning problems with bounded loss. Whendealing with unbounded loss, it is necessary to clip the loss to a finite range before training, which can resultin suboptimal performance, as demonstrated in in the numerical section. Some PAC-Bayes bounds for unbounded loss have been established in the literature, extending the requirementfrom bounded loss to sub-Gaussian loss (Theorem 5.1 of Alquier (2021)), loss that satisfies the so-calledhypothesis-dependent range (HYPE) condition (Haddouche et al., 2021), and loss controlled by some finitecumulant generating function (CGF) (Rodrguez-Glvez et al., 2023). More specifically, in (Haddouche et al.,2021), the loss requirement is relaxed to the existence of K(h) such that supzD(h, z) K(h), h H. Thisis known as the HYPE condition and is much weaker than the requirement for a bounded loss. However,commonly used cross-entropy loss still does not satisfy this condition without additional assumptions onthe boundedness of both the input and the model. Furthermore, our numerical experiments indicate thatminimizing bounds tailored for sub-Gaussian and CGF losses during training is also largely ineffective.Specifically, on CIFAR10 with CNN9, the bound values for sub-Gaussian and CGF losses are 5.17 and 5.08,respectively. These values almost do not decrease during training and are even higher than the initial test loss(with a value of 2.30) under random weight initialization, indicating that these bounds can not contribute toimproving the training performance. Lastly, the PAC-Bayes bound established under the (theoretically) weakest assumption is in Kuzborskij &Szepesvri (2019), where the loss is only required to have a finite second-order moment. The associatedbound holds with probability 1 ex, x 2:",
  "ln (1 + m2),(1)": "where = EhQ2(h; S) + EzD2(h; z). However, this bound is not easily amendable for training as theterm EhQEzD2(h; z) in is a population second-order moment of the loss with respect to the data andthe posterior distribution. Since the data is no longer i.i.d. when conditioned on the posterior, estimatingthis populations second-order moment becomes challenging.",
  "Condition under which the new bound will hold": "The new bound we shall propose is based on the following assumption of the loss function in Definition 4.1.Please note that the X in Definition 4.1 will later represent the training loss in the PAC-Bayes analysis, andE[X] will represent the population loss. Definition 4.1 (Exponential moment on finite intervals). Let X be a random variable defined on theprobability space (, F, P) and 0 1 2 be two numbers. We call any K > 0 an exponentialmoment bound of X over the interval , when",
  "holds for all": "We provide a few remarks to clarify the position of this definition in the literature.Remark 4.2 (The left-side moment). Similar to certain other PAC-Bayes conditions in the literature(Alquier, 2021), this definition only requires the left-side moment to be bounded. Specifically, it requiresE[exp ((E[X] X))] to be bounded for > 0, rather than for all R. The need for this left-side conditionarises from the goal of establishing an upper bound on the population loss. An upper bound on the populationloss E[X] in terms of the empirical loss X translates to upper bounding E[X] X. For positive , this leadsto the left-side condition.Remark 4.3 ( within a finite interval bounded away from 0). In practice, we set both 1 and 2 to positivevalues. We observe that restricting to a finite interval with positive 1, 2 often reduces the boundK. In contrast, all previous bounds use 1 = 0.Remark 4.4 (Non-negative loss). Since most loss functions in machine learning (e.g., Cross-Entropy, L1, MSE,Huber loss, hinge loss, Log-cosh loss, quantile loss) are non-negative, it is of great interest to analyze thestrength of Definition 4.1 under X 0. In this case, we can show that Definition 4.1 is weaker than thesecond-order moment condition. Lemma 4.5 (Comparison with the second-order-moment condition). For non-negative random variableX 0, the existence of K on the interval [0, ) in Definition 4.1 can be implied by the existence of thesecond-order moment EX2 < . The assumption X 0 in Lemma 4.5 can be further relaxed to X M with M > 0, as in this case therandom variable X + M is non-negative to which Lemma 4.5 can be applied.Remark 4.6 (Comparison with the first-order-moment condition). Still under the assumption X 0, whenthe 1 in Definition 4.1 is finite (bounded away from 0), the existence of K can be implied by the existenceof first-order moment. Indeed, by taking K = E[X] 1 , the inequality E[X] X E[X] (assumed X 0)immediately implies Equation (2). However, this argument does not hold when 1 0. Hence we cannot sayDefinition 4.1 is as weak as the first-order moment condition.",
  "Now, we generalize Definition 4.1 to the PAC-Bayes setting where the random variable is parameterized bymodels in a hypothesis space": "Let us first explain what we mean by random variables parameterized by models in a hypothesis space. Insupervised learning, we define X(h) as X(h) (h(x), y), where h is the model and is the misfit betweenthe model output h(x) and the data y. For a fixed model h, X(h) is a random variable whose randomnesscomes from the input pairs (x, y) D (D is the data distribution). Since X(h) varies with h, we call it arandom variable parameterized by the model h. Definition 4.7 (Exponential moment over hypotheses). Let X(h) be a random variable parameterized by thehypothesis h in some hypothesis space H (i.e., h H), and fix an interval with 0 1 < 2 . Let{P, } be a family of distribution over H parameterized by Rk. Then, we call any non-negativefunction K() an exponential moment bound for X(h) over the priors {P, } and the interval ,if the following holdsEhPE[exp ((E[X(h)] X(h)))] exp (2K()),",
  "log(EhPE[exp ((E[X(h)] X(h)))]).(3)": "Definition 4.7 is a direct extension of Definition 4.1 to random variables parametrized by a hypothesis space.Similar to Definition 4.1, when dealing with non-negative loss, the existence of the exponential momentbound Kmin is guaranteed, provided that the second-order moment of the loss is bounded, or provided thatthe first-order moment of the loss is bounded and 1 is bounded away from 0.Remark 4.8 (Dependency of K on prior parameters). The key difference in Definition 4.7 compared to priorwork is that we allow the exponential moment bound K to depend on the prior parameter , rather thanrequiring a single K to hold for all possible . This advantage carries over into the PAC-Bayes boundestablished in the next section. In practice, there are two options to compute K(): one is to use the upper bound derived in the proof ofLemma 4.5, and the other is to estimate Kmin directly from the data, via Equation (3). For both options,we need to estimate the expectation of some random variable over the prior distribution P for a given from the training data. Fortunately, this is much easier than estimating the expectation with respect tothe posterior distribution Q, as is required by previous work Kuzborskij & Szepesvri (2019), since in ourcase, after conditioning on the prior, the training data remains i.i.d., allowing a reliable approximation of thepopulation mean by the empirical mean when the data is abundant.",
  "where Q is the set of all probability distributions": "The proof of this theorem is available in Appendix A.1.Remark 4.10 (Asymptotic convergence rate). By setting , 1 = O(m1/2), we observe that the asymptoticbehavior of this bound aligns with the O(m1/2) convergence rate of popular PAC-Bayes bounds in theliterature.Remark 4.11 (Applicability on CE loss). This theorem, when combined with Lemma 4.5, guarantees that theO(m1/2) convergence rate is achieved for CE loss under a bounded second-order moment condition.",
  "m+ K,(4)": "where hides some absolute constant. Namely, both bounds have a K term, although they follow differentdefinitions. When the loss is assumed to be sub-Gassuain, K is defined as the sub-Gaussian norm of the loss(Theorem 5.1 of Alquier (2021)). Under the CGF condition, K is calculated based on the specific choice ofthe CGF (Rodrguez-Glvez et al., 2023) (see Appendix C.2 for our choice of CGF in this experiment). Forour bound, the K is chosen as our Kmin() for various choices of 1 and 2. This experiment is conductedon CIFAR10 using CNN9. We follow standard practice in PAC-Bayes training literature by using a Gaussianprior, fixing its mean to the random Kaiming initialization, and picking a universal variance for all the weights.According to Definition 4.7, our K depends on 1 and 2, and the prior parameter , which we set to theuniversal standard deviation (std) of the prior weight distribution. Since we observed that the variation of Kwith respect to 2 is very minimal, only plots K as a function of the prior std and 1. The K valuesin the previous two bounds (i.e., sub-Gaussian and CGF) do not depend on these parameters. Hence, theyare represented by two horizontal planes. We observe the following:",
  "our K values approach those in the CGF bound as 1 approaches 0 and the prior std becomes large;": "our K is significantly smaller than those in the other two bounds near the region of the optimalchoice of prior std and 1, which we determined by testing various pairs of 1 and prior standarddeviation for the combination that yields the tightest bound after optimized over the posterior. Thisexplains the advantage of making K a function of the prior std;",
  "since all other terms are the same, the smaller the K is, the tighter the bound is after optimizingover": "also indicates that the value of Kmin is more stable with respect to 1 than with respect tothe prior standard deviation, especially near the optimal parameter region, suggesting that choosing1 is easier. Therefore, in our algorithm, we fix 1 in advance while optimizing over the prior variance during training. It turns out that the same choice of 1 works for a wide variety of architectures,as shown in the numerical section;",
  "PAC-Bayes training based on the new bound": "With the relaxed requirements on the loss function, the proposed bound offers a basis for establishing effectiveoptimization over both the posterior and the prior. We will first outline the training process, which focuseson jointly optimizing the prior and posterior to avoid the complex hyper-parameter search over the prior asPrez-Ortiz et al. (2021), followed by a discussion of its theoretical guarantees. The procedure is similar tothe one in Dziugaite & Roy (2017), but has been adapted to align with our newly proposed bound. We begin by parameterizing the posterior distribution as Qw, where w Rd represents the parameters of theposterior. Next, we parameterize the prior as P, where Rk. We operate under the assumption that theprior has significantly fewer parameters than the posterior, that is, k d; the relevance of this assumptionwill become apparent upon examining Theorem 4.15. For our PAC-Bayes training, we propose to optimizeover all three variables: w, , and :",
  "We first make the following assumptions, which will be shown to automatically satisfy by the special Gaussianprior and posteriors": "Assumption 4.13 (Continuity of the KL divergence). Let Q be a family of posterior distributions, letP = {P, Rk} be a family of prior distributions parameterized by . We say the KL divergenceKL(Q||P) is continuous with respect to over the posterior family, if there exists some non-decreasingfunction 1(x) : R+ R+ with 1(0) = 0, such that |KL(Q||P) KL(Q||P)| 1( ), for all pairs, and for all Q Q.",
  "We now provide an end-to-end theorem that guarantees the performance of the PAC-Bayes training based onEquation (5) for general priors and posteriors": "Theorem 4.15 (PAC-Bayes bound for unbounded losses and trainable priors). Assume the loss (h, zi) asa random variable parametrized by model h satisfies Definition 4.7. Let Q be a family of posterior distributionQ = {Qw, w Rd} , let P = {P, Rk} be a family of prior distributions parameterized by . Letn() := N(, , ) be the covering number of the set of the prior parameters. Under Assumption 4.13 andAssumption 4.14, the following inequality holds for the minimizer ( w, , ) of Equation (P) and any , > 0with probability as least 1 with := (n() + 21",
  "The proof is available in Appendix A.2": "The theorem provides a generalization bound on the model learned as the minimizer of Equation (P) withdata-dependent priors. This bound contains the PAC-Bayes loss LP AC along with an additional correctionterm , which is notably absent in the traditional PAC-Bayes bound with fixed priors. Given that ( w, , )minimizes LP AC(, , , ), evaluating LP AC at its own minimizer ensures that the first term is small. If thecorrection term is also small, then the test error remains low. In the next section, we will delve deeper intothe condition for this term to be small. Intuitively, selecting a small helps to maintain low values for thefirst three terms in . Although a smaller increases the n() in the last term, this increase is moderatedbecause it is inside the logarithm and divided by the size of the dataset.",
  "Restricting to the Gaussian Families": "For the LP AC objective to have a closed-form formula, we employ the Gaussian distribution family. For easeof illustration, we introduce a new notation for the parametrization. Consider a model denoted as f, wheref represents the model architecture (e.g., a VGG net), and is the weight. In this context, f aligns withthe h discussed in earlier sections. Moving forward, we will use f to refer to the model instead of h. We define the posterior distribution of the weights as a Gaussian distribution centered around the trainableweight , with trainable variance ., i.e., the posterior weight distribution is N(, diag()), denoted byQ,1. The assumption of a diagonal covariance matrix implies the independence of the weights. We considertwo types of priors, both centered around the initial weight 0 of the model (as suggested by Dziugaite &Roy (2017)), but with different settings on the variance. Scalar prior: we use a universal scalar to encode the variance of all the weights in the prior, i.e., the weightdistribution of P is N(0, Id), where is a scalar. With this prior, the KL divergence KL(Q,||P0,)in Equation (P) is:",
  "1m + 2B is a constant depending on 1, , M, d, T, a, b, m2": "In the bound, the term LP AC(, , , ) is inherently minimized as it evaluates the function LP AC(, , , )at its own minimizer. The overall bound remains low if the correction term can be deemed insignificant.The logarithm term in the definition of grows very mildly with the dimension in general, so we can treat it(almost) as a constant. Thus, k 1m, from which we see that 1). (and therefore the bound) would besmall if priors degree of freedom k is substantially less than the dataset size m; 2). This bound still achievesthe asymptotic rate of O(m1/2) after optimizing over 1. We note that even if the corollary assumes thatthe parameters (i.e., mean and variance) of the Gaussian distribution are bounded, the random variableitself is still unbounded, so the loss is still unbounded. The proof and more discussions can be found inAppendix A.4.",
  "Training algorithm": "Estimating Kmin(): In practice, the function Kmin() must be estimated first. Since we showed inCorollary 4.16 and Remark 4.6 that Kmin() is Lipschtiz continuous and bounded, we can approximate itusing piecewise-linear functions. More explicitly, we use Equation (3) and Monte Carlo Sampling to estimateKmin on some discrete grid of , then we interpolate Kmin() using piecewise linear functions. More detailsare in Appendix B.1. Notably, since for each fixed , the prior is independent of the training data, this procedure of estimatingKmin() can be carried out before training. Recall that after Remark 4.8, we discussed two ways to estimateKmin, using Equation (3) or using its theoretical upper bound from Lemma 4.5. illustrates theadvantage of the former approach. Besides having a much smaller numerical value, estimating Kmin directlyfrom Equation (3) also involves fewer constraints compared to using the upper bound, which requires anon-negative loss with a bounded second-order moment. Two-stage PAC-Bayes training: Algorithm 1 outlines the proposed PAC-Bayes training algorithm withscalar prior. The version that uses the layerwise prior is detailed in Appendix B.2. Algorithm 1 contains twostages. Stage 1 performs pure PAC-Bayes training, and Stage 2 is an optional Bayesian refinement stage thatis only activated when the first stage does not sufficiently reduce the training loss. For Stage 1, althoughthere are several input parameters to be specified, one can use the same choice of values across very different",
  "for sampling one batch s from S do": "//Ensure non-negative variances exp(b), exp(v)P N(0; Id), Q, + N(0; diag())//Get the stochastic version of E Q,(f ; S)Draw one Q, and evaluate (f ; S)Compute the KL divergence as Equation (7)Compute as Equation (8)Compute the loss function L as LP AC in Equation (P)//Update all parametersb b + L",
  "Therefore, we only need to perform gradient updates on the other three variables, , ,": "The second stage of training: Gastpar et al. (2023); Nagarajan & Kolter (2019) showed that achievinghigh accuracy on certain distributions precludes the possibility of getting a tight generalization bound inoverparameterized settings. This implies that it is less possible to use reasonable generalization bound to fullytrain one overparameterized model on a particular dataset. It is also observed in our PAC-Bayes trainingexperiments that, oftentimes, minimizing the PAC-Bayes bound only (Stage 1) cannot make the trainingaccuracy reach 100%. If this happens3, we add a second stage to further increase the training accuracy.Specifically, in Stage 2, we continue to update the model by minimizing only EQ,(f; S) over , andkeep all other variables (i.e., , ) fixed to the solution found by Stage 1. This is essentially a stochasticgradient descent with noise injection, the level of which has been learned from Stage 1. The two-stage trainingis similar to the idea of the learning-rate scheduler (LRS). In LRS, the initial large learning rate introducesan implicit bias that guides the solution path towards a flat region (Cohen et al., 2021; Barrett & Dherin,2020), and the later lower learning rate ensures the convergence to a local minimizer in this region. Withoutthe large learning rate stage, it cannot reach the flat region; without the small learning rate stage, it cannotconverge to a local minimizer. For the two-stage PAC-Bayes training, Stage 1 (PAC-Bayes stage) guidesthe solution to flat regions by minimizing the generalization bound, and Stage 2 is necessary for an actualconvergence to a local minimizer. Regularizations in the PAC-Bayes training: By plugging the KL divergence Equation (7) into P, wecan see that in the case of Gaussian priors and posteriors, the PAC-Bayes loss is nothing but the originaltraining loss augmented by a noise injection and a weight decay, except that strength of both of them areautomatically learned during training. More discussions are available in Appendix B.3. Prediction: After training, we use the mean of the posterior as the trained model and perform deterministicprediction on the test dataset. In Appendix B.4, we provide some mathematical intuition of why thedeterministic predictor is expected to perform even better than the Bayesian predictor.",
  "Experiments": "In this section, we demonstrate the efficacy of the proposed PAC-Bays training algorithm through extensivenumerical experiments. Specifically, we conduct comparisons between our algorithm and existing PAC-Bayestraining algorithms, as well as conventional training algorithms based on Empirical Risk Minimization (ERM).Our approach yields competitive test accuracy in all settings and exhibits a high degree of robustness w.r.t.the choice of hyperparameters. Comparison with different PAC-Bayes bounds and existing PAC-Bayes training algorithms:We compared our PAC-Bayes training algorithm using the layerwise prior with baselines in Prez-Ortizet al. (2021): quad (Rivasplata et al., 2019), lambda (Thiemann et al., 2017), classic (McAllester, 1999), andbbb (Blundell et al., 2015) in the context of deep convolutional neural networks. The baseline PAC-Bayesalgorithms contain a variety of crucial hyperparameters, including variance of the prior (1e-2 to 5e-6), learningrate (1e-3 to 1e-2), momentum (0.95, 0.99), dropout rate (0 to 0.3) in the training of the prior, and the KLtrade-off coefficient (1e-5 to 0.1) for bbb. These hyperparameters were chosen by grid search. The batch sizeis 250 for all methods. Our findings, as detailed in , show that our algorithm outperforms the otherPAC-Bayes methods regarding test accuracy. It is important to note that all four baselines employed thePAC-Bayes bound for bounded loss. Therefore, they need to convert unbounded loss into bounded loss fortraining purposes. Various conversion methods were evaluated by Prez-Ortiz et al. (2021), and the mosteffective one was selected for producing the results presented. 3Note that there are cases when the second stage is not necessary, including but not limited to 1) the network is shallow 2)the dataset is simple 3) a good prior is chosen. In these cases, the training accuracy can already reach 100% in Stage 1.",
  "CNN978.6379.3978.3383.4978.5378.3585.46CNN1384.4784.4884.2285.4184.3084.4288.31CNN1585.3185.5185.2085.9584.9885.1387.55": "To demonstrate the necessity of our newly proposed PAC-Bayes bound for unbounded loss, we comparedthis new bound with two existing PAC-Bayes bounds for unbounded loss. One is based on the sub-Gaussianassumption (Theorem 5.1 of Alquier (2021)), while the other (Theorem 9 of Rodrguez-Glvez et al. (2023))assumes the loss function is a bounded cumulant generating function (CGF). It is important to note that, asof now, no training algorithms specifically leverage these PAC-Bayes bounds for unbounded loss. Therefore,for a fair comparison, we conducted an experiment by replacing our PAC-Bayes bound with the other twobounds and using the same two-stage training algorithm with the trainable layerwise prior. We also visualized the test accuracy when minimizing different PAC-Bayes bounds for unbounded loss in Stage1. As shown in , minimizing our PAC-Bayes bound can achieve better generalization performance.As discussed in Remark 4.12, the K terms in the two baseline bounds for unbounded loss are much largercompared to ours. This results in a smaller , which increases the coefficient of the KL divergence term andforces the posterior to remain close to the prior rather than fitting the data effectively. The details of the twobaseline bounds are in Appendix C.2. Comparison with ERM optimized by SGD/Adam with various regularizations: We tested ourPAC-Bayes training on CIFAR10 and CIFAR100 datasets with no data augmentation4 on various populardeep neural networks, VGG13, VGG19 (Simonyan & Zisserman, 2014), ResNet18, ResNet34 (He et al.,2016), and Dense121 (Huang et al., 2017) by comparing its performance with conventional empirical riskminimization by SGD/Adam enhanced by various regularizations (which we call baselines). The training ofbaselines involves a grid search for the best hyperparameters, including momentum for SGD (0.3 to 0.9),learning rate (1e-3 to 0.2), weight decay (1e-4 to 1e-2), and noise injection (5e-4 to 1e-2). The batch size wasset to be 128. We reported the highest test accuracy obtained from this search as the baseline results. For allconvolutional neural networks, our method employed Adam with a fixed learning rate of 1e-4.",
  "APPNP AdamW86.60.791.00.485.10.562.50.480.62.8scalar87.10.690.40.585.70.463.50.481.80.5": "Since the CIFAR10 and CIFAR100 datasets do not have a published validation dataset, we used the testdataset to find the best hyperparameters of baselines during the grid search, which might leadto a slightly inflated performance for baselines. Nevertheless, as presented in , the test accuracyof our method is still competitive. Please refer to Appendix C.4 for more details. Evaluation on graph neural networks: To demonstrate the broad applicability of the proposed PAC-Bayestraining algorithm to different network architectures, we evaluated it on graph neural networks (GNNs).Unlike CNNs, optimal GNN performance has been reported using the AdamW optimizer for ERM andenabling dropout. To ensure the best baseline results, we conducted a hyperparameter search over learningrate (1e-3 to 1e-2), weight decay (0 to 1e-2), noise injection (0 to 1e-2), and dropout (0 to 0.8) and reportedthe highest test accuracy as the baseline result. For our method, we used Adam and fixed the learning rateto be 1e-2 for all graph neural networks. We follow the convention for graph datasets by randomly assigning20 nodes per class for training, 500 for validation, and the remaining for testing. We tested four architectures GCN (Kipf & Welling, 2016), GAT (Velikovi et al., 2017), SAGE (Hamiltonet al., 2017), and APPNP (Gasteiger et al., 2018) on 5 benchmark datasets CoraML, Citeseer, PubMed,Cora and DBLP (Bojchevski & Gnnemann, 2017). Since there are only two convolution layers for GNNs,applying our algorithm with the scalar prior is sensible. For our PAC-Bayes training, we retained the dropoutlayer in the GAT as is, since it differs from the conventional dropout and essentially drops the edges of theinput graph. Other architectures do not have this type of dropout; hence, our PAC-Bayes training for thesearchitectures does not include dropout.",
  "(c) ERM training": ": Generalization gap (the difference between the training and the testing accuracy) in PAC-Bayestraining versus ERM training using ResNet18 on the CIFAR10 dataset. Each point represents an intermediatemodel during training, plotted according to its test accuracy versus training accuracy. The line y = x indicatesthe optimal, zero generalization gap. PAC-Bayes training has a smaller generalization gap throughout thetraining process (a) and remains stable despite changes in hyperparameters (b). In constant, ERMtraining (c) is very unstable to hyper-parameter changes. When comparing ERM with our method ina, we picked the best ERM result (the blue one) in c that achieved the best final test accuracy.The discontinuity it has around the testing accuracy of 87%, is due to the activation of the learning ratescheduler.",
  "Conclusion and Discussion": "In this paper, we demonstrated the great practical potential of PAC-Bayes training by proposing a numericallytighter PAC-Bayes bound and applying it to train deep neural networks. The proposed framework significantlyimproved the performance of PAC-Bayes training, making it nearly match the best results of ERM. We hopethis result inspires researchers in the field to further explore the practical implications of PAC-Bayes theoryand make these bounds more useful in practice.",
  "Ilja Kuzborskij and Csaba Szepesvri. Efron-stein pac-bayesian inequalities. arXiv preprint arXiv:1909.01931,2019": "Sungyoon Lee and Cheongjae Jang. A new characterization of the edge of stability based on a sharpnessmeasure aware of batch gradient distribution. In The Eleventh International Conference on LearningRepresentations, 2022. Gal Letarte, Pascal Germain, Benjamin Guedj, and Franois Laviolette. Dichotomize and generalize:Pac-bayesian binary activated deep neural networks. Advances in Neural Information Processing Systems,32, 2019.",
  "Vaishnavh Nagarajan and J Zico Kolter. Uniform convergence may be unable to explain generalization indeep learning. Advances in Neural Information Processing Systems, 32, 2019": "Arvind Neelakantan, Luke Vilnis, Quoc V Le, Ilya Sutskever, Lukasz Kaiser, Karol Kurach, and JamesMartens. Adding gradient noise improves learning for very deep networks. arXiv preprint arXiv:1511.06807,2015. Antonio Orvieto, Hans Kersting, Frank Proske, Francis Bach, and Aurelien Lucchi. Anticorrelated noiseinjection for improved generalization. In International Conference on Machine Learning, pp. 1709417116.PMLR, 2022. Maria Perez-Ortiz, Omar Rivasplata, Benjamin Guedj, Matthew Gleeson, Jingyu Zhang, John Shawe-Taylor,Miroslaw Bober, and Josef Kittler. Learning pac-bayes priors for probabilistic neural networks. arXivpreprint arXiv:2109.10304, 2021.",
  ") + K()": "Proof:In this proof, we extend our PAC-Bayes bound with data-independent priors to data-dependentones that accommodate the error when the prior distribution is parameterized and optimized over a finite setof parameters P = {P, Rk} with a much smaller dimension than the model itself. Let T(, , )be an -cover of the set , which states that for any , there exists a T(, , ) , such that|| || .",
  "with probability over 1": "Now, for the collection of s in the -net T(, , ), by the union bound, the PAC-Bayes bound uniformlyholds on the -net with probability at least 1 |T| = 1 n(). For an arbitrary , its distance to the-net is at most . Then under Assumption 4.13 and Assumption 4.14, we have:",
  "A.3KL divergence of the Gaussian prior and posterior": "For a k-layer network, the prior is written as P0,, where 0 is the random initialized model parameterand Rk+ is the vector containing the variance for each layer. The set of all such priors is denoted byP := {P0,, Rk, 0 }. In the PAC-Bayes training, we select the posterior distribution to becentered around the trained model parameterized by , with independent anisotropic variance. Specifically,for a network with d trainable parameters, the posterior is Q, := N(, diag()), where (the currentmodel) is the mean and Rd+ is the vector containing the variance for each trainable parameter. The setof all posteriors is Q := {Q,, , }, and the KL divergence between all such prior and posterior inP and Q is:",
  "1m + 2": "Proof: We first prove the two assumptions are satisfied by the Gaussian family with bounded parameterspaces. To prove Assumption 4.13 is satisfied, let vi = log 1/i, i = 1, ..., k and perform a change of variablefrom i to vi. The weight of prior for the ith layer now becomes N(0, eviIdi)), where di is the number oftrainable parameters in the ith layer. It is straightforward to compute",
  "22,": "where the first inequality used the property of the supremum, the p0,1(), p0,2() in the fourth linedenote the probability density function of Gaussian with mean 0 and variance parametrized by 1, 2 (i.e.,1,i, 2,i are the variances for the ith layer), the second inequality use the fact that if X(h) is a non-negativefunction of h and Y (h) is a bounded function of h, then",
  "dM. The same idea applies to the definition of": "Remark A.5. Due to the above remark, M, T, a, b can be treated as dimension-independent constants thatdo not grow with the network size d. As a result, the constants L1, L2, L in Corollary 4.16, are dominatedby d, and L1, L2, L = O(d). This then implies the logarithm term in scales as O(log d), which grows verymildly with the size. Therefore, Corollary 4.16 can be used as the generalization guarantee for large neuralnetworks.",
  "B.1Algorithms to estimate K()": "In this section, we explain the algorithm to compute K(). In previous literature, the moment bound K orits analog term in the PAC-Bayes bounds was often assumed to be a constant. One of our contributions is toallow K to vary with the variance of the prior, so if a small prior variance is found by PAC-Bayes training,then the corresponding K would also be small. We perform linear interpolation to approximate the functionKmin() defined in (2) of the main text. When is 1D, We first compute Kmin() on a finite grid of thedomain of , by solving Equation (25) below. With the computed function values on the grid {Kmin(i)}i ,we can construct a piecewise linear function as the approximation of Kmin().",
  "(25)": "where l P0,i, l = 1, ..., n, are samples from the prior distribution and are fixed when solving Equation (25)for Kmin(i). Equation (25) is the discrete version of the formula (2) in the main text. This optimizationproblem is 1-dimensional, and the function in the constraint is monotonic in K, so it can be solved efficientlyby the bisection method.",
  "where i is a random sample from the domain of . Since each i is k-dimensional, max(i) representsthe maximum of the k coordinates": "The idea of this formulation 26 is as follows, we use the 1D function K(max(i)) as a surrogate function ofthe original k-dimension function Kmin() (i.e. Kmin() K(max(i))). Then estimating this 1D surrogatefunction is easy by using the bisection method. This procedure will certainly overestimate the true Kmin()but since the surrogate function is also a valid exponential moment bound, it is safe to be used as a replacementfor the K() in our PAC-Bayes bound for training. In practice, we tried to use mean(i) to replace max(i)to mitigate the over-estimation, but the final performance stays the same. The details of the whole procedureare presented in Algorithm 2.",
  "B.3Regularizations in PAC-Bayes bound": "Only noise injection and weight decay are essential from our derived PAC-Bayes bound. Since many factors innormal training, such as mini-batch and dropout, enhance generalization by some sort of noise injection, it isunsurprising that they can be substituted by the well-calibrated noise injection in PAC-Bayes training. Likemost commonly used implicit regularizations (large lr, momentum, small batch size), dropout and batch-normare also known to penalize the loss functions sharpness indirectly. Wei et al. (2020) studies that dropoutintroduces an explicit regularization that penalizes sharpness and an implicit regularization that is analogousto the effect of stochasticity in small mini-batch stochastic gradient descent. Similarly, it is well-studiedthat batch-norm Luo et al. (2018) allows the use of a large learning rate by reducing the variance in thelayer batches, and large allowable learning rates regularize sharpness through the edge of stability Cohenet al. (2020). As shown in the equation below, the first term (noise-injection) in our PAC-Bayes boundexplicitly penalizes the Trace of the Hessian of the loss, which directly relates to sharpness and is quite similar",
  "Tr(diag()2(f ; D))": "The second regularization term (weight decay) in the bound additionally ensures that the minimizer foundis close to initialization. Although the relation of this regularizer to sharpness is not very clear, empiricalresults suggest that weight decay may have a separate regularization effect from sharpness. In brief, we statethat the effect of sharpness regularization from dropout and batch norm can also be well emulated by noiseinjection with the additional effect of weight decay.",
  "(27)": "Recall here and are the minimizers of the PAC-Bayes loss, obtained by solving the optimization problemEquation (P). Equation Equation (27) states that the deterministic predictor has a smaller prediction errorthan the Bayesian predictor. However, note that the last inequality in Equation (27) is derived under theassumption that the term 2(f ; D) is positive-semidefinite. This is a reasonable assumption as is the localminimizer of the PAC-Bayes loss, and the PAC-Bayes loss is close to the population loss when the number ofsamples is large. Nevertheless, since this property only approximately holds, the presented argument canonly serve as an intuition that shows the potential benefits of using the deterministic predictor.",
  "CExtended Experimental Details": "We conducted experiments using eight A5000 GPUs with four AMD EPYC 7543 32-core Processors. Tospeed up the training process for posterior and prior variance, we utilized a warmup method that involvedupdating the noise level in the posterior of each layer as a scalar for the first 50 epochs and then proceedingwith normal updates after the warmup period. This method only affects the convergence speed, not thegeneralization, and it was only used for large models in image classification.",
  "EhPE[exp ((E[X(h)] X(h)))] exp (2K())": "to hold for any in this range. One needs to be a little cautious when choosing the upper bound 2, becauseif it is too large, then the empirical estimate of EhPE[exp ((E[X(h)] X(h)))] would have too large of avariance. Therefore, we recommended 2 to be set to no more than 10 or 20. The choice of 1 also does notseem to be very crucial, so we have fixed it to 0.5 throughout. For large datasets (like in MNIST or CIFAR10), m is large. Then, according to Theorem 4.15, we can setthe range M, T, a, b of the trainable parameters to be very large with only a little increase of the bound (asM, T, a, b are inside the logarithm), and then during training, the parameters would not exceed these boundseven if we dont clip them. Hence, no clipping is needed for very large networks or with small networks withproper initializations. But when the dataset size m is small, or the initialization is not good enough, then thecorrection term could be large, and clipping will be needed. The clipping is also needed from the usual numerical stability point of view. As is in the denominator ofthe KL-divergence, it cannot be too close to 0. Because of this, in the numerical experiments on GNN andCNN13/CNN15, we clip the domain of at a lower bound of 0.1 and 5e 3, respectively. For the VGG andResnet experiments, the clipping is optional.",
  "+ KL(Q||P)) + (),(29)": "where () is a convex and continuously differentiable function defined on [0, b) for some b R+ suchthat (0) = (0) = 0 and EPESD[exp(((f; D) (f; S)))] exp (()) for all [0, b).There is no specific form of () provided in the original paper, but in order to achieve the normalm convergence rate of the PAC-Bayes bound, we need at least set () = KCGF ( 2). Amongthese, using = 2 gives the smallest K, so we used = 2 for the comparison in and 3.",
  "C.3Compatibility with Data Augmentation": "We didnt include data augmentation in the experiments in the main text. Because with data augmentation,there is no rigorous way of choosing the sample size m that appears in the PAC-Bayes bound. More specifically,for the PAC-Bayes bound to be valid, the training data has to be i.i.d. samples from some underlyingdistribution. However, most data augmentation techniques would break the i.i.d. assumption. As a result, ifwe have 10 times more samples after augmentation, the new information they bring in would be much lessthan those from 10 times i.i.d. samples. In this case, how to determine the effective sample size m to be usedin the PAC-Bayes bound is a problem. Since knowing whether a training method can work well with data augmentation is important, we carriedout the PAC-Bayes training with an ad-hoc choice of m, that is, we set m to be the size of the augmenteddata. We compared the grid-search result of SGD and Adam versus PAC-Bayes training on CIFAR10 withResNet18. The augmentation is achieved by random flipping and random cropping. The data augmentationincreased the size of the training sample by 128 times. The test accuracy for SGD is 95.2%, it is 94.3%for Adam, it is 94.4% for AdamW, and it is 94.3% for PAC-Bayes training with the layerwise prior. Incontrast, the test accuracy without data augmentation is lower than 90% for all methods. It suggests thatdata augmentation does not conflict with the PAC-Bayes training in practice.",
  "C.4Model analysis": "We examined the learning process of PAC-Bayes training by analyzing the posterior variance for differentlayers in models trained by Algorithm 3. Typically, batch norm layers have smaller values than convolutionlayers. Additionally, shadow convolution and the last few layers have smaller values than the middle layers.We also found that skip-connections in ResNet18 have smaller values than nearby layers, suggesting thatimportant layers with a greater impact on the output have smaller values. In Stage 1, the training loss is higher than the testing loss, which means the adopted PAC-Bayes bound isable to bound the generalization error throughout the PAC-Bayes training stage. Additionally, we observedthat the final value of K is usually very close to the minimum of the sampled function values. The averagevalue of experienced a rapid update during the initial 50 warmup epochs but later progressed slowly untilStage 2. The details can be found in and 10. Based on the figures, shadow convolution, and the lastfew layers have smaller values than the middle layers for all models. We also found that skip-connections inResNet18 and ResNet34 have smaller values than nearby layers on both datasets, suggesting that importantlayers with a greater impact on the output have smaller values. Computational cost: In PAC-Bayes training, we have four parameters , , , . Among these variables, can be computed on the fly or whenever needed, so there is no need to store them. We need to store , , ,",
  "C.5Node classification by GNNs": "We test the PAC-Bayes training algorithm on the following popular GNN models, tuning the learning rate(1e3, 5e3, 1e2), weight decay (0, 1e4, 1e3, 1e2), noise injection (0, 1e3, 5e3, 1e2), and dropout(0, 0.4, 0.8). The number of filters per layer is 32 in GCN (Kipf & Welling, 2016) and SAGE (Hamilton et al.,2017). For GAT (Velikovi et al., 2017), the number of filters is 8 per layer, the number of heads is 8, andthe dropout rate of the attention coefficient is 0.6. Fpr APPNP (Gasteiger et al., 2018), the number of filtersis 32, K = 10 and = 0.1. We set the number of layers to 2, achieving the best baseline performance. AReLU activation and a dropout layer are added between the convolution layers for baseline training only.Since GNNs are faster to train than convolutional neural networks, we tested all possible combinations ofthe above parameters for the baseline, conducting 144 searches per model on one dataset. We use Adamas the optimizer with the learning rate as 1e2 for all models using both training and validation nodes forPAC-Bayes training. We also did a separate experiment using both training and validation nodes for training. For baselines, weneed first to train the model to detect the best hyperparameters as before and then train the model againon the combined data. Our PAC-Bayes training can also match the best generalization of baselines in thissetting. All results are visualized in -8. The AdamW+val and scalar+val record the performances of thebaseline and the PAC-Bayes training, respectively, with both training and validation datasets for training.We can see that test accuracy after adding validation nodes increased significantly for both methods but still,the results of our algorithm match the best test accuracy of baselines. Our proposed PAC-Bayes training withthe scalar prior is better than most of the settings during searching and achieved comparable test accuracywhen adding validation nodes to training.",
  "C.6Few-shot text classification with transformers": "The proposed method is also observed to work on transformer networks. We conducted experiments on twotext classification tasks of the GLUE benchmark as shown in . SST is the sentiment analysis task,whose performance is evaluated as the classification accuracy. Sentiment analysis is the process of analyzingthe sentiment of a given text to determine if the emotional tone of the text is positive, negative, or neutral.QNLI (Question-answering Natural Language Inference) focuses on determining the logical relationshipbetween a given question and a corresponding sentence. The objective of QNLI is to determine whether thesentence contradicts, entails, or is neutral with respect to the question. We use classification accuracy as the evaluation metric. The baseline method uses grid search over thehyper-parameter choices of the learning rate (1e1, 1e2, 1e3), batch size (2, 8, 16, 32, 80), dropout ratio(0, 0.5), optimization algorithms (SGD, AdamW), noise injection (0, 1e5, 1e4, 1e3, 1e2, 1e1), andweight decay (0, 1e1, 1e2, 1e3, 1e4). The learning rate and batch size of our method are set to 1e3and 100 (i.e., full-batch), respectively. In this task, the number of training samples is small (80). As a result,the preset 2 = 10 is a bit large and thus prevents the model from achieving the best performance withPAC-Bayes training. We adopt BERT (Devlin et al., 2018) as our backbone and added one fully connected layer as the classificationlayer. Only the added classification layer is trainable, and the pre-trained model is frozen without gradient",
  "C.7Additional experiments stability": "We conducted extra experiments to showcase the robustness of the proposed PAC-Bayes training algorithm.Specifically, we tested the effect of different learning rates on ResNet18 and VGG13 models trained withlayerwise prior. Learning rate has long been known as an important impact factor of the generalization forbaseline training. Within the stability range of gradient descent, the larger the learning rate is, the better thegeneralization has been observed (Lewkowycz et al., 2020). In contrast, the generalization of the PAC-Bayestrained model is less sensitive to the learning rate. We do observe that due to the newly introduced noiseparameters, the stability of the optimization gets worse, which in turn requires a lower learning rate toachieve stable training. But as long as the stability is guaranteed by setting the learning rate low enough,our results, as , indicated that the test accuracy remained stable across various learning rates forVGG13 and Resnet18. The dash in the table means that the learning rate for that particular setting is toolarge to maintain the training stability. For learning rates below 1e4, we trained the model in Stage 1 formore epochs (700) to fully update the prior and posterior variance. We also demonstrate that the warmup iterations (as discussed at the beginning of this section) do not affectgeneralization. As shown in , the test accuracy is insensitive to different numbers of warmup iterations.Furthermore, additional evaluations of the effects of batch size (), optimizer (Tables 11), and 1 and2 ()"
}