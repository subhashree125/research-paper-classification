{
  "Abstract": "Many self-supervised learning methods are pre-trained on the well-curated ImageNet-1kdataset. In this work, given the excellent scalability of web data, we consider self-supervisedpre-training on noisy web sourced image-text paired data. First, we conduct a benchmarkstudy of representative self-supervised pre-training methods on large-scale web data in alike-for-like setting. We compare a range of methods, including single-modal ones that usemasked training objectives and multi-modal ones that use image-text constrastive train-ing. We observe that existing multi-modal methods do not outperform their single-modalcounterparts on vision transfer learning tasks. We derive an information-theoretical viewto explain these benchmark results, which provides insight into how to design a novel vi-sion learner. Inspired by this insight, we present a new visual representation pre-trainingmethod, MUlti-modal Generator (MUG), that learns from scalable web sourced image-textdata. MUG achieves state-of-the-art transfer performance on a variety of tasks and demon-strates promising scaling properties.",
  "Introduction": "Self-supervised representation learning (SSL) has attracted a considerable amount of attention recently asit offers the potential to reduce the reliance on labor-intensive human collected annotations when trainingmodels. The design of SSL methods is mainly guided by the InfoMax principle (Hjelm et al., 2019), whichshows that maximizing the mutual information I(X; Z) of the input data X and the learned representationZ can lead to better representations. In the case of visual data, discriminative contrastive learning (He et al.,2020; Chen et al., 2020d; 2021; 2020b;c; Shen et al., 2022; Bardes et al., 2022) and generative masked imagemodeling (Bao et al., 2022; He et al., 2022; Baevski et al., 2022; Wei et al., 2022; Peng et al., 2022) havebeen demonstrated to learn transferable representations from images by attempting to solve pre-definedpretext objectives that aim to indirectly optimize I(X; Z), achieving state-of-the-art results on popularcomputer vision benchmarks. Despite these remarkable advancements, most SSL methods are developedon the ImageNet-1k (Deng et al., 2009) dataset, which is well-curated and thus not representative of manyreal world use cases. As a result, these works do not fully exploit the potential scalability of SSL to largeruncurated datasets, e.g., many existing methods are only evaluated on the ImageNet-1k dataset (Xie et al.,2021a; He et al., 2020; 2022). There exists works on extending SSL to larger uncurated datasets (Wen et al.,2022; Xie et al., 2021a; Hnaff et al., 2021; Xie et al., 2021b; Wang et al., 2021), yet most of these worksfocus on how to obtain object centric representations (Wen et al., 2022), or target one specific downstreamtask, such as object detection (Wang et al., 2021; Xie et al., 2021b;a; Hnaff et al., 2021). In contrast to the previous single-modal SSL methods, pioneering multi-modal SSL methods (e.g.,CLIP (Radford et al., 2021) and ALIGN (Jia et al., 2021)) have shown impressive scalability due to theirability to train on easily collected large-scale web sourced image-text pairs (Sharma et al., 2018; Thomeeet al., 2016; Schuhmann et al., 2021). Multiple works have shown that the textual information present inweb image-text pairs can greatly benefit various downstream tasks (Mu et al., 2022; Kim et al., 2021; Yaoet al., 2022; Li et al., 2022b) and thus contribute to improved transfer learning performance. However, manyof these existing methods differ not only in the training losses they use, but also in the terms of the modelarchitectures and training data, which makes direct comparison difficult. To address this, in this work we",
  "(i) single-modal discriminative": ": Comparison of different vision pre-training paradigms that use images or image-text pairs. Fourparadigms are considered: (i) single-modal discriminative (e.g., SimCLR (Chen et al., 2020b)), (ii) single-modal generative (e.g., MAE (He et al., 2022)), (iii) multi-modal discriminative (e.g., CLIP (Radford et al.,2021)), and (iv) our proposed multi-modal generative approach named MUG. The multi-modal generativeparadigm simultaneously generates images and text using only image representations. conduct a benchmark study of single-modal SSL methods (i.e., with only images) and multi-modal SSLmethods (i.e., with image-text pairs) using the same web-sourced image-text datasets and training settings. Based on our benchmark study, we make two key observations.First, generative methods achieve thebest transfer performance. Second, and more surprisingly, the representations learned from multi-modalSSL methods do not outperform single-modal SSL ones.To explain these observations, we explore aninformation-theoretical motivated perspective.Starting from the InfoMax principle (Hjelm et al., 2019)for learning representations, we use the information-bottleneck theory to explain the transferability of deeprepresentations (Tishby & Zaslavsky, 2015; Shwartz-Ziv & Tishby, 2017; Alemi et al., 2017; Cui et al., 2022a).We show that generative methods approach a better upper bound on the mutual information I(X; Z), andthat introducing multiple generative objectives can further improve upon this bound. In , we group previous representative methods into three families: (i) single-modal discriminative (e.g.,SimCLR (Chen et al., 2020b)), (ii) single-modal generative (e.g., MAE (He et al., 2022)), and (iii) multi-modal discriminative (e.g., CLIP (Radford et al., 2021)). Empirical observations and theoretical analysesjointly inspire us to design a fourth paradigm, multi-modal generative pre-training (see (iv)). Underthis paradigm, we propose a novel vision learner for web image-text data pre-training, named MUlti-modalGenerator (MUG). MUG is composed of purely multi-modal generative objectives that attempt to generatethe multi-modal joint distribution from a single-modal image input. From an information-theoretical view,we show that MUG has a better upper bound for feature transferability, which helps the vision encoder learnmore transferable representations when compared to alternative paradigms. We present empirical results across a variety of downstream vision tasks to further validate the advantagesof MUG. For example, MUG outperforms the previous best performing methods by2.0% mIoU whentransferred to the ADE20k benchmark and by 0.5% on ImageNet-1k classification. Extensive ablations areconducted to help better illuminate the impact of critical model and design choices. Finally, visualizationsare provided that illustrate that our learned representations are capable of capturing the joint image-textdistribution.",
  "Vision Learners with Contrastive Learning": "Contrastive learning is a commonly used paradigm for single-modal self-supervised representation learning.Here, the pretext task is an instance discrimination one (Wu et al., 2018) which learns a representation bypulling positive examples that are generated from augmentations of the same image closer, while pushingnegative examples (e.g., different images from the same mini-batch) apart in the embedding space. Buildingon the basic instance discrimination task, SimCLR (Chen et al., 2020b;c) showed that larger mini-batchesand stronger augmentations can result in more effective representations. MoCo (He et al., 2020; Chen et al.,",
  "Published in Transactions on Machine Learning Research (August/2024)": "Gen. Cap.: a small motor scooter is parked on the Gen. Cap.: a white sheep standing in the middle of a Gen. Cap.: a small boat floating in the middle of a Gen. Cap.: a man riding a skateboard down a snow Gen. Cap.: a black and white dog laying on a couch Gen. Cap.: a train traveling down train tracks near trees Gen. Cap.: a collage of photos of a woman in Gen. Cap.: a man standing in the middle of a parking Gen. Cap.: a little girl sitting on a woman's lap Gen. Cap.: a horse that is standing in the dirt Gen. Cap.: a bicycle leaning against a wall next to a Gen. Cap.: a large black dog laying on top of a Gen. Cap.: a red and white train is parked in a Gen. Cap.: a bird is perched on top of a tree Gen. Cap.: a bird that is flying in the air Gen. Cap.: a person standing on a couch with a cat Gen. Cap.: a train on a train track Gen. Cap.: a brown and white horse standing next to a Gen. Cap.: a green and white bus driving down a road Gen. Cap.: a nighttime view of a city at night Gen. Cap.: a man and a woman are posing for a Gen. Cap.: a person standing in the middle of a pile Gen. Cap.: a cat sitting on a table next to a Gen. Cap.: a man and a woman standing next to each Gen. Cap.: a cat standing on top of a desk next Gen. Cap.: a person riding a scooter on a track Gen. Cap.: a young woman holding a white object in her Gen. Cap.: a living room filled with lots of furniture Gen. Cap.: a living room with a television and a wall Gen. Cap.: a blurry image of a snow covered area with Gen. Cap.: a penguin standing on top of a snow Gen. Cap.: a living room filled with furniture and a piano Gen. Cap.: a person standing in a living room with a Gen. Cap.: a dark room with a table and a lamp Gen. Cap.: a cat looking out of a window Gen. Cap.: a man laying in bed with his head on Gen. Cap.: a white dog standing on top of a lush Gen. Cap.: a living room with a couch and a window Gen. Cap.: a sailboat with two sailboats in the Gen. Cap.: a man is throwing a ball at a ball Gen. Cap.: a close up picture of a red and white Gen. Cap.: a woman standing in a kitchen preparing food Gen. Cap.: a living room filled with furniture and a window Gen. Cap.: a white dog standing in a grassy field Gen. Cap.: a train on the tracks near a fence Gen. Cap.: a baby sitting in a chair eating food Gen. Cap.: a white animal in a pool of water Gen. Cap.: a man and a woman standing next to each Gen. Cap.: a blurry image of a cat on the ground Gen. Cap.: a bird that is standing in the grass Gen. Cap.: a man laying on a couch with a pillow Gen. Cap.: a bottle of wine sitting on top of a Gen. Cap.: a boat in the middle of the water Gen. Cap.: a person jumping in the air with a skate Gen. Cap.: a woman and a man standing in a kitchen Gen. Cap.: a fighter jet sitting on top of an airport : Uncurated random samples on PASCAL-VOC images. For each triplet, we show the masked image(left), MUG reconstruction (middle), the ground-truth (right), and the generated captions by MUG. Themasking ratio is 75%.",
  "Vision Learners with Masked Image Modeling": "Inspired by the success of masked language modeling (MLM) using transformers (Vaswani et al., 2017)for natural language processing (Devlin et al., 2018; Brown et al., 2020), masked image modeling (Chenet al., 2020a; Bao et al., 2022; He et al., 2022; Xie et al., 2022) has been explored in the context of learn-ing representations using vision transformers (Dosovitskiy et al., 2021; Touvron et al., 2020). Specifically,iGPT (Chen et al., 2020a) learns a representation by predicting unknown pixels from a sequence of pixels.Instead of operating in pixel space, BEiT (Bao et al., 2022) predicts a set of discrete tokens from the encoderof DALLE (Ramesh et al., 2022; Van Den Oord et al., 2017), where the tokens are encoded versions of imagepatches as in (Ramesh et al., 2021). The training task was further simplified by MAE (He et al., 2022) andSimMIM (Xie et al., 2022), which only enforce the model to reconstruct the masked pixels of the input imageusing a 2 loss and do not require the use of discrete token encoders. The pretext task of masked image modeling can be viewed as optimizing log p(X|Z), where Z is therepresentation of the non-masked image X and p( X|Z) is the likelihood of generating the masked image partsX given the learned representation of the input tokens. This is equivalent to minimizing H(X|Z). Given thatI(X; Z) = H(X) H(X|Z), we can see that masked image modeling still follows the InfoMax (Hjelm et al.,2019) principle, where the maximization of I(X; Z) is performed by optimizing H(X|Z), instead of H(Z) asin contrastive learning. MIM has been shown to be more effective at learning transferable representationscompared to contrastive learning (He et al., 2022; Xie et al., 2022), indicating the effectiveness of generativepre-training.",
  "Vision Learners with Web Image-Text Pairs": "The vast majority of the previously discussed works have explored SSL in the context of single-modalwell-curated image datasets such as ImageNet-1k (Deng et al., 2009).However, it is relatively easy tocollect large-scale datasets containing image-text paired data from the web, e.g., images and associated textcaptions (Sharma et al., 2018; Schuhmann et al., 2021). Many approaches have recently began to studywhat impact this type of data has on learning visual representations, e.g., Radford et al. (2021); Mu et al.(2022); Kim et al. (2021); Yao et al. (2022); Li et al. (2022b); Yu et al. (2022); Desai & Johnson (2021).CLIP (Radford et al., 2021) explores this challenge by performing contrastive learning over paired imageand text data. By leveraging separate visual and text encoders, their training objective aims to maximizethe similarity between the learned embeddings of the paired image-text data from each training instanceand minimize the similarity between unpaired images and text. SLIP (Mu et al., 2022) builds on this, bylearning the representation by jointly performing contrastive learning on both image-text paired datasetslike CC3M (Sharma et al., 2018) and image-only datasets like ImageNet (Deng et al., 2009) using strongaugmentations (Chen et al., 2020b).SLIP showed that the added self-supervised signal on image-only",
  "Benchmarking SSL Methods on Web Data": "Given a dataset of image-text pairs D = {(xVi , xLi )} sampled from the joint distribution of p(XV , XL), wherean image is of size xVi RCHW , and a piece of text is of size xLi RL, our goal is to learn a visual featureencoder f : RCHW RD that maps an input image into a compact feature embedding and enableseffective transfer learning performance (He et al., 2019). Two families of self-supervised learning methods can be used to achieve this goal. This first is single-modalSSL, which uses only the images xVi within the dataset D. .1 and .2 introduce and describethe current most representative methods for single-modal SSL. The second approach is multi-modal SSL.This aims to learn an effective representation by aligning the representation of the input image with thecorresponding paired input text. A brief introduction to multi-modal SSL methods is provided in .3.As a preliminary motivation for our work, we first perform a benchmark analysis. We pre-train representativeSSL methods on the same web image-text dataset, for the same number of training epochs. Then, we evaluatethe transfer performance on downstream tasks by fine-tuning the resulting pre-trained models. Experimental setup.We categorize methods according to their pre-training paradigm, as discussedin .For single-modal SSL methods, we choose MoCoV3 (Chen et al., 2021) and SimCLR (Chenet al., 2020b) as representative discriminative methods, and MAE (He et al., 2022) as a representative gen-erative method. For image-text multi-modal SSL, we choose CLIP (Radford et al., 2021) and SLIP (Muet al., 2022) as discriminative methods, and also include CoCa (Yu et al., 2022), which is composed of bothdiscriminative and generative targets. Notably, in addition to these methods, we further implement anothermulti-modal baseline namely Masked Captioner (MAC), which generates text from masked images. It istrained using only a purely generative cross-modal objective. The details of MAC will be introduced in Sec-tion 4. For the pre-training hyper-parameters, we strictly follow the recommended setting in the originalpaper of each method. For a fair comparison, we pre-train all methods on a web dataset CC3M (Sharma",
  "\"! or\"!, \"\"": ": Left: Single/multi-modal discriminative methods have a narrow bottleneck and thus learn a lessinformative representation. Middle: Single-modal generative methods have a wide bottleneck and thuslearn a more informative representation. Right: Multi-modal generative methods have a wider bottleneckfor generating (e.g., recovering) the joint distribution of both modalities, and as a result learn an even moreinformative representation. : Like-for-like comparison of existing SSL pre-training methods. All models are pre-trained on theCC3M dataset (Sharma et al., 2018), and evaluated on ImageNet-1k (IN-1K) (Deng et al., 2009) via end-to-end fine-tuning. gen. and dist. are short hand for generative and discriminative. I and T denoteimages and text. For instance, SimCLR (Chen et al., 2020b) is pre-trained by discriminating images (disc.I). CLIP (Radford et al., 2021) is pre-trained by discriminating image-text pairs (disc. I&T ). CoCa (Yuet al., 2022) has two objectives: discriminating image-text pairs (disc.I&T ) and generating text fromimages (gen. T w. I). We also include a MAsked Captioner (MAC) baseline that generates text frommasked image patches (gen. T w. I).",
  "We make the following key observations from the results of the experiments in :": "Generative methods (e.g., MAE) achieve the best results. Across all methods, we can observe thatMAE (He et al., 2022) achieves the best performance. As mentioned in .1 and .2, weargue that discriminative pre-training and generative pre-training approach the InfoMax (Hjelm et al., 2019)principle via different mechanisms. Specifically, discriminative pre-training optimizes I(X; Zd) = H(Zd) H(Zd|X) via maximizing H(Zd), while generative pre-training optimizes I(X; Zg) = H(X) H(X|Zg) viaminimizing H(X|Zg) (Cui et al., 2022a), here Zd is the representation learned by discriminative pre-training,and Zg is the representation learned by generative pre-training. Suppose the models learned by generativepre-training and discriminative pre-training both achieve zero training loss, then I(X; Zd) will be H(Zd) andI(X; Zg) will be H(X). In the discriminative pre-training scenario, if we consider the Markov Chain of X Zd, we have I(X; X) I(X; Zd) from the data process inequality, i.e., H(X) I(X; Zd) = H(Zd). Thusgenerative pre-training has a better transferability upper bound compared to discriminative pre-training,and results in more informative and beneficial features for downstream tasks.",
  "zero training lossH(X)(3)": "Current multi-modal methods do not outperform single-modal ones. We can observe that the bestsingle-modal pre-training method outperforms the best multi-modal pre-training method by 1.3% on theImageNet-1k accuracy. Compared with our introduced MAC baseline, multi-modal discriminative methods(e.g., CLIP and CoCa) yield worse results. We believe the reason is that the information from the textmodality is greatly limited. A typical text annotation only contains partial information about the contents ofthe corresponding image. A prominent researcher has commented that language is a very low-bandwidthmethod for transmitting information: isolated words or sentences, shorn of context, convey little. (Browning& LeCun, 2022) It suggests the H(XL) could be much lower than H(XV ). In the case of the multi-modaldiscriminative method CLIP, a low H(XL) directly limits I(XL; Zd). Thus, poor transferring performance isobserved. When comparing SimCLR and SLIP we observe that SLIP falls behind SimCLR by 1.8% accuracy,which further supports the potential limitations of multi-modal discriminative training objectives. However,MAC is generative and alleviates this problem, but it still cannot outperform single-modal methods.",
  "H(XL) H(XV )(4)": "Discussions and opportunities. Inspired by the information-bottleneck theory (Tishby & Zaslavsky,2015; Shwartz-Ziv & Tishby, 2017; Alemi et al., 2017; Cui et al., 2022a; Hjelm et al., 2019), we illustrateto help understand our observations in . The first observation is that generative methods can helpthe model learn more transferable visual representations compared to discriminative ones due to a superiorI(X; Z) upper bound. In (left), we argue that discriminative methods have a narrow bottleneck anda low I(X; Z), thus a less informative representation learned. More detailed discussions are provided inAppendix Appendix A.6. In (middle), generative methods have a wider bottleneck, leading to more informative representa-tions. Given the side effects of multi-modal discriminative objectives (the second observation), developingmultiple generative objectives will encourage the model to learn more informative representations. As shownin (right), for an image-text pair, generating the joint distribution of both the image and text modal-ity will help the model to absorb as much information as possible from the data and approach a largerI(X; Z). Note that here we are using the term bottleneck to refer to the variance in the representation space Z withrespect to the input images X, i.e., how much the representation changes when the input is changed. One wayto understand this bottleneck is from the perspective of the rank of the feature matrix (Garrido et al., 2022).A higher rank would indicates that the model can capture more variance in the input, whereas a collapsedlower rank captures less information about the input. In contrast, a completely collapsed representationwould map all inputs to a same feature which has a rank of zero. In Appendix A.4, we provide a qualitativemeasure of the rank of different pre-trained models.",
  "Motivation": "Following the above observations, in this section we describe a more effective vision learner that learnsfrom web sourced image-text paired training data. We first formulate the mechanism that a multi-modalgenerative objective uses to learn a more transferable representation, in line with (right). We believethat introducing multiple generative objectives is feasible, where now the optimization objective is to learn",
  "maxH(XV ), H(XL)(zero training loss)(6)": "If we consider the fully optimized model to the optimal, then H(XV , XL|Z) = 0. As the lower bound ofI(XV , XL; Z) is greater than or equal to the lower bound of I(XV ; Z) because of the added variable XL,we can observe that generating the joint distribution of p(XV , XL) can at least learn an equally transferablerepresentation as single-modal generative pre-training. In practice, we observe that multi-modal generativepre-training achieves superior transferability.This design motivation is also in line with the discussionin .2. Overall this intuition motivates us to design a generative pre-training framework with multiplegeneration tasks using only the visual input and representation.",
  "MUG: MUlti-modal Generator": "Here we describe our novel multi-modal generative vision learner, named MUlti-modal Generator (MUG).The framework of MUG is presented in . It consists of an image encoder, an image decoder, and a textdecoder. Two tasks are involved, i.e., raw pixel and original caption generation. Raw pixel generation. Given a masked visual input xVi = M xVi , where M is the random generated 0-1mask following the procedure in MAE (He et al., 2022) where 0 indicates that the image region is maskedout and 1 indicates that it is visible to the model, denotes the element-wise product, the visual featureencoder f maps the masked input image to a latent representation tVi = f(xVi ). The latent representation tViis then fed into a visual decoder gV to generate xVi = gV (tVi ) which is the reconstruction of the non-maskedvisual input. We adopt the reconstruction loss from MAE (He et al., 2022) to train both f and gV :",
  "j=1log P(xLi,j|xLi,<j, tVi ),(8)": "where J is the maximum length of a textual caption, xLi,<j is the first j 1 tokens of the caption and xLi,jis the j-th token of the caption. The text generation is conditioned on the image features, which helpsto improve the transferability of the image encoder. Thus, the proposed text decoder has two parts, i.e.,single-modal and multi-modal layers. The single-modal part is constructed as an auto-regressive model viaa causal mask (Vaswani et al., 2017), where each word can only attend to the words before itself in a captionsequence. It only provides the text decoder with contextual information when generating the caption. Wedesign the multi-modal part using the image features as the key and value and the output from single-modallayers as the query in a cross-attention layer (Vaswani et al., 2017). This ensures the text generation isderived from the image features as the contextual information only serves as the query. Thus, the entire textdecoder can be seen as regrouping the image features tVi with contextual information to generate captions.",
  "MxVi xVi 2 C,(10)": "where is the standard deviation of the distribution. M and C are constants that do not affect the optimiza-tion process. Together with LL, we can see that our loss function in Eq. (9) is maximizing the log-likelihoodof both the generated image and the generated paired text. Thus the latent representation Z extracted bythe model is optimized to be more predictive of the image XV and the caption XL. In other words, theconditional entropy H(XV , XL|Z) is minimized via the two losses in our framework, maximizing the mutualinformation I(XV , XL; Z), and leading to a more transferable representation.",
  "Implementation Details": "Encoders and decoders.Our framework contains an image encoder f, and two decoders gV and gLfor decoding the latent representation to reconstruct the image and the textual caption, respectively. Theimage encoder f is implemented using a Vision Transformer (Dosovitskiy et al., 2021) backbone, and theimage decoder is implemented with the same architecture as in MAE (He et al., 2022).We implementthe text decoder gL following SimVLM (Wang et al., 2022). For efficient training, we train our encoder-decoder architecture using teacher-forcing (Williams & Zipser, 1989).Detailed implementation details,including architectures and hyper parameters, are provided in the Appendix. The code is available at Pre-training datasets.We train models on the publicly available CC3M (Sharma et al., 2018) andLAION400M (Schuhmann et al., 2021) datasets. Additionally, we also collect 200M web image-text pairs tostudy the scalability of MUG. The collected data has no filtering processing and as a result it potentiallycontains a large amount of noise which presents a challenge for multi-modal methods.We denote thisprivately collected dataset as W200M.",
  "Transfer Learning": "We begin by evaluating the transfer performance of different learned representations on a wide range ofdownstream tasks. Transfer learning is performed with conventional full end-to-end fine-tuning of the modelweights on target task (He et al., 2022). This highlights the superiority of MUG compared to other methods. Image classification on ImageNet-1k. First, we transfer learned representations to general image clas-sification on ImageNet-1k (Deng et al., 2009) using end-to-end fine-tuning. In addition to reporting perfor-mance on the validation images in ImageNet-1k, we also evaluate ImageNet-1k fine-tuned models on out-of-distribution datasets ImageNet-Adversarial (IN-A) (Hendrycks et al., 2021b) and ImageNet-Rendition (IN-R) (Hendrycks et al., 2021a). Results are reported in and . Compared to previous methods,MUG achieves the best top-1 accuracy 83.5% (with CC3M).When training MUG with ViT-S, ViT-B, andViT-L backbones, we also observe consistent improvements over the previous single-modal best performingMAE (He et al., 2022). Both comparisons illustrates that MUG surpasses previous methods by making useof scalable web image-text paired data. Notably for the results on IN-A and IN-R, MUG outperforms theprevious multi-modal methods by a large margin (i.e., >10% on IN-A) even through our model leveragesthe same supervision signal as these other multi-modal methods. This indicates that MUG is much moreeffective for learning representations that generalize to hard out-of-distribution cases. : Comparison of different backbone sizes(i.e., ViT-S, ViT-B, and ViT-L) for models pre-trained with CC3M for 400 epochs and thenfine-tuned on IN-1K. Top-1 accuracy is reported.Across all backbone sizes, we observe that MUGis superior to MAE in almost all cases.",
  "MUG (ours)ViT-L85.453.863.1": "Next, we evaluate pre-trained models using linear probingevaluation on ImageNet-1k. Results are reported in Ta-ble 4 and .Compared with discriminative pre-training methods, most generative methods obtain unsat-isfactory IN-1k linear probing results. MUG also suffersfrom unsatisfactory linear probing performance commonto other masked transformer models (He et al., 2022),where it has been hypothesized that linear probing re-quires discriminative representations, which cannot besatisfied by generative methods. For instance, SimCLRand MUG achieve 63.3% and 61.3% on the IN-1k val-idation set, respectively.However, in the case of IN-A and IN-R, generative methods like MUG achieve thebest results. We conjecture this is due to the fact thatdiscriminative methods can easily be overfit to the in-domain data, but conversely cannot achieve good out-of-distribution generalization. Additionally, compared withthe original MAE, MUG outperforms it by a large margin. Notably, when using the backbone ViT-S in Ta-ble 6, our method outperforms MAE by 11.3% top-1 accuracy. This indicates that MAE pre-trained models",
  "MUG (ours)52.361.367.6": "Fine-grained image classification. To illustrate thatour pre-trained models do not over-fit on ImageNet-1k-based evaluations, we further transfer learned representa-tions to fine-grained image datasets. Specifically, we re-port performance on iNaturalist-17 (iNat-17) (Van Hornet al., 2018), iNaturalist-18 (iNat-18), and Places365-Small (P365) (Zhou et al., 2017a) with end-to-end fine-tuning. Results are presented in . Similar to theresults of ImageNet-1k, MUG still outperforms the previ-ous SOTA method MAE on all evaluated cases. This further validates the success and robustness of MUGin transferring to image classification tasks. Moreover, we notice that the performance of MUG is relevantto the pre-training data domain. Concretely, iNat-17 and iNat-18 share a similar domain with IN-1K. Thus,the IN-1K MAE models perform better than CC3M MAE models.P365 shares a similar domain withCC3M, which leads to better results of CC3M-based models. Regardless of the fine-tuning data domain, theimprovements from MUG are stable and generalize across datasets. Semantic segmentation on ADE20K. We transfer MUG to a semantic segmentation task using theADE20K dataset (Zhou et al., 2017b). Experiments on ADE20K use the decode head from UperNet (Xiaoet al., 2018), and training recipes follow the default setting provided in mmsegmentation (Contributors, 2020).Here, the training resolution is set to 512 512, and the number of iterations is 160k. We evaluate modelswith different capacities in . We observe that our MUG approach results in significantly improvedtransfer performance across the ViT-S, ViT-B, and ViT-L backbones. We can see that the ViT-S MAE modelonly achieves 40.7% mIoU. Consistent, with the previous IN-1K linear probing results, MUG significantlyalleviates the performance drop on the small backbone by improving mIoU by 2.6%. Notably when usingthe larger ViT-L backbone, MUG still improves over MAE by 2.1% mIoU. Compared to CLIP and MoCoV3,the superiority of MUG supports our information-theoretical motivation, i.e., generative pre-training resultsin a better transferability upper bound compared to discriminative pre-training. Consequently, we observethat generative pre-training greatly benefits downstream fine-tuned performance.",
  "Ablation Experiments": "Ablation: Impact of image reconstruction and text generation losses.The most importantablation is the trade-off between the image reconstruction loss weight V and the caption generation lossL. We fix V and ablate the value of L. IN-1K fine-tuning experimental results are reported in .The best transfer performance is achieved when L = 0.1, and both larger and smaller values perform worse.This illustrates the trade-off between image reconstruction and text generation.",
  ": Reconstructions of masked images and captions from our MUG approach from the MS-COCO (top)and PASCAL-VOC (bottom) datasets": "Setting L = 0.0 makes MUG equivalent to MAE. Increasing L in the range of (0.05, 0.3] increases thetransfer performance, illustrating the effectiveness of our introduced text generation task.The reasonsfor the performance drop are two-fold: (i) As the CC3M pre-training data is web sourced, increasing Lemphasizes the noisy text generation task. This can be problematic as captions coupled with web imagescan be often irrelevant, e.g., an image contains a river, but its caption may be about hiking. (ii) As discussedin .2, the information contained in the text modality is greatly limited. Increasing L unavoidablyleads to over-fitting, which is unfavorable in representation learning. Therefore, a trade-off between imagereconstruction and text generation exists. We further study the effects of the image reconstruction target. We simply set V to 0.0 and set L = 1.0,which only permits the text generation task. Interestingly, setting V to 0.0 degrades MUG to our MACbaseline.Results are reported in (right), where a notable performance drop is observed.Thisvalidates that (1) the information in text modality alone is insufficient, and (2) motivates the need for imagegeneration for effective visual representation learning with image-text pairs. Ablation: Impact of the number of layers in text encoder.For the text generation task, bothsingle-modal and multi-modal layers are involved. We set the total number of layers to ten, and perform agrid search for the best result, see . We observe that setting the number of single-layers to one or two",
  "F.T. top-1 acc.83.383.583.082.582.1": "Ablation: Impact of form of text generation.Here we compare different text generation methods.There are two methods for text generation, i.e., masked language modeling (MLM) and auto-regressive mod-eling (captioning). We set the optimal L values for both methods, and observe that captioning outperformsMLM by 0.5% top-1 accuracy (83.5% v.s. 83.0%) on IN-1k. A harder task could help to approach a higherI(XV , XL; T). Captioning is regularized to sequentially predict words, while MLM is not regularized. Thetext generator is composed of single-modal and cross-modal parts, and the text generation methods actu-ally affects the learning mechanism of the single-modal part. Concretely, the auto-regressive learning targetforces the single-modal encoder to extract key words, e.g., the word after an article could be a key entitynoun. For masked language modeling, contextual words are possibly masked and predicted. However, suchnoisy and potentially meaningless reconstruction tasks likely do not benefit the vision encoder as much. Visualization: Reconstructed images with coupled text.In , we provide generated images andcaptions produced by MUG on the MS-COCO (Lin et al., 2014) and PASCAL-VOC datasets (Everinghamet al., 2015). The input to MUG only consists of a masked image and a start word (e.g., a). As observed,MUG can recover the main objects and entities in the raw images and annotated captions, even if most ofthe image pixels are masked. It is reasonable that MUG only describes visible pixels, e.g., in the middleof the second row, the left child is masked, and thus MUG only describes a woman and a man. Thesevisualizations show that the representation learned by MUG can generate the joint distribution p(XV , XL).",
  "Conclusion": "In this work, we first benchmarked different self-supervised representation learning methods and outlined twomain observations: (i) generative pre-training achieves the best transfer performance on web sourced datasets,and (ii) current multi-modal methods do not outperform single-modal ones. An information-theoretical viewwas developed to understand these benchmarking results and our observations. This inspired us to propose anew vision learner, titled MUlti-modal Generator (MUG), that makes use of web sourced image-text paireddata. MUG learns visual latent representations using two generative objectives and demonstrates stronggeneralizability across multiple transfer learning tasks with satisfactory scaling performance, and validatesour information-theoretical perspective.",
  "Mohamed Ishmael Belghazi, Aristide Baratin, Sai Rajeshwar, Sherjil Ozair, Yoshua Bengio, Aaron Courville,and Devon Hjelm. Mutual information neural estimation. In ICML, 2018. 18": "Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss,Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, ClemensWinter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, JackClark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Languagemodels are few-shot learners. In NeurIPS, 2020. 3",
  "Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. Natural adversarial exam-ples. In CVPR, 2021b. 9": "R Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Phil Bachman, Adam Trischler,and Yoshua Bengio. Learning deep representations by mutual information estimation and maximization.In ICLR, 2019. 1, 2, 3, 5, 6 Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V. Le, Yunhsuan Sung,Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy textsupervision. In ICML, 2021. 1",
  "A.1Implementation Details": "Pre-training. The default setting is in , and hyper-parameters mainly follow He et al. (2022) forfair comparisons. For each caption, we set the max length to 70 and use a percentage of 20% input wordsfor processing. For each word, we mask it, replace it with a random word, or delete it with a probability of50%, 10%, and 40%, respectively.",
  "configvalue": "optimizerAdamWbase learning rate1.5e-4weight decay0.05optimizer momentum1, 2=0.9, 0.95batch size4096learning rate schedulecosine decaywarmup epochs40image augmentationRandomResizedCroptext augmentationmask/replace/deleteV1.0L0.1 End-to-end fine-tuning on IN-1K, iNat-17, iNat-18, and P365.We fine-tune models with the widelyused recipe He et al. (2022); Bao et al. (2022) in except for the layer-wise decay rate is set to 0.7. On allevaluated datasets and pre-training models, we use thesame setting for fair comparisons. Linear probing. Settings are provided in . Wefollow He et al. (2022) to introduce an extra BatchNormlayer without affine transformation for improving linearprobing performances. For fair comparisons, all evaluatedmodels are equipped with the extra layer. Semantic segmentation on ADE20K. The ADE20Kfine-tuning setting can be found in this website. We advise settings for ViT-S and ViT-L by following Touvronet al. (2020) and Bao et al. (2022), respectively. All evaluated pre-training models are fine-tuned with thesame setting, and thus comparisons are fair.",
  "A.2Transfer Learning with Visual Prompts": "Recent progresses in transfer learning in the visual domain has demonstrated the effectiveness of usingvisual prompt tuning (VPT) (Jia et al., 2022). By adding a small set of learnable tokens to the visual tokensequence, VPT (Jia et al., 2022) can match, or even outperform, the performance of fully end-to-end fine-tuning of the pre-trained model. Thus VPT has been seen as an effective method for tuning a pre-trainedmodel. In this section, we demonstrate that MUG learned representations are effective for use by VPT byevaluating on several fine-grained benchmarks from the original VPT paper (Jia et al., 2022). The resultsare presented in . We can see that MUG improves the performance on all the data that we tested,indicating that the features learned by MUG are not only effective for full fine-tuning, but also effective forparameter efficient tuning, which is more practical in real applications.",
  "MUG (ours)LAION400M570.669.768.8": "see that in terms of linear probing, both MAE and MUG do not achieve a good performance compared toother methods. However, in the visual prompt tuning experiments, MUG demonstrates strong performance.As demonstrated in the original MAE paper, linear probing results are not always correlated with the finaltransfer performance of the learned representations, as linear probing misses the opportunity to make use ofstrong, but non-linear, features. Thus, we are more interested in the visual prompt tuning results as theybetter demonstrate the learned representations quality. The effective rank experiments in alsodemonstrate that MUG can learn high quality representations.",
  "A.4Assessing Transferability using RankMe": "There have been a number of works that assess the performance of a pre-trained model on a downstreamtask prior to fine-tuning the model on the actual downstream dataset, e.g., Garrido et al. (2022); Agrawalet al. (2022). RankMe (Garrido et al., 2022) is one such method that assesses the performance of a modelusing the effective rank of the features extracted by the model on a downstream task. This can be viewedas measuring part of the mutual information I(X; Z), as the rank of the features describe the variation ofthe features across dimensions. I(X; Z) describes not only the dimension-wise variation of the feature, butalso the magnitude of variation within each of the dimensions. As the measure of mutual information in high dimensional space is difficult (Belghazi et al., 2018), we leverageRankMe to show that MUG can indeed learn a superior visual representation. Here, this is measured interms of having higher effective rank compared to other methods on features extracted from the ImageNet-1kdataset. The results are shown in . Overall, we observe that discriminative methods like MoCoV3",
  "A.5Pre-training on Larger Datasets": "In this section we demonstrate the scability of MUG by pre-training models on larger datasets such asLAION400M (Schuhmann et al., 2021) and W200M. We mainly evaluate the performance of models using aViT-B backbone. For pre-training models on different sized datasets, we keep the total number of iterationsthe same across different datasets for a fair comparison. Fine-tuning on ImageNet-1k. We first compare the results of larger pre-training datasets for differentpre-training methods on ImageNet-1k using end-to-end fine-tuning. Top-1 accuracy performance is reportedin . We observe that similar trends from before still hold. Notably, MUG achieves the best resultsacross IN-1k, IN-A, and IN-R. This demonstrate the scalability of MUG, as the performance of MUG stillgrows in line with the size of the pre-training dataset. In addition, MUG is significantly better than othermulti-modal baselines.",
  "MUG (ours)LAION400M550.0": "Fine-tuning on ADE-20k. We also provide a comparison using larger pre-training datasets when fine-tuning for semantic segmentation downstream task on ADE-20k. The results are presented in . Itcan be observed that MUG can indeed improve the transfer learning performance as the pre-training datasetgrows. We note that MUG achieves the best transfer learning results, again indicating the effectiveness ofMUG.",
  "A.8Limitations and Border Impact": "One common limitation of models pre-trained on web sourced image-text pairs is that the models willinevitably be influenced by the open nature of internet data, such as biases or harmful speech. Thus themodels may learn unwanted behaviors that may cause actual harm to people. Given this limitation, resultingmodels should be tested thoroughly before they are deployed in real-world applications."
}