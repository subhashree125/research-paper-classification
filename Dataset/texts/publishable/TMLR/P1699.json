{
  "Abstract": "Do vision-language models (VLMs) pre-trained to caption an image of a durian learn visualconcepts such as brown (color) and spiky (texture) at the same time? We aim to answerthis question as visual concepts learned for free would enable wide applications such asneuro-symbolic reasoning or human-interpretable object classification. We assume that thevisual concepts, if captured by pre-trained VLMs, can be extracted by their vision-languageinterface with text-based concept prompts. We observe that recent works prompting VLMswith concepts often differ in their strategies to define and evaluate the visual concepts,leading to conflicting conclusions. We propose a new concept definition strategy based ontwo observations: First, certain concept prompts include shortcuts that recognize correctconcepts for wrong reasons; Second, multimodal information (e.g. visual discriminativeness,and textual knowledge) should be leveraged when selecting the concepts. Our proposedconcept discovery and learning (CDL) framework is thus designed to identify a diverse listof generic visual concepts (e.g. spiky as opposed to spiky durian), which are ranked andselected based on visual and language mutual information. We carefully design quantitativeand human evaluations of the discovered concepts on nine diverse visual recognition datasets,which confirm that pre-trained VLMs do learn visual concepts that provide accurate andthorough descriptions for the recognized objects. Code and models are publicly released. 1",
  "Introduction": "Can a vision-language model (VLM) pre-trained on images of California seagull learn visual concepts (Lakeet al., 2015; 2011; Lee et al., 2023; Sun et al., 2015), such as yellow legs and white belly, to describe animage of a Gentoo penguin, which the model might not see during training? Visual concepts such as color,shape, and texture help models generalize compositionally (Farhadi et al., 2009; Nagarajan & Grauman,",
  "California Gull, which is four-limbed": ": The top-selected text prompts by CLIP given a query image, VDES is proposed by Menon &Vondrick (2022), whereas CDL is our proposed approach. The design of concept prompts plays a critical roleon understanding whether VLMs learn visual concepts. We can observe that concept-augumented promptcan predict correct visual concepts (e.g., gray back and wings) when the prompt is associated with thecategory name (California seagull). When the category name is removed from the prompt (Column 2), theretrieved concepts are either non-visual or incorrect. We attribute this to the category name bias (Column3), as the correct category can be retrieved by CLIP even when the paired descriptions are randomly shuffledand thus irrelevant. We propose a concept discovery and learning (CDL) framework and demonstrate thatpre-trained VLMs can indeed learn visual concepts (e.g., Column 4). Correctly predicted concepts are ingreen , wrong concepts are in red , and non-visual concepts are in violet . Category names are in orange . 2018; Hsieh et al., 2024; Thrush et al., 2022; Stein et al., 2024), and can be incorporated into neuro-symbolic frameworks (Mao et al., 2019; Yi et al., 2018) or offer concept-based explanations for classificationdecisions (Koh et al., 2020). Our paper aims to investigate if VLMs, such as CLIP (Radford et al., 2021), learn visual concepts auto-matically when pre-trained on image and text pairs with contrastive learning objectives. We hypothesizethat the visual concepts, if captured by the pre-trained VLMs, can be directly extracted by prompting theirvision-language interface, without needing to probe their internal representations. Our research questioncan thus be formulated as discovering the visual concepts encoded by pre-trained VLMs, and evaluating thequality of the extracted concepts. Interestingly, several recent works (Menon & Vondrick, 2022; Yang et al., 2023b; Yun et al., 2023) reacheddifferent conclusions on the encoding of visual concepts in pre-trained VLMs.For example, Yun et al.(2023) observed that CLIP does not appear to recognize fine-grained visual attributes for birds (Wah et al.,2011), where the list of visual attributes is pre-defined by bird experts. In contrast,Menon & Vondrick(2022) demonstrated that object prompts with visual concepts proposed by a large language model (LLM)appear to provide interpretable object classification, as the concept descriptions are nicely correlated with therecognized object categories. As illustrated in , we attribute these discrepancies to their differentstrategies for extracting visual concepts in a pre-trained VLM: First, according to the first and secondcolumns of , certain shortcuts (e.g., the object category California seagull) may dominate the textprompts when recognizing visual concepts, reinforcing the object-concept correlations as given by a priorknowledge base, such as LLMs. A possible remedy is to discover general visual concepts shared by multipleobjects, without including the shortcuts in a text prompt. Second, according to the third column in ,some of concepts proposed by LLMs are not visually discriminative (e.g., a nocturnal bird) or cannot bereliably recognized by VLMs (e.g. concept prompts marked in red, or the expert prompts used by Yunet al. (2023)). They should be excluded from the list of visual concepts. More generally, in order to drawa convincing conclusion on whether pre-trained VLMs learn to encode visual concepts, one should discoverthe list of visually discriminative concepts and use them to prompt VLMs in a shortcut-free manner, andthen measure the quality and thoroughness of the discovered concepts, when they are tasked to recognizefine-grained objects and generate interpretable explanations with visual concepts. In this paper, we propose a novel concept discovery and learning framework that extracts shortcut-free andvisually discriminative concepts from pre-trained VLMs. To discover general concepts that do not includecategory specific shortcuts, we propose to utilize a large and diverse image captioning dataset to discover",
  "Published in Transactions on Machine Learning Research (01/2025)": "since it indicates that if a concept can be recognized from the image, the same concept should be relevant forthe paired caption, and vice versa. This helps filter out concepts that are non-visual (relevant according toLLM, but cannot be recognized visually by VLM, such as fish-eating and endangered animal), irrelevantto the objects of interest (can be recognized by VLM, but irrelevant according to LLM, such as conceptyellow-leg for recognizing food). Figure A1 shows some examples for the MI calculation.",
  "Related Work": "Vision-and-language models (VLMs) pretrained on unlabeled pairs of images and texts from the internet haveshown great success on multi-modal benchmarks. Based on the pre-training objectives, VLMs can be broadlycategorized into two primary types: contrastive VLMs and generative VLMs. Contrastive VLMs (Radfordet al., 2021; Alayrac et al., 2022; Jia et al., 2021; Yao et al., 2021) focus on learning joint representationsof images and texts by leveraging contrastive learning techniques (Chen et al., 2020). Generative VLMsLu et al. (2019); Wang et al. (2023); Chen et al. (2023); Liu et al. (2024) aim to generate coherent textdescriptions from images. Representations learned by these VLMs can be transferred to a wide range oftasks, such as image classification (Pratt et al., 2023), visual question answering (Li et al., 2023; Bai et al.,2023), and image and video captioning (Zhang et al., 2021; Yang et al., 2023a). Visual concepts, which represent the fundamental factors of variations (e.g. colors, shapes) (Hu et al., 2018)in the visual world, have been widely utilized to develop intepretable visual models that can generalizecompositionally (Farhadi et al., 2009; Lampert et al., 2009; Nagarajan & Grauman, 2018; Stein et al., 2024).Previous works (Sun et al., 2015; Hernandez et al., 2021; Lee et al., 2023) have proposed to discover textualterms associated to visual concepts to interpret visual systems.These concepts can be integrated intoneuro-symbolic frameworks (Mao et al., 2019; Yi et al., 2018) and offer concept-based explanations for visualrecognition (Koh et al., 2020). Contrastive VLMs, learning joint representation for images and texts, providea natural interface for discovering such visual concepts. However, it remains unclear whether VLMs pre-trained with contrastive objectives can learn discoverable visual concepts. Previous studies have exploredthe capability of VLMs to capture and compose primitive concepts (Yun et al., 2023; Yuksekgonul et al.,2022a) and bind visual concepts with objects (Lewis et al., 2022; Yamada et al., 2022). Yun et al. (2023)demonstrate that VLMs do not capture composable primitive concepts by intervening a linear classifierlearned from VLM-predicted concepts. Meanwhile, previous works (Pratt et al., 2023; Menon & Vondrick,2022) show that concept-augmented prompts do provide improvements for VLM-based image recognition. Inthis paper, we aim to address this discrepency and understand pretrained VLMs true capability of encodinginterpretable visual concepts. To interpret the decision of visual models with visual concepts, Koh et al. (2020) proposed Concept BottleneckModel (CBM) to decompose the visual recognition into concept classification and concept-category mapping.While an alternative approach (Bau et al., 2017; Kim et al., 2018; Hernandez et al., 2021) aims to developpost-hoc interpretation for visual models by directly analyzing the internal representation of the modelswithin the concept space, the CBM does not rely on the model having already learned concepts, instead,it simultaneously trains the concept classifier and linear concept-category mapper upon the model beinganalyzed with the image classification objective. As a result, the CBM can evaluate the concept learning",
  "Method": "We first describe a framework for extracting visual concepts from pre-trained VLMs and its application forobject recognition in .1. We show in .2 that concepts which are non-visual or includecertain shortcuts might lead to wrong correlations, where the concept activations do not correspond to howlikely the visual concepts are actually present in an image. In .3, we propose a concept discoverymethod to select shortcut-free visual concepts from a large image captioning dataset by utilizing VLM andLLM knowledge to evaluate the visual discrimination. We also propose a self-supervised learning frameworkto re-align the image-text interfaces of VLMs to improve the quality of selected concepts. In .4, wedescribe the method to select a compact set of concepts for specific object recognition tasks. In .5we propose a suite of quantitative and human evaluations on the interpretability, precision, thoroughness,and generalizability of the discovered visual concepts. Together they help us understand if pre-trained VLMslearn to encode visual concepts.",
  "Object Recognition via Visual Concepts": "Vision-language models such as CLIP (Radford et al., 2021) jointly learn an image encoder EI and a textencoder ET to align images and texts in a shared embedding space. Thanks to the flexibility of the image-language interface, several recent works (Pratt et al., 2023; Menon & Vondrick, 2022; Yun et al., 2023;Yang et al., 2023b; Yan et al., 2023a) attempted to construct semantically interpretable representationsby projecting an encoded visual embedding with basis defined by encoded text embeddings.The textembeddings are obtained by encoding manually designed or automatically generated text prompts thatare likely to correspond to visual concepts. Concretely, given an image I and a set of concept prompts P ofN concepts, one can project the visual features EI(I) into the space of concepts as an n-dimensional conceptactivation vector a = [a1, a2, ..., aN]. Each concept activation is computed as ai = sim(EI(I), ET (pi)) (1),where pi is the i-th concept prompt, and sim() is a similarity function, such as cosine similarity, that measuresthe similarity between the encoded visual and text embeddings. The activations are standardized with z-score normalization to ensure comparability across objects. Our paper assumes that the visual concepts, ifwell learned by a VLM, can be extracted as concept activations a via the text prompts.",
  "The concept activations can be utilized for multi-modal object recognition with a function f : RN RM": "that predicts object categories, where M is the total number of categories. When f is a linear functionf(a) = aW, the learned N M weight matrix W allows us to interpret how the visual concepts are utilizedfor object recognition. A higher positive weight wij indicates the i-th concept is deemed as important positiveevidence for recognizing the j-th object category, and a near-zero weight indicates the concept is deemedas irrelevant. The linear classifier that maps concept activations to categorical predictions is referred to asConcept Bottleneck Models (Koh et al., 2020) (CBM), which have been adopted to study concept learningwith VLMs in Yang et al. (2023b); Yun et al. (2023); Yan et al. (2023a). For zero-shot object recognitionwhen f() cannot be learned from data, some works (Pratt et al., 2023; Menon & Vondrick, 2022) assume thevisual concepts are category-specific (hence the object names are included in the prompts), and simplify f()to be a linear function fj(a) =1",
  "Do Prompted Activations Correspond to Visual Concepts?": "Large language models (LLMs) are often utilized as the knowledge source to propose the relevant visualconcept prompts given an object category (Pratt et al., 2023; Menon & Vondrick, 2022; Yang et al., 2023b;Yan et al., 2023a). One example is illustrated in , where for California Gull, concept prompts such asCalifornia Gull, which has a long, black tail are proposed. As discussed in .1, the linear functionf(a) allows us to identify the most important visual concepts for object recognition by picking the highestweighted wij concepts is for object j. One can then consider two proxy evaluations to measure whetherthe prompted concept activation ai actually corresponds to the visual concept i that is present in an image:First, by measuring the object classification accuracy, and assuming that the higher the accuracy is, themore precise the concept activations are. Second, by comparing the top ranked concepts for an image of acertain object category and those identified by human experts, which allows us to understand not only theprecision but also the thoroughness of the concepts. We observe that these proxy evaluations, while intuitive, require careful design of concept prompting strategyin order to draw conclusions on whether the concepts are actually encoded by the pre-trained VLMs. Thefirst issue we identify is the existence of certain shortcuts in the text prompts, leading to false positiveconclusions. For example, illustrates the concepts with the highest activations according to CLIP.Although the first column appears to indicate that most of the selected concepts are semantically correlatedwith the input image of a California Gull, it remains unclear whether the concepts are retrieved becausethey are recognized by CLIP or the class name is utilized as a shortcut. We perform a simple ablation toinvestigate this potential shortcut: In the second column, we observe that when we combine the categorynames with randomly shuffled descriptors, CLIP tends to align images with concepts that contain correctcategory names and wrong descriptors. This phenomenon indicates that the category names, instead of thevisual concepts, are used to generate the concept activations. We validate these qualitative observations in.2, where we demonstrate consistently across nine datasets that the zero-shot classification accuracyremains similar when LLM-generated concepts or randomly shuffled concepts are paired with category names,and that the accuracy drops significantly when category names are removed from the text prompts. A second issue is the existence of sub-optimal text prompts, leading to false-negative conclusions. Forexample, the concepts marked with purple background in are not visually recognizable (e.g. noc-turnal bird), and hence should not be considered as visual concepts. Similarly, we observe that the textprompts used by Yun et al. (2023) are designed by birding experts, which may not be in a friendly formatto serve as text prompts (e.g. crested head pattern). In order to address both issues, we propose to discover category-generic (hence no shortcuts) and visuallydiscriminative concepts directly from a pre-trained VLM (hence the text prompts are more friendly to theVLM). We then propose a suite of evaluations in order to draw robust conclusions on the precision andthoroughness of the discovered concepts.",
  "Visual Concept Discovery and Learning": "We propose to discover visual concepts from a large pool of diverse objects, so that the discovered conceptswould be shared by multiple objects and generalizable to different domains. Towards this goal, we adopt theConceptual Captions 3M (Sharma et al., 2018) dataset (CC-3M). CC-3M contains three million images andtheir captions, hence offering a huge repertoire of objects and their characteristics rendered in both imagesand text. As illustrated in , our overall goal is to identify the key objects and their associated visualconcepts from the image captions, with the help of a large language model as the knowledge source. Thecandidate visual concepts are then checked for their visual discriminativeness, by verifying if a candidatevisual concept can be reliably recognized from the corresponding image. Thus, we can select the conceptsthat can both be recognized by the VLM in the image and be deemed by the LLM as a relevant attributefor the object described in the caption. As objects in captions often hold specific dependencies, such as serving as the subject or object of an action,we employ Dependency Parsing (De Marneffe et al., 2006) along with a set of designed rules (see Appendixfor details) to retrieve the words and phrases that potentially correspond to objects in the captions (e.g.,",
  "Mutual Information": ": Illustration of our proposed concept discovery method. Given image-caption pairs, we first identifyobjects from the captions and utilize a large language model to propose candidate concepts for the objects.The concepts are then ranked by the agreement between VLM knowledge (concept recognition from theimage) and LLM knowledge (concept proposed based on the caption) based on mutual information.",
  "m categories": ": Illustration of the concept-based object recognition framework and our proposed concept learningmethod. We map the concept activations a to categories with the concept-category association matrix W.For object recognition, only W is optimized based on object classification supervision. For concept learning,we assume W is a binary matrix given by LLM knowledge, and learns to update a by fine-tuning the lastlayers of visual and text encoders in the VLM. We rely on the VLM recognized object labels as opposed toground truth object labels, hence the process is self-supervised. king penguin from a sentence a group of king penguins walking in the snow). We then leverage theLLM as an external knowledge base to obtain visual concepts for recognizing the objects. We design a setof prompts (e.g., What are useful visual features for distinguishing an {object} in a photo?) to query theLLM and retrieve relevant concepts for the object of interest. We take the union of the concepts discoveredfor all objects as the preliminary list of generic visual concepts. We then propose to filter the concepts so that the selected ones are also visually discriminative. Intuitively,a visual concept should be ranked higher when the VLM can recognize it from the image if and only if whenthe same concept is proposed by the LLM based on the image caption. Otherwise, the concept either is notspecific to the object of interest (i.e. can be recognized from the image but not proposed to describe anyobject mentioned in the caption), or likely to correspond to non-visual concepts (e.g. popular among kidsand social birds in ). Specifically, given a concept and a list of image-caption pairs, we definetwo variables X and Y , where X corresponds to the image-concept similarity as measured by a VLM, andthe Y is a binary indicator on the caption-concept correspondence according to an LLM. A high X value xindicates that the VLM can recognize the concept from the image well, and a higher Y value y indicates theconcept is deemed by the LLM as a relevant attribute for the object described in the caption.In practice,we compute x as the cosine similarity between the CLIP embedding of the concept and the image, and definey as a boolean indicating whether the concept is relevant to the object in the image, as determined by the",
  "which measures the amount of information gain of one variable by knowing another variable. A higher I(c)means that Xc and Yc are in agreement": "Although we can discover general and visually discriminative concepts from pre-trained VLMs, these conceptsare not always perfectly-align with images as shown in Column 2 of and Row 4 of . Wepropose a self-supervised fine-tuning method to refine the image-concept alignment of VLMs by leveragingthe knowledge already encoded in pre-trained VLMs and LLMs. We first perform the zero-shot classificationwith pre-trained VLMs to generate pseudo labels for training. We then construct a ground-truth concept-category association matrix WLLM with binary weights, where each weight is obtained by querying the LLMwith prompts like Does the {category_name} usually have the attribute {concept}?. As illustrated in, we project the image and concept embeddings into concept activations a using the linear projection.The prediction of image category is obtained by the matrix multiplication, where f(a) = aWLLM. Assumingthat the WLLM provides ground-truth concept-category association, the loss derived from the predicted labelsis utilized to optimize the image-concept alignment. The whole framework is self-supervised as it requiresno additional human annotations.",
  "Visual Concept Applications": "Our concept discovery and learning framework aims to obtain a list of generic and visually discriminativeconcepts. We propose to apply the discovered concepts on fine-grained object recognition benchmarks suchas bird classification (Wah et al., 2011) and food classification (Bossard et al., 2014) to evaluate their quality.For such applications, it is often desirable to obtain a compact and performant set of visual concepts (e.g.four-wheeled is useful for recognizing cars, but not so much for birds). Towards this goal, we first constructthe concept-category association matrix WLLM and perform concept learning only for the object categoriesin the target dataset, so that irrelevant concepts not used by any objects are automatically discarded. The list of remaining visual concepts can be further optimized based on their usefulness and generalizability.We first try to identify the visual concepts that can be reliably recognized from the target dataset. Were-purpose I(c) where Yc is now obtained by looking up the concept association matrix WLLM knowing theground truth object label for an image. A higher I(c) means the concept c is useful to recognize a subsetof object categories in the dataset (according to the LLM knowledge used to construct WLLM), and can bereliably recognized from the images when the concept is expected to appear. We also expect the selected concepts to be generalizable, that is, to benefit the recognition of unseen objects.We employ a heuristics where a visual concept is more likely to generalize if it is already used by manyknown object categories. The generalizability of a concept G(c) can hence be estimated by the ratio ofobject categories that contain the concept c over the total number of object categories. There exists a natural trade-off between the usefulness and generalizability of a visual concept, we hencecompute the weighted average I(c) + (1 ) G(c) to rank the concepts and apply a fixed budget onthe number of visual concepts to use for each downstream benchmark. We select based on classificationperformance on the validation set (a lower , namely more general concepts, is preferred whenever theaccuracy remains high).",
  "Experiments": "We conduct quantitative and human evaluations to evaluate the discovered and learned concepts. In Sec-tion 4.2, we demonstrate why we need short-cut free and visually discriminative concept discovery andlearning by showing that concept prompts in previous works might be category-biased. In .3 and.4, we aim to prove that pre-trained VLMs do learn visual concepts by demonstrating that thediscovered concepts can lead to accurate classification and exhibit high qualities including interpretability,precision, thoroughness and generalizability. In .5 we aim to explore how specific components inour proposed framework contribute to the quality of discovered concepts.",
  "Prompt DesignImageNetFoodCIFAR-100CIFAR-10CUBFlowersStanford CarsAircraftsOxford Pets": "Category Name71.691.875.996.263.177.477.336.193.5Name w/ LLM Concepts (Menon & Vondrick, 2022)75.092.477.796.663.578.977.637.492.2Name w/ Random Concepts70.191.675.495.062.178.776.536.092.4LLM Concepts only22.13.630.970.75.37.03.21.66.4 : Zero-shot classification on nine object recognition benchmarks. We observe that although aug-menting category names with LLM-generated concepts improves classification accuracy (e.g. VDES (Menon& Vondrick, 2022)), the method still mainly relies on category names in the text prompts, and it remainsunclear if the concepts included in the text prompts are properly recognized by pre-trained VLMs. Whenpaired with randomly shuffled concept in the prompts, the accuracy drops only moderately; when the cate-gory names are removed, the accuracy drops significantly.",
  "), Aircrafts (Maji et al., 2013) and Oxford Pets (Parkhi et al., 2012). The statistics and split of thedatasets are shown in the appendix": "Baselines: We compare with LaBo (Yang et al., 2023b) and LM4CV (Yan et al., 2023a), which are thestate-of-the-art works on concept-based visual recognition. Following the setting in LM4CV, we control thebottleneck sizes (number of concepts) to be the same for the baselines and our model for fair comparison.Besides full-shot classification, we also compare with LaBo on few-shot settings, where we select conceptsand train the model on limited data. It is infeasible for LM4CV to conduct few-shot classification since itrequires the whole training set on concept generation. Implementation Details: We use the same LLM GPT-3-text-davinci-002 to obtain descriptors as pre-vious works. We also use the same CLIP backbone (ViT-L-14) to compare with baseline models. Following(Yun et al., 2023), we use logistic regression to train the concept bottleneck models. We observe that theperformance of CBM is robust to the choice of hyperparameters and use the default Scikit-learn hyperpa-rameters for all datasets. For concept learning, we use the AdamW optimizer with 5e-4 learning rate and1e-4 weight decay to fine-tune the CLIP model, and we use the validation loss to select checkpoints. Forhuman evaluation experiments, we hire annotators from Amazon Mechanical Turk. For each example, weask three annotators to annotate and use the majority vote to obtain the result. We conduct StudentsT-test (Student, 1908) to evaluate the statistical significance of the human evaluation results. The detaileddesign of human evaluation and the statistical significance results are shown in the appendix.",
  "Concept-Augmented Prompts Are Category-Biased": "As discussed in .2, we conduct ablation experiments to understand if the concepts used in theprompts of previous zero-shot classification methods lead to improved classification accuracy. The resultsin are consistent with , both of which show that the concept-augmented text prompts donot offer conclusive evidence whether pre-trained VLMs learn to encode concepts. Even random conceptshave minimal impact on zero-shot performance (as shown in Row 2 and Row 3). In contrast, the removal ofcategory names results in a catastrophic decline in zero-shot performance (as shown in Row 2 and Row 4).These observations suggest that category names act as critical shortcuts in concept-augmented prompts. Besides category biases, the discovered concepts in previous works contain many non-visual concepts asillustrated in . We conduct human evaluation to compare our discovered concepts with previousworks by the proportion of non-visual concepts and concepts containing class names. For both baselines andour CDL, we randomly select 100 concepts for each dataset for human evaluation. The results are shownin . We can observe that CDL offers visual concepts that are more category-agnostic and visuallydiscriminative.",
  ": Few-shot classification evaluation with LaBo and our method": "fair comparison, we conduct the classification using our concept discovery and selection methods, withoutconcept learning. shows that our method consistently outperforms the baseline methods on alldatasets. We then adopt the few-shot learning setting, where we select the concepts (as described in 3.4) and train themodel with the few training examples. shows that CDL consistently outperforms LaBo, especiallywhen the number of training examples is smaller.",
  "Evaluation of the Discovered Concepts": "shows the evaluation results of the Interpretability, Precision, and Thoroughness of the baselinesand our method. For human evaluation of Precision, and Thoroughness, we randomly select 100 imagesfrom each dataset and ask 3 human workers to annotate the Precision and Thoroughness of top-weightedconcepts for each image as described in 3.5. More details are shown in Appendix Sec. (D). The Precision",
  "CDL0.27.010.313.9": ": Generalization evaluation for discovered concepts. The In-domain results refer to improvementson unseen categories in the same dataset, while the Cross-domain results refer to improvements on unseendomains. denotes the improvement of fine-tuned CLIP compared to the original CLIP. Higher improvementmeans better generalization. and Thoroughness of each dataset are calculated as the averages of these evaluations on the 100 images,with the results displayed on the Y-axis of . We can observe that despite having high classificationperformance, the baseline models discover concepts that exhibit unsatisfactory interpretability, precision,and thoroughness. Our concept discovery method provides significant improvements on all three metrics,and the self-supervised concept learning can further improve the concept quality. shows the in-domain and cross-domain generalization results. Our proposed CDL outperforms bothbaselines for its generalizability, especially for the cross-domain scenario when transferring from ImageNet toCUB. We observe that LaBo and LM4CV struggle with cross-domain generalization as they select completelydifferent concepts for different datasets and few common patterns can be learned with their methods.",
  "CC-3M0.27.010.313.9": ": Comparison of the concept discovered from CC-3M and other datasets on both in-domain andcross-domain generalization evaluation for the CUB task. The concepts discovered from CC-3M have bettergeneralizability in both cases. selection for downstream tasks. As shown in , the concepts discovered by the LLM-only method areof unsatisfactory classification accuracy, interpretability, precision and thoroughness, which demonstratesthe necessity of utilizing multi-modal information to select visually discriminative concepts. The resultsshow that our proposed multi-modal concept discovery method can effectively improve the classificationperformance and the interpretability, precision and thoroughness of the discovered concepts. Utilization of the CC-3M datasetTo analyze the effectiveness of the utilization of the CC-3M datasetduring concept discovery, we perform the proposed concept discovery method on the CC-3M (Sharma et al.,2018) dataset and the downstream datasets. The performances of both methods are similar on CUB (detailsare shown in Appendix), which indicates that our proposed concept discovery method can still provide signif-icant improvement on the classification and concept quality with only downstream datasets. Considering thecost of human evaluation for the Precision and Thoroughness, we select the CUB dataset as the downstreambenchmark for the generalization evaluation. The in-domain and cross-domain generalization results in indicate that the concepts discovered from CC-3M are more generalizable than the concepts discoveredfrom downstream datasets. In conclusion, the proposed concept discovery and learning method providesimprovement on classification performance and concept quality regardless of the utilization of datasets, anddiscovering concepts from the CC-3M datasets can enhance the generalization of concepts.",
  "Conclusion and Future Work": "In this paper, we investigate the question of whether pre-trained Vision-Language Models (VLMs) can encodeprimitive visual concepts. We first illustrate that category-biased concepts extracted from specific datasetsdo not offer conclusive evidence of the concept learning capacity of VLMs. To resolve this issue, we designa novel framework to discover category-agnostic and visually discriminative concepts from a large image-caption dataset with the help of VLMs and LLMs. To make use of the discovered concepts for downstreamtasks, we propose a novel method to build concept bottlenecks with a compact and performant set of conceptsfor a specific domain. We also propose a self-supervised concept learning framework to re-align the conceptknowledge in VLMs for the category classification in specific domains. To prove that VLMs do learn usefuland interpretable concepts, we propose a suite of comprehensive protocols to evaluate the quality of thediscovered concepts and perform exhaustive experiments including human evaluation. The experimentalresults demonstrate that VLMs do capture primitive concepts that can lead to effective, interpretable, andgeneralizable visual recognition.",
  ": Examples of the top weighted concepts correlated with the given images": "While we illustrate that VLMs do learn discoverable concepts, it is still significant to understand what kind ofconcept and compositionality knowledge cannot be learned in the contrastive learning-based pre-training ofVLMs. In future work, we plan to explore whether VLMs can capture the semantic and spatial relationshipsbetween concepts and utilize these relationships to perform complex multi-modal reasoning.",
  "Acknowledgement": "This work is in part supported by a gift from Adobe Research, a seed grant from NASA, and a Richard B.Salomon award for Chen Sun. We thank helpful discussions with Professors Ellie Pavlick and Stephen Bach.We thank Yue Yang, An Yang, Yu Wang for providing the open-sourced code for the baselines. Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc,Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model forfew-shot learning. Advances in Neural Information Processing Systems, 35:2371623736, 2022. Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, andJingren Zhou. Qwen-vl: A versatile vision-language model for understanding, localization, text reading,and beyond. arXiv preprint arXiv:2308.12966, 3, 2023. David Bau, Bolei Zhou, Aditya Khosla, Aude Oliva, and Antonio Torralba. Network dissection: Quantifyinginterpretability of deep visual representations. In Proceedings of the IEEE conference on computer visionand pattern recognition, pp. 65416549, 2017. Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool.Food-101mining discriminative componentswith random forests. In Computer VisionECCV 2014: 13th European Conference, Zurich, Switzerland,September 6-12, 2014, Proceedings, Part VI 13, pp. 446461. Springer, 2014. Kushal Chauhan, Rishabh Tiwari, Jan Freyberg, Pradeep Shenoy, and Krishnamurthy Dvijotham. Inter-active concept bottleneck models. In Proceedings of the AAAI Conference on Artificial Intelligence, pp.59485955, 2023. Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastivelearning of visual representations. In International conference on machine learning, pp. 15971607. PMLR,2020.",
  "IEEE conference on computer vision and pattern recognition, pp. 17781785. IEEE, 2009": "Evan Hernandez, Sarah Schwettmann, David Bau, Teona Bagashvili, Antonio Torralba, and Jacob An-dreas. Natural language descriptions of deep visual features. In International Conference on LearningRepresentations, 2021. Cheng-Yu Hsieh, Jieyu Zhang, Zixian Ma, Aniruddha Kembhavi, and Ranjay Krishna. Sugarcrepe: Fixinghackable benchmarks for vision-language compositionality. Advances in Neural Information ProcessingSystems, 36, 2024.",
  "arXiv preprint arXiv:2405.15476, 2024": "Qiyang Hu, Attila Szab, Tiziano Portenier, Paolo Favaro, and Matthias Zwicker. Disentangling factorsof variation by mixing them. In Proceedings of the IEEE Conference on Computer Vision and PatternRecognition, pp. 33993407, 2018. Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung,Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy textsupervision. In International conference on machine learning, pp. 49044916. PMLR, 2021. Been Kim, Martin Wattenberg, Justin Gilmer, Carrie Cai, James Wexler, Fernanda Viegas, et al. Inter-pretability beyond feature attribution: Quantitative testing with concept activation vectors (tcav). InInternational conference on machine learning, pp. 26682677. PMLR, 2018. Pang Wei Koh, Thao Nguyen, Yew Siang Tang, Stephen Mussmann, Emma Pierson, Been Kim, and PercyLiang. Concept bottleneck models. In International Conference on Machine Learning, pp. 53385348.PMLR, 2020. Jonathan Krause, Hailin Jin, Jianchao Yang, and Li Fei-Fei. Fine-grained recognition without part annota-tions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 55465555,2015.",
  "Martha Lewis, Qinan Yu, Jack Merullo, and Ellie Pavlick. Does clip bind concepts? probing compositionalityin large image models. arXiv preprint arXiv:2212.10537, 2022": "Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: bootstrapping language-image pre-trainingwith frozen image encoders and large language models. In Proceedings of the 40th International Conferenceon Machine Learning, ICML. JMLR.org, 2023. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollr, andC Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer VisionECCV 2014: 13thEuropean Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pp. 740755.Springer, 2014.",
  "Eleventh International Conference on Learning Representations, 2022": "Matthias Minderer, Alexey Gritsenko, Austin Stone, Maxim Neumann, Dirk Weissenborn, Alexey Doso-vitskiy, Aravindh Mahendran, Anurag Arnab, Mostafa Dehghani, Zhuoran Shen, et al.Simple open-vocabulary object detection. In European Conference on Computer Vision, pp. 728755. Springer, 2022. Tushar Nagarajan and Kristen Grauman. Attributes as operators: factorizing unseen attribute-object com-positions. In Proceedings of the European Conference on Computer Vision (ECCV), pp. 169185, 2018. Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large number of classes.In 2008 Sixth Indian Conference on Computer Vision, Graphics & Image Processing, pp. 722729. IEEE,2008.",
  "conference on computer vision and pattern recognition, pp. 34983505. IEEE, 2012": "Sarah Pratt, Ian Covert, Rosanne Liu, and Ali Farhadi.What does a platypus look like?generatingcustomized prompts for zero-shot image classification. In Proceedings of the IEEE/CVF InternationalConference on Computer Vision, pp. 1569115701, 2023. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, GirishSastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models fromnatural language supervision. In International conference on machine learning, pp. 87488763. PMLR,2021. Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned, hyper-nymed, image alt-text dataset for automatic image captioning. In Proceedings of the 56th Annual Meetingof the Association for Computational Linguistics (Volume 1: Long Papers), pp. 25562565, 2018.",
  "Yutaro Yamada, Yingtian Tang, and Ilker Yildirim. When are lemons purple? the concept association biasof clip. arXiv preprint arXiv:2212.12043, 2022": "An Yan, Yu Wang, Yiwu Zhong, Chengyu Dong, Zexue He, Yujie Lu, William Yang Wang, Jingbo Shang,and Julian McAuley. Learning concise and descriptive attributes for visual recognition. In 2023 IEEE/CVFInternational Conference on Computer Vision (ICCV), pp. 30673077. IEEE Computer Society, 2023a. An Yan, Yu Wang, Yiwu Zhong, Zexue He, Petros Karypis, Zihan Wang, Chengyu Dong, Amilcare Gentili,Chun-Nan Hsu, Jingbo Shang, et al.Robust and interpretable medical image classifiers via conceptbottleneck models. arXiv preprint arXiv:2310.03182, 2023b. Antoine Yang, Arsha Nagrani, Paul Hongsuck Seo, Antoine Miech, Jordi Pont-Tuset, Ivan Laptev, JosefSivic, and Cordelia Schmid. Vid2seq: Large-scale pretraining of a visual language model for dense videocaptioning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,pp. 1071410726, 2023a. Yue Yang, Artemis Panagopoulou, Shenghao Zhou, Daniel Jin, Chris Callison-Burch, and Mark Yatskar.Language in a bottle: Language model guided concept bottlenecks for interpretable image classification. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1918719197,2023b. Lewei Yao, Runhui Huang, Lu Hou, Guansong Lu, Minzhe Niu, Hang Xu, Xiaodan Liang, Zhenguo Li,Xin Jiang, and Chunjing Xu. Filip: Fine-grained interactive language-image pre-training. arXiv preprintarXiv:2111.07783, 2021.",
  "AHyperparameters": "We adjust hyperparameters according to the performance of the models on the validation dataset (ForImageNet, we randomly select 10% of the training set as the validation set). For in Equation 2, we setit to 0.7 for ImageNet dataset, 0.8 for Food-101, CIFAR-100, CUB-200 and Flowers-102 datasets and 0.9for CIFAR-10 dataset. According to Equation 2 in .4, a smaller will increase the generalizabilitybut decrease the discriminativeness of the selected concepts, and thus decrease classification performance.To achieve a trade-off between generalizability of the selected concepts and classification performance, wemonitor the classification performance when gradually decreasing . We pick the right before classificationperformance drops significantly.",
  "B.1Dependency Parsing Rules": "Given a caption, we perform Dependency Parsing to extract the grammatical relations from the caption. Weextract the nouns or phrases from the following dependencies as potential objects in an image caption: nsubj / nsubjpass: The nsubj (nominal subject) is the noun that performs the main action. Forexample, in the sentence the horse is eating grass, horse is the nominal subject. The nsubjpass(passive nominal subject) is the noun that performs the main action in a sentence of passive voice.For example, in the sentence The dog is led by a leash, dog is the passive nominal subject. Weextract the nominal and passive nominal subject as a word potentially corresponding to an objectin the image. dobj / iobj: The dobj (direct object) is the noun or noun phrase that receives the action of theverb. For example, in the sentence the horse is eating grass, grass is the direct object. The iobj(indirect object) is the noun or noun phrase that indicates to or for whom the action of the verb isdone. For example, in the sentence The man gives the girl a flower, girl is the indirect object.We extract the direct and indirect object as a word potentially corresponding to an object in theimage. amod: An amod (adjectival modifier) is an adjective that describes a noun (e.g. black dog, whitebird). We extract the amod and its object as a phrase potentially corresponding to an object inthe image. compound: The compound label indicates the word is part of a compound phrase like king penguin.Once select a word following above rules, we check whether it is part of a compound phrase. If so,we extract the whole phrase as the object.",
  "Px(x)Py(y).(3)": "The MI is high when X and Y are positively or negatively related (e.g. high x value indicates high y valueand low y value indicates low y value or vice versa). Given a concept and an image-caption dataset, the Xvariable corresponds to the image-concept similarity as measured by a VLM, and the Y variable is a binaryindicator on the caption-concept correspondence according to an LLM. Namely, a higher x indicates thatVLM can recognize the concept from the image well, and a higher y indicates the concept is deemed by LLMas a relevant attribute for the object described in the caption. As such, we prefer concepts with higher MI",
  "DHuman Evaluation Details": "We hire workers on to conduct human evaluation. To make our human evaluationmore robust, for one data point we ask three human workers to annotate. For the precision and thoroughnessmetric, we randomly select 100 images on each dataset to evaluate. For CIFAR-10, CIFAR-100, Food-101,Flowers-102 we choose the top 3 concepts to annotate.For ImageNet and CUB-200 we choose the top5 concepts to annotate.We annotate 2,200 data points for precision and around 3,000 data points forthoroughness. For the visual discriminability and category name containing, we utilize different methods toselect 100 concepts for each dataset to evaluate and report the average score. Hence we annotate 1,800 datapoints for those two task. In the annotation, we randomly shuffle the order of instances to remove possiblebiases. In order to validate the effectiveness of our human evaluation, we calculate the pairwise annotator agreementscore following LaBo. The average pairwise annotator agreement proportion on all datasets is 69.4%, whichis comparable to the 69.8% proportion in LaBo. We show some examples of our human evaluation interface in Figure A2.The examples shown are formeasuring precision of the concepts, and a similar interface is used to measure thoroughness, where thehumans are asked to build a complete concept list for an image.",
  "MI Score: 0.02": "Figure A1: Illustrations of Mutual Information calculation. We can observe that visually discriminativeconcepts such as white belly have high MI score because they can be reliably recognized from imagesthat are supposed to contain the concept according to LLM, and vice versa. The non-visual concepts likefish-eating and endangered animal have low MI scores because VLMs cannot recognize these conceptsfrom the images that are supposed to contain these concepts according to LLM. previous works. When p-value is lower than 0.05, the null hypothesis is rejected and out method performssignificantly better than the baseline method. From the results in Table A2 we can observe that both our",
  "FAblation Study on Effectiveness of CDL Framework": "In this section, we conduct ablation studies on the effect of different phases of our CDL framework. Wecompare the classification performance and interpretability of the concept-based image-recognition beforeand after concept learning to illustrate the effects of concept discovery and concept learning. The results in Table A4, A5 and A6 indicate that (1) it is the concept discovery that mainly contribute to theimprovement of classification performance and the concept learning would not affect the classification accu-racy; (2) both concept discovery and learning parts provide significant improvement for the interpretabilityof the concepts according to the intervention accuracy resutls.",
  "Accuracy": "Cross-domain Generalization LaBoCDL Figure A3: Few-shot classification performance of in-domain and cross-domain generalization, where denotes the improvement of fine-tuned CLIP compared to the original CLIP. The comparison between ourconcepts and LaBo concepts suggest that our concepts can provide better generalization and lead to betterperformance for few-shot recognition on different categories and domains.",
  "GFew-shot Performance about Generalization": "To further measure the in-domain and cross-domain generalization of the discovered concepts, we performfew-shot classification with the fine-tuned CLIP model on the unseen categories.The results in FigureA3 show that our discovered concepts can enable much better few-shot classification performance, whichsuggests that the encoded knowledge of our discovered concepts are more generalizable to unseen categoriesand domains.",
  "HBroader Application of Visual Concepts": "Besides image classification tasks, the discovered and learned visual concepts can also benefit tasks thatrequire object localization by localizing visual concepts in images. We integrate the visual concepts into theexisting object detection framework OWL proposed by Minderer et al. (2022) and conduct evaluations on theMS-COCO dataset (Lin et al., 2014). We first perform concept discovery and learning on the training data,and then replace the category names (e.g. dog) with selected visual concepts (e.g. four-legged mammaland \"floppy ears\") to train and evaluate the object detection model. The results are shown in Table A7. Thevisual concepts can enhance the object detection performance of the existing framework."
}