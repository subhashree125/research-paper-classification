{
  "Abstract": "Deep generative models (DGMs) have demonstrated great success across various domains,particularly in generating texts and images using models trained from offline data. Similarly,data-driven decision-making also necessitates learning a generator function from the offlinedata to serve as the policy. Applying DGMs in offline policy learning exhibits great potential,and numerous studies have explored in this direction. However, this field still lacks a compre-hensive review and so developments of different branches are relatively independent. In thispaper, we provide the first systematic review on the applications of DGMs for offline policylearning. We cover five mainstream DGMs, including Variational Auto-Encoders, Gener-ative Adversarial Networks, Normalizing Flows, Transformers, and Diffusion Models, andtheir applications in both offline reinforcement learning (offline RL) and imitation learning(IL). Offline RL and IL are two main branches of offline policy learning and are widely-adopted techniques for sequential decision-making. Notably, for each type of DGM-basedoffline policy learning, we distill its fundamental scheme, categorize related works based onthe usage of the DGM, and sort out the development process of algorithms in that field. Inaddition, we provide in-depth discussions on DGMs and offline policy learning as a summary,based on which we present our perspectives on future research directions. This work offersa hands-on reference for the research progress in DGMs for offline policy learning, and aimsto inspire improved DGM-based offline RL or IL algorithms. For convenience, we maintaina paper list on",
  "Introduction": "Offline Policy Learning is a machine learning discipline that leverages pre-existing, static datasets to learneffective policies for sequential decision-making or control.This paper covers its two primary branches:Offline Reinforcement Learning (Offline RL) and Imitation Learning (IL). Offline RL utilizes a pre-compiledbatch of experience data collected from other policies or human operators, typically comprising a series ofstate-action-reward-next state tuples. The objective of offline RL is to develop a policy that can maximizethe expected cumulative rewards, which may necessitate deviating from the behavior patterns observed inthe training data. On the other hand, IL trains a policy by mimicking an experts behaviors. The data usedfor IL should be demonstrated trajectories from experts. These trajectories, containing experts responsesto various scenarios, usually consist of a sequence of state-action pairs (Learning from Demonstrations, LfD)or state-next state pairs (Learning from Observations, LfO). Generative models are a class of statistical models that are capable of generating new data instances. Theylearn the underlying distribution of a dataset and can generate new data points with similar characteristics.Deep generative models (DGMs) are a subset of generative models that leverage the powerful representationalcapabilities of deep neural networks to model complex distributions and generate high-quality, realisticsamples.Notably, the application of DGMs in computer vision (CV) and natural language processing(NLP) has been highly successful. Examples include text-to-image generation using Diffusion Models (StableDiffusion (Rombach et al. (2022))), text-to-video generation with Diffusion Models and Transformers (Sora(Brooks et al. (2024))), large language models based on Transformers (ChatGPT (OpenAI (2023))), and soon. Offline policy learning bears similarities to CV and NLP, as all these domains involve learning from a setof offline data. While CV or NLP models generate images or texts based on given contexts, offline RL/ILmodels produce actions or trajectories conditioned on task scenarios. This similarity suggests that applyingDGMs to offline policy learning could potentially replicate the success witnessed in CV and NLP, leveragingtheir capability to model and generate complex data patterns. Great advances have been made in both DGMs and offline policy learning, and there are numerous workson applying state-of-the-art DGMs to aid the development of offline policy learning. This paper providesa comprehensive review of DGM-based offline policy learning. Specifically, the main content is categorizedby the type of DGMs, and we cover nearly all mainstream DGMs in this paper, including Variational Auto-Encoders, Generative Adversarial Networks, Normalizing Flows, Transformers, and Diffusion Models, whereTransformers and Diffusion Models are representatives of autoregressive and score-based generative models,respectively. For each category, we present related works in both offline RL and IL as two subsections. Ineach subsection (e.g., VAE-based IL in .3), we abstract the core schemes of applying that DGMin IL or offline RL, categorize related works by how the DGM is utilized, and provide a summary tableoutlining the representative works with their key novelties and evaluation benchmarks. It is noteworthy thatour paper is more than a survey: (1) For the introduction of each work, we include its key insights andobjective design, to help readers get the knowledge without referring to the original papers. (2) For eachgroup of works (e.g., .2.4 and 7.3.2), algorithms are introduced in different levels of details. The mostrepresentative ones are highlighted as a tutorial on that specific usage of DGMs for offline policy learning,while subsequent extension works are discussed more briefly to provide a comprehensive overview. (3) Wepresent research works within the same group using unified notations, aiming to clarify the relationshipsbetween their objectives and the evolution of algorithm designs. The main content begins with background on offline policy learning, highlighting existing challenges thatcan be addressed by DGMs. Notably, different DGMs could adopt distinct base offline RL/IL algorithms.For instance, three main branches of offline RL: dynamic-programming-based, model-based, and trajectory-optimization-based offline RL, are introduced and they are mainly used in .1.1, 2.1.2, 6.2.1, re-spectively. Then, in the order of Variational Auto-Encoders, Generative Adversarial Networks, NormalizingFlows, Transformers, and Diffusion Models, we present the background on each DGM and its applicationfor offline policy learning, in - 7. For the background of each DGM, we introduce its mathematicalbasics and specific model variants that are utilized in offline policy learning, such that necessary knowledgecan be conveniently looked up. Following the main content, in , we present in-depth discussions onthe main topic of this paper and provide perspectives on future research directions. For discussions (Section",
  "Published in Transactions on Machine Learning Research (08/2024)": "Ling Yang, Zhilong Zhang, Yang Song, Shenda Hong, Runsheng Xu, Yue Zhao, Yingxia Shao, Wentao Zhang,Ming-Hsuan Yang, and Bin Cui. Diffusion models: A comprehensive survey of methods and applications.CoRR, abs/2209.00796, 2022a. Rui Yang, Lin Yong, Xiaoteng Ma, Hao Hu, Chongjie Zhang, and Tong Zhang. What is essential for unseengoal generalization of offline goal-conditioned rl? In Proceedings of the 40th International Conference onMachine Learning, volume 202, pp. 3954339571, 2023b. Shentao Yang, Yihao Feng, Shujian Zhang, and Mingyuan Zhou. Regularizing a model-based policy sta-tionary distribution to stabilize offline reinforcement learning. In Proceedings of the 39th InternationalConference on Machine Learning, volume 162, pp. 2498025006, 2022b.",
  "Background on Offline Policy Learning": "In this section, we introduce two main branches of offline policy learning: offline reinforcement learning (RL)and imitation learning (IL). The purpose of this part is not to thoroughly review these two areas but toprovide necessary background for the applications of DGMs in offline policy learning. At the end of thissection, we discuss challenges in offline policy learning that can potentially be addressed by DGMs. All content is grounded in the fundamental framework of Markov Decision Process (Puterman (2014)),denoted as M = (S, A, T , r, 0, ). S is the state space, A is the action space, T : S A S isthe transition function, 0 : S is the distribution of the initial state, r : S A R is the rewardfunction, and is the discount factor.",
  "k+1 = arg maxEsDEa(|s)Qk+1(s, a)s.t. EsD [D((|s), (|s))] .(1)": "Here, the first and second equations are referred to as the policy evaluation and policy improvement steps,respectively. When updating the Q function, (s, a, r, s) are sampled from D but the target action a issampled by the being-learned policy k. If k(a|s) differs substantially from (a|s), out-of-distribution(OOD) actions, which have not been explored by , can be sampled. Further, the Q-function trained onD, i.e., Qk+1, may erroneously produce over-optimistic values for these OOD actions, leading the policyk+1 to generate unpredictable OOD behaviors. In online RL, such issues are naturally corrected whenthe agent interacts with the environment, attempting the actions it (erroneously) believes to be good andobserving that in fact they are not. In offline RL, interactions with the environment are not accessible, but,alternatively, the over-optimism can be controlled by limiting the discrepancy between and , as shown inEq. (1). The discrepancy measure D(||) has multiple candidates. For a comprehensive review, please referto (Levine et al. (2020)). Built upon this basic paradigm (i.e., Eq. (1)), we introduce some practical andrepresentative offline RL algorithms that focus on addressing the issue of OOD actions.",
  "k+1(a|s) = (a|s) expQk+1(s, a)//Z(s)(2)": "In this expression, 1/Z(s) serves as the normalization factor across all possible action choices at state s; isthe Lagrangian multiplier to convert the original constrained optimization problem to an unconstrained one,typically set as a constant rather than being optimized. In practice, such an policy can be acquired throughweighted supervised learning from D (Wang et al. (2020)), where expQk+1(s, a)serves as the weight for(s, a), that is:k+1 = arg maxE(s,a)Dlog (a|s) expQk+1(s, a)//Z(s)(3) Policy penalty methods, such as BRAC (Wu et al. (2019a)), utilize approximated KL-divergenceDKL((|s)||(|s)) = Ea(|s) [log (a|s) log (a|s)], where is learned via Behavioral Cloning (intro-duced in .2.1) from D to estimate and E denotes the estimated expectation via Monte Carlosampling, and modify the reward function as r(s, a) = r(s, a) DKL((|s)||(|s)) ( > 0). In this way,the deviation from is implemented as a penalty term in the reward function for the policy learning to avoiddeviating from not just in the current step but also in future steps. This method is usually implementedin its equivalent form (Wu et al. (2019a)) as below:",
  "k+1 = arg maxEsDEa(|s)Qk+1(s, a) DKL((|s)||(|s)).(4)": "One significant disadvantage of this approach is that it requires explicit estimation of the behavior policy, i.e.,, and the estimation error could hurt the overall performance of this approach. Instead, a recent work (Park et al. (2024)) proposes that EsD DKL((|s)||(|s))can be replaced by E(s,a)D [ log (a|s)], whichimplicitly constraints to remain close to the behavior policy. They further show through intensive empiricalresults that this objective design consistently outperforms aforementioned policy constraint methods (e.g.,AWR). Notably, the application of Diffusion Models in offline RL (i.e., .3.1) predominantly adheresto these policy constraint methods, suggesting potential future improvement. Support constraint methods, such as BCQ (Fujimoto et al. (2019)) and BEAR (Kumar et al. (2019)),propose to confine the support of the learned policy within that of the behavior policy to avoid OODactions, because constraining the learned policy to remain close in distribution to the behavior policy, asin the previous two methods, can negatively impact the policy performance, especially when the behaviorpolicy is substantially suboptimal. As an example, BEAR proposes to replace the discrepancy constraint inEq. (1) with a support constraint: { : S A | (a|s) = 0 whenever (a|s) < } 1. Withsuch a support constraint, the target actions a used in policy evaluation (i.e., the first equation in Eq. (1)),would all satisfy (a|s) and so are in-distribution actions, that is:",
  "maxTE(s,a,s)Dlog T (s|s, a), minrE(s,a,r)D(r(s, a) r)2(7)": "The approximated MDP M = (S, A, T , r, 0, ), where 0 is the empirical distribution of initial states inD, can then be used as proxies of the real environment, and (short-horizon) trajectories can be collectedby interacting with M using the being-learned policy , to form another dataset D. To mitigate the impact from the model bias of M, an estimation of the model uncertainty, i.e., u(s, a), isutilized as a penalty term to discourage the agent to visit uncertain regions (where u is high) in the state-action space, by setting the reward at (s, a) as r(s, a) = r(s, a)ru(s, a). (r > 0 is a hyperparameter.) Thisconservative way to avoid OOD behaviors resembles the policy penalty method in dynamic-programming-based offline RL. The uncertainty measure can be acquired through Bayesian Neural Networks (Chua et al.(2018)), or modeled as disagreement among estimations from an ensemble of Q-functions or approximatedenvironment models (Lu et al. (2022b)). Specifically, D can be generated as follows: s0 0(), a0 (|s0), r0 = r(s0, a0)ru(s0, a0), s1 T (|s0, a0), and so on. Finally, offline/off-policy RL algorithms canbe applied to train the policy on an augmented dataset Daug = DD + (1 D)D. Here, D is another hyperparameter. A typical (off-policy) actor-critic framework for this process is as below (Levineet al. (2020)): (Q is the target Q-function.)",
  "|D(s)|, where |D(s)|": "denotes the frequency of state s in D. Given that Ds coverage could be limited and certain states would have low visitationfrequencies, this can result in a high value of . However, as shown in Eq. (6), this same value is used in the Q-update forevery state, potentially leading to underestimation for other states in D.",
  "Trajectory-Optimization-based Offline Reinforcement Learning": "Trajectory optimization casts offline RL as sequence modelling (Prudencio et al. (2023)). One approachin this category is to model the distribution of augmented trajectories {0:T = (s0, a0, R0, , sT , aT , RT )}induced by the behavior policy , where Rt = Ti=t ri denotes the return-to-go (RTG). Each trajectory 0:T ,instead of being treated as a single data point, can be modeled/generated using autoregressive models likeTransformers, and the objective may take the form:",
  "t=1log (t|0:t1), log (t|0:t1) = log 1(st, Rt|0:t1) + log 2(at|0:t1, st, Rt)(10)": "The implementation of 1 and 2 can vary in different algorithms. After training, 2 can be used as the policyto generate trajectories of a target return R0 in an autoregressive manner: s0 0(), a0 2(|s0, R0),s1 T (|s0, a0), r0 = r(s0, a0), R1 = R0 r0, . The rationale behind this is that by learning from adistribution of return and trajectory pairs, the RTG-based policy 2 is expected to induce trajectories thatcorrespond to the conditioned RTG. Such a sequence modelling objective makes trajectory-optimization-based offline RL less prone to selectingOOD actions, because multiple state and action anchors throughout the trajectory prevent the learnedpolicy from deviating too far from the behavior policy . As shown above, the training is simply throughsupervised learning, which gets rid of the difficulty of dynamic programming or policy gradient optimizationas in previous two categories of offline RL. Also, trajectory-optimization-based offline RL performs well insparse reward settings, where temporal-difference methods typically fail, since they rely on dense rewardestimates to effectively propagate Q-values over long horizons. Among the five DGMs that we cover inthis paper, only Transformers belong to autoregressive models. Thus, we introduce representative works ofautoregressive trajectory modelling and their extensions in .2. We also provide detailed discussionson the fundamental limitations of these methods in .2.For instance, to recover near optimalpolicies with these methods, environment dynamics (i.e., T and r in M) should be nearly deterministicand the offline dataset D should provide good coverage of all possible return-trajectory pairs.Theseassumptions are somewhat strong compared to those needed for dynamic-programming-based approaches(Brandfonbrener et al. (2022)). Alternatively, due to the capability of Diffusion Models to handle high-dimensional data, they have beenutilized to directly model/generate complete trajectories {0:T = (s0, a0, , sT , aT )} as data points. Duringexecution, the target return R0 can serve as a conditioner to guide the trajectory generation. This approachrelies heavily on a deep understanding of Diffusion Models, and as such, the details are elaborated in thecorresponding .3.2. Unlike the previous two categories of offline RL, trajectory-optimization-basedoffline RL significantly depends on the capabilities of DGMs, such as Transformers and Diffusion Models,establishing a closer connection with them.",
  "Despite its simplicity, BC has several significant drawbacks": "First, the BC agent is trained on states generated by the expert policy but tested on states induced by itsown action. As a result, the distribution of states observed during testing can differ from that observedduring training, and the agent could encounter unseen states, which is known as the covariate shift problem(Pomerleau (1988)). BC agents do not have the ability to return to demonstrated regions when it encountersout-of-distribution (OOD) states, so deviations from the demonstrated behavior tend to accumulate over thedecision horizon, causing compounding errors. Here are some strategies for addressing this issue. InteractiveIL, such as DAgger (Ross et al. (2011)), assumes access to an online expert which can relabel the datacollected by the BC agent with correct actions, such that the agent can learn how to recover from mistakes.Alternatively, when the simulator is available, the BC agent can be trained to remain on the support ofdemonstrations through an additional RL process. For instance, Brantley et al. (2020) propose trainingan ensemble of policies, using the variance of their action predictions as a cost. The variance outside ofthe experts support would be higher, because the ensemble policies are more likely to disagree on statesnot encountered in the demonstrations. An RL algorithm can then be applied to minimize this cost, inconjunction with supervised learning using BC. The RL process helps guide the agent back to the expertdistribution, while BC ensures that the agent closely mimics the expert within that distribution. Second, it is difficult for a supervised learning approach to identify the underlying causes of expert actions,leading to a problem known as causal misidentification (De Haan et al. (2019)). Consequently, the BC agentmay fail to differentiate between nuisance correlates in the states and the actual causes of expert actions,making it challenging for the learned policy to generalize to OOD states. Strategies addressing this issueeither incorporate the causal structure of the interaction between the expert and environment into policylearning, or identify and eliminate nuisance correlates in observations. We have seen a series of studies thatapply VAEs to tackle causal misidentification, as detailed in .3.5. Notably, Florence et al. (2021) investigate a fundamental design decision of BC that has largely been over-looked: the form of the policy. Empirically, they show that using explicit continuous feed-forward networksas policies, i.e., a = (s), struggles to model discontinuities, making policies incapable of switching deci-sively between different behaviors. Instead, they propose Implicit BC, where an energy model E is utilizedto represent the policy . The policy definition and training method are as below:",
  "(12)": "{anegi}Ni=1 are generated negative examples (i.e., non-expert actions) corresponding to (s, a).Such anInfoNCE-style objective (van den Oord et al. (2018)) is to approximately maximize the expectation oflogeE(s,a)/ a eE(s,a). As a result, the expert action a would correspond to the lowest energy E(s, a).During inference, arg mina E(s, a) can be acquired through techniques like stochastic optimization, whichbrings higher computation cost but significantly better empirical performance in both simulated and real-world tasks. We have seen a lot of works that apply DGMs as the policy model to improve the policyexpressiveness or handle complex input/output data, which are introduced in later sections, but whethersuch an implicit design could improve DGM-based policies remains to be explored.",
  "Challenges of Offline Policy Learning": "In this section, we summarize the significant challenges faced by offline RL and IL, providing guidance forthe application of DGMs in these areas. Dynamic-programming-based offline RL relies on accurateestimations of some key quantities, such as the approximated behavior policy in policy penalty methodsand the expert support in support constraint methods, to avoid OOD behaviors. Achieving these accu-rate estimations poses a significant challenge. On the other hand, the pessimistic value method addressesthe overestimation issue common in other dynamic programming methods but can suffer from excessivepessimism, which may limit improvements beyond the behavior policy. Therefore, maintaining a balancebetween conservatism and optimism is crucial. Model-based offline RL necessitates an accurate worldmodel M and explicit uncertainty estimation. Modeling MDPs for environments characterized by complexdynamics, high-dimensional state spaces, and long decision horizons is particularly challenging. Further,uncertainty estimation, which serves to quantify the distributional shift, can significantly affect the perfor-mance of model-based offline RL. Trajectory-optimization-based offline RL requires a large amountof training data that adequately covers possible return-trajectory pairs. Additionally, it remains uncertain",
  "whether conditioning on the return-to-go ensures that a generator would produce trajectories achieving thatspecified return, highlighting some fundamental limitations of these methods": "BC can be improved by employing more expressive/advanced policy models or by addressing its inherentissues, such as covariant shift and causal misidentification.Methods that effectively mitigate covariantshift or compounding errors, without requiring online experts or simulators (i.e., relying solely on offlinedata), could be particularly beneficial. IRL is a more demanding technique as it seeks to infer not onlythe expert policy but also the underlying reward function. Currently, there is no algorithm that perfectlybalances computational efficiency with theoretical soundness in reward recovery. GAN-based IRL, such asGAIL, often encounters challenges related to training stability and sample complexity. Most IRL algorithmsdepend on simulators to train policies through RL. However, this survey focuses on algorithms that operateexclusively with offline data, and as such, we do not provide an extensive review of applying DGMs to IRL. From a data perspective, a significant challenge in offline RL is effectively utilizing unlabeled data, whichdoes not contain reward labels and is more readily available. As an example, some VAE-based unsupervisedskill discovery algorithms, such as (Eysenbach et al. (2019b); Ajay et al. (2021); Chen et al. (2023c)),attempt to extract skills from unlabeled data, which can then serve as policy segments for downstream RL.Similarly, a combination use of large amount of unlabeled data with a limited amount of high-quality labeleddata represents a challenging yet promising direction for data-efficient offline RL. As for IL, learning fromimperfect demonstrations can be a fruitful research direction. Given the difficulty in obtaining large numbersof optimal demonstrations from human experts, an IL agent may have to learn from demonstrations thatvary in levels of optimality to ensure sufficient training data. As examples, there are two extensions ofGAIL for learning from imperfect demonstrations (Wu et al. (2019b); Wang et al. (2021b)). They proposemechanisms to automatically weight demonstrations based on their quality and significance for training, anddevelop algorithms for learning from the weighted demonstrations. Finally, several extended setups for offline policy learning can be developed, including safety-critical, multi-agent, multi-task, and hierarchical settings. The safety-critical setting involves numerous safety constraintsthat the learned policy must adhere to. Multi-task learning aims to develop a policy that can either bedirectly applied to a variety of tasks or quickly adapted to them. Hierarchical learning usually requiresconstructing a two-level policy, where the lower level consists of skills specific to subtasks and the highlevel manages the transition between these skills. This structure is particularly beneficial for complex, long-horizon tasks that can be decomposed into a sequence of subtasks. Throughout - 7, we introduce aseries of works that apply DGMs to these extended setups. Additionally, for IL, learning from observations(LfO) is a more challenging setup, where the agent only has access to trajectories of expert states withoutknowledge of expert actions. Algorithms for this setup enable challenging tasks such as learning from videosand learning from agents with different embodiments (who share the same state space but have distinctaction sets). GAIfO (Torabi et al. (2018)) adapts the GAIL objective for LfO by matching the expert andagents state-transition distributions instead, and there are several VAE-based algorithms designed for LfOas detailed in .3. However, the overall research applying DGMs to LfO remains sparse, suggestingfurther exploration in this direction. In .1, we provide an in-depth discussion on offline policy learning and deep generative models(DGMs).In particular, we outline the base IL/offline RL frameworks to which each type of DGM hasbeen applied in , and the issues or extensions that each DGM has tried to resolve in ,corresponding to the challenges introduced in this section.We encourage readers interested in solvingspecific problems of offline policy learning to look up these tables and refer directly to the correspondingsections.",
  "Variational Auto-Encoders in Offline Policy Learning": "This section begins with background on Variational Auto-Encoders (VAEs), where we introduce their math-ematical foundations and model variants with a particular focus on those employed in offline policy learning.Subsequently, we present a comprehensive overview of the applications of VAEs in offline RL (.2)and IL (.3) as two subsections. Within each subsection, we categorize related research works basedon how VAEs are utilized and summarize the algorithm design paradigm as a tutorial. At the end of each",
  "subsection, we provide a table to show the representative algorithms with their key novelties and evaluationtasks, serving as a reference for future research in algorithm design and evaluation": "The sections focusing on other generative models, i.e., - 7, follow a similar structure with thissection. Considering the extensive scope of content addressed in this work, notations utilized for differentgenerative models are relatively independent. However, within the context of the same generative model,including its background and applications in offline policy learning, notations are consistent and can becross-referenced. Consistently across these sections, we use x PX() to represent data points in the offlinedataset and z PZ() to represent their latent representations.",
  "Background on Variational Auto-Encoders": "Variational Auto-Encoders (VAEs, Kingma & Welling (2014; 2019)) assume that the given data distributionPX(x) can be generated with a deep latent-variable model P(x) =P(z)P(x|z)dz, i.e., sampling acontinuous latent variable z from the prior P(z) and then generating x from the conditional (generative)model P(x|z). Its also assumed that P(z) (P(x|z)) comes from a parametric family of distributionsP(z) (P(x|z)). Intuitively, such P(x) can be seen as an infinite mixture of some base probability distri-butions P(x|z) (e.g., the Gaussian distribution) across a range of possible latent variables z, thus it can bequite flexible and powerful for modelling complex data distributions PX(x). To learn the mapping from z to x, an extra model that can infer the latent variables z from the trainingdata x is required, as zs are not provided with the offline data. However, according to the Bayes rule, thetrue posterior (inference) model P(z|x) = P(z)P(x|z)/P(x) is intractable in most cases. For example,if P(x|z) is implemented as a neural network, we do not have the analytical form for P(x) and so notfor P(z|x). Therefore, in VAEs, a generative model P(x|z) and corresponding inference model P(z|x)(for approximating the true posterior) are jointly learned. The most notable contributions of VAEs aretwo-fold: firstly, they propose a variational lower bound as a practical learning objective, and secondly,they introduce the reparameterization trick, which facilitates end-to-end training of both the generative andinference models. The objective function of VAEs is a variational lower bound (a.k.a., evidence lower bound (ELBO)) ofthe log likelihood log P(x), and its derivation process is widely adopted in algorithm designs related tolatent-variable models, so we show the process here:",
  "= EzP(|x) [log P(x|z)] DKL(P(z|x)||P(z))(14)": "The inequality in Eq. (13) uses the fact that KL divergence (i.e., DKL(||)) is non-negative (Csiszr & Shields(2004)). P and P are learned by maximizing ExPX()L,(x), which in turns maximizes ExPX() log P(x).When using neural networks for both the inference model P(z|x) and generative model P(x|z), L,(x)trains a specific type of auto-encoder, where P(z|x) and P(x|z) function as the encoder and decoderrespectively. Intuitively, this auto-encoder is trained by maximizing the data reconstruction accuracy (i.e.,the first term in L,(x)), while regularizing the distribution of z from the encoder to be close to the priordistribution of z. This regularization is necessary, since z is sampled from the encoder for data generation(i.e., z P(|x), x P(|z)) when training but, during evaluation, z is sampled from the prior distribution(i.e., z P(), x P(|z)). Usually, VAEs assume that P(z) = N(z; 0, I) and P(z|x) = N(z; (x; ), diag(2(x; ))). Here, assumingz Rd, I is a d d identity matrix, (x; ) Rd is a mean vector, and diag(2(x; )) Rdd is adiagonal covariance matrix with 21:d(x; ) as the diagonal elements. In this case, the analytical form of",
  "+ log 2i (x; ) 2i (x; ) 2i (x; )= L1,(x) + L2(x)(15)": "L2(x) and L1,(x) can be easily estimated with gradient backpropagation. However, when calculatingL1,(x), the gradient z log P(x|z) cannot be backpropagated to , since z are samples from the encoderrather than a function of . One solution is the reparameterization trick. Specifically, z can be reparame-terized as a deterministic function of by externalizing the randomness from the output to the input of theencoder, i.e., z = P(x, ) = (x; ) + (x; ) , N(0, I). ( Rd, and denotes the element-wiseproduct.) In this way, the generative models and corresponding inference models can be jointly trained usingstraightforward learning techniques (e.g., stochastic gradient descent (Amari (1993))).",
  "Next, we introduce some representative variants of VAEs, which are utilized in .2 and 3.3": "-VAE: Assuming that the data x PX() is generated by a groundtruth simulation processwith a number of independent generative factors, -VAE (Higgins et al. (2017)) is proposed tolearn latent variables z that encode each generative factor in separate dimensions. For example, asimulator might sample independent factors corresponding to the object shape, colour, and size togenerate an image of a small green apple; ideally, in the latent variable of the image, there shouldbe separate dimensions for each of these factors. To learn such an encoder P(z|x), the authorspropose to constrain the KL divergence between P(z|x) and an isotropic unit Gaussian distributionP(z) = N(z; 0, I) to encourage the dimensions of z to be conditionally independent, resulting inthe objective:max, EzP(|x) [log P(x|z)] s.t. DKL(P(z|x)||P(z)) < (16) Introducing a Lagrangian multiplier > 0, the equation above can be reformulated into an uncon-strained optimization problem: max,, EzP(|x) [log P(x|z)] [DKL(P(z|x)||P(z)) ]. Theyopt to fine-tune as a hyperparameter instead of learning it, leading to the final objective:",
  "max, EzP(|x) [log P(x|z)] DKL(P(z|x)||P(z))(17)": "When = 1, this recovers the original VAE formulation (i.e., Eq. (14)). controls the tradeoffbetween limiting the latent channel capacity (via the second term) and preserving the informationfor reconstructing the data sample (via the first term). Empirically, they find that its important toset > 1 in order to learn the required disentangled latent variables. Gated VAE: As mentioned in -VAE, disentangled latent variables can be viewed as data repre-sentations in which each element relates to an independent and semantically meaningful generatorfactor of the data. -VAE provides an unsupervised manner to learn such disentangled representa-tions (through constraining the KL divergence). However, consistent disentanglement has recentlybeen demonstrated to be impossible without inductive bias or subjective validation (Locatello et al.(2019)). Thus, Gated VAE (Vowels et al. (2020)) is proposed to incorporate domain knowledge asweak supervision to encourage disentanglement. Specifically, as a form of weak supervision, datacan be clustered such that each cluster comprises data sharing certain generative factors. Then, thelatent variable z can be split into several partitions, each of which capture the generative factors ofa specific cluster. Suppose that x1 and x2 are from the same cluster which is assigned to the i-thpartition of the latent variable, i.e., zi, the encoder and decoder can then be trained with a modifiedELBO: (which is a lower bound of log P(x2))",
  "L,(x1, x2) = EzP(|x1) [log P(x2|z)] DKL(P(z|x1)||P(z))(18)": "Note that the entire z is utilized to generate x2 from x1, but only gradients corresponding to zi arebackpropagated to the inference model P. The rationale is that x2 is intended to reconstruct onlythe shared generative factors with x1, and these factors should exclusively be captured by zi. GatedVAE requires domain knowledge as supervision, which may limit its general applicability.",
  "L,(x, c) = EzP(|x,c) [log P(x|z, c)] DKL(P(z|x, c)||P(z|c))(19)": "Compared with the original VAE formulation, a conditional variable c is introduced to each function,i.e., the prior, inference, and generative model. Following a derivation process similar with Eq. (13),it can be shown that Eq.(19) is a variational lower bound for log P(x|c), i.e., the conditionalgeneration likelihood. Further, they note that, during evaluation, latent variables are sampled fromP(z|c) rather than P(z|x, c), as only the conditions c are available. Thus, to enhance consistencyof the data generation process during training and evaluation, they introduce an additional objectiveterm to Eq. (19):",
  "Lhybrid,(x, c) = L,(x, c) + (1 )EzP(|c) [log P(x|z, c)](20)": "VQ-VAE: Unlike aforementioned VAE variants where z is continuous, VQ-VAE (van den Oord et al.(2017)) is an auto-encoder framework utilizing discrete latent variables. In particular, they assumethe latent variable space is a codebook [eT1 , , eTk ] Rkd, where k is number of categories and dis the dimension of the latent vector for each category (i.e., ei). Such discrete representations areusually easier to model than continuous ones and can be particularly advantageous in some situations.For example, in a dataset of images with several categories, categorized latent representations aremore appropriate, where each category is assigned a code from the codebook. In particular, theforward process of VQ-VAE is: zE = P(x), zD = ei (i = arg minj ||zE ej||2), x P(|zD). Here,the latent variable from the encoder, i.e., zE, is mapped to its nearest neighbour in the codebook,i.e., zD, which is then input to the decoder for data generation. The learnable parameters of VQ-VAE include , , and e1:k, for which the objective is as below: (zD and ei are defined as above; sgdenotes the stop-gradient operator.)",
  "L,,e1:k(x) = log P(x|zD) ||sg [P(x)] ei||22 ||P(x) sg [ei] ||22(21)": "The generative model is trained to reconstruct the data x, as in other VAE variants, via the first termof Eq. (21). The codebook parameters are trained to align with the encoded latent representationszE = P(x) through the second term. As for P, it is trained by both the first and third terms.Specifically, they assume log P(x|zD)",
  "zE= log P(x|zD)": "zD, such that the gradient of the first term on zDcan be backpropagated to . The third term, on the other hand, is to make sure that the encodercommits to an embedding from the codebook. While empirical evidence demonstrates VQ-VAEseffectiveness, its objective design largely relies on intuition, so additional theoretical underpinningwould be beneficial. VRNN: Chung et al. (2015) introduce a recurrent version of the VAE for the purpose of model-ing sequences of data (i.e., xT = [x0, , xT ]). At each time step t, the historical information(x<t, z<t) is involved for sequential decision making, where z<t are latent variables correspondingto x<t.Instead of directly concatenating the sequence of variables, i.e., (x<t, z<t), as the his-tory, a recurrent neural network (RNN) is adopted to embed the historical information recursively:ht = RNN(xt, zt, ht1), (t = 0, , T) 3. To involve historical information in decision-making, theprior, generative, and inference model are conditioned on the history embedding as: P(zt|ht1),P(xt|zt, ht1), and P(zt|xt, ht1), where ht1 embeds the history (x<t, z<t). The overall objective",
  "(22)": "where P,(z T|x T) = Tt=0 P(zt|xt, ht1) with ht defined as above. This objective is avariational lower bound of the log likelihood of the data sequence, i.e., log P(xT ), as shown inAppendix A of (Chung et al. (2015)). The introduction of historical information and explicit useof the Hidden Markov Model (Rabiner & Juang (1986)) (e.g., for defining P,(z T|x T))significantly improve the representational capability of VRNN for sequential data.",
  "max, E(s,a)DEzP(|a,s) [log P(a|z, s)] DKL(P(z|a, s)||P(z|s))(23)": "Corresponding to Eq. (19), s and a here work as the condition variable c and data sample x, respectively. Asintroduced in .1, the objective above constitutes a lower bound for E(s,a)D [log P(a|s)], i.e., thetypical supervised learning objective. After training, actions at state s can be sampled from the estimatedbehavior policy (|s) as: z P(|s), a P(|s, z). In practice, the reconstruction term log P(a|z, s) canbe replaced with ||a a||22 for continuous action spaces, and the prior P(z|s) can simply be chosen asN(z|0, I). Following the -VAE (i.e., Eq. (17)), a factor > 0 is usually introduced as the weight of theKL term in Eq. (23) to balance the two objective terms. With the estimated behavior policy , the policy penalty and support constraint offline RL methods intro-duced in .1.1 can be naturally applied. As a representative of support constraint methods, BCQ(Fujimoto et al. (2019)) trains such and defines the policy to learn based on as (|s) = a+(|s, a), wherea is a sample from (|s) and (|s, a) is a bounded and learnable residual term. is trained to maximizethe approximated Q-values as in standard RL 4. In this way, the action support of is constrained to beclose to the behavior policys. Similarly, inspired by (Shamir (2018)), AQL (Wei et al. (2021)) proposesto improve the estimation for the behavior policy using a residual generative model W1(W2G(s) + a),where a (|s) (as in BCQ) and W1,2, G() constitute a residual network. They claim that such a residualstructure could effectively reduce the estimation error (compared with using a) for the behavior policy . Regarding the benefits of using a VAE as the policy network, compared to feedforward networks composedsolely of fully-connected layers, VAEs excel at capturing the multiple modalities present in D, which couldbe collected by a diverse set of policies, by utilizing the latent variable z. Also, the action generation process,z N(0, I) and a P(|s, z), allows for stochastic action sampling. Compared to other deep generativemodels, VAEs may be less expressive but are more lightweight than Normalizing Flows and Diffusion Modelsand can provide more stable training than GANs. Based on these background knowledge, we provide a review of VAE-based offline RL algorithms in thefollowing subsections, with a particular focus on works whose primary novelty lies in the use of VAEs.One category of such algorithms seeks to enhance dynamic-programming-based offline RL via the use ofVAEs. They either modify the learning objective to further mitigate the issue of OOD behaviors, or apply 4For BCQ, multiple Q functions are learned simultaneously, and the target value for policy evaluation is specifically designedbased on these Q functions, which is though not our focus. Please refer to (Fujimoto et al. (2019)) for the details.",
  "As introduced in .1.1, there are four categories of algorithms for mitigating the issue of OOD actions.VAEs have been used to improve three categories among them, which are detailed as follows": "Applying support constraints: PLAS (Zhou et al. (2020)) proposes that the support constraint can besimply applied to the latent space of a VAE. In particular, they first estimate the behavior policy as a CVAE with Eq. (23). Ideally, after training (through maximizing the reconstruction accuracy of actions inducedby the behavior policy), for latent variables z which have high probabilities under P(z|s), the correspondingdecoder P(a|s, z) should output high-probability actions under the behavior policy distribution (i.e., in-distribution actions), since (a|s) is estimated asP(z|s)P(a|s, z) dz. In this case, they learn a latentspace policy (z|s) and use it in conjunction with the pretrained (and fixed) decoder P(a|s, z) as themapping from s to a. The output of is constrained to , i.e., the high probability area of the priordistribution N(z; 0, I), to ensure that (P )(a|s) outputs in-distribution actions. As introduced in .3, NF-based offline RL algorithms adopt the same algorithm idea. SPOT (Wu et al. (2022)) suggeststhat the constraint in BEAR (log ((s)|s) , s D) can be relaxed to EsD [log ((s)|s)] forpracticality. Then, the constrained policy improvement step (i.e., the second equation in Eq. (1)) can beconverted to: (We use and Q to represent the policy and Q function from now on, for simplicity.)",
  "As introduced in .1, Normalizing Flows enable exact density estimation, which can potentiallyeliminate the need for sample-based approximations as in Eq. (25)": "Applying policy penalty: TD3-CVAE (Rezaeifar et al. (2022)) proposes to replace the penalty term inEq. (4) (i.e, DKL((|s)||(|s))) with a prediction error b(s, a) = ||a P P(s, a)||, where = (P, P)is the pretrained CVAE for estimating . Intuitively, if an action a from the being-learned policy (|s)corresponds to a high prediction error under (|s), a is probably an OOD action and so (s, a) should beassigned with a high penalty. Further, they theoretically show the equivalence (under certain conditions)of b(s, a) and a KL-divergence penalty term DKL((|s)||b(|s)), where b(|s) = SoftMax(b(s, )/) and > 0 is a temperature parameter. Compared with DKL((|s)||(|s)), b(s, a) is easier to approximateand brings superior empirical performance. BRAC+ (Zhang et al. (2021a)) points out that estimatingthe penalty term DKL((|s)||(|s)) as DKL((|s)||(|s)) in BRAC requires generating a large number ofsamples to reduce the estimation variance. Therefore, they propose an upper bound for DKL((|s)||(|s)),which has an analytical form:",
  "(26)": "Here, the inequality is based on the fact that a CVAE = (P, P) is used to estimate andEzP(|s,a) [log P(a|s, z)] DKL(P(z|s, a)||P(z|s)) constitutes a variational lower bound for log (a|s),as shown in Eq. (19). Given that P(a|s, z), P(z|s), and P(z|s, a) all have Gaussian outputs and suppose",
  "minQc E(s,a,c,s)DQc(s, a) c + Ea(|s)Qc(s, a)2 EsD,a(|s)Qc(s, a)(27)": "Here, is defined based on = (P, P). In particular, a (|s), DKL(P(z|s, a)||P(z|s)) d, whereP(z|s) = N(z; 0, I) and d is a predefined threshold. Intuitively, (|s) produces OOD actions a of whichthe corresponding posterior distribution P(z|s, a) deviates significantly with its training target P(z|s), andthe expected cost Qc of OOD actions is trained to be high. MCQ (Lyu et al. (2022)) explores mild butenough conservatism for offline RL to mitigate the underestimation issue of CQL. Their algorithms designdoes not hinge on VAEs; instead, the VAE is employed solely for estimating as in previous works, so wedo not provide details here. For similar reasons, we skip introductions of UAC (Guan et al. (2023)) andO-RAAC (Urp et al. (2021)), both of which can be considered as extensions of BCQ, a support constraintmethod.",
  "Data Augmentation and Transformation": "Data augmentation: The provided offline data D may have limited coverage of the state-action spaceor lack diversity in behavioral patterns. Consequently, constraint- or pessimism-based algorithms may learnsub-optimal policies with limited generalization capabilities in the entire environment.VAEs have beenused for data augmentation, aiming at improving the coverage or diversity of the offline data. (1) ROMI(Wang et al. (2021a)) is a model-based data augmentation strategy utilizing reverse rollouts. They first learnthe backward dynamic model Trev(s|s, a), reward model r(s, a), and reverse policy rev(a|s) from D viasimple supervised learning, where s denotes the next state. Specifically, rev(a|s) is modeled with a CVAE(P, P), and its training objective is the same as Eq. (23) but to replace s with s. With a random samplest+1 from D, a reverse rollout (of length h) can be generated as below:",
  "i=0(28)": "Unlike the forward generation process, such a reverse manner prevents rollout trajectories that end in OODstates. Also, the CVAE makes it possible for stochastic inference: z N(0, I), a P(|s, z), which improvesthe diversity. These generated rollouts are then combined with D for the use of offline RL. This work isclosely related to model-based offline RL (.1.2). (2) To enable the learned policy to generalize toOOD states, SDC (Zhang et al. (2022)) suggests training the policy on perturbed states and motivating it torevert to in-distribution states from any state deviations. In particular, a forward dynamic model T (s|s, a)and a CVAE-based state transition model U(s|s) is learned from D through supervised learning. At a state",
  "s perturbed from s, the policy is trained to minimize MMDT (|s, (|s))||U(|s), i.e., to produce actions": "that can lead it back to the next state s in the original trajectory from the perturbation s. MMD denotesthe maximum mean discrepancy. (3) Han & Kim (2022) point out that the latent space of a VAE pretrainedon D can capture the data distribution in D. Based on that, they suggest selectively augmenting thedata region that is sparse in the original dataset through data generation with the VAE. However, thatpaper does not provide details on measuring sparsity through the latent space or data generation usingthe VAE. (4) KFC (Weissenbacher et al. (2022)) suggests inferring symmetries (Hambidge (1967)) of theunderlying dynamics of an environment using a VAE forward prediction model, and applying such symmetrytransformations to generate new data points as data augmentation. This work requires extensive knowledgeof control theory, so it will not be discussed in depth here.",
  "max,,e1:k E(s,a)Dlog P(a|ei) ||sg [P(s, a)] ei||22 ||P(s, a) sg [ei] ||22(29)": "Here, e1:k is the codebook and represents the k discretized actions; i = arg minj ||P(s, a)ej||2 is the indexof the nearest latent code for the embedding P(s, a). Applying the pretrained VQ-VAE encoder P on Dleads to discretized actions, i.e., (s, a) (s, ei). For discrete action spaces, the estimation of the constraintterms in offline RL, such as the approximated behavior policy (a|s) and KL divergence DKL((a|s)||(a|s))in policy penalty methods, could be easier and more accurate.",
  "VAE-based Multi-task/Hierarchical Offline RL": "Multi-task RL and hierarchical RL extend the basic RL setting. In particular, multi-task RL (Sodhani et al.(2021)) aims at learning a policy that can be directly applied to or quickly adapted to a distribution of tasks.Hierarchical RL, on the other hand, learns a hierarchical (two-level) policy for complex tasks which canusually be decomposed into a sequence of subtasks. In this case, low-level policies can be used to accomplisheach subtask, while the high-level policy coordinates the subtasks and the use of low-level policies. Forexample, in goal-achieving tasks, a goal-conditioned policy can be considered a multi-task policy, as it canbe used to reach multiple goals in an environment by changing the goal condition. However, if the goal isdistant, the entire path might be partitioned into several subgoals by the high-level policy, and to reach eachsubgoal, a corresponding low-level (subgoal-conditioned) policy can be employed. Next, we formally introduce these two setups and explore how VAEs can be utilized to enhance them. Byintroducing the most notable research in each category, i.e., BOReL for multi-task RL and OPAL & HiGoCfor hierarchical RL, our aim is to present the fundamental paradigms of these research directions as a tutorial. Multi-task RL: Given a multi-task offline dataset DM = [[ i,j = (si,j0 , ai,j0 , ri,j0 , , si,jT )]Ni=1]Mj=1, where iand j are indexes for the trajectory and task respectively, multi-task offline RL aims at learning a multi-taskpolicy that can be adapted to unseen tasks with zero- or few-shot training. These unseen tasks are requiredto be in the same distribution as the training ones. As a representative, BOReL (Dorfman et al. (2021))is proposed for the case where the reward and dynamic function (rj, Tj) vary with the task. To train amulti-task policy, a straightforward manner is to condition that policy on the task information (rj, Tj). Inparticular, they adopt the latent variable z 5 of a CVAE as a representation of the reward and dynamicfunction, and learns a latent-conditioned policy (a|s, z) as the multi-task policy. To this end, the CVAE istrained as follows:",
  "FLAPCVAEGenerating subgoals (Hierarchical RL)Real Robot": ": Summary of VAE-based offline RL algorithms. In Column 1, we list representative (but not all)algorithms in this section. These algorithms are grouped by their categories. Regarding the VAE types, -CVAE refers to an integration of CVAE and -VAE, where a weight is added to the KL term in CVAE. Theannotation (T) means that the VAE is implemented on trajectories rather than individual state transitions.The evaluation tasks are listed in Column 4. Most works are evaluated on D4RL (Fu et al. (2020)), whichprovides offline datasets for various tasks, including Locomotion (L), AntMaze (M), Adroit (A), Kitchen (K),Maze2d (M2d), etc. MuJoCo (Todorov et al. (2012)) and CARLA (Dosovitskiy et al. (2017)) are commonly-used simulators for robotic and self-driving tasks, respectively. Meta-MuJoCo (Dorfman et al. (2021)) is amulti-task version of MuJoCo. By Real Robot, we mean evaluations on real robotic platforms, which varyfrom one study to another. For other benchmarks, we provide their references here: GridWorld (Zintgrafet al. (2020)), MetaWorld (Yu et al. (2019c)), RoboSuite (Zhu et al. (2020)), Robomimic (Mandlekar et al.(2021a)), CALVIN (Mees et al. (2022b)). In this context, 0:T = (s0:T , r0:T 1, a0:T 1); a0:T 1 and (s0:T , r0:T 1) can be viewed as c and x in theCVAE framework. As shown in (Zintgraf et al. (2020)), ELBOt constitutes a variational lower bound forlog P(s0:T , r0:T 1|a0:T 1). The second equation in Eq. (30) is derived based on the Markov assumption(Puterman (2014)), and it can be observed from this equation that z is trained to embed informationregarding the initial state distribution, reward and dynamic functions, so as to reconstruct the task-specifictrajectory 0:T . Subsequently, the pretrained P(z|0:t), which is implemented as a recurrent neural networkas in VRNN (see .1), can be applied to D to infer the task embedding zt at each time step, leadingto a dataset of transitions in the form ((st, zt), at, rt).Note that (st, zt) can be viewed as an extendedstate st, and thus standard offline RL algorithms can then be directly applied to this dataset to learn alatent-conditioned policy (at|(st, zt)). Hierarchical RL: This category of algorithms try to learn a hierarchical policy (high(z|s), low(a|s, z))from the offline dataset. Intuitively, the agent would segment the whole task into a sequence of subtasks or",
  ", low (31)": "This objective is equivalent to a (-)CVAE ELBO, where s0 and work as the conditioner c and data xrespectively 6. However, unlike Eq. (23), here is trained on sets of trajectory segments rather than single-step transitions. This is because, as a skill policy, is expected to extend temporally. Also, the prior P(z|s)is not fixed but implemented as a neural network that takes s0 as input, to make sure the skill choice z is pre-dictable (by the high-level policy high(z|s)) given only the initial state s0. Applying the pretrained P(z|)on D and introducing the reward signals, a new dataset Dhigh= (si0, zi P(| i), h2t=0 rit, sih1)Ni=1 canbe obtained for training high(z|s) with any offline RL methods. With this hierarchical policy (high, low),the decision horizon of offline RL is effectively shorten (by a factor of h) and so OOD actions caused bythe accumulated distribution shift can be mitigated. Rosete-Beas et al. (2022) propose TACO-RL, whichis a very similar algorithm with OPAL but specifically tailored for goal-achieving tasks. HiGoC (Li et al.(2022)) is also a hierarchical framework for goal-achieving tasks, where the high-level part is a model-basedplanner 7 for generating the subgoal list and the low-level part is a goal-conditioned policy trained by offlineRL to reach each subgoal sequentially. The subgoals are not labeled in the dataset, so the low-level policyis trained in an unsupervised manner. Specifically, for low(at1|st1, st2), st2 (t2 > t1) is the subgoal andrandomly sampled from states after st1 in the same trajectory. To be robust to possible OOD subgoalsduring evaluation, a CVAE m(st|sth) = (P(z|st, sth), P(st|z, sth)) is pretrained on D for generatingthe subgoal st conditioned on the previous subgoal sth, where h is a predefined time interval for subgoalselections. With this CVAE, a perturbed subgoal can be generated based on the sampled one (i.e., st2) asP(P(st2, st2h) + , st2h) ( is a noise vector), which can replace st2 as the subgoal of low for robustness.In cases of high-dimensional states, such as images, adding noise to a well-defined low-dimensional embeddingspace (i.e., z in the VAE) is a more effective and reasonable approach. The pretrained decoder P(st|z, sth)can also be used for high-level planning. Specifically, a subgoal list can be generated by specifying a listof latent variables z1:k: sti P(|zi, stih), i [1, , k]. Such a subgoal list can then be evaluated bytask-relevant objectives. In this case, searching for an optimal list z1:k is literally a model predictive planningproblem and is more efficient than directly searching on the high-dimensional state space( i.e., st1:k). FLAP(Fang et al. (2022)) adopts a quite similar protocol with HiGoC, where the CAVE m is referred to as theaffordance model.",
  "Variational Auto-Encoders in Imitation Learning": "In this section, we offer an overview of VAE-based IL algorithms. First, we provide a tutorial-like overviewof the four schemes of VAE-based IL. Based on this, we introduce a categorization of all related works basedon how VAEs are utilized. The use of VAEs focuses on enhancing Behavioral Cloning, either from a data oralgorithmic perspective.",
  "Core Schemes": "Imitation Learning (IL) aims at recovering the expert policy (a|s) from a set of demonstrations DE. Be-havioral Cloning (BC) is a straightforward and widely-used IL framework (Pomerleau (1991)), as introducedin .2.1. With its special encoder-decoder structure, the VAE has been utilized to improve BC frommultiple perspectives. Here, we provide a summary of four such schemes. 6For a standard CVAE, the reconstruction term should be log P(|s0, z) = h1t=0 log (at|st, z)+h2t=0 log T (st+1|st, at).However, the transition function T is not trainable and so would not influence the gradient calculation.7Please refer to (Li et al. (2022)) for further details.",
  "max, E(s,a)DEEzP(|s,a) log P(a|s, z) DKL(P(z|s, a)||P(z|s))(32)": "This objective constitutes a variational lower bound for E(s,a)DE log (a|s), i.e., the BC objective. Aftertraining, z P(|s), a P(|s, z), where P(z|s) is a predefined prior and usually set as N(z; 0, I), can beused as the policy a (|s). With the latent variable, the VAE-based policy can model stochastic behaviorswith diverse modes. Given that (a|s) =P(a|s, z)P(z|s)dz, rather than using a fixed prior distributionP(z|s), Ren et al. (2020) propose an algorithm to fine-tune the prior online for specific tasks by optimizinga generalization bound from the PAC-Bayes theory. Scheme (2): The second scheme is based on representation learning (RepL), where the latent variable z isadopted as a representation of the state s and usually can be learned via state reconstructions:",
  "max, EsDEEzP(|s)Rec(s, z) DKL(P(z|s)||P(z)), maxE(s,a)DE,zP(|s) log (a|z)(33)": "where the reconstruction objective Rec(s, z) = log P(s|z) or ||s P(z)||22. Note that the first objectivein Eq.(33) is usually coupled with extra regularization terms to encourage the disentanglement of thelearned representation, or enable the fusion of state inputs s from multiple modalities, etc. Based on therepresentation learning, a policy (a|z) conditioned on the compact representation z can be learned withany IL algorithm. Compared with s, z is in a lower dimension and is less noisy for decision making. As aresult, RepL can effectively reduce the need of IL for large amounts of training data. In Eq. (33), thereare in total three functions (besides P(z) which may not be learnable): P(z|s), P(s|z), and (a|z). Onemanner is to pretrain P(z|s) and P(s|z) within the VAE framework and then train (a|z) with IL as shownin the second term of Eq. (33). When training , the parameters of P can either be frozen or not. Theother manner is to jointly train the three functions through an integrated objective, i.e., adding the twoobjectives in Eq. (33) together with an adjustable weight . In this case, the state reconstruction with aVAE can be viewed as an auxiliary task of IL. EIRLI (Chen et al. (2021a)) provides a systematic empiricalinvestigation of representation learning for imitation.They find that RepL using VAEs can effectivelyimprove the performance of vision-based IL. However, they also mention that the relative impact of usingrepresentation learning tends to be lower than the impact of adding data augmentations.This may bebecause usual RepL techniques (for images) tend to capture the most visually salient axes of variation (e.g.,the color, background, or objects) but the action choices are often determined by more fine-grained, localcues in the environment, which calls for RepL that is more specific to decision making. Additional examplesin this scheme include (Lee et al. (2019); Rahmatizadeh et al. (2018)).",
  "t=0[log P(at|st, z) + log P(st+1|st, at, z)](34)": "The decomposition of P(|z) makes use of the MDP model, where the three terms correspond to the initialstate distribution, policy distribution, and transition dynamic, respectively. P(s0|z) and P(st+1|st, at, z),which may be independent of z, are usually defined within the simulator and not required to model, solog P(|z) can be replaced with T 1t=0 log P(at|st, z) in the original objective.Note that P(|z) hasmultiple alternative decomposition forms. For example, T-VAE (Lu et al. (2019)) proposes to model it aslog P(s0:T |z) + log P(a0:T 1|z, s0:T ). Specifically, two RNNs are adopted to generate the state and actionsequences respectively, leading to a new definition of log P(|z): (The state sequence s0:T is generated beforethe action sequence a0:T 1; the generation of st/at is conditioned on hst1/hs,at1 which embeds the historys1:t1/(s1:t1, a1:t1).)",
  "maxf:XY I(Z, Y ; f) s.t. I(X, Z; f) Ic maxf:XY I(Z, Y ; f) I(X, Z; f)(36)": "where I() denotes the mutual information, Ic is the information constraint, and > 0 is the Lagrangianmultiplier. The goal of this framework is to learn an encoding Z that is maximally expressive about thetarget Y while being maximally compressive about the input X. Substituting X, Y with s, a, the functionto learn, i.e., f, is then the expert policy (a|s). VIB-based IL can be practically solved by the followingobjective, which is a lower bound of Eq. (36) (see (Alemi et al. (2017))):",
  "max,, E(s,a)DEEzP(|s) log P(a|z) DKL(P(z|s)||P(z))(37)": "This objective is similar with the VAE ELBO (i.e., Eq. (14)) in form but has two differences. First, thedecoder P here can predict variables different from the input of the encoder, whereas in VAEs, it is typicallytrained to reconstruct the input variable. Second, P(z) is not an assumed prior distribution of z as in VAEsbut a variational approximation of the marginal distribution of z, i.e., PZ(z) =PX(x)P(z|x)dx, and it isusually learned as a neural network. Both Scheme (2) and Scheme (4) can be regarded as forms of imitationlearning that incorporate effective representation learning. After training, to sample an action a (|s),the policy encoder first compresses the state to a representation, i.e., z P(|s), and then its decoderpredicts the action based on z, i.e., a P(|z). When dealing with high-dimensional and noisy observationss, using an information bottleneck can filter out redundant information while retaining essential knowledgefor action predictions in the representation z. A higher compression rate on the input often results in bettergeneralization of the learned policy across tasks. Next, we provide an overview of the applications of VAEs in IL. Most works in this direction focus onextending BC. From a data perspective, VAEs can increase BCs data efficiency in scenarios where original,task-specific data is limited, or enable the use of multi-modal input data (e.g., from various sensor types).These applications can mitigate the issue of insufficient demonstrations for IL, as noted in .3.Regarding algorithmic advancements, VAEs have been utilized for skill discovery to enable hierarchical IL,and to address the causal misidentification issue of BC.",
  "Improving Data Efficiency": "IL, especially BC, requires a large amount of training data to cover possible task scenarios for robust per-formance. However, expert-level demonstrations for a specific task, i.e., DE, are usually costly to acquire.Therefore, efficient data usage in IL is crucial. This can be achieved by leveraging the inherent structureswithin the data, or by gathering and processing task-related data from alternative sources for training. Thecapability of VAEs to extract latent representations plays a key role in facilitating these processes. Here, wepresent several notable works in this direction. Behavior Retrieval (Du et al. (2023)) is proposed for the case where vast task-unlabelled demonstrationsare available, i.e., Dprior.Specifically, Dprior may contain suboptimal behaviors for the current task ordemonstrations for a range of different but related tasks (following the same task distribution). To thisend, they propose to adopt a -VAE (i.e., (P, P) in Eq.(17)) to learn the latent representations of",
  "f((s1, a1), (s2, a2)) = z1 z22, (s1, a1) Dprior, (s2, a2) DE, zi P(|(si, ai)).(38)": "They claim that this method would effectively filter out sub-optimal or task-irrelevant data. As a final step,a policy (a|s) can be learned on DE Dret through BC. This representation learning protocol resemblesaforementioned Scheme (2). For robotic control tasks, state-only demonstrations from a third-person view (e.g., videos from a demon-strator) are usually more available than the ones in the first-person view.Observations from the first-and third-person views, i.e., sF and sT , correspond to the same true environment state s. Given a set ofdemonstrations synchronized across different viewpoints [(sFi , sT1i , , sTki )]Ni=1, where T1:k denotes k differ-ent third-person viewpoints, a common viewpoint-agnostic representation is required to make full use of thesedata. To realize this, VAE-TPIL (Shang & Ryoo (2021)) proposes to learn two VAEs for the first- andthird-person view data respectively, i.e., (PF , PF ) and (PT , PT ), and to disentangle the latent variableinto a viewpoint embedding v and a viewpoint-agnostic state embedding h. To realize this, besides usualreconstruction objective terms, auxiliary objectives are involved: (i = j, (h, v) P(s))",
  "Lv = EsTi||P(hTi , vTj ) sTi ||2, Lh = EsTi||P(hTj, vTi) sTi||2, LF = EsT ,sF||hT sg(hF )||2(39)": "The intuition behind these terms are: (a) The view embeddings vTiand vTj of states in the same view,i.e., sTi and sTj , should be semantically equivalent and so P(hTi , vTj ) should be able to recover sTi . (b) Theembeddings hTi and hTj corresponding to the same state from different views, i.e., Ti and Tj, should beinterchangeable for reconstructing sTi, since h is expected to be view-agnostic. (c) Similarly, hT and hF should be close as they are viewpoint-agnostic and correspond to the same true environment state. With thepretrained PF and PT , demonstrations across various viewpoints can be converted to viewpoint-agnosticlatent representations (i.e., h).Then, a representation-based policy (a|h) can be learned with any ILmethod. This algorithm can be viewed as a method for the challenging setting: Learning form Observations(LfO) (introduced in .3). GIRIL (Yu et al. (2020b)) suggests modelling the underlying transition from (st, at) to st+1 in DE (i.e.,its inherent structure) using a CVAE, where (st, at) and st+1 work as the condition c and data samplex respectively, besides imitating the policy (at|st).Then, an intrinsic reward can be defined as rt =||st+1 st+1||22, where st+1 P(|st, at).(Offline) RL can be applied to the extended demonstrations[(st, at, rt, st+1)] to further improve the imitator learned via BC, i.e., (at|st).Intuitively, this processencourages the agent to explore states with high prediction errors to reduce the state uncertainty, making itpossible to achieve better-than-expert performance.",
  "Managing Multi-Modal Inputs": "In real-world scenarios, decision-making inputs often come from multiple modalities, including RGB images,depth images, 3D point clouds, language instructions, and more. As a motivating example, consider the taskof navigating a drone in an outdoor environment, where multiple modalities emerge due to visual informationcaptured by various sensors and factors like the drones current pose and location. Coherently managing themulti-modal input is essential for the real-life applications of IL. In this context, VAEs have proven effectivefor fusing or unifying inputs from multiple modalities based on the use of latent embeddings. RGBD-VIB (Du et al. (2022)) proposes to fuse multi-modal inputs based on their uncertainty for evaluation,ensuring that inputs with higher certainty are given more weight in decision-making. To this end, a VAE(Pi(z|s), Pi(a|z), Pi(z)) is trained for each modality i [1, , N] using the VIB framework (i.e., Eq.(36)). Note that these VAEs share the same action space but vary in their state spaces. The uncertaintymeasure can then be acquired based on the pretrained VAEs as ui(s) = DKL(Pi(z|s)||Pi(z)). Intuitively,a large uncertainty ui(s) indicates that the current state in modality i may be out of the distribution of thetraining dataset. Finally, the action is a weighted sum of the output from each VAE decoder, where theweight assigned to each modality is proportional to Nj=1 uj(s) ui(s), inversely related to the uncertainty.",
  "VAE-based Skill Acquisition and Hierarchical Imitation Learning": "In the context of IL, skills are meaningful subsequences/patterns within the expert trajectories, which canbe potentially utilized in multiple related tasks. As an example, consider IL for cooking tasks, where thetasks can be diverse but often have overlapping patterns, such as, slicing, chopping, etc. As introduced inOPAL (see .2.4), VAEs can be used to extract such patterns via learnt latent embeddings (i.e.,using Eq. (31) which is a special case of aforementioned Scheme (3)). As in OPAL, after training, thedecoder P(a|s, z) can be viewed as the policy for skill z (i.e., low(a|s, z)), and the encoder P(z|) canbe used to parse the expert data into trajectories of (s, z) pairs for training the high-level policy high(z|s)with any IL method. Then, the hierarchical policy can be executed in a call-and-return manner: samplinga skill z high(|s), executing the corresponding policy a low(|s, z) for h time steps, sampling thenext skill, and so on. This whole process can be considered as a hierarchical extension of BC. Beyond thisstraightforward use of VAEs, some works have proposed to impose additional structures or properties, suchas disentanglement, to the latent space for improved skill acquisition (e.g., in interpretability). As mentionedin .1, being disentangled means that each dimension/partition of the latent variable is independentand semantically meaningful. One way to realize this is to directly incorporate an auxiliary loss term to thestandard VAE ELBO as regularization:",
  "where LELBO,(DE) usually takes the form of Eq. (31); is the parameter of an auxiliary network; > 0controls the tradeoff between the two loss terms": "SAILOR (Nasiriany et al. (2022)) defines Lreg,,(DE) = E(1,2)DE [f ((P(|1), (P(|2)))) t]2,where 1 and 2 belong to the same trajectory and are separated by t time steps, (P(|1)) denotesthe mean of the encoder output. Such a term can encourage the learned embedding to be predictable (forthe temporal difference between skills) and consistent, which is important for downstream policy learning 8This objective ensembles the VIB framework, i.e., Eq. (36), but overlooks the regulation on z. Also, training a separateencoder for each context is necessary, since dealing with different modalities requires different foundation models.",
  "with these skills. Note that the gradient from Lreg,,(DE) can be backpropagated to the encoder, allowingthis term to shape the learned skill embeddings": "Both OPAL and SAILOR would partition trajectories into segments of length h (i.e., the assumed skillhorizon) and then train a CVAE on these independent segments for skill acquisition (i.e., with Eq. (31)).CKA (Pasula (2020)), on the other hand, proposes a regularization term on the entire trajectory.Inparticular, if a trajectory can be decomposed into m skills in a sequence, the learned skill embeddings z1:mcan be regularized by a mutual information term Lreg,,(DE) = EDE,ziP(|i)I (mi=1 zi; ), where i isthe i-th segment of . The intuition is that, as a complex behavior, should be a combination of these skills.By maximizing the mutual information, the interpretability of the learned embeddings for the behavior can be enhanced. The reason to assume a summation structure mi=1 zi as the combination form is to enablea practical estimation of the mutual information term (see (Pasula (2020))). Aforementioned methods eitherassume a fixed skill horizon of h or require that the skill segmentation of the trajectory is readily available,thus only learning skill embeddings for each segment. CompILE (Kipf et al. (2019)) suggests a method toconcurrently infer both the skill boundaries and skill embeddings directly from the trajectory. A major benefit of skill acquisition lies in the potential for skills to be transferred across multiple relatedtasks.This transfer can facilitate the learning of a multi-task policy, (a|s, z, c), where z and c repre-sent the skill and task, respectively.In this case, (z, c) constitutes the policy condition and disentan-glement between z and c are required for interpretable and controlled behavior generation. To this end,TC-VAE (Noseworthy et al. (2019)) is proposed for the scenario where the task variables c are providedand task-irrelevant skill embeddings z need to be learned. A regularization term max min Lreg,,(DE) =max min E(,c)DE||f((P(|)))c||1 is adopted, where f is trained to predict the task from the meanof the encoder (P(|)). Intuitively, maximizing this objective over will encourage the learned latentspace z to be non-informative about the task c. SKILL-IL (Bian et al. (2022)), on the other hand, learnsz and c in the meantime as the latent variable of a VAE encoded from a set of multi-task demonstrations.To encourage the disentanglement, a Gated VAE (see .1) is adopted, where the latent variable ispartitioned into two subdomains for z and c respectively. Parameters related to subdomain z are updatedon data comprising different skills but within the same task. Similarly, parameters related to subdomain care trained with data corresponding to the same skill but from different tasks.",
  "Tackling Causal Confusion": "Causal confusion/misidentification happens when learnt imitator policies become strongly correlated oncertain nuisance variables present in the task environment, rather than identifying the causal variables thatexpert demonstrators actually exploited in decision-making. Thus, more information in the environmentcould yield worse IL performance as there would be more nuisance factors. For instance, in a driving task,the driver must brake when encountering obstacles or pedestrians. If the demonstration images include brakelight indicators on the dashboard, imitators might incorrectly associate the braking behavior with theseindicators. However, evaluation scenarios may not always feature these indicators, leading to potentiallycatastrophic outcomes (such as hitting a pedestrian). Therefore, to be maximally robust to distributionshifts (between the training and evaluation scenarios), a policy must rely solely on true causes of expertactions, thereby avoiding causal misidentification. As a seminal work, RCM-IL (De Haan et al. (2019)) resolves causal misidentification in IL by learning adisentangled representation z Rd of the state s, each dimension of which represents a disentangled factorof variation, and then differentiating between nuisance and actual causal factors in z. In particular, a -VAEis adopted to learn such a disentangled representation z following Eq. (17) (with x replaced by s). Afterpretraining the -VAE on DE, states s in DE can be replaced by their corresponding latent embeddingsz. Each of the d dimensions of z could represent either a causal or nuisance factor, and it is necessary tomask out the nuisance factors. The problem is then to find the correct mask vector m {0, 1}d. Withm, the policy can be defined on the masked representations as (a|m z), where denotes element-wisemultiplication and mz keeps only actual causal factors. De Haan et al. (2019) propose methods for findingthe correct m by querying experts or interacting with the environment, which does not rely on the use ofVAEs and so is not detailed here. Masked (Pfrommer et al. (2023)) adopts the same protocol as RCM-ILbut improves the algorithm for finding the mask vector. Specifically, it uses a carefully-designed statistical",
  "OREOVQ-VAEAddressing causal confusion (2)OpenAI Atari, CARLA": ": Summary of representative VAE-based IL algorithms. For each algorithm, we detail the type ofVAE used, how the VAE is applied, and the tasks on which it was evaluated. In Column 3, (1)-(4) referto the 4 schemes of VAE-based IL introduced in .3.1. As for the evaluation tasks, IL algorithmshas relatively more diverse choices than offline RL algorithms (as shown in ). By Real Robot orReal-world Navigation, we mean evaluations on real-world platforms, which vary from one study to another.We provide the references of these benchmarks here: dm_control (Tassa et al. (2018)), Procgen (Cobbeet al. (2020)), MAGICAL (Toyer et al. (2020)), 2D Navigation/Circle (Lu et al. (2019)), Minecraft (Gusset al. (2019)), Basketball Tracking (Felsen et al. (2018)), PEMS-SF Traffic (Dua et al. (2019)), Billiard BallTrajectory (Fragkiadaki et al. (2016)), RoboSuite (Zhu et al. (2020)), PyBullet (Coumans & Bai (20162021)), OpenAI Gym/Atari/MuJoCo (Brockman et al. (2016)), 3D Playroom (Lynch et al. (2019)), AirSim(Shah et al. (2017)), Franka Kitchen (Gupta et al. (2019a)), CALVIN (Mees et al. (2022b)), GridWorld(Zintgraf et al. (2020)), Synthetic Arcs (Noseworthy et al. (2019)), MIME Pouring (Sharma et al. (2018)),Craftworld (Devin (2024)), CARLA (Dosovitskiy et al. (2017)). hypothesis testing algorithm to efficiently mask out nuisance variables among the latent dimensions producedby the -VAE, which does not require expensive queries to the expert or environment for intervention. Undercertain assumptions, this algorithm is guaranteed not to incorrectly mask factors that causally influence theexpert. Please see (Pfrommer et al. (2023)) for details. Alternatively, Park et al. (2021) present OREO to ameliorate causal misidentification in IL, of which themain idea is to encourage the policy to uniformly attend to all semantic factors in the state to preventit from strongly exploiting certain nuisance factors.In particular, a VQ-VAE is trained on states fromdemonstrations using Eq. (21), where x is replaced with s DE. Since there could be multiple (say n)semantic factors within s, the latent representations from the VQ-VAE, i.e., zE and zD, are set to be 2Dmatrices in Rnd, where d is the dimension of the representation for each single factor. After pretraining,given a state s, its multi-factor representation zD can be acquired by querying the closet code for each rowof zE: (e1:k is the codebook, ej Rd, and zE,i Rd is the i-th row of zE.)",
  "zE P(|s), zD = [eTq(1), , eTq(n)], q(i) = arg minj[1, ,k]zE,i ej2.(42)": "The next step is to learn a policy through BC based on the multi-factor representation zD while reducing theinfluence of nuisance factors. Specifically, at each state s, a set of k binary random variables Mi {0, 1}, i [1, , k] are iid sampled from a Bernoulli distribution, and then a mask vector m corresponding to s can",
  "Generative Adversarial Networks in Offline Policy Learning": "Similar to the previous section, we divide the content here into three parts: background on GANs (.1), the applications of GANs in IL (.2) and offline RL (.3). Compared to GAN-basedoffline RL, GAN-based IL algorithms are relatively more. GAN-based IL primarily extends two fundamentalIRL algorithms: GAIL and AIRL, while GAN-based offline RL is mainly based on the model-based offlineRL.",
  "Background on Generative Adversarial Networks": "The Generative Adversarial Network (GAN, Goodfellow et al. (2014a)) is a generative model based on theminimax optimization. It consists of a generator G and discriminator D that are trained simultaneously tocompete against each other. The generator tries to capture the distribution of real data PX(x) and generatenew data examples x from initial noise z PZ(). The discriminator is usually a binary classifier used todiscriminate generated examples from real data as accurately as possible. Formally, the generator can bemodeled as a differentiable function that maps noise z following a certain prior distribution PZ() to samplesx in the data space X: z PZ(), x = G(z) PG(). While, the discriminators output D(x) estimates theprobability of a data point x being sampled from the true data distribution PX() rather than generated bythe generator. The training objective of GAN is formulated as a minimax game between G and D:",
  "minG maxD ExPX()[log D(x)] + EzPZ()[log(1 D(G(z)))](43)": "Here, D is trained to maximize the probability of assigning correct labels to both real data and fake onesfrom the generator: D(x) 1, D(G(z)) 0.For a certain G, the optimal discriminator is given by:DG(x) =PX(x) PX(x)+PG(x).Plugging this back in Eq.(43), the minimax problem can be converted to anoptimization problem with regard to the generator G: minG JS(PX()||PG()), where JS() denotes theJensen-Shannon divergence (Menndez et al. (1997)). Please refer to (Goodfellow et al. (2014a)) for thedetailed derivation. Thus, in essence, G is trained to generate samples that match the real data distribution.",
  "minG maxD ExPX()[log D(x|y)] + EzPZ()[log(1 D(G(z|y)))](44)": "Similarly, this objective can be converted to minG JS(PX(|y)||PG(|y)), i.e., matching the condi-tional distribution of the real data. With these conditional models, CGAN has the advantage ofhandling not only unimodal datasets but also multimodal ones like Flickr (Plummer et al. (2015)),which consists of diverse labeled image data. With the class label or text as the condition y, CGANcan be used for conditional generation. However, when the dataset contains multiple modalities butthe labels y are not given, InfoGAN (Chen et al. (2016)) proposes to introduce a latent code c (fol-lowing an assumed distribution PC()) to the generator. In this way, the data generation process ismodified into c PC(), z PZ(), x = G(z, c) PG(|c), where c is for capturing distinct modes inthe dataset and z targets the unstructured noise shared across the modes. The generator G is trainednot only with V (D, G) (i.e., Eq. (43)) but also to maximize the mutual information between themode variable C and the respective samples X|C = G(Z, C): minG maxD V (D, G) I(C, X|C)),such that G(Z, C) would not degenerate to a unimodal model. By replacing the mutual informationwith its practical lower bound, we have the objective for InfoGAN as below:",
  "Here, Q(x|c) is a variational posterior neural network trained to approximate the real but intractableposterior distribution P(c|x), and H(C) denotes the entropy of the mode variable C 9": "f-GAN: A large class of different divergences, including the KL divergence and JS divergence,are the so-called f-divergences (Csiszr & Shields (2004)).Specifically, given two continu-ous distributions P() and G(), the f-divergence between them is defined as Df(P()||Q()) =Q(x)fP (x)Q(x)dx, where f : R+ R should be a convex, lower-semicontinuous function satisfying f(1) = 0 10. Given a data distribution PX(), a corresponding generative model G can be learnedthrough minimizing Df(PX()||PG()). As proposed in f-GAN (Nowozin et al. (2016)), this can beapproximately solved via a minimax optimization problem:",
  "minG maxVExPX() [gf(V (x))] + ExPG() [f (gf(V (x)))](46)": "Here, V is a differentiable function without any range constraints on the output, gf is an outputactivation function specific to the f-divergence used, and f is the convex conjugate function off (Hiriart-Urruty & Lemarchal (2004)). With specific choices of f, f and gf, various types off-GAN can be achieved (see of (Nowozin et al. (2016))). For example, the original GANcan be recovered by setting f (t) = log(1 exp(t)) and gf(v) = log(1 + exp(v)) 11. In thisway, f-GAN provides a generalization of multiple variants of GANs, including the original GAN,LSGAN, and WGAN. Least Squares GAN (LSGAN): During the early training stage, the generated examples wouldsubstantially differ from the real data, but the original GAN objective provides only very smallpenalties for updating G, as shown in of (Goodfellow (2017)). To overcome this vanishinggradient problem, LSGAN (Mao et al. (2017)) replaces the cross-entropy loss used in the originalGAN objective with least-square losses:",
  "minD ExPX()(D(x) j)2+ EzPZ()(D(G(z)) i)2, minG EzPZ()(D(G(z)) k)2(47)": "9I(C, X|C) = H(C) H(C|X|C) = PC(c) log PC(c) dc + PC(c)PG(x|c) log P(c|x) dc dx PC(c) log PC(c) dc +PC(c)PG(x|c) log Q(c|x) dc dx. The same trick as in Eq. (13) is adopted here to get this inequality.10As an instance, when f(a) = a log a, Df recovers the KL divergence.11The discriminator in the original GAN D(x) would be a function of V (x), i.e., D(x) = 1/(1 + exp(V (x))). V (x) R andthus D(x) (0, 1).",
  "minG maxD ExPX() [D(x)] ExPG() [D(x)](48)": "Here, D is used to estimate the Wasserstein distance, hence its output is not limited to as inthe original GAN. Also, the neural network D is applied with weight clipping, ensuring that D isK-Lipschitz. Compared with the original GAN, WGAN improves the learning stability and providesmeaningful learning curves for parameter fine-tuning. Triple-GAN: To realize data classification and conditional generation in the meantime, Triple-GAN (Li et al. (2017a)) proposes to introduce a classifier C(y|x) to the original GAN framework.Here, x and y denote the data and label, respectively. Given a dataset of (x, y), empirical jointand marginal distributions of the real data and labels can be acquired as PX,Y (x, y), PX(x), andPY (y). PX,Y (x, y) can be approximated as either PX(x)PC(y|x) or PY (y)PG(x|y), corresponding tothe classification with C and conditional generation with G, respectively. Thus, C and G can betrained to match the joint distribution of (x, y). Here is the objective:",
  "(49)": "The first three terms resemble the GAN framework, where C and G are trained to fool the dis-criminator D by generating (x, y) close to the real data. The last two (cross entropy) terms pro-vide extra supervision for C using samples from the real and generated distributions. Note thatthe last objective term is only utilized for training C.It is shown in (Li et al. (2017a)) thatPX,Y (x, y) = PX(x)PC(y|x) = PY (y)PG(x|y) if and only if the equilibrium among the three players(C, G, D) is achieved in Eq. (49).",
  "Generative Adversarial Networks in Imitation Learning": "This section starts with a detailed introduction of the fundamental algorithms in this direction: GAIL andAIRL, as a tutorial, and then present extensions for each of them. Finally, we provide a spotlight on theintegrated use of GANs and VAEs for IL, as examples of synthesizing the use of different generative modelsfor offline policy learning.",
  "min H() + E(s,a)()[c(s, a)] E(s,a)E ()[c(s, a)](50)": "Here, (s, a) = (a|s) t=0 tP(st = s|) is the discounted occupancy measure (i.e., visit frequency) of(s, a) when taking , and H() = E(s,a)()[ log (a|s)] denotes the policy entropy.Intuitively, thisframework looks for a cost function c that assigns low costs to the expert policy E and high costs to any",
  "min H() + ( E), ( E) =supcRSA( E)T c (c)(51)": "That is, -regularized MaxEntIRL implicitly seeks a policy, whose occupancy measure is close to the experts,as measured by the convex conjugate of the regularizer, i.e., . As introduced in .1, GANs providea scalable manner for distribution matching. GAIL proposes that, with a certain design of (i.e., Eq. (13)in (Ho & Ermon (2016))), Eq. (51) can be converted to a GAN objective:",
  "minmaxD(0,1)SA H() + E(s,a)()[log(1 D(s, a))] + E(s,a)E ()[log D(s, a)](52)": "Compared with Eq. (50), we can see that c(s, a) is defined with the discriminator as log(1D(s, a)) 12. Also,the policy works as the generator G, so D and can be alternatively trained as in GANs to optimize Eq.(52). Specifically, in each training episode, is trained for several iterations to maximize the expected returnQ(s0, a0) = t=0 t( log(1 D(st, at)) log (at|st)), (s0, a0), with a widely-adopted RL algorithm TRPO (Schulman et al. (2015)) 13, and then D is trained for iterations to correctly classify samples from or E with Eq. (52). Compared with the original MaxEntIRL, which requires solving a complete RLproblem as the inner loop for each update of c, GAIL ensures both expressiveness and scalability, especiallyuseful for tasks in high-dimensional state/action spaces. Further, GAIL provides solid theoretical resultson the connection between IRL and GANs. As for the global optimality and convergence rate of GAIL,Zhang et al. (2020b) provide an analysis under the condition of using two-layer neural networks (with ReLUbetween the two layers) as function estimators and applying natural policy gradient for policy updates, whenthe underlying MDP belongs to the class of linear MDPs.",
  "exp(f(s,a))+1) as the activation func-tion on the last layer output (i.e., f(s, a)) of the discriminator as in GAN and GAIL, AIRL adoptsD(s, a) =exp(f(s,a))": "exp(f(s,a))+(a|s).They claim that, with this design, AIRL is equivalent to MaxEntIRL, un-der the assumption that trajectories = {s0, a0, , sT , aT , sT +1} follow the Boltzmann distribution (i.e.,P() 0(s0) T 1t=0 T (st+1|st, at) exp(tr(st, at))). However, the derivation provided in (Fu et al. (2017))is not rigorous 14. Further, they propose to replace D(s, a) with D(s, a, s) =exp(f(s,a,s)) exp(f(s,a,s))+(a|s), wheref(s, a, s) = g(s) + h(s) h(s). They state that, at optimality, g(s) and h(s) can recover the rewardand value function, respectively, if the real reward function is a function of s only and the environment isdeterministic 15. While its theoretical foundation is not robust, AIRL can be considered as a GAN-based ILalgorithm drawing inspiration from MaxEntIRL and well substantiated by empirical results. 12We equivalently substitute all D(s, a) in (Ho & Ermon (2016)) as 1 D(s, a) to be in line with GAN, where the true andfake data should be labeled as 1 and 0, respectively, by the discriminator D.13max Q(s0, a0) = t=0 t( log(1D(st, at)) log (at|st)) is equivalent to min H()+E(s,a)()[log(1D(s, a))]. > 0 is introduced here to balance the two objective terms.14In Appendix A.1 of (Fu et al. (2017)), the step log Z = EpTt=0 r(st, at)is questionable. Also, in its AppendixA.2, they erroneously mix up the use of t and t.15In Appendix A.4 of (Fu et al. (2017)), they claim that the global minimum of the discriminator objective is reached when = E, which is backed up by the original GAN paper (Goodfellow et al. (2014b)). However, the objective design of AIRL issignificantly distinct from the original GAN. Moreover, this claim forms the basis of the proof presented in its Appendix C ong(s) and h(s) recovering the reward and value functions respectively.",
  "Extensions of GAIL": "In , we provide an overview of representative works that extend GAIL. They either substitute theoriginal GAN framework with more advanced GAN variants (as introduced in .1) or expand the useof GAIL to other IL settings, including the multi-agent, multi-task, hierarchical, and model-based scenarios.Next, to provide a thorough review, we briefly enumerate other related works, which are further categorizedby their base algorithms. GAIL: Extensions of GAIL concentrate on tackling key challenges such as increasing the sample efficiencyfor policy learning, ensuring balanced training of the discriminator and policy, etc.(1) DGAIL (Zuoet al. (2020)) and SAM (Blond & Kalousis (2019)) replace the stochastic policy a (|s) in GAILwith a deterministic one a = (s) and, correspondingly, proposes a DDPG-based (Lillicrap et al. (2016))updating rule for . As an off-policy algorithm, DDPG is more sample efficient than TRPO (used in GAIL).BGAIL (Jeon et al. (2018)) improves the sample efficiency by approximating the posterior distribution of thediscriminators parameters in the ideal case where correct labels are assigned to both expert and generatedbehaviors. A more accurate estimation of the cost function can then be acquired through sampling from thatdistribution, consequently improving the policy training. (2) VAIL (Peng et al. (2019a)) suggests limitinginformation flow in the discriminator of GAIL (or AIRL) using the Variational Information Bottleneckframework (Tishby & Zaslavsky (2015)), to prevent the discriminator from converging too quickly and soproviding uninformative gradients to the generator. TRAIL (Zolna et al. (2020)) proposes to regularize thediscriminator through an auxiliary task, with the aim to discourage the agent to exploit spurious patterns inobservations which are associated with the expert label but task-irrelevant. (3) For applications demandingreliability and robustness, RS-GAIL (Lacotte et al. (2019)) presents a risk-sensitive version of GAIL byintroducing a constraint to the original GAIL objective, such that the conditional value-at-risk (Duffie &Pan (1997)) of the learned policy is at least as well as that of the expert. This algorithm is supported byrigorous theoretical validation. Multi-agent/Hierarchical GAIL: IGASIL (Hao et al. (2019)) extends GAIL to fully-collaborative multi-agent scenarios. It adopts a similar objective design with MAGAILs (listed in ) but suggests twomajor improvements for learning efficiency: training the policy with DDPG for enhanced sample efficiencyand adopting high-return trajectories from the policy as demonstrations for self-imitation learning. LikeOption-GAIL introduced in , OptionGAN and Directed-Info GAIL extend GAIL to the hierar-chical learning setting, taking advantage of the Mixture-of-Experts framework (Masoudnia & Ebrahimpour(2014)) and directed information maximization (Massey et al. (1990)), respectively. However, these two algo-rithms are not suitable for learning from demonstrations segmented by sub-tasks. Also, Directed-Info GAILtrains H and L in two separate stages, leading to suboptimality of the hierarchical policy = (H, L). InfoGAIL: Hausman et al. (2017) and Peng et al. (2022) address the same challenge as InfoGAIL, i.e.,IL from multi-modal but unstructured demonstrations. Hausman et al. (2017) achieve the same objectivefunction as InfoGAIL but derive it through a distinct perspective; Peng et al. (2022) proposes an alternativemanner to update Q (in InfoGAIL) through a VAE framework, where Q and serve as the encoder anddecoder, respectively. Ess-InfoGAIL (Fu et al. (2023)) extends InfoGAIL to manage demonstrations froma mixture of experts, wherein the quantity of demonstrations from each expert is imbalanced.Burn-InfoGAIL (Kuefler & Kochenderfer (2018)) improves InfoGAIL to reproduce expert behaviors over extendedtime horizons.",
  "Triple-GAIL(Triple-GAN)(Fei et al. (2020))": "min,C maxD 1H() + EE [log D(s, a, c)]+2E[log(1 D(s, a, c))]+(1 2)EC [log(1 D(s, a, c))]+4EE [ log C(c|s, a)] + 5E[ log C(c|s, a)];c can be viewed as labels for (s, a), provided indemonstrations; C(c|s, a) is utilized to classify (s, a),while (a|s, c) is the policy for the class c. The development and theoretical validationof Triple-GAIL exactly follow Triple-GAN(see .1). C and are trained bymatching the approximated joint occupancymeasures of (s, a, c): () and C(),with E (). The last two objectiveterms serve as extra supervision for C.Viewing c as options/skills, C and then constitute a hierarchical policy.",
  "MGAIL(GAN)(Baram et al. (2017))": "The objective of D is the same as the one of GAIL.(a|s; ) is trained by maximizing the returnJ(s0, a0; ), where the policy gradient, i.e., J0 ,can be approximated recursively as: (t = T 0)Jt = Ra + (Jt+1+ Jt+1sfa),Jts = Rs + Ras + Jt+1s(fs + fas);f is the learned dynamics: s = f(s, a), R(s, a)is the reward defined with D, Rs = R",
  "min maxD ENi=1 log(1 Di(s, ai))+": "EENi=1 log Di(s, ai), = (1, , N),E = (1E, , NE ). All agents share the state sbut choose their own action ai i(|s).The other agents i can be viewed as part ofthe environment for agent i. However, if iis unknown, the training environment would benon-stationary for i, as i are also being updated. A multi-agent extension of GAIL, whichlearns a Di and i for each agent.As agents interact with each other,Di and i are not learned independently.The papers theoretical results assumeiEis known, enabling the learning foreach agent i to be treated separately.Yet, in practice, better management for thenon-stationarity than MAGAIL is required.",
  "Option-GAIL(GAN)(Jing et al. (2021))": "min maxD H() + E [log(1 D(o, s, o, a))]+EE [log D(o, s, o, a)], = (H, L),o H(|s, o), a L(|s, o).The agent decides on its current option (a.k.a.,skill) o with H based on s and o (i.e., theprevious option), and then samples actions awith L subject to o. A hierarchical learning version of GAIL.Long-horizon tasks can be segmented to asequence of sub-tasks, each of which can bedone with an option o (i.e., sub-policy).This algorithm can recover a hierarchicalpolicy, useful for long-horizon tasks,from the expert data through matchingthe occupancy measure of (o, s, o, a)between and E, which follows GAIL.An EM version of Option-GAIL is proposed,in case the experts options are not labeled.",
  "MA-AIRL(GAN)(Yu et al. (2019a))": "maxDNi=1 EE [log Di(s, a)] + E [log(1 Di(s, a))],mini E [log(1 Di(s, a)) log Di(s, a)] , i = 1, , N;Di(s, a) = exp(f(s, a))/(exp(f(s, a)) + i(ai|s)),a = (a1, , aN), = (1, , N). All agents share astate s and have access to action decisions from theother agents, i.e., ai, during the training process. This is a multi-agent version of AIRL,which trains Di, i and applies AIRLto each agent. States and joint actionsare shared among agents. Theoreticalresults are based on Markov gamesand logistic stochastic best responseequilibirum. However, viewing aias part of the environment, theproblem and derivations degenerate tothe single-agent AIRL case.",
  "maxD EET 1t=0 log D(st, at|c) + E T 1t=0log(1 D(st, at|c)), (st, at) = ((ot1, st), (ot, at));min ET 1t=0 log(1 Dct ) log Dct1I(; C) 2I( O0:T |C), = {(st, at)}t=0:T 1,Dct =exp(f(st,at|c))": "exp(f(st,at|c))+(at|st,c) , = (H, L) and(at|st, c) = H(ot|st, ot1)L(at|st, ot). c and odenote the task and option variable, respectively.MH-AIRL can also be adopted, when c and oare not labeled in demonstartions, via anExpectationMaximization (EM) design. This algorithm integrates multi-tasklearning, hierarchical learning, andAIRL. is a hierarchical policy thatcan be applied to multiple tasks byconditioning on corresponding c.The objective design can be viewedas AIRL on the extended state-actionspace (st, at). As regularization, is trained to maximize the mutual/directed information I(; C)/I( O0:T |C) to build the causalrelationship between and the task/option variables.",
  ": Summary of representative works following AIRL": "(Qureshi et al. (2019)) proposes to implement h(s) as the empowerment function (Mohamed & Rezende(2015)), maximizing which would induce an intrinsic motivation for the agent to seek the states that havethe highest number of future reachable states. However, as claimed in AIRL, h(s) is designed to recover thevalue function at optimality, which is different from the empowerment function in definition. AIRL has beenextended to various IL setups, including model-based, off-policy, multi-task, and hierarchical IL. MAIRL(Sun et al. (2021)) simply replaces the GAIL objective in MGAIL (introduced in ) with the AIRLsand gets a model-based version of AIRL. Following the same intuition as DGAIL and SAM (introducedin .2.2), Off-policy-AIRL suggests adopting off-policy RL algorithms (specially, SAC (Haarnojaet al. (2018))) to train the generator in AIRL for improved sample efficiency. introduces MH-AIRL,which extends AIRL to support both multi-task and hierarchical learning. As related works, PEMIRL (Yuet al. (2019b)) and SMILe (Ghasemipour et al. (2019a)) are proposed for multi-task AIRL, while oIRL(Venuto et al. (2020)) and H-AIRL (Chen et al. (2023d)) focus on hierarchical AIRL.",
  "Integrating VAEs and GANs for Imitation Learning: A Spotlight": "Lastly, we spotlight some papers that utilize VAEs to enhance GAIL, serving as examples of integratingdifferent generative models for advanced imitation learning.GANs are recognized for their capacity togenerate sharp image samples, as opposed to the blurrier samples from VAE models.However, GANs,unlike VAEs, are susceptible to the mode collapse problem, meaning that they may only capture partialmodes in a multi-modal dataset. Therefore, the abilities of GANs and VAEs are highly complementary.As mentioned in , one approach, such as CGAIL and InfoGAIL, for solving the mode collapse issueis to train a discriminator and policy for each mode by conditioning them on the mode embedding. Suchembeddings could be acquired from a pretrained VAE. In particular, a VAE (P, P) can be trained to",
  "max, EDEEcP(|) [log P(|c)] DKL(P(c|)||PC(c))(54)": "where PC(c) is the predefined prior distribution, P(c|) and P(|c) work as the encoder and decoder of theVAE. log P(|c) can be decomposed as log (s0|c) + T 1t=0 [log (at|st, c) + log T(st+1|st, at, c)] based onthe MDP model. Through this unsupervised training process, the VAE is expected to capture the multiplemodes in the dataset and its encoder P(c|) can be employed to provide mode embeddings for the experttrajectories. Then, the mode-conditioned discriminator and policy can be trained via an objective similarwith CGAIL:",
  "(55)": "The pretrained VAE policy (a|s, c) can be used to initialize the GAIL policy (a|s, c). Through furthertraining with GAIL (i.e., Eq. (55)), the VAE policy can be refined by addressing the tendency to produceblurry imitations. Diverse GAIL (Wang et al. (2017)) and VAE-ADAIL (Lu & Tompson (2020)) arenotable examples of this paradigm. A more straightforward manner to integrate VAEs and GANs for ILis adding a regularizer min Es()DKL((a|s)||VAE(a|s)) to the GAIL objective, where VAE(a|s) is theVAE-based policy pretrained on DE (see .3.1). In this way, the GAIL policy is encouraged tobe close to the potentially multi-modal VAE policy. SAIL (Liu et al. (2020)) explores in this direction.Another significant advantage of VAEs over GANs is that they can provide semantically meaningful latentembeddings for the input data. As introduced in .2.3, such embeddings can be used as transformedtraining data to lower the difficulty of GAIL training.As an example, LAPAL (Wang et al. (2022b))proposes to adopt a CVAE to transform high-dimensional actions into low-dimensional latent vectors andapply GAIL on the latent space instead, in order to stabilize and accelerate the learning process.",
  "Policy Approximation using GANs": "As in dynamic-programming-based offline RL, to avoid OOD states and actions, the learned policy shouldbe close to the underlying behavior policy , which is typically realized through introducing a regularizationterm. Thus, the off-policy learning objective in model-based offline RL (i.e., the first objective in Eq. (8))can be improved as:maxQEsDaug,a(|s) [Q(s, a)] f(, )(56) where Q > 0 is the regularization coefficient. SGBCQ (Dong et al. (2023)) proposes to directly con-strain the action distribution of at each state to be close to the one of by implementing f(, ) asEsD [d((|s)||(|s))], where d() denotes a statistical divergence, resembling policy constraint methods(i.e., Eq. (2)). As introduced in .1, GANs can be used to model and minimize the JS divergencebetween two data distributions. Thus, SGBCQ gives out a practical framework to solve Eq. (56) based ona CGAN as below:",
  "minG QEsDaug,zPZ() [Q(s, G(z|s))] + EsD,zPZ()[log(1 D(G(z|s))](57)": "Compared with the CGAN objective (i.e., Eq. (44)), s and a here work as the conditional information y anddata point x, respectively. Intuitively, the stochastic policy a (|s) is implemented as a generator a =G(z|s), z PZ() and the generator is trained to maximize the expected Q-values and fool the discriminator",
  "D at the same time. Involving such a GAN training process encourages the policy to generate behaviorsclose to the demonstrated ones in distribution": "SDM-GAN (Yang et al. (2022b)) follows a similar protocol but chooses to regularize the stationary (s, a)distribution induced by to be close to the (s, a) distribution in D to avoid OOD behaviors. Specifically,they define f(, ) in Eq. (57) as d(T ()||T()), where T (s, a) can be approximated as the empiricaldistribution of (s, a) in D, T(s, a) = limT 1 T +1Tt=0 P(st = s, at = a|s0 0(), at (|st), st+1 T (|st, at)) is the stationary (s, a) distribution of under the real dynamic T . Assuming the dynamicfunction T is well-fitted (through the process shown in .1.2), d(T ()||T()) is upper bounded by:(Please refer to (Yang et al. (2022b)) for the derivation and definitions of the function classes G1,2.)",
  "minG QEsDaug,zPZ() [Q(s, G(z|s))] + E(s,a)DG[log(1 D(s, a)](59)": "Here, inspired by Eq. (58), DG contains (s, a)/(s, a) pairs collected in this way: s D, a (|s), s T (|s, a), a (|s), where a (|s) is implemented as a = G(z|s), z PZ(). The only differencebetween Eq. (57) and (59) is thus the way to generate samples. For Eq. (57), the (s, a) pairs are simplygenerated by s D, a (|s). The same authors have proposed a model-free variant of this algorithm(Yang et al. (2022c)), sharing similar motivations and designs. AMPL (Yang et al. (2022d)) adopts the same objective (i.e., Eq. (59)) for training , but proposes thatthe environment models T and r can be periodically updated by collecting new data with and applyingsupervised learning (e.g., with Eq.(7)) to update T and r.Interestingly, they theoretically show thatthe objective for , which combines the Q-function and distribution discrepancy, approximates an upperbound for J(, M), i.e, the negative expected return of the policy on the real MDP. On the other hand,DASCO (Vuong et al. (2022)) proposes that the two terms in the objective of may conflict with eachother, since fooling the discriminator requires mimicking all in-distribution actions, which can be suboptimal,but maximizing the Q function would mean avoiding low-return behaviors. Thus, they propose to introducean auxiliary generator Gaux to generate low-return samples and modify the objective as follows:",
  "minG,Gaux maxD E(s,a)D[log D(s, a)] + E(s,a)DG,Gaux[log(1 D(s, a))] QEsDaug,zPZ() [Q(s, G(z|s))] (60)": "Here, samples in DG,Gaux are generated through: s D, z PZ(), a = (G(z|s)+Gaux(z|s))/2. Both G andGaux are adopted to mimic the demonstrations, but only the primary generator G is trained to maximizethe Q-values. Theoretically, they prove that the optimal solution for the primary generator G will maximizethe probability mass of in-support samples that maximize the Q-function. All aforementioned algorithms utilize a very similar objective design, which can be viewed as a paradigmof using GANs for policy approximations in (model-based) offline RL. GOPlan (Wang et al.(2023a)) proposes an advantage-weighted CGAN objective, which is different in form from previous ones, forcapturing the multi-modal action distribution in goal-conditioned offline planning:",
  "minG maxD EgDE(s,a)Dg[w(s, a, g) log D(s, a|g)] + EsDg,zPZ()[log(1 D(s, G(z|s)|g))](61)": "Here, Dg denotes the partition of D that takes g as the goal; w(s, a, g) = exp(A(s, a, g)) denotes the weightfunction, where A(s, a, g) is a separately trained advantage function. This mechanism encourages the policyto produce actions that closely resemble high-quality (i.e., high-advantage) actions from the offline dataset,but no theoretical guarantee is provided. Although algorithms introduced in this category are proposed formodel-based offline RL, these methods can be transferred to the model-free setting seamlessly.",
  "World Model Approximation using GANs": "Beyond policy modeling, GANs can also be used to approximate the environment model M, as illustrated inMOAN (Yang et al. (2023a)), TS (Hepburn & Montana (2022)), S2P (Cho et al. (2022)), which is expectedto outperform traditional supervised learning methods (i.e., using Eq. (7)). To be specific, MOAN proposesthe following objective:",
  "minG NE(s,a,r,s)D [ log G(r, s|s, a)] + E(s,a)D,(r,s)G(|s,a)[log(1 D(s, a, r, s))](62)": "This design is similar with Eq. (59) in form, but replaces the Q-function term with a negative log-likelihood(i.e., log G(r, s|s, a)) that is commonly used for environment model approximation. In this CGAN frame-work, the conditional generator G(|s, a) is learned to predict s and r, working as the approximated rewardr(s, a) and transition function T (s|s, a). In particular, the generator G(|s, a) = N(mean(s, a), std(s, a)) isimplemented as a stochastic Gaussian model, and the standard deviation of the predictions, i.e., std(s, a),can be used as uncertainty measure (i.e., u(s, a) in .1.2). MOAN proposes to further include theconfidence level given by the discriminator, i.e., u(s, a) = std(s, a) + 2(1 D(r,s)G(|s,a)(s, a, r, s)). In-tuitively, if the variance of the estimation is high or the discriminator classifies the prediction as generateddata, i.e., D 0, the uncertainty level at this point (s, a) should be high. However, this uncertainty measuredesign is not backed up by theoretical analysis. TS proposes to learn environment models for stitching high-value segments from different trajectories toform higher-quality trajectories for offline RL. To realize this, they model the distribution P(a, r, s|s) =P(s|s)P(a|s, s)P(r|a, s, s). That is, they search for a next state s from the neighbourhood of s, whichcan come from a different trajectory and should have a higher probability P(s|s) & value V (s) than theoriginal next state of s. Then, they identify the most probable intermidiate a, r. In particular, they learnP(s|s), P(a|s, s), P(r|a, s, s) with the traditional supervised learning, CVAE, and WGAN (see .1), respectively. The WGAN objective is: (G(z|s, a, s) works as the approximation of P(r|a, s, s).)",
  "minG maxD E(s,a,s,r)D[D(s, a, s, r)] E(s,a,s)D,zPZ()[D(s, a, s, G(z|s, a, s))](63)": "This algorithm can be viewed as a model-based data augmentation method for offline RL using GANs.S2P, on the other hand, is a model learning approach for vision-based offline RL. It first approximates thetransition and reward model (i.e., T(s|s, a), r(s, a)) for the underlying states through traditional supervisedlearning, and then learns a generator to synthesize the image I/I that perfectly represents the correspondingstate s/s. The generator is conditioned on both the current state and previous image, i.e., I = G(z|s, I),and trained within a WGAN framework. Here, the GAN is used for data transformation to simplify the(vision-based) environment model approximation. In , we provide a summary of GAN-based offline RL algorithms. GANs can be employed for bothpolicy and world model approximations in model-based offline RL, primarily due to their ability to match adistribution (hidden in the offline data) by minimizing some statistical discrepancy.",
  "Background on Normalizing Flows": "Normalizing Flows (NFs, Kobyzev et al. (2021)) allow for generating data samples following complex distri-butions and exact density estimation of given data points, which are essential in probabilistic modeling tasks.Formally, NFs transform a simple, known probability distribution in the latent space (i.e., z PZ()) to acomplex, desired data distribution (i.e., x PX()) through a sequence of invertible and differentiable",
  "S2PWGANEnvironment Modeldm_control": ": Summary of GAN-based offline RL algorithms. Most algorithms in this category have been evaluatedon D4RL (Fu et al. (2020)), which provides multiple datasets for data-driven RL tasks, including Locomotion(L), AntMaze (M), Adroit (A), etc. MuJoCo Robotic Manipulation (Yang et al. (2023b)) provides offlinedatasets for a series of goal-conditioned robotic manipulation tasks, several of which can be utilized to assessthe OOD generalization capabilities of the policy. dm_control (Tassa et al. (2018)) includes 6 environments,which are typically used for evaluating vision-based RL algorithms. transformations. The mathematical representation of the transformation from x to z is given by a series ofbijections: F = FN FN1 F1. The data flow [x0, x1, , xN] (x0 = x, xN = z) within NFs adheresto the following process: for all 1 i N, xi = Fi(xi1), xi1 = F 1i(xi). The core principle behind thetransformation is the change of variable formula. Specifically, the probability density functions (PDF) of xand z are related as:",
  "PZ(z) = PX(F 1(z))| det D(F 1(z))|, PX(x) = PZ(F(x))| det D(F(x))|(64)": "where det D(F(x)) denotes the determinant of the Jacobian matrix of F(x).Given the transformationfunction F and the basic distribution PZ, the density of a data sample x can be exactly acquired as PX(x)defined in Eq. (64). Thus, the ability in exact density estimation of NFs relies on the fact that F is invertibleand differentiable. Conversely, the data generation can be done by first sampling a latent z PZ() and thenapply the generator function G = F 1. As for training, it is usually through maximizing the log-likelihoodof the target data distribution (Dinh et al. (2017)):",
  "maxFEX[log(PX(x))], log(PX(x)) = log(PZ(F(x))) + log(| det D(F(x))|)(65)": "For NFs, the generator G should be sufficiently expressive to accurately model the distribution of interest,while the transformation F should be invertible and the computation of the two directions mentioned above(i.e., data generation and density estimation), particularly the determinant calculation, must be efficient.Based on these requirements, various types of flows have been developed. Here, we briefly present somerepresentative variants. As mentioned above, G is composed of a series of generator modules Gi = F 1i. Foreach variant, we introduce its specific design of Gi, and we use m and n to denote the input and output of Gi,respectively. Note that we do not present complex mathematical details regarding calculations or objectivedesigns, as they are not essential for understanding the applications of NFs in offline policy learning, whichare introduced in .2 and 5.3. Coupling Flows enable highly expressive transformations via a coupling method. In particular, theinput m RD is partitioned into two subspaces: (mA, mB) Rd RDd, and the generator moduleGi is defined in the format: Gi(m) = (H(mA; (mB)), mB). Here, H(; ) : Rd Rd is a bijection,and is the parameter function. H of such form is also called the coupling function. In this case,the Jacobian of Gi is simply the Jacobian of H, and for efficient computation of determinants, His usually designed to be an element-wise bijection, i.e., H(mA; ) = (H1(mA1 ; 1), , Hd(mAd ; d))where each Hi(; i) : R R is a scalar bijection. The expressiveness of a coupling flow lies in thecomplexity of its parameter function , which, in practice, is usually modeled as neural networks.Algorithms in this category include NICE (Dinh et al. (2015)), RealNVP (Dinh et al. (2017)), Glow(Kingma & Dhariwal (2018)), NSF (Durkan et al. (2019)), Flow++ (Ho et al. (2019)), etc.",
  "Normalizing Flows in Imitation Learning": "NF-based IL algorithms either employ NFs for exact density estimation or leverage NFs proficiency in mod-elling complex policies to manage challenging task scenarios, which correspond to the normalizing direction(i.e., x z) and the generation direction (i.e., z x) of NFs respectively. Next, we present a comprehensivereview of works from both categories.",
  "Exact Density Estimation with Normalizing Flows": "As a paradigm, IL methods that adopt NFs as density estimators usually start with standard IL objectives,and optimize them as RL problems wherein the rewards necessitate estimation of specific distributions. Inthis case, NFs are introduced to model those distributions and provide exact density inference for rewardcalculation, which greatly enhance the training stability and performance. To be specific, we present somerepresentative works here. As proposed in (Ghasemipour et al. (2019b)), most IL methods can be viewed as matching the agentsstate-action distribution with the experts, by minimizing some f-divergence Df. As an example, CFIL(Freund et al. (2023)) chooses to minimize the KL divergence between the agents and experts state-actionoccupancy measure (as defined in .2.1):",
  ")(66)": "where J(, r) denotes the expected return (Sutton & Barto (2018)) of the policy under the reward functionr. An MAF (Masked Autoregressive Flow, Papamakarios et al. (2017)) is used to model the distributionsE(s, a) and (s, a) based on which the rewards can be acquired.Specifically, (s, a) can be viewed asx in the NF framework and the densities at (s, a), i.e., E(s, a) and (s, a), can be estimated using Eq.(64). However, rather than optimizing these two NFs independently with corresponding MLE objectives(i.e., Eq. (65)), they couple the modelling of these two distributions based on the optimality point of theDonsker-Varadhan form of the KL divergence (Donsker & Varadhan (1976)):",
  "In the equation above, optimality occurs at f (s, a) = log (s,a)": "E(s,a)+C where C R. Thus, through maximizingthe right-hand side of the equation above, the log ratio used as the reward function can be recovered.Specifically, they model f as f,(s, a) = log (s, a) log E(s, a) with the two flows and E. Given thisimproved estimator, the overall IL objective can be written as: (which combines Eq. (66) and (67))",
  "(s, a) DE) and rollout data collected by (i.e., (s, a) ())": "IL-flOw (Chang et al. (2022)) focuses on Learning from Observations (LfO), where the agent only getsaccess to a dataset of state sequences.This work also starts with the KL divergence but processes todecouple the policy optimization from the reward learning to avoid minimax optimization as in Eq. (68)and improve the training stability. In particular, the learning objective for the policy is to maximizeDKL(P(s|s)||PE(s|s)) which equals to:",
  "(69)": "The equality holds when environment dynamics are deterministic and invertible. The right hand side of Eq.(69) can be optimized with RL by setting the reward as rt = log PE(st+1|st) while maximizing the entropy ofthe policy, i.e., H((|st)). As a separate stage, before the RL training, PE(s|s) is modeled with a conditionalNF (i.e., conditioning the transformation function F on a fixed state variable s: s = F(z|s), z PZ()) basedon the demonstrations DE. Specifically, they select NSF (Durkan et al. (2019)) as the density estimator. SOIL-TDM (Boborzi et al. (2021b; 2022)) also focuses on LfO and starts with the same objective as IL-flOw.However, SOIL-TDM gets rid of the requirements for deterministic and invertible dynamics by estimating",
  "where r(st, at) = Est+1T(|st,at) log T(st+1|st, at) + log (at|st+1, st) + log PE(st+1|st). In this case,": "besides the expert state transition model PE(st+1|st), they adopt conditional NFs to model the agentstransition dynamics T(st+1|st, at) and the posterior distribution associated with , i.e., (at|st+1, st), forwhich they choose Real NVP (Dinh et al. (2017)). Note that the approximation of PE(st+1|st) is trainedon DE, while approximations of T(st+1|st, at) and (at|st+1, st) are trained on rollout data collected by during the RL process.",
  "maxE(s,a)DE [log (a|s)] + Es [log PE(s)] s.t. PE = arg maxPEsDE [log P(s)](72)": "where (s) denotes the occupancy measure of s induced by the policy . The training process can thenbe divided into two stages. At stage 1, they train an NF to model the expert state distribution PE(s)based on the demonstration set DE, which is, at stage 2, adopted to optimize the policy (a|s) using Eq.(72). Intuitively, this objective combines Behavioral Cloning (i.e., max E(s,a)DE [log (a|s)]) and RL withlog PE(s) as the reward (i.e., max Es [log PE(s)]). It is worthy noting that they utilize a ContinuousNormalizing Flow DCNF (Zhang et al. (2021b)) as density estimators, due to the enhanced modelingcapabilities and fewer model restrictions that Continuous Normalizing Flows offer Grathwohl et al. (2018).",
  "max E(s,a)DE [log P(s, a)] = max E(s,a)DE [log (a|s) + log P(s)](73)": "Here, the first term of the right-hand side is simply Behavioral Cloning.While, for the secondterm, according to (Ross & Bagnell (2010)), the gradient from it can be estimated as log P(s) E(s,a)B (|s) [ log (a|s)]. B(|s) models the distribution of states and actions that, following thecurrent policy , will eventually reach the given state s. That is, B(s, a|s) = (1 ) j=0 jP(st =s, at = a|st+j+1 = s). In GPRIL, B(s, a|s) is modeled as B1 (s|s) B2 (a|s, s), i.e., the product oftwo density functions each of which is modelled by a conditional MAF (Papamakarios et al. (2017)). To trainthese models, they collect training data using self-supervised roll-outs. In particular, they sample states,actions and target-states (i.e., (s, a, s)), where the separation in time between the s and s is selectedrandomly from a geometric distribution parameterized by . In this way, the collected triplets theoreticallysatisfy: (s, a) B(|s). As a subsequent research, the work (Schroecker & Jr. (2020)) utilizes the sameobjective function, but introduces an alternative method for estimating log P(s) from a goal-conditionedRL perspective. However, this algorithm is still in an early stage of development.",
  "Policy Approximation using Normalizing Flows": "As a potent generative model, Normalizing Flows can be directly adopted as policy networks for tacklingchallenging tasks. In this regard, recent works in off-policy RL have demonstrated that NFs can be usedto model continuous policies (Haarnoja et al. (2018); Ward et al. (2019); Mazoure et al. (2019)), which canlead to faster convergence and higher returns by enhancing exploration and supporting multi-modal action",
  "FlowPlanNAFARG(Trajectory Planner)KLHES-4D": ": Summary of NF-based IL algorithms.AR, CP, and ODE represent autoregressive, coupling,and ODE-based continuous flows, respectively.DE and G represent the two manners that NFs can beused, corresponding to the density estimator and generator, respectively.For DE, we specifically pointout the densities estimated, for which the definitions are available in .2.1. Regarding the typesof IL objectives, KL, LIL, and AIL denote the KL divergence, Likelihood-based IL, and Adversarial IL,respectively. Examples for AIL include GAIL and AIRL, which are introduced in .2.1. For thebenchmarks, MuJoCo (Todorov et al. (2012)) provides a series of robotic locomotion tasks; Robotic Insertion(Vecerk et al. (2017)) is a specific type of robotic manipulation task built on the MuJoCo engine; High-D(Krajewski et al. (2018)) and HES-4D (Meyer et al. (2019)) are two real-word driving dataset.",
  "distributions. Compared to commonly-employed diagonal Gaussian policies, where each action dimension isindependent of the others, those based on NFs offer greater expressiveness": "In the context of IL, Flow DAC (Boborzi et al. (2021a)) extends DAC (an off-policy IL algorithm Kostrikovet al. (2019)) by adopting a conditional version of Real NVP as the policy network. Specifically, the action aat state s is generated by: z PZ(), a = G(z|s) where G denotes the NF generator and s is the conditioner.The stochasticity of this policy comes from the latent distribution PZ(), and the action output of G canbe in any complex distribution. Similarly, FlowPlan (Agarwal et al. (2020)) adopts NFs as a trajectorygenerator/planner through sequentially generating the next state embedding based on historical states, andan NAF (Huang et al. (2018)) is adopted as the generative model, which is trained by minimizing the KLdivergence between the experts and generated trajectories. In , we provide a summary of NF-based IL algorithms. For each algorithm, we provide key informationincluding what type of NF and how the NF is utilized, its underlying IL framework, and evaluation tasks.",
  "Normalizing Flows for Reinforcement Learning with Offline Data": "As mentioned in the beginning of , we broaden the scope of RL-related works to include both offlineRL (.3.1) and online RL with offline data (.3.2). We believe that the approaches developedfor using offline data in online RL could be effectively adapted to enhance offline RL. Similar with NF-basedIL methods, algorithms in .3.1 and 5.3.2 can be categorized based on the usage of NFs, i.e., workingas either a density estimator or a function generator.",
  "k+1 = arg maxEsDEa(|s)Qk+1(s, a)s.t. PZ(G1(a|s)) > (s, a).(74)": "where (s, a) is a defined threshold. In this way, samples for updating the policy are restricted within thesafe area, which could contain both observed and unobserved points in the offline dataset. Thus, the policycan potentially perform exploration out of the given dataset and avoid visiting OOD actions, simultaneously.Note that this constraint on is more relaxed and practical compared with the one of BEAR (introduced in.1.1): { : S A | (a|s) = 0 whenever (a|s) < }, to avoid being over-conservative.",
  "Adopting Normalizing Flows in Online Reinforcement Learning with Offline Data": "To deepen the understanding of applying NFs to RL, we incorporate another line of works on online RLutilizing offline data. Still, these works can be categorized based on the usage of NFs: potent generators(Mazoure et al. (2019); Yan et al. (2022); Slack et al. (2022)) or density estimators (Wu et al. (2021)). With the same insights as Flow DAC (introduced in .2.2), SAC-NF (Mazoure et al. (2019))extends SAC, an off-policy RL algorithm (Haarnoja et al. (2018)), by adopting NFs (IAF (Kingma et al.(2016))) as the policy network, which could potentially improve exploration and support multi-modal actiondistributions. On the other hand, CEIP (Yan et al. (2022)) and SAFER (Slack et al. (2022)) leverage theoffline data by first learning a transformation from latent variables z to actions a using NFs as an action",
  "NF ShapingMAFARDE (PG(s, a))Robotic Insertion, Pick Place": ": Summary of NF-based (Offline) RL algorithms. AR and CP represent autoregressive and couplingflows, respectively. DE and G represent the two manners that NFs can be used, corresponding to the densityestimator and generator, respectively. Specifically, PG(s, a) denotes the state-action pair distribution in theoffline dataset. For the benchmarks, all the evaluation tasks shown in this table are built on the MuJoCo(Todorov et al. (2012)) engine; D4RL (Fu et al. (2020)) provides a bunch of challenging (robotic) tasksspecifically for offline RL, including Locomotion (L), AntMaze (M), Adroit (A), Kitchen (K); Office (Pertschet al. (2021)), FetchReach (Plappert et al. (2018)), Kitchen (from D4RL), Robotic Insertion, Pick Place(Vecerk et al. (2017)), and Operation (Slack et al. (2022)) all involve the manipulation of a robot arm. decoder and then training a high-level policy high(z|s) atop the transformation function using online RL,akin to CNF. Yet, in comparison to CNF, they extend the action decoder learning from different perspectives.In CEIP, the generator is conditioned on a concatenation of the current and next states, i.e., u = [s, s],instead of the state alone, which can better inform the agent of the state it should try to achieve with itscurrent action. Moreover, they propose a manner to utilize data from various yet related tasks, which aremore accessible than task-specific ones. Specifically, they first train a flow for each task i with correspondingdemonstrations. The generator is defined as a = Gi(z|u) = exp{ci(u)} z + di(u) where ci and di aretrainable deep nets, refers to the Hadamard product. This structure design follows Real NVP (Dinhet al. (2017)). Subsequently, they train a combination flow on demonstrations of the target task, with thegenerator defined as G(z|u) = (ni=1 i(u) exp{ci(u)}) z + (ni=1 i(u)di(u)). At this stage, they fix ciand di (i = 1, , n) and train the weighting functions i and i to optimize the combination flow for thedesignated task via supervised learning (i.e., Eq. (65)). In this way, they efficiently utilize demonstrationsfrom related tasks. While, SAFER emphasizes the safety aspect of the learned transformation. Specifically,the transformation should prevent selecting unsafe actions aunsafe in the environment, which are labeled inthe training dataset. To achieve this, the authors propose conditioning the flow on both the state s anda safety context c. Its important to note that c is not provided directly but must be inferred from theinformation available in the environment. Thus, SAFER simultaneously learns (1) an inference networkP(c|) to determine the current safety context c based on a sliding window of states and (2) a generatora = G(z|s, c) (Real NVP) conditioned on s and c, with the following objective:",
  "max, Es,a,aunsafeD, cP(|) [log P(a|s, c) log P(aunsafe|s, c)] DKL(P(c|)||PC(c))(75)": "Here, P(a|s, c) = PZ(G1 (a|s, c)), PC(c) denotes the assumed prior distribution of c. Intuitively, the firsttwo terms encourage safe actions while deterring unsafe ones, and, in conjunction with the final term, thevariable c is compelled to encapsulate useful information regarding safety.This algorithm serves as anillustration of how NFs and VAEs can be integrated. In this setup, P and P function as the encoder anddecoder of the VAE, respectively, with P being implemented as a Normalizing Flow. Unlike SAC-NF, CEIP, and SAFER, NF Shaping (Wu et al. (2021)) adopts NFs as density estimators.This is a method that combines (online) reinforcement learning and imitation learning by shaping thereward function (for RL) with a potential term learned from demonstrations.To be specific, they firsttrain an NF (specifically MAF Papamakarios et al. (2017)) to estimate the density of state-action pairsin the demonstration dataset, i.e., PG(s, a) = PZ(G1(a|s)).Then, they shape the reward function asrt = r(st, at, st+1) + (st+1, at+1) (st, at), where (s, a) = log(PG(s, a) + ), > 0 adjusts the weight,and > 0 is a small constant to prevent numerical issues. According to (Ng et al. (1999); Wiewiora (2003)),reward shaping can intensify the reward and greatly improve the learning efficiency. Intuitively, this potential",
  "term highlights regions where the demonstrated policy visits more frequently, signaling areas that should beexplored more": "To sum up, we note that the algorithms proposed in (Akimov et al. (2022); Yang et al. (2023d); Yan et al.(2022); Slack et al. (2022)) adopt similar ideas. They first learn an NF-based transformation from the latentspace to the real action space based on the offline data, and then train a high-level policy over the latentvariables with online or offline RL. The prelearned transformation provides a mapping from a simpler, morecontrollable distribution to the target distribution, which is usually complex and unknown. Also, we findthat Normalizing Flows are primarily used as exact density estimators in IL (.2), while mainlyfunctioning as expressive generators in (Offline) RL (.3). From and 7, we can observe thatmost works adopt autoregressive or coupling flows, which is probably due to their computation efficiency.As introduced in .1, continuous flows can potentially be more expressive in modelling complex datadistribution. Integrating these types of flows with challenging offline policy learning tasks, such as vision-based ones, could be a future direction. On the other hand, while almost all algorithms are evaluated onrobotic benchmarks such as MuJoCo or D4RL, there is a notable absence of comparative analysis amongthese algorithms. Given the diverse perspectives from which these algorithms are developed, it is challengingto compare them at the algorithmic level, thus comprehensive and fair empirical evaluations are essential todetermine their effectiveness and suitability for practical applications.",
  "Transformers in Offline Policy Learning": "Transformers can be used for offline RL (.2) or IL (.3) by modeling them as a problemof predicting the next token based on historical data. Notably, most works in this section follow a similaralgorithm design with Decision Transformer a seminal work of trajectory-optimization-based offline RL(.1.3). Due to their architectural design, transformers are particularly well-suited and effective formodeling time-series data such as decision trajectories, as introduced in .1.",
  "Background on Transformers": "Transformer (Vaswani et al. (2017)) is an important foundation model that has shown exceptional capabilityacross various areas, such as natural language processing (Kalyan et al. (2021)), computer vision (Han et al.(2022)), time series analysis (Wen et al. (2023)), and so on. Recently, there has been a growing trend indeveloping IL and offline RL algorithms based on transformers, with the hope to achieve sequential decision-making based on next-token predictions as in natural language processing.As shown in , thetransformer follows the widely-adopted encoder-decoder structure. The encoder maps the input (x1, , xn)to a sequence of embeddings (z1, , zn), and the decoder generates the output sequence autoregressively.That is, the decoder predicts one token ym+1 at a time, based on the input embeddings (z1, , zn) andpreviously-generated outputs (y1, , ym). Both the encoder and decoder are composed of a stack of Lidentical modules. For clarity, we present each layer in the module sequentially, following the data flow. Positional Encoding: After the token embedding layer, which is shared by (x1, , xn) and(y1, , ym), each element is converted to a d-dim vector. To enable the model to make use of theorder of tokens, information about the relative or absolute positions of tokens within the sequenceis injected through a positional encoding. For the i-th token, its positional encoding is also a d-dimvector, of which the j-th dimension is sin(i",
  "10000j/d ) if j is even and cos(i": "10000(j1)/d ) otherwise. Thepositional encoding and token embedding are then combined through summation. Per Vaswani et al.(2017), this (periodic-function-based) positional encoding design embeds relative position informa-tion and enables the model to manage sequences longer than those experienced in training. Self Multi-Head Attention (MHA): This component utilizes the attention mechanism. Givens queries and t key-value pairs, the attention function maps each query to a weighted-sum of thevalues, where the weight assigned to each value is computed as the compatibility of the query withthe key paired with that value. The queries, keys, and values can be represented as Q Rsdq,K Rtdk, and V Rtdv, respectively, and the outputs for all queries (i.e., O) can be computed",
  "V(76)": "Here, SoftMax is a row operator, and the output for query i, i.e., Oi, is a weighted sum of therows of V , where the weight for row j is propotional to the similarity of Qi (i.e., the i-th rowof Q) and Kj measured by their inner product Qi, Kj. The dot products grow large with dk,which would push the SoftMax function to regions with vanished gradients, so the factor 1/dkis introduced. The input embeddings Hx (in ) can potentially be used as the matrices Q,K, and V for self attention. However, to enable the model to jointly attend to information fromdifferent representation subspaces, a multi-head attention mechanism is adopted:",
  "MHA(Q, K, V ) = Concat(head1, , headh)W O, headi = Attention(QW Qi , KW Ki , V W Vi )(77)": "where Q = K = V = Hx Rnd, W Qi Rdd, W Ki Rdd, W Vi Rdd, W Oi Rdd, d = d/h,and Concat represents the concatenation operation. W Qi , W Ki , W Viconvert Hx to matrices in thei-th attention head, providing a distinct representation subspace. Its worthy noting that the outputof MHA belongs to Rnd, that is, the same in shape as its input. Add & Normalize: A residual connection (He et al. (2016)) is employed around each attentionlayer, followed by a layer normalization (Ba et al. (2016)). As common practice, these two operationsare adopted to stabilize training of (very) deep networks (by alleviating ill-posed gradients and modeldegeneration). Suppose the previous layer is F, which can be an MHA or Feed-forward Network asshown in , and its input is X, then the calculation of this Add & Normalize layer can bedenoted as LayerNorm(F(X) + X). Point-wise Feed-forward Network (FFN): FFN layers are important for a transformer toachieve good performance. Dong et al. (2021) observe that simply stacking MHA modules causes arank collapse problem (i.e., leading to token-uniformity), and FFN is an important building block tomitigate this issue. Specifically, a two-layer fully-connected network with a ReLU activation func-tion in the middle is applied to each of the n token embeddings separately, leading to n new d-dimembeddings as output. Masked Self MHA & Cross MHA: For the decoder, Hx is replaced by Hy Rmd, i.e.,embeddings of previously-generated outputs. For rationality, the query (of the Masked Self MHA)at each position is only allowed to attend with positions up to and including that position, as theother queries correspond to outputs yet to generate. This is realized within the Masked Self MHAby masking out corresponding compatibility values, i.e., Qi, Kj = , j > i (in Eq. (76)). Asfor the Cross MHA, its queries come from the previous decoder layer and key-value pairs are fromthe output of the encoder, i.e., Hz Rnd in . In this way, every query for predictingthe next token can attend over all positions in the input sequence (via the Cross MHA) and allpreviously-generated tokens (via the Masked Self MHA). Each encoder module (listed above) outputs an n d matrix, and each decoder module outputs an m dmatrix. Here, n and m denote the counts of elements in the input and output sequences, respectively, while drepresents the embedding dimension. This uniformity in matrix sizes enables the stacking of multiple encoder(or decoder) modules to create a deep model. Each encoder (or decoder) module will take the output from theprevious encoder (or decoder) module. Generally, the transformer architecture can be used in three differentways: encoder-only, decoder-only, or encoder-decoder.When using only the encoder, its outputserves as a representation of the input sequence, suitable for natural language understanding tasks such astext classification. While, if only the decoder is employed, the Cross MHA modules are removed and thisarchitecture is suitable for sequence generation tasks like language modeling. Applications of transformers inIL or offline RL usually adopt this decoder-only way. The overall encoderdecoder architecture is equippedwith the ability to perform both natural language understanding and generation, typically used in sequence-to-sequence modeling (e.g., neural machine translation).",
  ": The Transformer Architecture": "As a foundation model, the transformer architecture hassignificant advantages. Compared to fully-connectedlayers, the transformer is more flexible in handlingvariable-length inputs (i.e., with different n) and moreparameter-efficient, since the amount of network pa-rameters in a transformer is irrelevant to the sequencelength n 16. The convolutional layer uses a convo-lution kernel of size k < n to connect pairs of tokens,so, to link distant token pairs, a stack of convolutionallayers is required. However, with the attention mecha-nism, each pair of tokens can be connected in one MHAlayer and the time complexity for matching their cor-responding query and key is constant (i.e., irrelevantto n). This makes the transformer excel at capturinglong-range dependencies within a sequence. Recurrentneural networks produce a sequence of hidden stateshi, each of which is dependent on the previous hiddenstate hi1 and the input at position i. This sequentialnature makes parallel computation impossible.How-ever, in transformers, computations at each position areindependent, allowing for parallel execution that are es-sential for processing long sequences. Last, both convo-lutional and recurrent models make structural assump-tions over the inputs, suitable for image-like and timeseries data, respectively. However, the transformer has few prior assumptions on the data structure and thusis a more universal model. Empirical results have shown that the transformer has superior performance ona wide-range of tasks and a larger capacity to handle a huge amount of training data. As listed above, the transformer contains multiple types of modules.Numerous studies have exploredmodifications or replacements of these modules in the standard transformer architecture for improvements.For a comprehensive review, please refer to (Lin et al. (2022c)). On the other hand, the computation andmemory complexity of MHA modules are quadratic to the length of the input sequence (i.e., n), leadingto inefficiency at processing extremely long sequences. Various extensions have been developed to enhanceeither the computational or memory efficiency. Additionally, due to the minimal structural assumptionsmade by the transformer about the input data, transformers are prone to overfitting when trained on small-scale datasets. Attempts like pretraining the transformer on large-scale unlabeled data (Brown et al. (2020)),introducing sparsity assumptions (Child et al. (2019)) such as limiting the number and positions of key-valuepairs that each query can attend to, have been made to solve these issues. Finally, extensions to adapt thetransformer for particular downstream applications are possible. Studies detailed in .2 and 6.3 canbe viewed as examples of this type of extensions.",
  "Transformers in Offline Reinforcement Learning": "The content of this section is arranged as follows. First, we highlight pioneering studies on transformer-basedoffline RL in .2.1, along with a series of follow-up works for improvements in .2.2 and 6.2.3.Then, we explore the use of transformers in extended problem setups in .2.4, such as multi-agentand multi-task offline RL. In particular, for the multi-task setting, we focus on developing generalist agentsbased on the pretraining & fine-tuning scheme. Finally, we reflect on existing problems in this field anddiscuss potential future research directions in .2.5.",
  "Representative Works": "Inspired by the great success of high-capacity sequence generation models (such as the transformer) in naturallanguage processing (NLP), Chen et al. (2021b) propose to view offline RL as a sequence modeling problemand solve it with a Decision Transformer (DT). This algorithm exemplifies trajectory-optimization-basedoffline RL (introduced in .1.3), so we provide a detailed introduction of it, along with two otherseminal works, as a brief tutorial on this particular branch of offline RL methods. Given offline trajectories{ = (s0, a0, R0, , sT , aT , RT )}, where Rt = Ti=t ri denotes the return-to-go (RTG), the DT is trainedto predict the next action based on the previous k + 1 transitions:",
  "(78)": "where tk:t = (sj, aj, Rj, , st, Rt) (j = min(t k, 0)) is the input sequence; the two terms above arefor tasks with continuous and discrete actions, respectively. DT adopts a GPT-like architecture (Radfordet al. (2018)), which is a decoder-only transformer as introduced in .1. During evaluation rollouts,an initial target return R0 must be specified, which can be the highest achievable return for the task. Theinference trajectory is then generated autoregressively using the DT as follows: s0 0(), a0 (|s0, R0),s1 T (|s0, a0), r0 = r(s0, a0), , Rt = Rt1 rt1, at (|tk:t), , where (0, T , r) are componentsof the MDP. DT greatly simplifies offline RL by eliminating the necessity to fit value functions throughdynamic programming or compute policy gradients as in classical offline RL. Using such a (supervised-learning based) sequence modeling objective makes it less prone to selecting OOD actions, because multiplestate and action anchors throughout the trajectory prevent the learned policy from deviating too far fromthe behavior policy (a|s). Additionally, as a high-capacity model, the transformer has the potential toenhance both the scalability and robustness of the learned policy, similar to its applications in NLP. PerChen et al. (2021b), DT shows competitive performance compared with prior offline RL methods, such asConservative Q-Learning (CQL, Kumar et al. (2020)), especially for tasks with sparse and delayed rewardfunctions. However, Emmons et al. (2022) investigate what is essential for offline RL via supervised learning(RvS) through extensive experiments. Their findings indicate that a two-layer MLP model, utilizing a simplerinput format (i.e., (st, Rt) instead of tk:t) and the same objective (i.e., Eq. (78)), exhibits competitive, andin some cases superior, performance compared to DT, and DT underperforms prior offline RL algorithmson most benchmark tasks. This raises the questions whether it is necessary to adopt such a high-capacitymodel, i.e., the transformer, as the policy network, and under what scenarios RvS methods might outperformdynamic-programming-based offline RL methods. Trajectory Transformer (TT, Janner et al. (2021)) follows DT but utilizes more techniques from NLP,including tokenization, discretization, and beam search. In particular, they treat each dimension of the stateand action as a token and discretize them independently. Suppose the state and action have N and Mdimensions respectively, the objective is min E [L()], where L() is defined as follows:",
  "(79)": "This objective involves supervision on each dimension of the state/action, the reward, and the return-to-go.For the inference process, a beam search technique (Freitag & Al-Onaizan (2017)) is utilized. Specifically, Bmost-likely samples are kept when sampling ajt and Rt. Then, samples (a1:Mt, Rt) with the highest cumulativereward plus return-to-go (i.e, the estimated trajectory return t1i=0 ri + Rt) is selected for execution. Notethat, during the inference, s1:Ntand rt are acquired from the simulator. TT shows superior performancethan DT in some benchmarks as reported in (Janner et al. (2021)), but treating each dimension separatelywould introduce learning and sample inefficiencies, especially for high-dimensional tasks. Instead of modeling the policy, Q-Transformer (Chebotar et al. (2023)) proposes to learn a transformer-based Q-network. Similarly to TT, each dimension of the action is discretized and treated as a separatetime step. When training, they update the Q-function in a temporal difference (TD) manner, i.e., min-imizing the disagreement between the predicted Q-value Q(stk:t, a1:it ) and target Q-value Q(stk:t, a1:it ).",
  "Balancing Model Capacity with Training Data": "The performance of transformer-based offline RL relies heavily on the quantity and quality of the trainingdata. A series of studies have been proposed focusing on augmenting the offline dataset across various taskscenarios.(1) Based on TT, Bootstrapped Transformer (Wang et al. (2022a)) proposes to generatetrajectories from , i.e., the model being learned, and adopt them as additional training data to expand theamount and coverage of the offline dataset. The self-generated trajectories are filtered by their log likelihoodunder the policy , which indicates the quality and reliability of the training data. (2) SS-DT (Zhenget al. (2023)) introduces a data augmentation method for semi-supervised settings, where most trajecto-ries lack action labels and are in the format (s0, r0, , sT , rT ). Their approach simply involves trainingan inverse dynamics model, i.e., Tinv(at|st, st+1), on trajectories with action labels and then applying thismodel to predict actions for the unlabeled trajectories. (3) Given only sub-optimal trajectories, trajectory-optimization-based offline RL would fail. In this case, the agent needs to learn to stitch segments fromdifferent trajectories for an optimal policy. Value-based RL does not have the same issue as it pools infor-mation for each state across trajectories through the Bellman backup. In this case, QDT (Yamagata et al.(2023)) suggests learning a Q-function via CQL and replacing the RTG values (i.e., Rt) in the offline datasetwith the learned Q-values as augmentation for DT training. (4) CDT (Liu et al. (2023b)) proposes a dataaugmentation method for safe RL. Specifically, in a Constrained MDP (Altman (1998)), the DT conditionedon both R() and C() = Ti=0 ct (i.e., the constraint-violation cost of the trajectory) is expected to achievethe target trajectory return while ensuring that the accumulated cost remains lower than C(). However,during inference, unachievable (R(), C()) pairs may be given as conditions, which are not represented inthe offline dataset. Thus, as an augmentation, they suggest using the trajectories from the offline datasetthat achieve the highest return (which is lower than the unachievable return R()) without violating C()as the corresponding trajectories of (R(), C()) for offline training. (5) For tasks with sparse and delayedrewards, DT suffers from model degradation, since the RTG does not change within a trajectory. DTRD(Zhu et al. (2023a)) proposes to learn a reward shaping function r(s, a) to redistribute the delayed rewardto each time step. This is achieved by solving a bi-level optimization problem as below:",
  "t=0r(st, at), , and () = arg minLtrain(, )(80)": "Here, is the parameter of the policy ; Ltrain(, ) denotes the DT objective on the training dataset, whichis augmented by replacing the original sparse rewards rt with r(st, at); Lval((), ) is the DT objectiveon the augmented validation dataset. Intuitively, r is updated to ensure that the policy learned from thedataset, augmented with corresponding dense rewards, is optimal.They propose a practical alternativetraining framework for and , which however lacks convergence guarantee in theory. Another line of works in this category focus on modifying the DT architecture to explicitly make use ofthe structural patterns within the training trajectories. As mentioned in .1, introducing strutualassumptions/priors to the input data can greatly improve the learning efficiency and mitigate the overfittingissue when learning from small-scale datasets. (1) Based on the observation that each trajectory is a sequenceof state-action-reward triplets, StARformer (Shang et al. (2022)) proposes to first extract representations",
  "Mitigating Impacts from Environmental Stochasticity": "Another category of research works that follow DT propose to change the conditional information of theDT to enhance its performance in stochastic environments.(1) In the absence of determinism, a high-return offline trajectory could be the outcome of uncontrollable environmental randomness, rather thanthe result of the agents actions. Thus, goals that are independent from the environmental stochasticityare the only conditions that the agent can reliably achieve.ESPER (Paster et al. (2022)) and DOC(Yang et al. (2023c)) propose methods to learn such conditions automatically from the offline dataset. Theyshare a common intuition: to ensure the conditional variable to contain sufficient information on actionpredictions but no information regarding the environment dynamics (e.g., on predicting future rewards orstate transitions). Both algorithms use a contrastive learning framework, resulting in similar objectives.However, these methods require to learn at least five networks (so we choose not to show their objectiveshere), which lose the simplicity of DT and is even more complicated than traditional offline RL approaches.(2) SPLT (Villaflor et al. (2022)) proposes to model the policy as (at|(stk, atk, , st), zt ), where thecondition zt is sampled from an encoder that takes (stk, atk, , st, at) as input. Both the encoder and have DT-like architectures and are trained jointly in a CVAE framework (introduced in .1), where works as the decoder, (stk, atk, , st) and at work as the condition and data point respectively. zt isa d-dim discrete vector, each dimension of which has c categories. Extracted by the CVAE, each specificzt can be viewed as a mode contained in the offline data and can be assigned to as a generation condition.In the same manner, they learn a environment model (rt, st+1, Rt+1|(stk, atk, , st, at), zt ). Duringinference, with (, ) and by enumerating (zt , zt ), cdd predictions on future h-length trajectories ht canbe obtained, and the next action is selected from the mode: arg maxzt minzt R( ht ) for robustness, whereR( ht ) = h1i=0 rt+i + Rt+h. Through thorough enumeration on the future predictions, the influence broughtby the environmental randomness can be mitigated. (3) As in QDT (introduced in .2.2), CGDT(Wang et al. (2023d)) suggests training an additional Q-network to guide the learning of DT. The rationalebehind this method is that the Q-value, representing the expected return for each state-action pair, can helpmitigate the impact of environmental stochasticity that could be wrongly captured by single-trajectory RTGvalues.",
  "Transformers in Extended Offline Reinforcement Learning Setups": "DT has been extended to various offline RL setups, including model-based, hierarchical, multi-agent, andmulti-task offline RL. Unlike the previous three subsections, extensions in this part primarily expand on theproblem setup rather than improving the algorithm design with respect to DT. Thus, this part is not ourprimary focus and we only provide a taxonomy and brief introductions of the related works.",
  "Reflections and Future Works on Transformer-based Offline Reinforcement Learning": "We notice that there is a series of works on applying transformers to online partially-observable RL (Yuanet al. (2023a)).They adopt the transformer as the policy network in gradient-based or actor-critic RLalgorithms, with the hope to harness its ability to process long-horizon historical information for decisionmaking. These works focus on architectural modifications to the standard transformer, which are specifi-cally tailored for RL. For example, GTrXL (Parisotto et al. (2020)) and Catformer (Davis et al. (2021))suggest adjustments to the MHA module to facilitate a more stable RL training process; ALD (Parisotto& Salakhutdinov (2021)) adopts model distillation to improve the computation efficiency in transformer-based distributed RL; STT (Yang et al. (2022e)) and WMG (Loynd et al. (2020)) propose adaptions forthe transformer to process spatiotemporal coupling observations and factored observations, respectively, forenhanced sample efficiency. These architectural modifications, or potentially new ones, could be integratedwith policy gradient offline RL algorithms or DT-like return-conditioned supervised learning (RCSL) algo-rithms for performance improvement. Conducting an (empirical) comparison between these two categories(i.e., RCSL and policy gradient offline RL), both utilizing the same transformer architecture, would also bean intriguing study. Finally, we reflect on these DT-like RCSL methods. Although a series of improvements have been made,there are still fundamental limitations regarding these algorithms.According to (Brandfonbrener et al.(2022)), RCSL returns near-optimal policy under a set of assumptions that are stronger than those neededfor traditional RL algorithms, and RCSL alone is unlikely to be a general solution for offline RL problems. Inparticular, RCSL offers guarantees only when the environment dynamics (including the state transition andreward functions) are nearly deterministic, a priori knowledge of the optimal conditioning function (i.e., theRTG values) is available, and the return distribution of the provided offline trajectories can cover the possiblevalues of the conditioning function.These findings inspire several directions for future research.First,adapting RCSL algorithms to stochastic environments, ideally backed by theoretical optimality guarantees,is a clear necessity. Second, developing effective strategies for selecting RTG values as the conditions duringinference, or alternatively, exploring new conditioning functions, is crucial.Last, a significant challengelies in addressing out-of-distribution scenarios in case that the offline dataset lacks adequate coverage ofthe task scenarios. Another fundamental problem regarding RCSL is whether it is necessary to learn low-return behaviors from the offline dataset and to require strong alignment between the target return (i.e., thecondition of the DT) and realized return. As indicated in Eq. (78), the policy training involves imitatingtrajectories across a range of returns. However, only high-return policy is required during inference. This",
  "Gato (MT)A generalist agent for 604 cross-modality tasksSee Reed et al. (2022)": ": Summary of transformer-based offline RL algorithms. In Column 1, we list representative (butnot all) algorithms in this section, where Boot Transformer, Env Trans, MADT+Dist, and MG DT cor-respond to Bootstrapped Transformer, Environment Transformer, MADT+Distillation, and Multi-GameDT, respectively. These algorithms are grouped by their categories, with abbreviations MB, HRL, MA,PT, and MT denoting model-based, hierarchical, multi-agent, pretraining-based, and multi-task offline RL,respectively. The evaluation tasks are listed in Column 3. Most works are evaluated on D4RL (Fu et al.(2020)), which provides offline datasets for various tasks, including Locomotion (L), AntMaze (M), Adroit(A), Kitchen (K), Maze2d (M2d). Regarding the other benchmarks, we provide their references here: Atari(Bellemare et al. (2013)), Real Robot (Chebotar et al. (2023)), Bullet-safety-gym (Gronauer (2022)), Min-iGrid (Chevalier-Boisvert et al. (2018)), dm_control (Tassa et al. (2018)), Stochastic Benchmark (Pasteret al. (2022)), MuJoCo (Todorov et al. (2012)), FrozenLake (Foundation (2023)), CARLA (Dosovitskiyet al. (2017)), Hidden Order Discovery (Chen et al. (2022)), SMAC (Samvelyan et al. (2019)), Fill-In &Equal Space & Grid-World & Highway (Meng et al. (2021)), simple-tag & simple-world (Li et al. (2023b)).",
  "Transformers in Imitation Learning": "Similarly with DT, transformer-based IL utilizes a supervised learning paradigm (see .3.1), wherethe transformer is adopted as the policy backbone (see .3.2). We differentiate works for IL andoffline RL simply by if the reward or return is used as a condition of the policy. Notably, a significantadvantage of the transformer is its capability to process large quantities of training data across multiplemodalities, which enables the development of generalist agents, as detailed in .3.3.",
  "min EDE(at log (st, xt1 , xtk))2, or min EDE [ log (at|st, xt1, , xtk)](81)": "Here, xt = (st, at) or st, and is implemented as a decoder-only transformer (as introduced in .1). This objective is similar in form with the one of DT (i.e., Eq. (78)) but replaces (st, at, Rt) withxt. IL algorithms do not require reward signals but place an emphasis on the quality of demonstrations,which ideally should come from experts, as the learning process relies solely on imitation. Also, to makefull use of demonstrations, auxiliary supervision objectives, such as prediction errors on the next state (i.e.,the forward model loss) or the intermediate action between two consecutive states (i.e., the inverse modelloss), are often employed. These objectives complement the action prediction error (i.e., Eq. (81)) to aidthe policy learning process. As a representative, Behavior Transformer (BeT, Shafiullah et al. (2022))empirically shows that a standard transformer architecture (specifically minGPT (Brown et al. (2020))) cansignificantly outperform commonly-used policy networks, like MLP and LSTM, in learning from large-scale,human-generated demonstrations, which typically exhibit high variance and multiple modalities. To coverthe multiple modes in expert behaviors, the authors suggest dividing the action a into two components: itscorresponding action center a and the residual action a, such that a = a + a. The set of actioncenters can be acquired by applying K-means clustering to the expert actions. Correspondingly, they applya policy network with two prediction heads, one for the action center and the other for the residualaction. Explicitly employing clustering to identify the various modes in expert actions and utilizing dual-head predictions to reason the mode of each instance significantly aids in modelling multi-modal actions.However, this approach does not model the multi-modality that may exist in the joint distribution of (s, a).",
  "Policy Approximation using Transformers": "A series of studies (Kim et al. (2021a); Pan et al. (2022); Kim et al. (2022); Zhu et al. (2022); Chenet al. (2023a); Kim et al. (2023); Liang et al. (2023a)) have adopted transformers as the policy backbonefor imitation learning in complex control tasks, such as vision-based robotic manipulation and end-to-endself-driving. As introduced in .1, the transformer has various advantages over CNNs and RNNs.Firstly, the transformer is adept at processing time-series data (Kim et al. (2022)) by capturing the long-term temporal dependencies between inputs from different time steps. By using the current state as thequery and historical data as keys and values, the transformer-based agent can pinpoint vital informationin the history for the current decision-making through the attention mechanism. Secondly, the transformercan concurrently process various types of input data, such as texts and images (Kamath et al. (2023)),texts and voxels (Shridhar et al. (2022)), point clouds (Pan et al. (2022)), enabling it to be the backboneof an end-to-end deep learning agent.Each type of data can be embedded to vectors by (pretrained)domain-specific neural networks. Then, instead of simply concatenating all these embeddings for subsequentprocessing, the transformer treats each embedding as distinct tokens. As detailed in .1, each tokenpossesses its own query, key, and value, and can attend to the other tokens for more informative aggregation.Thirdly, the transformer is efficient in processing input that contains multiple entities by treating each entitys",
  "Transformer-based Generalist Imitation Learning Agents": "Another research focus in this field is language-conditioned IL, aiming at training (robot) agents to follow hu-man instructions. A language instruction, comprising a sequence of words, can be embedded into a sequenceof tokens (l1, , lm) through a pretrained language model or a predefined embedding table. (1) A straight-forward method for language-conditioned IL, as shown in TDT (Putterman et al. (2022)), is to incorporatelanguage tokens into the policy input, i.e., (at|st, xt1, , xtk, l1, , lm). Still, is implemented as adecoder-only transformer, which, as previously mentioned, is capable of processing multiple types of inputconcurrently. Perceiver-Actor (Shridhar et al. (2022)) adopts a similar design, but suggests using thePerceiver Transformer (Jaegle et al. (2022)) as the policy backbone to manage extra long sequences of input(e.g., a sequence of image/voxel patches from the vision input and word tokens from the language input).(2) MARVAL (Kamath et al. (2023)) uses an encoder-only transformer to process inputs comprising fourmodalities: instruction texts (l1, , lm), historical states and actions (xtk, , xt1), the current state st,and the current action candidates At. As discussed in .1, without the Masked MHA component asin the transformer decoder, each input token can attend to every other token for more informative aggrega-tion. However, each forward pass predicts only a single action at, making it less efficient in training than thedecoder-only transformer. In decoder-only transformers, the output token corresponding to the state input siis used to predict ai (i = 0, , t). In contrast, encoder-only transformers require a special action token, i.e.,CLS, as an input to attend to all other tokens and capture the fused representation of the entire sequence,and its corresponding output token is utilized to predict the current action at.Additionally, MARVALintroduces auxiliary tasks, including predicting the masked language tokens and predicting the proportionof the trajectory that has been completed, to increase the amount of supervision for more efficient use of thedemonstrations. (3) Instead of using instruction texts as conditions, Lang (Hejna et al. (2023)) proposes topredict corresponding instructions from the state sequence as an auxiliary task, to realize language-guided IL.Specifically, each demonstration trajectory is divided into m segments, with each segment (sTi, , sTt+11)associated with a subtask instruction l(i) = (l(i)1 , , l(i)bi ). A transformer encoder is applied to extract repre-sentations (z1, , zt) from the state sequence (s1, , st). These representations are expected to encompassinformation essential for predicting both actions and instructions, achieved by minimizing the following equa-tion: Tt=1 log (at|z1, , zt) mi=1bij=1 log P(l(i)j |l(i)1 , , l(i)j1, z1, , zTi1). This approach offersan alternative way to incorporate language instructions into action predictions and has proven effective whentraining with a limited amount of demonstrations. Language-conditioned IL can be viewed as an instance of multi-task IL, where language texts serve as taskcontexts. Next, we introduce three approaches to multi-task IL, including context-conditioned IL, pretraining& fine-tuning, and generalist agent development. (1) Generally speaking, (l1, , lm) could be substitutedwith any form of task context (Furuta et al. (2022)). For example, Dasari & Gupta (2020) suggest usingdemonstration videos from human operators, denoted as v, as contexts for corresponding tasks. They trainthe policy (at|st, , stk, v) using Eq. (81), supplemented by an inverse model auxiliary loss term. (2)Transformer Adapter (Liang et al. (2022)) presents an approach for pretraining and fine-tuning. Initially,a transformer policy (at|stk, atk, , st1, at1, st), without task contexts, is pretrained on extensivemulti-task demonstrations using Eq.(81).It is then fine-tuned with a limited amount of task-specificdemonstrations before application.For efficient fine-tuning, the pretrained models parameters are keptunchanged. Instead, lightweight adapters (Houlsby et al. (2019)) are introduced between the pretrainedmodels layers, with only these adapters being updated using the task-specific demonstrations. The structure",
  "MIATraining an interactive agent by imitation of human-humaninteractions and self-supervision via modality matching.3D Playhouse": ": Summary of transformer-based IL algorithms. Representative (but not all) algorithms in this sectionwith their key novelties and evaluation tasks are listed in this table. These algorithms are all grounded inthe Behavioral Cloning framework, hence the algorithmic and theoretical advancements in this section arerelatively modest.However, the learned transformer-based agents are evaluated on much more diverse,realistic, and challenging tasks. First, there are some commonly-used benchmarks. CARLA (Dosovitskiyet al. (2017)) is a simulated environment for self-driving tasks. Block Push (Florence et al. (2021)), FrankaKitchen (Gupta et al. (2019a)), RLBench (James et al. (2020)), MetaWorld (Yu et al. (2019c)) providea range of robotic manipulation tasks, featuring various types of robots and input modalities.Habitat(Ramakrishnan et al. (2021)) provides navigation tasks for embodied AI in large scale 3D environments.BabyAI (Chevalier-Boisvert et al. (2019)), Crafting (Chen et al. (2021c)), ALFREAD (Shridhar et al. (2020))offer instruction-conditioned tasks and datasets. Second, some algorithms have shown superior performanceon competitions. In the table, SMARTS refers to the NeurIPS 2022 Driving SMARTS Competition (Rasouliet al. (2022)), and ManiSkill refers to SAPIEN ManiSkill Challenge 2021 (Mu et al. (2021)). Third, inthe table, VIOLA, TDT, MARVAL, and MIA each develop their own environments or create large-scaletraining datasets tailored to their specific purposes. These efforts can be regarded as significant contributionsalongside their algorithm designs. Readers are encouraged to read the respective papers for details. of the adapter can be Adapter(X) = X + W A2 (GeLU(W A1 (X))), where GeLU (Hendrycks & Gimpel (2023))is the activation function and W A1,2 are the weight matrices of the adapter, and it is inserted between thepoint-wise FFN layer and Add & Normalization layer of the transformer (as shown in ). The intuitionbehind this paradigm is that agents can acquire a diverse set of behavioral priors through large-scale task-agnostic pretraining, which then enables them to be efficiently fine-tuned for specific tasks. (3) As introducedin .2.4, the ultimate goal of multi-task learning is a generalist agent, i.e., one model with a singleset of weights that can be directly applied to a wide range of tasks. Different from Gato, which is directlytrained on batches of prompt-conditioned demonstrations, DualMind (Wei et al. (2023)) introduces a dual-phase method. Specifically, in Phase I, the transformer policy (at|stk, atk, , st1, at1, st) is trainedwith self-supervised learning objectives to capture generic information of state-action transitions, where theself-supervision involves predicting the next state, predicting the current action, and reconstructing maskedactions based on the other elements. In Phase II, a prompt p, which can be either language or image tokens,",
  "(82)": "Here, f denotes the transformer to extract representations, is the overall policy which comprises multiplecomponents besides f, D is a discriminator (as in GANs) which is introduced to encourage f to generatedistinct representations for matched vision and text data (i.e., (sVt , sLt )) and unmatched ones (i.e., (sVt , sLt )).This modality matching auxiliary task, also used in (Mees et al. (2022a)), has shown to significantly improvethe agents performance. We summarize the representative algorithms discussed in this section in , highlighting their key nov-elties and evaluation tasks. As mentioned in the beginning of this section, transformer-based IL algorithmsfundamentally rely on the straightforward BC framework. However, these algorithms have demonstratedpromising outcomes in complex robotic manipulation tasks, both simulated and real-world. This leads to aninspiring paradigm for robust robotic learning, that is, using a potent foundation model trained by imitationlearning on extensive, high-quality demonstrations. Future research directions may include developing meth-ods to synthesize high-quality training data at lower costs or enhancing/replacing the foundational algorithmBC with more advanced alternatives.",
  "Background on Diffusion Models": "Diffusion models have demonstrated superior performance across multiple domains, such as computer vi-sion (Amit et al., 2021; Baranchuk et al., 2021), natural language processing (Austin et al., 2021b; Hoogeboomet al., 2021), and multi-modal learning (Avrahami et al., 2022; Ramesh et al., 2022), showcasing their impres-sive capabilities in generating detailed and diverse instances. Diffusion models contain two interconnectedprocesses: the forward diffusion process and the backward denoising process (Yang et al. (2022a)). Specifi-cally, the forward process is predefined and transforms the data with a certain (but unknown) distribution,i.e., x PX(), into a (standard Gaussian) random noise z. This process progressively corrupts the inputdata by adding a varying scale of noise at each diffusion step. Correspondingly, the reverse process uses aneural network as the denoising function to gradually undo the forward transformation to reconstruct thedata x from the random noise z. There exist three main formulations of diffusion models: Score-based Gen-erative Models (SGM) (Song & Ermon, 2019; 2020), Denoised Diffusion Probabilistic Models (DDPM) (Sohl-Dickstein et al., 2015; Ho et al., 2020; Nichol & Dhariwal, 2021), and Stochastic-Differential-Equations-basedmodels (Score SDE) (Song et al., 2020; 2021). Next, we introduce these formulations by illustrating theirforward/backward processes and learning objectives, while discussing their connections with each other.",
  "tS(xt, t)||2(84)": "Here, U[1, T] is a uniform distribution on [1, , T], (t)s are positive weighting functions. Thisobjective is derived based on the definition of the forward process, i.e., xt = x0 + t, N(0, I).For detailed derivations and the definition of (t), please refer to (Song & Ermon (2019)). Regardingthe backward generation process, the score functions S(xt, t), t [T, , 1], are sequentially usedas denoising functions to generate xt1 from xt.Finally, the data samples x0 PX() can beacquired. As mentioned, multiple score-based sampling schemes can be adopted in this process.We take the annealed Langevin dynamics sampling scheme (Song & Ermon (2019)) as an example:(xT = x0T , x0 = x00)",
  "N and st are hyperparameters, denoting the number of iterations and step size for denoising x0t tox0t1 (t = T 1)": "DDPM gradually adds random noise to the data over a series of time steps (x0, , xT ) in theforward process, where x0 = x, xT = z. In particular, the sample at each time step is drawn from aGaussian distribution conditioned on the sample from the previous time step: (1:T are predefined.)",
  "txt1, tI)(86)": "With Eq. (86), the sample at each step t can be expressed as a function of x0: xt = tx0+1 t,where t = ts=0(1 s), N(0, I) (Sohl-Dickstein et al. (2015)). T is designed to be close to0, so that z = xT approximately follows N(0, I). Conversely, through the reverse denoising process,xT is converted back to x0 step by step: (t = T 1)",
  "(88)": "Note that P(xT ) and F(xt|xt1) have analytical forms and this objective is an upper bound for thenegative log-likelihood Ex0PX() [ log P(x0)]. Although this objective can be directly optimizedthrough Monte Carlo sampling, Ho et al. (2020) propose a reformulation of it for variance reduction.Since all (forward or backward) transformations are based on Gaussian distributions, by specifyingthe variance schedule 1:T and fixing the backward variance (xt, t) to be tI, Eq. (88) can beconverted to: (Ho et al. (2020))",
  "dx = [f(x, t) g2(t)x log Pt(x)] dt + g(t) d w(92)": "where w denotes the standard Wiener process when time flows backwards, Pt(x) denotes the distri-bution of x at time t in the forward process. Similarly with SGM, the score function at each timext log Pt(xt) can be estimated with an NN-based function S(xt, t) through various score matchingtechniques (Vincent (2011); Song et al. (2019)). Further, in (Song et al. (2020)), they propose amore computationally-efficient denosing process by removing the random noise injection g(t)d w andprove that the resulting equation is a probability flow ordinary differential equation (ODE) whichshares the same marginal densities as those of the reverse-time SDE. Both equations allow samplingfrom the required data distribution, i.e., PX(x).",
  "Diffusion Models in Imitation Learning": "Most works regarding applying diffusion models in IL are based on the Behavioral Cloning (BC) framework(Pomerleau (1991)), which is introduced in .2.1. Diffusion models have shown superior performancein modeling complex, high-dimensional data distributions.Thus, many recent works have proposed toimprove BC by implementing the policy network as a diffusion model to model the conditional distributionPA|S(a|s) within the expert data DE. Further, as shown in .1, the objective for training DM (i.e.,Eq. (88)) provides an upper bound for the negative log-likelihood (i.e., the objective of BC), which naturallyconnects DM with BC. Next, we introduce these works in details, which are designed to address drawbacksof the original BC algorithm from multiple perspectives.",
  "Policy Approximation using Diffusion Models": "The first group of works (Pearce et al. (2023); Chi et al. (2023); Reuss et al. (2023)) try to improve theexpressiveness of the learned policy. The BC policy is trained to give out point estimates 17, whichprecludes it from capturing the multi-modality of the state-action pairs in DE. Moreover, is encouragedto learn an average distribution, as the objective (i.e., Eq. (11)) is to maximize an expectation, which wouldresult in bias towards more frequently occurring actions. Additionally, (a|s) is usually implemented as adiagonal Gaussian policy, thus the prediction of each action dimension is independent, potentially leadingto uncoordinated behaviours in high-dimensional action spaces. On the other hand, DM has shown greatpotential in generating high-dimensional samples that adhere to complex, multi-modal distributions, whileensuring the precision and diversity. In this case, Diffusion BC (Pearce et al. (2023)) is proposed to utilize",
  "t, t = T, , 1, aT N(0, I)(94)": "Further, Diffusion Policy (Chi et al. (2023)) is proposed to utilize DDPM for closed-loop action-sequenceprediction. In particular, it models the distribution P A|S(a|s) in DE, where s and a denote the previousTs states and future Tp actions, respectively. Ta out of the Tp actions are executed without replanning.This framework allows long-horizon planning, encourages temporal consistency of the action sequence, andremains responsive to the changing environment through receding horizon planning. In order to learn thejoint distribution of the Tp actions conditioned on the historical states a task that is inherently high-dimensional Chi et al. (2023) suggest modeling it as (at, t|st), which is trained and sampled with aDDPM framework as delineated in Eq. (93) and (94). Last, Reuss et al. (2023) propose BESO for goal-conditioned IL. Specifically, one or more future states withinthe same trajectory as (s, a) are used as the goal g. The objective is now to get a goal-conditioned policy(a|s, g), which is learned as a conditional score function S(at, t; s, g) in a Score SDE framework (Karraset al. (2022)). With S(at, t; s, g), the action at s targeting g can be generated with a Probability FlowODE, as introduced in .1. Goal-conditioned policies distill useful, goal-oriented behaviors, whichcan be integrated with a high-level planner for various downstream tasks, as in hierarchical RL. However,incorporating the goal conditioner amplifies the multimodal nature of the demonstrations, since the samegoal might be achieved via various distinct trajectories, underscoring the necessity of employing DM. Itsworthy noting that all three works suggest using transformers as the denoising function (i.e., , S) forsample quality, rather than the commonly-used U-Nets (Ronneberger et al. (2015)) for DM.",
  "Addressing Common Issues of Imitation Learning with Diffusion Models": "Beyond enhancing policy expressiveness, numerous studies have explored the use of DM to address otherchallenges in IL, including spurious correlations (Saxena et al. (2023)) and imperfect demonstrations (Wanget al. (2023e); Yuan et al. (2023b)). To be specific, due to spurious correlations, expressive models, suchas DM, may focus on distractors that are irrelevant to action prediction and thus fragile in real-worlddeployment, which resembles the causal misidentification issue introduced in .3.5. Thus, Saxenaet al. (2023) propose C3DM to improve the denoising process of Diffusion BC as follows:",
  "t, t = T, , 1, aT N(0, I) (95)": "Compared with Eq. (94), the only difference is to replace the conditioner s with st which is updated withthe denoising process. In particular, at each denosing iteration t, the image state s is zoomed into the regionaround the intermediate action, i.e., pos(at), to acquire more details for decision-making while ignoringdistractors in other regions. Note that this work assume the access to a transformation between the actionand state spaces to determine pos(at). Accordingly, they modify the training process in Eq. (93) by replacings with C(s; pos(a), t). Further, considering that expert demonstrations in real-world scenarios are often noisy,Wang et al. (2023e) propose DP-IL to adopt DM for purifying/denoising the demonstrations. Subsequently,any IL algorithm can be applied to these purified data. To be specific, based on the DDPM framework, theyview (s, a) as the data point x (in Eq. (89)) and learn a denoising function (xt, t) to recover the originalstate-action demonstrations (i.e., x0) from samples interrupted with Gaussian noise (i.e., xt). Note that thistraining process is based on a set of clean demonstartions. Then, for imperfect data points x, the learnedDM can be used to purify them by first adding random noise to x to get xt (i.e., via the forward process) andthen using in the reverse denoising process to get the purified data x0. Theoretically, they prove that xt",
  "SMILEDDPMPolicy GeneratorBCNoisydataMuJoCo": ": Summary of DM-based IL algorithms. Specifically, HD is short for high-dimensional. Re-garding the evaluation tasks, Kitchen (Gupta et al., 2019b), Block-Push & Push-T (Florence et al., 2022),Robomimic (Mandlekar et al., 2021b), CALVIN (Mees et al., 2022b) are robotic manipulation tasks in dif-ferent scenarios; CSGO (Pearce & Zhu, 2022) is a 3D First-person Shooter video game; Autodesk (Kogaet al., 2022) and Real Robot (Saxena et al., 2023) represent robotic manipulation tasks built on the Autodesksimulator and real robot platforms, respectively; MuJoCo (Todorov et al. (2012)) provides a series of roboticlocomotion tasks. and xt can be arbitrarily closed in distribution as t increases, and so trained on xt can be used to convertxt into purified demonstrations as well. Different from the two-stage framework in DP-IL, SMILE (Yuanet al. (2023b)) is proposed to jointly perform the automatic filtering of noisy demonstrations and learningof the expert policy. It is based on Diffusion BC (i.e., Eq. (93) and (94)), but changes the forward diffusionprocess of DDPM as follows:",
  "k=12k(96)": "Here, 0 denotes the underlying policy of the provided demonstrations. Through this equation, the diffusionprocess of SMILE is conducted on a policy level rather than on data points, and so the learned denoisingfunction can be used to compare the suboptimality among behavior policies underlying different trajec-tories (i.e., with Eq. (13) in (Yuan et al. (2023b))). Trajectories generated using policies noisier than thecurrently learned one are then filtered out from the training dataset. While not exclusively designed for DM-based IL, the studies by Shi et al. (2023) and Sridhar et al. (2023)introduce techniques namely, AWE and MCNN that further enhance the performance of DiffusionPolicy and Diffusion BC, respectively. Both techniques focus on mitigating the compounding errors of IL,i.e., prediction errors compounded over the decision horizon. AWE can be applied to automatically extractwaypoints within an expert trajectory. The waypoint sequences are notably shorter in horizon and can beused for waypoint-only imitation. On the other hand, MCNN does not modify the training data but enhancesthe policy for reduced compounding errors. During evaluation, it identifies the nearest neighbor state of thecurrent state in the expert dataset, and then blends the corresponding expert action with the output of thelearned policy network to formulate an error-constrained policy. In , we present a summary of DM-based IL methods.This includes their base IL algorithms,the specific IL challenges they aim to address, and how DM is used in resolving these issues. There areseveral promising future research directions in this field. Firstly, while most current works rely on DDPMdue to its robust performance, future studies could explore more advanced DMs, particularly those with",
  "Diffusion Models in Offline Reinforcement Learning": "Recently, there is an emerging body of advancements in applying diffusion models to offline RL (Zhu et al.(2023c)). In particular, diffusion models can be adopted as the policy, planner, or data synthesizer in thecontext of offline RL, as introduced in .3.1 - 7.3.3. Additionally, we present the applications of DMfor extended offline RL setups in .3.4. For each category, we introduce the seminal works in detailsas a tutorial on its paradigm, followed by a brief review of the extensions with a focus on their key novelties.",
  "Therefore, the parametric policy should be expressive enough to recover the optimal policy": "In this case, DM can be employed to model the policy .(1) SfBC (Chen et al. (2023b)) providesa straightforward manner to realize this. It first imitates the behavior policy (a|s) with a DM (a|s)(specifically, Score SDE) in an IL scheme as introduced in .2.1. Then, for any state s, N actions aresampled with (|s) as candidates, and one action is resampled from these candidates with exp( 1 Q(s, a))being the sampling weights. In this way, a (|s) is approximated via importance sampling. Note thatQ(s, a) can be learned with any offline RL protocol. IDQL (Hansen-Estruch et al. (2023)) adopts the sameresampling scheme but imitates (a|s) with DDPM. Additionally, rather than using exp( 1 Q(s, a)), theydesign a sampling weight based on the Implicit Q-Learning framework (Kostrikov et al. (2022)), which is aSOTA offline RL algorithm. (2) Alternatively, the Q-function Q(s, a) can be directly involved in trainingthe DM policy (a|s). In particular, Diffusion-QL (Wang et al. (2023f)) learns a DDPM-based policythrough the following objective:",
  "E(s,a)D [|Q(s, a)|]EsD,a0(|s) [Q(s, a0)](99)": "Here, LDM() is defined as Eq.(93) (i.e., an IL objective) and can be replaced by the correspondingtraining objective if a different type of DM is employed, is implied by the denoising function , a0 isthe action sample after being denoised for T iterations (with Eq. (94)), E(s,a)D [|Q(s, a)|] is incorporatedfor adaption to Q-functions with different scales. Intuitively, the Q-function acts as guidance in the reversegeneration process of the DM. Also, Eq. (99) mirrors Eq. (97), since both equations encourage the learnedpolicy to maximize Q-values while being closed to the behavior policy, and its noteworthy that DiffCPS(He et al. (2023b)) provides a theoretical connection between Eq. (99) and Eq. (97). 18At each time step, to sample a (|s), the entire denoising process (with T iterations) shown as Eq. (94) needs to beexecuted, thus the sampling efficiency of DM is essential.19As a common practice, can be fine-tuned as a hyperparameter, controlling the tradeoff between the two objective terms.",
  "at log (at|s) at log (at|s) + atQ(s, at)(100)": "As mentioned in .1, with the score function at log (at|s), samples at (|s) can be generatedvia various efficient score-based sampling schemes 20. Here, the score function at log (at|s) can be learnedwith Score-based DM from D, which is introduced in .1, but atQ(s, at) relies on the Q-functions over intermediate samples at, which is intractable.As potential solutions, QGPO propose acontrastive learning framework to estimate Q(s, at), t. While, CPQL employs Consistency Models asthe policy, reframes the score function estimation in Eq. (100) into an objective similar to Eq. (99), andtheoretically establishes their equivalence when the weights of the two terms in Eq. (99) are properly chosen. In , we provide a summary of these algorithms.All the algorithms in this category have beenevaluated on the D4RL benchmark. Readers can refer to corresponding papers for the numeric evaluationresults to compare their performance. In particular, we notice that three schemes of using diffusion modelsas policies for offline RL have been developed: SfBC & IDQL, QGPO & CPQL, and the other works thatfollow Diffusion-QL. Among them, QGPO & CPQL choose to directly learn the score function correspondingto the optimal policy, which is a quite challenging but promising direction for future works.",
  "Diffusion Models as Planners": "For model-based learning, a dynamic function T(sk+1|sk, ak) needs to be approximated from the datasetD first, and then planning over the optimal action sequence (a1, , aK ) can be done by maximizing thereturn while avoiding OOD actions, of which the objective is shown as Eq. (9). Note that, for clarity,we use superscript k to indicate the time step and subscript t to denote the iteration of the diffusionprocess. SGP (Suh et al. (2023)) proposes to implement the uncertainty measure u(s, a) (in Eq. (9)) as thenegative log-likelihood of (s, a) under a perturbed distribution, i.e., log P((s, a); D) where P(x; D) =1",
  "|D|": "xiD N(x; xi, 2I). Intuitively, this likelihood measures the distance of (s, a) to the dataset D anda large distance indicates a high uncertainty of the point (s, a) as it may be OOD. Then, to optimize Eq. (9)for trajectory planning, the score function a log P((s, a); D) needs to be estimated. As detailed in (Suhet al. (2023)), this estimation can be obtained as a denoising function in DM (specifically, SGM). A more widely-adopted manner for planning with DM, which is firstly proposed in Diffuser (Janneret al. (2022)), is to fold the two processes mentioned above:transition dynamic modeling and tra-jectory optimization regarding a1:K, as a trajectory modeling process using DM. Viewing trajectories = ((s1, a1), , (sK, aK)) as data points (i.e., x in Eq.(89)), DDPM is used to model the distribu-tion of in D, through Eq. (89) where a denoising function for trajectory generation (t, t) is learned.Then, the planning can be done by sampling trajectories starting from the current state using the learnedDM. The sampling process is similar with Eq. (94). However, as an RL algorithm, this process is guided bya (separately-trained) return function of the trajectory samples, i.e., J() 21: (t, t, and t are hyperpa- 20Note that at represents the sample at the t-th diffusion iteration rather than the action at time step t.21J() is trained to estimate the return of the original trajectory, i.e., J() = Kk=1 r(sk, ak), where can be the trajectorysample at any diffusion iteration, i.e., t.",
  "t1 G(|t, g) = N(t + tg, t), t =11 t(t t1 t(t, t)), g = J()|=t(101)": "This process is also known as classifier-guided sampling (CG (Dhariwal & Nichol (2021))), which iswidely adopted for conditional generations with diffusion models. Intuitively, the generation is guided by thegradient J() along which the expected return J() would be maximized, aligning it with RL. In (Janneret al. (2022)), the authors connect this guided sampling design with the control-as-inference frameworkof RL (Levine (2018)) and claim that trajectories from such a generation process follows the distributionP(|O1:K = 1), where Ok = 1 indicates the optimality of the time step k. During evaluation, the learnedDM can be applied as follows: (T is the real dynamic.)",
  "[t(s1) sk, t1 G(|t, g)]1t=T , ak 0(a1), sk+1 T (|sk, ak), k = 1, , K(102)": "To determine ak, trajectory samples at each iteration are forced to start with the current state, i.e., t(s1) sk (t = 1, , T). This follows the idea for solving inpainting problems (Sohl-Dickstein et al. (2015)), wherethe generation for the unobserved part is in a manner consistent with the observed constraints. Moreover,only the first action in the generated plan, i.e., 0(a1), is executed without replanning, which aligns with thereceding horizon control (Mayne & Michalska (1988)). Decision Diffuser (Ajay et al. (2023)) adopts similardesigns with two key modifications. First, instead of training a return function for classifier-guided sampling,they adopt classifier-free guided sampling (CFG (Ho & Salimans (2022))), for which a conditional andan unconditional denoising function, i.e., (t, t; J()) and (t, t; ), are jointly trained (with Eq. (89))by randomly dropping out the conditioner J(). The trajectory generation process is the same as Eq. (94),but replaces the standard denoising function (t, t) with (t, t; J()) + (1 )(t, t; ). Increasing thevalue of would decrease the diversity of samples but aligns the trajectory distribution more closely withP(|O1:K = 1). Notably, both CG and CFG can be utilized for generating samples that satisfy specificconditions y. In the context of RL, y can be the desired return J(), constraints to obey, goals or subgoalsto achieve (for multi-task or hierarchical RL), and so on. Second, only state sequences are modelled andpredicted with the DM, i.e, = (s1, , sK) and actions are predicted with a separate inverse dynamicmodel, i.e., ak = Tinv(sk, sk+1). This is because sequences over actions tend to be more high-frequency andless smooth, making them much harder to predict and model. Following Diffuser and Decision Diffuser, improvements have been made in multiple aspects such as thesampling process (SafeDiffuser, Discrete Diffuser), network structure (EDGI), training objective (PlanCP),and conditioners (TCD). We present a brief overview of these advancements in comparison to Diffuser andDecision Diffuser as follows. Notably, only TCD follows Decision Diffuser and the others follow Diffuser.SafeDiffuser (Xiao et al. (2023)) aims to ensure the safe generation of data in the diffusion process. Foreach iteration t, the diffusion dynamic ut = t1t tis re-optimized to satisfy certain safety constraints andthe resulting dynamic ut is used to update t1 as t +tut . t is the diffusion time interval which shouldbe small enough. They theoretically show that 0 generated in this manner fulfills the safety constraintswith probability almost 1. Discrete Diffuser (Coleman et al. (2023)) proposes three diffusion-guidancesampling techniques for generation in discrete state and action spaces, where Gaussian-based DM cannot beapplied. These guided sampling methods can work with discrete DM such as D3PM (Austin et al. (2021a)).EDGI (Brehmer et al. (2023)) proposes an equivariant network architecture for the scenario where anembodied agent operates n objects in a 3D environment. This network design takes geometric structures intoaccount and ensures equal likelihood of a trajectory and its counterpart for which specific spatial/temporaltranslations or permutations over objects are applied. Better generalization across scenarios, where suchtranslations or permutations happen, can thus be realized. PlanCP (Sun et al. (2023)) proposes to quantifythe uncertainty of DM using Conformal Prediction (Vovk et al. (2005)). In particular, M trajectories aregenerated from DM corresponding to M trajectories in D, each pair of which has a prediction error ei. Theconformity among {ei} reflects the uncertainty of the DM. Introducing this conformity term to the DDPMtraining objective (i.e., Eq. (89)) as an auxiliary term can potentially reduce the uncertainty of samplingwith DM. Compared with Decision Diffuser, TCD (Hu et al. (2023a)) utilizes more temporal information asthe conditioner of the denoising function to guide the sampling process. Specifically, following the insightof Decision Transformer (introduced in .2.1), the return-to-go and reward at the current time step",
  "Diffusion Models as Data Synthesizers": "The performance of offline RL is limited by the provided static dataset D. Once trained with D, theDM can potentially generate a variety of high-quality trajectory data. This newly generated data can thenbe employed to augment D, thereby creating a feedback loop to further improve the DM. Moreover, in anunseen but related task, the DMs inherent generality allows for generation of well-performing trajectories forfine-tuning the model in this novel environment. Several works have been developed in this direction followingeither Diffuser (Hwang et al. (2023); Liang et al. (2023b)) or Decision Diffuser (He et al. (2023a)). Specifically,AdaptDiffuser (Liang et al. (2023b)) suggests a rule-based method for filtering and enhancing generatedtrajectories = ((s1, a1), , (sK, aK)) from Diffuser, based on the return values and dynamic consistency.Starting with k = 1, a revised action ak is determined using a well-trained or defined inverse dynamicmodel Tinv(sk, sk+1) (with s1 = s1). Subsequently, the next state is adjusted according to the (learned)environment dynamic function as sk+1 = T (sk, ak). Trajectories with states sk significantly deviating fromtheir counterparts sk are filtered out. The remaining trajectories are further filtered based on their returnsJ() = Kk=1 r(sk, ak). Finally, the chosen trajectories are of high quality and can be used for effectivedata augmentation. In a more straightforward approach, MTDiff (He et al. (2023a)) models trajectories =((s1, a1, r1), , (sK, aK, rK)) conditioned on expert demonstrations from the same environment, denotedas y. Then, trajectory samples from the learned conditional denoising function (t, t; y) are directly usedfor data enhancement. For unseen tasks, a small amount of demonstrations can be used as prompts, i.e., y,for (t, t; y) to generate high-quality training data. These data can then be employed to adapt the DMpolicy to novel tasks.",
  "Diffusion Models in Extended Offline Reinforcement Learning Setups": "Diffusion models have been applied in multi-task, multi-agent, and hierarchical offline RL setups.Still,these methods are built on the foundational algorithms mentioned above, such as Diffusion-QL, Diffuser,and Decision Diffuser. MTDiff (He et al. (2023a)) and MetaDiffuser (Ni et al. (2023)) aim to learn task-conditioned planners to solve a distribution of tasks. They utilize offline data categorized by task, i.e., iDi,where i denotes the behavior policy of task i. Following Decision Diffuser, MTDiff incorporates an experttrajectory from task i, denoted as yi, as an extra condition of the planner specific to that task, representedas (t, t; J(), yi).Through training, the agent is expected to implicitly capture the transition modeland reward function stored in the prompt trajectory yi, and perform task-specific planning based on theseinternalized knowledge. When being evaluated in a new but related task, the denoising function conditionedon a corresponding demonstration yi can be directly used for planning. In contrast, MetaDiffuser explicitlymodels the reward and transition dynamics by learning an encoder, denoted as E, to encode trajectories intocompact representations. The encoder is jointly trained with the approximate reward and dynamic models,which serve as the decoder, within a VAE framework, resembling BoReL introduced in .2.4:",
  "max,, EiDi,y=E(),(sk,ak,rk,sk+1)log T(sk+1|sk, ak, y) + log r(rk|sk, ak, y)(103)": "In this way, y is trained to embed task-specific reward and dynamic functions. When planning for a specifictask i, the representation yi = E(i), i Di can be used as an extra condition of the planner. Notably,MetaDiffuser is based on Diffuser and it adopts the learned reward and dynamic models to define the samplingguidance g in Eq. (101) to enhance the dynamics consistency of the generated trajectories while encouraginga high return, detailed as Eq. (7) in (Ni et al. (2023)). DOM2 (Li et al. (2023d)) and MADiff (Zhu et al. (2023b)) target at fully cooperative, multi-agent offlineRL. Built upon Diffusion-QL, DOM2 learns a Q-function and DM-based policy for each agent following afully decentralized framework. This policy is tailored to map the individual observation of each agent to itsown action, which is trained with Eq. (99). Following Decision Diffuser, MADiff is developed for planningover the joint trajectories of all agents. Each agent i has a separate denoising function i for planning itscorresponding trajectory. To encourage global information interchange for better coordination, the denoising",
  "MTDiff (MTRL)DDPMPlanner &Synthesizer (DD)Meta-World-V2": ": Summary of DM-based offline RL algorithms. Some algorithms in this table can be categorizedas multi-agent RL (MARL), multi-task RL (MTRL), or hierarchical RL (HRL). When DM are used asplanners, most algorithms can be viewed as extensions of Diffuser (D) or Decision Diffuser (DD). Regardingthe benchmarks, nearly all algorithms in this category have been evaluated on D4RL (Fu et al. (2020)),which provides offline datasets for various data-driven RL tasks, including Locomotion (L), AntMaze (M),Adroit (A), Kitchen (K), and CARLA Autonomous Driving (C). dm_control (Tassa et al. (2018)), KukaRobot (Janner et al. (2022)), Unitree-go-running Margolis & Agrawal (2022), CartPole (Tedrake (2023)),Box-Pushing (Manuelli et al. (2020)) are a series of continuous (robotic) control tasks. Pixel-Based SingleIntegrator (Chou & Tedrake (2023)) requires dynamic learning and control over the pixel space. NeoRL (Qinet al. (2022)) is an industrial benchmark on financial decision making. Lastly, Multi-task MuJoCo (Mitchellet al. (2021)) and Meta-World-V2 (Yu et al. (2019c)) are widely-used benchmarks for multi-task/meta RL.MPE (2D control Lowe et al. (2017)), MAMuJoCo (robotic locomotion Peng et al. (2021)), SMAC (videogaming Samvelyan et al. (2019)), and MATP (trajectory prediction Alcorn & Nguyen (2021)) provide diverseevaluation tasks for multi-agent RL. function contains an attention layer to aggregate trajectory embeddings from other agents. Clearly, moreextensions can be developed regarding DM-based MARL, such as MARL algorithms for fully competitiveor mixed (partially cooperative/competitive) multi-agent tasks, and integration of DM with state-of-the-artcentralized training & decentralized execution (CTDE) MARL methods. LDCQ (Venkatraman et al. (2023)) and HDMI (Li et al. (2023c)) are proposed to learn a hierarchicalpolicy/planner from an offline dataset, which can be especially beneficial for long-horizon decision-making.LDCQ learns a high-level policy high(z|s) for skill selection, and low-level policies low(a|s, z) for each skill",
  "(104)": "Similarly with OPAL, after training, P is used as low and P is adopted to convert D to a set of transitionsin the form of (s1, z P(|), K1k=0 rk, sK) for training high. high is modeled as DM and trained in asimilar manner with SfBC (introduced in .3.1). Alternatively, for each trajectory in D, HDMIextracts its subgoal list following a heuristic method (Eysenbach et al. (2019a)). Then, two DMs are adoptedto model the sequence of subgoals and trajectories targeting at each subgoal, respectively. In particular, basedon Decision Diffuser, HDMI learns a high-level planner to generate the subgoal list z, guided by the trajectoryreturn, i.e., high(z,t, t; J()), and a low-level planner to produce the state sequence s corresponding to eachcertain subgoal z, i.e., low(s,t, t; z). Its worthy noting that fast sampling is essential for the application of DM for offline RL or IL. Besidesadopting Consistency Models, for DDPM-based methods, they usually limit the number of iterations of thereverse generation process to be relatively small and adopt a carefully-designed variance schedule, i.e., 0:T ,as proposed in (Xiao et al. (2022)). For SDE-based methods, they would solve a probability flow ODE forthe reverse generation process (as introduced in .1) with a fast solver: the DPM-solver (Lu et al.(2022a)). All algorithms mentioned in this section have been summarized in and categorized basedon the DM type and usage. When applying DM as planners, most algorithms are built upon DDPM andfollow the design of either Diffuser (D) or Decision Diffuser (DD).",
  "Discussions and Open Problems": "In this section, we provide in-depth discussions on deep generative models (DGMs) in offline policy learningand our perspectives on future research directions of this area, based on the main text in - 7.The discussions are centered around how DGMs have been used in offline policy learning, which provide acomprehensive summary of this paper and insightful ideas for future works. Note that, for simplicity, we useabbreviations of the DGMs: VAE - Variational Auto-Encoder, GAN - Generative Adversarial Network, NF- Normalizing Flow, DM - Diffusion Model.",
  "Discussions on Deep Generative Models and Offline Policy Learning": "A common usage of DGMs in offline policy learning: All DGMs can be utilized as policy functions inoffline policy learning. In the context of IL, they can be the student policy learned by imitating the expert,while, in offline RL, they can either be the approximated behavior policy or the RL policy. To be specific,we list their mathematical forms as follows, in the order they were introduced 22:",
  "DM: aT N(0, I), at1 G(|at, s), (t = T, , 1), a = a0.(105)": "All these DGM-based policies show superior expressiveness compared with the one using only feed-forwardneural networks, and each of them has unique advantages/disadvantages: (1) VAEs might be less expressivethan the others, but offers a lightweight model choice and relatively stable training process; (2) As mentionedin .2.4, GAN-based polices can generate sharp data samples but may suffer from the mode collapseissue, meaning that they may not be able to cover the multiple modes in the (s, a) distribution of the dataset 22P(z|s) and P(a|s, z) denote the prior and decoder of the VAE, respectively. PZ(z) is the assumed prior distribution of thelatent variable z. G(z|s) denotes the generator in the GAN or NF, and it is composed of a series of invertible and differentiablefunctions, i.e, G1 GN, in the NF. (|tk:t) represents a transformer-based policy, where tk:t can be (stk, atk, , st)in IL and (stk, atk, Rtk, , st, Rt) in offline RL. G(at1|at, s) represents the denosing process of DM, and the subscriptt denotes the denoising iteration.",
  "DP/Model-basedoffline RL,Trajectoryoptimization": ": A summary of the base offline RL/IL algorithms for DGM-based offline policy learning. Max-EntIRL (.2.1) is short for maximum causal entropy inverse RL. KL, LIL, and AIL (.2.1)denote the KL-divergence-based, Likelihood-based, and Adversarial IL framework, respectively. DP-basedoffline RL (.1.1) refers to the Dynamic-Programming-based offline RL. A summary of the base IL/offline RL algorithms for each DGM: We provide this summary as. (1) Regarding IL, most DGMs, including VAEs, Transformers, and DMs, select BC as the basealgorithm, of which the objective (Eq. (11)) is simply to maximize the log likelihood of expert state-actionpairs. However, there are differences in the realization. Specifically, as introduced in .3.1, VAE-based IL algorithms either adopt a VAE ELBO, which is a lower bound of the BC objective, or a VIB-basedframework, which is close in form with the ELBO. For transformer-based IL, besides the BC term, to makefull use of demonstrations, auxiliary supervision objectives, such as prediction errors on the next state (i.e.,the forward model loss) or the intermediate action between two consecutive states (i.e., the inverse modelloss), are often employed. As for DM-based IL, the objective for training DM (i.e., Eq. (88)) provides a lowerbound of the log-likelihood, which naturally connects DM with BC, as mentioned in the beginning of .2. On the other hand, VAEs, GANs, and NFs have been integrated with more advanced IL frameworks.As introduced in .2.1, the fundamental GAN-based IL algorithms are derived from MaxEntIRLand practically implemented as Adversarial IL (AIL) frameworks; NFs, as exact density estimators, have theflexibility to be adopted in various IL frameworks such as KL, LIL, and AIL, as illustrated in .2.1.In the representative work, AVRIL (introduced in .2.2), a VAE is employed to scale Bayesian IRL,but this direction remains relatively underexplored. (2) Three categories of offline RL algorithms, includingdynamic-programming-based, model-based, and trajectory-optimization-based offline RL, are introduced in.1 as necessary background. The corresponding categories for each DGM are listed in .More specifically, we note that VAE-based offline RL mainly follows policy penalty, support constraint,and pessimistic value methods, which are subcategories of dynamic-programming-based offline RL; while,DM-based offline RL mainly follows another subcategory of dynamic-programming-based offline RL policyconstraint methods. Seminal works of DGM-based offline policy learning: Regarding the applications in offline policylearning, developments for different DGMs are quite unbalanced, and even for the same type of DGM, theapplications in IL may be significantly more than the ones in offline RL, or vice versa. One main factoris whether there are seminal works in that category, since many research works would follow the seminalones for extensions. Here, we highlight the seminal works introduced in this paper: VAE - offline RL -BCQ (Fujimoto et al. (2019)), GAN - IL - GAIL (Ho & Ermon (2016)) and AIRL (Fu et al. (2017)),Transformer - offline RL - Decision Transformer (Chen et al. (2021b)) and Trajectory Transformer (Janneret al. (2021)), Diffusion Model - offline RL - Diffuser (Janner et al. (2022)) and Decision Diffuser (Ajay et al.(2023)). There are still no seminal works for NF-based offline policy learning, which explains why there arerelatively fewer works in . Notably, some seminal works pioneer new paradigms for offline policylearning. For example, GAIL converts IL/IRL to a distribution matching problem 23; Decision Transformerrealizes offline RL via trajectory optimization, which eliminates the necessity to fit value functions throughdynamic programming or to compute policy gradients; Diffuser folds the two processes in model-based offline 23Although GAIL- or AIRL-based algorithms are used for imitation learning from offline expert data, these algorithms alsorely on simulators in their learning process, because there is an inner (online) RL process in their algorithm designs. Similarly,most algorithms within .2.1 require simulators, as they also realize imitation learning via distribution matching (likeGAIL and AIRL), i.e., min d((s, a)||E (s, a)), and RL is a natural choice for optimization regarding the occupancy measureof the policy, i.e., (s, a).",
  "MT, MA,Hier, MB,Safety": ": A summary of the issues and extensions of offline policy learning targeted by the DGMs. MT,MA, Hier, MB represents multi-task, multi-agent, hierarchical, model-based learning, respectively. Safetyand generalization refer to if the learned policy can be safely applied to risky environments or be generalizableto unseen environments. RL: transition dynamic modeling and trajectory optimization, into a trajectory modeling process. Theseparadigm shifts are closely related to corresponding DGMs and make full use of their unique advantages,which open up promising directions for offline policy learning. On the other hand, simply using DGMs asfunction estimators in traditional offline RL or IL methods can be less attractive, unless the application ofDGMs offers superior scalability or generalization capabilities. A summary of the issues and extensions of offline policy learning that have been targeted bythe DGMs: In , we enumerate the IL/offline RL issues that each DGM has tried to solve, aswell as the extended IL/offline RL setups that have been explored by each DGM. Specifically, there are intotal six setup extensions, including multi-task (MT), multi-agent (MA), hierarchical (Hier), model-based(MB) learning, policy safety, and policy generalization. Readers can easily find the research works targetingat specific issues/extensions in corresponding sections. However, we note that the listed issues/extensionsshould not be considered as completely resolved by the DGMs, and future works can focus on the unsolvedor underexplored issues/extensions as detailed in .2.4.Here, we provide some remarks on thistable. (1) Low-quality training data is characterized by its limited coverage of the state-action space, alack of diversity in behavior patterns, or the inclusion of suboptimal/noisy demonstrations. (2) Multi-modaldemonstrations/actions refer to demonstrations/actions containing multiple distributional modes, while bymulti-modal input, we mean input from multiple sensors or in various formats (e.g., images and texts).(3) Learning from observations refer to imitating from sequences of states only, which is notably morechallenging than usual imitation learning. (4) Compared with BC, GAN-based IL can mitigate the issues ofinsufficient demonstrations and compounding errors. This is because GAN-based IL methods do more thanjust mimicking observed behaviors they reason about the underlying reward function from demonstrationsand utilizes it for further RL training, which complements the static demonstrations and enables the agent",
  "Perspectives on Future Research Directions": "Based on the discussions above and the main content in - 7, we present some perspectives onfuture research directions, and we categorize the content in this section as four aspects: data, benchmarking,theories, and algorithms. We believe more open problems can be gleaned from the detailed introductions inthe main content, and we have included comments on future works specific to certain DGMs in correspondingsections, such as .2.5 and the last paragraphs of , 5, 6.3, 7.2, 7.3.1.",
  "Future Works on Data-centric Research": "How would the offline policy learning agent evolve with the quality and quantity of the trainingdata?The performance of offline policy learning is closely related to the quality and quantity of theprovided offline data. Therefore, one potential research direction would be investigating how the datasetscharacteristics would impact the learned policy, as instructions for building datasets. (1) First, the staticdataset may contain noise (i.e., disturbed or misleading information) or suboptimal behaviors 24. In thiscase, open questions include how to measure the noise or suboptimality of the provided data and how robustcan the policy learning be to these data imperfections. The answers to these questions would vary withthe used algorithms, DGMs, or evaluation tasks, but the imperfection measure and robustness evaluationprotocol can be general and beneficial for algorithm development. These questions are important becauseperfect datasets are costly to build in scale and the tolerance for imperfection can greatly reduce the burdenfor data collection and process. (2) Second, the performance of an offline policy learning agent on evaluationtasks relies heavily on the coverage and diversity of the training data. An effective set of behaviors forpolicy learning should cover the possible task scenarios and provide various behavioral patterns/skills forthe agent to learn. How to measure the coverage and diversity of the dataset and how these propertieswould influence the generalization of the learned policy on evaluation tasks would be interesting questions. 24This would be an issue for IL, since IL requires expert-level demonstartions. However, for offline RL, the dataset couldcontain suboptimal behaviors as long as their rewards are correctly labeled. If there are only suboptimal trajectories in thedataset for offline RL, the learning difficulty would be high, since the agent needs to learn to stitch segments from varioustrajectories to form an optimal strategy.",
  "Future Works on Benchmarking": "Development of more realistic, challenging benchmarks. We notice that most DGM-based offline RLalgorithms are evaluated on D4RL, as shown in the tables of - 7, which is a standard benchmarkfor offline RL. However, widely-used benchmarks like D4RL, cannot fully show or evaluate the advantages ofDGM-based offline policy learning algorithms, as we have seen agents, which are implemented as two-layerneural networks and trained with a moderate amount of data, can already achieve excellent performance onthese benchmarks. To fully explore the potential and guide the development of DGM-based offline policylearning, more challenging benchmarks are required. (1) First, the benchmark dataset should be large in sizeto evaluate the scalability of the DGM-based algorithms. This includes evaluating the trend of performanceimprovement relative to the volume of training data, and determining whether DGM-based algorithms can",
  "Future Works on Theories": "Most works on applying DGMs in offline policy learning focus on algorithm designs rather than theoreticalanalysis, so more theoretical research can be done for this field. One promising direction is derivations of theconvergence rate or performance guarantee for the seminal works in DGM-based offline policy learning, suchas (Zhang et al. (2020b)) for GAIL and (Brandfonbrener et al. (2022)) for return-conditioned supervisedlearning algorithms like Decision Transformer, based on the theoretical results from either DGMs or offlineRL/IL. Moreover, we notice that there have been efforts on theoretically unifying the objectives of differentDGMs for improved learning performance and a deeper understanding of the relationship among DGMs,such as (Nielsen et al. (2020); Kingma & Gao (2023)). This group of works can greatly inspire developmentof new DGM-based offline policy learning algorithms or unified analysis of existing ones. Next, we outline some specific theoretical problems in this field. (1) First, regarding GAN-based IL (i.e., Sec-tion 4.2), so far, no approach has been developed that is both computationally efficient and can theoreticallyguarantee the recovery of the expert reward function. Also, many works propose to replace the on-policyRL within GAIL/AIRL (i.e., TRPO) with off-policy RL algorithms (e.g., DDPG) for improved sample ef-ficiency. However, this improvement lacks theoretical backup. With off-policy RL, the policy training ateach iteration is based on samples from multiple past iterations, during which the reward function keepschanging. As a result, the RL training is conducted in an unstationary MDP where typical RL algorithmswould theoretically fail. (2) Second, as mentioned in .2.5, return-conditioned supervised learning(e.g., Decision Transformer) alone is unlikely to be a general solution for offline RL problems, and it offersguarantees only when the environment dynamics are nearly deterministic. Thus, adapting these algorithmsto stochastic environments, ideally backed by theoretical optimality guarantees, is a clear necessity. Also, De-cision Transformer would imitate both high-return and low-return trajectories, and claims that the learningperformance in this manner is better than imitating high-return trajectories only. A theoretical explanationon how the low-return behavior learning would benefit the overall performance can be insightful. (3) Third,as introduced in .3.2, the algorithm design of Diffuser is mainly based on intuitions from similarproblems, such as the inpainting problem and receding horizon control, but lacks theoretical support. Tobe specific, in Eq. (102), to decide on the action choice a at state s, the trajectory generation is hardcodedto start with s, and then the action a that is right after s in the generated trajectory is adopted as a.",
  "From the discussions in .1 and the main content in - 7, we can identify many potentialfuture directions regarding the algorithm design. Here, we list some of them as examples": "Underexplored categories in the main text. For existing categories of algorithms, some of them arestill underexplored, such as GAN-based IL algorithms that do not rely on simulators (.2), NF-based IL where the NF works as the policy (.2.2), NF-based offline RL algorithms (.3.1),efficient algorithms on mitigating impacts from environmental stochasticity for transformer-based offline RL(.2.3), transformer-based offline RL which integrates traditional offline RL methods with transformerarchitectures (.2.5), DM-based offline policy learning where DMs work as data synthesizers (.3.3), and so on. Integrated use of various DGMs for offline policy learning. As mentioned in .1, each DGMhas distinct usages due to their special design and there have been a few algorithms managing to integratetwo DGMs together for improved offline policy learning performance. More in-depth exploration can bedone in this direction, especially for involving more DGMs in a unified offline policy learning framework.For example, we observe from that most DGM-based IL algorithms are based on BC, which is arelatively basic IL algorithm and has several fundamental issues 25, while NFs are integrated with variousadvanced IL frameworks due to its ability in exact density estimations. In this case, NFs can be integratedwith transformer-based DMs, where NFs extend the base IL framework and transformer-based DMs work as acore component of a generalist agent that can deal with multi-modal input and high-dimensional generations.Further, they can be integrated with VAEs, which can be used to extract task or subtask representationsfor multi-task or hierarchical learning, as introduced in .2.4. In addition, we notice that thereare some research working on integrating the advantages of various DGMs for computer vision tasks, suchas VQ-GAN (VAE+GAN+Transformer, Esser et al. (2021)), TransGAN (GAN+Transformer, Jiang et al.(2021)), DiffuseVAE (VAE+DM, Pandey et al. (2022)), DiTs (Transformer+DM, Peebles & Xie (2023)),which can be potentially utilized in offline policy learning. Unsolved issues and extensions of DGM-based offline policy learning. provides a summaryof the issues and extensions of IL and offline RL that have been explored by DGM-based algorithms, whichprovides indications for future works. (1) By checking the outlined challenges in .3, we can pinpointunresolved or inadequately-addressed issues by existing methods as future directions. For example, althoughthere have been GAN-based or DM-based algorithms targeting at the compounding error issue, the GAN-based ones rely on simulators and DM-based ones (i.e., AWE and MCNN in .2.2) are still in theearly stages of development. Similarly, in offline RL, reward sparsity is a significant issue, for which theexplorations made by NFs (i.e., NF Shaping in .3.2) and transformers (i.e., DTRD in .2.2)have tried reward shaping and reward function learning, which are both interesting directions awaiting furtherdevelopment. (2) Regarding the extensions, there can be in total six (or more) setup extensions: multi-task,multi-agent, hierarchical, model-based, safety, and generalization. illustrates the extensions thatare awaiting exploration for each DGM. The extension method can be potentially transferred among DGMs.For instance, as shown in .3.4, DM-based multi-task and hierarchical offline RL algorithms adoptthe same algorithm ideas as BOReL and OPAL (i.e., the VAE-based methods introduced in .2.4).Notably, multi-agent extensions for DGM-based offline policy learning require significant development, andexisting explorations (such as MAGAIL, MA-AIRL in .2 and DOM2, MADiff in .3.4)are still quite immature. In particular, developments of MARL algorithms for fully competitive or mixed 25As introduced in .2.1, BC is implemented as supervised learning.However, predictions made by , which islearned with BC and used for sequential decision-making, can impact future observations, thus breaching a fundamentalassumption of supervised learning (Ross & Bagnell (2010)): training inputs should be drawn from an independent and identicallydistributed population. Consequently, errors and deviations from the demonstrated behavior tend to accumulate over time,as minor mistakes lead the agent into areas of the observation space that the expert has not ventured into, known as thecompounding error. Additionally, BC learns policies merely through imitation, without engaging in reasoning, which restrictsthe generalization capability of the learned policy.",
  "(partially cooperative/competitive) multi-agent scenarios based on game theory, and integrations of DGMswith state-of-the-art CTDE MARL methods are essential future directions": "Development of generalist agents. One of the most exciting future directions for DGM-based offlinepolicy learning is developing a decision-making agent that can continually evolve with the amount of trainingdata & the size of the DGM-based policy network and can handle a range of tasks (including unseen ones).CV and NLP have made great progress in this direction, owing to the use of DGMs. To realize this foroffline policy learning, several things should be done. (1) First, the base IL or offline RL algorithm shouldbe compatible with deep and broad neural networks, so that its performance can grow with the size ofthe foundation model. This is also the reason why mainstream transformer-based and DM-based offlinepolicy learning methods (e.g., Decision Transformer and Diffuser) are based on supervised learning as inCV and NLP. Future works can work on developing deep foundation models that are compatible withpolicy-gradient methods or dynamic programming. (2) Second, besides constructing large-scale, high-qualitytraining datasets as mentioned in .2.1, some algorithm-driven efforts can be done. For example,algorithms that are robust to suboptimal training data or capable of processing multi-modal inputs shouldbe developed. Specifically, learning from demonstration videos (in third-person views) is a quite challengingbut promising research direction, as large-scale video datasets are more accessible and videos usually containmore learnable information than images or vectorized data. Agents capable of learning from videos havethe potential to continuously evolve by observing their surrounding environments during deployment. (3)Third, to enhance generalization, employing advanced meta-learning or multi-task learning techniques isadvisable. Adopting the pretraining + fine-tuning approach, similar to that used in large language andvision models, is also promising, since it enables the accumulation of training efforts. Notably, the moresignificant the difference between the source and target tasks, the more challenging it becomes to makepretraining effective, yet the more data can be utilized for training the agent. Future works in related areas. Finally, there are some related areas that remain to be explored. First,reviews on the applications of DGMs in Inverse Reinforcement Learning (IRL) or (Online) ReinforcementLearning have not yet been developed. IRL and RL are important approaches for sequential decision-making,but they both require online interactions with the environment. Second, as shown in , DGMs havebeen applied to dynamic-programming-based, model-based, and trajectory-optimization-based offline RL.There are some other branches in offline RL (Levine et al. (2020)), such as importance-sampling-based anduncertainty-estimation-based offline RL, which have the potential to be enhanced with DGMs. Notably,there is a subclass of dynamic-programming-based offline RL called one-step offline RL (Glehre et al.(2021); Brandfonbrener et al. (2021); Li et al. (2023a)). Instead of iteratively executing policy evaluationand policy improvement as in Eq. (1), one-step offline RL starts from learning an approximation of thebehavior policy, i.e., , from the offline dataset D, and its corresponding Q function Q (using temporaldifference updates). Once the Q learning phase converges, a policy can be learned with certain policyimprovement operators (e.g., the second line of Eq. (1)) until convergence, while keeping the Q functionQ fixed. Since the Q-learning stage is only executed once and does not involve the being-learned policy, the issues of OOD behaviors and overestimation (as mentioned in .1.1) can be strategicallyavoided. Empirical results from (Brandfonbrener et al. (2021)) show that one-step offline RL outperformsthe mainstream iterative offline RL algorithms (introduced in .1.1) in terms of training stabilityand policy efficacy, especially when the offline dataset lacks coverage of the state-action space. However, wehavent seen works applying DGMs to one-step offline RL yet, signaling potential areas for future research.Given the reliance of s performance on the accurately approximated and its corresponding Q function,DGMs could be used to model the behavior policy, leveraging their expressiveness and ability to handlehigh-dimensional states & multi-modal actions (as detailed in .1). DGMs can also be adopted tolearn the Q function, as exemplified by Q-transformer (Chebotar et al. (2023)) introduced in .2.1.Specifically, Li et al. (2023a) propose closed-form policy improvement operators that can be used in one-stepoffline RL, based on first-order approximations of the policy objective, which offer enhanced stability overgradient descent updates. However, their theoretical analyses assume a Gaussian behavior policy. Extendingthese closed-form solutions to accommodate non-Gaussian behavior policies would enable the use of moreadvanced DGMs, such as diffusion models and normalizing flows, as policy approximators. Third, we noticethat new foundation models and DGMs are continually emerging, such as Mamba (Gu & Dao (2023)) and",
  "Conclusion": "In this paper, we show a systematic review on the applications of DGMs in offline policy learning. In ,we provide an overview on offline policy learning methods and challenges. Then, in - 7, we introducethe applications of each DGM in both offline RL and IL, i.e., the two primary branches of offline policylearning. Each section includes both a tutorial and a survey on the respective topic. Notably, we cover nearlyall mainstream DGMs, including Variational Auto-Encoders, Generative Adversarial Networks, NormalizingFlows, Transformers, and Diffusion Models. Following these main content, we provide a summary on existingworks and our perspectives on future research directions in . As the first review paper in this field,we wish this work to be a hands-on reference for better understanding the current research progress regardingDGMs in offline policy learning and help researchers develop improved DGM-based offline RL/IL algorithms.",
  "Acknowledgement": "We would like to thank Hanhan Zhou for participating in discussions and assisting with material collection.We also express our special thanks to the editor and anonymous reviewers of TMLR for their valuablecomments, which significantly improved the manuscript. Martn Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S. Cor-rado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp,Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, JoshLevenberg, Dandelion Man, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster,Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, VijayVasudevan, Fernanda Vigas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu,and Xiaoqiang Zheng. TensorFlow: Large-scale machine learning on heterogeneous systems, 2015. URL Software available from tensorflow.org.",
  "Pieter Abbeel and Andrew Y. Ng. Apprenticeship learning via inverse reinforcement learning. In Proceedingsof the 21st International Conference on Machine Learning, volume 69. ACM, 2004": "Josh Abramson, Arun Ahuja, Arthur Brussee, Federico Carnevale, Mary Cassin, Felix Fischer, PetkoGeorgiev, Alex Goldin, Tim Harley, Felix Hill, Peter C. Humphreys, Alden Hung, Jessica Landon, Tim-othy P. Lillicrap, Hamza Merzic, Alistair Muldal, Adam Santoro, Guy Scully, Tamara von Glehn, GregWayne, Nathaniel Wong, Chen Yan, and Rui Zhu. Creating multimodal interactive agents with imitationand self-supervised learning. CoRR, abs/2112.03763, 2021.",
  "Shubhankar Agarwal, Harshit Sikchi, Cole Gulino, and Eric Wilkinson. Imitative planning using conditionalnormalizing flow. CoRR, abs/2007.16162, 2020": "Anurag Ajay, Aviral Kumar, Pulkit Agrawal, Sergey Levine, and Ofir Nachum. OPAL: offline primitivediscovery for accelerating offline reinforcement learning. In Proceedings of the 9th International Conferenceon Learning Representations. OpenReview.net, 2021. Anurag Ajay, Yilun Du, Abhi Gupta, Joshua B. Tenenbaum, Tommi S. Jaakkola, and Pulkit Agrawal. Isconditional generative modeling all you need for decision making? In Proceedings of the 11th InternationalConference on Learning Representations. OpenReview.net, 2023.",
  "Vladimir I Arnold. Ordinary differential equations. Springer Science & Business Media, 1992": "Jacob Austin, Daniel D. Johnson, Jonathan Ho, Daniel Tarlow, and Rianne van den Berg.Structureddenoising diffusion models in discrete state-spaces. In Advances in Neural Information Processing Systems34, 2021a. Jacob Austin, Daniel D Johnson, Jonathan Ho, Daniel Tarlow, and Rianne Van Den Berg.Structureddenoising diffusion models in discrete state-spaces. Advances in Neural Information Processing Systems,34:1798117993, 2021b. Omri Avrahami, Dani Lischinski, and Ohad Fried.Blended diffusion for text-driven editing of naturalimages. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.1820818218, 2022.",
  "Dmitry Baranchuk, Ivan Rubachev, Andrey Voynov, Valentin Khrulkov, and Artem Babenko. Label-efficientsemantic segmentation with diffusion models. arXiv preprint arXiv:2112.03126, 2021": "Jens Behrmann, Will Grathwohl, Ricky T. Q. Chen, David Duvenaud, and Jrn-Henrik Jacobsen. Invertibleresidual networks. In Proceedings of the 36th International Conference on Machine Learning, volume 97of Proceedings of Machine Learning Research, pp. 573582. PMLR, 2019. Marc G. Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning environment: Anevaluation platform for general agents. Journal of Artificial Intelligence Research, 47:253279, 2013.",
  "Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and WojciechZaremba. Openai gym, 2016": "TimBrooks,BillPeebles,ConnorHomes,WillDePue,YufeiGuo,LiJing,DavidSchnurr,Joe Taylor,Troy Luhman,Eric Luhman,Clarence Wing Yin Ng,Ricky Wang,and AdityaRamesh.Video generation models as world simulators, 2024.URL Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss,Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, ClemensWinter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, JackClark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Languagemodels are few-shot learners. In Advances in Neural Information Processing Systems 33, 2020. Catherine Cang, Kourosh Hakhamaneshi, Ryan Rudes, Igor Mordatch, Aravind Rajeswaran, Pieter Abbeel,and Michael Laskin. Semi-supervised offline reinforcement learning with pre-trained decision transformers,2022. URL",
  "Chang Chen, Yi-Fu Wu, Jaesik Yoon, and Sungjin Ahn. Transdreamer: Reinforcement learning with trans-former world models. CoRR, abs/2202.09481, 2022": "Changyou Chen, Chunyuan Li, Liquan Chen, Wenlin Wang, Yunchen Pu, and Lawrence Carin. Continuous-time flows for efficient inference and density estimation. In Proceedings of the 35th International Conferenceon Machine Learning, volume 80, pp. 823832. PMLR, 2018a. Cynthia Chen, Xin Chen, Sam Toyer, Cody Wild, Scott Emmons, Ian Fischer, Kuang-Huei Lee, Neel Alex,Steven H. Wang, Ping Luo, Stuart Russell, Pieter Abbeel, and Rohin Shah. An empirical investigation ofrepresentation learning for imitation. In Proceedings of the Neural Information Processing Systems Trackon Datasets and Benchmarks 1, 2021a.",
  "Daoming Chen, Ning Wang, Feng Chen, and Tony Pipe.Detrive: Imitation learning with transformerdetection for end-to-end autonomous driving. CoRR, abs/2310.14224, 2023a": "Huayu Chen, Cheng Lu, Chengyang Ying, Hang Su, and Jun Zhu. Offline reinforcement learning via high-fidelity generative behavior modeling. In Proceedings of the 11th International Conference on LearningRepresentations. OpenReview.net, 2023b. Jiayu Chen, Vaneet Aggarwal, and Tian Lan. A unified algorithm framework for unsupervised discoveryof skills based on determinantal point process. In Advances in Neural Information Processing Systems,2023c.",
  "Jiayu Chen, Tian Lan, and Vaneet Aggarwal. Hierarchical adversarial inverse reinforcement learning. IEEETransactions on Neural Networks and Learning Systems, 2023d": "Jiayu Chen, Dipesh Tamboli, Tian Lan, and Vaneet Aggarwal. Multi-task hierarchical adversarial inversereinforcement learning. In Proceedings of the 40th International Conference on Machine Learning, volume202, pp. 48954920, 2023e. Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter Abbeel, Ar-avind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning via sequence modeling.Advances in neural information processing systems, 34:1508415097, 2021b.",
  "Tian Qi Chen, Jens Behrmann, David Duvenaud, and Jrn-Henrik Jacobsen. Residual flows for invertiblegenerative modeling. In Advances in Neural Information Processing Systems 32, pp. 99139923, 2019": "Valerie Chen, Abhinav Gupta, and Kenneth Marino. Ask your humans: Using human instructions to improvegeneralization in reinforcement learning. In Proceedings of the 9th International Conference on LearningRepresentations, 2021c. Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel.Infogan: In-terpretable representation learning by information maximizing generative adversarial nets. Advances inneural information processing systems, 29, 2016.",
  "Maxime Chevalier-Boisvert, Lucas Willems, and Suman Pal. Minimalistic gridworld environment for openaigym. 2018": "Maxime Chevalier-Boisvert, Dzmitry Bahdanau, Salem Lahlou, Lucas Willems, Chitwan Saharia, Thien HuuNguyen, and Yoshua Bengio. Babyai: First steps towards grounded language learning with a human inthe loop. In International Conference on Learning Representations, volume 105, 2019. Cheng Chi, Siyuan Feng, Yilun Du, Zhenjia Xu, Eric Cousineau, Benjamin Burchfiel, and Shuran Song.Diffusion policy: Visuomotor policy learning via action diffusion. In Robotics: Science and Systems, 2023.",
  "Glen Chou and Russ Tedrake. Synthesizing stable reduced-order visuomotor policies for nonlinear systemsvia sums-of-squares optimization. CoRR, abs/2304.12405, 2023": "Kurtland Chua, Roberto Calandra, Rowan McAllister, and Sergey Levine. Deep reinforcement learning ina handful of trials using probabilistic dynamics models. In Advances in Neural Information ProcessingSystems, 2018. Junyoung Chung, Kyle Kastner, Laurent Dinh, Kratarth Goel, Aaron C. Courville, and Yoshua Bengio. Arecurrent latent variable model for sequential data. In Advances in Neural Information Processing Systems28, pp. 29802988, 2015. Karl Cobbe, Christopher Hesse, Jacob Hilton, and John Schulman. Leveraging procedural generation tobenchmark reinforcement learning. In Proceedings of the 37th International Conference on Machine Learn-ing, volume 119, pp. 20482056, 2020. Matthew Coleman, Olga Russakovsky, Christine Allen-Blanchette, and Ye Zhu. Discrete diffusion rewardguidance methods for offline reinforcement learning. In ICML 2023 Workshop: Sampling and Optimizationin Discrete Space, 2023. Robert Cornish, Anthony L. Caterini, George Deligiannidis, and Arnaud Doucet. Relaxing bijectivity con-straints with continuously indexed normalising flows. In Proceedings of the 37th International Conferenceon Machine Learning, volume 119, pp. 21332143. PMLR, 2020.",
  "Monroe D Donsker and SR Srinivasa Varadhan. Asymptotic evaluation of certain markov process expecta-tions for large timeiii. Communications on pure and applied Mathematics, 29(4):389461, 1976": "Ron Dorfman, Idan Shenfeld, and Aviv Tamar. Offline meta reinforcement learning - identifiability challengesand effective data collection strategies. In Advances in Neural Information Processing Systems 34, pp.46074618, 2021. Alexey Dosovitskiy, Germn Ros, Felipe Codevilla, Antonio M. Lpez, and Vladlen Koltun. CARLA: anopen urban driving simulator. In Proceedings of the 1st Annual Conference on Robot Learning, volume 78,pp. 116, 2017.",
  "Conor Durkan, Artur Bekasov, Iain Murray, and George Papamakarios. Neural spline flows. Advances inneural information processing systems, 32, 2019": "Scott Emmons, Benjamin Eysenbach, Ilya Kostrikov, and Sergey Levine. Rvs: What is essential for offline RLvia supervised learning? In Proceedings of the 10th International Conference on Learning Representations.OpenReview.net, 2022. Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis.In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1287312883,2021. Ben Eysenbach, Ruslan Salakhutdinov, and Sergey Levine. Search on the replay buffer: Bridging planningand reinforcement learning. In Advances in Neural Information Processing Systems 32, pp. 1522015231,2019a.",
  "Markus Freitag and Yaser Al-Onaizan. Beam search strategies for neural machine translation. arXiv preprintarXiv:1702.01806, 2017": "Gideon Joseph Freund, Elad Sarafian, and Sarit Kraus. A coupled flow approach to imitation learning. InProceedings of the 40th International Conference on Machine Learning, pp. 1035710372. PMLR, 2023. Huiqiao Fu, Kaiqiang Tang, Yuanyang Lu, Yiming Qi, Guizhou Deng, Flood Sung, and Chunlin Chen.Ess-infogail: Semi-supervised imitation learning from imbalanced demonstrations. In Advances in NeuralInformation Processing Systems 36, 2023.",
  "Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. D4RL: datasets for deep data-driven reinforcement learning. CoRR, abs/2004.07219, 2020": "Scott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without exploration.In Proceedings of the 36th International Conference on Machine Learning, volume 97, pp. 20522062.PMLR, 2019. Hiroki Furuta, Yutaka Matsuo, and Shixiang Shane Gu. Generalized decision transformer for offline hindsightinformation matching. In Proceedings of the 10th International Conference on Learning Representations.OpenReview.net, 2022. Seyed Kamyar Seyed Ghasemipour, Shixiang Gu, and Richard S. Zemel.Smile: Scalable meta inversereinforcement learning through context-conditional policies. In Advances in Neural Information ProcessingSystems 32, pp. 78797889, 2019a.",
  "Ian Goodfellow. Nips 2016 tutorial: Generative adversarial networks, 2017": "Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, AaronCourville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information processingsystems, 27, 2014a. Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron C.Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Information ProcessingSystems 27, pp. 26722680, 2014b. Will Grathwohl, Ricky TQ Chen, Jesse Bettencourt, Ilya Sutskever, and David Duvenaud. Ffjord: Free-formcontinuous dynamics for scalable reversible generative models. arXiv preprint arXiv:1810.01367, 2018.",
  "Albert Gu and Tri Dao.Mamba: Linear-time sequence modeling with selective state spaces.CoRR,abs/2312.00752, 2023": "Jiayi Guan, Shangding Gu, Zhijun Li, Jing Hou, Yiqin Yang, Guang Chen, and Changjun Jiang. Uac:Offline reinforcement learning with uncertain action constraint.IEEE Transactions on Cognitive andDevelopmental Systems, 2023. aglar Glehre, Sergio Gmez Colmenarejo, Ziyu Wang, Jakub Sygnowski, Thomas Paine, Konrad Zolna,Yutian Chen, Matthew W. Hoffman, Razvan Pascanu, and Nando de Freitas. Regularized behavior valueestimation. CoRR, abs/2103.09575, 2021. Abhishek Gupta, Vikash Kumar, Corey Lynch, Sergey Levine, and Karol Hausman. Relay policy learning:Solving long-horizon tasks via imitation and reinforcement learning. In Conference on Robot Learning,volume 100, pp. 10251037, 2019a. Abhishek Gupta, Vikash Kumar, Corey Lynch, Sergey Levine, and Karol Hausman. Relay policy learning:Solving long-horizon tasks via imitation and reinforcement learning. arXiv preprint arXiv:1910.11956,2019b. William H. Guss, Brandon Houghton, Nicholay Topin, Phillip Wang, Cayden Codel, Manuela Veloso, andRuslan Salakhutdinov. Minerl: A large-scale dataset of minecraft demonstrations. In Proceedings of the28th International Joint Conference on Artificial Intelligence, pp. 24422448. ijcai.org, 2019. Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximumentropy deep reinforcement learning with a stochastic actor.In Proceedings of the 35th InternationalConference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp. 18561865. PMLR, 2018.",
  "Jay Hambidge. The elements of dynamic symmetry. Courier Corporation, 1967": "Jungwoo Han and Jinwhan Kim.Selective data augmentation for improving the performance of offlinereinforcement learning. In International Conference on Control, Automation and Systems (ICCAS), pp.222226. IEEE, 2022. Kai Han, Yunhe Wang, Hanting Chen, Xinghao Chen, Jianyuan Guo, Zhenhua Liu, Yehui Tang, An Xiao,Chunjing Xu, Yixing Xu, et al. A survey on vision transformer. IEEE transactions on pattern analysisand machine intelligence, 45(1):87110, 2022. Philippe Hansen-Estruch, Ilya Kostrikov, Michael Janner, Jakub Grudzien Kuba, and Sergey Levine. IDQL:implicit q-learning as an actor-critic method with diffusion policies. CoRR, abs/2304.10573, 2023. Xiaotian Hao, Weixun Wang, Jianye Hao, and Yaodong Yang.Independent generative adversarial self-imitation learning in cooperative multiagent systems. In Proceedings of the 18th International Conferenceon Autonomous Agents and MultiAgent Systems, pp. 13151323, 2019. Karol Hausman, Yevgen Chebotar, Stefan Schaal, Gaurav S. Sukhatme, and Joseph J. Lim. Multi-modalimitation learning from unstructured demonstrations using generative adversarial nets. In Advances inNeural Information Processing Systems 30, pp. 12351245, 2017. Haoran He, Chenjia Bai, Kang Xu, Zhuoran Yang, Weinan Zhang, Dong Wang, Bin Zhao, and Xuelong Li.Diffusion model is an effective planner and data synthesizer for multi-task reinforcement learning. CoRR,abs/2305.18459, 2023a. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition.In 2016 IEEE Conference on Computer Vision and Pattern Recognition, pp. 770778. IEEE ComputerSociety, 2016.",
  "KS Holkar and Laxman M Waghmare. An overview of model predictive control. International Journal ofcontrol and automation, 3(4):4763, 2010": "Emiel Hoogeboom, Didrik Nielsen, Priyank Jaini, Patrick Forr, and Max Welling.Argmax flows andmultinomial diffusion: Learning categorical distributions.Advances in Neural Information ProcessingSystems, 34:1245412465, 2021. Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe, Andrea Ges-mundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for NLP. In Proceedingsof the 36th International Conference on Machine Learning, volume 97, pp. 27902799, 2019. Jifeng Hu, Yanchao Sun, Sili Huang, Siyuan Guo, Hechang Chen, Li Shen, Lichao Sun, Yi Chang, andDacheng Tao.Instructed diffuser with temporal condition guidance for offline reinforcement learning.CoRR, abs/2306.04875, 2023a.",
  "Shengchao Hu, Li Shen, Ya Zhang, and Dacheng Tao. Graph decision transformer. CoRR, abs/2303.03747,2023b": "Chin-Wei Huang, David Krueger, Alexandre Lacoste, and Aaron C. Courville. Neural autoregressive flows.In Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings ofMachine Learning Research, pp. 20832092. PMLR, 2018. Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Mohammadamin Barekatain, Simon Schmitt, andDavid Silver. Learning and planning in complex action spaces. In Proceedings of the 38th InternationalConference on Machine Learning, volume 139, pp. 44764486, 2021.",
  "Jrn-Henrik Jacobsen, Arnold W. M. Smeulders, and Edouard Oyallon. i-revnet: Deep invertible networks.In Proceedings of the 6th International Conference on Learning Representations, 2018": "Andrew Jaegle, Sebastian Borgeaud, Jean-Baptiste Alayrac, Carl Doersch, Catalin Ionescu, David Ding,Skanda Koppula, Daniel Zoran, Andrew Brock, Evan Shelhamer, Olivier J. Hnaff, Matthew M. Botvinick,Andrew Zisserman, Oriol Vinyals, and Joo Carreira. Perceiver IO: A general architecture for structuredinputs & outputs.In Proceedings of the 10th International Conference on Learning Representations.OpenReview.net, 2022.",
  "Rahul Kidambi, Aravind Rajeswaran, Praneeth Netrapalli, and Thorsten Joachims. Morel: Model-basedoffline reinforcement learning. In Advances in Neural Information Processing Systems, 2020": "Heecheol Kim, Yoshiyuki Ohmura, and Yasuo Kuniyoshi. Transformer-based deep imitation learning fordual-arm robot manipulation. In IEEE/RSJ International Conference on Intelligent Robots and Systems,pp. 89658972, 2021a. Heecheol Kim, Yoshiyuki Ohmura, and Yasuo Kuniyoshi. Memory-based gaze prediction in deep imitationlearning for robot manipulation. In International Conference on Robotics and Automation, pp. 24272433.IEEE, 2022. Heecheol Kim, Yoshiyuki Ohmura, Akihiko Nagakubo, and Yasuo Kuniyoshi.Training robots withoutrobots: Deep imitation learning for master-to-robot policy transfer. IEEE Robotics Automation Letters, 8(5):29062913, 2023. Kuno Kim, Shivam Garg, Kirankumar Shiragur, and Stefano Ermon.Reward identification in inversereinforcement learning. In Proceedings of the 38th International Conference on Machine Learning, volume139, pp. 54965505. PMLR, 2021b.",
  "Diederik P. Kingma and Max Welling.An introduction to variational autoencoders.Foundations andTrends in Machine Learning, 12(4):307392, 2019": "Durk P Kingma, Tim Salimans, Rafal Jozefowicz, Xi Chen, Ilya Sutskever, and Max Welling. Improvedvariational inference with inverse autoregressive flow. Advances in neural information processing systems,29, 2016. Thomas Kipf, Yujia Li, Hanjun Dai, Vincius Flores Zambaldi, Alvaro Sanchez-Gonzalez, Edward Grefen-stette, Pushmeet Kohli, and Peter W. Battaglia. Compile: Compositional imitation learning and execution.In Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings ofMachine Learning Research, pp. 34183428. PMLR, 2019.",
  "Yotto Koga, Heather Kerrick, and Sachin Chitta. On CAD informed adaptive robotic assembly. In IEEE/RSJInternational Conference on Intelligent Robots and Systems, pp. 1020710214. IEEE, 2022": "Ilya Kostrikov, Kumar Krishna Agrawal, Debidatta Dwibedi, Sergey Levine, and Jonathan Tompson.Discriminator-actor-critic: Addressing sample inefficiency and reward bias in adversarial imitation learn-ing. In Proceedings of the 7th International Conference on Learning Representations. OpenReview.net,2019. Ilya Kostrikov, Ashvin Nair, and Sergey Levine. Offline reinforcement learning with implicit q-learning. InProceedings of the 10th International Conference on Learning Representations. OpenReview.net, 2022. Robert Krajewski, Julian Bock, Laurent Kloeker, and Lutz Eckstein. The highd dataset: A drone dataset ofnaturalistic vehicle trajectories on german highways for validation of highly automated driving systems.In Proceedings of the 21st International Conference on Intelligent Transportation Systems, pp. 21182125.IEEE, 2018. Alex Kuefler and Mykel J. Kochenderfer. Burn-in demonstrations for multi-modal imitation learning. InProceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems, pp.10711078, 2018. Aviral Kumar, Justin Fu, Matthew Soh, George Tucker, and Sergey Levine. Stabilizing off-policy q-learningvia bootstrapping error reduction. In Advances in Neural Information Processing Systems 32, pp. 1176111771, 2019.",
  "Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for offline reinforce-ment learning. In Advances in Neural Information Processing Systems 33, 2020": "Jonathan Lacotte, Mohammad Ghavamzadeh, Yinlam Chow, and Marco Pavone. Risk-sensitive generativeadversarial imitation learning. In Proceedings of the 22nd International Conference on Artificial Intelli-gence and Statistics, volume 89, pp. 21542163, 2019. Kuang-Huei Lee, Ofir Nachum, Mengjiao Yang, Lisa Lee, Daniel Freeman, Sergio Guadarrama, Ian Fischer,Winnie Xu, Eric Jang, Henryk Michalewski, and Igor Mordatch. Multi-game decision transformers. InAdvances in Neural Information Processing Systems 35, 2022. Su-Jin Lee, Tae Yoon Chun, Hyoung Woo Lim, and Sang-Ho Lee. Path tracking control using imitationlearning with variational auto-encoder. In Proceedings of the 19th International Conference on Control,Automation and Systems, pp. 501505. IEEE, 2019.",
  "Anthony Liang, Ishika Singh, Karl Pertsch, and Jesse Thomason. Transformer adapters for robot learning.In CoRL 2022 Workshop on Pre-training Robot Learning, 2022. URL": "Hebin Liang, Zibin Dong, Yi Ma, Xiaotian Hao, Yan Zheng, and Jianye Hao.A hierarchical imitationlearning-based decision framework for autonomous driving. In Proceedings of the 32nd ACM InternationalConference on Information and Knowledge Management, pp. 46954701. ACM, 2023a. Zhixuan Liang, Yao Mu, Mingyu Ding, Fei Ni, Masayoshi Tomizuka, and Ping Luo. Adaptdiffuser: Diffusionmodels as adaptive self-evolving planners. In Proceedings of the 40th International Conference on MachineLearning, volume 202, pp. 2072520745, 2023b. Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, DavidSilver, and Daan Wierstra. Continuous control with deep reinforcement learning. In Proceedings of the4th International Conference on Learning Representations, 2016.",
  "Fangchen Liu, Zhan Ling, Tongzhou Mu, and Hao Su. State alignment-based imitation learning. In Pro-ceedings of the 8th International Conference on Learning Representations. OpenReview.net, 2020": "Xiao Liu, Fanjin Zhang, Zhenyu Hou, Li Mian, Zhaoyu Wang, Jing Zhang, and Jie Tang. Self-supervisedlearning: Generative or contrastive.IEEE Transactions on Knowledge and Data Engineering, 35(1):857876, 2023a. Zuxin Liu, Zijian Guo, Yihang Yao, Zhepeng Cen, Wenhao Yu, Tingnan Zhang, and Ding Zhao. Constraineddecision transformer for offline safe reinforcement learning. In Proceedings of the 40th International Con-ference on Machine Learning, volume 202, pp. 2161121630, 2023b. Francesco Locatello, Stefan Bauer, Mario Lucic, Gunnar Rtsch, Sylvain Gelly, Bernhard Schlkopf, andOlivier Bachem. Challenging common assumptions in the unsupervised learning of disentangled repre-sentations. In Proceedings of the 36th International Conference on Machine Learning, volume 97, pp.41144124, 2019. Ryan Lowe, Yi Wu, Aviv Tamar, Jean Harb, Pieter Abbeel, and Igor Mordatch. Multi-agent actor-critic formixed cooperative-competitive environments. In Advances in Neural Information Processing Systems 30,pp. 63796390, 2017.",
  "Yiren Lu and Jonathan Tompson. ADAIL: adaptive adversarial imitation learning. CoRR, abs/2008.12647,2020": "James Lucas, George Tucker, Roger B. Grosse, and Mohammad Norouzi. Understanding posterior collapsein generative latent variable models. In Deep Generative Models for Highly Structured Data, ICLR 2019Workshop. OpenReview.net, 2019. Jianlan Luo, Perry Dong, Jeffrey Wu, Aviral Kumar, Xinyang Geng, and Sergey Levine. Action-quantizedoffline reinforcement learning for robotic skill learning. In Conference on Robot Learning, pp. 13481361.PMLR, 2023.",
  "Jiafei Lyu, Xiaoteng Ma, Xiu Li, and Zongqing Lu. Mildly conservative q-learning for offline reinforcementlearning. In Advances in Neural Information Processing Systems 35, 2022": "Ajay Mandlekar, Danfei Xu, Josiah Wong, Soroush Nasiriany, Chen Wang, Rohun Kulkarni, Li Fei-Fei,Silvio Savarese, Yuke Zhu, and Roberto Martn-Martn. What matters in learning from offline humandemonstrations for robot manipulation. In Conference on Robot Learning, volume 164, pp. 16781690,2021a. Ajay Mandlekar, Danfei Xu, Josiah Wong, Soroush Nasiriany, Chen Wang, Rohun Kulkarni, Li Fei-Fei,Silvio Savarese, Yuke Zhu, and Roberto Martn-Martn. What matters in learning from offline humandemonstrations for robot manipulation. arXiv preprint arXiv:2108.03298, 2021b. Lucas Manuelli, Yunzhu Li, Peter R. Florence, and Russ Tedrake. Keypoints into the future: Self-supervisedcorrespondence in model-based reinforcement learning. In Proceedings of the 4th Conference on RobotLearning, volume 155, pp. 693710, 2020.",
  "ML Menndez, JA Pardo, L Pardo, and MC Pardo. The jensen-shannon divergence. Journal of the FranklinInstitute, 334(2):307318, 1997": "Linghui Meng, Muning Wen, Yaodong Yang, Chenyang Le, Xiyun Li, Weinan Zhang, Ying Wen, HaifengZhang, Jun Wang, and Bo Xu. Offline pre-trained multi-agent decision transformer: One big sequencemodel tackles all SMAC tasks. CoRR, abs/2112.02845, 2021. Gregory P Meyer, Ankit Laddha, Eric Kee, Carlos Vallespi-Gonzalez, and Carl K Wellington. Lasernet:An efficient probabilistic 3d object detector for autonomous driving. In Proceedings of the IEEE/CVFconference on computer vision and pattern recognition, pp. 1267712686, 2019.",
  "Mehdi Mirza and Simon Osindero. Conditional generative adversarial nets. arXiv preprint arXiv:1411.1784,2014": "Eric Mitchell, Rafael Rafailov, Xue Bin Peng, Sergey Levine, and Chelsea Finn. Offline meta-reinforcementlearning with advantage weighting.In Proceedings of the 38th International Conference on MachineLearning, volume 139, pp. 77807791, 2021. Shakir Mohamed and Danilo Jimenez Rezende. Variational information maximisation for intrinsically moti-vated reinforcement learning. In Advances in Neural Information Processing Systems 28, pp. 21252133,2015. Tongzhou Mu, Zhan Ling, Fanbo Xiang, Derek Yang, Xuanlin Li, Stone Tao, Zhiao Huang, Zhiwei Jia,and Hao Su. Maniskill: Generalizable manipulation skill benchmark with large-scale demonstrations. InProceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, 2021.",
  "OpenAI. Gpt-4 technical report, 2023. URL": "Yingwei Pan, Yehao Li, Yiheng Zhang, Qi Cai, Fuchen Long, Zhaofan Qiu, Ting Yao, and Tao Mei. Silver-bullet-3d at maniskill 2021: Learning-from-demonstrations and heuristic rule-based methods for objectmanipulation. In ICLR 2022 Workshop on Generalizable Policy Learning in Physical World, 2022. Kushagra Pandey, Avideep Mukherjee, Piyush Rai, and Abhishek Kumar. DiffuseVAE: Efficient, controllableand high-fidelity generation from low-dimensional latents. Transactions on Machine Learning Research,2022. ISSN 2835-8856. URL",
  "George Papamakarios, Iain Murray, and Theo Pavlakou. Masked autoregressive flow for density estimation.In Advances in Neural Information Processing Systems 30, pp. 23382347, 2017": "Emilio Parisotto and Ruslan Salakhutdinov. Efficient transformers in reinforcement learning using actor-learner distillation. In Proceedings of the 9th International Conference on Learning Representations. Open-Review.net, 2021. Emilio Parisotto, H. Francis Song, Jack W. Rae, Razvan Pascanu, aglar Glehre, Siddhant M. Jayaku-mar, Max Jaderberg, Raphal Lopez Kaufman, Aidan Clark, Seb Noury, Matthew M. Botvinick, NicolasHeess, and Raia Hadsell. Stabilizing transformers for reinforcement learning. In Proceedings of the 37thInternational Conference on Machine Learning, volume 119, pp. 74877498. PMLR, 2020. Jongjin Park, Younggyo Seo, Chang Liu, Li Zhao, Tao Qin, Jinwoo Shin, and Tie-Yan Liu. Object-awareregularization for addressing causal confusion in imitation learning.Advances in Neural InformationProcessing Systems, 34:30293042, 2021.",
  "Xue Bin Peng, Aviral Kumar, Grace Zhang, and Sergey Levine. Advantage-weighted regression: Simple andscalable off-policy reinforcement learning. CoRR, abs/1910.00177, 2019b": "Karl Pertsch, Youngwoon Lee, Yue Wu, and Joseph J. Lim. Demonstration-guided reinforcement learningwith learned skills. In Conference on Robot Learning, volume 164 of Proceedings of Machine LearningResearch, pp. 729739. PMLR, 2021. Samuel Pfrommer, Yatong Bai, Hyunin Lee, and Somayeh Sojoudi. Initial state interventions for decon-founded imitation learning. In Proceedings of the 62nd IEEE Conference on Decision and Control, pp.23122319. IEEE, 2023. Matthias Plappert, Marcin Andrychowicz, Alex Ray, Bob McGrew, Bowen Baker, Glenn Powell, JonasSchneider, Josh Tobin, Maciek Chociej, Peter Welinder, Vikash Kumar, and Wojciech Zaremba. Multi-goal reinforcement learning:Challenging robotics environments and request for research.CoRR,abs/1802.09464, 2018. Bryan A. Plummer, Liwei Wang, Chris M. Cervantes, Juan C. Caicedo, Julia Hockenmaier, and SvetlanaLazebnik.Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentencemodels. CoRR, abs/1505.04870, 2015.",
  "Aaron L Putterman, Kevin Lu, Igor Mordatch, and Pieter Abbeel. Pretraining for language conditionedimitation with transformers, 2022. URL": "Mengshi Qi, Jie Qin, Yu Wu, and Yi Yang. Imitative non-autoregressive modeling for trajectory forecastingand imputation. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1273312742. Computer Vision Foundation / IEEE, 2020. Rongjun Qin, Xingyuan Zhang, Songyi Gao, Xiong-Hui Chen, Zewen Li, Weinan Zhang, and Yang Yu.Neorl: A near real-world benchmark for offline reinforcement learning. In Advances in Neural InformationProcessing Systems 35, 2022. Ahmed Hussain Qureshi, Byron Boots, and Michael C. Yip. Adversarial imitation via variational inversereinforcement learning. In Proceedings of the 7th International Conference on Learning Representations.OpenReview.net, 2019.",
  "Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understandingby generative pre-training. mikecaptain.com, 2018": "Rafael Rafailov, Tianhe Yu, Aravind Rajeswaran, and Chelsea Finn. Offline reinforcement learning fromimages with latent space models. In Proceedings of the 3rd Annual Conference on Learning for Dynamicsand Control, volume 144, pp. 11541168, 2021. Rouhollah Rahmatizadeh, Pooya Abolghasemi, Ladislau Blni, and Sergey Levine. Vision-based multi-taskmanipulation for inexpensive robots using end-to-end learning from demonstration. In IEEE InternationalConference on Robotics and Automation, pp. 37583765. IEEE, 2018.",
  "Deepak Ramachandran and Eyal Amir. Bayesian inverse reinforcement learning. In Proceedings of the 20thInternational Joint Conference on Artificial Intelligence, pp. 25862591, 2007": "Santhosh Kumar Ramakrishnan, Aaron Gokaslan, Erik Wijmans, Oleksandr Maksymets, Alexander Clegg,John M. Turner, Eric Undersander, Wojciech Galuba, Andrew Westbury, Angel X. Chang, Manolis Savva,Yili Zhao, and Dhruv Batra. Habitat-matterport 3d dataset (HM3D): 1000 large-scale 3d environmentsfor embodied AI. In Proceedings of the Neural Information Processing Systems Track on Datasets andBenchmarks 1, 2021.",
  "Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditionalimage generation with clip latents. arXiv preprint arXiv:2204.06125, 2022": "Amir Rasouli, Randy Goebel, Matthew E. Taylor, Iuliia Kotseruba, Soheil Alizadeh, Tianpei Yang, Mont-gomery Alban, Florian Shkurti, Yuzheng Zhuang, Adam Scibior, Kasra Rezaee, Animesh Garg, DavidMeger, Jun Luo, Liam Paull, Weinan Zhang, Xinyu Wang, and Xi Chen.Neurips 2022 competition:Driving smarts, 2022. Scott E. Reed, Konrad Zolna, Emilio Parisotto, Sergio Gmez Colmenarejo, Alexander Novikov, GabrielBarth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, Tom Eccles, Jake Bruce,Ali Razavi, Ashley Edwards, Nicolas Heess, Yutian Chen, Raia Hadsell, Oriol Vinyals, Mahyar Bordbar,and Nando de Freitas. A generalist agent. Transactions on Machine Learning Research, 2022.",
  "Luigi M Ricciardi. On the transformation of diffusion processes into the wiener process. Journal of Mathe-matical Analysis and Applications, 54(1):185199, 1976": "Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjrn Ommer. High-resolutionimage synthesis with latent diffusion models. In IEEE/CVF Conference on Computer Vision and PatternRecognition, pp. 1067410685. IEEE, 2022. Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical imagesegmentation. In Medical Image Computing and Computer-Assisted InterventionMICCAI 2015: 18thInternational Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18, pp. 234241.Springer, 2015. Erick Rosete-Beas, Oier Mees, Gabriel Kalweit, Joschka Boedecker, and Wolfram Burgard. Latent plans fortask-agnostic offline reinforcement learning. In Conference on Robot Learning, volume 205 of Proceedingsof Machine Learning Research, pp. 18381849. PMLR, 2022. Stphane Ross and Drew Bagnell. Efficient reductions for imitation learning. In Proceedings of the 13thInternational Conference on Artificial Intelligence and Statistics, pp. 661668. JMLR Workshop and Con-ference Proceedings, 2010. Stphane Ross, Geoffrey J. Gordon, and Drew Bagnell. A reduction of imitation learning and structuredprediction to no-regret online learning. In Proceedings of the 14th International Conference on ArtificialIntelligence and Statistics, volume 15, pp. 627635, 2011.",
  "Stuart Russell. Learning agents for uncertain environments. In Proceedings of the 11th Annual Conferenceon Computational Learning Theory, pp. 101103. ACM, 1998": "Tim Salimans, Ian J. Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improvedtechniques for training gans. In Advances in Neural Information Processing Systems 29, pp. 22262234,2016. Mikayel Samvelyan, Tabish Rashid, Christian Schrder de Witt, Gregory Farquhar, Nantas Nardelli, TimG. J. Rudner, Chia-Man Hung, Philip H. S. Torr, Jakob N. Foerster, and Shimon Whiteson. The starcraftmulti-agent challenge. CoRR, abs/1902.04043, 2019.",
  "Vaibhav Saxena, Yotto Koga, and Danfei Xu. Constrained-context conditional diffusion models for imitationlearning. CoRR, abs/2311.01419, 2023": "Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt,Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, Timothy P. Lillicrap, and David Silver.Mastering atari, go, chess and shogi by planning with a learned model. Nature, 588(7839):604609, 2020. Julian Schrittwieser, Thomas Hubert, Amol Mandhane, Mohammadamin Barekatain, Ioannis Antonoglou,and David Silver. Online and offline reinforcement learning by planning with a learned model. In Advancesin Neural Information Processing Systems, pp. 2758027591, 2021.",
  "Ohad Shamir.Are resnets provably better than linear predictors?In Advances in Neural InformationProcessing Systems 31, pp. 505514, 2018": "Jinghuan Shang and Michael S. Ryoo. Self-supervised disentangled representation learning for third-personimitation learning. In IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 214221.IEEE, 2021. Jinghuan Shang, Kumara Kahatapitiya, Xiang Li, and Michael S. Ryoo. Starformer: Transformer withstate-action-reward representations for visual reinforcement learning. In Proceedings of the 17th EuropeanConference on Computer Vision, volume 13699, pp. 462479, 2022. Pratyusha Sharma, Lekha Mohan, Lerrel Pinto, and Abhinav Gupta.Multiple interactions made easy(MIME): large scale demonstrations data for imitation. In Proceedings of the 2nd Annual Conference onRobot Learning, volume 87, pp. 906915, 2018.",
  "Lucy Xiaoyang Shi, Archit Sharma, Tony Z. Zhao, and Chelsea Finn. Waypoint-based imitation learning forrobotic manipulation. CoRR, abs/2307.14326, 2023": "Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mottaghi, LukeZettlemoyer, and Dieter Fox. ALFRED: A benchmark for interpreting grounded instructions for everydaytasks. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1073710746,2020. Mohit Shridhar, Lucas Manuelli, and Dieter Fox.Perceiver-actor: A multi-task transformer for roboticmanipulation.In Conference on Robot Learning, CoRL 2022, 14-18 December 2022, Auckland, NewZealand, volume 205 of Proceedings of Machine Learning Research, pp. 785799. PMLR, 2022.",
  "Dylan Z Slack, Yinlam Chow, Bo Dai, and Nevan Wichers. Safer: Data-efficient and safe reinforcementlearning via skill acquisition. In ICML Decision Awareness in Reinforcement Learning Workshop, 2022": "Shagun Sodhani, Amy Zhang, and Joelle Pineau.Multi-task reinforcement learning with context-basedrepresentations. In Proceedings of the 38th International Conference on Machine Learning, volume 139 ofProceedings of Machine Learning Research, pp. 97679779. PMLR, 2021. Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learningusing nonequilibrium thermodynamics. In International conference on machine learning, pp. 22562265.PMLR, 2015. Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, and Surya Ganguli.Deep unsupervisedlearning using nonequilibrium thermodynamics. In Proceedings of the 32nd International Conference onMachine Learning, volume 37, pp. 22562265, 2015. Kihyuk Sohn, Honglak Lee, and Xinchen Yan. Learning structured output representation using deep con-ditional generative models. In Advances in Neural Information Processing Systems 28, pp. 34833491,2015.",
  "Yang Song and Stefano Ermon. Improved techniques for training score-based generative models. In Advancesin Neural Information Processing Systems 33, 2020": "Yang Song, Sahaj Garg, Jiaxin Shi, and Stefano Ermon. Sliced score matching: A scalable approach to densityand score estimation.In Proceedings of the 35th Conference on Uncertainty in Artificial Intelligence,volume 115, pp. 574584, 2019. Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and BenPoole.Score-based generative modeling through stochastic differential equations.arXiv preprintarXiv:2011.13456, 2020.",
  "Aron van den Oord, Yazhe Li, and Oriol Vinyals.Representation learning with contrastive predictivecoding. CoRR, abs/1807.03748, 2018": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ukasz Kaiser,and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30,2017. Matej Vecerk, Todd Hester, Jonathan Scholz, Fumin Wang, Olivier Pietquin, Bilal Piot, Nicolas Heess,Thomas Rothrl, Thomas Lampe, and Martin A. Riedmiller. Leveraging demonstrations for deep rein-forcement learning on robotics problems with sparse rewards. CoRR, abs/1707.08817, 2017. Siddarth Venkatraman, Shivesh Khaitan, Ravi Tej Akella, John M. Dolan, Jeff G. Schneider, and GlenBerseth. Reasoning with latent diffusion in offline reinforcement learning. CoRR, abs/2309.06599, 2023. David Venuto, Jhelum Chakravorty, Lonard Boussioux, Junhao Wang, Gavin McCracken, and Doina Pre-cup. oirl: Robust adversarial inverse reinforcement learning with temporally extended actions. CoRR,abs/2002.09043, 2020. Alexander Vidal, Samy Wu Fung, Luis Tenorio, Stanley Osher, and Levon Nurbekyan.Taming hyper-parameter tuning in continuous normalizing flows using the jko scheme. Scientific Reports, 13(1):4501,2023. Adam R. Villaflor, Zhe Huang, Swapnil Pande, John M. Dolan, and Jeff Schneider. Addressing optimismbias in sequence modeling for reinforcement learning. In Proceedings of the 39th International Conferenceon Machine Learning, volume 162, pp. 2227022283, 2022.",
  "Vladimir Vovk, Alexander Gammerman, and Glenn Shafer. Algorithmic learning in a random world, vol-ume 29. Springer, 2005": "Matthew J. Vowels, Necati Cihan Camgz, and Richard Bowden. Gated variational autoencoders: Incor-porating weak supervision to encourage disentanglement. In Proceedings of the 15th IEEE InternationalConference on Automatic Face and Gesture Recognition, pp. 125132. IEEE, 2020. Quan Vuong, Aviral Kumar, Sergey Levine, and Yevgen Chebotar.DASCO: dual-generator adversarialsupport constrained offline reinforcement learning. In Advances in Neural Information Processing Systems35, 2022.",
  "Yunke Wang, Minjing Dong, Bo Du, and Chang Xu. Imitation learning from purified demonstration. arXivpreprint arXiv:2310.07143, 2023e": "Zhendong Wang, Jonathan J. Hunt, and Mingyuan Zhou. Diffusion policies as an expressive policy class foroffline reinforcement learning. In Proceedings of the 11th International Conference on Learning Represen-tations. OpenReview.net, 2023f. Ziyu Wang, Josh Merel, Scott E. Reed, Nando de Freitas, Gregory Wayne, and Nicolas Heess.Robustimitation of diverse behaviors. In Advances in Neural Information Processing Systems 30, Long Beach,CA, USA, pp. 53205329, 2017. Ziyu Wang, Alexander Novikov, Konrad Zolna, Josh Merel, Jost Tobias Springenberg, Scott E. Reed, BobakShahriari, Noah Y. Siegel, aglar Glehre, Nicolas Heess, and Nando de Freitas.Critic regularizedregression. In Advances in Neural Information Processing Systems 33, 2020.",
  "Patrick Nadeem Ward, Ariella Smofsky, and Avishek Joey Bose. Improving exploration in soft-actor-criticwith normalizing flows policies. CoRR, abs/1906.02771, 2019": "Hua Wei, Deheng Ye, Zhao Liu, Hao Wu, Bo Yuan, Qiang Fu, Wei Yang, and Zhenhui Li. Boosting offlinereinforcement learning with residual generative modeling. In Proceedings of the 30th International JointConference on Artificial Intelligence, pp. 35743580. ijcai.org, 2021. Yao Wei, Yanchao Sun, Ruijie Zheng, Sai Vemprala, Rogerio Bonatti, Shuhang Chen, Ratnesh Madaan,Zhongjie Ba, Ashish Kapoor, and Shuang Ma. Is imitation all you need? generalized decision-makingwith dual-phase training. In IEEE/CVF International Conference on Computer Vision, pp. 1622116231,2023. Matthias Weissenbacher, Samarth Sinha, Animesh Garg, and Yoshinobu Kawahara. Koopman q-learning:Offline reinforcement learning via symmetries of dynamics. In Proceedings of the 39th International Con-ference on Machine Learning, volume 162, pp. 2364523667, 2022.",
  "Yifan Wu, George Tucker, and Ofir Nachum. Behavior regularized offline reinforcement learning. CoRR,abs/1911.11361, 2019a": "Yuchen Wu, Melissa Mozifian, and Florian Shkurti. Shaping rewards for reinforcement learning with imper-fect demonstrations using generative models. In IEEE International Conference on Robotics and Automa-tion, pp. 66286634. IEEE, 2021. Yueh-Hua Wu, Nontawat Charoenphakdee, Han Bao, Voot Tangkaratt, and Masashi Sugiyama. Imitationlearning from imperfect demonstration. In Proceedings of the 36th International Conference on MachineLearning, volume 97, pp. 68186827. PMLR, 2019b.",
  "Wei Xiao, Tsun-Hsuan Wang, Chuang Gan, and Daniela Rus. Safediffuser: Safe planning with diffusionprobabilistic models. CoRR, abs/2306.00148, 2023": "Zhisheng Xiao, Karsten Kreis, and Arash Vahdat. Tackling the generative learning trilemma with denoisingdiffusion gans. In Proceedings of the 10th International Conference on Learning Representations, 2022. Zhihui Xie, Zichuan Lin, Deheng Ye, Qiang Fu, Yang Wei, and Shuai Li. Future-conditioned unsupervisedpretraining for decision transformer.In Proceedings of the 40th International Conference on MachineLearning, volume 202, pp. 3818738203, 2023. Haoran Xu, Xianyuan Zhan, and Xiangyu Zhu. Constraints penalized q-learning for safe offline reinforcementlearning. In Thirty-Sixth AAAI Conference on Artificial Intelligence, pp. 87538760. AAAI Press, 2022a. Mengdi Xu, Yikang Shen, Shun Zhang, Yuchen Lu, Ding Zhao, Joshua B. Tenenbaum, and Chuang Gan.Prompting decision transformer for few-shot policy generalization. In Proceedings of the 39th InternationalConference on Machine Learning, volume 162, pp. 2463124645, 2022b.",
  "Yilun Xu, Ziming Liu, Max Tegmark, and Tommi S. Jaakkola. Poisson flow generative models. In Advancesin Neural Information Processing Systems 35, 2022c": "Yilun Xu, Ziming Liu, Yonglong Tian, Shangyuan Tong, Max Tegmark, and Tommi S. Jaakkola. PFGM++:unlocking the potential of physics-inspired generative models. In proceedings of the 40th InternationalConference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pp. 3856638591. PMLR, 2023. Taku Yamagata, Ahmed Khalil, and Ral Santos-Rodrguez.Q-learning decision transformer: Leverag-ing dynamic programming for conditional sequence modelling in offline RL. In Proceedings of the 40thInternational Conference on Machine Learning, volume 202, pp. 3898939007, 2023. Kai Yan, Alexander G. Schwing, and Yu-Xiong Wang. CEIP: combining explicit and implicit priors forreinforcement learning with demonstrations. In Advances in Neural Information Processing Systems 35,2022. Junming Yang, Xingguo Chen, Shengyuan Wang, and Bolei Zhang. Model-based offline policy optimizationwith adversarial network. In Proceedings of the 26th European Conference on Artificial Intelligence, volume372, pp. 28502857, 2023a.",
  "Shentao Yang, Zhendong Wang, Huangjie Zheng, Yihao Feng, and Mingyuan Zhou. A behavior regularizedimplicit policy for offline reinforcement learning. arXiv preprint arXiv:2202.09673, 2022c": "Shentao Yang, Shujian Zhang, Yihao Feng, and Mingyuan Zhou. A unified framework for alternating offlinemodel training and policy learning. In Advances in Neural Information Processing Systems 35, 2022d. Sherry Yang, Dale Schuurmans, Pieter Abbeel, and Ofir Nachum. Dichotomy of control: Separating whatyou can control from what you cannot. In Proceedings of the 11th International Conference on LearningRepresentations. OpenReview.net, 2023c.",
  "Yiming Yang, Dengpeng Xing, and Bo Xu. Efficient spatiotemporal transformer for robotic reinforcementlearning. IEEE Robotics Automation Letters, 7(3):79827989, 2022e": "Yiqin Yang, Hao Hu, Wenzhe Li, Siyuan Li, Jun Yang, Qianchuan Zhao, and Chongjie Zhang. Flow tocontrol: Offline reinforcement learning with lossless primitive discovery. In Proceedings of the 37th AAAIConference on Artificial Intelligence, pp. 1084310851. AAAI Press, 2023d. Lantao Yu, Jiaming Song, and Stefano Ermon. Multi-agent adversarial inverse reinforcement learning. InProceedings of the 36th International Conference on Machine Learning, volume 97, pp. 71947201, 2019a. Lantao Yu, Tianhe Yu, Chelsea Finn, and Stefano Ermon. Meta-inverse reinforcement learning with prob-abilistic context variables. In Advances in Neural Information Processing Systems 32, pp. 1174911760,2019b. Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman, Chelsea Finn, and Sergey Levine.Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning. In Proceedingsof the 3rd Annual Conference on Robot Learning, volume 100, pp. 10941100, 2019c. Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James Y. Zou, Sergey Levine, Chelsea Finn,and Tengyu Ma. MOPO: model-based offline policy optimization. In Advances in Neural InformationProcessing Systems 33, 2020a. Xingrui Yu, Yueming Lyu, and Ivor W. Tsang. Intrinsic reward driven imitation learning via generativemodel. In Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceed-ings of Machine Learning Research, pp. 1092510935. PMLR, 2020b.",
  "Xin Zhang, Yanhua Li, Ziming Zhang, and Zhi-Li Zhang. f-gail: Learning f-divergence for generative adver-sarial imitation learning. In Advances in Neural Information Processing Systems 33, 2020a": "Xin Zhang, Yanhua Li, Ziming Zhang, Christopher Brinton, Zhenming Liu, Zhi-Li Zhang, Hui Lu, andZhihong Tian. Stabilized likelihood-based imitation learning via denoising continuous normalizing flow.In Submission to the 10th International Conference on Learning Representations, 2021b. Yufeng Zhang, Qi Cai, Zhuoran Yang, and Zhaoran Wang. Generative adversarial imitation learning withneural network parameterization: Global optimality and convergence rate. In Proceedings of the 40thInternational Conference on Machine Learning, pp. 1104411054. PMLR, 2020b.",
  "Qinqing Zheng, Amy Zhang, and Aditya Grover. Online decision transformer. In Proceedings of the 39thInternational Conference on Machine Learning, volume 162, pp. 2704227059, 2022": "Qinqing Zheng, Mikael Henaff, Brandon Amos, and Aditya Grover. Semi-supervised offline reinforcementlearning with action-free trajectories. In Proceedings of the 40th International Conference on MachineLearning, volume 202, pp. 4233942362, 2023. Wenxuan Zhou, Sujay Bajracharya, and David Held. PLAS: latent action space for offline reinforcementlearning. In Proceedings of the 4th Conference on Robot Learning, volume 155 of Proceedings of MachineLearning Research, pp. 17191735. PMLR, 2020. Tianchen Zhu, Yue Qiu, Haoyi Zhou, and Jianxin Li. Towards long-delayed sparsity: Learning a bettertransformer through reward redistribution. In Proceedings of the 32nd International Joint Conference onArtificial Intelligence, pp. 46934701, 2023a.",
  "Zhengbang Zhu, Hanye Zhao, Haoran He, Yichao Zhong, Shenyu Zhang, Yong Yu, and Weinan Zhang.Diffusion models for reinforcement learning: A survey. CoRR, abs/2311.01223, 2023c": "Brian D. Ziebart, J. Andrew Bagnell, and Anind K. Dey. Modeling interaction via the principle of maximumcausal entropy. In Proceedings of the 27th International Conference on Machine Learning, pp. 12551262,2010. Luisa M. Zintgraf, Kyriacos Shiarlis, Maximilian Igl, Sebastian Schulze, Yarin Gal, Katja Hofmann, andShimon Whiteson.Varibad: A very good method for bayes-adaptive deep RL via meta-learning.InProceedings of the 8th International Conference on Learning Representations. OpenReview.net, 2020. Konrad Zolna, Scott E. Reed, Alexander Novikov, Sergio Gmez Colmenarejo, David Budden, Serkan Cabi,Misha Denil, Nando de Freitas, and Ziyu Wang. Task-relevant adversarial imitation learning. In Proceed-ings of the 4th Conference on Robot Learning, volume 155, pp. 247263. PMLR, 2020."
}