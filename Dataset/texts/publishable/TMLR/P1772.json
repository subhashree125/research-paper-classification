{
  "Abstract": "Advancements in recording techniques have enabled the ability to record thousands of neu-rons simultaneously, shifting the needs within the field of computational neuroscience tothat of powerful computational and statistical techniques. Copula-GP is a recently devel-oped state-of-the-art parametric mutual information estimator found to outperform othernovel non-parametric methods when utilized on highly dimensional data. Here, we utilizedCopula-GP together with Gaussian Process Factor Analysis (GPFA) to investigate the in-formation interaction between neuronal processes within the visual cortex of live mice andpupil dilation. We found usage of GPFA as a preprocessing step to Copula-GP was aneffective means of investigating neuronal dependence, allowing flexibility in analysis andfinding results in agreement with prior literature. We additionally extended Copula-GPwith a bagging framework, allowing for the aggregation of model estimations and allow-ing for more accurate estimation accuracy and representation of dependency shape. Wevalidated our bagging algorithm on simulated data sampled from known distributions, andutilized bagged Copula-GP with GPFA on said neuronal data to find results in agreementwith baseline Copula-GP but with more stability.",
  "Introduction": "Computational Neuroscience aims to answer how cognition, behavior, and learning are encoded in the brainthrough the use of machine learning and statistical methods on large amounts of neuronal data (Blundellet al., 2018; McDougal et al., 2017; Vu et al., 2018), with answers to these questions having applicationsin fields such as prosthetics, medicine, and machine learning (Jangid et al., 2021). Some of the primaryquestions on this frontier include: How is information encoded in populations of neurons, and how is thisinformation related to behaviors observed (Kudryashova et al., 2022)? As advances in recording techniques have allowed for the recording of hundreds to thousands of neuronsat once (Jun et al., 2017; Dombeck et al., 2007), the bottlenecks keeping researchers from answering thesecentral questions in neuroscience have shifted away from data-related issues to the purely computational andstatistical (Vu et al., 2018). Answering such questions with machine learning comes down to analysis of theintricate highly-dimensional multivariate dependencies (i.e. dependencies and correlations between manyrandom variables, many of which have different distributions) in recorded neuronal and behavioral data,much of which varies across spatial and temporal dimensions (Kudryashova et al., 2022; Jun et al., 2017;Glaser et al., 2019; Quian Quiroga & Panzeri, 2009). This becomes especially challenging once one considershow different behaviors can occur on a scale of hours or even days, whereas related neuronal activity canoccur on a scale of milliseconds (Pakan et al., 2018; Dombeck et al., 2007; Mathis et al., 2018), even more soas more variables are included in the data and the dependencies become even more computationally intenseto analyze accurately (the curse of dimensionality) (Koppen, 2000). Thus, in order to properly analyzesuch copious amounts of intricate high-dimensional data, we need methods that are both resistant to highdimensionality as well as equipped with the ability to properly analyze dependencies between temporally-disparate variables in time series.",
  "Published in Transactions on Machine Learning Research (12/2024)": ": Spike raster of a single drifting gratings stimulus presentation (duration of 2s), from the sessionutilized in model validation (session ID 756029989). Only those spikes with SNR lower bound of 3 and ISIviolation rate upper bound of 0.05 are shown. Note correlation in spike events present in the raster plot, aswell as heightened neuronal spike-rate at the beginning of stimulus presentation.",
  "Neuroscience Preliminaries": "In general, if a system of neurons encodes the information that goes into behaviors or cognition then weexpect uncertainty in neuron spikes to be captured in behaviors during recorded spike-trains, or in otherterms a significant quantity of mutual information and/or information interaction between spike signals andbehaviors (Gerstner et al., 2014). By extension, if we are able to accurately estimate mutual informationthen we should thusly be able to map systems of neurons to behaviors and cognition. As stated in the introduction, computational neuroscience aims to answer questions surrounding the encodingof cognition and behavior in parts of the brain. Hurdles in answering such questions in the past stemmed fromthe difficulty of recording groups of neurons behavior with certainty (Kudryashova et al., 2022), howeverwe now have the technology needed for accurate single-neuron resolution spike train trial recordings inthe form of Neuropixel probes (Steinmetz et al., 2021). As such, the hurdle of neuron-behavior-stimulusmutual information analysis is no longer centered on lack of data, moving instead to the need for accurateand efficient computational and statistical mutual information estimation techniques, as well as requiredrobustness to dimensionality when a large number of individual neurons and behavioral variables are analysed(Kudryashova et al., 2022; Onken & Panzeri, 2016; Vu et al., 2018).",
  "Related Work": "There are many choice methods in the realm of neuronal mutual information estimation. Popular novel onesinclude Bias-Improved Kraskov-Stgbauer-Grassberger (BI-KSG) (Gao et al., 2018) and the Mutual Informa-tion Neuronal Estimator (MINE) (Belghazi et al., 2021), both of which are effective non-parametric methods.The Copula-GP method implemented in the core paper by (Kudryashova et al., 2022) is a parametric estima-tor found to generally outperform MINE and BI-KSG in mutual estimation accuracy on highly-dimensionaldata ( 10 dimensions), and as such is our chosen method of neuronal mutual information analysis. It isa method based on copulas, a multivariate distribution with uniform marginals representative of cumula-tive distributions and a history of applications ranging from scientific analysis of dependency (as they aregenerally used in computational neuroscience (Jenison & Reale, 2004)) to the more purely analytical andpredictive (i.e. predicting the outcome of an insurance claim (Hu & OHagan, 2021)). We delve more intowhat copulas are and how theyre useful in section 4. Copulas have been used in mutual-information estimation and dependency analysis in a variety of in vivo andin silicon neuronal data, including spike data, 2-photon calcium imaging, and multi-modal neuronal datasets,and have been successfully used for both mutual information estimation and dependency shape analysis inneuronal data (Onken & Panzeri, 2016; Jenison & Reale, 2004; Shahbaba et al., 2014; Berkes et al., 2008).However, recent advances in GPU accelerated processing power (through the use of popular libraries suchas PyTorch (Paszke et al., 2019) and GPyTorch (Gardner et al., 2018)) have allowed for further refinementof copula implementations and expanded the number of practical use cases, allowing for the implementationof such a method for high-dimensional neuronal data, like that of Copula-GP (Kudryashova et al., 2022).",
  "Copulas: An Introduction": "Copulas are multivariate distributions with uniform marginal distributions, which themselves usually rep-resent marginal variables cumulative distributions.A d-dimensional copula function C(u1, u2, . . . , ud) :d is defined as a cumulative distribution function (CDF) of a vector on the unit hyper-cubed with uniform marginals U (Kudryashova et al., 2022; Safaai et al., 2018). Variables of non-uniformdistribution are attached to the marginals through the use of CDFs (which by definition map to the inter-val ), creating a copula as a joint CDF (Nelsen, 2006). Sklars theorem states that for a d-dimensionalrandom vector X = {X1, X2, , Xd} and its CDF FX with marginals F1, F2, , Fd, there exists a copulaC such that x Rd, x X,",
  "FX(x1, x2, . . . , xd) = C(F1(x1), F2(x2), . . . , Fd(xd)), xi R.(1)": "(Safaai et al., 2018; Van Vliet, 2023) What is most valuable about this construction is the allowance for therandom variables X1, X2, , Xd to take any distribution, allowing for meaningful dependency and mutualinformation analysis between variables of different probabilistic distributions (Safaai et al., 2018). We canalso condition the copula on some continuous variable y, allowing for the parametrization of the copula fora variable like time, phase, other marginals, etc (Kudryashova et al., 2022):",
  "i=1Cj,i|1, ,j1 (F(xj|x1, , xj1), F(xi+j|x1, , xj1)) .(3)": "A divide-and-conquer approach such as this allows us to mitigate the curse of dimensionality given suffi-cient performance in the bivariate case Mitskopoulos et al. (2022); Kudryashova et al. (2022). For flexibleparamerization of a copula, Gaussian Process (GP) can be used for parametrization. GP in essence attemptsto model a relationship between random variables within a stochastic process such that all collections of those",
  "u1u2 . . . ud.(6)": "(Ma & Sun, 2011; Jenison & Reale, 2004) As proven by Ma & Sun (2011), copula entropy is equivalent tothe negative mutual information of the marginals, i.e. I(X) = HC(X). In other words, through accurateestimation of a target distributions copula we are able to accurately estimate the mutual information betweenthat distributions marginal variables.",
  "Copula-GP": "The Copula-GP package was implemented in Python and deployed in Kudryashova et al. (2022), and is one ofthe few (if not only) practical implementations of a GP-treated copula vine model designed with GPU basedacceleration of computation via Pytorch (Paszke et al., 2019). It is also the first of such models designedwith use on neuronal data in mind, and as such literature on the uses of GP vine copulas on modellingneuronal data is heavily limited, with the main source for such claims being the core paper. In simplestterms, the package serves as a framework for GP copula model parameter estimation, copula model selection,and model deployment as a vine, with additional built in visualization and entropy extraction utility. Thecore paper also utilizes comparison to other popular methods of neuronal mutual information extraction,yielding superior results in highly-dimensional data. Any subsequent claim surrounding the Copula-GP package without a citation attached in this section comes courtesy of (Kudryashova et al.,2022). The vine construction implemented in Copula-GP used copula building blocks from five distinct families:independence, Gumbel, Gaussian, Frank, and Clayton copulas, with the independence copula preferredfor independent variables (see figure 6). The factorization of the vine is that of the C-vine described byequation 3, with accommodations made for GP-parameterization. To fully capture the tail dependenciesand negative correlations in relationships between marginal distributions, mixed copulas were utilized. Inthe core paper, these are defined as",
  "j=1j(Y )Cj(X|j(Y )),(7)": "where K is the number of elements, j is the concentration of the jth copula (cj), and j is the jth copulasparameter GPLink. The GPLink for each copula is determined by its copula variant, as shown in figure 2,with being defined by GPLink(f), where f is sampled from N(, K(X, X)) (the choice of GPLink",
  ",tM = 0,(8)": "where is the CDF of a standard normal distribution and fm is sampled from fm N(m,Km(Y, Y )).This gives us 2M 1 sets of hyper parameters to estimate, {}M kernel hyperparameters for each GPLink and {}M1 kernel hyperparameters for each concentration function , estimated via the methods describedin section E.1.",
  "Entropy and Mutual Information Estimation": "As Copula-GP models a copula distribution, possible states of variables can be sampled from it. As such, thejoint mutual information I(X1 : X2 : : Y ) = HC(X) HC(X|Y ) between parameter Y and multivariatedistribution X = {X1, X2, . . . } can be derived from C(X|Y ) (the C-vine copula Copula-GP estimates).Copula-GP possesses built-in capabilities for computing mutual information directly via the above equation,however doing so requires computation of nested integrals and as such is computationally intensive; in testing,when attempting to extract the mutual information of complex distributions we found our machine quicklyexhausted GPU memory when utilizing a single NVidia 2080Ti. We choose to find the MC estimate (Robert& Casella, 1999) of HC(X) (which is also utilized in the core paper), which involves sampling from thedistribution C(X|yi) with parameterization in S random values yi U(0, 1) and estimating the conditionalentropy as the mean",
  "Extension of Copula-GP: Bagging": "Bagging is a bootstrapping technique wherein multiple weak models are trained and their outcomes ag-gregated in some way to create a stronger model, typically reducing variance and increasing model bias.We propose a formal justification of copula bagging, taking inspiration from the Random Forest algorithmfor bagged regression (Breiman, 2001); we assume the existence of some true copula function C describinga multivariate distribution X. We take N samples {S1, S2, . . . , SN} and get a copula C(n) representing thebest copula fit possible on an individual sample Sn. We then come to a final estimate via a mean aggregationof our estimates. Suppose C(n) is a C-vine estimate, and let C(n)i,j represent the i-th copula in the j-th layerof the n-th estimate. We can create a mean mixture copula Ci,j representing the final estimate for thecorresponding true copula Ci,j as Ci,j =1NNn=1 C(n)i,j . As N , by the law of large numbers (and thefact that copulas are distributions) we find Ci,j Ci,j, and so C C. In other words, by fitting copulason individual spike-train trials and taking the aggregate, we estimate a copula that more closely aligns withthe true probabilistic relationship than those aggregated.",
  "BICn = 2 log L( Cn|X, ) + pn log |X|,(11)": "where log L( Cn|X, ) is the log-likelihood of copula Cn under observations X and estimated model parame-ters , and pn is the total number of estimated parameters of Cn (in the case of Copula-GP, each bivariatecopula estimated is a mixture copula and so possesses mixing and dependence parameters for each copulamixed; if the mixture is a singleton mixture, we only count dependence parameter. Note independence cop-ulas have no dependence parameter). BIC essentially rewards model log-likelihood with penalty in numberof parameters scaling with sample-size (Schwarz, 1978; Hu & OHagan, 2021), with small BIC values arebeing preferred (as negative as possible). We arrive at weights for the n-th model as",
  "BICm).(12)": "(Hu & OHagan, 2021) We also utilize the Akaike Information Criterion (AIC) for comparison, which is givenfor the n-th copula estimation as AICn = 2 log L(X| Cn, ) + 2pk and extract weights Wn,AIC accordinglyvia equation 12, replacing BICn with AICn. The AIC is similar to the BIC in that it rewards log-likelihoodbut has a more relaxed penalty in number of parameters.If a more complex copula matches the truedistribution, this can lead to possible improvements in aggregation accuracy, but can also lead to over-fittingvia over-parameterization (Zhou, 2021) (as we are already utilising mixtures of copulas with a high ceilingin number of parameters, we expect the latter to be true). We additionally include dynamic weighting ofestimates via calculating the above information criteria and their respective weights point-wise as opposedto setting weights to be constant over the observations X; we call point-wise aggregation dynamic baggingand constant weighting static bagging. We validate our bagging methods with different aggregation methodson data samples from randomly generated bivariate copulas in section 9. To aggregate C-vines, we bag one layer at a time; we first bag copulas in the current layer via our selectedmethod and gather cumulative conditional probabilities as pseudo-observations via the bagged copulasconditional cumulative distribution functions (ccdfs). We then utilize them for BIC, AIC, and R2 calculationsin the next layers bagging process, and in doing so effectively propagate the previous layers weightings tothe next layer.As subsequent bagged copulas will have worse BIC, AIC, and R2 calculations if theircorresponding C-vines previous layers were found to be bad fits, we in effect prioritize relationships withweaker conditioning using this method. Copula-GP estimates a parametric C-vine along a given 1-D parameterization input X, modeling eachindividual copula as a mixture and getting estimated dependence parameterizations {i,j(X)} and estimatedmixing parameterizations {i,j(Y )} for each of i mixture copulas (with each mixture copula being a mixof j(i) copula variants). As such, our actual implemented bagging procedure is to aggregate along theseestimated parameters for each unique copula variant in each mixture (we count each rotation variant of aClayton or Gumbel copula as unique; this algorithm is outlined more formally in the appendix, see algorithm1). In addition, we implemented model selection for each C-vine we aggregate, allowing for the bagged vineto contain copula mixtures possibly unexamined in the heuristic based model selection process. The C-vineaggregation process requires aggregation of copulas by layer and storing the cumulative distributions of eachcopula via the bagged copula ccdfs for weight extraction in the next layer.",
  "X = + d + , N(0, 2 )(13)": "with parameters (d, , 2 ). The Copula-GPFA process consists of GPFA applied to neuronal data to extractlatent trajectory estimations X1, X2, . . . , followed by fitting a Copula-GP C-vine estimator on neuronaltrajectories. Doing so, we are able to efficiently extract mutual information estimates describing not just theinformation interaction between the examined part of the brain and some parameterizing variable, but themutual information between the brain processes driving neurons instead of individual neurons themselves(Yu et al., 2009; Ma & Sun, 2011; Kudryashova et al., 2022), enhancing the interpretability of individualcopulas estimated. In addition, by utilizing GPFA to reduce a large number of neurons to a more manageableamount, we reduce the computational intensity of mutual information analysis; if we extract 13 trajectoriesfrom 150 neurons then from the naive estimate for the complexity of training Copula-GP estimators (seecore paper (Kudryashova et al., 2022)) we can come to an upper-bound of 1502 132 133 times fasterCopula-GP training.",
  "Bagged Copula-GP Validation Tests": "To validate our bagging methodology, we tested various Copula-GP aggregation methods alongside baselineunbagged Copula-GP on generated bivariate copulas (the true copula). C-vines are made up of bivariatecopula building blocks, with C-vine model selection essentially being made up of consecutive bivariate copulaselections.As such, validation alongside baseline Copula-GP on bivariate copulas alone is indicative ofimprovements in C-vine entropy estimation via bagging methods described. For all validation tests in this section, the task was to accurately replicate the true copulas parametricentropy and dependency shape, parameterized on some variable x. For validation, the weighted aggregationmethods utilized were 1. Copulas weighted point-wise dynamically by BIC / AIC on input, 2. Copulasweighted statically by BIC / AIC on input, and 3. naive average of copulas. In addition, we also examinecopula entropy point-wise root mean squared error (RMSE) of baseline and bagging methods. To generate the data, we drew 10000 samples from the true copula, splitting samples and their correspondingparameterizations into train and test sets (80:20 split). Validation tests 1 and 2 utilized parameterizationon a normally distributed variable x N(0.5, 0.2), restricted to the interval . Validation test 3 utilizeda parameterization in a time-based variable t scaling linearly from 0 to 1.Copula training and modelselection utilized train set samples, with accuracy validation utilizing metrics described on test set samples.All validations utilized the heuristic algorithm for individual estimator model selection, and for baggedestimations 4 estimators underwent individual training and model selection routines. We include randomseed, module version, and hardware information for test reproducability purposes in the appendix (see G). : True and predicted mean copula entropies HC (with 95% confidence interval included in parentheses)and point-wise RMSE of validation tests.Closest to actual / best scores in bold.We find that BICDynamically bagged Copula-GP consistently outperforms alternative methods in accuracy.",
  "Results": "We can see the outcome of HC(X) and interaction information I(X Y ) calculations alongside normalizedpupil dilation in figure 4. Bagged Copula-GP estimated a copula with mean entropy HC(X) = 13.4027bits (95% CI of 0.7069) and mean HC(X|Y ) = 7.3323 bits (95% CI of 5.9693). Copula entropy appearsto be highly correlated with pupil dilation, with decreases in copula entropy occurring with decreases inpupil dilation.In addition, the information interaction stays firmly negative, implying the multivariatedistribution of neuronal trajectories possesses statistical dependence on pupil dilation (thereby confirmingour hypothesis). From figure 5, we can see that like the bagged estimate the baseline negative copula entropy appears to bedependent on pupil dilation, with large dips occurring with pupil dilation. We found a mean negative copulaentropy HC(X) estimation of 13.4745 bits (95% CI of 3.0720) and a mean negative conditional copulaentropy HC(X|Y ) empirical estimation of 7.2221 bits (95% CI of 6.1190), with X being the neuronaltrajectories and Y being pupil dilation. However, we find the bagged estimate is less sensitive to smallchanges in pupil dilation than the baseline estimate.In addition, the bagged entropy estimate appears",
  "Validation Discussion": "In general, we found that bagged copula estimations utilizing 4 estimators possesses similar or better accuracyin copula entropy estimation than baseline. BIC dynamically aggregated copulas possessed the best accuracyfor all validation tests conducted, often with the best point-wise RMSE. We also found that a naive averageof copulas estimated performed surprisingly well, often times out performing other bagging methods. Fromvalidation test 1, we see that bagged estimates can catch dependencies in distributions that the baselineperceives as independent. In C-vine model selection, as copulas gain more conditioning marginal variablesrelationships tend to more closely resemble conditional independence (Kudryashova et al., 2022). As such,we may conclude that a bagged estimator might be more likely to pick up tail dependencies in conditionedrelationships where the baseline estimator might assume independence. From validation test 3 we find thatthe bagged estimator is more robust to transformations in dependency shape over time, and as such mayyield better accuracy when predicting marginal variables dependency shape for time-series. Finally, we notethat the BIC dynamically bagged estimator does not over-estimate copula entropy in validation.",
  "Exploration of Experimental Neuronal Dataset": "For this section, the objective was to utilize Copula-GPFA to confirm high statistical dependence between thevisual cortex and pupil dilation. The main data set we wished to explore is the Visual Coding: Neuropixelsdataset, a publicly available dataset containing spike-signals recorded from the visual cortex of live miceat single neuron spatial resolution utilizing novel Neuropixel probes, a high-fidelity and -resolution brainprobe developed in 2021 (Steinmetz et al., 2021). Due to the volume of in vivo data recorded, as well asthe quality metrics included in the data, it is an invaluable tool for analysis of underlying processes withinthe visual cortex. All technical claims surrounding the data in question is sourced from the technical white",
  "Runtime Breakdown": "Runtime of Copula-GPFA can be broken down into three steps: transformation into latent trajectories viaGPFA, fitting of multivariate distribution via Copula-GP on either part of whole data, and aggregationvia BIC dynamic bagging. GPFA was fit on 100 consecutive spike trains during Drifting Gratings stimuluspresentations, each of which have a uniform temporal length of 2s followed by a 1s interval during which nostimulus is presented. We fit the model for the entirety of the dataset with the dimensionality of the elbow ofthe log-likelihood plot (figure 9), reducing the original dimensionality of 178 down to a latent dimensionalityof 13. After GPFA application, little interim processing is needed: we removed drift and concatenated trialsas per the process described in section F.2 to produce semi-continuous data and mapped onto the domain(0, 1) via additional interim steps described in section F.2. We utilized Copula-GP to estimate C-vines over the concatenation of GPFA-treated trials, possessing 12layers (78 copulas total) with parameterization in normalized pupil dilation (as per steps described in D),testing both baseline unbagged and bagged Copula-GP. For baseline, Copula-GP was fit over the full semi-continuous data of 80 trials. For bagged Copula-GP, 4 individual models were fit over 4 semi-continuoussubsets of 20 trials each. For model ensembling, we utilized the BIC dynamically-weighted aggregation method. We estimated para-metric negative copula entropy HC(X|Y ) over time utilizing baseline and bagged estimates for 15 continu-ous trials, expecting either similar or lesser (more negative) copula entropy estimates of bagged Copula-GPas baseline, as was observed in validation.",
  "Discussion": "Copula-GPFA is able to efficiently provide accurate and meaningful dependency and infor-mation quantification analysis and results. We validated Copula-GPFA as a mutual information andinformation interaction extraction method on in vivo data extracted from mice, confirming significantly nega-tive information interaction between neuronal trajectories and pupil dilation and agreeing with prior findingslinking the visual cortex with pupil dilation (Bombeke et al., 2016; Ganea et al., 2020; Zylberberg et al.,2012). In addition, Copula-GPFA also allows for easy interpretability of findings, allowing for visualizationof both neuronal trajectories and information quantities over time. We also justify a significant reduction incomputational complexity via GPFA utilized with Copula-GP, using complexity estimates made in the corepaper.",
  "Further Validation, Optimization, and Use of Bagging Methods": "Our validations find that weighted copula estimate aggregation can yield effective improvements in copulatail dependency identification and copula entropy accuracy.The obvious extension to validations madewould of course be to confirm if such improvements firmly carry over to higher dimensions, however becauseCopula-GP implementation and the structure of the C-vine we believe that benefits found in the bivariatecase will propagate into higher dimensions. In addition, further refinement of aggregation methods maybe considered, i.e. clustering methods such as k-means as a way to select copula estimates for datasets.Use of parallel processing in training bagged estimators concurrently might also be a natural optimizationfor bagged Copula-GP; one could even bag bivariate copulas immediately after theyve completed modelselection on their subsets of data during vine training. That being said, benefits found in the bivariate case are still entirely applicable. Bivariate copulas are stilleffectively used in fields from computational finance (Cherubini et al., 2011) to bio-informatics (Ray et al.,2020). As such, the improvements made to Copula-GP in bivariate copula estimation via the addition ofdynamically weighted aggregation methods may be used to robustify bivariate copula selection effectivenessin such use cases. Finally, we believe there is a gap within the literature for a more comprehensive review of mutual informationestimation. While Copula-GP was tested against other commonly used methods in the core paper (KSG byKraskov et al. (2004), BI-KSG by Gao et al. (2016), and MINE by Belghazi et al. (2021)), other methods haverecently been produced with robustness to high-dimensionality. In particular, several recent novel DensityRatio Estimation (DRE) based methods have been produced, such as TRE by Rhodes et al. (2020), DRE-by Choi et al. (2022), and MDRE by Srivastava et al. (2023).",
  "plications (BigDataService), pp. 237240, March 2018. doi: 10.1109/BigDataService.2018.00042. URL": "Bence Bagi, Michael Brecht, and Juan Ignacio Sanguinetti-Scheck. Unsupervised discovery of behaviorallyrelevant brain states in rats playing hide-and-seek.Current Biology, 32(12):26402653.e4, June 2022.ISSN 09609822. doi: 10.1016/j.cub.2022.04.068. URL Mohamed Ishmael Belghazi, Aristide Baratin, Sai Rajeswar, Sherjil Ozair, Yoshua Bengio, Aaron Courville,and R. Devon Hjelm. MINE: Mutual Information Neural Estimation, August 2021. URL arXiv:1801.04062.",
  "Grard Biau and Erwan Scornet. A random forest guided tour. TEST, 25(2):197227, June 2016. ISSN1863-8260. doi: 10.1007/s11749-016-0481-7. URL": "Inga Blundell, Romain Brette, Thomas A. Cleland, Thomas G. Close, Daniel Coca, Andrew P. Davison,Sandra Diaz-Pier, Carlos Fernandez Musoles, Padraig Gleeson, Dan F. M. Goodman, Michael Hines,Michael W. Hopkins, Pramod Kumbhar, David R. Lester, Bris Marin, Abigail Morrison, Eric Mller,Thomas Nowotny, Alexander Peyser, Dimitri Plotnikov, Paul Richmond, Andrew Rowley, BernhardRumpe, Marcel Stimberg, Alan B. Stokes, Adam Tomkins, Guido Trensch, Marmaduke Woodman, andJochen Martin Eppler. Code Generation in Computational Neuroscience: A Review of Tools and Tech-niques. Frontiers in Neuroinformatics, 12, November 2018. ISSN 1662-5196. doi: 10.3389/fninf.2018.00068. URL Publisher: Frontiers. Klaas Bombeke, Wout Duthoo, Sven C. Mueller, Jens-Max Hopf, and C. Nico Boehler.Pupil size di-rectly modulates the feedforward response in human primary visual cortex independently of attention.NeuroImage, 127:6773, February 2016. ISSN 1053-8119. doi: 10.1016/j.neuroimage.2015.11.072. URL",
  "Weihao Gao, Sewoong Oh, and Pramod Viswanath. Demystifying Fixed k-Nearest Neighbor InformationEstimators, August 2016. URL arXiv:1604.03006": "Weihao Gao, Sewoong Oh, and Pramod Viswanath. Demystifying Fixed k -Nearest Neighbor Information Es-timators. IEEE Transactions on Information Theory, 64(8):56295661, August 2018. ISSN 1557-9654. doi:10.1109/TIT.2018.2807481. URL Con-ference Name: IEEE Transactions on Information Theory. Jacob Gardner, Geoff Pleiss, Kilian Q Weinberger, David Bindel, and Andrew G Wilson. GPyTorch: Black-box Matrix-Matrix Gaussian Process Inference with GPU Acceleration. In Advances in Neural InformationProcessing Systems, volume 31. Curran Associates, Inc., 2018. URL Wulfram Gerstner, Werner M. Kistler, Richard Naud, and Liam Paninski. Neuronal Dynamics: From SingleNeurons to Networks and Models of Cognition. Cambridge University Press, July 2014. ISBN 978-1-107-06083-8. Google-Books-ID: D4j2AwAAQBAJ. Joshua I. Glaser, Ari S. Benjamin, Roozbeh Farhoodi, and Konrad P. Kording. The roles of supervisedmachine learning in systems neuroscience. Progress in Neurobiology, 175:126137, April 2019. ISSN 0301-0082. doi: 10.1016/j.pneurobio.2019.01.008. URL",
  "Rick L. Jenison and Richard A. Reale. The Shape of Neural Dependence. Neural Computation, 16(4):665672, April 2004. ISSN 0899-7667. doi: 10.1162/089976604322860659. URL": "James J. Jun, Nicholas A. Steinmetz, Joshua H. Siegle, Daniel J. Denman, Marius Bauza, Brian Bar-barits, Albert K. Lee, Costas A. Anastassiou, Alexandru Andrei, aatay Aydn, Mladen Barbic, Tim-othy J. Blanche, Vincent Bonin, Joo Couto, Barundeb Dutta, Sergey L. Gratiy, Diego A. Gutnisky,Michael Husser, Bill Karsh, Peter Ledochowitsch, Carolina Mora Lopez, Catalin Mitelut, Silke Musa,Michael Okun, Marius Pachitariu, Jan Putzeys, P. Dylan Rich, Cyrille Rossant, Wei-lung Sun, KarelSvoboda, Matteo Carandini, Kenneth D. Harris, Christof Koch, John OKeefe, and Timothy D. Har-ris.Fully integrated silicon probes for high-density recording of neural activity.Nature, 551(7679):232236, November 2017.ISSN 0028-0836, 1476-4687.doi:10.1038/nature24636.URL",
  "Mario Koppen. The curse of dimensionality. 2000": "Alexander Kraskov, Harald Stgbauer, and Peter Grassberger. Estimating mutual information. PhysicalReview E, 69(6):066138, June 2004. doi: 10.1103/PhysRevE.69.066138. URL Publisher: American Physical Society. Nina Kudryashova, Theoklitos Amvrosiadis, Nathalie Dupuy, Nathalie Rochefort, and Arno Onken. Paramet-ric Copula-GP model for analyzing multidimensional neuronal and behavioral relationships. PLOS Compu-tational Biology, 18(1):e1009799, January 2022. ISSN 1553-7358. doi: 10.1371/journal.pcbi.1009799. URL Publisher:Public Library of Science. Rylan S. Larsen, Emily Turschak, Tanya Daigle, Hongkui Zeng, Jun Zhuang, and Jack Waters. Activationof neuromodulatory axon projections in primary visual cortex during periods of locomotion and pupildilation, December 2018. URL Pages: 502013Section: New Results.",
  "Jian Ma and Zengqi Sun.Mutual Information Is Copula Entropy.Tsinghua Science & Technology, 16(1):5154, February 2011. ISSN 1007-0214. doi: 10.1016/S1007-0214(11)70008-6. URL": "Alexander Mathis, Pranav Mamidanna, Kevin M. Cury, Taiga Abe, Venkatesh N. Murthy, Mackenzie Wey-gandt Mathis, and Matthias Bethge.DeepLabCut: markerless pose estimation of user-defined bodyparts with deep learning. Nature Neuroscience, 21(9):12811289, September 2018. ISSN 1546-1726. doi:10.1038/s41593-018-0209-y. URL Number:9 Publisher: Nature Publishing Group. Robert A. McDougal, Thomas M. Morse, Ted Carnevale, Luis Marenco, Rixin Wang, Michele Migliore,Perry L. Miller, Gordon M. Shepherd, and Michael L. Hines. Twenty years of ModelDB and beyond:building essential modeling tools for the future of neuroscience. Journal of Computational Neuroscience,42(1):110, February 2017. ISSN 1573-6873. doi: 10.1007/s10827-016-0623-7. URL Lazaros Mitskopoulos, Theoklitos Amvrosiadis, and Arno Onken. Mixed vine copula flows for flexible mod-eling of neural dependencies. Frontiers in Neuroscience, 16:910122, September 2022. ISSN 1662-453X.doi: 10.3389/fnins.2022.910122. URL",
  "A.OnkenandS.Panzeri.Mixedvinecopulasasjointmodelsofspikecountsandlocalfieldpotentials.December2016.URL": "Janelle MP Pakan, Valerio Francioni, and Nathalie L Rochefort. Action and learning shape the activity ofneuronal circuits in the visual cortex. Current Opinion in Neurobiology, 52:8897, October 2018. ISSN0959-4388. doi: 10.1016/j.conb.2018.04.020. URL Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, TrevorKilleen,Zeming Lin,Natalia Gimelshein,Luca Antiga,Alban Desmaison,Andreas Kopf,Ed-ward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner,Lu Fang, Junjie Bai, and Soumith Chintala.PyTorch:An Imperative Style, High-PerformanceDeep Learning Library.In Advances in Neural Information Processing Systems, volume 32. Cur-ran Associates, Inc., 2019. URL",
  "BenjaminRhodes,KaiXu,andMichaelU.Gutmann.TelescopingDensity-RatioEstima-tion.InAdvancesinNeuralInformationProcessingSystems,volume33,pp.49054916.CurranAssociates,Inc.,2020.URL": "Christian P. Robert and George Casella. Monte Carlo Integration. In Christian P. Robert and George Casella(eds.), Monte Carlo Statistical Methods, pp. 71138. Springer, New York, NY, 1999. ISBN 978-1-4757-3071-5. doi: 10.1007/978-1-4757-3071-5_3. URL Houman Safaai, Arno Onken, Christopher D. Harvey, and Stefano Panzeri. Information estimation usingnonparametric copulas. Physical Review E, 98(5):053302, November 2018. ISSN 2470-0045, 2470-0053. doi:10.1103/PhysRevE.98.053302. URL arXiv:1807.08018 [cs, math, q-bio, stat]. Eric Schulz, Maarten Speekenbrink, and Andreas Krause. A tutorial on Gaussian process regression: Mod-elling, exploring, and exploiting functions. Journal of Mathematical Psychology, 85:116, August 2018.ISSN 0022-2496.doi:10.1016/j.jmp.2018.03.001.URL",
  "Ben Van Vliet. Abe Sklars, March 2023. URL": "Mai-Anh T. Vu, Tlay Adal, Demba Ba, Gyrgy Buzski, David Carlson, Katherine Heller, Conor Lis-ton, Cynthia Rudin, Vikaas S. Sohal, Alik S. Widge, Helen S. Mayberg, Guillermo Sapiro, and Ka-fui Dzirasa. A Shared Vision for Machine Learning in Neuroscience. Journal of Neuroscience, 38(7):16011607, February 2018. ISSN 0270-6474, 1529-2401. doi: 10.1523/JNEUROSCI.0508-17.2018. URL Publisher: Society for Neuroscience Section: Tech-Sights.",
  "A.WilsonandH.Nickisch.KernelInterpolationforScalableStructuredGaus-sianProcesses(KISS-GP).March2015.URL": "Byron M. Yu, John P. Cunningham, Gopal Santhanam, Stephen I. Ryu, Krishna V. Shenoy, and ManeeshSahani. Gaussian-Process Factor Analysis for Low-Dimensional Single-Trial Analysis of Neural PopulationActivity. Journal of Neurophysiology, 102(1):614635, July 2009. ISSN 0022-3077, 1522-1598. doi: 10.1152/jn.90941.2008. URL Zhi-Hua Zhou. Why over-parameterization of deep neural networks does not overfit? Science China Informa-tion Sciences, 64(1):116101, January 2021. ISSN 1674-733X, 1869-1919. doi: 10.1007/s11432-020-2885-6.URL Ariel Zylberberg, Manuel Oliva, and Mariano Sigman. Pupil Dilation: A Fingerprint of Temporal Selec-tion During the Attentional Blink. Frontiers in Psychology, 3, August 2012. ISSN 1664-1078. doi:10.3389/fpsyg.2012.00316. URL Publisher: Frontiers.",
  "AMachine Specifications": "Experiments were ran on a machine utilizing 128 GB of system memory, an Intel Xeon Gold 6142 processor,an NVidia 2080, and an NVidia 2080Ti. Plots were made on a laptop with 8 GB of system memory and anApple M2 processor. The python version utilized for all computations and plots made was 3.10.6; Elephant1.0.0 and Copula-GP 0.0.5 were utilized for computations, and Matplotlib 3.6.1 and Seaborn 0.10.0 wereutilized for plot production.",
  "p(y) ), x X, y Y,(14)": "and the mutual information between X and Y as I(X : Y ) = H(X) H(X|Y ). Mutual information thusrepresents the reduction in bits of uncertainty surrounding X given the outcome of Y . In addition, thisquantity is symmetric (Shannon & Weaver, 1998), and so I(X : Y ) = H(Y ) H(Y |X) as well. In otherwords, mutual information describes statistical co-dependence between events X and Y .Inaddition, we note that mutual information is positive semi-definite ( 0), and so only ever represents areduction in uncertainty (this is intuitive; the unconditioned outcome of an event naturally encompasses allconditions). We extend this to the conditional mutual information of X and Y given the outcome of eventZ, I(X : Y |Z) = H(X|Z) H(X, Y |Z), where H(X, Y |Z) is the conditional joint entropy of X and Y giventhe outcome of Z, or the bits of uncertainty in the joint system of X and Y under condition z (Shannon &Weaver, 1998). Finally, mutual information is easily extendable to a definition of joint mutual information viareplacing singular entropy with its joint definition (as traditional entropy itself is not the primary concern ofthis paper, we will skip rigorously defining these terms for brevity) (Shannon & Weaver, 1998). Joint mutualinformation and its conditional counterpart are the primary quantities of interest we wish to estimate inthis paper as we wish to extract the interaction information between X and Y I(X Y ) = I(X|Y )I(X),representing the change in the mutual information between marginals of X when Y is learned, or the amountof uncertainty surrounding X captured in Y .",
  "CPython Version and Hardware Utilized": "Computations were made on the Edinburgh University compute server utilizing 128 GB of system memory,an Intel Xeon Gold 6142 processor, an NVidia 2080, and an NVidia 2080Ti. Plots were made on a laptop with8 GB of system memory and an Apple M2 processor. The python version utilized for all computations andplots made was 3.10.6; Elephant 1.0.0 and Copula-GP 0.0.5 were utilized for computations, and Matplotlib3.6.1 and Seaborn 0.10.0 were utilized for plot production. : Different GPLink functions used in Copula-GP for different copula types. GPLinks are used toparameterize dependence of the particular copula in a continuous variable. Courtesy of (Kudryashova et al.,2022).",
  "DData Retrieval and Cleaning": "Visual Coding - Neuropixels is split into recording sessions, where during each session a mouse test subjectwas exposed to varying visual stimuli, and is publicly available via the allensdk python package (visit and navigate to circuits and behavior, then Neuropixels). Stimuliwere presented in blocks of similar kinds of stimuli of varying duration. For this project, we examinespecifically the first 100 stimuli presentations of the first drifting gratings block (block 2), as this waywe mitigate variation in neuronal response due to differing stimulus length. In addition, within this blockindividual presentations are of uniform length (2 seconds of presentation, with a inter-presentation break of1 second), and so the block is easily divisible into separate spike train trials (the GPFA implementation usedis best suited for a trial-by-trial format).",
  "ECopula-GP Implementation": "The vine construction implemented in Copula-GP used copula building blocks from five distinct families:independence, Gumbel, Gaussian, Frank, and Clayton copulas, with the independence copula preferredfor independent variables (see figure 6). The factorization of the vine is that of the C-vine described byequation 3, with accommodations made for GP-parameterization. To fully capture the tail dependenciesand negative correlations in relationships between marginal distributions, mixed copulas were utilized. Inthe core paper, these are defined as",
  "j=1j(Y )Cj(X|j(Y )),(17)": "where K is the number of elements, j is the concentration of the jth copula (cj), and j is the jth copulasparameter GPLink. The GPLink for each copula is determined by its copula variant, as shown in figure 2,with being defined by GPLink(f), where f is sampled from N(, K(X, X)) (the choice of GPLinkdepends on the kind of copula; see 2). GP is also utilized to parameterize the concentrations j, which aredefined as",
  "E.1Copula-GP Model Selection and Parameter Estimation": "As stated in the body of this paper, the parameters for the distributions used in GP must be estimated. Inthe Copula-GP framework, this is accomplished through the use of stochastic variational inference (SVI),with SVI being scaled to high dimensions by means of Kernel Interpolation for Scalable Structured GaussianProcess (KISS-GP) (Wilson & Nickisch, 2015). These methods were specifically used for efficient implemen-tation in aforementioned python libraries PyTorch and GPyTorch (Paszke et al., 2019; Gardner et al., 2018).For model hyper-parameter selection, Watanabe-Akaike information criterion (WAIC) was used, a metricwhich aims to maximise the Akaike information criterion (AIC) by means of a Bayesian-approach (we morerigorously define AIC in section 7). Given f1, f2 and the true distribution g, WAIC examines the differencein log-likelihood i.e. I(g : f1)I(g : f2) = E(log f1(X)log f2(X)), and selects the model with the greatermutual information (Watanabe, 2012). The space of models is too large to find the optimal model whenconsidering the number of combinations of copulas shown in 6, and as such the core paper implements agreedy algorithm of minimising WAIC which can be used with all copula types and a heuristic algorithmspecifically tuned for certain combinations of copula types. For this project, we utilize the heuristic approachof copula selection.",
  "F.1Dimensionality Selection": "Before we can utilize GPFA, we first must isolate which number of dimensions n to reduce down to. Forthe in vivo dataset we chose n via investigating the log likelihood of GPFA fits extracted from a 3-foldcross validation, from target dimensionality n = 1 to 50. We then plotted the log likelihoods and saw bothwhere the elbow, or the point of maximum concave slope curvature representing a good enough dimensionto reduce down to (Antunes et al., 2018) of the log likelihood curve was. While there are computationalmethods for elbow selection (Antunes et al., 2018), we investigated few enough points to allow for the elbowto be selected ourselves visually.",
  "F.2Additional Post-GPFA Interim Processing Required": "Copula-GPs C-vine framework is fit on single-trial continuous, however Elephants GPFA implementationproduces trajectory data that is split into trials. A solution to this would be to concatenate trials trajectorywise, however per-trial drift in trajectory means can result in weak Copula-GP fit performance if these leadto large jump discontinuities and thus loss of smoothness in the data; the kernel for the GP-link functionsused in parameterization will require more restrictions as it encodes the smoothness of the data (amongother things) (Schulz et al., 2018; Yu et al., 2009). For the in vivo data, we found in figure 7 that driftoccurs in the 1 second inter-stimulus break in the average trial. As such, we crop this period out of eachtrial (50 points), and concatenate trials together trajectory-wise. While this is by far not a perfect solution,it allows the data to remain roughly smooth at the cost of continuity in time and residual (small) jumpdiscontinuities, as well as isolating the data to only when stimulus presentations are occurring. See figure 8for a example of trial-to-trial discontinuities created by this interim step.",
  "F.3Python Implementation of GPFA Used": "The implementation of GPFA utilized is sourced from the Elephant (Electrophysiology Analysis Toolkit)python library, with the packages 1.0.0 release (used for the contents of this paper) being published November10, 2023 (Denker & Kern, 2023). The Elephant package was specifically designed for use on neuronal data,motivated by a push to release a standardized python package for use in computation neuroscience. The",
  "(a)(b)": ": Trajectories extracted via GPFA (dimension n = 13) representing dynamics driving recordedneuronal activity. a are two trials trajectory data as functions in time that have been concatenated,giving the appearance of continuity. b is the mean trial. Note the clear vertical drift in mean andsingle trial trajectories post-stimulus presentation stop. Data is scaled to the range [0.01,0.99] to matchCopula-GPs input range of (0,1). Lines are transluscent for visibility purposes.",
  "(c)(d)": ": Figures 10a and 10b represent pupil dilation as function of time over all trials (duration 600s),raw (10a, pupil area in cm2 scaled logistically) and processed (10b, pupil area smoothed and normalized).Processing consisted of rolling mean (window of 10 entries), a robust scaling, and a min-max scaling. Notethe removal of strong outliers through processing, as well as the large range necessitating a logistic scalein values present in the raw recordings. c represents the distribution over all pupil dilation datautilized (100000 continuous samples / 100 trials). d represents the distribution over 1500 continuoussamples (15 trials). Note the use of logistic scale in figures 10a, 10c, and 10d."
}