{
  "Abstract": "Transformer models have been widely used to obtain high accuracy values in multiple fieldsincluding natural language processing (NLP), computer vision, and more. This superiorperformance typically comes at the expense of substantial computational overhead. Multi-head attention is the key factor in the success of Transformer models that has been foundto be computationally expensive. Significant research effort has been devoted to improvingattention compute efficiency by pruning redundant attention heads.A widely adoptedparadigm is to jointly learn a set of gate variables and apply thresholds on gate values toprune heads.Previous work shows a high level of sensitivity to threshold tuning whichcan limit subnetwork performance and prevent them from wider adoption in practice. Wepropose the notion of almost-sure sparsity to overcome this limitation and develop a genericframework for Pruning with Almost-Sure Sparsity (PASS) targets over attention heads. Tofurther boost efficiency, we design a novel technique, concentrator, based on which we developPASSCONC (PASS with CONCentrator). We also present a simple-yet-effective strategyto further improve subnetwork performance by clipping and selectively reopening learnedgates. We investigate PASS and PASSCONC on two widely studied architectures: encoder-decoder (ED) Transformer and encoder-only Transformer (e.g., BERT-base). Experimentson IWSLT14 German-to-English translation and GLUE benchmark tasks demonstrate thatour approaches outperform the SOTA by achieving up to 1.33 higher BLEU scores, 1.44%higher accuracy, and 60% higher attention speedups.",
  "Introduction": "Transformer models (Vaswani et al., 2017) have become a lead force in the study of natural language pro-cessing (NLP), computer vision, information retrieval, and other domains (Hua et al., 2024; 2023; Asai et al.,2024; Darcet et al., 2024; Ding et al., 2024). As Transformers grow deeper and larger, however, their ap-plication on longer contexts remains challenging because attention computation, which is at the heart ofTransformer architectures, is of quadratic time and memory complexity with respect to the input length(Dao et al., 2022). For example, Wang et al. (2020a) observed that attention computation typically accountsfor over 50% end-to-end latency of a GPT-2 model on multiple hardware platforms. Significant research efforts have been devoted to improving attention computation efficiency from two or-thogonal perspectives: reducing attention complexity and pruning attention heads. As a successful attempt",
  "(c) Pruning results": ": (a) Pruning attention heads by learning a set of gate variables. Gates take values from andapply to attention heads before summation. Heads of low gate values are more likely to be pruned. (b)With a Transformer model of 72 heads and a sparsity target aiming to prune 56 heads, previous probabilisticpruning SOTA (Xia et al., 2022) is highly sensitive to gate threshold tuning (a 0.5 threshold only prunes49 heads) while our approach consistently achieves the desired sparsity target (e.g., pruning 56 heads) in athreshold-independent manner. (c) On GLUE benchmark tasks (Wang et al., 2018), our approach achievesup to 1.4% higher average accuracy and 60% higher speedups than previous probabilistic pruning SOTA.Points in the plot represent the accuracy-speedup trade-offs achieved by different approaches under varioussparsity targets. Unpruned model performance is also included for reference purpose. at reducing attention complexity, sparse attention (Roy et al., 2021; Tay et al., 2020; Child et al., 2019)focuses on sparsifying the attention distribution over tokens for each head to improve efficiency. Linformer(Wang et al., 2020b) reduces the attention compute complexity from O(N 2) to O(N) with low-rank matrixapproximation. FlashAttention (Dao et al., 2022; Dao, 2023) focuses on memory access efficiency in attentioncomputation and achieves linear memory complexity by exploiting the asymmetric GPU memory hierarchyand minimizing unnecessary data transfers. However, implementing these approaches introduces extra chal-lenges, particularly in rewriting the attention arithmetic operations as well as the underlying CUDA kernelsto improve hardware utilization, which prevents them from wider adoption in practice. The second line of work focuses on pruning attention heads (Voita et al., 2019; Li et al., 2021; Xia et al.,2023) to achieve significant inference speedups without changing the arithmetic operations of attentionmodules and therefore can be applied to a majority of Transformer models with only minor training/fine-tuning configuration changes. These works utilize the fact that properly trained Transformers are highlyover-parameterized, and study how to extract efficient subnetworks by removing redundant heads withoutsignificant performance drops. A widely adopted paradigm is to jointly learn a set of trainable gate variablesfor each attention head, as shown in a. At test time, attention heads associated with low gate valuesare pruned subject to predefined thresholds or sparsity targets. Typically, Li et al. (2021) achieves user-specified attention head sparsity by iteratively applying the Gumbel-softmax trick (Gumbel, 1954) to selectthe top-K most important attention heads for given sparsity targets. At each training iteration, however,only selected attention heads get updated by the training optimizer (e.g., Adam (Kingma & Ba, 2015)) whichprevents models from being trained with more heads in early training stages and limits the final subnetworkperformance. Voita et al. (2019) and Xia et al. (2022) overcome this limitation by allowing all heads toparticipate in the training process and achieve sparsity in a probabilistic manner. Specifically, Voita et al.(2019) learns the probability distribution for gate values and sparsifies the models by regularizing the gateclosing probability (the likelihood that gate variables equal 0). Xia et al. (2022) follows a similar probabilisticforumulation and achieves the target sparsity in expectation by using the lagrangian multiplier (Wang et al.,2020c) to explicitly enforce sparsity constraints on attention heads. At test time, both Voita et al. (2019)and Xia et al. (2022) use a threshold (e.g., 0.5) on the gate closing probabilities to determine if an attentionhead can be pruned confidently. In practice, however, these probabilistic pruning approaches suffer from twolimitations. Firstly, setting correct thresholds is challenging and a mistakenly chosen threshold can lead tofailed sparsity targets, as illustrated in b. Secondly, thresholding gate variables at test time maybring significant architecture changes to Transformer models and lead to decayed subnetwork performance.",
  "Published in Transactions on Machine Learning Research (09/2024)": "Adina Williams, Nikita Nangia, and Samuel Bowman.A broad-coverage challenge corpus for sentenceunderstanding through inference. In Proceedings of the 2018 Conference of the North American Chapter ofthe Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers),pp. 11121122. Association for Computational Linguistics, 2018.URL Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, PierricCistac, Tim Rault, Rmi Louf, Morgan Funtowicz, et al. Transformers: State-of-the-art natural languageprocessing. In Proceedings of the 2020 conference on empirical methods in natural language processing:system demonstrations, pp. 3845, 2020.",
  "= arg min log P(D|)(1)": "where D is an observed dataset and = {1, 2, , ||} stands for the parameters of a parameterized model(e.g., a neural network). In real-world applications, we typically have model sparsity constraints to preventhigh inference latency or reduce memory footprints (Gupta & Agrawal, 2022). A recent line of work (Louizos",
  "= arg min log P(D| z)(2)": "where indicates component-wise multiplication between network parameters and the gate variables z.Typically, z is a latent variable following the posterior distribution p(z|D), which reflects the user-definedsparsity constraints.The probabilistic pruning approaches (Voita et al., 2019; Xia et al., 2022) aim tooptimize the expected likelihood over the posterior distribution of the gate variables z,",
  "= arg min log Ep(z|D)[P(D| z)](3)": "The objective function described by Eq. 3 is mathematically intractable when the posterior p(z|D) is apriori unknown. As an attempt to tackle such intractability, we can first derive the evidence lower bound ofthe log-likelihood in Eq. 3 which is a widely used technique in previous variational inference work (Vahdatet al., 2018a;b). Since we are interested in minimizing the negative log-likelihood, it gives us an upper boundfor the objective in Eq. 3 1,",
  "log Ep(z|D)[P(D| z)] Eq(z;)[log P(D| z)] + KL (q(z; )||p (z|D))(4)": "where q(z; ) is an approximate posterior distribution parameterized by = {1, 2, , ||}. Detailedderivation can be found in Appendix A.1. Minimizing this upper bound with respect to q(z; ) results inq(z; ) = p(z|D) and turns the inequality into an equality (Beal, 2003). By denoting this upper bound asL(, ), we can then formulate the learning problem as,",
  "We aim to jointly learn the optimal network parameters and the distribution of gate variables, , byminimizing the upper bound L(, )": "The foregoing analysis gives a generic framework to enforce sparsity over neural models which is agnosticto the underlying network structures. To prune attention heads, all we need is to assign each head a gatevariable and solve Eq. 5 with z = {z1, z2, , z|H|}, where H is set of all attention heads (see a).",
  "ratio log q(zi)": "p(zi|D) = . Though this surrogate circumvents the intractability issue, it is often challenging toidentify the right for a given sparsity target s (Li et al., 2021) . Other work (Xia et al., 2022) utilizessurrogates in the form of Lagrangian Multipliers (Wang et al., 2020c) to enforce sparsity in expectationfor a given target.Though this approach is able to achieve target sparsities in a probabilistic manner,its performance highly relies on the gate thresholds and may lead to limited subnetwork performance, asillustrated in Figures 1b and 1c. In light of the limitations of previous work, we introduce the notion of almost-sure sparsity and proposea novel surrogate which allows us to learn empirically good approximate posteriors as well as discoversubnetworks with desired target sparsities almost surely. The intuition behind the almost-sure sparsity isstraightforward. Note that a model has sparsity s provided a fraction s of the gates are closed in the network.From a probabilistic perspective, it is natural to ask a subnetwork to be confident about which gates should",
  "Learning Objective with Almost-sure Sparsity": "We aim to learn a good approximate posterior q(z; ) with desired almost-sure sparsity. In this paper, weadopt the Hard Concrete distribution (Louizos et al., 2018) as the basic form of the approximate posteriorq(z; ), given its continuous-discrete nature and its wide application in model pruning (Voita et al., 2019;Xia et al., 2022).",
  ": Values of Rbase and Rpass with = {1, 2} and s = 0.5": "Hard Concrete distribution has itssupport over the closed interval and non-zero probability massat 0 and 1.Hard Concrete dis-tribution is derived by stretchingand collapsing the Concrete distri-bution (Maddison et al., 2016), asillustrated in a.We in-troduce derivation details in Ap-pendix A.2. For each gate zi following Hard Concrete distribu-tion, the corresponding probabilitymass at 0 and 1 with respect toq(zi; i) are given as q(zi = 0; i) =",
  "sigmoid log i, q(zi = 1; i) = sigmoidi log11. For simplicity of notation, we de-": "note q0(i) := q(zi = 0; i), the gate closing probability, and q1(i) := q(zi = 1; i), the gate openingprobability. Due to the monotonicity of the sigmoid function, when i increases, q1(i) increases and q0(i)decreases, and gate zi is more likely to open. We further define qnb(i) = 1q0(i)q1(i) as the probabilityfor zi being non-binary. We use = 0.33, = 0.1, and = 1.1 by default, following previous work (Voitaet al., 2019). Clearly, the closing and opening probability of each zi z are differentiable functions of i ,as shown in b. By jointly learning with the network parameters, we are able to almost-surelyclose (resp. open) gates zi z by continuously increasing (resp. decreasing) the values of i , usinggradient-descent optimizers (e.g., Adam (Kingma & Ba, 2015)). At each training iteration, gates are sampledw.r.t. the learnt distribution and then applied to attention heads to achieve pruning. At the end of pruning, we want q(z; ) to achieve almost-sure sparsity for a given target s. Our strategy is todesign a learning objective that meets the desired almost-sure sparsity at its optimum, and optimize it alongwith model training. It is worth pointing out that there exists a family of learning objectives satisfying thiscriterion. However, not all of them can be easily optimized to their minimum, especially by gradient descentoptimizers (Kingma & Ba, 2015). For example, one may propose to minimize the following objective.",
  "(6)": "It can be easily seen that Rbase takes on its minimum value 0 when achieving almost-sure sparsity s. However,there exist local optima that may prevent gradient descent optimizers from converging to the global optimum.To illustrate this, for simplicity, we visualize the values of Rbase in a 2-gates setting z = {z1, z2} in .",
  "(c) Gate probability gradients": ": (a) Hard concrete distribution derived by streching-and-collapsing a Concrete distribution. (b)Closing and opening probability of gating variables are differentiable functions of i. (c) The gradient valuesof both gate closing and opening probabilities quickly approach 0 as i increases or decreases. Hard concretedistribution is parameterized with = 0.33, = 0.1, = 1.1, following previous work (Voita et al., 2019). With 2 gates and a sparsity target s = 0.5, we want one gate to be almost-surely closed and the other gatealmost-surely opened. In , such global optima correspond to the top-left and bottom-right cornerwhere one of 1 or 2 takes on a high value and the other takes on a low value. However, it can be clearlyobserved that there exist a local optimum in the top-right region which corresponds to the situation whereboth gates are open with high probability. In other words, with Rbase, if both 1 and 2 happen to takepositive values due to noise from the training process or bad initialization, the gradient descent direction willincrease the probability for both gates to be open and fail to meet the sparsity target s = 0.5 by deliveringoverly dense models. Under weak conditions2, we can prove that the gradient descent direction of Rbasealways leads to a higher opening probability for gate zi if i log(1",
  "Clipping and Reopening": "In practice, with proper training settings, the proposed approach can discover subnetworks with the desiredsparsities and high accuracy. Note that we approach almost sure sparsity by increasing or decreasing i with gradient-descent optimizers. However, as is increase or decrease, their gradients quickly converge to 0as illustrated in c. Consequently, gates closed (resp. opened) with high probability in early trainingstage are unlikely to be self-adaptively re-opened (resp.closed) in later training iterations by gradient-descent optimizers, which may lead to sub-optimal pruning results. We propose to resolve this issue with aclipping and selective reopening strategy. The idea of clipping has been widely used in training deep learningmodels to avoid gradient exploding and vanishing (Zhang et al., 2019; Koloskova et al., 2023). In this samespirit, we clip i to predefined ranges to alleviate the aforementioned issues caused by small gradients. Inour implementation, we empirically clip all is to the range to avoid the vanishing of gradientsto excessively small values (see c). Randomness has been widely observed to be helpful in neuralnetwork training (Bottou, 2010). To further incentivize training dynamics, we propose to randomly reopenclosed gates with respect to the gate quality. There is a line of work on how to measure gate qualities (Michelet al., 2019; Voita et al., 2019; Ding et al., 2017), among which we choose head confidence (Voita et al., 2019)in our implementation because it has been found to be an informative notion (Behnke & Heafield, 2020) andrequires little to no additional computation. The confidence of a head is the average maximum attentionweights of tokens in a set of sentences (Voita et al., 2019). We normalize confidence scores for each attentionhead and reopen almost-surely closed gates3 with a probability equal to the normalized scores. Notably, the clipping and reopening strategy is designed to be applied jointly. Without clipping to a properrange, an opened gate may remain fully open due to the nearly zero gradients and finally result in overly-dense subnetworks as the reopening strategy progresses. Similarly, without the randomly reopening strategy,the subnetwork may be trapped by certain local optimums and lead to suboptimal performance even withthe clipping strategy.",
  "log(1 gk1h), and r1h = wh + nh.wh denotes trainable parameter indicating head importance, nh Gumbel(0, 1) is Gumbel noise, and is a hyper-parameter that controls the annealing temperature": "Lagrangian Multiplier (LAG). A recent line of work (Xia et al., 2022; Wang et al., 2020c) employsLagrangian Multiplier (Wang et al., 2020c) to enforce sparsity in expectation. Given a sparsity target s,LAG trains models along with the regularization term Rlag = 1(s s) + 2(s s)2, where s is the expectedsparsity. 1 and 2 are trainable parameters and will be optimized jointly in training. Voita et al. (2019) (Voita). Voita et al. (2019) prunes attention heads by applying the stochastic approx-imation to L0 regularization (Louizos et al., 2018) to gate closing probabilities. Voita et al. (2019) achievespruning by jointly training models with the following regularization term Rvoita() = v|H|h=1(1 q0(h)),where v can be used to indirectly control the achieved sparsities. Following the previous work (Li et al.,2021), we use grid search to find the correct v values for each sparsity setting. As observed in Li et al. (2021), v is in general hard to tune and there could exist certain sparsity targetsthat cannot be achieved by tuning v, which makes Voitas approach not a good fit if the user demandsa sparse Transformer model of customized number of attention heads. In our work, we propose to enforcethe sparsity constraints explicitly using a novel sparsifier (see .2) and further push the envelope ofefficiency by developing the concentrator (see .3). The design of our sparsifier, concentrator, as wellas the clipping and reopening strategy has not been proposed in previous attention pruning literature andcontributes to the novelty of our work. We adopt the same implementation of DSP and Voita as in Li et al. (2021) 5. We implement LAG by trainingmodels with the regularization term Rlag = 1(ss)+2(ss)2, where s is the user-specified sparsity target,1 and 2 are trainable parameters, and s is the expected sparsity over all attention heads. Following Louizoset al. (2017), we estimate the value of each gate as zi = min{1, max{0, sigmoid(log(i)( ) + )}}, fori . The expected sparsity s over attention heads H is computed as s =1",
  "Protocols": "We express sparsity targets over attention heads H interchangeably as s (0, 1) and as integer K whereK = (1 s)|H|, the number of unpruned heads. Unless stated otherwise, for a given sparsity target K, weevaluate all methods by selecting the top-K most important heads w.r.t. the corresponding ranking metrics,i.e., the gate opening probabilities for PASS, PASSCONC, Voita, and LAG, and the head importance scorewh for DSP. Detailed hyper-parameter settings are in Appendix B. We test all methods on both architectureswith target tasks (30 training epochs for ED Transformer; 3 fine-tuning epochs for BERT-base as in Li et al.(2021)). All experiments are conducted on a high performance compute cluster equipped with NVIDIA P100GPUs (each with 12GB GPU RAM). Codebase is available on We evaluate the performance of PASS and PASSCONC in .2, demonstrate the speedups brought byPASSCONC in .3, validate the effectiveness of concentrator as well as the clipping and reopeningstrategy in .4, present the patterns of pruned attention heads in .5, and show moreperformance results in Appendix C.",
  "PASS and PASSCONC Improve Model Performance": "We investigate the model performance of subnetworks identified by PASS, PASSCONC and all baselinesunder various sparsity constraints. We compare all five methods on both ED Transformer and BERT-basemodels. The results are summarized in . Detailed performance results on each GLUE benchmarktask are in Appendix C. On IWSLT14 German-to-English translation task, PASS and PASSCONC outperform all 3 baselines in amajority of sparsity settings. When K = 16, both PASS and PASSCONC achieve BLEU scores of 32.7, whichis 1.3 higher than DSP, 1.8 higher than LAG, and 5.2 higher than Voita. On the GLUE benchmark tasks, weobserve a similar trend in high sparsity situations. When K = 16, PASS and PASSCONC achieve averagemodel accuracy of 86.27% and 85.25% respectively, while DSP drops to 84.47%, LAG drops to 83.84%, andVoita drops to 84.83%. When sparsity targets are low, PASS is able to match or outperform all 3 baselines,while PASSCONC can be outperformed by the strongest baseline DSP while still being comparable to theremaining two. One interesting observation is that Voita delivers surprisingly low accuracy in low sparsity settings (e.g.,K = 64) with GLUE benchmark tasks. The degraded performance can be attributed to its intrinsic sensitivityto the choice of v, which is used to indirectly control sparsity targets. Li et al. (2021) observed that a smallincrease in v (e.g., 0.0009 0.0014) may lead to drastic change of achieved sparsity (e.g., the numberof unpruned heads decreases from 30 to 11), which suggests that Voita is inadequate when users requiresubnetworks of pre-defined number of attention heads.",
  "PASSCONC Improves Model Efficiency": "We evaluate the attention speedups for subnetworks identified under various sparsity constraints, at inferencetime. We report the inference speedups in comparison to the unpruned model. The results are summarizedin and . Detailed results on each GLUE benchmark task can be found in Appendix C. On the GLUE benchmark tasks with BERT-base models, PASSCONC outperforms all baselines across amajority of sparsity constraints with great efficiency improvements and comparable or better accuracy (see",
  "(b) GLUE benchmark tasks": ": Subnetwork performance v.s. attention speedups on IWSLT14 De-En translation task and GLUEbenchmark tasks. We add dashed lines () to connect data points with the same sparsity constraints forbetter comparison. The unpruned model performance is included for reference purpose. ). When K = 16, PASSCONC achieves a 185.2% speedup, which is 60% higher than all baselines,and an average accuracy 85.25% that is also higher than DSP, LAG, and Voita. PASS has a better accuracybut a lower speedup. As the sparsity targets decrease (i.e, as K increases), the speedups achieved by allmethods in general goes down but PASSCONC always dominates the competition in terms of efficiency, at theprice of a relatively small drop in performance. On IWSLT14 German-to-English task with ED Transformermodel, PASSCONC outperforms all baseline methods in almost all sparsity settings (see ). WhenK = 16, PASSCONC achieves a 162.8% speedup, which is more than 20% higher than all baselines, with atleast 1.3 higher BLEU scores.",
  "Ablation Study": "Previous analysis of PASS and PASSCONC in .3 demonstrates the significant efficiency improve-ments brought about by the concentrator (see .3). In this section, we validate that (1) by introducingthe concentrator loss, PASSCONC is able to prune more attention layers entirely (a), and (2) theclipping and reopening strategy is necessary for PASSCONC to obtain significant efficiency improvements(b). We report results using BERT-base models and the GLUE benchmark tasks. In a, weobserve that PASSCONC with concentrator is able to prune up to 7 out of 12 attention layers with highsparsity targets, which is 3 layers more than not using concentrator and therefore validates its effectivenessin pruning redundant attention layers entirely to bring significant speedups. In b, we observe thatwithout the clipping and reopening strategy, the speedups achieved by PASSCONC can reduce by up to70%! This observation demonstrates the necessity of dynamically re-activating closed gates to help the modelconverge to cost-effective regions, as desired by the concentrator.",
  "(b) Clipping and Reopening": ": (a) Concentrator is critical for PASSCONC to prune more attention layers entirely.(b) Theclipping and reopening strategy is necessary for PASSCONC to obtain significant efficiency improvements.All results are reported with BERT-base models on the GLUE benchmark tasks. To disentangle the benefit of the clipping and reopening strategy from the proposed sparsity objective, weimplement the Voita approach with the clipping and reopening strategy and compare it with PASS andPASSCONC on the SST-2 dataset from the GLUE benchmark. Results are summarized in . Byclipping and reopening gates as Voita prunes attention heads, the accuracy of the resulting subnetworksdrastically drops to 50.9% when K = 16 while PASS and PASSCONC achieve accuracy at least 89.3%. Onepossible reason is that, the clipping and reopening strategy reopens fully closed gates along the pruningprocess which tends to continue reducing the subnetwork sparsity levels. Without our proposed sparsityobjective, Voita fails to converge to the desired sparsity levels and leads to decayed performance when weextract the sparse subnetworks according to the user-specified sparsity targets.",
  ":Voita augmented with clipping andreopening strategy leads to decayed subnetworkperformance": "We visualize the distribution of unpruned heads to bet-ter understand the dynamics of the pruning process (see). We report the frequency of heads unpruned byPASSCONC, averaged over all datasets (IWSLT14 De-Entranslation task for ED-Transformer and GLUE bench-mark tasks for BERT-base) and sparsity settings (K=16,32, 48, 64). In each layer, the heads are sorted by theunpruned frequency for clearer presentation. We also re-port the average number of unpruned heads per layerfor both ED-Transformer and BERT-base to provide aquantitative comparison (see Tables 3 and 4). For EDTransformer (a and table 3), we observe that theencoder self-attention heads are typically less importantthan cross-attention and decoder self-attention heads attest time, and therefore are pruned more frequently (theaverage number of unpruned encoder self-attention headsare at least 5 fewer than the other two attention types).This observation suggests the high level of redundancy inencoder attention heads from properly trained Enc-DecTransformers, which conforms with the observations inLi et al. (2021) and Voita et al. (2019). For BERT-base (b and table 4), we observe that the middlelayers (Layer 2,3,4,6) are likely to contain more unpruned heads, in comparison to the bottom (Layer 1) and",
  "Overhead Analysis": "To realize the efficiency improvements achieved by attention head pruning, we index the unpruned attentionheads and only perform matmul on the indexed tensors during inference. In Sections 4.2 to 4.4, we implementall approaches with indexing and evaluate the speedups w.r.t. the unpruned model using indexing to betterunderstand the efficiency improvements solely caused by different attention sparsities. In this subsection,we investigate the overhead of indexing by comparing our methods with indexing to the unpruned modelwithout indexing (matmul is performed on the original unpruned matrix), using a 72-head Encoder-DecoderTransformer on 2 Intel Broadwell CPUs @ 2.2GHz, and 1 NVIDIA P100 Pascal GPU. Comparison resultsare summarized in , where we report the average latency achieved by subnetworks of K unprunedheads on CPU and GPU devices separately.",
  "GPU (1e-4 s)1.61.63.24.689.110.8114.3CPU (1e-4 s)16.916.227.544.368.182.295.5102.996": "Intuitively, on CPU-only devices where matmul operation is dominantly time-consuming, the overhead ofindexing could be negligible and the subnetworks can achieve speed-ups up to 48 heads unpruned out of 72.In contrast, on devices specialized for in-parallel matmul operations such as GPUs, the indexing approachmay cause non-negligible overheads and head pruning achieves speed-ups only with high-sparsity targets,such as when less than 8 heads are retained from the subnetworks, as shown in . It is worth noting that, indexing in our work is used to reflect the attention sparsities and is only necessarywhen we assume no specialized software supports. With software supports such as high-performance sparsematrix multiplication libraries 6, the indexing operation is no longer necessary and can be completely removedto avoid overheads.",
  "Related Work": "Unstructured pruning has been well studied in the literature (Gupta & Agrawal, 2022) and dates back to Op-timal Brain Damage (LeCun et al., 1989). Unstructured pruning prunes individual parameters and identifiessubnetworks of high sparsity. However, unstructured pruning hardly achieves practical efficiency improve-ments without specialized software and hardware support (Xia et al., 2022). In contrast, structured pruningprunes groups of parameters within certain structures (e.g., channels and attention heads). Structured prun-ing has been widely explored in computer vision tasks (He et al., 2017) and has started to attract researchinterest in the NLP community. Research efforts have been devoted to designing pruning strategies at bothcoarse- and fine-grained levels (Xia et al., 2022; Prasanna et al., 2020a) over structures like feed-forwardlayers and attention heads. Previous work on attention head pruning (Li et al., 2021; Michel et al., 2019;Voita et al., 2019) either highly relies on threshold tuning or enforces hard structural constraints on modelstructure in the early training stages, which could lead to limited subnetwork performance. We focus onstructured pruning and propose the notion of almost-sure sparsity to overcome the above limitations. In addition to pruning, many other techniques have been developed to obtain inference efficiency for deeplearning models. Other than sparsity over the number of attention heads, a line of work (Roy et al., 2021;Child et al., 2019) focuses on sparsifying the attention distribution over tokens for each head to improveefficiency and head diversity. Recently, Correia et al. (2019) and Treviso et al. (2021) propose to adaptivelysparsify the attention distribution by enforcing low attention scores to be exactly 0 through -entmax (Peterset al., 2019). Linformer (Wang et al., 2020b) develops efficient self-attention design of linear complexity byusing low-rank matrix approximation. FlashAttention (Dao et al., 2022; Dao, 2023) focuses on I/O access inattention computation and achieves linear memory complexity by leveraging the hardware memory hierarchyand minimizing unnecessary data transfers.Other techniques include quantization (Shen et al., 2020),knowledge distillation (Hinton et al., 2015), parameter sharing (Ling et al., 2015), tensor decomposition(Oseledets, 2011), neural architecture search (Zhang et al., 2024; Zheng et al., 2023) and more. We referinterested readers to (Menghani, 2023; Gupta & Agrawal, 2022; Treviso et al., 2022) for a comprehensivesurvey.",
  "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep bidirec-tional transformers for language understanding. CoRR, abs/1810.04805, 2018. URL": "Dujian Ding, Ankur Mallick, Chi Wang, Robert Sim, Subhabrata Mukherjee, Victor Rhle, Laks V. S.Lakshmanan, and Ahmed Hassan Awadallah. Hybrid LLM: Cost-efficient and quality-aware query routing.In The Twelfth International Conference on Learning Representations, 2024. URL Yanzhuo Ding, Yang Liu, Huanbo Luan, and Maosong Sun. Visualizing and understanding neural machinetranslation. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics(Volume 1: Long Papers), pp. 11501159, Vancouver, Canada, July 2017. Association for ComputationalLinguistics. doi: 10.18653/v1/P17-1106. URL",
  "Ivan V Oseledets. Tensor-train decomposition. SIAM Journal on Scientific Computing, 33(5):22952317,2011": "Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and MichaelAuli. fairseq: A fast, extensible toolkit for sequence modeling. arXiv preprint arXiv:1904.01038, 2019. Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluationof machine translation. In Proceedings of the 40th annual meeting of the Association for ComputationalLinguistics, pp. 311318, 2002.",
  "Lakshay Sharma, Laura Graesser, Nikita Nangia, and Utku Evci. Natural language understanding with thequora question pairs dataset. arXiv preprint arXiv:1907.01041, 2019": "Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, Amir Gholami, Michael W Mahoney, and KurtKeutzer. Q-bert: Hessian based ultra low precision quantization of bert. In Proceedings of the AAAIConference on Artificial Intelligence, volume 34, pp. 88158821, 2020. Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, andChristopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. InProceedings of the 2013 conference on empirical methods in natural language processing, pp. 16311642,2013.",
  "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser,and Illia Polosukhin. Attention is all you need. CoRR, abs/1706.03762, 2017. URL": "Elena Voita, David Talbot, Fedor Moiseev, Rico Sennrich, and Ivan Titov.Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned. CoRR, abs/1905.09418, 2019.URL Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. GLUE: Amulti-task benchmark and analysis platform for natural language understanding. CoRR, abs/1804.07461,2018. URL",
  "Jingzhao Zhang, Tianxing He, Suvrit Sra, and Ali Jadbabaie. Why gradient clipping accelerates training: Atheoretical justification for adaptivity. arXiv preprint arXiv:1905.11881, 2019": "Shaokun Zhang, Xiawu Zheng, Guilin Li, Chenyi Yang, Yuchao Li, Yan Wang, Fei Chao, Mengdi Wang,Shen Li, and Rongrong Ji. You only compress once: Towards effective and elastic bert compression viaexploitexplore stochastic nature gradient. Neurocomputing, 599:128140, 2024. Xiawu Zheng, Chenyi Yang, Shaokun Zhang, Yan Wang, Baochang Zhang, Yongjian Wu, Yunsheng Wu, LingShao, and Rongrong Ji. Ddpnas: Efficient neural architecture search via dynamic distribution pruning.International Journal of Computer Vision, 131(5):12341249, 2023.",
  "where sigmoid(x) =1": "1+ex is the sigmoid function and 0 < < 1 is a constant. We can obtain the HardConcrete distribution by first stretching the support of the Concrete distribution from (0, 1) to (, ), where < 0, > 1, and then collapsing the probability mass on (, 0] (resp. [1, )) to the endpoint 0 (resp. 1).",
  "Assuming there is i having the equality 2fa(i) = fa(i).It is easy to show that for i i, theinequality 2fa(i) fa(i) holds true due to the monotonicity of fa(i) and fa(i), and hence we haveRbase()": "i 0 (the gradient descent direction). Notably, it indicates that, for i i, any gradient-descentoptimizer (e.g., Adam (Kingma & Ba, 2015)) will continue increasing the value of i which opens more gatesand leads to undesired low sparsity, as dicussed in .2. We can derive the value of i by solving",
  "We include the proof of Lemma 1 as follows": "Proof: We prove this lemma by showing that, in every possible case, the gradient descent direction of theobjective Rpass always leads to a subregion in the search space where the expected sparsity is no more thans and expected density is no more than 1 s. Recall that we define expected density as1||||i=1 q1(i), and the expected sparsity as1||||i=1 q0(i). For the simplicity of proof language, we use Ed to denote theexpected density and Es to denote the expected sparsity. Intuitively, as the expected density increases (resp.decreases), the expected sparsity will monotonically decrease (resp. increase). One important observation isthat, the sum of expected density and sparsity is always less than 1, due to the fact that q1(i) + q0(i) =1 qnb(i) and qnb(i) > 0. Given a sparsity target s, depending on the values of expected density and sparsity, there are three possiblesituations: (i) the easiest case is that the model is already neither over-sparse nor over-dense (i.e.,Ed 1 s and Es s). In this case, we can rewrite Rpass(, s) = 2 ||i=1 qnb(i). Minimizing Rpass(, s)amounts to minimizing qnb(i), which polarizes the gates to achieve the required almost-sure sparsity, untileither Ed = 1 s, Es < s or Ed < 1 s, Es = s both of which satisfy Lemma 1. (ii) high expecteddensity and low expected sparsity (i.e., Ed 1 s and Es s), in which case we can rewrite Eq. 7 toobtain Rpass(, s) = ||i=1 qnb(i)+s||||i=1 q0(i)(1s)||+||i=1 q1(i). Because qnb(i)+q1(i) =1q0(i), we can further simplify the equation to be Rpass(, s) = 2 ||i=1sq0(i). Clearly, minimizingRpass(, s) amounts to increasing q0(i) that leads to lower expected density and higher expected sparsityuntil Ed = 1 s, Es < s, which satisfies Lemma 1;(iii) low expected density and high expectedsparsity (i.e., Ed 1 s and Es s). Similarly, we can rewrite Rpass(, s) = 2 ||i=11 s q1(i).Minimizing Rpass(, s) amounts to increasing q1(i) that brings down expected sparsity as well as increasesexpected density until Ed < 1 s, Es = s, which satisfies Lemma 1; It is impossible to have both highexpected density and expected sparsity due to Es + Ed < 1. We have shown that Lemma 1 holds inall possible cases.2",
  "= base #train_itr / #n_step0(16)": "where base and 0 are hyper-parameters. We choose #n_step as 1, 000 in all experiments. We use the sameinverse square root learning rate schedules for model training as in Li et al. (2021). Values of correspondinghyper-parameters are summarized in . As for PASSCONC, c is defined as the minimal ratio between sparsifier gradients and concentrator gra-dients of i for all heads to ensure that the concentrator is not dominated by the sparsifier while the",
  "(17)": "c is adaptively updated in the middle of training, and set to 0 in both the early training phase and the lastfew training iterations to help improve model performance and achieve sparsity convergence. Specifically,c is set to 0 during the first 20, 000 iterations and the last 7, 000 iterations when training ED Transformermodels. For BERT-base models, c is set to 0 except for iterations between 2, 000 and 5, 000. As to the clipping range, there is a trade-off between choosing a larger range (e.g., ) and a smallerone (e.g., ). In general, a larger range allows us to better achieve subnetworks of desired almost-suresparsities but the gradients will vanish to excessively small values (see c) and make the gradient-descent optimization extremely slow. On the other hand, a small range allows efficient gradient-descentoptimization but the corresponding gates may be neither almost surely closed nor opened (see b)and therefore fail the almost-sure sparsity guarantees. For example, for a large range , the smallestgradient approaches 2e-5 (see c) which leads to slow optimization rates, while for a small range, the maximal gate open/close probability is only 55% (see b) that is neither almost surelyopened nor closed. We use a held-out set to empirically select as the clipping ranges because it givesa maximal gate open/close probability that is roughly 99% while the smallest gradient is above 0.003, whichis practically sufficient for efficient optimization according to our experiments.",
  "CMore Performance Results": "In this section, we provide detailed performance and speedup results on each of the 4 GLUE benchmarktasks: MNLI, QQP, QNLI, and SST-2 (see .1.1). Performance results are summarized in Tables 7and 8, which resemble our analysis in .2. In general, PASS and PASSCONC are able to matchor outperform all 3 baselines across a majority of experiment settings.Speedups achieved by differentapproaches are presented in Tables 9 and 10. Notably, PASSCONC outperforms all baselines in almost allcases which demonstrates its effectiveness in delivering efficiency improvements. Lastly, we report the Paretofront between model performance and speedups for subnetworks identified by different methods (see ),where PASSCONC dominates all other methods in most cases by achieving better performance-efficiencytrade-offs, similar to our observation in .3."
}