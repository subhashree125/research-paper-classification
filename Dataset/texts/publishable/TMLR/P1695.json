{
  "Abstract": "While deep learning (DL) performance is exceptional for many applications, there is noconsensus on whether DL or gradient boosted decision trees (GBDTs) are superior for tab-ular data. We compare TabNet (a DL model for tabular data), two simple neural networksinspired by ResNet (a DL model) and Catboost (a GBDT model) on a large UK insurerdataset for the task of claim reserving. This dataset is of particular interest for its largeamount of informative missing values which are not missing completely at random, high-lighting the impact of missing value handling on accuracy.Under certain missing valueschemes a carefully optimised simple neural network performed comparably to Catboostwith default settings. However, using less-than-minimum imputation, Catboost with de-fault settings substantially outperformed carefully optimised DL models - achieving thebest overall accuracy. We conclude that handling missing values is an important, yet oftenoverlooked, step when comparing DL to GBDT algorithms for tabular data.",
  "Introduction": "Many machine learning problems involve regressing or classifying with tabular or structured data. Sincetheir introduction, GBDTs have performed well on such tabular data (Friedman, 2001). Meanwhile, DL hasbecome the state of the art in many problems that involve unstructured data, e.g. images (He et al., 2016;Simonyan & Zisserman, 2015; Tao et al., 2020), audio (Ao et al., 2021), text (Baktha & Tripathy, 2017;Ziegler et al., 2019; Touvron et al., 2023), and their combinations (Radford et al., 2021; Rombach et al.,2022; Ramesh et al., 2021). Naturally, with the rise of research in DL, architectures have been proposed that claim to outperform GBDTson tabular data (Somepalli et al., 2021; Shavitt & Segal, 2018; Huang et al., 2020; Kadra et al., 2021).However, there is a lack of consensus on whether these architectures really are more accurate. Large studies(Grinsztajn et al., 2022; McElfresh et al., 2024; Borisov et al., 2022) have compared DL to GBDTs acrossmany datasets, many tasks and different computational budgets and suggest that GBDTs are on average themore accurate model for tabular data. The proposed reasons for the performance edge of GBDTs in theselarge studies remains an area of active research. Theories include the ability of GBDTs to ignore irrelevantvariables and model discontinuous functions (Grinsztajn et al., 2022). Importantly, neither the proposed DL",
  "Published in Transactions on Machine Learning Research (12/2024)": "Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, GirishSastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models fromnatural language supervision. International Conference on Machine Learning, pp. 87488763, 2021. Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, andIlya Sutskever. Zero-shot text-to-image generation. International Conference on Machine Learning, 139:88218831, 1824 Jul 2021. URL Pengzhen Ren, Yun Xiao, Xiaojun Chang, Po-yao Huang, Zhihui Li, Xiaojiang Chen, and Xin Wang. Acomprehensive survey of neural architecture search: Challenges and solutions. ACM Computing Surveys,54(4), may 2021. ISSN 0360-0300. doi: 10.1145/3447582. URL Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjarn Ommer. High-resolutionimage synthesis with latent diffusion models. Proceedings of the IEEE Conference on Computer Visionand Pattern Recognition, 2022. URL",
  "Background": "Car insurance is an important financial service with a 2024 global value of over 1.9 trillion USD, which isestimated to reach over 2 trillion USD by 2028 (Statista, 2024). Car insurance works on the principle thatinsurers charge customers a premium in return for obligations to provide financial support in the event ofcontractually agreed risks. Accurate pricing is vital for both the sustainable profit of the insurer and fairprices for customers.The process of determining a price for a prospective customer in car insurance iscomplex (Olivieri & Pitacco, 2015; Werner & Modlin, 2010). It comprises of three core steps i) estimatingthe expected value of payments to the customer over the duration of the contract ii) estimating currentliabilities for claims that are reported but not settled (RBNS) and iii) somehow sensibly combining the twoprior estimates into a price. The first step typically comprises finding a model for claim frequency and amodel for claim severity; the latter estimating the cost of a claim conditional on an accident. The secondstep comprises modelling the cost of claims conditional on them having already occurred and is called claimreserving. The third step combines the claim frequency, claim severity and claim reserve estimates using riskmodels and business considerations: such as profit margins, legal requirements, risk appetite and operationalcosts. The focus within this work will be on the second step: claim reserving. Specifically, we focus on outstandingclaim reserve modelling which is the process of predicting costs for claims that have been RBNS. Typically, outstanding claim reserve modelling is mainly done on a portfolio level. In other words, insurancecompanies predict the overall reserve requirement for a given time period, say a quarter, across all customers.Importantly, these forms of claim reserve modelling use no individual claim information, instead using historicportfolio level settlement aggregates. This is done with deterministic algorithms such as run-off triangles,the chain ladder (CL) method and the Bornhuetter-Ferguson algorithm (Bornhuetter & Ferguson, 1972); orstochastic extensions of said algorithms. We focus instead on individual claim reserve modelling, or micro-level reserving, an alternative methodof reserving. Individual claim reserve modelling predicts portfolio reserves from aggregating estimates perincident. There is not yet a consensus that individual claim reserving is more or less accurate than aggregatemodelling. Still, the hypothesised benefits of micro-level reserving are: greater insight into exposure profileswithin a portfolio; more signal (i.e. relevant covariates) should produce more accurate models; and the",
  "ability to adapt to trends that can be captured by covariates (Blier-Wong et al., 2021; Lopez et al., 2019;Delong & Wthrich, 2020)": "There is literature investigating the use of older machine learning (ML) algorithms such as CART (Breimanet al., 1984) and generalized linear models for individual claim reserving (Lopez et al., 2019; De Felice &Moriconi, 2019; Taylor et al., 2008; Wuthrich, 2018). Newer ML methods, such as neural networks (Delong& Wthrich, 2020; Delong et al., 2022; Kuo, 2020) and GBDTs (Duval & Pigeon, 2019) have also hadsome, limited, research. These works analysing micro-level reserving strategies broadly conclude that theirrespective models are either on par or better than an aggregate CL method, validating micro-level reservingin principle. However, there are only a few such works; their insurance fields vary; they use small sets ofcovariates and some use simulated data. This makes it difficult to know whether the results are relevant tocar insurance micro-level reserving. Furthermore, of considerable practical importance is that missing datais endemic to real insurance data (Fauzan & Murfi, 2018; Hanafy & Ming, 2021) and none of these worksgive any special focus to missing data. Finally, these works often report benchmarks against the CL methodinstead of overall accuracy which complicates the interpretation of results as disagreement with CL couldbe the consequence of more accurate modelling. To our knowledge, also noted by the survey of Blier-Wonget al. (2021), none compare modern ML methods directly to each other on real data. The lack of directcomparison means no conclusion can be drawn about the relative performance of newer ML methods forclaim reserving. Ultimately, both research in insurance and ML more broadly paints a blurry picture on the relative merits ofGBDTs and DL for micro-level reserving using tabular data. Furthermore, treatment or influence of missingvalues on accuracy is not investigated when comparing the methods. Although missing value handling hasbeen shown to be important in other fields (Herring et al., 2004) and as such could be important to reserving.This leaves reserving actuaries dealing with tabular data unclear on whether it is worth the investment toinvestigate and deploy these more modern ML algorithms nor the impact of missing data for said algorithms. The most relevant work to ours, comparing DL to GBDTs in insurance, is McDonnell et al. (2023). Theycompare the DL architecture TabNet (Arik & Pfister, 2021) to the GBDT implementation XGBoost. Theymodel discretised claim severity classification on a dataset with hundreds of thousands of claims. They findTabNet to be comparable to XGBoost, with marginally better F1 score. Although this is modelling claimseverity, not claim reserving, we note that claim severity and claim reserving both model costs conditionalon an accident occurring. However, claim severity is estimated before the accident occurs and claim reserveafter. From the perspective of regression, the only difference between micro-level reserving and claim severitymodelling is the number of covariates - as more is known after and accident occurs. In the work of McDonnellet al. (2023), although TabNet performs comparatively well to XGBoost, the models were evaluated onsynthetic data generated using a neural network (So et al., 2021) thus potentially biasing performancetowards DL as the model class was more likely to be correct. Furthermore, the casting of the regressionproblem into discretised classification and lack of missing data makes the findings less interpretable andtransferable.",
  "Data description": "The tabular data we model in consists of many hundreds of thousands of insurance claim featurevectors as rows, with hundreds of features as columns. This dataset has never been previously studied.The data is a combination of information available at policy issue (e.g. make and model of the car) andinformation available just after the time of claim reporting (e.g. accident date). The settlement value (SV)variable gives how much the insurer paid overall to settle a claim; inclusive of vehicle, personal and propertydamage. We aim to accurately predict SV for each claim to build a micro-level reserve, as described in. As we use supervised ML methods we only consider closed claims, i.e. there exists a SV to beused as a label. Commercial confidentiality prevents us from giving a more detailed description of the data. However, wepresent the missing data properties in the next section. We present other data characteristics and theirimplications for modelling and data processing in Appendix A.1; this includes time varying properties andhandling of high cardinality categoricals, such as postcode information.",
  "Missing data": "The dataset we study has extensive missing values. Over 50% of features contain missing values, thereforeignoring all features with missing values would drop the number of features by over half. This could drophighly informative features, e.g. details of additional drivers on a policy, which are missing in the majorityof claims. Furthermore, due to the interaction of missing values in multiple features there is no complete feature vectors,i.e. every row has at least one missing value. Therefore if we directly apply a strategy such as complete-caseanalysis (Little & Rubin, 2019, p. 47), where any row with missing values is dropped, the whole datasetwould be dropped. Instead, we can first drop features that are missing in more than a certain proportion ofcases and then run a complete-case analysis. This latter approach is also used to deal with missing valuesby Grinsztajn et al. (2022), one of the broad comparative studies mentioned in . We explore thismethod, along with alternative imputation approaches, calling this missing value handling strategy Drop in. Beyond the extent of missing values, the data presents a dependence of the response, SV, on the missingvalue structure. This can be shown by a large shift in the mean and standard deviation of the SV whenusing Drop at various missing value proportion thresholds. Smaller proportions of missing values in a featurevector are associated with substantially higher SV. This suggests that the data is not MCAR (Little & Rubin,2019, p. 13-23). Therefore, fitting a model under a Drop strategy will result in biased predictions, above andbeyond any bias introduced by the model or training algorithm. This bias also means the accuracy resultsof a model fit on Drop are not comparable to those of a model fit on imputed data. To summarise, missing values represent a large portion of our insurance dataset and are not MCAR asthe SV varies substantially conditional on the missing value structure. This highlights the importance ofinvestigating and choosing appropriate missing value handling strategies.",
  "Models": "In this section we start by defining notation, outlining some DL terms and then briefly give backgroundon the models used: i) Catboost, a GBDT model; ii) two ResNets, a general purpose feed forward DLmodel: ours and that of Gorishniy et al. (2021); and iii) TabNet, a DL architecture specifically designed toaccommodate tabular data. Within this section we do not aim to provide comprehensive details. Instead weaim to describe methods in sufficient detail to follow the hyperparameters tuned in .1.",
  "Catboost model": "Catboost (Prokhorenkova et al., 2018) is a GBDT (Friedman, 2001) with a special procedure for categoricalencoding and gradient estimation. Note that the Catboost algorithm details are complex and have manyconfigurable options. Here we only cover the relevant details of the base GBDT algorithm and briefly mentionthe core novel concepts proposed by Prokhorenkova et al. (2018). For removal of ambiguity, as the defaultbehaviour can vary depending on the execution hardware, we present details and use defaults for runningon a CPU opposed to a GPU.",
  "where Gt(x) is the tth weak learner, is a weighting factor, and G0(x) is an initial estimate, such as themean response of the training set": "Usually boosted models are built in a sequential fashion, e.g. the ith model incorporating i weak learnerswould be Fi = Fi1 + GP for i = 1, ..., T. The sequential construction of the model enables the procedureto be terminated early if validation performance is not improving i.e. return Fi(x) with i < T.",
  "Catboost: Pseudo-residual calculation and categorical encoding": "Catboost aims to improve performance on unseen data by reducing overfitting.The key innovations ofCatboost are twofold: i) how the pseudo-residuals, rt1, are approximated using Gt and ii) how categoricalvariables are encoded. Although we will describe the core idea of the improvement, there are further tech-nicalities and engineering modifications present in Prokhorenkova et al. (2018), e.g. to improve speed, thatwe do not describe. For the alteration to Gt fitting, the core idea is to fit Gt on data excluding the data point for which it willpredict rt1, i.e. to calculate Gt(xk) Catboost would fit Gt on data {xj : j < k}. This excludes the datapoint xk, and also generates different Gt for different data points. Likewise, Catboost follows this procedure for generating a categorical encoding. For a given data point xk,Catboost fits a target mean encoding (Pargent et al., 2022) on a discretized target for {xj : j < k} that arebefore the point encoded. Furthermore, when processing categoricals Catboost uses a novel algorithm toredefine category labels as the algorithm runs (feature combinations in the original work).",
  "ResNet MLP": "A ResNet, short for residual network, MLP is a feed-forward neural network (Murphy, 2022, p. 419) withadditive residual connections that skip layers. Without additional knowledge about the underlying structureof the data, an MLP is a simple general purpose DL architecture; and skip connections make training morestable (Murphy, 2022, p. 445). We implement a ResNet by using residual connections across building blocks, along with skip connectionsto the output. We use a building block layout of BatchNorm, ReLU and Dense as in He et al. (2016). Weadd Dropout following the example of Gorishniy et al. (2021) which proposed the best performing DLmodel (a ResNet) from McElfresh et al. (2024). For the sake of clarity, our ResNet MLP is not identical inarchitecture to Gorishniy et al. (2021), and by extension McElfresh et al. (2024). For details of the differenceswith Gorishniy et al. (2021) see Appendix A.3. To contextualise our findings we also present results usingthe ResNet architecture from Gorishniy et al. (2021), referring to it as RTDL ResNet. shows the layout of our ResNet MLP layers on the far left, with their combination into a sub-blockdenoted by A. The ResNet sub-block, A, is repeated in a residual pattern to form a high level block B shown on the right of . This higher level block B is in turn composed using skip connections into anoverall model. Each block has independently trainable parameters. In the context of deep learning, choosingthe architecture size (such as number of blocks, size of Dense layers in units etc.) is a part of the broaderproblem of hyperparameter tuning. Our approach involves choosing the number of B blocks to vary depth;and choosing the number of units used in every Dense layer to vary the width of the network. This tuningis further described in .1. We use a grid search, described in .1.3, to select the optimiserused, the initial learning rate of the optimiser, learning rate schedule (Murphy, 2022, p. 288), Dense layerweight regularisation strategy and regularisation intensity.",
  "TabNet model": "TabNet (Arik & Pfister, 2021) is a DL architecture specifically designed for tabular data. TabNet works bylearning a step that multiplies a subset of features by zero, conditional on the input. Then TabNet usesDL layers on the remaining non-zero parts to produce an intermediate decision vector. The architecturesequentially applies multiple steps. Each step can determine a different subset of features to set to zero so a feature that is set to zero for one step does not need to be zero for the following steps. In fact, thehyperparameters described below control how many distinct features can be selected and their potential forreuse across steps. As each step can select different subsets, each step can produce a different decision vector.Finally, the decisions from all steps are combined through a DL layer into a prediction.",
  "TabNet is a complex architecture for which we defer the detailed description to the original paper (Arik &Pfister, 2021). However, there are some key hyperparameters which we are required to tune": "The number of steps, S, determines the number of different feature subsets that are modelled to produce aprediction. With S = 1 only a single subset of the features is used, with more steps resulting in more featuresubsets. Intuitively, more steps increases the overall number of features used, but also increases the depthof the network and destabilises training. The so-called relaxation parameter, 1, is designed to encourage different feature subsets to be selectedat each step. When = 1 TabNet has the special property of being able to prevent reuse of features betweensteps. As increases, TabNet is more able to reuse features between steps. The sparsity regularisation coefficient, 0, is used to encourage more input features to be zeroed out ineach step. As increases the network can zero out more features, even if it decreases training accuracy.",
  "Experimental method": "To investigate the impact of preprocessing schemes for handling missing values, we first tuned the hyper-parameters of each model. Preliminary analysis, described in Appendix A.5, suggested the best hyperpa-rameters did not vary with preprocessing scheme. Therefore, the hyperparameter tuning process was doneindependently of later missing value investigation. To prevent data leakage, the last 15% of the data was set aside into a test set, Dtest, shown in .This Dtest was always withheld from training or validation procedures and only used to report the metricspresented in . Early stopping (Murphy, 2022, p. 448) was applied to improve training speed and prevent overfitting in bothhyperparameter tuning and final model training. Early stopping is a form of regularisation where out-of-sample model performance is evaluated at regular intervals on a dataset withheld from training. When theperformance on the withheld dataset decreases, the training algorithm is terminated. Preliminary analysisconfirmed there was no decrease in accuracy from using early stopping. .1 describes the tuning of hyperparameters discussed in . .2 describes the exper-imental setup used to investigate the impact of missing value handling and categorical encoding. Practicalcommentary on the training speeds of the algorithms can be found in Appendix A.2.",
  "Hyperparameter tuning": "Hyperparameter tuning on our ResNet and TabNet consisted of a grid search optimising for accuracy.For each modelling strategy a hyperparameter (HP) grid was subjectively chosen after initial trial anderror. Although grid search may be a common practice in industry, it is also well known to theoreticallyunderperform more principled methods of HPO such as Optuna (Akiba et al., 2019). As such, we also rerananalyses for our ResNet using Optuna with the same upper and lower bounds on numeric hyperparameter",
  ": Dataset partitioning strategy for both hyperparameter tuning and final evaluation, where DES": "and Dtrain are shuffled per replication of a given experiment. DES is a split of data used for early stopping.DHP is a split of data used for evaluation of hyperparameters. DHP is sampled randomly in time, Dtest isexclusively future data. values. Those results can be found in Appendix A.4 but we note that in this instance Optuna hyperparameteroptimisation (HPO) did not significantly change performance. For consistency with McElfresh et al. (2024),Optuna was used for HPO on the RTDL ResNet model and presented in , . A single data subset, DHP, was sampled once for accuracy evaluation of all models under any given HPconfiguration. For clarity, DHP was not a future partition of the data, but rather randomly sampled in time.",
  "For each node in the grid, we randomly split the remaining data (after removing Dtest and DHP) into Dtrain": "and DES subsets, illustrated in . Training was performed solely on Dtrain whilst DES was usedto trigger early stopping (ES). Once training was completed, the root mean squared error (RMSE) for agiven node was evaluated on DHP. This random splitting of Dtrain and DES, and subsequent evaluation ofRMSE on fixed DHP, was independently repeated 10 times for each HP node in the grid. This repeated splitsampling is called Monte-Carlo cross-validation (Kuhn et al., 2013, p. 71-72). The best HPs for a given model were chosen on the basis of the lowest RMSE averaged across the 10Monte-Carlo cross-validation samples. This best HP configuration for a given model was then used for themodel-to-model comparison as described in .2. The use of separate data subsets for early stopping, DES, and accuracy evaluation, DHP, allowed us toremove bias associated with evaluation on data that was indirectly used for training. This is an especiallyrigorous process in contrast to what is often done in practice, where one validation set would be used forboth early stopping and evaluation. However, as we had sufficient data, we opted for separate subsets tominimise potential bias.",
  "Catboost hyperparameter tuning": "Initial manual exploration of the hyperparameters gave no significant improvements in DHP RMSE. The stepsize, , was varied between 0.005 and 0.018. The ensemble size, T, was varied between 1000 and 10000. Meansquared error was used as the loss function, L. As no noticeable improvement came from heuristic tuning,a complete grid search was not performed and all hyperparameters were left as defaults for evaluation ofCatboost in model-to-model comparison. The only non-default choice was inclusion of early stopping whichwas used to speed up training, and had no noticeable effect on accuracy in the hyperparameter tuning stage.",
  "ResNet MLP tuning": "Currently, there is no principled way to construct a deep learning architecture, for example choosing depthand width, beyond intuition; trial and error; and neural architecture search (Ren et al., 2021). As neuralarchitecture search was prohibitively expensive from a computational perspective we instead opted for aheuristic trial and error approach for choosing our ResNet architecture. We began by arbitrarily choosing some expressive high level block, denoted B in . We built thisblock from sub-blocks, denoted A in . Sub-blocks A utilise a ResNet (He et al., 2016) layout oflayers as described in . We then chose the depth: i.e. the number of B blocks composed together prior to a Dense layer with a singleoutput unit, with no activation, for prediction. A depth of 3 was chosen; subjectively balancing simplicityof the model with expressive power. We then parameterised the width of a block with d, the number of units in each Dense layer of the ResNetblock, denoted A in . We varied d between heuristically identified limits wherein the model exhibitedunderfitting and the capacity to overfit training data. Underfitting was identified by both the training andvalidation error being similar and approximately constant per training epoch.Capacity to overfit wasidentified by the ability for the model to keep reducing training error whilst validation error is constant orgetting worse. These criteria were considered fulfilled when the validation loss was not improving whilsttraining loss was still decreasing after 1000 epochs. Varying d within the bounds of under and overfitting did not noticeably impact accuracy.As such, dwas chosen for a total model size of four hundred thousand parameters. This was between the numberof parameters which under- or overfitted.After the architecture was heuristically selected, grid searchhyperparameter optimisation followed. The optimisers compared were Adam (Kingma & Ba, 2014) andRMSProp (Tieleman & Hinton, 2012). Both L1 and L2 regularisation of kernel weights were used; coefficientsfor each were varied between 0.01 and 1. Exponential Decay (Murphy, 2022, p. 288) and Cosine Decay(Loshchilov & Hutter, 2016) learning rate schedules were both compared; where initial learning rate wasvaried between 0.001 and 0.01 for each.",
  "We aimed to investigate the impact of missing value handling through the comparison of each model underdifferent preprocessing schemes": "To enable a pairwise comparison of trained models, the data subsets for these experiments were differentfrom those of the hyperparameter tuning in .1. Instead of sampling different subsets per node ina grid search all experiments used the same set of 20 Monte-Carlo cross-validation samples of Dtrain andDES. These were sampled once prior to running any experiments, enabling pairwise model comparison. Asevaluation was performed on Dtest, there was no need for another withheld evaluation partition, like DHP. A given Dtrain subset was used for training all models with all missing value handling schemes, describedbelow, and the corresponding DES was used to trigger early stopping. After the models were trained, RMSEwas evaluated on Dtest for each of the 20 Dtrain and DES samples. The mean and standard error of the RMSEacross the 20 Monte-Carlo cross-validation samples for each model and preprocessing scheme are reportedin . These results are discussed in .",
  "investigate model capability to jointly use covariates and missing value structure through imputation. Themissing value handling strategies are summarised:": "Drop: Rows with any missing values were dropped after first removing features with more than 30%missing values as described in .1. From this scheme we aimed to isolate the ability ofeach modelling strategy to extract signal from observed covariates. Binarize: The whole dataset was converted into a binary representation of whether the covariatewas observed or not, see .This retained only the signal inherent in the missing valuestructure. With this scheme we isolate the ability of model strategies to regress against missingvalue structure. LT Min Impute: Missing values were imputed using less than the minimum of the numeric feature.This is the default missing imputation strategy of Catboost. The use of this strategy allows theweak learner trees, Gt in , to separate the missing values completely from observed values.However, this also means that should the tree split on an observed value it will include all missingvalues on the lesser side of the split.",
  "Categorical encoding": "Categorical encoding was performed using target mean encoding for the majority of the experiments, thiscompared the modelling strategies fairly by holding potential confounders in categorical encoding constant. In the interests of identifying the best performance, we also evaluated the original Catboost categoricalencoding with the Catboost GBDT algorithm, described in .2. We report the model names in with the suffix(CE) indicating that Catboost encoding was used; otherwise mean encoding was used.",
  "Effects of imputation scheme on other datasets": "To study the broader relevance of results obtained from our insurance dataset, further analyses were per-formed on two other datasets under Drop, Mean Impute and LT Min Impute. The leading two GBDTs andleading two DL algorithms from McElfresh et al. (2024) were compared on two datasets using the TabZillaframework. The datasets did not contain missing values; so MCAR missing values were simulated by removal.As MCAR data are not the focus of this case study we present the results in Appendix A.6. Overall, thesepreliminary analyses showed lesser impact from missing value handling than our not MCAR application butstill demonstrate the relevance of our work.",
  "ModelDrop(se)LT MinImpute(se)MeanImpute(se)Binarize (se)": "Catboost (CE)1969(10)1452(6.3)1519(17)2302(0.18)Catboost2060(1.7)1574(2.1)1524(5.2)2268(0.28)Our ResNet2046(1.2)1872(4.5)7855(1600)2288(1.8)RTDL ResNet2439(24)2007(20)2387(270)2282(2.4)TabNet2702(8.1)2748(5.3)3081(180)2754(5.1)Mean prediction2813(0.34)2764(0.020)2764(0.020)2764(0.020) The TabNet row of shows that TabNet substantially underperforms both Catboost and the ResNetMLP. Notably, TabNet performs on par with the Mean prediction benchmark. This indicates that TabNetis either not suitable for micro-level reserving with this dataset or, at best, that TabNet is very difficult totrain. This runs counter to published work on the use of TabNet for claim severity modelling (McDonnellet al., 2023), but in line with surveys of DL for structured data (Grinsztajn et al., 2022; McElfresh et al.,2024) where TabNet performed poorly. As mentioned in , this could potentially be explained by thefact that the results of McDonnell et al. (2023) could be biased because the modelled data is itself generatedfrom a neural network. The relative accuracy of the RTDL ResNet MLP row of agrees with McElfresh et al. (2024) inoutperforming TabNet and underperforming CatBoost. Although for Drop, LT Min Impute and Binarizethe RTDL ResNet has lower accuracy than our ResNet it is interesting to note that under Mean Impute theRTDL ResNet performs substantially better than any other DL algorithm. This highlights the sensitivity ofpreviously studied algorithms to imputation schemes. However, we note that the performance of Mean Impute is still generally worse than that of LT Min Imputeacross most models. Mean Impute demonstrates moderately high standard error for the RTDL ResNet andhigh mean RMSE and standard error for our ResNet and TabNet. The exceptionally high RMSE for ourResNet appeared to be due to the rare prediction of low value claims many orders of magnitude largerthan they were. It is unclear why this would be the case specifically for Mean Impute and our ResNet. Wehypothesise this could be due to Mean Impute making it difficult to distinguish meaningfully missing data,where absence of data is not due to random lack of records but from the reality of the data generatingprocess, e.g. no additional drivers on the policy. The Drop and Binarize columns of show that both the covariates and the missing value structureare important in the performance of both Catboost and ResNet, respectively offering approximately a 27%and 18% improvement over a Mean prediction benchmark. Interestingly, neither ResNet nor Catboost ispractically more accurate than the other when trained on either covariates (Drop) or missing value structure(Binarize) alone.This similarity of performance on Drop suggests studies comparing GBDTs and DL,mentioned in , do not have conclusions that are necessarily transferable to the context of claimreserve modelling. Those studies found a performance edge for GBDTs, notably either ignoring or in theabsence of missing data. Whereas we find, for this micro-level reserving dataset, Catboost and ResNet arepractically the same in terms of accuracy with missing data ignored, due to Drop.",
  "Conclusion": "In this paper we investigated four different imputation schemes, described in .2.1, for handlingmissing values using gradient boosted decision tree and deep learning models. We investigated the impactof these imputation schemes using a large, real world insurance dataset with not MCAR missing values.Under two of the missing value handling schemes there is no noticeable difference between Catboost andthe ResNets. However, using imputation, Catboost substantially outperforms the other models. This resulthighlights the importance of missing value handling in claim reserving. More broadly, this result adds a case study to the body of evidence that gradient boosted decision trees canoutperform deep learning for tabular data, but emphasises the importance of missing values in drawing thisconclusion. The significant impact of missing value handling on accuracy also suggests that when analysingdatasets with missing values, extra care should be taken choosing the missing value handling method andnot to just focus on model selection. Furthermore, our results suggest research comparing gradient boosteddecision trees to deep learning for tabular data could benefit from including more datasets with missingvalues, especially missing values that are not MCAR. The importance of including missing value handlingin comparisons was corroborated on other datasets using the Tabzilla repository (McElfresh et al., 2024) byinjecting MCAR missing values, described in Appendix A.6. However, with MCAR missing values the effectof missing value handling was lessened. Based on the analysis in this work we recommend exercising caution when using deep learning models forclaim reserving as they require thorough tuning and their interaction with imputation schemes is not under-stood. Furthermore, using Catboost for claim reserving has some practical advantages, beyond potentiallybetter accuracy with the correct missing value handling. Catboost is fast, robust and easy to use off-the-shelf. In comparison, deep learning methods require more expertise to deploy successfully: requiring botharchitecture selection and hyperparameter tuning. Even with the expertise required to select, implementand optimise deep learning models there is no compelling empirical or theoretical evidence that they arelikely to produce better results for claim reserving.",
  "International Conference on Communication and Signal Processing, pp. 20472050, 2017": "Christopher Blier-Wong, Hlne Cossette, Luc Lamontagne, and Etienne Marceau.Machine learning inP&C insurance: A review for pricing and reserving. Risks, 9(1):4, January 2021. ISSN 2227-9091. doi:10.3390/risks9010004. Vadim Borisov, Tobias Leemann, Kathrin Seler, Johannes Haug, Martin Pawelczyk, and Gjergji Kasneci.Deep neural networks and tabular data: A survey. IEEE Transactions on Neural Networks and LearningSystems, pp. 121, 2022.",
  "arXiv:1608.03983, 2016": "Kevin McDonnell, Finbarr Murphy, Barry Sheehan, Leandro Masello, and German Castignani. Deep learningin insurance: Accuracy and model interpretability using TabNet. Expert Systems with Applications, 217:119543, 2023. ISSN 0957-4174. doi: URL Duncan McElfresh, Sujay Khandagale, Jonathan Valverde, Vishak Prasad C, Ganesh Ramakrishnan, MicahGoldblum, and Colin White. When do neural nets outperform boosted trees on tabular data? Advancesin Neural Information Processing Systems, 36, 2024.",
  "ONS. Postal geographies - office for national statistics, Feb 2024. Accessed Feb 2024": "Florian Pargent, Florian Pfisterer, Janek Thomas, and Bernd Bischl. Regularized target encoding outper-forms traditional methods in supervised machine learning with high cardinality features. ComputationalStatistics, 37(5):26712692, November 2022. ISSN 1613-9658. doi: 10.1007/s00180-022-01207-6. Liudmila Prokhorenkova, Gleb Gusev, Aleksandr Vorobev, Anna Dorogush, and Andrey Gulin. Catboost:unbiased boosting with categorical features. Advances in Neural Information Processing Systems, 31, 2018.",
  "Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running average ofits recent magnitude. Coursera: Neural networks for machine learning, 4(2):2631, 2012": "Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothe Lacroix,Baptiste Rozire, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, EdouardGrave, and Guillaume Lample. Llama: Open and efficient foundation language models. arXiv preprintarXiv:2302.13971, 2023. Mike Van Ness, Tomas M. Bosschieter, Roberto Halpin-Gregorio, and Madeleine Udell. The Missing IndicatorMethod: From Low to High Dimensions. Proceedings of the 29th ACM SIGKDD Conference on KnowledgeDiscovery and Data Mining, pp. 50045015, August 2023. doi: 10.1145/3580305.3599911.",
  "A.1.2Time-varying properties": "depicts the mean claim value per month, with claim value and year of incident anonymised forcommercial confidentiality. The figure shows that mean claim value is clearly dependent on the time ofthe claim exhibiting both seasonality and trend. This has implications for both the evaluation of modelaccuracy and encoding of time data. When evaluating the accuracy of the ML models, described in , the time series nature of the datarequires the definition of the test set data to be in the future relative to all training and validation data.This is to prevent bias in the estimation of performance, a form of data leakage (Nisbet et al., 2009, p. 742). When considering how to encode the time variables, we aim to encode in such a way as to make it easier to fitseasonality and trend. To explicitly encode cyclic timestamp properties, we encode month and day-of-monthvariables separately. Having cyclic values in the input data, like month and day-of-month, intuitively makesit easier to fit conditional on cyclic seasons. In order to fit overall trend, we chose to also encode timestampsas a monotonic value, enabling the model to order claims in time from a single numeric value. We did thisat two resolutions: i) year and ii) seconds since the epoch, also known as Unix time. The goal of encoding",
  "A.1.3High-cardinality categorical variables": "The dataset contains some high-cardinality categorical variables: insuree car make with hundreds of cat-egories, insuree car model with thousands of categories and first half of insuree postcode with thousandsof categories. The typical one-hot encoding (Murphy, 2022, p. 23) of these features would dramaticallyblow up the dataset size in number of features. Therefore, we applied numeric encoding strategies such asimpact encoding (Pargent et al., 2022), also known as target mean encoding, and Catboosts novel categoricalencoding, described in .2.2. As the UK has 1.7 million postcodes (ONS, 2024), attempting to treat full postcodes as a categoricalvariable would give rise to at least hundreds of thousands of categories. Even target mean encoding sucha high cardinality categorical could exhibit high bias and high variance due to the low number of samplesper category; following the intuition outlined in Prokhorenkova et al. (2018). Only modelling the first halfof postcodes was used to decrease the cardinality of the variable to thousands of categories to potentiallyreduces the bias and variance of the mean encoding.",
  "A.2Training speed": "Using 12 Intel Xeon 6136 CPUs and approximately 60GB of RAM: Catboost trains on our dataset inthe order of 3 minutes per training run and the ResNet MLP architecture trains in the order of 45-120minutes per training run.TabNet trains in the order of 300 minutes per training run.Although bothCatboost and ResNet exhibited training times low enough to be retrained many times a day to keep upwith changes in underlying distribution; the significantly faster training time of Catboost enables greaterexperimentation. This is potentially relevant for other applications as it has been shown that the difference",
  "We use three repeating blocks prior to the regression head layer; McElfresh et al. (2024) use two": "Our ResNet MLP does not vary dimensionality of the hidden state - we keep constant dimensionalityof 256, in comparison to McElfresh et al. (2024) who step down to 128 and back up to 256 throughoutthe network. Ultimately, this and the above point result in our model having approximately 2.25xthe parameters of McElfresh et al. (2024).",
  "A.4Optuna HPO Results": "shows a table analogous to comparing results obtained using Optuna for the HPO instead ofgrid search. It can be seen that there is no substantive difference to the conclusions in of the workfrom swapping to Optuna HPO; as core points are made in comparison of Our ResNet MLP and Catboost(which did not undergo HPO).",
  "A.5Impact of imputation scheme on HPO": "This appendix presents preliminary experimental data used to justify the process of performing HPO in-dependently of imputation scheme; as described in the beginning of . The closeness in parametermagnitude value and the robustness of HPO performance to varying hyperparameters suggested it was ac-ceptable to reduce the computational complexity of experiments by performing HPO under one imputationscheme and evaluating on all. The LT Min Impute scheme was chosen as initial results suggested it wouldbe the scheme with the best performance; and as such chosen in an attempt to give a level playing field forbest performance across models.",
  "A.5.2Optuna based": "The optimum hyperparameters obtained using Optuna with LT Min Impute and Drop for our ResNet arepresented in . With corresponding final outcomes presented in . It can be seen from 7 thatalthough tuning with Drop gives different hyperparameters; and gives our ResNet more stable results forMean Impute; the relative rankings in the final would be unaffected.",
  "A.6TabZilla replication": "This appendix details the results obtained from comparing LT Min Impute, Mean Impute and Drop on someextra datasets and algorithms from the TabZilla Benchmark (McElfresh et al., 2024). Source code can befound at 30% of numeric values were randomly replaced with missing values from each dataset and then imputedusing the imputation schemes studied. Then the corresponding OpenML task was performed using Catboost,XGBoost, FTTransformer (Gorishniy et al., 2021) and the RTDL ResNet. We present the results by datasetin . From we can see that the overall impact of imputation schemes is not large and in some casescauses no performance difference between certain imputation schemes. However, the changes in accuracyrankings of the datasets under different imputation schemes demonstrates that in principle it is possible forthe rankings, and therefore results such as those in McElfresh et al. (2024), to be influenced by imputationscheme. We note that, by design, shows results from injected MCAR missing values and as such we",
  "do not attempt to interpret the absolute performance of any imputation scheme as MCAR data is not thefocus of our work": "In summary, the TabZilla replication with different imputation schemes demonstrates in principle that han-dling missing values could be important; but the MCAR nature of the injected missing values and low signalvalue of the numerics in the chosen datasets produces a less pronounced effect than our work with nonMCAR claim reserving. This highlights the importance of analyses using real world large datasets with notMCAR missing value structure in comparing GBDTs and DL models. : Relative accuracy and ranking per dataset per algorithm using the TabZilla repository underdifferent imputation schemes. We report TabZilla mean 10-fold cross-validation test accuracy; as extractedfrom the tuned aggregated results output.",
  "A.7Combining missing value handling strategies": "shows the performance obtained when combining both the LT Min Impute and Binarize approaches,turning each D dimensional feature vector into a 2D dimensional feature vector. For each original featurewe generate two features: one following LT Min Impute and another that is a binary indicator of whetherthe data is (Binarize). This could be interpreted as providing both features and missingness masks ofpresent features to the model. also includes results for LT Min Impute and Binarize from for reference. Although it is a natural question to investigate combinations of compatible methods, we findthat for our dataset there is no noticeable difference between combining LT Min Impute and Binarize andusing LT Min Impute on its own. :Dtest RMSE mean and standard error, se, over cross validation partitions. Lower RMSE isbetter. RMSE is to the nearest integer, se is to 2 significant figures. (CE) indicates Catboost encoding wasused for categorical encoding. corresponds to an Optuna HPO scheme."
}