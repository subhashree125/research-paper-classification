{
  "Abstract": "We introduce VBPI-Mixtures, an algorithm aimed at improving the precision of phyloge-netic posterior distributions, with a focus on accurately approximating tree-topologies andbranch lengths. Although Variational Bayesian Phylogenetic Inference (VBPI)a state-of-the-art black-box variational inference (BBVI) frameworkhas achieved significant successin approximating these distributions, it faces challenges in dealing with the multimodal na-ture of tree-topology posteriors. While advanced deep learning techniques like normalizingflows and graph neural networks have enhanced VBPIs approximations of branch-lengthposteriors, there has been a gap in improving its tree-topology posterior approximations.Our novel VBPI-Mixtures algorithm addresses this gap by leveraging recent advancementsin mixture learning within the BBVI domain. Consequently, VBPI-Mixtures can capturedistributions over tree-topologies that other VBPI algorithms cannot model. Across eightreal phylogenetic datasets and compared to the considered benchmarks, we show that VBPI-Mixtures result in lower-variance estimators of the marginal log-likelihood and smaller KLdivergences to an MCMC-based approximation of the true tree-topology posterior.",
  "Introduction": "Phylogenetic inference has a wide range of applications in various fields, such as molecular evolution, epi-demiology, ecology, and tumor progression, making it an essential tool for modern evolutionary research.Bayesian phylogenetics allows researchers to reason about uncertainty in their findings about the evolution-ary relationship between species. The posterior distribution over phylogenetic trees given the species data is, however, challenging to infer,since the latent space is a Cartesian product of the discrete tree-topology space and the continuous branch-length space. Furthermore, the cardinality of the tree-topology space grows as a double factorial of thenumber of species (taxa), making the marginal likelihood computationally intractable in most interestingproblem settings.",
  "(a)(b)(c)": ": Visualization of samples from the tree-topology posterior using a 1,000,000,000 iterations longMCMC run on (a) DS4, (b) DS7 and (c) DS8. Nodes represent unique tree-topologies and are colored basedon cluster assignments, illustrating the multimodality of the tree-topology posterior. The size of a node isdetermined by its sampling frequency, which is why nodes with low frequency appear black. More details inSec. 2. particularly popular. However, random walk Metropolis-Hastings MCMC methods (Huelsenbeck & Ronquist,2001; Hhna et al., 2016) rely on local operations to explore the tree-topology posterior, a limitation which isknown to require long MCMC runs in order to visit posterior regions which are separated by low-probabilitytree topologies (Whidden & Matsen IV, 2015). Sequential Monte Carlo methods for Bayesian phylogeny(Bouchard-Ct et al., 2012; Wang et al., 2015; 2020) have been proposed to avoid these local operations,but the resampling mechanism can filter out important trees in early steps of the algorithm as well as causedegeneracy, necessitating many particles. More recently, variational inference (VI) has been applied to Bayesian phylogenetics. In general, VI is oftenpromoted over sampling-based approaches in high-dimensional problems as a variational approximation of theposterior is obtained from optimization, making VI less vulnerable to the curse of dimensionality. However,in practice, it can be challenging to do VI without utilizing sampling. For example, in Koptagel et al. (2022)coordinate-ascent update equations are derived in the phylogenetic setting, but these are evaluated usingimportance sampling. In black-box VI (Ranganath et al., 2014), the gradients are instead taken of Monte-Carlo integrated esti-mates of the objective, typically the evidence lower bound (ELBO), using samples taken from the variationalapproximation. This approach has been successfully applied in the Variational Phylogenetic Bayesian In-ference (VBPI; Zhang & Matsen IV (2019)) framework, along with its extensions (Zhang, 2020; 2023). Inthese extensions, more complicated branch-length approximations have been proposed using normalizingflows (NFs; Rezende & Mohamed (2015)) and graph-neural nets (GNNs; Kipf & Welling (2017)). BBVI allows the practitioner to learn posterior approximations without deriving update equations or closed-form gradient formulations, the samples are taken from a distribution that is commonly known to concentrateon high-probability regions of the posterior, resulting in a learning procedure that does not sufficiently explorethe posterior distribution.This was addressed for continuous distributions in Ruiz et al. (2016), wheresamples were instead drawn from an overdispersed proposal distribution. In discrete hierarchical models,similar to the targets approximated by VBPI, however, insufficient exploration may result in low-level statesnot being properly modeled. As the tree-topology posterior is known to typically be multimodal with manysubpeaks (Whidden & Matsen IV, 2015) (visualized in ), it is thus of significant importance toencourage the tree-topology approximation to explore the posterior. We propose VBPI-Mixtures, a novel combination of two recent advances, VBPI and mixture learning inBBVI (Kviman et al., 2023). The mixture components cooperatively explore the tree-topology posteriorduring learning, and the increased flexibility of the VBPI-Mixtures results in approximations that can modelposteriors intractable for the vanilla VBPI (see Sec. 3). Using a toy experiment where we design a compli-cated hierarchical categorical target distribution and pick a variational family such that it does not containthe target distribution, we first show that the mixtures of these sub-optimal variational distributions achieve",
  "Published in Transactions on Machine Learning Research (11/2024)": "Oskar Kviman, Ricky Moln, Alexandra Hotti, Semih Kurt, Vctor Elvira, and Jens Lagergren. Cooper-ation in the latent space: The benefits of adding mixture components in variational autoencoders. InInternational Conference on Machine Learning, pp. 1800818022. PMLR, 2023. Clemens Lakner, Paul van der Mark, John P Huelsenbeck, Bret Larget, and Fredrik Ronquist. Efficiencyof markov chain monte carlo tree proposals in bayesian phylogenetics. Syst. Biol., 57(1):86103, February2008.",
  "Background": "Let B = {b(e) : e E()} denote the set of branch lengths for a topology, and E() is the set of edges in .Furthermore, the data, X = {X1, ..., XN} MN, are an observed sequences of length M on the N leavesof the phylogenetic tree. Each entry in Xm,n is a character in the alphabet , e.g., Xm,n {A, C, G, T} ifDNA sequences are considered. The posterior distribution over leaf-labeled phylogenetic trees,",
  "eE() eb(e). The likelihood,p(X|, B), can be evaluated in linear (in N) time using the standard dynamic programming algorithmproposed by Felsenstein (2003)": "Let a clade be a non-empty subset of the set of the N leaf labels, X. A subsplit is a partition of this cladeinto two lexicographically ordered, disjoint clades, while a split is simply a root subsplita bipartition ofX. Furthermore, a primary subsplit pair (PSP) is a subsplit conditioned on a splita tripartition of X. InAppendix A we additionally give a brief introduction to Bayesian phylogenetic inference for machine learningresearchers. Variational Inference in Bayesian PhylogeneticsThe VI-based approach to Bayesian phylogeneticsis to approximate p(B, |X) using a simpler distribution, q,(B, ) = q(B|)q(). Generally, in VI, theapproximations are learned by maximizing the evidence lower bound (ELBO),",
  "eE()q(b(e)|(e, ), 2(e, )).(4)": "Two different parameterizations of (e, ) and (e, ) have previously been proposed. The simpler approachis to let (e, ) = e/ and (e, ) = e/, where e/ denotes a split of in edge e. The parameters e/and e/ are shared among all tree topologies where e/ exists, resulting in an amortized mapping from atree topology to the parameters of q(B|). Additional local information about the given can be addedinto the parameterization of the approximation by using PSPs,",
  "where e denotes the set of PSPs neighboring to the split e/, and i is a learnable parameter associatedwith the i-th pair": "Multimodality of the tree-topology posteriorIn Whidden & Matsen IV (2015), modes are referredto as clusters of MCMC samples that are densely grouped in the tree-topology space and have high posteriordensity compared to their neighbors.They proposed a method for detecting and quantifying peaks bycalculating the reversible subtree pruning and regrafting distance between topologies, combined with theMrBayes MCMC posterior probability. By applying this method, it was identified that certain datasets hada high number of modes (e.g., DS1, DS4, DS5, DS6, DS7), which shows the complexity of the tree-topologyspace. In we visualize the multimodality of the posterior on three datasets. Mixtures in Black-Box VILearning mixtures of approximations in (black-box; Ranganath et al. (2014))VI (Nalisnick et al., 2016; Morningstar et al., 2021; Kviman et al., 2023) is a compelling off-the-shelf tech-nique to increase the flexibility of a variational approximation. Mixtures can be applied to any variationalapproximation, including an NF-based approximation or one for discrete latent variables, with little overhead. The objective function, the ELBO for mixtures, is estimated by sampling from each mixture component in astratified manner (Morningstar et al., 2021), or via multiple importance sampling techniques (Kviman et al.,",
  ".(6)": "To evaluate L(X; K, S), we approximate the s-th expectation by Monte-Carlo integration using simulationsfrom qs,s(B, ). Although there exist works where the mixture component weights are learned (Roederet al., 2017; Morningstar et al., 2021), we constrain ourselves to uniform mixture weights as in Kviman et al.(2023). Mixtures promote explorationNote that maximizing Eq. (6) w.r.t. the variational parameters will pro-mote the components to diversify and explore regions of the latent space that correspond to high-probabilityregions measured by p. That is, samples drawn from component s that result in high values in the numeratorwill indicate that the s-th component covers a part of the latent space corresponding to a high-probabilityregion, attracting the other components to this region, while the denominator penalizes similarities among",
  "2q1(1)+0 = 0+ 1": "2q2(2) = 0.5. However, as a single SBN, q, samples A or A and B or B independently,and in order to achieve q(1), q(2) > 0, it will have to assign a non-zero probability to all four subtrees.Specifically, say q(A) = and, consequently, q(A) = 1, while q(B) = and, consequently, q(B) = 1.It follows that q(3) = (1 ) and q(4) = (1 ). Finally, = q(1) = q(2) = (1 )(1 ) implies1 = + , which, in turn, implies that q(3) = 2 and q(4) = 2. That is, all four trees will either haveprobability 1/4 under q, or one of 3 and 4 will have a higher probability than 1 and 2. So, a single SBNcan only yield a distribution that is very different from p. The above example exemplifies that there are tree-topology distributions that cannot be modeled using asingle SBN, but can be modeled by a mixture of SBNs. There is no converse example, as a mixture cantrivially model a single SBN by letting q1() = q2() for all . In Appendix B we construct an example thatshows that these conflicting tree-topology posteriors can indeed occur for real DNA data.",
  "and LKs = log 1": "KKk=1 f(x, Bks, ks ), where Bks, ks are simulated from qs,s(B, ). Note that f(x, Bks, ks ),and so also LKs , is a function of the SBN parameters for all mixture components, i.e., 1, ..., S. However,we omit these as arguments to the function in order to avoid cluttered notation. Furthermore, the sets ofSBN parameters are disjoint (i.e. the mixture components do not share weights).",
  "(9)": "We make three important observations, (i) for S = 1, we retrieve the gradients used to train VBPI, (ii) asthe second term is negated, maximizing it promotes component i = s to be dissimilar from component s,diversifying the mixture. Finally, (iii) the first term is merely a scaled (by 1/S) version of the correspondingterm in the S = 1 case. Connecting to observation (iii), we conclude that extending the VIMCO estimatorto S > 1 cannot be trivially achieved without our derivation provided above. Furthermore, the analysis of the gradients of the importance weighted lower bound in (Mnih & Rezende,2016)using our notation, L(X; K)applies here, too. That is, the gradients in the second term in Eq.(9) are multiplied by normalized weights, ensuring that the norm of the weighted sum over all K gradientsis not greater than the norm of the largest term in the sum. This means that i will be updated mainlyaccording to gradients based on simulations scored highly by f, while mitigating the impact of gradientsfrom lower-scoring simulations. In the first term, on the other hand, all gradients are multiplied by the same LKi , indicating that the gradientsof high-scoring simulations will not receive more weight than low-scoring ones, causing high variance andslow learning.",
  "The VIMCO Estimator": "As concluded above, the second term in Eq. (9) is well-behaved, and we do not need variance-reductiontechniques to use it for learning in practice. The first term, however, requires attention in order to facilitateefficient learning. Fortunately, as the first term is merely a scaled version of the corresponding term in the VIMCO estimatorwhen S = 1, we can directly apply the localized learning signal strategy from Mnih & Rezende (2016) toobtain the VIMCO estimator for S 1,",
  "Experiments": "In Sec. 3, we argued that a single-component approximation will struggle to properly model all parts of thetarget distribution when learned with black-box VI. Below, in Sec. 4.1, we experimentally verify this claim,and, furthermore, confirm that mixture components collaborate in order to jointly cover the target density,resulting in more accurate approximations and efficient exploration. We then, in Sec. 4.2, demonstrate that the increased model flexibility and promotion of exploration translatesinto lower-variance estimators of the marginal log-likelihood and smaller KL divergence to an MCMC-basedapproximation of the true tree-topology posterior. We also visualize representations of VBPI-Mixtures onreal data. Code for all experiments is provided at Github.",
  "Exploring a Discrete Two-Level Hierarchical Model using Black-Box VI": "Here, we examine how mixtures explore a discrete hierarchical target distribution when learned via black-box VI. We construct this experiment to analyze the capacities of discrete mixture models versus non-mixture models when the variational family of each component does not contain the target distribution.The experiment is thus aligned with Example 3.1.",
  "KL1SSs=1 qs(z1, z2)p(z1, z2), i.e. the reverse KL. The CPD p(z2|z1) is a categorical distribution with": "n2 categories, conditioned on the sampled category in the previous level, z1, and p(z1) is a categorical dis-tribution with n1 categories. The family of variational approximations assumes independence between z1and z2, such that the s-th component is defined as qs(z1, z2) = q2s(z2)q1s(z1), where 2s are the learnablecategory probabilities over the n2 categories of component s. Hence a single component is insufficient tocapture the posterior accurately. Meanwhile, a mixture of these approximations will be flexible enough givena sufficiently large S. The parameters of p are drawn from a Dirichlet distribution with all concentration parameters equal to0.5, and the learning rates when inferring variational distributions are chosen based on a grid search on aseparate target distribution. All s are initialized uniformly over the categories. We compare an S = 10mixture model which uses Z = 1 Monte Carlo estimates to estimate the objective function, with an S = 1and Z = 10 model. This is to equate the number of samples used during learning. We learn the approximations using either K = 1 or K = 10 importance samples. That is, when K = 1 thedistributions are inferred by minimizing the reverse KL, and when K = 10 they are inferred by minimizingthe importance-weighted reverse KL. The accuracies of the distributions inferred with K = 1 are measuredin terms of the forward KL (see ). As an additional metric of accuracy, the distributions inferredusing K = 10 are evaluated also based on the variance of their importance weights (shown in b). Thatis, we sample 10000 importance weights, wi = p(zi1, zi2)/ 1",
  "S": "s qs(zi1, zi2),and compute the variance of these weights. Note that the total number of samples used is the same for allmodels (if S = 1 then Z = 10, and vice versa). The variance metric assesses the aptness of employing thedistributions as proposal distributions (lower is better). Regardless of choice of learning rate, the non-mixture models are not able to accurately capture the posterior.The mixture models can, on the other hand, accurately approximate the target distribution. In figures 3 and4a-4b, we report the mentioned metrics over iterations when n1 = 5 and n2 = 10. All models are trained for60k epochs, with decaying learning rates by 0.9 every 10k epochs.",
  "Posterior Approximations using Real Data": "We performed experiments on eight datasets (Hedges et al., 1990; Garey et al., 1996; Yang & Yoder, 2003;Henk et al., 2003; Lakner et al., 2008; Zhang & Blackwell, 2001; Yoder & Yang, 2004; Rossman et al.,2001) which we will refer to as DS1-8. These are popular datasets for evaluating Bayesian phylogeneticsmethods, and, as in Zhang & Matsen IV (2019); Zhang (2020); Moretti et al. (2021); Koptagel et al. (2022);",
  ": Evaluations of the accuracies of the approximations with respect to the target distribution describedin Sec. 4.1": "Zhang (2023); Zhang & Matsen IV (2024), we focus on learning the approximations of branch-length andtree-topology distributions. Following the referenced works, we assume the exponential branch-length priorhas rate 10 and a uniform prior over all unrooted trees (see Sec. 2 for details about the generative model).The substitution model is the Jukes-Cantor 69 model (Jukes et al., 1969), and the candidate trees, T , aregathered from ten replicates of 10000 ultrafast maximum likelihood bootstrap trees (Minh et al., 2013). The",
  "(a) DS5(b) DS6(c) DS8": ": Visualization of a uniformly weighted S = 2-component mixture of SBNs on (a) DS5, (b) DS6 and(c) DS8, where each node corresponds to a unique tree-topology. The upper row shows the distribution of fivemillion sampled tree topologies from the first component, where a node, , is colored blue if q1() > q2(),or orange otherwise. Vice versa for the lower row. The size of a node is determined by its sampling frequency,which is why nodes with low frequency appear black. The components clearly spread out, exploring differentparts of the space. implementation is based on the code provided by Zhang & Matsen IV (2024), and we trained all VBPImodels during 400,000 iterations, using the same hyperparameter settings as Zhang & Matsen IV (2019);Zhang (2020). Based on the study in Zhang & Matsen IV (2024), we let K = 10 during training. Additionally,all SBN-based models use the same pre-computed topology sets, T .",
  "Visualizing the Explorative Behaviour of Mixtures of SBNs": "To graphically confirm the power of employing mixtures of SBNs, we in visualize representations ofthe learned tree-topology posterior approximations for a subset of the different real datasets. The subsetwas selected based on the datasets where the explorative behavior of the approximations is most clear. Therepresentations for the other datasets, along with more implementation details, are included in the AppendixE.2. The SBNs correspond to VBPI-Mixtures without NFs. The components (upper vs. lower row) havejointly explored the space, partly specializing on disjoint sets of tree-topologies, verifying that the MISELBOobjective promotes coordinated exploration of the discrete latent space, as discussed in Sec. 3.",
  "Quantitative Evaluation of the Approximations": "We quantitatively evaluate the approximations by, first, computing their KL divergences to the true posterior,and by, secondly, benchmarking their marginal log-likelihood estimates.The results are averaged overfive independently trained models with different parameter initializations.All MrBayes (Huelsenbeck &Ronquist, 2001) results were produced using ten million long MCMC runs with four chains, sampling every100 iterations. Our methods are denoted MixS, MixNF,S, representing VBPI-Mixtures with PSPs or NFs,respectively. Mixtures that employ NFs share flow models, as described in Kviman et al. (2023). Statistical distances to the tree-topology posteriorHere, we compare statistical distances to thetree-topology posterior obtained from MrBayes as described above. In , the KL divergence fromthe posterior to the approximations, i.e., KL(p(|X)q()), is computed, where q represents a mixture of",
  "VBPI-NF0.07260.01100.05400.20932.21171.28420.25440.6018MixNF,S=20.06310.00590.04750.09652.03371.08830.11830.5199MixNF,S=30.05980.00510.03770.07691.95261.04610.08470.4567": ": Negative marginal log-likelihood estimates on DS1-8. All VBPI methods use 1000 importancesamples, and the results are averaged over 100 runs and three independently trained models. All modelsare trained with one Monte Carlo sample (Z) and ten importance samples (K), except for those modelswith names notated with specific Z- or K-values.Following Zhang & Matsen IV (2019); Zhang (2020;2023); Zhang & Matsen IV (2024), we bold font the results with the lowest standard deviations (shown inparentheses). Details in Sec. 4.2.2. Mixtures monotonically improve with S.",
  "SBNs, or a single SBN, from VBPI-NF. Lower is better, and, notably, VBPI-Mixtures consistently produceKL divergences across all datasets that monotonically decrease with S": "Marginal log-likelihood estimatesIn terms of marginal log-likelihood estimates, we benchmark ourmethods against the existing VBPI algorithms: VBPI with PSP parameterization (Zhang & Matsen IV,2019), VBPI-NF (Zhang, 2020) with ten RealNVPs (Dinh et al., 2016), and VBPI-GNN (Zhang (2023);EDGE and GGNN). Additionally, we compare our results with the stepping-stone (SS; Xie et al. (2011))method applied to MrBayes as well as to PhyloGFN (Zhou et al., 2024) and ARTree (Xie & Zhang, 2023).The results are given in . All models have been trained using a single Monte Carlo sample and tenimportance samples except for those marked with Z and K respectively. Following Zhang & Matsen IV(2019); Zhang (2020; 2023); Zhang & Matsen IV (2024), we bold font the results with the lowest standarddeviations. Rewarding low-variance estimates is motivated, as they imply, for instance, more reliable Bayesianmodel selections for downstream tasks. We can see that the PhyloGFN has achieved good performanceregarding standard deviation on three of the datasets. However, PhyloGFN does seem to struggle withproducing competitive mean marginal log-likelihood. For VBPI-Mixture, increasing the number of mixturecomponents results in significant improvements in terms of lower standard deviations (on all datasets) andhigher mean marginal log-likelihood scores (especially apparent on the more complex datasets, e.g. DS5-8). Trade-off between marginal log-likelihood and standard deviationsAs mentioned above, in theVBPI literature, the target metric in the marginal log-likelihood estimation task is frequently considered tobe the standard deviation. I.e., the lower the standard deviation, the better, with less focus on the meanmarginal log-likelihood results. The reason for this might be that the mean marginal log-likelihood scoreson DS1-8 are quite saturated, which naturally instead shifts the focus to the standard deviations of theestimates. In Xie & Zhang (2023), they alternatively motive the standard deviation criterion by pointing",
  "to the importance sampling domain, stating that the variance of the estimator is a good reflection of theaccuracy between the importance sampling distribution and the target distribution": "Nonetheless, as we observe that the mean log-likelihoods can be slightly improved when using Z = 3 MonteCarlo samples, we additionally plot the mean log-likelihood vs. standard deviation trade-off between ourS = 3 VBPI-NF-Mixture and the Z = 3 VBPI-NF. This provides the practitioner with more informationwhen deciding which method to employ. Please see . Compute infrastructure and run timesMost computations have been conducted on an AMD EPYC7742 where two cores have been used per run. Final runtimes are shown in . The table shows the jointrun time for both training and testing. Also worth noting is that multiple mixture components also multiplythe number of particles, so the majority of the time increase is due to the Felsenstein pruning algorithm forthe likelihood model evaluation, which grows linearly.",
  "Finally, and crucially, the code used was not optimized for run time, and so a wall-clock time is not an aptmetric for comparisons": ": Compute time reported in minutes for fitting the model as well as evaluating the marginal likelihoodand continuous estimate of ELBO while training every 5000 iterations using 1000 samples with a singleparticle. The Ufboot2 was run on i9-13900k, the command used for a single repetition for DS1 ./iqtree2-s DS1.fasta -bb 10000 -wbt -m JC69.",
  "Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. ICLR,2017": "Hazal Koptagel, Oskar Kviman, Harald Melin, Negar Safinianaini, and Jens Lagergren. Vaiphy: a variationalinference based algorithm for phylogeny. In Advances in Neural Information Processing Systems, 2022. Oskar Kviman, Harald Melin, Hazal Koptagel, Victor Elvira, and Jens Lagergren. Multiple importancesampling elbo and deep ensembles of variational approximations. In International Conference on ArtificialIntelligence and Statistics, pp. 1068710702. PMLR, 2022.",
  "Bui Quang Minh, Minh Anh Thi Nguyen, and Arndt von Haeseler. Ultrafast approximation for phylogeneticbootstrap. Mol. Biol. Evol., 30(5):11881195, May 2013": "Andriy Mnih and Danilo Rezende. Variational inference for monte carlo objectives. In Maria Florina Balcanand Kilian Q Weinberger (eds.), Proceedings of The 33rd International Conference on Machine Learning,volume 48 of Proceedings of Machine Learning Research, pp. 21882196, New York, New York, USA, 2016.PMLR. Antonio Khalil Moretti, Liyi Zhang, Christian A Naesseth, Hadiah Venner, David Blei, and Itsik Peer. Vari-ational combinatorial sequential monte carlo methods for bayesian phylogenetic inference. In Uncertaintyin Artificial Intelligence, pp. 971981. PMLR, 2021. Warren Morningstar, Sharad Vikram, Cusuh Ham, Andrew Gallagher, and Joshua Dillon. Automatic dif-ferentiation variational inference with mixtures. In International Conference on Artificial Intelligence andStatistics, pp. 32503258. PMLR, 2021.",
  "This introduction aims to briefly explain the basic concepts required to understand the generative modelprovided in the main text": "Phylogenetic trees capture evolutionary relationships among species and provides valuable insights intolifes evolutionary history. Within this domain, phylogenies are often depicted as bifurcating tree graphs,where nodes represent common ancestors, and branches (edges) signify evolutionary events and geneticdistances between species. This framework enables an understanding of species relatedness, ancestry, andthe evolutionary processes governing lifes diversity. Bayesian phylogenetic inference builds upon this framework by applying Bayesian statistical methods to inferthe evolutionary history. It allows for a probabilistic approach to model uncertainty and variation, consideringprior beliefs about evolutionary parameters and updating these beliefs as new data is incorporated. It iscommon to use DNA or protein sequences as data since it describes different attributes of the species, and theedges represent a mutation between species. Through sampling from a posterior distribution and utilizingtools like Markov Chain Monte Carlo (MCMC) methods or variational inference, Bayesian phylogeneticsoffers a robust and nuanced view of evolutionary relationships, integrating multiple sources of informationand providing a rigorous statistical foundation for evolutionary hypotheses. Mainly, two latent variables are regarded as important in Bayesian phylogenetic inference. First, the treetopology, , a binary tree with the observations assigned to its leaves. The tree-topology space grows as(2n 3)!! for rooted and (2n 5)!! for unrooted trees, where n is the number of leaves (observations/taxa).Furthermore, each edge, e, of the topology is associated with a positive continuous variable, the branchlengths, b(e). The Cartesian product of discrete and continuous spaces makes inference in phylogenetics achallenging task.",
  "BConflicting Tree Topologies": "Here we construct a realistic scenario where DNA sequences induce conflicting tree-topologies in the posteriorthat cannot be modelled by the vanilla SBN. On the other hand, they can be captured by VBPI-Mixtures. It is well-known that DNA data sometimes has conflicting signals. Here we construct a toy example todemonstrate how 1 and 2 can have higher posterior support than 3 and 4. For simplicity, we use theconnection between a lower parsimony score and a higher likelihood when branch lengths are short (to appearin the appendix). First, consider that the leaves in have DNA sequences with nucleotides at sites iand j specified in the table below (the first and second columns represent i and j, respectively).",
  ", (31)": "where the second term is the control variate. The control variate is, however, independent of the mixtureformulation (except for the 1/S scaling which does not affect the unbiasedness of the term). As such, sinceVIMCO is an unbiased estimator of the gradient (Mnih & Rezende, 2016), then so is our version of it.",
  "E.1The Two-Level Hierarchical Model": "Running a grid search for all five algorithms (S = 1, ..., 5), using n1 = 5 and n2 = 10, we found that thelearning rates 0.01, 0.1, 0.1, 0.2, and 0.25, respectively, were optimal. That is, these achieved the smallestKL divergences. For larger learning rates, S = 1 did not converge or converge to worse KL divergences. Theoptimal learning rates found in the grid searches were used in all subsequent experiments. In , we visualize the KL curves as functions of the number of training iterations. For all configurationsof K, n1 and n2, the S = 5 model performs best. The patternS = 1 converges slower and to worse KLdivergences than S > 2holds also when all models use the same number of importance samples, K (shownin Fig 7).",
  ": KL curves for the two-level hierarchical model using different configurations of K, n1 and n2": "assigning the most probable peak to a cluster and subsequently assigning all unassigned trees to the samecluster if their distance from the peak tree was within one standard deviation below the mean distance of allunassigned trees. This iterative process continued until all topologies were assigned, or until eight clusterswere reached. For graph creation, we employ the Graphviz layout known as Scalable Force-Directed Placement (SFDP),in conjunction with the NetworkX library Hagberg et al. (2008). The clusters are represented by differentcolors, and the size of each node is determined by the normalized sampling frequency in Fig 1, 8. Moreover,edges between the topologies are only displayed if their distance is exactly one. Its worth noting that therSPR distance measure is utilized, which counts the number of changes similar to the approach used in theMCMC method. To ensure that the visualization focused on the most credible information, we imposed a constraint bylimiting the nodes to the 95% most credible set.This ensured that only the most reliable nodes wereincluded.Additionally, to manage computational resources effectively, we set a maximum limit of 4096nodes for the graphs. A similar approach was used for , 9, with the main difference being that we sampled from thecomponents and displayed the joint set of topologies. The colors in this figure were based on which componentsampled the topology the most. This representation was considered an approximation of the posterior.",
  "not parallelize the parameter updates of the parameters of the mixture components. This can, however, bedone, in order to heavily decrease training time": "Additionally, using shared parameters for the mixture components can also be utilized, if the practitioneris running on a limited memory budget. However, shared weights would violate the assumption of disjointSBN parameter sets in Appendix D. Nonetheless, devising clever modifications to reduce the number ofparameters and training times for mixtures in black-box VI is an exciting future research field.",
  "GBroader Impact": "Bayesian phylogenetic inference algorithms are crucial for researchers to reason about uncertainty in theirevolutionary findings. Variational inference algorithms provide a compelling alternative to MCMC-basedalgorithms as a parametric approximation is obtained. This implies that VI, and VBPI specifically, can beused in settings where the application of MCMC is less straightforward, for instance in out-of-distributiondetection, or evaluation on held-out data. Also, as we have shown in our experiments in this paper, modelevaluation can be more robust when using VI over MCMC, resulting in a smaller variance of the estimatorof the marginal log-likelihood. This is an important feature for downstream tasks.",
  "(d) DS4(e) DS7": ": Visualization of a uniformly weighted S = 2-component mixture of SBNs on (a) DS1, (b) DS2 and(c) DS3, (d) DS4 and (e) DS7, where each node corresponds to a unique tree-topology. The upper row showsthe distribution of five million sampled tree topologies from the first component, where a node, , is coloredblue if q1() > q2(), or orange otherwise. Vice versa for the lower row. The size of a node is determinedby its sampling frequency, which is why nodes with low frequency appear black. The components clearlyspread out, exploring different parts of the space."
}