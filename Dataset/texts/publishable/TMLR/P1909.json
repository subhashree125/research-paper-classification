{
  "Abstract": "Likelihood-based deep generative models such as score-based diffusion models and variationalautoencoders are state-of-the-art machine learning models approximating high-dimensionaldistributions of data such as images, text, or audio. One of many downstream tasks theycan be naturally applied to is out-of-distribution (OOD) detection. However, seminal workby Nalisnick et al. which we reproduce showed that deep generative models consistentlyinfer higher log-likelihoods for OOD data than data they were trained on, marking an openproblem. In this work, we analyse using the gradient of a data point with respect to theparameters of the deep generative model for OOD detection, based on the simple intuitionthat OOD data should have larger gradient norms than training data. We formalise measuringthe size of the gradient as approximating the Fisher information metric. We show that theFisher information matrix (FIM) has large absolute diagonal values, motivating the use ofchi-square distributed, layer-wise gradient norms as features. We combine these features tomake a simple, model-agnostic and hyperparameter-free method for OOD detection whichestimates the joint density of the layer-wise gradient norms for a given data point. Wefind that these layer-wise gradient norms are weakly correlated, rendering their combinedusage informative, and prove that the layer-wise gradient norms satisfy the principle of (datarepresentation) invariance. Our empirical results indicate that this method outperforms theTypicality test for most deep generative models and image dataset pairings.",
  "Introduction": "Neural networks can be highly confident but incorrect when given inputs different to the distribution of datathey were trained on (Szegedy et al., 2014; Nguyen et al., 2015). While domain generalisation Zhou et al.(2021) and domain adaptation (Garg et al., 2023; Ganin et al., 2016) methods tackle this problem by learning",
  "Published in Transactions on Machine Learning Research (10/2024)": ": Lt1 follows the pattern from Nalisnick et al. (2019a) for a variety of t values. We replicate figure2 for t = 64 [Left] and t = 512 [Right] using batch size B = 5 (we use this batch size for reasons of limitedcompute.) : For diffusion models, Lt1 is most informative for anomaly detection for low values of t. Wecompute the AUROC values for all in/out dataset distribution pairings using t = 2n for n = 0, 1 . . . 9 andbatch size B = 5 (for reasons of limited compute).",
  "OOD detection: Problem Formulation": "Given training data x1 . . . xN drawn from a distribution p over the input space X RD, we define theproblem of OOD detection as assigning an OOD score S(x) to each x X such that points with low OODscores are semantically similar to points sampled from p. OOD detection is unsupervised if it is not givenclass label information at training time. The specific problem we are interested in is leveraging recent advances in deep generative models forunsupervised OOD detection. Here a deep generative model p is trained to approximate the distribution ofsome training data x1 . . . xN p, and S is a statistic derived from p (such as the model likelihood Nalisnicket al. (2019b), a latent variable hierarchy Schirrmeister et al. (2020); Havtorn et al. (2021), or combinationsthereof Morningstar et al. (2021)). In order to evaluate an OOD detection method, one is required to select semantically dissimilar surrogateout-distributions (e.g. a different dataset) to test against. Previous work has sought to define OOD detectionas a generic test against data sampled from any differing distribution Hendrycks & Gimpel (2017). Ouradditional requirement that the out-distribution is semantically dissimilar is motivated by recent theoreticalwork by Zhang et al. (2021a) showing that a single-sample test against all out-distributions is impossible.",
  "Likelihood-based methodology for unsupervised OOD detection": "Likelihood thresholding.Bishop (1994) proposed using the learned models negative log likelihood asan OOD score S(x) = log p(x). In their seminal paper, Nalisnick et al. empirically demonstrated thatthis approach fails for a wide variety of deep generative models (Nalisnick et al., 2019a). In particular theyshowed that certain image datasets such as SVHN are assigned systemically higher likelihoods than otherimage datasets such as CIFAR10, independent of the training distribution. We replicate this result (for aGlow model, a type of normalising flow, and for the first time a denoising diffusion model) in . Intheir follow up work Nalisnick et al. argue that, in the example of a standard Gaussian of large dimension D,samples close to the origin should be classified as OOD as the Gaussian annulus result Blum et al. (2020)demonstrates that the vast majority of samples from have a distance of D from the origin. Generalising thisto generative models, they argue that samples with likelihoods much higher than likelihoods of in-distributionsamples must be semantically atypical (Nalisnick et al., 2019b). They use this to motivate OOD scoringbased on the likelihood being too high or too low, defining the typicality Cover & Thomas (1991) score asS(x) = | log p(x) H| , where H is the average log-likelihood on some held-out training data. Likelihood ratios.The likelihood assigned by deep generative models has been shown to strongly correlatewith complexity metrics such as the compression ratio achieved by simple image compression algorithmsSerr et al. (2020), and likelihoods from other generative models trained on highly diverse image distributionsSchirrmeister et al. (2020), with the highest likelihoods being assigned to constant images. This is somewhatexpected, as for discrete data the negative log likelihood is directly proportional to the number of bits neededto encode the data under arithmetic coding. To add to these findings, in Appendix A.3 we use a very simplecomplexity metric TV , the total variation achieved by considering the image as a vector in 784, to showthat the whole of MNIST is contained in a set of bounded complexity with volume (Lebesgue measure) 10116.Thus a model needs to only assign a very low prior probability mass to this set for high likelihoods to be",
  "achieved, demonstrating the important connection between volume, complexity and model likelihoods whichwe hence discuss in 2.3": "Ren et al. (2019) argue in favour of using likelihood ratio tests in order to factor out the influence of thebackground likelihood, the models bias towards assigning high likelihoods to images with low complexity. Inpractice, this requires modelling the background likelihood via corruption of training data Ren et al. (2019),out-of-the-box and neural compressors Serr et al. (2020); Zhang et al. (2021b) or the levels of a modelslatent variable heirarchy Schirrmeister et al. (2020); Havtorn et al. (2021), leading to restrictions for thedata modalities or models to which the method can be applied to. In general there is a limited number ofuse cases whereby one can pre-specify the OOD distribution pOOD they are interested in well enough thatthey can evaluate the likelihood of the data under this OOD distribution log pOOD(x) (which is necessaryto evaluate the likelihood), without access to samples from this distribution at which point classificationbecomes a better-studied option.",
  "Representation dependence of the likelihood": "Le Lan & Dinh (2021) emphasise that the definition of likelihood requires choosing a method of assigningvolumes to the input space X. Specifically, datapoints could be represented as belonging to some other inputspace T , linked via a smooth invertible coordinate transformation T : X T . The model probability densityfor a given datapoint x X, which we denote pX (x), will thus differ from the probability density pT (t) ofthe corresponding point t = T(x) by a factor of the Jacobian determinant of T (Le Lan & Dinh, 2021):",
  "The volume element T": "x1 describes the change of volumes local to x as it is passed through T. This termcan grow or shrink exponentially with the dimensionality of the problem, making its effect counter-intuitivelylarge. As an empirical example in the case of image distributions, in and Appendix B.1 Fig 8 we considerthe case of a change of color model T RGBHSV from a Red-Green-Blue (RGB) to Hue-Saturation-Value(HSV) representation. We compute the induced change in bits per dimension as a scaled log-value of the",
  "log T RGBHSV": "x and report values for 20 non-cherry picked CIFAR-10images ranging from 0.18 to 1.76. For comparison the average BPD reported in the seminal paper by Nalisnicket al. (2019a) was 3.46 on CIFAR10, compared to 2.39 on SVHN when evaluating with the same model. Hence,if we use the likelihood for OOD detection, whether we classify a sample as OOD or not may flip for somesamples merely by changing how the data is represented. Motivated by the strong impact of the volume element, Le Lan & Dinh (2021) propose a principle of(representation) invariance: given a perfect model p of the data distribution, the outcome of an unsupervisedOOD detection method should not depend on how we represent the input space X. In theory likelihoodratios are representation-invariant (Le Lan & Dinh, 2021), however in practice the method used to generatethe background distribution often re-introduces dependence on the representation. For example Ren et al.",
  "Invariance of the gradient under invertible transformations": "To achieve a representation invariant OOD score (Le Lan & Dinh, 2021), we are thus motivated to quotientout the effect of the volume element in Eq. (8). We now present our first theoretical contribution, whichshows that methods based on the gradient of the log-likelihood do precisely this. Proposition 1. Let pX (x) and pT (t) be two probability density functions corresponding to the same modeldistribution p being represented on two different measure spaces X and T . Suppose these representationsencode the same information, i.e. there exists a smooth, invertible reparameterization T : X T such thatfor x X and t T representing the same point we have T(x) = t. Then, the gradient vector (log p) isinvariant to the choice of representation, and in particular, (log pT )(t) = (log pX )(x).",
  "Proof. See Appendix A.1. We prove analagous results for variational lower bounds (e.g. the ELBO of aVAE) in Appendix A.2": "Remark 1. Training a generative model p0 with initialisation parameters 0 with log-likelihood as the lossvia gradient-descent produces training trajectories 0, 1, . . . N which are representation-invariant. The interpretation of the above results is subtle. We would like to caution the reader by noting it does notmean that the inductive biases are discarded when the gradient is computed as inductive biases pertaining todistances between data points are frequently encoded in the parameter space. Further, remark 1 may explainwhy the likelihood can still be used to train deep generative models and allow them to generate convincingsamples when using a gradient-based optimisation algorithm, even though the likelihood value itself appearsuninformative for detecting if data is in-distribution.",
  "Layer-wise gradients are highly informative and differ in size by orders of magnitudes": "We are now interested in formulating a method which uses the intuitively plausible (see 1) and datarepresentation-invariant (see 2.4) score l(x) = {log p}(x) for OOD detection. A nave approach wouldbe to measure the size of the score vector by computing the L2 norm l(x)22 of the gradient Nguyen et al. (2019). We can view this L2 norm as the directional derivative of the log-likelihood log p in the direction ofits own gradient l(x), which can be intuited as a measure of how much the model can learn about thegiven datapoint with one small gradient update. In the following, we analyse this idea, demonstrating its limitations: We empirically find that the size ofthe norm of the score vector is dominated by specific neural network layers which the overall gradient normcannot capture. In , we train deep generative models (here: Glow Kingma & Dhariwal (2018) anddiffusion models Ho et al. (2020)) on a training dataset (here: CelebA). We then draw a batch of itemsfrom different evaluation datasets and compute the squared layer-wise L2-norm of the gradients of thelog-likelihood of a deep generative model with respect to the parameters j of the corresponding layer, i.e.",
  ". The histrogrammes in the left two columns plot fj(x1:B) for each layerseparately, the plots in the rightmost column shows their interaction in a scatterplot": "Two points are worth noting: We observe that for a given neural network layer (and different batches),the gradients are of a similar size, but across layers, the scale of the layer-wise gradient norms differs byorders of magnitudes. In particular, taking the norm over the entire score vector would overshadow thesignal of layers with a smaller norm by those on much larger magnitudes. Second, note that the layer-wisegradient norms do not strongly correlate for randomly selected layers. In particular, one may find two layerswith corresponding features fj(x1:B) and fk(x1:B) which allow us to separate training (in-distribution)",
  ". [Left and Middle] shows the": "two layer-wise gradients separately, [Right] shows their interaction in a scatter plot. In Appendix B Figures 9- 11, we provide our complete results, showing more layers from three likelihood-based generative models,each trained and evaluated on five datasets. from evaluation (OOD) data points with a line in this latent space. Considering an example, when fixingf1(x1:B) 7.5, large negative values of f2(x1:B) are in-distribution, and as they become more positive,they correspond to the out-of-distribution datasets CIFAR-10 and ImageNet32, respectively, with very highprobability. This renders the layer-wise information superior over the overall gradient for use as discriminativefeatures in OOD detection. We present our complete results in Appendix B Figs. 9-11, showing furtherlayers (histogrammes), also for other deep generative models (VAEs Kingma & Welling (2014)) and trainingdatasets (SVHN, CelebA, GTSRB, CIFAR-10 & ImageNet32).",
  "Approximating the Fisher Information Matrix": "In practise, the full FIM has P P entries, where P = || is the number of parameters of the deep generativemodel. This means that is too large to store in memory, and would furthermore be too expensive to computeand invert. For example our glow implementation has P 44 million parameters, and thus the FIM wouldrequire 7, 700 terabytes to store using a float32 representation. To develop a computable method, wetherefore need to find a way to approximate the FIM. What would be a good choice for this approximation? This problem is non-trivial due to its dimensionality. We start answering this question by computing theFIM in Eq. (2) restricted to a subset of parameters in two layers with parameters 1, 2 of a Glow Kingma &Dhariwal (2018) model trained on CelebA, using the Monte-Carlo (MC) approximation2",
  "We are now interested in operationalising our observations so far into an algorithm that can be used inpractice": "In addition to the diagonal dominance phenomenon enabling a layer-wise approximation via a diagonal matrix,recall that layer-wise gradients contain more information than the overall gradient norm as the scale of thegradient norms differs by orders of magnitudes. We are therefore motivated to consider each layer 1, 2 . . . Jin our model separately and combine the results as an OOD score in the second step. Specifically, if we selecta layer j we can consider a restricted model where only the parameters in this layer are variable, and theother layers are frozen. We can approximate the score statistic (2) on this restricted model, whose parametersare more homogeneous in nature. In practice, we take the score vector for a layer jl(x) and attempt toapproximatejl(x)2F IM, which should follow a 2 test with |j| degrees of freedom for in-distributiondata. As discussed in 3.3, we approximate the FIM restricted to this layer as a multiple of the identity. Fora batch of B (possibly equal to 1) of data points x1:B we define features fj, which via our identity-matrix",
  ".(4)": "Given that these layer-wise L2 norms fj should be proportional to a 2 distributed variable with a largedegree of freedom, we expect log fj to be normal-distributed Bartlett & Kendall (1946). In (furtherresults in Appendix B) we observe a good fit of log fj to a Normal distribution, empirically validating thisholds in spite of our approximations. This gives rise to a natural method of combining the layer-wise L2 norms: we simply fit Normal distributions toeach log-feature log fj independently, and then use the joint density as an in-distribution score. Algorithms1 to 3 summarise our proposed method. As with other unsupervised OOD detection methods (Nalisnicket al., 2019b), we assume the existence of a small fit set of data held-out during training to accurately fit toeach feature fj. Note that in practise our method is very straight-forward to implement, requiring only a fewlines of PyTorch code. Many other methods could possibly be constructed from our theoretical and empiricalinsights, and we will discuss potential future work in 6. In Appendix B.5 we observe a mild performance improvement, uniformly across datasets, with the jointdensity approach in comparison to using Fishers method Fisher (1938) when combining these statistics usingz-scores. We hypothesise that this could be due to the density being more robust to correlation betweenadjacent layers as noted in . Our presented method does not enjoy the full invariance under rescalingof the model parameters as the true score statistic (see 3.2). However, in Appendix A.4 we show that itdoes satisfy invariance when rescaling each layer individually, justifying our use of the density in this setting.Our method satisfies the desiderata of the principle of invariance (see 4), is hyperparameter-free, and isapplicable to any data modality and model with a differentiable estimate of the log-likelihood.",
  "Application to diffusion models": "A denoising diffusion model Sohl-Dickstein et al. (2015a); Ho et al. (2020) uses a chain of latent variables{xn}t=Tt=0 achieved by gradually adding noise (represented by the conditional distributions q(xt|xt1)) tothe initial distribution of images x = x0 until the final latent variable xT is approximately Gaussian withmean zero and identity variance. The inverse process is then learned by a model approximating p(xt1|xt).Diffusion models have gained in popularity due to their ability to produce samples with high visual fidelity andsemantic coherence, making them a natural candidate for OOD detection Graham et al. (2023). Nonetheless,the full variational lower bound on the log-likelihood defined by Ho et al. (2020) is expensive to compute as itrequires running |T| inference steps on the network modelling p(xt1|xt). For our setup, |T| = 1000 andrunning a full-forward/backward pass requires roughly 1 minute of compute per sample. Thus, we choose onecomponent of the variational lower bound, namely the one-step log-likelihood,",
  "l(x) = l(x0) = Ex1q(x1|x0) log p(x0|x1)": "We refer to Appendix B.4 for computational details, noting that our implementation using one sample fromq(x1|x0) only requires 1 pass on of inference on p(x0|x1). Despite this value being very different to theintractable full log-likelihood p(x0), in we observe the same open problem and phenomenon asNalisnick et al. (2019a) reported for the full likelihood estimates of other deep generative models. In AppendixB.4 we perform an ablation study on this one-step component of the variational lower bound used, findingthe result that for both our method and the typicality method Nalisnick et al. (2019b), the componentswhich are computed with less noise added to the image xt used as input to the model p(xt1|xt) are moreinformative for OOD detection, which could intuitively be understood as the noised image xt itself beingmore informative in this regard.",
  "In this section we review related work which uses gradients for unsupervised OOD detection": "In concurrent work to ours, Choi et al. (2021); Bergamin et al. (2022) each present an approximation toRaos score test (Radhakrishna Rao, 1948). They independently approached the problem from the directionsof training on the given sample of OOD data Xiao et al. (2020) and application of tests from classicalstatistics, respectively. These methods use approximations of the FIM from the field of optimization Amari(1998); Tieleman et al. (2012); Kingma & Ba (2015), whereas we use a simpler approximation tailored tothe task of unsupervised OOD detection, and complement this with our empirical observations of the FIMin 3.3. Bergamin et al. (2022) compute a score test across the whole model by approximating the FIM asa diagonal, with elements F = ( log p(x))2 + for a small hyperparameter = 108, which which isused in optimization for its damping effect Martens (2020) and helps to mitigate numerical instabilities whendividing by F. Our method differs in that it explicitly encodes the layer-homogeneity of the model (wherebyparameters in the same layer have similar gradient sizes and perform similar functions in the network), andthe predicted chi-square distribution of the score. We also note that layers which contain squared gradientvalues that are << 108 (see Appendix B.3 Figures 14 and 15) would have their information nullified withoutcareful tuning of , this can further be observed in 10 and 11 where there are entire layers which provideinformativity for OOD detection and and have an L2 norm of < 108. Choi et al. (2021) split the problem ofOOD detection layer-wise, but use the more complex EKFAC George et al. (2018) algorithm to account fordependencies between adjacent parameters. After some normalisation and additional processing steps, theauthors compute the ROSE metric by taking the maximum feature over some pre-selected subset of layers.Our method differs as it uses a holistic score influenced by all the model layers. Nguyen et al. (2019) are interested in using VAEs to detect anomalous web traffic in a semi-supervised setting,measuring the difference between a test gradient and labelled anomalous gradients. Our method differs in thatdoes not require anomalous examples. Kwon et al. (2020) computes a cosine similarity between the gradientsin the decoder of a VAE and the average gradients observed during training as their OOD metric of choice.In this work, we advocate for using the size of the gradient vector rather than its angle as done in Kwon et al.",
  "ImageNet321.00000.99951.00000.9480-": ": Comparison of the AUROC values (higher is better) of our method to the typicality test Nalisnicket al. (2019b) for batch sizes B = 1, 5. We train Glow Kingma & Dhariwal (2018) models on five naturalimage datasets (columns) and evaluate the ability of the model-method combination to reject the otherdatasets (rows). Bold indicates the element-wise higher value comparing both methods.",
  "ImageNet320.98621.00000.93090.8532-": ": VAE models Comparison of the AUROC values (larger values are better) of our method to thetypicality test Nalisnick et al. (2019b) for batch sizes B = 1, 5. We train VAE Kingma & Welling (2014)models on five natural image datasets (as columns) and evaluate the ability of the model-method combinationto reject the other datasets (as rows). Our VAE Kingma & Welling (2014) implementation uses entirely convolutional layers, in we notethat the samples produced approximate the colour palate of the train datasets well but have poor semanticcoherence. In table 7 we note poor performance for both our methods using this models as a backbone.",
  "Conclusion": "We analysed an approximation to the Fisher information metric for OOD detection. Our work has two keylimitations: First, while we have provided the most extensive empirical benchmark of deep generative models,OOD and in-distribution datasets, datasets beyond images and for instance large language models should betested. Second, while we focused on comparing it to the best performing, model-agnostic, hyperparameter-freeOOD method, further empirical benchmarking against other methods should be conducted. Future workshould investigate other, potentially more computationally expensive methods for approximating the Fisherinformation metric and its use in OOD detection.",
  "Saurabh Garg, Nick Erickson, James Sharpnack, Alex Smola, Sivaraman Balakrishnan, and Zachary C Lipton.Rlsbench: Domain adaptation under relaxed label shift. arXiv preprint arXiv:2302.03020, 2023": "Thomas George, Csar Laurent, Xavier Bouthillier, Nicolas Ballas, and Pascal Vincent. Fast approximatenatural gradient descent in a kronecker factored eigenbasis. In S. Bengio, H. Wallach, H. Larochelle,K. Grauman, N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing Systems.2018. Mark S. Graham, Walter H.L. Pinaya, Petru-Daniel Tudosiu, Parashkev Nachev, Sebastien Ourselin, and JorgeCardoso. Denoising diffusion models for out-of-distribution detection. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition (CVPR) Workshops, June 2023. Jakob D. Havtorn, Jes Frellsen, Sren Hauberg, and Lars Maale. Hierarchical vaes know what they dontknow. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference onMachine Learning. 1824 Jul 2021.",
  "James Martens. New insights and perspectives on the natural gradient method. Journal of Machine LearningResearch, 21(146):176, 2020": "Warren Morningstar, Cusuh Ham, Andrew Gallagher, Balaji Lakshminarayanan, Alex Alemi, and JoshuaDillon. Density of states estimation for out of distribution detection. In Arindam Banerjee and KenjiFukumizu (eds.), Proceedings of The 24th International Conference on Artificial Intelligence and Statistics.1315 Apr 2021. Eric Nalisnick, Akihiro Matsukawa, Yee Whye Teh, Dilan Gorur, and Balaji Lakshminarayanan. Do deepgenerative models know what they dont know? In International Conference on Learning Representations,2019a.",
  "Eric Nalisnick, Akihiro Matsukawa, Yee Whye Teh, and Balaji Lakshminarayanan.Detecting out-of-distribution inputs to deep generative models using typicality, arXiv, 2019b": "Anh Nguyen, Jason Yosinski, and Jeff Clune. Deep neural networks are easily fooled: High confidencepredictions for unrecognizable images. In Proceedings of the IEEE conference on computer vision andpattern recognition, 2015. Quoc Phong Nguyen, Kar Wai Lim, Dinil Mon Divakaran, Kian Hsiang Low, and Mun Choon Chan. Gee:A gradient-based explainable variational autoencoder for network anomaly detection. In 2019 IEEEConference on Communications and Network Security (CNS). IEEE, 2019. George Papamakarios, Eric Nalisnick, Danilo Jimenez Rezende, Shakir Mohamed, and Balaji Lakshmi-narayanan. Normalizing flows for probabilistic modeling and inference. The Journal of Machine LearningResearch, 22(1):26172680, 2021. C. Radhakrishna Rao. Large sample tests of statistical hypotheses concerning several parameters withapplications to problems of estimation. Mathematical Proceedings of the Cambridge Philosophical Society,44(1):5057, 1948. Jie Ren, Peter J. Liu, Emily Fertig, Jasper Snoek, Ryan Poplin, Mark Depristo, Joshua Dillon, and BalajiLakshminarayanan. Likelihood ratios for out-of-distribution detection. In H. Wallach, H. Larochelle,A. Beygelzimer, F. d'Alch-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information ProcessingSystems. 2019. Tim Salimans, Andrej Karpathy, Xi Chen, and Diederik P Kingma. Pixelcnn++: Improving the pixelcnnwith discretized logistic mixture likelihood and other modifications. arXiv preprint arXiv:1701.05517, 2017.",
  "Tijmen Tieleman, Geoffrey Hinton, et al. Lecture 6.5-rmsprop: Divide the gradient by a running average ofits recent magnitude. COURSERA: Neural networks for machine learning, 4(2):2631, 2012": "Dennis Ulmer, Lotta Meijerink, and Giovanni Cin. Trust issues: Uncertainty estimation does not enablereliable ood detection on medical tabular data. In Emily Alsentzer, Matthew B. A. McDermott, FabianFalck, Suproteem K. Sarkar, Subhrajit Roy, and Stephanie L. Hyland (eds.), Proceedings of the MachineLearning for Health NeurIPS Workshop. 11 Dec 2020. Aaron Van den Oord, Nal Kalchbrenner, Lasse Espeholt, Oriol Vinyals, Alex Graves, et al. Conditional imagegeneration with pixelcnn decoders. Advances in neural information processing systems, 29, 2016. Zhisheng Xiao, Qing Yan, and Yali Amit. Likelihood regret: An out-of-distribution detection score forvariational auto-encoder. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.),Advances in Neural Information Processing Systems. 2020.",
  "A.1Proof of proposition 1": "Proposition 1.Let pX (x) and pT (t) be two probability density functions corresponding to the same modeldistribution p being represented on two different measure spaces X and T . Suppose these representationsencode the same information, i.e. there exists a smooth, invertible reparameterization T : X T such thatfor x X and t T representing the same point we have T(x) = t. Then, the gradient vector (log p) isinvariant to the choice of representation, and in particular, (log pT )(t) = (log pX )(x).",
  "A.2Representation-Invariance of variational lower-bound gradients": "Assume the same setup as in A.1, but this time with a variational Bayesian method such as a VariationalAutoEncoder Kingma & Welling (2014) with latent variable given by z, decoder probability density pX (x|z)and encoder probability density q(z|x), noting that the decoder probability density is that which dependson X. The Evidence Lower Bound on the log-likelihood pX (x) is given by",
  "(d + 1)": "Application to MNIST We can navely apply this result to MNIST images y 2828 by set-ting d = 282 and drawing a snake pattern through our images as illustrated in , settingyij = x28(j1)+(1)j+1(i14)+14. Computing this numerically for the whole MNIST dataset, we see that = 102.9 is sufficiently large such that the whole MNIST dataset is contained in E(), which we can computehas an approximate measure of E() 10116.76 10116. Note that this is not the tightest bound onecould give; for example vertical variations are neglected and membership in E() does not restrict xi fromdrifting outside the set .",
  "A.4(Weak) parameterisation in-variance of our method": "Let , be two parameter spaces of the same model p, linked by the smooth invertible reparameterisationP : , such that for = P() we have p = p. In this setting, one can derive that the FisherInformation Metric Radhakrishna Rao (1948) is invariant under P, ie that for all x1, x2 X we havel(x1)F 1 l(x2)T = l(x1)F 1 l(x2)T (see (2) for our notation). As we merely approximate theFIM in our method we cannot make the same guarantee for all P, we can however prove a similar result if Plinearly rescales the layers: Proposition 4. As in 3.4 Let 1, 2 . . . J be the layers of our model. P : be a smooth invertiblereparameterisation of our model which linearly rescales the layers, ie P(1, 2 . . . J) = d11, d22 . . . dJJfor some constants d1, d2 . . . dJv R. Then, the resulting anomaly score of our method is invariant under P.",
  "A.5Comparison to classical invariance properties": "In classical statistics, there is a separate and notion of invariance that is incompatible with that proposed byLe Lan & Dinh (2021). The setup proposed by Lehmann et al. (1986) is one in which we consider a group oftransformations from the input space to itself g : X X which are sufficiently narrow such that there is acorresponding group of transformations to the parameter space g : that counteract the effect of thetransformation, formally defined by:",
  ".(8)": "One example of where this setup is applicable is applying dilations to an input space of a multivariatenormal distribution, whereby any linear dilation of the input space can be counteracted by a dilation of thecovariance matrix. This is not the case for the arbitrary transformations f considered in Proposition 1 ofLe Lan & Dinh (2021), which we cannot guarantee to be counteracted by some transformation of a generativemodels parameters. Even the simple example of the non-linear RGB-HSV transformation we give in 2.3can only approximately be counteracted by changing the generative models parameters. In contrast, thesetup proposed by Le Lan & Dinh (2021), implicitly considers transformations from arbitrary measure spacesf : X f(X), and considers the pullback:",
  "32 32,(11)": "where HSV is the Lebesgue measure in HSV-space, RGB is the Lebesgue measure in RGB-space, andpHSV and pRGB are corresponding probability density functions for any distribution defined over the setof images. We compute the Radon-Nikodym derivative dHSV dRGB in 11 by computing the pixel-wise Jacobiandeterminants of the RGB-HSV transformation T RGBHSV : R3 R3. In order to make the comparison fair,we dequantaize pixel xij R3 add a small amount of normally distributed noise ij N(0, I33), ie we setxij = xij + ij",
  "B.2Replications of": "In figures 9- 11 we provide robust replications of using randomly chosen layers. The layers are sortedwith the right-hand layer being the deepest\" (ie. the closest to the latent variables). We observe that thegradients are more separated for models trained on the semantically distinct datasets SVHN, CelebA & GTSRB,mirroring the superior performance our method achieves in these cases.",
  "B.3.2Windows into the FIM of a diffusion model": ": The strong diagonal of the FIM for diffusion models. We replicate with a diffusion model.As before, we randomly select two layers from a diffusion model trained on CelebA and plot the anapproximation of the FIM, this time using the gradients of the one-step log-likelihood. Note that the firstlayer selected has fewer than 50 weights, so we plot its entire layer-wise FIM. Again, we normalise the rowsand columns by the diagonal values F to enable cross-layer comparison.",
  "B.3.3Raw, single-layer FIMs of Glow models": ": Raw layer-wise FIMs for glow models. For each row of plots, we randomly select 4 layers fromglow models trained on (going from top to bottom) SVHN, CelebA, GTSRB, CIFAR-10 and ImageNet32. Wethen plot the raw FIM F values for max(50, ||) weights in these layers, using a separate colorbar per layerto account for the fact that the absolute sizes of the FIM elements vary by orders of magnitudes from layerto layer.",
  "ELt1 = KtExtq(x0) (xt, t)2 + Ct.(14)": "Here kt and Ct are constants independent of x0, x1 . . . xT and , which can thus be omitted from computations.To compute the expectation, we use one sample from the reverse process xt q(x0), motivated by ourfindings in section B.4.4 which show little to no performance gains from using five samples. Thus, we defineour set of likelihood proxies as in (14) as Lt(x) = (xt+1, t + 1) for a single sample of noise from thereverse process xt+1 q(x). Note that computing Lt(x) requires only one pass through the network, makingit very efficient to compute. In section B.4.3 we do an ablation study on the value of t used, motivating ourchoice of L0 in the application our method.",
  "B.4.2On Representation Dependence in Diffusion models": "When considering Le Lan & Dinh (2021)s results pertaining to representation dependence in the contextof diffusion models, we arrive at the interesting question as to whether the choice of representation shouldbe considered to affect the underlying distribution of the forward process q. Clearly the value we use inour method, L0 = p(x0|x1), is representation-dependent. In the strict sense, the values Lt for t > 0 arentrepresentation dependent, unless representation dependence is also considered to affect q, in which case thisbecomes more ambiguous. In figure 16 we report the negative result that the values of Lt for t > 0 also followthe pattern from Nalisnick et al. (2019a), whereby structured OOD data has higher values for Lt. We deferfurther debate on this issue to future work.",
  "B.4.3Ablation study on the value of t used for anomaly detection with diffusion models": "In this section, we evaluate using different values of t for the likelihood proxy l(x) = Lt1(x) which we use asinput for our anomaly detection method and typicality Nalisnick et al. (2019a). We summarise our results in by plotting the average AUROC acheived for each method across all 20 dataset pairings. In table 3we provide more granular results with the AUROC for each pairing individually. We note the intuitive resultthat the performance of our method gradually decays as t increases, corresponding to more noise being addedto the sample fed into the network. Overall, the average performance of our method at t = 1 is higher than",
  "B.5Using Fishers method in the place of density estimation": "In this section, we briefly investigate the use of Fishers method Fisher (1938) to compute the final anomalyscore when using the gradient L2-norm statistics f which we define in 3.4. Specifically, if we modify ourmethod by defining q(x) = min((f (x)), 1 (f (x)) to be the -th p-value from a two-tailed z-test andour final anomaly score as:",
  "CSupervised gradient-based methodology for classifiers": "For completeness, we discuss classifier based OOD detection methods using the gradient, noting that thesemethods are given label-information at train time and our representation-invariance result does not directlytranslate over to this paradigm. In order to compute gradients without a target label, This approach is hence supervised with respect toin-distribution and OOD labels which it requires. Liang et al. (2018) propose a method called ODIN whichuses the gradient with respect to the data: They backpropagate gradients to the input data to see how muchan input perturbation can change the softmax output of a classifier, following the intuition that OOD inputsmay be more sensitive and prone to a larger variation in the output distribution. Igoe et al. (2022) arecritical of the use of classifier gradients, instead advocating that most information can be recovered from thelayer representations. Behpour et al. (2023) propose projection of the gradient onto the space generated byin-distribution gradients, motivated as in Kwon et al. (2020) by the informativity of the gradient angle forOOD detection.",
  "DCode, models": "Our Glow implementation derives from a repository replicatingthe one used in Nalisnick et al. (2019a), with the only difference being we use a batch size of 64 in trainingrather than 512. See for samples from our models. Our diffusion model implementation derives from a PyTorch transcription at of that described in Ho et al. (2020). We train using Adamwith a learning rate of 3e4 for 10 epochs. Our model has T = 1000 timesteps which are uniformly sampledfrom in training and the U-Net backbone has dimension multiplicities of (1, 2, 4, 8). See Fig 19 for samplesfrom our models."
}