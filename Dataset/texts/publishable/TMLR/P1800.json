{
  "Abstract": "Dataset distillation or condensation aims to generate a smaller but representative subsetfrom a large dataset, which allows a model to be trained more efficiently, meanwhile eval-uating on the original testing data distribution to achieve decent performance. Previousdecoupled methods like SRe2L simply use a unified gradient update scheme for synthesizingdata from Gaussian noise, while, we notice that the initial several update iterations willdetermine the final outline of synthesis, thus an improper gradient update strategy maydramatically affect the final generation quality. To address this, we introduce a simple yeteffective global-to-local gradient refinement approach enabled by curriculum data augmen-tation (CDA) during data synthesis. The proposed framework achieves the current publishedhighest accuracy on both large-scale ImageNet-1K and 21K with 63.2% under IPC (ImagesPer Class) 50 and 36.1% under IPC 20, using a regular input resolution of 224224 withfaster convergence speed and less synthetic time.The proposed model outperforms thecurrent state-of-the-art methods like SRe2L, TESLA, and MTT by more than 4% Top-1accuracy on ImageNet-1K/21K and for the first time, reduces the gap to its full-data train-ing counterparts to less than absolute 15%. Moreover, this work represents the inauguralsuccess in dataset distillation on the larger-scale ImageNet-21K dataset under the standard224224 resolution. Our code and distilled ImageNet-21K dataset of 20 IPC, 2K recoverybudget are available at",
  ": ImageNet-1K comparison with SRe2L": "Dataset distillation or condensation (Wang et al.,2018) has attracted considerable attention acrossvarious fields of computer vision (Cazenavette et al.,2022b; Cui et al., 2023; Yin et al., 2023) and naturallanguage processing (Sucholutsky & Schonlau, 2021;Maekawa et al., 2023). This task aims to optimizethe process of condensing a massive dataset into asmaller, yet representative subset, preserving the es-sential features and characteristics that would allowa model to learn from scratch as effectively from thedistilled dataset as it would from the original large",
  "Published in Transactions on Machine Learning Research (11/2024)": "Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjrn Ommer. High-resolutionimage synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computervision and pattern recognition, pp. 1068410695, 2022. Ahmad Sajedi, Samir Khaki, Ehsan Amjadian, Lucy Z. Liu, Yuri A. Lawryshyn, and Konstantinos N. Pla-taniotis. Datadam: Efficient dataset distillation with attention matching. In Proceedings of the IEEE/CVFInternational Conference on Computer Vision (ICCV), pp. 1709717107, October 2023.",
  ": Motivation of our work. The left columnis the synthesized images after a few gradient up-date iterations from Gaussian noise. Middle and rightcolumns are intermediate and final synthesized images": "In this study, we extend our focus even beyondthe ImageNet-1K dataset, venturing into the un-charted territories of the full ImageNet-21K (Denget al., 2009; Ridnik et al., 2021) at a conven-tional resolution of 224224.This marks a pio-neering effort in handling such a vast dataset fordataset distillation task.Our approach harnessesa straightforward yet effective global-to-local learn-ing framework. We meticulously address each as-pect and craft a robust strategy to effectively trainon the complete ImageNet-21K, ensuring compre-hensive knowledge is captured. Specifically, follow-ing a prior study (Yin et al., 2023), our approachinitially trains a model to encapsulate knowledgefrom the original datasets within its dense parame-ters. However, we introduce a refined training recipethat surpasses the results of Ridnik et al. (2021) onImageNet-21K. During the data recovery/synthesisphase, we employ a strategic learning scheme wherepartial image crops are sequentially updated basedon the difficulty of regions: transitioning either fromsimple to difficult, or vice versa. This progression ismodulated by adjusting the lower and upper boundsof the RandomReiszedCrop data augmentation throughout varying training iterations. Remarkably, we ob-serve that this straightforward learning approach substantially improves the quality of synthesized data. Inthis paper, we delve into three learning paradigms for data synthesis linked to the curriculum learning frame-work. The first is the standard curriculum learning, followed by its alternative approach, reverse curriculumlearning. Lastly, we also consider the basic and previously employed method of constant learning.Motivation and Intuition. We aim to maximize the global informativeness of the synthetic data. BothSRe2L (Yin et al., 2023) and our proposed approach utilize local mini-batch datas mean and variancestatistics to match the global statistics of the entire original dataset, synthesizing data by applying gradientupdates directly to the image. The impact of such a strategy is that the initial few iterations set the stagefor the global structure of the ultimately generated image, as shown in . Building upon the insightsderived from the analysis, we can leverage the global-to-local gradient refinement scheme for more expressivesynthesized data, in contrast, SRe2L does not capitalize on this characteristic. Specifically, our proposedapproach exploits this by initially employing large crops to capture a more accurate and complete outline of",
  "HardestEasiest": "[0.08, 0.20][0.08, 0.40][0.08, 0.60][0.08, 0.80][0.08, 1.00][0.20, 1.00][0.40, 1.00][0.60, 1.00][0.80, 1.00] : Illustration of crop distribution from different lower and upper bounds in RandomResizedCrop.The first row is the central points of bounding boxes from different sampling scale hyperparameters. Thesecond and last rows correspond to 30 and 10 boxes of the crop distributions. In each row, from left to right,the difficulty of crop distribution is decreasing. objects, for building a better foundation. As the process progresses, it incrementally reduces the crop sizeto enhance the finer, local details of the object, significantly elevating the quality of the synthesized data.Global-to-local via Curriculum Sampling. RandomResizedCrop randomly crops the image to a certainarea and then resizes it back to the pre-defined size, ensuring that the model is exposed to different regionsand scales of the original image during training. As illustrated in , the difficulty level of the croppedregion can be controlled by specifying the lower and upper bounds for the area ratio of the crop. This canbe used to ensure that certain portions of the image (small details or larger context) are present in thecropped region. If we aim to make the learning process more challenging, reduce the minimum crop ratio.This way, the model will often see only small portions of the image and will have to learn from those limitedcontexts. If we want the model to see a larger context more frequently, increase the minimum crop ratio. Inthis paper, we perform a comprehensive study on how the gradual difficulty changes by sampling strategyinfluence the optimization of data generation and the quality of synthetic data for dataset distillation. Ourproposed curriculum data augmentation (CDA) is a heuristic and intuitive approach to simulate a global-to-local learning procedure. Moreover, it is highly effective on large-scale datasets like ImageNet-1K and 21K,achieving state-of-the-art performance on dataset distillation.The Significance of Large-scale Dataset Condensation. Large models trained on large-scale datasetsconsistently outperform smaller models and those trained on limited data (Dosovitskiy et al., 2020; Dehghaniet al., 2023; OpenAI, 2023). Their ability to capture intricate patterns and understand nuanced contextualinformation makes them exceptionally effective across a wide range of tasks and domains. These modelsplay a crucial role in solving complex industrial challenges and accelerating the development of AI-drivenproducts and services, thereby contributing to economic growth and innovation. Therefore, adapting datasetcondensation or distillation methods for large-scale data scenarios is vital to unlocking their full potential inboth academic and industrial applications.We conduct extensive experiments on the CIFAR, Tiny-ImageNet, ImageNet-1K, and ImageNet-21Kdatasets.Employing a resolution of 224224 and IPC 50 on ImageNet-1K, the proposed approach at-tains an impressive accuracy of 63.2%, surpassing all prior state-of-the-art methods by substantial margins.As illustrated in , our proposed CDA outperforms SRe2L by 46% across different architectures under50 IPC, on both 1K and 4K recovery budgets. When tested on ImageNet-21K with IPC 20, our methodachieves a top-1 accuracy of 35.3%, which is closely competitive, exhibiting only a minimal gap comparedto the model pre-trained with full data, at 44.5%, while using 50 fewer training samples.",
  "Related Work": "Dataset condensation or distillation strives to form a compact, synthetic dataset, retaining crucial informa-tion from the original large-scale dataset. This approach facilitates easier handling, reduces training time,and aims for performance comparable to using the full dataset. Prior solutions typically fall under four cat-egories: Meta-Model Matching optimizes for model transferability on distilled data, with an outer-loop forsynthetic data updates, and an inner-loop for network training, such as DD (Wang et al., 2020), KIP (Nguyenet al., 2021), RFAD (Loo et al., 2022), FRePo (Zhou et al., 2022), LinBa (Deng & Russakovsky, 2022), andMDC (He et al., 2024); Gradient Matching performs a one-step distance matching between models, suchas DC (Zhao et al., 2020), DSA (Zhao & Bilen, 2021), DCC (Lee et al., 2022), IDC (Kim et al., 2022b),and MP (Zhou et al., 2024a); Distribution Matching directly matches the distribution of original and syn-thetic data with a single-level optimization, such as DM (Zhao & Bilen, 2023), CAFE (Wang et al., 2022),HaBa (Liu et al., 2022a), KFS (Lee et al., 2022), DataDAM (Sajedi et al., 2023), FreD Shin et al. (2024),and GUARD (Xue et al., 2024); Trajectory Matching matches the weight trajectories of models trained onoriginal and synthetic data in multiple steps, methods include MTT (Cazenavette et al., 2022b), TESLA (Cuiet al., 2023), APM (Chen et al., 2023), and DATM (Guo et al., 2024). Moreover, there are some recent methods out of these categories that have further improved the existingdataset distillation. SeqMatch (Du et al., 2023) reorganizes the synthesized dataset during the distillationand evaluation phases to extract both low-level and high-level features from the real dataset, which canbe integrated into existing dataset distillation methods. Deep Generative Prior (Cazenavette et al., 2023)utilizes the learned prior from the pre-trained deep generative models to synthesize the distilled images.RDED (Sun et al., 2024) proposes a non-optimization method to concatenate multiple cropped realisticpatches from the original data to compose the distilled dataset.D3M (Abbasi et al., 2024) condensesan entire category of images into a single textual prompt of latent diffusion models. SC-DD (Zhou et al.,2024b) proposes a self-supervised paradigm by applying the self-supervised pre-trained backbones for datasetdistillation. EDC (Shao et al., 2024b) explores a comprehensive design space that includes multiple specific,effective strategies like soft category-aware matching and learning rate schedule to establishe a benchmarkfor both small and large-scale dataset distillation. Ameliorate Bias (Cui et al., 2024) studies the impactof bias within the original dataset on the performance of dataset condensation. It introduces a simple yeteffective approach based on a sample reweighting scheme that utilizes kernel density estimation. SRe2L (Yin et al., 2023) is the first and mainstream framework to distill large-scale datasets, such asImageNet-1K, and achieve significant performance. Thus, we consider it as our closest baseline. More specif-ically, SRe2L proposes a decoupling framework to avoid the bilevel optimization of model and synthesisduring distillation, which consists of three stages of squeezing, recovering, and relabeling. In the first squeez-ing stage, a model is trained on the original dataset and serves as a frozen pre-train model in the followingtwo stages. During the recovering stage, the distilled images are synthesized with the knowledge recoveredfrom the pre-train model. At the last relabeling stage, the soft labels corresponding to synthetic imagesare generated and saved by leveraging the pre-train model. Recently, distilling on large-scale datasets hasreceived significant attention in the community, and many works have been proposed, including (Sun et al.,2023; Liu et al., 2023; Chen et al., 2023; Shao et al., 2024a; Zhou et al., 2024a; Wu et al., 2024; Abbasiet al., 2024; Zhou et al., 2024b; Shao et al., 2024b; Xue et al., 2024; Qin et al., 2024; Gu et al., 2023; Maet al., 2024; Shang et al., 2024). Theses recent methods represent a comprehensive study and literatureon framework design space (Shao et al., 2024b) and adversarial robustness benchmarks (Wu et al., 2024)in dataset distillation. They are substantially different from our input-optimization-based approach. Addi-tionally, GUARD (Xue et al., 2024) incorporates curvature regularization to embed adversarial robustness,focusing on a different objective than our CDA.",
  "Preliminary: Dataset Distillation": "The goal of dataset distillation is to derive a concise synthetic dataset that maintains a significant proportionof the information contained in the original, much larger dataset. Suppose there is a large labeled datasetDo =(x1, y1) , . . . ,x|Do|, y|Do|, our target is to formulate a compact distilled dataset, represented as",
  "LDd()=E(x,y)Dd(Dd(x), y)(2)": "where is the regular loss function such as the soft cross-entropy, and Dd is model. The primary objective ofthe dataset distillation task is to generate synthetic data aimed at attaining a specific or minimal performancedisparity on the original validation data when the same models are trained on the synthetic data and theoriginal dataset, respectively. Thus, we aim to optimize the synthetic data Dd by:",
  "Dataset Distillation on Large-scale Datasets": "Currently, the prevailing majority of research studies within dataset distillation mainly employ datasets of ascale up to ImageNet-1K (Cazenavette et al., 2022b; Cui et al., 2023; Yin et al., 2023) as their benchmarkingstandards. In this work, we are the pioneer in showing how to construct a strong baseline on ImageNet-21K (the approach is equivalently applicable to ImageNet-1K) by incorporating insights presented in recentstudies, complemented by conventional optimization techniques. Our proposed baseline is demonstrated toachieve state-of-the-art performance over prior counterparts. We believe this provides substantial significancetowards understanding the true impact of proposed methodologies on dataset distillation task and towardsassessing the true gap with full original data training. We further propose a curriculum training paradigm toachieve a more informative representation of synthetic data. Following prior work in dataset distillation (Yinet al., 2023), we focus on the decoupled training framework, Squeeze-Recover-Relabel, to save computationand memory consumption on large-scale ImageNet-21K, the procedures are listed below:Squeeze: Building A Strong Pre-trained Model on ImagNet-21K. To obtain a squeezing model,we use a relatively large label smooth of 0.2 together with Cutout (DeVries & Taylor, 2017) and RandAug-ment (Cubuk et al., 2020), as shown in Appendix B.4. This recipe helps achieve 2% improvement over thedefault training (Ridnik et al., 2021) on ImageNet-21K, as provided in . 1Therefore, to ensure a fair and straightforward comparison, these baseline results in our experimental section have beentaken directly from the best evaluation performance reported by their original papers.",
  "Global-to-local Gradient Update via Curriculum": "In SRe2L (Yin et al., 2023) approach, the key of data synthesis revolves around utilizing the gradientinformation emanating from both the semantic class and the predictions of the pre-trained squeezing model,paired with BN distribution matching. Let (x, y) be an example x for optimization and its correspondingone-hot label y for the pre-trained squeezing model. Throughout the synthesis process, the squeezing modelis frozen to recover the encoded information and ensure consistency and reliability in the generated data.Let T (x) be the target training distribution from which the data synthesis process should ultimately learna function of desired trajectories, where T is a data transformation function to augment input samples tovarious levels of difficulties. Following Bengio et al. (2009), a weight 0 Ws(x) 1 is defined and appliedto example x at stage s in the curriculum sequence. The training distribution Ds(x) is:",
  "By integrating curriculum learning within the data synthesis phase, this procedure can be defined as:": "Definition 1 (Curriculum Data Synthesis). In the data synthesis optimization, the corresponding sequenceof distributions D(x) will be a curriculum if there is an increment in the entropy of these distributions,i.e., the difficulty of the transformed input samples escalates and becomes increasingly challenging for thepre-trained model to predict as the training progresses.",
  "Opt(s)Optimizer at step s": ":Illustration of global-to-localdata synthesis. This figure shows our spe-cific curriculum procedure in data synthe-sis to provide a comprehensive overviewof our dataset distillation framework. Itstarts with a large area (single bounding-box in each step) to optimize the image,building a better initialization, and thengradually narrows down the image areaof learning process so that it can focuson more detailed areas. Thus, the key for our curriculum data synthesis becomes howto design T (x) across different training iterations. The follow-ing section discusses several strategies to construct this in thecurriculum scheme.Baseline:Constant Learning (CTL). This is the regulartraining method where all training examples are typically treatedequally.Each sample from the training dataset has an equalchance of being transformed in a given batch, assuming no diffi-culty imbalance or biases across different training iterations.CTL is straightforward to implement since we do not have torank or organize examples based on difficulty. In practice, we useRandomResizedCrop to crop a small region via current crop ratiorandomly sampled from a given interval [min_crop, max_crop]and then resize the cropped image to its original size, formulatedas follows: xT RandomResizedCrop(xs, min_crop = l, max_crop = u)(6)where l and u are the constant lower and upper bounds of cropscale.Curriculum Learning (CL). As shown in Algorithm 1, in ourCL, data samples are organized based on their difficulty. Thedifficulty level of the cropped region can be managed by definingthe lower and upper scopes for the area ratio of the crop. This en-ables the assurance that specific crops of the image (small detailsor broader context) are included in the cropped region. For thedifficulty adjustment, the rate at which more difficult examplesare introduced and the criteria used to define difficulty are adjusted dynamically as predetermined using thefollowing schedulers. Distillation progress (%) 0.08 Crop ratio",
  "Constant": "0milestone100 Distillation progress (%) StepLinearCosine u l : Crop ratio schedulers of prior CTL solu-tion (left) and our Global-to-local (right) enabled bycurriculum. The colored regions depict the randomsampling intervals for the crop ratio value in eachiteration under different schedulers. Step. Step scheduler reduces the minimal scale by afactor for every fixed or specified number of iterations,as shown in right.Linear.Linear scheduler starts with a high initialvalue and decreases it linearly by a factor to a min-imum value over the whole training.Cosine. Cosine scheduler modulates the distributionaccording to the cosine function of the current itera-tion number, yielding a smoother and more gradualadjustment compared to step-based methods.As shown in , the factor distribution managesthe difficulty level of crops with adjustable u and lfor CTL and milestone for CL. Data Synthesis by Recovering and Relabeling.After receiving the transformed input xT , we update it by aligning between the final classification label andintermediate Batch Normalization (BN) statistics, i.e., mean and variance from the original data. This stageforces the synthesized images to capture a shape of the original image distribution. The learning goal forthis stage can be formulated as follows:",
  "xT = arg min ( (xT ) , y) + Rreg(7)": "where is the pre-trained squeezing model and will be frozen in this stage. During synthesis, only the inputcrop area will be updated by the gradient from the objective. The entire training procedure is illustrated in. After synthesizing the data, we follow the relabeling process in SRe2L to generate soft labels using",
  "2k (x) BNRVk2(8)": "where k is the index of BN layer, k (x) and 2k (x) are the channel-wise mean and variance in currentbatch data. BNRMkand BNRVkare mean and variance in the pre-trained model at k-th BN layer, which areglobally counted.Advantages of Global-to-local Synthesis. The proposed CDA enjoys several advantages: (1) Stabilizedtraining: Curriculum synthesis can provide a more stable training process as it reduces drastic loss fluctu-ations that can occur when the learning procedure encounters a challenging sample early on. (2) Bettergeneralization: By gradually increasing the difficulty, the synthetic data can potentially achieve better gener-alization on diverse model architectures in post-training. It reduces the chance of the synthesis getting stuckin poor local minima early in the training process. (3) Avoid overfitting: By ensuring that the synthetic datais well-tuned on simpler examples before encountering outliers or more challenging data, there is a potentialto reduce overfitting. Specifically, better generalization here refers to the ability of models trained on thedistilled datasets to perform well across a wider range of evaluation scenarios. However, avoiding overfittingparticularly refers to our curriculum strategy during the distillation process, where we use a flexible regionupdate in each iteration to prevent overfitting that could occur with a fixed region update.",
  "Datasets and Implementation Details": "We verify the effectiveness of our approach on small-scale CIFAR-100 and various ImageNet scale datasets,including Tiny-ImageNet (Le & Yang, 2015), ImageNet-1K (Deng et al., 2009), and ImageNet-21K (Ridniket al., 2021). For evaluation, we train models from scratch on synthetic distilled datasets and report theTop-1 accuracy on real validation datasets. Default lower and upper bounds of crop scales l and u are 0.08and 1.0, respectively. The decay is 0.92. In Curriculum Learning (CL) settings, the actual lower boundis dynamically adjusted to control difficulty, whereas the upper bound is fixed to the default value of 1.0 to",
  "Tiny-ImageNet": "Results on the Tiny-ImageNet dataset are detailed in the second group of and the first group of. Our CDA outperforms all baselines except DATM under 10 IPC. Compared to SRe2L, our CDAachieves average improvements of 7.7% and 3.4% under IPC 50 and IPC 100 settings across ResNet-{18,50, 101} validation models, respectively. Importantly, CDA stands as the inaugural approach to diminish theTop-1 accuracy performance disparity to less than 10% between the distilled dataset employing IPC 100 andthe full Tiny-ImageNet, signifying a breakthrough on this dataset.",
  "Easy (l = , u = u (1.0))44.9047.8846.3445.3543.4841.30Hard (l = l (0.08), u = )22.9934.7542.7644.6145.7644.90": "Constant Learning (CTL). We leverage a ResNet-18 and employ synthesized data with 1K recoveryiterations. As observed in , the results for exceedingly straightforward or challenging scenarios fallbelow the reproduced SRe2L baseline accuracy of 44.90%, especially when 0.8 in easy and 0.4 inhard type. Thus, the results presented in suggest that adopting a larger cropped range assists incircumventing extreme scenarios, whether easy or hard, culminating in enhanced performance. A noteworthy",
  "ImageNet-21K": "Pre-training Results. of the Appendix presents the accuracy for ResNet-18 and ResNet-50 onImageNet-21K-P, considering varying initial weight configurations. Models pre-trained by us and initial-ized with ImageNet-1K weight exhibit commendable accuracy, showing a 2.0% improvement, while modelsinitialized randomly achieve marginally superior accuracy. We utilize these pre-trained models to recoverImageNet-21K data and to assign labels to the synthetic images generated. An intriguing observation is theheightened difficulty in data recovering from pre-trained models that are initialized randomly compared tothose initialized with ImageNet-1K weight. Thus, our experiments employ CDA specifically on pre-trainedmodels that are initialized with ImageNet-1K weight.Validation Results.As illustrated in and the final group of , we perform validationexperiments on the distilled ImageNet-21K employing IPC 10 and 20. This yields an extreme compressionratio of 100 and 50. When applying IPC 10, i.e., the models are trained utilizing a distilled dataset thatis a mere 1% of the full dataset. Remarkably, validation accuracy surpasses 20% and 30% on ResNet-18 andResNet-{50, 101}, respectively. Compared to reproduced SRe2L on ImageNet-21K, our approach attainsan elevation of 5.3% on average under IPC 10/20. This not only highlights the efficacy of our approachin maintaining dataset essence despite high compression but also showcases the potential advancements inaccuracy over existing methods.",
  "global79.3481.2582.16cropped87.4882.4472.73": "Impact of Curriculum. To study the curriculums advantage onsynthetic image characteristics, we evaluate the Top-1 accuracy onCDA, SRe2L and real ImageNet-1K training set, using a mean of ran-dom 10-crop and global images. We employ PyTorchs pre-trainedMobileNet-V2 to classify these images. As shown in , CDA im-ages closely resemble real ImageNet images in prediction accuracies,better than SRe2L. Consequently, curriculum data augmentationimproves global image prediction and reduces bias and overfittingpost-training on simpler, cropped images of SRe2L.",
  ": Synthetic ImageNet-21K images (Plant)": "Visualization and Discussion. providesa comparative visualization of the gradient syntheticimages at recovery steps of {100, 500, 1K, 2K} toillustrate the differences between SRe2L and CDAwithin the dataset distillation process. SRe2L im-ages in the upper line exhibit a significant amountof noise, indicating a slow recovery progression inthe early recovery stage. On the contrary, due tothe mostly entire image optimization in the earlystage, CDA images in the lower line can establish thelayout of the entire image and reduce noise rapidly.And the final synthetic images contain more visualinformation directly related to the target class Plant. Therefore, the comparison highlights CDAs abilityto synthesize images with enhanced visual coherence to the target class, offering a more efficient recoveryprocess. More visualizations are provided in Appendix D. Synthesis Cost. We highlight that there is no additional synthesis cost incurred in our CDA to SRe2L (Yinet al., 2023) under the same recovery iteration setting. Specifically, for ImageNet-1K, it takes about 29 hoursto generate the distilled ImageNet-1K with 50 IPC on a single A100 (40G) GPU and the peak GPU memoryutilization is 6.7GB. For ImageNet-21K, it takes 11 hours to generate ImageNet-21K images per IPC on asingle RTX 4090 GPU and the peak GPU memory utilization is 15GB. In our experiment, it takes about 55hours to generate the entire distilled ImageNet-21K with 20 IPC on 4 RTX 4090 GPUs in total. Selectingrepresentative real images for input data initialization instead of the Gaussian noise initialization will be aneffective way to further accelerate image distillation and reduce synthesis costs. It can potentially reduce thenumber of recovery iterations required to achieve high-quality distilled data and this approach could lead tofaster convergence and lower synthesis costs. Nevertheless, we present our detailed training time on severaltested models, as shown in .",
  "Conclusion": "We presented a new framework focused on global-to-local gradient refinement through curriculum data syn-thesis for large-scale dataset distillation. Our approach involves a practical paradigm with detailed pertainingfor compressing knowledge, data synthesis for recovery, and post-training recipes. The proposed approachenables the distillation of ImageNet-21K to 50 smaller while maintaining competitive accuracy levels. Inregular benchmarks, such as ImageNet-1K and CIFAR-100, our approach also demonstrated superior per-formance, surpassing prior state-of-the-art methods by substantial margins. We further show the capabilityof our synthetic data on downstream tasks of cross-model generalization and continual learning. With therecent substantial growth in the size of both models and datasets, the critical need for dataset distillation onlarge-scale datasets and models has become increasingly prominent and urgent. Our future work will focuson distilling more modalities like language and speech. Limitations. Our proposed approach is robust to generate informative images, while we also clarify that thequality of our generated data is not comparable to the image quality achieved by state-of-the-art generativemodels on large-scale datasets. This difference is expected, given the distinct goals of dataset distillationversus generative models. Generative models aim to synthesize highly realistic images with detailed features,whereas dataset distillation methods focus on producing images that capture the most representative infor-mation possible for efficient learning in downstream tasks. Realism is not the primary objective in datasetdistillation.",
  "Justin Cui, Ruochen Wang, Yuanhao Xiong, and Cho-Jui Hsieh. Ameliorate spurious correlations in datasetcondensation. In Forty-first International Conference on Machine Learning, 2024": "Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin Gilmer, An-dreas Peter Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin, et al.Scaling visiontransformers to 22 billion parameters. In International Conference on Machine Learning, pp. 74807512.PMLR, 2023. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchicalimage database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248255. Ieee,2009.",
  "Yang He, Lingao Xiao, Joey Tianyi Zhou, and Ivor Tsang. Multisize dataset condensation. ICLR, 2024": "Elad Hoffer, Itay Hubara, and Daniel Soudry. Train longer, generalize better: closing the generalization gapin large batch training of neural networks. Advances in neural information processing systems, 30, 2017. Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolu-tional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.47004708, 2017. Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak PeterTang. On large-batch training for deep learning: Generalization gap and sharp minima. arXiv preprintarXiv:1609.04836, 2016. Jang-Hyun Kim, Jinuk Kim, Seong Joon Oh, Sangdoo Yun, Hwanjun Song, Joonhyun Jeong, Jung-Woo Ha,and Hyun Oh Song. Dataset condensation via efficient synthetic-data parameterization. In InternationalConference on Machine Learning, pp. 1110211118. PMLR, 2022a. Jang-Hyun Kim, Jinuk Kim, Seong Joon Oh, Sangdoo Yun, Hwanjun Song, Joonhyun Jeong, Jung-Woo Ha,and Hyun Oh Song. Dataset condensation via efficient synthetic-data parameterization. In Proceedings ofthe 39th International Conference on Machine Learning, 2022b.",
  "Peng Sun, Bei Shi, Daiwei Yu, and Tao Lin. On the diversity and realism of distilled dataset: An efficientdataset distillation paradigm. arXiv preprint arXiv:2312.03526, 2023": "Peng Sun, Bei Shi, Daiwei Yu, and Tao Lin. On the diversity and realism of distilled dataset: An efficientdataset distillation paradigm.In Proceedings of the IEEE/CVF Conference on Computer Vision andPattern Recognition (CVPR), 2024. Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv Jgou.Training data-efficient image transformers & distillation through attention. In International conferenceon machine learning, pp. 1034710357. PMLR, 2021. Kai Wang, Bo Zhao, Xiangyu Peng, Zheng Zhu, Shuo Yang, Shuo Wang, Guan Huang, Hakan Bilen,Xinchao Wang, and Yang You. Cafe: Learning to condense dataset by aligning features. In Proceedingsof the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022.",
  "BN0.01optimizerAdambase learning rate0.25momentum1, 2 = 0.5, 0.9batch size100learning rate schedulecosine decayrecovery iteration1,000augmentationRandomResizedCrop": "Due to the low resolution of CIFAR images, the default lower bound l needs to be raised from 0.08 (ImageNetsetting) to a higher reasonable value in order to avoid the training inefficiency caused by extremely smallcropped areas with little information. Thus, we conducted the ablation to select the optimal value for thedefault lower bound l in RandomResizedCrop operations in . We choose 0.4 as the default lowerbound l in Algorithm 1 to exhibit the best distillation performance on CIFAR-100. We adopt a small batchsize value of 8 and extend the training budgets in the following validation stage, which aligns with the strongtraining recipe on inadequate datasets.",
  "BN1.0optimizerAdambase learning rate0.1momentum1, 2 = 0.5, 0.9batch size100learning rate schedulecosine decayrecovery iteration4,000augmentationRandomResizedCrop": "Small IPC Setting Comparison. presents the result comparison among our CDA, DM (Zhao &Bilen, 2023) and MTT (Cazenavette et al., 2022b). Consider that our approach is a decoupled process ofdataset compression followed by recovery through gradient updating. It is well-suited to large-scale datasetsbut less so for small IPC values. As anticipated, there is no advantage when IPC value is extremely low, suchas IPC = 1. However, when the IPC is increased slightly, our method demonstrates considerable benefitson accuracy over other counterparts.Furthermore, we emphasize that our approach yields substantialimprovements when afforded a larger training budget, i.e., more training epochs.",
  "B.3ImageNet-1K": "Hyper-parameter Settings.We employ PyTorch off-the-shelf ResNet-18 and DenseNet-121 with theTop-1 accuracy of {69.8%, 74.4%} which are trained with the official recipe in a. And the recoverysettings are provided in c, and it is noteworthy that we tune and set distinct parameters BN andlearning rate for different recovery models in d. Then, we employ ResNet-{18, 50, 101, 152} (Heet al., 2016), DenseNet-121 (Huang et al., 2017), RegNet (Radosavovic et al., 2020), ConvNeXt (Liu et al.,2022b), and DeiT-Tiny (Touvron et al., 2021) as validation models to evaluate the cross-model generalizationon distilled ImageNet-1K dataset under the validation setting in b.",
  "Step45.4646.0846.6546.7546.8746.1345.2744.9742.4941.18Linear45.3946.3046.5946.5146.6047.1847.1347.3748.0647.78Cosine45.4145.4246.1546.9046.9347.4246.8647.3347.8048.05": "Cross-Model Generalization. To supplement the validation models on distilled ImageNet-1K in ,including more different architecture models to evaluate the cross-architecture performance. We have con-ducted validation experiments on a broad range of models, including SqueezeNet, MobileNet, EfficientNet,MNASNet, ShuffleNet, ResMLP, AlexNet, DeiT-Base, and VGG family models. These validation models areselected from a wide variety of architectures, encompassing a vast range of parameters, shown in .In the upper group of the table, the selected models are relatively small and efficient. There is a trend thatits validation performance improves as the number of model parameters increases. In the lower group, wevalidated earlier models AlexNet and VGG. These models also show a trend of performance improvementwith increasing size, but due to the simplicity of early model architectures, such as the absence of residualconnections, their performance is inferior compared to more recent models. Additionally, we evaluated ourdistilled dataset on ResMLP, which is based on MLPs, and the DeiT-Base model, which is based on trans-formers. In summary, the distilled dataset created using our CDA method demonstrates strong validationperformance across a wide range of models, considering both architectural diversity and parameter size.",
  "B.4ImageNet-21K": "Hyper-parameter Setting. ImageNet-21K-P (Ridnik et al., 2021) proposes two training recipes to trainResNet-{18, 50} models. One way is to initialize the models from well-trained ImageNet-1K weight and trainon ImageNet-21K-P for 80 epochs, another is to train models with random initialization for 140 epochs, asshown in a.The accuracy metrics on both training recipes are reported in .In ourexperiments, we utilize the pre-trained ResNet-{18, 50} models initialized by ImageNet-1K weight with theTop-1 accuracy of {38.1%, 44.2%} as recovery model. And the recovery setting is provided in c.Then, we evaluate the quality of the distilled ImageNet-21K dataset on ResNet-{18, 50, 101} validationmodels under the validation setting in b. To accelerate the ablation study on the batch size settingin , we train the validation model ResNet-18 for 140 epochs.",
  "CReverse Curriculum Learning": "Reverse Curriculum Learning (RCL). We use a reverse step scheduler in the RCL experiments, startingwith the default cropped range from l to u and transitioning at the milestone point to optimize the wholeimage, shifting from challenging to simpler optimizations.Other settings follow the recovery recipe onResNet-18 for 1K recovery iterations. shows the RCL results, a smaller step milestone indicates anearlier difficulty transition. The findings reveal that CRL does not improve the generated datasets qualitycompared to the baseline SRe2L, which has 44.90% accuracy."
}