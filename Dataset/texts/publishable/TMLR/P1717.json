{
  "Abstract": "We consider the problem of privacy protection in Reinforcement Learning (RL) algorithmsthat operate over population processes, a practical but understudied setting that includes, forexample, the control of epidemics in large populations of dynamically interacting individuals.In this setting, the RL algorithm interacts with the population over T time steps byreceiving population-level statistics as state and performing actions which can affect theentire population at each time step. An individuals data can be collected across multipleinteractions and their privacy must be protected at all times. We clarify the Bayesiansemantics of Differential Privacy (DP) in the presence of correlated data in populationprocesses through a Pufferfish Privacy analysis. We then give a meta algorithm that cantake any RL algorithm as input and make it differentially private. This is achieved by takingan approach that uses DP mechanisms to privatize the state and reward signal at each timestep before the RL algorithm receives them as input. Our main theoretical result shows thatthe value-function approximation error when applying standard RL algorithms directly tothe privatized states shrinks quickly as the population size and privacy budget increase. Thishighlights that reasonable privacy-utility trade-offs are possible for differentially private RLalgorithms in population processes. Our theoretical findings are validated by experimentsperformed on a simulated epidemic control problem over large population sizes.",
  "Introduction": "The increasing adoption of Reinforcement Learning (RL) algorithms in many practical applications suchas digital marketing, finance, and public health (Mao et al., 2020; Wang & Yu, 2021; Charpentier et al.,2020) have led to new, challenging privacy considerations for the research community. This is a particularlyimportant issue in domains like healthcare where highly sensitive personal information is routinely collectedand the use of such data in training RL algorithms must be handled carefully, in light of successful privacyattacks on RL algorithms (Pan et al., 2019). Privacy Preserving Reinforcement Learning is an active researcharea looking to address these concerns, mostly via the now widely accepted concept of Differential Privacy(DP) (Dwork et al., 2006), which confers formal plausible deniability guarantees for users whose data areused in training RL algorithms, thus offering them privacy protection. In this paper, we consider the setting where an RL agent interacts with a population of individuals andinvestigate how each individuals differential privacy can be protected. We assume interactions betweenindividuals are not visible to the RL agent but the agent receives population-level statistics at every timestep and can perform actions that affect the entire population. This class of environments, which we callpopulation processes, models settings such as the control of epidemic spread by government interventions(Kompella et al., 2020). As an individuals data is collected across multiple interactions, our goal is to ensurethat an individuals data contributions over all interactions are differentially private. To the best of our",
  "Related Work": "The earliest works to consider differential privacy in a reinforcement learning context were focused on thebandit or contextual bandit settings (Guha Thakurta & Smith, 2013; Mishra & Thakurta, 2015; Tossou& Dimitrakakis, 2016; 2017; Shariff & Sheffet, 2018; Sajed & Sheffet, 2019; Zheng et al., 2020; Dubey &Pentland, 2020; Ren et al., 2020; Chowdhury & Zhou, 2022b; Azize & Basu, 2022). A key negative resultis in Shariff & Sheffet (2018), who show that sublinear regret is not possible under differential privacy incontextual bandits, a result which also holds in the reinforcement learning setting. Differentially private reinforcement learning beyond the bandit setting has been primarily considered in apersonalization context. In Balle et al. (2016), the authors study policy evaluation under the setting wherethe RL algorithm receives a set of trajectories, and a neighbouring dataset is one in which a single trajectorydiffers. In a regret minimization context, there is a body of work on designing RL algorithms to satisfyeither joint differential privacy (Vietri et al., 2020; Luyo et al., 2021; Chowdhury & Zhou, 2022a; Ngo et al.,2022; Zhou, 2022; Qiao & Wang, 2023) or local differential privacy (Garcelon et al., 2020; Luyo et al., 2021;Chowdhury & Zhou, 2022a; Liao et al., 2023). In all of these works, the RL algorithm is framed as interactingwith users in trajectories or episodes, with each trajectory representing multiple interactions with a single user.A neighbouring dataset is then defined with respect to a neighbouring trajectory. Such DP-RL approachescannot be easily adapted for privacy protection in population processes, where (i) each interaction is withan entire population (rather than a single individual); (ii) a specific individual is typically not sampled inconsecutive time steps so there is not a corresponding notion of a trajectory; and (iii) an individuals datacould actually be present across all time steps, a low-probability event that we nevertheless have to handle. In Wang & Hegde (2019), the authors analyse the performance of deep Q-learning (Mnih et al., 2015) underdifferential privacy guarantees specified with respect to neighbouring reward functions. This notion of privacymakes natural sense when the reward function is viewed as an individuals private preferences but is alsoinapplicable to our setting as it does not consider the privacy of the state. In relation to our application domain and experimental results, the control of epidemics is a topical subjectgiven the prevalence of COVID-19 in recent years and many different practical approaches have been developed(Arango & Pelov, 2020; Charpentier et al., 2020; Colas et al., 2020; Berestizshevsky et al., 2021; Kompellaet al., 2020; Chen et al., 2022). Whilst the preservation of individual privacy has been considered in thecontext of public release of population-level epidemic statistics (Dyda et al., 2021; Li et al., 2023), we are notaware of other work that achieves individual privacy preservation in the modelling and control of epidemicsusing reinforcement learning.",
  ". Assuming the answer to the first question is positive, are good privacy-utility trade-offs possible fordifferentially private RL algorithms in population processes?": "We answer the first question in two parts. We first show through a concrete example (see Example 4) thathighly correlated data is possible in population processes and that some sensitive information that one maynaturally wish to protect, like a persons infection status during an epidemic, cannot always be protected.We then give, using the general Pufferfish Privacy framework (Kifer & Machanavajjhala, 2014), a precisesemantic definition of the secrets around individual participation in samples that can actually be protectedand show its equivalence to k-fold adaptive composition of DP mechanisms (see Lemma 1). Building on that,",
  "we then describe a family of DP-RL algorithms for population processes and show how differential privacyand correlated data are somewhat orthogonal issues in our set up": "On the second question, we note our DP-RL solution is a meta algorithm that takes any RL algorithm(whether online/offline or value-based/policy-based) as a black box and make it differentially private. Whilstsuch a modular solution is desirable for its simplicity and generality, the trade-off is a more difficult controlproblem because the underlying environment becomes partially observable to the RL agent. Standard methodsfor dealing with partial observability typically require expensive state-estimation techniques or sampling basedapproximations (Monahan, 1982; Shani et al., 2013; Kurniawati, 2022). Instead of using these methods, weanalyze the performance of standard RL algorithms as the sampled population size N and privacy budget increases. Under some assumptions, we obtain the following bound on the approximation error under privacy",
  ",(1)": "where Q is the optimal value function for a given (arbitrary) population process M, Q is the optimal valuefunction for the privatized form of M, and K is the dimension of the state space. (The precise statementis Theorem 3 in 4.2.) Note that the RHS of (1) goes to 0 as N and increases, and that such a resultis not possible in the personalization setting (Shariff & Sheffet, 2018). Whilst this result does not imply afinite-time sample complexity guarantee, we validate empirically on an epidemic control problem simulatedover large graphs that our DP-RL algorithm behaves well as the population size and privacy budget increases,as suggested by (1). Our results demonstrate that reasonable privacy-utility trade-offs are certainly possiblefor differentially private RL algorithms in population processes.",
  "Reinforcement Learning and Markov Decision Processes": "We consider a time-homogeneous Markov Decision Process (MDP) M = (S, A, P, r, ) with state space S,action space A, transition function P : S A D(S), reward function r : S A [0, rmax], discount factor [0, 1). The notation D(S) defines the set of distributions over S. The rewards are assumed to be boundedbetween 0 and rmax R. A stationary policy is a function : S D(A) specifying a distribution over actionsbased on a given state, i.e. at ( | st). A stationary deterministic policy assigns probability 1 to a singleaction in a given state. With a slight abuse of notation, we will define a stationary deterministic policy to havethe signature : S A. Let denote the set of all stationary policies. The action-value function (Q-value)of a policy is the expected cumulative discounted reward Q(s, a) = r(s, a) + EP, [t=1 tr(st, at)], wherethe expectation is taken with respect to the transition function P and policy at each time step. The valuefunction is defined as V (s) = Ea( | s)[Q(s, a)]. The Q-value satisfies the Bellman equation given byQ(s, a) = r(s, a) + EP [V (s)]. When considering the optimal policy, define V (s) = sup V (s) and Q(s, a) = sup Q(s, a). Theoptimal action-value function satisfies the bellman optimality equation given by Q(s, a) = r(s, a) +EP [maxa Q(s, a)]. There exists an optimal stationary deterministic policy such that V (s) =V (s). The greedy policy (s) = arg maxa Q(s, a) is in fact an optimal policy. We primarily consider the case when S is finite. In this case, it is helpful to view our functions as vectors andmatrices. We use P to refer to a matrix of size (|S| |A|) |S| where P ssa is equal to P(s | s, a) and Psa is alength |S| vector denoting P( | s, a). Similarly, we can view V as a vector of length |S| and Q and r asvectors of length |S| |A|. The Bellman equation can now be expressed as",
  "Published in Transactions on Machine Learning Research (12/2024)": "simplex using the sorting method from Shalev-Shwartz et al. (2006) before finding the nearest point in thestate space. The sorting method has complexity O(K log K) and finding the nearest point in the state spacecan be done in O(K). Algorithm 4 has a few minor differences to Algorithm 1. Firstly, the algorithm takes the target cumulativeprivacy parameters as input instead of the per-step privacy parameters. The per-step privacy budget iscalculated in line 5. Lines 10-21 of can be considered the RL algorithm in Algorithm 1 expanded. Thereplay buffer and target network are written such that they are not a part of the RL algorithm itself. This isdone to make clear that the buffer and target network maintain state across all iterations. Finally, DQNreturns a value function instead of a policy. This is a minor change however as the policy used is simply theepsilon-greedy policy with respect to the returned value function. The parameters used in each experiment are listed in . The neural network used in all experimentswas a 6 layer, fully connected MLP. The learning rate () is not listed as we use the default settings of theRMSProp optimizer in PyTorch to optimize the neural network.",
  "for all D in the Hamming-1 neighbourhood of D. That is, D may differ in at most one entry from D: thereexists at most one i [n] such that Di = Di": "A standard approach to privatising a query over an input dataset is to design a mechanism M that samplesnoise from a carefully scaled distribution and add it to the true output of the query. To scale the noise levelappropriately, the sensitivity of a query is an important parameter. Definition 2 (1 sensitivity). Let f be a function f : X n U.Let d : X n X n {0, 1} bea function indicating whether two inputs are neighbours.The sensitivity of f is defined as f =supx,xX n:d(x,x)=1 f(x) f(x)1.",
  "Pufferfish Privacy": "Pufferfish privacy was introduced in Kifer & Machanavajjhala (2014) and proposes a generalization ofdifferential privacy from a Bayesian perspective. In the Pufferfish framework, privacy requirements areinstantiated through three components: (i) S, the set of secrets representing functions of the data that wewish to protect; (ii) Q S S, a set of secret pairs that need to be indistinguishable to an adversary; and(iii) , a class of distributions that can plausibly generate the data. Typically is viewed as the beliefs thatan adversary holds over how the data was generated. Pufferfish privacy is defined as follows: Definition 3 (Pufferfish Privacy). Let (S, Q, ) denote the set of secrets, secret pairs, and data generatingdistributions and let D be a random variable representing the dataset. A privacy mechanism M is said tobe (, )-Pufferfish private with respect to (S, Q, ) if for all , D , for all (si, sj) Q, and for allw Range(M), we have",
  "We now describe how the problem of controlling population processes is modelled as an MDP and also theunderlying data-generation process": "The environment E is modelled as a stochastic population process evolving over N individuals. The RL agentis denoted by A . We assume there is a trusted data curator D that collects the data from the environment.At each time step, N individuals are randomly sampled (not necessarily uniformly) and their potentiallysensitive data is collected by the data curator. We consider the case where the interactions between individualsare unknown, i.e. D has no access to the graph sequence G1:T . This models many problem domains such asthe control of epidemics where the underlying interactions between people in a population are not visible anddecisions can only be made based upon population statistics. We consider the case of histogram queries. Theagent A picks actions at each time step depending upon the received state and also computes its rewardrt = r(st, at1) as a function of the current state and previous action. In summary, the environment, agent,and data curator interact in the following manner, given an initial graph G0 and status X0 generated fromsome distribution.",
  "The state space S thus consists of any vector z K that satisfies": "i[K] zi = 1 and zi = ci/N for someci {0, 1, . . . , N}. The action space A is application-dependent. In general, we consider the setting where aselected action at A modifies the edges in the graph. For example an action could correspond to cutting alledges for a subset of nodes, emulating the effect of quarantining individuals in an epidemic control context.Thus the graph at time t depends upon the action chosen and is distributed according to P(Gt | Gt1, at).These underlying transitions on individuals states and the graph structure will induce a transition functionP : S A D(S) over the state space that implicitly captures the impact of the agents actions. An individuals data is exposed via the state histogram query st = q(Dt). The state query has sensitivityq = 2/N as using individual js data instead of individual is data can change the counts in at most twobins of the histogram. Additionally an individual could be sampled multiple times by D over T interactionsso we need to ensure that their combined data over T steps are treated in a differentially private way. Example 1 (Epidemic Control). Throughout this paper we consider the Epidemic Control problem as aconcrete instantiation of our problem setting. One particular example is the Susceptible-Exposed-Infected-Recovered-Susceptible (SEIRS) process on contact networks (Pastor-Satorras et al., 2015; Nowzari et al.,",
  ": Visualisation of the parameters that govern the transitions between states for individuals in theSEIRS process over contact networks": "Example 2 (Countering Misinformation). The spread of misinformation in online social media is one ofthe key threats to society. There is good literature on different (mis)information percolation and diffusionmodels (Del Vicario et al., 2016; Van Der Linden, 2022) that take factors like homogeneity and polarisationinto account, as well as containment strategies like running counter-information campaigns to debunk orprebunk misinformation, possibly through targeting of influencers in social networks (Budak et al., 2011;Nguyen et al., 2012; Acemoglu et al., 2023; Ariely, 2023). Designing reinforcement learning algorithms thatcan detect and control spread of misinformation in a differentially private manner is another example of ourgeneral problem setting. Example 3 (Malware Detection and Control). Malware propagation models on large-scale networks (Yuet al., 2014) and smartphones (Peng et al., 2013) have long been a subject of interest in cyber security, withmore recent work focussing on malware propagation in internet-of-things (Li et al., 2020; Yu et al., 2022;Muthukrishnan et al., 2021). Designing reinforcement learning algorithms to detect and control the spreadof malware, especially unknown malwares whose signatures can only be discerned from collecting data onpotentially sensitive device-usage behaviour, is also an example of our general problem setting.",
  "Formalizing Privacy in the Presence of Correlated Data": "In this section we clarify the precise privacy protections we will provide under differential privacy in ourproblem setting. Protecting privacy in population processes presents some unique issues that are not typicallyencountered in the standard application of differential privacy. In particular, correlations between differentindividuals data can easily arise due to the fact that individuals interact with each other at each time stepand also over multiple time steps. Whilst differential privacys guarantees are a mathematical statementthat hold regardless of the process that generated the data, the semantics of these guarantees are oftenmisinterpreted when it is applied. This leads to misalignment between what one hopes to keep private andwhat is actually kept private. We illustrate this with an example. Example 4. Consider a highly contagious but long recovery flu spreading in a tight-knit community ofN individuals who live in the same location. The dataset is Dt = (Xt,i)iLt where Lt [N ] denotes asubset of individuals sampled for their flu status Xt,i {0, 1}. As the individuals live in the same location,their interaction graph is a fully connected graph. The goal is to release the number of infected individualsqt = iLt Xt,i at time t whilst still preventing an adversary from detecting whether a particular individual,say Alice, has the flu at that point in time. If the underlying data generating process is ignored, the statistic qthas sensitivity 1 and qt = qt +Lap(1/), where Lap(1/) is the noise generated from the Laplace mechanism, isan -differentially private statistic. However, given the flus characteristics and the fully connected interactiongraph, we can say with high probability either all individuals or no individuals are infected at any time t.",
  "Thus, an adversary could guess from the value of qt which of these two scenarios is the case, thereby alsorecovering Alices flu status with high probability": "It would be natural to hope that an individuals flu status could be protected under differential privacy butExample 4 illustrates that this may not be possible if the data is correlated and the adversary has additionalinformation about the problem. Solutions like Group Differential Privacy (Dwork & Roth, 2014), which addsnoise proportional to the largest connected component in a graph, and Dependent Differential Privacy (Liuet al., 2016), which adds noise proportional to the amount of correlation in the data, exist but the amountof additional noise that is required typically destroys utility. So what can differential privacy protect inpopulation processes in general and in special scenarios like Example 4? The answer, as we shall see, is thatdifferential privacy confers plausible deniability on an individuals participation or presence in a datasetsampled from the underlying population, but not the individuals actual status, which can sometimes beinferred. Example 4 highlights the need for privacy researchers to be explicit about the data generatingprocess, the adversarys assumptions, and what information one wishes to protect. This can be done usingPufferfish privacy and this is how we will make explicit the guarantees provided by differential privacy in ourproblem setting. Given the data-generation model described in 3.1, let D1:T be the sequence of sampled datasets, whereDt = (Xt,i)iLt denote the dataset produced at time t. Given a subset S of [T], we first define the booleanvariable (i,S) that indicates whether an individual is data is present in Dt for each t S and not anywhereelse:",
  "RS:|R|1{((i,S), (i,S\\R)}": "The additional element to specify in the Pufferfish framework is the data generating processes , representingthe possible ways an attacker believes the data could have been generated. We define each to be aparameterization of the form := {E , t,1, . . . , t,N}t=1...T , where E represents the underlying stochasticpopulation process and is a distribution over the graph and status sequences, and t,i is the probability thati Lt. We assume the attacker also has a distribution over the agents policy that is integrated out in E .Thus, we have",
  "j=Lt(1 t,j).(4)": "We do not place any restriction on E , which models the attackers prior belief over the likely sequence ofinteractions between individuals in the population and the attackers knowledge about the dynamics model ofthe underlying population process. The following result states the equivalence between (, )-Pufferfish privacy with parameters (S, Q, ) and(, )-differential privacy under T-fold adaptive composition (Dwork et al., 2010; Dwork & Roth, 2014). Thefull proof is given in Appendix A. Lemma 1. A family of mechanisms F satisfies (, )-differential privacy under T-fold adaptive compositioniff every sequence of mechanisms M = (M1, . . . , MT ), with Mi F, satisfies (, )-Pufferfish privacy withparameters (S, Q, ).",
  "Differentially Private Reinforcement Learning": "Our solution for differentially private reinforcement learning is presented in Algorithm 1. It is a meta algorithmthat takes as input an MDP environment M, a reinforcement learning algorithm RL, a privacy mechanism M,and parameters (, ) and T that specify the level of privacy that should hold over T interactions between theagent and the environment. The RL algorithm can be any method that takes a transition sample (s, a, r, s)and a policy old and outputs a new policy new = RL((s, a, r, s), old). (Appendix C describes a concreteinstantiation of Algorithm 1 using DQN as the RL algorithm.) Our approach is to privatise the inputs before the RL algorithm receives them. We begin by first showingthat Algorithm 1 satisfies the privacy guarantees specified in 3.2. We then characterize the resulting controlproblem under our privacy approach before providing a utility bound.",
  "Privacy Analysis": "Algorithm 1 is constructed using differential privacy tools to satisfy (, )-differential privacy under T-foldadaptive composition. Since an individuals data is used at each time t to output a state st, we need toprivatise st and ensure that all functions that take st as input are also differentially private. Algorithm 1does this by privatising every state using an -differentially private mechanism (lines 4 and 8). For instance,the Laplace mechanism with scale parameter 2/N (since q = 2/N) would satisfy -differential privacy. Asthe state space is discrete and bounded, we also need to project the output from the Laplace mechanism backto the closest element in the state space. (See Algorithm 2 for more details.) The projection operation isguaranteed to be differentially private by Lemma 2.",
  "Lemma 2 (Post-processing (Dwork & Roth, 2014)). Let M : X n Z be an (, )-differentially privatemechanism and f : Z Y an arbitrary function. Then the composition f M is (, )-differentially private": "In Algorithm 1, the action at is selected using the current policy t with the privatized state st as input atevery time step (line 6). Once the next state st+1 is sampled, it is immediately privatized to st+1 (lines 7 and8). The agent then receives reward rt = r(st+1, at). Since the action at and received reward rt are functionsof the privatized state, they are also guaranteed private by Lemma 2. Thus, the entire transition samplereceived by the RL algorithm is -differentially private.",
  "Theorem 1. Algorithm 1 satisfies (, )-differential privacy under T-fold adaptive composition, and equiva-lently (, )-Pufferfish privacy with parameters (S, Q, ) as defined in 3.2": "Truthfulness in Data CollectionA related question to privacy protection is an individuals willingnessto honestly disclose her data to the data curator. For example, in the context of epidemic control, eachindividual i may have a utility function ui : S mapping states to a positive real number, with statesthat represent high infection rates assigned lower values because they are, perhaps, more likely to attracta mandated lock-down of the community. Would an individual who is sampled for data collection gain anadvantage by misreporting her true infection status?",
  "Utility Analysis": "We now analyze the utility of our DPRL approach and present a theoretical result that bounds the approxi-mation error of the optimal value function under privacy from the true optimal value function. The analysiswe provide is asymptotic in nature and serves as an important step in establishing that good solutions arepossible in our problem setting. Whilst our approach to privacy in Algorithm 1 makes it easy to guarantee the differential privacy of anydownstream RL algorithm, the learning and control problems are made more difficult as the true underlyingprocess is now unobservable to the agent. visualizes the graphical model under our approach andhighlights that the state, privatized states and actions evolve according to a partially-observable markovdecision process (POMDP).",
  "P( | s, a) P( | s, a)1 L s s1": "Since the privatized environment evolves as a POMDP, the distribution for the privatized state st will ingeneral depend on the entire history of observed states, actions and rewards. However, when an MDP RLalgorithm is directly applied on top of privatized states, the transitions between privatized states are assumedto be Markovian. The induced transition model will however depend on the asymptotic state distributionunder a behaviour policy generating interactions. For our analysis, we consider an off-policy setting where astationary behaviour policy generates a sequence of privatized states, actions, and rewards. The inducedMarkovian transition model P : S A D(S) describes the asymptotic transition probabilities and isgiven by",
  "st+1SP(st+1 | st, at)PM(st+1 | st+1).(6)": "Here PM(s | s) = P(M(s) = s) denotes the distribution of the state privatization mechanism and P(st+1 | st, at)denotes the transition matrix of the underlying MDP. The transition model P depends upon the behaviourpolicy through the distribution (st | st, at), which is the asymptotic probability of the underlying statebeing st under when st is observed and at is performed. Using Bayes theorem, it can be expressed as",
  "Under Assumptions 1 and 3, the distributions (s | a) and (7) are well defined": "Under our privatisation scheme, the induced MDP on top of privatized states is given by M = (S, A, P , r, ).Note that the reward function does not change as the received rewards are functions of the privatizedstates. For any policy , the Q-value Q in M satisfies the Bellman equation Q = r + P V whereV (s) = Ea( | s)[ Q(s, a)]. Our analysis is performed using the projected laplace mechanism detailed in Algorithm 2. The projectedlaplace mechanism first applies additive noise to the input state as per the standard laplace mechanism. Werequire that the privacy mechanism we use has the state space as its output domain. Adding laplace noise nolonger guarantees that the output will be in the state space, and so we take the orthogonal projection backonto the state space. This operation is guaranteed to be differentially private by Lemma 2.",
  "where P(s | s1, a) =": "s2S P(s2 | s1, a)PM(s | s2). The first term can be viewed as the error due to privatisingthe output state from the transition model and the second term can be viewed as the error due to privatisingthe input state to the transition model. The first term is bound by first applying the Bretagnolle-Huber inequality. The KL divergence between Psaand Psa can be bound by noting that Psa is a convolution between the privacy mechanism and the truetransition model. The sum in the convolution can be reduced to a single element, leading to the bound:",
  "N and combining terms then gives the final result": "Theorem 3 highlights how the approximation error scales as the population sample size N and privacyparameter increase. For a given K, the approximation error decreases exponentially quickly as increasesand at a rate of N 1 2 as N increases. Thus, increasing the sampled population size is an important factor inattaining good quality solutions for RL in population processes. Importantly, there are components in theupper bound that depend on only one of N or . This implies that both quantities must be increased to drivethe error completely to zero. The bound also highlights that performance will degrade when the number ofstatuses of interest, K, increases. This is likely a function of the fact that the state space is discrete andscales exponentially with the dimension; its possible that formulating the problem with a query whose rangeis continuous and as a continuous reinforcement learning problem could avoid this issue. Nevertheless, ourtheoretical result shows the scaling behaviour of the approximation error and we corroborate this behaviourin our experiments.",
  "Experiments": "We present empirical results that corroborate our theoretical findings on the SEIRS Epidemic Control problemdetailed in 3. Our experiments on the Epidemic Control problem are representative of the class of populationprocess environments as alternate problems will primarily differ in their transition dynamics. We encapsulatethis by running experiments that vary the graph structure, population size, and transition parameters.",
  "DatasetNameNodesEdges": "Slashdot (Leskovec et al., 2009)82K82,168948,464Twitch (Rozemberczki & Sarkar, 2021)168k168,1146,797,557Gowalla (Cho et al., 2011)196K196,591950,327Youtube (Yang & Leskovec, 2012)1.1M1,134,8902,987,624 and is taken as a convex combination between two functions I(st) and C(at). The function I(st) returns theproportion of Exposed and Infected individuals at time t and C(at) returns the proportion of individualsquarantined by action at. We set = 0.8 in all experiments. For each graph, we run simulations that vary the population size and the target cumulative privacy budget to see how the RL algorithms performance changes as these key variables change. For given target cumulativeprivacy parameters (, ), we set the per-step privacy budget as",
  "T log(1/),(8)": "and = 0. Under Lemma 3, setting the per-step privacy budget in this manner only satisfies the target valueof for a certain range of values for . As the value of decreases, the gap between the privacy achieved andthe target privacy widens. In practice, values under 10 are of interest and we find that this can be achievedwhen T = 2e5 if 102. For more details see , Appendix C. The RL algorithm we use is the DQN algorithm (Mnih et al., 2015) initialized with an experience replay buffer(Lin, 1992). We refer to its differentially private version as DP-DQN. DP-DQN can only store privatizedtransitions in its replay buffer. The DQN algorithm and the environment interact in an online fashionand exploration is performed using epsilon-greedy. The projected laplace mechanism was used as the stateprivatisation mechanism. A full description of the DP-DQN algorithm, a concrete algorithm for computingthe projected laplace mechanism, and hyperparameters used is provided in Appendix C.",
  ": True reward received by DP-DQN in Experiment 1 (top row), Experiment 2 (middle row), andExperiment 3 (bottom row)": "Whilst the results in are useful for corroborating our theoretical experiments, it is also important toinvestigate how the policy learnt under our privacy setup performs. plots the true reward received byDP-DQN across different graphs as varies. Overall, DP-DQNs performance increases as increases acrossall networks. Additionally, performance under privacy is clustered much closer to the default performanceacross all values in the 82k and 196k graphs than in the private reward case. The plots for the 1.1Mgraph show that performance under high privacy is indistinguishable from default performance. The gap inperformance between the high privacy setting (i.e. {0.1, 0.5}) and the low privacy setting on the 1.1Mgraph suggest that in some cases there may be a threshold value for that needs to be crossed to obtainalmost optimal performance. Thus, we find that the policy learnt under our privacy setup can perform welland improves, drastically in some cases, as the privacy budget increases.",
  "Impact Statement": "As reinforcement learning algorithms see wider adoption and begin interacting with humans and their data,issues around protecting privacy become more pertinent. Our work can be seen as providing a promisingstart and solid theoretical foundation to providing privacy protections in an important problem class wherereinforcement learning may be applied in the future.",
  "Sayak Ray Chowdhury and Xingyu Zhou. Distributed differential privacy in multi-armed bandits. arXivpreprint arXiv:2206.05772, 2022b": "Cdric Colas, Boris Hejblum, Sbastien Rouillon, Rodolphe Thibaut, Pierre-Yves Oudeyer, Clment Moulin-Frier, and Mlanie Prague. EpidemiOptim: A toolbox for the optimization of control policies in epidemio-logical models, 2020. URL Michela Del Vicario, Alessandro Bessi, Fabiana Zollo, Fabio Petroni, Antonio Scala, Guido Caldarelli,H Eugene Stanley, and Walter Quattrociocchi. The spreading of misinformation online. Proceedings of theNational Academy of Sciences, 113(3):554559, 2016. Christos Dimitrakakis, Blaine Nelson, Zuhe Zhang, Aikaterini Mitrokotsa, and Benjamin IP Rubinstein.Differential privacy for Bayesian inference through posterior sampling. Journal of Machine LearningResearch, 18(11):139, 2017.",
  "Frank McSherry and Kunal Talwar. Mechanism design via differential privacy. In 48th Annual IEEESymposium on Foundations of Computer Science, pp. 94103. IEEE, 2007": "Nikita Mishra and Abhradeep Thakurta. (Nearly) optimal differentially private stochastic multi-arm bandits.In Proceedings of the Thirty-First Conference on Uncertainty in Artificial Intelligence, pp. 592601, 2015. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, AlexGraves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control throughdeep reinforcement learning. Nature, 518(7540):529533, 2015.",
  "Suppose M = (M1, . . . , MT ) satisfies (, ) Pufferfish privacy with parameters (S, Q, ). We show for anypair of neighbouring databases there is a that preserves differential privacy": "Let D1:T be an arbitrary sequence of datasets. For each individual i in D1:T , let D1:T be obtained from D1:Tby removing individual is data from one or more of the datasets. Thus, there exists S and R S such thatD1:T satisfies (i,S) and D1:T satisfies (i,S\\R). We choose = {(E , t,1, . . . , t,N}t=1..T to be the following. E can be any stochastic population process.For each t S, define t,i = 1/2, and t,j = 1 for each j Lt \\ S and t,k = 0 for each k / Lt. And for eacht / S, define t,j = 1 for each j Lt and t,k = 0 for each k / Lt. Given M satisfies (, ) Pufferfish privacy,we have for any y1:T :",
  "Choosing in this manner for all neighbouring database sequences and repeating the calculations provesthe final result": "In the proof of Lemma 1, we have only considered neighbouring datasets where an individuals data is dropped.The case when a neighbouring dataset D is obtained from a dataset D by replacing an individual is datawith another individual js data, where j is not in D, can be formulated using the following secrets and secretpairs:",
  "B.1Laplace and Projected Laplace Mechanism Properties": "The mechanism we consider using is the projected laplace mechanism. Denote by ML the laplace mechanismwhich outputs s = ML(s) = s + (Y1, . . . , YK), Yi Lap(q/). The projected laplace mechanism outputss = M(s) by first applying the laplace mechanism s = ML(s) and then takes the 2 projection back ontothe state space, i.e. s = arg minsS s s2.",
  "B :=s RK : s PK(s)2 <": "Note that PS(s) must be a point that minimizes the 2 distance to PK(s), as PK(s) is the minimumdistance to the simplex K and minimizing the distance to S thus involves minimizing the distance alongK from PK(s). Also for two neighbouring points s1, s2 S (i.e. s1 = s2 and closest in 2 distance), wemust have two dimensions i = j where s1,i = s2,i 1",
  "B.2Additional Lemmas": "The Simulation Lemma (Kearns & Singh, 2002; Agarwal et al., 2022) lets us bound the value function errorin terms of the error in the transition functions.Lemma 4 (Simulation Lemma). Let M = (S, A, r, P, ) and M = (S, A, r, P, ) be two MDPs that differonly in the transition model. Given a policy , let Q be the value function under in M and Q be thevalue function under in M. Then for all Q Q",
  "N .(28)": "The first term in inequality (27) follows by noting that P is L-Lipschitz by Lemma 5 and Assumption 2, andthat, for all x RK, x1 K x. The second term in inequality (27) follows by noting that the L1 normbetween distributions is bound by 2 and applying Proposition 2. Picking = 2",
  "Algorithm 4 Differentially Private DQN (DP-DQN)": "1: Input: Environment M = (S, A, r, P, ), initial state s0 S, privacy mechanism M : S R S.2: Parameters: cumulative privacy parameters (, ), time horizon T, batch size B, target update step D,population size N, learning rate , initial exploration rate start < 1, decay rate < 1."
}