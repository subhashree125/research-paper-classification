{
  "Abstract": "Spiking Neural Networks (SNNs) are biologically plausible models that have been identifiedas potentially apt for deploying energy-efficient intelligence at the edge, particularly forsequential learning tasks. However, training of SNNs poses significant challenges due tothe necessity for precise temporal and spatial credit assignment. Back-propagation throughtime (BPTT) algorithm, whilst the most widely used method for addressing these issues,incurs high computational cost due to its temporal dependency. In this work, we proposeS-TLLR, a novel three-factor temporal local learning rule inspired by the Spike-TimingDependent Plasticity (STDP) mechanism, aimed at training deep SNNs on event-basedlearning tasks. Furthermore, S-TLLR is designed to have low memory and time complexities,which are independent of the number of time steps, rendering it suitable for online learningon low-power edge devices. To demonstrate the scalability of our proposed method, wehave conducted extensive evaluations on event-based datasets spanning a wide range ofapplications, such as image and gesture recognition, audio classification, and optical flowestimation. S-TLLR achieves comparable accuracy to BPTT (within 2% for most tasks),while reducing memory usage by 5 50 and multiply-accumulate (MAC) operations by1.3 6.6, particularly when updates are restricted to the last few time-steps. 1",
  "Introduction": "Over the past decade, the field of artificial intelligence has undergone a remarkable transformation, drivenby a prevalent trend of continuously increasing the size and complexity of neural network models. While thisapproach has yielded remarkable advancements in various cognitive tasks (Brown et al., 2020; Dosovitskiyet al., 2021), it has come at a significant cost: AI systems now demand substantial energy and computationalresources. This inherent drawback becomes increasingly apparent when comparing the energy efficiency ofcurrent AI systems with the remarkable efficiency exhibited by the human brain (Roy et al., 2019; Gerstneret al., 2014; Christensen et al., 2022; Eshraghian et al., 2023). Motivated by this observation, the researchcommunity has shown a growing interest in brain-inspired computing. The idea behind this approach is tomimic key features of biological neurons, such as spike-based communication, sparsity, and spatio-temporalprocessing. Bio-plausible Spiking Neural Network (SNN) models have emerged as a promising avenue in this direction.SNNs have already demonstrated their ability to achieve competitive performance compared to more tra-ditional Artificial Neural Networks (ANNs) while significantly reducing energy consumption per inferencewhen deployed in the right hardware (Davies et al., 2018; Sengupta et al., 2019; Christensen et al., 2022;",
  "Published in Transactions on Machine Learning Research (12/2024)": ": Effects of using a decaying factor, pre, different from the leak spiking parameter ( = 0.5) tocompute the causal term on the eligibility trace, with constant parameters (post, post, pre) = (0.2, 1, 1)when the learning signal is generated using BP. Plots are based on five trials. The solid purple line representsthe median value, and the dashed black line represents the mean value.",
  "MethodMemoryComplexityTimeComplexityTemporalLocalLeverageNon-Causality": "BPTTTnTn2XXRTRL (Williams & Zipser, 1989)n3n4Xe-prop (Bellec et al., 2020)n2n2XOSTL (Bohnstingl et al., 2022)n2n2XETLP (Quintana et al., 2023)n2n2XOSTTP (Ortner et al., 2023)n2n2XOTTT (Xiao et al., 2022)nn2XS-TLLR (Ours)nn2 To overcome the above limitations, in this paper, we propose S-TLLR, a novel three-factor temporal locallearning rule inspired by the STDP mechanism. Specifically, S-TLLR is designed to train SNNs on event-based learning tasks while incorporating both causal and non-causal relationships between the timing ofpre- and post-synaptic activities for updating the synaptic strengths. This feature is inspired by the STDPmechanism from which we propose a generalized parametric STDP equation that uses a secondary activationfunction to compute the post-synaptic activity. Then, we take this equation to compute an instantaneouseligibility trace (Gerstner et al., 2018) per synapse modulated by a third factor in the form of a learningsignal obtained from the backpropagation of errors through the layers (BP) or by using fixed random feedbackconnections directly from the output layer to each hidden layer (Trondheim, 2016). Notably, S-TLLR exhibitsconstant (in time) memory and time complexity, making it well-suited for online learning on resource-constrained devices. In addition to this, we demonstrate through experimentation that including non-causal information in thelearning process results in improved generalization and task performance. Also, we explored S-TLLR in thecontext of several event-based tasks with different amounts of spatio-temporal information, such as imageand gesture recognition, audio classification, and optical flow estimation. For all such tasks, S-TLLR canachieve performance comparable to BPTT and other learning rules, with significantly lower memory andcomputation requirements.",
  "yi[t] = (ui[t] vth)(2)": "where ui represents the membrane potential of the i-th neuron, and wij is the forward synaptic strengthbetween the i-th post-synaptic neuron and the j-th pre-synaptic neuron. Additionally, is the leak factorthat reduces the membrane potential over time, vth is the threshold voltage, and is the Heaviside function.Thus, when ui reaches the vth, the neuron produces an output binary spike (yi). This output spike triggersthe reset mechanism, represented by the reset signal vthyi[t], which reduces the magnitude of ui.",
  "Spike-Timing Dependent Plasticity (STDP)": "STDP is a learning mechanism observed in various neural systems, from invertebrates to mammals, andis believed to play a critical role in the formation and modification of neural connections in the brain inprocesses such as learning and memory (Gerstner et al., 2014). STDP describes how the synaptic strength(wij) between two neurons can change based on the temporal order of their spiking activity. Specifically,STDP describes the phenomenon by which wij is potentiated if the pre-synaptic neuron fires just before thepost-synaptic neuron fires, and wij is depressed if the pre-synaptic neuron fires just after the post-synapticneuron fires. This means that STDP rewards causality and punishes non-causality. However, Anisimovaet al. (2022) suggests that STDP favoring causality can be a transitory effect, and over time, STDP evolvesto reward both causal and non-causal relations in favor of synchrony. Such general dynamics of STDP canbe described by the following equation:",
  "pretitjpreif ti tj (causal term)posttjtipostif ti < tj (non-causal term)(3)": "where (ti, tj) represents the magnitude of the change in the synaptic strength (wij), ti and tj representthe firing times of the post- and pre-synaptic neurons, pre and pre are strength and exponential decayfactor of the causal term, respectively. Similarly, post and post parameterize the non-causal effect. Notethat when post < 0, STDP favors causality, whereas post > 0, STDP favors synchrony.",
  "wij[t + 1] = wij[t] + wij[t](5)": "where yi[t] and xj[t] are binary values representing the existence of post- and pre-synaptic activity (spikes)at time t, respectively. Note that both summations in (4) can be computed forward in time as a recurrentequation of the form tr(xj)[t] = pretr(xj)[t 1] + xj[t], where tr(xj)[t] is a trace of xj. Hence, (4) can beexpressed as:wij[t] = preyi[t]tr(xj)[t] + postxj[t](tr(yi)[t] yi[t])(6)",
  "Existing methods for training SNNs": "Several approaches to train SNNs have been proposed in the literature. In this work, we focus on surrogategradients(Neftci et al., 2019; Li et al., 2021), and bio-inspired learning rules (Diehl & Cook, 2015; Thieleet al., 2018; Kheradpisheh et al., 2018; Bellec et al., 2020). Training SNNs based on surrogate gradient methods (Neftci et al., 2019; Li et al., 2021) extends the tradi-tional backpropagation through time (BPTT) algorithm to the domain of SNNs, where the non-differentiablefiring function is approximated by a continuous function during the backward pass to allow the propagationof errors. The advantage of these methods is that they can exploit the temporal information of individualspikes so that they can be applied to a broader range of problems than just image classification (Paredes-Valls & de Croon, 2021; Cramer et al., 2022).Moreover, such methods can result in models with lowlatency for energy-efficient inference (Fang et al., 2021a). However, training SNNs based on surrogate gradi-ents with BPTT incurs high computational and memory costs because BPTTs requirements scale linearlywith the number of time steps. Hence, such methods can not be used for training online under the hardwareconstraints imposed by edge devices (Neftci et al., 2019). Another interesting avenue is the use of bio-inspired learning methods based on the principles of synapticplasticity observed in biological systems, such as STDP (Bi & Poo, 1998; Song et al., 2000) or eligibilitytraces (Gerstner et al., 2014; 2018), which strengthens or weakens the synaptic connections based on therelative timing of pre- and post-synaptic spikes optionally modulated by a top-down learning signal. STDPmethods are attractive for on-device learning as they do not require any external supervision or error signal.However, they also have several limitations, such as the need for a large number of training examples andthe difficulty of training deep networks or complex ML problems (Diehl & Cook, 2015). In contrast, three-factor learning rules using eligibility traces (neuron local synaptic activity) modulated by an error signal,like e-prop (Bellec et al., 2020), can produce more robust learning overcoming limitations of unsupervisedmethods such as STDP. Nevertheless, such methods time and space complexity typically make them toocostly to be used in deep SNNs (especially in deep convolutional SNNs).",
  "Learning rules addressing the temporal dependency problem": "As discussed in the previous section, the training methods based on surrogate gradient using BPTT resultsin high-performance models. However, their major limitations are associated with high computational re-quirements that are unsuitable for low-power devices. Such limitations come from the fact that BPTT hasto store a copy of all the spiking activity to exploit the temporal dependency of the data during training. Inorder to address the temporal dependency problem, several methods have been proposed where the compu-tational requirements are time-independent while achieving high performance. For instance, the Real-TimeRecurrent Learning (RTRL) (Williams & Zipser, 1989) algorithm can compute exact gradients without thecost of storing all the intermediate states. Although it has not been originally proposed to be used on SNNs,it could be applied to them by combining with surrogate gradients (Neftci et al., 2019). More recently, othermethods such as e-prop (Bellec et al., 2020), OSTL (Bohnstingl et al., 2022), and OTTT (Xiao et al., 2022),derived from BPTT, allows learning on SNNs using only temporally local information (information that canbe computed forward in time). However, with the exception of OTTT, all of these methods have memoryand time complexities of O(n2) or worse, as shown in , making them significantly more expensivethan BPTT, when used for deep SNNs in practical scenarios (where n T). Moreover, since those methods(with the exception of RTRL) have been derived as approximations of BPTT, they only use causal relationsin the timing between pre- and post- synaptic activity, leaving non-causal relations (as those used in STDPshown in ) unexplored.",
  "Combining STDP and backpropagation": "As previously discussed, STDP has been used to train SNNs models in an unsupervised manner (Diehl &Cook, 2015; Thiele et al., 2018; Kheradpisheh et al., 2018). However, such approaches suffer from severedrawbacks, such as requiring a high number of timesteps (latency), resulting in low accuracy performance andbeing unable to scale for deep SNNs. So, to overcome such limitations, there have been some previous effortsto use STDP in combination with backpropagation for training SNNs, by either using STDP followed forfine-tuning with BPTT (Lee et al., 2018) or modulating STDP with an error signal (Tavanaei & Maida, 2019;Hu et al., 2017; Hao et al., 2020). However, such methods either do not address the temporal dependencyproblem of BPTT or do not scale for deep SNNs or complex computer vision problems.",
  "Overview of S-TLLR and its key features": "We propose a novel 3F learning rule, S-TLLR, which is inspired by the STDP mechanism discussed in.2. S-TLLR is characterized by its temporally local nature, leveraging non-causal relations in thetiming of spiking activity while maintaining a low memory complexity O(n). Regarding memory complexity, a conventional 3F learning rule requires an eligibility trace (eij) which involvesa recurrent equation as described in (8). In such formulation, the eij is a state requiring a memory thatscales linearly with the number of synapses (O(n2)). For the S-TLLR, we dropped the recurrent term andconsidered only the instantaneous term (i.e. = 0 in (8)). Instead of requiring O(n2) memory to store thestate of eij, we need to keep track of only two variables (f(yi) and g(xi)) with O(n) memory. Hence, eij[t]can be computed as the right-hand side of (6) which exhibits a memory complexity O(n). This low-memorycomplexity is a key aspect of S-TLLR since it enables the method to be used in deep neural models wheremethods such as Williams & Zipser (1989); Bellec et al. (2020); Bohnstingl et al. (2022); Ortner et al. (2023);Quintana et al. (2023) are considerably more resource-intensive. Finally, since BPTT is based on the propagation of errors in time, it only uses causal relations to computegradients, that is, the relation between an output y[t] and previous inputs x[t], x[t 1], . . . , x. Thesecausal relations are shown in (red dotted line). Also, methods derived from BPTT (Bellec et al.,2020; Bohnstingl et al., 2022; Ortner et al., 2023; Quintana et al., 2023; Xiao et al., 2022) use exclusivelycausal relations. In contrast, we took inspiration from the STDP mechanisms, which use both causal and",
  "Technical details and implementation of S-TLLR": "As discussed in the previous section, our proposed method, S-TLLR has the form of a three-factor learningrule, wij[t] = i[t]eij[t], involving a top-down learning signal i[t] and an eligibility trace, eij[t]. To computeeij[t], we use a generalized version of the STDP equation described in (4) that can use a secondary activationfunction to compute the postsynaptic activity.",
  "t=0ttpost(ui[t])(10)": "Here, is a secondary activation function that can differ from the firing function used in (2). We foundempirically that using a function (u) with(u)du 1 yields improved results, specific functions areshown in Appendix A.3. Furthermore, (10) considers both causal (first term on the right side) and non-causal (second term) relations between the timing of post- and pre-synaptic activity, which are not capturedin BPTT (or its approximations (Bellec et al., 2020; Bohnstingl et al., 2022; Xiao et al., 2022)). The causal(non-causal) relations are captured as the correlation in the timing between the current post- (pre-) synapticactivity and the low-pass filtered pre- (post-) synaptic activity. Note that (10) can be computed forward intime (expressing it in the form of (6)) and using only information locally available to the neuron as follows:",
  "t=Tli[t]eij[t](13)": "Here, is the learning rate, T is the total number of time steps for the forward pass, Tl is the initial timestep for which the learning signal is available, y is the ground truth label vector, yL[t] is the output vectorof layer L at time t, and L is the loss function. Note that the backpropagation occurs through the layers andnot in time, so the S-TLLR is temporally local. Depending on the task, good performance can be achievedeven if the learning signal is available just for the last time step (Tl = T). While our primary focus is on error-backpropagation to generate the learning signal, it is worth noting thatemploying random feedback connections, such as direct feedback alignment (DFA) (Trondheim, 2016; Frenkelet al., 2021), for this purpose is also feasible. In such cases, S-TLLR also exhibits spatial locality. We presentsome experiments in this direction in Appedix C.1. The S-TLLR algorithm for a multilayer implementationis shown in Algorithm 1.",
  "Computational improvements": "Here we analyzed the theoretical computational improvements in terms of the number of multiply-accumulate(MAC) operations and memory requirements of S-TLLR with respect to BPTT. First, we started by ex-panding the BPTT terms by replacing (1) on (7). Assuming an SNN model with L layers and N neuronsper layer, and appropriately factorizing the terms as described in Appendix B.1, we obtain the followingexpression for the gradients on the synaptic connections in layer l:",
  "t=0tty(l1)i[t](17)": "From (34), it can be observed that BPTT is not local in time as the updated at time step t depend onfuture time steps, as illustrated in . Therefore, the BPTT requires information on all the time steps,and consequently, its memory requirements scale linearly with the total number of time steps in the inputsequence (T). Then, the total memory required is estimated as follows:",
  "l=0N (l)(18)": "To estimate the number of operations, specifically MAC operations, we exclude any element-wise operations.Referring to (34), we ascertain that the number of operations depends on both the number of inputs andoutputs, resulting in a total of N (l) N (l1) operations. Additionally, we need to account for the operationsinvolved in propagating the learning signals to the previous layer, equating to N (l) N (l1). Consequently,the estimated number of operations can be calculated as follows:",
  "l=0N (l)(20)": "Here, the factor 2 is produced by the trace variables required to maintain the temporal information. For thenumber MACs, we first must note that S-TLLR only updates the weights when the learning signal is presentat time Tl, that is weights are updated T Tl times. Additionally, we account for the operations requiredby the non-causal terms. Therefore, the number of operations for S-TLLR is:",
  "We performed ablation studies on the DVS Gesture, DVS CIFAR10, N-CALTECH101, and SHD datasetsto evaluate the effect of the non-causal factor (post) on the learning process": "For this purpose, we train a VGG9 model, described in Appedix A, five times with the same random seedsfor 30, 30, and 300 epochs in the DVS Gesture, N-CALTECH101, and DVS CIFAR10 datasets, respectively.Similarly, a recurrent SNN (RSNN) model, described in Appedix A, was trained five times during 200 epochson the SHD. To analyze the effect of the non-causal term, we evaluate three values of post, 1, 0, and 1.According to (10), when post = 0, only causal terms are considered, while post = 1 (post = 1) meansthat the non-causal term is added positively (negatively). As shown in , for vision tasks, it can be",
  "DatasetModelTTl(post, pre, pre)post = 0post = +1post = 1": "DVS GestureVGG92015(0.2, 0.75, 1)94.61 0.73%94.01 1.10%95.07 0.48%DVS CIFAR10VGG9105(0.2, 0.5, 1)72.93 0.94%73.42 0.50%73.93 0.62%N-CALTECH101VGG9105(0.2, 0.5, 1)62.24 1.22%53.42 1.50%66.33 0.86%SHDRSNN10010(0.5, 1, 1)77.09 0.33%78.23 1.84%74.69 0.47% seen that using post = 1 improves the average accuracy performance of the model with respect to onlyusing causal terms post = 0. In contrast, for SHD, using post = 1 improves the average performanceover using only causal terms, as shown in . This indicates that considering the non-causal relationsof the spiking activity (either positively or negatively) in the learning rule helps to improve the networkperformance. An explanation for this effect is that the non-causal term acts as a regularization term thatallows better exploration of the weights space. Additional ablation studies are presented in Appedix C.3that support the improvements due to the non-causal terms.",
  "Image and Gesture Recognition": "We train VGG-9 and ResNet18 models during 300 epochs using the Adam optimizer with a learning rateof 0.001.The models were trained five times with different random seeds.The baseline was set usingBPTT, while the models trained using S-TLLR used the following STDP parameters (post, pre, post, pre):(0.2, 0.75, 1, 1) for DVS Gesture and (0.2, 0.5, 1, 1) for DVS CIFAR10 and N-CALTECH101. The test accuracies are shown in . In all those tasks, S-TLLR shows a competitive performancecompared to the BPTT baseline. In fact, for DVS Gesture and N-CALTECH101, S-TLLR outperforms theaverage accuracy obtained by the baseline trained with BPTT. Because of the small size of the DVS Gesturedataset and the complexity of the BPTT algorithm, the model overfits quickly resulting in lower performance.In contrast, S-TLLR avoids such overfitting effect due to its simple formulation and by updating the weightsonly on the last five timesteps. also includes results from previous works using spiking models onthe same datasets. For DVS Gesture, it can be seen that S-TLLR outperforms previous methods such asXiao et al. (2022); Shrestha & Orchard (2018); Kaiser et al. (2020), in some cases with significantly lessnumber of time-steps. In the case of DVS CIFAR10, S-TLLR demonstrates superior performance comparedto the baseline with BPTT when the learning signal is utilized across all time steps (Tl = 0). Furthermore,S-TLLR surpasses the outcomes presented in Zheng et al. (2021); Fang et al. (2021b); Li et al. (2021), yetit lags behind others such as Xiao et al. (2022); Deng et al. (2022); Meng et al. (2022). However, studiessuch as Deng et al. (2022); Meng et al. (2022) showcase exceptional results primarily focused on static taskswithout addressing temporal locality or memory efficiency during SNN training. Consequently, althoughserving as a reference, they do not fairly compare to S-TLLR. The most pertinent comparison lies with Xiaoet al. (2022), which shares similar memory and time complexity with S-TLLR. Notably, S-TLLR (Tl = 0,post = 1) exhibits a performance deficit of 0.67%. This difference is primarily attributed to the differencein batch size during training. While Xiao et al. (2022) uses a batch size of 128, we were constrained to 48due to hardware limitations. To validate this point, we trained another model utilizing only causal terms:S-TLLR (Tl = 0, post = 0), equating it to Xiao et al. (2022) considering the selection of STDP parametersand secondary activation function () in DVS CIFAR10 experiments. The comparison reveals that S-TLLR(post = 1) outperforms S-TLLR (post = 0) (equivalent to Xiao et al. (2022)) under the same conditions,further corroborating the advantages of including non-causal terms (post = 0) during training. Finally,when compared to BPTT, S-TLLR signifies a 5 memory reduction. By using the learning signal for thelast five time steps, it effectively diminishes the number of multiply-accumulate (MAC) operations by 2.6for DVS Gesture, and by 1.3 for DVS CIFAR10 and N-CALTECH101.",
  "DVS CIFAR10": "BPTT (Zheng et al., 2021)ResNet-1910-67.8%--BPTT (Fang et al., 2021b)PLIF (7 layers)201674.8%--TET (Deng et al., 2022)VGG-111012883.17 0.15%--DSR (Meng et al., 2022)VGG-111012877.27 0.24%--BPTT (Li et al., 2021)ResNet-1810-75.4 0.05%--OTTTA(Xiao et al., 2022)VGG-91012876.27 0.05%--BPTT (baseline)VGG-9104875.44 0.76%6.8218.12S-TLLR (Ours, Tl = 5, post = 1 )VGG-9104873.93 0.62%5.123.62S-TLLR (Ours, Tl = 0, post = 1 )VGG-9104875.6 0.10%10.263.62S-TLLR (Ours, Tl = 0, post = 0 )VGG-9104874.8 0.15%6.823.62BPTT (baseline)ResNet18104872.68 0.87%7.1328.14S-TLLR (Ours, Tl = 5)ResNet18104871.94 0.75%5.125.62S-TLLR (Ours, Tl = 0)ResNet18104874.5 0.64%10.245.62",
  "SHD": "ETLP (Quintana et al., 2023)ALIF-RSNN10012874.59 0.44%--OSTTP (Ortner et al., 2023)LIF-RSNN100-77.33 0.8%--BPTT (Bouanane et al., 2023)LIF-RSNN10012883.41--BPTT (Cramer et al., 2022)LIF-RSNN100-83.2 1.3--BPTT (baseline)LIF-RSNN10012870.57 0.960.0540.961S-TLLRBP (Ours)LIF-RSNN10012878.24 1.84%0.0960.019S-TLLRDFA (Ours)LIF-RSNN10012874.60 0.52%0.0960.019 1: # MAC and Memory are estimated for a batch size of 1 following the equations (23) and (22) described in the .3.2: Previous studies accuracy values are provided as reported in their respective original papers.",
  "Audio Classification": "In order to set a baseline, we train the same RSNN with the same hyperparameters using BPTT for fivetrials, and with the following STPD parameters (0.5, 1, 1, 1). As shown in , the model trained withS-TLLR outperforms the baseline trained with BPTT. The result shows the capability of S-TLLR to achievehigh performance and generalization. One reason why the baseline does not perform well, as suggested inCramer et al. (2022), is that RSNN trained with BPTT quickly overfits. This also highlights a nice propertyof S-TLLR. Since it has a simpler formulation than BPTT, it can avoid overfitting, resulting in a bettergeneralization. However, note that works such as Cramer et al. (2022); Bouanane et al. (2023) can achievebetter performance after carefully selecting the hyperparameters and using data augmentation techniques. Incomparison with such works, our method still shows competitive performance with the advantage of havinga 50 reduction in memory. Furthermore, we compared our results with Quintana et al. (2023); Ortner et al. (2023), which uses thesame RSNN network structure with LIF and ALIF (LIF with adaptative threshold) neurons and temporallocal learning rules. shows that using S-TLLR with BP for the learning signal results in better",
  "ModelsTrainingMethodTypeOD1AEEIF1AEEIF2AEEIF3AEEAEESum": "FSFNpost = 0.2 (Ours)S-TLLRSpiking0.500.761.191.003.45FSFNpost = 0.2 (Ours)S-TLLRSpiking0.540.781.281.093.69FSFNpost = 0 (Ours)S-TLLRSpiking0.500.771.251.083.60FSFN (baseline)BPTTSpiking0.450.761.171.023.40Apolinario et al. (2023)BPTTSpiking0.510.821.211.073.61Kosta & Roy (2023)BPTTSpiking0.440.791.371.113.78Hagenaars et al. (2021)BPTTSpiking0.450.731.451.173.80Zero prediction--1.081.292.131.886.38 performance than those obtained with other temporal local learning rules, with the advantage of having alinear memory complexity instead of squared. Moreover, using DFA to generate the learning signal resultsin competitive performance with the advantage of being local in both time and space.",
  "Event-based Optical Flow": "The optical flow estimation is evaluated using the average endpoint error (AEE) metric that measures theEuclidean distance between the predicted flow (ypred) and ground truth flow (ygt) per pixel. For consistency,this metric is computed only for pixels containing events (P), similar to Apolinario et al. (2023); Kosta &Roy (2023); Lee et al. (2020); Zhu et al. (2018a; 2019), given by the following expression:",
  "Pypredi,j ygti,j2(24)": "For this experiment, we trained a Fully-Spiking FlowNet (FSFN) model, discussed in Appedix A, with S-TLLR using the following STDP parameters (post, pre, pre) = (0.5, 0.8, 1) and post = [0.2, 0.2, 0]. Themodels were trained during 100 epochs using the Adam optimizer with a learning rate of 0.0002, a batch sizeof 8, and with the learning signal obtained from the photometric loss just for the last time step (Tl = 9). Asit is shown in , the FSFN model trained using S-TLLR with post = 0.2 shows a performance closeto the baseline implementation trained with BPTT. Although we mainly compared our model with BPTT,to take things into perspective, we include results from other previous works. Among the spiking models,our model trained with S-TLLR has the second-best average performance (AEE sum) in comparison withsuch spiking models of similar architecture and size trained with BPTT (Apolinario et al., 2023; Kosta &Roy, 2023; Hagenaars et al., 2021). The results indicate that our method achieves high performance on acomplex spatio-temporal task, such as optical flow estimation, with 5 less memory and a 6.6 reductionin the number of MAC operations by just updating the model in the last time step.",
  "Conclusion": "Our proposed learning rule, S-TLLR, can achieve competitive performance in comparison to BPTT on severalevent-based datasets with the advantage of having a constant memory requirement. In contrast to BPTT (orother temporal learning rules) with higher memory requirements O(Tn) (or O(n2)), S-TLLR memory is justproportional to the number of neurons O(n). Moreover, in contrast with previous works that are derived fromBPTT as approximations, and therefore using only causal relations in the spike timing, S-TLLR explores adifferent direction by leveraging causal and non-causal relations based on a generalized parametric STDPequation. We have experimentally demonstrated on several event-based datasets that including such non-causal relations can improve the SNN performance in comparison with temporal local learning rules usingjust causal relations. Also, we could observe that tasks where spatial information is predominant, such asDVS CIFAR-10, DVS Gesture, N-CALTECH101, and MVSEC, benefit from causality (post = 1). Incontrast, tasks like SHD, where temporal information is predominant, benefit from synchrony (post = 1).",
  "Siyuan Qiao, Huiyu Wang, Chenxi Liu, Wei Shen, and Alan Yuille. Micro-Batch Training with Batch-ChannelNormalization and Weight Standardization. 3 2019. URL": "Fernando M. Quintana, Fernando Perez-Pea, Pedro L. Galindo, Emre O. Neftci, Elisabetta Chicca, andLyes Khacef. ETLP: Event-based Three-factor Local Plasticity for online learning with neuromorphichardware. 1 2023. URL Kaushik Roy, Akhilesh Jaiswal, and Priyadarshini Panda.Towards spike-based machine intelligencewith neuromorphic computing.Nature, 575(7784):607617, 11 2019.ISSN 0028-0836.doi: 10.1038/s41586-019-1677-2. URL Abhronil Sengupta, Yuting Ye, Robert Wang, Chiao Liu, and Kaushik Roy. Going Deeper in Spiking NeuralNetworks: VGG and Residual Architectures. Frontiers in Neuroscience, 13:95, 3 2019. ISSN 1662-453X.doi: 10.3389/fnins.2019.00095. Xueyuan She, Saurabh Dash, and Saibal Mukhopadhyay. Sequence Approximation using Feedforward Spik-ing Neural Network for Spatiotemporal Learning: Theory and Optimization Methods. In InternationalConference on Learning Representations (ICLR), 2022.",
  "Ronald J. Williams and David Zipser. Experimental Analysis of the Real-time Recurrent Learning Algorithm.Connection Science, 1(1):87111, 1 1989. ISSN 0954-0091. doi: 10.1080/09540098908915631. URL": "Yujie Wu, Lei Deng, Guoqi Li, Jun Zhu, and Luping Shi. Spatio-temporal backpropagation for traininghigh-performance spiking neural networks. Frontiers in Neuroscience, 12(MAY):323875, 5 2018. ISSN1662453X. doi: 10.3389/FNINS.2018.00331/BIBTEX. URL www.frontiersin.org. Mingqing Xiao, Qingyan Meng, Zongpeng Zhang, Di He, and Zhouchen Lin. Online Training Through Timefor Spiking Neural Networks. In 36th Conference on Neural Information Processing Systems (NeurIPS2022), 10 2022. URL Hanle Zheng, Yujie Wu, Lei Deng, Yifan Hu, and Guoqi Li. Going Deeper With Directly-Trained LargerSpiking Neural Networks. Proceedings of the AAAI Conference on Artificial Intelligence, 35(12):1106211070, 5 2021. ISSN 2374-3468. doi: 10.1609/AAAI.V35I12.17320. URL Alex Zhu, Liangzhe Yuan, Kenneth Chaney, and Kostas Daniilidis. EV-FlowNet: Self-Supervised OpticalFlow Estimation for Event-based Cameras. In Robotics: Science and Systems XIV, pp.62. Robotics:Science and Systems Foundation, 6 2018a. ISBN 978-0-9923747-4-7. doi: 10.15607/RSS.2018.XIV.062. Alex Zihao Zhu, Dinesh Thakur, Tolga Ozaslan, Bernd Pfrommer, Vijay Kumar, and Kostas Daniilidis. TheMultivehicle Stereo Event Camera Dataset: An Event Camera Dataset for 3D Perception. IEEE Roboticsand Automation Letters, 3(3):20322039, 7 2018b. ISSN 2377-3766. doi: 10.1109/LRA.2018.2800793. Alex Zihao Zhu, Liangzhe Yuan, Kenneth Chaney, and Kostas Daniilidis. Unsupervised event-based learningof optical flow, depth, and egomotion.In Proceedings of the IEEE Computer Society Conference onComputer Vision and Pattern Recognition, volume 2019-June, pp. 989997. IEEE Computer Society, 62019. ISBN 9781728132938. doi: 10.1109/CVPR.2019.00108.",
  "ADatasets and experimental setup": "We conducted experiments on various event-based datasets, including DVS Gesture (Amir et al., 2017),N-CALTECH101 (Orchard et al., 2015), DVS CIFAR-10 (Li et al., 2017), SHD (Cramer et al., 2022),and MVSEC (Zhu et al., 2018b). These datasets encompass a wide range of applications, such as imageand gesture recognition, audio classification, and optical flow estimation. In this section, we outline theexperimental setup employed in , covering SNN architectures, dataset preprocessing, and lossfunctions.",
  "A.1Network architectures": "For experiments on image and gesture recognition, we use a VGG-9 model with the following structure: 64C3-128C3-AP2S2-256C3-256C3-AP2S2-512C3-512C3-AP2S2-512C3-512C3-AP2S2-FC. In this notation, 64C3signifies a convolutional layer with 64 output channels and a 3x3 kernel, AP2S2 represents average-poolinglayers with a 2x2 kernel and a stride of 2, and FC denotes a fully connected layer. In addition, instead of",
  "batch normalization, we use weight standardization Qiao et al. (2019) similar to Xiao et al. (2022). Also,the leak factor and threshold for the LIF models (1) are = 0.5 and vth = 0.8, respectively": "For experiments with the SHD dataset, we employ a recurrent SNN (RSNN) consisting of one recurrent layerwith 450 neurons and a leaky integrator readout layer with 20 neurons, following a similar configuration asin Ortner et al. (2023); Quintana et al. (2023). Both layers are configured with a leak factor of = 0.99,and the recurrent LIF layers threshold voltage is set to vth = 0.8. We utilize the Fully-Spiking FlowNet (FSFN) for optical flow estimation, as introduced by Apolinario et al.(2023). The FSFN adopts a U-Net-like architecture characterized by performing binary spike computationin all layers. Notably, we enhance the model by incorporating weight standardization (Qiao et al., 2019) inall convolutional layers. Additionally, our training approach involves ten time steps without temporal inputencoding, in contrast to Apolinario et al. (2023) that uses the encoding method proposed by Lee et al. (2020).Also, the leak factor and threshold of the LIF neurons used for our FSFN are 0.88 and 0.6, respectively.",
  "Here we describe the data pre-processing for each dataset:": "DVS Gesture: the dataset contains 11 hand gesture categories from 29 subjects under 3 illuminationconditions recorded with a Dynamic Vision Sensor (DVS) with a resolution of 128128 pixels. Therecordings were split into sequences of 1.5 seconds of duration, and the events were accumulatedinto 20 bins (event frames), with each bin having a 75 ms time window. Then, the event frameswere resized to a size of 32 32, while maintaining the positive and negative polarities as channels. N-CALTECH: is a spiking version of the original frame-based CALTECH101 dataset. It was pro-duced by recording the static images of the CALTECH101 dataset displayed on a LCD monitorusing an ATIS sensor (an event-based camera) while moving it. The recordings were wrapped intoten bins with the same time window size for each bin (30 ms). Then, the event frames were resizedto a dimension of 60 45. DVS CIFAR10:is a spiking version of 10000 samples from the frame-based CIFAR10 datasetrecorded with a event-based camera at a resolution of 128128 pixels. The recordings were wrappedinto ten bins with the same time window size for each bin. Then, the event frames were resizedto a dimension of 48 48. Additionally, a random crop with a padding of 4 was used for dataaugmentation. SHD: is an audio-classification dataset consisting of spoken digits ranging from 0 to 9 in both Englishand German. The audio waveforms were converted into spike trains using an artificial model of theinner ear auditory system. The events (spikes) in each sequence were wrapped into 100 bins, eachwith a time window duration of 10 ms. No data augmentation techniques were used for SHD. MVSEC: is a event-based dataset used for training and evaluating optical flow predictions. It con-tains stereo event-based data collected in various environments, including indoor flying and outdoordriving, along with the corresponding ground truth optical flow. The events between two consec-utive grayscale frames were wrapped into ten bins, keeping the negative and positive polarities aschannels. Then, the event frames are fed to the SNN model sequentially, similar to the approach inApolinario et al. (2023).",
  "A.3Loss functions and secondary activation functions ()": "For image, gesture, and audio classification tasks, we utilized cross-entropy (CE) loss and computed thelearning signal using (12) with ground truth labels (y). In contrast, for optical flow, we employed a self-supervised loss based on photometric and smooth loss, as detailed in Equation (5) in Lee et al. (2020). Regarding the generation of the learning signal (), in the context of image and gesture recognition, it isexclusively generated for the final five time steps (Tl = 5). For audio classification, we employ Tl = 90, while",
  "+ (10(ui[t] vth))2(28)": "Here, max(a, b) returns the maximum between a and b, and || represents the absolute value function. Theseactivation functions (25), (26), (27), and (28) are specifically used for SHD, DVS Gesture, DVS CIFAR10,and MVSEC, respectively. Note that the secondary activation function plays a role similar to that of the surrogate gradient in BPTT, sothe selected secondary functions were adapted from previous works that utilized similar functions in BPTTschemes. Specifically, (25) was adapted from Ortner et al. (2023), (26) from Apolinario et al. (2023), (27)from Wu et al. (2018), and (28) from Hagenaars et al. (2021).",
  "A.4.1Experimental setup: effects of non-causal terms on learning": "For the experiments conducted on DVS Gesture, N-CALTECH101, and DVS CIFAR10 datasets, we traineda VGG9 model for 30, 30, and 300 epochs, respectively. In all these experiments, we utilized the Adamoptimizer with a learning rate of 0.001. Additionally, the learning signal was presented only during thelast five time steps, meaning the model was updated and the error computed five times per sample. Forthese experiments, the parameters post, pre, and pre were kept constant, while post was varied tosimulate different scenarios: the absence of non-causal terms (post = 0), the inclusion of non-causal terms(post = +1), and the subtraction of non-causal terms (post = 1). This approach allowed us to evaluatethe impact of non-causal terms on the learning rule. For the experiments on the SHD dataset, we trained a recurrent spiking neural network (SNN) model for200 epochs. In this setup, we used the Adam optimizer with a learning rate of 0.0002 and a batch size of128. Similar to the previous experiments, post was varied among three values (1, 0, and +1) to assess theeffect of non-causal terms while keeping all other parameters constant.",
  "B.1BPTT analysis": "To analyze BPTT, we follow a similar analysis as Bellec et al. (2020). Here, we will utilize a three-layerfeedforward SNN as illustrated in Fig. B. Our analysis is based on a regression problem with the targetdenoted as y across T time steps, and our objective is to compute the gradients for the weights of the firstlayer (w(1)). The Mean Squared Error (MSE) loss function (L) is defined as follows:",
  "t=0 tty(0)[t] that at time-step t depends only on previous information (0, 1, ..., t 1, t),and an learning signal (L": "y(1)[t]) depends on information of future time steps (t + 1, t + 2, ..., T). Thosecomponents are visualized in Fig. B. Since the error signal depends on future time steps it can not becomputed locally in time, that is with information available only at the time step (t). Therefore, BPTT isnot a temporal local learning rule.",
  "B.2Example of real GPU memory usage": "This section shows the substantial memory demands associated with BPTT and their correlation with thenumber of time steps (T). To illustrate this point, we employ a simple regression problem utilizing a syntheticdataset (x, y), where x denotes a vector of dimension 1000 and y represents a scalar value. The batch sizeused is 512. The structure of the SNN model comprises five layers, structured as follows: 1000FC-1000FC-1000FC-1000FC-1FC. In addition, the loss is computed solely for the final time step as L = (y[T]y)2. Thisevaluation is conducted on sequences of varying lengths (10, 25, 50, 100, 200, and 300). To simplify, thesesequences are generated by repeating the same input (x) multiple times. Throughout these experiments, weutilized an NVIDIA GeForce GTX 1060 and recorded the peak memory allocation. The obtained resultsare visualized in Fig. B.2. These results distinctly highlight how the memory usage of BPTT scales linearlywith the number of time steps (T), while S-TLLR remains constant.",
  "C.1Effects of causality and non-causality factors using DFA": "In .1 of the main text, we examined the impact of introducing non-causal terms in the computationof the instantaneous eligibility trace (10) when using error-backpropagation (BP) to generate the learningsignal. In this section, we conduct a similar experiment, but this time, we employ direct feedback alignment(DFA) for the learning signal generation. As in .1, we vary the values of post, including 1, 0, and 1, to assess the effect of non-causalterms.Interestingly, when the learning signal is produced via random feedback with DFA, there is nosignificant difference observed when including non-causal terms or not. For example, in the RSNN model,using post = 1 yields slightly better performance, as depicted in Fig. C.1, but the difference is marginal.Similarly, for the VGG9 model, the performance of post = 1 and post = 0 is comparable and superior topost = 1, as shown in Fig. C.1. This suggests that a more precise learning signal, such as BP, may benecessary to fully exploit the benefits of non-causal terms.",
  ": GPU memory usage for BPTT and S-TLLR for a five-layer fully connected SNN models with adifferent number of time steps (T)": ":Evaluating the effect of post on DVS Gesture and SHD datasets using DFA for learning signalgeneration. Constant STDP parameters for DVS Gesture are (post, pre, pre) = (0.2, 0.75, 1), and for SHD,they are (post, pre, pre) = (0.5, 1, 1). The solid purple line represents the median value, and the dashedblack line represents the mean value averaged over five trials.",
  "C.2Effects of pre on the learning": "In this section, we explore the impact of using a pre value different from the leak parameter of the LIFmodel (1) on the S-TLLR eligibility trace (10). This parameter, which controls the decay factor of theinput trace, offers an opportunity to optimize the models performance. While previous works aiming toapproximate BPTT Xiao et al. (2022); Bellec et al. (2020); Bohnstingl et al. (2022) set pre equal to theleak factor (), our experiments, as depicted in Fig. C.1, suggest that using a slightly higher pre can leadto improved average accuracy performance.",
  "C.3Ablation studies on the secondary activation function ()": "In this section, we present ablation studies on the secondary activation function using the DVS Gesture,NCALTECH101, and SHD datasets, with the same configurations used for experiments on . Theresults are presented in , where it can be seen that independent of function using a non-zero postresult in better performance than using post = 0."
}