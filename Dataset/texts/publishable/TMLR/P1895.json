{
  "Abstract": "Deep Neural Networks (DNNs) have become an essential component in many applicationdomains, including web-based services. A variety of these services require high throughputand (close to) real-time features, for instance, to respond or react to users requests or to pro-cess a stream of incoming data on time. However, the trend in DNN design is towards largermodels with many layers and parameters to achieve more accurate results. Although thesemodels are often pre-trained, the computational complexity in such large models can still berelatively significant, hindering low inference latency. In this paper, we propose SelfXit , anend-to-end automated early exiting solution to improve the performance of DNN-based vi-sion services in terms of computational complexity and inference latency. SelfXit adopts theideas of self-distillation of DNN models and early exits specifically for vision applications.The proposed solution is an automated unsupervised early exiting mechanism that allowsearly exiting of a large model during inference time if the early exit model in one of the earlyexits is confident enough for final prediction. One of the main contributions of this paper isthat we have implemented the idea as an unsupervised early exiting, meaning that the earlyexit models do not need access to training data and perform solely based on the incomingdata at run-time, making it suitable for applications using pre-trained models. The resultsof our experiments on two vision tasks (image classification and object detection) show that,on average, early exiting can reduce the computational complexity of these services up to58% (in terms of FLOP count) and improve their inference latency up to 46% with a lowto zero reduction in accuracy.SelfXit also outperforms existing methods, particularly oncomplex models and larger datasets. It achieves a notable reduction in latency of 51.6% and30.4% on CIFAR100/Resnet50, with an accompanying increase in accuracy of 2.31% and0.72%, on average, compared to GATI and BranchyNet. We released code and replicationpackage at",
  "Introduction": "Deep Neural Networks (DNNs) are incorporated in real-world applications used by a wide spectrum ofindustry sectors including healthcare (Shorten et al., 2021; Fink et al., 2020), finance (Huang et al., 2020;Culkin & Das, 2017), self-driving vehicles (Swinney & Woods, 2021), and cybersecurity (Ferrag et al., 2020).These applications utilize DNNs in various fields such as computer vision (Hassaballah & Awad, 2020;Swinney & Woods, 2021), audio signal processing (Arakawa et al., 2019; Tashev & Mirsamadi, 2016),andnatural language processing (Otter et al., 2021).Many services in large companies such as Google andAmazon have DNN-based back-end software (e.g., Google Lens and Amazon Rekognition) with tremendousvolume of queries per second. For instance, Google processes over 99,000 searches every second (Mohsin,2022) and spends a substantial amount of computation power and time on the run time of their models (Xiang& Kim, 2019). These services are often time-sensitive and resource-intensive and require high availabilityand reliability. Now the question is how fast the current state-of-the-art (SOTA) DNN models are at inference time and towhat extent they can provide low-latency responses to queries. The SOTA model depends on the applicationdomain and the problem at hand. However, the trend in DNN design is indeed toward pre-trained large-scalemodels due to their reduced training cost (only fine-tuning) while providing dominating results (since theyare huge models trained on an extensive dataset). One of the downsides of large-scale models (pre-trained or not) is their high inference latency. Although theinference latency is usually negligible per instance, as discussed, a relatively slow inference can jeopardize aservices performance in terms of throughput when the QPS is high. In general, in a DNN-based software development and deployment pipeline, the inference stage is part of theso-called model serving process, which enables the model to serve inference requests or jobs (Xiang & Kim,2019) by directly loading the model in the process or by employing serving frameworks such as TensorFlowServing (Olston et al., 2017) or Clipper (Crankshaw et al., 2017). The inference phase is an expensive stage in the life cycle of a deep neural model in terms of time andcomputation costs (Desislavov et al., 2021).Therefore, efforts towards decreasing the inference cost inproduction have increased rapidly over the past few years. From a system engineering perspective, caching is a standard practice to improve the performance of softwaresystems, helping to avoid redundant computations.Caching is the process of storing recently observedinformation to be reused when needed in the future, rather than re-computation (Wessels, 2001; Maddah-Ali& Niesen, 2014). Caching is usually orthogonal to the underlying procedure, meaning that it is applied byobserving the inputs and outputs of the target procedure and does not engage with the internal computationsof the cached function. Caching effectiveness is best observed when the cached procedure often receives duplicated input while ina similar internal state, for instance, accessing a particular memory block, loading a web page, or fetchingthe books listed in a specific category in a library database. It is also possible to adopt a standard cachingapproach with DNNs (e.g., some work caches a DNNs output solely based on its input values (Crankshawet al., 2017)). However, it would most likely provide a meager improvement due to the high dimension andsize of the data (such as images, audios, texts) and low duplication among the requests. However, due to the feature extraction nature of deep neural networks, we can expect the inputs with similaroutputs (e.g., images of the same person or the same object) to have a pattern in the intermediate layersactivation values. Therefore, we exploit the opportunity to cache a DNNs output based on the intermediatelayer activation values. In this way, we can cache the results not by looking at the raw inputs but bylooking at their extracted features in the intermediate layers within the models forward-pass. The intermediate layers often have dimensions even higher than the input data. Therefore, we use shallowclassifiers (Kaya et al., 2019) to replace the classic cache-storing and look-up procedures. A shallow classifieris a supplementary model attached to an intermediate layer in the base model that uses the intermediatelayers activation values to infer a prediction. In the caching method, training a shallow classifier on a set",
  "of samples mimics the procedure of storing those samples in a cache storage, and inferring for a new sampleusing the shallow classifier mimics the look-up procedure": "In this paper, we propose early exiting the predictions made by standard classification models using shallowclassifiers trained using the samples and information collected at inference time.We first evaluate therationality of SelfXit in our first research question by measuring how it affects the final accuracy of thegiven base models and assessing the effectiveness of the parameters we introduce (tolerance and confidencethresholds) as a knob to control the early exit certainty.We further evaluate the method in terms ofcomputational complexity and inference latency improvements in the second and third research questions.We measure these improvements by comparing the FLOPs count, memory consumption, and inferencelatency for the original model vs. the early exit-enabled version that we built throughout this experiment.We observed a reduction of up to 58% in FLOPs, an acceleration of up to 46% in inference latency whileinferring on the CPU and up to 18% on GPU, with less than 2% drop in accuracy. SelfXit demonstratesremarkable performance across a range of models and datasets. By averaging across all confidence levelsgreater than 0.25, for the simplest model and dataset, CIFAR10-resnet50, it offers a substantial reductionof 52.2% and 32.4% in latency, with only a minor decrease of 6.9% and 1.6% in accuracy compared to theBranchyNet and GATI methods, respectively. Furthermore, in the case of the more complex CIFAR100-Resnet50 model and dataset, our method achieves a significant reduction in latency of 51.6% and 30.4%,while simultaneously enhancing the accuracy by 2.31% and 0.72% compared to the GATI and BranchyNetmethods.",
  "Inference Optimization": "There are two perspectives that address the model inference optimization problem. The first perspectivefocuses on optimizing the model deployment platform and covers a broad range of optimization goals (Yuet al., 2021).These studies often target deployment environments in resource-constrained edge devices(Liu et al., 2021; Zhao et al., 2018) or resourceful cloud-based devices (Li et al., 2020a). Others focus onhardware-specific optimizations (Zhu & Jiang, 2018) and inference job scheduling (Wu et al., 2020). The second perspective is focused on minimizing the models inference compute requirements by compressingthe model. Among model compression techniques, model pruning (Han et al., 2015; Zhang et al., 2018; Liuet al., 2019b), model quantization (Courbariaux et al., 2015; Rastegari et al., 2016; Nagel et al., 2019),and model distillation (Bucila et al., 2006; Polino et al., 2018; Hinton et al., 2015) are widely used. Theseideas alleviate the models computational complexity by pruning the weights, computing the floating-pointcalculations at lower precision, and distilling the knowledge from a teacher (more complex) model into astudent (less complex) model, respectively. These techniques modify the original model and often cause a",
  "Published in Transactions on Machine Learning Research (10/2024)": "We aim for end-to-end automation in the tool. However, currently, the user still needs to manually exportthe architecture specifications when using the NAS module and convert them to a proper Python imple-mentation (i.e., a PyTorch module implementing the architecture). The specifications are available to theuser through the experiments web GUI and also in the trial output files. This shortcoming is due to theNNI implementation, which does not currently provide access to the model objects within the experiments.We have created an enhancement suggestion on the NNI repository to support model object access (issue#4910).",
  "Early-Exits in DNNs": "Early exit generally refers to an alternative path in a DNN model which can be taken by a sample insteadof proceeding to the next layers of the model. Many previous works have used the concept of early exitfor different purposes (Xiao et al., 2021; Scardapane et al., 2020; Matsubara et al., 2022; Haseena Rahmathet al., 2023). Panda et al. (2016) is one of the early works in this area. They tried to terminate classificationby cascading a linear network of output neurons for each convolutional layer and monitoring the output ofthe linear network to decide about the difficulty of input instances and conditionally activate the deeperlayers of the network. But they have not mentioned anything about the inference time and accuracy/timetrade-off issue. BranchyNet (Teerapittayanon et al., 2016; Pacheco et al., 2021; Ebrahimi et al., 2022) alsoutilize their previous observation that features were learned in an early layer of a network to make an earlyexit.However, they require labeled data to train their models, rendering them unsuitable for use withunlabeled data. Shallow Deep Networks (SDN) (Kaya et al., 2019) points out the overthinking problem in deep neural networks.Overthinking refers to models spending a fixed amount of computationalresources for any query sample, regardless of their complexity (i.e., how deep the neural network should beto infer the correct prediction for the sample). Their research proposes attaching shallow classifiers to theintermediate layers in the model to form the early exits. Each shallow classifier in SDN provides a predictionbased on the values of the intermediate layer to which it is attached. (Hanawal et al., 2022) propose anunsupervised early exit method specifically designed for NLP tasks using Elastic BERT. Their approachutilizes a bandit-based learning algorithm (UEE-UCB) to dynamically select exit points in multi-exit DNNswithout requiring labeled data. However, the exit points are selected from a fixed set of candidate exits,which may not be optimal in all scenarios. In addition, exit models are kept as simple classifiers, limitingtheir overall performance. This approach is primarily tailored to BERT-based architectures, making it lessapplicable to more complex domains, such as vision-based tasks, which are the focus of our work. On the other hand, Xiao et al. (2021) incorporates the shallow classifiers to obtain multiple predictions foreach sample. In their method, they use early exits as an ensemble of models to increase the base modelsaccuracy. The functionality of the shallow classifiers in our proposed method is similar to that of SDN. However,the SDN method trains the shallow classifier using ground truth data from the training set and ignores theavailable knowledge in the original model. This constraint renders the proposed method useless when using apre-trained model without access to the original training data, which is commonly the case for practitioners.",
  "DNN Distillation and Self-distillation": "Among machine learning tasks, the classification category is one of the significant use cases where DNNshave been successful in recent years. Classification is applied to a wide range of data, such as classificationof images (Bharadi et al., 2017; Xia et al., 2021), text (Varghese et al., 2020), audio (Lee et al., 2009), andtime series (Zheng et al., 2014).",
  "DNN Prediction Early Exiting": "Clipper (Crankshaw et al., 2017) is a serving framework that incorporates early exiting DNNs predictionsbased on their inputs. Freeze inference (Kumar et al., 2019) investigates the use of traditional ML modelssuch as K-NN and K-Means to predict based on intermediate layer values. They show that the size andcomputation complexity of those ML models grows proportionally with the number of available samples, andtheir computational overheads far exceed any improvement. In the Learned Early Exits, Balasubramanianet al. (2021) extend the Freeze Inference by replacing the ML models with a pair of DNN models. A predictormodel that predicts the results and a binary classifier that predicts whether the result should be used as thefinal prediction. Their method uses ground-truth data in the process of training the predictor and selectormodels. In contrast, our method 1) only uses unlabeled inference data, 2) automates the process of earlyexit-enabling, 3) uses a confidence-based early exit hit determination, and 4) handles batch processing bybatch shrinking.",
  "Methodology": "In this section, we explain the method to convert a pre-trained deep neural model (which we call thebackbone) to its extended version with our early exiting method (called early exit-enabled model). Theearly exiting method adds one or more early-exit paths to the backbone, controlled by the shallow classifiers(which we call the early exit models), allowing the model to infer a decision faster at run-time for some testdata samples (early exit hits). Faster decisions for some queries will result in a reduced mean response time. Early Exit model is a supplementary model that we attach to an intermediate layer in the backbone, whichgiven the layers values provides a prediction (along with a confidence value) for the backbones output. Justa reminder that as our principal motivation, we assume that the original training data is unavailable for the",
  "A step-by-step guide on early exit-enabling an off-the-shelf pre-trained model from a user perspective containsthe following steps:": "1. Identify the candidate layers to be early exit This step involves analyzing each selected modelto identify potential positions for early exit layers. Layers whose outputs are independent of otherlayers states are chosen as candidates, enabling training possibilities. 2. Build an early exit model for each candidate Using neural architecture search (NAS), weevaluate all possible subsets for these early exit layers. NAS also scores the different architecturesdefined for our early exit models. 3. Assign confidence thresholds to built models to determine early exit hits Each early exitmodel features a softmax as the final layer. In this step, we assign confidence thresholds to theconstructed models to determine early exit hits. The probability value associated with the predictedoutput reflects the models confidence in its prediction.This confidence level for a given inputdetermines whether we accept that prediction as an early exit hit or continue processing throughthe rest of the backbone model. 4. Evaluate and optimize the early exit-enabled model In this step, we evaluate and optimizethe early exit-enabled model in the inference mode and also present our algorithm to update allearly exit layers. 5. Early-Exit Optimization Implementation In this step, we implement the algorithm and trainall early exit models using self-training (without using any ground truth) in the datasets we havementioned.",
  "Identifying candidate layers": "Choosing which layers to early exit is the first step towards early exit-enabling a model. A candidate layeris a layer that we will examine its values correlation to the final predictions by training an early exit modelbased on them. One can simply list all the layers in the backbone as candidates. However, since we launch asearch for an early exit model per candidate layer in the next step, we suggest narrowing the list by filteringout some layers with the following criteria:",
  "Some layers are disabled at inference time, such as dropouts and batch normalizations. These layersdo not modify their input values at inference time. Therefore, we cross them off the candidate list": "Some of the last layers in the model (close to the output layer, such as L15 in ) might not bevaluable candidates for early exiting, since the remaining layers might not have heavy computationsto reach the output. This is particularly important as each early exit model typically consists of atleast two layers. If the main path has only two layers remaining before the final result and softmax,adding an early exit at that point would not significantly improve inference time. Therefore, wediscard the last two layers (e.g., L15 and L16) from consideration. DNN models usually are composed of multiple components (i.e. first-level modules) consisting ofmultiple layers such as multiple residual blocks in ResNet models He et al. (2016)). We narrowdown the search space to the outputs layers in those components. We only consider the layers, for which, given their activation values, the backbones output isuniquely determined without any other layers state involved (i.e., the backbones output is a functionof the layers output). In other words, a layer with other layers or connections in parallel (such asL7-L11 and L13 in the ) is not suitable for early exiting, since the backbones output does notsolely depend on the layers output. While this approach has proven effective for the architectures wetested, we acknowledge that it may not generalize well to more complex architectures like Mixtureof Experts (MoE) models, where parallel experts are common. We have added this as a limitationof our method and suggest future exploration in this area.",
  "Building early exit models": "Building an early exit model to be associated with an intermediate layer in the backbone consists of findinga suitable architecture for the early exit model and training the model with that architecture. The detailsof the architecture search (search space, search method, and evaluation method) and the training procedure(training data extraction and loss function) are discussed in the following two subsections.",
  "Early Exit models architecture": "An early exit model can have an architecture of any depth and breadth size, as long as it provides morecomputational improvement than its overhead. In other words, it must have substantially less complexity(i.e., number of parameters and connections) than the rest of the layers in the backbone that come afterthe corresponding intermediate layer. The search space for such models would contain architectures withdifferent numbers and types of layers (e.g., a stack of dense and/or convolution layers). However, all modelsin the search space must produce a PD identical to the output of the backbone in terms of size (i.e, thenumber of classes) and activation (e.g., SoftMax or LogSoftMax). In our experiments, the search space consists of architectures with a stack of (up to 2) convolution layersfollowed by another stack of (up to 2) linear layers, with multiple choices of kernel and stride sizes for theconvolutions and neuron counts for the linear layers. However, users can modify or expand the search spaceaccording to their specific needs and budget. The objective of the search is to find a minimal architecture that converges and predicts the backbonesoutput with acceptable accuracy. Note that any accuracy given by an early exit model (better than random)can be helpful, as we will have a proper selection mechanism later in the process to only use the early exitpredictions that are (most likely) correct, and also to discard the early exit models yielding low computationalimprovement. The user can conduct the search by empirically sampling through the search space or by using a automatedNeural Architecture Search (NAS) tool such as Auto-Keras (Jin et al., 2019), Auto-PyTorch (Zimmer et al.,2021), Neural Network Intelligence (NNI) (Microsoft, 2021), or NASLib (Ruchte et al., 2020). However, weused NNI to conduct the search and customized the evaluation process to account for the models accuracyand their computational complexity.We have used the floating point operations (FLOPs) count as theestimation for the models computational complexity in this stage. Several factors influence the architecture of an early exit model for a given intermediate layer.Thesefactors include the dimensions of the intermediate target layer, its position in the backbone, and the data setspecifications, such as its number of target classes. For example, the first early exit models in the CIFAR100-Resnet50 and CIFAR10-Resnet18 experiments (shown as Cache 1 in ) have the same input size, butsince CIFAR100 has more target classes, it reasonably requires an early exit model with more learningcapacity. The architecture of each early exit model is further adjusted based on the specific requirements ofthe dataset, particularly the number of output classes. For example, while the early exit models for CIFAR10and CIFAR100 share similar input sizes, the early exit models for CIFAR100 are designed with increasedcapacity to handle its larger number of classes. This adaptation is guided by the suggestions provided by theNeural Architecture Search (NAS), ensuring that each early exit model is optimized for the dataset to whichit is applied. This approach is consistent across different datasets, such as ImageNet, where the early exitmodels are similarly adjusted to accommodate the 1,000 output classes. These architectural adjustmentsensure that the models remain efficient and capable of handling the varying complexities of different datasets.Therefore, using NAS to design the early exit models helps automate the process and alleviate deep learningexpert supervision in designing the early exit models. NAS tries to minimize the total accuracy no more than the tolerance, so for all possible subsets, there willbe a maximum range. This method does not guarantee the best score, but is a good solution. Subsets withthe best score will be selected as our early exit system which will be added inside the model to be trainedindividually by the predictions and perform early exits.",
  "Training an early exit model": "Figure (1) illustrates the schema of an early exit-enabled model consisting of the backbone (the dashed box)and the associated early exit models. The objective of an early exit model is to predict the output of thebackbone model, given the corresponding intermediate layers output, per input sample. Similarly to the backbone, early exit models are classification models. However, their inputs are the activationvalues in the intermediate layers. As suggested in self-distillation (Zhang et al., 2021), training an early exitmodel is essentially similar to distilling the knowledge from the backbone (final classifier) into the early exitmodel. Therefore, to distill the knowledge from the backbone into the early exit models, we need a medial dataset (MD) based on the collected inference data (ID). The medial data set for training an early exit modelassociated with an intermediate layer L in the backbone B consists of the set of activation values in the layerL and the PDs given by B per samples in the given ID, formally annotated as follows:",
  "where:": "MDL: Medial dataset for the early exit model associated with the layer LID: The collected inference data consisting of unlabeled samplesBL(i): Activation values in layer L given the sample i to the backbone BB(i) : The backbones PD output for the sample i Note that the labels in MDs are the backbones outputs and not the GT labels, as we assumed the GTlabels to be unavailable. We divide MDL into three partitions (MDT rainL, MDV alL, MDT estL) and use them,respectively, similarly to the common deep learning training and test practices. Similarly to the distillation method (Hinton et al., 2015), we use the Kullback-Leibler divergence (KLDiv)(Lovric, 2011) loss function in the training procedure. KLDiv measures how different the two given PDsare. Thus, minimizing the value of the KLDiv loss in MDT rainLtrains the early exit model to estimate theprediction of the backbone (B(i)). Unlike self-distillation, where (Zhang et al., 2021) train the backbone and shallow classifiers simultaneously,in our method, while training an early exit model, it is crucial to freeze the rest of the model including thebackbone and the other early exit models (if any) in the collection, to ensure that the training process doesnot modify any parameter not belonging to the current early exit model.",
  "Assigning confidence threshold": "The probability value associated with the predicted class (the one with the highest probability) is known asthe confidence of the model in the prediction. The early exit models prediction confidence for a particularinput will indicate whether we stick with that prediction (early exit hit) or proceed with the rest of thebackbone to the next or probably final exit (early exit miss). Confidence calibration means improving the model to provide accurate confidence.In other words, theconfidence of a well-calibrated model accurately represents the likelihood for that prediction to be correct(Guoet al. (2017)). An overconfident early exit model will lead the model to prematurely exit for some samplesbased on incorrect predictions, whereas an underconfident early exit model will bear a low early exit hitrate. Therefore, after building an early exit model, we also calibrate its confidence using MDV alLto better",
  "B CXXX": "distinguish the predictions that are more likely to be correct. Several confidence calibration methods arediscussed in Guo et al. (2017), among which temperature scaling (in the output layer) has been shown to bepractical and easy to implement. Having the model calibrated, we next assign a confidence threshold value to the model which will be used atinference time to determine the early exit hits and misses. When an early exit model identifies an early exithit, its prediction is considered the final prediction. However, when needed for validation and test purposes,we obtain the predictions from the early exit model and the backbone. Confidence calibration involves adjusting the predictive confidence of our early exit models to more accuratelyreflect their true performance, particularly how likely they are to be correct. This is crucial for makingreliable decisions during inference, especially when early exits from the model are considered based onthese confidence scores. To calibrate the confidence levels of our early exit models, we employ a thresholdthat measures confidence based on the model output during the validation phase. Specifically, each earlyexit model functions as a classifier: during both the validation and testing phases, it generates an outputthat carries an associated confidence value. This confidence value indicates the probability that the modelprediction is correct. An early exit models prediction (C) for an input to the backbone falls into one of the 5 correctness categorieslisted in table 1 with respect to the ground truth labels (GT) and the backbones prediction (B) for theinput. Among the cases where the early exit model and the backbone disagree, the BC predictions negatively affectthe final accuracy, and on the other hand, the BC predictions positively affect the final accuracy. Equation2 formulates an actual effect of an early exit model on the final accuracy.",
  ": The early exit modelF: The actual accuracy effect causes given as thresholdBC : Ratio of BC predictions by given as thresholdBC : Ratio of BC predictions by given as threshold": "However, since we used the unlabeled inference data to form the MDs, we can only estimate an upper boundfor the effect of the early exit model in the final accuracy. The estimation assumes that an incorrect earlyexit would always lead to an incorrect classification of the sample (BC). We estimate the change in theaccuracy upper bound an early exit model causes given a certain confidence threshold by its hit rate andearly exit accuracy:",
  ": The early exit modelF: The expected accuracy drop causes given as thresholdHR : Hit rate provided by given as thresholdCA : Early Exit accuracy provided by given as threshold": "Given the tolerance T for the drop in final accuracy, we assign a confidence threshold to each early exitmodel that yields no more than the expected drop in accuracy T/2n% in MDV alLaccording to equation 3,where n is the 1-based index of the early exit model in the setup. It is important to note that there are alternative methods to distribute the accuracy drop budget amongthe early exit models. For example, one can equally distribute the budget. However, as we show in theevaluations later in .6.1, we find it reasonable to assign more budget to the early exit modelsshallower positions in the backbone.",
  "Evaluation and optimization of the early exit-enabled model": "So far, we have a set of early exit layers and their corresponding early exit models ready for deployment. Thealgorithm 1 demonstrates a pseudoimplementation of a Python-style inference process for the model enabledfor early exit. When the early exit-enabled model receives a batch of samples, it proceeds layer-by-layersimilarly to the standard forward-pass. Once an early exit layers activation values are available, it will passthe values to the corresponding early exit model and obtain an early prediction with a confidence value persample in the batch. For each sample, if the corresponding confidence value exceeds the specified threshold,we consider it an early exit hit. Hence, we have the final prediction for the sample without passing it throughthe rest of the backbone. At this point, the prediction can be sent to the procedure awaiting the results(e.g., an API, a socket connection, or a callback). We shrink the batch by discarding the early exit hit itemsat each exit and proceed with a smaller batch to the next (or the final) exit.",
  ": end procedure": "So far in the method, we have only evaluated the early exit models individually, but to gain the highestimprovement, we must also evaluate their collaborative performance within the early exit-enabled model.Once the early exit-enabled model is deployed, each early exit model affects the hit rates of the following earlyexit models by narrowing the set of samples for which they will infer. More specifically, even if an early exitmodel shows promising hit rate and accuracy in individual evaluation, its performance in the deployment can",
  "The score equation accounts for both the improvement an early exit model provides through its early exithits within the subset and the overhead it produces for its early exit misses": "Using additional early exit layers may cause earlier detection of a class with a higher probability (with theuse of an appropriate confidence for early exit layers) before moving to the next layers, so the total accuracywould increase with the cost of memory consumption. The final schemas after applying the method on MobileFaceNet, EfficientNet, ResNet18, and ResNet50 arediscussed in the main text, with detailed illustrations provided in the Appendix (see ). This figuredemonstrates the chosen subsets and their associated early exit models for each backbone and data set.",
  "Early-Exit Optimization Implementation": "In this section, we present the implementation and application of early exit models for efficient and timelypredictions in three distinct tasks: image classification, object detection, and recommendation systems. Webegin by incorporating our proposed early exit approach into architectures widely used for image classificationtasks, i.e. MobileFaceNet, EfficientNet, ResNet18, and ResNet50, on benchmark datasets, i.e. CIFAR10,CIFAR100, ImageNet and LFW. Inspired by the promising results obtained in image classification, we further extend our methodology toaddress a critical real-world scenario: pedestrian detection in urban environments. For this purpose, weadopt the state-of-the-art Mask R-CNN model, renowned for its exceptional object detection capabilities.",
  "Updating Early Exits for Pedestrian Detection": "In object detection, pedestrians are one of the most common classes. To optimize the performance of ourearly exit framework for pedestrian detection, we explore three update strategies for the layers. Updating with the most confident pedestrians: In this approach, we selectively update the layerswith features extracted from regions that are confidently classified as pedestrians. By focusing on themost confident detections, we aim to enhance the early exit memorys relevance to crucial featuresassociated with pedestrians in the scene. Updating with the most confident object: We investigate updating the layers with features fromregions classified as the most confident object, regardless of whether it is a person or another class.This strategy is designed to ensure that the early exit memory reflects critical features representativeof the dominant object class in the scene. Updating with all detected objects: In this method, we update the layers with features from alldetected objects in the scene. While this approach may provide a broader context, it may introduceredundancy and bias towards the more prevalent classes. After testing these three early exit-updating approaches, the results supported updating with the mostconfident single object as the best-performing method.Training early exit layers with a sole focus onindividual objects, such as pedestrians, leads to non-convergence and a lack of meaningful learning. Even",
  "To address the above objective, we designed the following research questions (RQ):": "RQ1 To what extent can early exit models accurately predict the backbones output and the groundtruth data? This RQ investigates the core idea of early exiting as a mechanism to estimate the finaloutput earlier in the model. The assessments in this RQ consider the early exit models accuracyin predicting the backbones output (early exit accuracy) and predicting the correct labels (GTaccuracy). RQ2 To what extent can early exit-enabling improve computing requirements? In this RQ, we are in-terested in how early exit-enabling affects the models computation requirements. In these mea-surements, we measure the FLOPs counts and memory usage as metrics for the models computeconsumption. RQ3 How much acceleration does early exit enable on the CPU / GPU? In this RQ, we are interested inthe actual amount of end-to-end speedup that an early exit enabled model can achieve. We breakthis result down to CPU and GPU accelerations, since they address different types of computationduring the inference phase and thus may have been differently affected. RQ4 How does the early exit-enabled models accuracy/latency trade-off compare with other early exitmethods?In this research question, our aim is to assess and compare the performance of yourearly exit-enabled model against other existing early exit methods regarding the trade-off betweenaccuracy and latency in practical, real-world scenarios.",
  "Tasks and datasets": "Among the diverse set of real-world classification tasks that are implemented by solutions using DNN mod-els, we have selected two representatives: face recognition and object classification. Both tasks are quitecommonly addressed by DNNs and often used in large-scale services that have non-functional requirementssuch as: high throughput (due to the nature of the service and the large volume of input data) and aretime-sensitive. Face recognition models are originally trained on larger datasets such as MS-Celeb-1M (Guo et al., 2016) andare usually tested with different and smaller datasets such as LFW (Huang et al., 2008), CPLFW (Zhenget al., 2017), RFW (Wang et al., 2019), AgeDB30 (Moschoglou et al., 2017), and MegaFace (Kemelmacher-Shlizerman et al., 2016) to test the models against specific challenges, such as age / ethnic biases andrecognizing mask-covered faces. We used the labeled face in the wild (LFW) dataset for face recognition, which contains 13,233 images of5,749 individuals. We used images from 127 identities that have at least 11 images in the set so we can splitthem for training, validation, and testing.",
  "The proposed early exit enabler method is applicable to any deep classifier model. However, the results willvary for different models depending on their complexity": "Among the available face recognition models, we have chosen the well-known MobileFaceNet and EfficientNetmodels to evaluate the method, and we experiment with ResNet18 and ResNet50 for object classification. The object classification models are typical classifier models out-of-the-box. However, face recognition modelsare feature extractors that provide embedding vectors for each image based on the face / location features.They can still be used to classify a face-identity dataset. Therefore, we attached a classifier block to thesemodels and trained them (with the feature extractor layers frozen) to classify the images of the 127 identitieswith the highest number of images in the LFW dataset (above 10). It is important to note that since theadded classifier block is a part of the pre-trained model under study, we discarded the data portion usedto train the classifier block to ensure we still hold on to the constraint of working with pre-trained modelswithout access to the original training dataset. As stated previously, our pedestrian detection approach required the selection of an object detection techniquecapable of identifying pedestrians within images. We adopted the Mask R-CNN framework. This methodencompasses a backbone component (for which we employed ResNet50) and two additional sections thatconsume significant time and memory resources.",
  "Metrics and measurements": "Our evaluation metrics for RQ1 are ground truth (GT) accuracy and early exit accuracy. Early exit accuracymeasures how accurately an early exit model predicts the backbones output (regardless of correctness). TheGT accuracy applies to both the early exit-enabled model and each individual early exit model. However,early exit accuracy only applies to early exit models. Individual early-exit layers accuracies might be differentfrom the final early-exit-enabled models accuracy. This discrepancy arises because layer-wise accuracies arecalculated only for the cases where the input is classified at that specific layer (hit cases), which typicallyresults in higher accuracies. For example, of 5000 inputs, a particular layer may serve as an early exit foronly 5 inputs at a confidence level of 99%. Although this layer may exhibit high accuracy in these cases, itwould have only a slight impact on the overall final accuracy of the model. In RQ2, we compare the original models and their early exit-enabled version in terms of the average FLOPcount occurring for inference and their memory usage. We only measure the resources used in the inference.Specifically, we exclude the training-specific layers (e.g. batch normalization and dropout) and computations(e.g. gradient operations) in the analysis. The FLOP count takes into account the model architecture and input size and estimates the computationsrequired by the model to infer the input (Desislavov et al., 2021). In other words, the fewer FLOPs used forinference, the more efficient the model is in terms of compute and energy consumption. On the other hand, we report two aspects of memory usage for the models. The first is the total space usedto load the models in the memory (i.e, model size). This metric is essentially agnostic to the performance ofearly exit models and only considers the memory cost of loading them along with the backbone. In addition to the memory required for their weights, DNNs also allocate a sizeable amount of temporarymemory for buffers (also referred to as tensors) that correspond to intermediate results produced during theevaluation of the DNNs layers (Levental, 2022). Therefore, our second metric is the live tensor memoryallocations (LTMAs) during inference.LTMA measures the total memory allocated to load, move, andtransform the input tensor through the models layers to form the output tensor while executing the model. In RQ3, we compare the average inference latency of the original model with its early exit-enabled counter-part. Inference latency measures the time spent from passing the input to the model till it exits the model (byeither an early-exit or the final classifier in the backbone). Various factors affect inference latency includinghardware-specific optimizations (e.g., asynchronous computation), framework, and model implementation.In our measurements, the framework and model implementations are fixed as discussed in the AppendixA.1. However, to account for other factors, we repeat each measurement 100 times and report the averageinference latency recorded for each experiment. Further, to also account for the effects of asynchronouscomputations in GPU inference latency, we repeated experiments with different batch sizes.",
  "Baselines": "In this subsection, we discuss the baselines used to evaluate the proposed unsupervised early exit method.Specifically, we consider two prominent methods: BranchyNet (Teerapittayanon et al., 2016) and GATI(Balasubramanian et al., 2021), both of which aim to reduce the latency of deep neural network (DNN)inference while maintaining high accuracy. These baselines typically require access to labeled training data,which contrasts with our unsupervised approach.BranchyNet is designed to improve the efficiency of DNN inference by allowing certain samples to exit thenetwork early through additional side branch classifiers. These branches are strategically placed at variousdepths in the network to enable predictions at different levels of abstraction. During the training phase,BranchyNet jointly optimizes the loss functions of both the main branch and the side branches, effectivelyregularizing the network and mitigating vanishing gradients.The inference phase utilizes entropy-basedthresholds to determine whether a sample should exit early or continue through the deeper layers of thenetwork. In our implementation of BranchyNet as a baseline, we maintained the original supervised setup,but we adapted the training process to ensure a fair comparison. Specifically, we used the same data fortraining the early exit layers across all methods, which was a portion of the test data set (the other half wasnot used for evaluation). Furthermore, we froze the base model during the training of the side branches, acrucial difference from the original BranchyNet, which does not freeze the base model. For evaluation, weapplied BranchyNet to the remaining half of the test data set reserved for evaluation, allowing us to directlycompare its performance against our unsupervised method.GATI introduces a learned caching mechanism to accelerate DNN inference by exploiting temporal locality inprediction-serving workloads. GATI caches the hidden layer output of the DNN, enabling faster predictionsfor inputs that exhibit characteristics similar to previously processed data.The system employs simplemachine learning models as learned caches, which are continuously updated based on incoming data. UnlikeBranchyNet, GATI does not modify the DNN architecture but instead optimizes the inference process bydynamically skipping layers when a cache hit is detected. In our evaluation of GATI, we used a pre-trainedbase model and trained the cache layers using the same portion of the test dataset as other methods, ensuringconsistent data usage. Similarly to BranchyNet, we froze the base model during the training of the caches,differing from GATIs original approach, which does not freeze the base model. While both baselines rely onsupervised training, our method is designed to function in an unsupervised manner, particularly in scenarioswhere a base pre-trained model is accessible, but labeled data may not be available. This flexibility allowsour approach to be deployed in contexts where labeled data is scarce or unavailable, leveraging the testset itself for training early exits. To ensure a fair comparison with our method, we adapted both baselinesto this framework by using the same data for training and freezing the base model during the training ofearly-exit layers, which aligns with our approach. However, it is important to note that these baselines donot inherently support unsupervised training, which is a key contribution of our work. An important pointto note is that our Base model represents the upper bound of our work, as it is built on a pre-trained model.Our unsupervised method aims to make early-exit results resemble the Base models predictions rather thanconverging to the ground truth labels. Consequently, our upper bound is consistently the Base model, whileother methods may exhibit varying test accuracies due to additional training and reliance on labeled data.However, this does not necessarily apply to other baselines, such as BranchyNet and GATI, as they mayuse different approaches for training. Although our Base model demonstrates the best possible performancewithin the constraints of our approach, the same cannot be directly inferred for the other baselines, whichmay not achieve the same upper bound due to their reliance on labeled training data and other trainingtechniques.",
  "RQ1. How accurate are early exit models in predicting the backbone output and the groundtruth labels?": "In this RQ, we are interested in the performance of the built early exit models in terms of their hit rate,GT accuracy, and early exit accuracy. We break down the measurements into two parts. The first partcovers the individual performance of the early exit models over the whole test set without any other earlyexit model involved. The second part covers their collaborative performance within the early exit-enabledmodel.",
  "Early Exit models individual performance": "shows the individual performance of each early exit model against any confidence threshold value inthe CIFAR100-Resnet50 experiment. Figures demonstrating the same measurements for other experimentsare available in the Appendix C. We make three key observations here. First, deeper early exit models are more confident and accurate intheir predictions. For example, early exit 1 in has 33.36% GT accuracy and 35.74% early exitaccuracy, while these metrics increase to 78.60% and 95.38% for early exit 3, respectively. This observationagrees with the generally acknowledged feature extraction pattern in the DNNs deeper layers convey moredetailed information. The second key observation is the inverse correlation between the early-exit models accuracy (both GTand early-exit) and their hit rates. This observation highlights the reliability of confidence thresholds indistinguishing predictions that are more likely to be correct. For example, early exit 1 in , witha confidence threshold 20%, produces a hit rate of 35.24% but also a drop of 8.99% in the final accuracy.However, with a confidence threshold 60%, it produces a hit rate 4% and does not reduce the final accuracyby more than 0.1%. The third observation is that the early exit accuracy is higher than the GT accuracy in all cases. Thisdifference is because we have trained the early exit models to mimic the backbone only by observing itsactivation values in the intermediate layers and outputs.Since we have not assumed access to the GTlabels (which is the case for inference data collected at run-time) while training the early exit models, theyhave learned to make correct predictions only through predicting the backbones output, which might havebeen incorrect in the first place. On the other hand, we observed that the early exit models predict thecorrect labels for a portion of the samples for which the backbone misclassifies. For example, for 0.92%of the samples, early exit 3 (in ) correctly predicted the GT labels while the backbone failed (BCpredictions). This shows the potential of the early exit models to partially compensate for their incorrectearly exits (BC predictions) by correcting the backbone predictions for some samples (BC). This agreeswith the overthinking concept in SDN (as discussed in 2.3), since for this set of samples, the early exit modelshave been able to predict correctly in the shallower layers of the backbone.",
  "Early Exit models collaborative performance": "describes the collaborative performance of the early exit models within a confidence threshold of 0.9.In the table, we also report how each early exit models layer hits have affected the final accuracy. Here, we observe that while evaluating the early exit models on the subset of samples, which were missed bythe previous early exit models (the relatively more complex ones), the measured hit rate and GT accuracyare substantially lower compared to the evaluation on the whole dataset. This is in fact due to the fact thatthe simpler samples (less detailed and easier to classify) are resolved earlier in the model. More specifically,the hit rate decreases since the early exit models are less confident in their prediction for the more complexsamples, and the GT accuracy also decreases since the backbone is also less accurate for such samples.However, we observe that the early exit models still have high early exit accuracy, with a low impact onthe overall accuracy. This observation shows how the confidence-based early exiting method has effectivelyenabled the early exit models to provide early predictions and keep the overall accuracy drop within thegiven tolerance.",
  "shows the average amount of FLOPs computed for inference per sample. Here we observe thatshrinking the batches proportionally decreases the FLOPs count required for inference": "In addition, shows the memory used to load the models (i.e, the model size) and the total LTMAduring inference while inferring for the test set.As expected, the size of the early exit-enabled modelsis larger than the original model in all cases, since they include the backbone and the additional earlyexit models. However, the decrease in LTMA in all cases shows a reduced number of memory allocationsduring the inference time. Generally, a lower LTMA indicates smaller tensor dimensions (e.g., batch size,input, and operator dimensions) (Ren et al., 2021). However, in our case, since we do not change eitherof the dimensions, the lower LTMA is due to avoiding the computations in the remaining layers after earlyexit hits which require further memory allocations. A noteworthy observation from this table highlightsthe substantial memory usage of our object detection approach due to the sizeable model employed. Thisunderscores the notion that implementing early exiting for this purpose does not significantly amplify thememory requirements.",
  "Table (5) shows the average latency for the base models on CPU and GPU, vs. their early exit-enabledcounterparts, evaluated on the test set": "The first key observation here is the improvements on the CPU. This improvement is due to the low paral-lelism in the CPU architecture. Essentially, the CPU computation volume is proportional to the number ofsamples. Therefore, when a sample has an early exit, the remaining computation required to complete thetasks for the batch decreases proportionally. The FLOPs of all our early exit models, utilized by each input in the batch, are aggregated, resulting inlatency being dictated by the longest time observed in the batch. Consequently, a larger batch size tends toworsen latency, as it may lead to more frequent early exit misses. The second observation is the relatively lower latency improvement in the GPU. This observation showsthat shrinking a batch does not proportionally reduce the inference time on GPU, which is due to the highparallelism in the hardware. Shrinking the batch on GPU provides a certain overhead since it interruptsthe on-chip parallelism and hardware optimizations. This interruption forces the hardware to re-plan itscomputations, which can be time consuming. Thus, batch-scanning improvements can be insignificant onGPU. The third observation pertains to the time savings related to pedestrian detection, in contrast to theprimary model. This significant gain in efficiency is attributed to disregarding the additional layers of MaskR-CNN through our early exit strategy. further demonstrates how the batch size affects the improvement provided by early exiting. The keyobservation here is that increasing the batch size can negate the early exiting effect on the inference latency,",
  "ms1.71 ms13.68%322.05 ms1.84 ms16.75%642.19 ms1.98 ms9.21%1282.7 ms3.22 ms-19.43%": "which, as discussed, is due to fewer batches that are fully resolved through the early exit models and do notreach the last layers. In conclusion, the latency improvement here highly depends on the hardware used ininference and must be specifically analyzed per hardware environment and computation parameters such asbatch size. However, the method can still be useful when the model is not performing batch inferences (batchsize = 1). One can also use the tool and get the best prediction so far within the forward-pass process bydisabling batch shrinking. Doing so will generate multiple predictions per input sample, one per exit (earlyand final).",
  "Summary for RQ3": "Increasing batch size in early exit models can worsen latency due to more frequent early exit misses anddoes not proportionally reduce inference time on GPUs due to the disruption of hardware parallelism.In particular, significant time savings are achieved in pedestrian detection by bypassing extra layerswith an early exit strategy, although the benefits of early exiting on latency heavily depend on thespecific hardware and batch size used.",
  "RQ4. How does the early exit-enabled model accuracy / latency and computational costtrade-off compare with other early exit methods?": "In this RQ, we conducted a comprehensive evaluation of three distinct methods, including BranchyNet (Teer-apittayanon et al., 2016), GATI (Balasubramanian et al., 2021), and our proposed method in the same systemconfiguration. This evaluation included two different Resnets with two different data sets to thoroughly assessthe performance of each method. Figures 3 and 4 demonstrate a comprehensive analysis of the accuracy / latency / computational cost trade-off at various confidence levels for all methods evaluated for CIFAR10 and CIFAR100. Our findings reveal aremarkable adaptability of our proposed method to different confidence levels, particularly excelling in low-confidence scenarios. This adaptability underscores the effectiveness of our approachs training, showcasingits ability to perform exceptionally well in situations where traditional methods might falter. In contrast,as confidence levels increase, our method exhibits a slight latency increase, reflecting its ability to fine-tune confidence settings to suit various application requirements. This feature demonstrates the methodsadaptability and its potential for accommodating diverse use cases. presents the comparison of inference costs and accuracy across different methods (BranchyNet,GATI, and SelfXit ) using ResNet18 and ResNet50 models on CIFAR-10 and CIFAR-100 datasets.Thetable reports accuracy at various inference cost thresholds (25%, 50%, and 75%) relative to the originalmodels inference cost. The \"Max\" column indicates the highest accuracy achieved by each method withearly exits. This comparison highlights the trade-offs between inference cost and accuracy for the evaluatedmethods. This table demonstrates that our method has a better impact for more complex base models anddatasets.",
  ": CIFAR10 Accuracy/latency/computational cost comparison between the approaches with differentconfidence": ": Comparing the inference costs of Resnet18 and Resnet50 models on CIFAR-10 and CIFAR-100on different methods. 25%, 50%, and 75% report the early exit accuracy when we limit the averageinference cost to at most 25%, 50%, and 75% that of the original models of each method. Max reports thehighest accuracy early exits can achieve.",
  "BranchyNet65.769.572.374.1GATI66.266.869.773.3SelfXit67.670.073.979.4": "The primary observation is that our method exhibits the best latencies in various methods at differentconfidence levels. Another significant observation is that our methods performance scales positively withthe complexity of the model or dataset, leading to superior outcomes in terms of both accuracy and latency.As we add some lightweight layers for each model, it becomes evident that Resnet50 exhibits more pronounced",
  ": CIFAR100 Accuracy/latency/computational cost comparison between the approaches with differ-ent confidence": "improvements in our research. This observation is also shown in . By averaging across all confidencelevels greater than 0.25, For the simplest model and dataset, namely CIFAR10-Resnet18, we achieved asubstantial 52.2% and 32.4% reduction in latencies, with a modest 6.9% and 1.6% decrease in accuracy whencompared to the BranchyNet and GATI methods, respectively, while improving computational costs by 7.1%and 0.5%. In the case of a more complex model and dataset, such as CIFAR100-Resnet50, we achieved asubstantial reduction in latency of 51.6% and 30.4% while simultaneously improving the accuracy by 2.31%and 0.72% compared to the GATI and BranchyNet methods, respectively, while improving computationalcosts by 14.3% and 8.2%. BranchyNet, on the other hand, exhibits relatively consistent results at differentconfidence levels. Although this stability might be desirable in some scenarios, it lacks the adaptabilityand dynamic response that our method offers. Gati, while showing learned early exits, is marked by thecomplexity of its training process, which can be challenging to implement effectively.Moreover, GATIconsumes more memory resources during implementation, in contrast to the efficiency of SelfXit . Indeed, our method stands out as a dynamic and adaptable solution, requiring less extensive data and modelpreparation compared to approaches such as GATI. By completely freezing the base model during the trainingof early-exit layers, we achieve better results within limited training time, particularly in more complexmodels where other methods may require significantly more time to train effectively. The ability to configureconfidence levels further underscores its versatility for a broad spectrum of applications. However, it shouldbe noted that for simpler models and datasets, our method may not demonstrate a marked improvementover others, as they employ additional dense layers for early exiting, which can provide competitive results.Crucially, the most significant aspect of our work lies in the capability to update the early exit layerseffectively without reliance on ground-truth labels, aligning early-exit results closely with the base modelspredictions. This offers a substantial advantage in practical applications where such labels may not be readilyavailable.",
  "Summary for RQ4": "Our method demonstrates exceptional adaptability across different confidence levels, particularlyexcelling in low-confidence scenarios, which highlights its robust training and ability to fine-tunesettings for varied applications. It achieves the best latencies and scales positively with the complexityof the model or dataset, showing substantial improvements in both accuracy and computational costs,particularly with complex configurations like CIFAR100-Resnet50.",
  "Discussion, Limitations, and Future Directions": "The first limitation of this study is that the proposed method is limited to classification models, since itwould be more complicated for early exit models to predict the output of a regression model due to theircontinuous values. This limitation is strongly tied to the effectiveness of knowledge distillation in the caseof regression models. The method also does not take the internal state of the backbone (if any) into account, such as the hiddenstates in recurrent neural networks. Therefore, the effectiveness of the method for such models still needs tobe further assessed. Moreover, practitioners should take the underlying hardware and the backbone structure into account, asthey directly affect the final performance. On this note, as shown in .6.5, different models providedifferent performances in terms of inference latency in the first place; therefore, choosing the right model forthe task comes first, and early exiting can be helpful in improving the performance. As the main goal of our proposed approach is to decrease the inference time, depending on the applicationdomain, if a potential slight degradation of accuracy is not acceptable (e.g., in a safety critical system), usingour approach might not be a good fit. In dynamic environments where data distribution changes over time, maintaining the performance of earlyexit models may require periodic updates. Unlike conventional caching, early exit models cannot be up-dated in real-time. Therefore, retraining the early exit layers and readjusting confidence thresholds usingnewly collected inference samples become necessary to adapt to new data trends and maintain accuracy.While we did not perform these updates in our experiments, practitioners deploying this method shouldconsider triggers for updates, such as when a significant portion of new data has been collected or when thebackbone model is modified or retrained. Balancing the costs associated with retraining against applicationrequirements and resources is essential for effective maintenance. While our current comparison has yielded valuable results, we can explore the applicability of our approach toother large models, particularly in non-vision-based datasets, to assess its effectiveness in different domains.Given the growing importance of reducing latency and inference time, especially in Large Language Models(LLMs), our future research can focus on methods to further optimize and reduce costs for large-scaleindustries.",
  "Conclusion": "In this paper, we have shown that our automated early exiting approach is able to extend a pretrainedclassification DNN to an early exit-enabled version using a relatively small and unlabeled dataset. Therequired training data sets for early exiting models are collected just by recording the input items and theircorresponding backbone outputs at the inference time. We have also shown that the early exiting method canintroduce a significant improvement in the models computing requirements and inference latency, especiallywhen the inference is performed on the CPU.",
  "Maksim Levental. Memory planning for deep neural networks. ArXiv, abs/2203.00448, 2022": "Y. Li, Zhenhua Han, Quanlu Zhang, Zhenhua Li, and Haisheng Tan. Automating cloud deployment for deeplearning inference of real-time online services. IEEE INFOCOM 2020 - IEEE Conference on ComputerCommunications, pp. 16681677, 2020a. Yawei Li, Shuhang Gu, Kai Zhang, Luc Van Gool, and Radu Timofte. Dhp: Differentiable meta pruningvia hypernetworks. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August2328, 2020, Proceedings, Part VIII 16, pp. 608624. Springer, 2020b. Renju Liu, Luis Garcia, Zaoxing Liu, Botong Ou, and Mani B. Srivastava. Secdeep: Secure and performanton-device deep learning inference framework for mobile and iot devices. Proceedings of the InternationalConference on Internet-of-Things Design and Implementation, 2021. Zechun Liu, Haoyuan Mu, Xiangyu Zhang, Zichao Guo, Xin Yang, Kwang-Ting Cheng, and Jian Sun.Metapruning:Meta learning for automatic neural network channel pruning.In Proceedings of theIEEE/CVF international conference on computer vision, pp. 32963305, 2019a.",
  "Yecheng Xiang and Hyoseung Kim.Pipelined data-parallel cpu/gpu scheduling for multi-dnn real-timeinference. 2019 IEEE Real-Time Systems Symposium (RTSS), pp. 392405, 2019": "Yan Xiao, Ivan Beschastnikh, David S. Rosenblum, Changsheng Sun, Sebastian G. Elbaum, Yun Lin, andJin Song Dong. Self-checking deep neural networks in deployment. 2021 IEEE/ACM 43rd InternationalConference on Software Engineering (ICSE), pp. 372384, 2021. Fuxun Yu, Di Wang, Longfei Shangguan, Minjia Zhang, Xulong Tang, Chenchen Liu, and Xiang Chen. Asurvey of large-scale deep learning serving system optimization: Challenges and opportunities. ArXiv,abs/2111.14247, 2021. Linfeng Zhang, Chenglong Bao, and Kaisheng Ma. Self-distillation: Towards efficient and compact neuralnetworks.IEEE Transactions on Pattern Analysis and Machine Intelligence, PP:11, 03 2021.doi:10.1109/TPAMI.2021.3067100.",
  "A.1Implementation details": "We developed the early exit tool using PyTorch, which is accessible through the GitHub repository2. Thesoftware environment for our experiments included Ubuntu 20.04, Python 3.7, and PyTorch 1.1. shows the overall system design. The tool provides a NAS module, an optimizer module, and a deploymentmodule. The NAS module provides the architectures to be used per early exit model. The optimizer assignsthe confidence thresholds, finds the best subset of the early exit models, and provides evaluation reports.Lastly, the deployment module launches a Web server with the early exit-enabled model ready to servequeries.",
  "A.1.1NAS Module": "Existing NAS tools typically define different search spaces according to different tasks, which constrains theirapplicability to certain input types and sizes. Using such tools with input constraints defeats our methodsgeneralization and automation purpose, since the early exit models inputs can have any dimension and size.For example, ProxylessNAS (Cai et al., 2019) specializes in optimizing the performance of neural architecturefor a target hardware. However, it is only applicable for image classification tasks and requires certain inputspecifications (e.g., 3xHxW images normalized using given values). Similarly, Auto-PyTorch (Zimmer et al.,2021) and Auto-Keras are only applicable to tabular data sets, text, and images. We chose NNI from Microsoft (Microsoft, 2021) as it does not constrain the input of the model in termsof type, size, and dimensions.NNI also provides an extensible search space definition with support forvariable number of layers and nested choices (e.g., choosing among different layer types, each with differentlayer-specific parameters). Given the backbone implementation, the dataset and the search space, the module launches an NNI exper-iment per candidate layer to find an optimum early exit model for the layer. Each experiment launches aweb GUI for progress reports and results.",
  "A.1.2Optimizer and deployment modules": "Given the implementation of the backbone and the early exit models, the optimizer evaluates the early exitmodels, assigns their confidence thresholds, finds the best subset of the early exit models, and disables therest, and finally reports the relevant performance metrics for the early exit-enabled model and each earlyexit model. We used the DeepSpeed by Microsoft and the PyTorch profiler to profile the FLOP counts,memory usage, and latency values for the early exit models and the backbones. The user can use each module independently. Specifically, the user can skip the architecture search via theNAS module and provide the architectures manually to the optimizer, and the module trains them beforeproceeding to the evaluation.",
  "NAS with random search strategy can take approximately 1 hour for simpler models like ResNet18 and upto 3 hours for more complex models like ResNet50": "The tool also offers an extensive set of configurations. More specifically, the user can configure the tool touse one device (e.g., GPU) for training processes and the other (e.g., CPU) for evaluation and deployment. The deployment module launches a web server and exposes a WebSocket API to the early exit-enabledmodel. The query batches passed to the socket will receive one response per item, as soon as the predictionis available through either of the (early or final) exits.",
  "We used the backbone implementations and weights provided by the FaceX-Zoo (Wang et al., 2021) repositoryto conduct the experiments with LWF data set on the MobileFaceNet and EfficientNet models": "For experimenting with CIFAR10 and CIFAR100, we used the implementations provided by torchvision(Marcel & Rodriguez, 2010) and the weights provided by (Phan, 2021) and (Weiaicunzai, 2020). For Ima-geNet pre-trained weights for ResNets we used new torchvision version as well (maintainers & contributors,2016). All backbone implementations were modified to implement an interface that handles the interactionswith the early exit models, controls the exits (early exit hits and misses), and provides the relevant reportsand metrics. We document the use of the interface in the repository, so that users can experiment with newbackbones and datasets. We refer interested readers to a blog post on how to extract intermediate activationsin PyTorch (Bhaskhar, 2020) which introduces three methods to access the activation values. The introducedforward hooks method in PyTorch is very convenient for read-only purposes. However, our method requiresperforming actions based on the activation values, specifically, early exit lookup and batch shrinking, andavoiding further computation through the next layers. Therefore, we used the so-called hacker methodto access the activation values and perform these actions and provided the interface for easy replication ondifferent backbones.",
  "DAppendix: Ablation Study": "To investigate the need for using NAS in our work, we run an ablation study. The alternative to use NAS isinserting early exit models at every possible node, which can be memory-intensive and may even degrade thefinal performance and accuracy. Additionally, the number of early exit layers may also impact the results.Our default setup uses 2 dense layers per early exit model. This part of the ablation study assesses theimpact of increasing this number to 3, 4, and 5 dense layers. The following tables compare latency (GPUand CPU), accuracy, model size and FLOPs across these modes.",
  "Model TypeR18-C10R50-C10R18-C100R50-C100R18-INR50-INMFN-LFWEFN-LFWMRCNN-CSDLRM-Criteo": "With NAS10.11 ms14.62 ms9.39 ms9.02 ms30.23 ms38.74 ms16.91 ms27.98 ms562.3 ms7.67 msWithout NAS13.53 ms19.17 ms13.19 ms13.01 ms41.29 ms51.04 ms22.53 ms38.77 ms724.42 ms11.89 ms1 more layer11.87 ms18.12 ms12.19 ms12.12 ms33.31 ms45.41 ms21.25 ms34.68 ms689.11 ms10.63 ms2 more layers12.84 ms19.32 ms14.01 ms13.89 ms34.91 ms47.02 ms25.94 ms35.78 ms713.71 ms11.93 ms3 more layers13.17 ms22.12 ms16.71 ms15.89 ms36.06 ms48.68 ms26.41 ms37.52 ms743.71 ms13.31 ms having, for example, 10 early exit layers and processing all batches through them significantly increases thetime required and also enlarges the model size. In addition, memory consumption is negatively affected."
}