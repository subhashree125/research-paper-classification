{
  "Abstract": "Model merging offers an effective strategy to combine the strengths of multiple finetunedmodels into a unified model that preserves the specialized capabilities of each. Existingmethods merge models in a global manner, performing arithmetic operations across allmodel parameters. However, such global merging often leads to task interference, degradingthe performance of the merged model. In this work, we introduce Localize-and-Stitch,a novel approach that merges models in a localized way. Our algorithm works in two steps:i) Localization: identify tiny (1% of the total parameters) localized regions in the finetunedmodels containing essential skills for the downstream tasks, and ii) Stitching: reintegrate onlythese essential regions back into the pretrained model for task synergy. We demonstrate thatour approach effectively locates sparse regions responsible for finetuned performance, andthe localized regions could be treated as compact and interpretable representations of thefinetuned models (tasks). Empirically, we evaluate our method on various vision and languagebenchmarks, showing that it outperforms existing model merging methods under differentdata availability scenarios. Beyond strong empirical performance, our algorithm also facili-tates model compression and preserves pretrained knowledge, enabling flexible and continualskill composition from multiple finetuned models with minimal storage and computationaloverhead. Our code is available at",
  "Introduction": "Pretrained models (Devlin et al., 2018; Liu et al., 2019; Raffel et al., 2020; Radford et al., 2021) contain a wealthof rich and generalizable information, and finetuning these models for specific downstream tasks significantlyenhances performance compared to training from scratch (Chen et al., 2020b). With the growing popularityof the pretrain-finetune paradigm, a vast array of finetuned models have been made available on platformslike Hugging Face (Wolf et al., 2020), and many of them originate from the same pretrained models, such asCLIP (Radford et al., 2021). However, deploying multiple finetuned models independently, each for a differentdownstream task, incurs large storage and maintenance cost, and limits knowledge transfer across them.",
  "()": ": Localize-and-Stitch: Given n models {(i)ft }ni=1 finetuned from pre, we first localize regions containingskills acquired during finetuning through per-model binary masks {i}ni=1, then stitch the localized regions {i(i)ft }ni=1onto the pretrained model, where is the element-wise product. Empty nodes after the localization step mean thatthe mask is not activated at that position. Since the localized regions are tiny ( 1%), we reduce potential taskconflicts and make minimal changes to the pretrained model. Model merging offers a viable solution to these challenges by integrating the strengths of multiple finetunedmodels into a single model that retains the specialized capabilities of each. The key advantage of model mergingover traditional multi-task learning (MTL) (Caruana, 1997; Zhang & Yang, 2021; Hu et al., 2024; He et al., 2024)is its efficiency, in that it does not require joint training on data across all tasks, but only involves arithmeticoperations in the weight space. Existing methods merge models by averaging model parameters via arithmeticmean (Wortsman et al., 2022a; Ilharco et al., 2023), Fisher information (Matena & Raffel, 2022), regressionmean (Jin et al., 2022) or learned merging weights (Yang et al., 2023). Those methods all average the modelsin a global manner, meaning that they perform arithmetic operations to all parameters of the finetuned models.However, similar to the conflicting gradient problem in MTL (Yu et al., 2020; Liu et al., 2021), parametersin different finetuned models often have interference with each other, leading to suboptimal performanceof the merged model. Recent works find that redundant parameter updates in finetuning are sources ofconflicts (Yadav et al., 2023). Although the majority of model parameters are updated during finetuning, onlyvery few contribute to improving the performance on downstream tasks (Chen et al., 2020a; Hoefler et al., 2021). To overcome these limitations, we propose Localize-and-Stitch, an efficient algorithm that merges modelsin a localized manner. The algorithm () involves two steps: i) Localization: identify tiny localizedregions in the finetuned models containing essential skills for the downstream tasks. ii) Stitching: reintegrateonly these essential regions back into the pretrained model. In the experiments, we verify that the changes infinetuend parameters are highly redundant, as we can efficiently identify just 1% of the total parameters thatrecovers over 99% of the finetuned performance. We evaluate our method on various language and visiontasks, showing that it outperforms existing model merging methods under different data availability scenarios. Beyond the superior performance on model merging, our approach has several distinct advantages: i)Interpretability of task relations: each localized region encapsulates task-specific skills from thefinetuned models, and overlap among them is indicative of knowledge sharing. ii) Model compression:Our localization method enables compact representation of finetuned models, significantly reducing thestorage space to only 1% of the original without sacrificing performance. This enables flexible integrationof finetuned models capabilities with minimal storage and computational overhead. iii) Preservationof pretrained knowledge: By making minimal and localized changes to the pretrained model, our mergedmodel maintains its generalizability and achieves superior multi-task performance, effectively mitigatingcatastrophic forgetting associated with finetuning.",
  "Simple averaging (Wortsman et al., 2022a)merged = 1": "nni=1 (i)ftElement-wise meanTask arithmetic (TA) (Ilharco et al., 2023)merged = pre + ni=1 i tuned on a validation setFisher merging (Matena & Raffel, 2022)merged = ni=1 Fi(i)ft / ni=1 FiWeighted by Fisher information matricesRegMean (Jin et al., 2022)merged = (ni=1 Xi Xi)1 ni=1(Xi Xi(i)ft )Minimizes difference in merged and individual activations",
  "LocalizedTIES-Merging (Yadav et al., 2023)Trims the parameters in task vectors with small magnitudes, elect a sign at each positionof the task vector and only keep the parameters with the same sign": "Consensus TA/TIES (Wang et al., 2024b)Compute multi-task task vectors: MT L = merged pre with merged obtained by any merging method.Construct task masks: mt = 1{|t| |MT L t| t}.Apply consensus mask mconsensus = 1{t[T ] mt 2} on MT L. Task vectors. A task vector is the element-wise difference of the finetuned and pretrained parameters,denoted as i = (i)ft pre Rd. These vectors encapsulate the knowledge acquired during the finetuningprocess. This knowledge can be effectively manipulated through task arithmetic (Ilharco et al., 2023), whichinvolves performing arithmetic operations on task vectors to compose learned skills across tasks. Model merging. The goal of model merging is to efficiently aggregate the parameters of the n finetunedmodels into a single multi-task model merged without the need to retrain the model on the initial task-specificdata. The resulting merged model should perform well on all the tasks simultaneously. Existing methods perform merging in the general form merged = pre + ni=1 ii, and their difference mainlylies in the way of determining the scaling factors i. We introduce the baselines in , and categorizethem into global and localized methods based on whether the algorithm incorporates selection strategies toidentify which parameters to merge. Localized algorithms specifically target sparse and localized regions,while global algorithms merge parameters indiscriminately. We provide detailed comparisons with the twolocalized algorithms, in Appendix C and Appendix D respectively. Note that AdaMerging has two variants:one learns layer-wise weights and another learns task-wise weights. In this work, we refer to AdaMerging asthe layer-wise version because of its superior performance over its task-wise counterpart. Data requirements. Fisher merging (Matena & Raffel, 2022) requires over 256 data points per task to esti-mate Fisher information. RegMean (Jin et al., 2022) requires more than 1600 data points per task to computethe inner product matrices effectively. AdaMerging (Yang et al., 2023) needs access to the full unlabeled testset for entropy minimization. Consensus TA/TIES (Wang et al., 2024b) requires a validation set to tune hyper-parameters. In contrast, simple averaging (Wortsman et al., 2022a), task arithmetic (Ilharco et al., 2023) andTIES-Merging (Yadav et al., 2023) can be implemented without additional data. However, to achieve the bestperformance, both task arithmetic and TIES-Merging require tuning the hyperparameter on a validation set.",
  "Motivation and objectives": "Sparsity is important, but how to locate sparse regions is the key. Previous research identifies thatduring the finetuning stage, a significant portion of parameter updates is redundant, introducing interferencein model merging (Yadav et al., 2023). This underscores the need for locating sparse regions to reduce suchinterference. While the importance of sparsity is recognized, strategies for achieving it remain underexplored.Earlier approaches typically identify sparse regions through random selection (Yu et al., 2023) or selectingregions with the top-k% largest magnitudes in task vectors (Yadav et al., 2023). However, they often fallshort in identifying the most effective sparse regions for model merging. In , we evaluate the efficacyof different localization methods across twelve language tasks, comparing the quality of their localized regions(specified by the binary mask i). The performance is assessed on individual grafted models for each task,denoted as pre + i i, where i is the task vector of the i-th task and is the element-wise product.",
  "Published in Transactions on Machine Learning Research (12/2024)": "88 evaluations for 11 choices of on 8 tasks. In total, there are 128 task-specific evaluations required to selectthe two hyperparameters following Appendix A.2 in (Wang et al., 2024b), which we find very time-consumingin practice. In contrast, our algorithm does not require hyperparameter tuning. It operates by training on8-shot data for 10 epochs, a process we find highly efficient. Furthermore, hyperparameter tuning, particularly for scaling factors, has been shown to greatly influencemodel merging performance, making methods like Consensus TA sensitive to these choices as highlighted in(Tam et al., 2024). On the other hand, our method avoids these complexities, offering a more efficient androbust approach that enhances performance consistency.",
  "Localization": "Motivated by the importance of locating informative yet small sparse regions, we outline two objectives forlocalization in finetuned models: i) the regions should encapsulate essential skills acquired during finetuning,and ii) they should contain minimal number of parameters. The objectives are grounded in the findings of Panigrahi et al. (2023), which demonstrates that skillsdeveloped during finetuning are localizable. Specifically, grafting a small subset of finetuned parameters ontothe pretrained model can almost fully recover the performance of the finetuned model. Panigrahi et al. (2023)propose the following optimization problem to identify the localized parameters, and we adapt it in the modelmerging setting. With the constraint on the sparsity level s, on the i-th task with the loss function i andtask vector i, we optimize for a binary mask i such that only adding up the masked portion of the taskvector onto the pretrained model performs well on the i-th task",
  "(a) Jaccard similarity of pairwise task masks": "SST-2 CR MR MPQA TREC SUBJ QNLI SNLI MNLI RTE MRPC QQP SST-2 CR MR MPQA TREC SUBJ QNLI SNLI MNLI RTE MRPC QQP 1.00 0.94 0.96 0.89 0.07 0.11 0.10 0.11 0.09 0.04 0.07 0.16 0.94 1.00 0.95 0.91 0.05 0.10 0.10 0.12 0.11 0.11 0.13 0.11 0.96 0.95 1.00 0.90 0.03 0.14 0.07 0.10 0.04 0.02 0.09 0.13 0.89 0.91 0.90 1.00 0.00 0.06 0.08 0.12 0.13 0.10 0.14 0.06 0.07 0.05 0.03 0.00 1.00 0.14 0.08 0.10 0.09 0.00 0.16 0.00 0.11 0.10 0.14 0.06 0.14 1.00 0.06 0.13 0.07 0.05 0.15 0.03 0.10 0.10 0.07 0.08 0.08 0.06 1.00 0.31 0.41 0.58 0.53 0.61 0.11 0.12 0.10 0.12 0.10 0.13 0.31 1.00 0.78 0.71 0.34 0.55 0.09 0.11 0.04 0.13 0.09 0.07 0.41 0.78 1.00 0.70 0.48 0.56 0.04 0.11 0.02 0.10 0.00 0.05 0.58 0.71 0.70 1.00 0.54 0.64 0.07 0.13 0.09 0.14 0.16 0.15 0.53 0.34 0.48 0.54 1.00 0.51 0.16 0.11 0.13 0.06 0.00 0.03 0.61 0.55 0.56 0.64 0.51 1.00 0.0 0.2 0.4 0.6 0.8 1.0",
  "(b) Cosine similarity of masked task vectors": ": Our localized regions (each task with 1% of total parameters) have little pairwise overlap,with the majority of Jaccard similarity below 5%. The sentiment classification tasks (SST-2, CR, MR, MPQA)have relatively large overlap because they share similar skills in the overlapping regions, and we verify this by showingthat they have high cosine similarity of masked task vectors. where denotes the element-wise product. For the ease of optimization, we follow Panigrahi et al. (2023)to reparametrize the binary mask as a real-valued vector S. To control the sparsity, we additionally relaxthe L0 sparsity constraint to be L1. As a result, the optimization is reformulated as",
  ":= base (1 (S)) + (1 base) (S),(2)": "where base is the top-k% largest elements in the task vector, which serves as an initialization for theoptimization. There are two main advantages of formulation 1 over 2. Firstly, our formulation of S is morestraightforward, as we directly have = (S). In contrast, S in Equation (2) serves as a selector of whether totake the value from base, leading to more complex computation. Secondly, our approach uses the L1 constraintto control the sparsity in a more fine-grained manner, while Equation (2) does not have this constraint, andthey control the sparsity via early stopping instead. A detailed empirical performance comparison betweenour localization technique and the one in (Panigrahi et al., 2023) is presented in Appendix E. Note that the optimization is highly efficient, requiring as few as 8-shot data with 10 epochs of training usingSGD. A detailed ablation on the impact of data availability on the mask quality is shown in .4. Interpretation of task relationships.In , we validate that our localization method effectivelyidentifies task-specific regions with minimal overlap. In a, we report the Jaccard similarity of allpairwise task masks, namely for each pair of masks i and j, we compute |i j|/|i j|. The majorityof task pairs exhibit a Jaccard similarity below 5%, confirming minimal overlap. For the few pairs withJaccard similarity larger than 10% (upper left corner of a), we further compute the cosine similarityof their masked task vectors in b, and find that their cosine similarities are almost 1, indicatinghigh agreement of the parameters within the overlapped regions. Since these four tasks are all sentiment",
  "classification tasks, this phenomenon intuitively suggests a shared skill set across the tasks, located in theoverlapped regions. We elaborate our resolution for the overlapped regions in .3": "It is important to note that the meaningfulness of cosine similarity depends heavily on the presence ofa substantial overlap, as indicated by Jaccard similarity. In cases where overlap is minimal, high cosinesimilarity might imply a strong relationship due to well-aligned parameters. Yet, this interpretation could bemisleading without the broader context provided by Jaccard similarity, which could reveal that the actualinteraction between the tasks is limited. This understanding is crucial for accurately assessing the nature ofthe relationships between tasks based on their localized parameters. Distribution of localized regions.We analyze the distribution of the localized regions for languagetasks in RoBERTa-base models , both in terms of the layer index and the transformer components.For the layers, different tasks seem to occupy different layers, although the earlier layers in the networkseldomly appear in the localized regions. Interestingly, most of the localized regions concentrate in theLayerNorm (Ba, 2016) parameters. This pattern can possibly be attributed to a distribution shift observed inthe finetuning data compared to the pretraining data, necessitating adjustments to the LayerNorm parametersto accommodate this shift. The same plots for GPT2-XL and ViT can be found in Appendix B, and thefindings hold true for those models as well. Localization without validation data.In the rare cases where no labeled data is available, we adopta similar strategy as the Trim step in TIES-Merging (Yadav et al., 2023), which selects positions in taskvectors with the top-k% largest magnitudes, i.e., the parameters changed the most during finetuning. Werefer to our approach as Dataless Localize-and-Stitch. As shown in , to match the performanceof localization with validation data, the dataless version typically requires locating larger regions. Thisexpansion is necessary to encapsulate sufficient skills acquired during finetuning, but it also leads to increasedtask conflicts. Nevertheless, in , we show that our dataless version still outperforms all othermethods that do not require additional validation data. Despite the similarity, there are two key differences between Dataless Localize-and-Stitch and TIES:i) Smaller localized region: We find selecting the top-5% of parameters is sufficient for our pipeline,compared to the top-20% recommended by TIES-Merging. Our smaller selected region incurs less overlapping,leading to reduced task interference. Note that this is not the only advantage of our approach, as reducingthe threshold in TIES to be 5% does not yield an improved performance as demonstrated in Appendix C(Tables 12 and 13). ii) Better merging performance: We use Stitching described in the next sectionfor merging the localized regions, instead of the Elect procedure in TIES. The Elect approach in TIES",
  "Stitching": "After obtaining the binary mask for each task, we integrate these masks, and apply them to task vectors toconstruct the merged model. Given the sparsity, the masks generally activate different positions for differenttasks, minimizing overlap. However, in instances where overlaps occur that is, where multiple tasks sharethe same mask positions we address this by averaging the parameters in these regions. Specifically, foreach final mask i, the value at the k-th position, denoted i[k], is calculated as the reciprocal of the totalnumber of tasks that have a mask value of 1 at that position; if the original mask value i[k] is 0, it remains",
  "end forreturn merged = pre + ni=1 (i i)": "The complete algorithm is presented in Algorithm 1.Note that our stitching step does not involvetuning the scaling factors as other methodsmentioned in , which typically requiresgrid search or other optimization strategies fortuning. This distinction simplifies our method andavoids the computational overhead. A comparisonof runtime is provided in Appendix B. Remark.In Localize-and-Stitch, the majorityof computational overhead lies in the localizationstep, while the subsequent stitching process isnotably efficient. This distribution of workload isideal because the more intensive localization stepis performed separately on each individual finetunedmodel. This property provides simple extension incontinual learning settings: When integrating a newmodel into the existing merged model (or updatingany of the merged models), only the localizationstep for that new model incurs a cost, followedby stitching.This is in contrast to most modelmerging methods Jin et al. (2022); Ilharco et al.(2023); Yadav et al. (2023); Yang et al. (2023) whichnecessitate restarting the whole merging process,as the scaling factors of task vectors are tuned orlearned based on the performance of the mergedmodel.We provide an experiment of continuallearning in .4 to illustrate this advantage.",
  "Single-task finetuned-0.8110.905": "Simple averaging (Wortsman et al., 2022a)0.563 (0.696)0.658 (0.731)Task artihmetic (Ilharco et al., 2023)0.626 (0.777)0.692 (0.758)TIES (Yadav et al., 2023)0.600 (0.743)0.725 (0.796)Dataless Localize-and-Stitch0.734 (0.911)0.740 (0.818) Task arithmetic (Ilharco et al., 2023)0.675 (0.840)0.701 (0.778)TIES (Yadav et al., 2023)0.621 (0.772)0.736 (0.812)Fisher merging (Matena & Raffel, 2022)0.690 (0.863)0.683 (0.763)RegMean (Jin et al., 2022)0.739 (0.911)0.718 (0.792)AdaMerging (Yang et al., 2023)0.637 (0.790)0.801 (0.880)Concensus TA (Wang et al., 2024b)0.715 (0.889)0.737 (0.810)Consensus TIES (Wang et al., 2024b)0.695 (0.863)0.748 (0.822)Localize-and-Stitch0.759 (0.936)0.799 (0.880)",
  "Merging finetuned encoder-based language models": "Following Panigrahi et al. (2023), we finetune the RoBERTa-base (Liu et al., 2019) model on twelveGLUE (Wang et al., 2018) tasks.Specifically, the dataset suite includes six single-sentence tasks(SST-2 (Socher et al., 2013), CR (Hu & Liu, 2004), MR (Pang & Lee, 2005), MPQA (Wiebe et al., 2005),TREC (Voorhees et al., 1999), SUBJ (Pang & Lee, 2004)) and six pairwise-sentence tasks (QNLI (Wang et al.,2018), SNLI (Bowman et al., 2015), MNLI (Williams et al., 2017), RTE (Wang et al., 2018), MRPC Dolan& Brockett (2005), QQP (Iyer et al.)). The dataset details can be found in Appendix G. We present the results in , and leave the detailed per-task results in of Appendix A. The tableis structured into three blocks for clarity: the upper block displays the performance of individually finetunedmodels for each task, the middle block lists algorithms that operate without the need for validation data,whereas the lower block includes algorithms that require validation data. Note that both the middle and lowerblocks contain Task arithmetic and TIES because they are applicable with or without data. Both algorithmsare able to utilize validation data to tune the merging coefficients , as in merged = pre + ni=1 i. Wefollow common practice to search over {0.1, 0.2, , 1} to obtain the optimal coefficients. When no validationdata is available, we use their suggested merging coefficient of 0.4. From , regardless of data availability, our approach consistently outperforms other baselines. Notably,the dataless version of our algorithm provides more than 10% performance increase over task arithmeticand surpasses methods that depend on validation data (Fisher merging and AdaMerging), demonstrating itseffectiveness. Note that TIES-Merging, although sharing one similar step with our dataless version, performsworse than task arithmetic. This performance decrease is also observed in similar language evaluation settingswith similar model size Yadav et al. (2023). This phenomenon can be attributed to the two possible factors weidentify in .3: i) the larger localized regions of TIES potentially lead to more task conflicts; ii) the signelection mechanism it employs tends to be less effective in overlapping regions that involve only a few tasks, par-ticularly when just two are present. This can lead to suboptimal retention of essential task-specific information.We provide further analysis comparing Dataless Localize-and-Stitch and TIES-Merging in Appendix C.",
  "Single-task0.2730.4880.4720.411": "Simple averaging (Wortsman et al., 2022a)0.234 (0.857)0.390 (0.799)0.406 (0.860)0.344 (0.839)Task arithmetic (Ilharco et al., 2023)0.234 (0.857)0.390 (0.799)0.399 (0.845)0.341 (0.834)TIES (Yadav et al., 2023)0.233 (0.853)0.448 (0.918)0.310 (0.657)0.330 (0.809)Consensus TA (Wang et al., 2024b)0.234 (0.857)0.412 (0.844)0.373 (0.790)0.340 (0.831)Consensus TIES (Wang et al., 2024b)0.232 (0.850)0.395 (0.809)0.386 (0.818)0.337 (0.826)Dataless Localize-and-Stitch0.256 (0.938)0.394 (0.807)0.427 (0.905)0.359 (0.883)Localize-and-Stitch0.247 (0.905)0.388 (0.795)0.467 (0.989)0.367 (0.896)",
  "Merging finetuned vision models": "Following the practice in Ilharco et al. (2023), we finetune the CLIP (Radford et al., 2021) image encoderwith the ViT-B/32 (Dosovitskiy et al., 2021) architecture on eight image classification tasks, incorporatingdiverse categories of images such as remote sensing, satellite images and traffic signs. Specifically, the datasetsuite includes SUN397 (Xiao et al., 2016), Stanford Cars Krause et al. (2013), RESISC45 (Cheng et al., 2017),EuroSAT (Helber et al., 2019), SVHN (Netzer et al., 2011), GTRSB (Stallkamp et al., 2011), MNIST (LeCunet al., 2010) and DTD (Cimpoi et al., 2014). The details of each dataset can be found in Appendix G. We present the results in , and leave the detailed per-task results in of Appendix A. Similarly,even in the absence of validation data, the dataless version of our approach can outperform methodsrequiring validation data (Fisher merging and RegMean). When validation data is available, our method alsodemonstrates competitive performance. Note that AdaMerging, while achieving similar results as ours, imposesmore demanding data availability requirement, and incurs higher computational cost. It necessitates entropyminimization across the entire (unlabeled) test set, rendering it approximately 15 times slower than our method.",
  "Merging finetuned decoder-based language models": "Compared with encoder-only language models, decoder-based language models benefit from increased numberof parameters and perform well on complicated generative tasks. We use GPT2-XL (Radford et al., 2019) asthe base model, and obtain three supervised finetuned checkpoints from the Hugging Face model hub (Wolfet al., 2020), each tuned for distinct functionalities: general reasoning, scientific knowledge and truthfulnessrespectively. Further details about these models are specified in Appendix F. To assess these models, we use MMLU (Hendrycks et al., 2021), ARC (Clark et al., 2018) and TruthfulQA (Linet al., 2021) as evaluation datasets for the respective domains. Unlike datasets in the previous section,these are typically used in their entirety for evaluation, without a designated train-test split. However, usingthese datasets for both evaluation and localization could lead to data leakage. To prevent this, we use datafrom three surrogate datasets with similar purposes for localization, namely Alpaca (Taori et al., 2023),GSM8K (Cobbe et al., 2021) and HotpotQA (Yang et al., 2018). We compare our approach with other methods directly applicable in this setting in . Both versionsof Localize-and-Stitch noticeably outperforms other baselines. The result verifies that even for complexgenerative tasks, skills can still be localized within tiny regions of finetuned models. Moreover, this shows thatgood localization performance can be achieved without access to the original finetuning data; using similar datafrom other sources also suffices. This aligns with the finding from (Panigrahi et al., 2023) that localized regionsexhibit transfer among similar tasks, meaning that a localized region for one task can facilitate performance inrelated tasks. This further reduces the dependency on data availability, making our approach more versatile.Overall, these findings highlight the capability of Localize-and-Stitch to integrate the strengths frommultiple language models, demonstrating its effectiveness across a variety of linguistic challenges.",
  ": Sparsity-performance trade-off for our al-gorithm on the language tasks. Localization achievesthe best performance at sparsity around 1%, while thedataless version requires 5 10%": "Number of data per class 0.74 0.75 0.76 0.77 0.78 0.79 Average merged acc/F1 Localize-and-Stitch (1%)Dataless Localize-and-Stitch (5%) : With only 8-shot data, the performanceof our algorithm improves over the dataless ver-sion. With more data available, the performance ofour method continues to increase. Numbers in bracketsrepresent sparsity levels for each method. Here, we study the trade-off by presenting the performance of our approach on the language tasks at differentsparsity levels in . Across all models and tasks tested, we observe that a sparsity level around 1%yields the best results using our localization method, whereas dataless localization requires 5 10%. Whenthe localized regions are too small to retain adequate finetuned knowledge, the benefit of less overlap isdiminished. Conversely, when the localized regions are too large, although the regions possess sufficientfinetuned knowledge, the increased overlap among task-specific regions leads to more task interference. Effect of data availability.We present the performance of our method across various data availabilityscenarios with a localization region of 1% () on the language tasks. One clear trend is that withmore data, the quality of localization improves, resulting in enhanced performance of the merged model.Notably, even with as few as 8-shot data, the merged performance surpasses that of the dataless approach,highlighting its effectiveness under constrained data conditions. 0.700.750.80 ImageNet top-5 accuracy 0.5 0.6 0.7 0.8 Average accuracy on eight tasks",
  ": Our method retains the pretrainedskill the best due to the minimal updates (7% of totalparameters) to the pretrained model, while performswell on the eight merged tasks (upper right better)": "Avoid forgetting of pretrained knowledge.Pre-trained models contain rich and generalizable informa-tion, derived from their diverse training data. However,finetuning often incurs catastrophic forgetting of skillsin the pretrained model (He et al., 2021; Luo et al.,2023), which is carried over when these finetuned mod-els are merged. As our method makes minimal changeto the pretrained model, such forgetting is substantiallymitigated. For instance, in the vision setting where welocalize 1% parameters for each task, the total param-eters changed in our merged model compared with thepretrained model is only around 7% for the 8 tasks as aresult of minimal overlap. We evaluate the retention ofpretrained capabilities with a general vision task thatthe pretrained CLIP model excels, namely zero-shotImageNet classification (Deng et al., 2009), and reportthe results in . Our method most effectivelypreserves the pretrained performance, while achievessuperior performance on the eight merged tasks. Model compression through localization.Our localization approach enables a compact representationof the finetuned model. In our experiments, we find that localizing only 1% of the total parameters recoversover 99% of the performance achieved by single-task finetuning (full evaluation reported in Appendix B).The efficiency allows us to store only the masked task vectors for each task (i i) instead of the entirefinetuned models, without a noticeable loss in performance. Given the sparsity of these masked task vectors,",
  ": Comparison of performance (left) and efficiency (right) for continual learning. Localize-and-Stitch con-sistently outperforms baselines with generally constant runtime for each additional task": "we can store them in Compressed Sparse Row (CSR) format (Pissanetzky, 1984; Golub & Van Loan, 2013),which drastically reduces the model size to about 1% of the original. For example, a RoBERTa-base model,which typically requires 650MB of memory to store, can be represented using only a 7MB sparse matrix,achieving a memory reduction of 99%. Although we still need to store the full pretrained model, this storageoverhead will be amortized with more finetuned models. This model compression, combined with the easeof update mentioned in .3, enables flexible composition of skills from multiple finetuned modelswith minimal storage and computational overhead. Continual learning.As mentioned in .3, our approach is particularly efficient in the continuallearning setting. To illustrate the efficiency, we start from merging the SST-2 and CR RoBERTa models, andincrementally add 4 more tasks to simulate a continual learning setting. The tasks are selected by representative-ness (including sentiment analysis, sentence classification, NLI, etc). We compare our method against five base-lines: task arithmetic, TIES, Consensus TA, Consensus TIES and continual training. Following common prac-tice, the merging coefficients are tuned across {0.0, 0.1, ...0.9, 1.0}. The continual training results are obtainedby sequential training on each task, using the model from the previous task as the starting point. The resultsfrom demonstrate that: i) Performance: Localize-and-Stitch consistently outperforms the base-lines, with the performance margin increasing as more tasks are involved, showing its ability to reduce task inter-ference. ii) Runtime: The runtime for other baselines increases with each added task due to the need for hyper-parameter search from scratch and performance validations, while the runtime of Localize-and-Stitch gen-erally remains constant, reflecting its efficiency in continual learning scenarios. While continual trainingperforms better than other model merging baselines, it still falls short of Localize-and-Stitch. Thissuggests a potential avenue for enhancing continual learning: instead of finetuning on top of the last taskmodel, it may be advantageous to first finetune the pretrained model on the new task and then merge it withthe existing multi-task model on the old tasks. We leave further exploration of this approach for future work.",
  "Related works": "Model merging. Model merging aims at efficiently integrating multiple finetuned models into a single modelthat retains the capabilities of each. This approach enhances the efficiency, generalization and multi-taskcapabilities of finetuned models. In scenarios where models are trained on the same task with differenttraining configurations, Singh & Jaggi (2020); Ainsworth et al. (2022); Jolicoeur-Martineau et al. (2024) showthat merged models perform comparably to ensemble models but with significantly lower deployment costs.Additionally, Wortsman et al. (2022a;b) demonstrate that the merged model improves the out-of-distribution(OOD) robustness. When merging finetuned models from different tasks, the merged model can providebetter initialization for new tasks (Choshen et al., 2022; Gueta et al., 2023). Finetuned models with differentspecialized skills can also be combined to enhance multi-task capabilities (Ilharco et al., 2023; Tam et al.,2023; Matena & Raffel, 2022; Jin et al., 2022; Yang et al., 2023; Yu et al., 2023; Wang et al., 2024b;a). Morerecently, a new line of work has emerged that uses a mixture of experts (MoE) strategy (Jiang et al., 2023;Tang et al., 2024). Instead of a single unified model, the MoE approach incorporates routing mechanisms todirect inputs to task-specific networks. In this work, we primarily focus on merging specialized models intoa single unified model for enhancing multi-task performance. Similar to the gradient conflict problem (Yu",
  "et al., 2020; Liu et al., 2021) in multi-task learning, finetuned models also manifest conflict when mergedtogether, and our method provides an effective solution to this problem": "Our approach stands out with four key advantages: i) Localized merging: Instead of global merging, welocalize merging to specific regions with finetuned skills, effectively decreasing task conflicts. ii) Simplifiedprocess: Existing works often require computationally intensive grid search or optimization to determinethe optimal merging coefficients, while our stitching procedure does not have the requirement. iii) Dataflexibility: Our method works with or without validation data, and provides competitive results in variousdata availability scenarios. iv) Benefits beyond model merging: This includes interpretability of task relations,model compression and preservation of pretrained knowledge. Knowledge attribution. Recent works find that knowledge contained in language models is localizable,meaning that model behavior can be attributed to small subsets of model parameters. One line of workidentifies such regions to edit the knowledge contained in the networks. Sundararajan et al. (2017) usesintegrated gradients for knowledge attribution, which measures how sensitive each neurons gradient is to thechange of input. Dai et al. (2021) applies integrated gradients to edit factual knowledge contained in BERTmodels. However, the relationship between the editing success and the localized regions remains unclear (Haseet al., 2024). Knowledge attribution can also be applied for enhancing interpretability. Vig et al. (2020)applies causal mediation analysis (Pearl, 2022) to identify individual neurons contributing to gender bias. Recently, Panigrahi et al. (2023) optimizes for a binary mask to localize skills contained in finetunedlanguage models. There are two key differences between our localization formulation and theirs. Firstly, ourformulation of S is more straightforward, as we directly have = (S) in Equation (1). In contrast, Panigrahiet al. (2023) uses S as a selector of whether to take the value from the initial mask, leading to more complexcomputation. Secondly, our approach uses the L1 constraint to control the sparsity in a more fine-grainedmanner, while Panigrahi et al. (2023) does not have this constraint, and controls sparsity via early stopping.We empirically show that our localization formulation identifies regions with improved quality in Appendix E. Pruning. Similar to localization, pruning is a strategy to identify key regions in the parameter space that areimportant to model performance. Han et al. (2015a;b) propose magnitude pruning, which preserves weightswith high magnitudes, and the method inspires various variants (Zhu & Gupta, 2017; Paganini & Forde,2020; Zafrir et al., 2021). Parameter significance measured by performance sensitivity is another effectivecriteria for identifying important parameters (Sanh et al., 2020; Liang et al., 2021; Zhang et al., 2022). Forinstance, Lee et al. (2018) proposes SNIP score, which computes the change of loss when each neuron is setto 0. Pruning methods are widely applied to modern large language models (Zhang et al., 2023; Sun et al.,2023; Xia et al., 2023; Zhao et al., 2024; Cheng et al., 2024). The primary distinction between pruning and localization lies in their treatment of parameters outside the iden-tified regions. In pruning, these parameters are set to zero, effectively removing them, allowing the pruned net-work to function as a standalone model. Conversely, in localization, parameters outside the localized regions areretained at their pretrained values, requiring the localized regions to be combined with the pretrained model fordeployment. Despite these differences, the conceptual overlap between them suggests that pruning techniquescould be adapted for localization. Exploring such adaptations presents an interesting direction for future work.",
  "Conclusion": "In this work, we study the problem of task interference in the context of model merging. We find that globallymerging models typically leads to task interference, due to the parameter redundancy in task vectors. Totackle this challenge, we introduce Localize-and-Stitch, which performs localized merging via sparse taskarithmetic. We first identify tiny regions in the finetuned models that contain essential skills acquired duringfinetuning, and stitch only those regions back onto the pretrained model. Empirical evaluation on variousvision and language benchmarks validate the effectiveness of our approach. Beyond model merging, ourapproach performs effective model compression, which compresses the model size to be 1% of the originalwithout sacrificing performance. Additionally, Localize-and-Stitch also excels at retaining the pretrainedknowledge. Overall, our approach offers a novel pathway for flexible and continual skills composition fromfinetuned models with minimal storage and computational overhead.",
  "Rich Caruana. Multitask learning. Machine learning, 28:4175, 1997": "Tianlong Chen, Jonathan Frankle, Shiyu Chang, Sijia Liu, Yang Zhang, Zhangyang Wang, and MichaelCarbin. The lottery ticket hypothesis for pre-trained bert networks. Advances in neural informationprocessing systems, 33:1583415846, 2020a. Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastivelearning of visual representations. In International conference on machine learning, pp. 15971607. PMLR,2020b.",
  "Leshem Choshen, Elad Venezian, Noam Slonim, and Yoav Katz. Fusing finetuned models for better pretraining.arXiv preprint arXiv:2204.03044, 2022": "Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi. Describingtextures in the wild. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.36063613, 2014. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and OyvindTafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprintarXiv:1803.05457, 2018. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, MatthiasPlappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math wordproblems. arXiv preprint arXiv:2110.14168, 2021.",
  "Song Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections for efficientneural network. Advances in neural information processing systems, 28, 2015b": "Peter Hase, Mohit Bansal, Been Kim, and Asma Ghandeharioun. Does localization inform editing? surprisingdifferences in causality-based localization vs. knowledge editing in language models. Advances in NeuralInformation Processing Systems, 36, 2024. Tianxing He, Jun Liu, Kyunghyun Cho, Myle Ott, Bing Liu, James Glass, and Fuchun Peng. Analyzing theforgetting problem in pretrain-finetuning of open-domain dialogue response models. In Proceedings of the16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume,pp. 11211133, 2021. Yifei He, Shiji Zhou, Guojun Zhang, Hyokun Yun, Yi Xu, Belinda Zeng, Trishul Chilimbi, and Han Zhao.Robust multi-task learning with excess risks. In Proceedings of the 41st International Conference onMachine Learning, volume 235 of Proceedings of Machine Learning Research, pp. 1809418114. PMLR,2127 Jul 2024. URL Patrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth. Eurosat: A novel dataset and deeplearning benchmark for land use and land cover classification. IEEE Journal of Selected Topics in AppliedEarth Observations and Remote Sensing, 12(7):22172226, 2019. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt.Measuring massive multitask language understanding. Proceedings of the International Conference onLearning Representations (ICLR), 2021. Torsten Hoefler, Dan Alistarh, Tal Ben-Nun, Nikoli Dryden, and Alexandra Peste. Sparsity in deep learning:Pruning and growth for efficient inference and training in neural networks. Journal of Machine LearningResearch, 22(241):1124, 2021.",
  "Minqing Hu and Bing Liu. Mining and summarizing customer reviews. In Proceedings of the tenth ACMSIGKDD international conference on Knowledge discovery and data mining, pp. 168177, 2004": "Yuzheng Hu, Ruicheng Xian, Qilong Wu, Qiuling Fan, Lang Yin, and Han Zhao. Revisiting scalarization inmulti-task learning: A theoretical perspective. Advances in Neural Information Processing Systems, 36,2024. Gabriel Ilharco, Marco Tulio Ribeiro, Mitchell Wortsman, Ludwig Schmidt, Hannaneh Hajishirzi, and AliFarhadi. Editing models with task arithmetic. In The Eleventh International Conference on LearningRepresentations, 2023. URL",
  "Bo Pang and Lillian Lee. Seeing stars: Exploiting class relationships for sentiment categorization with respectto rating scales. arXiv preprint cs/0506075, 2005": "Abhishek Panigrahi, Nikunj Saunshi, Haoyu Zhao, and Sanjeev Arora. Task-specific skill localization infine-tuned language models. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt,Sivan Sabato, and Jonathan Scarlett (eds.), Proceedings of the 40th International Conference on MachineLearning, volume 202 of Proceedings of Machine Learning Research, pp. 2701127033. PMLR, 2329 Jul2023. URL",
  "Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models areunsupervised multitask learners. 2019": "Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from naturallanguage supervision. In International conference on machine learning, pp. 87488763. PMLR, 2021. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou,Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer.Journal of machine learning research, 21(140):167, 2020.",
  "Sidak Pal Singh and Martin Jaggi. Model fusion via optimal transport. Advances in Neural InformationProcessing Systems, 33:2204522055, 2020": "Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, andChristopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. InProceedings of the 2013 conference on empirical methods in natural language processing, pp. 16311642,2013. Johannes Stallkamp, Marc Schlipsing, Jan Salmen, and Christian Igel. The german traffic sign recognitionbenchmark: a multi-class classification competition. In The 2011 international joint conference on neuralnetworks, pp. 14531460. IEEE, 2011.",
  "Adina Williams, Nikita Nangia, and Samuel R Bowman. A broad-coverage challenge corpus for sentenceunderstanding through inference. arXiv preprint arXiv:1704.05426, 2017": "Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, PierricCistac, Tim Rault, Rmi Louf, Morgan Funtowicz, et al. Transformers: State-of-the-art natural languageprocessing. In Proceedings of the 2020 conference on empirical methods in natural language processing:system demonstrations, pp. 3845, 2020. Mitchell Wortsman, Gabriel Ilharco, Samir Ya Gadre, Rebecca Roelofs, Raphael Gontijo-Lopes, Ari S Morcos,Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, et al. Model soups: averaging weights ofmultiple fine-tuned models improves accuracy without increasing inference time. In International conferenceon machine learning, pp. 2396523998. PMLR, 2022a. Mitchell Wortsman, Gabriel Ilharco, Jong Wook Kim, Mike Li, Simon Kornblith, Rebecca Roelofs,Raphael Gontijo Lopes, Hannaneh Hajishirzi, Ali Farhadi, Hongseok Namkoong, et al. Robust fine-tuning of zero-shot models. In Proceedings of the IEEE/CVF conference on computer vision and patternrecognition, pp. 79597971, 2022b.",
  "Task arithmetic0.8780.802TIES0.8750.812Localize-and-Stitch0.8800.831": "Where are the localized regions? We analyze the distribution of the localized regions for both languageand vision tasks in , both in terms of the layer index and the transformer components. For thelayers, different tasks seem to occupy different layers, although the earlier layers in the network seldomlyappear in the localized regions. Interestingly, most of the localized regions concentrate in the LayerNormparameters. This pattern can possibly be attributed to a distribution shift observed in the finetuning datacompared to the pretraining data, necessitating adjustments to the LayerNorm parameters to accommodatethis shift.",
  "Recovered proportion0.9700.9930.9940.9920.9880.9850.9990.9830.989": "Full grafted performance. We evaluate the quality of the localized regions by the grafted performance.For the i-th task, we only add up the localized regions in the task vectors back to the pretrained model,i.e., pre + i i. The results with a localization region of 1% is shown in Tables 7 to 9. For almost all tasks,using only the tiny localized region recovers nearly 99% of the finetuned performance. For GPT2-XL, theperformance is slightly worse because we cannot use the evaluation data for the localization step. However,the results are still strong even with surrogate datasets with similar purposes, demonstrating the flexibilityand robustness of our algorithm. Overall, this shows that our localization approach is effective in locatingregions containing essential skills acquired during finetuning, and the localized regions can be viewed ascompact representations of the finetuned models.",
  "Fisher Merging293.33Task arithmetic (tuned)6562.14TIES-Merging (tuned)24042.43RegMean22987.54AdaMerging81326.57Consensus TA7361.52Consensus TIES26042.43Localize-and-Stitch5130.05": "Runtime. We compare the runtime of Localize-and-Stitch and other baselines. We divide the algorithmsinto two categories: dataless and requiring data. Note that task arithmetic and TIES-Merging can fall in bothcategories, with the difference of whether performing hyperparameter tuning (scaling factor for both, andsparsity for TIES). For the hyperparameter tuning, we follow the common practice in Ilharco et al. (2023);Yadav et al. (2023) to grid search over {0.1, 0.2 , 1} for the scaling factor and {0.1, 0.2, 0.3} for the sparsity. We report the runtime in Tables 10 and 11 for merging twelve NLP tasks with RoBERTa-base. For the datalessalgorithms, simple averaging and task arithmetic are very efficient, as they only involve arithmetic operationson the weights. Both TIES and our dataless version requires sorting the task vectors to get the top-k%largest elements, so the runtime is slower. Compared with TIES, we do not have the step for resolving signconflicts, so it takes less time. For algorithms requiring data, Fisher merging is the most efficient, as it uses adiagonal estimate of the Fisher information matrices with little data (256 per task). Both task arithmetic andTIES-Merging show substantial time increase, as they need to do grid search on 9 and 27 hyperparametersrespectively as well as evaluating on the validation data in each run. AdaMerging takes significantly moreruntime to execute compared with others, and the reason could be that entropy minimization converges slowly,as we observe that AdaMerging requires around 500 epochs to converge. Compared with other algorithms,Localize-and-Stitch executes in a relatively short amount of time, showing its effectiveness.",
  "Dataless Localize-Stitch (5%)0.6690.6470.7680.7460.8170.7260.9730.5760.740TIES (5%)0.5200.5520.6690.6830.8740.6060.9820.4800.671TIES (20%)0.5980.5860.7070.7970.8620.7210.9830.5420.725": "The first step of both algorithms is similar: select the top-k% largest positions in the task vector. Theprimary difference lies in the selection threshold: Dataless Localize-and-Stitch selects the top-5%, whileTIES selects the top-20%. However, as shown in .1, we find that when a localized region alreadycontains sufficient task-specific knowledge, including more parameters only introduces more task interference.This observation could partially explain our superior performance. However, this is not the only limitation inTIES, as reducing the threshold in TIES to be 5% does not yield an improved performance as demonstratedin Tables 12 and 13. In the subsequent merging step, Dataless Localize-and-Stitch can be viewed as a simplified versionof TIES. When dealing with overlap for the localized regions, Dataless Localize-and-Stitch simplyaverages the parameters in these overlapping area. On the other hand, TIES first sums positive and negativeparameters separately at each overlapping position, and determines the dominant sign based on their totalmagnitudes, a process akin to a weighted majority vote. Then, TIES only keep the parameter values thataligns with the elected sign, and compute the mean. Number of tasks occupying the same parameter 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 Proportion among overlapped parameters",
  ": When dealing with two conflicting tasks,the sign election stage of TIES is not effective.Theperformance of TIES is consistently worse than simpleaveraging on all parameters": "This approach by TIES might be more advantageous when overlapping regions involve a larger number oftasks. The rationale is that with more tasks contributing to an overlap, the process of sign determination andselective averaging may more accurately capture the consensus of task vectors for all tasks as a whole. However,when only two tasks are involved (which is often the case as shown in ), TIES may only retain param-eters predominantly from the task with the larger magnitude at each position. In such scenarios, important",
  "DComparison between Localize-and-Stitch and Consensus Merging": "Given the similarity of our approach with Consensus Merging (Wang et al., 2024b), we compare them indetail. In , we have demonstrated Localize-and-Stitch has superior performance, and we providefurther empirical analysis as follows. Localization area. Our method is able to localize as little as 1% of total parameters, compared to 20%required by (Wang et al., 2024b). Our small localized regions lead to less task interference, better multi-taskperformance and much better compression rates. In Consensus Merging, the sparsity is controlled by thehyperparameter , which is chosen among {0.2, 0.3, 0.4, 0.5, 0.6}. Note that is not directly used as the sparsity,rather, it serves as a scaling factor to determine the magnitude threshold for localization, as shown in Equation(5) in (Wang et al., 2024b). Intuitively, a larger imposes a stricter threshold, resulting in a smaller localizedregion. We present the resulting average sparsity corresponding to each choice of in . While theresulting sparsity might vary depending on the specific tasks and models, we find that all choices of leadto an average sparsity of at least 25%, which is substantially larger than the 1% sparsity achieved by ourlocalization method.",
  "Consensus TA1%0.4770.518Consensus TIES1%0.4630.524Dataless Localize-and-Stitch5%0.7340.740Localize-and-Stitch1%0.7590.799": "Runtime comparison. The larger runtime observed () for Consensus TA compared to our methodis primarily due to its extensive hyperparameter tuning process. Take merging vision models as an example,for the formulation of (Wang et al., 2024b), the hyperparameter tuning process is as follows: i) Tune sparsityfor all tasks, which requires 40 evaluations for 5 choices of on 8 tasks. ii) Tune scaling factor , which requires",
  "EDetails on localization": "Here, we detail the skill attribution method in Panigrahi et al. (2023) and explain the difference with ourformulation. Panigrahi et al. (2023) aims to localize task-specific skills contained in finetuned languagemodels. They introduce model grafting, where for given pretrained and finetuned model parameters pre andft, they graft parameters of ft in the region onto the pretrained model as",
  "Si = arg minSRdi (pre + (S) i) + (S)1,": "There are two main differences between the formulations. Firstly, our formulation of S is more straightforward,as we directly have = (S). In contrast, S in Equation (3) serves as a selector of whether to take the valuefrom base, leading to more complex computation. Secondly, our approach uses the L1 constraint to controlthe sparsity in a more fine-grained manner, while Equation (3) does not have this constraint, and the authorscontrol the sparsity via early stopping.",
  "Merged (Ours)0.8960.8960.8490.8280.7820.8200.7340.6210.5800.6330.8200.6510.759Merged (Panigrahi et al., 2023)0.8970.8950.8470.8310.8160.8030.7270.6490.5800.6330.8190.6560.763": "We present the performance comparison of the two localization methods in Tables 16 and 17. In both cases,our approach with Equation (1) outperforms Panigrahi et al. (2023). The performance may come fromthe fact that Panigrahi et al. (2023) use early stopping to control the sparsity, which results in incompleteoptimization for the masks. We also report the merged performance by following the same stitching process.On the language tasks, the performance is similar, while on the vision tasks, our localization leads to bettermerged performance.",
  "Rachneet/gpt2-xl-alpaca": "They are all finetuned on the original release of the GPT2-XL model openai-community/gpt2-xl. Theselection of the three models and associated tasks is a result of an extensive evaluation process. After testingdozens of finetuned GPT-2XL checkpoints, we establish specific criteria to ensure the relevance and rigor ofour experiments: The checkpoints should",
  "DTD (Cimpoi et al., 2014). The Describable Texture Dataset contains 5,640 texture images in thewild with 47 classes": "SUN397, RESISC45 and DTD are under the Creative Commons Attribution-ShareAlike 4.0 InternationalLicense. Stanford Cars is under the ImageNet License. EuroSAT is under MIT License. MNIST is underGnu General Public License. GTRSB and SVHN are under CCBY-SA License. Language datasetsFollowing the practice in Panigrahi et al. (2023), we use the following 12 datasets forthe language part of our experiments. The majority comes from the GLUE benchmark (Wang et al., 2018).",
  "MR (Pang & Lee, 2005). The Movie Review dataset consists of movie reviews with binary sentimentlabels": "MPQA (Wiebe et al., 2005). The Multi-Perspective Question Answering dataset contains newsarticles and text documents manually annotated for opinions and other private states includingbeliefs, emotions, sentiments, etc. Here, we use it for binary sentiment classification. TREC (Voorhees et al., 1999). The Text REtrieval Conference (TREC) dataset contains 6k questionsphrased by users and categorized into a small number of categories. The task is to classify thequestions into these categories.",
  "SUBJ (Pang & Lee, 2004). The SUBJectivity dataset contains 10k movie reviews with an annotationof whether the review describes something subjective or objective about the movie": "QNLI (Wang et al., 2018). The Question-answering NLI dataset is converted from the StanfordQuestion Answering Dataset (SQuAD) (Rajpurkar et al., 2016), which contains questions andthe paragraphs that contain the answer to the corresponding questions. QNLI converts SQuADinto sentence pair classification by forming a pair between each question and each sentence in thecorresponding context, where the task is to predict whether the context contains the answer to thequestion.",
  "MNLI (Williams et al., 2017). The Multi-Genre Natural Language Inference Corpus is a collection of433k sentence pairs annotated with textual entailment information": "RTE (Wang et al., 2018). The Recognizing Textual Entialment dataset contains a series of textualentailment challenges, including RTE1 (Dagan et al., 2005), RTE2 (Haim et al., 2006), RTE3 (Gi-ampiccolo et al., 2007) and RTE5 (Bentivogli et al., 2009). The neutral and contradiction classes arecombined into a no entailment class. MRPC Dolan & Brockett (2005). The Microsoft Research Paraphrase Corpus consists of sentencepairs from online news sources, with human annotations of whether the sentences in the pair aresemantically equivalent. Since the classes are imbalanced, we report the F1 score.",
  "The experiments are run on NVIDIA RTX A6000 GPUs with 48GB memory": "Finetuning. For the experiments on RoBERTa-base, we perform the finetuning process following the sameprocedure as Panigrahi et al. (2023). Specifically, we use a batch size of 4 and a learning rate of 2e-5 tofinetune on each of the language tasks for 10 epochs with the SGD optimizer. For the experiments on CLIPViT, we directly use the finetuned checkpoints provided in Ilharco et al. (2023) with the data preprocessingstep provided in (Yang et al., 2023). The finetuned models in the GPT2-XL experiments in provided inAppendix F. Localization. Following the practice in Panigrahi et al. (2023), in the localization step, we initialize thetrainable real-valued vector S as the mask for top-k% largest entries in the task vector. Since the actual mapis rounded from (S) but not S, we choose the initial values of S to be either 0 or 3, as (3) is sufficientlyclose to 1. To achieve a sparsity level of 1%, we use the learning rate 1e7, batch size 16, L1 regularizationfactor 1e-5 and perform the optimization for 10 epochs on 64-shot data from each task. Following commonpractice in Panigrahi et al. (2023); Yadav et al. (2023), we only perform localization in the transformerblocks, and do not consider embedding layers. Baselines. We use both task arithmetic and TIES-Merging in a dataless manner, meaning that we directly usetheir recommended hyperparameters without tuning it. To be specific, for task arithmetic, the recommendedscaling factor is 0.4. For TIES-Merging, the recommended scaling factor is 1 and sparsity level is 20%. Thisensures a fair comparison with Dataless Localize-and-Stitch, which we also apply a fixed sparsity levelacross all experiments, namely 5%."
}