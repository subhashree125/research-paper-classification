{
  "Abstract": "Deep neural networks have exhibited remarkable performance in a variety of computer vi-sion fields, especially in semantic segmentation tasks.Their success is often attributedto multi-level feature fusion, which enables them to understand both global and local in-formation from an image.However, multi-level features from parallel branches exhibitsdifferent scales, which is a universal and unwanted flaw that leads to detrimental gradientdescent, thereby degrading performance in semantic segmentation. We discover that scaledisequilibrium is caused by bilinear upsampling, which is supported by both theoreticaland empirical evidence. Based on this observation, we propose injecting scale equalizers toachieve scale equilibrium across multi-level features after bilinear upsampling. Our proposedscale equalizers are easy to implement, applicable to any architecture, hyperparameter-free,implementable without requiring extra computational cost, and guarantee scale equilibriumfor any dataset. Experiments showed that adopting scale equalizers consistently improvedthe mIoU index across various target datasets, including ADE20K, PASCAL VOC 2012, andCityscapes, as well as various decoder choices, including UPerHead, PSPHead, ASPPHead,SepASPPHead, and FCNHead.",
  "Introduction": "Deep neural networks have shown remarkable performance, especially in the computer vision field. Theirsubstantial modeling capability has enabled us to develop significantly accurate models with rich imagefeatures for a wide range of vision tasks, including object detection and semantic segmentation. One challenge in computer vision tasks is understanding both the global and local contexts of an image (Reddiet al., 2018; Tu et al., 2022). Indeed, the cascade architecture of a deep neural network faces difficulty inunderstanding multiple contexts of an image owing to the single-level feature it uses. To address this problem,modern vision networks have employed a parallel architecture that aggregates multi-level features in differentspatial sizes to extract both global and local information from an image. For semantic segmentation as anexample, multi-level feature fusion has been adopted in numerous models such as UPerNet (Xiao et al.,2018), PSPNet (Zhao et al., 2017), DeepLabV3 (Chen et al., 2017), DeepLabV3+ (Chen et al., 2018b), FCN(Long et al., 2015), and U-Net (Ronneberger et al., 2015). Although the parallel architecture builds multiple fastlanes to facilitate multi-level features to contributeto output, if certain features are not involved in the fusion, they simply waste computational resources.Initially, all feature branches should be exploited to explore their potential usefulness, and after training,their optimal combination should be obtained. Thus, the underlying assumption of multi-level feature fusionis that, at least in an initialized state, all multi-level features will participate in producing a fused feature.However, we claim that existing architectural design for multi-level feature fusion in semantic segmentationhas a potential problem of scale disequilibrium, which yields unwanted bias that diminishes the contribution",
  "Published in Transactions on Machine Learning Research (11/2024)": ": Scale equalizer with simple implementation (left) and efficient implementation (right). For efficientimplementation, pre-computed global mean and std are applied to the weight and bias of the fusion layer inadvance, as depicted by the gray dotted line. Here, the main training requires only black dotted lines, whichmaintains the same computational cost compared with the case without scale equalizers. Furthermore, wecan remove the bias correction due to the subsequent batch normalization. changing the pipeline from [ConvBatchNormReLUUP] to [ConvReLUUPBatchNorm]. This pipelineyields a normalized feature with a consistent scale but requires extra computational cost. Because batchnormalization computes the mean and standard deviation (std) of the current incoming feature map acrossthe mini-batch, its computational complexity increases with the larger size of the feature (Huang et al.,2018). The computational complexity of the backward operation further increases with the larger size ofthe feature map because the derivative for batch normalization is much more complicated (Yao et al., 2021).Consequently, applying batch normalization to an upsampled feature causes a significantly more expensivecomputation compared with that of a non-upsampled feature. Considering this problem, we alternativelyexplore a computationally efficient solution to acquire scale equilibrium. Here, we propose scale equalizer, a module to be injected after bilinear upsampling but before concatenation.To achieve scale equilibrium at minimal cost, we design the scale equalizer as simple as possible. Specif-ically, our proposed scale equalizer normalizes target feature x using global mean and global std asScaleEqualizer(x) := (x )/. The global mean and std are scalars computed from the target featurex across the training dataset, which can be performed before training. Once the global mean and std areobtained, they can be set as fixed constants during training, which simplifies forward and backward op-erations for the scale equalizer. By contrast, mean and std are not constants for common normalizationoperations such as batch normalization or layer normalization because they use a mean and std of a currentincoming feature. Thus, compared with existing normalization operations, the proposed scale equalizer canbe implemented with little extra cost. Scale Equalizers Equalize ScalesNow consider multi-level feature fusion with scale equalizers, wherethe scale equalizer is applied after bilinear upsampling of each branch but before concatenation. The con-catenation subject ScaleEqualizeri(UPri(Pi)) exhibits zero mean and unit variance, which assures scaleequilibrium. Because the scale equalizer uses empirically measured values of the global mean and std, thescale equilibrium does not require architectural restrictions or specific conditions on weight. In other words,scale equilibrium is always guaranteed for any dataset and any architecture of segmentation network. Efficient Implementation via InitializationIn fact, matching intra-layer gradient scales via scaleequalizers can be interpreted as establishing a valid initialization. For multi-level feature fusion y =",
  "FormulationThis study considers the standard framework for supervised learning of semantic segmenta-tion networks because it is a representative task using multi-level feature fusion (). Let I RHW C": "be an input image to a semantic segmentation network, where H, W is the size of the image and C repre-sents the number of image channels. The objective of semantic segmentation is to generate a semantic maskY RHW Nc that classifies each pixel in the image I into one of the Nc categories. A deep neural network,which comprises an encoder and decoder, is employed as a semantic segmentation model that outputs Y fromthe input image I. The encoder is a backbone network with several stages where the input image first goesthrough. The decoderalso referred to as the headuses a set of intermediate feature maps {Ci} from theencoder to produce the segmentation output Y. To quantify a difference between the prediction Y and theground truth Y, a loss function such as pixel-wise cross-entropy is used. With gradient descent optimizationfor the loss function, the encoder and decoder are trained together on an image-label pair dataset by thefine-tuning strategy, where the encoder begins with a pretrained weight whereas the decoder is trained fromscratch.",
  "Multi-Stage Feature Fusion": "ObjectiveSo far, we have discussed the need for scale equalizers for multi-level feature fusion.Theobjective here is to compare the segmentation performance before and after injecting scale equalizers intomulti-stage feature fusion.We considered extensive setups, such as the choice of backbone and targetdataset.For the backbone network, we employed recent vision transformers that achieved state-of-the-art performance. Nine backbones of Swin-{T, S, B} (Liu et al., 2021), Twins-SVT-{S, B, L} (Chu et al.,2021), and ConvNeXt-{T, S, B} (Liu et al., 2022) pretrained on ImageNet-1K (Deng et al., 2009) wereexamined, where T, S, B, and L stand for tiny, small, base, and large models, respectively. These encodersrequire bilinear upsampling for multi-stage feature fusion. Targeting multi-stage feature fusion, we employedUPerHead (Xiao et al., 2018). Two datasets were examined, including the ADE20K (Zhou et al., 2019) andPASCAL VOC 2012 (Everingham et al., 2015). HyperparametersTo follow common practice for semantic segmentation,training recipes fromMMSegmentation (Contributors, 2020) were employed. For training with Swin and Twins encoders, AdamWoptimizer (Loshchilov & Hutter, 2019) with weight decay 102, betas 1 = 0.9, 2 = 0.999, and learning rate6 105 with polynomial decay of the 160K scheduler after linear warmup were used. For training withConvNeXt encoders, AdamW optimizer with weight decay 5102, betas 1 = 0.9, 2 = 0.999, learning rate104 with polynomial decay of the 160K scheduler after linear warmup, and mixed precision training (Mi-cikevicius et al., 2018) were used. The training was conducted on a 4 GPU machine, and SyncBN (Zhanget al., 2018) was used for distributed training. We measured the mean intersection over union (mIoU) andreported the average of five runs with different random seeds. DatasetsThe ADE20K dataset contains scene-centric images along with the corresponding segmentationlabels. A crop size of 512 512 pixels was used, which was obtained after applying mean-std normalizationand a random resize operation using a size of 2048512 pixels with a ratio range of 0.5 to 2.0. Furthermore,a random flipping with a probability of 0.5 and the photometric distortions were applied. The objective wasto classify each pixel into one of the 150 categories and train the segmentation network using the pixel-wisecross-entropy loss. The same goes for the PASCAL VOC 2012 dataset with 21 categories, and we followedthe augmented PASCAL VOC 2012 dataset. ResultsWe observed that injecting scale equalizers into multi-stage feature fusion improved the mIoUindex compared with the same models without scale equalization (). The mIoU increases of about+0.1 to +0.4 were consistently observed across all setups of nine backbones and two datasets. Note thatthe scale disequilibrium arises within the decoder at the concatenation layer after upsampling. Therefore,",
  "NotationMeaning": "IAn input image to a semantic segmentation network.H, W, CHeight, width, and the number of channels for the input image.YPredicted result for semantic segmentation.NcThe number of classes to be classified in the semantic segmentation task.{Ci}A set of intermediate feature maps {Ci} from the encoder.{Li}Laterals UPerHead.{Fi}Outputs of the top-down pathway of the FPN.{Pi}Outputs of the FPN, which is concatenation subjects with optional bilinear upsampling.UPrr blinear upsampling.rUpsampling ratio in bilinear upsampling.hA convolutional unit block.ZFused feature after multi-level feature fusion.sOutput stride, i.e., downsampling ratio at the last stage.nbThe number of branches in multi-level feature fusion.xiith concatenation subject in multi-level feature fusion.wiith weight of the linear layer in multi-level feature fusion.bBias in the linear layer in multi-level feature fusion.yThe fused feature y = i wixi + b in multi-level feature fusion.ywiPartial derivative of y with respect to wi.E[x]Mean of a feature x.Var[x]Variance of a feature x.LLoss function such as the pixel-wise cross-entropy loss function.Learning rate used in gradient descent.Scale term in batch normalization, which is initialized to one.Shift term in batch normalization, which is initialized to zero.WWeight matrix.xFeature vector.ReLURectified Linear Unit as ReLU(x) = max(0, x).BatchNormBatch normalization operation.WfuseWeight matrix of the convolutional layer in the convolutional unit block of fusion.ZfuseAn intermediate result that is obtained after the convolutional layer of fusion with Wfuse.XArbitrary feature.N(, 2)Normal distribution with mean and variance 2. modules, such as the pyramid pooling module (PPM) (Zhao et al., 2017), feature pyramid network (FPN)(Lin et al., 2017), and convolutional unit block, which is composed of convolution, batch normalization, andReLU operation. Firstly, each of the three feature maps {C2, C3, C4} is subjected to a convolutional unitblock to yield laterals {L2, L3, L4}. Additionally, the last lateral L5 is produced from the last feature mapC5 using the PPM module that is described in .2. Now, a subnetwork called FPN performs thetop-down pathway to laterals to obtain its output Fi = Li + UP2(Fi+1) for i {2, 3, 4} and F5 = L5, wherethe operation UPr denotes r bilinear upsampling (Appendix A). Subsequently, FPN applies a convolu-tional unit block hi to each result to yield its output Pi = hi(Fi) for i {2, 3, 4, 5}. The set of FPN output{P2, P3, P4, P5} has the same spatial size as encoder features {C2, C3, C4, C5}, keeping their downsamplingratios {4, 8, 16, 32}. Thus, feature fusion for FPN outputs requires 2i2 bilinear upsampling for each Pi toensure that they share the same spatial size of H/4 W/4. Finally, they can be concatenated together withrespect to channel dimension and fused with a convolutional unit block h as",
  "Single-Stage Feature Fusion": "ObjectiveNow we examine single-stage feature fusion. The target encoder was ResNet-101 (He et al.,2016), which was modified to exhibit output stride s = 8 and was pretrained on ImageNet-1K. We targetedmost standard and popular decoders, including FCNHead (Long et al., 2015), PSPHead (Zhao et al., 2017),ASPPHead (Chen et al., 2017), and SepASPPHead (Chen et al., 2018b). The target datasets were theCityscapes (Cordts et al., 2016) and ADE20K datasets. HyperparametersSimilar to .1, training recipes from MMSegmentation were employed. Fortraining on the Cityscapes dataset, stochastic gradient descent with momentum 0.9, weight decay 5 104,and learning rate 102 with polynomial decay of the 80K scheduler were used. The same goes for trainingon the ADE20K dataset while using the 160K scheduler instead. The training was conducted on a 4 GPUmachine, and SyncBN was used for distributed training. We measured the mIoU and reported the averageof five runs with different random seeds. DatasetsThe Cityscapes dataset contains images of urban street scenes along with the correspondingsegmentation labels. A crop size of 1024 512 pixels was used, which was obtained after applying mean-stdnormalization and a random resize operation using a size of 2048 1024 pixels with a ratio range of 0.5 to2.0. Furthermore, a random flipping with a probability of 0.5 and the photometric distortions were applied.The objective was to classify each pixel into one of the 19 categories and train the segmentation networkusing the pixel-wise cross-entropy loss. Experiments on the ADE20K followed the description in .1. ResultsSimilarly, injecting scale equalizers consistently improved the mIoU index across all setups of fourdecoder heads and two datasets (), which verifies the effectiveness of scale equalizers for any choiceof architecture. See the Appendix D for more experimental results.",
  "Z = h([C5; UPH/s(P1); UPH/2s(P2); UPH/3s(P3); UPH/6s(P6)]),(2)": "where H = W is assumed for notational simplicity. Similarly, the fused feature Z is then subjected to a1 1 convolution and a 4 bilinear upsampling to yield a predicted semantic mask Y, which has the size ofH W Nc. ASPPHead and OthersASPPHead refers to the head deployed in DeepLabV3 (Chen et al., 2017). Ituses atrous convolution (Yu & Koltun, 2016; Chen et al., 2018a), which generates empty space between eachelement of the convolutional kernel. To extract both global and local information from a feature map, theASPPHead adopts multiple atrous convolutions with various atrous rates in parallel. For the last featuremap C5, the first branch applies a series of global average pooling (GAP), convolutional unit block, andbilinear upsampling to restore the spatial size prior to the GAP. Each of the other four branches applies aconvolutional unit block whose convolutional operation adopts an atrous rate {1, a, 2a, 3a}, where a = 96/s.The results from the five branches are concatenated together with respect to channel dimension, and thena convolutional unit block h is applied to fuse them. Denoting the outputs of convolutional unit blocks inparallel branches as {PGAP, P1, Pa, P2a, P3a}, fusing them is represented as",
  "Z = h([UPH/s(PGAP); P1; Pa; P2a; P3a]).(3)": "Similarly, the fused feature Z is then subjected to a 1 1 convolution and a s bilinear upsampling to yielda predicted semantic mask Y, which has the size of H W Nc. In DeepLabV3+ (Chen et al., 2018b),a variant called SepASPPHead is developed using depthwise separable convolutions instead, while keepingthe same decoder architecture. This single-stage feature fusion has also been used in other segmentationnetworks such as FCN (Long et al., 2015) and U-Net (Ronneberger et al., 2015), which progressively repeatsfusion for two features with upsampling at each time. Summary and GeneralizationAs reviewed above, modern decoders of segmentation networks performmulti- or single-stage feature fusion, which we collectively refer to as multi-level feature fusion. Althougheach decoder has a distinct architecture, their feature fusions share a similar design pattern (). Usingsingle or multiple encoder features, certain operations are applied in parallel branches, and the convolutionalunit block in the ith branch generates the ith feature map Pi for i {1, , nb} for the number of branchesnb. Because the spatial size of each feature map Pi differs, optional ri bilinear upsampling is neededto assure the same spatial size. For notational simplicity, 1 bilinear upsampling is defined as the identityoperation. Because the set of encoder features for fusion includes a feature map that does not require bilinearupsampling, at least one branch exhibits the upsampling ratio ri = 1, whereas others use ri > 1. The fused",
  "Problem Statement": "As reviewed above, the decoder of the segmentation network includes a module to fuse features of variedsizes. Here, we claim that multi-level feature fusion requires explicit scale equalization because they exhibitdifferent scales, which causes scale disequilibrium on gradients (). To understand feature scale, this study uses feature variance. Other measures such as the norm depend onthe size of the feature, whereas variance provides a suitably scaled result with respect to its size. Owing tothe effectiveness of variance in understanding feature scales, it has been adopted in several pieces of literature(Glorot & Bengio, 2010; He et al., 2015; Klambauer et al., 2017). We also employ the mean of a feature tounderstand its representative value as occasion arises. Using variance, we describe the scale disequilibriumas follows.",
  "w1 ] > Var[ y": "w2 ],whose landscape is difficult to optimize through gradient descent. (Bottom) Achieving scale equilibriumVar[x1] = Var[x2] stabilizes the landscape and corresponding gradient descent optimization with respect tow1 and w2. Proposition 3.1. Consider a multi-level feature fusion, where a concatenated feature [x1; x2] is subjectedto a linear layer with weight [w1, w2] and bias b to yield the fused feature y = w1x1 + w2x2 + b. When thetwo features x1 and x2 are on different scales, i.e., Var[x1] = Var[x2], the gradients of the fused feature withrespect to the corresponding weight exhibit scale disequilibrium, i.e., Var[ y",
  "w1 ] = 10Var[ y": "w2 ], and thus gradient descent on w2 is on a ten times smaller scale than w1,which slows down the training on w2 (). However, gradient descent optimizers inherently assume scaleequilibrium on gradients (Zeiler, 2012): For gradient descent wi wi L wi , the weight initializer sets thesame scale of initial weight Var[wi] for weights within the same linear layer, and common gradient descentuses a single learning rate without scale discrimination, which leads to difficulty in capturing differentgradient scales Var[ L wi ]. Note that existing literature (Glorot & Bengio, 2010; He et al., 2015; Klambaueret al., 2017; Bachlechner et al., 2021) have discussed matching gradient scales across inter-layers to ensurestable gradient descent without poor training dynamics such as vanishing or exploding gradients. On top ofinter-layer gradient scales, we claim to equalize intra-layer gradient scales. For the feature fusion scenario,matching the intra-layer gradient scales Var[ y",
  "w1 ] = Var[ y": "w2 ] requires scale equalization for the subjectsof concatenation: Var[x1] = Var[x2]. Achieving scale equilibrium eliminates the hidden factor that causesdegradation in gradient descent optimization, which enhances the training of the segmentation network aswell as the performance of semantic segmentation. Furthermore, the gradient scale indicates the amount of contribution: A smaller scale on the gradient orfeature indicates less contribution to the predicted mask. For example, when the last feature map thatcontains rich, high-level image information contributes little to the predicted mask, the quality of the seg-mentation result would degrade. To use the last feature map while supplementing its deficient informationusing multi-level features, it is desirable to ensure a balanced contribution from the multi-level features. Note",
  ".(6)": "This property also implies that any architecture can be freely chosen before the input of the convolutional unitblock. Furthermore, batch normalization guarantees a consistent mean and variance for each channel (Ioffe& Szegedy, 2015). This channel-wise normalization is preferable because the output features from multiplebranches are concatenated with respect to channel dimension. These characteristics of batch normalizationexplain why it is still preferred for the decoder of segmentation networks, despite the existence of severalalternatives, such as layer normalization, which does not perform channel-wise normalization (Ba et al., 2016).In summary, batch normalization provides a feature with a consistent scale, which allows the concatenationof several features from convolutional unit blocks. Bilinear Upsampling Breaks Scale EquilibriumHowever, even with batch normalization, featurescales exhibit disequilibrium when subsequently using bilinear upsampling. Consider a multi-level featurefusion for {P1, P2}, where each feature is an output of a convolutional unit block, and the latter P2 needsr bilinear upsampling with r > 1 to become the same spatial size as P1. Fusing them requires computing",
  "Zfuse = Wfuse[P1; UPr(P2)],(7)": "which is an intermediate result after convolutional layer of fusion with Wfuse. Here, we investigate the scaleof concatenation subjects. Although convolutional unit blocks on parallel branches assure consistent scalesfor {P1, P2}, scales of concatenation subjects {P1, UPr(P2)} are not guaranteed to be equal. Indeed, weclaim that scale disequilibrium occurs during this feature fusion due to bilinear upsampling. Specifically, weprove that bilinear upsampling decreases feature variance:",
  "Theorem 3.2. Bilinear upsampling decreases feature variance, i.e., Var[UPr(X)] < Var[X] for upsamplingratio r > 1 and feature X that is not a constant feature": "The constant feature here indicates a vector with the same constant elements. Note that bilinear upsamplingconserves feature meanbut not feature variance. Furthermore, variance provides a suitably scaled resultwith respect to its size, which ensures that the decreased variance is not caused by the increased sizedue to upsampling. The decreased variance is caused by the linear interpolation function used in bilinearupsampling, which does not conserve the second moment that is included in the variance. See the Appendix Afor a detailed proof and further discussion.",
  "Thus, injecting scale equalizers is equivalent to adopting an auxiliary initializer with wi = wi/i and b =b": "i wii/i. This auxiliary initializer means calibrating the weights and bias in the linear layer of fusionin advance using expected feature scales ().Furthermore, because batch normalization followssubsequently (), the latter for bias correction is actually not needed, whereas the former for weightcalibration is still needed to control the scales of concatenation subjects.For UPerHead as a concreteexample, weights in the convolutional layer of fusion are partitioned into four groups with respect to channeldimension, and the weights in each group wi are re-scaled via the global std i. In summary, after primaryinitialization of the decoder, we compute the global mean and std for each target feature, apply the auxiliaryinitializer to weights, and then proceed with the main training (Algorithm 1). This implementation requiresno additional computational cost during main training, which enables us to achieve scale equilibrium forfree. As mentioned earlier, there may be other ways to achieve scale equilibrium by introducing complicatedoperations. Nevertheless, to demonstrate the effectiveness of scale equilibrium under the same computationalcost, we opt for injecting scale equalizers and their efficient implementation through auxiliary initialization,which achieves scale equilibrium for free. Notes on Advantage of Scale EqualizationEven though the initial state exhibited scale disequilibriumfor existing multi-level feature fusion, if neural network parameters are well optimized during training, it mayachieve scale equilibrium after training. Nevertheless, our claim is that it would be better to achieve scaleequilibrium from the initial state to avoid poor training dynamics. Additionally, note that the proposed scaleequalizer does not introduce new learnable parameters in the neural network; rather, it behaves as constantscaling with fixed values. Therefore, even though scale equalizers are injected, the models representationability remains the same. While maintaining the same representation ability, the advantage of scale equalizerscomes from the easier optimization in gradient descent by avoiding poor training dynamics (.1). Thisbehavior would be rather similar to the initialization method of a neural network. For example, applyingXavier or He initialization (Glorot & Bengio, 2010; He et al., 2015) facilitates optimization in gradientdescent, but they do not introduce new learnable parameters.",
  "Decoderw/o Scale EQw/ Scale EQDiffw/o Scale EQw/ Scale EQDiff": "FCNHead (Long et al., 2015)76.57876.972+0.39439.78039.958+0.178PSPHead (Zhao et al., 2017)79.39479.858+0.46443.97044.228+0.258ASPPHead (Chen et al., 2017)79.31279.720+0.40844.85445.004+0.150SepASPPHead (Chen et al., 2018b)80.44880.592+0.14445.14445.486+0.342 this problem is related to the architectural design of the decoder. Using a larger encoder network, albeithaving much expressivity, cannot solve this problem. This explains why scale equalization matters fromtiny to large backbone models. Furthermore, our method goes beyond the trade-off between computationalcost and performance. The proposed method does not introduce additional layers; while keeping the samearchitecture and expressive power of the deep neural network, the performance gain of the proposed methodcan be obtained in actually free (.2). In other words, scale equalization provides a free performancegain without incurring additional computational expenses.",
  "feature, which achieves an equalized scale across concatenation subjects. This behavior can also be achievedwith other normalization layers, such as GroupNorm and LayerNorm": "Although the use of a normalization layer after upsampling can be another solution to achieve scale equi-librium, it causes increased computational costs due to the enlarged size of the feature map. Because meanand std are computed for the incoming feature map, the larger size of the feature map requires much com-putational cost for computing the mean and std. Furthermore, the computed mean and std are used fornormalization of each element, which leads to increased complexity overall. This behavior can be verified through simulation.Using x RNCHW for N = 16 and C = 128,we simulated 8 bilinear upsampling and compared the computation time required for the four pipelines:[ConvBatchNormReLUUP], [ConvReLUUPBatchNorm], [ConvReLUUPGroupNorm], and [ConvReLUUPLayerNorm]. summarizes the results. We observed that the existing [ConvBatchNormReLUUP] pipeline consistently exhibited faster computation, whereas modified pipelines showed signifi-cantly slower computation. Note that the computational cost in case of injecting scale equalizers is equalto the existing pipeline of [ConvBatchNormReLUUP] because it can be implemented without extra cost",
  "(.2, Algorithm 1); therefore, in terms of computational complexity, we claim that our proposedmethod is superior compared with the use of a normalization layer": "In addition to computational costs, we compared segmentation performance when using modified pipelines(). We empirically observed that modifying the existing ordering of [ConvBatchNormReLUUP]has a side effect of degraded segmentation performance, which outweighs possible advantages. This phe-nomenon was consistently confirmed for BatchNorm, GroupNorm, and LayerNorm. Although GroupNormor LayerNorm might yield improved performance compared with BatchNorm, their performances were evenbelow the baseline performance of the existing pipeline of [ConvBatchNormReLUUP]. Furthermore, ap-plying a normalization layer after an upsampled feature requires much computational cost. Consideringboth computational cost and segmentation performance, the best pipeline is our proposed pipeline of [ConvBatchNormReLUUPScaleEQ]. In consideration of this, we opt for keeping the existing pipeline withoutmodification in its ordering while injecting a scale equalizer at the end of the pipeline.",
  "Scale Equalization for Other Tasks": "We find that the use of upsampling and multi-level feature fusion is prevalent in the machine learningcommunity. Although we focused on the multi-level feature fusion in semantic segmentation networks asa prime example, our scale equalization matters for other multi-level feature fusion tasks.Specifically,scale equalization generally matters for modern encoder-decoder networks. For example, monocular depthestimation networks have used the encoder-decoder architecture, whose multi-level feature fusion exhibitsscale disequilibrium similar to the semantic segmentation networks. In consideration of this, we additionally verified scale equalization for a monocular depth estimation network.The target model was GEDepth (Yang et al., 2023), where its feature fusion module concatenates upsampledand non-upsampled feature maps. Using the KITTI dataset (Geiger et al., 2013), we trained the model withand without scale equalizers in the feature fusion module ().We observed that injecting scaleequalizers improved the performance of the monocular depth estimation task, which connotes that scaleequalization similarly matters for other tasks where encoder-decoder architecture is deployed.",
  "Qualitative Analysis": "provides segmentation examples for the ADE20K dataset. We find that injecting scale equalizersleads to a better understanding of the global context of images.Specifically, with scale equalizers, theglobal layout is better captured for large parts. Indeed, our analysis says that existing multi-level featurefusion suffers from scale disequilibrium, which lowers the contribution of the last feature map that containsrich global information. Here, injecting scale equalizers facilitates the last feature map to be involved inmulti-level feature fusion, which leads to a better understanding of the global context of the image.",
  "Conclusion": "This study discussed the scale disequilibrium in multi-level feature fusion for semantic segmentation tasks.First, we reviewed the mechanisms of existing segmentation networks, which perform multi- or single-stagefeature fusion. We demonstrated that the existing multi-level feature fusion exhibits scale disequilibriumdue to bilinear upsampling, which causes degraded gradient descent optimization. To address this problem,injecting scale equalizers is proposed to guarantee scale equilibrium for multi-level feature fusion. Experi-",
  "Liang-Chieh Chen, George Papandreou, Florian Schroff, and Hartwig Adam. Rethinking Atrous Convolutionfor Semantic Image Segmentation. CoRR, abs/1706.05587, 2017": "Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L. Yuille. DeepLab:Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully ConnectedCRFs. IEEE Trans. Pattern Anal. Mach. Intell., 40(4):834848, 2018a. Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig Adam. Encoder-Decoderwith Atrous Separable Convolution for Semantic Image Segmentation. In ECCV (7), volume 11211, pp.833851, 2018b. Xiangxiang Chu, Zhi Tian, Yuqing Wang, Bo Zhang, Haibing Ren, Xiaolin Wei, Huaxia Xia, and ChunhuaShen. Twins: Revisiting the Design of Spatial Attention in Vision Transformers. In NeurIPS, pp. 93559366, 2021.",
  "Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: A large-scale hierarchicalimage database. In CVPR, pp. 248255, 2009": "Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Un-terthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and NeilHoulsby. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. In ICLR, 2021. Mark Everingham, S. M. Ali Eslami, Luc Van Gool, Christopher K. I. Williams, John M. Winn, and AndrewZisserman. The Pascal Visual Object Classes Challenge: A Retrospective. Int. J. Comput. Vis., 111(1):98136, 2015.",
  "Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia.Pyramid Scene ParsingNetwork. In CVPR, pp. 62306239, 2017": "Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei Fu, JianfengFeng, Tao Xiang, Philip H. S. Torr, and Li Zhang. Rethinking Semantic Segmentation From a Sequence-to-Sequence Perspective With Transformers. In CVPR, pp. 68816890, 2021. Bolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fidler, Adela Barriuso, and Antonio Torralba.Semantic Understanding of Scenes Through the ADE20K Dataset. Int. J. Comput. Vis., 127(3):302321,2019.",
  "AProof of Theorem 3.2": "NotationIn this section, we prove Theorem 3.2.Here, we consider 1D bilinear upsampling, because2D bilinear sampling is a straightforward extension of it. For a given sequence X = {X1, , Xn} of sizen, applying r bilinear upsampling yields a sequence UPr(X) of size rn.Because these sequences arediscrete, bilinear upsampling can be thought of as a coordinate transformation, which transforms coordinate{p1, , pn} into coordinate {q1, , qrn}.We follow the common option of bilinear upsampling to setalign_corners=False, where each coordinate is regularly spaced and shares its center. Within pi and pi+1for i {1, , n1}, the upsampled coordinate has r regularly spaced points as qr(i0.5)+j = pi+(2j1)l forl = pi+1pi 2rand j {1, , r}. Because bilinear upsampling is a piece-wise linear interpolation, we representthe piece-wise linear function as f(x) where 1) for coordinate point pi, we define f(pi) := Xi for i {1, , n},2) following the behavior of align_corners=False, we define f(x) := X1 on left outer interval x (, p1)and f(x) := Xn on right outer interval x (pn, +), and 3) on ith interval x (pi, pi+1), we define f(x)as a linear line f(x) := aix + bi connecting two points (pi, Xi) and (pi+1, Xi+1) where ai = Xi+1Xi",
  ": Example of two coordinates and a piece-wise linear function for 4 bilinear upsampling": "ObjectiveFirstly, we know that bilinear upsampling conserves feature mean, i.e., E[UPr(X)] = E[X].Using the above-mentioned notation, this property can be represented as Exp[f(x)] = Exq[f(x)]. In fact,mean-conservation holds by the definition of the two coordinates because they share the center. Now, weinvestigate feature variance before and after bilinear upsampling. Because Var[X] = E[X2] (E[X])2, weinspect the second moment E[X2] for the two coordinates. In other words, we compare Exp[(f(x))2] andExq[(f(x))2].",
  ": Illustration of two coordinates on the ith interval of x (pi, pi+1)": "For qr(i0.5)+j on the left half area with j {1, , r/2}, we have pi qr(i0.5)+j = (2j 1)l. Becausef(x) is defined as f(x) = aix+bi on the ith interval x (pi, pi+1), we know that f(x1)f(x2) = ai(x1 x2)and f(x1) + f(x2) = ai(x1 + x2) + 2bi. Thus, we have",
  "= a2i l(2j 1){(pi+1 pi) + (qr(i+0.5)+1j qr(i0.5)+j)} 0.(23)": "Equality holds if ai = 0, which requires Xi+1 = Xi. In summary, Eq. 16 can be written as the sum of jthterms for j {1, , r/2}, where each jth term is always non-negative. Thus, we conclude that Pi Qi 0for i {1, , n 1}. So far, we have compared the sub-terms of Eqs. 10 and 11 for two coordinates on each of the ith intervalfor i {1, , n 1}. We are left with two sub-terms on the left outer interval and the right outer interval.Fortunately, by the definition of bilinear upsampling on outer intervals, we have",
  "Thus, on the outer intervals, sub-terms of Eqs. 10 and 11 are identical": "Finally, from Eqs. 10, 11, 23, and 25, we obtain Exp[(f(x))2] Exq[(f(x))2]. Note that equality holdsif ai = 0, i.e., Xi+1 = Xi for all i {1, , n 1}, which is only satisfied if the original feature is aconstant feature. However, because we consider original data that is not a sequence of constants, we knowthat at least one of ai exhibits ai = 0, which yields Exp[(f(x))2] > Exq[(f(x))2]. This inequality leads toVarxp[f(x)] > Varxq[f(x)]. Therefore, we conclude that applying bilinear upsampling decreases variance."
}