{
  "Abstract": "We present the workflow of Online Iterative Reinforcement Learning from Human Feed-back (RLHF) in this technical report, which is widely reported to outperform its offlinecounterpart by a large margin in the recent large language model (LLM) literature. How-ever, existing open-source RLHF projects are still largely confined to the offline learningsetting. In this technical report, we aim to fill in this gap and provide a detailed recipethat is easy to reproduce for online iterative RLHF. In particular, since online human feed-back is usually infeasible for open-source communities with limited resources, we start byconstructing preference models using a diverse set of open-source datasets and use the con-structed proxy preference model to approximate human feedback. Then, we discuss thetheoretical insights and algorithmic principles behind online iterative RLHF, followed by adetailed practical implementation. Our trained LLM achieves impressive performance onLLM chatbot benchmarks, including AlpacaEval-2, Arena-Hard, and MT-Bench, as wellas other academic benchmarks such as HumanEval and TruthfulQA. We have shown thatsupervised fine-tuning (SFT) and iterative RLHF can obtain state-of-the-art performancewith fully open-source datasets. Further, we have made our models, curated datasets, andcomprehensive step-by-step code guidebooks publicly available.",
  "Introduction": "Reinforcement Learning from Human Feedback (RLHF) (Christiano et al., 2017; Ziegler et al., 2019) hasbecome as a key technique for integrating human preference signals into machine learning methods.Inparticular, RLHF has become a standard component in the post-training pipe line of foundation LargeLanguage Models (LLMs), which serves to align the outputs of these models with human values such ashelpfulness, harmlessness, and honesty (Ouyang et al., 2022; Bai et al., 2022a). Notable examples includethe revolutionary closed-source ChatGPT (OpenAI, 2023), Claude (Anthropic, 2023), and Gemini (Teamet al., 2023), as well as the powerful open-source models like Zephyr (Tunstall et al., 2023), Starling (Zhuet al., 2023), and LLaMA-3 (Meta, 2024). In particular, since the introduction of ChatGPT, RLHF hasattracted significant interest in a diverse set of communities. However, compared to the supervised fine-tuning that is rather well studied with many great open-source projects like Open-Hermes (Teknium, 2023)and Vicuna (Zheng et al., 2023), RLHF remains relatively under-explored within the open-source community. The first four authors are core contributors to this project listed in random order. The full authorship contribution state-ments are provided in Appendix A. Email: {hanze.dong, b.pang, yingbo.zhou, dsahoo, cxiong}@salesforce.com, {wx13,hwang264, hanzhao, nanjiang, tozhang}@illinois.edu.Project leads.",
  "Published in Transactions on Machine Learning Research (09/2024)": "Haoxiang Wang, Yong Lin, Wei Xiong, Rui Yang, Shizhe Diao, Shuang Qiu, Han Zhao, and Tong Zhang.Arithmetic control of llms for diverse user preferences: Directional preference alignment with multi-objective rewards. arXiv preprint arXiv:2402.18571, 2024. Zhilin Wang, Yi Dong, Jiaqi Zeng, Virginia Adams, Makesh Narsimhan Sreedhar, Daniel Egert, OlivierDelalleau, Jane Polak Scowcroft, Neel Kant, Aidan Swope, and Oleksii Kuchaiev.Helpsteer: Multi-attribute helpfulness dataset for steerlm, 2023.",
  "where (z) = 1/(1 + exp(z)) is the sigmoid function": "Although the BT model may not fully capture the complex human preference, it tends out to be a usefulapproximation to connect the learning objective of RLHF with reward maximization and has achievedtremendous success in making ChatGPT (Ouyang et al., 2022) and Claude (Bai et al., 2022a). Meanwhile,in response to the imperfect nature of BT model, the goal in this RLHF formulation is to optimize thefollowing KL-regularized target:",
  ") and (2) (offline) direct preference learning (e.g., DPO) approaches (Zhao et al., 2023; Rafailov et al.,2023; Azar et al., 2023; Tang et al., 2024)": "DRL-based framework. The DRL-based framework consists of two stages. In the first stage, a rewardmodel is trained. Specifically, given a preference dataset Doff = {(x, aw, al)}, where aw is a response preferredover al given the instruction or prompt x. The log-likelihood function of the BT model can be expressed as:",
  "(a|x)": "This approach has been employed by ChatGPT (Ouyang et al., 2022) and Claude (Bai et al., 2022a) andhas contributed to the alignment of LLaMA-2 (Touvron et al., 2023). However, it is known that even inthe best case, tuning the DRL method to its best performance requires extensive efforts in hyper-parameterselection and code-level optimization (Choshen et al., 2019; Engstrom et al., 2020).This becomes evenmore challenging in the context of LLMs, as fine-tuning LLMs is computationally expensive and searchingthe complicated hyper-parameters configuration is generally infeasible. Additionally, the PPO algorithmrequires loading multiple LLMs simultaneously, including the actor (policy), critic (value network), rewardmodel, and reference model (for KL estimation), which places significant pressure on GPU memory, especiallyfor resource-constrained open-source projects. Direct preference learning. In view of the above issues of PPO, there is an innovative line of workthat directly learns from human preference datasets without explicitly constructing a reward function (Zhaoet al., 2023; Rafailov et al., 2023; Azar et al., 2023). Among these methods, the direct preference optimization(DPO) algorithm is particularly popular. It leverages Equation 3 to formulate reward as a function of policyand directly optimizes the following loss function using the preference dataset Doff:",
  ".(5)": "In the ideal case where there is no optimization error, the minimizer of Equation (5) is the same as thetwo-staged DRL framework (Rafailov et al., 2023; Azar et al., 2023). Meanwhile, direct preference learningalgorithms are generally easier to tune and require fewer computational resources compared to DRL methods.Considering these factors, in our project, we focus on direct preference learning algorithms while leaving thestudy of the DRL-based framework for future research. The open-source project Zephyr (Tunstall et al., 2023) serves as a milestone to popularize the DPO algorithm,where the authors provide a comprehensive guide for training LLMs through vanilla DPO and distillationfrom the teacher model ChatGPT. Following the Zephyr framework, many open-source models have beenfine-tuned using vanilla DPO and their qualities are largely improved compared to their SFT counterparts.Impressively, on various LLM benchmarks leaderboards, such as those reported by (Dubois et al., 2023;Zheng et al., 2023), models fine-tuned using DPO predominantly exhibit superior alignment and effectiveness. While the vanilla offline direct preference learning algorithms are useful in some case studies, they also facecertain challenges. Specifically, they are considered offline because they learn from an offline preferencedataset collected prior to the alignment process. We can formulate this data collection process as:",
  "x d0, a1 1D, a2 2D,y BerP(a1 a2|x, a1, a2).(6)": "Here, (1D, 2D) represent two behavior policies, often taken as 0, other open-sourced models or proprietarymodels. The term offline learning implies that we cannot further query the preference oracle P during thetraining process. However, the finite dataset Doff fails to cover the entire prompt-response space and theresulting policy model often performs poorly when faced with out-of-distribution data (Burns et al., 2023).",
  "Online Iterative RLHF": "In contrast, the Claude project (Bai et al., 2022a) and LLaMA-2 project (Touvron et al., 2023) have demon-strated that online iterative RLHF can significantly improve model performance.The process of onlineiterative RLHF, as formally formulated in Xiong et al. (2023), can be summarized as follows. Given thepre-collected preference dataset D = Doff (if applicable, otherwise empty), for each iteration t [T]:",
  "update D D Dt": "The effectiveness of online data can be intuitively understood as continuously collecting new online datato strategically explore the prompt-response space and mitigating the out-of-distribution (OOD) issue. Wealso refer readers to Xiong et al. (2023) for a detailed theoretical explanation for online iterative RLHF.The hybrid formulation presented here is mainly for generality and is motivated by the LLaMA-2 project(Touvron et al., 2023). This formulation is also loosely related to some of the previous RL literature like Xieet al. (2021); Song et al. (2022). We notice that the results presented in LLaMA-2 and Claude are based on the deep RL method, PPO,and the data, models, and training details are not fully accessible to the open-source community. Moreover,compared to the vanilla offline DPO, its online iterative version is still largely under-explored in the literature.Xiong et al. (2023) made the first step towards understanding the advantage of online exploration in DPOtraining from a theoretical perspective, and the main purpose of this work is to provide a detailed guidanceto make the online iterative RLHF pipeline more accessible to the open-source community so that otherscan easily reproduce.",
  "Human Feedback Approximation": "Ideally, the online preference signal is sampled from a representative group of human labelers. However,human feedback is extremely expensive in practice, which the open-source community usually cannot afford.In the literature, there is a line of work showing that training a proxy preference model, and using thepreference model to give proxy labels in a semi-supervised manner improve the model performance (Donget al., 2023; Yuan et al., 2023; Liu et al., 2023a; Hoang Tran, 2024). We conjecture that this is because thereward model (discriminator) usually generalizes better than the policy (generator). In particular, Hoang Tran (2024) shows that if the preference model (reward model) is trained on a diverseset of preference datasets, the Pair-RM (Jiang et al., 2023) with only 0.4B parameters can provide iterativepreference learning with meaningful signals so that the resulting model1 achieves an impressive AlpacaEval-2length-control win rate of 26.4%. Motivated by this line of work, we first train a proxy preference (reward)model based on the diverse open-source preference datasets in and then use the resulting model toprovide preference signals for the subsequent iterative RLHF.",
  "We have presented many related works in the area of RLHF in the previous subsections. For completeness,we also include a more comprehensive literature review in this subsection": "RLHF and RLHF algorithms.The idea of learning from relative feedback could date back to thestudy of dueling bandit (Joachims et al., 2007; Yue et al., 2012) and the current RLHF framework was firstpopularized by Christiano et al. (2017), which served to direct the attention of the deep RL community tothe preference-based feedback. Then, these techniques were further introduced to fine-tune LLMs for thesummarization task (Ziegler et al., 2019; Stiennon et al., 2020). The dominant RLHF framework used forthe modern LLM alignment was first thoroughly developed in Instruct-GPT (ChatGPT) (Ouyang et al.,2022), Claude (Bai et al., 2022a), and LLaMA-2 (Touvron et al., 2023) projects. These works typicallyinvolve constructing a reward model based on the MLE of the Bradley-Terry model, and then using thePPO algorithm to optimize the reward signals with KL regularization. One notable exception is that theLLaMA-2 uses a mixture of rejection sampling fine-tuning (Dong et al., 2023; Wang et al., 2024) and PPOin their RLHF pipeline. We refer the interested readers to Bai et al. (2022a); Touvron et al. (2023) for adetailed description. However, the use of PPO in RLHF has limitations. It is known to be unstable (Choshenet al., 2019), sensitive to implementation (Engstrom et al., 2020), and resource-intensive (Yuan et al., 2023).Despite some efforts to improve PPO in the context of RLHF (Li et al., 2023; Chan et al., 2024; Changet al., 2024; Zhong et al., 2024), reproducing the successful results achieved with PPO is challenging forthe open-source community due to these limitations as it requires extensive efforts and resources that theopen-source communities usually cannot afford. In recognition of these issues of PPO, a line of work studiesthe (offline) direct preference learning algorithms, including Slic (Zhao et al., 2023), DPO (Rafailov et al.,2023), IPO (Azar et al., 2023), KTO (Ethayarajh et al., 2024), ARM (Pang et al., 2024), and GPO (Tanget al., 2024). These algorithms skip the reward modeling step, and optimize a designed loss target on theoffline preference dataset directly (hence the name). It is widely observed that the direct preference learningalgorithms are much more stable than the PPO, and achieve impressive performance evaluated by standardbenchmarks (Tunstall et al., 2023; Dubois et al., 2023; Zheng et al., 2023). Despite the advances offered by these direct preference learning algorithms, their implementation is typicallyoffline and off-policy. This means they operate on preference datasets that were previously collected byother modelsoften powerful, proprietary teacher models like ChatGPTbefore the training process begins(Tunstall et al., 2023; Cui et al., 2023). RLHF benefits from online (iterative) learning. Roughly speaking, online iterative learning meansthat we will deploy the intermediate models and query human feedback for the responses of these models.Intuitively, this strategy can help to mitigate the OOD issue of the learned reward model (Gao et al., 2023),and its advantages have been reported in Ouyang et al. (2022); Touvron et al. (2023) for the PPO-basedframework. Even when the additional feedback is derived from a proxy reward constructed from the same",
  ": Illustration of the Bradley-Terry (BT) model and preference model": "offline dataset (similar to semi-supervised learning), iterative rejection sampling fine-tuning (RAFT) (Donget al., 2023) and DPO based on samples from the target distribution estimator have been shown to outperformthe original offline counterparts (Pang et al., 2024; Liu et al., 2023a). Furthermore, recent works (Xionget al., 2023; Xu et al., 2023b; Hoang Tran, 2024; Yuan et al., 2024b; Swamy et al., 2024; Chen et al., 2024b;Ye et al., 2024; Guo et al., 2024; Rosset et al., 2024; Tajwar et al., 2024; Calandriello et al., 2024; Wu et al.,2024) have demonstrated that online iterative variants of direct preference learning algorithms significantlyoutperform their offline counterparts. In particular, we refer the interested readers to Guo et al. (2024) forthe extensive experimental results with different offline base algorithms.",
  "Reward Modeling as Human Feedback Approximation": "We present the details of preference model construction in this section, where we study both the rewardmodeling as MLE of the BT model and the general preference model. We consider two versions of thetraining set: mix1: HH-RLHF + SHP + UltraFeedback + Summarization, which is the mixture of datasetused to train the current state-of-the-art open-source model. (Stiennon et al., 2020), and mix2: all theopen-source datasets we collect, with details provided in .",
  "Bradley-Terry Reward Model and Preference Model": "Bradley-Terry model construction. We follow the previous works (Ouyang et al., 2022; Bai et al., 2022a)to initialize the reward model using the SFT model2. We replace the last layer with a linear head to predicta scalar score suitable for preference learning. The reward model is trained using the negative log-likelihoodloss function, enabling maximum likelihood estimation (MLE). This loss function is defined as:",
  "LLaMA-3-8B-itPrompting-93.644.371.373.5LLaMA-2-13BBTmix196.455.555.062.4LLaMA-3-8B-itBTmix299.465.187.886.4LLaMA-3-8B-itPreferencemix298.365.889.794.7": "Preference model construction. A (pairwise) preference model takes a prompt x and two responsesa1, a2 as the input and predicts the probability of P(a1 a2|x, a1, a2) (Jiang et al., 2023). We follow Zhaoet al. (2023); Liu et al. (2023a) to leverage the LLMs capability as a next-token predictor for preferencemodeling. Specifically, for a given preference pair (x, a1, a2, A), where A indicates that the first response ispreferred, the pair is formatted as an instruction-following task:",
  "instruction = [CONTEXT] {x} [RESPONSE A] {a1} [RESPONSE B] {a2}, and label = A": "If the second response is preferred, we replace the label A with B. Then, we simply treat the preferencemodeling as an instruction-following task to fine-tune the model on these instruction-label pairs. To mitigateposition bias (the preference model may prefer the response that is given in the position of RESPONSEA), the order of the responses is randomized during data formatting. During inference, if we denote theprobability of decoding A as pA and the probability of decoding B as pB, then P(a1 a2|x, a1, a2) is takenas pA/(pA + pB). We train the LLaMA-3-8B-based preference model for one epoch. The samples are packedinto blocks with length 3072 and a global batch size of 128 is used. The learning rate is set to lr = 5 106,and a cosine learning rate schedule with a warm-up ratio of 0.03 is employed. We mention in passing thatit is possible to include detailed rubrics in the data format to further improve the preference dataset, whichwe leave for future work (Qin et al., 2023).",
  "Evaluation Result": "We evaluate the models using the RewardBench (Lambert et al., 2024), a benchmark designed to assessreward model capabilities in four categories: Chat, Chat-Hard, Safety, and Reasoning. The main evaluationresults are in . It is evident that without explicit training, the prompting approach is inferior to boththe BT model and the preference model across all metrics. Meanwhile, the preference model outperformsthe BT model in reasoning tasks related to coding and math. We also notice that with more data, especiallydata specified in coding, math, and safety, the reward model trained by mix2 achieves higher accuracy insafety and reasoning compared with early versions of attempts. In particular, we use the Ultra-RM-13B asa reference model in , where we can observe that the extra data related to safety and reasoning (aswell as a stronger base model) largely contribute to the superior performance of our reward model. Length bias in reward modeling. It is known that the LLMs aligned by RLHF usually give longerresponses (Xiong et al., 2023; Yuan et al., 2024b;b), where the length bias also exists in the reward models,likely influenced by the preference data used.To better understand this bias, we randomly sample 2Kprompts from the prompt set and use the SFT model to generate 8 responses per prompt. Then, we computethe lengths and rewards of the responses and plot the heatmaps of the Pearson correlation coefficient betweenthem in . Clearly, both of the two reward models are biased toward the longer responses to somedegree. In comparison, UltraRM-13B demonstrates a stronger bias, as we observe that the mean coefficientof the UltraRM-13B (left) is 0.19, while it is 0.06 for our BT reward (right). This may partially result fromthe use of additional Capybara, OpenOrca, and UltraInteract, whose preferred responses are shorter thanthe rejected responses. We will return to the ablation study of the impacts of the reward models in .",
  "Supervised Fine-Tuning": "The base model used in this project is LLaMA-3-8B. To ensure the reproducibility and openness of theproject, we perform SFT by ourselves to obtain the initial policy 0. We collect a set of high-quality instruc-tion datasets for SFT, such as ShareGPT (Chiang et al., 2023), SlimOrca (Lian et al., 2023b), MathInstruct(Yue et al., 2023), and Evol-Instruct (Xu et al., 2023a) (see the Appendix for a full list). The training iscarried out for one epoch with a learning rate of 2 105. A cosine scheduler is employed, and the globalbatch size is set to 32 with a warm-up ratio of 0.03. To accelerate training, we follow Diao et al. (2023);Tunstall et al. (2023) to pack the samples and use a block size of 8192.",
  "Iterative Direct Preference Learning: Theoretical Insights and Algorithmic Principles": "From a theoretical perspective, one of the most critical metrics in the reinforcement learning process is theuncertainty estimator . For example, considering the linear case, r = , (x, a) such that Rd, B.For any two policies 1t , 2t , we define the information gain as t(, 1t , 2t ) = E1t (x, a1t)E2t (x, a2t)1t( is a constant coefficient), which is the projection of the new feature difference to the historical featurecovariance matrix t = I + t1s=1 Exd0,a11s,a22s((x, a1) (x, a2))((x, a1) (x, a2)).",
  "We present the main algorithmic framework in Algorithm 1 for the online iterative RLHF, and summarizethe key principles and algorithmic ideas as follows": "Hybrid batch learning. We formulate a slightly more general framework to combine an initial offlinedataset with online data collected during training, hence the name hybrid learning, similar to the recipe ofClaude (Bai et al., 2022a) and LLaMA-2 (Touvron et al., 2023). We also use a large batch size m for sparseupdates.",
  "Non-symmetric structure to balance exploitation and exploration. The framework also features anon-symmetric structure by involving a main agent and an enhancer": "Main agent aims to learn . Specifically, for each iteration, the first agent, referred to as themain agent, always takes the optimal policy under the MLE rMLE of the historical data, which canbe viewed as a fully exploitation of the information we collected so far; Enhancer aims to assist the main agents learning. Since the main agent solely exploitsthe historical data, it is effective only when we can continuously obtain new information about thealignment problem from the newly collected online data or the offline data Doff has provided enoughcoverage (which is unlikely to hold in practice as we discuss in .1). The enhancer, therefore,explores in the direction where there is more uncertainty relative to the main agents policy 1t(measured by mt (, 1t , )), while maintaining a moderate KL divergence with 1t .",
  "Theorem 1 (Informal Theorem 2 in (Xiong et al., 2023)). For any precision parameter > 0, with a batchsize m = O( de": "2 ) and other suitable choices of hyper-parameters, then, with high probability, after at mostT = O(de) iterations, we can find a t0 [T] so that J()J(t0)+Ext0d0DKL((|xt0)t0(|xt0)) ,where J() = Exd0[Ea(|x)[r(x, a)] DKL((|x)0(|x))] is the KL-regularized value. Here O hidessome log factors and de is a complexity measure of the RLHF problem. In particular, if the reward functioncan be embedded into a d-dimensional space that is linear in the feature map of : X A Rd, we havede = d. (Zhong et al., 2022; Liu et al., 2023b)",
  "We now shift our focus from the theoretical insight to the practical implementation. We provide an illustra-tion of our implementation in": "The MLE policy. Since the main agent only exploits the data, we can run DPO on the historical data toapproximate the optimal policy under the rMLE: MLEt. We remark that while we use DPO here due to itssimplicity, the Algorithm 1 can be implemented by combining it with any oracle algorithms (e.g., PPO andInfoNCA (Chen et al., 2024a)) that are approximations of the KL-regularized optimization problem. Exploration policy. The primary challenge lies in the choice of enhancer policy for exploration. Recallthat our goal is to find an enhancer policy that maximizes the relative uncertainty to the main agent from the",
  ": end for7: Output: the best policy in (11:T ) by a validation set": "confidence set defined in Equation (8). Unfortunately, the uncertainty estimator does not have an analyticalform except the linear case. But the main insight we can derive here is to maximize the policy differencewith 1t , while maintaining a moderate KL divergence. This motivates us to use model variants of 1t . Wediscuss some popular heuristic implementations here. Adjusting Temperature and Training Steps. In the project of Claude (Bai et al., 2022a), theauthors choose to use the models with different training steps as (1t , 2t ). For instance, if we runPPO for 2 epoch in total, we may take 1t as the model saved at the end of the first epoch and take2t as the one saved at the end of the second epoch. Additionally, the LLaMA-2 project (Touvronet al., 2023) adjusts the sampling temperature of 1t to induce 2t . These modifications introducediversity in the models and facilitate exploration. Rejection Sampling is another popular ensemble-based exploration approach (Nakano et al., 2021;Dong et al., 2023; Gulcehre et al., 2023). In the context of LLMs, it is typically restricted to thebest-of-n sampling. Specifically, we sample n independent responses by 1t for each prompt, andthen use a preference/reward function to rank the responses and take the one with the highestreward as the final output. In other words, we take 2t as the best-of-n variant of 1t . In this way,the 2t enlarges the margins between 1t and provides exploration. Meanwhile, in this case, the KLdivergence between the two policies is upper bounded by log n n1",
  "nand is usually far better thanthis conservative estimation (Beirami et al., 2024);": "After the first submission of this work, there is a line of works proposing to use biased DPO loss inthe online iterative framework to encourage exploration (Xie et al., 2024; Zhang et al., 2024; Cenet al., 2024), whose idea originates from the theoretical RL study (Xiong, 2023; Liu et al., 2023b).Specifically, they add a SFT loss term, also known as the feel-good term, into the loss functionso that the algorithm favors the model that is more optimistic. We refer the interested readers tothese works for a more detailed algorithm description and empirical results. We also integrate theoption of adding such a bias term in the revision of our public code. In our experiments, we use the DPO to approximate the computational oracle and implement DPO withthe open-source package TRL3. We run DPO with the reference model 0 (the SFT model) on the historicaldata for 2 epochs to get the MLE policy MLEt. We use a cosine learning rate scheduler with a peak learningrate of 5e-7 and 0.03 warm-up ratio. We use a global batch size of 128 and use a KL coefficient of = 0.1.To accelerate training, we do not restart from 0 at each iteration as in Bai et al. (2022a); Xiong et al. (2023)but use the last-iteration model as the initial checkpoint and use 0 as the reference model. In this way, thedata used for training is the same as that of Bai et al. (2022a) and Xiong et al. (2023) but is of a differentorder. We do not see performance regression with this choice, and it saves us for half of the training time. To facilitate exploration, we combine the temperature tuning with the rejection sampling strategy withn = 8. Instead of fixing 1t = MLEt(like the center of confidence set) and optimizing the 2t solely to be thebest-of-8 variant of MLEt, we take 1t and 2t as the best-of-8 policy and worst-of-8 policy induced by MLEt.",
  "t1": ": Illustration of our implementation of iterative direct preference learning. In iteration t = 1, thehistorical dataset is empty, and the resulting policy model MLE1is the same as its initialization, 0, whichis the SFT model checkpoint. After that, the historical dataset grows with preference data collected fromprevious iterations. In other words, we take the best response and the worst response as ranked by the reward model to get apreference pair. In this case, we jointly optimize the two policies to maximize their difference (measured bythe uncertainty), which tends to be more efficient in practice and enjoys the same theoretical guarantee asstated in Theorem 1. This choice is similar to Hoang Tran (2024); Pace et al. (2024); Yuan et al. (2024b);Xu et al. (2024). We also drop the pair where 1t and 2t give the same response, which implies that theuncertainty in this direction is already small. For this round of experiments, we still use the reward functiontrained as the MLE of the BT reward model to rank the responses for the following reasons. First, to rankn responses, the complexity of using the reward model is linear in n, while it is far more complicated withthe pairwise preference model. Second, during the early experiments, we observe significant length bias inthe iterative RLHF. Therefore, we would like to explore the strategy to mitigate the length bias, and it isrelatively easier to penalize the reward value with the length of the response. Finally, the BT reward modelis comparable with the preference model except for the reasoning task and it may be already satisfactory forour goal. We leave a more comprehensive comparison between the BT reward model and preference modelfor future study. Prompt set, and data generation. We collect prompts from UltraFeedback (Cui et al., 2023), HelpSteer(Wang et al., 2023), OpenOrca (Lian et al., 2023a), UltraInteract (Yuan et al., 2024a), Capybara (Daniele& Suphavadeeprasit, 2023) and DIBT-10K4 and prepare the full prompt set.In our experiments, we usea subset of 60K prompts and iterate for three iterations, so 20K prompts are used to generate 20K x 16responses per iteration. To accelerate data generation, we use VLLM (Kwon et al., 2023) for inference. Weset the max generation length as 2048, and use a sampling temperature of 1.0/0.7 without any top-k/top-pstrategy. To offer a more intuitive comprehension of our prompts collection, we provide visualization plotsin the Appendix ().",
  "We evaluate the models by standard benchmarks, including AlpacaEval-2, MT-Bench, and Chat-Arena-Hard.Details are provided in the Appendix": "We also measure the ability of the resulting models using academic benchmark, including GSM-8K (Cobbeet al., 2021), MMLU (Hendrycks et al., 2020), HumanEval (Chen et al., 2021), TruthfulQA (Lin et al.,2021), ARC (Clark et al., 2018), and MBPP (Austin et al., 2021). These benchmarks evaluate the modelsability in coding, reasoning, and general knowledge. In particular, it is known that RLHF alignment canintroduce performance degeneration in reasoning, calibration (providing accurate confidence estimates), andtruthfulness capabilities (generating accurate and factual responses), which is also referred to as the alignmenttax in the literature (Ouyang et al., 2022; Bai et al., 2022a; OpenAI, 2023). Therefore, evaluating our modelon these benchmarks is crucial to understanding the impact of iterative RLHF on these specific aspects.",
  "Main Results": "Online iterative RLHF significantly improves conversation quality. We evaluate our models con-versation abilities using AlpacaEval-2, MT-Bench, and Chat-Arena-Hard, (results in ). Compared toother open-source models with less than 10B parameters, our model outperforms them on the conversationand instruction-following benchmarks with a significant margin. Notably, our model trained with iterativeDPO consistently outperforms that of vanilla offline DPO (DPO baseline). This demonstrates the advantageof online iterative RLHF. Moreover, our model outperforms the Tulu-2-DPO-70B and GPT-3.5-turbo-1106,which are aligned by DPO or PPO and are much larger than our base model. These results show that the",
  "online iterative RLHF can effectively adjust the style of the model responses, thus improving the conversationquality": "Academic Task. As RLHF can impact a models reasoning and calibration abilities, typically in a negativeway (Bai et al., 2022a; Ouyang et al., 2022; OpenAI, 2023), we compare our models performance on academicbenchmarks () with the SFT checkpoint and other baselines. We dont observe significant performanceregression compared to the SFT baseline. Interestingly, our iteratively DPO-aligned model even outperformsthe SFT model in GSM-8K, MMLU, TruthfulQA, and ARC benchmarks. We believe that these increasedcapacities of the model are injected in the pre-training stage and SFT stage, and iterative DPO helps itleverage them more effectively. This is because the 60K alignment data used in the iterative RLHF areorders of magnitude less than those used in the previous two stages. Remark 1. The RLHF-aligned models based on some initial checkpoints and more epochs can approach oreven outperform the state-of-the-art closed-source models like GPT-4 and Claude on benchmarks. However,we remark that we should be more careful in interpreting these results because the test sets of the benchmarksare finite and may not be representative enough to capture the complicated real-world scenarios. Moreover,the increased possibility of small models overfitting the benchmarks may lead to benchmark hacking, whichmeans that the real capacity of a model with a high score is still limited. In particular, while it is possibleto get even higher results on the benchmark (e.g., 44.84 in LC AlpacaEval-2 and 35.7 in Chat-Arena-hard,but the performance on academic benchmarks drops significantly), we presented our current model by humanevaluation on some randomly chosen test prompts. We also found that GPT-based evaluation highly dependson the configuration. Our model obtains 37.2 LC win-rate (45.4 win-rate) with alpaca eval gpt4 turbo fnconfig, which has better agreement with human evaluation. Ablation study on filtering data with length penalty. We observed that the aligned models responselength was significantly longer than the SFT baseline (potentially due to reward model bias as shown in).To address this, we conducted an ablation study by incorporating a length penalty into thereward function:",
  "r(x, a) = r(x, a) |a|,(9)": "where |a| is the number of characters of the response. We compare the model trained with this penalty tothe vanilla version and report the results in . As expected, the length penalty effectively mitigatedthe length bias, leading to shorter responses. In particular, the model trained with length penalty achievesa superior length-control AlpacaEval-2 win rate, as well as better results on some academic benchmarks.This demonstrates the advantage of mitigating length bias and motivates us to study the verbosity issue inreward modeling further. Finally, we notice that the model trained with length penalty is worse in the Chat-Arena-Hard benchmark. This may suggest that we also need a length-control version for this benchmark toprovide a more reasonable evaluation.",
  "End Note and Future Direction": "In this technical report, we study the workflow of the online iterative RLHF, which leverages on-policysampling and external preference signals from a proxy preference model trained on a diverse set of open-sourcepreference datasets. The resulting model demonstrates impressive performance on standard benchmarks,and the report provides detailed instructions for reproducing the results, including data, code, models, andhyper-parameter choices. There are still many potential directions to explore. First, as we can see in , the iterative RLHFheavily relies on the quality of the preference signal. In this project, we use a proxy scalar reward modeltrained on a diverse set of open-source datasets to approximate human feedback. It would be interestingto see whether we can design a more effective strategy to model different types of preference signals, like amulti-head reward (Wang et al., 2024) and classification-based activation strategy (Touvron et al., 2023).Second, while the rejection sampling seems to be a good heuristic exploration strategy, it is still interestingto see whether we can design more effective ways for exploration. Finally, most of the models after RLHFtend to reply the prompts with much longer responses. Such a length bias is further amplified in the iterativeRLHF framework. We presented a preliminary study on this issue by leveraging an additional length penaltyin reward for data filtering. It would be interesting to see whether we can further mitigate this issue byadditional algorithmic designs or post-training techniques.",
  "Anthropic. Introducing claude. 2023. URL": "Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, EllenJiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXivpreprint arXiv:2108.07732, 2021. Mohammad Gheshlaghi Azar, Mark Rowland, Bilal Piot, Daniel Guo, Daniele Calandriello, Michal Valko,and Remi Munos. A general theoretical paradigm to understand learning from human preferences. arXivpreprint arXiv:2310.12036, 2023. Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain,Stanislav Fort, Deep Ganguli, Tom Henighan, et al.Training a helpful and harmless assistant withreinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022a. Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, AnnaChen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai: Harmlessness fromai feedback. arXiv preprint arXiv:2212.08073, 2022b.",
  "Ralph Allan Bradley and Milton E Terry. Rank analysis of incomplete block designs: I. the method of pairedcomparisons. Biometrika, 39(3/4):324345, 1952": "Collin Burns, Pavel Izmailov, Jan Hendrik Kirchner, Bowen Baker, Leo Gao, Leopold Aschenbrenner, YiningChen, Adrien Ecoffet, Manas Joglekar, Jan Leike, et al. Weak-to-strong generalization: Eliciting strongcapabilities with weak supervision. arXiv preprint arXiv:2312.09390, 2023. Daniele Calandriello, Daniel Guo, Remi Munos, Mark Rowland, Yunhao Tang, Bernardo Avila Pires,Pierre Harvey Richemond, Charline Le Lan, Michal Valko, Tianqi Liu, et al. Human alignment of largelanguage models through online preference optimisation. arXiv preprint arXiv:2403.08635, 2024. Shicong Cen, Jincheng Mei, Katayoon Goshvadi, Hanjun Dai, Tong Yang, Sherry Yang, Dale Schuurmans,Yuejie Chi, and Bo Dai. Value-incentivized preference optimization: A unified approach to online andoffline rlhf. arXiv preprint arXiv:2405.19320, 2024.",
  "Huayu Chen, Guande He, Hang Su, and Jun Zhu. Noise contrastive alignment of language models withexplicit rewards. arXiv preprint arXiv:2402.05369, 2024a": "Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan,Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger,Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder,Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Fe-lipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, SuchirBalaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam,Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, KatieMayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and WojciechZaremba. Evaluating large language models trained on code. 2021.",
  "Luigi Daniele and Suphavadeeprasit. Amplify-instruct: Synthetically generated diverse multi-turn conversa-tions for effecient llm training. arXiv preprint arXiv:(coming soon), 2023. URL": "Shizhe Diao, Rui Pan, Hanze Dong, Ka Shun Shum, Jipeng Zhang, Wei Xiong, and Tong Zhang. Lmflow: Anextensible toolkit for finetuning and inference of large foundation models. arXiv preprint arXiv:2306.12420,2023. Hanze Dong, Wei Xiong, Deepanshu Goyal, Yihan Zhang, Winnie Chow, Rui Pan, Shizhe Diao, JipengZhang, KaShun SHUM, and Tong Zhang. RAFT: Reward ranked finetuning for generative foundationmodel alignment. Transactions on Machine Learning Research, 2023.ISSN 2835-8856.URL Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, PercyLiang, and Tatsunori B Hashimoto. Alpacafarm: A simulation framework for methods that learn fromhuman feedback. arXiv preprint arXiv:2305.14387, 2023. Logan Engstrom, Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Firdaus Janoos, Larry Rudolph, andAleksander Madry. Implementation matters in deep policy gradients: A case study on ppo and trpo. arXivpreprint arXiv:2005.12729, 2020. Kawin Ethayarajh, Yejin Choi, and Swabha Swayamdipta. Understanding dataset difficulty with V-usableinformation. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and SivanSabato (eds.), Proceedings of the 39th International Conference on Machine Learning, volume 162 ofProceedings of Machine Learning Research, pp. 59886008. PMLR, 1723 Jul 2022.",
  "Leo Gao, John Schulman, and Jacob Hilton. Scaling laws for reward model overoptimization. In InternationalConference on Machine Learning, pp. 1083510866. PMLR, 2023": "Caglar Gulcehre, Tom Le Paine, Srivatsan Srinivasan, Ksenia Konyushkova, Lotte Weerts, Abhishek Sharma,Aditya Siddhant, Alex Ahern, Miaosen Wang, Chenjie Gu, et al. Reinforced self-training (rest) for languagemodeling. arXiv preprint arXiv:2308.08998, 2023. Shangmin Guo, Biao Zhang, Tianlin Liu, Tianqi Liu, Misha Khalman, Felipe Llinares, Alexandre Rame,Thomas Mesnard, Yao Zhao, Bilal Piot, et al. Direct language model alignment from online ai feedback.arXiv preprint arXiv:2402.04792, 2024.",
  "Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. Instruction tuning with gpt-4.arXiv preprint arXiv:2304.03277, 2023": "Zhen Qin, Rolf Jagerman, Kai Hui, Honglei Zhuang, Junru Wu, Jiaming Shen, Tianqi Liu, Jialu Liu, DonaldMetzler, Xuanhui Wang, et al. Large language models are effective text rankers with pairwise rankingprompting. arXiv preprint arXiv:2306.17563, 2023. Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D Manning, and ChelseaFinn. Direct preference optimization: Your language model is secretly a reward model. arXiv preprintarXiv:2305.18290, 2023. Corby Rosset, Ching-An Cheng, Arindam Mitra, Michael Santacroce, Ahmed Awadallah, and TengyangXie. Direct nash optimization: Teaching language models to self-improve with general preferences. arXivpreprint arXiv:2404.03715, 2024.",
  "Gokul Swamy, Christoph Dann, Rahul Kidambi, Zhiwei Steven Wu, and Alekh Agarwal. A minimaximalistapproach to reinforcement learning from human feedback. arXiv preprint arXiv:2401.04056, 2024": "Fahim Tajwar, Anikait Singh, Archit Sharma, Rafael Rafailov, Jeff Schneider, Tengyang Xie, Stefano Ermon,Chelsea Finn, and Aviral Kumar. Preference fine-tuning of llms should leverage suboptimal, on-policy data.arXiv preprint arXiv:2404.14367, 2024. Yunhao Tang, Zhaohan Daniel Guo, Zeyu Zheng, Daniele Calandriello, Remi Munos, Mark Rowland,Pierre Harvey Richemond, Michal Valko, Bernardo Avila Pires, and Bilal Piot. Generalized preferenceoptimization: A unified approach to offline alignment. arXiv preprint arXiv:2402.05749, 2024. Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Sori-cut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly capable multimodalmodels. arXiv preprint arXiv:2312.11805, 2023.",
  "Li* Tianle, Chiang Wei-Lin, Dunlap Evan, Frick nad Lisa, Zhu Banghua, Gonzalez Joseph E., and IonStoica. From live data to high-quality benchmarks: The arena-hard pipeline, April 2024. URL": "Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bash-lykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tunedchat models. arXiv preprint arXiv:2307.09288, 2023. Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul, Younes Belkada, ShengyiHuang, Leandro von Werra, Clementine Fourrier, Nathan Habib, et al. Zephyr: Direct distillation of lmalignment. arXiv preprint arXiv:2310.16944, 2023.",
  "Yue Wu, Zhiqing Sun, Huizhuo Yuan, Kaixuan Ji, Yiming Yang, and Quanquan Gu. Self-play preferenceoptimization for language model alignment. arXiv preprint arXiv:2405.00675, 2024": "Tengyang Xie, Nan Jiang, Huan Wang, Caiming Xiong, and Yu Bai. Policy finetuning: Bridging sample-efficient offline and online reinforcement learning. Advances in neural information processing systems, 34:2739527407, 2021. Tengyang Xie, Dylan J Foster, Akshay Krishnamurthy, Corby Rosset, Ahmed Awadallah, and AlexanderRakhlin. Exploratory preference optimization: Harnessing implicit q*-approximation for sample-efficientrlhf. arXiv preprint arXiv:2405.21046, 2024.",
  "Wei Xiong. A sufficient condition of sample-efficient reinforcement learning with general function approxi-mation. The Hong Kong University of Science and Technology, 2023": "Wei Xiong, Hanze Dong, Chenlu Ye, Ziqi Wang, Han Zhong, Heng Ji, Nan Jiang, and Tong Zhang. Iterativepreference learning from human feedback: Bridging theory and practice for rlhf under kl-constraint. 2023. Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and DaxinJiang.Wizardlm: Empowering large language models to follow complex instructions.arXiv preprintarXiv:2304.12244, 2023a.",
  "Jing Xu, Andrew Lee, Sainbayar Sukhbaatar, and Jason Weston. Some things are more cringe than others:Preference optimization with the pairwise cringe loss. arXiv preprint arXiv:2312.16682, 2023b": "Yifan Xu, Xiao Liu, Xinghan Liu, Zhenyu Hou, Yueyan Li, Xiaohan Zhang, Zihan Wang, Aohan Zeng,Zhengxiao Du, Wenyi Zhao, et al. Chatglm-math: Improving math problem-solving in large languagemodels with a self-critique pipeline. arXiv preprint arXiv:2404.02893, 2024. Chenlu Ye, Wei Xiong, Yuheng Zhang, Nan Jiang, and Tong Zhang. A theoretical analysis of nash learningfrom human feedback under general kl-regularized preference. arXiv preprint arXiv:2402.07314, 2024. Lifan Yuan, Ganqu Cui, Hanbin Wang, Ning Ding, Xingyao Wang, Jia Deng, Boji Shan, Huimin Chen, Ruob-ing Xie, Yankai Lin, Zhenghao Liu, Bowen Zhou, Hao Peng, Zhiyuan Liu, and Maosong Sun. Advancingllm reasoning generalists with preference trees, 2024a.",
  "Yao Zhao, Rishabh Joshi, Tianqi Liu, Misha Khalman, Mohammad Saleh, and Peter J Liu. Slic-hf: Sequencelikelihood calibration with human feedback. arXiv preprint arXiv:2305.10425, 2023": "Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,Zhuohan Li, Dacheng Li, Eric Xing, et al.Judging llm-as-a-judge with mt-bench and chatbot arena.arXiv preprint arXiv:2306.05685, 2023. Han Zhong, Wei Xiong, Sirui Zheng, Liwei Wang, Zhaoran Wang, Zhuoran Yang, and Tong Zhang. Gec:A unified framework for interactive decision making in mdp, pomdp, and beyond.arXiv preprintarXiv:2211.01962, 2022.",
  "All authors provided valuable contributions to this project, each bringing unique expertise and insights thatwere crucial for its success": "HD first demonstrated that iterative DPO algorithm can achieve state-of-the-art performance; wrote adevelopment version code for SFT, iterative RLHF; contributed to the training of SFT model and BT-RM; conducted extensive experiments on training and hyper-parameter tuning of iterative RLHF; deliveredthe released BT reward model; provide preference dataset for final BT-RM and some initial versions of theprompt data; conducted the RM evaluation and GPT-based evaluation of the generative models; contributedto paper writing; contributed to the public version of iterative RLHF code. WX wrote the codes for the Bradley Terry reward model and conducted most of the experiments forboth reward and preference model training; delivered the released pairwise preference model; contributedto the preference dataset search and hyper-parameter tuning; initiated and organized the online iterativeRLHF project; wrote the initial code for the online iterative DPO and prepared its public version on GitHub;contributed to the evaluation of the reward and preference models; assisted in the collection and the cleaningof the preference dataset; wrote the paper. BP conducted most of the final SFT and RLHF experiments and delivered the released SFT and RLHFmodel; independently wrote a development version code for SFT and iterative RLHF; developed the SFTrecipes (data, hyper-parameter, model selection); conducted extensive experiments on the training andhyper-parameter tuning of SFT, offline and iterative RLHF; conducted the GPT-based evaluation and allthe academic benchmarks; contributed to paper writing. HW initiated the training code of the pairwise preference model, and conducted experiments in the trainingof the Bradley Terry reward model and pairwise preference model; collected, filtered, and deduplicated theprompt set; contributed to the preference dataset collection and data cleaning; contributed to the evaluationand analysis of the reward and preference models; made substantial writing contributions to the rewardmodeling section and created illustrative figures for the algorithmic frameworks and dataset visualization.",
  "Following the Pair-RM (Jiang et al., 2023) and LLaMA-2 (Touvron et al., 2023), we use a mixture of open-source datasets as the training set. Here is a brief introduction to the datasets:": "HH-RLHF (Bai et al., 2022a) is a pairwise preference dataset where each sample is accompaniedby a conversation history and two alternative responses written by an early Claude model with 52Bparameters. The preferences of the responses are annotated by humans. SHP (Ethayarajh et al., 2022) is sourced from Reddit and includes examples from 18 subreddits,such as askacademia, askbaking, askengineers, and changemyview. Each example is a Reddit postwith a question/instruction and a pair of top-level comments. One comment is preferred by moreReddit users than the other. All preferences and responses are provided by humans. Following Cuiet al. (2023), only samples with a score ratio > 2 are used, and at most 5 pairs are taken for eachprompt. HelpSteer (Wang et al., 2023). This open-source dataset (Wang et al., 2023) contains prompts,responses, and five human-annotated attributes (helpfulness, correctness, coherence, complexity, andverbosity) ranging from 0 to 4. The prompts are generated using a mixture of template-generatedand human-generated methods, while responses are generated by an in-house LLM. The authorsgenerate up to 4 responses per prompt, and we can construct pairwise comparisons based on them.",
  "HH-RLHF115092160.482.273.6LLMHuman115396SHP31003186.2173.688.8HumanHuman93301HelpSteer8592530116.489.3LLMHuman37131PKU-SafeRLHF-30K697521.570.474.6LLMHuman26874": "UltraFeedback63591161.5279.5211.1LLMGPT-4340025UltraInteract76086507.4396.6416.7LLMGPT-4161927CodeUltraFeedback9938172.8427.6400.6LLMGPT-3.550156Argilla-Math235236.5276.5265.3LLMGPT-42418OpenOrca6791153.3165.4260.5LLMGPT-46926Capybara14740634.5348.4401.9LLMGPT-414811 : A summarization of open-source preference datasets. Prompt Len. represents the average promptlength in terms of tokens, and Preferred/Rejected Len.stands for the average length of preferred orrejected responses. All of these lengths are averaged over all pairs and we use the tokenizer of LLaMA-3-8B. Completion marks the data source of the text completions for prompts. We apply pre-processingtechniques to all these datasets and delete the noisy samples from the original dataset. For the datasetwhose prompt is with multiple responses, we include all the possible comparisons except those with the samescore/ranking to compute the total number of comparison pairs. PKU-SafeRLHF (Ji et al., 2024). This dataset (Ji et al., 2024) consists of 30k+ expert comparisondata. Each sample includes two responses to a question and two preference signals for helpfulnessand safety, respectively. The responses are generated by open-source chatbots, and the preferencesignals are merged through the results of 14 harm category multi-class classficiation. UltraFeedback (Cui et al., 2023) consists of 64k prompts from diverse resources (including Ul-traChat, ShareGPT, Evol-Instruct, TruthfulQA, FalseQA, and FLAN) and the authors generate 4responses per prompt using 4 different LLMs sampled from a diverse set of state-of-the-art open-source LLMs. The preference is from GPT-4 based on a fine-grained annotation instruction, whichcontains 4 different aspects, namely instruction-following, truthfulness, honesty and helpfulness. Thedataset collection strategy of UltraFeedback has also influenced many subsequent works.",
  "CodeUltraFeedback is generated similarly with the Ultrafeedback but focuses on the coding task.The annotation is from GPT-3.5": "UltraInteract (Yuan et al., 2024a) is a preference dataset designed for complex reasoning tasks.The authors collect a preference tree for each instruction, with the instruction being the root andeach action a node. A trajectory is a root-to-leaf path consisting of a sequence of actions. Pairedcorrect and incorrect nodes or trajectories are used for preference learning. Distilabel-Capybara5 is a preference dataset of multi-turn dialogues whose prompts are takenfrom Daniele & Suphavadeeprasit (2023), where the responses are generated by open-source LLMsand preferences are generated by GPT-4.",
  "Distilabel-Orca6 is collected similarly with Capybara but with the prompts from Lian et al.(2023a)": "The training of LLMs is highly data-dependent. To ensure high-quality training data, we conduct a filteringprocess on the open-source datasets we use. This process removes low-quality and meaningless samples.Additionally, conversations with empty rounds or incorrect labels (implied by the other features of thedataset) are eliminated. Furthermore, in datasets where absolute scores are available, pairwise comparisonswith small margins are excluded as these preference signals tend to be noisy (Bansal et al., 2023). Thisprocess roughly deletes 10% of the data. We summarize the statistics of the open-source datasets that areused for the training in and prepare them, as well as our data filtering script, on the huggingface.",
  "Mix2: all the datasets in": "The Mix1 dataset is similar to the construction of UltraFeedback (Cui et al., 2023) with an additionalsummarization dataset. In comparison, the Mix2 consists of more reasoning preference pairs (math andcode) and safety data. We also consider three different approaches to model the preference signals, includingprompting in the LLM-as-a-judge manner (Zheng et al., 2023), reward modeling as the MLE of the BTreward model, and the preference model.",
  "B.2Benchmark Details": "AlpacaEval-2 (Dubois et al., 2023): This benchmark focuses on single-turn conversations and consistsof 805 test prompts covering various topics. The models are compared head-to-head with GPT-4-Preview (11/06) to compute the win rate. The same GPT-4 model is used as the judge. To mitigatethe length bias of GPT-4, a length-control variant of the benchmark is also proposed. MT-Bench (Zheng et al., 2023): This benchmark is a multi-turn benchmark and includes 160 testprompts from 8 different areas. The model should first answer an initial question, and then a pre-defined follow-up question. The models responses are then rated by the GPT-4 model with a scalefrom 1-10, and the final score is computed as the average score of two turns. Chat-Arena-Hard (Tianle et al., 2024): This benchmark consists of 500 test prompts from the livedata in Chatbot Arena, a crowd-sourced platform for LLM evaluations. The prompts evaluate themodels ability in specificity, domain knowledge, complexity, problem-solving, creativity, technicalaccuracy, and real-world application. In addition to the agreement to human preference, comparedwith AlpacaEval-2 and MT-Bench, Chat-Arena-Hard further enjoys a clear separability among dif-ferent models.",
  "PromptVisualization.WeprovidethevisualizationgeneratedonNomicAtlas7withthenomic-embed-text-v1.5 text embedding model (Nussbaum et al., 2024)": "SFT Data List.We collect open-sourced instruction-finetuning data for our SFT model training. Thefollowing data is included: ShareGPT (Chiang et al., 2023), Evol-Instruct (Xu et al., 2023a), SlimOrca (Lianet al., 2023b), MathInstruct (Yue et al., 2023), Magicoder-Evol-Instruct (Wei et al., 2023), GPT4-LLM(Peng et al., 2023), OrcaMath (Mitra et al., 2024), GPTeacher (Teknium1, 2023), UltraInteract (Yuan et al.,2024a).",
  "CCase Studies": "To showcase the significant improvements achieved through online RLHF, we conducted a qualitative analysisof the generated responses. We observed that, after applying online RLHF, the responses became moredetailed and better formatted, often utilizing bullet points, highlights, bold text, and enumeration. Thisenhanced clarity and structure were key factors contributing to the improved win-rate observed in ourexperiments. Below, we present several examples that illustrate these improvements.",
  "Offline RLHF Response": "Cactus plants thrive in well-draining soil that is porous and allows excess water to escape quickly. The ideal soil forcactus should have a pH level between 6.0 and 7.5, which is slightly acidic to neutral. The soil should also be low innutrients, as cactus plants generally prefer a nutrient-poor environment.The following soil types are suitable for cactus plants:1. Cactus-specific soil mix: This is a commercial soil mix designed specifically for cactus plants. It typically containsa blend of sand, coconut coir, perlite, and peat moss or sphagnum moss. This mix provides excellent drainage andaeration while retaining moisture.2. Desert soil mix: A desert soil mix can also be suitable for cactus plants. It usually consists of a blend of sand,perlite, andor vermiculite. This mix is well-draining and allows water to escape quickly.3. Sandy soil: A sandy soil with a high percentage of sand is suitable for cactus plants. It provides excellent drainageand allows water to escape quickly.4. Gravel and cactus mix: Layering a cactus mix on top of a layer of small rocks or gravel can also be a good option.This mimics the natural environment of cactus plants in the desert and allows water to drain away quickly.Its essential to avoid using heavy, clay-based soils, as they can retain too much water, leading to root rot and otherissues for cactus plants. Additionally, its essential to ensure that the soil is well-aerated, as cactus plants requiregood air circulation around their roots.",
  "Online RLHF Response": "Cactus plants thrive in well-draining soil that allows excess water to pass through and does not hold moisture forlong periods. The ideal soil for cactus plants is a mix that mimics their natural desert habitat. Here are somecharacteristics of suitable cactus soil:1.**Coarse Texture**: The soil should have a coarse texture, allowing water to drain quickly.This is usuallyachieved by using materials like sand, perlite, or pumice. These materials help prevent waterlogged roots and rootrot, which are common issues for cacti.2.**Loamy Soil**: A loamy soil that is a combination of sand, silt, and clay is also suitable.It retains somemoisture while still allowing for proper drainage. Adding some coarse materials like perlite or pumice to the loamysoil can enhance drainage further.3. **Low Nutrient Content**: Cacti have shallow root systems and do not require a lot of nutrients. Therefore, asoil with low nutrient content, such as a cactus-specific potting mix, is ideal. Dont use regular garden soil, as itmay have too many nutrients and can lead to excessive growth and weak stems.4. **Acidic to Slightly Alkaline**: Cactus plants prefer slightly acidic to slightly alkaline soil, with a pH range of6.0 to 7.5. This range is generally neutral and does not require additional adjustments.5. **Dry and Aerated**: The soil should be dry on the surface to prevent moisture buildup, but still aerated enoughto allow oxygen to reach the roots. This promotes healthy root growth and prevents issues like root rot.To create a suitable cactus soil mix, you can use a combination of the following ingredients:- 2 parts cactus or succulent potting mix (which usually contains peat, perlite, and coconut coir)- 1 part coarse sand (such as sharp sand or builders sand)- 1 part perlite or pumice (to improve drainage)Mix these ingredients thoroughly and use the resulting soil for your cactus plants. You can also purchase commercialcactus and succulent soil mixes from a nursery or garden center, which are usually well-suited for these types ofplants.Remember to always allow the soil to dry out completely between waterings, as cactus plants are adapted to survivein arid conditions. Overwatering is a common cause of cactus issues, so be cautious when watering your cacti."
}