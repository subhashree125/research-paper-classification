{
  "Abstract": "We study best-arm identication (BAI) in the xed-budget setting. Adaptive allocationsbased on upper condence bounds (UCBs), such as UCBE, are known to work well in BAI.However, it is well-known that its optimal regret is theoretically dependent on instances,which we show to be an artifact in many xed-budget BAI problems. In this paper wepropose an UCB exploration algorithm that is both theoretically and empirically ecientfor the xed budget BAI problem under a Bayesian setting. The key idea is to learn priorinformation, which can enhance the performance of UCB-based BAI algorithm as it hasdone in the cumulative regret minimization problem. We establish bounds on the failureprobability and the simple regret for the Bayesian BAI problem, providing upper bounds oforder O( K/n), up to logarithmic factors, where n represents the budget and K denotes thenumber of arms. Furthermore, we demonstrate through empirical results that our approachconsistently outperforms state-of-the-art baselines.",
  "Introduction": "We study best-arm identication (BAI) in stochastic multi-armed bandits (Audibert et al., 2010; Karninet al., 2013; Even-Dar et al., 2006; Bubeck et al., 2009; Jamieson et al., 2014; Kaufmann et al., 2015). In thisproblem, the learning agent sequentially interacts with the environment by pulling arms and receiving theirrewards, which are sampled i.i.d. from their distributions. At the end, the agent must commit to a singlearm. In the standard bandit setting, the agent maximizes its cumulative reward (Lai & Robbins, 1985; Aueret al., 2002; Lattimore & Szepesvri, 2019; Zhu & Rigotti, 2021). In xed-budget BAI (Audibert et al., 2010;Karnin et al., 2013; Jamieson & Talwalkar, 2015; Li et al., 2018), the agent maximizes the probability ofchoosing the best arm within a xed budget. In xed-condence BAI, the agent minimizes the budget toattain a target condence level for identifying the best arm (Even-Dar et al., 2006; Audibert et al., 2010;Karnin et al., 2013). Here we focus on xed-budget BAI. Adaptive allocations based on upper condence bounds (UCBs) are known to work well in xed-budget BAI.For example, UCBE (Audibert et al., 2010) is optimal, with failure probability decreasing exponentially upto logarithmic factors. However, it relies on a plug-in approach of an unknown problem complexity term,learning to the adaptive variant performing signicantly worse (Karnin et al., 2013). As a result, phase-basedalgorithms with uniform exploration in each phase, such as successive rejects (SR) (Audibert et al., 2010) andsequential halving (SH) (Karnin et al., 2013), have been shown to work better in practice. Furthermore, itshould be noted that irrespective of the choice of the algorithms, i.e., UCB-based algorithms or phase-basedalgorithms, the optimal regret that decays exponentially are conditioned on the gaps between the maximalarm and the other arms not being small. If the gaps are small, the regret may decay polynomially instead ofexponentially, as we will demonstrate in the next section.",
  "Published in Transactions on Machine Learning Research (10/2024)": "Recently, Bayesian BAI has received attention. Russo (2020) proposed Bayesian algorithms, top-two variantsof Thompson sampling (TTTS), that are tailored to identifying the best arm. His analysis focused on thefrequentist consistency and rate of convergence of the posterior distribution. Shang et al. (2020) followed hiswork and proposed a variant of TTTS for justifying its use for xed-condence guarantees. Dierent fromtheirs, we propose a variant of UCB exploration by using the prior information and show its eciency byanalyzing the error probability and the simple Bayes regret. Komiyama et al. (2023) and Azizi et al. (2023)derived a lower bound for this setting and a two-phase algorithm that matches it. However, empirically theiralgorithm works badly as shown in Appendix. Recently, Atsidakou et al. (2023) introduced a Bayesian versionof the SH algorithm and provided prior-dependent bound on the probability of error in multi-armed bandits.Furthermore, Nguyen et al. (2024) established upper prior-dependent bounds on the expected probability oferror of prior-informed BAI in Structured Bandits. Our Bayesian BAI formulation is also related to Bayesian optimization (Snoek et al., 2012) which assumes aGaussian process prior and updates the posterior with new observations. Similar to Bayesian optimization,the Bayesian BAI setting uses a Gaussian prior to learn conguration evaluations. However, unlike Bayesianoptimization, the Gaussian prior in the Bayesian BAI setting models reward means of individual arms.",
  "Exponentially Decaying Bounds in Fixed-budget BAI: An Artifact": "Consider a xed-budget BAI problem having K arms with mean k, k [K], and a horizon of n rounds (orbudgets). In round t [n], the agent pulls arm It [K] and observes its reward, drawn independently ofthe past. At the end of round n, the agent selects an arm Jn. The BAI problem concerns whether the nalrecommendation Jn is the optimal one or not. For sake of simplicity, we will assume that there is a uniqueoptimal arm. Let i = arg maxk[K] k be the optimal arm and = i. Some BAI xed-budget algorithms, such as UCBE and SH, are considered (nearly) optimal since they canachieve an exponentially decaying failure probability that depends on the instance. However the propertyof exponentially decaying failure probability is conditioned. To illustrate this point, we will use UCBE as anexample. Audibert et al. (2010) denes the problem complexity of the BAI problem",
  "Kn1/2": "Unfortunately, the small-gap condition min (54n1 log n)1/2 may not be small in practice. For instance,in a BAI problem with n = 10000, the small-gap regime is dened by min 0.223 which is not consideredsmall by any means. Even when considering the lower bound, the small-gap issue remains prevalent. In Audibert et al. (2010), itis demonstrated that for Bernoulli rewards with parameters in [p, 1 p], where p (0, 1/2), the probabilityof error for UCBE satises",
  "n1/2": "However, it is worth noting that the small-gap condition min ((32n)1 log n)1/2 may not be small inpractical scenarios. For instance, in a xed-budget BAI problem with n = 1000, the small-gap regime isdened by min 0.0147 which may not be considered very small in many xed-budget BAI problems either.",
  "Therefore, the exponentially decaying bounds on failure probability can be regarded as an artifact in manyxed-budget BAI problems": "At last, the presence of the small-gap problem also aects the choice of exploration degree, as the upperbound of UCBE necessitates the parameter to be less than 25(n K)/(36H). However, in these scenarios,this bound is on the order log n, which leads to logarithmic exploration instead of linear exploration in thexed-budget BAI problem.",
  "Scenario with Full-Information": "Now, let us consider a two-arm bandit problem and examine a scenario where we have complete information:each arm k = 1, 2 is pulled n times and its outcomes are observed. We assume that k = 1 is the optimalarm. In this instance, we can assume that the mean reward of each arm k follows a normal distribution:k N(k, n12), where 2 represents the variance of reward noise. The probability of error can bebounded as follows:",
  "A Bayesian Formulation for Best-Arm Identication": "In this paper we assume that the reward, denoted as rk, associated with arm k, is generated from an(unknown) distribution with a mean k. We assume that the reward noise, represented as rk k, adheres toa 2-sub-Gaussian for a constant > 0. We introduce the assumption of random arm means on the BAIproblem. Specically, we assume that the mean arm reward k of each arm k [K] follows the followingmodelk = 0 + k ,(3) where k N(0, 20) and N(0, 20) is a Gaussian distribution with zero mean and variance 20. As a result,the mean reward of arm k, k, is a stochastic variable with mean 0 and variance 20. Recently, Komiyamaet al. (2023) considers a Baysian BAI setting where they assume the uniform continuity of the conditionalprobability density functions. Dierent from theirs, we make a parametric perspective on priors k. Ourmodel setting has 20 to represent the variability of the arm means. With a lower variance 20, the dierencesamong the arms are smaller. Therefore, it is harder to learn the optimal arm, as the variability of the armmeans is smaller. The priors k, k [K], are taken into account through (0, 20). Dierent from the algorithms that rely on H, in the Bayesian BAI setting k for k = i, can be arbitrarilysmall. Fortunately we can control the probability that the gap k is less than for any > 0. This isour key in the Bayesian BAI problem. Dene",
  ",": "for any > 0. The probability e represents the likelihood that the optimal arm i is at least better thanthe other arms. In other words, it reects the probability of obtaining a gap between i and the other armsthat is less than based on the prior distribution. In the BAI problem, decreases as the numbers of pullsincreases, allowing for control over the probability. This highlights the inherent diculty of the Bayesian BAIproblem when dealing with the prior distribution of (k)k[K].",
  "For arm k and round t, we denote by Tk,t the number of its pulls by round t, and by rk,1, . . . , rk,Tk,t thesequence of its associated rewards": "We use Gaussian likelihood function to design our algorithm. More precisely, suppose that the likelihoodof reward rk,Tk,t at time t, given k, were given by the pdf of Gaussian distribution N(k, 2), where wetake 2 = 12 for 0 < 1. We emphasize that the Gaussian likelihood model for rewards is only usedabove to design the algorithm. The assumptions on the actual reward distribution are the 2-sub-Gaussianassumption. This setup is analogous to the one described in Agrawal & Goyal (2013).",
  ": end for7: Return estimated best arm Jn = arg maxk[K] k,n": "In RUE, the priors (k)k[K] are taken into account through the variances 20 and 2. Various methods forobtaining consistent estimators of 20 and 2 are available, including the method of moments, maximumlikelihood, and restricted maximum likelihood. See Robinson (1991) for details. One practical implication ofthis is that unlike UCBE, our algorithm focuses on learning the prior to implement the algorithm. This featureof RUE can have surprising practical benets.",
  "of the Gaussian likelihood designed in the algorithm. The condition on implies that a larger 2 than 2": "is required in the Gaussian likelihood. This requirement is analogous to that in Agrawal & Goyal (2013).Moreover, KnmK+1 = O(K/n) when mK 2. Typically when K 20, 2, we have 1, which implliesm 202/25. Clearly, the condition mk 2 permits 20 = O(1/K). Hence, Theorems 5.1 and 5.2 statethat the upper bounds on the failure probability and the simple Bayes regret are O(",
  "Hb/n) respectively": "Here we characterize the hardness of the task using the quantity Hb = (K + 20 2)2, which relies on theparameter set of bandits (K, 20, 2). The quantity Hb increases as the number of arms K increases, thenoise variance 2 increases, or the variability of the arms means 2 decreases. Obviously, Hb = O(K) when20 = O(1) or 20 = O(1/K), given that 2 = O(1). In this case, the upper bounds are O(",
  "K/n), even when 20 are small (i.e., 20 = O(1/K))": "As demonstrated in , the upper bound on the failure probability of UCBE exhibits an exponential decaythat is conditioned on the value of min. When min is small, their bound degenerates to a polynomiallydecay, resulting in a failure probability of O(Kn), where > 0 depends on how small min is. In contrast,the bounds provided in Theorems 5.1 and 5.2 are instance-independent, meaning that they solely depend onthe budget n and the bandit class dened by the number of arms and the variances, denoted as (K, 20, 2),for which RUE is designed. These bounds are not inuenced by the specic instances within the class, ensuringtheir independence from the particular characteristics of each instance. Nevertheless, as shown in ,20 characterizes the variability of the arms means k in relation to 0, thereby permitting very small min.Consequently, the small-gap issue does not pose a signicant challenge in our algorithm.",
  "where the last step is from applying Theorem 3.1. Therefore, the rst term of the decomposition in (7) isbounded": "At last, we investigate the second term of the decomposition in (7). Given our analysis of the algorithm,it is imperative to note that these models may be entirely unrelated to the actual reward distribution.Consequently, we cannot make the assumption that the posterior distribution of kgiven the historicaldatais Gaussian, with the posterior mean k,t. Instead, we opt for a decomposition approach, as detailedbelow. Denote 1 = 1 + K120/(20 + 2). Dene the following event:",
  "Experiments": "We conduct two kinds of experiments. In .1, k are random, where various settings are considered. In.2, k are xed. Note that our modeling assumptions are violated here. We show these experimentsof xed k because they are benchmarks established by Audibert et al. (2010) and Karnin et al. (2013). Our baselines include the state-of-the-art Successive Rejects (SR) (Audibert et al., 2010), Sequential Halving(SH) (Karnin et al., 2013), the UCB-exploration (UCBE) (Audibert et al., 2010), and Top-Two Thompsonsampling (TTTS) (Russo, 2020). As mentioned in .2, in the implementation of RUE, we use theestimates of the variances 20 and 2. Therefore, no hyperparameters are required for RUE. UCBE is implementedwith parameter a = 2n/H, since this parameter works the best overall according to Audibert et al. (2010)and Karnin et al. (2013). We do not report the adaptive variant of UCBE because it performs much worse thanUCBE; even worse than SH (Karnin et al., 2013). Note UCBE is infeasible since it requires the knowledge of aproblem complexity parameter H, which depends on gaps. Additionally, we assess the two-stage algorithmproposed by Komiyama et al. (2023), but it does not perform well in our experiments (see in theAppendix). Consequently, we exclude their method from our comparison.",
  "Random k": "For random k, we have the following three setups:(R1) Gaussian rewards with mean k and variance 2 = 1, where k U(0, 0.5) for k [K].(R2) The same k as R1, but the rewards are Bernoulli rewards with means k.(R3) Bernoulli rewards with k U(0, 0.5) for k [K].These setups allow us to explore how the performance of RUE compare against benchmarks under variousdistributions of noise and reward means. Although we assume a Gaussian distribution in our Bayesian BAIformulation, we also evaluate the performance of RUE for R3 when the assumption does not hold. We report the performance for K = 20. Due to the randomness of k, the gaps and the diculty of BAIvaries. Therefore, we conduct our experiments on 50 sampled 1, . . . , K. For each set, we evaluate theperformance of RUE and state-of-the-art algorithms, and then report the average performance. Like the caseof xed k, we also set a = 2 through three random setups. The maximum budgets are set to N = 5000 for R1 and R2, and to N = 12000 for R3. We choose thesevalues based on the median complexity terms in our experiments, which are H 2000 for R1 and R2, andH 5500 for R3. Then we study various budget settings n {N/4, N/2, N}, so that we can show how theperformance varies when the budget is less than H and double of H. In , we report the average performance over 50 experiments, and also boxplots of the relativeperformance of the baselines SR, SH, UCBE, and TTTS with respect to RUE. We observe that, except forperforming better or worse than TTTS in R3, RUE dominates other methods in all three setups, and evenworks better than UCBE. For example, for the case of n = N/2 in R3, the error probabilities of RUE are smaller23%, 24%, and 31%, respectively, than UCBE, SH, and SR. These results shows the exibility of RUE acrossvarious distributions of reward noise and across various distributions of reward means.",
  "(c) Setup R3": ": Random k. Upper panel: the average performance among 50 experiments. The bars denote thestandard error of the mean among 50 experiments. The lower panel: the error dierence of the performanceof the baselines, SR, SH, and UCBE, respectively, with respect to that of RUE among all 50 experiments. (F2) Two groups of suboptimal arms: k = 0.45 for k = 2, . . . , 8 and k = 0.3 otherwise.(F3) Three groups of suboptimal arms: k = 0.48 for k = 2, . . . , 5, k = 0.4 for k = 6, . . . , 13 and k = 0.3otherwise.(F4) Arithmetic: The suboptimality of the arms form an arithmetic series where 2 = 0.5 1/(5K) andK = 0.25.(F5) Geometic: The suboptimality of the arms form an geometric series where 2 = 0.5 1/(5K) andK = 0.25.(F6) One real competitor: 2 = 0.5 1/(10K) and k = 0.45 for k = 3, . . . , K.In all setups, the reward distributions are Bernoulli and the mean reward of the best arm is 0.5. The numberof arms is K = 20. We also examine K {40, 80} in of Appendix, to show how RUE scales with K. We set 2H as the maximal budget for matching the hardness and for the limit of resources. Then we studyvarious budget settings n {H/2, H, 2H}, so that we can show the performance when the budgetis less or more than H. In RUE, we plug in the estimators of the variances 2 and 20 as in Zhu & Kveton(2022a). shows results for our six problems. We have the following observations. First, RUE consistentlyoutperforms SH and SR in all problems (except for the n = H/2, H of F1, where its a little worse thanSR). Take F2 as an example. For the budget H/2, H, and 2H}, the error probabilities of RUE aresmaller 10%, 20%, and 66%, respectively, than SH. Second, comparing to the infeasible UCBE, RUE outperformsit in F3,F5-F6, performs similarly to it in F4, and performs worse than it in F1 and F2. Third, comparing toTTTS, RUE outperforms it in most cases but performs slightly worse in n = 2H} for F4 and F6. Fourth,comparing with various K {40, 80} in of Appendix, we observe that the outperformance of RUEover others grows as K increases. In summary, the observations suggest that RUE is expected to work well invarious domains.",
  "SRSHUCBETTTSRUE": "0.00.10.20.30.4 K=40 budget error probability 0.00.20.4 K=80 budget error probability 1e+052e+054e+05 : The error probability of the dierent algorithms in Setup F4 with more arms, 40 and 80 arms (leftand right subgures respectively). The results are averaged over 1000 independent executions (all standarderrors are less than 0.016 and not reported). For K = 80, we set N = 400000 as the maximal budget for thelimit of resources when 2H is too big.",
  "Conclusions": "We introduce a formulation of the Bayesian xed-budget BAI problem by modeling the arm means, andpropose RUE, an ecient, instance-independent UCB exploration for xed-budget BAI. We empirically showthat RUE outperforms SH and SR in broad domains, even works better than or similarly to the infeasible UCBEin various domains. We derive O( K/n) bounds on its Bayesian failure probability and simple Bayes regret.Inspired by Li et al. (2018), which demonstrates that BAI can be applied to hyper-parameter optimization,our proposed RUE has potential for use in hyper-parameter optimization. This application warrants seriousinvestigation in the future. Nevertheless, an inherent limitation of this study is the absence of a corresponding lower bound, as obtainingone for xed-budget BAI is a challenging task (Qin, 2022). Another limitation is that the analysis in thispaper only focuses on the Gaussian settings. Nevertheless, RUE does not assume any distributional form,since the setting in (3) does not assume any particular distribution, but only assumes that the rst- andsecond-order moments of k are bounded. Moreover, our empirical results show that RUE works well in broaddomains where the Gaussian assumptions are violated. Therefore, an interesting question is to provide thebound on the failure probability under sub-Gaussian settings. In our analysis, we assume that 2 and 20 are known. However, in practice, we substitute these parameterswith their estimates.Investigating the eect of this substitution in the Bayesian setting is extremelychallenging since we need to integrate the error probability over the posterior of these parameters. We leavethis challenging problem as a future direction for research. Nevertheless, we expect this eect to be smallsince the agents estimates of these parameters should converge to their true values as the agent gathers moredata.",
  "First we provide the following lemma, which provides a basic tool for bounding the gap k. Then weshow that the probability e depends on 20 and ln K by following Lemma A.1": "Lemma A.1. Assume Xk for k = 1, . . . , K, are independent and identically distributed from N(0, 1). DenoteX(1) X(2) X(K) the non-increasing re-ordering of X1, . . . , XK. Then there exists a constant C > 0,such that for all integers K 2 and for > 0,",
  "c1,t + k.(21)": "Proof. (20) is obviously true at time t = 2K + 1. We assume that it holds at time t 2K + 1. If It+1 = k,then Tk,t+1 = Tk,t, thus it still holds. If It+1 = k, it means that k,t + ck,t 1,t + c1,t. Note that on E, wehave that1,t + c1,t 1, and k,t + ck,t k + (1 + )ck,t.",
  "A.7More Results of Experiments": "Performance of the two-stage algorithm We show the empirical studies of the two-stage algorithm. Foroverall checking the performance, we check it under various q = 0.1, 0.2, . . . , 0.9, we report their performanceunder Setup F4 in . The gure shows the Two-Stage algorithm is bad under various q. 0.00.40.8 q error probability 0.10.30.50.70.9 n=7003n=14006n=28012",
  ": The error probability of the Two-Stage algorithm with various q values, averaged over 1000independent executions (results in standard deviations of less than 0.016)": "Impact of arm number K. We ran the experiments with n = 20, 40, 80 arms in order to examine how theperformance of each algorithm scales as the number of arms grow. We report the result on the arithmeticsetting (Setup F4) in , where K = 40 and 80 are shown. Comparing various K, the benet of usingRUE increases as K increases."
}