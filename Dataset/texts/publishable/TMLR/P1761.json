{
  "Abstract": "Reconstructing jets, which provide vital insights into the properties and histories of sub-atomic particles produced in high-energy collisions, is a main problem in data analyses ofcollider physics. This intricate task deals with estimating the latent structure of a jet (bi-nary tree) and involves parameters such as particle energy, momentum, and types. WhileBayesian methods offer a natural approach for handling uncertainty and leveraging priorknowledge, they face significant challenges due to the super-exponential growth of potentialjet topologies as the number of observed particles increases. To address this, we introducea Combinatorial Sequential Monte Carlo approach for inferring jet latent structures. As asecond contribution, we leverage the resulting estimator to develop a variational inferencealgorithm for parameter learning. Building on this, we introduce a variational family using apseudo-marginal framework for a fully Bayesian treatment of all variables, unifying the gen-erative model with the inference process. We illustrate our methods effectiveness throughexperiments using data generated with a collider physics generative model, highlightingsuperior speed and accuracy across a range of tasks.",
  "Introduction": "Reconstructing jets in particle physics deals with estimating a high-quality hierarchical clustering. A com-prehensive approach to this process also involves inference on model parameters, which could provide insightsinto our understanding of quantum chromodynamics (QCD), i.e. the theory of the strong interaction betweenquarks mediated by gluons. Hierarchical clustering forms a natural data representation of data generated bya Markov tree, and has been applied in a wide variety of settings such as entity resolution for knowledge-bases(Green et al., 2012; Vashishth et al., 2018), personalization (Zhang et al., 2014), and jet physics (Cacciariet al., 2008; Catani et al., 1993; Dokshitzer et al., 1997; Ellis & Soper, 1993). Typically, work has focusedon approximate methods for relatively large datasets (Bateni et al., 2017; Monath et al., 2019; Naumovet al., 2020; Dubey et al., 2014; Hu et al., 2015; Monath et al., 2020; Dubey et al., 2020; Monath et al.,2021). However, there are relevant use cases for hierarchical clustering that require exact or high-qualityapproximations on small to medium-sized datasets (Greenberg et al., 2020; 2021). This paper deals withone of these use cases: reconstructing the latent hierarchy of jets in particle physics. Within this context,",
  "Leaves =ObservedParticles": ": Jets as binary trees. Left: Schematic representation of the production of a jet at CERNs LHC.Incoming protons collide, producing two new particles (light blue). Each new particle undergoes a sequenceof binary splittings until stable particles (solid blue) are produced and measured by a detector. Right: In jetreconstruction, only the leaf nodes are observed and the tree topology is inferred. Each latent tree topologyrepresents a different possible splitting history. Bayesian methods provide a natural approach for handling uncertainty, but the super-exponential scalingof the number of hierarchies with the size of the datasets presents significant difficulties, i.e. the number oftopologies grows as (2N 3)!!, with N being the number of leaves. This super-exponential growth in thespace of configurations makes brute force and exact methods intractable.",
  "Jet physics": "During high-energy particle collisions, such as those observed at the Large Hadron Collider (LHC) at CERN,collimated sprays of particles called jets are produced. The jet constituents are the observed final-stateparticles that hit the detector and are originated by a showering process (described by QCD) where aninitial (unstable) particle goes through successive binary splittings. Intermediate (latent) particles can beidentified as internal nodes of a hierarchical clustering and the final-state (observed) particles correspond tothe leaves in a binary tree. provides a schematic representation of this process. This results in severalpossible latent topologies corresponding to a set of leaves. This representation, first suggested in Louppeet al. (2019), connects jets physics with natural language processing (NLP) and biology.",
  "Collider data analysis": "A main problem in data analyses of collider physics deals with estimating the latent showering process(hierarchical clustering) of a jet, which is needed for subsequent tasks that aim to identify the creationof different types of sub-atomic particles. The final goal is to perform precision measurements to test thepredictions of the Standard Model of Particle Physics and explore potential models of new physics particles,thereby advancing our comprehension of the universes fundamental constituents. The improved performanceof deep learning jet classifiers (Butter et al., 2019) (for the initial state particles) over traditional clustering-based physics observables gives evidence of the limitedness of current clustering algorithms since the rightclustering should be optimal for classification tasks. Thus, high-quality approximations in this context wouldbe highly beneficial for data analyses in experimental particle physics.",
  "Jet simulators": "Currently, there are high-fidelity simulations for jets, such as Pythia (Sjostrand et al., 2006), Herwig (Bellmet al., 2016), and Sherpa (Gleisberg et al., 2009). These simulators are grounded in QCD but make severalapproximations that introduce parametric modeling choices that are not predicted from the underlying the-ory, so they are commonly tuned to match the data. Many tasks in jet physics can be framed in probabilisticterms (Cranmer et al., 2021). In particular, we consider the challenges of calculating the maximum likelihoodhierarchy given a set of leaves (jet constituents), the posterior distribution of hierarchies, as well as estimat-ing the marginal likelihood. Also, these quantities are relevant to tune the parameters of the simulators.While these formulations are helpful conceptually, they are not practical in current high-fidelity simulations",
  "Published in Transactions on Machine Learning Research (12/2024)": "In all experiments, tcut is an exogenous variable, and in full physics simulations its value is determined by theenergy scale where there is a qualitative change in the theory that describes the process, i.e. we switch froma showering to a hadronization process, described by different models of physics. The difference between theenergy of the initial state particle (root of the binary tree) and this energy scale (tcut) affects the numberof final state particles in the Monte Carlo sampling. Given that Ginkgo does not include a full quantumchromodynamics (QCD) modeling of the splitting likelihood, we choose tcut to control the distribution onthe number of final state particles and mimic full physics simulation processes. As such, the exact value isnot relevant and the developed algorithms capabilities are independent of it. : Inferred log-normal pseudo-marginal distri-bution for the parameters = (1, 2) of the heavyresonance jet, estimated using Ncsmc. Contours rep-resent the log-conditional likelihood, with stochasticgradient steps on LNCSMC highlighted in red. Simulators rooted in QCD present significant chal-lenges due to their complex likelihoods. Rewritingthese simulators is a substantial endeavor, demand-ing considerable expertise and effort, with special-ists often dedicating entire careers to mastering theintricacies of QCD. Real-world data utilization ne-cessitates strict adherence to the models embeddedwithin these simulators, further complicating thetask. Moreover, the lack of currently available pow-erful quantum computers adds another layer of dif-ficulty, prompting high-energy researchers, such asthose at Iris-Hep, to rely on approximations likeGinkgo (Cranmer et al., 2019a). The Csmc method (Wang et al., 2015) and theVariational Combinatorial Sequential Monte Carlo(Vcsmc) method(Moretti et al., 2021) both usea specific generative model for biology based on acontinuous time Markov chain.Our paper repre-sents the first adaptation of Sequential Monte Carlomethods for jet reconstruction in particle physics.This is achieved by adapting the Ginkgo model andredefining the parent node splitting likelihood re-cursively (Eq. 12) to ensure that, at the final Csmcrank event, the support of the proposal distributionaligns with that of the target distribution.",
  "Jet clustering": "For each jet produced at the LHC, there is an inference task on the latent hierarchy that typically involves10 to 100 particles (leaves).Though this is a relatively small number of elements, exhaustive solutionsare intractable, and current exact methods, e.g., Greenberg et al. (2020; 2021), have limited scalability.The industry standard uses agglomerative clustering techniques, which are greedy and based on heuris-tics (Cacciari et al., 2008; Catani et al., 1993; Dokshitzer et al., 1997; Ellis & Soper, 1993), typically findinglow-quality hierarchical clusterings.Regarding likelihood-based clustering (applied to Ginkgo datasets),previous work Greenberg et al. (2020) introduced a classical data structure and dynamic programming al-gorithm (the cluster trellis) that exactly finds the marginal likelihood over the space of configurations andthe maximum likelihood hierarchy. Also, an A* search algorithm combined with a trellis data structure thatfinds the exact maximum likelihood hierarchy was introduced in Greenberg et al. (2021). Finally, Cranmeret al. (2022) pairs Ginkgo with the cluster trellis (Greenberg et al., 2020), to use the marginal likelihood todirectly characterize the discrimination power of the optimal classifier (J. Stuart & Arnold, 1994; Cranmer& Plehn, 2007) as well as to compute the exact maximum likelihood estimate for the simulators parameters.While these works provide exact algorithms that extend the reach of brute force methods, they have anexponential space and time complexity, becoming intractable for datasets with as few as 15 leaves. For thisreason, Greenberg et al. (2020; 2021) also provide approximate solutions at the cost of finding lower-qualityhierarchies.",
  "Bayesian inference": "A recent body of research has melded variational inference (VI) and sequential search. These connections arerealized through the development of a variational family for hidden Markov models, employing SequentialMonte Carlo (Smc) as the marginal likelihood estimator (Maddison et al., 2017; Naesseth et al., 2018;Le et al., 2018; Moretti et al., 2019; 2020; 2021). Within the field of Bayesian phylogenetics (the studyof evolutionary histories), various methods have been proposed for inference on tree structures. Commonapproaches include local search algorithms like random-walk Mcmc (Ronquist et al., 2012) and sequentialsearch algorithms like Combinatorial Sequential Monte Carlo (Csmc) (Bouchard-Ct et al., 2012; Wanget al., 2015). Mcmc methods also handle model learning. Dinh et al. (2017) proposes ppHmc which extendsHamiltonian Monte Carlo to phylogenies. Evaluating the likelihood term in Mcmc acceptance ratios canbe challenging. As a workaround, particle Mcmc (Pmcmc) algorithms use Smc to estimate the marginallikelihood and define Mcmc proposals for parameter learning (Wang & Wang, 2020). Pseudo-marginal methods are a class of statistical techniques used to approximate difficult-to-compute prob-abilities, typically by introducing auxiliary random variables to form an unbiased estimate of the target prob-ability (Andrieu & Roberts, 2009). Beaumont (2003) introduced a method in genetics to sample genealogiesin a fully Bayesian framework. Tran et al. (2016) utilizes pseudo-marginal methods to perform variationalBayesian inference with an intractable likelihood. Our work is a synthesis of Wang et al. (2015) and Morettiet al. (2021) in that we introduce a variational approximation on topologies using Smc and a VI frameworkto learn parameters.",
  "of our knowledge, this marks the first adaptation of Smc methods to jet reconstruction in particlephysics": "2. We leverage the resulting Smc estimators to develop two approximate posteriors on jet hierarchiesand correspondingly two VI methods for parameter learning.We illustrate the effectiveness ofboth methods through experiments using data generated with Ginkgo (Cranmer, Kyle et al., 2021),highlighting superior speed and accuracy across various tasks. 3. In order to circumvent parametric modeling assumptions, we propose a unification of the generativemodel and the inference process. Building upon the point estimators, we define a distinct variationalfamily over global and local parameters for a fully Bayesian treatment of all variables. 4. We show how partial states and re-sampled indices generated by Smc can be interpreted as auxil-iary random variables within a pseudo-marginal framework, thus establishing connections betweenvariational pseudo-marginal methods and Vsmc (Naesseth et al., 2018; Moretti et al., 2021).",
  "Ginkgo generative model": "In this subsection we provide an overview of the generative process in Ginkgo as well as jet (binary tree)reconstruction during inference. As mentioned in the introduction, Ginkgo is a semi-realistic model designedto simulate a jet. The branching history of a jet is depicted as a binary tree structure = (V, E) where denotes topology, V comprises the set of vertices, and E the set of edges. Each node is characterizedby a 4D (energy-momentum) vector z = (E R+, p R3) where E denotes energy and p = (px, py, pz)denotes momentum in the respective dimensions. The squared mass t = t(z) := E2 |p|2 is calculatedusing the energy-momentum vector z. The terminal nodes (or leaf nodes), represented as X = {x1, , xN},correspond to the observed energy-momentum vectors measured at the detector. The tree topology andthe energy-momentum vectors associated with internal nodes, denoted as Z = {z1, , zN1}, are latentvariables in the model.",
  "Generative process": "In Ginkgo the generative process begins with the splitting of a parent (root) node with invariant mass squaredtP into two children, as shown schematically in (left). The process is characterized by a cutoff masssquared tcut, and a rate parameter for the exponential distribution governing the decay. During generation,as long as the invariant mass squared of a node exceeds the cutoff value (tP > tcut), that node is promotedto be a parent and the algorithm recursively splits it. The squared masses of each new left (L) and right (R)child nodes (tL, tR) are obtained from sampling from the exponential distribution f(t|, tP ) defined in Eq. 1,with parameters specific to each child (L, R). Finally, once tL and tR are sampled, the corresponding energy-momentum vectors (zL, zR) for the (L, R) nodes are derived from tP , tL and tR following energy-momentumconservation rules, i.e. applying a 2-body particle decay (see (Cranmer, Kyle et al., 2021; Cranmer et al.,2019b) for more details). Next, we specify the exponential distribution",
  "tiP ,(1)": "where the first term (1 e)1/tiP is a normalization factor, i {L, R}, tLP = tP , and tRP = (tP tL)2.To satisfy energy-momentum conservation (tL + tR < tP ), tL is sampled before tR. Thus, in Ginkgothe left child mass squared tL is sampled first with tLP = tP and then an auxiliary value tRp = (tP tL)2",
  "(b) Node splitting likelihood reconstruction process": ": Illustration of the Ginkgo generative and reconstruction processes. (a) Ginkgo starts with a parentnode characterized by a 4-vector zp = (E, p) and invariant mass squared tP = E2 |p|2. If tP is greaterthan the cut off value tcut, then the parent node splits (we have a particle decay). The left and right nodesinvariant mass squared (tL, tR) are sampled from a truncated exponential distribution defined in Eq. 1. (b)The splitting likelihood reconstruction process of a node, defined in Eq. 4 begins with two child nodes Land R along with their respective 4-vectors zL and zR. The 4-vector for the parent node P is calculatedas zP = zL + zR and then tP = t(zP ).Next, we obtain tL = t(zL), tR = t(zR) and define tLP = tP ,and tRP = (tP tL)2. Finally, the left splitting term likelihood (tL, , tcut, tLP , tP ) and the right one(tR, , tcut, tRP , tP ) are evaluated.",
  "Jet reconstruction during inference": "In addition to the generative model, we also need to be able to assign a likelihood value to a proposed jetclustering (binary tree) during inference. To do this we use the same general form for the jets likelihoodbased on a product of likelihoods over each splitting. In order to evaluate this we need to first reconstructeach parent from its left and right children. Different tree topologies give rise to different tP values for theinner nodes and thus different likelihoods. Notably, in Ginkgo the likelihood of a tree is expressed in termsof the product (in linear space) of all splitting likelihoods (specified in Eq. 4 and referred to as the partiallikelihood) of a parent into two children. The likelihood of a splitting connects parent with child nodes (i.e.we have a likelihood of sampling a child with squared mass t given a parent with squared mass tP ). Thus,parent and child nodes are not independent. However, splitting likelihoods of different parent nodes areindependent, given a tree. For a set of observed energy-momentum vectors X = {x1, , xN} (leaf nodes),parameters , and a tree topology , the likelihood of a splitting history can be evaluated efficiently.",
  "f(ti|, tiP ),tP > tcutFs(tcut, tP ),tP tcut ,(2)": "where i {L, R} (note that to fix a degree of freedom, tLP = tP , and tRP = (tP tL)2) and tcut is thecutoff mass squared scale for the binary splitting process to stop (if ti tcut, the corresponding node is aleaf of the binary tree). We introduce the cumulative density function Fs(tcut, tP ) for a given generativeprocess to stop, i.e. the probability of having sampled a value of tP < tcut, as",
  "ABCD": ": Illustration of the Ncsmc framework: In Ncsmc, all possible one-step ahead topologies, whichamount toNr2in total, are systematically enumerated. For the state A, B, C, D, the enumerated topologiesinclude (top): A, B, C, D, (center): A, B, C, D, and (bottom): B, A, C, D. Subsequently, M = 1 sub-branchlengths are stochastically sampled for each edge. Following this, the sub-weights or potentials are computed(right), and a single candidate is randomly selected proportional to its sub-weight (or potential) to createthe new partial state. In the above, akr1 denotes the ancestor index of the resampled random variable and the partial state skr = T kris sampled by proposing forest T kr q(|Takr1r1 ) from a Uniform distribution. Similarly, the proposal",
  "Bayesian jet reconstruction": "The goal of jet reconstruction is to infer the splitting history and properties of subatomic particles pro-duced during high-energy collisions. This involves estimating various parameters such as particle momenta,positions, and types. Bayesian methods provide a natural framework for modeling uncertainty and incor-porating prior information into the reconstruction process. Let X = {x1, , xN} denote the matrix ofobserved energy-momentum vectors of the Ginkgo model. The posterior distribution can be expressed asfollows:",
  "(7)": "In the context of Auto-Encoding Variational Bayes (Aevb, both Q(, |X) and P(, , X) are jointlytrained (Kingma & Welling, 2013; Rezende et al., 2014). To approximate the expectation in Eq.7, MonteCarlo samples from Q(, |X) are averaged, and these samples are reparameterized using a deterministicfunction of a random variable that is independent of .",
  "Combinatorial Sequential Monte Carlo": "Csmc, tailored for phylogenetic tree models, approximates a sequence of increasing probability spaces,ultimately aligning with Eq. 5 (Wang et al., 2015). Csmc employs sequential importance resampling across{r}N1r=1 steps to approximate both the unnormalized target distribution and its normalization constant,denoted as , constituting the numerator and denominator in Eq. 5, by K partial states {skr}Kk=1 Sr toform a distribution (see Wang et al. (2015) or Appendix B),",
  "k=1wkrskr (s)s S .(8)": "Csmc, in contrast to standard Smc techniques, manages a combinatorial set representing the realm of treetopologies alongside the continuous branch lengthsboth of which are characteristic features of phyloge-nies (Wang et al., 2015).Partial states (Monte Carlo samples) are resampled at each rank r, ensuringsamples remain in high-probability regions, and importance weights are defined as:",
  "ZCSMC.(11)": "In the same work, Moretti et al. (2021) introduces Nested Combinatorial Sequential Monte Carlo (Ncsmc), anefficient proposal distribution, providing an exact approximation to the intractable locally optimal proposalfor Csmc. We provide a review of Ncsmc in Appendix C. The VI algorithms Vcsmc and Vncsmc thatutilize the estimators ZCSMC and ZNCSMC each introduce a structured approximate posterior that exhibitsfactorization across rank events. Each state, denoted as sr, is uniquely characterized by its topology, acollection of trees forming a forest, and the corresponding branch lengths. To facilitate reparameterization,discrete terms are either removed from gradient estimates or transformed into Gumbel-Softmax randomvariables.",
  "= FA FB FC =FA": ": The likelihood FA for a sub-tree defined onleaf nodes D, E, F and G is defined as the recursiveproduct of splitting likelihoods FB and FC. The in-termediate target (s3) for the partial state s3 alsoincludes the probability of singletons H and I denotedFH and FI. .1 adapts the Csmc approach to performinference on jet tree structures. .2 refor-mulates Vcsmc for inference on global parameters..2.1 utilizes Vcsmc methodology to learnparameters as point estimates. .2.2 definesa prior on the model parameters to construct a vari-ational approximation on both global and local pa-rameters. The resulting approach is interpreted asa variational pseudo-marginal method establishingconnections between pseudo-marginal methods (An-drieu & Roberts, 2009) and Variational Combinato-rial Sequential Monte Carlo (Naesseth et al., 2018;Moretti et al., 2021).",
  "Inference on Tree Structures": "Sequential Monte Carlo (Smc) methods (Naesseth et al., 2019; Chopin & Papaspiliopoulos, 2020) are designedto sample from a sequence of probability spaces, where the final iteration converges to the target distribution.However, adapting Csmc for the Ginkgo model requires a crucial modification: ensuring that the splittinglikelihood at the final coalescent event reflects the dependence on the entire sub-tree splitting history, notjust the most recent split. This dependence is essential for accurately capturing the recursive structure ofthe tree. Recall that the splitting likelihood is defined as the product of all splitting likelihoods (see Eq. 4).",
  "F(tLL, tLR, , tcut ) F(tRL, tRR, , tcut )": "In the above, the pair i, j (L, R) (L, R) defines tij as the mass squared of the j child of the current icoalescent node and F (tL, tR, , tcut ) for leaf nodes simply evaluates to 1. Eq. 12 represents the tree (sub-tree) likelihood (with root node having squared mass tP ) as the recursive product of splitting likelihoods.Each term brings its normalization, and the overall normalization is correctly expressed as the product ofindividual ones. In practice we use dynamic programming to maintain a running sum of cumulative logprobabilities across rank events. The Csmc resampling step illustrated in is now dependent upon thefull sub-tree splitting history, reflecting the recursive nature of the coalescent process. In a slight abuse of notation, the probability (skr) of partial state skr (recall r {1, , N 1} denotes thecoalescent event and k denotes the Monte Carlo sample) is defined as the product of the probabilities of all",
  "Fully Bayesian Inference Using a Variational Pseudo-Marginal Framework": "Simulators rooted in quantum chromodynamics are frequently calibrated to align with the data. However,training a simulator separately from the inference process can lead to inefficiencies. To address this, wepropose a modification of the posterior distribution defined in Eq. 5 to include a prior on along withvariational parameters to learn the proposal distribution in Eq. 13. We define a log-normal distribution over log N(|, ) so that can be marginalized along with . The target distribution can now be specifiedas follows:",
  "P(X).(14)": "The generative model parameters can be defined as := = {, } or as the output of a neural networkand the proposal parameters = {, } used in the variational approximation to Eq. 14 can be shared orseparately trained. The pseudo-marginal framework is designed to sample from a posterior distribution such as the one defined inEq. 5 when the marginal likelihood p(X|) cannot be evaluated directly. We would normally be interested incomputing the posterior distribution over splitting topologies and decay parameters defined in Eq. 5; however,the marginal likelihood p(X|) is intractable. Given access to a function g(u; X, ) accepting random numbersu r(u) that can be evaluated pointwise, assume g(u; X, ) returns a non-negative unbiased estimate ofP(X|):Er(u) [g(u; X, )] =g(u; X, )r(u)du = p(X|) .(15) In our setup, g(u; X, ) is defined in Eq. 13 and the auxiliary random variables u := (s1:K1:R , a1:K1:R1) aregenerated via the Csmc or Ncsmc algorithm. Let p(, u) be a joint target distribution over and u:",
  "Experiments": "The industry standard in particle physics uses agglomerative clustering techniques, which are greedy (Cacciariet al., 2012). Beam Search provides a straightforward and significant improvement. Thus, we consider bothGreedy and Beam Search as standard, relevant and efficient baselines, also applied in cited works Greenberget al. (2020; 2021). We simulated 100 jets using Ginkgo running comparisons with Greedy Search, BeamSearch and Cluster Trellis.",
  "MAP estimate": "(left) provides a scatterplot comparing log-conditional likelihood values. Across 100 simulated jets,Vncsmc with K, M = (256, 1) returns a higher likelihood on all 100 cases against Greedy Search (left) and99 cases against Beam Search (center). Notably, Vncsmc simultaneously conducts inference and learning,a feature lacking in Greedy Search and Beam Search, which rely on the user providing values. Furthermore,Vncsmc yields probability distributions over topologies, while Greedy Search and Beam Search yield singletopologies.",
  "Running Time and Complexity": "We generated jets with N = {4, , 64} leaf nodes and profiled the running time of Vcsmc, Cluster Trellis,Greedy Search and Beam Search averaged across 3 random seeds.All experiments were performed ona Google Cloud Platform n1-standard-4 instance with an Intel Xeon CPU 4 vCPUs and 15 GB RAMwithout leveraging GPU utilization. (left) illustrates convergence of the Csmc and Ncsmc conditional likelihood to the exact cluster trellismarginal likelihood as K increases. plots the inferred Log-Normal Pseudo-Marginal Distributionfor the parameters = (1, 2) of the Heavy Resonance Jet, estimated through Ncsmc. Contours of thelog-conditional likelihood are shown with stochastic gradient steps taken on LNCSMC highlighted in red. Next, we compare in (center) the log conditional likelihood returned by VCSMC, Greedy Search andBeam Search with the exact Maximum a Posteriori (MAP) clustering value calculated with the cluster trellistechnique described in Greenberg et al. (2020), for a dataset of 100 jets. We see that VCSMC returns highquality hierarchies, with log likelihood values close to the exact ones. (right) reports the running times on a log scale in seconds. Vcsmc is an order of magnitude fasterthan Beam Search on N = 20 leaf nodes. Beam search entails managing a list of log-likelihood pairs at eachlevel and for each beam size b. The given list is sorted, iterated over, and selectively only a single topologyis retained at a time. This incurs a complexity of O(b2N log b + bN 3 log N). Typically b > N, in whichcase the complexity can be prohibitively slow, but for b < N 2 log N, the complexity becomes O(bN 3 log N).In contrast, Ncsmc is O(KN 3M) and the Csmc is O(KNM), where K, N and M denote the number ofMonte Carlo samples, the number of leaf nodes (observed particles), and the number of subsamples.",
  "Conclusion": "Variational and pseudo-marginal methods are broadly applicable across a range of combinatorial and continu-ous problems. In Bayesian phylogenetics, these methods facilitate efficient estimation of marginal likelihoodsover an equivalent factorial search space of tree topologies, (2N 3)!!, allowing for uncertainty in both treestructure and branch lengths (Moretti et al., 2021; Zhang & Matsen IV, 2019; Zhang et al., 2021). Addi-tionally, in social and biological network analysis, variational techniques are central to hierarchical clusteringand stochastic block models for complex, structured latent spaces (Zhou, 2015; Zhang & IV, 2018; Greenberget al., 2021). Monte Carlo methods, widely used to model self-avoiding random walks on combinatorial struc-tures, provide insight into diffusion and spatial distribution phenomena that bridge into statistical mechanicsand more complex particle interaction models (Madras & Slade, 1996; Grosberg et al., 2006). These tech-niques also extend to probabilistic graphical models and Bayesian networks, where factorized approximationsreduce computational burden, supporting inference on high-dimensional data (Zhang & IV, 2018). We have introduced the first adaptation of Csmc for unbiased and consistent jet reconstruction, proposingapproximate posteriors and variational inference (VI) techniques for both point and distribution estimators.Our approach significantly improves both speed and accuracy, paving the way for broader adoption ofvariational methods in collider data analyses.This work not only provides a robust framework for jetreconstruction but also sets the stage for future advancements in the application of advanced approximateinference methods to high-energy physics experiments.",
  "Matteo Cacciari, Gavin P. Salam, and Gregory Soyez. The anti-kt jet clustering algorithm. JHEP, 04:063,2008. doi: 10.1088/1126-6708/2008/04/063": "Matteo Cacciari, Gavin P. Salam, and Gregory Soyez. Fastjet user manual: (for version 3.0.2). The EuropeanPhysical Journal C, 72(3), March 2012. ISSN 1434-6052. doi: 10.1140/epjc/s10052-012-1896-2. URL S Catani, Yuri L Dokshitzer, M H Seymour, and B R Webber.Longitudinally invariant kt clusteringalgorithms for hadron hadron collisions. Nucl. Phys. B, 406:187224, 1993. doi: 10.1016/0550-3213(93)90166-M.",
  "Kyle Cranmer, Sebastian Macaluso, and Duccio Pappadopulo.Toy Generative Model for Jets Package. 2019b": "Kyle Cranmer, Matthew Drnevich, Sebastian Macaluso, and Duccio Pappadopulo. Reframing jet physicswith new computational methods.EPJ Web of Conferences, 251:03059, 2021.doi: 10.1051/epjconf/202125103059. URL Kyle Cranmer, Matthew Drnevich, Lauren Greenspan, Sebastian Macaluso, and Duccio Pappadopulo. Com-puting the bayes-optimal classifier and exact maximum likelihood estimator with a semi-realistic generativemodel for jet physics. Machine Learning and the Physical Sciences, NeurIPS, abs/2002.11661, 2022. URL Cranmer, Kyle, Drnevich, Matthew, Macaluso, Sebastian, and Pappadopulo, Duccio. Reframing jet physicswith new computational methods. EPJ Web Conf., 251:03059, 2021. doi: 10.1051/epjconf/202125103059.URL",
  "T. Gleisberg, Stefan. Hoeche, F. Krauss, M. Schonherr, S. Schumann, F. Siegert, and J. Winter. Eventgeneration with SHERPA 1.1. JHEP, 02:007, 2009. doi: 10.1088/1126-6708/2009/02/007": "Spence Green, Nicholas Andrews, Matthew R. Gormley, Mark Dredze, and Christopher D. Manning. En-tity clustering across languages. In Conference of the North American Chapter of the Association forComputational Linguistics: Human Language Technologies (NAACL-HLT), 2012. Craig S. Greenberg, Sebastian Macaluso, Nicholas Monath, Ji-Ah Lee, Patrick Flaherty, Kyle Cranmer,Andrew McGregor, and Andrew McCallum. Data Structures \\& Algorithms for Exact Inference in Hier-archical Clustering. 2 2020. URL Craig S. Greenberg, Sebastian Macaluso, Nicholas Monath, Avinava Dubey, Patrick Flaherty, Manzil Zaheer,Amr Ahmed, Kyle Cranmer, and Andrew McCallum. Exact and approximate hierarchical clustering usinga*, 2021.",
  "Antonio Khalil Moretti, Zizhao Wang, Luhuan Wu, Iddo Drori, and Itsik Peer. Particle smoothing variationalobjectives. CoRR, abs/1909.09734, 2019": "Antonio Khalil Moretti, Zizhao Wang, Luhuan Wu, Iddo Drori, and Itsik Peer. Variational objectives forMarkovian dynamics with backward simulation. European Conference on Artificial Intelligence, 2020. Antonio Khalil Moretti, Liyi Zhang, Christian A. Naesseth, Hadiah Venner, David Blei, and Itsik Peer.variational combinatorial sequential monte carlo methods for bayesian phylogenetic inference. In Cassiode Campos and Marloes H. Maathuis (eds.), Proceedings of the Thirty-Seventh Conference on Uncertaintyin Artificial Intelligence, volume 161 of Proceedings of Machine Learning Research, pp. 971981. PMLR,2730 Jul 2021. URL",
  "Minh-Ngoc Tran, David J. Nott, and Robert Kohn. Variational bayes with intractable likelihood, 2016": "Shikhar Vashishth, Prince Jain, and Partha Talukdar. Cesi: Canonicalizing open knowledge bases usingembeddings and side information. In Proceedings of the 2018 World Wide Web Conference on World WideWeb, pp. 13171327. International World Wide Web Conferences Steering Committee, 2018. Liangliang Wang, Alexandre Bouchard-Ct, and Arnaud Doucet. Bayesian phylogenetic inference using acombinatorial sequential Monte Carlo method. Journal of the American Statistical Association, 01 2015.",
  "Yixin Zhang, Vladimir N Minin, and Frederick A Matsen IV. Variational inference in phylogenetics. InProceedings of the 38th International Conference on Machine Learning, pp. 1231112321. PMLR, 2021": "Yuchen Zhang, Amr Ahmed, Vanja Josifovski, and Alexander Smola. Taxonomy discovery for personalizedrecommendation. In Proceedings of the 7th ACM international conference on Web search and data mining.ACM, 2014. Mi Zhou. Infinite edge partition models for overlapping community detection and link prediction. Proceedingsof the 18th International Conference on Artificial Intelligence and Statistics, 38:11351143, 2015.",
  ". The set of partial states at the final rank R = N 1 corresponds to the target space X": "The likelihood, as represented in Eq. 12, and the probability measure are specifically defined within thescope of the target space, denoted as SR = X. Its important to note that these definitions apply exclusivelyto the target space of trees and not to the broader sample space encompassing partial states, denoted asSr<R, which consists of forests containing disjoint trees. The Sum-Product algorithm is primarily utilizedto derive a maximum likelihood estimate for a tree. However, partial states are explicitly characterized ascollections of these disjoint trees or leaf nodes. To extend the target measure to encompass the samplespace Sr<R, a practical approach is to treat all elements of the jump chain as trees, as elaborated in Wanget al. (2015).",
  "CNested Combinatorial Sequential Monte Carlo": "We provide a review of the Ncsmc algorithm from Moretti et al. (2021). The Ncsmc method performs astandard Resample step (line 4), similar to Csmc methods, iterating over rank events. In each iteration,Ncsmc explores all possible one-step ahead topologies (Nr2) and samples sub-branch lengths for each ofthem (line 5-7). Importance sub-weights or potential functions are evaluated for these sampled look-aheadstates (line 8). The ancestral partial state is then extended to a new partial state by choosing a topologyand branch length based on their respective weights (line 11). The final weight for each sample is calculatedby averaging over the potential functions (line 12). For a visual representation of this procedure, please referto .",
  "ELog Conditional Likelihood P(X|, )": ": Log-conditional likelihood P(X|, ) values for Vcsmc (blue) with K = {8, 16, 32, 64, 128, 256}samples and Vncsmc (red) with K = {8, 16, 32, 64, 128, 256} and M = 1 samples averaged across 5 randomseeds. Greater values of K result in a more constrained ELBO and higher log-likelihood values while reducingstochastic gradient noise.Vncsmc with K 8 explores higher probability spaces than the likelihoodreturned by the simulator, as depicted by the green trace for reference. Vncsmc achieves convergence infewer epochs than Vcsmc and yields higher values, all while maintaining lower stochastic gradient noise.Notably, even Vncsmc with (K, M) = (8, 1) in the top-left plot (red) outperforms Vcsmc with K = 256 inthe bottom-right plot (blue).",
  ". tp > max(tl, tr) for all inner nodes": "Recall that the ELBO is a function of the weight matrix, which is of dimensions (R, K), and contains allweights of the K particles across R iterations. Each entry in the matrix represents the corresponding weightof some partial state. In Vcsmc and Vncsmc, resampling ensures that we not only extend upon partial states of valid non-zeroprobability, but we also arrive at K valid trees at the final rank event. We note that both Greedy Searchand Beam Search often fail to find any valid trees because they reach a set of partial states where no viabletree can be constructed."
}