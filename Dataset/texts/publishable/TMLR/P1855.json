{
  "Abstract": "The remarkable advances in deep learning have led to the emergence of many off-the-shelfclassifiers, e.g., large pre-trained models. However, since they are typically trained on cleandata, they remain vulnerable to adversarial attacks. Despite this vulnerability, their supe-rior performance and transferability make off-the-shelf classifiers still valuable in practice,demanding further work to provide adversarial robustness for them in a post-hoc manner.A recently proposed method, denoised smoothing, leverages a denoiser model in front of theclassifier to obtain provable robustness without additional training. However, the denoiseroften creates hallucination, i.e., images that have lost the semantics of their originally as-signed class, leading to a drop in robustness. Furthermore, its noise-and-denoise procedureintroduces a significant distribution shift from the original distribution, causing the denoisedsmoothing framework to achieve sub-optimal robustness. In this paper, we introduce Fine-Tuning with Confidence-Aware Denoised Image Selection (FT-CADIS), a novel fine-tuningscheme to enhance the certified robustness of off-the-shelf classifiers. FT-CADIS is inspiredby the observation that the confidence of off-the-shelf classifiers can effectively identify hal-lucinated images during denoised smoothing. Based on this, we develop a confidence-awaretraining objective to handle such hallucinated images and improve the stability of fine-tuningfrom denoised images. In this way, the classifier can be fine-tuned using only images thatare beneficial for adversarial robustness. We also find that such a fine-tuning can be doneby merely updating a small fraction (i.e., 1%) of parameters of the classifier. Extensiveexperiments demonstrate that FT-CADIS has established the state-of-the-art certified ro-bustness among denoised smoothing methods across all 2-adversary radius in a variety ofbenchmarks, such as CIFAR-10 and ImageNet.",
  "Introduction": "Despite the recent advancements in modern deep neural networks in various computer vision tasks (Radfordet al., 2021; Rombach et al., 2022; Kirillov et al., 2023), they still suffer from the presence of adversarialexamples (Szegedy et al., 2013) i.e., a non-recognizable perturbation (for humans) of an image often foolsthe image classifiers to flip the output class (Goodfellow et al., 2014).Such adversarial examples can",
  "Published in Transactions on Machine Learning Research (11/2024)": ": CIFAR-10 and ImageNet certified top-1 accuracy. We report the best certified accuracy amongthe models trained with {0.25, 0.50, 1.00}, followed by the clean accuracy of the corresponding model inparentheses. RS denotes methods based on randomized smoothing without a denoising procedure, and DSdenotes methods based on denoised smoothing.indicates training the classifier with Gaussian-augmentedimages,indicates direct use of the off-the-shelf classifier without fine-tuning,indicates fine-tuning of thedenoiser,indicates fine-tuning the off-the-shelf classifier, andindicates parameter-efficient fine-tuningof the off-the-shelf classifier (Hu et al., 2022). The highest certified accuracy in each column is bold-faced. indicates that extra data is used in the pre-training.",
  "Consequently, denoised smoothing with such classifiers leads to a drop of the certified accuracy, especiallyin the large 2-radius regime, i.e., high Gaussian variance (see b)": "Contribution.In this paper, we aim to address the aforementioned issues of denoised smoothing bydesigning a fine-tuning objective for off-the-shelf classifiers that distinguishes between hallucinated images,i.e., images that have lost the original semantics after denoising, and non-hallucinated images, i.e., imagesthat maintain the original semantics after denoising.To this end, we propose to use the likelihood ofdenoised images, i.e., confidence, of the off-the-shelf classifier with respect to the originally assigned classas a proxy for determining whether an image is hallucinated and then fine-tune the classifier with non-hallucinated images only. Consequently, we have developed a confidence-aware training objective based onthe likelihood of denoised images to effectively discriminate hallucinated images (see ). Specifically, we propose a scalable and practical framework for fine-tuning off-the-shelf classifiers, coined Fine-Tuning with Confidence-Aware Denoised Image Selection (FT-CADIS), which improves certified robustnessunder denoised smoothing.In order to achieve this, two new losses are defined: the Confidence-awareselective cross-entropy loss and the Confidence-aware masked adversarial loss. Two losses are selectivelyapplied only to non-hallucinated images, thereby ensuring that the overall training process avoids over-optimizing hallucinated samples, i.e., samples that are harmful for generalization, while maximizing therobustness of smoothed classifiers. Our particular loss design is motivated by Jeong et al. (2023), who werethe first to investigate training objectives for randomized smoothing depending on sample-wise confidenceinformation. We demonstrate that our novel definition of confidence in randomized smoothing, specificallythrough the ratio of non-hallucinated images from a denoiser, can dramatically stabilize the confidence-awaretraining, overcoming its previous limitation of severe accuracy degradation (e.g., see b). In our experiments, we have validated the effectiveness of our proposed method on standard benchmarks forcertified 2-robustness, i.e., CIFAR-10 (Krizhevsky, 2009) and ImageNet (Russakovsky et al., 2015). Ourresults show that the proposed method significantly outperforms existing state-of-the-art denoised smoothingmethods in certified robustness across all 2-norm setups, while updating only 1% of the parameters of off-the-shelf classifiers on ImageNet. In particular, FT-CADIS significantly improves the certified robustness inthe high Gaussian variance regime, i.e., high certified radius. For instance, FT-CADIS outperforms the bestperforming baseline, i.e., diffusion denoised (Carlini et al., 2023), by 29.5% 39.4% at = 2.0 for ImageNetexperiments.",
  "Preliminaries": "Adversarial robustness and randomized smoothing. We assume a labeled dataset D = {(xi, yi)}ni=1sampled from P, where xi X Rd and yi Y := {1, ..., K}, and aim to develop a classifier f : X Ywhich correctly classifies a given input x into the corresponding label among K classes, i.e., f(xi) = yi. Adversarial robustness refers to the worst-case behavior of f; given a sample x X and the correspondinglabel y Y, it requires f to produce a consistent output under any perturbation Rd which preserves theoriginal semantic of x. Here, is commonly assumed to be restricted in some 2-norm in Rd, i.e., 2 for some positive . For example, Moosavi-Dezfooli et al. (2016); Carlini et al. (2019) quantify adversarialrobustness as average minimum distance of the perturbations that cause f to flip the originally assignedlabel y, defined as:",
  ".(1)": "The primary obstacle in achieving adversarial robustness lies in the difficulty of evaluating and optimizing forit, which is typically infeasible because f is usually modeled by a complex, high-dimensional neural network.Randomized smoothing (Cohen et al., 2019; Lecuyer et al., 2019) addresses this challenge by constructing anew robust classifier g from f, instead of directly modeling robustness with f. In particular, Cohen et al.(2019) models g by selecting the most probable output of f under Gaussian noise N(0, 2I), defined as:",
  "R(g; x, y) 1(pg(x, y)) =: R(g, x, y),wherepg(x, y) := P[f(x + ) = y],(3)": "provided that g(x) = y, i.e., y is the most probable output of f under Gaussian noise. Otherwise, we haveR(g; x, y) := 0. Here, is the cumulative distribution function of the standard Gaussian distribution. Weremark that higher pg(x, y), i.e., average accuracy of f(x + ), results in higher robustness. Denoised smoothing. In randomized smoothing, it is crucial that f consistently classifies perturbed imagescorrectly. Salman et al. (2020) have proposed to define f based on concatenating a Gaussian denoiser, denotedas denoise(), with any off-the-shelf classifier fclf, i.e., trained with non-perturbed images, a method referredto as denoised smoothing:",
  "f(x + ) := fclf(denoise(x + )) .(4)": "Denoised smoothing provides a more scalable framework for randomized smoothing. First, we only needoff-the-shelf pre-trained classifiers (rather than noise-specialized classifiers), which is widely investigated anddeveloped (Dosovitskiy et al., 2020; Bao et al., 2022; Radford et al., 2021). Second, recent advancementsin diffusion models (Ho et al., 2020; Nichol & Dhariwal, 2021; Dhariwal & Nichol, 2021) have producedappropriate denoisers for this approach.Previous efforts (Lee, 2021; Carlini et al., 2023) have furtherdemonstrated the potential of denoised smoothing in achieving the state-of-the-art certified robustness whencombined with recently advanced pre-trained classifiers and diffusion models. Parameter-efficient fine-tuning. LoRA (Hu et al., 2022) is a widely-used parameter-efficient fine-tuningmethod that originated from language models. It applies a low-rank constraint to approximate the updatematrix at each layer of the Transformers self-attention layer, significantly reducing the number of trainableparameters for downstream tasks. During fine-tuning, all the parameters of the original model are frozen,and the update of the layer is constrained by representing them with a low-rank decomposition. A forwardpass h = W0x can be modified as follows:",
  "Problem description and Overview": "In this paper, we investigate how to effectively elaborate an off-the-shelf classifier fclf within a denoisedsmoothing scheme. We remark that the robustness of the smoothed classifier g from denoised smoothingof fclf depends directly on the accuracy of the denoised images (see Eq. (3) and (4)). Therefore, one mayexpect that improving fclf for clean images is sufficient to improve the generalizability and robustness ofg (Carlini et al., 2023), assuming that the denoised images follow the pre-training distribution with cleanimages (Salman et al., 2020), i.e., the denoised images preserve the semantics of the original clean images.However, this assumption is not true; the noise-and-denoise procedure of denoised smoothing often suffersfrom distribution shifts and hallucination issues so that the resulting denoised images have completelydifferent semantics from the original labels (see a).",
  "(b) Non-hallucinated images": ": Examples of denoised images for FT-CADIS on ImageNet at = 1.00. We visualize (a) hallucinatedimages and (b) non-hallucinated images after the noise-and-denoise procedure. The red/green box indicatesthe areas where the original semantic of the image is corrupted/preserved, respectively. To alleviate these issues, we aim to develop a fine-tuning scheme for fclf to properly handle denoised samples.One straightforward strategy would be to fine-tune fclf by minimizing the cross-entropy loss with all denoisedimages (Carlini et al., 2023):",
  "i=1CEfclfdenoise(x + i), y, i N(0, 2I),(6)": "where CE denotes the cross-entropy loss, and M denotes the number of noises. Here, we note that thisapproach treats both non-hallucinated and hallucinated samples equally among the denoised samples. How-ever, fine-tuning fclf with hallucinated samples, i.e., denoise(x+i) does not resemble the class y, is harmfulfor the generalizability since Eq. (6) forces the classifier fclf to remember non-y-like hallucinated images asy. Our contribution lies in resolving this issue by introducing (1) a confidence-aware selection strategy todistinguish between hallucinated and non-hallucinated images and (2) a fine-tuning strategy that excludeshallucinated samples from the optimization process.",
  "Confidence-aware denoised image selection": "We propose a confidence-aware selection strategy to identify hallucinated images and non-hallucinated imageswithin a set of denoised images. Consider the denoised images Dx = {denoise(x+1), ..., denoise(x+M)}for a given clean image x and the number of noises M. We aim to find non-hallucinated images within Dxthat an off-the-shelf classifier fclf classifies as the assigned label y, i.e., fclf shows the highest confidence fory among all possible classes. Conversely, if fclf classifies denoised images as a label other than y, we definesuch denoised images as hallucinated images, i.e., samples that no longer preserve the core semantic of y.Accordingly, the set of non-hallucinated images Dx,nh Dx is defined as follows:",
  "Dx,nh = {x|fclf(denoise(x + i)) = y, i [1, ..., M]} .(7)": "We remark that the off-the-shelf classifier fclf is pre-trained with clean images, rather than denoised images.Thus, at the beginning of the fine-tuning, fclf often fails to correctly assign Dx,nh due to the distributionshift from clean images to denoised images. Thus, we update Dx,nh at each training iteration using Eq. (7)for a more accurate assignment of non-hallucinated images.",
  "xDx,nhCEfclfx), y.(8)": "In other words, we optimize our classifier with the non-hallucinated images, while the hallucinated imagesare excluded from our training objective. This prevents the drop in accuracy of fclf caused by being forcedto remember wrong semantics not relevant to the assigned class y. It also allows for fclf to properly learnthe distribution of denoised images, which is largely different from its pre-training distribution with cleanimages. Here, we find that training with the objective in Eq. (8) slows down the overall training procedure sinceDx,nh = sometimes occurs at the start of training. This is mainly due to the distribution shift from thepre-training clean image distribution to the denoised images, i.e., fclf fails to classify denoised images due toinsufficient exposure to denoised images. To resolve this cold-start problem, we add the most-y-like denoisedimage, i.e., a denoised image with the largest logit for y, to Dx,nh when it is empty. Confidence-aware masked adversarial loss. We also propose a simple strategy to further improve therobustness of the smoothed classifier g, i.e., the certified accuracy of g at large 2-norm radius. Specifically, weapply the concept of adversarial training (Madry et al., 2018; Zhang et al., 2019a; Wang et al., 2019; Salmanet al., 2019; Jeong et al., 2023) to our denoised smoothing setup; we carefully create more challenging images,and then additionally learn these images during fine-tuning. Here, the main challenge is to ensure that theadversarial images preserve the core semantic of the original image, thereby maintaining generalizability whileimproving robustness. However, as illustrated in , some clean images are prone to be hallucinatedafter the noise-and-denoise procedure.Therefore, adversarial training in denoised smoothing should becarefully designed to avoid incorporating hallucinated images. To this end, we propose to create adversarial examples based only on images that are unlikely to be hallu-cinated, i.e., clean images x with Dx,nh = Dx. Specifically, we apply our adversarial loss based on a simplecondition of Dx,nh = M:",
  "LMAdv := 1[|Dx,nh| = M] maximaxi i2KL(fclf(x + i ), y),(9)": "where KL(, ) indicates the Kullback-Libler divergence and i := denoise(x+i)x is the difference betweeneach denoised image and the original clean image. To find the adversarial perturbation i , we perform aT-step gradient ascent from each i with a step size of 2 /T, while projecting i to remain within an2-ball of radius : viz., the projected gradient descent (PGD) (Madry et al., 2018). For the adversarialtarget y, we adapt the consistency target from the previous robust training method (Jeong et al., 2023) toour denoised smoothing setup by letting the target be the average likelihood of the denoised images, i.e.,y :=1MMi=1 Softmaxfclfdenoise(x + i). Overall training objective. Building on our proposed training objectives LSCE and LMAdv, we now presentthe complete objective for our Fine-Tuning with Confidence-Aware Denoised Image Selection (FT-CADIS).Based on our confidence-aware denoised image selection scheme, Confidence-aware selective cross-entropyloss and Confidence-aware masked adversarial loss are applied only to non-hallucinated images Dx,nh toimprove both generalizability and robustness of the smoothed classifier.The overall loss function is asfollows:",
  "Experiments": "We verify the effectiveness of our proposed training scheme for off-the-shelf classifiers by conducting compre-hensive experiments. In .1, we explain our experimental setups, such as training configurations andevaluation metrics. In .2, we present the main results on CIFAR-10 and ImageNet. In .3,we conduct an ablation study to analysis the component-wise effect of our training objective.",
  "Experimental setup": "Baselines.We mainly consider the following recently proposed methods based on denoised smoothing(Salman et al., 2020; Lee, 2021; Carlini et al., 2023; Jeong & Shin, 2024) framework. We additionally comparewith other robust training methods for certified robustness based on randomized smoothing (Lecuyer et al.,2019; Cohen et al., 2019; Salman et al., 2019; Jeong & Shin, 2020; Zhai et al., 2020; Horvth et al., 2022a;Yang et al., 2022; Jeong et al., 2021; Horvth et al., 2022b; Jeong et al., 2023). Following the previous works,we consider three different noise levels, {0.25, 0.50, 1.00}, to obtain smoothed classifiers. CIFAR-10 configuration. We follow the same classifier and the same denoiser employed by Carlini et al.(2023). Specifically, we use the 86M-parameter ViT-B/16 classifier (Dosovitskiy et al., 2020) which is pre-trained and fine-tuned on ImageNet-21K (Deng et al., 2009) and CIFAR-10 (Krizhevsky, 2009), respectively.We use the 50M-parameter 3232 diffusion model from Nichol & Dhariwal (2021) as the denoiser.Weprovide more detailed setups in Appendix B.2. ImageNet configuration. We use the 87M-parameter ViT-B/16 classifier which is pre-trained on LAION-2B image-text pairs (Schuhmann et al., 2022) using OpenCLIP (Cherti et al., 2023) and fine-tuned onImageNet-12K and then ImageNet-1K. Compared to the previous state-of-the-art method, diffusion de-noised (Carlini et al., 2023) based on BEiT-large model (Bao et al., 2022) with 305M parameters, we usea much smaller off-the-shelf classifier (30% parameters). We also adopt parameter-efficient fine-tuning withLoRA (Hu et al., 2022), i.e., the number of parameters updated through fine-tuning is only 1% of the totalparameters. We use the same denoiser employed by Carlini et al. (2023), i.e., 552M-parameter 256256unconditional model from Dhariwal & Nichol (2021). We provide more detailed setups in Appendix B.2. Evaluation metrics. We follow the standard metric in the literature for assessing the certified robustnessof smoothed classifiers : the approximate certified test accuracy at r, which is the fraction of the test set thatCertify (Cohen et al., 2019), a practical Monte-Carlo-based certification procedure, classifies correctly witha radius larger than r without abstaining. Throughout our experiments, following Carlini et al. (2023), weuse N = 100, 000 noise samples to certify robustness for entire CIFAR-10 test set and N = 10, 000 samplesfor 1,000 randomly selected images from the ImageNet validation set (note that RS methods in b useN = 100, 000). We use the hyperparameters from Cohen et al. (2019), specifically n0 = 100 and = 0.001.In ablation study, we additionally consider another standard metric, the average cerified radius (ACR) (Zhai",
  "Cross-entropy loss LCE (Carlini et al., 2023)0.63354.445.839.333.228.122.417.3": "method (Carlini et al., 2023) by 35.5% 39.9% at = 1.00. FT-CADIS also outperforms every randomziedsmoothing techinque up to a radius of 1.00. Even though our method slightly underperforms at higherradii in terms of certified accuracy, we note that FT-CADIS is the only denoised smoothing method whichachieves a reasonable robustness at > 1.00. This means that our FT-CADIS effectively alleviates thedistribution shift and hallucination issues observed in previous methods based on denoised smoothing (Carliniet al., 2023). We provide the detailed results in Appendix B.5. Results on ImageNet. In b, we compare the performance of the baselines and FT-CADIS onImageNet, which is a far more complex dataset than CIFAR-10. In summary, FT-CADIS outperforms allexisting state-of-the-art methods in every radii. In particular, our method surpasses the certified accuracyof diffusion denoised (Carlini et al., 2023) by 9.9% at = 2.00. In , we also compare the architectureand trainable parameters of each method. Our method even shows remarkable parameter efficiency, i.e., weonly update 0.9M parameters, which is 3% of Jeong et al. (2023) and 0.2% of Jeong & Shin (2024). Theoverall results highlight the scalability of FT-CADIS, indicating its effectiveness in practical applicationswith only a small parameter updates. We provide the detailed results in Appendix B.5 and further discussthe efficiency of LoRA (Hu et al., 2022) on FT-CADIS in Appendix F.",
  "Ablation study": "In this section, we conduct an ablation study to further analyze the design of our proposed losses, the impactof updating the set of non-hallucinated images, and the component-wise effectiveness of our method. Unlessotherwise stated, we report the test results based on a randomly sampled 1,000 images from the CIFAR-10test set. Effect of overall loss design. presents a comparison of variants of LFT-CADIS, including: (a)removing the non-hallucinated condition of LSCE in Eq. (8), (b) removing the masking condition of LMAdv inEq. (9), and (c) training with cross-entropy loss LCE only. In summary, we observe that (a) using only non-hallucinated images for LSCE achieves better ACR and effectively balances between accuracy and robustness.Additionally, we find that (b) the mask Dx,nh = M in LMAdv is crucial for stable training, as it preventsthe optimization of adversarial images that have lost the semantic of the original image; and (c) FT-CADIS",
  "demonstrates higher robustness and ACR by combining Confidence-aware selective cross-entropy loss andConfidence-aware masked adversarial loss": "Effect of Confidence-aware masked adversarial loss design. We further investigate the components ofConfidence-aware masked adversarial loss. presents three variants of LMAdv in Eq. (9): (a) replacingthe consistent target y with the assigned label y, (b) substituting the outer maximization with an average-case, and (c) combining both (a) and (b). Overall, we find that our proposed LMAdv demonstrates superiorACR compared to the variants, achieving the highest certified robustness while maintaining satisfactoryclean accuracy. It shows that both design choices, i.e., maximizing loss over adversarial images and usingsoft-labeled adversarial targets, are particularly effective. Effect of iterative update of Dx,nh. Our FT-CADIS iteratively updates the set of non-hallucinatedimages, i.e., denoise(x + ) Dx,nh, to deal with the distribution shift from the pre-training distribution(clean images) to fine-tuning distribution (denoised images). shows the effect of Dx,nh on varying {0.25, 0.50, 1.00}. For all noise levels, the iterative update strategy shows higher ACR with higher robustness.We find that the fine-tuning classifier increases the ratio of applying LMAdv (see in Appendix D), i.e.,fclf gradually classifies all the denoised images of x correctly, thereby focusing on maximizing robustnessand achieving a better trade-off between accuracy and robustness (Zhang et al., 2019a).",
  "Effect of . In the fine-tuning objective of FT-CADIS in Eq. (10), determines the ratio between LMAdv": "and LSCE. a illustrates how affects the certified accuracy across different radii, with varying in{0.5, 1.0, 2.0, 4.0, 8.0} and = 0.50. As increases, the robustness at high radii improves although the cleanaccuracy decreases, i.e., the trade-off between clean accuracy and robustness. Effect of M. b shows the impact of M on the model when varying M {1, 2, 4, 8}. The robustnessof the smoothed classifier improves as M increases, while the clean accuracy decreases. With a higher M,the model is exposed to more denoised images included in Dx,nh, reducing the distribution shift from cleanimages to denoised images. This increases the confidence of the smoothed classifier, i.e., the accuracy ondenoised images, resulting in more robust predictions.",
  "Related Work": "Certified adversarial robustness. Recently, various defenses have been proposed to build robust classifiersagainst adversarial attacks. In particular, certified defenses have gained significant attention due to theirguarantee of robustness (Wong & Kolter, 2018; Wang et al., 2018a;b; Wong et al., 2018). Among them,randomized smoothing (Lecuyer et al., 2019; Li et al., 2019; Cohen et al., 2019) shows the state-of-the-artperformance by achieving the tight certified robustness guarantee over 2-adversary (Cohen et al., 2019). Thisapproach converts any base classifier, e.g., a neural network, into a provably robust smoothed classifier bytaking a majority vote over random Gaussian noise. To maximize the robustness of the smoothed classifier,the base classifier should be trained with Gaussian-augmented images (Lecuyer et al., 2019; Cohen et al.,2019; Salman et al., 2019; Zhai et al., 2020; Jeong & Shin, 2020; Jeong et al., 2023). For instance, Salmanet al. (2019) employed adversarial training (Madry et al., 2018) within the randomized smoothing framework,while Jeong & Shin (2020) suggested training a classifier using simple consistency regularization. Moreover,Jeong et al. (2023) introduced sample-wise control of target robustness, motivated by the accuracy-robustnesstrade-off (Tsipras et al., 2019; Zhang et al., 2019a) in smoothed classifiers. However, training base classifiersspecifically for Gaussian-augmented images requires large training costs and thus these methods suffer fromscalability issues in complex datasets, e.g., the accuracy drops severely in the ImageNet dataset. Denoised smoothing. Denoised smoothing alleviates the aforementioned scalability issue of randomizedsmoothing by introducing denoise-and-classify strategy. This approach allows randomized smoothing tobe applied to any off-the-shelf classifier trained on clean images, i.e., not specifically trained on Gaussian-augmented images, by adding a denoising step before feeding Gaussian-augmented images into the classifier.In recent years, diffusion probabilistic models have emerged as an ideal choice for the denoiser in the denoisedsmoothing scheme. In particular, Lee (2021) have initially explored the applicability of diffusion models indenoised smoothing, and Carlini et al. (2023) further observe that combining the latest diffusion modelswith an off-the-shelf classifier provides a state-of-the-art design for certified robustness. Meanwhile, Jeong &Shin (2024) investigate the trade-off between robustness and accuracy in denoised smoothing, and proposeda multi-scale smoothing scheme that incorporates denoiser fine-tuning. Our work aims to improve the certified robustness of smoothed classifiers in denoised smoothing, which isdetermined by the average accuracy of the off-the-shelf classifiers under denoised images. We improve suchrobustness by addressing hallucination and distribution shift issues of denoised images. Specifically, we focuson filtering out hallucinated images based on the confidence of off-the-shelf classifiers, and then fine-tuneoff-the-shelf classifiers with non-hallucinated images.",
  "Conclusion": "We propose FT-CADIS, a scalable fine-tuning strategy of off-the-shelf classifiers for certified robustness.Specifically, we propose to use the confidence of off-the-shelf classifiers to mitigate the intrinsic drawbacksof the denoised smoothing framework, i.e., hallucination and distribution shift. We also demonstrate thatthis can be achieved by updating only 1% of the total parameters. We hope that our method could be ameaningful step for the future research to develop a scalable approach for certified robustness. Limitation and future work.In this work, we apply an efficient training technique for off-the-shelfclassifiers based on LoRA (Hu et al., 2022). Nevertheless, certification remains a bottleneck for throughput,due to its majority voting process involving a large number of forward inferences, i.e., N = 100, 000. Animportant future work would be to accelerate the certification process for a more practical deployment ofour method. In addition, certain public vision APIs do not allow us to access the underlying off-the-shelfclassifiers, i.e., black-box. In such cases, our method is not directly applicable, and further research ontraining methods that are independent of model parameters, such as prompt-tuning (Jia et al., 2022), willbe necessary.",
  "Acknowledgements": "This work was conducted by Center for Applied Research in Artificial Intelligence (CARAI) grant fundedby Defense Acquisition Program Administration (DAPA) and Agency for Defense Development (ADD)(UD230017TD), and supported by Institute for Information & communications Technology Promotion (IITP)grant funded by the Korea government (MSIT) (No.RS-2019-II190075, Artificial Intelligence Graduate SchoolProgram (KAIST); No. RS-2019-II190079, Artificial Intelligence Graduate School Program (Korea Univer-sity)), and by Culture, Sports and Tourism R&D Program through the Korea Creative Content Agency grantfunded by the Ministry of Culture, Sports and Tourism in 2024 (Project Name: International CollaborativeResearch and Global Talent Development for the Development of Copyright Management and ProtectionTechnologies for Generative AI, Project Number: RS-2024-00345025).",
  "Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. BEiT: BERT pre-training of image transformers. InInternational Conference on Learning Representations, 2022": "Nicholas Carlini, Anish Athalye, Nicolas Papernot, Wieland Brendel, Jonas Rauber, Dimitris Tsipras, IanGoodfellow, Aleksander Madry, and Alexey Kurakin. On evaluating adversarial robustness. arXiv preprintarXiv:1902.06705, 2019. Nicholas Carlini, Florian Tramer, Krishnamurthy Dj Dvijotham, Leslie Rice, Mingjie Sun, and J Zico Kolter.(Certified!!)Adversarial robustness for free!In The Eleventh International Conference on LearningRepresentations, 2023. Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell Wortsman, Gabriel Ilharco, Cade Gordon,Christoph Schuhmann, Ludwig Schmidt, and Jenia Jitsev.Reproducible scaling laws for contrastivelanguage-image learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition, pp. 28182829, 2023.",
  "Mikls Z Horvth, Mark Niklas Mller, Marc Fischer, and Martin Vechev.Robust and accuratecompositional architectures for randomized smoothing. arXiv preprint arXiv:2204.00487, 2022b": "Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al.LoRA: Low-rank adaptation of large language models. In International Conference on Learning Repre-sentations, 2022. Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q Weinberger. Deep networks with stochas-tic depth. In Computer VisionECCV 2016: 14th European Conference, Amsterdam, The Netherlands,October 1114, 2016, Proceedings, Part IV 14, pp. 646661. Springer, 2016.",
  "Jongheon Jeong and Jinwoo Shin. Multi-scale diffusion denoised smoothing. Advances in Neural InformationProcessing Systems, 36, 2024": "Jongheon Jeong, Sejun Park, Minkyu Kim, Heung-Chang Lee, Do-Guk Kim, and Jinwoo Shin. SmoothMix:Training confidence-calibrated smoothed classifiers for certified robustness. Advances in Neural Informa-tion Processing Systems, 34:3015330168, 2021. Jongheon Jeong, Seojin Kim, and Jinwoo Shin. Confidence-aware training of smoothed classifiers for certifiedrobustness. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pp. 80058013,2023.",
  "Ilya Loshchilov and Frank Hutter. SGDR: Stochastic gradient descent with warm restarts. In InternationalConference on Learning Representations, 2022": "Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deeplearning models resistant to adversarial attacks. In International Conference on Learning Representations,2018. Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. DeepFool: A simple and accuratemethod to fool deep neural networks. In Proceedings of the IEEE Conference on Computer Vision andPattern Recognition, pp. 25742582, 2016.",
  "Alexander Quinn Nichol and Prafulla Dhariwal.Improved denoising diffusion probabilistic models.InInternational Conference on Machine Learning, pp. 81628171. PMLR, 2021": "Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, GirishSastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models fromnatural language supervision. In International Conference on Machine Learning, pp. 87488763. PMLR,2021. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjrn Ommer. High-resolutionimage synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition, pp. 1068410695, 2022. Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, AndrejKarpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet Large ScaleVisual Recognition Challenge. International Journal of Computer Vision (IJCV), 115(3):211252, 2015.doi: 10.1007/s11263-015-0816-y. Hadi Salman, Jerry Li, Ilya Razenshteyn, Pengchuan Zhang, Huan Zhang, Sebastien Bubeck, and GregYang. Provably robust deep learning via adversarially trained smoothed classifiers. Advances in NeuralInformation Processing Systems, 32, 2019. Hadi Salman, Mingjie Sun, Greg Yang, Ashish Kapoor, and J Zico Kolter. Denoised smoothing: A provabledefense for pretrained classifiers. Advances in Neural Information Processing Systems, 33:2194521957,2020. Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti,Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. LAION-5B: An open large-scale dataset for training next generation image-text models. Advances in Neural Information ProcessingSystems, 35:2527825294, 2022.",
  "Zhuolin Yang, Linyi Li, Xiaojun Xu, Bhavya Kailkhura, Tao Xie, and Bo Li. On the certified robustness forensemble models and beyond. In International Conference on Learning Representations, 2022": "Runtian Zhai, Chen Dan, Di He, Huan Zhang, Boqing Gong, Pradeep Ravikumar, Cho-Jui Hsieh, and LiweiWang. MACER: Attack-free and scalable robust training via maximizing certified radius. In InternationalConference on Learning Representations, 2020. Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric Xing, Laurent El Ghaoui, and Michael Jordan. Theo-retically principled trade-off between robustness and accuracy. In International Conference on MachineLearning, pp. 74727482. PMLR, 2019a. Jingzhao Zhang, Tianxing He, Suvrit Sra, and Ali Jadbabaie. Why gradient clipping accelerates training: Atheoretical justification for adaptivity. In International Conference on Learning Representations, 2019b.",
  "B.1Datasets": "CIFAR-10 (Krizhevsky, 2009) consists of 60,000 RGB images of size 3232, with 50,000 images for trainingand 10,000 for testing. Each image is labeled as one of 10 classes. We apply the standard data augmentation,including random horizontal flip and random translation up to 4 pixels, as used in previous works (Cohenet al., 2019; Salman et al., 2019; Zhai et al., 2020; Jeong & Shin, 2020; Jeong et al., 2021; 2023). No additionalnormalization is applied except for scaling the pixel values from to [0.0, 1.0] when converting imageinto a tensor. The full dataset can be downloaded at kriz/cifar.html. ImageNet (Russakovsky et al., 2015) consists of 1.28 million training images and 50,000 validation images,each labeled into one of 1,000 classes. For the training images, we apply 224224 randomly resized croppingand horizontal flipping.For the test images, we resize them to 256256 resolution, followed by centercropping to 224224. Similar to CIFAR-10, no additional normalization is applied except for scaling thepixel values to [0.0, 1.0]. The full dataset can be downloaded at",
  "B.2Training": "Noise-and-Denoise Procedure. We follow the protocol of Carlini et al. (2023) to obtain the denoisedimages for fine-tuning. Firstly, the given image x is clipped to the range as expected by the off-the-shelf diffusion models. Then, the perturbed image is obtained from a certain diffusion time step accordingto the target noise level. Finally, we adopt a one-shot denoising, i.e., outputting the best estimate for thedenoised image in a single step, resulting in a denoised image within the range of . Since this rangediffers from the typical range of assumed in prior works, we set the target noise level to twice the usuallevel for training and certification. A detailed implementation can be found at and the algorithm is provided in Algorithm 2. CIFAR-10 fine-tuning. We conduct an end-to-end fine-tuning of a pre-trained ViT-B/16 (Dosovitskiyet al., 2020), considering different scenarios of {0.25, 0.50, 1.00} for randomized smoothing. The same isapplied to both the training and certification. As part of the data pre-processing, we interpolate the datasetto 224224. Our fine-tuning follows the common practice of supervised ViT training. The default settingis shown in a. We use the linear lr scaling rule (Goyal et al., 2017): lr = base lr batch size 256.The batch size is calculated as batch per GPU number of GPUs accum iter // number of noises, whereaccum iter denotes the batch accumulation hyperparameter. ImageNet fine-tuning. We adopt LoRA (Hu et al., 2022) to fine-tune a pre-trained ViT-B/16 (Dosovitskiyet al., 2020) in a parameter-efficient manner. We use the same training scenarios as for CIFAR-10. Aspart of the data pre-processing, we interpolate the dataset to 384384. The default setting is shown inb. Compared to end-to-end fine-tuning, we reduce the regularization setup, e.g., weight decay, lrdecay, drop path, and gradient clipping. For LoRA fine-tuning, we freeze the original model except forthe classification layer. Then, LoRA weights are incorporated into each query and value projection matrixof the self-attention layers of ViT. For these low-rank matrices, we use Kaiming-uniform initialization forweight A and zeros for weight B, following the official code. To implement LoRA with ViT, we refer to",
  "B.3Hyperparameters": "In our proposed loss functions (see Eqs.(8), (9), and (10)), there are two main hyperparameters: thecoefficient for the Confidence-aware masked adversarial loss, and the attack radius of Confidence-awaremasked adversarial loss. We have determined the optimal configurations for two hyperparameters througha simple grid search on over and over [0.125, 0.25, 0.5, 1.0]. For CIFAR-10, we use = 1.0, 2.0, 4.0 for = 0.25, 0.50, 1.00, respectively. Assuming that denoise(x+) x with high probability, we adopt a small = 0.25 by default, which is increased to 0.50 after 10 epochsonly for = 1.00. For ImageNet, we use = 2.0, 1.0, 2.0 for = 0.25, 0.50, 1.00 respectively, and is fixed",
  "ConfigurationValue": "OptimizerAdamW (Loshchilov & Hutter, 2019)Optimizer momentum1, 2 = 0.9, 0.999Base learning rate5e-4 ( = 0.25, 0.50), 1e-4 ( = 1.00)Weight decaystart, end = 0.04, 0.4 (cosine schedule)Layer-wise lr decay (Clark et al., 2020; Bao et al., 2022)0.65Batch size128Learning rate schedulecosine decay (Loshchilov & Hutter, 2022)Warmup epochs (Goyal et al., 2017)3Training epochs30 (early stopping at 20)Drop path (Huang et al., 2016)0.2Gradient clipping (Zhang et al., 2019b)0.3",
  "OptimizerAdamW (Loshchilov & Hutter, 2019)Optimizer momentum1, 2 = 0.9, 0.999Base learning rate2e-4 ( = 0.25), 4e-4 ( = 0.50, 1.00)": "Weight decaystart, end = 0.02, 0.2 ( = 0.25)start, end = 0.01, 0.1 ( = 0.50, 1.00)Layer-wise lr decay (Clark et al., 2020; Bao et al., 2022)0.8 ( = 0.25), 0.9 ( = 0.50, 1.00)Batch size128Learning rate schedulecosine decay (Loshchilov & Hutter, 2022)Warmup epochs (Goyal et al., 2017)1Training epochs10 (early stopping at 5)Drop path (Huang et al., 2016)0.0Gradient clipping (Zhang et al., 2019b)1.0",
  "LoRA rank r4LoRA scaler 4": "at 0.25 for all noise levels. Although the number of noises M and the number of attack steps T can also betuned for better performance, we fix M = 4 and T = 4 for CIFAR-10. For ImageNet, we fix M = 2 andT = 1 to reduce the overall training cost. Additional training configurations are provided in . Due tothe extensive training cost of large models, we have adjusted some training configurations for the ablationstudy, e.g., the warmup and training epochs are reduced to 2 and 20, with doubled after 10 epochs.",
  "B.4Computing infrastructure": "In summary, we conduct our experiments using NVIDIA GeForce RTX 2080 Ti GPUs for CIFAR-10, NVIDIAGeForce RTX 3090 and NVIDIA RTX A6000 GPUs for ImageNet. In the CIFAR-10 experiments, we utilize4 NVIDIA GeForce RTX 2080 Ti GPUs for fine-tuning per run, resulting in 8 hours of training cost. Duringthe certification, we use 7 NVIDIA GeForce RTX 2080 Ti GPUs for data splitting, taking 9 minutes perimage (with N = 100, 000 for each inference) to perform a single pass of smoothed inference. In the ImageNetexperiments, we utilize 4 NVIDIA RTX A6000 GPUs for fine-tuning per run, observing 51 hours of trainingcost. During the certification, 8 NVIDIA GeForce RTX 3090 GPUs are used in parallel, taking 4 minutesper image (with N = 10, 000 for each inference) to complete a single pass of smoothed inference.",
  "Ratio of |Dx,nh|=M": ": Change in the ratio of |Dx,nh| = M, i.e., ratio of clean images x satisfying the masking conditionof LMAdv, during fine-tuning on CIFAR-10 with = 1.00, depending on whether Dx,nh is being updated ornot. In the legend, red indicates that Dx,nh is iteratively updated, while orange indicates that Dx,nh is fixed.",
  "EDetails on Certifying Robustness of FT-CADIS": "We simply follow the common evaluation framework of the baselines (Carlini et al., 2023; Jeong & Shin,2024).In denoised smoothing framework (Salman et al., 2020), the robustness of the smoothed classi-fier g is guaranteed based on the accuracy of the off-the-shelf classifier fclf under denoised images, viz.,P[fclf(denoise(x + )) = y] =: pg(x, y). However, since fclf is a high-dimensional neural network, this ac-curacy cannot be computed directly. Instead, we estimate it using the practical Monte-Carlo based algorithmCertify from Cohen et al. (2019).",
  "ViT-B/16(Dosovitskiy et al., 2020)0.9M8.4": "In , we compare the time complexity of different methods. Firstly, our FT-CADIS (without LoRA(Hu et al., 2022)) largely outperforms Multi-scale Denoised (Jeong & Shin, 2024), i.e., 0.743 1.013 inACR, with significantly smaller training costs, i.e., 32 11.2 in GPU days. Furthermore, LoRA reducesthe training time of FT-CADIS by 25% and the trainable parameters by 99% compared to full parameterfine-tuning, while maintaining ACR on par with full parameter fine-tuning. We note that the efficiency ofLoRA can be further improved through advancements of GPU infrastructure or low-level code optimization.Since LoRA is generally applicable to other robust training techniques, we hope that our work initiates theresearch direction on alleviating the large cost of robust training. Meanwhile, Diffusion Denoised (Carlini et al., 2023) proposes to obtain a robust classifier without fine-tuning.While they achieve reasonable robustness in an extremely efficient manner, i.e., no fine-tuning costs, theysuffer from the fundamental limitation associated with hallucination effect of the denoiser (see a).Due to this bottleneck, we find that the robustness of Diffusion Denoised degrades particularly at largeradii (see b). One of our main contributions is identifying and addressing such hallucination issue,achieving improved robustness, i.e., 0.896 1.013 in ACR.",
  "FT-CADIS (Ours)ResNet-110 (1.7M)93.7%0.75468.860.953.945.238.329.423.4ViT-B/16 (85.8M)97.9%0.80672.264.157.248.140.334.125.9": "In , we investigate the relationship between our proposed method and pre-trained classifiers. Theresults show that our proposed method still outperforms Carlini et al. (2023) on ResNet-110 (He et al.,2016), i.e., a much smaller architecture than ViT-B/16 (Dosovitskiy et al., 2020). Also, we observe that thecertified robustness of our method improves as we use more advanced pre-trained classifiers, e.g., FT-CADISbased on ViT-B/16 largely improves the results based on ResNet-110. These findings demonstrate that theeffectiveness of our method is not restricted to specific classifiers and can be further enhanced with continuousadvancements in this field.",
  "JAnalysis on Training Stability of FT-CADIS": ": Plots of (1) Confidence-aware selective cross-entropy loss LSCE, (2) Confidence-aware masked adver-sarial loss LMAdv, (3) Overall training objective LFT-CADIS, and (4) Top-1 accuracy from our main experimentson CIFAR-10 with = 0.25. In this section, we demonstrate that our training objective LFT-CADIS remains stable throughout the fine-tuning process. As mentioned in .3, our overall objective is composed of Confidence-aware selectivecross-entropy loss LSCE and Confidence-aware masked adversarial loss LMAdv. In , we show that (1)the training loss, including the adversarial loss, converges smoothly without oscillation, and (2) the trainingaccuracy also converges well."
}