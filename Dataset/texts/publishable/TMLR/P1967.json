{
  "Abstract": "A common assumption in meta-learning is that meta-training and meta-test tasks are drawnfrom the same distribution. However, this assumption is often not fullled. Under such taskshift, standard meta-learning algorithms do not work as desired since their unbiasednessis no longer maintained.In this paper, we propose a new meta-learning method calledImportance Weighted Meta-Learning (IWML), which preserves unbiasedness even undertask shift. Our approach uses both labeled meta-training datasets and unlabeled datasetsin tasks obtained from the meta-test task distribution to assign weights to each meta-training task. These weights are determined by the ratio of meta-test and meta-trainingtask densities. Our method enables the model to focus more on the meta-training tasks thatclosely align with meta-test tasks during the meta-training process. We meta-learn neuralnetwork-based models by minimizing the expected weighted meta-training error, which isan unbiased estimator of the expected error over meta-test tasks. The task density ratio isestimated using kernel density estimation, where the distance between tasks is measured bythe maximum mean discrepancy. Our empirical evaluation of few-shot classication datasetsdemonstrates a signicant improvement of IWML over existing approaches.",
  "Introduction": "The ability to learn quickly from a limited number of examples is a major distinction between humans andarticial intelligence (AI). Humans can acquire new tasks rapidly, leveraging their past learning experiences.On the other hand, most modern AI methodologies are trained specically for a single task, which restrictstheir utility to that particular task alone. Therefore, when faced with new tasks, it is required to newlygather a huge amount of data for training. However, collecting sucient training data is often expensive andtime-consuming, presenting a considerable challenge in real-world applications. Meta-learning, a widely usedapproach adopted to address this challenge, employs the transferable knowledge gained from meta-trainingtasks to boost the learning ecacy for meta-test tasks (Vilalta & Drissi, 2002; Nichol et al., 2018; Vanschoren,2018; Santoro et al., 2016; Khodak et al., 2019). When formulating a meta-learning approach, it is generallyassumed that the meta-training and meta-test tasks are drawn from the same distribution (Yoon et al., 2018;Li et al., 2017; Rajeswaran et al., 2019; Yin et al., 2019; Zintgraf et al., 2019). This assumption, however,is frequently unfullled. This mismatch hampers our ability to learn transferable knowledge that could bebenecial for meta-test tasks, thereby leading to erroneous predictions. We refer to this phenomenon, wheremeta-training and meta-test tasks follow dierent distributions, as task shift. Standard meta-learning algorithms (Finn et al., 2017; Snell et al., 2017; Garnelo et al., 2018) optimize modelparameters by minimizing the expected meta-training error. However, these methods fall short under taskshifts, as the expected meta-training error does not provide an unbiased estimate of the expected meta-test",
  "Published in Transactions on Machine Learning Research (09/2024)": "Masashi Sugiyama, Taiji Suzuki, Shinichi Nakajima, Hisashi Kashima, Paul Von Bnau, and Motoaki Kawan-abe. Direct importance estimation for covariate shift adaptation. Annals of the Institute of StatisticalMathematics, 60:699746, 2008. Baochen Sun and Kate Saenko. Deep coral: Correlation alignment for deep domain adaptation. In ComputerVisionECCV 2016 Workshops: Amsterdam, The Netherlands, October 8-10 and 15-16, 2016, Proceedings,Part III 14, pp. 443450. Springer, 2016.",
  "We propose a task density estimation method by leveraging KDE in conjunction with the MMDdistance": "To the best of our knowledge, our work represents the rst eort to tackle the issue of task shift inmeta-learning by leveraging unlabeled datasets from tasks derived from the meta-test task distribu-tion. Furthermore, our experimental results demonstrate that our method can accurately estimatereasonable weights, even when the tasks utilized for weight computation are dierent from those inthe meta-test phase. We empirically demonstrate that our proposed method outperforms existing meta-learning algo-rithms, the weighted meta-learning method, and state-of-the-art task augmentation strategies undera task shift.",
  "Covariate shift": "The covariate shift problem is a common issue in conventional supervised learning scenarios (Shimodaira,2000). This problem arises when supervised learning algorithms fail to provide an unbiased estimator be-cause the feature vectors used for training and test come from dierent distributions, while the conditionaldistributions of the output given the feature vectors remain unchanged. The covariate shift problem is related to the task shift problem in that both involve dierences in distributionsbetween the training and test phases. However, the covariate shift problem occurs within a single task, where",
  "Unsupervised domain adaptation": "Although the methods developed to address Unsupervised Domain Adaptation (UDA) and the proposedmethod both make use of unlabeled test datasets to improve performance, these UDA techniques are typicallydesigned for scenarios involving only two tasks: training and test. As such, they are not adaptable to meta-learning frameworks, which involve multiple meta-training and meta-test tasks (Finn et al., 2017; Yao et al.,2019; Snell et al., 2017). UDA aims to transfer knowledge from a labeled source domain to an unlabeled target domain. The dominantapproaches for UDA focus on acquiring domain-invariant features and can be categorized into two principalstrategies. The rst strategy aims to explicitly minimize domain discrepancies by utilizing measures such asMaximum Mean Discrepancy (MMD) (Long et al., 2015; 2017; Borgwardt et al., 2006) to evaluate domainsimilarities. Meanwhile, other studies introduce metrics based on second-order or higher-order statisticaldierences (Sun et al., 2016; Sun & Saenko, 2016). An alternative approach employs adversarial training todevelop domain-invariant features (Saito et al., 2018; Sankaranarayanan et al., 2018), a method pioneered bythe work on Domain-Adversarial Neural Networks (DANN) (Ganin & Lempitsky, 2015; Ganin et al., 2016).This technique involves training a domain discriminator to distinguish between source and target domainswhile concurrently training a feature extractor to deceive the discriminator, leading to the alignment offeatures.",
  "Meta-learning": "A number of frameworks have been proposed for meta-learning (Hospedales et al., 2021), including gradient-based (Finn et al., 2017; Li et al., 2017; Yoon et al., 2018; Finn et al., 2018), metric-based (Snell et al.,2017; Koch et al., 2015; Vinyals et al., 2016), and black box-based (Garnelo et al., 2018; Mishra et al.,2017; Hochreiter et al., 2001) methods. Among them, Model-agnostic Meta-Learning (Finn et al., 2017),Prototypical Networks Snell et al. (2017), and Conditional Neural Processes (Garnelo et al., 2018) serve asrepresentative methods for each respective framework. These existing meta-learning methods assume thatmeta-training and meta-test tasks adhere to the same distribution, and they optimize model parametersby minimizing the expected meta-training error. In .2, we explain that in cases of task shift, dueto inconsistencies between expected meta-training and meta-test errors, optimizing model parameters byminimizing the expected meta-training error does not guarantee a reduction in meta-test error. Yao et al. (2021b) have conducted a study on meta-learning and proposed a method called Adaptive TaskScheduler (ATS). Although both ATS and our method assign weights to each meta-training task, there areseveral dierences between the two methods. The main dierence between our proposed method and ATSlies in the problems they aim to solve. ATS introduces weights to optimize the generalization capacity of themodel and to avoid meta-overtting, whereas our proposed method addresses meta-learning under task shift.These are two distinct problems, and because of the dierences in the problems they solve, the methods forcalculating weights are also dierent. ATS calculates weights based on the loss on the query set and thegradient similarity between the support set and the query set. In contrast, our method calculates weightsbased on the ratio of meta-test and meta-training task densities. While ATS is eective in optimizing thegeneralization capacity of the model and avoiding overtting, it is not designed to address task shift issues.",
  "Weighted meta-learning": "Cai et al. (2020) have conducted a study on meta-learning under task shift and proposed a method calledWeighted Meta-Learning (WML). In WML, it is assumed that there is only one meta-test task, and weightsare assigned to each meta-training task to minimize the distance between the single meta-test task andmultiple meta-training tasks. While we also introduce a method that assigns weights to meta-training tasks,our approach diers from WML in several aspects. (1) WML assumes there is only one meta-test task,whereas our proposed method can consider multiple meta-test tasks by using the meta-test distribution. (2)WML requires recalculating the weights when applied to a new meta-test task, whereas our proposed methodcan be applied to unseen meta-test tasks that follow the meta-test task distribution without the need forweight recalculation. (3) WML necessitates labeled datasets from tasks involved in the meta-test phase forweight computation. In contrast, our proposed method allows for the use of unlabeled datasets from tasksthat are drawn from the meta-test distribution but do not appear in the meta-test phase to compute weights.(4) WML is restricted to regression scenarios and constrained to the use of square and hinge loss as its lossfunctions. Conversely, our proposed method is not only applicable in regression scenarios but also extendsto multi-classication scenarios, oering exibility with no limitations on the choice of the loss function,parameter learning methods, and models.",
  "Task augmentation strategies": "The objective of task augmentation strategies in meta-learning is to generate tasks that are not providedduring the meta-training phase, thereby enhancing the performance of unknown tasks that are presentedduring the meta-test phase. To accomplish this objective, several existing methods have been developed.For instance, Meta-Aug (Rajendran et al., 2020) enhances a task by adding random noise to the labels ofboth support and query sets, while Meta-Mix (Yao et al., 2021a) blends support and query examples withina task. Furthermore, Adversarial Task Up-sampling (ATU) (Wu et al., 2022) aims to train a network thatup-samples tasks adversarially, generating tasks that align with the true task distribution. These task augmentation methods can improve performance for unseen tasks by generating additional tasksin scenarios where meta-training and meta-test tasks share the same task distribution. However, when themeta-training and meta-test tasks follow dierent task distributions, the newly generated tasks, derivedfrom the meta-training task distribution, fail to improve the performance on tasks from the meta-test taskdistribution.",
  "Problem setting": "In this paper, we consider a scenario where there is a task shift, meaning that the meta-training and meta-test tasks follow dierent probability distributions, but the supports of meta-training and meta-test taskdistributions are overlapped. During the meta-training phase, consider we have a set of meta-training tasks {T TRt}T TRt=1, each drawn frompTR(T ). Additionally, we also have T TE meta-test tasks obtained from pTE(T ), where each task is associated with unlabeled dataset UTEt= {(xTEtn )}N TEtn=1. Here, xTEtn is the D-dimensional feature vector of the n-th instancein the t-th meta-test task, and N TEtis the number of instances. It is important to note that these meta-testtasks, associated with unlabeled data, may dier from those in the meta-test phase. The settings for themeta-test phase are consistent with standard meta-learning.",
  "x,yQ(y, f(x, S; )),(2)": "is the loss on the query set given the support set, NQ is the number of paired data in the query set, isparameters of model f, and f(x, S; ) is the output of the model for input x, adapted using support setS. The loss function represented by (y, f(x, S; )) measures the discrepancy between true label y at inputfeature vector x and its estimate f(x, S; ). Similarly, the expected error over target tasks is calculated fromassociated support set S and query set Q:",
  "RTE = ET TEtpTE(T )[E(S,Q)DTEt [L(Q|S; )]].(3)": "In the situation where pTR(T ) = pTE(T ), the expected meta-training error RTR matches the expected er-ror over meta-test tasks RTE. Therefore, optimizing the model parameters by minimizing RTR ensures acorresponding reduction in RTE. However, when pTR(T ) = pTE(T ), referred to as task shift, expected meta-training error RTR and expected error over meta-test tasks RTE do not match: RTR = RTE. Consequently,general meta-learning methods are designed to minimize RTR by optimizing model parameters . However,in cases of task shift, this approach does not ensure that RTE will be minimized as well.",
  "pTR(qt(x)),(7)": "where qt(x) represents the feature vector distribution of task Tt. In many meta-learning studies (Yao et al.,2019; Wu et al., 2022), task Tt is represented by its corresponding labeled dataset Dt = {(xtn, ytn)}Ntn=1. Inour setting, we only have unlabeled datasets in meta-test tasks obtained from the meta-test task distributionin the meta-training phase, which does not allow us to represent the task by its labeled dataset. Therefore,",
  ",(9)": "where qTRt (x) represents the feature vector distribution of t-th meta-training task, qTEt (x) represents thefeature vector distribution of t-th meta-test task, h2 is a smoothing scalar parameter, commonly referredto as the bandwidth, and d(qt(x), qTEt (x)) represents the distance between qt(x) and qTEt (x). Normalizationfactors denoted as ZTE and ZTR, are employed to ensure that the sum of the estimated densities equals one.They are calculated by:",
  "(13)": "where k is a kernel. Considering that Ut and UTEtare sets of high-dimensional feature vectors, such as acollection of images, we can use the deep kernel (Wilson et al., 2016) for the calculating MMD. Utilizingdeep kernels allows for the acquisition of representations particularly tailored to the given data, which can",
  "where UTRt= {xTRtn }NTRtn=1 drawn from qTRt (x)": "Through the importance estimation process, the weight wt of meta-training task T TRtis calculated by Eq. 5.The meta-test and meta-training densities of meta-training task in Eq. 5 are estimated using Eq. 8 and Eq. 9,based on feature vectors in {DTRt }T TRt=1 and {UTEt}T TEt=1. The importance estimation procedures of IWML areshown in Algorithm 1.",
  "is calculated by Algorithm 1 using t-th meta-training dataset and unlabeled meta-test datasets{UTEt}T TEt=1": "In Step 2, prototypical networks (Snell et al., 2017) and model-agnostic meta-learning (Finn et al., 2017) areused to train parameters in our experiments. The parameters can be trained with various meta-learningmethods, such as Meta-SGD (Li et al., 2017), and neural processes (Garnelo et al., 2018). Similarly to ,parameters of encoder g also can be trained with various methods. The meta-training procedures of IWMLare shown in Algorithm 2.",
  "Datasets": "We evaluated our proposed method using three datasets: miniImageNet, Omniglot, and tieredImageNet. TheminiImageNet dataset, rst introduced by Vinyals (Vinyals et al., 2016), is a subset of the larger ILSVRC-12dataset (Russakovsky et al., 2015). It consists of 60,000 color images, each 8484 pixels in resolution, dividedinto 100 classes. Each class contains 600 examples. Omniglot dataset (Lake et al., 2011) includes 1,623 unique, hand-drawn characters from 50 dierent al-phabets. Each character is associated with 20 images, each 2828 pixels in resolution, all of which wereproduced by unique human subjects. The tieredImageNet dataset, initially proposed by (Ren et al., 2018). It comprises 779,165 color images in608 classes, grouped into 34 higher-level nodes within the ImageNet human-curated hierarchy. This set ofnodes is further divided into 20, 6, and 8 disjoint sets for training, validation, and test, respectively. Ren et",
  "al. (2018) contend that splitting closer to the root of the ImageNet hierarchy leads to a more challengingbut more realistic scenario, where test classes are less similar to training classes": "For both miniImageNet and Omniglot, we simulated a task shift scenario by splitting all classes withineach dataset into two clusters based on MMD distance. Specically, rst, we randomly selected two classes.Next, for each class, we calculated its MMD distance to these two selected classes using Eq. 13, clusteringeach class to the one it was closer to. To ensure an adequate number of classes in each cluster, we re-splitthe miniImageNet datasets if a cluster contained fewer than 15 classes. For Omniglot, we mandated that acluster must have no fewer than 50 classes. Let denote the two clusters A and B. The meta-training-validationdatasets consisted of all classes within cluster A and randomly selected 10% of the classes within cluster B.The meta-training datasets comprised 70% of the classes from the meta-training-validation datasets, alsoselected without replacement. The meta-validation datasets consisted of the remaining 30% of the classesfrom the meta-training-validation datasets that were not selected for meta-training. The meta-test datasetswas composed of 45% of the classes randomly selected from cluster B not used for meta-training-validation.The unlabeled datasets for importance estimation was made up of the remaining 45% of the classes fromcluster B not used for meta-training-validation or meta-test. To ensure overlap between the meta-trainingand meta-test task distributions, the meta-training-validation datasets included all classes from cluster A,as well as 10% of the classes from cluster B. For tieredImageNet, because the test classes are less similarto the training classes, we followed the original papers (Ren et al., 2018) method to split the training,validation, and test datasets. The unlabeled datasets used for importance estimation consisted of 40 classes",
  "Comparative Methods": "We compared the proposed method with the following methods: (1) Model-agnostic Meta-Learning (MAML)(Finn et al., 2017), which is a gradient-based meta-learning algorithm. (2) Prototypical Networks (PN)(Snell et al., 2017), which is a metric-based algorithm. (3) Meta-Mix (Yao et al., 2021a) and AdversarialTask Up-sampling (ATU) (Wu et al., 2022), which are state-of-the-art task augmentation strategies for meta-learning, as we introduced in .5. and (4) Weighted Meta-Learning (WML) (Cai et al., 2020), whichis a method aimed at addressing task shift, as we introduced in .4. While MAML, PN, Meta-Mix,and ATU leverage labeled meta-training datasets during their meta-training phase, WML and our methodutilize both labeled meta-training datasets and unlabeled datasets in tasks obtained from the meta-test taskdistribution. To mimic real-world situations more closely, We use unlabeled datasets from tasks that dierfrom the meta-test task.",
  "Experimental Settings": "Our network architecture, closely mirroring the one used in the referenced study (Vinyals et al., 2016),comprises four convolutional blocks. Each block consists of a 33 convolution with 128 lters, a subsequentbatch normalization layer (Ioe & Szegedy, 2015), a ReLU nonlinearity, and nally a 22 max-pooling layer.When applied to 2828 Omniglot images and 8484 miniImageNet images, this architecture produces a 64-dimensional and 1600-dimensional output space, respectively. MAML, Meta-Mix, and ATU all necessitatea classier at the nal layer. In alignment with the baseline classier utilized in the study (Vinyals et al.,2016), we feed the output of the last layer into the soft-max function. The loss function is dened as thecross-entropy error between the predicted and the actual label. For PN, we employ the Euclidean distanceto compute the distance metric. We carry out a single gradient update with a xed step size = 0.05 in theinner loop, while Adam (Kingma & Ba, 2014) serves as the outer loop optimizer with step size = 104.Our model and WML were trained using PN, Meta-Mix, and ATU were trained using MAML. Due to theoriginal WML being limited to regression scenarios and constrained to the use of squared and hinge loss asits loss functions, in our experiments, to adapt WML for classication scenarios, we use Eq. 12 to calculatethe distance between tasks. During the meta-training phase, we trained the proposed IWML and the comparative methods using 250,000meta-training tasks with a meta-batch size of 50. Since both WML and the proposed IWML require unla-beled data to compute weights, we used 30 importance estimation tasks, which consist only of unlabeled data,for weight estimation. However, the other comparative methods did not use any additional unlabeled dataduring the meta-training phase. We employed 30 validation tasks for early stopping. In the meta-test phase,we computed the classication accuracy based on an average of over 30 meta-test tasks. Meta-training, vali-dation, meta-test, and importance estimation tasks were randomly generated from their respective datasets,each task was constructed using a 5-way setting with 1-shot, 2-shot, or 3-shot congurations for miniIma-geNet and tieredImageNet, and 20-way with the same shot variations for the Omniglot datasets. We appliedthe same settings as used in training the PN on each dataset for training encoder g. For smoothing scalarparameters and h used in Eq. 14 and Eq. 9, we designate both as task-specic smoothing scalar parame-ters. For , its value for each task is determined by calculating the median of {g(xn) g(xn)2}Nn=1.Similarly, for h, it is determined using the median of {d(q(x), qTRt (x))}T TRt=1. Additionally, h in Eq. 8, Eq. 10,and Eq. 11 was determined in the similar calculation with Eq. 9.",
  "Results": "We show the performance of the three datasets in Tables 1, 2, and 3. On all three datasets, the proposedmethod consistently outperforms the baseline methods. WML outperforms MAML and PN but is inferior tothe proposed method. One reason for this is that WML faces diculty in computing more accurate weightsfor each meta-training task, especially when using tasks that align with the meta-test task distribution butwill not appear in the meta-test phase. In contrast, our method calculates weights by estimating the densityof each meta-training task within the distributions of both meta-test and meta-training tasks. This enablesour method to compute accurate weights, even when utilizing tasks that conform to the meta-test taskdistribution but are absent from the meta-test phase. The performance of Meta-Mix and ATU does notimprove over MAML. One reason is that, in the case of task shift, due to the dierent distributions betweenmeta-training and meta-test tasks, they fail to generate tasks similar to the meta-test tasks using the givenmeta-training tasks. However, the proposed method enables the model to leverage additional unlabeled datafrom the meta-test task distribution, allowing it to focus more on meta-training tasks that closely align withthe meta-test tasks during the meta-training process, resulting in better performance. shows thetraining time for PN, WML, and the proposed method. Although our method takes longer than PN, it ismuch shorter than that of WML. This is because WML assumes that there is only one meta-test task duringthe meta-test phase. When there are multiple meta-test tasks during the meta-test phase, WML needs totrain specic model parameters for each meta-test task.",
  ").(17)": "The results in indicate a performance decline for the proposed method when MMD is not used. Thisis because MMD can more accurately reect the distribution dierences between two sets of data comparedto the mean of point-to-point distances. Similarly, performance also decreases when the deep kernel is notused. This is because deep kernels can obtain feature representations specically suited to the given datathrough an encoder, potentially enhancing the performance of KDE compared to general kernel functions.",
  "OursWithout MMDWithout deep kernel34.4 3.132.52.832.92.7": ": Ablation study: The averaged accuracies (%) and standard errors on the meta-test tasks under the1-shot 5-way setting on the miniImageNet dataset were obtained using the proposed method with dierentMMD and deep kernel choices of Eq. 8 and Eq. 9. For the encoder g, we evaluated the performance when trained using only labeled meta-training data andwhen trained using joint labeled meta-training data and unlabeled data obtained from the meta-test taskdistribution. The results are presented in , where \"ours\" indicates the performance using only labeleddata, and \"ours with autoencoder\" indicates the performance using both labeled and unlabeled data. In theexperiment using both labeled and unlabeled data, we considered g as part of an autoencoder (Rumelhartet al., 1986).The training process was divided into two stages: (1) When training with labeled meta-training data, a PN was added and trained together with the autoencoder. The autoencoder was trained byminimizing both the error of the PN and the reconstruction error. (2) When training with unlabeled data,the autoencoder was trained by minimizing the reconstruction error alone. The results in indicate that training the encoder g using joint labeled meta-training data andunlabeled data obtained from the meta-test task distribution achieves better performance compared totraining with only labeled meta-training data.This is because when the encoder is trained solely withlabeled meta-training data, it struggles to derive suitable representations for data from the meta-test taskdistribution due to task shift, leading to decreased performance. In contrast, training g with both labeledmeta-training data and unlabeled data from the meta-test task distribution allows the encoder to moreeectively obtain suitable representations for data from both task distributions, thereby enhancing overallperformance.",
  "Visualization of the task shift": "We visualize the task shift in the miniImageNet dataset under a 5-way 1-shot setup. Specically, we randomlyselected 50 meta-training tasks from the meta-training datasets and 50 meta-test tasks from the meta-testdatasets. We used t-SNE (Van der Maaten & Hinton, 2008) with the MMD distance for visualization. Theresults shown in reveal that although there is a signicant dierence between the distribution ofmeta-training tasks and meta-test tasks, a portion of the meta-training tasks are closer to the meta-test",
  "tasks. The proposed method can assign higher weights to these meta-training tasks that are closer to themeta-test tasks": "shows examples of images in meta-training tasks with weights 1.324/0.816 and those in a meta-test task. As depicted in (c), the data in meta-test tasks consists of images related to animalsand insects. The proposed method assigned a high weight to a task depicted in (a) that containsmore animal and insect images. In contrast, tasks with fewer animal images are assigned lower weights asshown in (b). This indicates that our method enables us to nd meta-training tasks that closelyresemble the meta-test tasks. By assigning high weights to these tasks, we can ensure that the distributionof meta-training tasks aligns as closely as possible with that of the meta-test tasks.",
  "Conclusion": "We proposed a meta-learning method that improves performance under task shift, where the task distribu-tions dier between the meta-training and meta-test phases. In the proposed method, weights are assignedto tasks using the density ratio between these distributions. Experiments demonstrate that our methodoutperforms existing approaches. However, the proposed method has its limitations. Specically, (1) Ourevaluation focuses on the miniImageNet, Omniglot, and tieredImageNet datasets, which, although widelyused, do not capture the full range of challenges posed by more complex benchmarks such as Meta-Dataset(Triantallou et al., 2019), VTAB (Zhai et al., 1910), and WILDS (Sagawa et al., 2021).Future workwill aim to address this by applying the method to these benchmarks and assessing its robustness in morechallenging scenarios. (2) When the accessible unlabeled dataset and the meta-test dataset follow dierentdistributions, the performance of our proposed method will decrease. (3) when the density of meta-trainingtasks used as the denominator in the density ratio nears zero, there is a risk that the calculation of weightsbecomes unfeasible. Therefore, it is preferable to estimate weights directly without estimating densities. Forexample, methods proposed to directly estimate weights to address covariate shift (Sugiyama et al., 2008;Bickel et al., 2009). In future research, we plan to extend the proposed method to estimate weights directlywithout needing to estimate the density of tasks.",
  "Broader Impact Statement": "The proposed method uses unlabeled datasets derived from tasks that adhere to the meta-test task distribu-tion to compute weights for each meta-training task. Consequently, there is a potential risk that users mightuse biased datasets to calculate these weights without careful consideration, potentially leading to biasedpredictions. We encourage research into developing methods for automatically detecting biased datasets.",
  "Yaroslav Ganin and Victor Lempitsky. Unsupervised domain adaptation by backpropagation. In Interna-tional Conference on Machine Learning, pp. 11801189. PMLR, 2015": "Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Franois Laviolette,Mario March, and Victor Lempitsky. Domain-adversarial training of neural networks. Journal of MachineLearning Research, 17(59):135, 2016. Marta Garnelo, Dan Rosenbaum, Christopher Maddison, Tiago Ramalho, David Saxton, Murray Shanahan,Yee Whye Teh, Danilo Rezende, and SM Ali Eslami.Conditional neural processes.In InternationalConference on Machine Learning, pp. 17041713. PMLR, 2018.",
  "Aravind Rajeswaran, Chelsea Finn, Sham M Kakade, and Sergey Levine.Meta-learning with implicitgradients. Advances in Neural Information Processing Systems, 32, 2019": "Mengye Ren, Eleni Triantallou, Sachin Ravi, Jake Snell, Kevin Swersky, Joshua B Tenenbaum, HugoLarochelle, and Richard S Zemel. Meta-learning for semi-supervised few-shot classication. arXiv preprintarXiv:1803.00676, 2018. David E Rumelhart, Georey E Hinton, and Ronald J Williams.Learning internal representations byerror propagation, parallel distributed processing, explorations in the microstructure of cognition, ed. derumelhart and j. mcclelland. vol. 1. 1986. Biometrika, 71(599-607):6, 1986. Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, AndrejKarpathy, Aditya Khosla, Michael Bernstein, et al.Imagenet large scale visual recognition challenge.International Journal of Computer Vision, 115:211252, 2015. Shiori Sagawa, Pang Wei Koh, Tony Lee, Irena Gao, Sang Michael Xie, Kendrick Shen, Ananya Kumar,Weihua Hu, Michihiro Yasunaga, Henrik Marklund, et al. Extending the wilds benchmark for unsupervisedadaptation. arXiv preprint arXiv:2112.05090, 2021. Kuniaki Saito, Kohei Watanabe, Yoshitaka Ushiku, and Tatsuya Harada. Maximum classier discrepancyfor unsupervised domain adaptation. In Proceedings of the IEEE Conference on Computer Vision andPattern Recognition, pp. 37233732, 2018. Swami Sankaranarayanan, Yogesh Balaji, Carlos D Castillo, and Rama Chellappa. Generate to adapt: Align-ing domains using generative adversarial networks. In Proceedings of the IEEE Conference on ComputerVision and Pattern Recognition, pp. 85038512, 2018. Adam Santoro, Sergey Bartunov, Matthew Botvinick, Daan Wierstra, and Timothy Lillicrap. Meta-learningwith memory-augmented neural networks. In International Conference on Machine Learning, pp. 18421850. PMLR, 2016.",
  "Huaxiu Yao, Ying Wei, Junzhou Huang, and Zhenhui Li.Hierarchically structured meta-learning.InInternational Conference on Machine Learning, pp. 70457054. PMLR, 2019": "Huaxiu Yao, Long-Kai Huang, Linjun Zhang, Ying Wei, Li Tian, James Zou, Junzhou Huang, et al. Im-proving generalization in meta-learning via task augmentation. In International Conference on MachineLearning, pp. 1188711897. PMLR, 2021a. Huaxiu Yao, Yu Wang, Ying Wei, Peilin Zhao, Mehrdad Mahdavi, Defu Lian, and Chelsea Finn. Meta-learning with an adaptive task scheduler. Advances in Neural Information Processing Systems, 34:74977509, 2021b.",
  "Jaesik Yoon, Taesup Kim, Ousmane Dia, Sungwoong Kim, Yoshua Bengio, and Sungjin Ahn. Bayesianmodel-agnostic meta-learning. Advances in Neural Information Processing Systems, 31, 2018": "Xiaohua Zhai, Joan Puigcerver, Alexander Kolesnikov, Pierre Ruyssen, Carlos Riquelme, Mario Lucic, JosipDjolonga, Andre Susano Pinto, Maxim Neumann, Alexey Dosovitskiy, et al. A large-scale study of represen-tation learning with the visual task adaptation benchmark. arxiv 2019. arXiv preprint arXiv:1910.04867,1910. Luisa Zintgraf, Kyriacos Shiarli, Vitaly Kurin, Katja Hofmann, and Shimon Whiteson. Fast context adap-tation via meta-learning. In International Conference on Machine Learning, pp. 76937702. PMLR, 2019."
}