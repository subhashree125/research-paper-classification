{
  "Abstract": "Optimal transport (OT) is a powerful framework to compare probability measures, a funda-mental task in many statistical and machine learning problems. Substantial advances havebeen made in designing OT variants which are either computationally and statistically moreefficient or robust. Among them, sliced OT distances have been extensively used to mitigateoptimal transports cubic algorithmic complexity and curse of dimensionality. In parallel,unbalanced OT was designed to allow comparisons of more general positive measures, whilebeing more robust to outliers. In this paper, we bridge the gap between those two conceptsand develop a general framework for efficiently comparing positive measures. We notablyformulate two different versions of sliced unbalanced OT, and study the associated topol-ogy and statistical properties. We then develop a GPU-friendly Frank-Wolfe like algorithmto compute the corresponding loss functions, and show that the resulting methodology ismodular as it encompasses and extends prior related work. We finally conduct an empiri-cal analysis of our loss functions and methodology on both synthetic and real datasets, toillustrate their computational efficiency, relevance and applicability to real-world scenariosincluding geophysical data.",
  "Introduction": "Many machine learning tasks involve aligning objects such as images, graphs, datasets or their representationsafter transformations. This is particularly relevant in transfer learning tasks like domain adaptation (Fatraset al., 2021) or multimodal machine learning (Baltruaitis et al., 2018). These objects can be convenientlyrepresented as positive measures, i.e., a set of samples associated with non-negative weights.Aligningthen consists in minimizing a distance or discrepancy between two measures.It is crucial to choose ameaningful discrepancy that has desirable statistical, robustness and computational properties. In particular,some settings require comparing arbitrary positive measures, i.e., measures whose total mass can have anarbitrary value, as opposed to probability distributions whose total mass is equal to 1. In cell biology, forinstance, measures are used to represent and compare gene expressions of cell populations, and the totalmass corresponds to the population size (Schiebinger et al., 2019).",
  "Published in Transactions on Machine Learning Research (12/2024)": "shows that all curves share the same slope w.r.t n, for any d and for both SUOT and USOT. Thisexperiment is consistent with the dimension-free rate we established in Theorem 3.4 for SUOT. Interestingly,it also reveals that the dimension-free rate holds for USOT as well in that specific setting. More experimentsand/or theoretical justification are needed to verify if this holds for more general distributions.",
  "In what follows, M+(Rd) denotes the set of all positive Radon measures of finite mass on Rd. For any M+(Rd), supp() is the support of , and m() =": "Rd d(x) < + is the mass of . For M+(Rd)and a map T : Rd Rp, T# is the pushforward measure of by T, defined for all A Rd as T#(A) =T 1(A). Let z be the Dirac measure at z and for n 1, define the empirical measure n = ni=1 wiZi,where (Zi)ni=1 are n independent and identically distributed (i.i.d.) samples from M+(Rd), and wi > 0.For any convex function : R R {+}, we denote by its Legendre transform, i.e., for x R,(x) = supy0 xy (y). We will also use the notation (x) = (x). For , M+(Rd), is the product measure, and for f, g : Rd R, denote by f g : Rd Rd R the mapping defined as(f g)(x, y) = f(x)+g(y) for all x, y Rd. Sd1 { Rd : = 1} is the unit sphere, and for Sd1, : Rd R is the mapping defined as (x) = , x for all x Rd.",
  "where 1 and 2 denote the marginal distributions of with respect to (w.r.t.) the first and second variablerespectively": "When 1 = 2 and 1(x) = 0 for x = 1, 1(x) = + otherwise, (2) boils down to the Kantorovichformulation of OT (or balanced OT), denoted by OT(, ). Indeed, in that case, D1(1|) = D2(2|) = 0if 1 = and 2 = , D1(1|) = D2(2|) = + otherwise. Under other suitable choices of entropy functions 1 and 2, UOT(, ) is more robust than OT(, ), sinceit can discard outliers and compare and with different masses. We refer to (Sjourn et al., 2022a,.2) for a detailed discussion on the choice of entropies and its consequences on the transport plancomputed by UOT. Two common choices are i(x) = |x 1| and i(x) = (x log(x) x + 1), where > 0is a characteristic radius w.r.t. Cd. They respectively correspond to Di = TV (total variation distance(Chizat et al., 2018a)) and Di = KL (Kullback-Leibler divergence), and operate differently: KL smoothsout geometric outliers, while TV either keeps or removes samples (Sjourn et al., 2022a). The GHK distancecorresponds to (2) with Cd(x, y) = ||x y||2 and Di = iKL (Liero et al., 2018).",
  "where for i {1, 2}, i (x) i (x) with i (x) supy0 xy i(y) the Legendre transform of i, andf g Cd means that for (x, y) , f(x) + g(y) Cd(x, y)": "When clear from the context, we will omit the dependence on (, ) and write D(f, g) instead of D(f, g; , ).The Legendre transform of i is well known for typical choices of i-divergences. For example, if Di = iKL,then i (x) = i(ex/i 1). Based on Proposition 2.3, one can compute UOT(, ) by optimizing a pair of continuous functions (f, g).However, UOT(, ) is known to be computationally intensive (Pham et al., 2020), which motivates the de-velopment of methods able to scale to the large dimensions and sample sizes encountered in ML applications.",
  "Sliced Optimal Transport": "Among the many workarounds that have been proposed to overcome the OT computational bottleneck(Peyr et al., 2019), Sliced OT (Rabin et al., 2012) has attracted a lot of attention due to its computationalbenefits and theoretical guarantees. Definition 2.4 (Sliced OT). Let Sd1 { Rd: = 1} be the unit sphere in Rd. For Sd1,denote by : Rd R the linear map such that for x Rd, (x) , x. Let be the uniform probabilityover Sd1. Consider (, ) M+(Rd) M+(Rd). The Sliced OT problem is defined as",
  "where for any measurable function f and M+(Rd), f is the push-forward measure of by f, i.e., forany measurable set A R, f(A) (f 1A), f 1(A) {x Rd : f(x) A}": "Since ( , ) are two measures supported on R, OT( , ) is defined in terms of a cost functionC1 : RR R, and can be efficiently computed. Therefore, SOT(, ) can provide significant computationaladvantages over OT(, ) in large-scale settings. In practice, if = ni=1 ixi and = ni=1 iyi arediscrete measures, the standard procedure for approximating SOT(, ) consists in sampling m i.i.d. samples{j}mj=1 from , then computing OT(j ), (j )for j = 1, . . . , m. This second step involves sorting then support points of and (Peyr et al., 2019, .6), thus involves O(n log n) operations per j. SOT(, ) relies on the Kantorovich formulation of OT, thus SOT(, ) < + only when m() = m(),and may not provide meaningful comparisons in presence of outliers. To overcome such limitations, priorworks have proposed slicing a particular instance of UOT that is partial OT (Bonneel & Coeurjolly, 2019;Bai et al., 2023), for which D is the total variation distance. More precisely, noting POT the UOT problemwith D = TV, they consider for , M+(Rd) the problem",
  "Sd1 POT( , )d().(5)": "For the 1D partial OT problem, Bonneel & Coeurjolly (2019) solve a one dimensional injective partialassignment in quasilinear complexity, but which does not allow for mass destruction in the source measure,while Bai et al. (2023) proposed an efficient procedure with a quadratic worst case complexity. However,their algorithms only apply to measures whose samples have constant mass (e.g., i = j = 1). In the nextsection, we generalize their line of work and propose a new way of combining sliced OT and unbalanced OT.",
  "Sliced Unbalanced OT and Unbalanced Sliced OT": "We present two new scalable and robust OT problems, by combining the unbalanced and slicing strategiesin two different ways. We conduct a theoretical analysis of both strategies and provide a comparison of thetwo. For ease of exposition, all proofs of the results in this section are provided in Appendix A. First, we propose to slice the unbalanced OT problem: we average the UOT problem over different projectionsof the compared measures, similar to the approach of sliced partial OT (Bonneel & Coeurjolly, 2019; Baiet al., 2023). We refer to this problem as Sliced Unbalanced OT (SUOT) and introduce it in .1.Next, we explore the reverse strategy, i.e., we unbalance the sliced OT problem: the weights of SUOT arepenalized to introduce imbalance, analogous to how UOT relates to OT. We call this method UnbalancedSliced OT (USOT) and present it in .2.",
  "with ()1, ()2 the marginal distributions of": "By definition, SUOT is a specific instance of the class of sliced probability divergences (Nadjahi et al.,2020a), where the base divergence is chosen as UOT. SUOT can also be interpreted as a general expressionof the sliced partial OT problem (Bonneel & Coeurjolly, 2019; Bai et al., 2023): while the latter imposesDi = iTV, SUOT allows for the use of arbitrary -divergences. In the following, we establish a set of theoretical properties for SUOT with different choices of -divergencesand cost functions C1. First, we identify sufficient conditions for which the solution of (6) exists. Proposition 3.2 (SUOT: Existence of solutions). Assume that C1 is lower-semicontinuous and that either(i) 1, = 2, = +, or (ii) C1 has compact sublevels on R R and 1, + 2, + inf C1 > 0. Then,the solution of SUOT(, ) exists, in the sense that for any , there exists M+(R R) attainingthe infimum in UOT( , ). The assumptions of Proposition 3.2 are met for some settings of interest, including D1 = D2 = KL (since = +), or D1 = D2 = TV and C1(x, y) = |xy|p (p 1) (since = 1): see (Sjourn et al., 2022a,.1) for more details. Next, we show some topological properties of SUOT. In the next proposition, we prove that SUOT preservesthe metric properties of UOT, which is consistent with (Nadjahi et al., 2020a, Proposition 1). In .3,we study the metrization of the weak-topology with SUOT. Proposition 3.3 (SUOT: Metric properties). Suppose UOT is non-negative, symmetric and/or definiteon M+(R) M+(R). Then, SUOT is respectively non-negative, symmetric and/or definite on M+(Rd) M+(Rd).If there exists p [1, +) s.t.for any , , M+(R), UOT1/p(, ) UOT1/p(, ) +UOT1/p(, ), then SUOT1/p(, ) SUOT1/p(, ) + SUOT1/p(, ). By Proposition 3.3, establishing the metric axioms of UOT between univariate measures (as detailed in(Sjourn et al., 2022a, .3.1)) is sufficient to prove the metric properties of SUOT between mul-tivariate measures. For example, since GHK is a metric for the order p = 2 (Liero et al., 2018), so is theinduced SUOT.",
  "(ii) Assume for M with M M+(R), E|UOT(, n)| (n).Then, for M with M { M+(Rd) : Sd1, M}, E|SUOT(, n)| (n)": "Note that the expectations in Theorem 3.4 are taken with respect to the samples of the empirical measures,which are random. Theorem 3.4 shows that SUOT enjoys a dimension-free sample complexity, even whencomparing multivariate measures. This advantage is recurrent of sliced divergences (Nadjahi et al., 2020b)and further motivates their use on high-dimensional settings. The sample complexity rates (n) or (n)can be deduced from the literature on UOT for univariate measures. For instance, in the GHK setting, therate is given by (n) n1/2 for measures with compact, convex support and continuously differentiabledensities (Vacher & Vialard, 2023, Corollary 3.4), and a suitable class M can be defined. Finally, we derive the dual formulation of SUOT and prove that strong duality holds. This result has impor-tant practical implications, as we will leverage it in to develop a methodology for computing SUOT.Note that the computation of SUOT involves integration with respect to , which generally cannot be donein closed from, as is the case for most sliced divergences. Since our goal is to develop a practical and imple-mentable method, we will consider the Monte Carlo approximation commonly used by practitioners to com-pute sliced divergences (Nadjahi et al., 2020a): we approximate SUOT(, ) as",
  "We first prove that the solution of (9) exists under the same conditions as those for SUOT outlined inProposition 3.2": "Proposition 3.7 (USOT: Existence of solutions). Assume that C1 is lower-semicontinuous and that either(i) 1, = 2, = +, or (ii) C1 has compact sublevels on R R and 1, + 2, + inf C1 > 0. Then,the solution of USOT(, ) exists: there exists (1, 2) M+(Rd) M+(Rd) attaining the infimum in (9).",
  "Sd1 g dK(); , .(10)": "Since USOT does not belong to the class of sliced divergences, establishing its sample complexity is morechallenging compared to SUOT. Based on the literature, one standard technique involves deriving coveringnumber bounds on the space of the dual potentials of USOT. This theoretical question is highly non-trivialgiven the complex structure of E, and as such is out of the scope of this paper. Nevertheless, we investigatethe sample complexity on empirical settings: our experimental results presented in Appendix C.5 suggestthat USOT might also enjoy a dimension-free rate.",
  "In addition to the theoretical analysis previously conducted for SUOT and USOT independently, this sectionprovides further insights to better grasp the differences between these two strategies": "First, by comparing Definition 3.1 with Definition 3.6, SUOT and USOT clearly differ at the conceptual level.Specifically, SUOT(, ) penalizes the marginals of for , where is the coupling that transportsmass from to . In contrast, USOT(, ) directly regularizes the marginals of the coupling between and . To illustrate this difference, we consider (, ) M+(R2) M+(R2) with contaminated byoutliers, then compute SUOT(, ) and USOT(, ). We plot (, ) and the sampled projections (k)k(, left), the marginals of (k)k obtained with SUOT(, ) (, center), and the marginals of((k))k with USOT(, ) (, right). We observe that the source outliers in have been successfullyremoved by USOT(, ) for all k, while they may still appear with SUOT(, ) (e.g., , center: notethe bimodal marginal in blue for = 120). This difference is due to the marignal penalization terms inUSOT(, ), which operate directly w.r.t. (, ) rather than their projections ( , ), unlike SUOT(, ). A question of particular interest regarding probability divergences is how they relate to each other, specificallywhether they yield equivalent topologies. We explore this question for SUOT and USOT. To do so, we",
  "and (x, y) Rd Rd, Cd(x, y) = x yp, with p [1, +)": "Next, we prove that UOT(, ) can be upper-bounded by a functional of SUOT(, ) when (, ) havecompact supports, by adapting the reasoning from Bonnotte (2013, Lemma 5.1.4) to our setting and con-sidering the duals of UOT (Proposition 2.3) and SUOT (Theorem 3.5) instead of the dual of OT and SOT.Most arguments in (Bonnotte, 2013) adapt well to our setting, but establishing a Lipschitz condition on theintegrand of the dual required a more technical approach. To this end, we prove Lemma A.13, which resultsin a different constant value, denoted as c(m(), m(), , R).Theorem 3.11. Let X Rd be a compact set with radius R.Define the cost functions as C1(s, t) =|s t|p , (s, t) R2, and Cd(x, y) = x yp, (x, y) Rd Rd, with p [1, +). Assume either, (i)D1 = D2 = KL; or (ii) p = 1 and D1 = D2 = TV. Then, for any (, ) M+(X) M+(X),",
  "where c(m(), m(), , R) is a constant depending on m(), m(), , R, which is non-decreasing in m() andm()": "We show the equivalence of SUOT, USOT and UOT by combining Theorem 3.11 and Theorem 3.10, assumingthat the constant c(m(), m(), , R) does not depend on m(), m(). This occurs, for example, when themasses of and are uniformly bounded; that is, there exists M R+ such that m() M and m() M.",
  "The equivalence of SUOT, USOT and UOT is a key result for proving that SUOT and USOT metrize weak": "convergence, provided that UOT does (as in the GHK setting (Liero et al., 2018, Theorem 7.25)). Recallthat a sequence of positive measures (n)nN converges weakly to M+(Rd) (denoted by n ) if,for any continuous and bounded f : Rd R, limn+fdn =fd. Theorem 3.12 (Metrizability of the weak topology by SUOT, USOT). Assume the conditions in The-orem 3.11 are met.Let (n)nN be a sequence of measures in M+(X) and M+(X), whereX Rd is a compact set with radius R.Then, SUOT and USOT metrize the weak convergence, i.e.,n limn+ SUOT(n, ) = 0, and n limn+ USOT(n, ) = 0. The metrizability of weak convergence was not studied in related work, including in existing instances of ourframework, such as partial OT (Bonneel & Coeurjolly, 2019; Bai et al., 2023). In addition to complementingprior work, our result paves the way for other research directions. For instance, it can be used to justifythe well-posedness of approximating an unbalanced Wasserstein gradient flow (Ambrosio et al., 2005) usingSUOT, as done for SOT in (Candau-Tilh, 2020; Bonet et al., 2022). Unbalanced Wasserstein gradient flowshave been a key tool in deep learning theory, e.g., to prove global convergence of one-hidden layer neuralnetworks (Chizat & Bach, 2018; Rotskoff et al., 2019).",
  "Background: Frank-Wolfe Algorithm and Application to One-Dimensional Unbalanced OT": "FW is a popular iterative first-order optimization algorithm for solving maxxE H(x), where E is a compactconvex set and H : E R a concave, differentiable function.The procedure consists in maximizing alinear approximation of H at each iteration: given the current iterate xt, FW solves the linear oraclert+1 arg maxrE H(xt), r, then performs xt+1 = (1 t+1)xt + t+1rt+1 with stepsize t+1 typicallychosen as t+1 =2",
  "+t+1. We refer to this step as FWStep and report the pseudo-code in Appendix B.2": "Sjourn et al. (2022b) apply FW to solve a translation-invariant formulation of the dual of UOT(, ) for(, ) M+(R)M+(R), and show that the linear oracle in FWStep is the dual of OT(t, t) where (t, t)are normalized versions of (, ), i.e., m(t) = m(t) = 1. Therefore, computing UOT amounts to solve asequence of OT problems, which can efficiently be done since (t, t) are univariate probability measures.The expression of (t, t) depend on the input measures (, ), the current iterates (ft, gt) and the penaltycoefficients (1, 2).",
  "Sd1 g dK(); , (14)": "where H(f, g; , ) supR D(f + , g ; , ). These alternative duals are translation-invariant since,for any R, H(f + , g ; , ) = H(f, g; , ). If (1, 2) are smooth and strictly concave, then themaximizer in H, denoted by (f, g), exists and is unique. In particular, when D1 = 1KL and D2 = 2KL,(f, g) admits an analytical expression, which is given in the normalization routine (Algorithm 1). This isconvenient as it avoids the need for approximate solvers to compute H(f, g; , ).",
  "Return (, )": "Frank-Wolfe iterations.We then apply FW to solve (13) and(14). We show that each iteration consists in solving a particularsliced OT problem between probability measures that depend onthe input (, ) and the iterates. To clarify this point, we presentbelow the updates of FWStep tailored for each problem, startingwith SUOT. Proposition 4.1 (Frank-Wolfe iterations for SUOT). Let (, ) M+(Rd) M+(Rd) and consider solving (13) with FW. Assumethat (1, 2) are smooth and strictly concave.Given current it-erates (f t, gt)supp(K) E, the solutions of the linear oracle(rt, st)supp(K) are the dual potentials of",
  "Sd1 OT( t, t)dK(), where (t, t) are measures givenby t = f t + (f t, gt) and t = gt (f t, gt)": "Proposition 4.1 shows that each FW iteration for solving the translation-invariant dual of SUOT(, )reduces to solving a balanced sliced OT problem: by (Sjourn et al., 2022b, Proposition 1), the measures(t, t) have the same mass, i.e., m(t) = m(t). When using KL-based penalty terms, the procedure forcomputing (t, t) is detailed in Algorithm 1, and reports the closed-form expression of (f t, gt).",
  "end forReturn USOT(, ), (favg, gavg) as in (10)": "Each iteration requires computing the dual potentials of a sliced OT problem, which is non-trivial: previousimplementations related to sliced OT only output the value of the loss, SOT(, ), typically in the contextof training generative models (Deshpande et al., 2019; Nguyen et al., 2020).We thus design two novelimplementations in PyTorch (Paszke et al., 2019) to compute the dual potentials of sliced OT. The firstone leverages that the gradient of OT(, ) w.r.t. (, ) are optimal (f, g), which allows to backpropagateOT( , ) w.r.t. (, ) to obtain (r, s). The second one computes them in parallel on GPUs usingtheir closed form, which to the best of our knowledge, is a new sliced algorithm. We call SlicedDual(, )the step returning optimal (r, s) solving OT( , ) for all supp(K), and refer to Appendix B.3 forthe algorithms.",
  "Building on Proposition 4.1 and the discussion above, we develop the FW methodology to computeSUOT(, ) and detail it in Algorithm 2. Next, we derive the FW iterates for USOT(, )": "Proposition 4.2 (Frank-Wolfe iterations for USOT). Let (, ) M+(Rd) M+(Rd) and considersolving (14) with FW. Assume that (1, 2) are smooth and strictly concave.Given current iterates(f t, gt)supp(K) E, the solutions of the linear oracle (rt, st)supp(K) are the dual potentials ofSOT(t, t), where (t, t) are measures given by t = (favg + (favg, gavg)) and t = (gavg (favg, gavg)), with favg(x)",
  "Sd1 gt((y))dK()": "The resulting FW methodology, detailed in Algorithm 3, also leverages the Norm and SlicedDual routines.The key difference from SUOT(, ) is in where the integral over supp(K) is performed, leading to adifferent balanced sliced OT problem to solve. Marginals of UOT/USOT.The optimal primal marginals of UOT and USOT are geometric normal-izations of inputs (, ) with discarded outliers. Their computation involves the Norm routine detailed inAlgorithm 1, using optimal dual potentials.This is how we compute marginals in and in theexperiments of : see Appendix B.4 for more details.",
  "Convergence Properties and Complexity": "Convergence and stochastic Frank-Wolfe. Our theoretical setting verifies the assumptions of (Lacoste-Julien & Jaggi, 2015, Theorem 8), thus ensuring fast convergence of our methods. The number of FWiterations needed to converge remains low in our experiments. We give in Appendix B.5 empirical evidencesthat few iterations of FW (F 20) suffice to reach numerical precision. Formally, the preceding algorithms assume that the functional H is given through integrals over the hyper-sphere, describing the set of all possible directions . However, in practice, SOT is computed by Monte-Carloapproximations, i.e., drawing a fixed number K of directions (k)Kk=1 and solving independently the different1D OT problems. In the specific case of SUOT, this does not change much: K FW procedures are ran",
  ": Ablation on BBCSport of": "independently (eventually in parallel) over the fixed set of directions. The case of USOT relies on a globalFW scheme, where favg, gavg are computed w.r.t. a fixed distribution K = (1/K) Kk=1 k. This empiricaldistribution of directions can be considered fixed throughout the FW iterations, or can be drawn indepen-dently for each iteration of the FW procedure. This actually corresponds to a Stochastic FW algorithm,which also converges as our setting verifies the assumptions of (Hazan & Luo, 2016, Theorem 3). We callthis procedure Stochastic USOT, which corresponds to Algorithm 3 except that (k)Kk=1 are sampled at eachiteration. Since this procedure performs well in our experiments (e.g., ) and Ek[K] = , thissuggests the dual in Theorem 3.9 holds for . Algorithmic complexity. FW algorithms and its variants have been widely studied theoretically. Com-puting SlicedDual has theoretically a complexity O(KN log N), where N is the number of samples, andK the number of projections of K. However, we note that the sorting operation, which yields the superlinear complexity, can be computed once for all FW iterations. Consequently, the overall complexity ofSUOT and USOT is thus O(KN log N + FN), where F is the number of FW iterations needed to reachconvergence, with a O(N) complexity. Thus, our formulation enjoy a similar complexity than SOT, whichis particularly appealing. However, Stochastic USOT is more costly, as each iteration requires sorting dataprojected along newly-sampled (k)Kk=1. Its complexity is therefore O(KFN log N). We finally note thatdue to the independent nature of the treatments of every projections, computing both Norm and SlicedDualoperations can be done in parallel, leveraging GPU computations when available. Extension to non-Euclidean settings. Interestingly, our algorithms offer great modularity, in the sensethey can easily be used to compute unbalanced versions of existing variants of SOT. Indeed, while suchvariants differ in the one-dimensional representations of and they use, they all consist in solving 1DOT problems to compare and , which our FW strategy can solve. To illustrate this point, we combinedour FW routine with hyperbolic SOT (Bonet et al., 2023c) to compare measures supported on hyperbolicspaces: see Appendix C.3.",
  "Experiments": "This section presents a set of numerical experiments, which illustrate the effectiveness, robustness andcomputational efficiency of USOT1. We first showcase the benefit of USOT over SUOT and SOT on adocument classification task.Then, we consider experiments in very large scale settings such as colortransfer on every pixels and the computation of barycenters of geophysical datasets.",
  "Document classification": "We first consider a document classification problem (Kusner et al., 2015). Documents are represented asdistributions of words embedded with word2vec (Mikolov et al., 2013) in dimension d = 300. Let Dk be thek-th document and xk1, . . . , xknk Rd be the set of words in Dk. Then, Dk = nki=1 wki xki where wki is thefrequency of xki in Dk normalized s.t. nki=1 wki = 1. Given a loss function L, the document classificationtask is solved by computing the matrixL(Dk, D)",
  "Color transfer": "Color transfer is a long-standing problem in OT, which dates back to the seminal work of Rabin et al. (2010).It consists in aligning the color distributions of two images. While previous works, e.g. (Ferradans et al.,2013; Bonneel et al., 2016), considered color palettes to deal with the complexity of OT, we illustrate thescalability of our methods by considering here the full distributions of pixels within images, in a way similarto (Bonneel & Coeurjolly, 2019). We express the color transfer as a gradient flow, where every pixel is asample in the 3D RGB color space. Formally, let (t) =1NNi=1 xi(t), =1MMj=1 yj, where (resp.) represents the color distribution of the source (resp. target) image. The SOT gradient flow performingcolor transfer consists in iterating the following scheme: X(t + 1) = X(t) XSOT((t), ), where (t) isthe color distribution of the source image at iterations t, supported by pixels from X(t). One of the major",
  "Barycenter of geophysical data": "OT barycenters are an important topic of interest (Le et al., 2021) for their ability to capture mass changesand spatial deformations over several reference measures. In order to compute barycenters under the USOTgeometry on a fixed grid, we employ a mirror-descent strategy similar to (Cuturi & Doucet, 2014a, Algorithm(1)) and described more in depth in Appendix C. We compute unbalanced sliced OT barycenter for climatemodel data. Ensembles of multiple models are commonly employed to reduce biases and evaluate uncer-tainties in climate projections (Sanderson et al., 2015; Thao et al., 2022). The commonly used Multi-ModelMean approach assumes models are centered around true values and averages the ensemble with equal orvarying weights. However, spatial averaging may fail in capturing specific characteristics of the physicalsystem at stake, and we propose to use USOT barycenter instead. We use the ClimateNet dataset (Prabhatet al., 2021), and more specifically the TMQ (precipitable water) indicator.The ClimateNet dataset isa human-expert-labeled curated dataset which captures tropical cyclones (TCs), among other things. Tosimulate the output of several climate models, we take a specific instant (first date of 2011) and apply theelastic deformation from TorchVision (Paszke et al., 2019) in an area close to the eastern part of the U.S.A.As a result, we obtain 4 different TCs, as shown in the first row of . The classical L2 spatial mean is",
  "Conclusion": "We proposed two losses merging unbalanced and sliced OT, with theoretical guarantees and an efficientand modular Frank-Wolfe algorithm.We illustrate the performance improvement over SOT on variousexperiments, and described novel applications of unbalanced OT barycenters of positive measures, with anew case study on geophysical data. These novel results and algorithms pave the way to numerous newapplications of sliced variants of OT, and we believe that our contributions will motivate practitionersto further explore their use in general ML applications, without the cumbersome task of pre-processingprobability measures. We thank the anonymous reviewers for their valuable comments. KF was supported by NSERC Discoverygrant (RGPIN-2019-06512) and a Samsung grant. CB was supported by project DynaLearn from LabexCominLabs and Rgion Bretagne ARED DLearnMe, and by the ANR PEPR PDE-AI. NC was supportedby the ANR AI Chair OTTOPIA ANR-20-CHIA-0030",
  "Luigi Ambrosio, Nicola Gigli, and Giuseppe Savar. Gradient flows: in metric spaces and in the space ofprobability measures. Springer Science & Business Media, 2005. (Cited on p. 8)": "Yikun Bai, Bernhard Schmitzer, Matthew Thorpe, and Soheil Kolouri. Sliced optimal partial transport. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1368113690,2023. (Cited on p. 2, 4, 5, 8, 9, 12, 39) Tadas Baltruaitis, Chaitanya Ahuja, and Louis-Philippe Morency. Multimodal machine learning: A surveyand taxonomy.IEEE transactions on pattern analysis and machine intelligence, 41(2):423443, 2018.(Cited on p. 1)",
  "Jules Candau-Tilh. Wasserstein and sliced-wasserstein distances. Masters thesis, Universit Pierre et MarieCurie, 2020. (Cited on p. 8)": "Laetitia Chapel, Rmi Flamary, Haoran Wu, Cdric Fvotte, and Gilles Gasso. Unbalanced optimal transportthrough non-negative penalized linear regression. Advances in Neural Information Processing Systems, 34:2327023282, 2021. (Cited on p. 12, 37) Lenaic Chizat and Francis Bach.On the global convergence of gradient descent for over-parameterizedmodels using optimal transport. Advances in neural information processing systems, 31, 2018. (Cited onp. 2, 8) Lenaic Chizat, Gabriel Peyr, Bernhard Schmitzer, and Franois-Xavier Vialard. An interpolating distancebetween optimal transport and fisherrao metrics. Foundations of Computational Mathematics, 18:144,2018a. (Cited on p. 3) Lenaic Chizat, Gabriel Peyr, Bernhard Schmitzer, and Franois-Xavier Vialard. Unbalanced optimal trans-port: Dynamic and kantorovich formulations. Journal of Functional Analysis, 274(11):30903123, 2018b.(Cited on p. 2)",
  "Marco Cuturi and Arnaud Doucet. Fast computation of wasserstein barycenters. In International conferenceon machine learning, pp. 685693. PMLR, 2014b. (Cited on p. 41)": "Pinar Demetci, Rebecca Santorella, Manav Chakravarthy, Bjorn Sandstede, and Ritambhara Singh. Scotv2:Single-cell multiomic alignment with disproportionate cell-type representation. Journal of ComputationalBiology, 29(11):12131228, 2022. (Cited on p. 2) Ishan Deshpande, Yuan-Ting Hu, Ruoyu Sun, Ayis Pyrros, Nasir Siddiqui, Sanmi Koyejo, Zhizhen Zhao,David Forsyth, and Alexander G Schwing.Max-sliced wasserstein distance and its use for gans.InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1064810656,2019. (Cited on p. 2, 10)",
  "Richard Mansfield Dudley. The speed of mean glivenko-cantelli convergence. The Annals of MathematicalStatistics, 40(1):4050, 1969. (Cited on p. 2)": "Kilian Fatras, Younes Zine, Rmi Flamary, Remi Gribonval, and Nicolas Courty. Learning with minibatchwasserstein : asymptotic and gradient properties. In Silvia Chiappa and Roberto Calandra (eds.), Pro-ceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics, volume 108of Proceedings of Machine Learning Research, pp. 21312141, Online, 2628 Aug 2020. PMLR. (Cited onp. 2) Kilian Fatras, Thibault Sejourne, Rmi Flamary, and Nicolas Courty. Unbalanced minibatch optimal trans-port; applications to domain adaptation. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38thInternational Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research,pp. 31863197. PMLR, 1824 Jul 2021. (Cited on p. 1, 2) Sira Ferradans, Nicolas Papadakis, Julien Rabin, Gabriel Peyr, and Jean-Franois Aujol.Regularizeddiscrete optimal transport. In Scale Space and Variational Methods in Computer Vision, pp. 428439,2013. (Cited on p. 12) Rmi Flamary, Nicolas Courty, Alexandre Gramfort, Mokhtar Z Alaya, Aurlie Boisbunon, Stanislas Cham-bon, Laetitia Chapel, Adrien Corenflos, Kilian Fatras, Nemo Fournier, et al. Pot: Python optimal trans-port. The Journal of Machine Learning Research, 22(1):35713578, 2021. (Cited on p. 12) Aude Genevay, Lnaic Chizat, Francis Bach, Marco Cuturi, and Gabriel Peyr.Sample complexity ofsinkhorn divergences. In The 22nd international conference on artificial intelligence and statistics, pp.15741583. PMLR, 2019. (Cited on p. 30) Ziv Goldfeld and Kristjan Greenewald. Sliced mutual information: A scalable measure of statistical depen-dence. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan (eds.), Advancesin Neural Information Processing Systems, volume 34, pp. 1756717578. Curran Associates, Inc., 2021.(Cited on p. 2)",
  "Soheil Kolouri, Kimia Nadjahi, Umut Simsekli, Roland Badeau, and Gustavo Rohde. Generalized slicedwasserstein distances. Advances in neural information processing systems, 32, 2019. (Cited on p. 2)": "Stanislav Kondratyev, Lonard Monsaingeon, and Dmitry Vorotnikov. A fitness-driven cross-diffusion systemfrom population dynamics as a gradient flow. Journal of Differential Equations, 261(5):27842808, 2016.(Cited on p. 2) Matt Kusner, Yu Sun, Nicholas Kolkin, and Kilian Weinberger. From word embeddings to document dis-tances. In International conference on machine learning, pp. 957966. PMLR, 2015. (Cited on p. 11, 12,37)",
  "Simon Lacoste-Julien and Martin Jaggi. On the global linear convergence of frank-wolfe optimization variants.Advances in neural information processing systems, 28, 2015. (Cited on p. 10)": "Khang Le, Huy Nguyen, Quang M Nguyen, Tung Pham, Hung Bui, and Nhat Ho.On robust optimaltransport: Computational complexity and barycenter computation.In M. Ranzato, A. Beygelzimer,Y. Dauphin, P.S. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information ProcessingSystems, volume 34, pp. 2194721959, 2021. (Cited on p. 13) Tam Le and Truyen Nguyen. Entropy partial transport with tree metrics: Theory and practice. In Inter-national Conference on Artificial Intelligence and Statistics, pp. 38353843. PMLR, 2021. (Cited on p.2) Matthias Liero, Alexander Mielke, and Giuseppe Savar. Optimal entropy-transport problems and a newhellingerkantorovich distance between positive measures. Inventiones mathematicae, 211(3):9691117,2018. (Cited on p. 2, 3, 4, 5, 6, 8, 19, 21, 29, 30, 36)",
  "Khai Nguyen, Nhat Ho, Tung Pham, and Hung Bui. Distributional sliced-wasserstein and applications togenerative modeling. arXiv preprint arXiv:2002.07367, 2020. (Cited on p. 2, 10)": "Khai Nguyen, Tongzheng Ren, Huy Nguyen, Litu Rout, Tan Minh Nguyen, and Nhat Ho. Hierarchical slicedwasserstein distance. In The Eleventh International Conference on Learning Representations, 2023. (Citedon p. 2) Ruben Ohana, Kimia Nadjahi, Alain Rakotomamonjy, and Liva Ralaivola. Shedding a pac-bayesian lighton adaptive sliced-wasserstein distances. In Proceedings of the 40th International Conference on MachineLearning, 2023. (Cited on p. 2)",
  "Gabriel Peyr, Marco Cuturi, et al. Computational optimal transport: With applications to data science.Foundations and Trends in Machine Learning, 11(5-6):355607, 2019. (Cited on p. 2, 4)": "Khiem Pham, Khang Le, Nhat Ho, Tung Pham, and Hung Bui. On unbalanced optimal transport: Ananalysis of Sinkhorn algorithm. In Hal Daum III and Aarti Singh (eds.), Proceedings of the 37th Inter-national Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pp.76737682. PMLR, 1318 Jul 2020. (Cited on p. 2, 4, 12) Benedetto Piccoli and Francesco Rossi. Generalized wasserstein distance and its application to transportequations with source. Archive for Rational Mechanics and Analysis, 211:335358, 2014. (Cited on p. 30) Prabhat, K. Kashinath, M. Mudigonda, S. Kim, L. Kapp-Schwoerer, A. Graubner, E. Karaismailoglu,L. von Kleist, T. Kurth, A. Greiner, A. Mahesh, K. Yang, C. Lewis, J. Chen, A. Lou, S. Chandran,B. Toms, W. Chapman, K. Dagon, C. A. Shields, T. OBrien, M. Wehner, and W. Collins. Climatenet: anexpert-labeled open dataset and deep learning architecture for enabling high-precision analyses of extremeweather. Geoscientific Model Development, 14(1):107124, 2021. doi: 10.5194/gmd-14-107-2021. (Citedon p. 13)",
  "We provide the formal statement and detailed proof on the existence of a solution for both SUOT and USOT,as mentioned in": "Proposition A.1. (Existence of minimizers) Assume that C1 is lower-semicontinuous and that either(i) 1, = 2, = +, or (ii) C1 has compact sublevels on R R and 1, + 2, + inf C1 > 0. Then thesolution of SUOT(, ) and USOT(, ) exist, i.e., the infimum in (6) and (9) is attained. More precisely,there exists (1, 2) which attains the infimum for USOT(, ) (see Equation 9). Concerning SUOT(, ),there exists for any supp() a plan attaining the infimum in UOT( , ) (see Equation 2). Proof. We leverage (Liero et al., 2018, Theorem 3.3) to prove this proposition. In the setting of SUOT,if such assumptions (i) or (ii) are satisfied for (, ), then they also hold for ( , ) for any Sd1.Hence, UOT( , ) admits a solution .",
  "m())": "In both settings the above bounds implies coercivity of the functional of USOT w.r.t.the masses ofthe measures (1, 2, ). Thus there exists M > 0 such that m(1) = m(2) = m() < M, otherwiseUSOT(, ) = +. By the Banach-Alaoglu theorem, the set of bounded measures (1, 2) is compact, andthe set of plans with such marginals is also compact because Rd is Polish and C1 is lower-semicontinuous(Santambrogio, 2015, Theorem 1.7). Because the functional of USOT is lower-semicontinuous in (1, 2, )and we can restrict optimization over a compact set, we have existence of minimizers for USOT by standardproofs of calculus of variations.",
  "m()) + m()2(1": "m()). Then we consider an anchor dual point b = ((f), (g), f, g) tobound L over a compact set. We take f = 0, g = 0, which are always admissible since we take C1(x, y) 0.Then, since we assume there exists pi 0 in dom(i ), we take f = p1 and g = p2. For these potentials onehas:",
  "L((1, 2), b) = 1(p1)m() p1m(1) + 2(p2)m() p2m(2)": "Note that the functional at this point only depends on the masses of the marginals (1, 2). Since (p1, p2) 0the set of (1, 2) such that L((1, 2), b) 1(0)m() + 2(0)m() is non-empty (at least in a neighbour-hood of (1, 2) = (0, 0), and that (m(1), m(2)) are uniformly bounded by some constant M > 0. By theBanach-Alaoglu theorem, such set of measures is compact for the weak* topology.",
  "Plugging the above relation in the functional L yields the desired result on the dual of USOT and ends theproof": "We mention a strong duality result which is very general and which we use in the proof of Theorem 3.9.This result is taken from (Liero et al., 2018, Theorem 2.4) which itself takes it from (Simons, 2006). Proposition A.2. (Liero et al., 2018, Theorem 2.4) Consider two sets A and B be nonempty convex setsof some vector spaces. Assume A is endowed with a Hausdorff topology. Let L : A B R be a functionsuch that",
  "X Gd, which indeed means that we have the desired permu-tation between supremum and integral": "Lemma A.4. Let p [1, +) and assume that C1(x, y) = |x y|p. Consider two positive measures (, )with compact support. Assume that the measure K is discrete, i.e., K =1KKi=1 i with i Sd1,i = 1, . . . , n. Then, one can swap the integral over the sphere and the supremum in the dual formulation ofSUOT, such that",
  "F : (, (f), (g)) fd( ) +gd( )": "Since the measures (, ) have compact support, then by Lemma A.5, the supremum is attained over a subsetof dual potentials of Y such that for any fixed X, (f, g) are Lipschitz-continuous and bounded, thusuniformly equicontinuous functions (with constants independent of ). By the Ascoli-Arzela theorem, the setof uniformly equicontinuous functions is compact for the uniform convergence. Hence, for any X, thereexists a sequence of dual potentials (f,n, g,n) which uniformly converges to optimal dual potentials (f, g)(up to extraction of subsequence). Besides, we have OT( , ) = F(, f, g) and F(, (f,n), (g,n)) OT( , ) as n +. Denote Fn() F(, (f,n), (g,n)) and OT() OT( , ). In order toapply Lemma A.3, we need to prove that the convergence of (Fn())nN to OT( , ) is uniform w.r.t., i.e., supX |Fn() OT()| 0 as n +.",
  "which means that supX |Fn() OT()| 0, thus concludes the proof": "Lemma A.5. Let p [1, +) and C1(x, y) = |x y|p. Consider two positive measures (, ) M+(Rd)whose support is such that Cd(x, y) = ||x y||p R. Then for any Sd1, one can restrict without loss ofgenerality the problem UOT( , ) as a supremum over dual potentials satisfying f(x)+g(y) C1(x, y),uniformly bounded by M and uniformly L-Lipschitz, where M and L do not depend on .",
  "by the same constant L": "Remark: Extending Theorem 3.9.We conjecture that Theorem 3.9 also holds when is the uniformmeasures over Sd1, since the above holds for any N N and N converges weakly* to . Proving thisresult would require that potentials (f, g) are also regular (i.e., Lipschitz and bounded) w.r.t Sd1.This regularity is proved in (Xi & Niles-Weed, 2022) assuming (, ) have densities, but remains unknownfor discrete measures. Since discretizing corresponds to the computational approach, we assume it to bediscrete, so that no additional assumption than boundedness on (, ) is required. For instance, such resultremains valid for semi-discrete UOT computation.",
  "A.3Metric properties: Proof of Proposition 3.3 and Proposition 3.8": "Proof of Proposition 3.3. Metric properties of SUOT.Symmetry and non-negativity are immedi-ate.Assume SUOT(, ) = 0.Since is the uniform distribution on Sd1, then for any Sd1,UOT( , ) = 0, and since UOT is assumed to be definite, then = . By (Bogachev & Ruas, 2007,Proposition 3.8.6), this implies that and have the same Fourier transform. By injectivity of the Fouriertransform, we conclude that = , hence SUOT is definite. The triangle inequality results from applying",
  "= SUOT1/p(, ) + SUOT1/p(, )": "Proof of Proposition 3.8. Metric properties of USOT. Let (, ) M+(Rd). Non-negativity is imme-diate, as USOT is defined as a program minimizing a sum of positive terms. SOT is symmetric, thus when1 = 2, we obtain symmetry of the functional w.r.t. (, ). Assume D is definite, i.e., D(|) = 0implies = . Assume now that USOT(, ) = 0, and denote by (1, 2) the optimal marginals attainingthe infimum in (9). USOT(, ) = 0 implies that SOT(1, 2) = 0, D(1|) = 0 and D(2|) = 0. Thesethree terms are definite, which yields = 1 = 2 = , hence the definiteness of USOT. The Partial OTsetting (i.e., D = TV) is treated in Appendix A.7.",
  "ESUOT1/p(, ) SUOT1/p(n, n) 2(n)1/p.(23)": "Proof. Since UOT1/p satisfies non-negativity, symmetry and the triangle inequality on M+(R) M+(R),SUOT1/p verifies these three metric properties on M+(Rd)M+(Rd) by Proposition 3.3, and we can deriveits sample complexity as follows. For any , in M+(Rd) with respective empirical approximations n, n,applying the triangle inequality yields for p [1, +),UOT1/p(, ) UOT1/p(n, n) UOT1/p(n, ) + UOT1/p(n, ) .(24)",
  "Theorem A.7. Let (, ) M+(Rd) M+(Rd). Then, SUOT(, ) USOT(, )": "Proof. To show that SUOT(, ) USOT(, ), we use a sub-optimality argument. Let be the solutionUSOT(, ) and denote by (1, 2) the marginals of . For any Sd1, denote by the solution ofOT( 1, 2). By definition of USOT, the marginals of are given by ( 1, 2). Since the sequence() is suboptimal for the problem SUOT(, ), one has",
  "Proof of Theorem 3.11": "Theorem A.9. Let X be a compact subset of Rd with radius R and consider , M+(X). Additionally, letp [1, +) and assume C1(x, y) = |xy|p for (x, y) R and Cd(x, y) = xyp for (x, y) Rd. Let > 0and assume D1 = D2 = KL. Then, UOT(, ) c SUOT(, )1/(d+1), where c = c(m(), m(), , R) isa non-decreasing function of m() and m().",
  "(2R + )dUOT( , ) ,(50)": "where (49) is obtained by taking the supremum of (48) over the set of potentials ( f, g) such that for u [R, R], (x, ) Bd(0, 2R + ) Sd1, f(u) = f(x u), g(u) = g(x u), which is included in the set ofpotentials (f , g) s.t. f : R R, g : R R and f g C1. Therefore,",
  "where (73) results from the definition of push-forward measures. We conclude the proof by observing thatthe supremum in (73) is taken over a subset of E(Rd)": "Lemma A.11. (Santambrogio, 2015, Proposition 1.11) Let p [1, +) and assume Cd(x, y) = x yp.Let , with compact support, such that Cd(x, y) Rp for (x, y) supp() supp(). Then without lossof generality the dual potentials (f, g) of UOT(, ) satisfy f(x) [0, R] and g(y) [R, R].",
  "m()]": "Proof. Consider the translation-invariant dual formulation (74): if (f, g) are optimal, then for any R,(f +, g) are also optimal. We leverage the structure of the dual constraint f g Cd with Lemma A.11.Since for (x, y) supp() supp(), Cd(x, y) R, then without loss of generality, f(x) [0, R] andg(y) [R, R]. The potentials (f, g) are optimal for the translation-invariant dual energy, and we need abound for the original dual functional (3). To this end, we leverage Lemma A.12 to compute the optimal",
  "The Kullback-Leibler setting is treated here. The Partial OT setting (i.e., D = TV) is treated in Ap-pendix A.7": "Proof. Let (n) be a sequence of measures in M+(X) and M+(X), where X Rd is compact with radiusR > 0. First, we assume that n . Then, by (Liero et al., 2018, Theorem 2.25), under our assumptions,n is equivalent to limn+ UOT(n, ) = 0. This implies that limn+ SUOT(n, ) = 0 andlimn+ USOT(n, ) = 0, since by Theorem 3.11 and non-negativity of SUOT (Proposition 3.3),",
  "SUOT(n, ) USOT(n, ) UOT(n, )": "Conversely, assume either that limn+ SUOT(n, ) = 0 or limn+ USOT(n, ) = 0. First assumethere exists M > 0 such that for large enough n N, m(n) M, then by Theorem 3.11, there exists c > 0such that UOT(n, ) cSUOT(n, )1/(d+1). Since c is doesnt depend on the masses (m(n), m()),it does not depend on n. By Theorem 3.11, it yields metric equivalence between SUOT, USOT and UOT,thus limn+ UOT(n, ) = 0. By (Liero et al., 2018, Theorem 2.25), we eventually obtain n , whichis the desired result.",
  "log m()": "m(). Note that the pair (, ) are feasible dual potentials for the constraint f g Cd, becausethe cost Cd is positive in our setting. The property of push-forwards measures means that for any Sd1,one has m( ) = m(). Therefore, we obtain the following bounds for n large enough.",
  "A.7Properties of sliced partial OT": "We provide in this subsection the proofs of Proposition 3.3, Theorems 3.11 and 3.12 for the setting of slicedpartial OT. To this end, we rely on a formulation for SUOT and USOT when D1 = D2 = TV, which weprove below. Equation 76 is proved in (Piccoli & Rossi, 2014) and can then be applied to SUOT: we includeit for completeness. Equation 77 is our contribution and is specific to USOT.",
  "Sd1 f((x))dN() and gavg(x) =": "Sd1 g((x))dN(). This formula on imposes favg(x) and gavg(x) . Furthermore, since we perform a supremum w.r.t. (favg, gavg)where attains a plateau, then without loss of generality, we can impose the constraint favg(x) andgavg(x) , as it will have no impact on the optimal dual functional value. Thus we have that ||favg|| and ||gavg|| .To obtain the Lipschitz property, we use the constraint that f() g() C1 forany supp(N), as well as (Santambrogio, 2015, Proposition 3.1). Thus by using c-transform for thecost C1(x, y) = |x y|, we can take w.l.o.g f() = g() with f() a 1-Lipschitz function. Thus w.l.o.gwe can perform the supremum over (f) E, and rephrase the functional as desired, since we have that(favg) = favg.",
  "We can now prove Proposition 3.3, Theorems 3.11 and 3.12 in the setting of sliced Partial OT. All thoseresults are summarized in the following statement": "Theorem A.15. (Properties of Sliced Partial OT) Assume C1(x, y) = |x y| and D1 = D2 = TV.Then, USOT satisfies the triangle inequality.Additionally, for any (, ) M+(X) where X Rd iscompact with radius R, UOT(, ) c(, R) SUOT(, )1/(d+1), and USOT and SUOT both metrize theweak convergence.",
  "= USOT(, ) + USOT(, )": "Note that reusing Lemma A.14, we have that SUOT is a sliced integral probability metric over the spaceof bounded and Lipschitz functions. More precisely, we satisfy the assumptions of (Nadjahi et al., 2020b,Theorem 3), so that one has UOT(, ) c(, R)(SUOT(, ))1/(d+1). To prove that USOT and SUOT metrize the weak* convergence, the proof is very similar to that of The-orem 3.12 detailed above. Assuming that n implies SUOT(n, ) 0 and USOT(n, ) 0 isalready proved in Appendix A.6. To prove the converse, the proof is also the same, i.e., we use the propertythat SUOT, USOT and UOT are equivalent metrics, which holds as we assumed that supports of (, ) arecompact in a ball of radius R. Note that since the bound UOT(, ) c(, R)(SUOT(, ))1/(d+1) holdsindependently of the measures masses, we do not need to uniformly bound m(n), compared to the KLsetting of Theorem 3.12.",
  "B.2Frank-Wolfe methodology for computing UOT": "Background: FW for UOT.Our approach to compute SUOT and USOT takes inspiration from theconstruction of (Sjourn et al., 2022b). It consists in applying a Frank-Wolfe (FW) procedure over the dualformulation of UOT. Such approach is equivalent to solve a sequence of balanced OT problems betweenmeasures (, ) which are iterative renormalizations of (, ). While the idea holds in wide generality, it isespecially efficient in 1D where OT has low algorithmic complexity, and we reuse it in our sliced setting. FW algorithm consists in optimizing a functional H over a compact, convex set C by optimizing its lineariza-tion H. Given a current iterate xt of FW algorithm, one computes rt+1 arg maxrC H(xt), r, andperforms a convex update xt+1 = (1 t+1)xt + t+1rt+1. One typically chooses the learning rate t =2",
  "f(x) (1 )f(x) + r(x)g(y) (1 )g(y) + s(y)Return (f, g)": "In the setting of UOT, one would take C = {f g Cd}. However, this set is not compact as it contains(, ) for any R.Thus, Sjourn et al. (2022b) propose to optimise a translation-invariant dualfunctional H(f, g; , ) supR D(f + , g ; , ), with D defined in (3). Similar to the balanced OTdual, one has H(f + , g ; , ) = H(f, g; , ), thus one can apply (Santambrogio, 2015, Proposition1.11) to assume w.l.o.g. that, e.g., f(0) = 0 and restrict to a compact set of functions. We emphasize thatFW algorithm is well-posed to optimize H, but not D. Note that once we have the dual variables (f, g) maximizing H, we retrieve optimal dual variables maximizingD as (f + (f, g), g (f, g)), where (f, g) arg maxR D(f + , g ; , ). The KL setting whereD1 = 1KL and D2 = 2KL is especially convenient, because (f, g) admits a closed form, which avoidsiterative subroutines to compute it. In that case, it reads",
  ".(93)": "We summarize the FW algorithm for UOT in the proposition below. We refer to (Sjourn et al., 2022b) formore details on the algorithm and pseudo-code. We adapt this approach and result for SUOT and USOT. Proposition B.1. (Sjourn et al., 2022b)Assume is smooth.Given current iterates (f (t), g(t)),the linear FW oracle of UOT(, ) is OT((t), (t)), where (t) = (f (t) + (f (t), g(t))) and (t) =(g(t) (f (t), g(t))). In particular, one has m((t)) = m((t)), thus the balanced OT problem alwayshas finite value. More precisely, the FW update reads",
  "B.3Implementation of Sliced OT to return dual potentials": "Recall from , Algorithms 2 and 3 and more precisely, Propositions 4.1 and 4.2, that FW linearoracle is a sliced OT program, i.e., a set of OT problems computed between univariate distributions ofM+(R). Therefore, a key building block of our algorithm is to compute the loss and dual variables of theseunivariate OT problems. We explain below how one can compute the sliced OT loss and dual potentials. Thecomputation of the loss consists in implementing closed formulas of OT between univariate distributions, asdetailed in (Santambrogio, 2015, Proposition 2.17). More precisely, when C1(x, y) = |x y|p and (, ) M+(R), then",
  "Backpropagate L w.r.t. (, )Return (f, g) as gradients of L w.r.t. (, )": "The implementation of the dual potentials using 1D closed forms relies on the north-west corner rule prin-ciple, which can be vectorized in PyTorch in order to be computed in parallel. The contribution of ourimplementation thus consists in making such algorithm GPU-compatible and allowing for a parallel compu-tation for every slice simultaneously. We stress that this constitutes a non-trivial piece of code, and we referthe interested reader to the code in our supplementary material for more details on the implementation.",
  "B.4Output optimal sliced marginals": "In all our algorithms, we focus on dual formulations of SUOT and USOT, which optimize the dual potentials.However, one might want the output variables of the primal formulation (See Definition 3.6). In particular,the marginals of optimal transport plans are interesting because they are interpreted as normalized versionsof inputs (, ) where geometric outliers have been removed. We detail where this interpretation comes fromin the setting of UOT, and then give how it is adapted to SUOT and USOT. In particular, we justify thatthe Norm routine suffices to compute them.",
  "B.5Convergence of Frank-Wolfe iterations: Empirical analysis": "We display below an experiment on synthetic dataset to illustrate the convergence of Frank-Wolfe iterations.We also provide insights on the number of iterations that yields a reasonable approximation: a few iterationssuffices in our practical settings, typically F = 20. The results are displayed in . We consider the empirical distributions (, ) computed over respec-tively, N = 400 and M = 500 samples over the unit hypercube d, d = 10. Moreover, is slightly shiftedby a vector of uniform coordinates 0.5 1d. We choose = 1 and report the estimation of SUOT(, ) andUSOT(, ) through Frank-Wolfe iterations. We estimate the true values by running F = 5000 iterations,and display the difference between the estimated score and the true values. Appendix B.5 shows thatnumerical precision is reached in a few tens of iterations. As learning tasks do not usually require an esti-mation of losses up to numerical precision, we think that it is hence reasonable to take F 20 in numericalapplications.",
  "Movie Reviews.The movie reviews dataset (Pang et al., 2002) is composed of 1000 positive and 1000negative reviews. We take five different random 75/25 train/test split. The data can be found in": "Goodreads.This dataset, proposed in (Maharjan et al., 2017), and which can be found at is composed of 1003 books from 8 genres. A firstpossible classification task is to predict the genre. A second task is to predict the likability, which is abinary task where a book is said to have success if it has an average rating 3.5 on the website Goodreads( The five train/test split are randomly drawn with 75/25 proportions.",
  "All documents are embedded with the Word2Vec model (Mikolov et al., 2013) in dimension d = 300. Theembedding can be found in": "In this experiment, we report the results averaged over 5 random train/test split. For discrepancies which areapproximated using random projections, we additionally average the results over 3 different computations,and we report this standard deviation in . Furthermore, we always use 500 projections to approximatethe sliced discrepancies. For Frank-Wolfe based methods, we use 10 iterations, which we found to be enoughto have a good accuracy. We added an ablation of these two hyperparameters in . We report theresults obtained with the best for USOT and SUOT computed among a grid {104, 5 104, 103, 5 103, 102, 101, 1}. For USOT, the best is consistently around 5 103 for the Movies and Goodreadsdatasets, and around 5104 for the BBCSport dataset. We used a second finer grid and reported the resultsobtained with = 0.00021 on BBCSport, = 0.004 for Goodreads on the likability task and = 0.003 forthe genre task. For SUOT, the best obtained was 0.01 for the BBCSport dataset, 1.0 for the movies datasetand 0.5 for the goodreads dataset. For UOT, we used = 1.0 on the BBCSport dataset. For the moviesdataset, the best obtained on a subset was 50, but it took an unreasonable amount of time to run on thefull dataset as the runtime increases with (see (Chapel et al., 2021, )). On the goodreads dataset,it took too much memory on the GPU. For Sinkhorn UOT, we used = 0.1 and = 1.0 on the BBCSportand = 0.001, = 0.1 on the Goodreads dataset, and = 0.01 and = 0.1 on the Movies dataset. Note",
  "C.1.3Additional experiments": "Runtime.We report in the runtime of computing the different discrepancies between each pair ofdocuments, and in the full runtimes. On the BBCSport dataset, the documents have in average 116words, thus the main bottleneck is the projection step for sliced OT methods. Hence, we observe that OTruns slightly faster than SOT and the sliced unbalanced counterparts. Goodreads is a dataset with largerdocuments, with on average 1491 words by document. Therefore, as OT scales cubically with the numberof samples, we observe here that all sliced methods run faster than OT, which confirms that sliced methodsscale better w.r.t. the number of samples. In this setting, we were not able to compute UOT with the POTimplementation in a reasonable time. Computations have been performed with a NVIDIA Tesla V100 GPU.",
  "SUOTAverage (103s)13.91.2114.320.95Full (s)37707193": "Ablations.We plot in accuracy as a function of the number of projections and the number ofiterations of the Frank-Wolfe algorithm. We averaged the accuracy obtained with the same setting describedin Appendix C.1.2, with varying number of projections K {4, 10, 21, 46, 100, 215, 464, 1000} and number ofFW iterations F {1, 2, 3, 4, 5, 10, 15, 20}. Regarding the hyperparameter , we selected the one returningthe best accuracy, i.e., = 5 104 for USOT and = 102 for SUOT.",
  "BBCSportMoviesGoodreads genreGoodreads like": "OT94.5574.4455.2271.00UOT96.73---Sinkhorn UOT95.4572.4853.5567.81SOT89.390.7666.950.4550.090.5165.600.20SUOT90.120.1567.840.3750.150.0466.720.38USOT93.520.0469.210.3752.670.6267.780.39SUSOT92.730.2769.530.5351.930.5367.330.26SUOT (+CV on )90.000.5967.400.6449.670.7966.430.44USOT (+CV on )92.610.5568.640.2952.067.2066.610.72USOT (Unnormalized)860.56---SOPT (Unnormalized)87.270.20--- with other unbalanced sliced methods such as SOPT (Bai et al., 2023). We chose to compare with thiscompetitor since their code is available in Python. However, a numerical restriction of their algorithm isthat it only outputs measures with constants weights, i.e., distributions = ixi and = jyj wherei = j = 1, but the number of samples in and may differ. Under this modeling assumption, the totalmass of each measure corresponds to the number of words in the sentence. We performed the comparison onthe BBC dataset, using 500 projections for both SOPT and USOT. Unfortunately, the quadratic footprintof computing the similarity kernel does not scale reasonably for SOPT for larger datasets such as Movies orGoodreads, especially because their algorithm is not GPU-compatible compared to ours. We cross-validatedthe parameter {p.10k, k 0, 6, p {1., 5.}} The result is detailed in the table below. What is noticeable is that the performance degrades for both USOTand SOPT using this parametrization. Furthermore, we observed that the paramater yielding the bestaccuracy is much smaller for unnormalized measures than for the best one for normalized histograms (i.e.,1e 5 here compared to 1e 3 with normalized measures). Our interpretation of this observation is thatconsidering unnormalized measures adds an additional information of the sentence length via the masses of(, ). It seems that this additional information dominates the comparison of measures, instead of focusingon the measures support (i.e., the word embedding) which encodes the semantic information of words. When is large the kernel value of USOT/SOPT is mainly dictated by the mass (i.e., sentence length) comparison.Thus smaller seems to give less importance on sentence length, hence a better performance. We alsonote that performance of SOPT and USOT on unnormalized measures are rather similar. It means that forthe choice of marginal prior D = TV or D = KL does not significantly matter for this specific task,compared to the preprocessing normalization of measures.",
  "end while": "We illustrate this algorithm with several examples of interpolation in . We propose to compute aninterpolation between two measures located on a fixed grid of size 200 200 with different values of i inDi = iKL. For illustration purposes, we construct the source distribution as a mixture of two Gaussianswith a small and a larger mode, and the target distribution as a single Gaussian. Those distributions arenormalized over the grid such that both total norms are equal to one (which is not required by our unbalancedsliced variants but grants more interpretability and possible comparisons with SOT). a shows theresult of the interpolation at three timestamps (t = 0.25, 0.5 and 0.75) of a SOT interpolation (within thissetting, 1 = 1 t and 2 = t). As expected, the two modes of the source distribution are transported overthe target one. We verify in b that for a large value of 1 = 2 = 100, the USOT interpolationbehaves similarly as SOT, as expected from the theory. When 1 = 2 = 0.01, the smaller mode is notmoved during the interpolation, whereas the larger one is stretched toward the target (c). Finally, ind, an asymmetric configuration of 1 = 0.01 and 2 = 100 allows to get an interpolation when onlythe big mode of the source distribution is displaced toward the target. In all those cases, the mirror-descentalgorithm 7 is run for 500 iterations. Even for a large grid of 200 200, those different results are obtainedin a 2 3 minutes on a commodity GPU, while the OT or UOT barycenters are untractable with a limitedcomputational budget.",
  "C.3Unbalanced version of hyperbolic SOT": "To illustrate the modularity of our FW algorithm, we aim at comparing synthetic mixtures of WrappedNormal Distribution on the 2-hyperbolic manifold H (Nagano et al., 2019), so that the FW oracle is hyperbolicsliced OT (Bonet et al., 2023c). The parameter characterizes on H any geodesic curve passing throughthe origin, and each sample is projected by taking the shortest path to such geodesics. Once projected on ageodesic curve, we sort data and compute SOT w.r.t. hyperbolic metric dH. We consider the 2-hyperbolicmanifold on the Poincar disc. As illustrated in , the input measure (in red) is a mixture of 3isotropic normal distributions, with a mode at the top of the disc playing the role of an outlier. The measure is a mixture of two anisotropic normal distributions, whose means are close to two modes of , but areslightly shifted at the discs center. We show on the impact of the parameter = 1 = 2 on theoptimal marginals of USOT.",
  ": KDE estimation (kernel ed2H/) of optimal (1, 2) of USOT(, ) when Di = KL": "This experiment illustrates several take-home messages, mentioned in . First, the optimal marginals(1, 2) are renormalisation of (, ) accounting for their geometry, which are able to remove outliers forproperly tuned . When is large, (1, 2) (, ) and we retrieve SOT. When is too small, outliersare removed, but we see a shift of the modes, so that modes of (1, 2) are closer to each other, but do notexactly correspond to those of (, ). Second, note that such plot cannot be made with SUOT, since theoptimal marginals depend on the projection (see ). Third, we emphasize that we are indeed ableto reuse any variant of SOT.",
  "C.4Choice and interpretation of hyperparameter": "An immediate drawback of our framework is the induced additional computational cost w.r.t. SOT. Whilethe above experimental results show that SUOT and USOT improve performance significantly over SOT,and though the complexity is still sub-quadratic in number of samples, our FW approach uses SOT as asubroutine, rendering it necessarily more expensive. Additionally, another practical burden comes from theintroduction of extra hyperparameters (1, 2), which may be tuned using cross-validation. Therefore, afuture direction would be to derive efficient strategies to tune (1, 2), maybe w.r.t. the applicative context,and further complement the possible interpretations of as a threshold for the geometric informationencode by C1, Cd. While we leave the automation of tuning (1, 2) for future works, we provide below somedetails and intuitions on the choice of for the previous experiments. We hope these insights will help thepractitioner on how they should chose tune this additional parameter. General intuition on .The parameter when 1 = 2 = can be understood as a characteristicdistance to decide whether or not two sample should be matched by the coupling in the primal formulationof (2). Typically, transportation happens for samples (x, y) such that Cd(x, y) , while samples such thatCd(x, y) are interpreted as geometric outliers, and are discarded in the matching (x, y). In the case ofSUOT and USOT, there is somehow a similar interpretation, but not for the same quantities, and we relyon their definitions (Equations 7 and 10), as well as the constraint set E in Theorem 3.9. One sees that for SUOT(, ) we have a set of 1D-UOT problems between ( , ), thus the thresholdinterpretation holds depending on whether C1((x), (y)) or C1((x), (y)) . In particular thedependence in explains why the outlier threshold depends on the considered projection. Note also weconsider C1 instead of Cd. For USOT(, ) it is different because the marginals (1, 2) which we optimize in Equation (9) areindependent of , and common to all projections.Informally speaking, we interpret that the thresh-old value to discard a matching between (x, y) depends on whether some quantity proportional to Sd1 C1((x), (y))d(x, y)d() is larger or smaller than . This quantity is not properly defined asit depends on the optimized variables (), hence the informality of our intuition. However, we wish toemphasize that the parameter should be interpreted differently between SUOT and USOT. As highlighted",
  "experimentally for document classification in , we observe that the value of yielding the bestperformance is not the same for each loss": "Choice of for hyperbolic data.In , the hyperbolic distance between overlapping modes is0.96, while distance from side modes to the top red outlier is 2.83. Thus, a proper choice of should lie inbetween, which seems consistent with the observation of for = 1. Indeed we see that we havea satisfying trade-off between removing the top mode and preserving the crescent shape structure of mainblue modes. Choice of for barycenter experiments.For the barycenter, we used insights from whichinterpolates circular blobs using asymmetric (1, 2), where 1 is the parameter penalizing the input measuresfidelity, and 2 the parameter of the barycenter. For (especially line (d)), we also took assymetric(1, 2) with large 2 = 1e4 for the barycenter to force data matching. Then for inputs 1 = 1e1 is roughlythe distance between cyclones (see ), to keep them in the barycenter.All in all, we force thebarycenter to match the cyclone structure which matters most, while any structure who would be beyondthis 1 distance between input measure would be discarded. Interpretation of for document classification.In this task the measures support are given by wordembedding in high dimension, for which we have no intuition of what is for instance the characteristic distancebetween different semantic clusters, and thus no idea on how should be tuned. For this reason (and moregenerally in ML tasks), we need to perform a cross-validation over this hyperparameter. We would like tocomment the dependence of the document classification accuracy w.r.t. , which can be observed in .One can notice that as increases, the accuracy increases until it reaches a peak, until then it decreasesto reach a plateau as . When , SUOT and USOT converge to SOT (see Definitions 2.4and 3.6), and we get similar performances. As 0, marginals (1, 2) are allowed to differ significantlyfrom inputs (, ), meaning that SUOT/USOT almost ignore input data. Therefore, should be tuned toextract information from inputs while removing noise. In , the peaks correspond to such optimal, and the gain in performance justify the use of SUOT/USOT over SOT.",
  "C.5Illustration of the sample complexity": "We investigate the sample complexity of SUOT and USOT in practice and report the results in .Our goal is to empirically verify Theorem 3.4 for SUOT, and explore the convergence rate for USOT. Tothis end, we consider = = N(0d, Id) and compute SUOT(n, n) and USOT(n, n) for differentnumber of samples n and dimension d. This allows us to explore the convergence rate of SUOT(n, n) toSUOT(, ) = 0 (respectively, of USOT(n, n) to USOT(, ) = 0) as a function of n and d."
}