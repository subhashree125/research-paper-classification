{
  "Abstract": "In selective classification (SC), a classifier abstains from making predictions that are likelyto be wrong to avoid excessive errors. To deploy imperfect classifierseither due to intrinsicstatistical noise of data or for robustness issue of the classifier or beyondin high-stakesscenarios, SC appears to be an attractive and necessary path to follow. Despite decades ofresearch in SC, most previous SC methods still focus on the ideal statistical setting only, i.e.,the data distribution at deployment is the same as that of training, although practical datacan come from the wild. To bridge this gap, in this paper, we propose an SC framework thattakes into account distribution shifts, termed generalized selective classification, that coverslabel-shifted (or out-of-distribution) and covariate-shifted samples, in addition to typical in-distribution samples, the first of its kind in the SC literature. We focus on non-training-basedconfidence-score functions for generalized SC on deep learning (DL) classifiers, and proposetwo novel margin-based score functions. Through extensive analysis and experiments, weshow that our proposed score functions are more effective and reliable than the existingones for generalized SC on a variety of classification tasks and DL classifiers. The code isavailable at",
  "Introduction": "In practice, classifiers almost never have perfect accuracy. Although modern classifiers powered by deepneural networks (DNNs) typically achieve higher accuracy than the classical ones, they are known to beunrobust: perturbations of inputs that are inconsequential to human decision making can easily alter DNNclassifiers predictions (Carlini et al., 2019; Croce et al., 2020; Hendrycks & Dietterich, 2018; Liang et al.,2023), and more generally, shifts in data distribution in deployment from that in training often cause sys-tematic classification errors. These classification errors, regardless of their source, are rarely acceptable forhigh-stakes applications, such as disease diagnosis in healthcare. To achieve minimal and controllable levels of classification error so that imperfect and unrobust classifierscan be deployed for high-stakes applications, a promising approach is selective classification (SC): samplesthat are likely to be misclassified are selected, excluded from prediction, and deferred to human decisionmakers, so that the classification performance on the remaining samples reaches the desired level (Chow,1970; Franc et al., 2023a; Geifman & El-Yaniv, 2017). For example, by flagging and passing uncertain patientcases that it tends to mistake on to human doctors, an intelligent medical agent can make confident andcorrect diagnoses for the rest. This conservative classification framework not only saves doctors efforts,but also avoids liability due to the agents mistakes.",
  "Published in Transactions on Machine Learning Research (10/2024)": ": Summary of AURC- for . The AURC numbers are on the 102 scalethe lower, the better.The score functions proposed for SC are highlighted in gray, and the rest are originally for OOD detection.The best AURC numbers for each coverage level are highlighted in bold, and the 2nd and 3rd best scores areunderlined.",
  "Type C errors: errors made on covariate-shifted samples, i.e., samples drawn from a different inputdistribution DX where DX = DX but with groundtruth labels from Y": "It is clear that in practical deployment of classifiers, samples can come from the wild, and hence Type A,Type B and Type C errors can coexist. In order to ensure the reliable deployment of classifiers in high-stakesapplications, we must control the three types of errors, jointly. Unfortunately, previous research falls short ofa unified treatment of these errors. Classical SC (Chow, 1970) focuses on rejecting samples that cause In-Derrors (Type A), whereas the current out-of-distribution (OOD) detection research (Yang et al., 2021; Parket al., 2023) focuses on detecting label-shifted samples (Type B). Although Hendrycks & Gimpel (2016);Granese et al. (2021); Xia & Bouganis (2022); Kim et al. (2023) have advocated the simultaneous detectionof samples that cause Type A and Type B errors, their approaches still treat the problem as consisting of twoseparate tasks, reflected in their separate and independent performance evaluation on OOD detection andSC. Regarding the challenge posed by Type C errors, existing work (Hendrycks & Dietterich, 2018; Croceet al., 2020) focuses primarily on obtaining classifiers that are more robust to covariate shifts, not on rejectingpotentially misclassified samples due to covariate shiftsthe latter, to the best of our knowledge, has notyet been explicitly considered, not to mention joint rejection together with Type A and Type B errors. In this paper, our goal is to close the gap and consider, for the first time, rejecting all three types of errorsin a unified framework. For brevity, we use the umbrella term distribution shifts to cover both label shiftsand covariate shifts, which are perhaps the most commonly seen types of distribution shifts, with the caveatthat practical distribution shifts can also be induced by other sources. So, we call the unified frameworkconsidered in this paper selective classification under distribution shifts, or generalized selective classification.Another key desideratum is practicality. With the increasing popularity of foundation models and associateddownstream few-shot learners (Brown et al., 2020; Radford et al., 2021; Yuan et al., 2021), accessing massiveoriginal training data becomes increasingly more difficult. Moreover, there are numerous high-stakes domainswhere training data are typically protected due to privacy concerns, such as healthcare and finance. Theseapplied scenarios call for SC strategies that can work with given pretrained classifiers and do not requireaccess to the training data, which will be our focus in this paper. Our contributions include: We advocate a new SC framework, generalized selective classification, which rejects samples that couldcause Type A, Type B and Type C errors jointly, to improve classification performance over the non-rejected samples. With careful review and reasoning, we argue that generalized SC covers and unifies thescope of the existing OOD detection and SC, if the goal is to achieve reliable classification on the selectedsamples. (Sections 2.3 and 2.4) Focused on non-training-based (or post-hoc) SC settings, we identify a critical scale-sensitivity issue ofseveral SC confidence scores based on softmax responses (.1) which are popularly used andreported to be the state-of-the-art (SOTA) methods in the existing SC literature (Geifman & El-Yaniv,2017; Feng et al., 2023). We propose two confidence scores based on the raw logits (v.s. the normalized logits, i.e., softmax re-sponses), inspired by the notion of margins (.2). Through careful analysis (.3) andextensive experiments (), we show that our margin-based confidence scores are more reliable forgeneralized SC on various dataset-classifier combinations, even under moderate distribution shifts.",
  "Selective classification (SC)": "Consider a multiclass classification problem with input space X Rn, label space Y = {1, . . . , K}, and datadistribution DX,Y on X Y. A selective classifier (f, g) consists of a predictor f : X RK and a selectorg : X {0, 1} and works as follows:",
  "(selection risk) Rs, = EDX,Y[(f(x), y)gs,(x)]/s,,(lower the better),(3)": "Because a high coverage typically comes with a high selection risk, there is always a need for risk-coveragetradeoff in SC. Most of the existing work considers to be the standard 0/1 classification loss (Chow, 1970;El-Yaniv et al., 2010; Geifman et al., 2018), and we also follow this convention in this paper. A classicalcost-based formulation is to optimize the risk-coverage (RC) tradeoff (Chow, 1970)",
  "also depend on the posterior probabilities (Pietraszek, 2005; Geifman & El-Yaniv, 2017; Franc et al., 2023a;El-Yaniv et al., 2010)": "Training-based scoresDue to the intractability of true posterior probabilities in practice, many previousmethods focus on learning effective confidence-score functions from training data. They require access totraining data and learn parametric score functions, often under cost-based/constrained formulations andtheir variants for the RC tradeoff. This learning problem can be formulated together with (Chow, 1970;Pietraszek, 2005; Grandvalet et al., 2008; El-Yaniv et al., 2010; Cortes et al., 2016; Geifman & El-Yaniv,2019; Liu et al., 2019; Huang et al., 2022; Gal & Ghahramani, 2016; Lakshminarayanan et al., 2017; Geifmanet al., 2018; Maddox et al., 2019; Dusenberry et al., 2020; Lei, 2014; Villmann et al., 2016; Corbire et al.,2019) or separately from training the classifier (Jiang et al., 2018; Fisch et al., 2022; Franc et al., 2023a).However, Feng et al. (2023) has recently shown that these training-based scores do not outperform simplenon-training-based scores described below.",
  ": Deploy the selector gs, based on Eq. (2)": "Manually designed (non-training-based) scoresThis family works with any given classifier and doesnot assume access to the training set. This is particularly attractive when it comes to modern pretrainedlarge DNN models, e.g., CLIP (Radford et al., 2021), Florence (Yuan et al., 2021), and GPTs (Brown et al.,2020), for which obtaining the original training data and performing retraining are prohibitively expensive,if not impossible, to typical users. Algorithm 1 shows a typical use case of SC with non-training-basedscores. Different confidence scores have been proposed in the literature. For example, for support vectormachines (SVMs), confidence margin (the difference of the top two raw logits) has been used as a confidencescore (Fumera & Roli, 2002; Franc et al., 2023a); see also .2. For DNN models, which is our focus,confidence scores are popularly defined over the softmax responses (SRs). Assume that z RK contains theraw logits (RLs) and is the softmax activation. The following three confidence-score functions",
  "i (zi) log (zi),(7)": "are popularly used in recent work, e.g., Feng et al. (2023); Granese et al. (2021); Xia & Bouganis (2022).Although simple, SRmax can easily beat existing training-based methods (Feng et al., 2023). On the otherhand, these SR-based score functions generally follow the plug-in principle by assuming that SRs approximateposterior probabilities well (Franc et al., 2023a). Unfortunately, this assumption often does not hold inpractice, and bridging this approximation gap is a major challenge for confidence calibration (Guo et al.,2017; Nixon et al., 2019). However, Zhu et al. (2022) reveals that recent calibration methods may evendegrade SC performance.",
  "SC under distribution shifts: generalized SC": "In this paper, we consider SC under distribution shifts, or generalized selective classification. Shifts betweentraining and deployment distributions are common in practice and can often cause performance drops indeployment (Quinonero-Candela et al., 2008; Rabanser et al., 2019; Koh et al., 2021), raising reliabilityconcerns for high-stakes applications in the real world. In this paper, we use the term distribution shiftsto cover both covariate and label shiftsperhaps the most prevalent forms of distribution shifts (see thebeginning of for their definitions)jointly.Although the basic set-up for our generalized SCframework remains the same as that of Eqs. (1) and (2), we need to modify the definitions for selection riskand coverage in Eq. (3) to take into account potential distribution shifts:",
  ": Deploy the OOD detector according to Eq. (9)": "Here, sOOD() is a confidence-score function indicating the likelihood that the input is an In-D sample, and is again a tunable cutoff threshold. Although by the literal meaning of OOD both covariate and label shiftsare covered by DOODX, the literature on OOD detection focuses mainly on detecting label-shifted samples,i.e., covariate-shifted DOODXinduced by label shifts (Liu et al., 2020; Sun et al., 2021; Wang et al., 2022; Sunet al., 2022). OOD detection is commonly motivated as an approach to achieving reliable predictions: underthe assumption that DOODXis induced by label shifts only, any OOD samples will cause misclassificationand hence should be excludedclearly aligned with the goal of SC. Algorithm 2 shows the typical use caseof OOD (label-shift) detectors, and its similarity to SC shown in Algorithm 1 is self-evident.However,OOD detection clearly aims for less than generalized SC in that: (1) even if the OOD detection is perfect,misclassified sampleseither as In-D or due to distribution shiftsby imperfect classifiers are not rejected,and (2) practical OOD detectors may fail to perfectly separate In-D and OOD samples, OOD detected butcorrectly classified In-D samples are still rejected, hurting the classification performance on the selectedsamples; see Appendix C for an illustrative example. Therefore, if we are to achieve reliable predictions byexcluding samples that are likely to cause errors, we should directly follow the generalized SC instead of theOOD detection formulation.",
  ":Visualization of thenormalized AURC-the areain blue divided by the coveragevalue": "Other related conceptsBesides OOD detection, OOD generalizationfocuses on correctly classifying In-D and covariate-shifted samples, with-out considering prediction confidence and selection to improve predictionreliability; open-set recognition (OSR) focuses on correctly classifying In-D samples, as well as flagging label-shifted samples; see Geng et al. (2020)for a comprehensive review. In contrast, generalized SC covers all In-D,label-shifted, and covariate-shifted samples, the widest coverage comparedto these related concepts, and targets the most practical and pragmaticmetricclassification performance on the selected samples. Prior work on SC with distribution shiftsAlthough the existingliterature on SC is rich (Zhang et al., 2023), research work that considersSC with potential distribution shifts is very recent and focuses only onlabel shifts: Xia & Bouganis (2022); Kim et al. (2023) perform In-D SC andOOD (label shift) detection together with a confidence score that combinesan SC score and an OOD score, but they still evaluate the performanceof In-D SC and OOD detection separately. Mller et al. (2023); Cattelan& Silva empirically show that existing OOD scores are not good enoughfor SC tasks with covariate/label-shifted samples; Cattelan & Silva proposes ways to refine these scores withthe help of additional datasets to optimize performance. Franc et al. (2024) provides theoretical insights onSC with In-D and label-shifted samples. In contrast, we focus on identifying better confidence scores forgeneralized SCthat covers both In-D and covariate/label-shifted samples and maximizes the utility of theclassifier, and unify the evaluation protocol (see .4).",
  "Few words on implementing Algorithm 1 in practice": "In the practical implementation of generalized SC for high-stakes applications after Algorithm 1, it is nec-essary to select a cutoff threshold based on a calibration set to meet the target coverage, or more likelythe target risk level. However, in this paper, we follow most existing work on SC and do not touch on issuessuch as how the calibration set should be constructed and how the threshold should be selectedwe leavethese for future work. Our evaluation here, again, as most existing SC work, is only about the potential ofspecific confidence-score functions for generalized SC, measured by the RC curve, AUPC, and normalizedAURC-s, directly on test sets that consist of In-D, OOD, and covariate-shifted samples.",
  "Scale sensitivity of SR-based scores": "As discussed in .2, most manually designed confidence scores focus on DNN models and are based onsoftmax responses (SRs), assuming that SRs closely approximate true posterior probabilitiesclosing suchapproximation gaps is the goal of confidence calibration. However, effective confidence calibration remainselusive (Guo et al., 2017; Nixon et al., 2019), and the performance of SR-based score functions is sensitiveto the scale of raw logits and hence that of SRs, as explained below.",
  "/2,": "2/2], equal variance0.15 I, and equal weight 1/4. If we treat each component of the mixture as a class and consider theresulting 4-class classification problem, it is easy to see that the optimal 4-class linear classifier is f(x) =[w1, w2, w3, w4]x, with the decision rule arg maxj{1,2,3,4} wj x; see (a) for visualization of the datadistribution and decision boundaries (i.e., the lines x1 = 0 and x2 = 0). Moreover, this f(x) is also a Bayesoptimal classifier as well as the maximum a posterior (MAP) classifier, for our particular problem here. Now,given any input x, we consider scaled raw logits f(x) for different scale factors = 0.1, 1, 2, 4 and plot theresulting RC curves for SRmax, SRdoctor, and SRent, respectively; see (b)-(d). For reference, we alsoinclude the RC curves based on the true posterior probabilities (denoted as spost), which are available forour simple data model here. We can observe that for SR-based functions (SRmax, SRdoctor, and SRent),",
  "(a) Data and classifier visualization(b) SRmax(c) SRdoctor(d) SRent": ": RC curves for (b) SRmax, (c) SRdoctor, and (d) SRent, calculated based on scaled (by factor 0.1,1.0, 2.0, and 4.0, respectively) raw logits from the optimal 4-class linear classifier using data shown in (a).The RC curves for RLconf-M and spost are also plotted for reference, where RLconf-M is one of our proposedconfidence-score functions.",
  "their RC curves and hence the associated AURCs vary as changes, and these curves approach a commoncurve (RLconf-M, which we will explain below) as becomes large": "Why it happens?The above observations are not incidental. To see why the curves change with respectto , note that for a given test set {xi} and a fixed classifier f, the RC curve for any score function s isfully determined by the ordering of s(xi)s (Franc et al., 2023a). But this ordering is sensitive to the scaleof the raw logits for all three SR-based score functions: SRmax, SRdoctor, and SRent. Take s = SRmax asan example and consider any sample x with its corresponding raw logits z sorted in descending order (i.e.,z(1) z(2) ) without loss of generality. Then for any scale factor > 0 applied to z, we have the score",
  "j e(z(j)z(1)).(10)": "This means that the score is determined by all the scaled logit gaps (z(j) z(1))s. Moreover, due to theinner exponential function, small gaps gain more emphasis as increases, and all gaps receive increasinglymore emphasis as decreases. Such a shifted emphasis can easily change the order of scores for two datasamples, depending on how different their raw logits are distributed. Clearly, ez(1)/",
  "This implies that the asymptotic RC curve as for all three score functions is fully determined by thescore function z(1) z(2)!": "ImplicationsThe sensitivity of the RC curves, and hence of the performance, of these SR-based scoresto the scale of raw logits is disturbing. It implies that one can simply change the overall scale of the rawlogitswhich does not alter the classification accuracy itselfto claim better or worse performance of anSR-based confidence-score function for selective classification, making the comparison of different SR-basedscores shaky. Unfortunately, between the limiting cases 0 and , there is no canonical scaling.",
  "(wyx + by)/wy2 maxj{1,...,K}\\y (wj x + bj)/wj2 ,(11)": "where y arg maxj{1,...,K}(wj x + bj)/wj2. In other words, it is the difference between the top twosigned distances of x to all K hyperplanes. Intuitively, the larger the geometric margin, the more confidentthe classifier is in classifying the sample following the largest signed distancea clearer winner earns moretrust. Although the interpretation is intuitive, the geometric margin is not popularly used in multiclassSVM formulations, likely due to its non-convexity. Instead, a popular proxy for the geometric margin is theconvex confidence margin:",
  "(wyx + by) maxi{1,...,K}\\y (wi x + bi),(12)": "with the decision rule y arg maxj{1, ,K} wj x + bj; see Appendix A. Despite its numerical convenience,the confidence margin loses geometric interpretability compared to the geometric margin, and it can besensitive to the scaling of wj. We study both margins in this paper. Margins in DNNsTo extend the idea of margins to a DNN classifier f(x) parameterized by , we viewall but the final linear layer as a feature extractor, denoted as f e. So, for each sample x, the logit outputtakes the form z = W f e(x) + b, and thus the signed distance of the representation f e(x) to each decisionhyperplane in the representation space is: dj = (wj f e(x) + bj)/wj2j {1, . . . , K}. Assume sortedsigned distances and logits, i.e., d(1) d(2) . . . d(K) and z(1) z(2) . . . z(K). The geometric marginand the confidence margin are defined as",
  "RLgeo-M d(1) d(2)andRLconf-M z(1) z(2), respectively.(13)": "Note that both RLgeo-M and RLconf-M are computed using the raw logits without softmax normalization; zsand ds may not have the same ordering due to the scale of wj2. In fact, RLconf-M is applied in LeCunet al. (1989) to formulate an empirical rejection rule for a handwritten recognition system, although nodetailed analysis or discussion is given on why it is effective. Despite the simplicity of these two notions ofmargins, we have not found prior work that considers them for SC except for LeCun et al. (1989). Scale-invariance propertyAn attractive property of margin-based score functions is that their SC per-formance is invariant w.r.t. the scale of raw logits. This is because changing the overall scale of the rawlogits does not change the order of scores assigned by either the geometric or the confidence margin. In thisregard, margin-based score functions are much more preferred and reliable than SR-based scores for SC.Another interesting point is that the limiting curve depicted in (b)-(d) is induced by the confidencemargin, as is clear from Lemma 3.1 and the discussion following it.",
  "Analysis of rejection patterns": "We continue with the toy example in .1 to show another major difference between the SR-based andthe margin-based score functionsthey have different rejection patterns for given coverage levels. We willsee that margin-based score functions induce favorable rejection patterns and can hence be used for reliablerejection even under moderate covariate shifts. For comparison, we also consider the maximum raw logit(denoted as RLmax) to show that a single logit in multiclass classification is not a sensible confidence score.",
  "(a-3)(b-3)(c-3)(d-3)": ": Further analysis of the numerical example in .1. Case 1, Case 2, and Case 3 correspondto the original dataset in .1, the dataset after small perturbations, and the dataset after substantialperturbations, respectively. Here, (a-)s are the RC curves achieved by different selection scores; (b-)s arevisualizations of the samples (one color per class), decision boundaries (dashed blue line) and the rejectedsamples (black crosses) at coverage 0.8 by RLgeo-M; (c-)s visualize the rejected samples (black crosses) atcoverage 0.8 by SRmax; and (d-)s present the histogram of the robustness radius of the selected samples inby all score functions. Case 1: We use the same setup as in the numerical experiment in .1 (see also ), and plot in (a-1) the RC curves induced by the various confidence-score functions2. It is clear that RLgeo-Mperforms the best. To better understand the difference between RLgeo-M and other score functions, westudy their rejection patterns: we visualize in (b-1)&(c-1) the samples rejected at 0.8 coveragefor RLgeo-M and SRmax, respectively; see visualization of other score functions in Appendix D, whoserejection patterns are similar to that of SRmax. An iconic feature of RLgeo-M is that it prioritizes rejectingsamples closer to decision boundaries, whereas SR-based scores prioritize rejecting samples close to theorigin.Conceptually, the former rejection pattern is favorable, as the goal of SC is exactly to rejectuncertain samples on which classifiers decisions can be shaky. More precisely, the difference in rejection",
  "Comparison with nontraining-based score functions using pretrained models": "SetupsWe take different pretrained DNN models in various classification tasks and evaluate SC perfor-mance on test datasets composed of In-D and distribution-shifted samples jointly. Specifically, our evaluationtasks include (i) ImageNet (Russakovsky et al., 2015), the most widely used testbed for image classification,with a covariate-shifted version ImageNet-C (Hendrycks & Dietterich, 2018) composed of synthetic pertur-bations, and OpenImage-O (Wang et al., 2022) composed of natural images similar to ImageNet but withdisjoint labels, i.e., label-shifted samples; (ii) iWIldCam (Beery et al., 2020) test set provides two subsets ofanimal images taken at different geo-locations, where one is the same as the training set serving as In-D andthe other at different locations as a natural covariate-shifted version; (iii) Amazon (Ni et al., 2019) test setprovides two subsets of review comments by different users, producing In-D and natural covariate-shiftedtest samples for a language sentiment classification task; (iv) CIFAR-10 (Krizhevsky et al., 2009), a smallimage classification dataset commonly used in previous training-based SC works, together with CIFAR-10-C(perturbed CIFAR-10) and CIFAR-100 (with disjoint labels from CIFAR-10), popularly used covariate-shiftedand label-shifted versions of CIFAR-10. Tables 1 and 2 summarize the pretrained models and datasets.",
  "CIFARCIFAR-10 (val)10 - 10,000CIFAR-10-C (severity 3)10,000 19CIFAR-10010,000*All types of corruptions": "Evaluation metricsWe report both the RC curves and the AURC- where {0.1, 0.5, 1} as discussedin .4. Note that when plotting the RC curves, we omit SRdoctor because it almost overlaps withSRmax, which is also observed by Xia & Bouganis (2022). Results on ImageNetWe show in the RC curves of the various score functions on the pretrainedmodel EVA, for different combinations of subsets of test data, as summarized in . The most strikingis in (c), which collects the results for evaluation on mixup of In-D and label-shifted samples: exceptfor RLgeo-M, RLconf-M and KNN, the selection risks of other score functions do not follow a monotonicdecreasing trend as coverage decreases. As coverage approaches zero, their selection risks spike up, almost tothe risk level at full coverage (i.e., error rate on the whole set). This is because the other score functions donot indicate prediction confidence well in this setting and hence fail to sufficiently separate right and wrong 4In OOD detection, scores are usually dependent on the training data. However, these post-hoc scores can also be appliedas nontraining-based SC scores as Algorithm 1, by replacing DX and DOODXin Algorithm 2 with DcaliX,Y.5Five times the number of classes in each task from . We do not sample five points per class, as in practice thecalibration set DcaliX,Y may be imbalanced.6See in Appendix E for the model card information to retrieve these timm models.7",
  "(a) In-D (ImageNet)(b) In-D + Shift (Cov)(c) In-D + Shift (Label)(d) In-D + Shift (both)": ": RC curves of different confidence-score functions on the model EVA for ImageNet. (a)-(d) areRC curves evaluated using samples from (a) In-D samples only, (b) In-D and covariate-shifted samples only,(c) In-D and label-shifted samples only, and (d) all samples, respectively. We group the curves by whetherthey are originally proposed for SC setups (solid lines) or for OOD detection (dashed lines). : Summary of AURC- for . The AURC numbers are on the 102 scalethe lower, the better.The score functions proposed for SC are highlighted in gray, and the rest are originally for OOD detection.The best AURC numbers for each coverage level are highlighted in bold, and the 2nd and 3rd best scores areunderlined.",
  "ViM5.487.118.315.318.0510.45.837.8913.45.358.1210.7": "Results on iWildCam & AmazonWe report in and the SC performance of differentscore functions on iWildCam and Amazon. Similar to the ImageNet experiment above, scores designed forOOD detection (RLmax, Energy, KNN and ViM) do not have satisfactory performance in SC. By contrast,existing SR-based scores (SIRC, SRmax, SRent and SRdoctor) all demonstrate better SC potential thanOOD score functions, and our margin-based score functions (RLconf-M and RLgeo-M) perform on par withthe SR-based scores.",
  "(a) In-D(b) In-D + Shift (Cov)(c) In-D(d) In-D + Shift (Cov)": ": RC curves of different confidence-score functions on the model FLYP for iWildCam and the modelLISA for Amazon. (a)&(c) are RC curves evaluated using In-D samples only and (b)&(d) are RC curvesevaluated using both In-D and covariate-shifted samples. : Summary of AURC- for . The AURC numbers are on the 102 scalethe lower, the better.The score functions proposed for SC are highlighted in gray, and the rest are originally for OOD detection.The best AURC numbers for each coverage level are highlighted in bold, and the 2nd and 3rd best scores areunderlined.",
  "SRdoctor1.453.8710.11.383.6210.11.145.1312.21.886.7013.9": "RLmax29.121.424.725.524.827.91.265.2112.51.986.8814.4Energy35.228.329.936.133.234.41.265.3712.81.986.8814.4KNN6.4011.115.38.165.1010.712.114.318.216.116.520.1ViM13.410.715.76.986.4712.22.338.7215.03.5510.416.7 a faithful comparison of selection scores with a fixed classifier10. As shown above, score functions designedfor OOD detection perform poorly for generalized SC, so here we focus on comparing our margin-basedand SR-based score functions with ScNet. We first train ScNet using the training set of CIFAR-10 andImageNet, respectively; see Appendix F for training details. After training, we fix both the classificationand the selection heads and compute the scores and selection risks using the test setup shown in : (i)the ScNet selection score is taken directly from the selection head, and (ii) the margin-based and SR-basedscores are computed using the classification head. ResultsWe show in the RC curves achieved using ScNet, SR-based, and margin-based scores. Forthe CIFAR experiment shown in (a)&(b), ScNet and RLconf-M perform comparably and are better thanSRmax and SIRC, whereas for the ImageNet experiment in (c)&(d), RLconf-M, RLgeo-M, SRmax andSIRC perform comparably and are better than ScNet.11 Surprisingly, ScNet does not always lead to the best 10We do not consider training-based score functions such as Liu et al. (2019); Huang et al. (2022) due to the ambiguity incalculating their SR responses. During their training, a virtual class abstention is added and the softmax normalization isapplied on all logitsincluding that of the virtual class, so it is unfair either simply dropping the abstention logit during testfor score calculation or keeping the abstention logit but modifying the score calculation procedure. Retraining a classifier withthe same settings but without the abstention logit is also unfair due to the requirement of a fixed classifier. Furthermore, Fenget al. (2023) reports that the above selection methods (Liu et al., 2019; Huang et al., 2022) are not as effective as they claim.11Existing training-based SC works so far have only reported SC (In-D) performance on CIFAR-10 dataset and have notexperimented with ImageNet using the full training set. Our results on CIFAR-10 dataset faithfully reproduce the result originallyreported in Geifman & El-Yaniv (2019).",
  "Summary of experimental results": "From all above experiments, we can conclude that (i) existing nontraining-based score functions for OODdetection do not perform well for generalized SC, not helping achieve reliable classification performance afterrejecting low-confidence samples, and (ii) our proposed margin-based score functions RLgeo-M and RLconf-Mconsistently perform comparably to or better than existing SR-based scores on all DL models we have tested,especially in the low-risk regime, which is of particular interest for high-stakes problems. These confirm thesuperiority of RLgeo-M and RLconf-M as effective confidence-score functions for SC even under moderatedistribution shifts for risk-sensitive applications. In most of our experiments, RLgeo-M and RLconf-M perform similarly; only in rare cases, e.g. (a) and (b), RLconf-M slightly outperforms RLgeo-M. However, we do not think it is sufficient to conclude thatRLconf-M is better than RLgeo-M, or vise versa. Recall how RLconf-M and RLgeo-M are defined in Eqs. (11)and (12) and their associated decision rules, the current practice of training DL classifiers is in favor ofRLconf-M12. Thus, understanding the difference in behavior of RLgeo-M and RLconf-M is likely to also involveinvestigation of the training process, which we will leave for future work.",
  "Conclusion and discussion": "In this paper, we have proposed generalized selective classification, a new selective classification (SC) frame-work that allows distribution shifts. This is motivated by the pressing need to achieve reliable classificationfor real-world, risk-sensitive applications where data can come from the wild in deployment. GeneralizedSC covers and unifies existing selective classification and out-of-distribution (OOD) detection, and we haveproposed two margin-based score functions for generalized SC, RLgeo-M and RLconf-M, which are not basedon training: they are compatible for any given pretrained classifiers. Through our extensive analysis andexperiments, we have shown the superiority of RLgeo-M and RLconf-M over numerous recently proposed non-training-based score functions for SC and OOD detection. As the first work that touches on generalizedSC, our paper can inspire several lines of future research, including at least: (i) to further improve the SCperformance, one can try to align the training objective with our SC confidence-score functions here, i.e., 12The cross-entropy loss is the most commonly used and minimizing it can be viewed as approximating maximizing theconfidence margin. To see this, without loss of generality, assume that the magnitudes of the raw logits are ordered z1 >z2 > > zK and that the true label of the current sample is class 1. Then the cross-entropy loss for the current sample is",
  "Sara Beery, Elijah Cole, and Arvi Gjoka.The iwildcam 2020 competition dataset.arXiv preprintarXiv:2004.10340, 2020": "Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss,Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, ClemensWinter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, JackClark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Languagemodels are few-shot learners. arXiv preprint arXiv:2005.14165, 2020. Nicholas Carlini, Anish Athalye, Nicolas Papernot, Wieland Brendel, Jonas Rauber, Dimitris Tsipras, IanGoodfellow, Aleksander Madry, and Alexey Kurakin. On evaluating adversarial robustness. arXiv preprintarXiv:1902.06705, 2019.",
  "Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009": "Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive uncer-tainty estimation using deep ensembles. Advances in neural information processing systems, 30, 2017. Yann LeCun, Bernhard Boser, John Denker, Donnie Henderson, Richard Howard, Wayne Hubbard, andLawrence Jackel. Handwritten digit recognition with a back-propagation network. Advances in neuralinformation processing systems, 2, 1989.",
  "Weitang Liu, Xiaoyun Wang, John Owens, and Yixuan Li.Energy-based out-of-distribution detection.Advances in neural information processing systems, 33:2146421475, 2020": "Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie.Aconvnet for the 2020s. In Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition, pp. 1197611986, 2022. Ziyin Liu, Zhikang Wang, Paul Pu Liang, Russ R Salakhutdinov, Louis-Philippe Morency, and MasahitoUeda. Deep gamblers: Learning to abstain with portfolio theory. Advances in Neural Information Pro-cessing Systems, 32, 2019. Wesley J Maddox, Pavel Izmailov, Timur Garipov, Dmitry P Vetrov, and Andrew Gordon Wilson. A simplebaseline for bayesian uncertainty in deep learning. Advances in neural information processing systems, 32,2019.",
  "Stephan Rabanser, Stephan Gnnemann, and Zachary Lipton. Failing loudly: An empirical study of methodsfor detecting dataset shift. Advances in Neural Information Processing Systems, 32, 2019": "Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, GirishSastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models fromnatural language supervision. In International conference on machine learning, pp. 87488763. PMLR,2021. Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, AndrejKarpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge.International journal of computer vision, 115:211252, 2015.",
  "Yiyou Sun, Yifei Ming, Xiaojin Zhu, and Yixuan Li. Out-of-distribution detection with deep nearest neigh-bors. In International Conference on Machine Learning, pp. 2082720840. PMLR, 2022": "Thomas Villmann, Marika Kaden, Andrea Bohnsack, J-M Villmann, T Drogies, Sascha Saralajew, andBarbara Hammer. Self-adjusting reject options in prototype based classification. In Advances in Self-Organizing Maps and Learning Vector Quantization: Proceedings of the 11th International WorkshopWSOM 2016, Houston, Texas, USA, January 6-8, 2016, pp. 269279. Springer, 2016. Haoqi Wang, Zhizhong Li, Litong Feng, and Wayne Zhang. Vim: Out-of-distribution with virtual-logitmatching. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp.49214930, 2022.",
  "Ross Wightman. Pytorch image models. 2019": "Guoxuan Xia and Christos-Savvas Bouganis. Augmenting softmax information for selective classificationwith out-of-distribution data. In Proceedings of the Asian Conference on Computer Vision, pp. 19952012, 2022. Saining Xie, Ross Girshick, Piotr Dollr, Zhuowen Tu, and Kaiming He. Aggregated residual transforma-tions for deep neural networks. In Proceedings of the IEEE conference on computer vision and patternrecognition, pp. 14921500, 2017.",
  "ALinear SVM and margins": "We first consider binary classification. Assume training set {(xi, yi)}i[N] ([N] .= {1, . . . , N}), where yi {+1, 1} and for notational simplicity, we assume that an extra 1 has been appended to the original featurevectors so that we only need to consider the homogeneous form of the predictor: f(x) = wx. The basicidea of SVM is to maximize the worst signed geometric margin, which makes sense no matter whether thedata are separable or not:",
  "minw ws. t. yiwxi 1 i [N],(17)": "where Eq. (17) is our textbook hard-margin SVM (except for the squared norm often used in the objective).A problem with Eq. (17) is that the constraint set is infeasible for inseparable training data. To fix this issue,we can allow slight violations in the constraint and penalize these violations in the objective of Eq. (17),arriving at",
  "AUROC ()0.7650.944AUPR ()0.9870.997FPR@TPR=0.95 ()0.8160.279": "The commonly used evaluation metrics for OOD detection donot reflect the classification performance (Franc et al., 2023b).Here we provide a quantitative supporting example, in com-parison with the RC curve for generalized SC. OOD (mostly label-shift) detection as formulated in Eq. (9) canbe viewed as a binary classification problem: selected and re-jected samples form the two classes. So pioneer work on OODdetection, such as Hendrycks & Gimpel (2016), proposes toevaluate OOD detection in a manner similar to that of binaryclassification, e.g., using the Area Under the Receiver Operat-ing Characteristic (AUROC) curve (Davis & Goadrich, 2006) and Area Under the Precision-Recall curve(AUPR) (Saito & Rehmsmeier, 2015) to measure the separability of In-D and OOD samples.14 However,two important aspects are missing in OOD detection, and hence also its performance evaluation, if we areto focus on the performance on the accepted samples:",
  ". In-D samples that might have been correctly classified can be rejected due to poor separation of In-D andOOD samples, leading to worse classification performance on the selected part": "To demonstrate our points quantitatively, we take the pretrained model EVA15 from timm (Wightman, 2019)that achieves > 88% top 1 accuracy on the ImageNet validation set. We then mix ImageNet validation set(In-D samples) with ImageNet-O (OOD samples, label shifted) (Hendrycks & Dietterich, 2018), and evaluatetwo score functions s1 and s216 using both generalized SC formulation (via RC curves) and OOD detection(via AUROC and AUPR). According to , s2 is considered superior to s1 by all metrics for OOD detection. Correspondingly,from (a) and (b), we observe that the scores of the label-shifted samples (green) and those of theIn-D samples (blue and orange) are more separated by s2 than by s1. However, we can also quickly noticeone issue: In-D samples are not completely separated from OOD samplesa threshold intended to rejectlabel-shifted samples will inevitably reject a portion of In-D samples at the same time, even though a largeportion of In-D samples have been correctly classified (blue); In-D samples that can be correctly classified(blue) are less separated from those misclassified ones (orange) by s2 than by s1. This problem cannot berevealed by the OOD metrics in , but is captured by the RC curves in (c) where the selectionrisk of s2 (blue) increases as more OOD samples are rejected (TPR from 0.95 to 0.1 as indicated by the 14A single-point metric, False Positive Rate (FPR) at 0.95 True Positive Rate (TPR), is also popularly used as a compan-ion (Liang et al., 2017; Wang et al., 2022; Liu et al., 2020; Djurisic et al., 2022; Sun et al., 2022; Yang et al., 2022).15See Appendix E for model card information. This model is also used in the experiments of .16s1 is our proposed RLconf-M and s2 is ViM.",
  "(a) s1 score distributions(b) s2 score distributions(c) RC curves": ": Score distributions of s1 and s2 (a)-(b) and their RC curves (c). In (a) and (b), In-D samplesthat are correctly classified by EVA are shown in blue, while In-D samples that are incorrectly classifiedare shown in orange; OOD samples (label-shifted) are shown in green. The vertical dashed lines in (a)-(c)corresponds to different True-Positive-Rate cutoffs in the AUROC metric in OOD detection.",
  "FTraining details for ScNet": "We use the unofficial PyTorch implementation17 of the original SelectiveNet (Geifman & El-Yaniv, 2019) dueto the out-of-date Keras environment of the original repository18. The PyTorch implementation follows thetraining method proposed in Geifman & El-Yaniv (2019) and faithfully reproduces the results of CIFAR-10experiment reported in the original paper. We add the ImageNet experiment on top of the PyTorch code, asit is not included in the original code or the paper. summarizes the key hyperparameters to producethe results reported in this paper.",
  "HAblation experiments for the KNN score": "We show in the SC performance of the KNN score on models EVA, ConvNext, ResNext, and VOLO,respectively, on ImageNet with all In-D and distribution-shifted samples. We can observe that (i) the SCperformance of KNN is sensitive to the choice of hyperparameter k, and (ii) our selection k = 2 achieves thebest SC performance for KNN score on our ImageNet task.",
  "In-D (ImageNet)In-D + Shift (Cov)In-D + Shift (Label)In-D + Shift (both)": ": RC curves of different confidence-score functions on models ConvNext, ResNext and VOLO fromtimm for ImageNet. The four columns are RC curves evaluated using samples from In-D only, In-D andcovariate-shifted only, In-D and label-shifted only, and all, respectively. We group the curves by whetherthey are originally proposed for SC (solid lines) or for OOD detection (dashed lines)."
}