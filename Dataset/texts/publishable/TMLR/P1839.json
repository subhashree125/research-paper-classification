{
  "Abstract": "Multi-objective reinforcement learning (MORL) closely mirrors real-world conditions andhas consequently gained attention. However, training a MORL policy from scratch is morechallenging as it needs to balance multiple objectives according to differing preferences duringpolicy optimization. Demonstrations often embody a wealth of domain knowledge that canimprove MORL training efficiency without specific design. We propose an algorithm i.e.demonstration-guided multi-objective reinforcement learning (DG-MORL), which is the firstMORL algorithm that can use prior demonstrations to enhance training efficiency seamlessly.Our novel algorithm aligns prior demonstrations with latent preferences via corner weightsupport. We also propose a self-evolving mechanism to gradually refine the demonstrationset and avoid sub-optimal demonstration from hindering the training. DG-MORL offers auniversal framework that can be utilized for any MORL algorithm. Our empirical studiesdemonstrate DG-MORLs superiority over state-of-the-art MORL algorithms, establishingits robustness and efficacy. We also provide the sample complexity lower bound and theupper bound of Pareto regret of the algorithm.",
  "Introduction": "Single-objective reinforcement learning (SORL) has exhibited promise in various tasks, such as Atari Games(Mnih et al., 2013), robotics (Schulman et al., 2017), and smart home energy management (Lu et al., 2022a).However, most real-world challenges often encompass multiple objectives. For example, there is always a needbalance between safety and time efficiency in driving or between heating-energy usage and cost-saving forhome heating. These real-world scenarios extend SORL into multi-objective reinforcement learning (MORL)(Hayes et al., 2022). In MORL, the target is to learn an optimal policy set that maximizes the cumulativediscounted reward vector. Users can set preferences to scalarize the reward vector to pick a proper candidatefrom the optimal policy set. However, MORL introduces additional complexities and significantly amplifies the challenges already presentin SORL. The challenges include: (i) Sparse reward: In scenarios where rewards are sparse, agents struggleto acquire sufficient information for policy improvement. This challenge, widely recognized in reinforcementlearning (RL) (Oh et al., 2018; Ecoffet et al., 2019; 2021; Wang et al., 2023), can slow down the trainingprocess, heighten the risk of reward misalignment, and may result in suboptimal policies or even trainingfailure; (ii) Hard beginning: At the onset of training, policies often face substantial challenges in improving(Li, 2022; Uchendu et al., 2023). This stems from the agents lack of prior knowledge about the task. Thoughexploration strategy like -greedy can be used to gradually mitigate this it is typically inefficient in the earlystages of training; (iii) Derailment: The agent may steer off the path returning to promising areas of state",
  "Published in Transactions on Machine Learning Research (11/2024)": "Complexity and Long Horizon: The intrinsic complexity of the MO-Hopper, MO-Ant and MO-Humanoid,along with their extended horizon, might complicate the learning process. When the agent learns an effectivepolicy that surpasses some of the guide policies, it might inadvertently forget how to surpass others,particularly those aligned with different preferences. These observations indicate that while having a sufficient number of demonstrations is beneficial, there is anuanced balance to be struck. Too many demonstrations, especially in complex environments, can introducenew challenges that potentially offset the benefits of additional information. Remarkably, the DG-MORL algorithm demonstrates its robust learning capabilities by outperforming theGPI-PD baseline (except in MO-Ant, it needs more training steps to surpass GPI-PD, see .3.1.However, it can catch up the performance of GPI-PD at last.), even when provided with a limited number ofinitial demonstrations. This aspect of DG-MORL underscores its efficacy in leveraging available resources toachieve superior learning outcomes, highlighting its potential for applications where abundant demonstrationdata may not be readily available.",
  "(i) We propose the first MORL algorithm (DG-MORL) that can leverage demonstrations to improve trainingperformance": "(ii)We introduce a self-evolving mechanism to adapt and improve the quality of guiding data by graduallytransferring from prior generated demonstrations to the agents self-generated demonstrations. This mechanismsafeguards the agent against sub-optimal demonstrations. We outline the related work in . In we present the preliminary of this paper. In , we formally introduce the DG-MORL algorithm and provide theoretical analysis for our algorithm. Weillustrate the baseline algorithms, benchmark environments, experiment setting, metrics, and demonstrateand discuss the experiment results in . We discuss the limitations of our method in . Thepaper is concluded in .",
  "Multi-objective reinforcement learning": "MORL methods are categorized as single-policy methods and multi-policy methods (Yang et al., 2019),distinguished by the number of policies. In single-policy methods, MORL reduces to SORL. The agentuses the SORL learning method to maximize the cumulative reward vector scalarized by a given preference(Mannor & Shimkin, 2001; Tesauro et al., 2007; Van Moffaert et al., 2013). However, when the preference isnot given, single-policy algorithms cannot work properly. The multi-policy methods aim at learning a policy set to approximate the Pareto front. A common methodis to train a single policy multiple times upon the different preferences (Roijers et al., 2014; Mossalam et al.,2016). Pirotta et al. (2015) introduced a policy-based algorithm to learn a manifold-based optimal policy.These methods suffer from the curse of dimensionality when the preference space grows and their scalabilityis limited in complex tasks. To overcome this, policy conditioned on preference to approximate a generaloptimal policy were proposed, where Abels et al. (2019), and Yang et al. (2019) use vectorized value functionupdates. Distinguished from the other two methods, Kllstrm & Heintz (2019) use a scalarized value functionapproximation. By directing the policy training with updating the gradient according to the alignment ofvalue evaluation and preference, these methods are improved. Basaklar et al. (2023) used a cosine similaritybetween Q-values and preference to achieve an efficient update, while Alegre et al. (2023) used generalizedpolicy improvement (GPI) (Puterman, 2014) to guide the policy to update along with the preference thatcan bring the largest improvement to the policy.",
  "Prior demonstration utilization approaches": "Methods using prior demonstrations for RL usually are a combination of imitation learning (IL) and RL. Onedirect way to initialize an RL agent with demonstration is to use behavior cloning to copy the behavior patternof the prior policy (Rajeswaran et al., 2017; Lu et al., 2022a). This method cannot work well with value-basedRL frameworks (Uchendu et al., 2023). Furthermore, behavior cloning cannot work with multi-objectivesettings if the preference is not known beforehand. Vecerik et al. (2017); Nair et al. (2018); Kumar et al. (2020) have proposed methods for integrating prior datainto replay memory. These techniques have been advanced through the use of offline datasets for pre-training,followed by policy fine-tuning (Nair et al., 2020; Lu et al., 2022b). Ball et al. (2023) proposed a simple wayto use offline data. However, these methods typically require an offline dataset. In contrast, our approachoperates under less stringent assumptions, relying only on a few selected demonstrations. A Bayesian methodology offers another avenue for employing demonstration data.Ramachandran &Amir (2007) proposed the Bayesian Inverse Reinforcement Learning (BIRL) method, modeling the InverseReinforcement Learning (IRL) problem as a Bayesian inference process. By combining prior knowledgewith evidence from expert behaviors, they can effectively infer the posterior distribution of the rewardfunction, improving the efficiency and accuracy of policy learning. Choi & Kim (2013) proposed a Bayesiannon-parametric approach to construct reward function features in Inverse Reinforcement Learning (IRL).Their method uses the Indian Buffet Process (IBP). It integrates domain expert prior knowledge by havingexperts provide atomic features representing key factors influencing rewards and enabling more accuratereward reconstruction. Choi & Kim (2012) proposed a non-parametric Bayesian IRL method using theDirichlet Process Mixture Model (DPMM) to infer an unknown number of reward functions from unlabelledbehavior data. This approach can automatically determine the appropriate set of reward functions basedon the data, without predefining the number. It is particularly effective in handling expert demonstrationswith different goals. They also developed an efficient Metropolis-Hastings sampling algorithm leveragingposterior gradients to estimate the reward functions. However, these works are more from IRL and are notspecifically designed for MORL. This is different from our method which uses demonstrations to guide thepolicy training but not to infer the reward function in a MORL setting. Furthermore, the absence of extension and empirical validation of these methods in MORL settings hindersdirect comparisons to our method. Additionally, multi-objective imitation learning remains under-explored,",
  "Effective exploration approaches": "Several approaches have been introduced to improve the exploration efficiency, including reward shaping (Nget al., 1999; Mannion, 2017), curiosity-driven exploration (Barto et al., 2004), and model-based reinforcementlearning (MBRL) (Polydoros & Nalpantidis, 2017; Alegre et al., 2023). However, they still have notable shortcomings. For example, the reliance on domain expertise for rewardshaping (Devidze et al., 2022); The inherent vulnerability trapped in local optima in curiosity-drivenexploration (Ecoffet et al., 2021); The computation overload and inaccuracy of environment model constructionfor MBRL. It is necessary to effectively explore the environment so that the model is closer to the groundtruth of the environment dynamics. However, this has an additional computational cost when the environmentis hard to explore, the environment model may not be accurate enough and this will impact the trainingadversely. Using demonstration has the following advantages to mitigate these drawbacks. It allows for the integrationof domain knowledge without an explicit reward-shaping function. This can alleviate the reliance on domainexpertise; Guided by a demonstration, the agent can return to promising areas that it might have struggled torevisit before the curiosity intrinsic signal is used up; It does not require an environment model and thereforefrees from computational demands and model uncertainties.",
  "Preliminaries": "We formalise a multi-objective Markov decision process (MOMDP) as M := (S, A, T , , , R) (Hayes et al.,2022), where S and A are state and action space; T : S A S is a probabilistic transition function; [0, 1) is a discount factor; : S0 is the initial state probability distribution; R : S A S Rd",
  "t=0 trt|s, a], whereq(s, a) is a d-dimensional vector denoting the expected vectorized return by following policy , rt is thereward vector on t step": "The multi-objective value vector of policy with the initial state distribution is: v := Es0[q(s0, (s0)],where v is the value vector of policy , and the i-th element of v is the returned value of the i-th objectiveby following policy . The Pareto front (PF) consists of a set of nondominated value vectors. The Paretodominance relation (p) is: v p v (i : vi vi )(i : vi > vi ). We say that v is nondominatedwhen all element in v are not worse than any v that at least one element of v is greater than the otherv. The Pareto front is defined as PF := {v| s.t. v p v} To select an optimal policy from the PF, the criteria are usually given by the user and therefore termedas user-defined preference. A utility function is used (Hayes et al., 2022) to map the value vector to ascalar. One of the most frequently used utility functions is the linear utility function (Hayes et al., 2022), i.e.u(v, w) = vw = v w, where w is on a simplex W : di wi = 1, wi 0. The Convex Coverage Set (CCS) is a finite convex subset of the PF (Hayes et al., 2022). CCS := {v PF|w s.t. v PF, v w v w}. When a linear utility function is used, the CCS comprises allnon-dominated values, making the PF and CCS equivalent. In the cutting-edge MORL methods (Yang et al., 2019; Alegre et al., 2023), is typically trained throughlearning a vectorized Q function that can adapt to any preference weight vector. These methods still sufferfrom poor sample efficiency as they search for the CCS by starting from the original point in solution space.See (a), traditional MORL method starts from the red point, and searches to expand to the orangeCCS. Ideally, by giving demonstrations, the training process can be transformed into (b). Thepolicy is informed by domain knowledge, allowing it to start from the red points (demonstrations). Thisshift significantly reduces the distance between the known solution and the CCS, thereby reducing sample",
  "Demonstrations corner weight computation": "When addressing the issue of Demonstration-Preference Misalignment, it is noteworthy that a single demon-stration can be optimal under an infinite number of preference weights as the weight simplex is continuous.Sampling and iterating through the entire simplex of preferences is computationally expensive and impractical.To mitigate this challenge, we utilize the concept of corner weights (Roijers, 2016) i.e. denoted as Wcorner, todiscretize the continuous simplex into several critical points where the policy undergoes essential changes. We now give a clear explanation of the corner weights. When using different weights to scalarize the returnof the objectives, we get different scalar values. If we plot these scalar values against the weights, we geta piecewise linear convex curve or surface. Corner weights are specific combinations of weights where theslope of this curve changes. In simpler terms, they are the corners or edges on the graph of the scalarizedvalue function. As shown in , the points depicted by the blue lines are the corner weights accordingto the curve, the y-axis is the return value while the x axis is the weight of objective 1 (assuming this is a2-objective problem). These points are significant because they represent weights where the optimal strategyor policy change. According to Roijers (2016), that the maximum possible improvement to the set of knownreturns occurs at one of the corner weights. This means that if were going to find a better policy, its mostlikely to happen when we look at these specific weights. See a more rigorous definition of the corner weightsin Definition 1.",
  "iwi = 1, wi 0, i}(1)": "where V + represents a matrix in which the elements of the set V serve as the row vectors augmented by -1sas the column vector. The element x=(w1 ,..., wd , vw) in P consists of the element of the weight vector andits scalar return value. Corner weights refer to the weights located at the vertices of P. Byiteratingconductthedemonstrations,wecangetasetofvectorizedreturnsVd.Wedenote the convex coverage set of the demonstration results as CCSd.The corner weightseffectivelypartitionthesolutionspaceandclassifythepolicies.Thechoiceoftheopti-mal policy maximizing utility for a given weight varies when traversing across corner weights.By computing Wcorneron CCSd,we can assign corresponding weights to the demonstrations. : Corner Weights Showcase: The points de-picted by the blue lines are the corner weights accordingto the curve, the y-axis is the return value while thex axis is the weight of objective 1 (assuming this is a2-objective problem). Employing corner weights efficiently reduces thecomputational burden by calculating on a finite setWcorner rather than on an infinite weight simplex W.Additionally, the significance of using corner weightsin training is emphasized by the theorem:",
  "(w) = uCCSw max uw(2)": "where (w) is the difference between the utility ofoptimal policy given weight w and the utility ob-tained by a policy , where denotes thecurrent agents policy set.Theorem 1 suggeststhat training along a corner weight allows for fasterlearning of the optimal policy (set), as it offersthe potential for achieving maximum improvement.Assuming the prior demonstration is the knownoptimal solution, by replacing the CCS in equa-tion 2 with CCSd, Equation 2 can be rewritten as:d(w) = uCCSdw maxuw. wc =arg maxwWcorner(w). It further exploits the theorem to select the cornerweight wc from Wcorner that denotes the largest gapbetween the performance of the current policy andthe demonstration to increase the training efficiency. We have elucidated the calculation of the cornerweights set and the selection of an appropriate candi-date corner weight for aligning the demonstration. With this approach, the issue of demonstration-preferencemisalignment is addressed.",
  "Self-evolving mechanism": "An inaccurate assumption exists that the use of demonstrations should naturally lead to performanceimprovements. It is too optimistic as sub-optimal demonstrations can easily impact the training process andcause low performance. We propose a self-evolving mechanism to tackle the problem of sub-optimal initialdemonstrations. It is designed to improve upon given demonstrations for better guidance. This is anothermain contribution of our work. In the following sections, the demonstrations are referred to as guide policies.Although a demonstration generally does not correspond to a policy, it can be considered a fixed, hard-codedpolicy that merely executes actions regardless of the state observation. The policies developed by the agentare called exploration policies.",
  "Multi-stage curriculum": "To make full use of the demonstration, the mixed policy is mostly controlled by the guide policy g atthe beginning of training. This can navigate the agent to a more promising state than randomly exploring.The control of the guide policy is diluted during training and taken over by the exploration policy. However,swapping the guide policy with the exploration policy introduces the challenge of the policy shift. A multi-stagecurriculum pattern, as suggested by Uchendu et al. (2023), is implemented. This strategy involves graduallyincreasing the number of steps governed by the exploration policy, facilitating a smoother transition andmitigating the policy shift issue.",
  "Algorithm": "Compared with JSRL Uchendu et al. (2023), DG-MORL introduces the integration of the corner weightsupport to align the provided demonstration with proper preferences; The self-evolving mechanism of DG-MORL substantially reduces the need for numerous demonstrations to attain strong performance1. Anotherdeparture is that DG-MORL entails a controlling swap between the set of guide policies to one explorationpolicy. Given a preference weight vector w, a guide policy upon which the exploration may gain the largestimprovement is selected as shown in Equation 3.",
  "g = arg maxg(w) = arg maxg(uw uew )(3)": "We provide a detailed explanation of the DG-MORL in Algorithm 1. To initiate the process, a set of initialdemonstrations is evaluated, and the return values are added to CCSd. Dominated values are subsequentlydiscarded, and assume that these retained values represent optimal solutions. The corner weights are thencomputed based on these values2. Next, a suitable guide policy is selected by using Equation 3. The episode horizon is denoted as H. The guidepolicy control scale is denoted as h, i.e. for the first h steps, the guide policy controls the agent, while for thenext H h steps, the exploration policy is performed. The exploration policys performance is periodicallyevaluated. When improved performance is demonstrated, the parameter h is reduced to grant the explorationpolicy more controllable steps. The superior demonstration is then added to the demonstration repository,",
  "Lower bound discussion": "When the exploration policy is 0-initialized (all Q-values are zero initially), a non-optimism-based method, e.g.-greedy, suffers from exponential sample complexity when the horizon expands. We extend the combinationlock (Koenig & Simmons, 1993; Uchendu et al., 2023) under the multi-objective setting (2 objectives), andvisualize an example in . In the multi-objective combination lock example, the agent encounters three distinct scenarios where it canearn a non-zero reward vector, each scenario is determined by a specific preference weight vector. The threepreference weight vectors are , [0.5, 0.5], and . These vectors correspond to the agents varyingpreferences towards two objectives: When the agent demonstrates an extreme preference for objective 1, itmay receive a reward vector of . Conversely, if the agent shows an extreme preference for objective 2, itcould be awarded a reward vector of .",
  ": Lower bound example: MO combination lock": "In the scenario where the agents preference is evenly split between the two objectives, as indicated by apreference vector of [0.5, 0.5], theres a possibility for it to obtain a reward vector of [0.5, 0.5]. This outcomereflects a balanced interest in both objectives. In this multi-objective combination lock scenario, the outcomesfor the agent vary significantly based on its preference weights and corresponding actions. When the preferenceweights are set to , the agents goal is to reach the red state so1:h. To achieve this, it must consistentlyperform the action ao1 from the beginning of the episode, which enables it to remain in the red state andsubsequently receive a reward feedback of . Similarly, with preference weights of , the agent needsto execute the action ao2 right from the start to eventually reach a reward vector of . For a balancedpreference of [0.5, 0.5], the agent is expected to take a specific action, abalance, at step H 1 to transition tothe state sbalance:h and receive a balanced reward of [0.5, 0.5]. This time point is referred to as the mutationpoint. If the agent derails from these prescribed trajectories, it only receives a reward of . Moreover,any improper action leads the agent to a blue state, from which it cannot return to the correct trajectory(derailment). As the agent is 0-initialized, it needs to select an action from a uniform distribution. The preference given isalso sampled from a uniform distribution. Then if the agent wants to know the optimal result for any of thethree preferences, it needs at least 3H1 samples. If it thoroughly knows the optimal solution of one of thethree preferences, it can achieve an expected utility of 0.3. There exist several (K) paths in this MOMDPinstance delivering good rewards. To find all good policies, the agent needs at least to see K 3H1 samples.We formalize this in Theorem 2. The scenario where preferences precisely match the three specific weights of , [0.5, 0.5], and isconsiderably rare, as preferences are typically drawn from an infinite weight simplex. In practical trainingsituations, encountering a variety of preferences can lead to conflicting scenarios for the agent. This conflictarises because different preferences may necessitate different actions or strategies, potentially causing theagents policy to experience forgetfulness or an inability to consistently apply learned behaviors acrossvarying preferences. Such a situation complicates the training process and increases sample complexity. Theorem 2. (Koenig & Simmons, 1993; Uchendu et al., 2023) When using a 0-initialized exploration policy,there exists a MOMDP instance where the sample complexity is exponential to the horizon to see a partiallyoptimal solution and a multiplicative exponential complexity to see an optimal solution.",
  "Upper bound of DG-MORL": "We now give the upper bound of DG-MORLs regret. We start by making assumptions about the qualityof the guide policy, i.e., it can cover some of the features that can be visited under the optimal policy, andthe coverage rate is restricted by a concentrate ability coefficient. Then we give a Lemma to show that",
  "dgh,w((s)) C(4)": "dh,w((s)) and dgh,w((s)) represents the probability distribution of feature (s) being visited at time steph under the preference weight w, while following the optimal policy or guide policy g respectively. Cis the concentratability coefficient (Rashidinejad et al., 2021; Guo et al., 2023). It quantifies the extent towhich the guide policy g covers the optimal policy in the feature space and restricts the probability ofthe optimal policy visiting a certain feature (s) to not exceed C times the probability of the guide policyvisiting that feature. Nonetheless, Assumption 1 is formulated within contextual bandit which does not sufficiently capture thecomplexities of sequential decision-making processes. We extend Assumption 1 to the context of sequentialdecision-making and incorporates a self-evolving mechanism, thereby establishing Lemma 3. As a result ofthe self-evolving mechanism, the guide policy g progressively enhances its performance over time. Lemma 3. For a MOMDP, the sequence concentration coefficient Cseq(t) under preference weight w is definedas the maximum value of the concentration coefficient across all time steps h while with the self-evolvingmechanism:",
  "dgh ((s)) CP areto(t) CP areto(t0)(6)": "We now give a few key definitions to calculate the upper bound of regret. One critical concept is the Paretosuboptimality gap. This term refers to the measure of distance between the current policys performance on aspecific arm of the multi-objective contextual bandit (MOCB) and the true Pareto optimal solution for thatarm. Definition 2. (Pareto suboptimality gap (Lu et al., 2019)) Let x be an arm in X, i.e. the arm set of theMOCB. The Pareto suboptimality gap x is defined as the minimal scalar 0 so that x becomes Paretooptimal by adding to all entries of its expected reward.",
  "We can now give the upper bound of the sum of the sequence Pareto regret during the training for T rounds": "Theorem 5. (Performance Guarantee) Assuming that the environment satisfies the Markov property, andthere exists a feature mapping function such that for any policy both Q(s, a) and (s) depend only on(s). The DG-MORL algorithm guarantees that the guiding policy g progressively approaches the optimalpolicy through self-evolution mechanisms. f(t) is an abstract regret function, it depends on the specificalgorithm we used to train e. During each training round t [T], the algorithm executes policy t, and thesum of sequence Pareto regret PRseq(T) is bounded by:",
  "t=1tseq THRmaxC(10)": "where C = CP areto(t0) + f(t0), CP areto(t0) and f(t0) are the Pareto concentration coefficient and regretfunction at the first round of training. This is to get a conservative upper bound as CP areto(t0) CP areto(t)because of the self-evolving mechanism and f(t0) f(t) because of the policy improvement.Rmax =maxs,a r(s, a)1, we use this 1-norm to get a relative conservative and general upper bound4 4While we acknowledge that using preference weight might add specificity, we prefer to use the 1-norm since it relaxes theupper bound. This approach ensures that our theorem remains valid even in the worst-case scenario.",
  "h=hg+1e,tsh": "Thus, the total regret is :tseq = g,tseq + e,tseqWith the help of the self-evolving mechanism, g can improve overtime. According to Lemma 3, at h timestep,the expected regret of g,t is:Esh[g,tsh ] Rmax CP areto(t) In the worst case, the reward difference is Rmax, and the difference in state distribution is controlled byCP areto(t). In the worst case, each time step can cause the maximized regret. The total expected regret forg,t is:g,th h Rmax CP areto(t)",
  "t=1(ht Rmax CP areto(t) + (H ht) f(t) Rmax)": "where ht is the number of time steps controlled by the guided policy during the t-th training round. Duringthe training process, the number of time steps controlled by the guided policy decreases over time, i.e., htdecreases with t. Since CP areto(t) and f(t) decreases with t, we approximate it by CP areto(t0) and f(t0),the value at the final round. We approximate that the steps controlled by g and e are all the maximizednumber of steps, i.e. H, the upper bound on the total regret is:",
  "Experiments": "In this section, we introduce the baselines, benchmark environments and metrics. We then illustrate anddiscuss the results. We have also provided the sensitivity analysis about the how different qualities andquantities of the initial demonstrations can influence the training process in the Appendix. Please note that,our DG-MORL does not require specific preferences to be covered by demonstrations. In MORL, as somepreferences may share the same optimal policy, if the true PF is not known, it is impossible to cover allimportant preferences by demonstrations.",
  "We use two state-of-the-art MORL algorithms as baselines: GPI linear support (GPI-LS) and GPI prioritizedDyna (GPI-PD) (Alegre et al., 2023). GPI-PD algorithm is model-based and GPI-LS is model-free": "The GPI-LS and GPI-PD are based on generalized policy improvement (GPI) (Puterman, 2014) whilethe GPI-PD algorithm is model-based and GPI-LS is model-free. The GPI method combines a set ofpolicies as an assembled policy where an overall improvement is implemented (Barreto et al., 2017; 2020).It was introduced to the MORL domain by Alegre et al. (2022). The GPI policy in MORL is defined asGP I(s|w) arg maxaAmaxqw(s, a). The GPI policy is at least as well as any policy in (policy set). The GPI agent can identify the most potential preference to follow at each moment rather than randomly pick apreference as in Envelope MOQ-learning. This facilitates faster policy improvement in MORL. Furthermore,the model-based GPI-PD can find out which piece of experience is the most relevant to the particularcandidate preference to achieve even faster learning. The two baseline algorithms GPI-LS and GPI-PD cannotutilize demonstrations. There is not yet a MORL algorithm that can use demonstrations in training andDG-MORL is the first one. As indicated in the literature (Alegre et al., 2023), the Envelope MOQ-learning algorithm (Yang et al., 2019),SFOLS (Alegre et al., 2022), and PGMORL (Wurman et al., 2022) are consistently outperformed by theGPI-PD/LS. Therefore, if our algorithm can surpass GPI-PD/LS, it can be inferred that it also exceeds theperformance of these algorithms. We also use a no-self-evolving version of DG-MORL in the ablation studyas a baseline.",
  "Implementation detail": "The code of this work is available on We use the sameneural network architecture as the GPI-PD implementation in all benchmarks, i.e. 4 layers of 256 neuronsin DST and Minecart, 2 layers with 256 neurons for both the critic and actor in MO-Hopper, MO-Antand MO-Humanoid. We use Adam optimizer, the learning rate is 3 104 and the batch size is 128 in allimplementations. As for the exploration, we adopted the same annealing pattern -greedy strategy. In DST,the anneals from 1 to 0 during the first 50000 time steps. In Minecart, the is annealed from 1 to 0.05in the first 50000 time steps. For the TD3 algorithm doing MO-Hopper, MO-Ant and MO-Humanoid, wetake a zero-mean Gaussian noise with a standard deviation of 0.02 to actions from the actor-network. Allhyper-parameters are consistent with literature (Alegre et al., 2023). The initial demonstration data are from different sources to validate the fact that any form of demonstrationcan be used as it can be converted to action sequences. The demonstration for DST is from hard-codedaction sequences which are sub-optimal. Minecart uses the demonstrations manually played by the author,i.e. us, while the MO-Hopper, MO-Ant and MO-Humanoid, use the demonstration from a underdevelopedpolicy from the TD3 algorithm.",
  "Benchmark environments and metrics": "We conduct the evaluation on MORL tasks with escalating complexity: from an instance featuring discretestates and actions, i.e. Deep Sea Treasure (DST) (Yang et al., 2019; Alegre et al., 2023) to tasks withcontinuous states and discrete actions, i.e. Minecart (Abels et al., 2019; Alegre et al., 2023). We also test itin control tasks with both continuous states and actions, i.e. MO-Hopper (Basaklar et al., 2023; Alegre et al.,2023), MO-Ant, and MO-Humanoid. We assess the candidate methods using a variety of metrics sourcedfrom Hayes et al. (2022).",
  "Benchmark environment setting": "The deep sea treasure (DST) environment is a commonly used benchmark for the MORL setting (Vamplewet al., 2011; Abels et al., 2019; Yang et al., 2019; Alegre et al., 2023), see (a). The agent needs tomake a trade-off between the treasure value collected and the time penalty. It is featured by a discrete statespace and action space. The state is the coordinate of the agent, i.e. [x,y]. The agent can move either left,",
  ": (a) Deep sea treasure; (b) Minecart; (c) MO-Hopper; (d) MO-Ant; (e) MO-Humanoid": "right, up, down in the environment. The reward vector is defined as r = [rtreasure, rstep], where rtreasure isthe value of the treasure collected, and rstep = 1 is a time penalty per step. The treasure values are adaptedto keep aligned with recent literature (Yang et al., 2019; Alegre et al., 2023). The discounted factor = 0.99. Minecart. The Minecart environment (Abels et al., 2019; Alegre et al., 2023), see (b), is featuredwith continuous state space and discrete action space. It provides a relatively sparse reward signal. Theagent needs to balance among three objectives, i.e. 1) to collect ore 1; 2) to collect ore 2; 3) to save fuel.The minecart agent kicks off from the left upper corner, i.e. the base, and goes to any of the five minesconsisting of two different kinds of ores and does collection, then it returns to the base to sell it and getsdelayed rewards according to the volume of the two kinds of ores. This environment has a 7-dimension continuous state space, including the minecarts coordinates, the speed,its angle cosine value and sine value, and the current collection of the two kinds of ores (normalized by thecapacity of the minecart). The agent can select actions from the 6-dimension set, i.e. mine, turn left, turnright, accelerate, brake, idle. The reward signal is a 3-dimensional vector consisting of the amount of the twokinds of ore sold and the amount of fuel it has used during the episode. The reward for selling ores is sparseand delayed while the reward signal of fuel consumption is a constant penalty plus extra consumption whendoing acceleration or mining. MO-Hopper, MO-Ant and MO-Humanoid are extensions of the Mujoco continuous control tasks from OpenAIGym (Borsa et al., 2019). They are used in literature to evaluate MORL algorithm performance in robotcontrol (Basaklar et al., 2023; Alegre et al., 2023). MO-Hopper. The agent controls the three torque of a single-legged robot to learn to hop. This environmentinvolves two objectives, i.e. maximizing the forward speed and maximizing the jumping height. It has acontinuous 11-dimensional state space denoting the positional status of the hoppers body components. Thisenvironment has a 3-dimensional continuous action space, i.e. the different torques applied on the three joints.We use the same reward function and discounted factor = 0.99 (Alegre et al., 2023).",
  "i a2i is the afeedback of the agent healthiness": "MO-Ant. The ant robot is a 3D model comprising a freely rotating torso with four legs attached. Each legconsists of two segments, making a total of eight body parts for the legs. The objective is to synchronize themovement of all four legs to propel the robot forward (toward the right) by exerting torques on the eighthinges that connect each legs two segments to the torso. This configuration results in nine body parts andeight hinges in total that need to be coordinated for movement. This environment has an 8-dimensionalcontinuous action space and 27-dimensional continuous state space (Schulman et al., 2015). We use thediscounted factor = 0.99.rx = vx + C(13)",
  "where vx and vy is the x-velocity and the y-velocity, h and hinit are the hoppers relative jumping height, C isthe a feedback of the agent healthiness": "MO-Humanoid. The 3D bipedal robot is engineered to mimic human movement, featuring a torso (abdomen)equipped with two legs and two arms. Each leg is composed of three segments, simulating the upper leg,lower leg, and foot, while each arm consists of two segments, analogous to the upper and lower arms. Theprimary objective of this simulation environment is to enable the robot to walk forward as swiftly as possiblewhile maintaining balance and avoiding falls. This environment has a 17-dimensional continuous action spaceand 376-dimensional continuous state space (Tassa et al., 2012). We use the discounted factor = 0.99.",
  "where vx is the forward speed, control is the sum control cost of the joints C is the a feedback of the agenthealthiness": "To more effectively compare the exploration efficiency of our algorithm with baseline methods, we imposeconstraints on episode lengths across different environments. Specifically, we set the DST environment toterminate after 100 steps, the Minecart environment to conclude after 30 steps, and the MO-Hopper, MO-Antand MO-Humanoid to end after 500 steps. This limitation intensifies the difficulty of the tasks, compellingthe agent to extract sufficient information from a restricted number of interactions. This constraint ensuresa more rigorous assessment of each algorithms ability to efficiently navigate and learn within these morechallenging conditions. The summary of benchmark settings is illustrated in .",
  "To thoroughly compare our algorithms with baselines, we use multiple metrics (Hayes et al., 2022)": "Expected utility (EU) (Zintgraf et al., 2015): Compared to many other metrics, such as the hypervolumemetric, EU is better suited for comparing different algorithms because they are specifically designed toevaluate an agents ability to maximize user utility, which is consistently our primary objective. It is calculatedas EU() = EwW[max vw]. The expectation in this work is the average over 100 evenly distributedweights from W. Hypervolume (Zitzler & Thiele, 1999): The Hypervolume metric quantifies the volume in the value-space thatis Pareto-dominated by a set of policies within an approximate coverage set. We use reference point inall benchmark environments except in Minecart.",
  "Result": "In this section, we show the results of the evaluations of EU, PF, Hypervolume, Sparsity, and the ratio ofHypervolume and Sparsity. We conduct 5 independent runs with randomly picked seeds. For the initialnumber of demonstrations, we used 10 for DST, 7 for Minecart, and 5 for MO-Hopper, MO-Ant, andMO-Humanoid. We have conducted a sensitivity study of different initial demonstration quantities is inAppendix A.1. The sensitivity study of qualities of the guidance policy is in Appendix A.2. The experimentsare run on a machine with 12th Gen Intel(R) Core(TM) i9-12900 CPU, NVIDIA T1000 graphic card and 32GB memory. For the number of initial demonstrations, we use 10 demonstrations for DST, 7 demonstrationsfor Minecart, and 5 demonstrations for MO-Hopper, MO-Ant and MO-Humanoid.",
  "Main experiment result": "The evaluation of the EU of the training process is shown in (the evaluation frequency is shown in thetitle of each diagram). Demonstrations from different sources are used to train DG-MORL, i.e. hard-codedaction sequences in DST and Minecart, and the underdeveloped policy of a TD3 agent in MO-Hopper,MO-Ant, and MO-Humanoid. This showcases our hypothesis that DG-MORL can work with demonstrationsfrom any source. All evaluations of DG-MORL are solely with its exploration policy. To statistically andreliably evaluate the results of EU, we use stratified bootstrap confidence intervals (with 50000 runs) andinterquartile mean (IQM) (Agarwal et al., 2021), i.e. IQMs are depicted with solid lines, while 95% confidenceintervals are illustrated by color-matched shaded areas. In , the result shows that DG-MORL has learned a better or at least competitive policy than thebaselines in the first evaluation in most benchmarks. An exception occurs in MO-Ant, where DG-MORLstarts with an EU approaching 0. This may be due to the more conflicting objectives in MO-Ant, whereinitial demonstrations display contradictory behaviors, e.g. exclusively moving along the x-axis or y-axis.Starting from a demonstration committed to a single direction can impede progress in improving the policygoing in the alternative direction. Conversely, DG-MORL in MO-Humanoid exhibits a notably strongstart, attributed to its objectives of forward movement and control cost reduction, which approximatelyalign as a single goal of advancing with minimal control energy (similar to MO-Hopper). This alignmentresults in less contradictory demonstrations than MO-Ant, as further evidenced by the sparse points on thePF illustrated in . This phenomenon indicates that the degree of conflict among objectives canbe a potential factor influencing the training process of DG-MORL. However, after training DG-MORLachieves better performance than all of the baselines, surpassing the initial demonstrations and reaching thestandard of the best achievable performance5. In comparison, GPI-PD also attains a commendable level ofperformance in the early stages, but during the whole training process, it shows larger variations and inferior 5For DST and Minecart, where a known PF exists, the black dashed line indicating the best performance represents theresults from the actual PF. As MO-Hopper, MO-Ant, and MO-Humanoid do not have known PFs, we trained 5 single-objectiveTD3 (SO-TD3) agents with preferences , [0.75, 0.25], [0.5, 0.5], [0.25, 0.75], . Each agent is trained for 1.5M steps,therefore the training budget is 7.5M steps. We use the result as the best performance (Note that these best performances canbe exceeded as they only from policy trained with significantly more steps). Our algorithm is trained by 0.225M steps, whichtakes only 3% training budget compared to SO-TD3.",
  "Ablation study": "We conduct an ablation study on DG-MORL by excluding the self-evolving mechanism. The agent reliessolely on the initial demonstrations, as opposed to progressively enhancing the guidance demonstration withself-generated data. The result is shown in yellow in , , and DG-MORL (no self-evolving)in . The absence of this mechanism makes DG-MORL fail to reach the performance levels of thefully equipped DG-MORL setup (except in DST), however, it still outperform the baselines and the initialdemonstrations in most benchmarks (except the MO-Humanoid, the instance without self-evolving saw a dropin the middle.). The findings from the ablation study confirm the following: 1. The self-evolving mechanismis effective. 2. Generally, using demonstrations to guide MORL training leads to improved performance.",
  "Limitation": "DG-MORL has pioneered the use of demonstrations to inform MORL training. However, our method hassome limitations. One limitation of this work is that DG-MORL is only applicable to linear preference weights.It needs further improvement to work with non-linear preferences. Another limitation is that DG-MORL hasnot been evaluated in a non-stationary environment. It is worthwhile to explore how a demonstration canguide the training process in non-stationary MORL settings. We will leave these to the future. Additionally,since DG-MORL in the MO-Ant scenario does not achieve jump-start learning, exploring how to effectivelyuse demonstrations to aid training in environments with relatively contradictory objectives is worth furtherinvestigation.",
  "Conclusion": "We proposed the pioneering DG-MORL algorithm that uses demonstrations to enhance MORL trainingefficiency. Our algorithm has overcome the challenge of sparse reward, the hard beginning of training, andthe possible derailment from a complex environment. We address the challenges of demonstration-preferencemisalignment and demonstration deadlock by implementing corner weight computation of the demonstrations.We introduced the self-evolving mechanism to handle sub-optimal demonstrations or the case with limitednumber of demonstrations. Empirical evidence confirms DG-MORLs superiority over state-of-the-art acrossvarious environments. Additionally, our experiments highlight DG-MORLs robustness by excluding self-evolving and under other unfavoured conditions. We also provided theoretical insights into the lower boundand upper bound of DG-MORLs sample efficiency. This work was financially supported by the Irish Research Council under Project ID: GOIPG/2022/2140.The authors have applied a CC BY 4.0 public copyright license to the Author Accepted Manuscript versionarising from this submission to ensure Open Access. We would also like to express our sincere appreciation tothe reviewers for their insightful and constructive feedback, which has played a crucial role in refining ourresearch and strengthening the final manuscript. Axel Abels, Diederik Roijers, Tom Lenaerts, Ann Now, and Denis Steckelmacher. Dynamic weights inmulti-objective deep reinforcement learning. In International conference on machine learning, pp. 1120.PMLR, 2019. Rishabh Agarwal, Max Schwarzer, Pablo Samuel Castro, Aaron C Courville, and Marc Bellemare. Deepreinforcement learning at the edge of the statistical precipice. Advances in neural information processingsystems, 34:2930429320, 2021. Lucas N Alegre, Ana Bazzan, and Bruno C Da Silva. Optimistic linear support and successor features as abasis for optimal policy transfer. In International Conference on Machine Learning, pp. 394413. PMLR,2022. Lucas N Alegre, Ana LC Bazzan, Diederik M Roijers, Ann Now, and Bruno C da Silva. Sample-efficientmulti-objective learning via generalized policy improvement prioritization. In Proceedings of the 2023International Conference on Autonomous Agents and Multiagent Systems, pp. 20032012, 2023. Philip J Ball, Laura Smith, Ilya Kostrikov, and Sergey Levine. Efficient online reinforcement learning withoffline data. In Proceedings of the 40th International Conference on Machine Learning, pp. 15771594,2023. Andr Barreto, Will Dabney, Rmi Munos, Jonathan J Hunt, Tom Schaul, Hado P van Hasselt, and DavidSilver. Successor features for transfer in reinforcement learning. Advances in neural information processingsystems, 30, 2017.",
  "Adrien Ecoffet, Joost Huizinga, Joel Lehman, Kenneth O Stanley, and Jeff Clune. First return, then explore.Nature, 590(7847):580586, 2021": "Siyuan Guo, Lixin Zou, Hechang Chen, Bohao Qu, Haotian Chi, S Yu Philip, and Yi Chang. Sample efficientoffline-to-online reinforcement learning. IEEE Transactions on Knowledge and Data Engineering, 2023. Conor F Hayes, Roxana Rdulescu, Eugenio Bargiacchi, Johan Kllstrm, Matthew Macfarlane, MathieuReymond, Timothy Verstraeten, Luisa M Zintgraf, Richard Dazeley, Fredrik Heintz, et al. A practical guideto multi-objective reinforcement learning and planning. Autonomous Agents and Multi-Agent Systems, 36(1):26, 2022. Johan Kllstrm and Fredrik Heintz. Tunable dynamics in agent-based simulation using multi-objectivereinforcement learning. In Adaptive and Learning Agents Workshop (ALA-19) at AAMAS, Montreal,Canada, May 13-14, 2019, pp. 17, 2019.",
  "Peter Vamplew, Richard Dazeley, Adam Berry, Rustam Issabekov, and Evan Dekker. Empirical evaluationmethods for multiobjective reinforcement learning algorithms. Machine learning, 84:5180, 2011": "Kristof Van Moffaert, Madalina M Drugan, and Ann Now. Scalarized multi-objective reinforcement learning:Novel design techniques. In 2013 IEEE symposium on adaptive dynamic programming and reinforcementlearning (ADPRL), pp. 191199. IEEE, 2013. Mel Vecerik, Todd Hester, Jonathan Scholz, Fumin Wang, Olivier Pietquin, Bilal Piot, Nicolas Heess, ThomasRothrl, Thomas Lampe, and Martin Riedmiller. Leveraging demonstrations for deep reinforcementlearning on robotics problems with sparse rewards. arXiv preprint arXiv:1707.08817, 2017.",
  "Guojian Wang, Faguo Wu, Xiao Zhang, Jianxiang Liu, et al. Learning diverse policies with soft self-generatedguidance. International Journal of Intelligent Systems, 2023, 2023": "Peter R Wurman, Samuel Barrett, Kenta Kawamoto, James MacGlashan, Kaushik Subramanian, Thomas JWalsh, Roberto Capobianco, Alisa Devlic, Franziska Eckert, Florian Fuchs, et al. Outracing championgran turismo drivers with deep reinforcement learning. Nature, 602(7896):223228, 2022. Jie Xu, Yunsheng Tian, Pingchuan Ma, Daniela Rus, Shinjiro Sueda, and Wojciech Matusik. Prediction-guidedmulti-objective reinforcement learning for continuous robot control. In International conference on machinelearning, pp. 1060710616. PMLR, 2020. Runzhe Yang, Xingyuan Sun, and Karthik Narasimhan. A generalized algorithm for multi-objective re-inforcement learning and policy adaptation. Advances in neural information processing systems, 32,2019. Luisa M Zintgraf, Timon V Kanters, Diederik M Roijers, Frans Oliehoek, and Philipp Beau. Qualityassessment of morl algorithms: A utility-based approach. In Benelearn 2015: proceedings of the 24th annualmachine learning conference of Belgium and the Netherlands, 2015.",
  "ASensitivity study": "It is widely acknowledged in machine learning that both the quantity and quality of data play crucial rolesin enhancing the outcomes. To understand the extent and manner in which each of these factors impactsthe learning process, we have carried out a series of sensitivity studies. We continue to utilize the same fivebenchmark environments and the EU metric. However, the focus shifts to a comparison among DG-MORLagents rather than among other baseline algorithms. In the first part, we explore the sensitivity of the DG-MORL algorithm to the quantity of initial demonstrations.The second part of this section examines how the quality of data impacts the algorithms performance. Forbetter comparison, the GPI-PD result is included for reference depicted by a blue dashed line.",
  "A.1Sensitivity to initial demonstration quantity": ", 8, 9, 10, and 11 shows DG-MORL algorithms sensitivity to the quantity of initial demonstrationsin the three benchmark environments separately. The figures presented depict the average results of 5independent runs for the sake of clarity. It should be noted that scenarios without any initial demonstrationsare not taken into account, as the presence of such demonstrations is a fundamental prerequisite for DG-MORL.We ensure that the process commences with a minimum number of demonstrations equivalent to the numberof objectives, e.g. for DST, as there are 2 objectives, the least amount of demonstration provided should be 2as well. Though over time, there are more demonstrations added by the self-evolving mechanism, the outcomes ofour research reveal a clear relationship between the quantity of initial demonstrations and the performanceof the DG-MORL algorithm, particularly evident in the DST environment. Specifically, there is a positivecorrelation observed: as the number of initial demonstrations increases, there is a notable enhancement intraining efficiency and a corresponding improvement in policy performance. This trend is consistent in the Minecart environment as well. Here too, the quantity of initial demonstrationsplays a significant role in influencing the performance of the DG-MORL algorithm. The increased numberof initial demonstrations provides more comprehensive guidance and information, which in turn facilitatesmore effective learning and decision-making by the algorithm. This consistency across different environmentsunderscores the importance of initial demonstration quantity as a key factor in the effectiveness of theDG-MORL algorithm. An intriguing observation from the Minecart experiment was that an increase in the number of initialdemonstrations actually led to lower performance at the beginning of training. This phenomenon was notevident in the DST experiment, suggesting a unique interaction in the Minecart environment. We hypothesizethat this initial decrease in performance is attributable to the complexity of the Minecart environment whenpaired with a large set of demonstrations. In such a scenario, the agent may struggle to simultaneously focuson multiple tasks or objectives presented by these demonstrations, leading to a temporary dip in performanceat the early stages of training. This is indicative of an initial overload or confusion state for the agent, asit tries to assimilate and prioritize the extensive information provided by the numerous demonstrations.However, as training progresses and the agent becomes more familiar with the environment and its varioustasks, it starts to overcome this initial challenge. With time, the agent effectively integrates the insightsfrom the demonstrations, leading to improved performance. This dynamic suggests that while a wealth ofdemonstrations can initially seem overwhelming in complex environments, they eventually contribute to theagents deeper understanding and better performance, underscoring the importance of considering both thequantity and nature of demonstrations in relation to the environments complexity. The results from the MO-Hopper, MO-Ant further support the notion that a greater number of initialdemonstrations tends to yield better performance. However, in MO-Humanoid, the quantity of initialdemonstrations have no significant influence on the learning process.This may be because that MO-Humanoid environment from the reward side is more like a single objective problem and any reasonabledemonstration can provide adequate guidance.",
  ": e. MO-Humanoid": "However, an interesting deviation was observed in MO-Hopper and MO-Humanoid where the performancewith fewer initial demonstrations was marginally better than with more. This outcome implies that, beyonda certain point, additional demonstrations might not always contribute positively to the learning process.Potential reasons for this include: Conflicting Action Selection: More demonstrations could introduce complexity in terms of conflicting actionchoices in similar states under different preferences. This conflict might hinder the agents ability to learn acoherent and effective policy. This is further emphasized by the result of the MO-Ant, as the nature of thetwo objectives is very conflicting. The learning process is therefore falling behind the GPI-PD curve at thestart, though the one with more demonstrations catches up the performance of GPI-PD at last.",
  "A.2Sensitivity to initial demonstration quality": "The quality of initial demonstrations emerges as another factor influencing the training process. While the self-evolving mechanism can to some degree mitigate this impact, it is still presumed that initial demonstrationsof higher quality are associated with more effective learning outcomes because they may lead the agent tomore potential areas in the state space. Figures 12, 13, 14, 15, and 16 collectively illustrate the DG-MORL algorithms responsiveness to the qualityof initial demonstrations across three different environments. These figures compare the overall performanceof the DG-MORL agent when initialized with varying levels of demonstration quality. Additionally, theypresent the EU for each set of demonstrations, indicated by dashed lines in colors corresponding to eachdemonstration standard. In the DST environment, for instance, the learning outcomes for all three categories of initial demonstrations(high, medium, and low quality) eventually reach the environments upper-performance limit. However,noteworthy differences are evident in the initial phases of training. The DG-MORL agent initialized withthe highest quality demonstrations achieves significantly better performance right from the first evaluationround. The performance curve of the agent with medium-quality demonstrations shows a slightly quickerascent compared to the one with lower-quality demonstrations. This pattern underscores the impact of initialdemonstration quality on both the speed and efficiency of learning, especially in the early stages of training. In the Minecart, MO-Hopper, MO-Ant and MO-Humanoid environments, the performance outcomes for thethree sets of initial demonstrations reveal a notable observation: the quality of the demonstrations has arelatively minor influence on the final learning results. This outcome contrasts with the more pronouncedeffect seen in the DST environment. In Minecart, MO-Hopper, MO-Ant and MO-Humanoid, despite variationsin the quality of the initial demonstrations (high, medium, and low), the differences in the final learningoutcomes are less significant (but still higher). This suggests that, in these particular environments, theDG-MORL algorithm is capable of achieving comparable levels of performance regardless of the initial qualityof demonstrations. This could be attributed to the inherent characteristics of the environments, where the algorithm might havemore flexibility or capability to compensate for the initial quality of demonstrations during the learningprocess. It highlights the adaptive nature of the DG-MORL algorithm and suggests that its performance isnot solely dependent on the quality of initial demonstrations, especially in certain types of environments. Similar to the observations made in the experiment evaluating the impact of demonstration quantity, theDG-MORL algorithm consistently outperformed the GPI-PD method in all scenarios (except in MO-Ant,where DG-MORL only caught up with the performance of GPI-PD at last. But it does have outperformedGPI-PD in longer training steps, see .3.1, therefore does not harm the superiority of DG-MORL)tested for demonstration quality. This consistent superiority across different conditions and environmentsempirically validates the efficacy and robustness of the DG-MORL algorithm. It demonstrates that DG-MORLcan effectively utilize the information provided in the demonstrations, whether of high or lower quality, to"
}