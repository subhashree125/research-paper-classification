{
  "Abstract": "Combining the strengths of many existing predictors to obtain a Mixture of Experts which issuperior to its individual components is an effective way to improve the performance withouthaving to develop new architectures or train a model from scratch. However, surprisingly, wefind that navely combining off-the-shelf object detectors in a similar way to Deep Ensembles,can often lead to degraded performance. We identify that the primary cause of this issue isthat the predictions of the experts do not match their performance, a term referred to asmiscalibration. Consequently, the most confident detector dominates the final predictions,preventing the mixture from leveraging all the predictions from the experts appropriately.To address this, when constructing the Mixture of Experts for object detection, we proposeto combine their predictions in a manner which reflects the individual performance ofthe experts; an objective we achieve by first calibrating the predictions before filteringand refining them. We term this approach the Mixture of Calibrated Experts (MoCaE)and demonstrate its effectiveness through extensive experiments on 5 different detectiontasks, showing that it: (i) improves object detectors on COCO and instance segmentationmethods on LVIS by up to 2.5 AP; (ii) reaches state-of-the-art on COCO test-devwith 65.1 AP and on DOTA with 82.62 AP50; (iii) outperforms single models consistentlyon recent detection tasks such as Open Vocabulary Object Detection. Code is available at:",
  "Introduction": "Deep Ensembles (DEs) (Lakshminarayanan et al., 2017) is an effective method for obtaining improvedperformance by simply training multiple models before combining their predictions at inference time. Providingthat compute is accessible, and inference time is not a significant issue, this approach provides a significantboost in performance at minimal cost. Another variant of this approach, the Mixture of Experts (MoE) is inpractice achieved by combining different predictors (Jacobs et al., 1991; Jordan & Jacobs, 1994; Xu et al.,1994; Yuksel et al., 2012; Sukhbaatar et al., 2024). Given that these experts will typically behave differentlyfor different data samples, one would thus expect that the model is able to leverage the benefits of one whilstignoring the contributions of the other poorer models. Interestingly, when considering object detectors, we",
  "(c) Uncalibrated confidence scores": ": Piecharts showing % of detections from three similarly performing detectors in their resulting MoEson COCO dataset. (a) MoE of uncalibrated detectors, (b) MoE of calibrated detectors, and (c) histogramof confidence scores. observe that navely combining experts in the standard way often leads to a degradation in performance,resulting in an MoE that is completely unable to leverage the strengths of the individual experts in certainsituations. We identify that the primary reason for this is due to a failure when combining the predictions, such that thefinal output does not respect the individual performance of the experts, an issue known as miscalibration(Guo et al., 2017) implying that the predicted confidences do not match the accuracy. This inconsistencyresults in the most confident detector dominating the final predictions, regardless of its accuracy. To illustrate,we combine RS R-CNN, ATSS and PAA with different characteristics making them non-trivial to combine.Specifically, RS R-CNN (Oksuz et al., 2021a) is a two-stage detector optimizing a ranking-based loss function,whereas ATSS (Zhang et al., 2020b) and PAA (Kim & Lee, 2020) are both one-stage detectors both trained byfocal loss (Lin et al., 2020) using different anchor-to-object assignment approaches. As can be seen in (a),RS R-CNN dominates the predictions due to its high level of confidence, which are shown in (c). It is natural to ask why is this specific to MoE? and not present in DE? For DE, the main source of variationstems from the initialisation and other stochastic processes present in the optimisation, leading to similarhistograms of predictive confidences. However, for an MoE, despite the fact that the experts perform similarly,there is a vast diversity in the mechanisms to arrive at the predictions in object detection: such as the use ofan additional auxiliary localisation head (Tian et al., 2019; Zhang et al., 2020b; Jiang et al., 2018; Huanget al., 2019; Kim & Lee, 2020) or the choice of classifier, which commonly vary between a softmax (Ren et al.,2017; Carion et al., 2020; Dai et al., 2016; Bolya et al., 2019) or sigmoid classifiers for each class (Zhang et al.,2020b; Lin et al., 2020; Kim & Lee, 2020; Zhu et al., 2021). Furthermore, different backbones (Pinto et al.,2022), loss functions (Mukhoti et al., 2020) and the training length (Mukhoti et al., 2020; Oksuz et al., 2023)can drastically affect the confidence of the model. Consequently, given the vast diversity of detectors, and the corresponding differences in their associatedconfidences for the predictions, it is imperative that for an effective MoE to be constructed, their confidencesmust match their performance; that is, they are said to be calibrated (Guo et al., 2017; Oksuz et al., 2023).To address this, we propose Mixture of Calibrated Experts (MoCaE), which calibrates the individual expertsand combines the predictions using our refinement strategy, an approach we term as Refining Non-MaximumSuppression (NMS). As post-hoc calibrators are more effective compared to the existing training-timeapproaches (Munir et al., 2022; Pathiraja et al., 2023; Munir et al., 2023a;b) for object detection (Kuzucuet al., 2024; Oksuz et al., 2023), we employ Isotonic Regression (IR) Zadrozny & Elkan (2002) or LinearRegression (LR) as post-hoc calibrators while calibrating the experts. This makes our method an example ofa practical use-case as well as a benefit of calibrating of object detectors. Furthermore, utilizing the benefitsof post-hoc calibrators, MoCaE is extremely simple to implement and leverage while combiningoff-the-shelf detectors.The resulting effects of our method can be seen in (b) and in , in which",
  "(f) Ground Truth Objects": ": Detections are color-coded. red: RS R-CNN, blue: ATSS, green: PAA. (a-c) Outputs of thedetectors on an example image. RS R-CNN misses the surfboard, ATSS misses a person, PAA has anotable localisation error for the person in front seat. (d-f) The detections from MoE of uncalibrateddetectors; MoE of calibrated detectors; and the ground truth. (d) is dominated by the most confident RSR-CNN and misses the surfboard. After calibration in (e), all objects are detected accurately by improvingeach expert. MoCaE is able to detect all objects in the scene with good localisation quality and avoids the false-positive(FP) of the third person picked up by RS R-CNN and ATSS, but not by PAA. Overall, our contributions canbe summarized as: We show that due to the diversity in training regimes for different detectors they consequently becomemiscalibrated in vastly different ways, resulting in MoEs where the most confident expert dominates.",
  "To address this, we propose MoCaE which first calibrates the experts and then combines theirpredictions through Refining NMS as our refinement mechanism while making the prediction": "We show that MoCaE yields significant gain over the single models and DEs on different realworld challenging detection tasks: such as (i) improving object detectors by up to 2.5 AP; (ii)reaching state-of-the-art (SOTA) on COCO test-dev with 65.1 AP and on DOTA for rotatedobject detection with 82.62 AP; (iii) outperforming single models consistently on recent detectiontasks such as Open Vocabulary Object Detection (OVOD).",
  "Background and Notation": "Given that the set of M objects in an image X is represented by {bi, ci}M where bi R4 is a boundingbox and ci {1, . . . , K} its class; the goal of an object detector is to predict the bounding boxes and theclass labels for the objects in X, f(X) = {ci,bi, pi}N, where ci,bi, pi represent the class, bounding boxand confidence score of the ith detection respectively and N is the number of predictions. In general, thedetections are obtained in two steps, f(X) = (h g)(X) (Ren et al., 2017; Lin et al., 2020; Carion et al.,2020; Sun et al., 2018): where g(X) = {brawi, prawi}Nraw is a neural network predicting raw detections withbounding boxes brawiand predicted class distribution prawi. Then, h() consists of post-processing steps toobtain the final detections from raw detections. Commonly, h() includes discarding the detections predictedas background; NMS to remove the duplicates; and keeping useful detections, normally achieved via top-ksurvival, where typically k = 100 for COCO. Further discussion is provided in App. A.",
  "Enabling Accurate MoEs via MoCaE": "We seek to examine the inconsistency of calibration errors for different detectors before proceeding to proposeMoCaE. Specifically, in Sec. 3.1, we highlight the many reasons why detectors differ significantly in theirconfidence, and the consequences of this when constructing an MoE. To address this, Sec. 3.2 proposesMoCaE, which calibrates the individual detectors and refine their predictions.",
  "Why do different detectors produce vastly different confidences?": "Among different factors causing this difference, one major factor is related to the parameterisation of thepredictive function. For example, some recent detectors employ an additional auxiliary head to predictlocalisation confidence (Tian et al., 2019; Zhang et al., 2020b; Jiang et al., 2018; Huang et al., 2019; Kim& Lee, 2020). Consequently, the auxiliary head choice such as centerness (Tian et al., 2019; Zhang et al.,2020b) or IoU (Jiang et al., 2018; Kim & Lee, 2020) as well as the aggregation function such as multiplication(Tian et al., 2019; Zhang et al., 2020b) or geometric mean (Kim & Lee, 2020) provides significant variationin the confidence scores. Architectural difference can also manifest itself in the type of the detector, whichcan be fully convolutional one-stage (Zhang et al., 2020b; Tian et al., 2019), two-stage (Ren et al., 2017;Cai & Vasconcelos, 2018), bottom-up (Law & Deng, 2018; Duan et al., 2019) as well as transformer-based(Carion et al., 2020; Zhu et al., 2021). Another major factor causing the confidence incompatibility across thedetectors is the used classifier, which is commonly vary between a softmax (Ren et al., 2017; Carion et al.,2020; Dai et al., 2016; Bolya et al., 2019) or sigmoid classifiers for each class (Zhang et al., 2020b; Lin et al.,2020; Kim & Lee, 2020; Zhu et al., 2021). Besides, different backbones (Pinto et al., 2022), training objectives(Mukhoti et al., 2020; Chen et al., 2020; Oksuz et al., 2020; Wang et al., 2020; Lin et al., 2020; Lin et al.,2017b; Li et al., 2020; 2019; Kahraman et al., 2023; Oksuz et al., 2021b) and the training length (Mukhotiet al., 2020; Oksuz et al., 2023) affect the confidence of the model. How different are the predicted confidences? To evaluate this, we leverage the recent work of Oksuzet al. (2023), requiring the confidence of the predictions to match the product of classification (cls.) andlocalisation (loc.) performance (perf.) using the following criterion:",
  "= pi, pi ,(1)": "where Bi(pi) is the set of true-positive (TP) boxes with the confidence score of pi, and b(i) is the ground-truthbox that bi matches with. In practice, the confidence space is split into J bins to compute the calibrationerror based on equation 1. Then, LaECE (Oksuz et al., 2023) measures the absolute difference between theaccuracy and the associated confidence:",
  "pcj IoUc(j).(3)": "We report LaECE in Tab. 1 where we see that RS R-CNN, PAA and ATSS, three similar performing detectorsin terms of AP, have vastly different LaECE, implying different confidence predictions. Moreover, in ,we display the reliability plots for RS R-CNN and ATSS, which shows that RS R-CNN is significantly moreconfident than ATSS. An uncalibrated Mixture of Experts For the purposes of exposition, we first show that navely combiningthese uncalibrated detectors results in a poor MoE, where the most confident detector dominates the MoEregardless of its accuracy. To show this, similar to (Casado-Garca & Heras, 2020), we construct a VanillaMoE which aggregates the uncalibrated predictions from RS R-CNN (Oksuz et al., 2021a), ATSS (Zhanget al., 2020b) and PAA (Kim & Lee, 2020) using NMS. (a) shows that Vanilla MoE is dominated bythe most confident RS R-CNN with a small contribution from less confident PAA and almost no detectionsfrom the least confident ATSS. Furthermore, we see in (d), that it is unable to identify the surfboardand produces a FP for a person. As a result, while one would expect an MoE to detect more objects thanindividual detectors and obtain a better recall, Tab. 2 shows the opposite; Vanilla MoE yields a lower AverageRecall (AR) compared to ATSS and PAA. This clearly indicates that navely obtaining MoE will normally bebiased and lead to an ineffective mixture. We have now highlighted that a fundamental issue with constructing an MoE, is that a situation can oftenarise when one of the detector dominates the predictions. However, this is not necessarily a deficiency, asone would expect an accurate detector to dominate the predictions when combined with inaccurate ones.Conceptually, we want the MoE to combine predictions based on their performance, which can be inferredthrough the confidence estimates provided at test time. However, as shown above, it is imperative thatthese predictions are calibrated. Therefore, to appropriately construct the MoE, we calibrate the expertsindividually, before filtering the predictions in our refinement strategy. As we show in Sec. 4, this enablesreliable contributions from each detector and an effective MoE.",
  "Calibrating Individual Experts": "Having identified the issue with the Vanilla MoE, the question naturally arises as to how we calibrate thesingle detectors to address its deficiency. As opposed to the classification task, object detection jointly solvesboth classification and regression tasks; and also involves post-processing steps that influence the accuracyof the detector. Therefore, it is not straightforward as to what objective the calibrator should have and atwhich stage of the pipeline it should be applied. A natural choice would be to calibrate the scores such thatit helps the crucial aggregation stage (e.g., NMS). This stage does not require training and has significantimpact on the accuracy of a detector. For simplicity, lets consider the standard NMS, which groups the detections that have an IoU with themaximum-scoring detection larger than a predefined IoU threshold. Then, within that group, NMS survivesthe detection with the largest score and removes the remaining detections from the detection set. In sucha setting, as also discussed by the recent works (Li et al., 2020; 2019; Kahraman et al., 2023; Jiang et al.,2018; Zhang et al., 2020b), the ideal confidence that should be transferred to the NMS is the IoU of thedetection with the object. This will guide NMS to pick accurately-localised detections for the objects detectedby multiple detectors. Furthermore, if an object is detected by a single less confident detector, aligning theconfidence with IoU implies that the scores of the TPs are to be promoted. Thus, the TPs of a less confidentdetector will not be dominated by the FPs of more confident ones unlike the case in (a). Following thisintuition, we call a detector calibrated if it yields a confidence that matches the IoU, implying",
  "where Bi(pi) is the set of detection boxes with the confidence score of pi and b(i) is the ground-truth boxthat bi has the highest IoU with": "From an optimization perspective, calibrating each expert to meet the criterion in Eq. 4 requires us to designan objective that maps the output confidence of each bounding box to a calibrated one. Though there canbe several ways to design such an objective, we take a rather simple approach where we learn a post-hoccalibrator : using the input-target pairs ({pi, IoU(bi, b(i))}) obtained on a held-out validationset. Specifically, we parameterise () as a simple LR model, containing only two learnable parameters or anIR model (Zadrozny & Elkan, 2002; Oksuz et al., 2023; Kuzucu et al., 2024). Thereby being easily applicableto any off-the-shelf detector without adding any notable overhead. Tab. 1 and Tab. 2 show an example case using IR, where the LaECE improves and the resulting MoE has ahigher recall than the uncalibrated MoE and single models. Please refer to App. B and App. D for moredetails. Furthermore, Theorem 1 indicates that Eq. 4 is the optimal choice in terms of Average Precision(AP) in obtaining an MoE for object detection. Theorem 1. Assume that E different experts are combined in the form of an MoE and the detection set ofe-th expert for class c is denoted by Be. Given the union of these detections (before aggregation) is denoted byBraw = Ee=1 Be, the MoE using perfectly calibrated experts in terms of Eq. 4 yields optimal AP for class cover any possible MoEs following Lemma 1. Accordingly, denoting the number of TPs in Braw by NT P (Braw)and the number of ground-truth objects for class c by M > 0, the resulting AP is NT P (Braw)",
  "Refining NMS for Aggregating Detections": "Another critical component of the MoE is aggregating the combined detections. There is a very high chancethat more than one detector produces the same detection; therefore we aim to suppress these duplicatedetections targeting the same object and obtain detections with high localisation quality. As aforementioned,NMS is a method that fits for this purpose and, as we observe experimentally, does provide highly competitiveresults. However, it is rigid in nature when removing overlapping detections, thereby not utilizing the richinformation provided by multiple MoEs. To address this, we present Refining NMS that simply combinesSoft NMS (Bodla et al., 2017) with Score Voting (Kim & Lee, 2020). Before we present the details, we notethat we use Refining NMS only for object detection as extending score voting to instance segmentation androtated object detection is not trivial, hence left as a future work.",
  "Calibrate": ": MoCaE pipeline. Given an image X, each detector follows its own pipeline including postprocessing(in orange) and outputs {p,b}. Without any modification to the pipeline of each detector, we calibrate theconfidence scores of each detector and aggregate them via Refining NMS providing the detections of MoCaE. Given the set of detections from all experts as B = {(bi, pi)}Ni=1 from a class ci, NMS survives the onewith highest confidence (say (b, p)) and removes all boxes that have IoU with b more than a pre-definedthreshold IoUNMS from B. This process takes place until there is no remaining detection either to be survivedor removed. Differently, instead of removing the detections completely, Soft NMS decreases the confidencescore of a highly overlapping detection as a function of their overlap with {b, p}. In particular, we use alinear decay while decreasing the confidence of these boxes, such that",
  "pkif IoU(bk,b) < IoUNMSpk (1 IoU(bk,b))else.(5)": "This less rigid removal mechanism results in higher recall. Following Soft NMS, we employ Score Voting(Kim & Lee, 2020) to obtain a bounding box with better localisation for each surviving box after Soft NMS.Specifically, the refined box bi is obtained by considering B as the set of detections from all experts beforeSoft NMS (i.e., including the ones removed by Soft NMS):",
  "Experiments": "In this section we seek to first outline the criticality of calibrating individual object detectors when constructingan MoE (Sec. 4.1). Second, we present the effectiveness of MoCaE on many standard detection benchmarksand show that it reaches state-of-the-art on COCO for object detection and DOTA for rotated boundingbox detection (Sec. 4.2). Then, we highlight the reliability of MoCaE under domain shift (Sec. 4.3). Finally,we outline the limitations (Sec. 4.4) of MoCaE. Our extensive experiments with 19 different detectors on 7datasets show that MoCaE is consistently superior to the single models, Vanilla MoE and Deep Ensembles.",
  "Effect of Calibration on Obtaining MoEs": "To demonstrate the effect of calibration, we use the common COCO dataset (Lin et al., 2014). Similar to(Kuppers et al., 2022), we randomly split COCO val set with 5K images into two, and use 2.5K imagesfor testing as COCO minitest and the remaining 2.5K images as COCO minival to analyse calibration.Specifically, we find it sufficient to use 500 images for calibrating the detectors. In our experiments, wemainly use COCO-style AP and also report (i) AP50, AP75 as the APs measured at IoU thresholds 0.50and 0.75; as well as (ii) APS, APM and APL to present the accuracy on small, medium and large objects.In terms of models, here we combine RS R-CNN, ATSS and PAA with ResNet-50 (He et al., 2016) withFPN (Lin et al., 2017a) backbone. These detectors have different characteristics making them non-trivial tocombine. Specifically, RS R-CNN (Oksuz et al., 2021a) is a two-stage detector optimizing a ranking-based",
  "BRS-DETR (Yavuz et al., 2024) trains Co-DETR with a ranking-based loss, and DINO (Zhang et al., 2023) is essentially an improved version of Deformable DETR": "We obtain an IR calibrator for each detector and improve their calibration as shown in Tab. 4. Then, wecombine them all and in pairs in Tab. 5, which demonstrates that calibrated MoEs perform notably better inalmost all combinations. For example, the gap between calibrated and uncalibrated MoEs is 1.4 AP onceBRS-DETR and DINO are combined. We note that, in the single exception of combining Co-DETR andDINO, the calibrated MoE performs similar to the uncalibrated one. This is expected as the uncalibratedconfidence distribution of these two detectors are very similar (as shown in Fig. A.11(a,c)), in which casecalibration does not make a difference. Finally, both Tab. 3 and Tab. 5 show that Refining NMS has a positiveeffect on the resulting mixture. Resulting MoCaE (i.e., using calibrated and Refining NMS) outperforms thebest uncalibrated MoE (i) by 2 AP in Tab. 3 and (ii) by 1 AP in Tab. 5. Even with fewer models, MoCaE is superior to DEs Next we compare MoCaE, Vanilla MoE and DEs.We calibrate the components of DEs while combining them, though we do not observe a significant effectfrom the calibration in their performance (see App. D for details); which is an expected outcome. Tab. 6shows that the DEs perform consistently better than the single models; validating them as strong baselines.The main observation in Tab. 6 is that combining different types of few detectors into an MoE performssignificantly better than DEs. Specifically, MoCaE with only two detectors, ATSS and PAA, outperforms allDEs, each with five components. Also, combining three detectors by MoCaE performs 1.1 AP better than itsclosest counterpart DE. This is because the same type of detectors make similar errors, which yields less gainonce they are combined together. However, different types of detectors complement each other thanks totheir diversity. Finally, MoCaE outperforms Vanilla MoE by 2 AP in this setting. Our final model obtains45.5 AP and outperforms the best single model in all AP variants significantly.",
  "Benchmarking MoCaE on Various Tasks": "In this section, we demonstrate that combining off-the-shelf detectors via MoCaE improves single detectorson various detection tasks up to 2.5 AP, which is a significant performance improvement. Our MoCaE reachesSOTA results on COCO dataset among public models and on DOTA dataset for rotated object detection.Specifically, we evaluate on four different tasks: object detection (COCO (Lin et al., 2014)), rotated objectdetection (DOTA (Xia et al., 2018)), open vocabulary object detection (COCO and ODinW35 (Li et al.,2022a)) and instance segmentation (LVIS (Gupta et al., 2019)). For these tasks, we use a total of 15 differentdetectors include one-stage and two-stage, convolutional, transformer-based ones and foundation models.",
  "ATSS with transformer-based dynamic head (Dai et al., 2021) and again Swin-L backbone": "These detectors differ from each other in terms of the pretraining data, backbone or architecture as summarizedin App. D. Tab. 7 shows that our MoCaE reaches 59.0 AP with a gain of 2.4 AP on this challenging settingas well. As our gain here is similar to that of Tab. 6, we can easily say that the gain of our MoCaE has notsaturated in this stronger setting, which is commonly the opposite in the literature.",
  "Tab. 8 shows that MoCaE reaches SOTA with 65.1 AP on COCO test-dev and outperforms all existing publicdetectors by 0.7 AP. This further shows the effectiveness of MoCaE": "Rotated Object Detection on DOTA We now investigate MoCaE for rotated object detection on DOTAv1.0 dataset (Xia et al., 2018) with 15 classes. DOTA is also a challenging dataset comprising of aerial imagesthat are very dense in terms of objects. Specifically, DOTA dataset has 67.1 objects on average per image.We use all 458 images in the validation set to calibrate the detectors and report AP50 on the test set bysubmitting our results to the evaluation server. We combine LSKN (Li et al., 2023) and RTMDet (Lyu et al.,2022) as two recent SOTA detectors. Following the literature, we use NMS with an IoU threshold of 0.35 asSoft NMS and Score Voting are not straightforward to use in this task. Tab. 9 suggests that we establish anew SOTA with 82.62 AP50 on DOTA; improving the previous SOTA by 0.77. Having examined the classes,",
  "MoCaE47.771.752.129.251.568.7(Ours)+1.1+2.2+1.7+2.5+1.41.8": "we note that our improvement originates mostly from the classes with relatively lower performance; with theexception of the class bridge which performs marginally worse. For example, on soccer-field, roundabout,harbor classes where the single detectors have between 70 80 AP50, the improvement is around 3 AP50.These gains enable us to demonstrate the ability of MoCaE to set a new SOTA in rotated object detection. Open Vocabulary Object Detection (OVOD) We now investigate the effect of MoCaE on OVOD task(Li et al., 2022b;a). OVOD task is a recently proposed challenging task in which the aim is to detect theobjects pertaining to the classes in a given text prompt. This requires the models to be able to interpretthe given text prompt as well as the image and yield the detection results. Furthermore, the text promptcan contain phrases that are not necessarily in the training data. For this challenging task, we combine twostrong and recent models:",
  "Grounding DINO (Liu et al., 2024), a transformer-based detector, and MQ-Det (Xu et al., 2023), an anchor-based OVOD relying on GLIP (Li et al., 2022b)": "We obtain the calibrators on a subset of the Objects365 dataset (Shao et al., 2019), which is included inthe pretraining data for both of these models. Therefore, the calibrators are also limited to the pretrainingdata only. In our evaluation, we evaluate the models on COCO and ODinW-35(Li et al., 2022a) datasetsfollowing the common convention (Li et al., 2022b; Liu et al., 2024; Xu et al., 2023). Note that ODinW-35is a challenging dataset with 35 different subdatasets, some of which are substantially different from thepretraining dataset. To illustrate, ODinW-35 includes subdatasets specifically for potholes on the road andinfrared images of dogs and people. Tab. 10 shows that MoCaE improves the single models on both ofthese datasets on all performance measures notably. As an example, the median AP of the best single modelon ODinW-35 increases from 13.8 to 14.9, which suggests an 8% relative gain. Instance Segmentation Given that MoCaE is beneficial for object detection, one would expect it toimprove performance on the instance segmentation task. To verify this, we first use LVIS (Gupta et al.,2019) as a long-tailed dataset for instance segmentation with more than 1K classes. Following its standardevaluation, we also report the AP on rare (APr), common (APc) and frequent (APf) classes. Similar toCOCO, we reserve 500 images from val set to calibrate the detectors, and test our models on the remaining19.5K images of the val set. We combine three diverse off-the-shelf Mask R-CNN variants in a MoE:",
  "How Reliable is MoCaE?": "As we are ensembling detectors in the form of MoE, it is naturally to evaluate how reliable MoCaE is. Toevaluate this, we investigate how MoCaE performs under domain shift (Michaelis et al., 2019; Hendrycks &Dietterich, 2019), and also test MoCaE on the recently proposed Self-aware Object Detection (SAOD) task(Oksuz et al., 2023). Here, we use our setting in Sec. 4.1, in which we combine RS R-CNN, ATSS and PAA.That is, we use the calibrators trained on clean COCO and do not train a new calibrator. Domain Shift (Synthetic and Natural) Following the convention (Michaelis et al., 2019; Oksuz et al.,2023; Munir et al., 2022), we apply 15 ImageNet-C style corruptions (Hendrycks & Dietterich, 2019) under 5different severities for synthetic domain shift on COCO. Similar to the clean data, we observe in Tab. 13 thatcombining only three models (RS R-CNN, ATSS and PAA) outperforms DEs with five components thanks tothe diversity of the detectors. Here we see, that the performance over the best single model improves by 1.5AP. Next, we evaluate the performance of MoCaE on Objects45K (Oksuz et al., 2023). Please note that thisdataset has the same set of classes as in COCO, but collected and annotated separately, thereby implying anatural domain shift and a similar setting was used in (Harakeh & Waslander, 2021). Tab. 13 shows that theDEs do not provide notable gains in this setting. For example, while PAA 5 only improves single PAAonly by 0.7 AP, MoCaE improves the best single model by 2 AP, outperforming all DEs and Vanilla MoE. Self-aware Object Detection (SAOD) Finally, we evaluate MoCaE on the recently proposed SAOD task(Oksuz et al., 2023), which requires the object detectors to be self-aware. This task requires detectors toprovide reliable uncertainty estimates along with accurate and calibrated detections in a holistic manner;which is evaluated using the Detection Awareness Quality (DAQ). To evaluate the models on this task, weconvert RS R-CNN, ATSS, PAA, Vanilla MoE and MoCaE to a self-aware detector following (Oksuz et al.,2023). MoCaE improves the DAQ of the best single model from 40.9 to 42.9 and outperforms Vanilla MoEby 0.5 DAQ. Overall, this suggest that MoCaE is more reliable than the single detectors. Details areprovided in App. D due to space limitation.",
  "Conclusions": "A direct result of the vastly different training regimes employed in training object detectors is that theirpredictions are miscalibrated such that some are more confident than the others. This lack of consistency makesconstructing an MoE in a nave approach futile, as the most confident detector dominates the predictions,even though its performance may not warrant this weighting. Consequently, to address this we introducedMoCaE as a simple, principled and effective approach, which first calibrates the individual detectors beforecombining them appropriately through our refinement strategy. Specifically, in the calibration stage wealigned the confidence with the IoU of the detection with the object that it overlaps the most with. Weshowed that this is an effective calibration target, resulting in accurate MoEs with consistent gains acrossdifferent detection tasks, reaching SOTA on many challenging detection benchmarks such as COCO andDOTA. Whilst our choice of calibration function performed well on the settings demonstrated here, wefurther observe that increased gains can be achieved if the community develops more sophisticated calibrationmethods, an objective we leave to future work.",
  "ngela Casado-Garca and Jnathan Heras. Ensemble methods for object detection. In European Conferenceon Artificial Intelligence, 2020": "Kai Chen, Jiaqi Wang, Jiangmiao Pang, Yuhang Cao, Yu Xiong, Xiaoxiao Li, Shuyang Sun, Wansen Feng,Ziwei Liu, Jiarui Xu, Zheng Zhang, Dazhi Cheng, Chenchen Zhu, Tianheng Cheng, Qijie Zhao, Buyu Li,Xin Lu, Rui Zhu, Yue Wu, Jifeng Dai, Jingdong Wang, Jianping Shi, Wanli Ouyang, Chen Change Loy,and Dahua Lin. MMDetection: Open mmlab detection toolbox and benchmark. arXiv, 1906.07155, 2019. Kean Chen, Weiyao Lin, Jianguo li, John See, Ji Wang, and Junni Zou. Ap-loss for accurate one-stage objectdetection. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), pp. 11, 2020. Bowen Cheng, Ishan Misra, Alexander G. Schwing, Alexander Kirillov, and Rohit Girdhar. Masked-attentionmask transformer for universal image segmentation. In Conference on Computer Vision and PatternRecognition (CVPR), 2022.",
  "Jifeng Dai, Yi Li, Kaiming He, and Jian Sun. R-FCN: Object detection via region-based fully convolutionalnetworks. In Advances in Neural Information Processing Systems (NeurIPS), 2016": "Xiyang Dai, Yinpeng Chen, Bin Xiao, Dongdong Chen, Mengchen Liu, Lu Yuan, and Lei Zhang. Dynamichead: Unifying object detection heads with attentions. In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition (CVPR), pp. 73737382, June 2021. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby.An image is worth 16x16 words: Transformers for image recognition at scale. In International Conferenceon Learning Representations, 2021. URL",
  "Published in Transactions on Machine Learning Research (10/2024)": "Table A.29: Oracle MoE and pitfalls of MoCaE. Object detection performance on COCO mini-test. Whenthe single methods have significant performance gap, then MoCaE might not perform well as the calibratorsare imperfect. Oracle MoE is an MoE in which each expert is perfectly calibrated. As a result, all expertshave 0 LaECE in Oracle MoE. Following Theorem 1, Oracle MoE outperforms all single detectors.",
  "Vitaly Feldman. Does learning require memorization? A short tale about a long tail. CoRR, abs/1906.05271,2019. URL": "Vitaly Feldman and Chiyuan Zhang. What neural networks memorize and why: Discovering the long tail viainfluence estimation. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advancesin Neural Information Processing Systems, volume 33, pp. 28812891. Curran Associates, Inc., 2020. Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger. On calibration of modern neural networks. InDoina Precup and Yee Whye Teh (eds.), Proceedings of the 34th International Conference on MachineLearning, volume 70 of Proceedings of Machine Learning Research, pp. 13211330. PMLR, 2017.",
  "Robert A Jacobs, Michael I Jordan, Steven J Nowlan, and Geoffrey E Hinton. Adaptive mixtures of localexperts. Neural computation, 3(1):7987, 1991": "Ganesh Jawahar, Subhabrata Mukherjee, Xiaodong Liu, Young Jin Kim, Muhammad Abdul-Mageed, LaksV. S. Lakshmanan, Ahmed Hassan Awadallah, Sebastien Bubeck, and Jianfeng Gao. Automoe: Neuralarchitecture search for efficient sparsely activated transformers, 2023. URL Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford,Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel,Guillaume Bour, Guillaume Lample, Llio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, PierreStock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Thophile Gervet, ThibautLavril, Thomas Wang, Timothe Lacroix, and William El Sayed. Mixtral of experts, 2024.",
  "Ananya Kumar, Percy S Liang, and Tengyu Ma. Verified uncertainty calibration. In Advances in NeuralInformation Processing Systems (NeurIPS), volume 32, 2019": "Fabian Kuppers, Jan Kronenberger, Amirhossein Shantia, and Anselm Haselhoff. Multivariate confidencecalibration for object detection. In The IEEE/CVF Conference on Computer Vision and Pattern Recognition(CVPR) Workshops, 2020. Fabian Kuppers, Jonas Schneider, and Anselm Haselhoff. Parametric and multivariate uncertainty calibrationfor regression and object detection. In Safe Artificial Intelligence for Automated Driving Workshop in TheEuropean Conference on Computer Vision, 2022.",
  "Hyungtae Lee, Sungmin Eum, and Heesung Kwon. Me r-cnn: Multi-expert r-cnn for object detection. IEEETransactions on Image Processing, 29:10301044, 2020. doi: 10.1109/TIP.2019.2938879": "Chunyuan Li, Haotian Liu, Liunian Harold Li, Pengchuan Zhang, Jyoti Aneja, Jianwei Yang, Ping Jin,Houdong Hu, Zicheng Liu, Yong Jae Lee, and Jianfeng Gao. Elevater: A benchmark and toolkit forevaluating language-augmented visual models, 2022a. Liunian Harold Li, Pengchuan Zhang, Haotian Zhang*, Jianwei Yang, Chunyuan Li, Yiwu Zhong, LijuanWang, Lu Yuan, Lei Zhang, Jenq-Neng Hwang, Kai-Wei Chang, and Jianfeng Gao. Grounded language-image pre-training. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),2022b. Xiang Li, Wenhai Wang, Xiaolin Hu, Jun Li, Jinhui Tang, and Jian Yang. Generalized focal loss v2: Learningreliable localization quality estimation for dense object detection. In IEEE/CVF Conference on ComputerVision and Pattern Recognition (CVPR), 2019. Xiang Li, Wenhai Wang, Lijun Wu, Shuo Chen, Xiaolin Hu, Jun Li, Jinhui Tang, and Jian Yang. Generalizedfocal loss: Learning qualified and distributed bounding boxes for dense object detection. In Advances inNeural Information Processing Systems (NeurIPS), 2020.",
  "Bin Lin, Zhenyu Tang, Yang Ye, Jiaxi Cui, Bin Zhu, Peng Jin, Jinfa Huang, Junwu Zhang, Munan Ning, andLi Yuan. Moe-llava: Mixture of experts for large vision-language models, 2024": "Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollr, andC Lawrence Zitnick. Microsoft COCO: Common Objects in Context. In The European Conference onComputer Vision (ECCV), 2014. Tsung-Yi Lin, Piotr Dollr, Ross B. Girshick, Kaiming He, Bharath Hariharan, and Serge J. Belongie.Feature pyramid networks for object detection. In IEEE/CVF Conference on Computer Vision and PatternRecognition (CVPR), 2017a.",
  "Tsung-Yi Lin, Priya Goyal, Ross B. Girshick, Kaiming He, and Piotr Dollr. Focal loss for dense objectdetection. In IEEE/CVF International Conference on Computer Vision (ICCV), 2017b": "Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollr. Focal loss for dense object detection.IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 42(2):318327, 2020. Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, HangSu, Jun Zhu, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection.In The European Conference on Computer Vision (ECCV), 2024.",
  "Chengqi Lyu, Wenwei Zhang, Haian Huang, Yue Zhou, Yudong Wang, Yanyi Liu, Shilong Zhang, and KaiChen. Rtmdet: An empirical study of designing real-time object detectors. ArXiv, 2022": "C. Michaelis, B. Mitzkus, R. Geirhos, E. Rusak, O. Bringmann, A. S. Ecker, M. Bethge, and W. Brendel.Benchmarking robustness in object detection: Autonomous driving when winter is coming. In NeurIPSWorkshop on Machine Learning for Autonomous Driving, 2019. Jishnu Mukhoti, Viveka Kulharia, Amartya Sanyal, Stuart Golodetz, Philip Torr, and Puneet Doka-nia.Calibrating deep neural networks using focal loss.In H. Larochelle, M. Ranzato, R. Hadsell,M.F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp.1528815299. Curran Associates, Inc., 2020. URL Muhammad Akhtar Munir, Muhammad Haris Khan, M. Saquib Sarfraz, and Mohsen Ali. Towards improvingcalibration in object detection under domain shift. In Advances in Neural Information Processing Systems(NeurIPS), 2022. Muhammad Akhtar Munir, Muhammad Haris Khan, Salman Khan, and Fahad Khan. Bridging precision andconfidence: A train-time loss for calibrating object detection. In IEEE Conference on Computer Visionand Pattern Recognition (CVPR), 2023a. Muhammad Akhtar Munir, Salman Khan, Muhammad Haris Khan, Mohsen Ali, and Fahad Khan. Cal-DETR:Calibrated detection transformer. In Thirty-seventh Conference on Neural Information Processing Systems,2023b. URL Luks Neumann, Andrew Zisserman, and Andrea Vedaldi. Relaxed softmax: Efficient confidence auto-calibration for safe pedestrian detection. In NIPS MLITS Workshop on Machine Learning for IntelligentTransportation System, 2018. Jeremy Nixon, Michael W. Dusenberry, Linchuan Zhang, Ghassen Jerfel, and Dustin Tran. Measuringcalibration in deep learning. In IEEE/CVF Conference on Computer Vision and Pattern Recognition(CVPR) Workshops, June 2019.",
  "Kemal Oksuz, Baris Can Cam, Emre Akbas, and Sinan Kalkan. Rank & sort loss for object detection andinstance segmentation. In The International Conference on Computer Vision (ICCV), 2021a": "Kemal Oksuz, Baris Can Cam, Fehmi Kahraman, Zeynep Sonat Baltaci, Sinan Kalkan, and Emre Akbas.Mask-aware iou for anchor assignment in real-time instance segmentation. In The British Machine VisionConference (BMVC), 2021b. Kemal Oksuz, Baris Can Cam, Sinan Kalkan, and Emre Akbas. One metric to measure them all: Localisationrecall precision (lrp) for evaluating visual detection tasks. IEEE Transactions on Pattern Analysis andMachine Intelligence, pp. 11, 2021c. Kemal Oksuz, Tom Joy, and Puneet K. Dokania. Towards building self-aware object detectors via reliableuncertainty quantification and calibration. In Conference on Computer Vision and Pattern Recognition(CVPR), 2023. Bimsara Pathiraja, Malitha Gunawardhana, and Muhammad Haris Khan.Multiclass confidence andlocalization calibration for object detection. In IEEE/CVF Conference on Computer Vision and PatternRecognition (CVPR), 2023.",
  "Francesco Pinto, Philip H. S. Torr, and Puneet K. Dokania. An impartial take to the cnn vs transformerrobustness contest. In The European Conference on Computer Vision (ECCV), 2022": "Maciej Piro, Kamil Ciebiera, Krystian Krl, Jan Ludziejewski, Micha Krutul, Jakub Krajewski, SzymonAntoniak, Piotr Mio, Marek Cygan, and Sebastian Jaszczur. Moe-mamba: Efficient selective state spacemodels with mixture of experts, 2024. Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster R-CNN: Towards real-time object detectionwith region proposal networks. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI),39(6):11371149, 2017. Carlos Riquelme Ruiz, Joan Puigcerver, Basil Mustafa, Maxim Neumann, Rodolphe Jenatton, Andr SusanoPinto, Daniel Keysers, and Neil Houlsby. Scaling vision with sparse mixture of experts. In A. Beygelzimer,Y. Dauphin, P. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems,2021. Shuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang Yu, Xiangyu Zhang, Jing Li, and Jian Sun.Objects365: A large-scale, high-quality dataset for object detection. In IEEE/CVF International Conferenceon Computer Vision (ICCV), 2019. Sheng Shen, Le Hou, Yanqi Zhou, Nan Du, Shayne Longpre, Jason Wei, Hyung Won Chung, Barret Zoph,William Fedus, Xinyun Chen, Tu Vu, Yuexin Wu, Wuyang Chen, Albert Webson, Yunxuan Li, VincentZhao, Hongkun Yu, Kurt Keutzer, Trevor Darrell, and Denny Zhou. Mixture-of-experts meets instructiontuning:a winning combination for large language models, 2023.",
  "Saining Xie, Ross B. Girshick, Piotr Dollr, Zhuowen Tu, and Kaiming He. Aggregated residual transformationsfor deep neural networks. arXiv, 1611.05431, 2016": "Lei Xu, Michael Jordan, and Geoffrey E Hinton.An alternative model for mixtures of experts.InG. Tesauro, D. Touretzky, and T. Leen (eds.), Advances in Neural Information Processing Systems,volume 7. MIT Press, 1994. URL Yifan Xu, Mengdan Zhang, Chaoyou Fu, Peixian Chen, Xiaoshan Yang, Ke Li, and Changsheng Xu. Multi-modal queried object detection in the wild.In Advances in Neural Information Processing Systems(NeurIPS), 2023. Feyza Yavuz, Baris Can Cam, Adnan Harun Dogan, Kemal Oksuz, Emre Akbas, and Sinan Kalkan. Bucketedranking-based losses for efficient training of object detectors. In European Conference on Computer Vision(ECCV), 2024.",
  "Seniha Esen Yuksel, Joseph N Wilson, and Paul D Gader. Twenty years of mixture of experts. IEEEtransactions on neural networks and learning systems, 23(8):11771193, 2012": "Bianca Zadrozny and Charles Elkan. Transforming classifier scores into accurate multiclass probabilityestimates. In Proceedings of the eighth ACM SIGKDD international conference on Knowledge discoveryand data mining, pp. 694699, 2002. Hao Zhang, Feng Li, Shilong Liu, Lei Zhang, Hang Su, Jun Zhu, Lionel Ni, and Heung-Yeung Shum. DINO:DETR with improved denoising anchor boxes for end-to-end object detection. In The Eleventh InternationalConference on Learning Representations, 2023. URL Hongkai Zhang, Hong Chang, Bingpeng Ma, Naiyan Wang, and Xilin Chen. Dynamic r-cnn: Towards highquality object detection via dynamic training. In The European Conference on Computer Vision (ECCV),2020a. Shifeng Zhang, Cheng Chi, Yongqiang Yao, Zhen Lei, and Stan Z. Li. Bridging the gap between anchor-basedand anchor-free detection via adaptive training sample selection. In IEEE/CVF Conference on ComputerVision and Pattern Recognition (CVPR), 2020b.",
  "Boyan Zhou, Quan Cui, Xiu-Shen Wei, and Zhao-Min Chen. Bbn: Bilateral-branch network with cumulativelearning for long-tailed visual recognition, 2020": "Yanqi Zhou, Tao Lei, Hanxiao Liu, Nan Du, Yanping Huang, Vincent Zhao, Andrew M Dai, zhifeng Chen,Quoc V Le, and James Laudon. Mixture-of-experts with expert choice routing. In Advances in NeuralInformation Processing Systems, volume 35. Curran Associates, Inc., 2022. Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable {detr}: Deformabletransformers for end-to-end object detection. In International Conference on Learning Representations(ICLR), 2021.",
  "ARelated Work": "Mixture of Experts (MoEs)The main aim of combining multiple experts in the form of an MoE is toleverage the expertise of each expert, which ideally specialises in different subpopulations of the input data(Jacobs et al., 1991; Jordan & Jacobs, 1994; Xu et al., 1994; Yuksel et al., 2012). In the literature, this aim isachieved by a variety of techniques. Some of the methods aggregates the predictions of individual experts(Casado-Garca & Heras, 2020). Broadly speaking, from this perspective, DEs (Lakshminarayanan et al.,2017), in which multiple models are trained and then aggregated, can also be considered as an example in thisgroup. Another set of methods (Zhou et al., 2020; Lee et al., 2020; Wang et al., 2022b; Zhang et al., 2022;Cai et al., 2021; Ruiz et al., 2021), as the majority of the techniques proposed in the deep learning era, allowsthe experts to share certain network components among experts, which are then combined (or selected) bya gating (a.k.a. routing) mechanism for the sake of efficiency. Essentially, this gating mechanism decideson which expert to rely on conditioned on a particular subpspace of the input space.One particular andintuitive use-case of this group of methods is long-tailed classification (Zhou et al., 2020; Wang et al., 2022b;Cai et al., 2021; Zhang et al., 2022), in which different experts specialise on different classes with variouscardinalities, i.e. the classes with few or many training examples. The shared features are then routed by agating mechanism to leverage experts specialisation in an efficient manner. Another effective application ofMoEs is within the domain of variational autoencoders (Wu & Goodman, 2018; Shi et al., 2019; Joy et al.,2022), where different experts are generally utilised for different modalities. Ensemble Methods in Object DetectionDespite their aforementioned success in the classificationliterature, ensembling detectors either in the form of a DE by using the same model with different initialisationsor as an MoE by combining different type of models has received very little attention (Lee et al., 2020;Casado-Garca & Heras, 2020). Among the few existing works, Casado-Garca & Heras (2020) combine aset of detectors through various aggregation strategies, such as unanimous agreement between the detectors.Therefore, this method combines off-the-shelf detectors, and accordingly, we refer to as Vanilla MoE in thepaper as a baseline. However, we note that it does not consider either calibrating the detector or advancedaggregation techniques unlike our work. As the second work in this domain, Multi-Expert R-CNN(Leeet al., 2020) is designed to exploit multiple experts in the R-CNN family, in which different R-CNN modelscorrespond to different experts for a specific RoI. Consequently, it is not applied to a wide range of differentdetectors such as the common one-stage detectors or transformer-based detectors. Calibration in Object DetectionAs extensively studied for classification, calibration refers to thealignment of accuracy and confidence of a model (Guo et al., 2017; Nixon et al., 2019; Kumar et al., 2019;Wang et al., 2021; Mukhoti et al., 2020; Cheng & Vasconcelos, 2022). Specifically, a classifier is said to becalibrated if it yields an accuracy of p on its predictions with a confidence of p for all p . Earlierdefinitions for the calibration of detectors (Kuppers et al., 2020; Neumann et al., 2018) extend this definitionwith an objective to align the confidence of a detector with its precision,",
  "P(ci = ci|pi) = pi, pi ,(A.7)": "where P(ci = ci|pi) denotes the precision as the ratio of correctly classified predictions among all detections.Extending from this definition, Oksuz et al. (2023) take into account that object detection is a joint task ofclassification and localisation. Thereby defining the accuracy as the product of precision and average IoU ofTPs, calibration of object detectors requires the following to be true",
  "P(ci = ci|pi)EbiBi(pi)[IoU(bi, b(i))] = pi, pi ,(A.8)": "where Bi(pi) is the set of TPs with the confidence of pi and b(i) is the object that bi matches with. Then,LaECE is obtained by discretizing the confidence score space into J bins for each class. Specifically for classc, denoting the set of detections by Dc and those in the jth bin by Dcj as well as the average confidence,",
  "Finally, the detector LaECE is the average of LaECEcs over classes in the dataset, measuring the calibrationerror for a detector as a lower-better measure": "Comparative SummaryDifferent from the existing few works on the ensemble methods of object detectors,we comprehensively investigate how to obtain MoEs in object detection using off-the-shelf detectors. Whiledoing so, unlike existing work, (i) we identify that miscalibration of different detectors prevents them to becombined properly due to the peculiarities of the detectors; (ii) we introduce a strong aggregation techniquewhich we refer to as Refining NMS combining Soft NMS and Score Voting from previous work; and (iii) weemploy a very diverse set of detectors from a wide range of detection tasks. As for the miscalibration of thedetectors, we rely on LaECE in Eq. A.8. Consequently, MoCaE follows the main aim of the MoE as thepredictions from each expert are aggregated in a way that the strength of each detector is leveraged as wecomprehensively demonstrate in our experiments. And besides, from the MoE perspective Refining NMScan be considered as a form of a gating (or routing) mechanism, which decides the best way to combine thedetections from different experts.",
  "BFurther Details on Calibration": "Throughout our paper, we calibrate the final confidence scores. One can also consider in object detectionthat, the raw confidence scores (before postprocessing) can be calibrated as well. For this reason, we firstpresent early calibration in App. B.1. Then, for the sake of completeness, we discuss why we prefer post-hoccalibration methods in App. B.2 as well as how to measure the calibration error of MoCaE based on Eq. (4)in App. B.3.",
  "B.1Early Calibration of Object Detectors for Obtaining MoEs": "Up to now, we discussed calibrating final confidence scores pi similar to Kuppers et al. (2020); Oksuz et al.(2023), which we show as late calibration in . Besides, we also investigate the effectiveness of earlycalibration by calibrating the raw probabilities prawiof the detectors as illustrated in Fig. A.5. While we findboth approaches to perform similar in MoEs, we use late calibration as it is simpler owing to less numberof final detections. Still, as the first to investigate early calibration, we present an additional use-case ofearly calibration in App. D in which we show that it reduces the sensitivity of the model to the backgroundremoval threshold in post-processing.",
  "B.2Choice of the Calibration Method": "There are multiple calibration methods used for object detection including training-time (Munir et al., 2022;2023a; Pathiraja et al., 2023) and post-hoc calibration methods (Oksuz et al., 2023; Zadrozny & Elkan, 2002;Guo et al., 2017). Ideally, we would expect a calibration method to easily generalize to a wide range ofdetectors as well as detection tasks. However, training-time calibration methods are typically incorporatedinto only a small set of object detectors and not tested on a variety of relevant tasks such as the rotatedbounding box detection, which makes them unfit for purposes. On the other hand, post-hoc calibrationmethods formalize calibration as a regression task from the predicted final confidence to target confidence,making them easily applicable to any detectors and any detection task. Throughout this work, we investigateLinear Regression (LR) and Isotonic Regression (IR) (Zadrozny & Elkan, 2002) considering the criterion inEq. 4. We further observe that a Class-agnostic (CA) IR calibrator for each detector obtained on 500 imagesis sufficient for MoCaE. App D present extensive experimental analyses.",
  "B.3Measuring the Calibration Error for MoE": "We introduce our calibration criterion in Eq. 4. This criterion requires the confidence of a detection to alignwith its IoU with the ground truth box that the detection overlaps the most. As stated earlier, computingthe calibration error based on this criterion corresponds to using an IoU threshold of 0 to validate TPs. Thisis equivalent to using precisionc(j) = 1 for class c in Eq. 2, which then reduces to",
  "pcj IoUc(j) .(A.10)": "where Dc denotes the set of detections for class c; Dcj is the set of detections in the jth bin for class c; pcjis the average confidence of the detections in Dcj; andIoUc(j) is the average IoU of the detections in Dcj.Following Oksuz et al. (2023), we use J = 25 and average over LaECEc of classes for the detector LaECE. In addition to LaECE, here we define Localisation-aware Average Calibration Error (LaACE) and Localisation-aware Maximum Calibration Error (LaMCE) similar to the way how Expected Calibration Error (ECE) isextended to Average Calibration and Maximum Calibration Errors. We find LaACE and LaMCE useful asthey reduce the dominance of certain bins on the calibration error as in the case of LaECE. This is especiallyimportant for early calibration from which thousands of confidence scores are obtained from a single image,most of which have a confidence close to 0. Specifically, in our case, we define LaACEc for class c as",
  "Lemma 1 discusses the conditions under which an optimal AP can be achieved for a given set of pre-NMSdetections": "Lemma 1. Given a set of detection boxes for class c, denoted by Braw = {braw1,braw2, ...,brawL} , we first assumethat the post-processing (NMS in this case) does not remove TPs and can remove duplicates in Braw*. Let us *A TP is a detection that has at least an IoU with a ground-truth of where is the IoU threshold to validate TPs. In thecase of more than one detection satisfy this criterion, the common convention is to accept the detection with the highest score asa TP and the remaining ones are duplicates, which are counted as FPs while computing the AP.",
  ". B is to have NT P (Braw), that is, no TP is to be removed in Braw by postprocessing. Note that this ishandled by post-processing as an assumption in the Lemma": "2. The minimum confidence score of TPs in B is to be higher than the maximum confidence score ofFPs. This is also ensured considering (i) the assumption that is IoU(bi, bk) > IoU( bj, bk) if pi > pjfor all i = j and k is the ground truth that each detection has maximum IoU with; and (ii) theduplicates are removed by NMS. As a result, all TPs are ranked higher than all FPs. 3. B is to include the detection with the largest IoU with each ground truth k.*. NMS selects thedetection with the highest confidence score among a group of overlapping detections. Consideringthat NMS does not remove any TPs and IoU(bi, bk) > IoU( bj, bk) for all i and j if pi > pj and i = jholds, NMS survives the detections with the best localisation quality as they have the highest scoresin their groups. As a result, this condition is also satisfied. To compute the area under the precision-recall curve, we need the precision and recall pairs. Please notethat once the aforementioned criteria are satisfied, the precision will be 1 when recall interval is between[0, NT P (Braw)",
  "On the Optimality of Eq. 4Based on Lemma 1, we now present our theorem and its proof": "Theorem 1. Assume that E different experts are combined in the form of an MoE and the detection set ofe-th expert for class c is denoted by Be. Given the union of these detections (before aggregation) is denoted byBraw = Ee=1 Be, the MoE using perfectly calibrated experts in terms of Eq. 4 yields optimal AP for class cover any possible MoEs following Lemma 1. Accordingly, denoting the number of TPs in Braw by NT P (Braw)and the number of ground-truth objects for class c by M > 0, the resulting AP is NT P (Braw)",
  "M": "Proof. Please note that it is trivial to show that the detections in B of the perfectly calibrated detectorsensures the requirement in Lemma 1 that IoU(bi, bk) > IoU( bj, bk) if pi > pj for all i = j and k is theground truth that each detection has maximum IoU with. This is because the calibration target ensures thatpi = IoU(bi, bk) for each detection i. As a result, following from Lemma 1, the theorem holds. Please note that it is trivial to show that Theorem 1 generalizes to the cases in which the dataset involvesmultiple classes and COCO-style AP is used. This is because the former case is estimated as the average ofthe APs over different classes and the latter is simply the average of the APs over different IoU thresholds(please see footnote * for further discussion). Consequently, as the class-wise APs are optimal, so do thedataset AP and the COCO-style AP. Please refer to App. D.7 for the experiment presenting the effect ofTheorem 1 using an Oracle MoE. * When we consider the AP for a single , this criteria is not mandatory to be satisfied as the conventional AP considerlocalisation performance loosely(Oksuz et al., 2021c). However, the localisation performance is an important aspect of an objectdetector and it is considered by various performance measures such as COCO-style AP, which corresponds to the average overAPs with 10 different thresholds or LRP Error(Oksuz et al., 2021c). As a result, while we consider the AP for a single inthis section, in order for our theoretical justifications to be applicable to other performance measures, we also take this criterionin account.",
  "D.1Further Details on Used Models": "We provide the details of the used models as follows. We again note that we havent trained any model butused off-the-shelf detectors with the exception of DEs. Here we provide further details on the used detectors.Still, as it is not feasible to provide all of the details, we also present the papers and repositories that weborrow these off-the-shelf models in order to ensure the reproducibility of our results.",
  "Object DetectionWe use two different configurations. In the first one, we employ three detectors withResNet-50 (He et al., 2016) with FPN (Lin et al., 2017a) backbone. These detectors are:": "Rank & Sort R-CNN (RS R-CNN) (Oksuz et al., 2021a) is a recent representative of the two-stageR-CNN family (Ren et al., 2017; Dai et al., 2016; Zhang et al., 2020a) optimizing a ranking-basedloss function, Adaptive Training Sample Selection (ATSS) (Zhang et al., 2020b) is a common one stage baseline, Probabilistic Anchor Assignment (PAA) (Kim & Lee, 2020) relies on the one-stage ATSS architecturebut with a different anchor assignment mechanism and postprocessing of the confidence scores. We obtain RS R-CNN and ATSS from (Oksuz et al., 2023) and PAA from (Chen et al., 2019). All thesedetectors are trained for 36 epochs using multi-scale training data augmentation in which the shorter side ofthe image is resized within the range of for RS R-CNN and ATSS and for PAA. We donot use Soft NMS and Score Voting for the single detectors.",
  "Again, we obtain YOLOv7 and dynamic head from mmdetection (Chen et al., 2019) and use the officialrepository of QueryInst (Fang et al., 2021)": "Furthermore, to improve the SOTA on COCO test-dev, we use two of the most recent and strong publicdetectors, EVA (Fang et al., 2023) and Co-DETR (Zong et al., 2023). EVA (Fang et al., 2023) is a foundationmodel for computer vision that utilises Cascade Mask R-CNN (Cai & Vasconcelos, 2018) to perform objectdetection wheras Co-DETR (Zong et al., 2023) is a recent transformer-based detector. For both of them, wedirectly consider the official repositories and do not change any settings, including the Soft-NMS for EVA(Fang et al., 2023). Rotated Object DetectionFor rotated object detection, we use RTMDet and LSKN as two differentdetectors. We obtain RTMDet again from mmdetection (which is also the official repository for RTMDet)and LSKN from its official repository (Li et al., 2023).",
  "Open-Vocabulary Object DetectionWe use two of the most recent works on vision-language foundationmodels literature, which can be listed as follows:": "Grounding DINO (Liu et al., 2024) is a transformer-based detector. MQ-GLIP from the recent (Xu et al., 2023), an anchor-based detector that introduces multi-modalqueries on top of GLIP (Li et al., 2022b). For both Grounding Dino (Liu et al., 2024) and MQ-GLIP (Xu et al., 2023), we consider the versionsemploying Swin-T (Liu et al., 2021) as the backbone. We do not perform any prompt engineering and do notchange any settings. Furthermore, we directly utilise the official GitHub repositories for both of the models.",
  "Mask R-CNN with ResNet-50, softmax classifier, trained with Seesaw Loss(Wang et al., 2020) but noRFS": "We obtain Vanilla Mask R-CNN and Seesaw Loss from mmdetection. As for Mask R-CNN trained withRS Loss, we use the official repository of RS Loss (Oksuz et al., 2021a) in which it is trained for 12 epochsusing multi-scale training augmentation. The other Mask R-CNN variants also employ multi-scale trainingaugmentation and the Vanilla Mask R-CNN is trained for 12 epochs as well. Differently, Mask R-CNNwith Seesaw Loss is trained for 24 epochs and uses the mask normalization technique proposed in the samepaper(Wang et al., 2020).",
  "D.2.1Further Details and Ablation on Calibration": "We use CA IR and LR to calibrate the models given the final scores in MoCaE, i.e., late calibration. Morespecifically, we use CA IR in all of the experiments unless otherwise explicitly specified. One notable exceptionwhere we use the CA LR instead of IR is the SOTA experiment in Tab. 8 in which we observe that CA LRperforms 0.1 AP compared to CA IR. Please note that, we obtain those calibrators on a held-out validationset consisting of only 500 images. This section presents further experiments to validate these design choices. Validating the CalibratorWhile calibrating the predictive distribution of the classifiers, commonly theaccuracy of the classifier is preserved. However, this might not be the case once the predictive confidenceinstead of the distribution is calibrated, which is the case in the common calibrators for object detection andin our case (Oksuz et al., 2023). This is mainly because the ranking among the detections can change unlessthe calibrator is a monotonically increasing function of the confidence. Therefore, we would ideally expect acalibrator to improve the calibration by at least preserving the accuracy of the detector. To ensure that, weinvestigate LR and IR both CA and Class-wise (CW) on late as well as early calibration in Tab. A.15. Here,we obtain the calibrators on COCO minival and report the calibration error and accuracy on COCO minitest.We observe in the red cells in Tab. A.15 that all calibrators, except CA IR and CA IR, decrease AP especiallyfor early calibration. Furthermore, these calibrators improve LaECE in all cases. These observations onaccuracy and calibration led us to choose CA IR and CA LR while calibrating the single models in MoCaE. 500 Images are Sufficient for calibration in MoCaEEquipped with the aforementioned insights, wethen investigate the sufficient number of images for calibration to enable MoEs using MoCaE. Our aim hereis to determine the cardinality of the held-out validation set to properly calibrate the models, which can helpthe practitioners to avoid reserving redundant data for this held-out validation set. Following the literature(Guo et al., 2017; Oksuz et al., 2023), we obtain calibrators on a held-out validation set (COCO minival)and then report the results on the test set (COCO minitest). Furthermore, we obtain several calibrators onboth early and late calibration settings by using different number of images. Fig. A.6 presents how LaECE,LaACE and LaMCE change when the cardinality of the hold-out validation set changes on three differentdetectors. We observe in general that calibration errors drop significantly even when only 50 images areused to learn the calibrators. Through introducing more images, while we do not observe a notable gainin LaECE (Fig. A.6(a)), LaACE (Fig. A.6(b)) and LaMCE (Fig. A.6(c)) continue to improve especiallyfor early calibration, implying the necessity for these calibration errors. Overall, as we have not observed anotable gain after 500 images, we keep 500 images on the held-out validation set while training the calibrators.A noteworthy point is that 500 images is a small number images compared to large training sets in objectdetection, demonstrating the ease of applicability of using MoCaE. Comparison of Early and Late CalibrationHere, we provide additional insights on early and latecalibration.In Tab. A.16, we can see that while both approaches improve single models, late calibrationperforms slightly better than early calibration consistently. Furthermore, in practical terms, late calibrationis significantly simpler as the number of confidence scores obtained before post-processing is significantlylarger than those obtained after postprocessing (i.e., final detections). To illustrate this more concretely, RSR-CNN outputs 1K proposals for each image as raw detections, thus resulting with a single image containing80K raw confidence scores for the COCO dataset and more than 1M scores for LVIS as each proposal hasa score for each class. In addition, a very large amount of these raw detections do not even overlap withany objects, complicating the problem due to this imbalanced nature of the data. Furthermore, the numberof raw confidence scores is significantly larger for one-stage detectors (ATSS and PAA in our case) as suchdetectors make predictions directly from a very large number of anchors; making early calibration even more",
  "CW LR43.061.347.128.247.454.6CA LR44.062.748.129.048.555.8CA IR44.763.148.929.249.058.2": "impractical for them*. On the other hand, we use only top-100 detections in COCO and top-300 detectionsin LVIS for each image following the evaluation specification of these datasets. Thereby resulting in morepractical scenarios with significantly smaller number of detections for late calibration compared to early.Consequently, considering its slight accuracy gain as well as simplicity, we prefer late calibration over early toobtain MoEs in MoCaE.",
  "MoCaE (RS R-CNN, ATSS, PAA) calibration on train45.463.050.0MoCaE (RS R-CNN, ATSS, PAA) calibration on val45.563.250.0": "Effect of Different Calibration Methods on MoEsWhile we choose CA IR as our calibration methodin MoCaE, here we present how different calibration methods perform in obtaining MoEs. Specifically, weuse late calibration with CA LR and CW LR as these two methods also preserve the accuracy of singlemodels as shown in Tab. A.17. Tab. A.17 presents the results where we can see that CA calibrators performbetter than CW LR. Also, while CA LR obtains on par performance with CA IR while combining ATSS andPAA, it performs worse once RS R-CNN is in the mixture. This might be because the calibration error of CALR is higher than CA IR (for late calibration) in terms of all calibration measures as shown in Tab. A.17). Ablation of MoCaE with Early CalibrationTab. A.18 presents a more detailed version of the Tab. 14included in the paper. In this version, we also include early calibration, which performs similar with latecalibration as shown in Tab. A.18. Can we use training set to calibrate experts in MoCaE?Following the common convention in theclassification literature (Mukhoti et al., 2020; Guo et al., 2017) and the recently-proposed approaches inobject detection (Kuzucu et al., 2024; Oksuz et al., 2023), we obtain the post-hoc calibrators on a held-outvalidation set while using MoCaE. In our experiments, this held-out validation set includes 500 images. Whilethis is usually a quite small dataset to keep for calibration, we still investigate if a subset of the training setcan be used for the purpose of combining detectors. To do so, we randomly sample 500 images from thetraining set of COCO and LVIS, and report the results in Tab. A.19 and Tab. A.20. The main observation inthese tables is that while calibration on a subset of the training set and on a validation set perform almoston-par on COCO, there is a significant performance gap (1.0 AP) in favor of using validation set for LVIS.Please note that, LVIS is a long-tailed dataset with around 1K different classes and it has been shown in theliterature that the models tend to overfit for the datasets with long-tailed examples (Feldman & Zhang, 2020;",
  "Linear, IoUNMS = 0.6544.839.9Gaussian, NMS = 0.2043.740.6Gaussian, NMS = 0.4044.440.8Gaussian, NMS = 0.6044.840.6Gaussian, NMS = 0.8044.740.2Gaussian, NMS = 1.0044.639.9": "Feldman, 2019), implying a larger generalisation gap (i.e., the difference between the test and the trainingerrors) for such datasets. Subsequently, Mukhoti et al. (2020)and Bai et al. (2022) showed that the largergeneralisation gap implies a larger calibration error. Accordingly, the results in Tab. A.20 shows the resultingdrawback of using training sets for calibration compared to using a held-out validation set. Specifically,calibrating on validation set outperforms calibrating on a subset of the training set by 1.0 mask AP. We alsonote that, calibrating on the training set still outperforms Vanilla MoE by 1.5 mask AP, which demonstratesthe importance of the calibration in obtaining an MoE.",
  "D.2.2Sensitivity of MoCaE to Design Choices in Refining NMS": "Refining NMS combines Soft NMS and Score Voting. Specifically, Soft NMS can be linear or gaussian;furthermore both Soft NMS (either linear or gaussian) and Score Voting have hyper-parameters. Here, weinvestigate the sensitivity of MoCaE to such design choices using Soft NMS as an example using RS R-CNN,ATSS and PAA for COCO; and the setting described in Sec. 4 for LVIS. We can easily see in Tab. A.21 thatVanilla MoE does not benefit properly from Soft NMS without calibration. For example, there is no gain forLinear Soft NMS, the performance degrades for the Gaussian Soft NMS on COCO and the gain is only 0.4for LVIS. This is expected as a single hyper-parameter to reconciliate the scores all detectors might not besufficient especially for the Gaussian Soft NMS. On the other hand, after calibration, we consistently seethe gains for our MoCaE: MoCaE benefits slightly on COCO dataset both for linear and gaussian cases;and besides, the gain on LVIS is 1.0 mask AP. This is because, the scores are compatible for each detector",
  "+1.8+0.7+2.6+1.3+1.8+2.7": "after calibration and a single hyperparameter allows Soft NMS to properly adjust the scores from differentdetectors. We choose Linear Soft NMS on COCO resulting in the best results for Vanilla MoE and MoCaE.For LVIS, we use Gaussian Soft NMS with NMS = 0.40 for MoCaE. In a similar way, we validate thehyper-parameter of Score Voting as 0.04.",
  "D.2.3MoCaE is Complementary to Multiscale Testing": "Multiscale test augmentation is another way of improving the detection performance of a detector, whichhas been commonly used in the literature (Oksuz et al., 2020; Zhang et al., 2020b; Liu et al., 2018; Duanet al., 2019). In this section, we show that MoCaE is complementary to multiscale testing. To show this,we first apply multi-scale testing to RS R-CNN, ATSS and PAA as the individual detectors. Specifically,while we use only images with 800x1333 resolution for inference, here we use images with 400x667, 800x1333and 1200x2000 resolutions as three different scales. Tab. A.22 shows that this way of testing improves theperformance of the single models between 0.8 to 1.6 AP. Then, we combine the outputs of these models withmultiscale augmentation using Vanilla MoE, and MoCaE and make two more important observations in thesame Tab. A.22: (i) The performance of MoCaE with multi-scale detectors improves by 0.9 AP compared tothe MoCaE with detectors without test time augmentation; and (ii) MoCaE improves the single best detectorwith multiscale testing by 1.8AP. These observations lead to the conclusion that MoCaE is complementaryto multiscale testing.",
  "D.3.1The Effect of Calibration on DEs": "DEs combine the same models that are trained from different initialization of the parameters. Ideally, theexpectation over the predictive distributions of the components in a DE yields the prediction of the DE.This can be easily obtained for classifiers which predict a categorical distribution over the classes given aninput image. On the other hand, it is not straightforward to use DEs for detectors as there is no clear wayto associate detections from different detectors. As a result, similar to MoCaE, we obtain DEs by usinglate calibration as shown in (a), which turns out to be an effective method. To see that, we firstpresent the single model performance of the five different components comprising DEs in Tab. A.23. Then,from these single detectors, we obtain DEs for PAA with and without calibration using the standard NMS.Tab. A.24 shows that this way of obtaining DEs is effective as the performance increases when the numberof components increases. We observe that increasing the number of components improve the performance",
  "PAA 544.461.748.629.149.059.344.461.748.628.949.059.3": "between 0.1 0.3 AP. On the other hand, as there is no incompatibility among different detectors in a DE,the effect of calibration is not notable for DEs. Still, having observed that PAA with 2 components has aslightly better (0.1 AP) performance once calibrated, in our comparisons we use DEs with calibration.",
  "D.3.2DEs with Less Components and Refining NMS": "In Tab. 6, we compared MoCaE with DEs with 5 components. Note that in this case, the DEs have morecomponents, implying a higher number of parameters compared to our MoCaE with 2 or 3 components. Forthe sake of completeness and provide a more fair comparison to our MoCaE with a maximum of 3 components, Tab. A.25 extends our comparison in Tab. 6 by including (i) the DEs with 3 components and (ii) the DEswith Refining NMS. Tab. A.25 presents that with equal number of components, our MoCaE outperforms thebest DE with 3 components by 1.4 AP (44.1 of PAA 3 vs 45.5 AP of MoCaE). Furthermore, in the casethat Refining NMS is used for MoCaEs, their performance consistenty improves; showing the effectiveness ofour aggregator. Still, the performance of the best DE, i.e., PAA with 5 components and Refining NMS, is 0.5AP lower compared to our MoCaE; demonstrating the effectiveness of our approach.",
  "D.4.1Complementary Nature of MoCaE": "Ideally, an MoE aims to combine the individual components in such a way that they complement each otherin different subsets of the input space to achieve a better performance. Here, we show that this is, in fact,the case also for MoCaE. Similar to our previous analyses, we combine ATSS, PAA and RS R-CNN andreport the results on COCO mini-test. Differently, in order to be able to discuss the results, we utilize 12supercategories such as animal, food, which are already present in COCO dataset. Fig. A.8 shows the resultsin which we can easily observe that different experts prevail on different supercategories. To illustrate, RSR-CNN (Oksuz et al., 2021a) is the best single model in the appliance supercategory, ATSS (Zhang et al.,2020b) is the best single model in the kitchen supercategory and PAA (Kim & Lee, 2020) is the best in theindoor supercategory. A noteworthy point is thatMoCaE performs better than any of its components in allof the supercategories, achieving notable improvements over the entirety of the input space. This effectively",
  "D.4.2The Contribution of MoCaE on Different Performance Aspects": "We now analyse what performance aspects, among localisation, recall and precision, are affected by MoCaEby expliting three different analyses tools from the detection literature. First, we use TIDE (Bolya et al.,2020) that defines oracle APs as the APs obtained when FP and false-negative (FN) errors are completelymitigated. Then, the difference between the oracle and actual AP correspond to the error of a detector for aspecific performance aspect. More specifically, while obtaining Oracle FP AP, the FP detections are simplyremoved from the final detection set. As for Oracle FN AP, the FN objects are removed from the dataset.Fig. A.7(a) shows that MoCaE variants perform similar to the single detectors in terms of FP Error whilethey clearly outperform them on FN Error. This indicates that one of the contributions of MoE is to find theobjects that are not detected by at least one of the individual detector as illustrated on and Tab. 2.However, TIDE analysis does not provide insight on the localisation quality which is mainly targeted byRefining NMS. Therefore, in our second analysis we exploit AR defined as the average of the recall valuesover 10 IoUs from 0.50 to 0.95. Aligned with the observation in TIDE analysis, Fig. A.7(b) presents thatMoCaE improves AR of the single detectors. Furthermore, the performance gain mainly originates from theimprovement in small and medium objects as MoCaE does not improve the performance on large objectsnotably. This indicates that the resulting MoE is especially stronger than single detectors in more challengingobject categories. Using Refining NMS further boosts the AR performance as it improves the localisationperformance, which is critical for recalls with higher IoUs. To investigate the benefit of MoE in practicaluse-cases, we finally conduct an LRP analysis (Oksuz et al., 2021c; 2018) in Fig. A.7(c). In this figure, oLRPFP, FN and Loc correspond to the components Optimal LRP Error defined as 1-Precision, 1-Recall and theaverage IoU Error of TP detections. Aligned with our previous analysis, MoE mainly decreases the recallerror and MoCaE with Refining NMS mainly contributes to the oLRP Loc component, outperforming allindividual models in the end. These three different analyses confirm that MoCaE with NMS mainly decreasesthe recall error of the detector and using Refining NMS contributes to the localisation error.",
  "(b) Early Calibrated": "Figure A.9: The effect of background removal threshold on AP and NMS processing time for (a) uncalibratedand (b) early calibrated detectors on COCO. The area of the dots are proportional to the NMS processingtime of the detectors. This threshold is typically set to 0.05, in which case PAA and RS R-CNN have largeNMS processing time in (a). Once uncalibrated, the detectors follow different trends and are sensitive tothe threshold in terms of AP and NMS processing time. In (b), early calibration (i) aligns the detector byreducing this sensitivity, and (ii) allows using 0.05 by both maximizing the AP and reducing NMS processingtime for the over-confident PAA and RS R-CNN. F R-CNN refers to Faster R-CNN. Table A.26: The values used in Fig. A.9. Early calibration regularizes the behaviour of the detectors byreducing their sensitivity to background removal threshold with respect to AP and NMS time. NMS time ismeasured in terms of ms using a single Nvidia 1080ti GPU.",
  "D.5A Use Case for Early Calibration: Reducing the Sensitivity to Background Removal Threshold": "Here, we investigate an additional use-case of early calibration in which it reduces the sensitivity of thedetectors to background removal threshold in terms of both AP and efficiency. As AP provably benefits frommore detections (Oksuz et al., 2023), detectors prefer a small background removal threshold as the first step ofpost-processing ((b)). To illustrate, 0.05 is the common choice for COCO (Zhang et al., 2020b; Kim &Lee, 2020; Ren et al., 2017) and it is as low as 104 for LVIS (Chen et al., 2019; Gupta et al., 2019). Whilethis convention is preferred by AP, it can easily increase NMS processing time especially for over-confidentdetectors. This is because, for such detectors, the background removal step accepts redundant true-negatives(TNs), which should have been rejected. Hence, due to this large number of redundant TNs propagated tothe NMS, NMS processing time significantly increases. To illustrate on PAA, which uses a threshold of 0.05",
  "MoEsVanilla MoE42.588.739.918.173.629.219.682.1MoCaE42.987.640.117.173.529.819.181.8": "for COCO, NMS takes 29.2 ms/image on a Nvidia 1080Ti GPU, while it only takes 0.6 ms/image for ATSSand Faster R-CNN (F R-CNN). This difference among the detectors can easily be noticed by comparingthe areas of the dots at 0.05 in Fig. A.9(a)*. Specifically, we observed for PAA that 45K detections arepropagated to the NMS per image on average. After early calibration, this number of detections from thesame threshold reduces to 2K per image, which now enables NMS to take only 0.8 ms/image as ideallyexpected. Fig. A.9(b) presents that NMS takes consistently between 0.6 to 0.8 ms/image for all detectors asthe behaviour of the detectors are aligned.",
  "MoEsVanilla MoE42.7MoCaE - Ours45.6+3.6": "Details of the SAOD taskSelf-aware object detec-tion task (SAOD) (Oksuz et al., 2023) provides a compre-hensive framework to evaluate the robustness of objectdetectors. Specifically, the performance aspects that arejointly considered in this task are out of distribution de-tection, calibration, domain shift as well as the accuracy.Specifically, SAOD evaluates an object detector in termsof the following criteria:",
  "Balanced Accuracy (BA) measures the OOD performance as the harmonic mean of TPR and TNRin this binary classification problem": "LaECE measures the calibration performance, as discussed in App A. In-Distribution Quality (IDQ) combines accuracy and calibration performance as the harmonicmean of 1-LRP (Oksuz et al., 2018) and 1-LaECE on in-distribution data DID. Analogously, fordomain-shifted data, T(DID), IDQT is used. Finally, Distribution-Awareness Quality (DAQ) as the main performance measure of the task, unifiesthese measures. Specifically, DAQ is a higher the better measure, defined as the harmonic mean ofBA, IDQ and IDQT. Implementation DetailsBased on the definition of this task, the authors also propose an algorithm toconvert any detector to a self-aware one*. We follow the proposed algorithm to make RS R-CNN, ATSS andPAA, as well as Vanilla MoE and MoCaE self-aware. We use the General Object Detection setting in (Oksuzet al., 2023) as our models are trained on COCO, aligned with this dataset. We utilise the official SAODcode throughout our experiments by keeping all the settings. Discussion of the ResultsThe results are presented in Tab. A.27 in which MoCaE improves DAQmeasure, the main performance measures of this task, up to +2 points compared to the best single modelwhile also showing notable improvements in terms of the rest of the measures. Furthermore, MoCaE alsooutperforms Vanilla MoE. This highlights that the MoCaE is more reliable than its counterparts in termsof the reliablity aspects considered within the SAOD (Oksuz et al., 2023) framework. When we examinethe individual robustness aspects, we can easily see that MoCaE outperforms all of the single detectors andVanilla MoE in terms of calibration and accuracy both on in-distribution and domain-shifted data. On theother hand, we observe that the OOD performance drops from 1 BA compared to the best single model.In the following, we further discuss why this is the case and provide more insight. Is Lower OOD Performance a Pitfall of MoCaE or its Strength?MoCaE combines individualdetectors to ideally benefit from the strength of each detector in the mixture. This commonly manifests itselfto increase the recall performance of the individual components, i.e. Average Recall (AR), as we presented inTab. 2 while motivating MoCaE as well as in Fig. A.7. AR consistently increases in such cases due to thefact that the detections of the individual experts are combined properly in in-distribution images. Now, consider an alternative case, in which the input image is out-of-distribution (OOD), on which eachdetection is a FP as an OOD image does not contain an in-distribution object (as defined by (Oksuz et al.,2023)). In this specific case, combining the detectors properly might result in more FPs easily. To illustrate ona toy example, assume there are two images X1 and X2 and k represents the number of maximum detectionsin an image, which is typically k = 100. Also assume that, an overconfident detector yields a high-confidentFP on an out-of-distribution (OOD) image X1 with k 1 lower confident detections. As for X2 this detectordoes not have a high-confident detection. This detector accepts the OOD image X1 and rejects X2. A secondbut an underconfident detector yields very low confidence detections on X1 and one relatively confidentdetection on X2. In contrary to X1, this detector accepts the OOD image X2 and rejects X1. When wecombine these two detectors without calibration, the resulting Vanilla MoE will mimic the overconfidentdetector, potentially rejecting X2 and accepting X1. Unfortunately, calibrating these two detectors in theform of MoCaE will have a confident detection in each image, potentially resulting in the acceptance of bothOOD images. This is why Vanilla MoE mimics the most confident detector in Tab. A.27. In this specificcase, the most confident detector, RS R-CNN has the largest BA, and consequently Vanilla MoE outperformsMoCaE in terms of OOD performance.",
  "MoEs ofVanilla MoE60.176.366.044.165.176.6N/AN/AAll SingleMoCaE61.778.368.148.766.976.3N/AN/AModelsOracle MoE86.797.494.178.089.895.5N/AN/A": "Following from the comparison of OOD performances of MoEs, the question arises why detectors generatehigh confidence detections on OOD images and whether there is a benefit of having them. More specifically,we conjecture that this is mainly a result of object discovery as the DOOD merely contains the classes thatare not exactly present in DID, in which objects of semantically similar classes can still exist across thetwo. To support this claim, we design an experiment on the same OOD split. In this experiment, we usethe bounding box annotations already present in the OOD split and check whether which of the modelsfind the most number of objects. We do not distinguish among the classes as all the classes in OOD split(SinObj110K-OOD) pertains to the unknown class for all of the models we use. We report AR in Tab. A.28to evaluate the models with respect to their object discovery characteristics. The results show that MoCaEoutperforms all individual models and Vanilla MoE with a significant margin in terms of AR; demonstratingthat it finds more objects compared to the other models. This is because, unsurprisingly, the confidentdetections of individual models correspond to the objects and combining them after calibration in MoCaEresults in finding more unknown objects. As a result, while MoCaE does not perform the best in terms ofimage-level OOD detection, it is, in fact, a better alternative for object discovery.",
  "D.7The Effect of Theorem 1 using an Oracle MoE": "We proved in Theorem 1 that the calibration target that we designed in Eq. 4 provides the optimal AP undercertain assumptions on post-processing. Here, we show that the assumptions are, in fact, not very strictand significantly higher AP values can easily be observed once the individual detectors in the mixture areperfectly calibrated as suggested in Theorem 1. To do so, we design an Oracle MoE, in which we replace theconfidence score of the detections by the corresponding calibration target in Eq. 4 such that the detectors arenow perfectly calibrated. In Oracle MoE, we use the standard NMS to keep its design simple. Tab. A.29presents the results of this Oracle MoE combining different object detectors on COCO mini-test. Whenonly two detectors, EVA and Co-DETR, are combined using Oracle MoE, we observe that the gain of theOracle MoE compared to the best single model is more than 15AP, which is very significant. Furthermore,as the number of components increases, the performance of Oracle MoE consistently increases up to 86.7AP and 97.4 AP50 when all of the eight detectors (with very different performances) are combined. The",
  "D.8Limitations of MoCaE": "On the importance of performance difference among the detectorsAs discussed in Sec. 4.4 in thepaper, the main limitation of MoCaE (as well as Vanilla MoE) is that it benefits from combining similarlyperforming detectors. This is presented in Tab. A.29 in which we can see that combining low-performingdetectors (e.g., RS R-CNN or YOLOv7) with EVA and Co-DETR decreases the performance of MoE to anAP less than the best single detector. On the other hand, as just presented in App. D.7, this is not thecase for Oracle MoE. This suggests that while the calibration decreases the LaECE of the single detectorssignificantly (Tab. A.29), they are still far from being well-calibrated to construct an accurate MoE. This isbecause, we keep the capacity of the calibrators low, that is Class-agnostic LR and IR are monotonicallyincreasing function of the predicted confidence; thereby preserving the ranking among the detections andthe AP of each detector. On the other hand, this capacity limitation might not result in better MoEs thanthe individual components once their performance gap is high. Furthermore, the Oracle MoE consistentlyimproves upon the individual detectors even the performance gap is significantly high as shown in Tab. A.29.This demonstrates the importance of the calibration quality in obtaining an accurate MoE as well as thepotential to improve the object detectors using MoEs, which is an open problem. MoCaE requires more resource than single modelsAnother limitation of MoCaE stems from usingmultiple models during inference, which is also the case for DEs. Still, as each detector can process the inputin parallel, the overhead introduced by calibration would be negligible when a separate GPU is allocatedfor each detector. As an example, on an Intel i7 3.60GHz CPU, obtaining the isotonic regression calibratorand inference take 12.0 ms and 0.30 ms/image respectively. This inference cost is negligible as it increasesthe inference time of ATSS only by 0.2% and EVA by 0.005%. Besides, comparing (b) with (a),we showed that calibration balances the contribution of the detectors to MoE. However, PAA, as the mostaccurate detector. has still the lowest contribution with %25.61 in (b). This is because LaECE aftercalibration in Tab. 1 is still non-zero for the detectors. Therefore, better calibration methods could give riseto more accurate MoEs.",
  "D.9Full Version of the Tables Pruned in the Paper": "Tab. A.30 presents the detailed results on LVIS dataset for instance segmentation; Tab. A.31 includes theperformance of all classes for DOTA dataset; Tab. A.32 shows the performance over different object scales;and Tab. A.33 includes COCO minitest results for common object detectors as well as their main differencesin terms of pretraining data and the backbone. These tables are excluded from the main paper due to thespace limitation.",
  "D.10Reliability Diagrams of the Utilized Models": "For further insights, we include the reliability diagrams of the object detectors and instance segmentationmodels in Fig. A.10-Fig. A.14. The figures show that the calibration performance of the detectors improvesafter using post-hoc calibrators, enabling us to construct an effective MoE using MoCaE. Here, we use500 images to calibrate the detectors. This is the reason of the small difference between LaECE values ofFig. A.10 and Tab. 1 which uses all 2500 images in COCO minival for calibration."
}