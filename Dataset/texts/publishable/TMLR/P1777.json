{
  "The Responsible Foundation Model Development Cheatsheet:A Review of Tools & Resources": "Shayne Longpre, MITStella Biderman,EleutherAIAlon Albalak,UC Santa Barbara, SynthLabsHailey Schoelkopf,EleutherAIDaniel McDuff,University of WashingtonSayash Kapoor,Princeton UniversityKevin Klyman,Stanford University, Harvard UniversityKyle Lo,Allen Institute for AIGabriel Ilharco,University of WashingtonNay San,Stanford UniversityMaribeth Rauh,Google DeepMindAviya Skowron,EleutherAIBertie Vidgen,ML Commons, Contextual AILaura Weidinger,Google DeepMindArvind Narayanan,Princeton UniversityVictor Sanh,HuggingFaceDavid Adelani,University College London, MasakhanePercy Liang,Stanford UniversityRishi Bommasani,Stanford UniversityPeter Henderson,Princeton UniversitySasha Luccioni,HuggingFaceYacine Jernite,HuggingFaceLuca Soldaini,Allen Institute for AIReviewed on OpenReview: https: // openreview. net/ forum? id= tH1dQH20eZ",
  "Abstract": "Foundation model development attracts a rapidly expanding body of contributors, scien-tists, and applications. To help shape responsible development practices, we introduce theFoundation Model Development Cheatsheet: a growing collection of 250+ tools and re-sources spanning text, vision, and speech modalities. We draw on a large body of priorwork to survey resources (e.g. software, documentation, frameworks, guides, and practicaltools) that support informed data selection, processing, and understanding, precise andlimitation-aware artifact documentation, efficient model training, advance awareness of theenvironmental impact from training, careful model evaluation of capabilities, risks, andclaims, as well as responsible model release, licensing and deployment practices. The processof curating this list, enabled us to review the AI development ecosystem, revealing whattools are critically missing, misused, or over-used in existing practices. We find that (i) toolsfor data sourcing, model evaluation, and monitoring are critically under-serving ethical andreal-world needs, (ii) evaluations for model safety, capabilities, and environmental impactall lack reproducibility and transparency, (iii) text and particularly English-centric analysescontinue to dominate over multilingual and multi-modal analyses, and (iv) evaluation ofsystems, rather than just models, is needed for capabilities to be assessed in context.",
  "Introduction": "As the capabilities (stn et al., 2024; Team et al., 2023; Gomez, 2024; Anthropic, 2024a; Radford et al.,2023; Brooks et al., 2024) and market prospects (Vipra & Korinek, 2023; McElheran et al., 2024) of artificialintelligence have quickly expanded, so have the communities of developers, scientists, and contributors whobuild foundation models (Bommasani et al., 2021). The fields growth has spurred widespread adoptionof many tools and resources used to build, deploy, evaluate, and govern large foundation models (FMs).However, these nascent practices are often immature. Many outstanding resources are neglected, in partfor lack of discoverability, or awareness of good practices. To address these gaps, we conduct a focusedsurvey, not of scientific literature (which already exists for many FM development topics (Albalak et al.,2024; Zhao et al., 2023a; Chang et al., 2023)), but of resources for FM development such as datasets, softwarepackages, documentation, guides, frameworks, and practical tools. In particular, this resource curation istailored to responsible development practices for newer, smaller, or mid-sized development teams. Large FMdevelopment organizations, such as Google, OpenAI, or Meta, with substantial user bases, should adhere tomore rigorous and product-specific best practices than outlined in this review. We release the FoundationModel Development Cheatsheet, the repository of annotated tools for text, speech, and vision models, andopen it for public contributions. For each phase of model development, our contributions are summarized as (i) a survey of relevant tools andresources, (ii) a synthesis of the literatures recommended practices and use of those tools, and (iii) a reviewof the limitations and omissions of existing resources. The cheatsheet serves as a succinct guide of the surveyand recommended practices, prepared by foundation model developers for foundation model developers.The intended audience is a range of foundation model developers, including academic researchers, startupcompanies, research labs, who are pretraining from scratch, or simply finetuning, big and small. Our survey and recommendations hope to bring wide attention to tools across several phases of development.First, we suggest resources that support informed data selection, processing, and understanding ( 3 and 4).Data prepared without sufficient due diligence can lead to unintended consequences (e.g. risks to privacy,copyright, or generating sensitive content), marginalization (e.g. by inadvertently filtering out certaindistributions), or unexpected model behaviors (e.g. train/test overlap or security vulnerabilities). Next wesurvey resources for precise and limitation-aware artifact documentation (). When new datasets arereleased, setting their governance standards early will avoid misuse later. Foundation model training can befinancially and environmentally expensive. We aggregate resources for efficient model training ()and estimating a models scaling behavior and environmental impact (). Advance awareness of thesequantities can inform more efficient training practices. For once models are trained, we provide evaluationframeworks, taxonomies of risk, and benchmarks for a variety of evaluation criteria (). Best practicessuggest models should be evaluated for their intended uses, as well as some unforeseen misuses or harms.Developers should design naturalistic evaluations for these settings rather than relying on available butpoorly fitting tools (Biderman et al., 2024b; Liao & Xiao, 2023). And our evaluation frameworks suggestevaluation metrics should be contextualized, to avoid over-claiming or misunderstanding the limitationsof the reported numbers. Lastly, our survey informs responsible model release and deployment practices(), so developers can make informed selections of licenses and release mechanisms, to addressmisuse risks. Beyond the survey of resources, we review the existing ecosystem of tools and resources. For each segment ofmodel development, we examine the limitations, omissions, and opportunities for improvement of existingtooling and common practices. Summarized in , we find: Tools for data sourcing, model evaluation, and monitoring are critically under-serving responsible de-velopment needs and real-world needs. For instance, they often fail to have sufficient documentation,imitate real use cases, or accurately reflect licensing permissions.",
  "Methodology & Guidelines": "We develop the following methodology to guide the collection of these tools and resources, as well as ouranalysis. First, weve divided the model development pipeline into several phases, illustrated in .Despite the visual representation, we acknowledge these phases are frequently interrelated rather thansequential. We also include categories that have been identified by existing scholarship as necessary toresponsible development, even though they are frequently omitted from development pipelines: such asdocumentation, environmental impact estimation, or risks and harms evaluation. Next, authors are allocatedto these phases based on their areas of expertise, or to modalities (text, vision, or speech) across a few phasesof development. Each author surveys the literature in their segment of model development to find a mix of(a) relevant tools, in the form of repositories, APIs, or interfaces, (b) scientific literature that directly surveysor guides development decisions, and (c) frameworks or standards for development (such as documentationor risk taxonomies). Due to the breadth of categories, the literature survey is inevitably non-exhaustive, butreflects a dedicated search for the most prominent tools in each area. Certain phases of development, suchas model training, have thousands of available repositories, so we curate a criteria for inclusion (outlinedbelow) that prioritizes certain qualities: popularity, usefulness, and advancing responsible practices. Criteria for Inclusion.These principles for inclusion, described below, are incomplete and subjective, butwe still believe sufficiently rigorous to make for a thorough and useful compilation of resources. The resourcesare selected based on a literature review for each phase of foundation model development. Inclusion ispredicated on a series of considerations, including: the popularity of the tool on Hugging Face or GitHub,the perceived helpfulness as a development tool, the extent and quality of the documentation, the insightsbrought to the development process, and, in some cases, the lack of awareness a useful resource has receivedin the AI community. For an example of this last consideration, in .2 we try to include moreFinetuning Data Catalogs for lower resource languages, that often receive less attention than ones featuredmore prominently on Hugging Faces Dataset Hub. Rather than sharing primarily academic literature as inmost surveys, we focus on tools, such as data catalogs, search/analysis tools, evaluation repositories, and,selectively, literature that summarizes, surveys or guides important development decisions. Further, we hopeto make the coverage more comprehensive with an open call for community contributions. Scope & Limitations.Weve compiled resources, tools, and papers that guide model development, andwhich we believe will be especially helpful to nascent (and often experienced) foundation model developers.However, this guide is far from exhaustiveand heres what to consider when using it:",
  "Published in Transactions on Machine Learning Research (12/2024)": "Fangyuan Xu, Yixiao Song, Mohit Iyyer, and Eunsol Choi. A critical evaluation of evaluations for long-formquestion answering. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.), Proceedings of the61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 32253245,Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.181.URL",
  "Pretraining Data Sources": "Pretraining corpora consist of millions of pieces of content, from documents, images, videos, or speech record-ings, often scraped directly from the web. Model pretraining represents the fundamental step in instillingfoundation models with their abilities to represent syntax, grammar, reasoning, and world knowledge (Devlinet al., 2018; Brown et al., 2020; Chowdhery et al., 2023). Consequently, it is important to carefully curate thedata composition, including the mix of sources, characteristics, and preprocessing decisions (Longpre et al.,2023b). However, the vast scale of this content often means its is shallowly documented and understood,despite community efforts to unpack it (Dodge et al., 2021; Elazar et al., 2023). We highlight a few of the most popular pretraining corpora which have accumulated deeper documentationand analysis. In the text domain, web scrapes from common crawl (commoncrawl.org), or OSCAR ( (Surez et al., 2019; Laippala et al., 2022) are the base ingredient for most pretraining",
  "In this section we critically review the current state of resources for data sourcing, from our survey": "The community would benefit from more accurate and comprehensive licensing, provenance, and creatorconsent information for existing datasets. Many datasets tend to be under-documented (Bandy & Vincent,2021; Sambasivan et al., 2021), or erroneously documented (Longpre et al., 2023b), with 65% of HuggingFacedataset licenses either omitted or incorrectly labelled. This is especially true of large data collections that havere-packaged and sometimes re-licensed hundreds of diverse datasets, each with different documentationstandards (Longpre et al., 2023a; Sanh et al., 2021). The tools used to discover, select, and verify the datasetproperties are under-developed, especially with respect to concerns of creator consent, copyright infringement,and related terms of use. For creator consent, initial opt-in/opt-out tooling has yet to be widely adopted. While",
  "Data Preparation Best Practices": "Tools for searching and analysing can help developers better understand their data, and there-fore understand how their model will behave; an important, but often overlooked, step of modeldevelopment. Data cleaning and filtering can have an immense impact on the model characteristics, thoughthere is not a one size fits all recommendation. The references provide filtering suggestions based onthe application and communities the model is intended to serve. When training a model on data from multiple sources/domains, the quantity of data seen fromeach domain (data mixing) can have a significant impact on downstream performance. It is commonpractice to upweight domains of high-quality data; data that is known to be written by humans andhas likely gone through an editing process such as Wikipedia and books. However, data mixing is anactive area of research and best practices are still being developed.",
  "Data Search, Analysis, and Exploration": "A critical step to understanding a dataset is to explore and analyze what it contains. In particular, exploringtraining datasets with search and analysis tools can help practitioners develop a nuanced intuition for whatexists in the data, and therefore can help to predict what behaviors the model will exhibit. Tools for search, analysis, and exploration can take a few forms. Some tools are aimed at understanding thehigh-level statistics of a dataset such as the length of inputs, frequency of specific n-grams, the languages inthe corpus, possible biases, or the existence of undesirable content. For example, tools such as WIMBD (Elazaret al., 2023) and Infini-gram (Liu et al., 2024b) allow users to perform n-gram searches through commonlyused pretraining datasets, and provide starting points for building a search index over any arbitrary dataset.The ROOTS search tool 4 (Piktus et al., 2023) additionally allows users to search with fuzzy n-grams over theROOTS corpus, and similarly, the clip-retrieval tool 5(Beaumont, 2022) allows users to search for nearestneighbor images and text from a multimodal corpus (e.g. LAION-5B (Schuhmann et al., 2022). Furthermore,the HuggingFace Data measurements tool6 gives users access to statistics such as the most common n-grams,lengths of data points, and distribution over labels for a number of pretraining and finetuning datasets. Yetmore tools exist to explore the data manually and including the Data Provenance Explorer (Longpre et al.,2023b) for text datasets, Googles Know Your Data tool 7 for vision datasets and NVIDIAs Speech DataExplorer 8 for speech datasets. In most cases, it is highly recommended to explore datasets from the perspective of high-level statistics aswell as getting to know the data by looking at individual data points. For instance, text data can have a widedistribution of lengths, topics, tones, formats, licenses, and even diction, and understanding each of thesedimensions will require a different tool. We recommend that developers use the many available tools tosearch and analyze their datasets.",
  "Data Cleaning, Filtering, and Mixing": "Once the contents of a dataset are understood, the next step is to clean and filter the dataset to adjust thedatasets distribution towards desirable content. Filtering and cleaning do so by removing unwanted data fromthe dataset. They can improve training efficiency as well as ensure that data has desirable properties, including:high information content, desired languages, low toxicity, and minimal personally identifiable information.Data mixing is another important component of data preparation, where the mixture proportions of datadomains (e.g. scientific articles, GitHub, and books) have been shown to dramatically affect downstreamperformance (Gao et al., 2020; Xie et al., 2023a; Albalak et al., 2023). A first step for filtering text data is by language, where there is a plethora of tools including langdetect 9,cld3 10, OpenLID 11 (Burchell et al., 2023), GlotLID 12 (Kargaran et al., 2023), and FastText 13 (Grave et al.,2018). The majority of modern language identification methods have been built on top of the FastTextmodel used in the CCNet pipeline (Wenzek et al., 2020). In addition to filtering by language, datasets arecommonly cleaned using heuristics (e.g. remove documents with fewer than 5 words, or remove lines thatstart with \"sign-in\") which have been implemented through a number of different tools including the Dolmatoolkit 14 (Soldaini et al., 2023), Lilac 15, and DataTrove 16(Penedo et al., 2024), as well as DataComp 17 (Gadreet al., 2023) for image-text pairs. While heuristic filtering methods can remove significant quantities of data,they can be brittle. Model-based methods (e.g. remove data which is dissimilar to a known high-qualitycorpus or possibly toxic content) such as DSIR 18 (Xie et al., 2023b) and Detoxify 19 (Hanu & Unitary, 2020)can be used as additional filtering that allow for much more flexibility than heuristics. In addition to cleaningand filtering, mixing is another component of dataset design which requires careful consideration. Manydataset mixing ratios have been determined by heuristics and human judgement. For example, the Pile (Gaoet al., 2020) and Llama (Touvron et al., 2023) upweight domains that have likely gone through an editingprocess, such as books and Wikipedia articles. Tools for automated data mixing are, as of writing, very limited.However, academic research projects such as DoReMi 20 (Xie et al., 2023a) and Online Data Mixing 21 (Albalaket al., 2023) provide GitHub repositories that can be repurposed for new datasets. Cleaning, filtering, and mixing are crucial components of designing an appropriate dataset, but due to thevast space of possible filters and downstream uses of ML models there is no one-size-fits-all recommendation.Practitioners looking to clean and filter their datasets should first use the search, analysis, and explorationtools to determine how to design appropriate filters, and iteratively improve the filtering. For more detailsand recommendations on cleaning, filtering, and mixing, see the recent survey by Albalak et al. (2024).",
  "Data Deduplication": "Data deduplication is an important preprocessing step where duplicated documents, or chunks within adocument, are removed from the dataset. Removing duplicates can reduce the likelihood of memorizingundesirable pieces of information such as boilerplate text, copyrighted data, and personally identifiableinformation. Additionally, removing duplicated data improves training efficiency by reducing the totaldataset size. Deduplication generally relies on one of four methods: URL matching, hashing methods, string metrics,or model representations, which can classify data as either exact or fuzzy matches. We suggest using the",
  "Data Decontamination": "Data decontamination is the process of removing evaluation data from the training dataset. This importantstep in data preprocessing ensures the integrity of model evaluation, ensuring that metrics are reliable andnot misleading. Prior to training a model on a dataset, it is important to decontaminate that dataset from the desiredevaluation datasets. BigCode 25 and Carper AI 26 both implement contamination detection through the useof MinHashLSH, which can be used to detect contamination prior to training a model. One concern withsome modern models is that the model developers may not disclose their training data, so methods and toolshave been developed to determine whether a model was trained on a specific dataset. For example canarystrings, unique sequences of characters, can be included in training datasets, and Jagielski (2023) explainhow to interpret canary exposure, which can identify whether a model was trained on the specified dataset.One example of canary strings can be found in the BIG-bench dataset. 27 In addition to canary strings, Shiet al. (2023) propose Min-K% probability, a method for finding possible contamination. 28 One important note for practitioners who are performing decontamination prior to training is to see datadeduplication methods for inspiration. For example, data deduplication methods that use exact matching(e.g. Bloom filters from Dolma) are also good candidates for decontamination.",
  "Data Auditing": "Auditing datasets is an essential component of dataset design. You should always spend a substantial amountof time reading through your dataset, ideally at many stages of the dataset design process. Many datasetshave problems specifically because the authors did not do sufficient auditing before releasing them. Thetools outlined in the data search, analysis, & exploration section ae typically sufficient to track the evolutionof a dataset as its being created. However, there are also tools that can be used to audit previously createddatasets. The Data Provenance Initiative 29 (Longpre et al., 2023b) is a good resource that documents the source,license, creator, and other metadata for over 1,800 text finetuning datasets. The Have I Been Trained? 30 toolcan assist in finding and detecting data within LAION datasets. See the blog post by Jernite (2023) for moredetails on auditing datasets.",
  "Data Documentation": "When releasing new data resources with a model, it is important to thoroughly document the data (Bender& Friedman, 2018; Holland et al., 2020; Gebru et al., 2021; Bommasani et al., 2024). Documentation allowsusers to understand its intended uses, legal restrictions, attribution, relevant contents, privacy concerns, andother limitations. An example of how to describe and document data governance decisions can be found inthe BLOOM projects report (Jernite et al., 2022). The StackV2 (Lozhkov et al., 2024) is another example of acarefully curated and well documented dataset. Data documentation can also be a way to empower modeltrainers and downstream users of AI systems to It is common for datasets to be widely used by practitionerswho may be unaware of undesirable properties (David, 2023). While many data documentation standardshave been proposed, their adoption has been uneven, or when crowdsourced, as with Hugging Face Datasets,they may contain errors and omissions (Lhoest et al., 2021a; Longpre et al., 2023b).",
  "Data Governance": "Releasing all datasets involved in the development of a Foundation Model, including pretraining, fine-tuning,and evaluation data, can facilitate external scrutiny and support further research. However, releasing andhosting the data as it was used may not always be an option, especially when it includes data with externalrights-holders; e.g., when data subjects privacy, intellectual property, or other rights need to be taken intoaccount. Proper data governance practices can be required at the curation and release stages to account forthese rights. In some jurisdictions, projects may be required to start with a Data Management Plan that requires developersto ensure that the data collection has a sufficient legal basis, follows principles of data minimization, andallows data subject to have sufficient visibility into and control over their representation in a dataset (CNILresource sheet). Data curation steps to that end can include respecting opt-out preference signals (Spawning,HaveIBeenTrained), or applying pseudonymization or PII redaction (BigCode Governance card). Once a dataset is released, it can be made available either broadly or with access control based on researchneeds (ROOTS, BigCode PII training dataset). Developers can also enable data subjects to ask for removalfrom the hosted version of the dataset by providing a contact address (OSCAR, PAraCrawl), possibly",
  "Review": "A shift away from evaluating models, and towards evaluating systems. Existing evaluation practices andreporting often focus on probing individual models, in unconstrained settings. However, deployed systemsoften operate in the context of multiple interactive models, moderation endpoints, sophisticated decodingstrategies, and rule-based constraints, that make up a system. More pointedly, for dual-use systems, thecontext of use outside the system is what defines the presence of harm: Narayanan & Kapoor (2024) arguethat defenses against misuse must primarily be located outside models. Prior work has emphasized theimportance of the system (Dobbe, 2022) and context (Raji & Dobbe, 2023) in diagnosing and resolving AIsafety challenges. As a result, efforts to evaluate models alone are inherently limited, both in their findings,and informing effective changes. Safety problems in particular are better addressed by evaluations thatconsider the context (e.g. attack vector, deployed use setting), as well as the interactions between elements ofa foundation model system (of which only one is a the model itself). However, these types of evaluations canbe difficult in the absence of transparency into the components of deployed systems (Bommasani et al., 2023).Recent work has even shown that independent researchers can face significant obstacles in fairly evaluatingproprietary systems (Longpre et al., 2024a). A shift away from evaluating on static toxicity/safety benchmarks, and towards evaluating on real-world,dynamically changing naturalistic observations, as well as human-interaction studies. Existing widely usedmodel risk and safety evaluations, such as RealToxicityPrompts (Gehman et al., 2020b), or bias benchmarkssuch as BBQ (Parrish et al., 2022), BOLD (Dhamala et al., 2021), and HolisticBias (Smith et al., 2022a),have been effective at testing models superficial bias tendencies. However, these evaluations are static, notgrounded in real-world contexts, and pay no heed to the human-interaction elementi.e. the actual systemusers. These limitations suggest several dimensions of improvement for future safety benchmarks: Naturalistic observations Collecting natural interactions with users enable researchers to identifyrealistic tasks and prompts, to simulate real harms. For instance, WildChat (Zhao et al., 2023b)collects voluntary user interactions with OpenAI systems through proxy interfaceshowever thesedatasets may be heavily skewed by the types of users that adopt it. Domain expert designed tasks LegalBench (Guha et al., 2024) offers an alternative naturalisticdesign, whereby evaluations are hand-crafted by the practitioners (in this case, legal professionals)in the field of evaluation. These benchmarks distinguish themselves from existing evaluation suitesin their relevance to natural, real-world usage. Human-interaction studies Most evaluations are non-interactivethey target the model, withoutreal-world users. This form of analysis can identify model flaws, but falls short of investigating theactual affects, harms, and interaction patterns on users. Lee et al. (2023a) proposes a framework toevaluate interactive user experience, without which notions of harm and safety are under-developed.Le Ferrand et al. (2022) illustrates the importance of interactive evaluation, particularly on non-Western users.40 These three evaluation dimensions enable more realistic studies of user interaction with foundation modelsystems. Collecting naturalistic or interactive data can also provide continuous and dynamic data sources,which are particularly beneficial when there is rapid model development (or the habit of training on priorevaluation sets). More generally, research into dynamic and evolving benchmarks, that test beyond thetraining set distribution, are an important research direction (Yu et al., 2023; Kiela et al., 2021). A shift away from reporting evaluations, to releasing reproducible evaluation scripts. There are dozens ofchoices that affect the results of an evaluation (Anthropic, 2023). Some choices include, but are not limited to:",
  "Developers should be thoughtful about the effects of train-time decisions and be aware of thetrade-offs and potential downstream effects prior to training": "Due to the large economic and environmental costs incurred during model training, makingappropriate use of training best practices and efficiency techniques is important in order to not wastecomputational or energy resources needlessly. Thousands of tools and resources have been developed for model training. While it is not feasible tocomprehensively list them, we focus our efforts on a subset of tools that are well documented, emphasizecomputational efficiency, or educational resources. For a more comprehensive list of training tools, we pointthe reader to surveys, such as the survey of foundation model training and serving systems (Zhou et al.,2024), the survey of efficient federated learning methods (Woisetschlger et al., 2024), or this comprehensivesurvey of foundation models (Zhou et al., 2023), which can be used to trace their training setup. Foundation models are by their design frequently reused and applied for numerous diverse downstream uses.This takes the form of a multi-stage training process throughout models lifestyles, starting with traininga strong base from scratch (pretraining) and followed by further refinement or adaptation to new usecases (fine-tuning). For this reason, all stages of training must be done with care: especially for pretrainedmodels or models that will otherwise be deployed widely or used as a base for further extensions, thedecisions made by model trainers at train-time can have outsized impacts on the models characteristics, bothpositive and negative, or on the field as a whole. Here, we cover a selection of existing resources for model training. We include frequently-used codebases formodel training that may be useful entry points for new developers to the field, but note that we cannot coverall existing options. We additionally briefly discuss resources where practitioners can learn about improvingthe resource-efficiency of their training, as well as educational resources for learning about model training.",
  "Fine-tuning": "Fine-tuning, or other types of adaptation performed on foundation models after pretraining, are an equallyimportant and complex step in model development and have the ability to significantly steer the behaviorsand characteristics of the end model. Fine-tuned models are also more frequently deployed than base models,making their safety and usability very important. Here we discuss a subset of finetuning resources that arewell documented, flexible, and some of which cater to computational efficiency. While fine-tuning is significantly less resource-intensive than pretraining, there are still many relevantconsiderations for practitioners. Use of widely adopted tools such as libraries designed for fine-tuning(Axolotl (OpenAccess-AI-Collective), trlX (Havrilla et al., 2023), Levanter ( can ensure greater ecosystem compatibility of resulting models, or reduce the barrier toexperimentation by abstracting away common pitfalls or providing guidance on effective hyperparameters.Similarly, the use of techniques such as QLoRA (Dettmers et al., 2023) or other popular parameter-efficientfine-tuning approaches and libraries (peft (Mangrulkar et al., 2022) , Otter, LLaMA-Adapter, LLaVA (Li et al.,2023a; Zhang et al., 2023a; Liu et al., 2023a)) can allow for fine-tuning for lower cost or on more accessiblehardware.",
  "Efficiency and Resource Allocation": "Knowledge of training best practices and efficiency techniques can reduce costs to train a desired modelsignificantly. Beyond systems-level optimizations and approaches to increase efficiency, the most importanttechnique is determining the most efficient allocation of resources, such as allocating compute between modelsize and dataset size for a given budget for the best results. The most common approach to solving thisproblem is to apply scaling laws (Kaplan et al., 2020; Hoffmann et al., 2022; Muennighoff et al., 2023b), acommon tool for cheaply extrapolating findings across scales of cost in order to make decisions based onsmaller-scale experiments for a final model training run. Additionally, practitioners seeking to embrace an open approach to model development should considerhow their decisions when training a foundation model may have impacts long after that models creation",
  "Training models at any scale can be quite daunting to newer practitioners. However, there are optionsavailable for learning about how to train and run foundation models": "Resources which themselves collate and provide reading lists or recommendations for learning about large-scale ML training (EleutherAI Cookbook (Anthony et al., 2024), ML Engineering Open Book (Bekman)) canbe an especially useful place to begin. Additionally, minimal codebases created as educational examples suchas NanoGPT (Karpathy, 2023), or other blog posts detailing fundamental concepts in training or runningfoundation models may be useful (Chen, 2022; Anthony et al., 2023). We recommend that practitionersreview these resources and use them to guide further reading about model training and usage.",
  "Recommendations": "Models should be released with accompanying documentation and easy-to-run code for training, evaluationand inference. Document model thoroughly to the extent possible. These are critical to avoiding misuse andharms and enabling developers to effectively build on your work. A well documented environment, code,and versions of the appropriate datasets. Be thoughtful about the type of license to use for artifacts. Open source is a technical term and standard witha widely accepted definition (Initiative, 2024). If there is a risk of misuse then consider behavioral restrictionsfrom a standardized tool (McDuff et al., 2024). Frameworks for monitoring and shaping model usage have become more prevalent as policymakers haveattempted to constrain certain end uses of foundation models. Several approaches include adverse eventreporting, watermarking, and restricting access to models in limited ways. Consider providing guidance tousers on how to use your models responsibly and openly stating the norms you hope will shape model use.",
  "Estimating Environmental Impact": "The environmental impact of AI is of increasing concern (Schwartz et al., 2020). Estimating the environmentalimpact of model development is a challenging task due to the number of relevant, hard to compute, and oftenunrecorded variables. For instance, to estimate the Life Cycle Assessment (LCA) of model development(Klpffer, 1997), variables include: model size, architecture, duration of training, number of training runs,storing and transferring data, hardware manufacturing, the specific type and setup of the hardware, networkconfiguration, the geographic location of the data center, the carbon intensity of the energy grid, the powerusage effectiveness (PUE) of cooling, overhead, and the broader data center infrastructure, as well as a similarset of questions for various deployment/inference settings (Patterson et al., 2021). Many of these variablesare also infeasible to precisely estimate, given for instance, that Nvidia does not disclose the carbon footprintof its GPUs (Luccioni et al., 2023). Another challenge includes the variety of relevant environmental outcomemeasures, from CO2 emissions, to energy footprint, or water use. Existing tools to measure environmental impact rely on a series of assumptions and estimates based on theavailable information. Perhaps the most accurate tools are those built into cloud services, which are ableto trace the hardware configurations and geographical locations of data centers but may not otherwise bepublicly available to users. The Azure Emissions Impact Dashboard ( AWS carbon footprint tool ( and Google Cloud Carbon Foot-print measurement system ( are three such sys-tems, available only when using their cloud services directly. Independent systems, such as Carbontracker(Anthony et al., 2020), CodeCarbon (Schmidt et al., 2021), or the Experimental Impact Tracker (Hendersonet al., 2020) offer basic estimate modeling and reporting, based on limited input information. Similarly,Li et al. (2023c) provides an estimate tool for the water usage footprint of language model training anddeployment. ML CO2 Impact ( improves these repositories with awrapper interface for easier use. However, these services are forced to trade-off between ease of use andaccuracy, as significant input information is required to obtain precise results, which imposes a burden onusers and widespread adoption.",
  "Effective use of resources": "Several decisions made prior to model training can have significant impacts on the upstream and downstreamenvironmental impact of a given model. Empirical scaling laws can be used to find the best allocationof resources. Kaplan et al. (2020); Hoffmann et al. (2022) estimate the optimal model size and trainingduration, given a training compute budget. And Aghajanyan et al. (2023) investigates the equivalent efficientcompute allocation for multi-modal settings. When working with text training data that is constrained, recentwork explores how to allocate compute efficiently (Muennighoff et al., 2023b). For models frequently useddownstream, it is important to consider the inference footprint and inference cost during model creation(Gadre et al., 2024b), to minimize the environmental impact of inference. For further resources and discussion,see 6.3.",
  "Model evaluation is an essential component of machine learning research. However many machinelearning papers use evaluations that are not reproducible or comparable to other work": "One of the biggest causes of irreproducibility is failure to report prompts and other essentialcomponents of evaluation protocols. This would not be a problem if researchers released evaluationcode and exact prompts, but many prominent labs (OpenAI, Anthropic, Meta) have not done so formodel releases. When using evaluation results from a paper that does not release its evaluation code,reproduce the evaluations using a public codebase. Examples of high-quality documentation practices for model evaluations can be found in Brownet al. (2020) (for bespoke evaluations) and Black et al. (2022); Scao et al. (2022); Biderman et al. (2023)(for evaluation using a public codebase). Expect a released model to be used in unexpected ways. Accordingly, try to evaluate the model onbenchmarks that are most related to its prescribed use case, but also its failure modes or potentialmisuses.",
  "Capabilities": "Many modern foundation models are released with general conversational abilities, such that their use casesare poorly specified and open-ended. This poses significant challenges to evaluation benchmarks that areunable to critically evaluate so many tasks, applications, and risks fairly or systematically. As a result, it isimportant to carefully scope the original intentions for the model, and to tie evaluations to those intentions.Even then, the most relevant evaluation benchmarks may not align with real use, and so developers shouldqualify their results, and carefully supplement them with data from real user/human evaluation settingswhere feasible. For language models, common capabilities benchmarks include those that evaluate models on narrow taskssuch as software engineering (Jimenez et al., 2023), topic classification (Adelani et al., 2023), and explainingcode (Muennighoff et al., 2023a). More comprehensive evaluation suites, such as the Language ModelEvaluation Harness (Gao et al., 2023) and HELM (Liang et al., 2023), are also common. Leaderboardslike LMSys Chatbot Arena (Zheng et al., 2023) offer another type of capability evaluation based on humanfeedback. There are far fewer capability evaluations for other modalities. Comprehensive evaluation suites exist forvision models as well (Lee et al., 2023b; Awadalla et al., 2023), but they are relatively less well developed.There are also a number of common benchmarks for evaluating vision models on a large number of tasks(Gadre et al., 2023; Liu et al., 2023b; Fu et al., 2023). Evaluations for speech models capabilities are stillnascent, with the OpenASR Leaderboard,32 which ranks models based on their Word Error Rate and Real-Time Factor, and the Edinburgh International Accents of English Corpus (Sanabria et al., 2023) as leadingexamples. The cheatsheet includes common benchmarks as of December 2023, but we caution that each comes withsubstantial limitations. For instance, many benchmarks based on multiple choice exams are not indicative ofreal user questions, and can be gamed with pseudo-data contamination. Additionally, while leaderboardsare exceedingly popular, model responses are often scored by other models, which have implicit biases tomodel responses that are longer, and look similar to their own (Dubois et al., 2023).",
  "Harm & Hazard Taxonomies": "Taxonomies provide a way of categorising, defining and understanding risks and hazards created throughthe use and deployment of AI systems. Some taxonomies focus primarily on the types of interactions withmodels that create a risk of harm (often called hazards) whereas others focus on the negative effects thatthey lead to (often called harms). Some taxonomies focus on existing issues, such as models that createhate speech or child abuse material, as well as more intangible or indirect forms of harm, such as the risk thatmodels perpetuate biases and stereotypes, and misrepresent social groups. Other taxonomies are focusedon the longer-term threats posed by more sophisticated models, such as ultra-personalised disinformation,cybersecurity threats, and military use (Brundage et al., 2018). Some work has also focused on categorisingcatastrophic or existential risks presented by Artificial General Intelligence, such as rogue AI agents andChemical, Biological, Radiological, Nuclear and high-yield Explosive weapons (Carlsmith, 2022; Hendryckset al., 2023; Bucknall & Dori-Hacohen, 2022). Further, a few taxonomies also assess the available evidence forthe risks and hazards, discuss their impact, and offer mitigation strategies (Deng et al., 2023; Kapoor et al.,2024; Klyman, 2024). Many taxonomies are released with an associated dataset, which can be used to either train models tominimise safety risks or to evaluate those risks. Several datasets have been released as benchmarks (e.g.Wang et al., 2024), which can be used to track progress across the community. We provide a non-exhaustivelist of existing taxonomies with datasets that are available open-source. We also note forthcoming workplanned by organisations like ML Commons, which aims to standardise assessment of AI safety risks byintroducing a new benchmark, comprising a taxonomy and dataset.33. 1. TrustLLM is a benchmark that covers six dimensions in English, including truthfulness, safety,fairness, robustness, privacy, and machine ethics (Sun et al., 2024). The benchmark comprises over30 datasets from existing research. In the paper, they test 16 open-source and proprietary models,and identify critical safety weaknesses. 2. SafetyBench is a benchmark that covers eight categories of safety, in both English and Chinese (Zhanget al., 2023b). Categories include Offensiveness; Unfairness and Bias; Physical Health; Mental Health;Illegal Activities; Ethics and Morality; and Privacy and Property. Unlike most safety evaluationdatasets, SafetyBench comprises multiple choice questions which makes automated evaluation ofmodels far easier. In the paper, they test 25 models and find that GPT-4 consistently performs best. 3. DecodingTrust is a benchmark that covers eight dimensions of AI safety and trustworthiness inEnglish (Wang et al., 2024). It covers a range of safety criteria such as Toxicity; Stereotypes; AdversarialRobustness; Out-of-Distribution Robustness; Privacy; Machine Ethics; And Fairness. The benchmarkhas a leaderboard that is hosted on HuggingFace. 4. HarmBench is a standardized evaluation framework for automated redteaming of LLMs in English(Mazeika et al., 2024). It covers 18 widely used red teaming methods, such as Persona, stochastic-fewshot, PEZ and GBDA. The benchmark has been designed with both seven semantic categories (e.g.Cybercrime, Misinformation and Bioweapons) and four functional categories (e.g. Standardbehaviours). In the paper, 33 LLMs are tested against HarmBench. 5. BigBench (Srivastava et al., 2023) and HELM (Liang et al., 2023) contain tests that are related tosafety, such as toxicity, bias and truthfulness in BigBench and toxicity, bias, disinformation, copyrightinfringement and truthfulness in HELM. Both make use of the widely-used RealToxicityPromptsdataset (Gehman et al., 2020a). Terms such as toxicity and offensiveness have been criticised insome papers for being overly broad and easy to misinterpret (Vidgen et al., 2019), and more recentwork has tended to use more fine grained terms. 6. Individual datasets have also been released that can be used to assess specific safety risks of models,such as SimpleSafetyTests which tests for clear-cut safety problems (Vidgen et al., 2023) and XSTestwhich tests for model false refusal (Rttger et al., 2024).",
  "Model developers have used various techniques to address risks and mitigate harms. Broadly, these attemptsfall into three categories, roughly ordered by effectiveness": "In-context learning. In-context learning can be used to add instructions to a models system prompt to steerthe models outputs. For example, OpenAIs DALL-E 3 model included instructions to avoid outputtingcopyrighted characters35. In some sense, this is the easiest model-level intervention: developers do not needto retrain or fine tune the model, neither do they need to rely on external API calls, such as to third-partycontent moderation endpoints. However, this comes at a cost: system prompts are easy to jailbreak, and as aresult, interventions based on in-context learning might be more brittle compared to other guardrails. Model alignment. One of the most prominent approaches for mitigating risks and harms is aligning modelswith preference data. This usually involves supervised fine tuning or training reward models that steer theoutputs of language models. Popular techniques include reinforcement learning with human (RLHF) andAI (RLAIF) feedback (Bai et al., 2022; Ouyang et al., 2022). To help end users understand the performanceof reward models, Lambert et al. (2024) develop a leaderboard for evaluating reward models for variousdesiderata, including safety. Still, model alignment techniques can be brittle against simple modifications tothe model, such as fine tuning (Qi et al., 2023), even when the model is fine tuned using benign data (Heet al., 2024). Guardrails on model inputs and outputs. The two interventions above rely on changing the model behaviorvia in-context learning or fine tuning. However, guardrails on model inputs and outputs that lie outside themodel might be a more robust intervention for preventing harmful content generation. For example, severalmodel developers provide moderation endpoints that can be used to filter inappropriate user requests ormodel outputs. These can be general-purpose endpoints that can be modified for filtering and moderation(e.g., Coheres classification endpoint modified for toxicity detection36 or Anthropics suggested prompt forcontent moderation37) as well as endpoints that are specifically built for content moderation (e.g., PerspectiveAPI (Lees et al., 2022) and OpenAIs moderation endpoint 38). Googles PaLM and Gemini models allowAPI users to set thresholds based on the safety likelihood and severity of model outputs39). The examples above are all from closed model developers. Recently, several open models have also beenproposed for moderating model inputs and outputs. For example, Llama-Guard by Meta (Inan et al., 2023)can be used to filter harmful content. Nvidias NEMO guardrails allow model providers to add programmaticguardrails to filter content (Rebedea et al., 2023).",
  "choice benchmarks have both versions with and without the answer choices given in the prompt.(See MMLU as a widely used dataset with inconsistencies in use and standardization.41": "Decoding strategies The decoding algorithm and its hyperparameter choices, such as the temperatureand sampling probabilities, will affect model behavior. For systems, rather than models, responsescan be the result of multiple iterations or models, or rely on external tools or sources. Ensemblingtechniques like self consistency (Wang et al., 2022) also improve performance significantly.",
  "Evaluation metric The choice of evaluation metric can impact the apparent magnitude of differencesbetween models, and even their ranking (Schaeffer et al., 2024)": "Human review setup For human preference evaluations, several details affect fair evaluation: whetherthe response selection is sufficiently model-blind, the attentiveness and expertise of the annotators tothe given topics, and the chosen rubric. Human preferences can also be skewed by the same factorsas model-based evaluations (Hosking et al., 2024; Xu et al., 2023; Wu & Aji, 2023) Without transparency and reproducibility integrated into the evaluation procedure, the axes of designfreedom can allow developers to game results, and prevent fair, apples-to-apples comparisons. For thesereasons, only evaluation scripts that are directly executable by third-parties provide verifiable reproducibility.Executable evaluation scripts also allow auditors to unpack the choices made in evaluation. Auditors canquickly experiment with different prompt formats, decoding parameters, and evaluation metrics, to shedlight on the scientific veracity of the claims. While evaluation documentation is helpful, the required breadthof information inevitably leads to subtle omissions, and even if the information is comprehensive, it does notprovide verifiable reproducibility. For these reasons, we believe the evidence is clear that AI safety reportingis not sufficiently reliable or trustworthy without verifiable execution scripts. Ground harm and hazard taxonomies in empirical observations. Existing taxonomies of harm are oftencreated to cluster existing safety benchmarks, which are mostly detached from real observations (Sun et al.,2024; Zhang et al., 2023b; Hendrycks et al., 2023). As the taxonomies of harm can guide practitioners priorities,this could lead to neglected or over-emphasized areas of safety research. It is essential that future harmtaxonomies are strongly grounded in empirical or naturalistic observations, through research conductedwith real users, rather than hypothetical situations envisioned by researchers. Extending risks and harms studies to multimodal and highly sensitive attacks. A great deal of researchinto risks and harms is focused exclusively on text, in English, and on more conservative safety risks suchas toxicity and bias (Gehman et al., 2020a; Hartvigsen et al., 2022). However, recent work has emphasizedthe particular risks of generative image, speech, and video models, being used to create deepfakes, NCIIor CSAM (Kapoor et al., 2024; Thiel et al., 2023b; Lakatos, 2023a). Other work has illustrated the muchgreater efficacy of jailbreaks and attacks in non-English languages, as compared to English (Yong et al., 2023).Multimodal jailbreaking is an emerging research area, with some nascent work (Qi et al., 2024; Shayeganiet al., 2023; Niu et al., 2024). Research into risks and harms from autonomous weapons systems (AWS) isalso highly under-explored, and an emerging risk (Simmons-Edler et al., 2024; Longpre et al., 2022).",
  "Document models thoroughly to the extent possible. Model documentation is critical to avoidingmisuse and harms, as well as enabling developers to effectively build on your work": "Open source is a technical term and standard with a widely accepted definition that is maintainedby the Open Source Initiative (OSI) (Initiative, 2024). Not all models that are downloadable or thathave publicly available weights and datasets are open-source; open-source models are those that arereleased under a license that adheres to the OSI standard. The extent to which responsible use licenses are legally enforceable is unclear. While licensesthat restrict end use of models may prevent commercial entities from engaging in out-of-scope uses,they are better viewed as tools for establishing norms rather than binding contracts. Choosing the right license for an open-access model can be difficult. Apache 2.0 is the mostcommon open-source license, while responsible AI licenses with use restrictions have seen growingadoption. Consider using one of the available tools for selecting the right open-source license for yourmodel artifacts. Frameworks for monitoring and shaping model usage have become more prevalent as policymakershave attempted to constrain certain end uses of foundation models. Several approaches include adverseevent reporting, watermarking, and restricting access to models in limited ways. Consider providingguidance to users on how to use your models responsibly and openly stating the norms you hope willshape model use.",
  "Model Documentation": "When models, code or applications are released, whether openly or not, it is important that they aredocumented thoroughly. Documentation should specify how to use the model, recommended and non-recommended use cases, potential harms, state or justify decisions made during training, and more. Docu-menting models is important not just for responsible development, but also to enable other developers toeffectively build on a model. Models are not nearly as useful as artifacts if not properly documented. Modelcards (Mitchell et al., 2019) are widely adopted standard for documenting models. Several tools have beendeveloped that support the creation of model cards42.",
  "Reproducibility": "Code to reproduce results are an important complement to other forms of documentation Kapoor et al.(2023). Releasing assets that reproduce results mean that scientific claims can be verified, and that systemscan be interrogated, tested and audited Missing, incomplete, or poorly documented code hinders progress.There are tools that help make model training, inference and evaluation reproducible. Anaconda and Dockermake necessary environments and dependencies easier to manage. Google Colab and Jupyter notebooksenable easily shareable code snippets and organized tutorials. The Language Model Evaluation Harnessprovides a framework for prompting and testing generative language models on a large number of differentevaluation tasks (Gao et al., 2023).",
  "Usage Monitoring": "Some open foundation model developers attempt to monitor the usage of their models, whether by water-marking model outputs or gating access to the model. The cheatsheet provides resources related to usagemonitoring, including examples of how to watermark content, guidance on appropriate use, guidance onreporting adverse events associated with model use43, and ways to limit some forms of access to models. Sev-eral of these approaches have significant drawbacks: for example, there are no known robust watermarkingtechniques for language models and there are limits to watermarking for image models (Kirchenbauer et al.,2023; Saberi et al., 2023). As with many of the sections above, usage monitoring remains an area of activeresearch.",
  "In this section we critically review the current state of resources for model release and monitoring, from oursurvey": "Reproducibility, especially through executable code, benefits all parties. The research community hasbenefited substantially from openness and transparency in how foundation model artifacts are documentedand released. As have proprietary developers who benefit from the rapid prototyping and innovations ontheir released technical systems, propelled by the open community. Our review of resources suggests thatoften the most widely adopted tools are those that are not just well documented/described, but also thosethat are easy to run with executable code. Usage monitoring remains challenging, and offers both advantages and disadvantages. Existing toolssuch as watermarking for AI outputs (Kirchenbauer et al., 2023; Saberi et al., 2023) or model fingerprinting(Xu et al., 2024) offer some degree of verification regarding the source of data or models. However, theirrobustness to adversarial removal, or to detection, remain dubiousthis is particularly true for text data.This can result in over-confidence that these methods provide a panacea, and instead result in false positives.There are also open questions on privacy as to the right for individuals to produce content without attribution.We hesitate to prescribe these nascent solutions broadly, without fully understanding the particular contextunder consideration for the data, the model, and their potential uses and abuses.",
  "Discussion": "General-purpose AI systems require both general-purpose and case-specific development tools.General-purpose AI systems are used for an increasingly broad, and often unforeseen, range of applications (Zhaoet al., 2023b; Schillaci, 2024; Longpre et al., 2024b). These include consumer-oriented uses including creativecomposition, information seeking, brainstorming, and reasoning, as well as industry applications in softwaredevelopment, entertainment, law, medicine, and journalism (Bommasani et al., 2021; Brigham et al., 2024).Across these uses, the common denominator is often the foundation model, while the system setting,expectations, and user base vary. Supporting these broad uses will likely require a suite of tools informedby naturalistic uses in each setting, to understand the real risks and shortcomings of a given application(Lin et al., 2024; Kapoor et al., 2024). The tools compiled in this work are aimed at the foundation modellayer, but may miss other essential components of responsible development or evaluation at the systemor application level. For these reasons, we suggest these tools are a starting point, but not in themselvessufficient for responsible developmentthat requires a deeper study of the intended use. For example, inmedicine there are likely to be considerably different considerations regarding foundation model behaviorcompared to those in creative tasks. Although the tools here can help, it is unlikely that they will address allthe nuanced aspects of different vertical applications. Recommendations across Development Stages.We have synthesized recommendations for each devel-opment stage and here we note some commonly recurring themes. First, throughout data sourcing, modeltraining, and evaluation there is a lack of transparency, documentation, and reproducibility. Starting docu-mentation early and maintaining it throughout a project makes this process easier. Reproducibility, whilenot a substitute for documentation, provides a starting point for downstream developers to understand,iterate, and improve prior practices. We especially recommend that closed-source evaluations release theirevaluation scripts, to disambiguate the many evaluation details and settings that can complicate results. Fortransparency, environmental impact metrics from data centers and consumer-level dashboards could providemore fine-grained information. A common theme across development stages is also the dearth of tools for non-English, internationallyrepresentative and multi-modal systems. Especially for data sourcing, these tools are lacking. In addition,datasets are often shaped by their availability and expedient collection processes. For instance, many text-to-image datasets rely on collecting captions, but less effort has been invested in collecting interspersed text andimages, with more complex and realistic relationships. This makes many datasets ill-fitting to the necessarydistribution, quality, and diversity of more specialized tasks. Synthetic data has demonstrated promise as away to fill some of these gaps. Lastly, we recommend that evaluation procedures shift away from evaluating models toward evaluatingentire systems, which may include the settings of deployment, input/output filters, other guardrails, andthe user interface. Tools for evaluation of models in-the-field are much less mature than those for staticbenchmark testing. We encourage practitioners to expand upon this type of testing. These systems aredeployed to interact with people, and other software in complex environments. These interactions need to bepart of the evaluation cycle.",
  "Nanotron. 2024": "David Ifeoluwa Adelani, Hannah Liu, Xiaoyu Shen, Nikita Vassilyev, Jesujoba O Alabi, Yanke Mao, HaonanGao, and Annie En-Shiun Lee. Sib-200: A simple, inclusive, and big evaluation dataset for topic classificationin 200+ languages and dialects. arXiv preprint arXiv:2309.07445, 2023. Armen Aghajanyan, Lili Yu, Alexis Conneau, Wei-Ning Hsu, Karen Hambardzumyan, Susan Zhang, StephenRoller, Naman Goyal, Omer Levy, and Luke Zettlemoyer. Scaling laws for generative mixed-modal languagemodels. In International Conference on Machine Learning, pp. 265279. PMLR, 2023.",
  "Stability AI. Stable audio tools. Github Repo. URL": "Alon Albalak, Yi-Lin Tuan, Pegah Jandaghi, Connor Pryor, Luke Yoffe, Deepak Ramachandran, Lise Getoor,Jay Pujara, and William Yang Wang. FETA: A benchmark for few-sample task transfer in open-domaindialogue. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (eds.), Proceedings of the 2022 Conferenceon Empirical Methods in Natural Language Processing, pp. 1093610953, Abu Dhabi, United Arab Emirates,December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.emnlp-main.751. URL",
  "Alon Albalak, Liangming Pan, Colin Raffel, and William Yang Wang. Efficient online data mixing for languagemodel pre-training. arXiv preprint arXiv:2312.02406, 2023": "Alon Albalak, Yanai Elazar, Sang Michael Xie, Shayne Longpre, Nathan Lambert, Xinyi Wang, NiklasMuennighoff, Bairu Hou, Liangming Pan, Haewon Jeong, et al. A survey on data selection for languagemodels. arXiv preprint arXiv:2402.16827, 2024. Zaid Alyafeai, Maraim Masoud, Mustafa Ghaleb, and Maged S Al-shaibani. Masader: Metadata sourcingfor arabic text and speech data resources. In Proceedings of the Thirteenth Language Resources and EvaluationConference, pp. 63406351, 2022. Alex Andonian, Quentin Anthony, Stella Biderman, Sid Black, Preetham Gali, Leo Gao, Eric Hallahan, JoshLevy-Kramer, Connor Leahy, Lucas Nestler, Kip Parker, Michael Pieler, Shivanshu Purohit, Tri Songz, WangPhil, and Samuel Weinbach. GPT-NeoX: Large scale autoregressive language modeling in PyTorch, 8 2021.URL",
  "Stas Bekman. Machine learning engineering open book. GitHub Repo. URL": "Emily M. Bender and Batya Friedman. Data statements for natural language processing: Toward mitigatingsystem bias and enabling better science. Transactions of the Association for Computational Linguistics, 6:587604, 2018. doi: 10.1162/tacl\\_a\\_00041. URL Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle OBrien, Eric Hallahan,Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. Pythia: A suitefor analyzing large language models across training and scaling. In International Conference on MachineLearning, pp. 23972430. PMLR, 2023.",
  "Abeba Birhane, Vinay Uday Prabhu, and Emmanuel Kahembwe. Multimodal datasets: misogyny, pornogra-phy, and malignant stereotypes. arXiv preprint arXiv:2110.01963, 2021": "Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, ConnorLeahy, Kyle McDonell, Jason Phang, et al. Gpt-neox-20b: An open-source autoregressive language model.arXiv preprint arXiv:2204.06745, 2022. Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz,Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent videodiffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael SBernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities and risks offoundation models. arXiv preprint arXiv:2108.07258, 2021.",
  "Rishi Bommasani, Kevin Klyman, Shayne Longpre, Sayash Kapoor, Nestor Maslej, Betty Xiong, Daniel Zhang,and Percy Liang. The foundation model transparency index, 2023": "Rishi Bommasani, Kevin Klyman, Shayne Longpre, Betty Xiong, Sayash Kapoor, Nestor Maslej, ArvindNarayanan, and Percy Liang. Foundation model transparency reports. arXiv preprint arXiv:2402.16268,2024. Natalie Grace Brigham, Chongjiu Gao, Tadayoshi Kohno, Franziska Roesner, and Niloofar Mireshghallah.Breaking news: Case studies of generative ais use in journalism. arXiv preprint arXiv:2406.13706, 2024. Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, TroyLuhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Video generation models asworld simulators. 2024. URL",
  "R Brown, C Webber, and JG Koomey. Status and future directions of the energy star program. Energy, 5(27):505520, 2002": "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Nee-lakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, GretchenKrueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, ChrisHesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner,Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners.In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neural InformationProcessing Systems, volume 33, pp. 18771901. Curran Associates, Inc., 2020. URL Miles Brundage, Shahar Avin, Jack Clark, Helen Toner, Peter Eckersley, Ben Garfinkel, Allan Dafoe, PaulScharre, Thomas Zeitzoff, Bobby Filar, Hyrum Anderson, Heather Roff, Gregory C. Allen, Jacob Steinhardt,Carrick Flynn, Sen higeartaigh, Simon Beard, Haydn Belfield, Sebastian Farquhar, Clare Lyle, RebeccaCrootof, Owain Evans, Michael Page, Joanna Bryson, Roman Yampolskiy, and Dario Amodei. The malicioususe of artificial intelligence: Forecasting, prevention, and mitigation, 2018. Benjamin S. Bucknall and Shiri Dori-Hacohen. Current and near-term ai as a potential existential risk factor.In Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society, AIES 22, pp. 119129, New York,NY, USA, 2022. Association for Computing Machinery. ISBN 9781450392471. doi: 10.1145/3514094.3534146.URL",
  "Carol Chen.Transformer inference arithmetic. 2022": "Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, PaulBarham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modelingwith pathways. Journal of Machine Learning Research, 24(240):1113, 2023. Danish Contractor, Daniel McDuff, Julia Katherine Haines, Jenny Lee, Christopher Hines, Brent Hecht,Nicholas Vincent, and Hanlin Li. Behavioral use licensing for responsible ai. In Proceedings of the 2022 ACMConference on Fairness, Accountability, and Transparency, pp. 778788, 2022.",
  "Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning ofquantized llms, 2023": "Sunipa Dev, Emily Sheng, Jieyu Zhao, Aubrie Amstutz, Jiao Sun, Yu Hou, Mattie Sanseverino, Jiin Kim,Akihiro Nishi, Nanyun Peng, and Kai-Wei Chang. On measures of biases and harms in NLP. In YulanHe, Heng Ji, Sujian Li, Yang Liu, and Chua-Hui Chang (eds.), Findings of the Association for ComputationalLinguistics: AACL-IJCNLP 2022, pp. 246267, Online only, November 2022. Association for ComputationalLinguistics. URL",
  "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectionaltransformers for language understanding. arXiv preprint arXiv:1810.04805, 2018": "Jwala Dhamala, Tony Sun, Varun Kumar, Satyapriya Krishna, Yada Pruksachatkun, Kai-Wei Chang, andRahul Gupta. Bold: Dataset and metrics for measuring biases in open-ended language generation. InProceedings of the 2021 ACM conference on fairness, accountability, and transparency, pp. 862872, 2021. Mark Dingemanse and Andreas Liesenfeld. From text to talk: Harnessing conversational corpora for humaneand diversity-aware language technology. In Proceedings of the 60th Annual Meeting of the Association forComputational Linguistics (Volume 1: Long Papers), pp. 56145633, 2022.",
  "Kate Downing. Ai licensing cant balance open with responsible. The Law Office of Kate DowningsBlog, 2023. URL": "Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, PercyLiang, and Tatsunori B Hashimoto. Alpacafarm: A simulation framework for methods that learn fromhuman feedback. arXiv preprint arXiv:2305.14387, 2023. Yanai Elazar, Akshita Bhagia, Ian Magnusson, Abhilasha Ravichander, Dustin Schwenk, Alane Suhr, PeteWalsh, Dirk Groeneveld, Luca Soldaini, Sameer Singh, Hanna Hajishirzi, Noah A. Smith, and Jesse Dodge.Whats in my big data?, 2023.",
  "The Free Software Foundation.What is free software?, 2024.URL Last accessed on 2024-02-20": "Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng,Ke Li, Xing Sun, et al. Mme: A comprehensive evaluation benchmark for multimodal large languagemodels. arXiv preprint arXiv:2306.13394, 2023. Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, RyanMarten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, Eyal Orgad, Rahim Entezari, Giannis Daras,Sarah Pratt, Vivek Ramanujan, Yonatan Bitton, Kalyani Marathe, Stephen Mussmann, Richard Vencu,Mehdi Cherti, Ranjay Krishna, Pang Wei Koh, Olga Saukh, Alexander Ratner, Shuran Song, HannanehHajishirzi, Ali Farhadi, Romain Beaumont, Sewoong Oh, Alex Dimakis, Jenia Jitsev, Yair Carmon, VaishaalShankar, and Ludwig Schmidt. Datacomp: In search of the next generation of multimodal datasets, 2023. Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, RyanMarten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, et al. Datacomp: In search of the next generationof multimodal datasets. Advances in Neural Information Processing Systems, 36, 2024a. Samir Yitzhak Gadre, Georgios Smyrnis, Vaishaal Shankar, Suchin Gururangan, Mitchell Wortsman, RulinShao, Jean Mercat, Alex Fang, Jeffrey Li, Sedrick Keh, et al. Language models scale reliably with over-training and on downstream tasks. arXiv preprint arXiv:2403.08540, 2024b. Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, HoraceHe, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse text for language modeling.arXiv preprint arXiv:2101.00027, 2020. Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, LaurenceGolding, Jeffrey Hsu, Alain Le Noach, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa,Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite,Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot language model evaluation, 12 2023.URL",
  "Aidan Gomez. Introducing command r+: A scalable llm built for business. 4 2024. URL:": "Edouard Grave, Piotr Bojanowski, Prakhar Gupta, Armand Joulin, and Tomas Mikolov. Learning wordvectors for 157 languages. In Proceedings of the International Conference on Language Resources and Evaluation(LREC 2018), 2018. Neel Guha, Julian Nyarko, Daniel Ho, Christopher R, Adam Chilton, Alex Chohlas-Wood, Austin Peters,Brandon Waldon, Daniel Rockmore, Diego Zambrano, et al. Legalbench: A collaboratively built benchmarkfor measuring legal reasoning in large language models. Advances in Neural Information Processing Systems,36, 2024. Suchin Gururangan, Mitchell Wortsman, Samir Yitzhak Gadre, Achal Dave, Maciej Kilian, Weijia Shi, JeanMercat, Georgios Smyrnis, Gabriel Ilharco, Matt Jordan, Reinhard Heckel, Alex Dimakis, Ali Farhadi,Vaishaal Shankar, and Ludwig Schmidt. open_lm: a minimal but performative language modeling (lm)repository, 2023. URL GitHub repository.",
  "Laura Hanu and team Unitary. Detoxify, November 2020. URL": "Thomas Hartvigsen, Saadia Gabriel, Hamid Palangi, Maarten Sap, Dipankar Ray, and Ece Kamar. Toxigen: Alarge-scale machine-generated dataset for adversarial and implicit hate speech detection. arXiv preprintarXiv:2203.09509, 2022. Alexander Havrilla, Maksym Zhuravinskyi, Duy Phung, Aman Tiwari, Jonathan Tow, Stella Biderman,Quentin Anthony, and Louis Castricato. trlX: A framework for large scale reinforcement learning fromhuman feedback. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing,pp. 85788595, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.530. URL",
  "Luxi He, Mengzhou Xia, and Peter Henderson. Whats in your\" safe\" data?: Identifying benign data thatbreaks safety. arXiv preprint arXiv:2404.01099, 2024": "Peter Henderson, Jieru Hu, Joshua Romoff, Emma Brunskill, Dan Jurafsky, and Joelle Pineau. Towardsthe systematic reporting of the energy and carbon footprints of machine learning. The Journal of MachineLearning Research, 21(1):1003910081, 2020. Peter Henderson, Mark Krass, Lucia Zheng, Neel Guha, Christopher D Manning, Dan Jurafsky, and DanielHo. Pile of law: Learning responsible data filtering from the law and a 256gb open-source legal dataset.Advances in Neural Information Processing Systems, 35:2921729234, 2022.",
  "Yacine Jernite. Training data transparency in ai: Tools, trends, and policy recommendations. In Hugging FaceBlog, 2023": "Yacine Jernite, Huu Nguyen, Stella Biderman, Anna Rogers, Maraim Masoud, Valentin Danchev, SamsonTan, Alexandra Sasha Luccioni, Nishant Subramani, Isaac Johnson, et al. Data governance in the ageof large-scale data-driven language technology. In Proceedings of the 2022 ACM Conference on Fairness,Accountability, and Transparency, pp. 22062222, 2022. Carlos E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan.Swe-bench: Can language models resolve real-world github issues? arXiv preprint arXiv:2310.06770, 2023. Jacob Kahn, Morgane Rivire, Weiyi Zheng, Evgeny Kharitonov, Qiantong Xu, Pierre-Emmanuel Mazar,Julien Karadayi, Vitaliy Liptchinsky, Ronan Collobert, Christian Fuegen, et al. Libri-light: A benchmark forasr with limited or no supervision. In ICASSP 2020-2020 IEEE International Conference on Acoustics, Speechand Signal Processing (ICASSP), pp. 76697673. IEEE, 2020. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray,Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprintarXiv:2001.08361, 2020. Sayash Kapoor, Emily F. Cantrell, Kenny Peng, Thanh Hien Pham, Christopher A. Bail, Odd Erik Gundersen,Jake M. Hofman, Jessica R. Hullman, Michael A. Lones, Momin M. Malik, Priyanka Nanayakkara, Russel A.Poldrack, Inioluwa Deborah Raji, Michael Roberts, Matthew J. Salganik, Marta Serra-Garcia, Brandon MStewart, Gilles Vandewiele, and Arvind Narayanan. Reforms: Reporting standards for machine learningbased science. ArXiv, abs/2308.07832, 2023. URL Sayash Kapoor, Rishi Bommasani, Kevin Klyman, Shayne Longpre, Ashwin Ramaswami, Peter Cihon, AspenHopkins, Kevin Bankston, Stella Biderman, Miranda Bogen, et al. On the societal impact of open foundationmodels. arXiv preprint arXiv:2403.07918, 2024. Siddharth Karamcheti, Suraj Nair, Ashwin Balakrishna, Percy Liang, Thomas Kollar, and Dorsa Sadigh.Prismatic vlms: Investigating the design space of visually-conditioned language models. arXiv preprintarXiv:2402.07865, 2024.",
  "Anton Korinek and Jai Vipra. Market concentration implications of foundation models: The invisible hand ofchatgpt. 2023": "Julia Kreutzer, Isaac Caswell, Lisa Wang, Ahsan Wahab, Daan van Esch, Nasanbayar Ulzii-Orshikh, AllahseraTapo, Nishant Subramani, Artem Sokolov, Claytone Sikasote, et al. Quality at a glance: An audit ofweb-crawled multilingual datasets. Transactions of the Association for Computational Linguistics, 10:5072,2022. Sneha Kudugunta, Isaac Rayburn Caswell, Biao Zhang, Xavier Garcia, Derrick Xin, Aditya Kusupati, RomiStella, Ankur Bapna, and Orhan Firat. Madlad-400: A multilingual and document-level large auditeddataset. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track,2023. Veronika Laippala, Anna Salmela, Samuel Rnnqvist, Alham Fikri Aji, Li-Hsin Chang, Asma Dhifallah,Larissa Goulart, Henna Kortelainen, Marc Pmies, Deise Prina Dutra, et al. Towards better structuredand less noisy web data: Oscar with register annotations. In Proceedings of the Eighth Workshop on NoisyUser-generated Text (W-NUT 2022), pp. 215221, 2022. S Lakatos. A revealing picture: Ai-generated undressingimages move from niche pornography discussionforums to a scaled and monetized online business. Technical report, Technical report, Graphika, Dec 2023.URL graphika ..., 2023a.",
  "Santiago Lakatos. A Revealing Picture: AI-Generated Undressing Images Move from Niche PornographyDiscussion Forums to a Scaled and Monetized Online Business. Technical report, December 2023b. URL": "Nathan Lambert, Valentina Pyatkin, Jacob Morrison, LJ Miranda, Bill Yuchen Lin, Khyathi Chandu, NouhaDziri, Sachin Kumar, Tom Zick, Yejin Choi, et al. Rewardbench: Evaluating reward models for languagemodeling. arXiv preprint arXiv:2403.13787, 2024. Hugo Laurenon, Lucile Saulnier, Thomas Wang, Christopher Akiki, Albert Villanova del Moral, TevenLe Scao, Leandro Von Werra, Chenghao Mou, Eduardo Gonzlez Ponferrada, Huu Nguyen, Jrg Fro-hberg, Mario ako, Quentin Lhoest, Angelina McMillan-Major, Gerard Dupont, Stella Biderman, AnnaRogers, Loubna Ben allal, Francesco De Toni, Giada Pistilli, Olivier Nguyen, Somaieh Nikpoor, MaraimMasoud, Pierre Colombo, Javier de la Rosa, Paulo Villegas, Tristan Thrush, Shayne Longpre, SebastianNagel, Leon Weber, Manuel Muoz, Jian Zhu, Daniel Van Strien, Zaid Alyafeai, Khalid Almubarak,Minh Chien Vu, Itziar Gonzalez-Dios, Aitor Soroa, Kyle Lo, Manan Dey, Pedro Ortiz Suarez, AaronGokaslan, Shamik Bose, David Adelani, Long Phan, Hieu Tran, Ian Yu, Suhas Pai, Jenny Chim, VioletteLepercq, Suzana Ilic, Margaret Mitchell, Sasha Alexandra Luccioni, and Yacine Jernite. The bigscienceroots corpus: A 1.6tb composite multilingual dataset. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave,K. Cho, and A. Oh (eds.), Advances in Neural Information Processing Systems, volume 35, pp. 3180931826.Curran Associates, Inc., 2022. URL Hugo Laurenon, Lucile Saulnier, Lo Tronchon, Stas Bekman, Amanpreet Singh, Anton Lozhkov, ThomasWang, Siddharth Karamcheti, Alexander Rush, Douwe Kiela, et al. Obelics: An open web-scale filtereddataset of interleaved image-text documents. Advances in Neural Information Processing Systems, 36, 2024a.",
  "Hugo Laurenon, Lo Tronchon, Matthieu Cord, and Victor Sanh. What matters when building vision-language models? arXiv preprint arXiv:2405.02246, 2024b": "ric Le Ferrand, Steven Bird, and Laurent Besacier. Learning from failure: Data capture in an australianaboriginal community. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics(Volume 1: Long Papers), pp. 49884998, 2022. Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch,and Nicholas Carlini. Deduplicating training data makes language models better. In Smaranda Muresan,Preslav Nakov, and Aline Villavicencio (eds.), Proceedings of the 60th Annual Meeting of the Association forComputational Linguistics (Volume 1: Long Papers), pp. 84248445, Dublin, Ireland, May 2022. Associationfor Computational Linguistics. doi: 10.18653/v1/2022.acl-long.577. URL Mina Lee, Megha Srivastava, Amelia Hardy, John Thickstun, Esin Durmus, Ashwin Paranjape, Ines Gerard-Ursin, Xiang Lisa Li, Faisal Ladhak, Frieda Rong, et al. Evaluating human-language model interaction.Transactions on Machine Learning Research, 2023a. Tony Lee, Michihiro Yasunaga, Chenlin Meng, Yifan Mai, Joon Sung Park, Agrim Gupta, Yunzhi Zhang,Deepak Narayanan, Hannah Benita Teufel, Marco Bellagente, et al. Holistic evaluation of text-to-imagemodels. arXiv preprint arXiv:2311.04287, 2023b. Alyssa Lees, Vinh Q Tran, Yi Tay, Jeffrey Sorensen, Jai Gupta, Donald Metzler, and Lucy Vasserman. A newgeneration of perspective api: Efficient multilingual character-level transformers. In Proceedings of the 28thACM SIGKDD Conference on Knowledge Discovery and Data Mining, pp. 31973207, 2022. Quentin Lhoest, Albert Villanova del Moral, Yacine Jernite, Abhishek Thakur, Patrick von Platen, Suraj Patil,Julien Chaumond, Mariama Drame, Julien Plu, Lewis Tunstall, et al. Datasets: A community library fornatural language processing. In Proceedings of the 2021 Conference on Empirical Methods in Natural LanguageProcessing: System Demonstrations, pp. 175184, 2021a. Quentin Lhoest, Albert Villanova del Moral, Yacine Jernite, Abhishek Thakur, Patrick von Platen, SurajPatil, Julien Chaumond, Mariama Drame, Julien Plu, Lewis Tunstall, Joe Davison, Mario ako, GunjanChhablani, Bhavitvya Malik, Simon Brandeis, Teven Le Scao, Victor Sanh, Canwen Xu, Nicolas Patry,Angelina McMillan-Major, Philipp Schmid, Sylvain Gugger, Clment Delangue, Tho Matussire, LysandreDebut, Stas Bekman, Pierric Cistac, Thibault Goehringer, Victor Mustar, Franois Lagunas, Alexander Rush,and Thomas Wolf. Datasets: A community library for natural language processing. In Proceedings of the2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pp. 175184,Online and Punta Cana, Dominican Republic, November 2021b. Association for Computational Linguistics.URL",
  "Pengfei Li, Jianyi Yang, Mohammad A Islam, and Shaolei Ren. Making ai less\" thirsty\": Uncovering andaddressing the secret water footprint of ai models. arXiv preprint arXiv:2304.03271, 2023c": "Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang,Deepak Narayanan, Yuhuai Wu, Ananya Kumar, Benjamin Newman, Binhang Yuan, Bobby Yan, Ce Zhang,Christian Cosgrove, Christopher D. Manning, Christopher R, Diana Acosta-Navas, Drew A. Hudson,Eric Zelikman, Esin Durmus, Faisal Ladhak, Frieda Rong, Hongyu Ren, Huaxiu Yao, Jue Wang, KeshavSanthanam, Laurel Orr, Lucia Zheng, Mert Yuksekgonul, Mirac Suzgun, Nathan Kim, Neel Guha, NiladriChatterji, Omar Khattab, Peter Henderson, Qian Huang, Ryan Chi, Sang Michael Xie, Shibani Santurkar,Surya Ganguli, Tatsunori Hashimoto, Thomas Icard, Tianyi Zhang, Vishrav Chaudhary, William Wang,Xuechen Li, Yifan Mai, Yuhui Zhang, and Yuta Koreeda. Holistic evaluation of language models, 2023.",
  "Q Vera Liao and Ziang Xiao. Rethinking model evaluation as narrowing the socio-technical gap. arXivpreprint arXiv:2306.03100, 2023": "Bill Yuchen Lin, Yuntian Deng, Khyathi Chandu, Faeze Brahman, Abhilasha Ravichander, Valentina Pyatkin,Nouha Dziri, Ronan Le Bras, and Yejin Choi. Wildbench: Benchmarking llms with challenging tasks fromreal users in the wild. arXiv preprint arXiv:2406.04770, 2024. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollr, andC Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer VisionECCV 2014: 13thEuropean Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pp. 740755. Springer,2014.",
  "Jiacheng Liu, Sewon Min, Luke Zettlemoyer, Yejin Choi, and Hannaneh Hajishirzi. Infini-gram: Scalingunbounded n-gram language models to a trillion tokens. arXiv preprint arXiv:2401.17377, 2024b": "Ruibo Liu, Jerry Wei, Fangyu Liu, Chenglei Si, Yanzhe Zhang, Jinmeng Rao, Steven Zheng, Daiyi Peng, DiyiYang, Denny Zhou, et al. Best practices and lessons learned on synthetic data for language models. arXivpreprint arXiv:2404.07503, 2024c. Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang,Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? arXiv preprintarXiv:2307.06281, 2023b. Kyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kinney, and Daniel S Weld. S2orc: The semantic scholaropen research corpus. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,pp. 49694983, 2020.",
  "Shayne Longpre, Marcus Storm, and Rishi Shah. Lethal autonomous weapons systems & artificial intelligence:Trends, challenges, and policies. MIT Science Policy Review, 3(1):4756, 2022": "Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V Le, BarretZoph, Jason Wei, et al. The flan collection: Designing data and methods for effective instruction tuning.arXiv preprint arXiv:2301.13688, 2023a. Shayne Longpre, Robert Mahari, Anthony Chen, Naana Obeng-Marnu, Damien Sileo, William Brannon,Niklas Muennighoff, Nathan Khazam, Jad Kabbara, Kartik Perisetla, et al. The data provenance initiative:A large scale audit of dataset licensing & attribution in ai. arXiv preprint arXiv:2310.16787, 2023b. Shayne Longpre, Sayash Kapoor, Kevin Klyman, Ashwin Ramaswami, Rishi Bommasani, Borhane Blili-Hamelin, Yangsibo Huang, Aviya Skowron, Zheng-Xin Yong, Suhas Kotha, et al. A safe harbor for aievaluation and red teaming. arXiv preprint arXiv:2403.04893, 2024a. Shayne Longpre, Robert Mahari, Ariel Lee, Campbell Lund, Hamidah Oderinwale, William Brannon, NayanSaxena, Naana Obeng-Marnu, Tobin South, Cole Hunter, et al. Consent in crisis: the rapid decline of the aidata commons. arXiv preprint arXiv:2407.14933, 2024b. Shayne Longpre, Robert Mahari, Naana Obeng-Marnu, William Brannon, Tobin South, Katy Gero, SandyPentland, and Jad Kabbara. Data authenticity, consent, & provenance for ai are all broken: what will it taketo fix them? arXiv preprint arXiv:2404.12691, 2024c. Anton Lozhkov, Raymond Li, Loubna Ben Allal, Federico Cassano, Joel Lamy-Poirier, Nouamane Tazi,Ao Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, et al. Starcoder 2 and the stack v2: The next generation.arXiv preprint arXiv:2402.19173, 2024.",
  "Michael Matena and Colin Raffel. Merging models with fisher-weighted averaging, 2022": "Mantas Mazeika, Long Phan, Xuwang Yin, Andy Zou, Zifan Wang, Norman Mu, Elham Sakhaee, NathanielLi, Steven Basart, Bo Li, David Forsyth, and Dan Hendrycks. Harmbench: A standardized evaluationframework for automated red teaming and robust refusal, 2024. Mark Mazumder, Colby Banbury, Xiaozhe Yao, Bojan Karla, William A Gaviria Rojas, Sudnya Diamos, GregDiamos, Lynn He, Alicia Parrish, Hannah Rose Kirk, Jessica Quaye, Charvi Rastogi, Douwe Kiela, DavidJurado, David Kanter, Rafael Mosquera, Will Cukierski, Juan Ciro, Lora Aroyo, Bilge Acun, Lingjiao Chen,Mehul Smriti Raje, Max Bartolo, Sabri Eyuboglu, Amirata Ghorbani, Emmett Daniel Goodman, AddisonHoward, Oana Inel, Tariq Kane, Christine Kirkpatrick, D. Sculley, Tzu-Sheng Kuo, Jonas Mueller, TristanThrush, Joaquin Vanschoren, Margaret Warren, Adina Williams, Serena Yeung, Newsha Ardalani, PraveenParitosh, Ce Zhang, James Y. Zou, Carole-Jean Wu, Cody Coleman, Andrew Ng, Peter Mattson, andVijay Janapa Reddi. Dataperf: Benchmarks for data-centric AI development. In Thirty-seventh Conference onNeural Information Processing Systems Datasets and Benchmarks Track, 2023. URL Daniel McDuff, Tim Korjakow, Scott Cambo, Jesse Josua Benjamin, Jenny Lee, Yacine Jernite, Carlos MuozFerrandis, Aaron Gokaslan, Alek Tarkowski, Joseph Lindley, et al. On the standardization of behavioraluse clauses and their adoption for responsible licensing of ai. arXiv preprint arXiv:2402.05979, 2024. Kristina McElheran, J Frank Li, Erik Brynjolfsson, Zachary Kroff, Emin Dinlersoz, Lucia Foster, and NikolasZolas. Ai adoption in america: Who, what, and where. Journal of Economics & Management Strategy, 2024. Brandon McKinzie, Zhe Gan, Jean-Philippe Fauconnier, Sam Dodge, Bowen Zhang, Philipp Dufter, DhrutiShah, Xianzhi Du, Futang Peng, Floris Weers, et al. Mm1: Methods, analysis & insights from multimodalllm pre-training. arXiv preprint arXiv:2403.09611, 2024. Angelina McMillan-Major, Zaid Alyafeai, Stella Biderman, Kimbo Chen, Francesco De Toni, Grard Dupont,Hady Elsahar, Chris Emezue, Alham Fikri Aji, Suzana Ili, et al. Documenting geographically andcontextually diverse data sources: The bigscience catalogue of language data and resources. arXiv preprintarXiv:2201.10066, 2022. Sewon Min, Suchin Gururangan, Eric Wallace, Weijia Shi, Hannaneh Hajishirzi, Noah A Smith, and LukeZettlemoyer. Silo language models: Isolating legal risk in a nonparametric datastore. In NeurIPS 2023Workshop on Distribution Shifts: New Frontiers with Foundation Models, 2023. Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, ElenaSpitzer, Inioluwa Deborah Raji, and Timnit Gebru. Model cards for model reporting. In Proceedings of theconference on fairness, accountability, and transparency, pp. 220229, 2019. Niklas Muennighoff, Qian Liu, Armel Zebaze, Qinkai Zheng, Binyuan Hui, Terry Yue Zhuo, Swayam Singh,Xiangru Tang, Leandro von Werra, and Shayne Longpre. Octopack: Instruction tuning code large languagemodels. arXiv preprint arXiv:2308.07124, 2023a. Niklas Muennighoff, Alexander M Rush, Boaz Barak, Teven Le Scao, Aleksandra Piktus, Nouamane Tazi,Sampo Pyysalo, Thomas Wolf, and Colin Raffel. Scaling data-constrained language models. arXiv preprintarXiv:2305.16264, 2023b.",
  "Zhenxing Niu, Haodong Ren, Xinbo Gao, Gang Hua, and Rong Jin. Jailbreaking attack against multimodallarge language model. arXiv preprint arXiv:2402.02309, 2024": "Akintunde Oladipo, Mofetoluwa Adeyemi, Orevaoghene Ahia, Abraham Owodunni, Odunayo Ogundepo,David Adelani, and Jimmy Lin. Better quality pre-training data and t5 models for African languages. InHouda Bouamor, Juan Pino, and Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical Methodsin Natural Language Processing, pp. 158168, Singapore, December 2023. Association for ComputationalLinguistics. doi: 10.18653/v1/2023.emnlp-main.11. URL",
  "OpenAccess-AI-Collective. Axolotl. GitHub Repo. URL": "Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang,Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions withhuman feedback. arXiv preprint arXiv:2203.02155, 2022. URL Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. Librispeech: an asr corpus based onpublic domain audio books. In 2015 IEEE international conference on acoustics, speech and signal processing(ICASSP), pp. 52065210. IEEE, 2015. Alicia Parrish, Angelica Chen, Nikita Nangia, Vishakh Padmakumar, Jason Phang, Jana Thompson, Phu MonHtut, and Samuel Bowman. Bbq: A hand-built bias benchmark for question answering. In Findings of theAssociation for Computational Linguistics: ACL 2022, pp. 20862105, 2022.",
  "Keiran Paster, Marco Dos Santos, Zhangir Azerbayev, and Jimmy Ba. Openwebmath: An open dataset ofhigh-quality mathematical web text. arXiv preprint arXiv:2310.06786, 2023": "David Patterson, Joseph Gonzalez, Quoc Le, Chen Liang, Lluis-Miquel Munguia, Daniel Rothchild, DavidSo, Maud Texier, and Jeff Dean. Carbon emissions and large neural network training. arXiv preprintarXiv:2104.10350, 2021. Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, HamzaAlobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. The refinedweb dataset for falcon llm:outperforming curated corpora with web data, and web data only. arXiv preprint arXiv:2306.01116, 2023.",
  "Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei. Kosmos-2:Grounding multimodal large language models to the world, 2023": "Aleksandra Piktus, Christopher Akiki, Paulo Villegas, Hugo Laurenon, Grard Dupont, Alexandra SashaLuccioni, Yacine Jernite, and Anna Rogers. The roots search tool: Data transparency for llms. arXiv preprintarXiv:2302.14035, 2023. Vineel Pratap, Qiantong Xu, Anuroop Sriram, Gabriel Synnaeve, and Ronan Collobert. MLS: A Large-Scale Multilingual Dataset for Speech Research. In Proc. Interspeech 2020, pp. 27572761, 2020. doi:10.21437/Interspeech.2020-2826.",
  "Paul Rttger, Hannah Rose Kirk, Bertie Vidgen, Giuseppe Attanasio, Federico Bianchi, and Dirk Hovy. Xstest:A test suite for identifying exaggerated safety behaviours in large language models, 2024": "Mehrdad Saberi, Vinu Sankar Sadasivan, Keivan Rezaei, Aounon Kumar, Atoosa Chegini, Wenxiao Wang,and Soheil Feizi. Robustness of ai-image detectors: Fundamental limits and practical attacks. arXiv preprintarXiv:2310.00076, 2023. Nithya Sambasivan, Shivani Kapania, Hannah Highfill, Diana Akrong, Praveen Paritosh, and Lora M Aroyo.Everyone wants to do the model work, not the data work: Data cascades in high-stakes AI. In CHI,CHI 21, New York, NY, USA, 2021. Association for Computing Machinery. ISBN 9781450380966. doi:10.1145/3411764.3445518. URL Ramon Sanabria, Nikolay Bogoychev, Nina Markl, Andrea Carmantini, Ondrej Klejch, and Peter Bell. Theedinburgh international accents of english corpus: Towards the democratization of english asr. In ICASSP2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 15. IEEE,2023. Victor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin,Arnaud Stiegler, Teven Le Scao, Arun Raja, et al. Multitask prompted training enables zero-shot taskgeneralization. ICLR 2022, 2021. URL Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili, Daniel Hesslow, Roman Castagn,Alexandra Sasha Luccioni, Franois Yvon, Matthias Gall, et al. Bloom: A 176b-parameter open-accessmultilingual language model. arXiv preprint arXiv:2211.05100, 2022.",
  "Roy Schwartz, Jesse Dodge, Noah A Smith, and Oren Etzioni. Green ai. Communications of the ACM, 63(12):5463, 2020": "Erfan Shayegani, Yue Dong, and Nael Abu-Ghazaleh. Jailbreak in pieces: Compositional adversarial attackson multi-modal language models. In The Twelfth International Conference on Learning Representations, 2023. Weijia Shi, Anirudh Ajith, Mengzhou Xia, Yangsibo Huang, Daogao Liu, Terra Blevins, Danqi Chen, andLuke Zettlemoyer. Detecting pretraining data from large language models. arXiv preprint arXiv:2310.16789,2023.",
  "Riley Simmons-Edler, Ryan Badman, Shayne Longpre, and Kanaka Rajan. Ai-powered autonomous weaponsrisk geopolitical instability and threaten ai research. arXiv preprint arXiv:2405.01859, 2024": "Shivalika Singh, Freddie Vargus, Daniel Dsouza, Brje F Karlsson, Abinaya Mahendiran, Wei-Yin Ko, HerumbShandilya, Jay Patel, Deividas Mataciunas, Laura OMahony, et al. Aya dataset: An open-access collectionfor multilingual instruction tuning. arXiv preprint arXiv:2402.06619, 2024. Eric Michael Smith, Melissa Hall, Melanie Kambadur, Eleonora Presani, and Adina Williams. im sorry tohear that: Finding new biases in language models with a holistic descriptor dataset. In Proceedings of the2022 Conference on Empirical Methods in Natural Language Processing, pp. 91809211, 2022a. Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared Casper, ZhunLiu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, Elton Zhang, Rewon Child, Reza YazdaniAminabadi, Julie Bernauer, Xia Song, Mohammad Shoeybi, Yuxiong He, Michael Houston, Saurabh Tiwary,and Bryan Catanzaro. Using deepspeed and megatron to train megatron-turing nlg 530b, a large-scalegenerative language model, 2022b. Luca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Authur, BenBogin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar, Valentin Hofmann, Ananya Harsh Jha, SachinKumar, Li Lucy, Xinxi Lyu, Ian Magnusson, Jacob Morrison, Niklas Muennighoff, Aakanksha Naik, CrystalNam, Matthew E. Peters, Abhilasha Ravichander, Kyle Richardson, Zejiang Shen, Emma Strubell, NishantSubramani, Oyvind Tafjord, Evan Pete Walsh, Hannaneh Hajishirzi, Noah A. Smith, Luke Zettlemoyer,Iz Beltagy, Dirk Groeneveld, Jesse Dodge, and Kyle Lo. Dolma: An Open Corpus of Three Trillion Tokensfor Language Model Pretraining Research. Allen Institute for AI, Tech. Rep, 2023. Luca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Authur, Ben Bogin,Khyathi Chandu, Jennifer Dumas, Yanai Elazar, et al. Dolma: an open corpus of three trillion tokens forlanguage model pretraining research. arXiv preprint arXiv:2402.00159, 2024. Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch,Adam R. Brown, Adam Santoro, Aditya Gupta, Adri Garriga-Alonso, Agnieszka Kluska, Aitor Lewkowycz,Akshat Agarwal, Alethea Power, Alex Ray, Alex Warstadt, Alexander W. Kocurek, Ali Safaya, Ali Tazarv,Alice Xiang, Alicia Parrish, Allen Nie, Aman Hussain, Amanda Askell, Amanda Dsouza, Ambrose Slone,Ameet Rahane, Anantharaman S. Iyer, Anders Andreassen, Andrea Madotto, Andrea Santilli, AndreasStuhlmller, Andrew Dai, Andrew La, Andrew Lampinen, Andy Zou, Angela Jiang, Angelica Chen, AnhVuong, Animesh Gupta, Anna Gottardi, Antonio Norelli, Anu Venkatesh, Arash Gholamidavoodi, Arfa",
  "George Stoica, Daniel Bolya, Jakob Bjorner, Pratik Ramesh, Taylor Hearn, and Judy Hoffman. Zipit! mergingmodels from different tasks without training, 2024": "Pedro Javier Ortiz Surez, Benot Sagot, and Laurent Romary. Asynchronous pipeline for processing hugecorpora on medium to low resource infrastructures. In 7th Workshop on the Challenges in the Management ofLarge Corpora (CMLC-7). Leibniz-Institut fr Deutsche Sprache, 2019. Lichao Sun, Yue Huang, Haoran Wang, Siyuan Wu, Qihui Zhang, Yuan Li, Chujie Gao, Yixin Huang, WenhanLyu, Yixuan Zhang, Xiner Li, Zhengliang Liu, Yixin Liu, Yijue Wang, Zhikun Zhang, Bertie Vidgen, BhavyaKailkhura, Caiming Xiong, Chaowei Xiao, Chunyuan Li, Eric Xing, Furong Huang, Hao Liu, Heng Ji,Hongyi Wang, Huan Zhang, Huaxiu Yao, Manolis Kellis, Marinka Zitnik, Meng Jiang, Mohit Bansal,James Zou, Jian Pei, Jian Liu, Jianfeng Gao, Jiawei Han, Jieyu Zhao, Jiliang Tang, Jindong Wang, JoaquinVanschoren, John Mitchell, Kai Shu, Kaidi Xu, Kai-Wei Chang, Lifang He, Lifu Huang, Michael Backes,Neil Zhenqiang Gong, Philip S. Yu, Pin-Yu Chen, Quanquan Gu, Ran Xu, Rex Ying, Shuiwang Ji, SumanJana, Tianlong Chen, Tianming Liu, Tianyi Zhou, William Wang, Xiang Li, Xiangliang Zhang, Xiao Wang,Xing Xie, Xun Chen, Xuyu Wang, Yan Liu, Yanfang Ye, Yinzhi Cao, Yong Chen, and Yue Zhao. Trustllm:Trustworthiness in large language models, 2024. Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut,Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly capable multimodalmodels. arXiv preprint arXiv:2312.11805, 2023.",
  "Together AI. Redpajama-data-v2: An open dataset with 30 trillion tokens for training large language models.Blog post on Together AI, Oct 2023. URL": "Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothe Lacroix,Baptiste Rozire, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundationlanguage models. arXiv preprint arXiv:2302.13971, 2023. Ahmet stn, Viraat Aryabumi, Zheng-Xin Yong, Wei-Yin Ko, Daniel Dsouza, Gbemileke Onilude, NeelBhandari, Shivalika Singh, Hui-Lee Ooi, Amr Kayid, et al. Aya model: An instruction finetuned open-accessmultilingual language model. arXiv preprint arXiv:2402.07827, 2024.",
  "Jai Vipra and Anton Korinek. Market concentration implications of foundation models: The invisiblehand of chatgpt. The Brookings Institution, 2023. URL": "Boxin Wang, Weixin Chen, Hengzhi Pei, Chulin Xie, Mintong Kang, Chenhui Zhang, Chejian Xu, Zidi Xiong,Ritik Dutta, Rylan Schaeffer, Sang T. Truong, Simran Arora, Mantas Mazeika, Dan Hendrycks, ZinanLin, Yu Cheng, Sanmi Koyejo, Dawn Song, and Bo Li. Decodingtrust: A comprehensive assessment oftrustworthiness in gpt models, 2024. Changhan Wang, Morgane Riviere, Ann Lee, Anne Wu, Chaitanya Talnikar, Daniel Haziza, Mary Williamson,Juan Pino, and Emmanuel Dupoux. Voxpopuli: A large-scale multilingual speech corpus for representationlearning, semi-supervised learning and interpretation. arXiv preprint arXiv:2101.00390, 2021. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, andDenny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprintarXiv:2203.11171, 2022. Alex Warstadt, Aaron Mueller, Leshem Choshen, Ethan Wilcox, Chengxu Zhuang, Juan Ciro, Rafael Mosquera,Bhargavi Paranjabe, Adina Williams, Tal Linzen, and Ryan Cotterell. Findings of the BabyLM challenge:Sample-efficient pretraining on developmentally plausible corpora. In Alex Warstadt, Aaron Mueller,Leshem Choshen, Ethan Wilcox, Chengxu Zhuang, Juan Ciro, Rafael Mosquera, Bhargavi Paranjabe,Adina Williams, Tal Linzen, and Ryan Cotterell (eds.), Proceedings of the BabyLM Challenge at the 27thConference on Computational Natural Language Learning, pp. 134, Singapore, December 2023. Associationfor Computational Linguistics. doi: 10.18653/v1/2023.conll-babylm.1. URL Guillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzmn, ArmandJoulin, and Edouard Grave. CCNet: Extracting high quality monolingual datasets from web crawl data. InNicoletta Calzolari, Frdric Bchet, Philippe Blache, Khalid Choukri, Christopher Cieri, Thierry Declerck,Sara Goggi, Hitoshi Isahara, Bente Maegaard, Joseph Mariani, Hlne Mazo, Asuncion Moreno, Jan Odijk,and Stelios Piperidis (eds.), Proceedings of the Twelfth Language Resources and Evaluation Conference, pp. 40034012, Marseille, France, May 2020. European Language Resources Association. ISBN 979-10-95546-34-4.URL",
  "Zheng Xin Yong, Cristina Menghini, and Stephen Bach. Low-resource languages jailbreak gpt-4. In SociallyResponsible Language Modelling Research, 2023": "Dingli Yu, Simran Kaur, Arushi Gupta, Jonah Brown-Cohen, Anirudh Goyal, and Sanjeev Arora. Skill-mix:a flexible and expandable family of evaluations for ai models. In The Twelfth International Conference onLearning Representations, 2023. Renrui Zhang, Jiaming Han, Chris Liu, Peng Gao, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, HongshengLi, and Yu Qiao. Llama-adapter: Efficient fine-tuning of language models with zero-init attention, 2023a. Zhexin Zhang, Leqi Lei, Lindong Wu, Rui Sun, Yongkang Huang, Chong Long, Xiao Liu, Xuanyu Lei, JieTang, and Minlie Huang. Safetybench: Evaluating the safety of large language models with multiple choicequestions, 2023b. Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang,Junjie Zhang, Zican Dong, et al. A survey of large language models. arXiv preprint arXiv:2303.18223, 2023a. Wenting Zhao, Xiang Ren, Jack Hessel, Claire Cardie, Yejin Choi, and Yuntian Deng. (inthe) wildchat: 570kchatgpt interaction logs in the wild. In The Twelfth International Conference on Learning Representations, 2023b. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. arXivpreprint arXiv:2306.05685, 2023. Ce Zhou, Qian Li, Chen Li, Jun Yu, Yixin Liu, Guangjing Wang, Kai Zhang, Cheng Ji, Qiben Yan, Lifang He,et al. A comprehensive survey on pretrained foundation models: A history from bert to chatgpt. arXivpreprint arXiv:2302.09419, 2023. Jiahang Zhou, Yanyu Chen, Zicong Hong, Wuhui Chen, Yue Yu, Tao Zhang, Hui Wang, Chuanfu Zhang, andZibin Zheng. Training and serving system of foundation models: A comprehensive survey. arXiv preprintarXiv:2401.02643, 2024. Wanrong Zhu, Jack Hessel, Anas Awadalla, Samir Yitzhak Gadre, Jesse Dodge, Alex Fang, Youngjae Yu,Ludwig Schmidt, William Yang Wang, and Yejin Choi. Multimodal c4: An open, billion-scale corpus ofimages interleaved with text. Advances in Neural Information Processing Systems, 36, 2024.",
  "AContributions": "To create this cheatsheet, a variety of contributors were asked to propose resources, papers, and tools relevantto open foundation model development. Those resources were grouped into sections, which were eachcurated by a subset of the contributors. We list the main curators of each section, listed alphabetically below.However, it is important to note that many contributors advised across sections, and helped with preparingthe interactive cheatsheet tool. Nay San led the speech modality, and Gabriel Ilharco led the vision modality."
}