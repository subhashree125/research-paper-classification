{
  "Abstract": "Industrial anomaly detection is crucial for quality control and predictive maintenance, but itpresents challenges due to limited training data, diverse anomaly types, and external factorsthat alter object appearances. Existing methods commonly detect structural anomalies,such as dents and scratches, by leveraging multi-scale features from image patches extractedthrough deep pre-trained networks. However, significant memory and computational demandsoften limit their practical application. Additionally, detecting logical anomaliessuch as im-ages with missing or excess elementsrequires an understanding of spatial relationships thattraditional patch-based methods fail to capture. In this work, we address these limitations byfocusing on Deep Feature Reconstruction (DFR), a memory- and compute-efficient approachfor detecting structural anomalies. We further enhance DFR into a unified framework, calledULSAD, which is capable of detecting both structural and logical anomalies. Specifically, werefine the DFR training objective to improve performance in structural anomaly detection,while introducing an attention-based loss mechanism using a global autoencoder-like networkto handle logical anomaly detection. Our empirical evaluation across five benchmark datasetsdemonstrates the performance of ULSAD in detecting and localizing both structural andlogical anomalies, outperforming eight state-of-the-art methods. An extensive ablation studyfurther highlights the contribution of each component to the overall performance improvement.Our code is available at",
  "Introduction": "Anomaly detection (AD) is a widely studied problem in many fields that is used to identify rare events orunusual patterns (Salehi et al., 2022). It enables the detection of abnormalities, potential threats, or criticalsystem failures across diverse applications such as predictive maintenance (PdM) (Tang et al., 2020; Choiet al., 2022), fraud detection (Ahmed et al., 2016; Hilal et al., 2022), and medicine (Tibshirani & Hastie,2007; Fernando et al., 2021). Despite its importance and widespread applicability, it remains a challengingtask as characterising anomalous behaviours is difficult and the anomalous samples are not known a priori(Ruff et al., 2021). Therefore, AD is often defined as an unsupervised representation learning problem (Panget al., 2020; Reiss et al., 2022) where the training data contains predominantly normal samples. The aim isto learn the normal behaviour using the samples in the training set and identify anomalies as deviations fromthis normal behaviour. This setting is also known as one-class classification (Ruff et al., 2018). Our study focuses on Industrial Anomaly Detection (IAD) (Bergmann et al., 2019), with an emphasis on thedetection of anomalies in images from industrial manufacturing processes. Image-based IAD methods assignan anomaly score to each image. Further, this study looks into anomaly localization where each pixel of animage is assigned an anomaly score. It enables fine-grained localization of the anomalous regions in the image.Over the years, it has attracted attention from both industry and academia as AD can be used for varioustasks like quality control or predictive maintenance, which are of primal interest to industries. Despite the",
  "Published in Transactions on Machine Learning Research (09/2024)": "Mohammadreza Salehi, Hossein Mirzaei, Dan Hendrycks, Yixuan Li, Mohammad Hossein Rohban, andMohammad Sabokrou. A unified survey on anomaly, novelty, open-set, and out of-distribution detection:Solutions and future challenges. Transactions on Machine Learning Research, 2022. ISSN 2835-8856. URL Wuqin Tang, Qiang Yang, Kuixiang Xiong, and Wenjun Yan.Deep learning based automatic defectidentification of photovoltaic module using electroluminescence images. Solar Energy, 201:453460, 5 2020.ISSN 0038-092X. doi: 10.1016/J.SOLENER.2020.03.049.",
  "Related Work": "Several methods have been proposed over the years for addressing Industrial AD (Bergmann et al., 2019; 2022;Jezek et al., 2021). They can be broadly categorized into feature-embedding based and reconstruction-basedmethods. We briefly highlight some relevant works in each of the category. For an extended discussion on theprior works we refer the readers to the survey by Liu et al. (2024). Feature Embedding-based methods. There are mainly three different types of IAD methods whichutilize feature embeddings from a pre-trained deep neural network: memory bank (Defard et al., 2021; Rothet al., 2022; Lee et al., 2022), student-teacher (Zhang et al., 2023; Batzner et al., 2024), and density-based(Gudovskiy et al., 2021; Yu et al., 2021). The main idea of memory bank methods is to extract featuresof normal images and store them in a memory bank during the training phase. During the testing phase,the feature of a test image is used as a query to match the stored normal features. There are two mainconstraints in these methods: how to learn useful features and how to reduce the size of the memory bank.While PatchCore (Roth et al., 2022) introduces a coreset selection algorithm, CFA (Lee et al., 2022) clustersthe features in the memory bank to reduce the size of the memory bank. Nonetheless, the performanceof the memory bank methods heavily depends on the completeness of the memory bank, which requiresa large number of normal images. Moreover, the memory size is often related to the number of trainingimages, which makes these methods not suitable for large datasets or very high-dimensional images. Inthe student-teacher approach, the student network learns to extract features of the normal images, similarto the teacher model. For anomalous images, the features extracted by the student network are differentfrom the teacher network. Batzner et al. (2024) propose to use an autoencoder model in addition to thestudent network to identify logical anomalies. For leveraging the multiscale feature from the teacher networkto detect anomalies at various scales, Deng & Li (2022) propose Reverse Distillation. Zhang et al. (2023)extended it by proposing to utilize two student networks to deal with structural and logical anomalies. Yanget al. (2020) propose to learn a deep neural network for learning to reconstruct the features of the normalimages extracted using the pre-trained backbone. For density-based methods, first, a model is trained tolearn the distribution of the features obtained from normal samples. Then, during inference, anomalies aredetected based on the likelihood of features extracted from the test images. PaDiM (Defard et al., 2021) uses",
  ": Overview of the end-to-end architecture of ULSAD": "a multivariate Gaussian to estimate the density of the features corresponding to the samples from the normalclass while FastFlow (Yu et al., 2021) and CFLOW (Gudovskiy et al., 2021) utilize normalizing flows. Reconstruction-based methods. Reconstruction-based methods assume that encoder-decoder modelstrained on normal samples will exhibit poor performance for anomalous samples. However, relying solelyon the reconstruction objective can result in the model collapsing to an identity mapping. To addressthis, structural assumptions are made regarding the data generation process. One such assumption is theManifold Assumption, which posits that the observed data resides in a lower-dimensional manifold withinthe data space. Methods leveraging this assumption impose a bottleneck by restricting the encoded spaceto a lower dimensionality than the actual data space. Common deep reconstruction models used includeAE or VAE-based approaches. Advanced strategies encompass techniques like reconstruction by memorisednormality (Gong et al., 2019), model architecture adaptation (Lai et al., 2019) and partial/conditionalreconstruction (Yan et al., 2021; Nguyen et al., 2019). Generative models like GANs are also widely employedfor anomaly detection, as the discriminator inherently calculates the reconstruction loss for samples (Zenatiet al., 2018). Variants of GANs, such as denoising GANs (Sabokrou et al., 2018) and class-conditional GANs(Perera et al., 2019), improve anomaly detection performance by increasing the challenge of reconstruction.Some methods utilize the reconstructed data from GANs in downstream tasks to enhance the amplification ofreconstruction errors for anomaly detection (Zhou et al., 2020). Lastly, DRM (Zavrtanik et al., 2021) trainsan additional discriminative network alongside a reconstruction network to improve the AD performance. In this paper, we focus on feature embedding-based methods motivated by their effectiveness in the currentSOTA methods. Specifically, we build on DFR (Yang et al., 2020), which has several benefits. First, it ismemory-efficient as it does not rely on a memory bank of extracted features, unlike PatchCore (Roth et al.,2022). Second, unlike PaDim (Defard et al., 2021), it does not make any assumption about the distribution ofthe extracted features. Third, it is computationally efficient and less impacted by the curse of dimensionalityas it operates in the lower-dimensional latent space of a deep neural network. Last, by avoiding the use ofper-pixel distance in its reconstruction objective, it is less prone to false positives (Assran et al., 2023).",
  "The ULSAD Framework for Anomaly Detection": "We propose ULSAD, a framework for simultaneously detection and localization of anomalies in images asshown in . Firstly, we utilize a feature extractor network for extracting low-dimensional features fromhigh-dimensional images, which we discuss in .1. Then, for the detection of both structural andlogical anomalies, we rely on a dual-branch architecture. The local branch detects structural anomalies withthe help of a feature reconstruction network applied to the features corresponding to patches in the image.We elaborate on this in .2. Conversely, the global branch, as discussed in .3, detects logical",
  "anomalies using an autoencoder-like network, which takes as input the image. Lastly, we provide an overviewof the ULSAD algorithm in .4 followed by a discussion on the inference process in .5": "We consider a dataset D = {(Xi, yi)}ni=1 with n samples where Xi X is an image and yi {0, 1} is thecorresponding label. We refer to the normal class with the label 0 and the anomalous class with the label1. The samples belonging to the anomalous class can contain either logical or structural anomalies. Wedenote the train, validation and test partitions of D as Dtrain, Dvalid and Dtest, respectively. The training andvalidation sets contains only normal samples, i.e., y = 0. For the sake of simplicity, we refer to the trainingset as DN = {X | (X, 0) Dtrain}. The test set Dtest includes both normal and anomalous samples.",
  "Feature Extractor": "High-dimensional images pose a significant challenge for AD (Reiss et al., 2022). Recent studies have shownthat deep convolutional neural networks (CNNs) trained on ImageNet (Russakovsky et al., 2015) capturediscriminative features for several downstream tasks. Typically, AD methods (Salehi et al., 2021; Defard et al.,2021; Yoon et al., 2023) leverage such pre-trained networks to extract features maps corresponding to partiallyoverlapping regions or patches in the images. Learning to detect anomalies using the lower-dimensionalfeatures is beneficial as it results in reduced computational complexity. A key factor determining the efficiencyof such methods is the size of the image patches being used, as anomalies can occur at any scale. To overcomethis challenge, feature maps are extracted from multiple layers of the CNNs and fused together (Salehi et al.,2021; Roth et al., 2022; Yang et al., 2020). Each element in a feature map obtained from different layers of aconvolutional network corresponds to a patch of a different size in the image depending on its receptive field.Thus, combining feature maps from multiple layers results in multi-scale representation of the image patches,which we refer to as patch features. Similar to DFR, we extract low-dimensional feature maps by combining features from multiple layers of afeature extractor which is a pre-trained CNN N parameterized by . In this paper, we consider ResNet-likearchitectures for N. With the increasing number of layers, the computation becomes increasingly expensiveas the resulting tensor becomes high-dimensional. In order to overcome this, we consider two intermediate ormid-level features. Our choice is guided by the understanding that the initial layers of such deep networkscapture generic image features, while the latter layers are often biased towards the pre-training classificationtask (Roth et al., 2022). We denote the features extracted at a layer j for an image X as N j(X). Followingthis convention, we express the feature map U U = Rchw produced by the Feature Aggregator (FA) asa concatenation of N j(X) and N j+1(X) obtained from layers j and j + 1 of N. Furthermore, to facilitatethe concatenation of features extracted from multiple layers of the extractor N, the features at the lowerresolution layer j + 1 are linearly rescaled by FA to match the dimension of the features at layer j. We definean invertible transformation f : Rchw Rck where k = h w to convert tensor to matrix andvice versa using f 1. The function f can be computed in practice by reshaping the tensor to obtain a 2Dmatrix. Now, using f, we compute Z = f(U). We denote each patch feature within the feature map Z byzk = Z[:, k] = U[:, h, w], where k = (h 1) w + w, h {1, 2, . . . , h}, w {1, 2, . . . , w}.",
  "Detecting Structural Anomalies": "Having defined Z in the previous section, we elaborate on the local branch of ULSAD for the detection of subtlelocalized defects in the images, i.e. structural anomalies. Specifically, our goal is to learn the reconstructionof the patch features using the dataset DN composed of only normal images. Therefore, we can identify thestructural anomalies when the network fails to reconstruct a patch feature during inference.",
  ": Feature Reconstruction Network": "Feature Reconstruction Network (FRN). As shownin , ULSAD utilizes a convolutional encoder-decoderarchitecture with a lower-dimensional bottleneck for learn-ing to reconstruct the feature map U using the trainingdataset DN. First, the encoder network Ne compressesthe feature U to a lower dimensional space, which in-duces the information bottleneck. It acts as an implicitregularizer, preventing generalization to features corre-sponding to anomalous images. The encoded represen-",
  "Detecting Logical Anomalies": "Although the feature reconstruction task discussed in .2 allows us to detect structural anomalies, itis not suited for identifying logical anomalies that violate the logical constraints of normal images. Recallthat such violations appear in the form of misplaced, misaligned, or surplus objects found in normal images.If we consider the example of misaligned objects, the previously discussed approach will fail as it focuses onthe individual image patches, which would be normal. It is the overall spatial arrangement of objects in theimage which is anomalous. Thus, to identify such anomalies, our goal is to learn the spatial relationshipsamong the objects present in the normal images of the training dataset DN. We achieve this with the globalbranch of ULSAD, shown in , which leverages the entire image and not just its individual patches.",
  ": Global Branch of ULSAD": "In order to achieve our goal, we start by analyz-ing the feature maps extracted using the pre-trained network N. Pre-trained CNNs tend tohave similar activation patterns for semanticallysimilar objects (Tung & Mori, 2019; Zagoruyko& Komodakis, 2017). In , we visual-ize four self-attention maps computed from thefeatures of a pre-trained Wide-Resnet50-2 net-work. It can be seen that in the first map, allthe items for the semantic class fruits receivea high attention score. The remaining attentionmaps focus on individual semantic concepts likeoranges, cereal and plate, respectively.Based on this observation and inspired by theattention-transfer concept for knowledge distil-lation (Zagoruyko & Komodakis, 2017; Tung &Mori, 2019), we propose to learn the spatial relationships (Dosovitskiy et al., 2021) among the patch featuresin U obtained from normal images. Recall that each patch feature corresponds to a patch in the image.Therefore, learning the spatial relationships among the patch features would allow us to learn the spatialrelationships among the patches in the image. This forces ULSAD to learn the relative positions of objects in",
  "c).(3)": "Then, the attention map A Rck is computed as A = ZW . For learning the spatial relations using Aas our target, we use a convolutional autoencoder-like network N = Ne Nd where Ne is the encoderand Nd is the decoder. Similar to a standard autoencoder, Ne compresses the input image X to a lowerdimensional space. However, Nd maps the encoded representation to the feature space U, which has a lowerdimension than the input space X. We denote the output of N as U = N(X). A direct approach would be to compute the self-attention map for U and minimize its distance from A.However, it makes the optimization problem computationally challenging as each vector in U is coupledwith every other vector by the network weights N (Zhang et al., 2023). To overcome this, we compute thecross-attention map A Rck between U and U. Given Z = f(U), we first compute W as:",
  "k=1lv(ak, ak) + g ld(ak, ak),(5)": "where ak = A[:, k], ak = A[:, k] and g 0 controls the effect of ld. A limitation of this approach isthat autoencoders usually struggle with generating fine-grained patterns as also observed by prior works(Dosovitskiy & Brox, 2016; Assran et al., 2023). As a result, the global branch is prone to false positives inthe presence of sharp edges or heavily textured surfaces due to the loss of high-frequency details. To addressthis limitation, we utilize the FRN N in the local branch to learn the output U. Recall that the outputof FRN U R2chw has 2c number of channels to simultaneously generate two feature maps U andU, both having dimension c h w. Out of which, U is used for learning the patch features. Here, wedefine the loss Llg to relate the local feature map U with the global feature map U as:",
  "k=1lv(zk, zk) + g ld(zk, zk),(6)": "where Z = f(U). Therefore, during inference, a difference between the U and U indicates the presenceof logical anomalies. The benefits of such a framework are two-fold: (1) it allows for learning the spatialrelationships in the normal images while reducing the chance of having false positives, and (2) doubling thechannels in the decoder allows sharing the encoder architecture, reducing the computational costs.",
  "ULSAD Algorithm Overview": "An overview of ULSAD is outlined in Algorithm 1, which can simultaneously detect structural and logicalanomalies. Firstly, we pass a normal image X from the training dataset DN through the feature extractor Nto obtain feature maps U. We normalize the features (line 4, Algorithm 1) with the channel-wise mean andstandard deviation computed over all the images in DN. We do not include this step in Algorithm 1 asthe calculation is trivial. Instead, we consider the values and to be given as input parameters for thesake of simplicity. Secondly, we obtain U by passing U through the feature reconstruction network N (line7, Algorithm 1). Recall that, U has a dimension 2c h w which can be decomposed into two featuremaps U and U each with a dimension c h w. The feature reconstruction loss Lpl is then computed between Z and Z, where Z = f(U) and Z = f(U). Thirdly, we obtain the features U by passing the inputsample X through the autoencoder N. Then for learning the spatial relationships from the normal images,we compute Lpg between the self-attention map of Z and the cross-attention map between Z and Z = f(U) (line 15, Algorithm 1). In the fourth step, we compute the loss Llg between Z and Z = f(U). Finally, themodel parameters and are updated based on the gradient of the total loss (line 21 22, Algorithm 1).The end-to-end pipeline is illustrated in .",
  "where U = f 1( Z) and U = f 1( Z)": "Since M l and M g may have different ranges of anomaly scores, we normalize each map independently. Thisnormalization ensures consistent score ranges and prevents noise in one map from overwhelming anomaliesdetected in the other. Given the variability in anomaly score distributions across datasets, we adopt aquantile-based normalization method, which makes no assumptions about the underlying score distribution. To normalize the maps, we generate two sets of anomaly maps: Ml = {M l | X Dvalid} and Mg = {M g |X Dvalid}, using images from the validation set Dvalid. For each set, we pool together the pixel valuesfrom all the anomaly maps in that set to compute the empirical quantiles at significance levels and .Specifically, for the local anomaly maps, the quantiles are denoted as ql and ql, while for the global anomalymaps, the quantiles are denoted as qg and qg. Values below q are considered normal, while those above qare marked as highly abnormal.",
  ",": "where 1hw is a matrix of ones. Mapping the empirical quantiles at and to values of 0 and 0.1 helpshighlight the anomalous regions on a 0-to-1 color scale for visualization. Normal pixels are assigned a scoreof 0, while pixels with scores between q and q gradually increase in color intensity. Pixels with scoresexceeding q change more rapidly toward 1. Note that this transformation does not affect AU-ROC scores,as these depend only on the ranking of the scores.",
  "while the test set contains 290 anomalous and 451 normal images": "MVTec AD (Bergmann et al., 2019). It consists of images from industrial manufacturing across 15categories comprised of 10 objects and 5 textures. In totality, it contains 3,629 normal images for training.For evaluation, 1,258 anomalous images with varying pixel level defects and 467 normal images. MVTec-Loco (Bergmann et al., 2022). An extension of MVTec dataset, it encompasses both localstructural anomalies and logical anomalies violating long-range dependencies. It consists of 5 categories, with1,772 normal images for training and 304 normal images for validation. It also contains 1568 images, eithernormal or anomalous, for evaluation. MPDD (Jezek et al., 2021). It focuses on metal part fabrication defects. The images are captured invariable spatial orientation, position, and distance of multiple objects concerning the camera at different lightintensities and with a non-homogeneous background. It consists of 6 classes of metal parts with 888 trainingimages. For evaluation, the dataset has 176 normal and 282 anomalous images. VisA (Zou et al., 2022). It contains 10,821 high-resolution images (9,621 normal and 1,200 anomalousimages) across 12 different categories. The anomalous images contain different types of anomalies such asscratches, bent, cracks, missing parts or misplacements. For each type of defect, there are 15-20 images, andan image can depict multiple defects. Evaluation metrics. We measure the image-level anomaly detection performance via the area under thereceiver operator curve (AUROC) based on the assigned anomaly score. To measure the anomaly localizationperformance, we use pixel-level AUROC and area under per region overlap curve (AUPRO). Furthermore,following prior works (Roth et al., 2022; Gudovskiy et al., 2021; Bergmann et al., 2019), we compute theaverage metrics over all the categories for each of the benchmark datasets. Moreover, for ULSAD, we report allthe results over 5 runs with different random seeds. Baselines. We compare our method with existing state-of-the-art unsupervised AD methods, namelyPatchCore (Roth et al., 2022), PaDim (Defard et al., 2021), CFLOW (Gudovskiy et al., 2021), FastFLOW(Yu et al., 2021), DRM (Zavrtanik et al., 2021), Reverse Distillation (RD) (Deng & Li, 2022), EfficientAD(Batzner et al., 2024) and DFR (Yang et al., 2020). In this study, we only consider baselines that are capableof both anomaly detection and localization. Implementation details. ULSAD is implemented in PyTorch (Paszke et al., 2019). For the baselines, wefollow the implementation in Anomalib (Akcay et al., 2022), a widely used AD library for benchmarking. InULSAD, we use a Wide-ResNet50-2 pre-trained on ImageNet (Zagoruyko & Komodakis, 2016) and extractfeatures from the second and third layers, similar to PathCore (Roth et al., 2022). We use a CNN for theautoencoder N in the global branch and the feature reconstruction network N in the local branch. Itconsists of convolution layers with LeakyReLU activation in the encoder and deconvolution layers in thedecoder. The architecture is provided in the Appendix A. Unless otherwise stated, for all the experiments,we consider an image size of 256 256. We train ULSAD over 200 epochs for each category using an Adamoptimizer with a learning rate of 0.0002 and a weight decay of 0.00002. We set = 0.9 and = 0.995 unlessspecified otherwise. For the baselines, we use the hyperparameters mentioned in the respective papers.",
  "Evaluation Results": "We summarize the anomaly detection performance of ULSAD in and the localization performance in. On the BTAD dataset, we improve over the DFR by approximately 2% in detection. Inspectingthe images from the dataset, we hypothesize that the difference stems from the use of a global branch inULSAD as the structural imperfections are not limited to small regions. For localization, Reverse Distillationperforms better owing to the use of anomaly maps computed per layer of the network. We can observe similarimprovements over DFR on the MVTec dataset. Although PatchCore provides superior performance onMVTec, it should be noted that even without using a memory bank, ULSAD provides comparable results. Then,we focus on more challenging datasets such as MPDD and MVTecLOCO. While MPDD contains varyingexternal conditions such as lighting, background and camera angles, MVTecLOCO contains both logicaland structural anomalies. We can observe improvements over DFR ( 12 16%) in both datasets. This",
  "MethodBTADMPDDMVTecMVTec-LOCOVisA": "PatchCore (Roth et al., 2022)96.85 | 71.4898.07 | 90.8497.71 | 91.1575.77 | 69.0997.93 | 85.12CFLOW (Gudovskiy et al., 2021)96.60 | 73.1197.42 | 88.5697.17 | 90.1476.99 | 66.9398.04 | 85.29DRM (Zavrtanik et al., 2021)59.04 | 22.4886.96 | 70.0475.01 | 49.7263.69 | 40.0671.31 | 54.68EfficientAD (Batzner et al., 2024)82.13 | 54.3797.03 | 90.4496.29 | 90.1170.36 | 66.9697.51 | 84.45FastFlow (Yu et al., 2021)96.15 | 75.2793.60 | 76.8996.44 | 88.7975.55 | 53.0497.32 | 81.70PaDiM (Defard et al., 2021)97.07 | 77.8094.51 | 81.1896.79 | 91.1771.32 | 67.9797.09 | 80.80Reverse Distillation (Deng & Li, 2022)97.85 | 81.4797.83 | 91.8697.25 | 93.1268.55 | 66.2898.68 | 91.77DFR (Yang et al., 2020)97.62 | 59.0697.33 | 90.4694.93 | 89.4261.72 | 69.7897.90 | 91.72",
  "| 0.20 | 0.35": "Analysis of main components.We investigate the impact of keycomponents in ULSAD as presentedin .Initially, we set bothl and g to 0, focusing solely ondifferences in magnitude when com-puting Lpg, Llg, and Lpl. The firstrow corresponds to using only thelocal branch. In the third row, theconsistency loss Lpg is applied tocapture spatial relationships for de-tecting logical anomalies. However,when used in isolation, it limits ULSADs performance to detecting only logical anomalies and fails to capturelocalized structural anomalies. Additionally, as discussed in .3, the global branch is prone to falsepositives in the presence of sharp edges or heavily textured surfaces. When incorporating Llg, which connectsthe global and local branches, we observe a significant improvement in performance, as shown in the fifthrow. For the sake of completeness, we also consider here a variant of the consistency loss Lpg where wecompute the 2 distance between the feature maps instead of computing the self- and cross-attention maps.We refer to the alternative in the table as Ldpg. We observe that the difference between the two variantsbecomes negligible when combined with Llg (row 4 and 5). Further, incorporating differences in directionwhen computing Lpg, Llg, and Lpl leads to improved performances across all settings as shown in the last fiverows. Overall, the best performance is obtained when both Llg and Lpg is used while considering differencesin both direction and magnitude for computing the losses.",
  ": Ablation study of the backbone network": "Effect of backbone. We investigate the impactof using different pre-trained backbones in ULSADin . We can observe that the overall bestperformance is obtained by using a Wide-ResNet101-2 architecture in both detection and localization.More specifically, for detection, Wide-ResNet vari-ants are more effective than the ResNet architec-tures, whereas, for localization performance mea-sured using Pixel AUROC, the deeper networks suchas ResNet152 and Wide-ResNet101-2 seem to haveprecedence over their shallower counterparts. Over-all, we can see that performance is robust to thechoice of pre-trained model architecture. In our experiments, we utilize a Wide-ResNet50-2 architecturewhich is used by most of our baselines for fair comparison. Effect of normalization. We analyze the impact of the quantile-based normalization on the performancemetrics by considering multiple values for and . The results are shown in . It can be seen thatthe final performance is robust to the choice of and .",
  "Memory and Computational Complexity": "We report the computational cost and memory requirements of ULSAD compared to the baselines in .For this analysis, we ran inference on the test samples in the MVTecLOCO dataset using an NVIDIA A100GPU. We measured throughput with a batch size of 32, as a measure of computational complexity, followingEfficientAD (Batzner et al., 2024). Throughput is defined as the number of images processed per secondwhen processing in batches. ULSAD demonstrates higher throughput than most baselines while maintainingcompetitive anomaly detection and localization performance. In addition to throughput, we also report peakGPU memory usage in to highlight the memory efficiency of ULSAD. It is evident that ULSAD requiresapproximately one-third of the memory compared to retrieval-based methods such as PatchCore, which isone of the state-of-the-art methods for IAD. For DFR (Yang et al., 2020), we follow the authors approach byusing a multiscale representation, concatenating features from 12 layers of the pre-trained network N foranomaly detection. This approach results in reduced throughput and increased memory usage, as shown in. With our proposed modifications in ULSAD, we achieve superior performance using features fromonly 2 layers of N, drastically reducing memory requirements to approximately one-third of DFRs andincreasing throughput by approximately two times.",
  "Limitations": "For training ULSAD, we follow the common assumption in unsupervised anomaly detection (Ruff et al., 2021;Chandola et al., 2009; Roth et al., 2022; Batzner et al., 2024) that the training dataset is clean, meaningit contains no anomalous samples. This setup is known in the literature as one-class classification (Ruffet al., 2018). However, this assumption could impact performance in real-world scenarios where anomaliesare unknown a priori. Investigating the effects of dataset contamination (Wang et al., 2019; Jiang et al.,2022; Yoon et al., 2022; Perini et al., 2023; 2022) is an active area of research, which is beyond the scopeof our current work. We leave for future research the analysis of contaminations impact on ULSAD and thedevelopment of strategies to make the learning process robust in the presence of anomalies.",
  "Conclusion": "Our study focuses on Deep Feature Reconstruction (DFR), a memory- and compute-efficient method fordetecting structural anomalies. We propose ULSAD, a unified framework that extends DFR to detect bothstructural and logical anomalies using a dual-branch architecture. In particular, we enhance the local branchstraining objective to account for differences in the magnitude and direction of patch features, thereby improving",
  "Reproducibility Statement": "We provide extensive descriptions of implementation details (in .1), algorithm (in Algorithm 1) andcode to help readers reproduce our results. Every measure is taken to ensure fairness in our comparisonsby adopting the most commonly adopted evaluation settings in the anomaly detection literature. Morespecifically, we use the Anomalib library for our experiments, whenever applicable, for comparing theirperformance with ULSAD. For methods such as DFR (Yang et al., 2020), which is not implemented in Anomalib,we refer to the original codebase provided by the authors.",
  "Varun Chandola, Arindam Banerjee, and Vipin Kumar. Anomaly detection. ACM Computing Surveys, 41(3):158, 7 2009. ISSN 0360-0300. doi: 10.1145/1541880.1541882. URL": "Heejeong Choi, Donghwa Kim, Jounghee Kim, Jina Kim, and Pilsung Kang. Explainable anomaly detectionframework for predictive maintenance in manufacturing systems. Applied Soft Computing, 125:109147, 82022. ISSN 1568-4946. doi: 10.1016/J.ASOC.2022.109147. Thomas Defard, Aleksandr Setkov, Angelique Loesch, and Romaric Audigier. PaDiM: a Patch DistributionModeling Framework for Anomaly Detection and Localization. In International Conference on PatternRecognition, volume 12664 LNCS, pp. 475489. Springer Science and Business Media Deutschland GmbH,11 2021. ISBN 9783030687984. doi: 10.48550/arxiv.2011.08785. URL Hanqiu Deng and Xingyu Li. Anomaly Detection via Reverse Distillation From One-Class Embedding. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp.97379746, 2022.",
  "Han Gao, Huiyuan Luo, Fei Shen, and Zhengtao Zhang. Towards Total Online Unsupervised AnomalyDetection and Localization in Industrial Vision, 5 2023. URL": "Dong Gong, Lingqiao Liu, Vuong Le, Budhaditya Saha, Moussa Reda Mansour, Svetha Venkatesh, andAnton Van Den Hengel. Memorizing Normality to Detect Anomaly: Memory-augmented Deep Autoencoderfor Unsupervised Anomaly Detection. Proceedings of the IEEE International Conference on ComputerVision, 2019-October:17051714, 4 2019. ISSN 15505499. doi: 10.48550/arxiv.1904.02639. URL Denis Gudovskiy, Shun Ishizaka, and Kazuki Kozuka. CFLOW-AD: Real-Time Unsupervised AnomalyDetection with Localization via Conditional Normalizing Flows. Proceedings - 2022 IEEE/CVF WinterConference on Applications of Computer Vision, WACV 2022, pp. 18191828, 7 2021. doi: 10.1109/WACV51458.2022.00188. URL Waleed Hilal, S. Andrew Gadsden, and John Yawney. Financial Fraud: A Review of Anomaly DetectionTechniques and Recent Advances. Expert Systems with Applications, 193:116429, 5 2022. ISSN 0957-4174.doi: 10.1016/J.ESWA.2021.116429. Stepan Jezek, Martin Jonak, Radim Burget, Pavel Dvorak, and Milos Skotak. Deep learning-based defectdetection of metal parts: Evaluating current methods in complex conditions. International Congress onUltra Modern Telecommunications and Control Systems and Workshops, 2021-October:6671, 2021. ISSN2157023X. doi: 10.1109/ICUMT54235.2021.9631567. Xi Jiang, Jianlin Liu, Jinbao Wang, Qiang Nie, Kai WU, Yong Liu, Chengjie Wang, and Feng Zheng. SoftPatch:Unsupervised Anomaly Detection with Noisy Data. Advances in Neural Information Processing Systems, 35:1543315445, 2022. URL",
  "Jiaqi Liu, Guoyang Xie, Jinbao Wang, Shangnian Li, Chengjie Wang, Feng Zheng, and Yaochu Jin. Deepindustrial image anomaly detection: A survey. Machine Intelligence Research, 21(1):104135, 2024": "Pankaj Mishra, Riccardo Verk, Daniele Fornasier, Claudio Piciarelli, and Gian Luca Foresti. VT-ADL:A Vision Transformer Network for Image Anomaly Detection and Localization. IEEE InternationalSymposium on Industrial Electronics, 2021-June, 4 2021. doi: 10.1109/ISIE45552.2021.9576231. URL Duc Tam Nguyen, Zhongyu Lou, Michael Klar, and Thomas Brox. Anomaly Detection With Multiple-Hypotheses Predictions. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36thInternational Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research,pp. 48004809. PMLR, 7 2019. URL",
  "Guansong Pang, Chunhua Shen, Longbing Cao, and Anton van den Hengel. Deep Learning for AnomalyDetection: A Review. ACM Computing Surveys, 54(2), 7 2020. doi: 10.1145/3439950. URL": "Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury Google, Gregory Chanan, TrevorKilleen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kpf Xamla, EdwardYang, Zach Devito, Martin Raison Nabla, Alykhan Tejani, Sasank Chilamkurthy, Qure Ai, Benoit Steiner,Lu Fang Facebook, Junjie Bai Facebook, and Soumith Chintala. PyTorch: An Imperative Style, High-Performance Deep Learning Library. Advances in Neural Information Processing Systems, 32, 2019. Pramuditha Perera, Ramesh Nallapati, and Bing Xiang. OCGAN: One-class novelty detection using gans withconstrained latent representations. Proceedings of the IEEE Computer Society Conference on ComputerVision and Pattern Recognition, 2019-June:28932901, 6 2019. ISSN 10636919. doi: 10.1109/CVPR.2019.00301. Lorenzo Perini, Vincent Vercruyssen, and Jesse Davis. Transferring the Contamination Factor betweenAnomaly Detection Domains by Shape Similarity. Proceedings of the AAAI Conference on ArtificialIntelligence, 36(4):41284136, 6 2022. ISSN 2374-3468. doi: 10.1609/AAAI.V36I4.20331. URL Lorenzo Perini, Paul-Christian Brkner, and Arto Klami. Estimating the Contamination Factors Distributionin Unsupervised Anomaly Detection. In Proceedings of the 40th International Conference on MachineLearning, pp. 2766827679. PMLR, 7 2023. URL Tal Reiss, Niv Cohen, Eliahu Horwitz, Ron Abutbul, and Yedid Hoshen. Anomaly Detection Requires BetterRepresentations. In European Conference on Computer Vision, pp. 5668, 10 2022. ISBN 9783031250682.doi: 10.1007/978-3-031-25069-9{\\_}4. URL Karsten Roth, Latha Pemula, Joaquin Zepeda, Bernhard Schlkopf, Thomas Brox, and Peter Gehler.Towards Total Recall in Industrial Anomaly Detection. In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition, pp. 1431814328, 2022. doi: 10.48550/arxiv.2106.08265. URL Lukas Ruff, Robert Vandermeulen, Nico Goernitz, Lucas Deecke, Shoaib Ahmed Siddiqui, Alexander Binder,Emmanuel Mller, and Marius Kloft. Deep One-Class Classification. In International Conference on MachineLearning, pp. 43934402. PMLR, 2018. URL Lukas Ruff, Jacob R. Kauffmann, Robert A. Vandermeulen, Gregoire Montavon, Wojciech Samek, Marius Kloft,Thomas G. Dietterich, and Klaus Robert Muller. A Unifying Review of Deep and Shallow Anomaly Detection.Proceedings of the IEEE, 109(5):756795, 5 2021. ISSN 15582256. doi: 10.1109/JPROC.2021.3052449. Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, AndrejKarpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet Large ScaleVisual Recognition Challenge. International Journal of Computer Vision, 115(3):211252, 12 2015. ISSN15731405. doi: 10.1007/S11263-015-0816-Y/FIGURES/16. URL Mohammad Sabokrou, Mohammad Khalooei, Mahmood Fathy, and Ehsan Adeli. Adversarially Learned One-Class Classifier for Novelty Detection. Proceedings of the IEEE Computer Society Conference on ComputerVision and Pattern Recognition, pp. 33793388, 12 2018. ISSN 10636919. doi: 10.1109/CVPR.2018.00356. Mohammadreza Salehi, Niousha Sadjadi, Soroosh Baselizadeh, Mohammad H. Rohban, and Hamid R. Rabiee.Multiresolution knowledge distillation for anomaly detection. Proceedings of the IEEE Computer SocietyConference on Computer Vision and Pattern Recognition, pp. 1489714907, 11 2021. ISSN 10636919. doi:10.1109/CVPR46437.2021.01466. URL",
  "Frederick Tung and Greg Mori. Similarity-preserving knowledge distillation. In Proceedings of the IEEE/CVFinternational conference on computer vision, pp. 13651374, 2019": "Siqi Wang, Yijie Zeng, Xinwang Liu, En Zhu, Jianping Yin, Chuanfu Xu, and Marius Kloft. EffectiveEnd-to-end Unsupervised Outlier Detection via Inlier Priority of Discriminative Network. Advances inNeural Information Processing Systems, 32, 2019. Xudong Yan, Huaidong Zhang, Xuemiao Xu, Xiaowei Hu, and Pheng-Ann Heng. Learning Semantic Contextfrom Normal Samples for Unsupervised Anomaly Detection. Proceedings of the AAAI Conference onArtificial Intelligence, 35(4):31103118, 5 2021. URL",
  "Jie Yang, Yong Shi, and Zhiquan Qi.DFR: Deep Feature Reconstruction for Unsupervised AnomalySegmentation. Neurocomputing, 424:922, 12 2020.doi: 10.1016/j.neucom.2020.11.018.URL": "Jinsung Yoon, Kihyuk Sohn, Chun-Liang Li, Sercan O. Arik, Chen-Yu Lee, and Tomas Pfister. Self-supervise,Refine, Repeat: Improving Unsupervised Anomaly Detection. Transactions on Machine Learning Research(TMLR), 2022. URL Jinsung Yoon, Kihyuk Sohn, Chun-Liang Li, Sercan O. Arik, and Tomas Pfister. SPADE: Semi-supervisedAnomaly Detection under Distribution Mismatch. Transactions on Machine Learning Research, 11 2023.URL Jiawei Yu, Ye Zheng, Xiang Wang, Wei Li, Yushuang Wu, Rui Zhao, and Liwei Wu. FastFlow: UnsupervisedAnomaly Detection and Localization via 2D Normalizing Flows. arXiv preprint arXiv:2111.07677, 11 2021.doi: 10.48550/arxiv.2111.07677. URL",
  "Sergey Zagoruyko and Nikos Komodakis. Wide Residual Networks. British Machine Vision Conference 2016,BMVC 2016, 2016-September:187, 5 2016. doi: 10.5244/C.30.87. URL": "Sergey Zagoruyko and Nikos Komodakis. Paying more attention to attention: Improving the performance ofconvolutional neural networks via attention transfer. In International Conference on Learning Representa-tions, 2017. URL Vitjan Zavrtanik, Matej Kristan, and Danijel Skoaj. DRM - A discriminatively trained reconstructionembedding for surface anomaly detection. Proceedings of the IEEE International Conference on ComputerVision, pp. 83108319, 8 2021.ISSN 15505499.doi: 10.1109/ICCV48922.2021.00822.URL",
  "Jie Zhang, Masanori Suganuma, and Takayuki Okatani. Contextual Affinity Distillation for Image AnomalyDetection. arXiv preprint arXiv:2307.03101, 7 2023. URL": "Kang Zhou, Yuting Xiao, Jianlong Yang, Jun Cheng, Wen Liu, Weixin Luo, Zaiwang Gu, Jiang Liu, andShenghua Gao. Encoding Structure-Texture Relation with P-Net for Anomaly Detection in Retinal Images.In European conference on computer vision, volume 12365 LNCS, pp. 360377. Springer, 2020. doi: 10.1007/978-3-030-58565-5{\\_}22. URL Yang Zou, Jongheon Jeong, Latha Pemula, Dongqing Zhang, and Onkar Dabeer. Spot-the-difference self-supervised pre-training for anomaly detection and segmentation. In European Conference on ComputerVision, pp. 392408. Springer, 2022.",
  "AImplementation Details": "ULSAD is implemented in PyTorch (Paszke et al., 2019). Specifically, we used the Anomalib (Akcay et al., 2022)library by incorporating our code within it. It helps us have a fair comparison as we use the implementationsof baselines from Anomalib. Moreover, we used a single NVIDIA A4000 GPU for all the experiments unlessmentioned otherwise. The architecture of FRN and global autoencoder-like model is provided in and6, respectively.",
  "B.1Performance on MVTecLOCO: Logical and Structural AD": "In Tables 22, 23, 24, 25, 26, and 27, we report the anomaly detection and localization results on MVTecLOCO separately for structural and logical anomalies. It can be observed that although PatchCore performsslightly better than ULSAD on structural anomalies, ULSAD delivers competitive results on logical anomalies.Moreover, as discussed previously in , the improvement in performance with PatchCore comes withthree times the memory requirement. Therefore, we consider ULSAD to be an efficient and effective approachfor the detection and localization of both logical and structural anomalies.",
  "categoryl = 0l = 0.01l = 0.5l = 0.9l = 1.0": "breakfast_box78.64 | 88.28 | 74.2279.2 | 88.51 | 74.3577.86 | 87.79 | 71.2777.95 | 86.89 | 67.0679.44 | 86.96 | 65.36juice_bottle97.82 | 92.14 | 89.2497.76 | 92.23 | 89.3897.56 | 88.78 | 88.1697.36 | 84.39 | 84.6397.08 | 83.61 | 83.47pushpins72.4 | 69.81 | 65.6972.77 | 69.84 | 65.6879.92 | 74.49 | 69.0376.98 | 74.35 | 63.1776.53 | 73.69 | 65.18screw_bag66.42 | 66.6 | 64.3967.18 | 68.47 | 65.9268.06 | 69.13 | 66.2267.56 | 69.33 | 63.6166.34 | 69.31 | 62.09splicing_connectors73.05 | 59.04 | 73.372.84 | 59.15 | 73.2972.29 | 62.66 | 72.3972.79 | 64.33 | 70.7472.36 | 64.5 | 70.57",
  "breakfast_box79.22 | 90.87 | 78.3282.45 | 88.49 | 71.0371.36 | 87.64 | 67.3883.36 | 89.34 | 72.36": "juice_bottle96.3 | 91.17 | 88.8998.08 | 87.06 | 88.0591.14 | 91.84 | 85.9297.46 | 88.81 | 87.71pushpins79.86 | 84.89 | 77.9782.46 | 87.62 | 80.4978.64 | 81.14 | 65.1288.07 | 74.22 | 66.45screw_bag66.58 | 68.83 | 66.1165.11 | 70.04 | 62.8162.09 | 67.32 | 62.9270.6 | 71.58 | 67.01splicing_connectors80.53 | 73.47 | 75.4482.85 | 73.03 | 75.1369.33 | 55.02 | 63.6981.27 | 74.94 | 74.89"
}