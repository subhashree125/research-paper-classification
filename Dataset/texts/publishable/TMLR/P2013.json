{
  "Abstract": "Prompt learning is an effective means of fine-tuning multi-modal foundation models suchas CLIP. Despite existing success, the inner mechanism of multi-modal prompt learninghas not been well understood. In this work, we identify an inductive bias of multi-modalprompt learning, which we refer to as view bias, that the learned prompts may extract onlya partial subset of useful features (views) and ignore others. This bias can undermine themodels generalization ability, particularly under distribution shifts. We further observe thatindependently trained prompts have distinct view biases, contrary to the existing belief thatthey may converge to similar local optima due to having the same cross-modal representationmatching objective. Based on our observations, we propose Multi-modal Matching Multi-Prompt Learning (M3PL), which incorporates multiple paired prompts and a cross-modalcontrastive regularizer that facilitates the prompt pairs to encapsulate a broader spectrum ofviews. Extensive experiments show that M3PL effectively boosts the models generalizationcapability, achieving state-of-the-art performance under various distribution shifts.",
  "Introduction": "Recent advancements in Vision-Language pre-trained Models (VLMs) such as CLIP (Radford et al., 2021)and ALIGN (Jia et al., 2021) have demonstrated impressive open-vocabulary generalization capabilitiesacross various downstream tasks (Li et al., 2022b; Ramesh et al., 2022; Tevet et al., 2022). However, thelarge scale of VLMs and the scarcity of high-quality training data often make fine-tuning the entire modelcostly.In response, prompt learning, which appends additional, learnable continuous vectors (prompts)",
  "River": "Sea or Lake Proportion of predicted labels for misclassified images(%) Prompt 1 Prompt 2 Prompt 3 66.68 48.40 71.63 48.33 71.20 41.00 71.37 45.43 70.40 44.87 71.11 57.52 ImageNet(ID)EuroSAT(OOD) Performance(%) Zero-shot CLIPCoOpCoCoOpPromptSRCMaPLeMPL(ours) All learnable features Predictive features for OOD test # 1 Prompt 2 Predictive features for OOD test # 2 Predictive features for OOD test # 3 Prompt 1 Prompt 3 ID predictive features : (Left) Zero-shot performance of CLIP ViT-B/16 on EuroSAT after few-shot fine-tuning on Ima-geNet. Existing prompt learning methods compromise the original generalization capability of CLIP. (Mid-dle) The distribution of predicted labels for misclassified images on EuroSAT of three independently trainedprompts with nearly identical ID test accuracies. Different prompts exhibit distinct predictive distributions.(Right) An illustration of view bias of different prompts. Each prompt only learns a partial set of OOD-useful features and thus can only solve certain OOD tasks but not all of them. to VLMs while keeping pre-trained weights intact, has emerged as an efficient alternative for fine-tuningVLMs (Zhou et al., 2022b; Khattak et al., 2023b; Lu et al., 2022; Khattak et al., 2023a; Zhou et al., 2022a). Although previous prompt learning methods (Zhou et al., 2022b;a) have significantly enhanced the in-distribution (ID) performance of the fine-tuned models, their improvements in out-of-distribution (OOD)settings are still limited. In particular, on datasets where both image and text exhibit substantial distri-bution shifts, existing prompt-based methods may even underperform zero-shot CLIP. As an example, inthe EuroSAT (Helber et al., 2019) satellite dataset, existing methods reduce the OOD accuracy of CLIP by0.07% to 7.40% after few-shot fine-tuning on ImageNet (Deng et al., 2009), as shown in (left). Why do existing prompt learning methods reduce OOD robustness? To answer this question, it is necessaryto analyze what is actually learned by prompts and how it relates to generalization. In existing work, itis believed that what is learned by prompts is roughly uniquely determined by the training data and theprompt learning objective (Chen et al., 2023). However, through an empirical study of the mistakes madeby independently trained prompts in OOD settings, we challenge this belief.In particular, we observethat prompts with nearly identical ID accuracies can make very different OOD mistakes. For example, asillustrated in (middle), a set of learned prompts with almost the same ID performance exhibitsdistinct incorrect image predictions.This phenomenon implies that prompts optimized under the sameconditions may converge to different local optima, where the model use different features for prediction. Aswill be detailed in , similar phenomena also manifest in many datasets with distribution shifts. To investigate the inner mechanism of the above phenomenon, we need to first characterize the learnedfeatures of multi-modal prompts. However, the existing analysis is also limited in this direction: to ourknowledge, the most relevant work is by Oymak et al. (2023), which studies uni-modal instead of multi-modalprompt learning. Moreover, they focus on analyzing the roles of the attention mechanism in prompt learningwithout characterizing the learned features of the prompts. To overcome this limitation, we theoreticallyanalyze multi-modal prompt learning under a structured feature model. Compared to the work by Oymaket al. (2023), our analysis draws inspiration from recent studies on the feature learning process of neuralnetworks (Allen-Zhu & Li, 2023; Shah et al., 2020) and analyzes the interaction between prompts and inputsin different modalities. Through our analysis, we show that (1) prompt learning can be viewed as a featureselection process that selects pre-trained features to match visual and textual representations on downstreamtasks, and (2) due to the multi-solution nature of the feature selection schemes, prompts may only selecta subset of useful features (views), which we term as view bias. For ID data, since the features useful forprediction are often redundant (Guyon & Elisseeff, 2003), view bias does not impact test performance muchand may even mitigate overfitting. However, in OOD scenarios where not all features useful in ID data are",
  "Published in Transactions on Machine Learning Research (09/2024)": "The results in demonstrate that the FGVCAircraft dataset exhibits low similarity with ImageNet,aligning with observations from the experiment where prompt-based fine-tuning algorithms generally under-perform zero-shot CLIP on FGVCAircraft in cross-dataset generalization settings. Conversely, the EuroSATdataset shows higher similarity to the ImageNet distribution, which partially explains the differing perfor-mance of M3PL on these datasets.",
  "Related Work": "Vision-language pre-trained models and downstream task adaptation.Vision-Language pre-trained Models (VLMs) have achieved remarkable performance in few-shot and zero-shot recognition tasksby leveraging large-scale image-text paired training data to align vision and text representations (Li et al.,2021; Jia et al., 2021; Radford et al., 2021; Kim et al., 2021). Supported by the expressive power of language,VLMs gain an understanding of open-world visual concepts, enabling them to adapt to various applications,including object detection and segmentation (Li et al., 2022a; Xu et al., 2022; Gu et al., 2022; Li et al.,2022b), image generation (Ramesh et al., 2022; Patashnik et al., 2021), action recognition (Tevet et al.,2022; Wang et al., 2021), etc. While VLMs provide generalizable representations, how to efficiently adaptthem to downstream tasks remains an important challenge. Prior work has proposed parameter-efficient tun-ing methods based on CLIP, including adapter-based (Gao et al., 2023; Zhang et al., 2022) and prompt-basedmethods (Zhou et al., 2022b;a). Our work introduces a multi-modal multi-prompt learning framework that,while maintaining parameter-efficiency during adaptation, enhances the robustness of the adapted models. Prompt learning. Prompt learning originated in the NLP domain. Early methods used expert knowledgeto manually construct prompts, also known as prompt engineering (Brown et al., 2020; Petroni et al., 2019).Later, Jiang et al. (2020); Shin et al. (2020) proposed to automatically search for templates, and Li &Liang (2021); Tsimpoukelli et al. (2021); Liu et al. (2023); Lester et al. (2021) extended the search to thecontinuous representation space. Recently, prompt learning has been introduced to vision tasks. Jia et al.(2022) incorporated learnable prompts in vision models. CoOp (Zhou et al., 2022b) and CoCoOp (Zhou et al.,2022a) add a learnable single prompt in the language branch of CLIP. MaPLe (Khattak et al., 2023a) extendsthis approach to both vision and language branches. PromptSRC (Khattak et al., 2023b) incorporates self-regularization into the prompt learning process. ProDA (Lu et al., 2022) and PLOT (Chen et al., 2023)learn multiple prompts only in the language branch; ProDA assumes a Gaussian distribution for prompts,while PLOT employs a two-stage optimization strategy based on local features and optimal transport theory.Unlike these methods, M3PL does not require modifying the objective or assuming a parameter distribution,enabling the learning of diverse prompts in a simpler and minimally constrained manner. Wang et al. (2023)",
  "Preliminaries": "This section briefly reviews the prompt learning framework based on CLIP, which our method is built upon.Empirically, we can also apply our method to other image-text pre-trained backbones such as SigLIP (Zhaiet al., 2023) in a similar fashion. Here we only focus on CLIP for simplicity. CLIP architecture. CLIP comprises both an image encoder and a text encoder and performs zero-shotclassification by matching the visual representation with different textual representations corresponding todifferent labels. Our implementation is based on CLIP with Vision Transformer (ViT) (Dosovitskiy et al.,2021) as its image encoder. Concretely, denote CLIPs image encoder as f and text encoder as g, withparameters denoted by f and g, respectively. Both encoders consist of L multi-head self-attention layers.In the vision branch, the input image X is initially divided into N fixed-size patches {x1, . . . , xN}. Next, thispatch sequence is embedded as tokens {z1, . . . , zN} and concatenated with a learnable classification token z0clsto form the input sequence Z0 = {z0cls, z01, . . . , z0N} of the first multi-head self-attention layer. Similarly, wedenote Zi = {zicls, zi1, . . . , ziN} as the input sequence for the (i + 1)-th layer. Finally, the classification tokenzLcls from the output of the L-th transformer layer is mapped to a d-dimensional vector in CLIPs alignedrepresentation space, serving as the visual representation v = f(Z0; f) Rd. In the language branch, thelabel is concatenated with a fixed template, such as a photo of {label}, to serve as input. This input isthen tokenized and embedded to form the input text token sequence W0 = {w0SOS, w01, . . . , w0K, w0y, w0EOS}(assuming the template has K tokens), where w0y represents the token corresponding to the class label.Similar to the vision side, the wLSOS from the output of the L-th transformer layer is mapped to a d-dimensional vector, serving as the textual representation t = g(W0; g) Rd.",
  "where cos(, ) denotes the cosine similarity, is a temperature parameter, v is the output of the input image,and tk represents the textual representation corresponding to label yk": "Prompt learning based on CLIP. We employ the basic method of Independent Vision-Language Prompt-ing (IVLP) (Rasheed et al., 2023) to elucidate the fundamental principles of prompt learning. At the inputlayer, Nv and Nt learnable tokens serve as visual and textual prompts, denoted by p0v and p0t, respectively.In the vision branch, p0v is concatenated directly with Z0, while in the language branch, p0t replaces the cor-responding tokens in W0, resulting in new input sequences Z0 = {p0v, Z0} and W0 = {w0SOS, p0t, w0y, w0EOS}.Given a prompt depth J, prompts will be added to the first J layers of the transformer.At the i-thlayer, the input sequences are Zi1 = {pi1v, Zi1} and Wi1 = {wi1SOS, pi1t, wi1y, wi1EOS}.Note thatthe output tokens at the positions of the previous layers prompts are replaced with new learnable tokensadded in the subsequent layer. Ultimately, we obtain the visual and textual representations denoted byv = f( Z0; f, {piv}J1i=0 ) and t = g(W0; g, {pit}J1i=0 ), respectively. During training, the pre-trained parame-ters f and g are frozen and only the learnable prompts are optimized.",
  "-65%-83%-87%-82%-88%-79%-85%-84%-88%-75%-80%": "Experimental settings. For the prompt learning method, we employ the baseline IVLP (Rasheed et al.,2023) as described in . Following standard experimental settings (Zhou et al., 2022b; Rasheed et al.,2023), we train a CLIP ViT-B/16 (Radford et al., 2021) on ImageNet in a few-shot fashion, by randomlysampling 16 images per class in training.Under identical training conditions (using the same few-shottraining data on ImageNet and hyperparameters), we independently optimize a set of prompt pairs withdifferences only in their random Gaussian initialization. Main results. As illustrated in (middle), although all trained prompts achieve nearly identicalImageNet (ID) test accuracy, their label prediction distributions of misclassified images on the EuroSATdataset (OOD) exhibit significant differences. For instance, the first prompt tends to misclassify samplesas Highway or Road whereas the third prompt tends to categorize them as Permanent Crop Land.This phenomenon implies that prompts optimized under identical conditions can converge to different localoptima, resulting in the divergence in their prediction distributions when significant distribution shifts occur. The ubiquity of view bias. To show that view bias also exists in datasets other than EuroSAT, for everydataset, we compute its Jenson-Shannon (JS) divergence (relative to EuroSAT) between independently-trained prompts predicted label distributions for misclassified images. As shown in , the average",
  "Analysis and Methodology": "In this section, we theoretically analyze multi-modal prompt learning with the representation matchingobjective and characterize the features learned by multi-modal prompts. First, by investigating the role ofthe softmax-attention mechanism in prompt learning, we show that the representation matching objectivecan be decomposed into complementary terms that isolate the feature selection effect of visual and textualprompts (.1). Then, we analyze the innate multi-solution nature of prompt learning under a linearfeature model and further relate this to view bias and OOD generalization failure (.2). Motivatedby our analysis, we then introduce the M3PL framework, showing that the view bias of single prompt paircan be mitigated by aggregating the output of multiple prompt pairs (.3), and further propose across-modal contrastive regularizer to facilitate the learning of more diverse views in different prompt pairs(.4).",
  "Prompt Learning as Feature Selection": "Self-attention model. We begin our analysis by introducing a model of single-head self-attention, whichserves as a primary building block of transformers. Concretely, let Zin = (z0, . . . , zN) R(N+1)d0 be theinput sequence of the self-attention layer with z0 being the representation token (zcls on the vision branchand wEOS on the language branch). The output of the layer is then defined as",
  "Zout = ZinWQW K ZinZinW ,(2)": "where WQ Rd0m, WK Rd0m and W Rd0d are model weights, and is a softmax nonlinearity thatacts row-wise when taking into a matrix as input. We consider the case where the weights WQ Rd0m,WK Rd0m, and W Rd0d have been pre-trained and keep frozen during prompt learning. The finalrepresentation v Rd is then mapped from the representation token in Zout, given by its first row:",
  "Our main observation here is that v is a weighted mixture of the raw input Zin and the prompt p. In otherwords, v takes the form ofv = v + (1 )W p,(5)": "where the weighting coefficient is obtained by expanding and reweighting the original softmax-attentionmap in Eq. (3), with its concrete form detailed in Appendix B.1. In multi-modal prompt learning, bothvision branch and language branch have their learnable prompts. To avoid confusion, in what follows weshall use v(v) and t(t) to denote visual and textual representations, respectively. For other parameters, wewill use subscripts v and t to denote if they belong to vision branch or language branch. Feature selection effect of prompts. We then introduce the common representation matching objectivein multi-modal prompt learning. For a C-way classification problem with training distribution D, multi-modal prompt learning aims to minimize",
  ",(6)": "where for every label y {1, . . . , C}, ty denotes the textual representations of y, and sim(, ) : Rd Rd Ris a similarity measure. In practice, sim(, ) is often the cosine similarity as in Eq. (1). In our analysis, weassume sim(, ) to be the inner product , . Note that inner-product and cosine similarity are equivalentif we normalize the representations before calculating the loss. In practice, normalizing the representationsoften results in comparable classification performance to using unnormalized representations (Radford et al.,2021). We consider a binary classification setting with y {1, 1} and = 1. This allows us to derive acleaner form of the loss function that reveals the role of multi-modal prompts, which is formally shown byProposition 1.",
  "Proof. The complete proofs of Proposition 1 and the following propositions are deferred to Appendix B": "Remarks. Proposition 1 shows that the multi-modal prompt learning objective can be decomposed intoterms that reflect the similarity between (1) the textual prompt and the visual representation, (2) thevisual prompt and the textual representation, (3) visual and textual prompts, and (4) visual and textualrepresentations. In particular, the first two terms can be viewed as a feature selection mechanism that allowsthe model to emphasize the task-related features in both visual and textual representations by adjusting ptand pv. This also justifies the advantage of multi-modal prompts as it makes the model expressible enough toaccommodate distribution shifts in both vision and text domains, which we empirically verify in Secion 6.6.",
  "View Bias and OOD Generalization": "Multi-solution property of prompt learning. Given Proposition 1, our key insight on multi-modalprompt learning is that minimizing LCE can lead to multiple representation matching schemes that givesimilar training risks, resulting in the observed view bias of different prompts. As an example, given inputZin from a class y {1, 1}, we assume that each input token zi for i {0, . . . , N} is a linear combinationof a set of orthogonal, unit-norm features fj, j {1, . . . , l} with each feature fj Rd. Similar assumptionsare common in analyzing the feature learning process of neural networks, and prior work has shown that itcan capture many practical feature learning characteristics (Allen-Zhu & Li, 2023; Zhang et al., 2024). For",
  "Pr(Zin,y)Dv, ty > v, ty= Pr(Zin,y)D[v, ty > v, ty].(8)": "Remarks. Proposition 2 reflects a distribution shift scenario where only a feature subset S remains usefulin the test distribution D. In an extreme case, if this useful subset S does not overlap with the featuresubset S extracted by prompt learning, then prompt learning would essentially lead to no improvement intest accuracy since no additional useful feature is properly conditioned during prompting. To make mattersworse, when the learned features S have spurious correlations with labels (Simon, 1954; Schlkopf et al., 2021)or contain large noise, over-reliance on those features by prompt learning may even decrease distributionalrobustness. This is consistent with our empirical observations that in some cases, prompt learning does notimprove the performance of CLIP under large distribution shifts and sometimes even decreases it.",
  "M3PL: Multiple Prompt Pairs and View Aggregation": "Motivated by the above analysis, this section proposes M3PL that aims to mitigate the intrinsic flaw of viewbias in prompt learning by introducing multiple, paired multimodal prompts and aggregating their views. Incorporating multiple prompt pairs. Specifically, building upon the vanilla prompt learning approachin , for each layer in the first J layers of CLIPs vision branch, we introduce M sets of learnableprompts, denoted as pv,1 = {pjv,1}J1j=0 , . . . , pv,M = {pjv,M}J1j=0 . Symmetrically, in the language branch, wealso add M sets of learnable prompts in the first J layers denoted by pt,i for i {1, . . . , M}. For each i, wetreat the visual prompt set pv,i and the textual prompt set pt,i as a prompt pair. The input sequences forthe i-th prompt pair and the j-th layer in the vision and language branches are then given by",
  "Contrastive Regularization": ": The M3PL framework. We introduce multiple paired visual and textual prompts and jointlyoptimize each prompt pair using representation matching (.3). Meanwhile, we randomly samplefrom the multiple prompt representations corresponding to each example for contrastive regularization tofurther enhance the learning of more diverse prompts (.4). We use superscripts to denote the indicesof prompt pairs in the figure for visual clarity.",
  ",(10)": "where for each y {1, . . . , C}, ty,i denotes the textual representation corresponding to the label y for thei-th prompt pair. During inference, we average the prediction logits obtained from all prompt pairs. Exploiting view bias by aggregating different views. The key intuition of our approach is that aswe empirically observe in Figures 1 and 2, independently trained prompts tend to have distinct view biases.Hence, aggregating them naturally results in a richer collection of useful features. Formally, Proposition 3demonstrates that if independently optimized prompts extract independent feature subsets, then aggregatingthem by simply averaging their representation matching scores can benefit OOD generalization. Proposition 3 (Effectiveness of aggregating multiple views). Under the same conditions as in Proposition 2,consider M prompts that each independently extracts a feature subset Si, i {1, . . . , M} with |Si| = s andthe elements in each Si uniformly drawn from {1, . . . , l}. We then have",
  "lsM": "Remarks. Proposition 3 assumes a scenario where different prompts learn independent views, while onlysome of them remain useful in OOD data. Since we cannot determine which views are useful solely based onID data, simply aggregating all of them seems to be a fair approach as adopted by M3PL. Yet, such aggre-gation may also induce redundant views, which is indeed observed in our experiments (see Appendix C.4.1).",
  "Cross-Modal Contrastive Regularization": "To further enhance the diversity of the learned views of different prompts, we introduce a cross-modalcontrastive regularization penalty.The main idea is to maximize the representation difference betweendifferent prompt pairs while matching the representations in the same prompt pair.Concretely, givena batch of B examples {(X1, y1), . . . , (XB, yB)}, for every example, we randomly sample a prompt pair{pv,r(i), pt,r(i)}, where r(i) {1, . . . , M} denotes the prompt pair index for the i-th example in the batch.We then calculate the cross-model contrastive regularization penalty by",
  "where > 0 is the balancing coefficient": "Explanation on contrastive regularization. Contrastive loss aims to pull positive examples togetherand push negative examples apart. Here, we treat the visual and textual representations of the same inputwith the same prompt pair as positive examples and all other cases as negative examples. In other words,representations with different prompts would become negative examples and are thus pushed apart, evenfor the same input. This relates to the effect of class collision that have been observed in the contrastivelearning literature (Goyal et al., 2023). However, instead of mitigating this effect, we actively leverage it toencourage different prompt pairs to learn more diverse views. We empirically verify this in Secion 6.6.",
  "Protocols for Evaluating Generalization Performance": "In prompt learning, the average accuracy on OOD test sets is commonly used to evaluate a models gen-eralization performance (Zhou et al., 2022a; Khattak et al., 2023a;b). However, Taori et al. (2020) pointsout that OOD accuracy is insufficient to reflect the accuracy drop under distribution shifts after fine-tuning.And Miller et al. (2021) finds through large-scale experiments that there is a strong correlation betweena models OOD and ID performance, suggesting that improvements in OOD accuracy cannot be entirelyattributed to the fine-tuning methods. Instead, it may simply be due to better fit on the ID distribution.Therefore, to comprehensively evaluate the generalization performance of prompt learning methods, we pro-pose the effective robustness ratio, inspired by Taori et al. (2020), as a complementary metric to averageOOD accuracy. Its expression is as follows:",
  "where f0 is the zero-shot CLIP, f is the fine-tuned model, and accood() denotes the average OOD accuracy": "Discussion on effective robustness ratio. This metric measures the relative accuracy drop under dis-tribution shifts for the fine-tuned model compared to the pre-trained CLIP. Generally, (f) 0% indicatesthat the model has overfitted to the ID distribution. For (f) (0%, 100%), the larger (f), the smaller thecompromise of fine-tuning methods on the generalization ability of CLIP and the greater the generalizationability. In particular, when the ID and OOD distributions are nearly identical, (f) approaches 100%.",
  "Experimental Settings": "Base-to-new generalization.This setting validates the models capacity to generalize unseen classesduring fine-tuning. We equally divided the classes of each dataset into new and base sections. After trainingon base classes, the model is directly zero-shot tested on new classes. Cross-dataset generalization. To verify the generalization performance of our method when both thevision and language modalities distributions shift during testing, we fine-tune the model on ImageNet andthen conduct zero-shot testing directly on other downstream datasets. Domain generalization. Unlike the previous settings, which exhibit significant distribution shifts in bothvision and language modalities, DG shows distribution shifts only in the vision modality and is not the mainfocus of our method. Nonetheless, our proposed M3PL still achieves comparable performance in the DGsetting, slightly surpassing previous prompt-based algorithms. Details are provided in Appendix C.3.3. Implementation details. Following MaPLe (Khattak et al., 2023a), we employ the ViT-B/16 based CLIPas the backbone. We use a few-shot setting that samples 16 shots per class and report the results averagedover three runs. For M3PL, we use a normal distribution with a mean of zero to randomly initialize theprompts, and increase the variance with the number of prompts (M) to ensure diversity. In our experiments,we set M to 8. Due to the use of Lcontrast, we use a larger batch size while reducing the training iterationsto compensate for the computation overhead. Since ProDA lacks an official implementation, we report theresults in Derakhshani et al. (2023) for the Base-to-New setting. For other baselines, we reproduce the resultsbased on the provided hyperparameters. Please refer to Appendix C.1 for additional training details. Datasets.For cross-dataset generalization and from base-to-new generalization settings, we follow theprotocols of Zhou et al. (2022a;b); Khattak et al. (2023a) and consider 11 recognition datasets, includingImageNet (Deng et al., 2009) and Caltech101 (Fei-Fei et al., 2004) for generic recognition, OxfordPets (Parkhiet al., 2012), StanfordCars (Krause et al., 2013), Flowers102 (Nilsback & Zisserman, 2008), Food101 (Bossardet al., 2014) and FGVCAircraft (Maji et al., 2013) for fine-grained classification, SUN397 (Xiao et al., 2010)for scene classification, DTD (Cimpoi et al., 2014) for texture recognition, EuroSAT (Helber et al., 2019) forsatellite image recognition, and UCF101 (Soomro et al., 2012) for action recognition . Baselines. We use zero-shot CLIP (Radford et al., 2021), CoOp (Zhou et al., 2022b), CoCoOp (Zhou et al.,2022a), ProDA (Lu et al., 2022), MaPLe (Khattak et al., 2023a), and PromptSRC (Khattak et al., 2023b).",
  "Base-to-New Generalization": "In the generalization from base to new classes, shifts in both modalities occur due to partial observationsduring fine-tuning. In , M3PL demonstrates superior performance across all average metrics on 11datasets, comprising base and new class test accuracy, harmonic mean accuracy, and effective robustnessratio.In tests on new classes, M3PL consistently outperforms the state-of-the-art PromptSRC in 9/11datasets, improving the average accuracy by 1.05% without compromising base class accuracy. It is worthmentioning that on the larger-scale dataset ImageNet, M3PL surpasses PromptSRC by 1.13% in zero-shotnew class test accuracy. Full results are detailed in Appendix C.3.1.",
  "Cross-Dataset Generalization": "illustrates that M3PL substantially improves both the average zero-shot test accuracy and theeffective robustness ratio in the cross-dataset generalization setting with shifts in both vision and languagemodalities.Compared to zero-shot CLIP, existing methods only achieve a modest increase of 0.61% inaverage accuracy, whereas M3PL realizes a substantial improvement of 2.16%. Even excluding the superiorperformance on the EuroSAT dataset, where accuracy increased by 9.12% compared to zero-shot CLIP, M3PLstill demonstrates an average accuracy gain of 1.39%. Against the state-of-the-art PromptSRC, M3PL excelsin 8/10 target datasets, boosting the effective robustness ratio by 2.8 times without markedly affecting IDperformance. These results highlight the exceptional robustness of our framework in handling distributionshifts. Full results are detailed in Appendix C.3.2.",
  "Performance Analysis": "While the empirical evidence in implies the prevalence of view bias in prompt learning, our proposedM3PL algorithm, which leverages view bias, yields varying degrees of improvement across different datasetsin the cross-dataset generalization setting. This section provides an in-depth analysis of this phenomenon. Theoretical interpretation. As shown by Proposition 1, minimizing the representation matching objectivecan be viewed as implementing a feature selection mechanism for both visual and textual pre-trained features.Hence, the degree of improvement of M3PL on a specific dataset depends on not only prompt learning butalso the overall quality and adaptability of CLIPs pre-trained features and features that are learnable indownstream ID data. For example, if pre-trained features are not predictive or the downstream ID datalacks predictive features under distribution shifts, prompt learning may not improve the OOD performancemuch. Empirically, to examine the quality and adaptability of pre-trained and ID features, we design two com-plementary metrics. (1) Informativeness: the generalization potential of CLIPs pre-trained features ona specific target dataset, measured by the average performance of zero-shot CLIP and the linear probe onCLIPs features on this dataset. (2) Transferability: the distributional similarity between target datasetsand ImageNet, measured by the average of the cosine similarity between visual and textual representationsof examples from the two datasets. We then examine the linear relationship between those metrics andM3PLs performance gains compared to zero-shot CLIP. More details are provided in Appendix D. Results. On the EuroSAT dataset, where CLIPs pre-trained features capability and the visual-textual jointdistribution similarity are both high, only M3PL fully realizes the above theoretical potential. Conversely,on the FGVCAircraft dataset, both metrics are lower, resulting in poor prompt learning performance. Nev-ertheless, M3PL still performs best among existing prompt-based methods. Performance on other datasetscan also be explained by these two metrics. For a detailed analysis, refer to Appendix D.3. Reduction in JS Divergence. Moreover, we report in the relative average JS divergence betweenthe predicted label distributions of aggregated prompts trained with different random seeds and initializa-tions using M3PL. Comparing it to the results without M3PL, we observe that the average JS divergencesignificantly decreases in all datasets, with an average reduction rate of 80%. This further demonstratesthe effectiveness of M3PL in mitigating the view bias problem.",
  "Ablation Study": "Effectiveness of M3PL. As shown in , the baseline IVLP, impaired by view bias, shows negligibleimprovement in term of OOD accuracy over zero-shot CLIP. In contrast, integrating multiple promptssignificantly enhances OOD accuracy (rows 2-4), supporting our theoretical analysis in .3. Theincorporation of visual prompts further improves model performance (rows 3-4), corroborating our analysisof the multi-modal prompt learning objective in .1. The introduction of the matching design inLmulti (row 4) proves more effective than scenarios where interplay exists among different prompt pairs (row3). In addition, without Lcontrast, the OOD accuracy of M3PL is already 1.68% higher than CLIP, indicatingthat it effectively exploits view bias and enhances generalization under distribution shifts. Please refer toAppendix C.4.1 for an ablation study on the number of prompts.",
  "More Results on SigLIP": "We extend our experiments to the new state-of-the-art vision-language pre-training model, SigLIP (Zhaiet al., 2023), which uses pairwise sigmoid loss. The results demonstrate that M3PL similarly enhances thegeneralization performance of fine-tuned SigLIP, validating the scalability and universality of our approachas a robust prompt learning method for large multi-modal models. For detailed experimental results, pleaserefer to Appendix C.5.",
  "Discussion": "Limitations. M3PL adopts a straightforward aggregation strategy of averaging different prompts logitscores. While being simple and empirically effective, this design choice may lead to suboptimal generalizationon specific OOD tasks due to feature redundancy. Additionally, our experiments are currently limited toCLIP/SigLIP ViT-B/16 and visual recognition tasks, although we expect that our results may also hold forother backbones as well. Future work. The currently rapidly evolving test-time prompt tuning methods (Shu et al., 2022) couldpotentially serve as an effective means to filter the optimal prompts learned by M3PL. Furthermore, ourmethod has the potential to extend to larger-scale VLMs and more diverse tasks. We hope that M3PL,a theoretically grounded, highly scalable, and minimally constrained framework, will establish itself as auniversal baseline of regularized prompt learning methods and facilitate future research in this domain.",
  "Broader Impact Statement": "This work is devoted to developing more robust ways to fine-tune VLMs. Therefore, it may benefit the broadresearch area of building machine learning models that are robust, generalizable, and trustworthy. However,it may also inherit the negative societal impact of the original VLMs, such as potential misuse cases, biasedoutput, and privacy and security concerns. This work was supported in part by the National Key Research and Development Program of China underSTI 2030-Major Projects 2021ZD0200300, and in part by the National Natural Science Foundation of Chinaunder Grant 62176133.",
  "Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101 mining discriminative componentswith random forests. In ECCV, pp. 446461, 2014": "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Nee-lakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, GretchenKrueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter,Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christo-pher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models arefew-shot learners. In NeurIPS, volume 33, pp. 18771901, 2020.",
  "Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchicalimage database. In CVPR, pp. 248255, 2009": "Mohammad Mahdi Derakhshani, Enrique Sanchez, Adrian Bulat, Victor G. Turrisi da Costa, Cees G.M.Snoek, Georgios Tzimiropoulos, and Brais Martinez. Bayesian prompt learning for image-language modelgeneralization. In ICCV, pp. 1523715246, 2023. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Un-terthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and NeilHoulsby. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021.",
  "Isabelle Guyon and Andr Elisseeff. An introduction to variable and feature selection. Journal of machinelearning research, 3(Mar):11571182, 2003": "Patrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth. Eurosat: A novel dataset and deeplearning benchmark for land use and land cover classification. IEEE Journal of Selected Topics in AppliedEarth Observations and Remote Sensing, 12(7):22172226, 2019. Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai,Tyler Zhu, Samyak Parajuli, Mike Guo, Dawn Song, Jacob Steinhardt, and Justin Gilmer. The many facesof robustness: A critical analysis of out-of-distribution generalization. In ICCV, pp. 83408349, October2021a.",
  "Wonjae Kim, Bokyung Son, and Ildoo Kim. Vilt: Vision-and-language transformer without convolution orregion supervision. In ICML, volume 139, pp. 55835594, 2021": "Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for fine-grained cate-gorization. In Proceedings of the IEEE International Conference on Computer Vision (ICCV) Workshops,June 2013. Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning.In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 30453059, 2021.",
  "Boyi Li, Kilian Q Weinberger, Serge Belongie, Vladlen Koltun, and Rene Ranftl. Language-driven semanticsegmentation. In ICLR, 2022a": "Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare, Shafiq Joty, Caiming Xiong, and Steven Chu HongHoi.Align before fuse: Vision and language representation learning with momentum distillation.InNeurIPS, volume 34, pp. 96949705, 2021. Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jianwei Yang, Chunyuan Li, Yiwu Zhong, LijuanWang, Lu Yuan, Lei Zhang, Jenq-Neng Hwang, Kai-Wei Chang, and Jianfeng Gao. Grounded language-image pre-training. In CVPR, pp. 1096510975, 2022b. Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. In Proceedingsof the 59th Annual Meeting of the Association for Computational Linguistics and the 11th InternationalJoint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 45824597, 2021.",
  "Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew Blaschko, and Andrea Vedaldi. Fine-grained visualclassification of aircraft. arXiv preprint arXiv:1306.5151, 2013": "John P Miller, Rohan Taori, Aditi Raghunathan, Shiori Sagawa, Pang Wei Koh, Vaishaal Shankar, PercyLiang, Yair Carmon, and Ludwig Schmidt.Accuracy on the line: on the strong correlation betweenout-of-distribution and in-distribution generalization. In ICML, volume 139, pp. 77217735, 2021. Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large number of classes.In 2008 Sixth Indian Conference on Computer Vision, Graphics & Image Processing, pp. 722729, 2008.",
  "Harshay Shah, Kaustav Tamuly, and Aditi Raghunathan. The pitfalls of simplicity bias in neural networks.In NeurIPS, 2020": "Taylor Shin, Yasaman Razeghi, Robert L. Logan IV, Eric Wallace, and Sameer Singh. AutoPrompt: ElicitingKnowledge from Language Models with Automatically Generated Prompts. In Proceedings of the 2020Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 42224235, 2020. Manli Shu, Weili Nie, De-An Huang, Zhiding Yu, Tom Goldstein, Anima Anandkumar, and Chaowei Xiao.Test-time prompt tuning for zero-shot generalization in vision-language models. In NeurIPS, volume 35,pp. 1427414289, 2022.",
  "(30)": "We then formally characterize the above probability (Denoting the event that the inequality 30 holds by E).Since here we work with the simple case that different prompts extract independent feature subsets that areuniformly drawn from {1, . . . , l}, the probability that at least one prompt extracts features in S is given by",
  "C.2Datasets": "ImageNet (Deng et al., 2009): The ImageNet dataset contains over 14 million high-resolution images,manually annotated, and categorized into 1000 classes. It is widely used for image classification and objectdetection tasks. Caltech101 (Fei-Fei et al., 2004): The Caltech101 dataset includes 101 object categories and 1 backgroundcategory, with 9k images. The number of images per category ranges from 40 to 800, with an average ofabout 50 images per category. OxfordPets (Parkhi et al., 2012): The OxfordPets dataset comprises 7349 images of cats and dogs, dividedinto 37 categories.Each category contains approximately 200 images, suitable for pet recognition andclassification tasks. StanfordCars (Krause et al., 2013): The StanfordCars dataset consists of 16,185 images of cars, categorizedinto 196 classes. Each class represents a specific car model and manufacturing year, primarily used for carclassification and recognition tasks. Flowers102 (Nilsback & Zisserman, 2008): The Flowers102 dataset includes 8189 images of flowers, catego-rized into 102 classes. The number of images per category ranges from 40 to 258, with an average of about80.",
  "C.3Full Results": "In this section, we report the average accuracy and standard deviation from three runs with three differentrandom seeds in three generalization benchmarks. It is important to note that all baselines are reproducedusing the official configuration file parameters on the same random seeds and hardware as our experiments,ensuring fairness in comparison.",
  "C.3.1Base-to-New Generalization": "The full experimental results in the base-to-new generalization setting are shown in . Please notethat, due to the absence of an official implementation for ProDA (Lu et al., 2022), we report only the resultsprovided by Derakhshani et al. (2023) in of the main text, and do not include the full results in theappendix.",
  "C.3.3Domain Generalization": "Datasets. For DG, we use four ImageNet-derived datasets with different domain shifts: ImageNetV2 (Rechtet al., 2019), ImageNet-Sketch (Wang et al., 2019), ImageNet-A (Hendrycks et al., 2021b), and ImageNet-R (Hendrycks et al., 2021a). The full experimental results in the domain generalization setting are shown in . Unlike the previoustwo settings, in the DG setting, only the visual modality experiences shifts. Although existing methods haveachieved commendable results in this scenario, our M3PL still attains enhancements in both the averagetarget dataset accuracy and the effective robustness ratio.",
  "C.4.1Effectiveness of M3PL": "presents the variation curves of both in-distribution (ID) and out-of-distribution (OOD) accuracywith the changing number of prompts (M) in the cross-dataset setting. The ID accuracy initially increasesslightly with an increase in M and then decreases, aligning with our analysis in .This trendis attributed to the view bias bias of prompts.When M is relatively small, the aggregation of usefulfeatures from different views enhances ID test accuracy. However, as M further increases, redundant featuresexacerbate overfitting. In contrast, the OOD accuracy significantly rises before gradually decreasing. This isbecause when M is too small, the insufficient variety of views leads to inadequate coverage of OOD predictivefeatures, leading to a rapid improvement in OOD performance as M increases. But with a larger M, thedominance of redundant and irrelevant features from the introduced views deteriorates the performance.Considering the trends in both ID and OOD changes, we opt for M = 8 to trade off performance withcomputational cost.",
  ": Ablation on aggregation strategy in the cross-dataset setting": "Rather than use multiple prompt pairs and aggregate their results, PromptSRC (Khattak et al., 2023b)employs Gaussian weighted prompt aggregation (GPA), which temporally aggregates the results of a singleprompt pair across its training trajectory.Here we compare the effectiveness of the two techniques in. As shown in the table, GPA yields little improvement due to the same view obtained from a singleoptimization trajectory, which is consistent with our analysis that a single optimization trajectory may failto capture a broad range of views.",
  "C.4.4Prompt Length and Depth": ": The ablation experiments on the prompt length in the cross-dataset generalizationsetting (M = 8, = 0, J = 3). The left vertical axis represents the ID test accuracy on ImageNet, and theright vertical axis indicates the average zero-shot OOD test accuracy across target datasets. The trends ofID accuracy and average OOD accuracy with the prompt length are depicted by curves with circular andsquare markers, respectively. displays the results of ablation experiments on prompt length in a cross-dataset generalizationsetting. The results indicate that both ID and OOD test accuracies generally exhibit an initial increasefollowed by a decrease. Consequently, we select a prompt length of 2 to trade off the performance betweenID and OOD scenarios. : The ablation experiments on the prompt depth in the cross-dataset generalizationsetting (M = 8, = 0, and prompt length 2).The left vertical axis represents the ID test accuracyon ImageNet, and the right vertical axis indicates the average zero-shot OOD test accuracy across targetdatasets. The trends of ID accuracy and average OOD accuracy with the prompt depth J are depicted bycurves with circular and square markers, respectively. presents the ablation study results regarding the depth of prompts J in the cross-dataset general-ization. It is observed that both ID and OOD test accuracies generally follow an initial increase followed bya decrease, with OOD test accuracy being more significantly influenced by J. We opted for J = 3 to tradeoff the performance between ID and OOD settings.",
  "C.5Full Results on SigLIP": "Our experimental design in the main text primarily focuses on CLIP. In this section, we report additionalresults on the latest state-of-the-art image-text pre-trained model, SigLIP (Zhai et al., 2023), to furthervalidate our methods scalability and generalization ability. Specifically, we use the pre-trained SigLIP-B/16as the backbone and use the same hyperparameters as in our CLIP experiments. We evaluate the performanceof the zero-shot SigLIP model, Independent V-L Prompting (IVLP), and M3PL under the standard Base-to-New generalization setting, as shown in . The results demonstrate that compared to the baselinemethod IVLP, M3PL achieves a significant improvement in out-of-distribution (OOD) performance (+8.03%)while maintaining in-distribution (ID) performance, corroborating the theoretical analysis in .2.",
  ": Few-shot linear probe performance (%) on the target datasets": "Experimental settings. We adhere to the few-shot linear probe setup in Zhou et al. (2022b), sampling16 instances per class and reporting the average results across three random seeds. Consistent with thecross-dataset generalization setting discussed in , we employ ViT-B/16 as the backbone for CLIP. Results in reveal that on the FGVCAircraft dataset, both zero-shot CLIP and linear probe CLIPdemonstrate notably low performance, indicating the inadequacy of CLIPs pre-trained features for thisdataset. Conversely, the significant improvement with linear probe CLIP on the EuroSAT dataset highlightsthe generalization potential of CLIPs pre-trained features on this distribution. In practical prompt learning scenarios, samples from the target dataset distribution are unavailable. There-fore, we evaluate the generalization potential of CLIPs pre-trained features on each dataset by averagingthe performance of zero-shot CLIP and few-shot linear probe CLIP.",
  "Visual Similarity0.32890.34330.27810.43590.40970.51030.2924Textual Similarity0.12530.19100.20170.24070.21960.30140.1993Average Similarity0.22710.26720.23990.33830.31470.40590.2459": ": Estimated similarity between the target dataset and the ImageNet distribution. Thesimilarity refers to the minimum pairwise cosine similarity between category representations of the targetdatasets and ImageNet. Experimental settings. Since prompt learning adapts to downstream tasks through few-shot learningusing frozen CLIP pre-trained features, we measure the similarity between target datasets and ImageNetusing representations from the vision and text encoders of zero-shot CLIP. Specifically, for visual similarity, wecalculate the pair-wise cosine similarity between the average representation of test images from each categoryin the target dataset and the average representation of few-shot images from each category in ImageNet,selecting the minimum value as the measure of visual similarity. For textual similarity, we use the fixedtemplate a photo of label as input, compute the pair-wise cosine similarity between text representationsof each category in the target dataset and ImageNet, and again select the minimum value as the measureof textual similarity. Ultimately, the average of visual and textual similarities is taken as the estimatedsimilarity between the distributions of the target dataset and ImageNet.",
  ": Two metrics and performance gains of M3PL compared to zero-shot CLIP": "In this section, we use multivariate linear regression to explain the performance improvements of our proposedM3PL model (relative to zero-shot CLIP), based on two metrics derived from the previous sections. The firstmetric measures the generalization potential of CLIPs original pre-trained features on the target dataset,indicated by the average performance of zero-shot CLIP and few-shot linear probe CLIP. The second metric,the average cosine similarity of textual and visual representations, estimates the distribution similaritybetween the target dataset and ImageNet. We utilize these two metrics as independent variables in a simplemultivariate linear model, with the performance gain of M3PL as the dependent variable.",
  "where i is the regression coefficient and is the intercept, X0 represents the average performance (infor-mativeness), X1 the average similarity (transferability), and Y the performance gain of M3PL": "The fitting results in a Multiple R of 0.952 and an R2 of 0.906, indicating a strong fit and demonstratingthe interpretability of our method regarding the performance on the target dataset.Furthermore, bothcoefficients 0 = 3.647 and 1 = 47.429 are positive, suggesting that the performance improvement of M3PLon a given target dataset positively correlates with both the generalization potential of CLIPs pre-trainedfeatures on that dataset and the datasets similarity to ImageNet. As shown in , the EuroSAT and SUN397 datasets exhibit high average performance and averagesimilarity metrics, which correlate with their significant performance enhancements. Conversely, the FGV-CAircraft dataset shows lower values in these metrics, resulting in the poorest performance of M3PL. TheStanfordCars and UCF101 datasets, while having high average performance, are constrained by low averagesimilarity, limiting their gains to less than 1%. In contrast, the DTD and Flowers datasets benefit fromhigher average similarity and average performance, respectively, achieving improvements exceeding 1.5%."
}