{
  "Abstract": "Despite the outstanding performance of transformers in both language and vision tasks, theexpanding computation and model size have increased the demand for efficient deployment.To address the heavy computation and parameter drawbacks, quantization is frequentlystudied in the community as a representative model compression technique and has seenextensive use on ConvNets. However, due to the unique properties of transformers, theextreme low-bit quantization applications are still limited and underexplored. In this paper,we identify the difficulty of transformer-based low-bit quantization-aware training on itsunique variation behaviors, which significantly differ from ConvNets. The term variationis defined based on comprehensive quantitative analysis in three hierarchies: various modulequantization sensitivities, outliers in static weight and activation distribution, and oscillationin dynamic parameter fluctuations. These variations of transformers bring instability to thequantization-aware training (QAT) and negatively influence the performance. We explorethe best practices to alleviate the variations influence during low-bit transformer QAT andpropose a variation-aware quantization scheme for both vision and language transformers.We extensively verify and demonstrate our scheme can alleviate the variation and improvethe performance of transformers across various models and tasks.For the 2-bit Swin-Tand binary BERT-base, our solutions achieve a 3.35% and 1.4% accuracy improvementover previous state-of-the-art methods on the ImageNet-1K dataset and GLUE benchmark.Codes and models are available at",
  "Quantization Methods": ": Left: ImageNet-1K Top-1 accuracy vs. BitOPs com-parison of 2/3/4-bit quantized ViT models using LSQ+ (Bhalgatet al., 2020) and our method. Right: GLUE performance com-parison of different binary (1-1-1-bit) BERT models. Transformer-based models have achievedimpressiveaccuracyacrossmultiplemodalities including a variety of computervision(Zhang et al., 2020a;b; Kirillovet al., 2023), natural language (Devlinet al., 2018; Chowdhery et al., 2022;Touvron et al., 2023; Achiam et al.,2023),acoustic and speech Di et al.(2021); Li et al. (2023a); Gao et al. (2024)tasks.Despite the intrinsic superiorityoftransformerarchitecture,theirre-markable performance also comes fromthe parameter numbers.For instance,Swin-L (Liu et al., 2021a) has a totalnumberofparametersof197MwithFLOPs of 34.5G. Dehghani et al. (2023)scales vision transformers to 22B for better performance. The language model GPT-3 (Brown et al., 2020)",
  "Published in Transactions on Machine Learning Research (10/2024)": "for ConvNet mainly because the training of ConvNets is more stabilized, and improving the trainingstability by reducing the minibatch variation with the better soft label will not significantly improvethe performance. Oscillation-aware Bin Regularization (OBQ) is specifically designed for transformers based on com-paring the weight and activation distribution. The OBQ will not work on ConvNet QAT mainlybecause weight oscillation and activation outliers are minor challenges in ConvNet architectures.",
  "Related Work": "Transformer-based Models: Transformer (Vaswani et al., 2017) was proposed based on an attentionmechanism and demonstrated remarkable performance across various benchmarks such as GLUE (Wanget al., 2018) and SQuAD (Rajpurkar et al., 2016).BERT (Devlin et al., 2018), RoBERTa (Liu et al.,2019a), XL-Net Yang et al. (2019), GPT Achiam et al. (2023), LLaMA Touvron et al. (2023) have servedas an essential building block in modern NLP pipelines. Inspired by the success in NLP, Vision Transform-ers(ViTs) (Dosovitskiy et al., 2020) utilize multi-head self-attention treating an image as patches/tokens.The attention mechanism can help capture both short-range and long-range visual dependencies. Variousextensions of ViTs (Touvron et al., 2021; Liu et al., 2021a; Wu et al., 2021; Dong et al., 2022; Hatamizadehet al., 2023) and more applications (Zheng et al., 2021; Arnab et al., 2021) are still emerging. Quantization Techniques: Quantization techniques aim to replace the full-precision weights and acti-vations with lower-precision representation. Based on the quantization intervals, they can be categorizedinto uniform and non-uniform quantization. While uniform quantization (Huang et al., 2022) with uniformquantization interval has better hardware affinity and efficiency, Non-uniform quantization (Li et al., 2019),due to its flexible representation, can usually better allocate the quantization values to minimize the quanti-zation error and achieve better performance. In addition, the quantization methods can also be classified asquantization-aware training (QAT) (Bhalgat et al., 2020; Liu et al., 2023a;c) and post-training quantization(PTQ) (Nagel et al., 2020; Fang et al., 2020; Wang et al., 2020; Lee et al., 2023; Xiao et al., 2023a) basedon whether to retrain a model with quantized weights and activations or start with a pre-trained modeland directly quantize it without extra training. The majority of previous transformer quantization methods,such as Liu et al. (2021c), PTQ4ViT (Yuan et al., 2021), FQ-ViT (Lin et al., 2021), ZeroQuant (Yao et al.,2022), and NoisyQuant (Liu et al., 2023b) focused on PTQ of transformers. Due to the intrinsic restrictionof PTQ, these methods only perform 8-bit or 6-bit quantization. In this work, we focus on low-bit ( 4-bit)uniform QAT setting. Knowledge Distillation: The concept of knowledge distillation is first proposed in (Hinton et al., 2015),where the core insight is to encourage student models to emulate the distribution of teacher models predic-tion. The prediction distribution of teacher models contains more information than the one-hot labels. Morerecently, various knowledge distillation methods (Cho & Hariharan, 2019; Park et al., 2019; Tung & Mori,2019; Mirzadeh et al., 2020; Shen & Xing, 2021; Li et al., 2023b) have been proposed for better efficiency",
  "X = LN(Xi + MHSA(Xi)),XO = LN(X + FFN(X)),(1)": "where Xi, X, and Xo are this transformer blocks input, intermediate representation, and output. TheMHSA module consists of h heads, and each head performs inner products with a scaling factor and a softmaxoperation. For the i-th head, input Xi is projected into query, key, and value vectors with multiplicationwith learnable weight matrix WQ,i, WK,i, WV,i, which can be written as:",
  "dk)Vi,(3)": "where 1/dk is the scaling factor for normalization. MHSA further concatenates the output of these headsto improve the representative capacity and projects to the output by multiplication with a learnable weightmatrix Wo:MHSA(Xi) = Concat(head1, head2, ..., headh)Wo.(4) Quantization. Given the real-value data to be quantized as xr, the scale factor s of the quantizer, thenumber of positive quantization levels QP , and the number of negative quantization levels QN, we can havethe quantizer qb that output the b-bit quantized representation of the input real value as xq = qb(xr) :",
  "xq = qb(xr) = s clip(xr/s, QN, QP ),(5)": "where is the rounding function that rounds the input to the nearest integer, clip(x, r1, r2) return xwith all value below r1 set to be r1 and all values above r2 set to be r2. For the unsigned quantization,QN = 0, QP = 2b 1.While for the quantization of signed data, QN = 2b1, QP = 2b1 1.Tosolve the problem that the gradient cannot back-propagate in Equation 5, the straight-through estimator(STE) (Bengio et al., 2013) is utilized to approximate the gradient during quantization-aware training. Thegradient of the rounding operation is approximated as 1 in the quantization limit. In the back-propagationwith STE, the gradient of the loss L with respect to the real-value data xr is set to be:",
  ")": ": An overview of the variation in transformers of different hierarchies: various quantization sensi-tivities of different modules, outlier in weight and activation distributions, and oscillation phenomenon indynamic parameter updates. ResNet with an accuracy loss of less than 1%, exhibits significant accuracy degradation of over 2% (Linet al., 2021) when applied to 8-bit quantization of DeiT on the same ImageNet-1K dataset.However,there is a paucity of analyses detailing the reasons behind transformers heightened sensitivity comparedto ConvNets. In this section, we will give a comprehensive analysis of the fundamental challenge in low-bit quantization of transformers referred to in this work as variation. As depicted in , we definethe term variation to include three components: (1) different quantization sensitivity of each module, (2)variance of weight and activation frequency distribution compared to ConvNets, and (3) abnormal weightoscillation phenomenon during QAT. We will explore the variation in sensitivity in .1 and delve intothe variation in distribution and its subsequent side-effect of oscillation phenomenon in Sections 4.2 and 4.3.",
  "Quantization Sensitivity": "Prior studyLi et al. (2022b) conducted a quantization robustness analysis on transformers, concludingthat the GELU activation function substantially mitigates performance during the quantization process.However, their experiments relied on post-training quantization (PTQ), which stands in stark contrastto quantization-aware training (QAT). Moreover, their experimental methodology lacked a comprehensiveanalysis of different components at a more granular level, such as the quantization impact on query, key, andvalue weight matrices. In this section, we aim to disentangle the intricacies of transformer quantization byexecuting an in-depth leave-one-out analysis employing low-bit QAT.",
  "None (FP Model)73.7591.8781.0095.2575.8191.74All68.2288.5670.2185.5072.5989.90": "All, except FFN69.4789.6072.7788.0273.0690.22All, except MHSA71.2890.6677.9392.5975.1091.21All, except query in MHSA69.6689.9473.1589.7473.6690.43All, except key in MHSA69.9289.8173.0989.9873.5890.37All, except value in MHSA70.7290.4075.8090.6674.1590.84 In terms of quantization methods, we employ LSQ+(Bhalgat et al., 2020). All components except for theanalysis target will be quantized, while the analysis target will be retained at full precision. The results ofDeiT-T (3-bit), Swin-T (2-bit), and SReT-T (3-bit) on the ImageNet-1K are presented in . The resultsacross various models and bitwidth settings indicate that MHSA, particularly the value weight matrices, are",
  ": The accuracy degradation compared to thefull-precision model when a specific head in a layer ofTransformer is quantized. The label h-l in abscissa indi-cates the head h in layer l is quantized": "While we have fully exploited the clue that thequantization sensitivity of MHSA is higher thanother components in transformers, another criti-cal clue is that some heads in MHSA are moreimportant than other heads in Transformer-basedmodels. To empirically verify this clue, we applyan analysis to quantize various heads in differentlayers in transformers. The target heads are quan-tized to 2-bit while the remaining components arequantized to 8-bit.The results of DeiT-T withthree heads in a layer and 12 layers are shownin .The results that some heads havehigher accuracy degradation show that the quan-tization sensitivity of different heads at differentlayers varies.The first and last few layers aremore sensitive to quantization. Additionally, the heads in the same layer show a variation in quantizationrobustness. For example, in layer 8 of the quantized model, the lower precision of head 0 (shown in 8-0 in) will result in a higher accuracy drop compared to the two parallel heads in the same layer.",
  "Distribution Outlier": "In .1, we have demonstrated that transformers suffer from significant variation in the sensitivity toquantization. However, previous mixed precision quantization research on ConvNet has also discovered thatdifferent parts of models have various quantization robustness. To fully understand why the sensitivity toquantization in transformers is higher than ConvNets, we visualize and quantify the distribution of differentmodules inside full-precision ConvNets and transformers to compare the real distribution variation oftransformers and ConvNets as shown in (b). To give an intuitive result on the variation of ConvNets and transformers, we first visualize the weightdistribution across different channels in full precision ResNet-18 (He et al., 2016) and DeiT-T. The resultsare shown in . Based on our investigation, the ResNet-18 model shares a similar distribution acrossdifferent channels, while the weight distribution varies significantly in different modules in the transformer-based model DeiT-T. If layer-wise quantization methods that work well for ConvNets are directly applied totransformers, the variation of distribution will result in high quantization error, which is one of the reasonsthat the ConvNet-based method failed on low-bit transformers. For the activation distribution, previous research (Wei et al., 2022; Bondarenko et al., 2023) has pointed outoutliers appear regularly and consistently across multiple layers in transformers. To quantify the fluctuationin the latent real-valued activation magnitude, we proceed to calculate the Average Standard Deviationof the Absolute Mean (SDAM) of the real-valued activation magnitude within each module of ConvNetsand transformers. The SDAM metric has been previously employed to evaluate the stability and fairnessof training in prior studies (Liu et al., 2021b). The corresponding results of the SDAM comparison aretabulated in . These numerical findings corroborate that the variability associated with transformerssurpasses that of ConvNets with respect to the activation distribution.",
  "Weight Oscillation in Training": "High variation in weight and activation distribution can lead to suboptimal quantization, thereby inducingincreased quantization errors. In QAT, certain modules fail to learn meaningful representation during theoptimization process. This effect and its association with distribution variation have been investigated inAdamBNN (Liu et al., 2021b), where the notion of flip-flop was introduced, signifying the change in quan-tization results of weights at specific iterations. We observed that low-bit transformers are also subject toa comparable effect, termed oscillation. This denotes the circumstance where the latent weights fluctuatearound the boundary of adjacent quantization bins during quantization-aware training. As per our under-standing, Nagel et al. (2022) is one of the few works probing into these effects. However, it restricts its scopeto ConvNets and their impact on batch normalization, a technique not employed in transformers. We takethe initiative to identify and analyze this oscillation phenomenon specific to transformers. An illustration of the oscillation phenomenon is shown in .Conventionally, the distribution offull-precision initialization adheres to a Gaussian distribution. There exist only a limited number of latentweights that precisely coincide with the optimal quantization value. However, when certain real-value weightswrt cross the quantization boundary at iteration t, the update of real weights |wrt wrt1| triggers an updatein the quantized value by a constant value |q(wrt ) q(wrt1)| = s. Here, s represents the quantization scaleand constitutes the length of a quantization bin in a uniform quantization scheme. As indicated by the STEdetailed in Equation 6, the gradient of the real value is assigned a value identical to this quantized value,resulting in a consistent gradient that encourages the real value to once again traverse the quantizationboundary. We further observe the side effects of transformers in the QAT. As shown in a, the weights associatedwith MHSA tend to accumulate around the quantization threshold following a certain number of epochs.b presents an example of this oscillatory behavior within the weights of transformers. This oscillationadversely influences the training and leads to substantial quantization error. The formulation of a solutionto prevent this phenomenon through the reduction of variation and mitigation of the impact will be theessential challenge in transformer quantization. In Appendix C, we quantitatively verify our observationacross multiple transformer-based models and different layers, proving that weight oscillation phenomenonis a common challenge in transformer QAT. 0.060.040.02 0.000.020.040.06 Weight DistributionQuantization Bin Center",
  "Best Practices of Transformer Quantization": "As observed in , there exists a substantial variation in transformers across three hierarchies: quanti-zation sensitivity, distribution outlier, and weight oscillation. Motivated by these observations, we aim to findthe best practices to mitigate the impacts of these variations for effective and efficient low-bit transformerquantization. Our variation-aware solutions incorporate several crucial components: a module-dependentquantization scheme, training with variation-aware knowledge distillation, and a regularization strategy tosuppress oscillatory behaviors.",
  "Module-dependent Quantization": "The scale factor s is the most important parameter in our quantization setting and will be optimized duringthe quantization-aware training. Our exploration in .1 establishes a substantial variation in thesensitivity of distinct modules to quantization. However, conventional implementations of transformer quan-tization often overlook this characteristic. In view of the variability observed in transformers, we propose amodule-dependent quantization scheme that facilitates the learning of the quantization scale s at the gran-ular module level (query, key, and value in distinct heads of MHSA). This approach contrasts with previouslayer-wise or head-wise quantization methods that assigned a uniform scale to differing modules. Instead,we implement scale-learning quantization at a higher resolution, thereby promoting a finer granularity. In addition, the outliers in the distribution located in .2 pose the challenge of an imbalanced gradientscale as these outliers will cause weight updates at a larger scale. Previous work (Bhalgat et al., 2020) hasalready pointed out the negative impact of an imbalance gradient scale. To overcome this challenge, weadopt a module-wise gradient scaling that balances the weights and scale factor gradient, fully consideringthe distribution variation in different modules. We multiply the loss of scale factor s by a gradient scale g thatencodes the magnitude of the weights in this module, which can be formulated as L",
  "QP ||w||1 , where": "||w||1 computes the L1-norm of weights in the quantized module. For the modules with higher variation, theL1-norm of weights will be higher than average, and the update of scale factor s will be decreased to ensurethat the outliers of the distribution do not influence the scale factor. Compared to the baseline quantization scheme LSQ, the proposed module-dependent quantization signifi-cantly differs from it as we use a mixed granularity for learnable scale factors considering the variationin the module sensitivity and scale the gradient considering the variation in weight distribution.",
  "Knowledge Distillation": "Another practical solution to reduce the variation mentioned in .2 and help stabilize the trainingis the Knowledge Distillation (KD) scheme. The core insight of KD is to train our quantized transformermodels with a full-precision model as the teacher. The loss function is designed to enforce the similaritybetween the output distribution of the full-precision teacher and quantized transformer student:",
  "i=1pTfc (Xi) log(pSqc (Xi)),(8)": "where the KD loss is defined as the cross-entropy between the output distributions pc of a full-precisionteacher Tf and a quantized transformer student Sq. Xi is the input sample. c and N denote the classesand the number of samples, respectively. Note that one hot label is not involved in training in our setting.The KD scheme helps our model converge fast because it learns the mapping directly from the full-precisionteacher, which contains richer information. Previous research (Yuan et al., 2020; Zhou et al., 2020; Menonet al., 2021) also points out that KD loss can be seen as a regularization term to reduce the variance duringthe training, which makes the training more stable and alleviates the influence of the distribution variation.Here we only employ KD loss as the sole objective to optimize the model, which is more effective withadequate supervision signal in KD (Shen et al., 2021b). Specifically for ViTs, one disadvantage of the conventional KD scheme is that generating the predictionpTfcof the teacher Tf consumes a relatively long time, which makes the training inefficient. To tackle this",
  "Ours1-1-179.9/79.287.186.291.936.173.880.959.274.9 (1.4)": "challenge, we followShen & Xing (2021) to utilize a multi-crop KD scheme that first random crops Mregions from one image Xi, and inputs each cropped image to the teacher model Tf to get the soft labelpTfc (Xi,m), m M, where m is the index of the cropped region. The soft label is stored together with itscoordinates. In the training phase, we directly load the soft label and cropping parameter from the storagefor the training with KD. The loss function of this multi-crop KD (MCKD) scheme is:",
  "m=1pTfc (Xi,m) log(pSqc (Xi,m)).(9)": "The higher quality of the soft label generated by this scheme would reduce the variation within a mini-batchto a greater extent. Meanwhile, the data and its corresponding label are loaded the same as the trainingwithout knowledge distillation, where the time for inference with the teacher model is saved. We furthershow in the experiment that this multi-crop KD scheme improves performance by reducing variation andsignificantly boosts efficiency.",
  "Oscillation-aware Bin Regularization": "In the analysis of .3, we identify that the weight distribution variance in the transformer causedoscillation, leading to instability during training. In the view of distribution in each quantization bin, themajority of the weights oscillate between both sides of the quantization bin. To suppress the oscillation duringQAT, we regularize the weight distribution with an Oscillation-aware Bin Regularizer (OBR) to encouragethe real-value weights to be close to the quantization bin center. The proposed OBR can be formulated as",
  "n=1V(wrn,m)),(10)": "where wrm, wqm, wrn,m represent the real value and quantized value of weights in module m, and real valueweights in the quantization bin n, respectively. || ||2 computes the L2-norm and V() computes variance forall quantization bins with more than two elements. Unlike the previous weight regularization (Chmiel et al.,2020) applied in quantization which only considers the global weight distribution, we minimize the globalquantization error and local distribution variance in a specific quantization bin. Ideally, the distribution ofthe weights in a quantization bin is regularized to be a Dirac delta distribution which can largely suppressthe oscillation during training. The final optimization target is L = LKD +LOBR, where is the weightingcoefficient to balance between LKD and LOBR. To make sure that the regularization does not influence thelearning of scale factors at the very early stage of training, we gradually increase the coefficient duringtraining by applying a cosine annealing schedule following Nagel et al. (2022).",
  "average bitwidth for mixed-precision quantization": "et al., 2018) for GLUE. For ViTs, due to the fact that the first (patch embedding) and the last (classification)layer are more sensitive to perturbation compared to intermediate layers, we fix their bitwidth to 8-bitfollowing previous work (Yang & Jin, 2021). Training Details. We adopt real-value pre-trained weights as initialization. The full-precision ViTs aretrained from scratch, and the full-precision BERT for various tasks are obtained from public repository1. Ourbaseline quantization method is LSQ+ (Bhalgat et al., 2020). Details of all hyper-parameters and settingsare shown in and in the Appendix.",
  "Comparison with State-of-the-Art Methods": "NLP Tasks. compares our variation-aware solution with existing methods for BERT-base on theGLUE benchmark under binarization (1-bit) setting. The binarized BERT-base with our solution establishesa new state-of-the-art across most tasks in GLUE and improves the average accuracy by 1.4% compared toBiT (Liu et al., 2022). Vision Tasks. fairly compares our variation-aware solution with existing methods for DeiT-T,SReT-T, Swin-T, and Swin-S on the ImageNet-1K dataset. We also report the results of baseline LSQ+ withknowledge distillation using the same teacher model to show that performance improvement cannot simplybe summarized as learning from the teacher model. Compared with the FP model, our 4-bit quantized DeiT-T achieves 74.71% Top-1 accuracy with a 0.96% absolute gain. Compared with the previous quantizationmethods, our model also demonstrates remarkable improvement. For example, our 4-bit Swin-T achievesa Top-1 accuracy of 82.42%, which has an absolute gain of 2.83% compared to Q-ViT (Li et al., 2022b).Our method is especially effective for low-precision 2-bit quantization, as our 2-bit Swin-T and Swin-S yields77.66% and 78.11% Top-1 accuracy, which is 3.35% and 5.1% higher than previous state-of-the-art methods. Training Efficiency. Our methods show better training efficiency on ViTs with the help of a multi-cropknowledge distillation scheme.The better quantization scheme and regularization also help our modelsconverge faster than previous methods with the same training configurations. We only train our modelswith 150 epochs, sufficient to outperform previous methods with 300 epochs shown in . The totaltraining time for our DeiT-T with 4 NVIDIA A100 GPUs is 57.3 hours, significantly lower than baselinemethods shown in .",
  "Ablation Study": "We first perform an overall ablation experiment on 4-bit quantized DeiT-T and 2-bit Swin-T to look intothe effectiveness of all proposed modules. The results are shown in . From the average StandardDeviation of the Absolute Mean (SDAM) and accuracy results, we can see that each module helps alleviatethe variation influence and improve the performance of quantized transformers. The following subsectionsprovide a more detailed ablation study of each module.",
  "Ours74.7192.022.13e-277.6693.682.97e-2": "Ours w/o Multi-crop Knowledge Distillation73.5691.522.30e-274.1592.202.95e-2Ours w/o Module-dependent Quantization73.7991.547.15e-274.8092.338.89e-2Ours w/o Oscillation-aware Bin Regularization74.2291.413.79e-275.9193.056.13e-2 Multi-crop Knowledge Distillation for ViTs. compares the Top-1 accuracy of 4-bit quantizedDeiT-T without knowledge distillation, with vanilla KD, and with our multi-crop KD of different teachers.The results demonstrate an improvement in both accuracy and efficiency.The teacher model of higheraccuracy can improve the performance of student ViTs regardless of architecture. The training time can alsobe reduced as the soft label is extracted before the training. The time in does not include the timefor soft label generation, which can be ignored when we have to apply QAT on different models and settings. 1.000.750.500.250.000.250.500.751.001.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00",
  ":Loss landscape visualization of the 4-bit quantizedSwin-T using the baseline (LSQ+ quantization) method and ourmodule-dependent quantization method": "Module-dependentQuantization.The module-dependent quantization ap-plies a finer-grained quantization schemeat the module level and scales the scalefactors gradients to ensure the scalefactor update is not influenced by thevariation in transformers.We visualizethe loss landscape showing the smooth-ness of optimization following Li et al.(2018) shown in b. Compared tothe baseline quantized model, the morecentralized and smoother loss landscapereflectsthattheproposedquantiza-tion scheme substantially improves thetraining stability and efficiency. Oscillation-awareBinRegulariza-tion. To better know how our oscillation-aware bin regularization can help alleviate the oscillation, wequantify the degree of oscillation during training by measuring the frequency of this phenomenon over time.",
  "f t = m ot + (1 m) f t1, whereot = (xintt= xintt1) (sign(tint) = sign(tprevint )).(12)": "We define the weights as oscillating weights at iteration t as f t >0.005.The Top-1 Accuracy of 3-bitquantized SReT-T and the percentage of oscillating weights are shown in . From the results, wecan see a clear negative correlation between weight oscillation percentage and model performance.Theproposed Oscillation-aware Bin Regularization (OBR) with a gradually increasing coefficient helps stabilizethe training to achieve higher model accuracy.",
  "Hardware Cost Comparison": "One direct solution to solve the variation in the quantization sensitivity of different modules is mixed-precision quantization (MPQ), which assigns various bitwidth to different components. However, searchingfor or learning the optimal bitwidth assignment is difficult and time-consuming, especially for QAT. Amongthe existing Transformer MPQ methods Liu et al. (2021c); Li et al. (2022b); Xiao et al. (2023b); Ranjan &Savakis (2024); Xu et al. (2024), Q-ViT Li et al. (2022b) is the only work that applies mixed-precision toQAT while the remaining all targets PTQ. Under the same average bitwidth setting, our module-dependentscaling scheme is a more efficient solution to the quantization sensitivity variation than MPQ. To quantitatively examine the inference efficiency of our method, We compare the hardware utilization ofthe Q-ViT Li et al. (2022b) MPQ solution with ours, including multiply-accumulate (MAC) units in terms ofarea and power dissipation. We implemented the MAC operator by Verilog HDL and utilized Cadence Genusto obtain the synthesized area under TSMC 40nm technology and 0.5GHz clock frequency. Specifically, thebitwidth of the partial sum is set to 32bit in case of overflow. The results are listed in , where theMPQ approach imposes significant overhead in terms of hardware costs compared to our efficient module-dependent scheme. The details of mixed-precision settings are listed in the Appendix.",
  "Conclusion": "In this work, we have provided a comprehensive understanding of the complexities associated with transform-ers low-bit quantization. Through an in-depth analysis of quantization sensitivity, contrasting ConvNetswith transformers, and monitoring the weight oscillation during training, we elucidate that the variationbehavior inherent to transformers poses considerable challenges to low-bit quantization-aware training. Toaddress the challenges presented by variation, we explore the best practice and propose an effective variation-aware quantization technique, including module-dependent quantization and scaling, variation-aware knowl-edge distillation, and oscillation-aware bin regularization. Through extensive demonstrations, we have shownthat our proposed solution to reduce variation in transformers results in state-of-the-art performance acrossvarious transformer architectures on both vision and language tasks and significantly improves efficiency. This research was supported by National Natural Science Foundation of China/HKSAR Research GrantsCouncil Joint Research Scheme under Grant N_HKUST627/20 and by HKSAR RGC General ResearchFund (GRF) #16208823.",
  "Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton.Layer normalization.arXiv preprintarXiv:1607.06450, 2016": "Haoli Bai, Wei Zhang, Lu Hou, Lifeng Shang, Jin Jin, Xin Jiang, Qun Liu, Michael Lyu, and Irwin King.Binarybert: Pushing the limit of bert quantization. In Proceedings of the 59th Annual Meeting of the As-sociation for Computational Linguistics and the 11th International Joint Conference on Natural LanguageProcessing (Volume 1: Long Papers), pp. 43344348, 2021.",
  "Yoshua Bengio, Nicholas Lonard, and Aaron Courville.Estimating or propagating gradients throughstochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013": "Yash Bhalgat, Jinwon Lee, Markus Nagel, Tijmen Blankevoort, and Nojun Kwak. Lsq+: Improving low-bit quantization through learnable offsets and better initialization.In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition Workshops, pp. 696697, 2020. Yelysei Bondarenko, Markus Nagel, and Tijmen Blankevoort. Quantizable transformers: Removing outliersby helping attention heads do nothing. In Thirty-seventh Conference on Neural Information ProcessingSystems, 2023. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.Advances in neural information processing systems, 33:18771901, 2020.",
  "Jang Hyun Cho and Bharath Hariharan. On the efficacy of knowledge distillation. In Proceedings of theIEEE/CVF international conference on computer vision, pp. 47944802, 2019": "Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts,Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling languagemodeling with pathways. arXiv preprint arXiv:2204.02311, 2022. Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin Gilmer, An-dreas Peter Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin, et al.Scaling visiontransformers to 22 billion parameters. In International Conference on Machine Learning, pp. 74807512.PMLR, 2023. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchicalimage database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248255. Ieee,2009.",
  "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectionaltransformers for language understanding. arXiv preprint arXiv:1810.04805, 2018": "Shangzhe Di, Zeren Jiang, Si Liu, Zhaokai Wang, Leyan Zhu, Zexin He, Hongming Liu, and Shuicheng Yan.Video background music generation with controllable music transformer. In Proceedings of the 29th ACMInternational Conference on Multimedia, pp. 20372045, 2021. Yifu Ding, Haotong Qin, Qinghua Yan, Zhenhua Chai, Junjie Liu, Xiaolin Wei, and Xianglong Liu. Towardsaccurate post-training quantization for vision transformer. In Proceedings of the 30th ACM InternationalConference on Multimedia, pp. 53805388, 2022. Peiyan Dong, Lei Lu, Chao Wu, Cheng Lyu, Geng Yuan, Hao Tang, and Yanzhi Wang. Packqvit: Fastersub-8-bit vision transformers via full and packed quantization on the mobile. In Thirty-seventh Conferenceon Neural Information Processing Systems, 2023. Xiaoyi Dong, Jianmin Bao, Dongdong Chen, Weiming Zhang, Nenghai Yu, Lu Yuan, Dong Chen, andBaining Guo. Cswin transformer: A general vision transformer backbone with cross-shaped windows. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1212412134,2022. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Un-terthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth16x16 words: Transformers for image recognition at scale. In International Conference on Learning Rep-resentations, 2020. Jun Fang, Ali Shafiee, Hamzah Abdel-Aziz, David Thorsley, Georgios Georgiadis, and Joseph H Hassoun.Post-training piecewise linear quantization for deep neural networks. In European Conference on ComputerVision, pp. 6986. Springer, 2020. Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq: Accurate post-training quan-tization for generative pre-trained transformers. In The Eleventh International Conference on LearningRepresentations, 2023a. Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. OPTQ: Accurate quantization for gen-erative pre-trained transformers. In The Eleventh International Conference on Learning Representations,2023b. Shengyi Gao, Zhe Chen, Guo Chen, Wenhai Wang, and Tong Lu. Avsegformer: Audio-visual segmentationwith transformer. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pp. 1215512163, 2024.",
  "Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv preprintarXiv:1503.02531, 2015": "Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, MarcoAndreetto, and Hartwig Adam.Mobilenets: Efficient convolutional neural networks for mobile visionapplications. arXiv preprint arXiv:1704.04861, 2017. Xijie Huang, Zhiqiang Shen, Shichao Li, Zechun Liu, Hu Xianghong, Jeffry Wicaksana, Eric Xing, andKwang-Ting Cheng. Sdq: Stochastic differentiable quantization with mixed precision. In InternationalConference on Machine Learning, pp. 92959309. PMLR, 2022. Norman P Jouppi, Cliff Young, Nishant Patil, David Patterson, Gaurav Agrawal, Raminder Bajwa, SarahBates, Suresh Bhatia, Nan Boden, Al Borchers, et al. In-datacenter performance analysis of a tensorprocessing unit. In Proceedings of the 44th annual international symposium on computer architecture, pp.112, 2017. Patrick Judd, Jorge Albericio, Tayler Hetherington, Tor M Aamodt, and Andreas Moshovos. Stripes: Bit-serial deep neural network computing. In 2016 49th Annual IEEE/ACM International Symposium onMicroarchitecture (MICRO), pp. 112. IEEE, 2016. Jangho Kim, Jayeon Yoo, Yeji Song, KiYoon Yoo, and Nojun Kwak.Finding efficient pruned networkvia refined gradients for pruned weights. In Proceedings of the 31st ACM International Conference onMultimedia, pp. 90039011, 2023. Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao,Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al.Segment anything.arXiv preprintarXiv:2304.02643, 2023. Jung Hyun Lee, Jeonghoon Kim, Se Jung Kwon, and Dongsoo Lee. Flexround: Learnable rounding based onelement-wise division for post-training quantization. In International Conference on Machine Learning,pp. 1891318939. PMLR, 2023.",
  "Shih-Yang Liu, Zechun Liu, and Kwang-Ting Cheng. Oscillation-free quantization for low-bit vision trans-formers. In International Conference on Machine Learning, pp. 2181321824. PMLR, 2023a": "Yijiang Liu, Huanrui Yang, Zhen Dong, Kurt Keutzer, Li Du, and Shanghang Zhang. Noisyquant: Noisy bias-enhanced post-training activation quantization for vision transformers. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition, pp. 2032120330, 2023b. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, LukeZettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXivpreprint arXiv:1907.11692, 2019a. Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swintransformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVFInternational Conference on Computer Vision, pp. 1001210022, 2021a. Zechun Liu, Haoyuan Mu, Xiangyu Zhang, Zichao Guo, Xin Yang, Kwang-Ting Cheng, and Jian Sun.Metapruning:Meta learning for automatic neural network channel pruning.In Proceedings of theIEEE/CVF international conference on computer vision, pp. 32963305, 2019b. Zechun Liu, Zhiqiang Shen, Shichao Li, Koen Helwegen, Dong Huang, and Kwang-Ting Cheng. How doadam and training strategies help bnns optimization. In International Conference on Machine Learning,pp. 69366946. PMLR, 2021b. Zechun Liu, Barlas Oguz, Aasish Pappu, Lin Xiao, Scott Yih, Meng Li, Raghuraman Krishnamoorthi, andYashar Mehdad. Bit: Robustly binarized multi-distilled transformer. Advances in neural informationprocessing systems, 35:1430314316, 2022. Zechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre Stock, Yashar Mehdad, Yangyang Shi,Raghuraman Krishnamoorthi, and Vikas Chandra. Llm-qat: Data-free quantization aware training forlarge language models. arXiv preprint arXiv:2305.17888, 2023c.",
  "Antonio Polino, Razvan Pascanu, and Dan Alistarh. Model compression via distillation and quantization.In International Conference on Learning Representations, 2018": "Haotong Qin, Yifu Ding, Mingyuan Zhang, YAN Qinghua, Aishan Liu, Qingqing Dang, Ziwei Liu, and Xian-glong Liu. Bibert: Accurate fully binarized bert. In International Conference on Learning Representations,2021. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang.Squad: 100,000+ questions formachine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in NaturalLanguage Processing, pp. 23832392, 2016.",
  "Frederick Tung and Greg Mori. Similarity-preserving knowledge distillation. In Proceedings of the IEEE/CVFInternational Conference on Computer Vision, pp. 13651374, 2019": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ukasz Kaiser,and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30,2017. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. Glue: A multi-task benchmark and analysis platform for natural language understanding. In International Conferenceon Learning Representations, 2018. Peisong Wang, Qiang Chen, Xiangyu He, and Jian Cheng. Towards accurate post-training network quantiza-tion via bit-split and stitching. In International Conference on Machine Learning, pp. 98479856. PMLR,2020. Xiuying Wei, Yunchen Zhang, Xiangguo Zhang, Ruihao Gong, Shanghang Zhang, Qi Zhang, Fengwei Yu, andXianglong Liu. Outlier suppression: Pushing the limit of low-bit transformer language models. Advancesin Neural Information Processing Systems, 35:1740217414, 2022. Xiuying Wei, Yunchen Zhang, Yuhang Li, Xiangguo Zhang, Ruihao Gong, Jinyang Guo, and Xianglong Liu.Outlier suppression+: Accurate quantization of large language models by equivalent and optimal shiftingand scaling. arXiv preprint arXiv:2304.09145, 2023. Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan, and Lei Zhang. Cvt: Introduc-ing convolutions to vision transformers. In Proceedings of the IEEE/CVF International Conference onComputer Vision, pp. 2231, 2021. Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han. Smoothquant: Accurateand efficient post-training quantization for large language models. In International Conference on MachineLearning, pp. 3808738099. PMLR, 2023a. Junrui Xiao, Zhikai Li, Lianwei Yang, and Qingyi Gu. Patch-wise mixed-precision quantization of visiontransformer. In 2023 International Joint Conference on Neural Networks (IJCNN), pp. 17. IEEE, 2023b. Qizhe Xie, Minh-Thang Luong, Eduard Hovy, and Quoc V Le. Self-training with noisy student improvesimagenet classification.In Proceedings of the IEEE/CVF conference on computer vision and patternrecognition, pp. 1068710698, 2020.",
  "Linjie Yang and Qing Jin. Fracbits: Mixed precision quantization via fractional bit-widths. In Proceedingsof the AAAI Conference on Artificial Intelligence, volume 35, pp. 1061210620, 2021": "Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le. Xlnet: Gen-eralized autoregressive pretraining for language understanding. Advances in neural information processingsystems, 32, 2019. Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong Li, and Yuxiong He. Zero-quant: Efficient and affordable post-training quantization for large-scale transformers. Advances in NeuralInformation Processing Systems, 35:2716827183, 2022. Chong Yu, Tao Chen, Zhongxue Gan, and Jiayuan Fan. Boost vision transformer with gpu-friendly spar-sity and quantization. In Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition, pp. 2265822668, 2023. Li Yuan, Francis EH Tay, Guilin Li, Tao Wang, and Jiashi Feng. Revisiting knowledge distillation via labelsmoothing regularization. In Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition, pp. 39033911, 2020.",
  "Zhihang Yuan, Chenhao Xue, Yiqi Chen, Qiang Wu, and Guangyu Sun. Ptq4vit: Post-training quantizationframework for vision transformers. arXiv preprint arXiv:2111.12293, 2021": "Dong Zhang, Hanwang Zhang, Jinhui Tang, Xian-Sheng Hua, and Qianru Sun. Causal intervention forweakly-supervised semantic segmentation. Advances in Neural Information Processing Systems, 33:655666, 2020a. Dong Zhang, Hanwang Zhang, Jinhui Tang, Meng Wang, Xiansheng Hua, and Qianru Sun. Feature pyramidtransformer. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328,2020, Proceedings, Part XXVIII 16, pp. 323339. Springer, 2020b. Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei Fu, JianfengFeng, Tao Xiang, Philip HS Torr, et al. Rethinking semantic segmentation from a sequence-to-sequenceperspective with transformers. In Proceedings of the IEEE/CVF conference on computer vision and patternrecognition, pp. 68816890, 2021. Helong Zhou, Liangchen Song, Jiajie Chen, Ye Zhou, Guoli Wang, Junsong Yuan, and Qian Zhang. Rethink-ing soft labels for knowledge distillation: A biasvariance tradeoff perspective. In International Conferenceon Learning Representations, 2020.",
  "AAdditional Quantization Sensitivity Analysis": "In our paper, the leave-one-out quantization experiments are carried out to verify the quantization sen-sitivity. By quantizing every module except one, a more precise estimation of the real sensitivity of eachmodule to quantization can be achieved. In practical quantization-aware training situations, most modulesoperate with low precision and are interconnected. Additionally, we provide another form of quantize-one-module-only analysis that only quantizes specific parameters. The results are listed in , which alsoproves that MHSA is the most sensitive module to the quantization perturbation.",
  "BTraining Details and Dynamics": "When training BERT-base across different datasets in the GLUE benchmark, most hyper-parameters are thesame. These shared settings include weight decay of 1e-2, warmup proportion of full epochs 10%, linear lrscheduler, and Adam optimizer. No data augmentation is applied, and the teacher model for the knowledgedistillation is the full-precision pre-trained model of different tasks. For the training epochs, max sequencelength, batch size, and learning rate, the settings for different tasks are listed in .",
  "Epoch6620402004040200Max Seq Length1281281286464128128128Batch Size163216166416832Initial lr2e-42e-42e-42e-45e-45e-45e-45e-4": "When comparing the results of ViT quantization using our methods with other methods in the experiments,we use the training settings and hyper-parameters shown in .Generally, most of these hyper-parameters and training settings are the same across different ViT models and different bitwidths settings.We found that applying the proposed Oscillation-aware Bin Regularization (OBR) is more effective for low-bitquantization, including 3-bit and 2-bit. The different performance of OBR among different bitwidth is mainlybecause penalizing the oscillation during QAT will harm the normal optimization of latent weights, whichis more prominent in higher bitwidth. Accordingly, we only apply OBR to the 2-bit and 3-bit quantization. compares the training loss and Top-1 test accuracy for 4-bit quantized DeiT-T using our method andLSQ+ (Bhalgat et al., 2020). The core advantages of both effectiveness and efficiency are shown here. Interms of effectiveness, our method can achieve higher Top-1 accuracy and has a more stable loss scheme.For efficiency, our method helps the model converge faster, with only half of the total training epochs.",
  "CGeneralizability of the Observations": "In .3, we reveal that the weight and activation distribution outliers will potentially lead to anoscillation phenomenon in QAT, which harms the training stability and quantized model performance. Inthis section, we further generalize the research target of oscillation phenomenon to various transformer-basedmodels and conduct a finer granularity analysis on the oscillation of different layers. We use the quantitativeoscillation percentage in QAT defined in .3 to investigate more transformer-based models anddive into specific layers. The results of the average oscillation percentage of DeiT-T (W4A4, W3A3),Swin-T (W2A2), and BERT-base (W1A1E1) are listed in . We also include ResNet-18 (W4A4)and MobileNetV2 (W2A2) for comparison. The QAT methods are LSQ and all models are initialized fromfull-precision pretrained models. As can be seen from the results, all transformer-based models suffer fromthis weight oscillation phenomenon, while it is not observed in ConvNets. These results further prove thatweight oscillation phenomenon is a common challenge in transformer QAT. We also include a per-layer oscillation analysis of Swin-T (W2A2) in , showing that oscillationphenomenon occurs across all linear weight layers in transformers. Another finding in this per-layer oscillationanalysis is that weights in self-attention have a higher oscillation ratio compared to MLP, which correspondsto our conclusion that MHSA is more sensitive to quantization compared to other components.",
  "DAttention Map Visualization on ViTs": "To demonstrate how our quantization approach preserves the representational capacity of ViT models, weillustrate the attention map of the quantized Swin-T following (Dosovitskiy et al., 2020) and (Abnar &Zuidema, 2020). We fuse the attention heads utilizing maximum operators and exclude low attention pixelsto better accentuate the prominent object within the image. As shown in , our quantized Swin-T exhibits superior representational capacity by maintaining a more relative ranking within the attentionmap.This distinction becomes more pronounced when the ViT model is quantized to 3-bit and 2-bitrepresentations.For the baseline LSQ+ quantization (Bhalgat et al., 2020), the attention substantiallydeteriorates and distributes uniformly across the given input when quantized to extremely low bit-widths.However, our 2-bit quantized Swin-T is still capable of segmenting the salient object region effectively.",
  "EHardware Utilization Experiments Detail": "The unit area and power dissipation of multiply-accumulate (MAC) units under different bitwidth settingsare listed in . For mixed-precision quantization schemes such as Q-ViT Li et al. (2022b), the finalarea should be the maximum area for all bitwidth combinations, and the power dissipation is the weightedaverage over different settings. Considering the non-linear growth for the power dissipation regarding thebitwidth, a single-precision quantization scheme is better in terms of efficiency than mixed-precision underthe same average bitwidth. In addition, most existing mixed-precision accelerators only support power-of-two-bits arithmetic, which poses another challenge for optimal assignment searching or learning.",
  "FOverhead of Module-dependent Quantization": "Applying quantization-aware training on finer granularity will introduce computation overhead. However, ourmodule-depend quantization (MDQ) only performs per-head-tensor quantization instead of finer granularity,such as per-token or per-channel quantization. This per-head-tensor quantization scheme only introducesminimal trainable parameters (scaling factors) into the training. In addition, the MDQ is only applied toMHSA, and we use layer-wise quantization on feed-forward networks. shows the GPU memoryconsumption and the real training time per epoch on a single NVIDIA 80G A100 GPU with a batch sizeof 32 on a Swin-T. In addition, the proposed fine-grained quantization scheme helps QAT converge faster.With module-dependent quantization with only 150 training epochs, we can outperform vanilla layer-wiseLSQ with 300 epochs. Considering both the performance improvement and faster convergence of QAT, webelieve the proposed module-dependent quantization is both efficient and effective.",
  "GGeneralization to ConvNets": "One original motivation for this research is to answer the question: Why cant we directly apply the ConvNetQAT method to Transformers? This question further leads to other motivations listed in our introductionsection. We believe the proposed techniques in this work are novel solutions to the specific variation challengeswe located in transformer-based models and will be hard to generalize to ConvNet QAT. More concretely: Module-dependent Quantization (MDQ) applies fine-grained quantization by assigning different scalefactors to tensors in various heads of MHSA. There is no such module in ConvNets. Therefore, we cannot directly apply our MDQ to ConvNet QAT. In addition, previous work on ConvNets Nagel et al.(2021) found that applying channel-wise quantization does not always lead to better performancethan layer-wise quantization.",
  "HAdditional Discussion with Related Research": "Distribution Outliers Our analysis of distribution outliers is different from that of SmoothQuant Xiao et al.(2023a). We compare the distribution of transformers to ConvNets and quantify this distribution outlierby the Standard Deviation of the Absolute Mean (SDAM). In contrast, SmoothQuant solely investigateshow weights and activations are distributed in LLM and only gives qualitative analysis. In addition, ourwork targets quantization-aware training (QAT) that could eliminate these outliers by updating weightsduring gradient-based training. SmoothQuant researches post-training quantization (PTQ), which does notupdate the model. This further leads to different solutions for our multi-crop knowledge distillation andre-parameterization in SmoothQuant. Oscillation OFQ Liu et al. (2023a) is one research also investigating the training instability of visiontransformer. The solutions in Liu et al. (2023a) are the statistical weight quantization (StatsQ) scheme,confidence-guided annealing that freezes the weights, and query-key reparameterization. No solution in thiswork is similar to our Oscillation-aware Bin Regularization (OBQ), which is simple and effective. In addition,oscillation is only one part of the challenges faced in transformer quantization, and our research providesa more comprehensive analysis, including other variations and performance on transformers for languagetasks. Quantization Sensitivity and Granularity While previous research Yang & Jin (2021); Lou et al. (2019)on ConvNet quantization has discussed the granularity of quantization, we would like to highlight that com-ponents in transformers are very different from ConvNets. While ConvNet basically consists of similar convand linear layers, transformer-based models comprise more heterogeneous components with diverging char-acteristics. More concretely, the head-wise quantization in this work is the unique quantization granularitydesigned for multi-head self-attention modules in transformers, and this head-wise quantization can not beapplied to ConvNets. GPTQ Frantar et al. (2023a) is another work exploring the finer-grained quantiza-tion for LLMs. While GPTQ Frantar et al. (2023a) employs finer-grain quantization for PTQ, our analysisand solution are designed for QAT. Considering the trainable parameter overhead for the learnable scaleand bias in LSQ Bhalgat et al. (2020), the finer quantization granularity used in LLM PTQ, such as per-token in GPTQ and per-channel quantization, are impractical in transformer QAT. Therefore, the head-wisequantization in this research achieves the optimal trade-off between efficiency and performance."
}