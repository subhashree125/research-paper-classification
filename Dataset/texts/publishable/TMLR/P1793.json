{
  "Abstract": "Bilevel optimization problems have been actively studied in recent machine learning re-search due to their broad applications. In this work, we investigate single-loop methodswith iterative differentiation (ITD) for nonconvex bilevel optimization problems. For deter-ministic bilevel problems, we propose an efficient single-loop ITD-type method (ES-ITDM).Our method employs historical updates to approximate the hypergradient. More impor-tantly, based on ES-ITDM, we propose a new method that avoids computing Hessians. ThisHessian-free method requires fewer backpropagations and thus has a lower computationalcost. We analyze the convergence properties of the proposed methods in two aspects. Weprovide the convergence rates of the sequences generated by ES-ITD based on the Kurdyka-ojasiewicz (KL) property. We also show that the Hessian-free stochastic ES-ITDM has thebest-known complexity while has cheaper computation. The empirical studies show thatour Hessian-free stochastic variant is more efficient than existing Hessian-free methods andother state-of-the-art bilevel optimization approaches.",
  "Introduction": "Bilevel optimization problems arise in various machine learning scenarios, including game theory Stackelberg(1952), meta-learning (Franceschi et al., 2018; Zgner & Gnnemann, 2019; Finn et al., 2017a; Snell et al.,2017), hyperparameter optimization (Franceschi et al., 2017; Pedregosa, 2016; Grazzi et al., 2020; Mehra &Hamm, 2021; Maclaurin et al., 2015), and reinforcement learning (Hong et al., 2020). Please refer to (Liuet al., 2022b). A bilevel optimization problem involves two optimization problems, wherein one problem(the upper-level problem) includes the solution of another optimization problem (the lower-level problem).A typical formulation of this problem takes the following form:",
  "Published in Transactions on Machine Learning Research (October/2024)": "Next we estimate xyG(xl, yl) and yyG(xl, yl). We first present the following basic properties shown inNesterov & Spokoiny (2017), see equation (21), Lemma 3 and Lemma 5 in Nesterov & Spokoiny (2017)respectively.Lemma 7. Let h : Rn R be a differentiable function with L-Lipschitz gradient. Define h(x) = Eu[h(x +u)], where > 0 and u is a standard Gaussian random vector. We have that:",
  "J(y(x)) := xyG(x, y(x))T yyG(x, y(x))1": "The method proposed in Ghadimi & Wang (2018) replaced (x, y(x)) in the above formula with (xl, yl+1),where l is the iteration.Franceschi et al. (2017) provided forward and backward ways to approximateJ(y(xl)) directly. Ji et al. (2021) also used the backward way to approximate J(y(xl)). To reduce the timeand space complexity of estimating f(xl), Shaban et al. (2019) proposed a truncated back-propagationto approximate J(y(xl)). However, all above mentioned methods are double loop methods, which can besophisticated and computationally expensive. To enhance the efficiency of ITD-based methods, single-loop techniques have been proposed in prior work(Yang et al., 2021; Guo et al., 2021; Khanduri et al., 2021; Li et al., 2022; Chen et al., 2022a; Hong et al.,2020). Guo et al. (2021) introduced SVRB, which updates yl+1 using a single gradient descent step. Chenet al. (2022a) proposed STABLE, which updates x and y based on continuous-time dynamics. One chal-lenge faced by SVRB and STABLE is the need to compute a matrix inverse in each iteration, which iscomputational expensive. An alternative approach is to use the Neumann series, as seen in Khanduri et al.(2021); Yang et al. (2021); Hong et al. (2020). In particular, the inverse yyG(x, y)1 is approximated bybi=1 (I yyG(x, y))i with some b N+.The greater b is, the less error this approximation has. However,if y is not a good approximation of y(x), which is likely the case for most existing single-loop methods thatonly possess one inner loop to update y, the Neumann series approach to approximate J(y(x)) can still resultin significant errors. In this study, we introduce a novel approach for approximating the Jacobian J(y(x)). Another limitation in many existing ITD-type methods relies heavily on the calculation of the Hessian, itsinverse, or the multiplication of the Hessian with a vector. These computations incur high computationalcosts.Additionally, scenarios may arise where computing the Hessian of G is challenging or where Glacks second-order differentiability.For instance, in cases where the lower-level problem involves robustregression, G can take the form of the Huber loss, which is not twice differentiable (Huber, 1964; Hastieet al., 2009). Consequently, there has been a pursuit of methods that avoid Hessian computation. Gu et al.(2021) applied Gaussian smoothing (GS) techniques (Nesterov & Spokoiny, 2017), enabling their methodto bypass the computation of gradients of G. However, Gu et al. (2021) does not leverage any first-orderinformation from F and G, potentially leading to a larger discrepancy between the approximation and thetrue hypergradient. Similarly, Sow et al. (2022) employed GS techniques to estimate the Jacobian of y(x).Nevertheless, their resulting method needs to compute the full gradient of G, which is often impractical inreal-world applications. Additionally, the methods introduced in Gu et al. (2021); Sow et al. (2022) involvea double-loop structure, which can be inefficient. Thus, in this work, we propose a stochastic single loopmethod that avoids calculating the Hessian of G. Another aspect that is overlooked in single-loop bilevel methods is the convergence analysis of the generatedsequences, specifically the convergence of (xl, yl). Sequential convergence holds significance as it illustratesthe methods behavior in the long run. For example, it helps determine whether the accumulated point of thegenerated sequence is a stationary point of (1), whether the generated sequence achieves global convergence,and the its convergence rate. The third goal of this work is to provide sequential convergence guarantees forthe single-loop bilevel method.",
  "The proposed ES-ITD method makes use of much information about lower level updates as thedouble loop method has but retains the single loop computation cost": "We propose a computationally efficient Hessian free method for the stochastic bilevel optimizationproblem (7). Based on a natural extension of ES-ITDM using the stochastic gradients or Hessiansof F and G, we use the Gaussian smoothing techniques to approximate the stochastic Hessian of G.The resulting method is called Hessian free stochastic ES-ITD method (HF-SES-ITD). Comparedto current single loop method, HF-SES-ITD method only uses the first order information to approx-imate the Jacobian of y(x). Compared with Hessian vector multiplication, our method ismuch cheaper because it only needs computing gradient vector inner product. We provide convergence analysis of the proposed methods. For the deterministic method, we analyzethe convergence of the sequences {xl}, {yl} and {Jl} generated by ES-ITDM, where Jl is theapproximation of the Jacobian of y(xl). To this end, we propose a new potential function. Weshow that the iterative value of the potential function is nonincreasing. After that, we show that thesuccessive changes of the generated sequence converge to zero. We show that any accumulation pointof {xl} is a stationary point of (1). Furthermore, under the Kurdyka-ojasiewicz (KL) property, wederive the convergence rates of {(xl, yl, Jl)}. Especially, when the potential function is a KL functionwith exponent 1 2, we show that {(xl, yl, Jl)} converges linearly. As far as we know, this is thefirst work that provides the convergence rate of the sequences generated by single loopmethods. Compared with sequential convergence analysis for methods solving general nonconvexoptimization problems, the nested formula for updating Jl+1 in our method makes a technicalchallenge in our sequential convergence analysis nontrivial. We also give convergence guarantees ofthe stochastic variant of ES-ITDM. We show the resulted method has a complexity of O(2) toreach an -stationary point under mild assumptions. We evaluate our methods via the hyper-parameters learning task. We first compare our methodswith the current Hessian-free methods (Gu et al., 2021; Sow et al., 2022). Then we compare ourmethods with other popular single loop bilevel optimization methods in Franceschi et al. (2017);Grazzi et al. (2020); Ji et al. (2021); Grazzi et al. (2020); Sow et al. (2022); Guo et al. (2021); Chenet al. (2022a). In both comparisons, our stochastic Hessian free fully single loop method has betterperformance in both time and accuracy. : Comparisons between our method and ESJ , BA, ITD-Bio, FMM = Forward Mode Method, TTSA,SUSTAIN, MRBO, SVRB, STABLE. HVM = Hessian Vector Multiplication; ds= diminishing stepsize; cs=constant stepszise; denotes an ( + variance errors)-accuracy for f(xmaxiter).",
  "Notation and Preliminaries": "In this paper, we denote Rn the n-dimensional Euclidean space with inner product , and Euclidean norm . We denote the spectrum norm of a matrix A Rnm as A and the Frobenius norm of A as AF .For a ramdom variable defined on a probability space (, , P), we denote its expectation as E. Givenan event A, the conditional expectation of is denoted as EA(). We say an extended-real-valued function f : Rn is proper if domf = {x Rn : f(x) < }is not empty and f never equals . We say a proper function f is closed if it is lower semicontinuous.A proper closed function is said to be level-bounded if for any a R, the set {x : f(x) a} is bounded.For a function F : Rn+m R, we denote the function F(x, y) with respect to y for a fixed x as F(x, )and denote the function F(x, y) with respect to x for a fixed y as F(, y). Following Rockafellar & Wets(1998), the regular subdifferential of a proper function f at x domf is defined as f(x) := { Rn :lim infzx, z=x",
  "zx 0}. The subdifferential of f at x domf is defined by f(x) := { Rn :": "xk with xk x and f(xk) f(x), k with k f(xk), k}. For x domf, we define f(x) =f(x) = . We denote domf := {x : f(x) = }. We say x is a sationary point of f if 0 f(x). For atwice differential function F : Rm Rn R, we denote xF(x, y) and yF(x, y) as the partial gradientsF (x,y)",
  "xl+1 = xl xF(xl, yl+1) + (Jl+1)T yF(xl, yl+1)": "If K = 1, we get a single loop bilevel method. However, the resulting Jacobian approximation becomesJ(yl,1(xl)) = xyG(xl, yl,0).xyG(xl, yl,0) does not make use of previous information about the ycoordinate. Even if we choose yl,0 in Algorithm 1 to be the last update yl, J(yl,1(xl)) still does not make useof the previous information such as {yyG(xl, yl)}l or {xyG(xl, yl)}l. Thus, simply changing the doubleloop method to a single loop method by letting K = 1 will lose information in the previous iterates. However,we notice that (3) implies",
  "Comparing (6) with (3), there are the following differences:": "At iteration l, computing J(yl,K(xl))yF(xl, yl+1) based on (3) needs O(K2) Hessian-vector mul-tiplications (HVM), where K is the iterations of the inner loop.The cost of each HVM isO(max{n, m}) using the technic in Pearlmutter (1994).Here, We have two equivalent ways toupdate Jl+1yF(xl, yl+1): either by using (5) and making use of Jl, or by using (6) without involv-ing Jl. When applying deduction (5) in the algorithm, we need n+1 HVM computations. However,when using (5) to update Jl+1, we must compute Jl+1 = Jl I yyG(xl, yl) xyG(xl, yl),which involves an RnmRmm matrix operation. When calculating JlyyG(xl, yl), it requires nHVM computations. Therefore, in each iteration, we actually need n+1 HVM computations, whichis less than O(k2). Thus, our method is advantageous when n + 1 k2. On the other hand, whenusing (6), we require l2 HVM computations. In summary, our method outperforms the classical ITDmethod when either n + 1 k2 or l2 < k2. The formula in (6) makes use of historical updates {x0, . . . , xl} and {y0, . . . , yl}, while (3) onlydepends on {xl}. Although making use of historical updates requires more storage, (6) makes useof more information to approximate the Jacobian of y(xl).",
  ": end for": "The idea of Algorithm 3 is as follows. We aim to approximate xyG(x, y; Sl) and yyG(x, y; Sl) with the first-order information of G. xyG(x, y; Sl) and yyG(x, y; Sl) consist of the gradients of {yjG(xl, yl; Sl)}mj=1with respect to x and y. Inspired by Nesterov & Spokoiny (2017), we use Gaussian smoothing technique asfollows. Pick Q N, > 0 and > 0. For j = 1, . . . , Q, generate ulj N(0, I) Rn and vlj N(0, I) Rm.For q = 1, . . . , m, let",
  "Now, we show the global convergence properties of the sequences generated by Algorithm 2. To this end, weintroduce the Kurdyka-ojasiewicz (KL) property": "Definition 1 (Kurdyka-ojasiewicz Function). We say a proper closed function f : Rn (, ]satisfies the Kurdyka-ojasiewicz (KL) property at x domf with exponent [0, 1) if there are a (0, ], a neighborhood V of x and a0 > 0 such that dist(0, f(x)) a0(f(x) f(x)) for any x V withf(x) < f(x) < f(x) + a. A proper closed function f satisfying the KL property with exponent [0, 1) atevery point in dom f is called a KL function with exponent . Many functions are KL functions. It is known that proper closed semi-algebraic functions (i.e., functionswhose graphs are unions and intersections of polynomial functions) satisfy the KL property, Attouch et al.(2010); Li & Pong (2018); Attouch et al. (2013); Bolte et al. (2017). Semi-algebraic functions include widelyused losses such as quadratic loss, L2 loss, Huber loss, hinge loss, and 0-1 loss. KL property is a generalproperty in convergence analysis when the considered function is not smoothness.",
  ", 1), {(xl, yl, Jl+1)} converges sublinearly": "Remark 2. Note that, together with Corollary 1, Theorem 2 shows that the limiting point x, to which thesequence xl converges, is a stationary point of (1). In addition, the limiting point y, to which the sequenceyl converges, is the optimal solution of the lower-level problem in (1) defined with x. Remark 3. Since the lower-level problem is strongly convex, the lower-level minimizer is unique. When thelower-level problem is semi-algebraic, and since y(x) is the infimum projection of a semi-algebraic function,its graph is also the intersection of polynomial functions, making y(x) semi-algebraic, as well as its JacobianJ(y(x)). If the upper-level objective is also semi-algebraic, then the potential function H is a KL function,satisfying the assumption in Theorem 2.",
  "Experiments": "In this section, we test the efficacy of the proposed algorithms: Algorithm 2 and Algorithm 3 on the hyper-representation learning task Franceschi et al. (2018).2 Hyper-representation refers to a shared representation(or shared deep neural network) across multiple tasks in a meta-learning framework. The parameters inthe shared representation are referred to as hyperparameters. Let (; ) denote the hyper-representationmapping, parameterized by . When applying this to solve a specific classification task, a linear layer wis added on top of the hyper-representation, and only the parameters w are trained, while the parameters in the shared \"hyper-representation\" remain fixed. To train the parameters in the hyper-representation,[Franceschi et al.(2018)] formulated it as the bilevel programming problem (1), where the upper-levelobjective minimizes a validation loss, and the lower-level objective minimizes a task-specific training loss.Specifically, we have the following problem:",
  "Conclusions": "In this paper, we focused on studying the bilevel optimization problem and proposed a new single-loopITD method that is more efficient in approximating the Jacobian of the lower-level solution with respectto the upper-level variable. We proposed a Hessian-free stochastic bilevel optimization. Based on a naturalstochastic extension of ES-ITDM, we first proposed a Hessian-free stochastic ES-ITDM. This Hessian-freevariant eliminates the need to compute the Hessian vector multiplication, thus potentially leading to fasterimplementation. We theoretically analyze the proposed methods. For the deterministic method, we investi-gate the global convergence rates of the generated sequences. As for the stochastic method, we conductedan analysis of its complexity. Our methods were validated using the hyper-representation learning task.In experiments, our Hessian-free stochastic ES-ITDM demonstrated greater efficiency compared to existingHessian-free methods.",
  "Hdy Attouch and Jrme Bolte. On the convergence of the proximal algorithm for nonsmooth functionsinvolving analytic features. Math. Program., 116(1-2):516, 2009": "Hdy Attouch, Jrme Bolte, Patrick Redont, and Antoine Soubeyran. Proximal alternating minimizationand projection methods for nonconvex problems: An approach based on the kurdyka-lojasiewicz inequality.Math. Oper. Res., 35(2):438457, 2010. Hdy Attouch, Jrme Bolte, and Benar Fux Svaiter. Convergence of descent methods for semi-algebraic andtame problems: proximal algorithms, forward-backward splitting, and regularized gauss-seidel methods.Math. Program., 137(1-2):91129, 2013.",
  "Ziyi Chen, Bhavya Kailkhura, and Yi Zhou.A fast and convergent proximal algorithm for regularizednonconvex and nonsmooth bi-level optimization. 2022b. URL": "Ziyi Chen, Bhavya Kailkhura, and Yi Zhou. An accelerated proximal algorithm for regularized nonconvex andnonsmooth bi-level optimization. Mach. Learn., 112(5):14331463, 2023. doi: 10.1007/s10994-023-06329-6.URL Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deepnetworks. In Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney,NSW, Australia, 6-11 August, 2017a. Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deepnetworks. In Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney,NSW, Australia, 6-11 August, 2017b. Luca Franceschi, Michele Donini, Paolo Frasconi, and Massimiliano Pontil. Forward and reverse gradient-based hyperparameter optimization.In Proceedings of the 34th International Conference on MachineLearning, ICML 2017, Sydney, NSW, Australia, 6-11 August, 2017. Luca Franceschi, Paolo Frasconi, Saverio Salzo, Riccardo Grazzi, and Massimiliano Pontil.Bilevel pro-gramming for hyperparameter optimization and meta-learning. In Proceedings of the 35th InternationalConference on Machine Learning, ICML 2018, Stockholmsmssan, Stockholm, Sweden, July 10-15, 2018.",
  "Saeed Ghadimi and Mengdi Wang. Approximation methods for bilevel programming. 2018. URL": "Stephen Gould, Basura Fernando, Anoop Cherian, Peter Anderson, Rodrigo Santa Cruz, and Edison Guo.On differentiating parameterized argmin and argmax problems with application to bi-level optimization.2016. URL Riccardo Grazzi, Luca Franceschi, Massimiliano Pontil, and Saverio Salzo. On the iteration complexity ofhypergradient computation. In Proceedings of the 37th International Conference on Machine Learning,ICML 2020, 13-18 July, 2020.",
  "Peter J. Huber. Robust estimation of a location parameter. Ann. Math. Statist., 35(1):73101, 1964": "Kaiyi Ji, Junjie Yang, and Yingbin Liang. Bilevel optimization: Convergence analysis and enhanced design.In Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July, 2021. Prashant Khanduri, Siliang Zeng, Mingyi Hong, Hoi-To Wai, Zhaoran Wang, and Zhuoran Yang. A near-optimal algorithm for stochastic bilevel optimization via double-momentum. In Advances in Neural In-formation Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021,NeurIPS 2021, December 6-14, 2021. Jeongyeol Kwon, Dohyun Kwon, Stephen Wright, and Robert D. Nowak. A fully first-order method forstochastic bilevel optimization. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engel-hardt, Sivan Sabato, and Jonathan Scarlett (eds.), International Conference on Machine Learning, ICML2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research,pp. 1808318113. PMLR, 2023. URL",
  "Junyi Li and Heng Huang.Provably faster algorithms for bilevel optimization via without-replacementsampling. arXiv preprint arXiv:2411.05868, 2024": "Junyi Li, Bin Gu, and Heng Huang. A fully single loop algorithm for bilevel optimization without hessianinverse. In Thirty-Sixth AAAI Conference on Artificial Intelligence, AAAI 2022, Thirty-Fourth Conferenceon Innovative Applications of Artificial Intelligence, IAAI 2022, The Twelveth Symposium on EducationalAdvances in Artificial Intelligence, EAAI 2022 Virtual Event, February 22 - March 1, 2022. Bo Liu, Mao Ye, Stephen Wright, Peter Stone, and Qiang Liu.Bome!bilevel optimization madeeasy: A simple first-order approach.In Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave,K. Cho, and A. Oh (eds.), Advances in Neural Information Processing Systems 35: Annual Confer-ence on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, Novem-ber 28 - December 9, 2022, 2022a.URL Risheng Liu, Pan Mu, Xiaoming Yuan, Shangzhi Zeng, and Jin Zhang. A generic first-order algorithmicframework for bi-level programming beyond lower-level singleton. In Proceedings of the 37th InternationalConference on Machine Learning, ICML 2020, 13-18 July, 2020. Risheng Liu, Jiaxin Gao, Jin Zhang, Deyu Meng, and Zhouchen Lin. Investigating bi-level optimization forlearning and vision from a unified perspective: A survey and beyond. IEEE Trans. Pattern Anal. Mach.Intell., 44(12):1004510067, 2022b.",
  "R. Tyrrell Rockafellar and Roger J.-B. Wets. Variational Analysis, volume 317 of Grundlehren der mathe-matischen Wissenschaften. Springer, 1998": "Amirreza Shaban, Ching-An Cheng, Nathan Hatch, and Byron Boots. Truncated back-propagation for bileveloptimization. In The 22nd International Conference on Artificial Intelligence and Statistics, AISTATS2019, 16-18 April, Naha, Okinawa, Japan, 2019. Jake Snell, Kevin Swersky, and Richard S. Zemel. Prototypical networks for few-shot learning. In Advances inNeural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems2017, December 4-9, Long Beach, CA, USA, 2017.",
  "Bo Wen, Xiaojun Chen, and Ting Kei Pong. A proximal difference-of-convex algorithm with extrapolation.Comput. Optim. Appl., 69(2):297324, 2018": "Junjie Yang, Kaiyi Ji, and Yingbin Liang. Provably faster algorithms for bilevel optimization. In Advances inNeural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems2021, NeurIPS 2021, December 6-14, 2021. Peiran Yu, Ting Kei Pong, and Zhaosong Lu. Convergence rate analysis of a sequential convex programmingmethod with line search for a class of constrained difference-of-convex optimization problems. SIAM J.Optim., 31(3):20242054, 2021. Peiran Yu, Junyi Li, and Heng Huang. Dropout enhanced bilevel training. In The Twelfth InternationalConference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net,2024. URL",
  "AAdditional Preliminaries": "For any matrices A and B, we denote trace(AT B) := A, B. Given independent random variables 1, . . . , pand a function f(1, . . . , p), we denote the conditional expectation of g with respect to i as Ei|Ag(1, . . . , p). Under Assumptions 1 and 2, we have the following properties (Lemma 2.1 and Lemma 2.2 in Ghadimi &Wang (2018)) about f in (1).Lemma 1. Consider (1) and suppose Assumptions 1 and 2 hold.Then y(x) is Lipschitz continuous",
  "H0 liml Hl <": "Taking the N in the above inequality to , we have that liml xl+1 xl = 0, liml y(xl)yl+1 = 0and liml J(y(xl)) Jl+1F = 0.This together with the boundedness of {(xl+1, y(xl+1))} and thecontinuity of y(x) and J(y(x)) w.r.t x guaranteed by Lemma 1 and Lemma 2, we have that {yl} and {Jl+1}are bounded.",
  "2xl+1 xl.(25)": "Proof. First, the boundedness of follows from Corollary 2. Now, let (x, x, y, y, J) be any pointin .Then there exists a sequence {(xlj, xlj1, ylj+1, Jlj+1)}j such that limj(xlj, xlj1, ylj+1, Jlj+1) =(x, x, y, J). Thanks to Lemmas 1 and Lemma 2, we have that H is continuous. Thus,",
  "(H(Z) H)dist(0, H(Z)) 1": "when Z {Z : dist(Z, ) < } {Z : H < H(Z) < H + }. Since is the set of accumulation points ofZl, we know that there exists l1 such that dist(Zl, ) < when l l1. In addition, using Theorem 4, weknow that there exists l2 such that H(zl) < H + when l l2. Thus, when l max{l1, l2}, we have that",
  "where the last inequality uses (30). This inequality implies that the sequence {(xl, yl, Jl)} and {Zl} areconvergent": "Now, we show the convergence rate when H is a KL function with exponent . First, when = 0, thereexists l such that Hl == H when l l. In fact, suppose to the contrary that Hl > H for some l > l. SinceZl is convergent (denote liml Zl = Z) and Hl is nonincreasing thanks to Theorem 4, noting that (s) = csand the KL inequality holds for large l, it holds that dist(0, H(Zl)) 1",
  "2yl+1 yl2,": "where the second inequality is because yl+1 in Step 3 of Algorithm 2 is the minimizer of arg min G(xl, yl) +yG(xl, yl; Sl), yl+1 yl+12yl+1 yl2 and the objective of this subproblem is strongly convex withmodulus1. Taking the conditional expectation of Sl on both sides of the above inequality and recallingAssumption 3, for l 0, we have that",
  "+ 3(1 + d2J )2yyG(xl, yl) yyG(xl, yl)2 Jl2,": "where the first inequality uses a1 + a2 + a3 + a42 (1 + d2J)a21 + (1 + d2J )a2 + a3 + a42 (1 + d2J)a21 +3(1 + d2J )a22 + 3(1 + d2J )a32 + 3(1 + d2J )a42. Using Assumption 2 (ii), the above inequality can befurther passed to",
  "(xi,yi)DT ,l(T (xi; ), yi) + Cw2,": "where l() denotes the cross entropy loss, DT , and DV, are training and validation dataset for a randomlysampled meta task. Here = {i}iDT are hyper-representations and C 0 is a tuning parameter togaurantee the inner problem to be strongly convex. In experiment, we set C = 0.01. The Omniglot dataset includes 1623 characters from 50 different alphabets and each character consists of20 samples. We follow the experimental protocols of Vinyals et al. (2016) to to divide the alphabets totrain/validation/test with 33/5/12, respectively. We perform N-way-K-shot classification, more specifically,for each task, we randomly sample N characters from the alphabet over that client and for each character,we sample K samples for training and 15 samples for validation. We augment the characters by performingrotation operations ( multipliers of 90 degrees). We use a 4-layer convolutional neural networks and eachconvolutional layer has 64 filters of 33 and is followed by batch-normalization layers Finn et al. (2017b).The parameters of convolutional layers are treated as hyper-representation and the last linear layer is thefined tune inner parameters. For all experiments, we use mini-batch size 4, outer learning rate 0.1, innerlearning rate 0.4, perform 4 inner gradient steps. In particular, for F2SA, we set the Lagrange multiplier as2."
}