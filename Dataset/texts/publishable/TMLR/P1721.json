{
  "Abstract": "Model extraction attacks are designed to steal trained models with only query access, as isoften provided through APIs that ML-as-a-Service providers offer. Machine Learning (ML)models are expensive to train, in part because data is hard to obtain, and a primary incentivefor model extraction is to acquire a model while incurring less cost than training from scratch.Literature on model extraction commonly claims or presumes that the attacker is able tosave on both data acquisition and labeling costs. We thoroughly evaluate this assumptionand find that the attacker often does not. This is because current attacks implicitly relyon the adversary being able to sample from the victim models data distribution.Wethoroughly research factors influencing the success of model extraction. We discover thatprior knowledge of the attacker, i.e. access to in-distribution data, dominates other factorslike the attack policy the adversary follows to choose which queries to make to the victimmodel API. Our findings urge the community to redefine the adversarial goals of ME attacksas current evaluation methods misinterpret the ME performance.",
  "Introduction": "Modern ML models are valuable intellectual property, in part because they are expensive to train (Shariret al., 2020; Patterson et al., 2021), and model extraction (ME) attacks threaten their confidentiality. Inan ME attack, the adversary attempts to steal a victim model over query access, in order to obtain anapproximate copy of that model with similar performance (Tramr et al., 2016). In the most common classof ME attacks, the adversary uses queries to the victim model as a training set for obtaining a copy of thatmodel1. ME attacks are a looming threat for ML-as-a-Service providers, whose business model depends on paidquery access to their models over APIs. In fact, Kumar et al. (2020) outlines that ME is one of the mostconcerning threats to the industry. Typically, the claimed motivation to conduct ME attacks is to avoidthe costs involved in training a model from scratch. Most notably, this refers to the data collection anddata labeling costs, and sometimes mentions the training computational costs.The implicit assumption Work performed while the author was visiting the Vector Institute.Work done for the most part while the author was a Postdoctoral Fellow at the Vector Institute.1 illustrates such a typical attack, while other types of ME attacks are discussed in .",
  "Published in Transactions on Machine Learning Research (12/2024)": "= 0, T = 2 (real model) = 0, T = 1 (real model) = 99, T = 2= 99, T = 1 = 95, T = 2= 95, T = 1 = 90, T = 2= 90, T = 1 = 75, T = 2= 75, T = 1 baseline attack",
  "Related work": "Model extraction attacks, also called model stealing, were first explored by Tramr et al., who proposed anattacker that queries the victim model to label its dataset and trains a model to match these predictions.Various works extended the attack with the goal of achieving similar task accuracy while minimizing thenumber of queries required. Most of these attacks either assume access to a surrogate dataset (Correia-Silvaet al., 2018; Orekondy et al., 2019a) or to a portion of the real training set (Rakin et al., 2021), or userandom queries (Krishna et al., 2019; Chandrasekaran et al., 2020) in domains like NLP. We discuss therelation between ME to active learning in Appendix A. Truong et al. attested that, in order to successfullyextract the victim model, the attackers dataset must share semantic or distributional similarity to the realtraining dataset; otherwise resulting in an insufficient extraction accuracy.To mitigate this issue, theypropose a data-free model extraction attack (DFME) that does not require a surrogate dataset. Inspired bydata-free knowledge distillation (Lopes et al., 2017), they train a generative model to synthesize queries whichmaximize disagreement between the attackers and the victims models. A similar method was also proposedby MAZE (Kariyappa et al., 2021). (Lin et al., 2023) investigated ways to reduce the query complexity ofdata-free approaches. These results align with our analysis and show that the adversary can compensate forthe absence of prior knowledge by using a very large query budget.",
  "Background": "In this section, we first discuss the different costs involved in model extraction and connect them with the MEliterature; we cover related works in detail in . We then describe how prior literature approachedcost reduction using random data, why this can work theoretically, and what assumptions are required.Subsequently, we describe the additional mechanism we use to disambiguate the effect of the attack policyfrom increased query budget in ME.",
  "Definitions": "In what follows, we refer to in-distribution data as IND and out-of-distribution data as OOD. Note thatwe follow the conventions of the field and use these terms loosely to describe the distribution that trainingdata comes from for the victim model as IND, while OOD refers to any data that comes from a differentdistribution. We define an ME adversary as 3-tuple (DIND, DOOD, ), where DIND represents the adversarys access to theIND, i.e. its prior knowledge, DOOD represents the adversarys unrelated OOD query distribution, and represents the adversarys query selection policy. When attacking, the adversary uses to select samplesto be queried from its IND and OOD distributions. Then, the queries are issued to the victim model toobtain labels. These labels can be either full probability vectors, top-k probabilities, or label-only. In ourevaluations, we focus on the stronger setting of full probability vectors. At last, the adversary trains a modelon the labeled query set. Importantly, since attacks in the current literature vary all three from the above,it is hard to pinpoint why a given attack seemingly performs better could it be a better attack policy orperhaps more informative data? This is the question we answer.",
  "Primer on ML costing": "We are not aware of any ME literature that formally defines what makes model extraction attacks successful.While high-fidelity extraction attackers explicitly expect to extract a model within a given error, intuitively,accuracy-driven ME can be considered successful if stealing a model costs less than developing it. That is,as long as the accuracy of the stolen model matches the victim it is a success. The overall cost of producingand deploying a model can be broken down into three main parts: (1) data collection, (2) data labeling, and(3) model training. We note that there are other costs corresponding to ML infrastructure, however theyare not relevant to the current ME threat model.",
  "(c)": ": Consider a linear classifier for which the decision boundary is given by the line y = x. Anattacker attempts to steal the model (i.e. find the corresponding = 1 from the example above). Thegreen region x is in-distribution behaviour that the attacker wants to replicate, the red regionx is out of distribution and is not important for the task. The left case requires a singleparameter to be approximated, the middle needs 5, whereas the right requires 9. Now we turn to CIFAR10 as a cost case study note that CIFAR10 is one of the most evaluated benchmarksfor ME. Here we assume CIFAR10 dataset with 60k samples that can be labeled for 35$50+1025$ = 2000$with Google Cloud (Cloud, 2024) and 0.04$60000 = 2400$ with Amazon Sagemaker (Amazon, 2024). Datacollection here is practically free, since per category one can scrape the internet freely. We model the defendercost as c = n (pl + cc) and the attacker cost as ca = na (pla + cca), where n is the number of points toannotate, pl is the per-label cost and cc is the data collection cost. We assume that the attacker wants tosucceed in the attack and thus attackers cost is less than that of a defender n (pl + cc) > na (pla + cca).First, we consider the attacker who extracts a model with DFME with 20 million queries. If we assume thatthe defender used Google Cloud to label their data, we find that the attackers labels have to cost less than0.00012$:",
  "M pla < 2, 400$ pla < 0.00012$": "The queries in DFME are synthetic, therefore there is no data collection cost (cca = 0). Next, we consider anME attack that only utilizes 5% = 3000 datapoints of prior knowledge, and no additional queries of any sort.Here, attacker breaks even at 0.8$ per-label. Note that with 5% of data, as we show later in .1, it isindeed possible to extract a model, while remaining well within a reasonable budget in the current labelingmarket. At the same time, extraction with DFME stops being cost-effective when the attacker queries areeven minimally priced by the defender. This case study suggests that the claim over the cost-effectiveness of ME is not so trivial, and should becarefully considered in different use-cases. An attacker must estimate the cost elements defined above, namelysample complexity, data collection, and data labeling, in order to properly justify performing the attack. Wenow turn to the investigating the cost effectiveness of current ME attacks in the literature.",
  "Methodology": "A common threat model assumes IND data is scarce and expensive to obtain, thus many attackers attemptto reduce their data collection costs through OOD queries. Note that this is the most common way for MEliterature to reduce costs. But how might this work? Hyperbolizing, this implies that by asking questionsabout e.g. a dog, one may learn something about unrelated concept of a building. In this section, we firstprovide intuition for when such extraction is possible. We then build on this intuition and design an explicitmechanism to measure how much OOD queries can possibly contribute to attack performance. Warm-up: Linear classification Consider a linear classifier that splits the input space into two decisionregions, and the corresponding decision boundary is defined by the line y = x. Here, (x, y) denotes the",
  "Sampling complexity intuition": "When useful responses are limited to the IND region only, the attackers success is dominated by the percent-age of queries sampled from IND, which is a function of the attackers prior knowledge of the victims truedistribution. In this scenario, the weakest attacker, with no prior knowledge, is the random guess attackerthat samples useful information with probability |Xuseful|/|X|, where X is the entire input domain and Xusefulis the IND part of the domain. The strongest attacker here has precise knowledge of the IND region withinX and can sample only from this region, which results in samples useful for the attackers goal. This isthe strongest attacker, as no ME defense can truly defend against such an attacker unless it corrupts theutility of the model as it predicts on the IND domain. Instead, the average-case attacker has partial priorknowledge about the true distribution. As such, the probability of sampling IND is a function of this prior,which indicates the overlap between the true distribution and the attackers query distribution. We estimatethe probability of randomly sampling IND for models considered in this section and find it equals 0.00001%for CIFAR10 models and 0.01% for MNLI. We expand on the procedure in Appendix E.",
  "On hardness of OOD detection": "Recently, Tramer (2021) demonstrated that robust adversarial example detection is as hard as robust classi-fication. Here, robustness of the detector refers to consistent detection over the -radius around a datapoint, representing the maximal distance for an adversarial example.Fundamentally, Tramer demon-strates that robust detection over an region around a given (not necessarily adversarial) data pointimplies, albeit inefficient, an ability to successfully classify for at least /2 region.One way to reason",
  "Out-of-Distribution instrumentation": "In we empirically evaluate our hypothesis that ME attacks make an implicit assumption about theusefulness of OOD queries. We show that when this assumption does not hold, and the OOD region is notindicative of the IND behaviour, ME attack success is reduced to being a function of prior knowledge. Dueto space limitations, we only briefly describe the details of the model instrumentation, and refer the readerto Appendix F for full details. Given the original victim model Vo, we create a hybrid victim model Vh by combining Vo with an additionalmodule Vf with different, or additional, decision boundaries.This additional model is used to providepredictions for OOD queries that differ from the predictions they would have gotten from the original model.For each query x, the hybrid model Vh applies some decision rule R to classify x as IND or OOD, anduses the corresponding model for prediction. We design the additional model Vf such that the decisionboundaries of both models would have similar smoothness properties; thus, learning the fake boundaries isexpected to be of the same level of difficulty, with nearly statistically indistinguishable output distributions.We discuss this further in Appendix F.4. For the decision rule R, we apply some pre-defined threshold overthe prediction confidence of Vo, with a softmax temperature of 2 to better calibrate R. If the confidence ishigher than , the hybrid model returns Vo(x), otherwise it returns Vf(x). Prediction confidence serves as anaive OOD detector (Hendrycks & Gimpel, 2016; DeVries & Taylor, 2018) and represents one of the simplestsettings for the attacker. More advanced OOD detectors will strictly improve the separation of IND andOOD queries, therefore decreasing the number of false negative queries, i.e. OOD queries that are classifiedIND and are provided with a meaningful prediction by Vo. For the fake model Vf we fit a Gaussian MixtureModel (GMM) for each class, and split the input domain into centroids defined by an anchor point. Theseare used to assign queries to one of the GMMs for prediction with a random yet consistent class predictionaround anchor point. For a given query sample x, we compute its feature representation and find the nearest L2 anchor point. Wesample a fake prediction y using the GMM corresponding to the chosen anchor point and return it as thefake model prediction Vf(x) = y. The construction of the GMMs and their anchor points, discussed in detailin Appendix F, results in OOD samples being wrongly predicted while still exhibiting similar smoothness",
  "(.3) Can model extraction be used to reduce the data costs? Sometimes, assuming thatOOD responses provide task-informative answers, data costs can be reduced": "(.4) Can model extraction be used both to reduce the data costs and at the same time usesmall number of queries?Sometimes, if OOD only reveals limited information about the INDbehaviour, an ME attacker must either have prior knowledge of the distribution and the ability tosample IND samples or have a very large query budget and traverse the input domain. (.5) Can model extraction be used to reduce data labeling costs? Sometimes; we find thatin practice the victim model provides no additional benefits other than serving as a labeling oracle.For some use cases, there are no more cost-effective sources for obtaining labels; for example, formedical data. Experimental Setting.To answer the questions described above, we evaluate a range of ME attacks oncommon vision and language benchmarks. We measure the attackers performance as the accuracy differencebetween the victim and the attacker on the original task. We define a successful attack as one that producesa model that performs on a task nearly as well as the victim model. Unless stated otherwise, in all ourexperiments we assume that the attacker has a black-box access to the victim model: the adversary issues aquery, and the victim model responds with a vector that indicates the probability of classifying the input ineach of the task classes. We note that this setting provides the most information to the adversary. In otherwords, it advantages the ME adversary. We define by baseline attacker an attacker that has access to a randomly sampled subset of the victimstrue training dataset and uses this data only to query the victim and train the attack model. Since MEquality is mainly affected by the quality of the attacker queries, real victim training data represents one ofthe best query sets for the attacker (compressed datasets e.g. Wang et al. (2018) or disjoint IND data canpotentially lead to an equally capable extraction set). Throughout our work, we quantify different levels ofprior knowledge by the percentage of the data available to the attacker and evaluate the attack accuracy asa function of this measure. We also evaluate ME adversaries that utilize a mixture of IND and OOD data. Vision. For vision tasks, we evaluate the CIFAR-10 dataset (Krizhevsky et al., 2009), for which we followthe setting and training details described by the state-of-the-art DFME attack (Truong et al., 2021). Weadditionally evaluate the Indoor67 (Quattoni & Torralba, 2009), CUBS200 (Wah et al., 2011) and Cal-tech256 (Griffin et al., 2007) datasets, and follow the setting and training details described by the KnockoffNets attack (Orekondy et al., 2019a), one of the strongest ME attacks. Due to space limitation, we providethe Knockoff Nets results in Appendix G, and focus the discussion on CIFAR-10. In all experiments we usethe pretrained victim models provided by the authors.",
  "(b) MNLI": ": Comparison between the attack performance in the soft-label setting, i.e., full probability vector,and the label-only setting.Evaluated for the baseline attacker, that utilizes only prior knowledge, andattackers that can utilize additional queries from other distributions (random images, SVHN, and CIFAR-100 in the case of CIFAR-10, and random sentences, nonsensical sentences, and wiki for the MNLI setting).Results demonstrate that soft-labels aid the attacker mainly in the lower prior knowledge settings and forquery distributions that are significantly different from the true distribution. In other cases, the victimmodel serves as a labeling oracle, and the soft labels provide little gain over the label-only access. Wasserstein distance 0.0 0.2 0.4 0.6 0.8 1.0 Probability of sampling in-distribution",
  "Does model extraction work?": "We start off by answering the foundational question does ME work? Our answer to this is yes, ME attacksdefinitely work, and it is indeed possible to approximate a blackbox model from just queries, matchingperformance to the victim model. As seen in , and as evident in the large number of works in thisfield, there are many settings in which an ME adversary can obtain a model that has a desirable performanceover the task of interest. However, not all queries are equally useful for ME. To demonstrate this, we evaluate an ME attacks onreal models that use queries sampled from data distributions that differ from the victim models trainingdistribution, i.e. OOD queries. Here, we consider a number of possible distributions, where some are closerto the true distribution than others. Namely, for CIFAR-10, we use: Random - uniformly sampled randomimages; Surrogate - real images sampled from a surrogate (SVHN and CIFAR-100) dataset; DFME -synthetic queries generated using the DFME attack. For Indoor67, CUBS200, and Caltech256, we evaluateonly the surrogate variant using the ImageNet dataset, as described by Orekondy et al.. Results are pre-sented in Appendix G. For MNLI we use: Random - nonsensical sequence of random words built from theWikiText-103 corpus (Merity et al., 2017), sampled letter-by-letter; Nonsensical - nonsensical sequence ofreal words built from the WikiText-103 corpus, sampled word-by-word; Wiki - real sentences sampled fromthe WikiText-103 corpus. We investigate the case where the query budget is constant and is set to the size of the original trainingdataset. Higher query budgets can perhaps provide a more successful extraction, but require the cost ofcollecting and labeling each sample to be even lower to allow a cost effective attack, as discussed in .2.For each prior knowledge percentage x%, we fill the remaining queries with any of the previously describedalternative data sources. We evaluate for up to 50% prior knowledge, because we find that the attackersquery set is dominated by IND samples for higher proportions and is thus less informative. We includethe case where all queries are sampled from the query distribution, and the attacker has no (0%) priorknowledge. Note that DFME requires a significantly higher query budget and incurs a high computationalcost, therefore we evaluate this setting for a query budget of 20M and with up to 30% prior knowledge.",
  "Can ME be used with only a few queries?": "ME seeks to obtain a copy of the victim model while incurring reduced costs compared to training fromscratch. In .2 we note that one way to reduce costs is to limit the interaction with the victimmodel to the minimum that is, reduce the overall ME query budget. In this subsection we investigate suchreduction and discover that it is indeed sometimes possible. In we demonstrate that limiting thequery budget for all cases but DFME, still leads to the high attack performance. What is more, even thebaseline attacker with only 10% of the data, i.e. reducing the query complexity by a factor of 10, managesto develop a model with a relatively high performance, i.e. 79.21% compared to 95.54% for CIFAR-10 and77.85% compared to 83.64% for MNLI; with 50% it approaches the original test accuracy of the model,i.e. 92.09% for CIFAR-10 and 81.94% for MNLI. We find that the baseline attacker with more than 50% ofthe original victim training set can extract the model (see ). Although we observe this phenomenonconsistently in the case above, in our answer to the next question we will show that such good performanceheavily depends on the prior IND knowledge or requires OOD data that, as described in , informsthe attacker about the IND performance.",
  "(e) wiki(MNLI)": ": As our OOD detection rule is distinguishing between in and out of distribution queries based onthe model confidence, we evaluate the effect of decreasing the models confidence by increasing the softmaxtemperature T from 1 to 2. Results shown in this plot show that this indeed even further lowers the benefitthe attacker gains by utilizing OOD queries. this is out of scope for our paper. We propose a method for measuring the reliance of ME attacks on theinformativeness of OOD regions, as part of a bigger discussion over the role of prior knowledge and thecommon methodology of ME attacks, and do not investigate the utilization of it as part of any defensemechanism.",
  "Can ME be used to reduce data costs?": "A common threat model assumes that IND is scarce and expensive. As such, it is desirable to reduce datacollection costs by avoiding, or minimizing the use of IND queries, and prioritizing the use of OOD queries.We find that in some cases ME attacks can indeed do so. More precisely, in we show that OODqueries provide a feasible substitute for IND queries in lower prior knowledge settings. However, we claim thatthis is only possible because OOD queries inform the adversary about the IND performance i.e. IND decisionboundaries can be inferred from the OOD ones, either because the distributions are sufficiently similar orbecause the model optima are predictable. To explicitly estimate if all OOD queries will aid ME performance,we instrument the model with the OOD detector as described in .3. Our instrumentation, in essence,allows us to control how much information can be derived about the IND from the OOD region. Per , an intelligent attacker would have to distinguish between IND and OOD samples to bypass theinstrumentation or otherwise waste queries and potentially learn non-existent decision boundaries. A poorattack policy would inevitably learn fake decision boundaries, while a strong attacker would learn to avoidthem. Theoretically, in .1 we discuss that with no prior knowledge, it should be improbable foran attacker to sample from the IND region alone. Furthermore, in .2 we argue that when samplesfrom both regions are considered, prior knowledge is required to distinguish between the two. We empirically evaluate this argument using the methodology described in .3. We show that whenthe informativeness of the OOD region is deteriorated, OOD queries cannot replace the need for expensiveIND queries. We evaluate different threshold values () against an attacker that, in addition to its priorknowledge samples, uses additional Dtrain queries, i.e. the size of the original training dataset.ForDFME, we allow that attacker to use 20M additional queries. Different threshold values influence the resultsby changing the number of additional queries that are predicted by Vf versus Vo the lower the threshold,the better for the attacker. We measure the false-positive rate (FPR) for the different threshold valuesin Appendix F.3. The results in clearly demonstrate the described effect. As the attacker has less prior knowledge, itis more reliant on the benefit of the additional queries. As such, it is more impacted by the fake boundariesthat control OOD informativeness. The attack accuracy is thus reduced closer to, or below, the accuracyof the baseline, which makes no additional queries. This effect is weaker for some of the settings in ourNLP task, specifically the nonsensical and wiki queries. This is due to the similarity between the true datadistribution and these query distributions. We discuss this observation in more detail in Appendix H.",
  "Can ME be used both to reduce the data costs and use only a few queries?": "Both Q2 and Q3 assessed that ME costs could be reduced by either using substitute data or reducing thenumber of queries; but can an adversary achieve both? The discussion of Q3 gives a clear answer to thisquestion - no. We showed that unless the OOD dataset is chosen in such a way as to inform the attacker about the INDbehavior, the attacker must either rely on the IND data available to them or utilize a very large querybudget. It is impossible to achieve both in this setting. Therefore, our answer to this question would besometimes, as it depends on how much information can be derived about the IND from the OOD region.",
  "(b) 30% MNLI": ": Comparison between the convergence rate of an attacker that uses the victims full probabilityvector output (soft labels), an attacker that utilizes a label-only access to the victim model, and an attackerthat uses the real ground truth labels. In all cases the attacker has access to 30% of the true training samples.The attacker does not learn faster by attacking the victim model, and only benefits from the victim modelwhen it has little prior knowledge over the true data distribution. In Q2, Q3, and Q4, we discussed ME assuming a reduction of costs on data collection and the overall numberof queries. We concluded that in general, such reduction is unattainable, and OOD queries do not triviallyreplace IND queries. In this section, we turn to label cost reduction. Label cost reduction could be achieved in two ways: by reducing the cost of a single query or, alternatively,by increasing the amount of information that a single query provides. Note that an increase in the formerdoes not always mean that overall cost increases, as combined with the increase in the latter, they can costless overall. We find that in the classification setting considered in this work, we could not find a method to consistentlyincrease the informativeness of the response, suggesting that the victim model merely serves as a labelingoracle, even in cases where seemingly more information is provided than just a label. To demonstrate this, werevisit our baseline attacker and compare its performance when trained on: 1) soft-label i.e. full probabilityvector; 2) using label-only; and finally, 3) using the real ground truth labels. Results presented in demonstrate that the attackers performance is nearly equivalent in all three cases. This phenomenon alignswith the prior literature (Orekondy et al., 2019b).Additionally, we examined each of the cases from acomputational perspective and compared their convergence rates. In we present this comparisonfor the case where the attacker utilizes 30% prior knowledge. We observe that the convergence rate is similaracross the three attackers, i.e. the attacker does not even learn faster by querying the victim model.",
  "In Appendix C we additionally show this for the 5% and 60% prior knowledge settings and observe a similarphenomenon": "We additionally examine the response informativeness effect for attackers who can utilize additional queriesfor other query distributions. Results, presented in Appendix D, show that for lower prior knowledge settings,and for query distributions that significantly differ from the true distribution, soft-labels provide some benefitover label-only access. However, this effect diminishes as the attacker uses more prior knowledge, or acquiresaccess to a query distribution which is closer to the true distribution. In light of the finding that the victim model serves as a labeling oracle, how cost-effective is querying thevictim? We discussed current annotations costs in .2, where we suggested that for many use cases,including current ME benchmarks, crowdsourcing annotations may be a cost effective alternative. However,this is not true for all cases, as e.g. medical data will most likely have higher data collection and labelingcosts. We therefore conclude that it is sometimes possible to reduce labeling costs by not running ME andinstead finding an alternative labeling provider.",
  "Conclusion": "In this paper we investigate the common assumption that ME attacks are more cost-effective than traininga model from scratch. Primarily, it is assumed that OOD data can be utilized for reducing data collectioncosts, and by using a limited number of queries to the victim model both labeling costs and training costscan be reduced as well. We show that often this is not the case. We demonstrate that the performance ofan attacker with a reasonable query budget is bounded by their access to IND data. Augmenting the INDdata with data from another distribution relies on informative responses for task-irrelevant queries. We showthat decorrelation of OOD from IND responses changes the attacker-victim dynamic, where attacks becomemuch less cost-effective, forcing the attacker to collect IND data. We show that given sufficient IND data,the victim model mainly serves as a labeling oracle, which in turn is not necessarily more cost-effective thanonline labeling services. We thus conclude that the adversarial goals and incentives of ME attacks should beredefined. We would like to acknowledge our sponsors, who support our research with financial and in-kind contribu-tions: Amazon, Apple, CIFAR through the Canada CIFAR AI Chair, DARPA through the GARD project,Intel, Meta, NSERC through the Discovery Grant, the Ontario Early Researcher Award, and the SloanFoundation. Resources used in preparing this research were provided, in part, by the Province of Ontario,the Government of Canada through CIFAR, and companies sponsoring the Vector Institute. We would alsolike to thank CleverHans lab group members for their feedback.",
  "Google Cloud. Ai platform data labeling service pricing, 2024": "Jacson Rodrigues Correia-Silva, Rodrigo F Berriel, Claudine Badue, Alberto F de Souza, and Thiago Oliveira-Santos. Copycat cnn: Stealing knowledge by persuading confession with random non-labeled data. In 2018International Joint Conference on Neural Networks (IJCNN), pp. 18. IEEE, 2018. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchicalimage database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248255. Ieee,2009.",
  "Kurt Hornik, Maxwell Stinchcombe, and Halbert White.Multilayer feedforward networks are universalapproximators. Neural networks, 2(5):359366, 1989": "Xing Hu, Ling Liang, Lei Deng, Shuangchen Li, Xinfeng Xie, Yu Ji, Yufei Ding, Chang Liu, TimothySherwood, and Yuan Xie. Neural network model extraction attacks in edge devices by hearing architecturalhints. arXiv preprint arXiv:1903.03916, 2019. Weizhe Hua, Zhiru Zhang, and G Edward Suh. Reverse engineering convolutional neural networks throughside-channel information leaks. In Proceedings of the 55th Annual Design Automation Conference, pp. 16,2018. Matthew Jagielski, Nicholas Carlini, David Berthelot, Alex Kurakin, and Nicolas Papernot. High accuracyand high fidelity extraction of neural networks. In 29th USENIX security symposium (USENIX Security20), pp. 13451362, 2020. Sanjay Kariyappa and Moinuddin K Qureshi. Defending against model stealing attacks with adaptive mis-information. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,pp. 770778, 2020. Sanjay Kariyappa, Atul Prakash, and Moinuddin K Qureshi. Maze: Data-free model stealing attack usingzeroth-order gradient estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision andPattern Recognition, pp. 1381413823, 2021.",
  "Jonah OBrien Weiss, Tiago Alves, and Sandip Kundu. Ezclone: Improving dnn model extraction attack viashape distillation from gpu execution profiles. arXiv e-prints, pp. arXiv2304, 2023": "Rina Okada, Zen Ishikura, Toshiki Shibahara, and Satoshi Hasegawa. Special-purpose model extractionattacks: Stealing coarse model with fewer queries. In 2020 IEEE 19th International Conference on Trust,Security and Privacy in Computing and Communications (TrustCom), pp. 19952000, 2020. doi: 10.1109/TrustCom50675.2020.00273. Tribhuvanesh Orekondy, Bernt Schiele, and Mario Fritz. Knockoff nets: Stealing functionality of black-boxmodels.In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp.49544963, 2019a.",
  "Tribhuvanesh Orekondy, Bernt Schiele, and Mario Fritz. Prediction poisoning: Towards defenses againstdnn model stealing attacks. arXiv preprint arXiv:1906.10908, 2019b": "Soham Pal, Yash Gupta, Aditya Shukla, Aditya Kanade, Shirish Shevade, and Vinod Ganapathy. Activethief:Model extraction using active learning and unannotated public data. Proceedings of the AAAI Conferenceon Artificial Intelligence, 34(01):865872, Apr. 2020. doi: 10.1609/aaai.v34i01.5432. URL David Patterson, Joseph Gonzalez, Quoc Le, Chen Liang, Lluis-Miquel Munguia, Daniel Rothchild, DavidSo, Maud Texier, and Jeff Dean. Carbon emissions and large neural network training. arXiv preprintarXiv:2104.10350, 2021.",
  "Tongzhou Wang, Jun-Yan Zhu, Antonio Torralba, and Alexei A. Efros. Dataset distillation, 2018": "Adina Williams, Nikita Nangia, and Samuel Bowman.A broad-coverage challenge corpus for sentenceunderstanding through inference. In Proceedings of the 2018 Conference of the North American Chapterof the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers),pp. 11121122. Association for Computational Linguistics, 2018. URL Yun Xiang, Zhuangzhi Chen, Zuohui Chen, Zebin Fang, Haiyang Hao, Jinyin Chen, Yi Liu, Zhefu Wu,Qi Xuan, and Xiaoniu Yang. Open dnn box by power side-channel attack. IEEE Transactions on Circuitsand Systems II: Express Briefs, 67(11):27172721, 2020.",
  "V": ": Illustration of model extraction attacks. In this setting, the victim model is trained on sometraining data that is not accessible by the adversary. The adversary attempts to steal the victim model overquery access, in order to obtain an approximate copy of that model with similar performance. Typically, theadversary achieves this by querying the victim model to collect an attacker dataset. This, in turn, is usedto train a stolen, surrogate copy that mimics the victims behavior.",
  "AA note on current literature": "Active learning is a branch of semi-supervised learning which focuses on finding query efficient-trainingregimes, i.e. it explores methods that can learn a task with the least number of questions to the oracle.Chandrasekaran et al. formulated that the process of model extraction is very similar to active learning andsuggested that improvements in query synthesis active learning should directly translate to model extraction.This relationship works in both directions and implies that greater performance in model extraction directlytranslated to better active learning regimes. Some of the current literature reports an ability to extract complex models with a handful of queriese.g. Tramr et al. claims successful extraction of a Multilayer Perceptron with around a thousand queriesor Zanella-Beguelin et al. extracts SST-2 BERT with around two thousand queries. This suggests that thereexists (very) query-efficient training strategies, with oracles providing labels for otherwise unlabeled data.Yet, in practice, literature in active learning reports less impressive results in these settings. For example,sophisticated state-of-the-art regimes on CIFAR-10 report improvements up to 7% for 5% and up to 5% for10% of dataset (Yoo & Kweon, 2019; Beck et al., 2021; Yi et al., 2022). Most importantly, the improvementsare similarly dominated by the original data access. Indeed, our paper questions the apparent free lunch reported in model extraction literature, and suggeststhat extraction reported is mostly an artifact of underlying data access, drastically overestimating potencyof the attacks. To best understand the underlying attack performance it is imperative to consider the benefitof model extraction for no-data settings and cover them extensively, focusing on the low 05% regions.",
  "BEffect of surrogate dataset over attack performance": "In .1 we investigated the performance gain of using additional queries drawn from a different datadistribution. We evaluated this for a query budget of the size of the original training dataset. This budgetrepresents the maximal query complexity that can still be considered successful. Here, we further extend theresults presented in and evaluate the effect of the number of additional queries from various datasources. Results are presented in . We show that, in most cases, larger numbers of additional queries only havea limited effect, which is also very dependent on the quality of the queries and the level of prior knowledge.Adversaries with higher levels of prior knowledge are less affected by the additional queries. The DFMEqueries, while being out-of-distribution, do perform very well, but at the cost of prohibitively high querybudgets.",
  "(g) wiki (MNLI)": ": Evaluating the effect of adding different amounts of additional queries for a given level of attackerprior knowledge. Plots (a)-(d) present attacks over the CIFAR-10 victim model and (e)-(f) are for the MNLIvictim model. In all plots, a query budget of 0 represents the baseline attack accuracy, as presented infig. . Results show that adding larger amounts of additional queries has a limited effect, which isdependent on the query quality and amount of prior knowledge.",
  "CAttack convergence rate comparison": "Following the discussion in .5, we further investigate the actual benefit of attacking the model ratherthan training from scratch, from the computational perspective. We examine the convergence rate of theattack accuracy. presents a comparison between the convergence rate of the three cases - an attackerthat uses the victims soft labels (i.e. full probability vectors), an attacker that has a label-only access to thevictim model, and an attacker that uses the real (ground truth) labels. We show that the convergence rate issimilar across the three attackers, i.e. the attacker does not learn faster by querying the victim model. Weshow that in cases where the attacker has limited prior knowledge over the data distribution, e.g. in the 5%case, the attacker does get some benefit from using the victims soft labels; however, this benefit disappearswith increased prior knowledge.",
  "DEffect of response informativeness": "In .5 we discussed the role of ME in reducing data labeling costs. We found that, in the settingconsidered in this work, ME attacks serve as a labeling oracle. To demonstrate this, we showed that whentraining the baseline attacker with soft-label access to the victim model, i.e,. when the victim model respondsto each issued query with a full-probability vector, we did not get any significant improvement over caseswhere the attacker was trained with label-only access or even the real ground truth labels. In order to furtheremphasize the limited possible gain an attacker can get by obtaining labeling through an ME attack, wecompare the difference in attack performance between soft-label access and label-only access for attackersthat can utilize additional queries from other query distribution.",
  "(f) 60% MNLI": ": Comparison between the convergence rate of an attacker that uses the victims full probabilityvector output (soft labels), an attacker that utilizes a label-only access to the victim model, and an attackerthat uses the real ground truth labels. In all cases the attacker has access to 30% of the true training samples.The attacker does not learn faster by attacking the victim model, and only benefits from the victim modelwhen it has little prior knowledge over the true data distribution. Similar to the results presented in , we evaluate this for a constant query budget, fixed to the sizeof the original training set. Due to the high computational cost of DFME, we omit this setting in thisevaluation. The results, presented in , demonstrate that soft-labels aid the attacker mainly in the lower priorknowledge settings and for query distributions that are significantly different from the true distribution. Inother cases, the victim model serves as a labeling oracle, and the soft labels provide little gain over thelabel-only access. For example, in the CIFAR-10 case, an attacker that utilizes additional random queriesand has access to 1% of prior knowledge can improve its performance by additional 20.8%, from 26.43% to47.24% attack accuracy. However, the same attacker with 10% prior knowledge can only improve by 1.25%,from 81.31% to 82.56%, and another attacker with 1% prior knowledge that can utilize SVHN queries canonly increase by 7.19%, from 68.14% to 75.33%. In the case of MNLI, the benefit of soft labels is evenweaker, due to the similarity between the distributions, as further discusses in Appendix H.",
  "ESampling complexity intuition": "In .1 we discuss the intuition behind the complexity of sampling IND queries with or without priorknowledge over the distribution. Here, we provide a toy example of this complexity for different levels ofprior knowledge, modeled by the overlap between the IND and the attackers query distribution. We thenextend intuition for our considered models, and estimate the complexity of sampling IND in this setting.",
  "E.1Sampler bias toy example": "We explore the relation between a prior knowledge level and the attackers probability of successfully samplingfrom the useful domain. We denote the sampler from the useful domain, i.e. the in-distribution domain,as Vs N(v, v), and the attackers sampler, i.e. the distribution from which queries are drawn, asAs N(a, a). We model the prior knowledge level as the Wasserstein distance between both distributions. plots the probability of sampling from the informative overlap region as a function of Wassersteindistance between Vs and As when and are sampled uniformly. Small differences in sampling distributions,i.e. less prior knowledge, result in a significant reduction in in-distribution sampling probability. This, inturn, results in wasted queries, as sampling outside of the overlap is not informative, and in reduced modelcapacity, as the attacker wastes capacity on learning the irrelevant OOD region. This holds even in caseswhere distributions overlap significantly. Note that in practice, with more dimensions, the volume wouldoverlap less, and the useful sampling probability would be even further reduced. RandomTrue Distribution CIFAR-10MNLI 0.0 0.2 0.4 0.6 0.8 1.0",
  "(b) T = 2": ": Estimating the sampling complexity of our real victim models by measuring the percentage ofrandom queries that are predicted by the model with high confidence, e.g. above 90%. This is in comparisonto the complexity when using queries from the true distribution, sampled from the test set. We evaluatethis for both the original model, with a softmax temperature T=1 (a), and for a less confident model, witha high softmax temperature T=2 (b).",
  "E.2Sampling complexity in real models": "In the previous section, we explore the sampling complexity of an attacker in a toy setting. In this section,we extend this notion to a more realistic scenario of estimating the sampling complexity for our real victimmodels. For this, we attempt to estimate the in-distribution volume and the complexity of sampling within itby measuring the number of random queries in this volume. Similarly to our OOD control module, describedin detail in Appendix F, we define a query as in-distribution, i.e. inside the volume, by observing the modelsconfidence in predicting this query. High confidence queries, above some predefined confidence threshold, areconsidered to be in-distribution. Therefore, for the task of volume estimation, we measure the percentage ofrandom queries that are above this threshold. In this section, we use a threshold value of 90. In (a) we present our estimation in both victim models. As can be seen, only 59% of the randomqueries are sampled from within the volume in the case of the CIFAR-10 model, and only 45% in the case ofthe MNLI model. We additionally compare this to the sampling complexity when using real in-distributiondata, by measuring the percentage of samples from the test set that are predicted by the model with ahigh confidence. In this case, where we have significant prior knowledge over the distribution, the samplingcomplexity is drastically decreased.",
  "FControlling OOD informativeness - full implementation details": "In this section, we elaborate on the implementation details behind our methodology for evaluating the effectof limiting the utility of OOD queries, described shortly in .3. Given the original victim model Vo,we create a hybrid victim model Vh by combining the original victim model Vo with an additional modulewith different, or additional, decision boundaries Vf. For each query x, we apply a decision rule R over x,to determine which of the two modules, Vo or Vf, should be used for predicting this query. The decision rule R is implemented by applying a threshold value over the prediction confidence of Vo.If a query x has a low prediction confidence, we define it as an OOD query and return the predictionVf(x). Otherwise, we define it as IND and return Vo(x). We calibrate R by increasing the models Softmaxtemperature to 2, as discussed in more detail in Appendix F.2. Thresholding the prediction confidence servesas a naive approximation for OOD detection, and using more sophisticated OOD detection methods willonly increase the effect of modifying the OOD behaviour. As such, the exploration of different decision rulesis out of scope for this work. As for the fake model Vf, we implement it by fitting a Gaussian Mixture Model (GMM) for each class ofthe training data. For each class c {1, 2, ..., C}, where C is the number of classes, we sample S = 5000training data points labeled as class c, i.e. {(x1, c), (x2, c), ..., (xS, c)}. We then use the victim model Vo tocompute the predictions of this set, i.e. the logits. We fit a GMM to this set of logits { y1, y2, ..., yS}, whereyi = Vo(xi). For each class, we create m = 5 anchor points A1c, . . . , Amcthat will be used to assign queries for thisclass. For this, we first compute the feature representations of the data samples used for fitting the classGMM,i.e. {(x1, c), (x2, c), . . . , (xS, c)}, using some feature extractor . Then, we cluster these feature vectors{(x1), . . . , (xS)} into m clusters using the Kmeans clustering algorithm. We define the anchor points tobe the centroids of each cluster. For vision tasks, we use a pre-trained ResNet34 for the feature extractor ;for NLP tasks we use a pre-trained BERT-base model. At last, we sample m permutations j : C C, for j [m]. The permutations would ensure that no querywould be predicted using its real assigned class, therefore avoiding tail-of-distribution samples classifiedas OOD samples (i.e. false-positives samples), to be correctly labeled and leak information about true INDbehaviour. For a given query sample x, we compute its feature representation (x) and find the nearest anchor point Aji,in terms of the L2 distance between (x) and all C m anchor points. The anchor point Aji represent the jth anchor point of the ith class. We then permute the assigned class i using the jth permutation,i.e. i = j[i],and sample a fake logit y from the GMM we fitted earlier to class i. This value is returned as the complexmodel prediction, Vf(x) = y. This construction requires the evaluation of a subset of the training data used for fitting the GMMs. Theevaluation, fitting, and clustering process are done only once; hence, it is relatively computationally inexpen-sive. During inference, each query directed to the fake model Vf adds some computational cost of computingits feature representation, and performing a nearest neighbor search. However, this is only true for OODsamples and false-positive IND samples. Most legitimate users queries, i.e. the true-positive queries, arecompletely unaffected by the introduction of this additional module. It is important to note that while our method decreases the utility of existing ME attacks, as shown in ourresults, we do not present it as a defense mechanism but rather as a method for evaluating our hypothesis. Anefficient defense mechanism can be designed based on these principles, with some additional effort. However,",
  "F.1Effect of anchor points and permutations": "The design of our proposed OOD module Vf satisfies two main objectives. The first, and most important,is to avoid leaking real IND behaviour by the predictions made by Vf. The second, was to lure the attackerto waste effort and capacity in learning fake and more complex decision boundaries, and therefore reducingfurther its performance over the real IND decision boundaries. To obtain the first goal, we introduced the permutation mechanism. As mentioned before, some tail-of-distribution samples are falsely classified as OOD samples, e.g.false-positives. As these samples share thesame distribution as the anchor points, when searching for the nearest neighbor anchor point, they would becorrectly assigned to their real class. When no permutation is applied, this will result in a prediction thatresembles the original prediction given by Vo, and therefore leaks information. To avoid this, the matchinganchor point should direct the sample to a different class by using some class-level permutation. To obtain the second goal, we introduce multiple new decision spaces by using multiple permutations perclass. As a result, two samples that were initially assigned to one class can now be directed to two differentclasses, placing a new decision boundary between them. For this, we use multiple (m = 5) anchor points perclass, and each anchor point is coupled with a different permutation. Therefore, two samples x1 and x2 thatwere assigned to two anchor points related to class i: A1i and A2i , will now we reassigned to two differentclasses. x1 would be predicted using the GMM fitted for class j = 1(i) and x2 would be predicted usingthe GMM fitted for class k = 2(i). To better demonstrate the effect of using multiple permutations and anchor points, we provide an ablationstudy in . We compare the performance of our OOD module in 4 different settings: (i) using oneanchor point per class and no permutations (ii) using one anchor point per class with class-level permutation(iii) using 5 anchor points per class with the same permutation shared between all anchor points (iv) ourproposed method - 5 anchor points per class with 5 different permutations. These would be denoted as \"1anchor, no perm\", \"1 anchor, 1 perm\", \"5 anchors, 1 perm\", \"5 anchors, 5 perms\", respectively. The biggesteffect can be attributed to the simple addition of permutations; however, it can be further emphasized byincorporating the additional anchor points and multiple permutations.",
  "F.2Softmax temperature influence": "In order to calibrate R to better distinguish between IND and OOD samples, we increase the models Softmaxtemperature to 2. By doing so, we force the model to be less confident, which results in more queries beingpredicted by Vf and not Vo. As shown in , a temperature value of 2 indeed detects most of theOOD queries. In we demonstrate the effect of the temperature value over the behaviour of ourcontrolled informativness experiment and show that a better calibrated decision rule indeed further lowersthe benefit the attacker gains by utilizing OOD queries. It is important to note that this also increases the FPR of the IND samples,i.e.the percentage of INDsamples predicted by Vf instead of Vo. A higher FPR results in a decline in model test accuracy. However,as shown in , and described in more detail in Appendix F.3, this decrease is not linearly dependenton the exact FPR.",
  "F.3Impact of": "The value of the confidence threshold determines the utility extraction difficulty trade-off. In .3we evaluate our OOD control mechanism on different values of , and observe the effect each has over theattack performance. In order to verify that we are not trivially rejecting all inputs, we additionally measurethe false-positive rate (FPR) for the different threshold values. In , we detail the relation betweenthe threshold value, the FPR, and the effect on the victims test accuracy. For lower values, most queriesare predicted using the original Vo, i.e. the victim models utility is barely harmed since most queries areanswered by the real model. In this case, the attackers performance is barely affected, as it can still getinformative responses by issuing OOD queries. As increases, more queries including some IND onesare predicted by the fake model Vf. This makes it more difficult for an attacker to infer which decisionboundaries are IND. We discussed the underlying complexity in detail in Sections 4.1 and 4.2. As can beseen, the attack accuracy dramatically decreased, which verifies that the OOD behaviour leaks almost noinformation about the IND behaviour. To further demonstrate this, we present in a comparisonbetween the labels predicted by Vo and those predicted by Vf for actual OOD queries (i.e. true positives)as well as tail-of-distribution IND samples that were detected as OOD (i.e. false positives). We show thiscomparison for an attacker that utilizes additional SVHN queries, 30% prior knowledge, and for a thresholdof = 95. It is clear that, although the fake labels are heavily biased towards one class for the additionalqueries, in both cases, they are uncorrelated with victim models predicted labels.",
  "(c) Additional (SVHN)": "class 0class 1class 2class 3class 4class 5class 6class 7class 8class 9 : (a) The agreement between the attacker model and the victim model, for the CIFAR-10 task andan attacker utilizing SVHN additional queries. The agreement is separated into the real task (Vo) and thefake task (Vf). We can see that fake task agreement is higher than random guessing (10%), which impliesthat the attacker was able to learn the fake, irrelevant task, and waste some capacity.(b)-(c): Comparison between the labels predicted by Vo (x-axis) and the labels predicted by Vf (y-axis), foran attacker that utilizes additional SVHN queries, 30% prior knowledge, and for a threshold of = 95. Eachbar i demonstrates the distribution of the new fake labels that would have been predicted by Vo as classi. The y-axis is normalized to show the percentage of samples from each class. Although the fake modelspredictions are biased towards one class for the additional queries, in both cases they are un-correlated withthe victim models predictions, and therefore present new decision boundaries for the OOD queries. model parameter budget, SGD learns the smoothest boundaries first and only gets to the other boundariesif the capacity permits (Ben Arous et al., 2021). This has real practical implications on the fake regionsthat we add to the model. Namely, we can not add decision boundaries that are more complex than the realtask, since SGD learns to ignore them in light of the real decisions, limiting the extent to which we can addarbitrary complexity into the models. In we have discussed the query budget and model capacity that the attacker must spend if it cannot distinguish between the task-related (IND) and unrelated (OOD) queries. To verify, we explicitly checkthat the attacker indeed learned both the task related and unrelated knowledge, and did not ignore thepredictions made by Vf due to the SGD bias described above. For this reason, we investigate the agreement between the attacker model and both Vo and Vf, for the SVHNadditional queries case. We separate the agreement of the samples predicted by Vo and the samples predictedby Vf. demonstrates that the attacker model agreement with Vf is higher than chance level, whichis 10% in this case (for the 10 CIFAR-10 classes). This proves that the attacker learned both the real andthe fake task and, as such, wasted capacity on learning irrelevant decision boundaries.",
  "GKnockoff Nets Evaluation": "In addition to the CIFAR-10 and MNLI datasets, we evaluate our main experiments on the Indoor67 (Quat-toni & Torralba, 2009), CUBS200 (Wah et al., 2011) and Caltech256 (Griffin et al., 2007) datasets. For this,we follow the setting and training details considered by the Knockoff Nets attack (Orekondy et al., 2019a),and use the pre-trained victim models provided by the authors. Orekondy et al. provides two strategiesfor sampling queries (i) random - in which the queries are sampled uniformly at random from some querydistribution (ii) adaptive - in which the queries are sampled according to a learned policy . We note thatthere is no official implementation for the adaptive strategy, and thus we reimplement the method with thedetails provided by Orekondy et al.. We simplify the hierarchy to be one-level deep, and omit the coarse-to-",
  "(c) Caltech256": ": We evaluate the effect of augmenting the attackers queries with additional queries sampled fromthe ImageNet dataset, in comparison to our baseline attacker, which only use its prior knowledge. We fixthe query budget to be the size of the original training set to provide a fair comparison between the differentattackers. It can be seen that, as the attacker has more prior knowledge over the true distribution, it does notgain much benefit by augmenting the query set. Since the ImageNet dataset does share some distributionalsimilarity with the true training data distributions, we observe a significant improvement from utilizing itin the lower prior knowledge settings.",
  "For the attacker model, we use a ImageNet pretrained ResNet-34 architecture.We refer the readerto Orekondy et al. for full training details": "Similar to the evaluations in .1, we show in that ME indeed works, and the model canbe approximated with blackbox query access. To better visualize this, we compare the performance of abaseline attacker, that only uses its prior knowledge over the distribution, with that of the OOD-basedattacker. Here, the baseline attacker is assumed to have access to either a randomly or adaptively sampledsubset of the victims true training dataset. Following Orekondy et al. we use ImageNet (Deng et al., 2009)as the surrogate (additional) dataset, from which we either sample randomly or using the adaptive strategy.We follow the setting described in .1, and fix the query budget to be the size of the original trainingdataset. The results, presented in align with the results in .1, as they show that as theattacker has more prior knowledge, it benefits less from utilizing additional queries. We do observe that inthe lower prior knowledge settings, the ImageNet additional queries do provide a significant improvement,especially for the Indoor67 and Caltech256 datasets. We hypothesize that this is due to a high similaritybetween the distributions. This also aligns with the main findings of our paper. additionally answers the questions described in .2 and .3. Namely, ME can beused with only a few queries, as the attack is succesfull both when bounding the query budget to the size ofthe original training set, and when using just a smaller fraction of IND samples. Moreover, the results alsoshow that the use of ImageNet queries as OOD data is indeed effective and can help reduce the data costsinvolved with using expensive IND samples. However, as we discussed in and further evaluated in .4, the answers to the previousquestions depends on the implicit assumption that the IND decision boundaries can be inferred from theOOD ones. We evaluate this assumption in this case study as well, following the methodology describedin .3. We set the models softmax temperature to 2, and use Dtrain additional queries.",
  "(f) Caltech256 (adaptive)": ": Empirical evaluation of the risk posed by an attacker with some prior knowledge over the truedata distribution. The prior knowledge is expressed as access to a percentage of the true training set. Inmost cases, an attacker with more then 50% access can nearly fully extract the model, however, in this case,the extraction is not more efficient than training from scratch. The attacker does not gain much by queryingthe victim model versus using the real labels, which also seems to be equivalent to a label-only access to thevictim model. This shows that, other than performing as a labeling oracle, the extraction is meaningless inthis case. demonstrates that limiting the informativeness of OOD queries indeed decreases the benefit theattacker gained by utilizing additional ImageNet queries, even to the point of performing worse than thebaseline in the higher prior knowledge settings.This again aligns with our findings in .4 andsuggests that the attacker can not acheive both goals, and in the absence of prior knowedlge, correspondingto higher data collection costs, a high query budget must be used. Aligning with the findings reported in .5, we present in a comparison between the attackperformance using soft-label access to the victim model to that of an attacker with access to the real(ground truth) labels or label-only access to the victim model. This comparison provides the same evidenceas described in .5. It demonstrates that attacking the victim model is merely using the victimmodel as a labeling oracle in the absence of access to the real labels, and the attacker does not gain muchadditional benefit. This phenomena was also observed by Orekondy et al..",
  "(d) Wiki": ": We compare the confidence values of the victim model over the true data distribution versus thedifferent types of out-of-distribution queries. Results show that the model tends to be highly confident forboth in-distribution as well as for some of the out-of-distribution queries, making it harder for the OODcomponent to make any impact for lower threshold values. This result can explain the surprising success ofusing random nonsensical queries in the language domain versus the poor results of using random queries inthe vision domain.",
  "HNLP distribution similarity": "The results presented in show that the effect of limiting the leakage from OOD behaviour to INDbehaviour is weaker for some of the settings in our NLP task, specifically the nonsensical and wiki queries.This might seem confusing. The wiki queries are broadly equivalent to the surrogate dataset queries that arecommonly used in the vision domain, however they exhibit a different response in our evaluation. As bothrandom images and random nonsensical sentences have no real meaning, one might expect similar behaviourbetween them. In the past, nonsensical queries were even used as a benchmark under the name of randomqueries e.g. by Truong et al.. This difference between the vision and NLP domains is due to the similarity between the true data distri-bution and the aforementioned query distribution. Although the queries are drawn from either a randomnonsensical distribution in the nonsensical case, or a from a different corpus in the wiki case, we observethat many common words are shared between all three distributions. This results in similar model behaviourbetween the OOD queries and the true data. It can explain the success of the queries in this domain, whichcomes in significant contrast to the lack of success of random or some surrogate queries in the vision domain,where the input space is a continuous pixel space rather than a (relatively small) discrete dictionary ofwords. The similarity between distributions results in high confidence values for both in-distribution andout-of-distribution queries, making it difficult to separate them and apply our method. We show the confi-dence distribution for each query type in . In the case of the random queries, where each letter issampled, and the sentences are not composed of real words, we can observe a significantly lower gain fromthe additional queries, which is more in line with the findings from the vision domain.",
  "ITask and model complexity": "In this section, we investigate the relationship between task and model complexity, as well as the querycomplexity of model extraction attacks. In we observe that for simpler tasks, such as classificationover the SVHN and SST-2 datasets, even an adversary with little prior knowledge can successfully extractthe model, e.g. 5% data access. This is in contrast to the results we demonstrated for the CIFAR-10 andMNLI datasets, which are significantly harder tasks to learn. We additionally investigate the effect of the size of the victim model architecture. In addition to the ResNet-34-8x CIFAR-10 victim model evaluated so far, we trained two CIFAR-10 victim models: a larger ResNet-50-8x and a smaller ResNet-18-8x. Note that we trained these models ourselves, while for the ResNet-34-8xarchitecture, we used the pre-trained model by Truong et al.. The results, presented in , show that",
  "(b) SST-2": ": The baseline attacker is able to successfully extractsome victim models very easily, with as little as 5% of the trainingdata. We hypothesize that this is due to the inherent linearity ofthese datasets. 15102030405060708090 100 percentage 0.4 0.5 0.6 0.7 0.8 0.9 accuracy ResNet-50-8xResNet-34-8xResNet-18-8x : We investigate whether thevictim model size has any impact onthe attack success.We observe noimpact when increasing (ResNet-50-8x) or decreasing (ResNet-18-8x) themodel size.All models are incredi-bly over-parameterized and, therefore,the differences between the models areinsignificant in terms of the attackcomplexity. this has minimal impact on the attack success. Lack of differences here can be explained by the fact thatall three models share similar accuracy - 94.22% for ResNet-18-8x, 95.54% for ResNet-34-8x, and 93.72% forResNet-50-8x. As previously noted, the models in practice mainly serve as labeling oracles, meaning the difference in theattack performance is connected to the model accuracy. Having said that, it is worth mentioning that wehave not performed a thorough hyperparameter search in the training of the models. This should not affectthe validity of the results and should only cause a slightly lower accuracy.",
  "Prior Knowledge": "(% of training data size) 0.3 0.4 0.5 0.6 0.7 0.8 baseline attack= 0 (real model)= 99= 95= 90= 75 : An ablation study of the use of multiple anchor points and permutations in the instrumentationof the OOD module. Comparing between 4 settings shows that indeed our proposed method (\"5 anchors, 5perms\") results with the biggest degradation of the attackers performance even when given access to higherlevels of prior knowledge."
}