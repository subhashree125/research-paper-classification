{
  "Abstract": "We address the critical challenge of applying feature attribution methods to the transformerarchitecture, which dominates current applications in natural language processing and beyond.Traditional attribution methods to explainable AI (XAI) explicitly or implicitly rely on linearor additive surrogate models to quantify the impact of input features on a models output.In this work, we formally prove an alarming incompatibility: transformers are structurallyincapable of representing linear or additive surrogate models used for feature attribution,undermining the grounding of these conventional explanation methodologies. To addressthis discrepancy, we introduce the Softmax-Linked Additive Log Odds Model (SLALOM),a novel surrogate model specifically designed to align with the transformer framework.SLALOM demonstrates the capacity to deliver a range of insightful explanations with bothsynthetic and real-world datasets. We highlight SLALOMs unique efficiency-quality curveby showing that SLALOM can produce explanations with substantially higher fidelity thancompeting surrogate models or provide explanations of comparable quality at a fraction oftheir computational costs. We release code for SLALOM as an open-source project online at",
  "Introduction": "The transformer architecture (Vaswani et al., 2017) has been established as the status quo in modern naturallanguage processing (Devlin et al., 2018; Radford et al., 2018; 2019; Touvron et al., 2023). However, thecurrent and foreseeable adoption of large language models (LLMs) in critical domains such as the judicialsystem (Chalkidis et al., 2019) and the medical domain (Jeblick et al., 2023) comes with an increased needfor transparency and interpretability. Methods to enhance the interpretability of an artificial intelligence(AI) system are developed in the research area of Explainable AI (XAI, Adadi & Berrada, 2018; Gilpinet al., 2018; Molnar, 2019; Burkart & Huber, 2021). A recent meta-study (Rong et al., 2023) shows thatXAI has the potential to increase users understanding of AI systems and their trust therein. Local featureattribution methods that quantify the contribution of each input to a decision outcome are among the mostpopular explanation methods, and a variety of approaches have been suggested for the task of computingsuch attributions (Kasneci & Gottron, 2016; Ribeiro et al., 2016; Sundararajan et al., 2017; Lundberg & Lee,2017; Covert et al., 2021; Modarressi et al., 2022). It remains hard to formally define the contribution of an input feature for non-linear functions. Recent work(Han et al., 2022) has shown that common explanation methods do so by implicitly or explicitly performing alocal approximation of the complex black-box function, denoted as f, using a simpler surrogate function gfrom a predefined class G. For instance, Local Interpretable Model-agnostic Explanations (LIME, Ribeiro",
  "Local SLALOMapproximation": ": Transformers cannot be well explained through additive models. Left: We exemplarilyshow the log odds for the outputs of a BERT model and a linear Nave-Bayes model (linear) assigning eachword a weight trained on the IMDB movie review dataset. The token colors indicate the weights assigned bythe linear model. We pass two sequences to the models independently and in concatenation. For the linearmodel, the output of the concatenated sequence can be described by the sum, but this is not the case forBERT. We show that this phenomenon is not due to a non-linearity in this particular model but stems froma general incapacity of transformers to represent additive functions. Right: To overcome this difficulty, wepropose SLALOM, a novel surrogate model specifically designed to better approximate transformer models.",
  "et al., 2016) or input gradient explanations (Baehrens et al., 2010) use a linear surrogate model to approximatethe black-box f; the models coefficients can be used as the feature contributions": "Surrogate model explanations have the advantage that they directly describe the behavior of the model inthe proximity of a specific input, i.e., under small perturbations, a property known as fidelity (Guidottiet al., 2018; Nauta et al., 2023). Fidelity can be quantified through the difference between the predictionmodels outputs and the surrogate models outputs (Yeh et al., 2019; Zhou et al., 2019). Explanations thatquantitatively describe the prediction models output under perturbations with low error have high fidelity. An implication of models with high representative capacity and high-fidelity explanations is the recoveryproperty: If the true relation f between features and labels in the data is already within the function class G,the model will learn this function and we can effectively reconstruct the original f from the explanations.For example, suppose that the black-box function we consider is of linear form, i.e., f(x) = wx, and hasbeen correctly learned. In this case, a gradient explanation as well as continuous LIME (C-LIME, Agarwalet al., 2021) will recover the original models parameters up to an offset (Han et al., 2022, Theorem 1).Shapley value explanations (Lundberg & Lee, 2017) possess a comparable relationship: It is known thatthey correspond to the pointwise feature contributions of Generalized Additive Models (GAM, Bordt & vonLuxburg, 2023). The significance of recovery properties lies in their role when explanations are leveragedto gain insights into the underlying data. Particularly when XAI is used for scientific applications such asdrug discovery (Mak et al., 2023), preserving the path from the input data to the explanation through alearned model is crucial. However, such guarantees can only be provided when surrogate function class G caneffectively mimic the models learned relation, at least within some local region. In this study, we demonstrate that the transformer architecture, the main building block of LLMs such asthe GPT models (Radford et al., 2019), is inherently incapable of learning additive models on the inputtokens, both theoretically and empirically. By additive models, we refer to models that assign each tokena weight. The sum of the individual token weights then gives the output of the additive model. Linearmodels are a subset of this class. We formally prove that simple encoder-only and decoder-only transformersstructurally cannot represent such additive models due to the attention mechanisms softmax normalization,which necessarily introduces token dependencies over the entire sequence length. An example is illustrated in (left). Our finding that the function spaces represented by additive models and transformers are",
  "Published in Transactions on Machine Learning Research (12/2024)": "value score 2.0 2.5 3.0 3.5 4.0 4.5 5.0 5.5 importance score by not no don didn eyes however left music best never good love though every without great should top wasn together saw find lost nothing big co instead less maybe leave killed rather yes despite probably average ##da wouldn changed bad hot doesn higher loss famous caused wrong rememberjustice watched problems looks poor attempt removed leaves easy officers lack perfect shut losing strength supposed slow easily unknown worth anyway revolution trouble unlike classic avoid ends surprised clean fun apparentlycool liked attempted worse jane anymore aren wondered somehow weekend worst weak favorite glad ill talent interesting guilty waste rounds upset barbara excellent terrible withdrew unfortunately bat loves error relevant dragged excuse hopes expecting challenged suspect connects kills batmanbunch frustration arguing horrible avoided awful imagination gorgeous cute crap reliable stiff rapper singsscar bored useless ##wo romeo disbelief realistic wasted crosby wiping masculine offended suppress pathetic irritation invading impending recipe##hawk hearings sucks manly ##phobic pointless vomit hilarious avoids ##maid inspirational distrust",
  "(2) To mitigate these issues, we propose the Softmax-Linked Additive Log Odds Model (SLALOM),which uses a combination of two scores to quantify the role of input tokens": "(3) We theoretically analyze SLALOM and show that (i) it can be represented by transformers (i.e., thefidelity property), (ii) it can be uniquely identified from data (i.e., the recovery property), and (iii) itis highly efficient to estimate. (4) Experiments on synthetic and real-world datasets with common language models (LMs) confirm themismatch between surrogate models and predictive models, underline that two scores cover differentangles of interpretability, and that SLALOM explanations can be computed that have substantiallyhigher fidelity or efficiency than competing techniques.",
  "Related Work": "Explainability for transformers. Various methods exist to tackle model explainability (Molnar, 2019;Burkart & Huber, 2021). Furthermore, specific approaches have been devised for the transformer architecture(Vaswani et al., 2017): As the attention mechanism at the heart of transformer models is supposed to focus onrelevant tokens, it seems a good target for explainability methods. Several works turn to attention patternsas model explanation techniques. A central attention-based method is put forward by Abnar & Zuidema(2020), who propose two methods of aggregating raw attentions across layers, flow and rollout. Brunner et al.(2020) focus on effective attentions, which aim to identify the portion of attention weights actually influencingthe models decision. While these approaches follow a scalar approach considering only attention weights,Kobayashi et al. (2020; 2023) propose a norm-based vector-valued analysis, arguing that relying solely onattention weights is insufficient and the other components of the model need to be considered. Building onthe norm-based approach, Modarressi et al. (2022; 2023) further follow down the path of decomposing thetransformer architecture, presenting global-level explanations with the help of rollout. Beyond that, manymore attention-based explanation approaches have been put forward (Chen et al., 2020; Hao et al., 2021;Ferrando & Costa-juss, 2021; Qiang et al., 2022; Sun et al., 2023; Yang et al., 2023) and relevance-propagationmethods such as LRP have been adapted to the transformer architecture (Achtibat et al., 2024). A drawbackwith these model-specific explanations remains the implementational overhead that is required to adapt thesemethods for each architecture. On the formal side, there is no explicit method to quantitatively predict thetransformer models behavior under perturbations leaving the fidelity of these explanations unclear. Model-agnostic XAI. In contrast to transformer-specific methods, researchers have devised model-agnosticexplanations that can be applied without precise knowledge of a models architecture. Model-agnosticlocal explanations like LIME (Ribeiro et al., 2016), SHAP (Lundberg & Lee, 2017) and others (Shrikumaret al., 2017; Sundararajan et al., 2017; Smilkov et al., 2017; Xu et al., 2020; Covert et al., 2021, etc.) are aparticularly popular class of explanations that are applied to LMs as well (Szczepaski et al., 2021; Schirmeret al., 2023; Dolk et al., 2022). Surrogate models are a common subform (Han et al., 2022), which locallyapproximate a black-box model through a simple, interpretable function. Linking models and explanations. Prior work has distilled the link between classes of surrogate modelsthat can be recovered by explanations (Agarwal et al., 2021; Han et al., 2022, Theorem 3). Notable worksinclude Garreau & von Luxburg (2020), which provides analytical results on different parametrizations ofLIME, and Bordt & von Luxburg (2023), which formalizes the connection between Shapley values and GAMsfor classical Shapley values as well as n-Shapley values that can also model higher-order interactions.",
  "L": ": Transformer architecture.Ineach layer l=1, . . . , L, input embeddings h(l1)ifor each token i are transformed into outputembeddings h(l)i .When detaching the partprior to the classification head (cls), we seethat the output only depends on the last em-bedding h(L1)1and attention output s1. In this work, we focus on classification problems of tokensequences. For the sake of simplicity, we initially considera 2-class classification problem with labels y Y = {0, 1}.We will outline how to generalize our approach to multi-classproblems in Appendix C.1.",
  "The input consists of a sequence of tokens t=t1, . . . , t|t|": "where |t| 1, . . . , C is the sequence length that can spanat most C tokens (the context length).All tokens ti inthe sequence t stem from a finite size vocabulary V, i.e.,ti V, i = 1, . . . , |t|. To transform the tokens into a repre-sentation amenable to processing with computational meth-ods, the tokens need to be encoded as numerical vectors.To this end, an embedding function e : V Rd is used,where d is the embedding dimension. Let ei = e(ti) be theembedding of the i-th token. The output is given by a logitvector l R|Y|, such that softmax(l) contains individualclass probabilities.",
  "The common transformer architecture": "Many popular LMs follow the transformer architecture introduced by Vaswani et al. (2017) with only minormodifications. We will introduce the most relevant building blocks of the architecture in this section. Acomplete formalization is given in Appendix B.1. A schematic overview of the architecture is visualizedin . Let us denote the input embedding of token i = 1, . . . , |t| in layer l 1, . . . , L by h(l1)iRd,where h(0)i =ei. The core component of the attention architecture is the attention head.1 For each token, aquery, key, and a value vector are computed by applying an affine-linear transform to the input embeddings.Keys and queries are projected onto each other and normalized by a row-wise softmax operation resulting inattention weights ij , denoting how much token i is influenced by token j. The attention output fortoken i can be computed as si = |t|j=1 aijvj, where vj Rdh denotes the value vector for token j. The finalsi are projected back to dimension d by a projection operator P : Rdh Rd before they are added to thecorresponding input embedding h(l1)ias mandated by skip-connections. The sum is then transformed by anonlinear function that we denote by ffn : Rd Rd, finally resulting in a transformed embedding h(l)i . Thisprocedure is repeated iteratively for layers 1, . . . , L such that we finally arrive at output embeddings h(L)i.To perform classification, a classification head cls : Rd R|Y| is put on top of a token at some index r (how",
  "Encoder-only and decoder-only models": "Practical architectures can be seen as parametrizations of the process described previously. We introduce theones relevant to this work in this section. Commonly, a distinction is made between encoder-only models,that include BERT (Devlin et al., 2018) and its variants, and decoder-only models such as the GPT models(Radford et al., 2018; 2019). Encoder-only models. Considering BERT as an example of an encoder-only model, the first token is usedfor the classification head, i.e., r = 1. Usually, a special token [CLS] is prepended to the text at position 1.However this is not strictly necessary for the functioning of the model. Decoder-only models. In contrast, decoder-only models like GPT-2 (Radford et al., 2019) add theclassification head on top of the last token for classification, i.e., r = |t|. A key difference is that in GPT-2and other decoder-only models, a causal mask is laid over the attention matrix, resulting in i,j = 0 for j > i.This encodes the constraint that tokens can only attend to themselves or to previous ones. To make our model amenable to theoretical analysis, the transformer model in our analysis contains oneslight deviation from practical models. We do not consider positional embeddings, which are added to theembeddings based on their position in the sentence. We will empirically demonstrate that using positionalembeddings does not affect the validity of our findings on practical models.",
  "Analysis": "Let us initially consider a transformer with only a single layer and head. Our first insight is that theclassification output can be determined only by two values: the input embedding at the classification token r,h(0)r , and the attention output sr. This can be seen when plugging in the different steps:",
  "for all possible input sequences": "The transformer shows surprising behavior when considering sequences of identical tokens but of differentlengths, i.e., [], , . . . We first note that the sum of the attention scores is bound to be |t|j=1 arj = 1.The output of the attention head will thus be a weighted average of the value vectors vi. We find thatthe first-layer value vectors vj(tj) are determined purely by the input tokens tj (cf. full formalization inAppendix B.1). For a sequence of identical tokens, we will thus have the same value vectors, resulting inidentical vectors being averaged. This makes the transformer produce the same output for each of thesesequences. This contradicts the form in (2), where the output should successively increase by w(ti). We arenow ready to state our result, which formalizes this intuition for the more general class of additive modelswhere the token weight may also depend on its position i in the input (equivalent to a GAM).",
  "Exploiting the similar form allows us to generalize the main results to more layers recursively": "Corollary 4.3 (Multi-Layer transformers cannot learn additive models either). Under the same conditionsas in Proposition 4.1, a stack of multiple transformer blocks as the model F, neither has a parametrizationsufficient to represent the additive model. Practical considerations. As stated earlier, the transformer model in our analysis does not considerpositional embeddings that are added on the token embeddings.However, this does not have majorramifications in practice: While the transformer would be able to differentiate between sequences of differentlengths with positional embeddings in theory, the softmax operation must be inverted for any input sequenceby the linear feed-forward block that follows the attention mechanism. This is a highly non-linear operationand the number of possible sequences grows exponentially with the context length and vocabulary size.Learning-theoretic considerations suggest that this inversion is impossible for reasonably-sized networksas outlined in Appendix C.2. We will confirm our results with empirical findings obtained exclusively onnon-modified models with positional embeddings.",
  "A Surrogate Model for Transformers": "In the previous section, we theoretically established that transformer models struggle to represent additivefunctions. While this must not necessarily be considered a weakness, it certainly casts doubts on the suitabilityof additive models as surrogate models for explanations of transformers. For a principled approach, weconsider the following four requirements to be of importance:",
  "(d) GPT-2": ": Transformers fail to learn linear models. We train different models on a synthetically sampleddataset where the log odds obey a linear relation to the features. Fully connected models (2-layer ReLUnetworks with different hidden layer widths) capture the linear form of the relationship well despite someestimation error (a). However, common transformer models fail to model this relationship and output almostconstant values (b)-(d). This does not change with more layers. (2) Learnability. The surrogate model should be easily representable by common transformers. Learn-ability is crucial because using a surrogate model that is hard to represent for the predictive modelwill likely result in low-fidelity explanations. (3) Recovery. If the predictive model falls into the surrogate models class, the fitted surrogate modelsparameters should match those of the predictive model. Together with learnability, recovery ensuresthat a model can pick up the true relations present in the data and they can be re-identified by theexplanation, which is essential, e.g., in scientific discovery with XAI.",
  "The Softmax-Linked Additive Log Odds Model": "To meet the requirements, we propose a novel discriminative surrogate model that explicitly models thebehavior of the softmax function. Instead of only assigning a single weight w to each token, we separate twocharacteristics: We introduce the token importance as a map s : V R and a token value in form of a mapv : V R. Subsequently, we consider the following discriminative model:",
  "tjt exp(s(tj)).(4)": "Due to the shift-invariance of the softmax function, we observe that the maps s and s given by s() = s()+result in the same softmax-score and thus the same log odds model for any input t.Therefore, theparameterization would not be unique. To this end, we introduce a normalization constraint on the sumof token importances for uniqueness. Formally, we constrain it to a user-defined constant R such thatV s() = , where natural choices include {0, 1}. We refer to the discriminative model given inEqn. (4) together with the normalization constraint as the softmax-linked additive log odds model (SLALOM). As common in surrogate model explanations, we can fit SLALOM to a predictive models outputs globally orlocally and use tuples of token importance scores and token values scores, (v(), s()) to give explanationsfor an input token . While the value score provides an absolute contribution of to the output, its tokenimportance s() determines its weight with respect to the other tokens. For instance, if only one token is present in a sequence, the output is only determined by its value score v(). However, in a sequence ofmultiple tokens, the importance of each token with respect to the others and thereby the contribution ofthis tokens value is determined by the token importance scores s. This intuitive relation makes SLALOMinterpretable, thereby satisfying Property (1).",
  "Theoretical properties of SLALOM": "We analyze the proposed SLALOM theoretically to ensure that it fulfills Properties (2) and (3), Learnabilityand Recovery, and subsequently provide efficient algorithms to estimate its parameters (4). First, we showthat unlike linear models, SLALOMs can be easily learned by transformers. Proposition 5.1 (Transformers can fit SLALOM). For any maps s, v, and a transformer with an embeddingsize d and head dimension dh with d, dh 3, there exists a parameterization of the transformer to reflectSLALOM in Equation (4) together with the normalization constraint. This statement can be proven by explicitly constructing the corresponding weight matrix (cf. Appendix B.5).This proposition highlights that unlike linear models there are simple ways for the transformer to representrelations governed by SLALOMs. We demonstrate this empirically in our experimental section and concludethat SLALOM fulfills Property (2). For Property (3), Recovery, we make the following proposition: Proposition 5.2 (Recovery of SLALOMs). Suppose query access to a model G that takes sequences of tokenst with lengths |t| 1, . . . , C and returns the log odds according to a non-constant SLALOM on a vocabularyV, normalization constant R, but with unknown parameter maps s : V R, v : V R. For C 2, wecan recover the true maps s, v with 2|V| 1 forward passes of F.",
  "This statement confirms property (3) and shows that SLALOM can be uniquely re-identified when we ruleout the corner case of constant models. We prove it in Appendix B.6": "Complexity considerations. Computational complexity can be a concern for XAI methods. To estimateexact Shapley values, the models output on exponentially many feature coalitions needs to be evaluated.However, as the proof of Proposition 5.2 shows, to estimate SLALOMs parameters for an input sequenceof V tokens, only 2|V| 1 forward passes are required, verifying Property (4). We empirically show thatcomputing SLALOM explanations is about 5 faster than computing SHAP explanations when using thesame number of samples in our experimental section.",
  "Numerical algorithms for computing SLALOMs": "Having derived SLALOM as a better surrogate model, we require numerical algorithms to estimate v and s.Unfortunately, the strategy derived in Proposition 5.2 using a minimal number of samples is numericallyunstable. We make two key implementation choices for SLALOM to be used as an explanation technique.First, we can control the sample set of features and labels obtained through queries of the predictive model.Second, we can use different optimization strategies to fit SLALOM on this sample set. We suggest twoalgorithms to fit SLALOMs post-hoc on input-output pairs of a trained predictive model: SLALOM-eff. The first version of the algorithm to fit SLALOM models is designed for maximum efficiencywhile maintaining reasonable performance across several XAI metrics. Obtaining a large dataset of input-output pairs can incur substantial computational costs as a forward pass of the models needs to be executedfor each sample. To speed up this process, SLALOM-Eff uses very short sequences (we use only two tokensin this work) randomly sampled from the vocabulary for this purpose. To efficiently fit the surrogate model,we perform stochastic gradient descent on SLALOMs parameters using the mean-squared-error loss betweenthe score output by SLALOM and the score by the predictive model as an objective. SLALOM-eff is ourdefault technique used unless stated otherwise. SLALOM-fidel. We provide another technique to fit SLALOM optimized for maximum fidelity underinput perturbations such as token removals. To explain a specific sample, we sample input where we removeup to K randomly sampled tokens. The sequences with tokens removed and their predictive model scores areused to fit the model, similar to LIME (Ribeiro et al., 2016). Instead of SGD, we can leverage optimizersfor Least-Square-Problems to fit the parameters iteratively, however incurring a higher latency. We providedetails and pseudocode for both fitting routines in Appendix D.",
  "(d) Recovering parametersof SLALOM dataset": ": Verifying properties with synthetic data: SLALOM describes outputs of transformermodels well (a, b). We fit SLALOM to the outputs of the BERT and GPT-2 models trained on the linearsynthetic dataset. The linear and GAM models (despite having C/2=15 more parameters) do not matchthe transformers behavior. We provide another empirical counterexample and additional quantitative resultsin Appendix F.1. Verifying recovery (c, d). We verify the recovery property on a second synthetic datasetwhere features and labels obey a SLALOM relation. We train a 2-layer DistilBERT model on the data andfit SLALOM to the trained model. We can recover the original logit scores (c) and see a strong connectionbetween original SLALOM parameters and the recovered ones (d). These findings verify the learnability andrecovery properties. More results in Appendix F.2.",
  "tit i exp(s(ti)),(5)": "where i = 1 if a token is present and i = 0 if it is absent. We observe that setting i = 0 has the desiredeffect of making the output of the soft-removal model equivalent to that of the standard SLALOM on asequence without this token. Taking the gradients at = 1 we obtainFi",
  "Experimental Evaluation": "We run a series of experiments to show the mismatch between surrogate model explanations and thetransformers. Specifically, we verify that (1) real transformers fail to learn additive models, (2) SLALOMbetter captures transformer output, (3) SLALOM models can be recovered from fitted models with tolerableerror, (4) SLALOM scores are versatile and align well with linear attribution scores and human attention, and(5) that SLALOM performs well in faithfulness metrics and has substantially higher fidelity than competingtechniques. For experiments (1)-(3), we require knowledge of the ground truth and use synthetic datasets.To demonstrate the practical strengths of our method, all the experiments for (4) and (5) are conducted onreal-world datasets and in comparison with state-of-the-art XAI techniques.",
  "Experimental setup": "LM architectures.We study three representative transformer language model architectures in ourexperiments. In sequence classification, mid-sized transformer LMs are most popular on platforms such asthe Huggingface hub (Huggingface, 2023) often based on the BERT-architecture (Devlin et al., 2018), whichis reflected in our experimental setup. To represent the family of encoder-only models, we deploy BERT",
  "Evaluation with known ground truth": "Transformers fail to capture linear relationships. We empirically verify the claims made in Proposi-tion 4.1 and Corollary 4.3. To ascertain that the underlying relation captured by the models is additive, weresort to a synthetic dataset with a linear relation. The dataset is created as follows: First, we sample differentsequence lengths from a binomial distribution with a mean of 15. Second, we sample words independentlyfrom a vocabulary of size 10. This vocabulary was chosen to include positive words, negative words, andneutral words, with manually assigned weights w {1.5, 1, 0, 1, 1.5}, that can be used to compute a linearlog odds model. We evaluate this model and finally sample the sequence label accordingly, thereby ensuringa linear relation between input sequences and log odds. We train transformer models on this dataset andevaluate them on sequences containing the same word (perfect) multiple times. Our results in show that the models fail to capture the relationship regardless of the model or number of layers used. InAppendix A, we show how this undermines the recovery property with Shapley value explanations. Fitting SLALOM as a surrogate to transformer models. Having demonstrated the mismatch betweenadditive functions and transformers, we turn to SLALOM as a more suitable surrogate model. As shownin Proposition 5.1, transformers can easily fit SLALOMs, which is why we hypothesize that they shouldmodel the output of such a model well in practice. We fit the different surrogate models on a dataset of inputsequences and real transformer outputs from our linear synthetic dataset and observe that linear models andadditive models fail to capture the relationship learned by the transformer as shown in (a, b). On thecontrary, SALO manages to model the relationship well, even if it has considerably less trainable parametersthan the additive model (GAM). Verifying recovery. We run an experiment to study whether, unlike linear models, SLALOM can be fittedand recovered by transformers. To test this, we sample a second synthetic dataset that exactly follows therelation given by SLALOM. We then train transformer models on this dataset. The results in (c, d)show that the surrogate model fitted on transformer outputs as a post-hoc explanation recovers the correctlog odds mandated by SLALOM (c) and that there is a good correspondence between the true modelsparameters and the recovered models parameters (d).",
  "Examining real-world predictions from different angles": "We increase the difficulty and deploy SLALOM (fitted using SLALOM-eff) to explain predictions on real-world datasets. As there is no ground truth for these datasets, it is challenging to evaluate the quality of theexplanations (Rong et al., 2022). To better understand SLALOM explanations, we study them from severalangles: We compare to linear scores obtained when fitting a Nave-Bayes Bag-of-Words (BoW) model, scoreson removal and insertion benchmarks (Tomsett et al., 2020; DeYoung et al., 2020), the human attentionscores available on the Yelp-HAT dataset (Sen et al., 2020), and provide qualitative results. Explaining Sentiment Classification. We show qualitative results for explaining a movie review in. The figure shows that both negative and positive words are assigned high importance scoresbut have value scores of different signs. Furthermore, we see that some words (the) have positive valuescores, but a very low importance. This means that they lead to positive scores on their own but are easily",
  ": Evaluation of SLALOM scores (values, importance, lin.) with std. errors across explanationquality measures highlights that SLALOMs different scores serve different purposes. IMDB dataset shown": "overruled by other words. We compare the SLALOM scores obtained on 100 random test samples to a linearNave-Bayes model (obtained though counting class-wise word frequencies) as a surrogate ground truth ina through the Spearman rank correlation. We observe good agreement with the value scores v andthe combined linearized SLALOM scores (lin, see .4). Predicting Human Attention. To study alignment with a user perspective, we predict human attentionfrom SLALOM scores. We compute AU-ROC for predicting annotated human attention as suggested in Senet al. (2020) in b. We use absolute values of all signed explanations as human attention is unsignedas well. In contrast to the previous experiments, where value scores were more effective than importances,we observe that the importance scores are often best at predicting where the human attention is placed. Insummary, these findings highlight that the two scores serve different purposes and cover different dimensionsof interpretability. SLALOM offers higher flexibility through its 2-dimensional representation. We also use this opportunity to study the applicability of SLALOM to larger and non-transformer models.To this end, we train Mamba and BLOOM models (Le Scao et al., 2023) with a classification head on the",
  "(c) Token highlighting (linearized SLALOM, GPT-2)": ": Explaining a real review with SLALOM (qualitative results). SLALOM assigns two scores to eachtoken (a,b) and can be used to compute attributions via its linearization (c). We observe that the impactfulwords have high importances and the value scores indicate the sign of their contribution (positive or negativewords). See (Appendix) for fully annotated plots.",
  "(c) GPT-2": ": Assessing Fidelity. We plot the MSE for predicting model outputs under token removal and findthat SLALOMs predictions have up to 70% less error than the closest competitor when up to 10 randomtokens from a sentence are removed (log-y plots). We interpret LRP scores as a linear model. Yelp dataset. Our results in the lower part of b prove that SLALOM can be well applied to largermodels with sizes of up to 7B. For BLOOM the results are promising and reach the same level as for theclassical transformer models. SLALOM can also be applied to non-transformer models like Mamba (Gu &Dao, 2023) due to its model-agnostic nature. Due to its general expessivity, the explanations are capableof predicting human attention (at least the value scores), but we observe a slight reduction in quality. Wethus see SLALOMs main scope with models that follow the classical transformer paradigm. We show thatSLALOMs results for BoW correlations and human attention prediction are in the same range and oftenoutperform competing XAI techniques in Appendix F.3.2, but defer a comparative analysis to the nextsections. Assessing Fidelity. Having established the roles of its components, we verify that SLALOM can produceexplanations that have substantially higher fidelity than competing surrogate or non-surrogate explanationtechniques. To assess this, we remove up to 10 tokens from the input sequences and use the explanations topredict the change in model output using the surrogate model (SLALOM or linear). We compare the twoSLALOM versions to baselines such as LIME (Ribeiro et al., 2016), Kernel-SHAP (Lundberg & Lee, 2017),Gradients (Simonyan et al., 2013), and Integrated Gradients (IG, Sundararajan et al., 2017) and layer-wiserelevance propagation (LRP) for transformers (Achtibat et al., 2024), a non-surrogate technique. We reportthe Mean-Squared-Error (MSE) between the predicted change and the observed change when running themodel on the modified inputs in . We observe that SLALOM-fidel offers substantially higher fidelitywith a reduction of 70% in MSE for predicting the output changes over the second-best method (LRP). Othersurrogate approaches and LRP remain cluttered together, potentially highlighting the frontier of maximumfidelity possible with a linear surrogate model.",
  ": Runtime comparison us-ing 5000 samples to estimate sur-rogate models": "Evaluating XAI Metrics. There are several other metrics to quantifythe quality of explanations and to compare different explanation tech-niques. As a sanity check and to show that SLALOM explanations do notlag behind other techniques in established metrics, we run the classicalinsertion/removal benchmarks (Tomsett et al., 2020). For the insertionbenchmark, we successively add the tokens with the highest attributionsto the sample, which should result in a high score for the target class.We iteratively insert more tokens and compute the Area Over the Per-turbation Curve (AOPC, see DeYoung et al. (2020)), which should below for insertion. This metric quantifies the alignment of explanationsand model behavior but only considers the feature ranking and not theassigned score. For surrogate techniques (LIME, SHAP, SLALOM) weuse 5000 samples each. Our results in Tab. 1c highlight that linearized SLALOM scores outperform LIMEand LRP and perform on par with SHAP. Removal results for IMDB and results on YELP with similar",
  "findings are deferred to (Appendix). In conclusion, this shows that on top of SLALOMs desirableproperties, it is on par with other techiques in common evaluation metrics": "Computational Costs. Finally, we take a look at the computational costs of the methods, which aremainly determined by sampling the dataset to fit the surrogate model. We provide the runtimes to explain asample on our hardware when using 5000 samples to estimate surrogates in with more results inAppendix F.6. We observe that SHAP incurs the highest computational burden. Among surrogate modelexplanations SLALOM-eff is the most efficient, being about 5 more efficient that SHAP and 2 moreefficient than LIME. Nevertheless, non-surrogate techniques are far more efficient as they require only one orfew (IG steps) forward or backward passes, but suffer from other disadvantages (e.g., implementation effort,no explicit way to predict model behavior). Overall, our results highlight that the two SLALOM fittingroutines can produce explanations of comparable utility to other surrogate models at a fraction of the costs,or produce explanations with higher fidelity at similar costs due to structurally better alignment betweensurrogate and predictive models.",
  "Discussion and Conclusion": "In this work, we established that transformer networks are inherently incapable of representing linear oradditive models commonly used for feature attribution. We prove that the function spaces learnable bytransformers and linear models are disjoint when ruling out trivial cases. This may explain similar incapacitiesobserved in time-series forecasting (Zeng et al., 2023), where they seem incapable of representing certainrelations. To address this shortcoming, we have introduced the Softmax-Linked Additive Log Odds Model(SLALOM), a surrogate model for explaining the influence of features on transformers and other complexLMs through a two-dimensional representation. Our work still has certain limitations that could be addressed in future work. SLALOM is specifically designedto explain the behavior of transformer models and therefore aligned with the classes of functions commonlyrepresented by transformers. However, it would not be a suitable choice to explain models capturing a linearrelationship. While SLALOM is generally applicable to any token-based LMs, we recommend using SLALOMonly when the model is known to have attention-like non-linearities. Our results indicate that performance forthese models is highest. We also note that SLALOM operates at the token level by assigning each individualtoken importance and value scores. Contextual or higher-order interpretability considering the meaningand impact of phrases, clauses, or sentences is not covered by SLALOM. To complement this theoreticalfoundation, future work will include further evaluation of SLALOM from a user-centric perspective, forinstance, on human-centered evaluation frameworks (Colin et al., 2022). From a broader perspective, wehope that this research paves the way for advancing the interpretability and theoretical understanding ofwidely adopted transformer models.",
  "Broader Impact Statement": "This paper presents theoretical work on better understanding feature attributions in the transformer framework.We advise using caution when using our XAI technique or other model explanation as all explanations presentonly a simplified view of the complex ML model. Our method works best with models of the transformerarchitecture. Besides that, we do not see any immediate impact which we feel must be specifically highlightedhere.",
  "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectionaltransformers for language understanding. arXiv preprint arXiv:1810.04805, 2018": "Jay DeYoung, Sarthak Jain, Nazneen Fatema Rajani, Eric Lehman, Caiming Xiong, Richard Socher, andByron C Wallace. Eraser: A benchmark to evaluate rationalized nlp models. In Proceedings of the 58thAnnual Meeting of the Association for Computational Linguistics, pp. 44434458, 2020. Alexander Dolk, Hjalmar Davidsen, Hercules Dalianis, and Thomas Vakili. Evaluation of lime and shap inexplaining automatic icd-10 classifications of swedish gastrointestinal discharge summaries. In ScandinavianConference on Health Informatics, pp. 166173, 2022.",
  "Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprintarXiv:2312.00752, 2023": "Riccardo Guidotti, Anna Monreale, Salvatore Ruggieri, Franco Turini, Fosca Giannotti, and Dino Pedreschi.A survey of methods for explaining black box models. ACM computing surveys (CSUR), 51(5):142, 2018. Tessa Han, Suraj Srinivas, and Himabindu Lakkaraju. Which explanation should i choose? a functionapproximation perspective to characterizing post hoc explanations. Advances in Neural InformationProcessing Systems, 35:52565268, 2022. Yaru Hao, Li Dong, Furu Wei, and Ke Xu. Self-attention attribution: Interpreting information interactionsinside transformer. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp.1296312971, 2021.",
  "Huggingface. Hugging face - models: Most downloaded sequence classification models, 2023. URL": "Katharina Jeblick, Balthasar Schachtner, Jakob Dexl, Andreas Mittermeier, Anna Theresa Stber, JohannaTopalis, Tobias Weber, Philipp Wesp, Bastian Oliver Sabel, Jens Ricke, et al. Chatgpt makes medicine easyto swallow: an exploratory case study on simplified radiology reports. European radiology, pp. 19, 2023. Gjergji Kasneci and Thomas Gottron.Licon: A linear weighting scheme for the contribution ofinputvariables in deep artificial neural networks. In Proceedings of the 25th ACM international on conference oninformation and knowledge management, pp. 4554, 2016. Goro Kobayashi, Tatsuki Kuribayashi, Sho Yokoi, and Kentaro Inui. Attention is not only a weight: Analyzingtransformers with vector norms. In 2020 Conference on Empirical Methods in Natural Language Processing,EMNLP 2020, pp. 70577075. Association for Computational Linguistics (ACL), 2020.",
  "Sasan Maleki, Long Tran-Thanh, Greg Hines, Talal Rahwan, and Alex Rogers. Bounding the estimationerror of sampling-based shapley value approximation. arXiv preprint arXiv:1306.4265, 2013": "Ali Modarressi, Mohsen Fayyaz, Yadollah Yaghoobzadeh, and Mohammad Taher Pilehvar. Globenc: Quanti-fying global token attribution by incorporating the whole encoder layer in transformers. In Proceedingsof the 2022 Conference of the North American Chapter of the Association for Computational Linguistics:Human Language Technologies, pp. 258271, 2022. Ali Modarressi, Mohsen Fayyaz, Ehsan Aghazadeh, Yadollah Yaghoobzadeh, and Mohammad Taher Pilehvar.DecompX: Explaining transformers decisions by propagating token decomposition. In Proceedings ofthe 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp.26492664, 2023.",
  "Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language modelsare unsupervised multitask learners. OpenAI blog, 1(8):9, 2019": "Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. \" why should i trust you?\" explaining the predictionsof any classifier. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discoveryand data mining, pp. 11351144, 2016. Yao Rong, Tobias Leemann, Vadim Borisov, Gjergji Kasneci, and Enkelejda Kasneci. A consistent andefficient evaluation strategy for attribution methods. In International Conference on Machine Learning, pp.1877018795. PMLR, 2022. Yao Rong, Tobias Leemann, Thai-Trang Nguyen, Lisa Fiedler, Peizhu Qian, Vaibhav Unhelkar, Tina Seidel,Gjergji Kasneci, and Enkelejda Kasneci. Towards human-centered explainable ai: A survey of user studiesfor model explanations. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023.",
  "Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert, a distilled version of bert:smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108, 2019": "Miriam Schirmer, Isaac Misael Olgun Nolasco, Edoardo Mosca, Shanshan Xu, and Jrgen Pfeffer. Uncoveringtrauma in genocide tribunals: An nlp approach using the genocide transcript corpus. In Proceedings of theNineteenth International Conference on Artificial Intelligence and Law, pp. 257266, 2023. Cansu Sen, Thomas Hartvigsen, Biao Yin, Xiangnan Kong, and Elke Rundensteiner. Human attention mapsfor text classification: Do humans and neural networks focus on the same words? In Proceedings of the58th annual meeting of the association for computational linguistics, pp. 45964608, 2020. Avanti Shrikumar, Peyton Greenside, and Anshul Kundaje. Learning important features through propagatingactivation differences. In International conference on machine learning, pp. 31453153. PMLR, 2017.",
  "Chih-Kuan Yeh, Cheng-Yu Hsieh, Arun Suggala, David I Inouye, and Pradeep K Ravikumar. On the (in)fidelity and sensitivity of explanations. Advances in neural information processing systems, 32, 2019": "Ailing Zeng, Muxi Chen, Lei Zhang, and Qiang Xu. Are transformers effective for time series forecasting? InProceedings of the AAAI conference on artificial intelligence, volume 37, pp. 1112111128, 2023. Zihan Zhou, Mingxuan Sun, and Jianhua Chen. A model-agnostic approach for explaining the predictions onclustered data. In 2019 IEEE international conference on data mining (ICDM), pp. 15281533. IEEE,2019.",
  "i=1w(ti).(6)": "We create a dataset of 10 words (cf. ) and train transformer models on samples from this dataset. Wesubsequently create sequences that repeatedly contain a single token (in this case, =perfect), pass themthrough the transformers, and use Shapley values (approximated by Kernel-Shap) to explain their output.The result is visualized in , and shows that a fully connected model (two-layer, 400 hidden units,ReLU) recovers the correct scores, whereas transformer models fail to reflect the true relationship. Thisshows that explanation methods that are explicitly or implicitly based on additive models lose their ability torecover the true data-generating process when transformer models are explained.",
  "SHAP values": ": SHAP values do not recover linear functions F for transformers. We compute SHAP valuesfor token sequences that repeatedly contain a single token with a ground truth score of 1.5 (i.e., F([])=1.5,F()=3.0, ...) such that the ground truth attributions should yield 1.5 independent of the sequencelength. While this approximately holds true for a fully connected model, BERT and GPT-2 systematicallyoverestimate the importance for short sequences and underestimate it for longer ones.",
  "B.1Formalization of the transformer": "Many popular LLMs follow the transformer architecture introduced by Vaswani et al. (2017) with only minormodifications. We will introduce the most relevant building blocks of the architecture in this section. Aschematic overview of the architecture is visualized in . Let us denote the input embeddings forLayer l 1, . . . L by H(l1) = [h(l1)1, . . . , h(l1)|t|] R|t|d, where a single line hi contains the embedding for token i. The input embeddings of the first layer consist of the token embeddings, i.e., H(0) = E, whereE = [e1, . . . , e|t|] R|t|d is a matrix of the individual token embeddings. At the core of the architecturelies the attention head. For each token, a query, key, and a value vector are computed by applying an",
  ",(9)": "where 1|t| R|t| denotes a vector of ones of length |t|, b(l)Q , b(l)V , b(l)K Rdh, W (l)Q , W (l)K , W (l)V Rddh, aretrainable parameters and dh denotes the dimension of the attention head.2 Keys and queries are projectedonto each other and normalized by a row-wise softmax operation,",
  "Note that an attention output can be computed as si = |t|j=1 aijvj, where vj denotes the value vector in the": "line corresponding to token j in V = [v1, . . . , v|t|] and aij = A(l)i,j. The final si are projected back to theoriginal dimension d by some projection operator P : Rdh Rd before they are added to the correspondinginput embedding h(l1)idue to the skip-connections. The sum is then transformed by a nonlinear functionthat we denote by ffn : Rd Rd. In summary, we obtain the output for the layer, h(l)i , with",
  "h(l)i= ffnl(h(l1)i+ P (si)).(12)": "This procedure is repeated iteratively for layers 1, . . . , L such that we finally arrive at output embeddingsH(L). To perform classification, a classification head cls : Rd R|Y| is put on top of a token at classificationindex r (how this token is chosen depends on the architecture, common choices include r {1, |t|}), suchthat we get the final logit output l = clsh(L)r. The logit output is transformed to a probability vectorvia another softmax operation. Note that in the two-class case, we obtain the log odds F(t) by taking thedifference () between logits",
  "B.2Proof of Proposition 4.1": "Proposition B.1 (Proposition 4.1 in the main paper). Let V be a vocabulary and C 2, C N be amaximum sequence length (context window). Let wi : V R, i 1, ..., C be any map that assigns a tokenencountered at position i a numerical score including at least one token V with non-zero weight wi() = 0for some i 2, . . . , C. Let b R be an arbitrary offset. Then, there exists no parametrization of the encoderor decoder single-layer transformer F such that for every sequence t = [t1, t2, . . . , t|t|] with length 1 |t| C,the output of the transformer network is equivalent to",
  ",(15)": "where r is the token index on which the classification head is placed. Note that with r = |t| for the decoderarchitecture, the sum always goes up to |t| (for the encoder architecture this is always true). As all tokens inthe sequence have a value of , we obtain h(0)r= e(tr) = e(). The first input to the final part will thus beequal for all sequences tk. We will now show that the second part will also be equal. We compute the value, key, and query vectors for . v, k, q Rdh correspond to one line in the respectivekey, query and value matrices. As the inputs are identical and we omit positional embeddings in this proof,all lines are identical in the matrices. This results in",
  "1k v = v,(21)": "as rj and v are independent of the token index j. We observe that the total input to final part g isindependent of k in its entirety, as the first input e() is independent of k and the second input is independentof k as well. As g is a deterministic function, also the log odds output will be the same for all input sequencestk and be independent of k. By the condition we have a non-zero weight wj() = 0 for some j 2. Inthis case, there are two sequences tj1 (length j1) and tj (length j) consisting of only token , where theoutputs of the additive model (GAM) follow",
  "= g(h(0)r , sh=1r, . . . , sh=Hr)(27)": "As before, we can make the same argument, if we show that all inputs to g are the same. This is straight-forward, as we can extend the argument made for one head for every head, because none of the head candifferentiate between the sequence lengths. The first input will still correspond to h(0)r= e(), which resultsin the same contradiction.",
  "B.3Corollary: Transformers cannot represent linear models": "Corollary B.2 (Transformers cannot represent linear models). Let the context window be C > 2 and supposethe same model as in Proposition 4.1. Let w : V R be any weighting function that is independent of thetoken position with w() = 0 where for at least one token V. Then, the single layer transformer cannotrepresent the linear model",
  "Proof. We show the result by induction with the help of a lemma": "Lemma: Suppose a set S of sequences. If (1) for every sequence t S the input matrix H(l) = [h(l)1 , . . . , h(l)|t|]will consist of input embeddings that are identical for each token i, and (2) single input embeddings also havethe same value for every sequence t S, in the output H(l+1) (1) the output embeddings will be identical forall tokens i and (2) they will have equal value for all the sequences t S considered before. For the encoder-only architecture, the proof from Proposition 4.1 holds analogously for each token outputembedding (in the previous proof, we only considered the output embedding at the classification token r).Without restating the full proof the main steps consist of",
  "For the decoder-only architecture, for token i, the attention weights are taken only up to index i resulting ina weight of 1": "i for each previous token and a weight of 0 (masked) for subsequent ones. However, with thesum also being equal to 1 and the value vectors being equivalent, there is no difference in the outcome. Thisproves the lemma. Having shown this lemma, we consider a set S of two sequences S = {tj1, tj} where kj1 contains j1repetitions of token and tj contains j repetitions of token . We chose j 2, such that wj() = 0, whichis possible by the conditions of the theorem. We observe that for H(0), the embeddings are equal for eachtoken and their value is the same for both sequences. We then apply the lemma for layers 1, . . . , L, resultingin the output embeddings of H(L) being equal for each token, and most importantly identical for tj1 and tj. As we perform the classification by F(t) = clsh(L)r, this output will also not change with the sequencelength. This result can be used to construct the same contradiction as in the proof of Proposition 4.1.",
  "F(t) = (cls (ffn (e(t0) + P(sr))))(42)": "The projection operator is linear, which can set to easily forward in input by setting P I. Due to the skipconnection of the feed-forward part, we can easily transfer the second part through the first ffn part. In theclassification part, we output the third component and zero by applying the final weight matrix",
  "B.6Proof of Proposition 5.2": "Proposition B.5 (Proposition 5.2. in the main paper). Suppose query access to a model G that takessequences of tokens t with lengths |t| 1, . . . , C and returns the log odds according to a non-constant SLALOMon a vocabulary V with unknown parameter maps s : V R, v : V R. For C 2, we can recover the truemaps s, v with 2|V| 1 queries (forward passes) of F.",
  "We have obtained the values scores v for each token through |V| forward passes. To identify the tokenimportance scores s, we consider token sequences of length 2": "We first note that if the SLALOM is non-constant and |V| > 1, for every token V, we can find anothertoken for which v() = v(). This can be seen by contradiction: If this would not be the case, i.e., wecannot find a token with a different value v(), all tokens have the same value and the SLALOM wouldhave to be constant. For |V| = 1, SLALOM is always constant and does not fall under the conditions of thetheorem. We now select an arbitrary reference token V. We select another token for which v() = v(). By theprevious argument such a token always exists if the SLALOM is non-constant. We now compute relativeimportances w.r.t. that we refer to as . We let () = s()s() denote the difference of the importancebetween the importance scores of tokens , V. We set () = 0",
  "C.1Generalization to multi-class problems": "We can imagine the following generalizing SLALOM to multi class problems as follows: We keep an importancemap s : V R that still maps each token to an importance score as previously. However, we now introduce avalue score map vc : V R for each class c Y. Additionally to requiring",
  "C.2Practical Considerations": "Our theoretical model contains slight deviations from real-world transformers to make it amendable totheoretical analysis. To represent token order, common architectures use positional embeddings, tying theembedding vectors to the token position i. The behavior that we show in this works analysis does howeveralso govern transformers with positional embeddings for the following reason: While the positional embeddingscould be used by the non-linear ffn part to differentiate sequences of different length in theory, our proofsshow that to represent the linear model, the softmax operation must be inverted for any input sequence.This is a highly nonlinear operation and the number of possible sequences grows exponentially at a rateof |V|C with the context length C. Learning-theoretic considerations (e.g., Bartlett et al., 1998) show thatthe number of input-output pairs the two-layer networks deployed can maximally represent is boundedby O (dnhidden log(dnhidden)), which is small (d=786, nhidden=3072 for BERT) in contrast to the number ofsequences (C = 1024, |V| 3 104). We conclude that the inversion is therefore impossible for realisticsetups and positional embeddings can be neglected, which is confirmed by our empirical findings. Common models such as BERT also use a special token referred to as CLS-token where the classificationhead is placed on. In this work, we consider the CLS token just as a standard token in our analysis. In ourempirical sections, we always append the CLS token as mandated by the architecture to make the sequencesvalid model inputs.",
  "DAlgorithm: Local SLALOM approximation": "We propose two algorithms to compute local explanations for a sequence t = [t1, . . . , t|t|] with SLALOMscores. In particular, we use the Mean-Squared-Error (MSE) to fit SLALOM on modified sequences consistingof the individual tokens in the original sequence t. To speed up the fitting we can sample a large collection ofsamples offline before the optimization.",
  "D.1Efficiently fitting SLALOM with SGD": "For the efficient implementation SLALOM-eff given in Algorithm 1 we sample minibatches from thiscollection in each step and perform SGD steps on them. We perform this optimization using b = 5000 samplesin this work. We use sequences of n = 2 random tokens from the sample for SLALOM-eff, making theforward passes through the model highly efficient.",
  "D.2Fitting SLALOM through iterative optimization": "For the high-fidelity implementation SLALOM-fidel (Algorithm 2) we first use a different set of sequencesand model scores to fit the surrogate: We delete up to 5 tokens from the original sequence randomly tocreate the estimation dataset (similar to LIME). The fitting algorithm optimized for maximum fidelity usesan iterative optimization scheme to fit SLALOM models. It works by iteratively fitting v and s to thedataset obtained. Denote by f Rb the model scores obtained for the b input sequences ti, i = 1...b. Asthe SLALOM model in Equation (4) is a linear combination of the values score weighted by the normalizedimportance score, we can set up a matrix A, where element ai,j =exp(s(ti))",
  "E.1.1Dataset construction: Linear Dataset": "We create a synthetic dataset to ensure a linear relationship between features and log odds. Before samplingthe dataset, we fix a vocabulary of tokens, ground truth scores for each token, and their occurrence probability.This means that each of the possible tokens already comes with a ground-truth score w that has been manuallyassigned. The tokens, their respective scores w, and occurrence probabilities are listed in . Samples ofthe dataset are sampled in four steps that are exectued repeatedly for each sample:",
  "end whilereturn v, s mean(s) # normalize s to zero-mean": "Algorithm 2 Local high-fidelity SLALOM approximation (SLALOM-fidel)Require: Sequence t, trained model F (outputs log odds), max. number of deletions n, learning rate ,batch size r, sample pool size b, number of steps cInitialize s = 0, s = 0 unique ti tB b samples of random sequences of length n obtained through deleting up to n tokens randomly from t.Precompute F(B[i]), i = 1, . . . , b# perform model forward-pass for each sample in pool",
  ": Tokens in the linear dataset with their corresponding weight": "compute a BoW importance scores for input tokens of the BERT model on the IMDB dataset by countingthe class-wise occurrence probabilities. We select 200 tokens randomly from this dataset. We use these scoresas value scores v but multiply them by a factors of 2 as many words have very small BoW importances. Inrealistic datasets, we observed that value scores v are correlated with the importance scores s. Therefore, wesample",
  "For the results in , we query the models with sequences that contain growing numbers of the workperfect, i.e. [perfect, perfect, ...]. We prepend a CLS token for the BERT models": "For the results in (a,b), we then sample 10000 new samples from the linear synthetic dataset (havingthe same distribution as the training samples) and forward them through the trained transformers. Themodel log odds score together with the feature vectors are used to train the different surrogate models, linearmodel, GAM, and SLALOM. For the linear model, we fit an OLS on the log odds returned by the model. Weuse the word counts for each of the 10 tokens as a feature vector. The GAM provides the possibility to assigneach token a different weight according to its position in the sequence. To this end, we use a different featurevector of length 30 10. Each feature corresponds to a token and a position and is set to one if the token i ispresent at this position, and set to 0 otherwise. We then fit a linear model using regularized (LASSO) leastsquares with a small regularization constant of = 0.01 because the system is numerically unstable in itsunregularized form.",
  "E.4Training Details for real-world data experiments": "Training details. In these experiments, we use the IMDB (Maas et al., 2011) and Yelp (Asghar, 2016)datasets to train transformer models on. Specifically, the results in a are obtained by training 2-layerversions of BERT, DistilBERT and GPT-2 with on 5000 samples from the IMDB dataset for 5 epochs,respectively. We did not observe significant variation in terms of number of layers, so we stick to the simplermodels for the final experiments. For the experiments in b we train and use 6-layer versions of theabove models for 5 epochs on 5000 samples of the Yelp dataset. We report the accuracies of these models in and additional hyperparameters in .",
  "IMDB0.880.900.74Yelp0.860.880.88": ": Accuracies of models used in this paper. For IMDB (|trainset| = 5000), we use 2-layer versionsof the models. For Yelp, (|trainset| = 5000), we use 6-layer versions of the models. For both datasets, the|testset| = 100. The models are trained for 5 epochs after which we found the accuracy of the model on thetest set to have converged.",
  "given by w() = (#occ. of in class 1)+": "(#occ. of in class 0)+. We use Laplace smoothing with = 40. The final correlations arecomputed over a set of 50 random samples, where we observe good agreement between the Nave Bayes scores,and the value and linearized SLALOM scores, respectively. Note that the importance scores are consideredunsigned, such that we compute their correlation with the absolute value of the Naive Bayes scores. SLALOM vs. Human Attention. The Yelp Human Attention (HAT) (Sen et al., 2020) dataset consists ofsamples from the original Yelp review dataset, where for each review real human annotators have been askedto select the words they deem important for the underlying class (i.e. positive or negative). This results inbinary attention vectors, where each word either is or is not important according to the annotators. Sinceeach sample is processed by multiple annotators, we use the consensus attention map as defined in Sen et al.(2020), requiring agreement between annotators for a token to be considered important to aggregate theminto one attention vector per sample. Since HAT, unlike SLALOM, operates on a word level, we map eachwords human attention to each of its tokens (according to the employed models specific tokenizer).To compare SLALOM scores with human attention in b, we choose the AU-ROC metric, where thebinary human attention serves as the correct class, and the SLALOM scores as the prediction. We observehow especially the importance scores of SLALOM are reasonably powerful in predicting human attention.Note that the human attention scores are unsigned, such that we also use absolute values for the SLALOMvalue scores and the linearized version of the SLALOM scores for the HAT prediction.",
  "F.1Fitting SLALOM as a Surrogate to Transformer outputs": "We provide an additional emprical counterexample for why GAMs cannot describe the transformer output in. The example provides additional intuition for why the GAM is insufficient to describe transformersacting like a weighted sum of token importances. We report Mean-Squared Errors when fitting SLALOM to transformer models trained on the linear datasetin . These results underline that SLALOM outperforms linear and additive models when fitting themto the transformer outputs. Note that even if the original relation in the data was linear, the transformerdoes not represent this relation such that the SLALOM describes its output better. We present additionalqualitative results for other models in that support the same conclusion.",
  "F.2Fitting SLALOM on Transformers trained on data following the SLALOM distribution": "We report Mean-Squared Errors in the logit-space and the parameter-space between original SLALOM scoresand recovered scores. The logit output are evaluated on a test set of 200 samples that are sampled fromthe original SLALOM. We provide these quantitative results in in for the parameter space.In logits the differences are negligibly small, and seem to decrease further with more layers. This findinghighlight that a) transformers with more layers still easily fit SLALOMs and such model can be recovered in",
  "Assign s(perfect) = s(worst) = 0Expected SLALOM output: -0.05": ": A simple empirical counterexample for why GANs cannot describe transformer output. We reportrounded scores by a real 4-layer BERT model (similar behavior was observed for other layers/architectures)and iteratively fit the GAM F(t) = tit wi(ti) to match observed outputs on the two tokens perfect andworst. We quickly arrive at a contradiction for the GAM. On the contrary, we can assign SLALOM scoresthat model this behavior with minor error. Because transformers behave like a weighted sum of importances,GAMs are insufficient to model their behavior. In conjunction with (a,b) this underlines that GAMsand linear models are insufficient as surrogates.",
  ": MSE (100), logit space, averaged over 5 runs": "parameters space. The results on the MSE in parameter space show no clear trend, but are relatively smallas well (with the largest value being MSE=0.015 (note that results in the table are multiplied by a factor of100 for readability). Together with our quantitative results in (c,d), this highlights that SLALOMhas effective recovery properties.",
  "(d) GPT-2 (L=12)": ": SLALOM describes outputs of transformer models well. Fitting SLALOM to the outputsof the models shown in using the synthetic dataset. We show results for a sequence containingrepetitions of the token perfect. Note however that the models were trained on a larger dataset of randomsequences samples as described in Appendix E.1.1, but these sequences were chosen for visualization purposes.Results on additional models. Despite having C/2=15 more parameters than the SLALOM model, theGAM model does not describe the output as accurately. We provide quantitative results in .",
  "F.3.2Correlation with Naive-Bayes Scores": "We compare the scores obtained with SLALOM with the scores obtained with other methods in ,obtaining scores that are reliable with SLALOM-eff (value scores and linear scores) in particular. WhileSHAP achieves higher correlation on BERT, SLALOM achieves higher correlation than LIME and SHAPon all models and higher correlations than LRP for GPT-2 while obtaining slightly inferior values for theBERT-based architectures.",
  "F.3.3Human Attention": "In , we show qualitative results for a sample from the Yelp-HAT dataset. After fitting SLALOMon top of the resulting model, we can extract the importance scores given to each token in the sample. Wecan see that the SLALOM scores manage to identify many of the tokens which real human annotators alsodeemed important for this review to be classified as positive. We also show qualitative results for the othermethods. However, we suggest caution when interpreting explanations visually without ground truth. Weargue that (1) theoretical properties of explanations (2) comparing to a known ground truth as well as (3)consideration of metrics from different domains, e.g., faithfulness, human perspective, are required to allowfor a comprehensive evaluation. This is the approach taken in our work.",
  "(a) Human annotation": "the most delicious steak i ' ve ever had ! also the most expensive , but it was totally worth it . our waiter was incredible , as was his assistant , and we loved the vibe of the restaurant . not too stuff y , really fun , great cocktail s . if i ' m ever back in vegas , i ' d love to return .",
  "(b) SLALOM-eff": "the most delicious steak i ' ve ever had ! also the most expensive , but it was totally worth it . our waiter was incredible , as was his assistant , and we loved the vibe of the restaurant . not too stuff y , really fun , great cocktail s . if i ' m ever back in vegas , i ' d love to return .",
  "(c) SHAP": "the most delicious steak i ' ve ever had ! also the most expensive , but it was totally worth it . our waiter was incredible , as was his assistant , and we loved the vibe of the restaurant . not too stuff y , really fun , great cocktail s . if i ' m ever back in vegas , i ' d love to return .",
  "(d) LIME": "the most delicious steak i ' ve ever had ! also the most expensive , but it was totally worth it . our waiter was incredible , as was his assistant , and we loved the vibe of the restaurant . not too stuff y , really fun , great cocktail s . if i ' m ever back in vegas , i ' d love to return .",
  "(e) AttnLRP": ": Qualitative comparisons of attribution maps. We provide attribution maps for the differenttechniques in this figure. Many words deemed important by human annotators are likewise highlighted bySLALOM and other techniques. value score 0.5 0.0 0.5 1.0 1.5 2.0 2.5 importance score this movie was so frustrating . everything seemed energetic and i totally prepared to have a good timeat leastthought ' d be able stand itbut , wrong first the weird loop ##ing ? like watching \" america s fun ##nies ##t home videos damn parentshated them much stereo - typical latino family need speak with person responsible for we talk that little girl who always hanging on someonejust her had mention now final scene trans ##cend ##s must say glorious ##ly bad full of ##ness is its own what crap ##py dancing horrible beautiful once",
  "(a) BERT": "value score 0.5 0.0 0.5 1.0 1.5 importance score movie was so frustrating . seemed energeticand totallyprepared to have a good time at least thought'd be able standit , wrong the weirdloop ing ? like watching \" 's n iest \".damn parents hated them muchstereo- typ ical family need speak with person responsible for this talk little girl who always hanging onsomeone just her had mention final scene transc ends must say glor iously bad full of ness that is its own crappy dancing rible beautiful once",
  "LMv-scoreslin.v-scoreslin.LIMESHAPIGGradLRP": "GPT-2 0.747 0.024 0.753 0.024 0.726 0.021 0.727 0.021 0.444 0.028 0.849 0.015 0.292 0.026 0.290 0.026 0.740 0.025BERT0.657 0.038 0.667 0.038 0.865 0.012 0.863 0.013 0.797 0.022 0.859 0.013 0.249 0.028 0.281 0.029 0.855 0.017DistilBERT0.645 0.033 0.642 0.033 0.813 0.017 0.813 0.018 0.746 0.025 0.854 0.013 0.201 0.026 0.243 0.028 0.768 0.024",
  ": Additional results for removal/insertion tests: We show results on the IMDB dataset for removal aswell as insertion and removal on the Yelp datset": "compared to baselines such as LIME, SHAP, Grad (Simonyan et al., 2013), and Integrated Gradients (IG,Sundararajan et al., 2017). For the insertion benchmarks, the tokens with the highest attributions are insertedto quickly obtain a high prediction score to the target class. For the deletion benchmark, the tokens with thehighest attributions are deleted from the sample to obtain a low score for the target class. We subsequentlydelete/insert more tokens and compute the Area Over the Perturbation Curve (AOPC) as in DeYounget al. (2020), which should be high for deletion and low for insertion. In addition to the insertion results inc, the removal results are shown in a. We show results for the Yelp dataset in b andc. We claim that linear SLALOM scores perform on par with LIME and SHAP in this benchmark,but do not always outperform them in this metric. For surrogate techniques (LIME, SHAP, SLALOM) weuse 5000 samples each.",
  "F.5Error Analysis for non-transformer models": "We also investigate the behavior of SLALOM for models that do not precisely follow the architecture describedin the Analysis section of this paper. In the present work, we consider an attribution method that is specificallycatered towards the transformer architecture, which is the most prevalent in sequence classification. We advisecaution when using our model when the type of underlying LM is unknown. In this case, model-agnosticinterpretability methods may be preferred. However, we investigate this issue further: We applied our SLALOM-eff approach to a simple, non-transformersequence classification model on the IMDB dataset, which is a three-layer feed-forward network based on a",
  "TF-IDF representation of the inputs. We compute the insertion and deletion Area-over-perturbation-curvemetrics that are given in": "These results show that due to its general expressivity, the SLALOM model also succeeds to provideexplanations for non-transformer models that outperform LIME and SHAP in the removal and insertion tests.We also invite the reader to confer b, where we show that SLALOM can predict human attention forlarge models, including the non-transformer Mamba model (Gu & Dao, 2023).",
  "F.6Runtime analysis": "We ran SLALOM as well as other feature attribution methods using surrogate models and compared theirruntime to explain a single classification of a 6-layer BERT model. We note that the runtime is mainlydetermined by the number of forward passes to obtain the samples to fit the surrogates. While this numberis independent of the dataset size, longer sequences require more samples for the same approximation quality.The results are shown in . While IG and Gradient explanations are the quickest, they also require backward passes which have largememory requirements. As expected, the computational complexity for surrogate model explanation (LIME,SHAP, SLALOM) is dominated by the number of samples and forward passes done. Our implementationof SLALOM is around 2x faster than LIME and almost 5x faster than SHAP (all approaches useda GPU-based, batching-enabled implementation), which we attribute to the fact that SLALOM can be fittedusing substantially shorter sequences than are used by LIME and SHAP. We are interested to find out how many samples are required to obtain an explanation of comparable quality toSHAP. We successively increase the number of samples used to fit our surrogates and report the performancein the deletion benchmark (where the prediction should drop quickly when removing the most importanttokens). We report the Area over the Perturbation Curve (AOPC) as before (this corresponds to their",
  "SHAP (nsamples=auto)0.9135 0.0105SLALOM, 500 samples0.9243 0.0105SLALOM, 1000 samples0.9236 0.005SLALOM 2000 samples0.9348 0.005SLALOM, 5000 samples0.9387 0.005SLALOM, 10000 samples0.9387 0.005": ": Ablation study on the number of samples required to obtain good explanations. The resultshighlight that a number as low as 500 samples can be sufficient to fit the surrogate model at a qualitycomparable to SHAP. Comprehensiveness metric of ERASER (DeYoung et al., 2020), higher scores are better). We comparethe performance to shap.KernelExplainer.shap_values(nsamples=auto) method of the shap package in. Our results indicate that sampling sizes as low as 500 per explained instance (which is as low aspredicted by our theory, with average sequence length of 200) already yields competitive results.",
  "F.7Applying SLALOM to Large Language Models": "Our work is mainly concerned with sequence classification. In this application, we observe mid-sized modelslike BERT to be prevalent. On the huggingface hub, among the 10 most downloaded models on huggingface, 9are BERT-based and the remaining one is another transformer with around 33M parameters3 (as of September2023). In common benchmarks like DBPedia classification4, the top-three models are transformers withtwo of them also being variants of BERT. We chose our experimental setup to reflect this. Nevertheless,we are interested to see if SLALOM can provide useful insights for larger models as well and thereforeexperiment with larger models. To this end, we use a model from the BLOOM family (Le Scao et al.,2023) with 7.1B parameters as well as the recent Mamba model (2.8B) (Gu & Dao, 2023) on the Yelp-HATdataset and compute SLALOM explanations. Note that the Mamba model does not follow the transformerframework considered in this work. We otherwise follow the setup described in and assess whetherour explanations can predict human attention. The results on the bottom of b highlight that this isindeed the case, even for larger models. The ROC scores are in a range comparable to the ones obtainedfor the smaller models. For the non-transformer Mamba model we observe a drop in the value of theimportance scores. This may suggest that value scores and linearized SLALOM scores are more reliable forlarge, non-transformer models. Applying SLALOM to blackbox models. Finally, we would like to emphasize that SLALOM, asa surrogate model can be applied to black-box models as well. To impressively showcase this, we applySLALOM to OpenAIs GPT-4 models via the API (Appendix F.7). We use the larger GPT-4-turbo andsmaller GPT-4o-mini for comparison. We prompt the model with the following template to classify the reviewand only output either 0 or 1 as response. SYSTEM: You are assessing movie reviews in an online forum. Your goal is to assess the reviewsoverall sentiment as overall negative (label 0) or overall positive (label 1). Your will see areview now and you will output a label. Make sure to only answer with either 0 or 1.USER: <the review> We then use the token probabilities returned by the API to compute the score. We create 500 samples asa training dataset for the surrogate model and fit SLALOM the model using SLALOM-eff. We obtain theimportance plots shown in . This highlight that SLALOM scales up to large models. However, wewould like to stress that there can be no guarantees as we have no knowledge about the specific structure ofthe model.",
  "(a) GPT-4-turbo": "this movie was so frustrating . everything seemed energetic and i was totally prepared to have a good time . i at least thought i 'd be able to stand it . but , i was wrong . first , the weird looping ? it was like watching \" america 's fun niest home videos \". the damn parents . i hated them so much . the stereo - typical latino family ? i need to speak with the person responsible for this . we need to have a talk . that little girl who was always hanging on someone ? i just hated her and had to mention it . now , the final scene transc ends , i must say . it 's so glor iously bad and full of bad ness that it is a movie of its own . what crappy dancing . horrible and beautiful at once .",
  "F.8Case study: Identifying Vulnerabilites and Weaknesses in classification models with SLALOM": "In our public code repository, we provide a case study to highlight how SLALOM can be used to uncoverpractical flaws in transformer classification models. We use a trained BERT model on IMDB sentimentclassification as an example and compute SLALOM scores for the tokens in the first 15 samples of the testset ( 1200 tokens). SLALOMs scores make it easy to see which tokens have a potentially severe impact onthe movie review outcome. We first select all tokens with an importance score > 2, all other words have nosubstantial impact, in particular when added to a larger sequence. We provide a visualization of these tokensin . We then make some interesting discoveries mainly based on value scores:",
  ". There are more words that are negatively interpreted by the model than positive words": "2. Out of the words that have highly negative value scores (+importance >2), we identify several wordsthat are some that are not directly negatively connotated, e.g., anyway, somehow, never,anymore, probably, doesn, maybe, without, however, surprised We then show that by a few (4) minor modification steps, e.g., adding some of these words to a review, wecan change the classification decision for a review from positive to negative, without essentially altering itscontent (i.e., we manually construct an adversarial example.) This highlights how SLALOM can intuitivelyhelp to uncover 1) the influential tokens that contribute most to the decision and 2) allow for a fine-grainedanalysis that can help uncover spurious concepts and give practitioners an intuitive understanding of amodels weaknesses."
}