{
  "Abstract": "AI agents have the potential to aid users on a variety of consequential tasks, includingconducting scientific research. To spur the development of useful agents, we need bench-marks that are challenging, but more crucially, directly correspond to real-world tasks ofinterest. This paper introduces such a benchmark, designed to measure the accuracy ofAI agents in tackling a crucial yet surprisingly challenging aspect of scientific research:computational reproducibility. This task, fundamental to the scientific process, involves re-producing the results of a study using the provided code and data. We introduce CORE-Bench(Computational Reproducibility Agent Benchmark), a benchmark consisting of 270 tasksbased on 90 scientific papers across three disciplines (computer science, social science, andmedicine). Tasks in CORE-Bench consist of three difficulty levels and include both language-only and vision-language tasks. We provide an evaluation system to measure the accuracy ofagents in a fast and parallelizable way, saving days of evaluation time for each run comparedto a sequential implementation. We evaluated two baseline agents: the general-purposeAutoGPT and a task-specific agent called CORE-Agent. We tested both variants using twounderlying language models: GPT-4o and GPT-4o-mini. The best agent achieved an accuracyof 19% on the hardest level of tasks, showing the vast scope for improvement in automatingroutine scientific tasks. Having agents that can reproduce existing work is a necessary steptoward building agents that can conduct novel research and could verify and improve theperformance of other research agents. We hope that CORE-Bench can improve the state ofreproducibility and spur the development of future research agents.",
  "Introduction": "An article about computational science in a scientific publication is not the scholarship itself,it is merely advertising of the scholarship. The actual scholarship is the complete softwaredevelopment environment and the complete set of instructions which generated the figures.(Buckheit & Donoho, 1995) Computational reproducibility, the ability to reproduce the results of a scientific study using the data andcode provided by its authors, is fundamental to scientific research (Medicine, 2019). Yet, recent studies",
  "Published in Transactions on Machine Learning Research (01/2025)": "Daniel Stockemer, Sebastian Koehler, and Tobias Lentz.Data Access, Transparency, and Replica-tion:New Insights from the Political Behavior Literature.PS: Political Science & Politics, 51(4):799803, October 2018.ISSN 1049-0965, 1537-5935.doi:10.1017/S1049096518000926.URL Minyang Tian, Luyu Gao, Shizhuo Dylan Zhang, Xinan Chen, Cunwei Fan, Xuefei Guo, Roland Haas, Pan Ji,Kittithat Krongchon, Yao Li, Shengyan Liu, Di Luo, Yutao Ma, Hao Tong, Kha Trinh, Chenyu Tian, ZihanWang, Bohao Wu, Yanyu Xiong, Shengzhu Yin, Minhui Zhu, Kilian Lieret, Yanxin Lu, Genglin Liu, YufengDu, Tianhua Tao, Ofir Press, Jamie Callan, Eliu Huerta, and Hao Peng. SciCode: A Research CodingBenchmark Curated by Scientists, July 2024. URL arXiv:2407.13168[cs]. Ana Trisovic, Matthew K. Lau, Thomas Pasquier, and Merc Crosas. A large-scale study on researchcode quality and execution. Scientific Data, 9(1):60, February 2022. ISSN 2052-4463. doi: 10.1038/s41597-022-01143-6. URL Publisher: NaturePublishing Group. Benjamin D. K. Wood, Rui Mller, and Annette N. Brown. Push button replication: Is impact evaluationevidence for international development verifiable? PLOS ONE, 13(12):e0209416, December 2018. ISSN1932-6203. doi: 10.1371/journal.pone.0209416. URL Publisher: Public Library of Science.",
  "CORE-Bench: Evaluating agents on computational reproducibility": "As the capabilities of AI agents continue to expand, many claims have been made about their ability toautonomously conduct research (Lu et al., 2024). But reproducing existing research is easier than conductingnew research, especially when new research requires reproducing earlier baselines for comparison. Recent work has introduced several benchmarks to evaluate language models and agents on various tasksrelated to computer programming and scientific research. These include benchmarks for conducting machinelearning experiments (Huang et al., 2023), research programming (Tian et al., 2024), scientific discovery(Majumder et al., 2024), performing scientific reasoning and citation tasks (Press et al., 2024; Xu et al., 2024),",
  "Benchmark Construction": "We decompose the task of verifying computational reproducibility into two sub-tasks: code reproducibilityand result reproducibility. This paper and benchmark focus on code reproducibility, which means running thecode and obtaining the results the capsule is supposed to produce, not result reproducibility, which is checkingwhether the code results match what is reported in the paper. Code reproducibility is by far the more timeconsuming part for a human. Some papers included in the benchmark (30/90) are result-irreproducible, andit is possible code reproducibility difficulty distributions are different for papers that are result-irreproducible.For the remainder of the paper, when we refer to reproducible, we mean code-reproducible. Verifying code reproducibility requires significant domain expertise and can be labor-intensive, even forexperienced researchers. This makes it particularly challenging to build a benchmark where the reproducibilityof each paper is verified. It can take a few hours to test the reproducibility of a paper in the wild, so verifyingabout a hundred papers from diverse fields would be impractical.",
  "Install all required libraries anddependencies, determine (and run)the correct command to reproducethe code from the task prompt, andperform information extraction overthe code output": "To address this, we drew papers and repositories from CodeOcean capsules (See ), which are knownto be code-reproducible with little effort (Clyburne-Sherin et al., 2019). We selected a set of 90 reproduciblepapers from CodeOcean using the process outlined in and . We split the dataset into 45papers for training and 45 for testing. For each paper, we manually created a set of task questions aboutthe outputs generated from a successful reproduction of the paper (Appendix A.3 provides details on taskquestion construction). These questions assess whether an agent has correctly executed the code and retrievedthe results. For instance, an agent could be asked to report the test accuracy of a model, an axis label of afigure, or another reproduced result. Some tasks have a single task question, while others consist of multiple.We ensure each task has at least one question that cannot be solved by guessing (e.g. a question with anopen-ended numerical answer), and a task is marked as correct only if all of the task questions are answeredcorrectly, which ensures all tasks cannot be solved by guessing. We focus on evaluating if agents can verify the code to be reproducible (not whether the results in a paper areconsistent with the code), which is why all tasks in CORE-Bench have been verified to be code reproducible.Since we measure an agents performance by their ability to answer task questions about the expected outputof code, we do not include irreproducible capsules, because the task questions would be impossible to answer.However, it follows that high performing agents could be used to check if code is irreproducible, since thefailure of a high-performing agent to reproduce code would indicate a problem with the code, not the agent.",
  "Why use CORE-Bench?": "Skills and modalities. Solving the tasks in CORE-Bench requires many skills, including understandinginstructions, debugging code, retrieval, and interpreting results from a wide range of disciplines. The skillsnecessary to perform well on CORE-Bench reflect many skills necessary to reproduce new research. Tasks require interpreting both text and image output from code. The vision-based questions (e.g. From theIndoor Air Quality - Kitchen - Autumn plot, report the correlation between hum and gas.) require extractingresults from attributes of figures, graphs, plots, or PDF tables. The text-based questions (e.g. Report thetest accuracy of the neural network after epoch 10.) include extracting results from command line text, PDF",
  "(b) Example of evaluation criteria": ": During task execution, the agent must interpret the task prompt, set up the code in the capsule,run the code, and populate the specified result in the provided JSON file. For evaluation, we manuallyreproduced each capsule in the benchmark three times. We determine if an agent correctly solves a task ifthe agents reported results for all questions fall within a 95% prediction interval for every task questionof the results from the three manual runs. Prediction intervals provide a range in which we expect futureobservations to fall, accounting for stochasticity in the code outputs (Spence & Stanley, 2016). text, and tables or text in HTML, markdown, or Latex. Capsules can have vision-based questions, text-basedquestions, or both (See Table A2), and capsules have codebases in either Python or R (See Table A1). Real-world computational reproducibility tasks. When constructing our benchmark, we focus onits construct validity, which is about how well a test measures real-world performance (Biderman et al.,2024; Raji et al., 2021; Kapoor & Narayanan, 2023). CORE-Bench tasks correspond closely to tasks thatresearchers must accomplish so that improved performance on the benchmark can directly lead to improvedcomputational reproducibility norms. First step toward research agents. The first step toward completing new scientific research is the abilityto reproduce existing scientific work. Building agents that excel at reproducibility is a necessary, and yetmore attainable step toward building agents that can conduct novel research.",
  "We evaluated all agents on CORE-Bench split by difficulty: CORE-Bench-Easy, CORE-Bench-Medium, andCORE-Bench-Hard": "Baseline agents. We developed and evaluated two variants of the AutoGPT agent (Significant Gravitas,2024) on the benchmark: AutoGPT, which was not prompted or given any tools specific to CORE-Bench, andthe CORE-Agent family of agents, which were prompted and modified for enhanced performance on each ofthe three difficulty levels of CORE-Bench. 1. AutoGPT: This agent is largely unmodified from the popular general-purpose AutoGPT agent, but wecreated another tool for the agent called query_vision_language_model, which takes as input an imageand a query, and outputs OpenAI APIs response to the image query. This allows the agent to analyzeresults in figures and plots4. We included this modification in AutoGPT because the ability to query avision language model is not specific to CORE-Bench. Other minor changes can be found in Appendix D.1.",
  "Results": "Overall, CORE-Agent with GPT-4o is the top performing agent on all three levels of the benchmark, solving58.52% of tasks on CORE-Bench-Easy, 55.56% on CORE-Bench-Medium, but only 19.26% on CORE-Bench-Hard.We report all results in this section on the test split unless otherwise mentioned, since we used the train splitwhile developing the agent (see Figure A1 for train set results). Our results demonstrate that generalist agents can be effectively adapted to specific tasks with minimaleffort, yielding significant performance improvements. For instance, AutoGPT with GPT-4o scored just 4.44%",
  "Accuracy varies by difficulty level": "Agents generally performed the highest on CORE-Bench-Easy, followed by CORE-Bench-Medium andCORE-Bench-Hard. For instance, CORE-Agent with GPT-4o-mini scored 42.22%, 30.37%, and 14.07% on thethree levels, respectively (See ). These results are expected, since CORE-Bench-Easy is designed to be the easiest task with the code outputsalready provided in the environment. CORE-Bench-Medium is slightly harder, requiring agents to use aprovided Docker command to replicate the papers results. CORE-Bench-Hard is significantly harder, requiringagents to install all dependencies and libraries and determine the correct command necessary to reproducerelevant results.",
  "Task specific modifications improve accuracy, especially for weaker models": "Comparing performance when fixing the LLM model, we observed that AutoGPTs performance improvedsubstantially with only slight modifications. This adaptability seems to be particularly advantageous forweaker LLMs, where small changes provide crucial guardrails and task guidance. With the GPT-4o back-end,a few modifications to the prompt and the programmatic check of the output format boosted the performanceon CORE-Bench-Easy performance from 33.33% to 58.52%. The differences were even starker when usingGPT-4o-mini: performance improved from 6.67% to 42.22%. Our results highlight the adaptability of generalist agents, demonstrating significant performance gains fromminimal, task-specific adjustments. We hypothesize that agents that use stronger models in the future willrequire even fewer task-specific modifications to perform well on a given task.",
  "Stronger models lead to higher accuracy despite a lower token budget": "We ran AutoGPT and CORE-Agent using both GPT-4o and GPT-4o-mini with an API cost limit of $4. Eventhough the per-token cost of GPT-4o-mini is less than 5% than that of GPT-4o, which allows for longersessions before hitting the cost limit, GPT-4o still outperformed GPT-4o-mini on both agents. Despite havingthe same cost limits, GPT-4o-mini powered agents tended to be 3-5x cheaper than GPT-4o agents. In allsettings, the average per-task cost was cheapest on CORE-Bench-Easy, followed by CORE-Bench-Medium andCORE-Bench-Hard (). To evaluate the impact of our $4 cost limit on performance, we ran CORE-Agent on the CORE-Bench-Hard witha $10 cost limit on the train set. With the new limit, GPT-4o-mini performance remained unchanged, andGPT-4os performance increased modestly from 26% to 31% (). Note that GPT-4o-mini outperformedGPT-4o for lower cost limits under around $2.50. Increasing the cost limit did not greatly increase accuracy because when agents succeeded at tasks, theysucceeded quickly (the average cost of successful tasks for CORE-Agent and GPT-4o was $0.54, compared to",
  "Written questions are easier than vision questions": "Agents consistently performed better on text-based questions than vision-based questions. CORE-Agent withGPT-4o got 58.70% vision questions correct and 87.88% written questions correct on CORE-Bench-Easy onthe test set. Similarly, CORE-Agent with GPT-4o-mini got 36.96% of vision questions correct and 81.81% ofwritten questions correct. Vision questions are harder because they typically require analyzing results fromfigures, whereas written answers are often directly found in the terminal output. Agents were sometimesunable to find the relevant figure if multiple output files are generated. Even once found, analyzing theoutput can be difficult, as past work as also shown (Xu et al., 2024; Majumdar et al., 2024).",
  "Python tasks are much easier than R": "Agents performed much better on Python tasks than R tasks (). One reason is that R outputs wereoften more difficult to parse, since many R capsules generate full PDF manuscripts which the agent has toread through. Another reason is that installing the requirements and dependencies for R packages can takemuch longer than for Python. Computer Science tasks are disproportionately in Python, which partiallyexplains why they are easier than the other two disciplines.",
  ": Performance of CORE-Agent using GPT-4o vs GPT-4o-mini on the test set by discipline andprogramming language. Error bars are one standard deviation calculated from three trials": "outputted to the terminal. If the code output was written to multiple files, such as in different figures, agentsstruggled to determine which figure was relevant and had the correct information. Often, agents would useinformation from the incorrect figure to answer the question (Appendix D.3.1). On CORE-Bench-Medium, AutoGPT struggled to follow instructions to execute the Docker command toreproduce the code and would sometimes get thrown off by competing instructions. For instance, theagent might read the README file, and attempt to reproduce the code manually, without using Docker(Appendix D.3.2). CORE-Agent, however, tended to not struggle on this because of the task-specific instructions,and mistakes were usually caused by retrieval issues as described above. On CORE-Bench-Hard, in addition to the retrieval issues described above (which accounted for 23% of failuresfor CORE-Agent with GPT-4o on the test set), agents struggled with installing the dependencies for runningcode repositories (accounting for 57% of failures) and running the correct commands to reproduce the paper(accounting for 20% of failures). Agents often did not finish resolving dependency version issues before hittingthe cost limit, getting stuck attempting to install the same library multiple times (Appendix D.3.3).",
  "Better guardrails are needed to deploy safe agents": "In one case, the agent attempted to search for the CodeOcean repository online to look for the requirementsfor missing dependencies. Although the agent tried to create an account on CodeOcean, it could not view theCodeOcean website since it required JavaScript (Appendix D.3.4). This points to the need for mechanisms torestrict the actions taken by the agent. We have updated the release version of our evaluation harness torestrict access to the CodeOcean.com domain. Since AutoGPT can execute arbitrary actions on the web, better guardrails should be developed to ensureagents exhibit safe and expected behavior (He et al., 2024). For instance, there are no existing safeguardspreventing simple agent errors such as creating thousands of accounts on a website. For this paper, we didnot incorporate web browsing restrictions for our agents since their inability to render JavaScript preventedmost damaging actions from being taken out. However, as agents advance, developers should implementadditional safety checks.",
  "AISI. Inspect, 2024. URL": "Rose L. Andrew, Arianne Y.K. Albert, Sebastien Renaut, Diana J. Rennison, Dan G. Bock, and TimVines. Assessing the reproducibility of discriminant function analyses. PeerJ, 3:e1137, August 2015. ISSN2167-8359. doi: 10.7717/peerj.1137. URL Anya Belz, Shubham Agarwal, Anastasia Shimorina, and Ehud Reiter. A Systematic Review of ReproducibilityResearch in Natural Language Processing. In Paola Merlo, Jorg Tiedemann, and Reut Tsarfaty (eds.),Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics:Main Volume, pp. 381393, Online, April 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.eacl-main.29. URL Stella Biderman, Hailey Schoelkopf, Lintang Sutawika, Leo Gao, Jonathan Tow, Baber Abbasi, Alham FikriAji, Pawan Sasanka Ammanamanchi, Sidney Black, Jordan Clive, Anthony DiPofi, Julen Etxaniz, BenjaminFattori, Jessica Zosa Forde, Charles Foster, Mimansa Jaiswal, Wilson Y. Lee, Haonan Li, Charles Lovering,Niklas Muennighoff, Ellie Pavlick, Jason Phang, Aviya Skowron, Samson Tan, Xiangru Tang, Kevin A.Wang, Genta Indra Winata, Franois Yvon, and Andy Zou. Lessons from the Trenches on ReproducibleEvaluation of Language Models, May 2024. URL arXiv:2405.14782[cs].",
  "Matteo Brivio and ar ltekin. [Re] Exploring the Representation of Word Meanings in Context. ReScienceC, 9(2):#5, July 2023. doi: 10.5281/zenodo.8173658. URL": "Bradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc V. Le, Christopher R, and AzaliaMirhoseini. Large Language Monkeys: Scaling Inference Compute with Repeated Sampling, July 2024.URL arXiv:2407.21787 [cs]. Jonathan B. Buckheit and David L. Donoho. WaveLab and Reproducible Research. In Anestis Antoniadis andGeorges Oppenheim (eds.), Wavelets and Statistics, pp. 5581. Springer, New York, NY, 1995. ISBN 978-1-4612-2544-7. doi: 10.1007/978-1-4612-2544-7_5. URL Federico Cassano, John Gouwar, Daniel Nguyen, Sydney Nguyen, Luna Phipps-Costin, Donald Pinckney,Ming-Ho Yee, Yangtian Zi, Carolyn Jane Anderson, Molly Q Feldman, Arjun Guha, Michael Greenberg,and Abhinav Jangda. MultiPL-E: A Scalable and Extensible Approach to Benchmarking Neural CodeGeneration, 2022. URL Version Number: 4.",
  "Christian Collberg and Todd A. Proebsting. Repeatability in computer systems research. Communicationsof the ACM, 59(3):6269, February 2016. ISSN 0001-0782, 1557-7317. doi: 10.1145/2812803. URL": "Paul Gertler, Sebastian Galiani, and Mauricio Romero. How to make replication the norm. Nature, 554(7693):417419, February 2018. doi: 10.1038/d41586-018-02108-9. URL Bandiera_abtest: a Cg_type: Comment Publisher: Nature Publishing GroupSubject_term: Research data, Research management, Publishing. Kimberly J. Gilbert, Rose L. Andrew, Dan G. Bock, Michelle T. Franklin, Nolan C. Kane, Jean-SbastienMoore, Brook T. Moyers, Sbastien Renaut, Diana J. Rennison, Thor Veen, and Timothy H. Vines.Recommendations for utilizing and reporting population genetic analyses: the reproducibility of ge-netic clustering using the program structure. Molecular Ecology, 21(20):49254930, 2012. ISSN 1365-294X.doi: 10.1111/j.1365-294X.2012.05754.x.URL _eprint: Tom E. Hardwicke, Maya B. Mathur, Kyle MacDonald, Gustav Nilsonne, George C. Banks, Mallory C.Kidwell, Alicia Hofelich Mohr, Elizabeth Clayton, Erica J. Yoon, Michael Henry Tessler, Richie L. Lenne,Sara Altman, Bria Long, and Michael C. Frank. Data availability, reusability, and analytic reproducibility:evaluating the impact of a mandatory open data policy at the journal Cognition. Royal Society OpenScience, 5(8):180448, August 2018. doi: 10.1098/rsos.180448. URL Publisher: Royal Society. Tom E. Hardwicke, Manuel Bohn, Kyle MacDonald, Emily Hembacher, Michle B. Nuijten, Benjamin N.Peloquin, Benjamin E. deMayo, Bria Long, Erica J. Yoon, and Michael C. Frank. Analytic reproducibilityin articles receiving open data badges at the journal Psychological Science : an observational study. RoyalSociety Open Science, 8(1):201494, January 2021. ISSN 2054-5703. doi: 10.1098/rsos.201494. URL",
  "Jimmy Koppel. Everyones talking about Sakanas AI scientist. But no-ones answering the big question: isits output good?, August 2024. URL": "Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rmi Leblond, Tom Eccles,James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien de Masson dAutume,Igor Babuschkin, Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov, JamesMolloy, Daniel J. Mankowitz, Esme Sutherland Robson, Pushmeet Kohli, Nando de Freitas, KorayKavukcuoglu, and Oriol Vinyals. Competition-Level Code Generation with AlphaCode. Science, 378(6624):10921097, December 2022. ISSN 0036-8075, 1095-9203. doi: 10.1126/science.abq1158. URL arXiv:2203.07814 [cs]. David M. Liu and Matthew J. Salganik. Successes and Struggles with Computational Reproducibility:Lessons from the Fragile Families Challenge. Socius, 5:2378023119849803, January 2019. ISSN 2378-0231.doi: 10.1177/2378023119849803. URL Publisher: SAGEPublications. Victor Livernoche and Vidya Sujaya. [Re] A Reproduction of Automatic Multi-Label Prompting: Simple andInterpretable Few-Shot Classification. ReScience C, 9(2):#33, July 2023. doi: 10.5281/zenodo.8173735.URL",
  "Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, and David Ha. The AI Scientist:Towards Fully Automated Open-Ended Scientific Discovery, August 2024. URL arXiv:2408.06292 [cs]": "Arjun Majumdar, Anurag Ajay, Xiaohan Zhang, Pranav Putta, Sriram Yenamandra, Mikael Henaff,Sneha Silwal, Paul Mcvay, Oleksandr Maksymets, Sergio Arnaud, Karmesh Yadav, Qiyang Li, BenNewman, Mohit Sharma, Vincent Berges, Shiqi Zhang, Pulkit Agrawal, Yonatan Bisk, Dhruv Ba-tra, Mrinal Kalakrishnan, Franziska Meier, Chris Paxton, Alexander Sax, and Aravind Rajeswaran.OpenEQA: Embodied Question Answering in the Era of Foundation Models.pp. 1648816498,2024. URL Bodhisattwa Prasad Majumder, Harshit Surana, Dhruv Agarwal, Bhavana Dalvi Mishra, Abhijeetsingh Meena,Aryan Prakhar, Tirth Vora, Tushar Khot, Ashish Sabharwal, and Peter Clark. DiscoveryBench: TowardsData-Driven Discovery with Large Language Models, July 2024. URL [cs].",
  "METR. Vivaria, 2024. URL": "Florian Naudet, Charlotte Sakarovitch, Perrine Janiaud, Ioana Cristea, Daniele Fanelli, David Moher, andJohn P. A. Ioannidis. Data sharing and reanalysis of randomized controlled trials in leading biomedicaljournals with a full data sharing policy: survey of studies published in The BMJ and PLOS Medicine.BMJ, 360:k400, February 2018.ISSN 0959-8138, 1756-1833.doi: 10.1136/bmj.k400.URL Publisher: British Medical Journal Publishing Group Section:Research. Pepijn Obels, Danil Lakens, Nicholas A. Coles, Jaroslav Gottfried, and Seth A. Green. Analysis of Open Dataand Computational Reproducibility in Registered Reports in Psychology. Advances in Methods and Practicesin Psychological Science, 3(2):229237, June 2020. ISSN 2515-2459. doi: 10.1177/2515245920918872. URL Publisher: SAGE Publications Inc. Joelle Pineau, Philippe Vincent-Lamarre, Koustuv Sinha, Vincent Lariviere, Alina Beygelzimer, FlorencedAlche Buc, Emily Fox, and Hugo Larochelle. Improving Reproducibility in Machine Learning Research(AReport from the NeurIPS 2019 Reproducibility Program). Journal of Machine Learning Research, 22(164):120, 2021. ISSN 1533-7928. URL",
  "Significant Gravitas. AutoGPT, September 2024. URL original-date: 2023-03-16T09:21:07Z": "Koustuv Sinha, Maurits Bleeker, Samarth Bhargav, Jessica Zosa Forde, Sharath Chandra Raparthy, JesseDodge, Joelle Pineau, and Robert Stojnic. ML Reproducibility Challenge 2022. ReScience C, 9(2):#46,July 2023. doi: 10.5281/zenodo.8200058. URL Jeffrey R. Spence and David J. Stanley. Prediction Interval: What to Expect When Youre Expecting . . . AReplication. PLOS ONE, 11(9):e0162874, September 2016. ISSN 1932-6203. doi: 10.1371/journal.pone.0162874.URL Public Library of Science.",
  "A.1Original CodeOcean Dataset": "To obtain a dataset of all 5,090 capsules on CodeOcean and their corresponding environment files, we wrotea webscraper that downloads the metadata of every capsule from CodeOcean. We then manually exportedeach capsule from CodeOceans web interface to obtain the environment files. Finally, we filtered the capsulesin this dataset based on the ten criteria outlined in .",
  "A.3Task question construction": "To write task questions for each capsule, we examined the capsules results folder after a successful reproduciblerun on CodeOceans web interface and chose outputs from any of the files in the results for the agent toextract. These outputs could include a models accuracy, the axis label of a figure, or any other relevantmetric. Then, for each output, we manually write a prompt instructing the agent to report the correspondingvalue. Since a single paper can have multiple outputs, CORE-Bench consists of 90 capsules and 181 taskquestions. The number of task questions per capsule ranges from one to eight.",
  "A.4Breakdown of task questions by discipline, modality, and language": "When choosing capsules to include in CORE-Bench, we attempted to have a similar number of capsules fromeach discipline. We provide a breakdown of capsules from each discipline in the train and test sets in Table A3.CodeOcean contains 1,259 computer science capsules written in Python or R, 270 social science capsuleswritten in Python or R, and 128 medical sciences capsules written in Python or R. Due to the limitedavailability of social science and medical sciences capsules that fulfilled all of our criteria (See ), ourfinal benchmark contains more computer science capsules than capsules of other disciplines. CORE-Bench alsoconsists of a similar number of Python and R capsules (See Table A1) and a similar number of vision-basedand language-based task questions (See Table A2).",
  "BHarness Details": "Our evaluation harness runs all agents on virtual machines using Azure. For non-GPU capsules, we use aStandard_E2as_v5 machine type, and for GPU capsules, we use a Standard_NC4as_T4_v3 machine type.All VMs run Ubuntu Linux and have an 80 GB disk attached. The harness initially creates a VM for each task-agent pair and copies over the capsule files and agent files tothe VM. Once the files are copied over, the harness runs the agent on the VM. The capsule only downloadsthe results and deletes the VM once the agent creates a file called task_completed.log in the home directory.This log file can be empty or can contain any logging information that the developer wishes to save from therun. On occasion, the harness may fail to download the results of an agent from a VM due to an Azure error(for example, timing out when attempting to create a virtual machine). In this case, you should re-run theexperiment with the resume flag, which will only start VMs for unfinished tasks. When running the benchmark on multiple capsules, please be aware that you will incur billing charges forall instances. If you need to manually delete a VM capsule (if the harness code gets interrupted), you mustdelete all associated resources with the VM, (i.e. the network interface, the public IP, the disk, and thevirtual network) associated with the instance. It is not sufficient to only delete the instance itself.",
  "C.2Confidence Intervals on Test Set": "We ran CORE-Agent experiments with GPT-4o and GPT-4o-mini three times to generate a 95% confidenceinterval over the mean accuracy and mean cost (See Table A4). The accuracy of the top-performing agenthad a CI of under 5 percentage points on all difficulty levels. Overall, the accuracy of GPT-4o-mini had alarger CI on results than GPT-4o, suggesting it is a less reliable model to use.",
  "C.3Agent Failures vs Incorrect Responses": "There are two ways an agent could fail a task: by answering a task question incorrectly, or by not answeringa task question at all due to getting stuck at some earlier stage. We compare the accuracy rate (answering alltask questions correctly) to the full attempt rate (giving an answer to all task questions) in Table A5. Largegaps between the accuracy rate and full attempt rate indicate that the agent could be made more reliableand techniques such as resampling responses could improve performance.",
  "C.4Task Completion Time on Test Set": "We report the average amount of time (in seconds) each agent takes to complete tasks at all three levels(See Table A6). CORE-Bench-Easy is by far the quickest level to complete since the code outputs are alreadygiven to the agent, so it does not need to run code. CORE-Bench-Medium tasks take longer since the agentneeds to run the provided Docker command and wait for it to finish. CORE-Bench-Hard tasks take by far thelongest, since the agent needs to install repositories and potentially debug running the code.",
  "C.5Pass@k on the Test Set": "On the test set, the pass@1 accuracy of CORE-Agent with GPT-4o on CORE-Bench-Hard was 20.00% and thepass@3 accuracy was 28.88% (See Figure A2). Similarly, with GPT-4o-mini, the pass@1 accuracy was 13.33%and the pass@3 accuracy was 24.44%. Since the performance could be improved simply by re-running themodel, strategies like running the agents multiple times and choosing the best outputs could be promising.Past work has shown retrying or increasing temperature between retries can be enough to drastically improveperformance (Kapoor et al., 2024; Hassid et al., 2024; Brown et al., 2024; Li et al., 2022).",
  "C.6Passk on the Test Set": "To measure the reliability of agents, we report the passk metric, which is defined as the percentage of tasksfor which all k task trials are successful (Yao et al., 2024). The pass1 accuracy of CORE-Agent with GPT-4oon CORE-Bench-Hard was 20.00% and the pass3 accuracy was 6.66%. Similarly, the pass1 accuracy ofCORE-Agent with GPT-4o-mini on CORE-Bench-Hard was 13.33% and the pass3 accuracy was 4.44% (SeeFigure A3). The results suggest that the underlying stochasticity of the agent caused it to not consistentlysolve the same tasks. Increasing the reliability of agents such that they can consistently solve problems theyare capable of solving is a challenging problem.",
  "D.1AutoGPT Bug Fixes and Changes": "In addition to the modifications to AutoGPT described in the main text, we implemented two other changesfor both AutoGPT and CORE-Agent. We implemented these changes for both agents and did not considerthem as task-specific modifications since the changes are not specific to CORE-Bench and would improve theagent in many domains. 1. Truncating tool output: If a tool invoked by AutoGPT generates an output that is too long, weupdated the code to truncate the output to include the beginning and end, rather than return anerror. We found this change helps the agent better use tools when the outputs are long. 2. Using the shell to execute all Bash commands: AutoGPT uses the subprocess module toexecute commands on the command line. However, the default setting was to set shell=False wheninvoking subprocess.run, which prevented the agent from using shell-specific commands such as &&",
  "D.3.3Being unable to install the correct version of dependencies (CORE-Bench-Hard)": "In capsule-8807709, CORE-Agent with GPT-4o installed network-diffusion version 0.14.4. However, oneof the import statements (from network_diffusion import MultiSpreading) threw an error because theimport was only supported in version 0.6). The agent successfully realized it may need to install an olderlibrary version and performed a web search to see which version was applicable, but could not find the correctresult within the cost constraint.",
  "Step 18:Executed pipinstallipykernel tqdm successfully , installing": "packages: ipykernel -6.29.5 , comm -0.2.2 , debugpy -1.8.5 , ipython -8.26.0 ,jupyter -client -8.6.2 , jupyter -core -5.7.2 , matplotlib -inline -0.1.7 , nest -asyncio -1.6.0 , pyzmq -26.1.0 , tornado -6.4.1 , traitlets -5.14.3 , jedi -0.19.1 ,prompt -toolkit -3.0.47 , stack -data -0.6.3 , asttokens -2.4.1 , executing -2.0.1 ,pure -eval -0.2.3.",
  "D.3.4Attempting to look up the capsule on CodeOcean (CORE-Bench-Hard)": "Also in capsule-8807709, CORE-Agent with GPT-4o, after being unable to locate the requirements.txt filein the repository, attempted to look up the capsule on CodeOcean online. The agent ultimately did notsucceed because JavaScript is required to render CodeOcean, which the agent did not have access to throughits web browsing capabilities. However, this example highlights the care that agent developers must takeduring evaluation.",
  "McCullough et al. (2006) reports an approximate number of papers reproduced. Authors state thatthey analyzed greater than 150 papers, with less than 15 replicated": "We manually analyzed the papers from the 2022 Machine Learning Reproducibility Challenge (Sinha et al.,2023). Of the 44 papers submitted to the challenge, 28 attempted to reproduce papers where both data andcode were fully available. 10 of those 28 papers were only partially reproduced. We consider papers to befully reproduced if all the main claims of the paper completely hold, even if the reproduced quantitativeresults slightly deviate from the original results. For example, we consider Livernoche & Sujaya (2023) asuccessful reproduction of the original paper because authors validate the original papers claims and resultsfall within the standard deviation reported in the original paper. On the other hand, we consider papers tohave reproducibility errors if all the main claims of the paper cannot be reproduced, or if result values fromthe original paper deviate significantly from those of the reproduced paper. For example, we treat Brivio &ltekin (2023) as an unsuccessful reproduction because the highest accuracy score from the reproducedpaper deviates significantly from the original paper although the original hypothesis was verified. We do notconsider the results of additional experiments not contained in the original paper. Of the fully reproducedpapers, many codebases contained errors, outdated packages, or limited documentation, requiring researchersto modify the codebase during the reproduction process."
}