{
  "Abstract": "In deep Reinforcement Learning (RL), value functions are typically approximated using deepneural networks and trained via mean squared error regression objectives to fit the true valuefunctions. Recent research has proposed an alternative approach, utilizing the cross-entropyclassification objective, which has demonstrated improved performance and scalability of RLalgorithms. However, existing study have not extensively benchmarked the effects of thisreplacement across various domains, as the primary objective was to demonstrate the efficacyof the concept across a broad spectrum of tasks, without delving into in-depth analysis. Ourwork seeks to empirically investigate the impact of such a replacement in an offline RL setupand analyze the effects of different aspects on performance. Through large-scale experimentsconducted across a diverse range of tasks using different algorithms, we aim to gain deeperinsights into the implications of this approach. Our results reveal that incorporating thischange can lead to superior performance over state-of-the-art solutions for some algorithmsin certain tasks, while maintaining comparable performance levels in other tasks, howeverfor other algorithms this modification might lead to the dramatic performance drop. Thisfindings are crucial for further application of classification approach in research and practicaltasks1.",
  "Introduction": "In the realm of deep Reinforcement Learning (RL), the conventional approach to approximating valuefunctions has long relied on employing the Bellman optimality operator alongside mean squared error (MSE)regression objectives, owing to the continuous nature of the task at hand. However, insights from otherdomains of machine learning have illuminated the potential benefits of employing classification objectiveseven in scenarios where regression seems as a natural choice (Rothe et al., 2018; Rogez et al., 2019). This shifthas been attributed to various hypotheses, including the stability of gradients (Imani et al., 2024), improvedfeature representation (Zhang et al., 2023), and implicit biases (Stewart et al., 2023).",
  "Published in Transactions on Machine Learning Research (11/2024)": "Rothe et al., 2018; Rogez et al., 2017). Within the RL domain, some studies have experimented withemploying classification objectives as a workaround, albeit without conducting comprehensive analyses ofthis modification (Schrittwieser et al., 2020; Hafner et al., 2023; Hessel et al., 2021; Hansen et al., 2023).Categorical distributional RL (Bellemare et al., 2017) works are also relevant for the considered topic, wherethe classification is also used, however usage of classification instead of regression is not a central topic inthis reseach direction. Additionally, several works in offline RL have demonstrated the benefits of utilizingclassification objectives for various tasks, albeit lacking in-depth analyses of this specific component and itselements (Kumar et al., 2022; Springenberg et al., 2024). To the best of our knowledge, (Farebrother et al., 2024) represent the first and only study to make regressionreplacement with classification a central research question. The authors compared different methods ofconverting regression targets into classification targets, providing experimental results across a diverse arrayof tasks encompassing Atari games, robotics, and natural language processing problems in online and offlineRL setups. Their study revealed that HL-Gauss (Imani & White, 2018) represents the optimal approachfor representing RL regression targets with categorical distributions while this was not the case for thesupervised regression tasks (Imani & White, 2018). The authors assert that cross-entropy serves as a \"drop-in\"replacement for MSE in RL, leading to a more stable training process and enhanced scalability with deepneural network architectures such as Transformers (Vaswani et al., 2017) or ResNets (He et al., 2016). Ourwork draws primary inspiration from (Farebrother et al., 2024) and aims to provide a more in-depth analysisof this phenomenon in offline RL, which we believe holds significant potential benefits for both offline RLresearchers and practitioners.",
  "Offline Reinforcement Learning": "The RL problem is conventionally framed as a Markov Decision Process characterized by a tuple (S, A, P, R, ),where: S Rn denotes the state space, A Rm represents the action space, P : S A S is the transitionfunction, R : S A R is the reward function, and (0, 1) is the discount factor. The objective of RL isto find a policy : S A that maximizes the expected sum of discounted rewards: t=0 tR(st, at). Thisentails the policy learning process, where the agent interacts with its environment by observing environmentalstates, taking actions in response, and receiving corresponding rewards. Offline RL presents a departure from the traditional RL setup in that the agent relies solely on a pre-collecteddataset D collected by external agents. This paradigm introduces novel challenges, such as the estimation ofvalue functions for out-of-distribution state-action pairs, thus giving rise to an entire subfield dedicated toaddressing these challenges.",
  "i=1pTi (s, a, ) log pi(s, a, )": "Farebrother et al. (2024) demonstrated that the HL-Gauss method (Imani & White, 2018) is particularlyeffective for mapping continuous values into bins within this framework. HL-Gauss employs a normaldistribution analog for mapping values into neighboring bins, with a hyperparameter determining thedistributions breadth. The authors recommend tuning the ratio / as a more interpretable hyperparameter,with a default value of 0.75 chosen based on statistical considerations and it has shown good empirical result.",
  "Methodology": "Our methodology is straightforward: we select several offline RL algorithms and adapt them for cross-entropyloss, as outlined in .2. By default, we determine the values of vmin and vmax by computing allpossible discounted returns within a given dataset and identifying the minimum and maximum values, whichaligns naturally with the offline RL setup. Herein, we provide brief descriptions of the algorithms employedin our study. We selected three algorithms from the most prominent families of offline RL approaches: policy regularization,implicit regularization, and Q-function regularization. These categories encompass the majority of widely-recognized offline RL methods, as noted in prior work (Levine et al., 2020), making them representativechoices for illustrating the general applicability of our framework. By selecting these specific algorithms, weaim to cover diverse approaches in offline RL while demonstrating that our classification loss can seamlesslyintegrate with various methods across these categories. The choice of particular algorithms is based on resultsfrom Tarasov et al. (2024b) where a wide range of offline RL algorithms were compared against each other.We provide pseudocodes for the algorithms with cross-entropy loss in Appendix B. Revisited BRAC (ReBRAC). ReBRAC (Tarasov et al., 2024a) stands as a minimalist, state-of-the-artensemble-free algorithm for both offline and offline-to-online RL. It employs MSE to penalize deviations fromactions present in the dataset. Built upon TD3+BC (Fujimoto & Gu, 2021), ReBRAC incorporates severalmodifications that significantly enhance its performance. The Q-loss function takes the form:",
  "E(s,a,s, a)D,a(s)(Q(s, a, ) [R(s, a) + (Q(s, a, ) (a a)2)])2": "where denotes a penalty weight. Although only the target network part differs, the bin mapping is conductedin the same manner, with the penalty subtracted beforehand. We select ReBRAC as an exemplary algorithmwith policy regularization (i.e. forcing the policy to commit actions simillar to dataset) due to its highperformance, simplicity, and its encounter with the Q-function divergence problem while solving certainD4RL AntMaze tasks.",
  "E(s,a,s)D(Q(s, a, ) [R(s, a) + V(s)])2": "Once again, the target component allows for classification without additional manipulations. Regularizationis achieved through a specially designed V-function loss, without explicit penalties for the policy or valuefunctions. We opt for IQL as it is the best example of algorithms within this family. Large-Batch SAC (LB-SAC). LB-SAC (Nikulin et al., 2022) is an instance of the ensemble-based SAC-Nalgorithm (An et al., 2021), utilizing large-batch optimization to reduce ensemble size. SAC-N comprises NQ-functions, each with the following loss:",
  "(s, a)": "Ensemble-based approaches are known for their efficacy across many offline RL tasks. According to theabove statement, SAC-N can be also considered as an example of offline RL algorithm with Q-functionregularization. We opt for LB-SAC instead of the original SAC-N due to computational constraints that arisedue to the huge ensemble sizes (up to 500 critics) required in original SAC-N.",
  "We conducted our experiments using three sets of tasks from the D4RL benchmark (Fu et al., 2020):Gym-MuJoCo, AntMaze, and Adroit. We utilized all datasets within each set": "For ReBRAC in subsection 4.2, we employed the best hyperparameters as outlined in Tarasov et al. (2024a).When exploring parameter search in subsection 4.3, we utilized the same hyperparameter grids for ReBRACand IQL as in Tarasov et al. (2024a) and used a custom grid for LB-SAC due to computational constraints.For a comprehensive overview of the experimental details and hyperparameters, refer to Appendix A andAppendix C. The evaluation protocol is also taken from Tarasov et al. (2024a), where hyperparameters search is done usingfour random seeds and separate set of seeds is used for the evaluation with the exception that we utilizedten random seeds for the final evaluation for ReBRAC and IQL and only four seeds for LB-SAC, due tocomputational constraints. Note, that because of the chosen evaluation protocol tuned hyperparametersmight perform worse than non-tuned which characterizes the sensitivity to the random initialization.",
  "Is classification plug-and-play?": "Our initial goal was to substitute MSE with cross-entropy without modifying any other aspects of thealgorithms. We fixed the number of bins m to 101, representing a reasonable number of classes. We set/ = 0.75 by default as was proposed by Farebrother et al. (2024). Results for this modification are presentedin the CE columns of for Gym-MuJoCo, for AntMaze, and for Adroit. We alsoprovide rliable (Agarwal et al., 2021) metrics which support all of the further claims in a more readableway in Appendix I. It is worth noting that, for ReBRAC in the AntMaze tasks here and further, we used",
  "large-batch optimization, similar to that used in Gym-MuJoCo, which was previously hindered by Q-functiondivergence": "The original ReBRAC algorithm exhibited minimal performance variation with the introduction of cross-entropy in Gym-MuJoCo tasks, except for the random dataset where a notable performance drop was observed.However, on average, the score remained relatively stable. In the case of AntMaze, where ReBRAC facedQ-function divergence issues, the introduction of classification mitigated this problem, resulting in notableimprovement in average performance. However, in Adroit, where ReBRAC struggled with overfitting due tosmall dataset sizes, classification did not alleviate this issue and led to decreased performance across mostscenarios. In contrast, both IQL and LB-SAC experienced a significant performance drop in Gym-MuJoCo tasks. IQLsperformance in AntMaze dramatically decreased, rendering the algorithm unable to solve medium and largetasks with this modification. Meanwhile, LB-SACs performance did not change significantly for AntMaze butimproved for the umaze task. Notably, in the Adroit task, IQLs performance was significantly boosted in thepen environment, with relatively minor changes observed in other scenarios. However, LB-SACs performancedropped across most tasks. Our primary hypothesis for explaining the successful application of classification with ReBRAC and itsfailure with IQL and LB-SAC is that ReBRAC heavily relies on policy regularization. Consequently, pluggingclassification into Q networks does not strongly affect its offline RL component compared to the impactobserved in IQL or LB-SAC. : Average normalized score over the final evaluation and ten (four for LB-SAC) unseen random seedson Gym-MuJoCo tasks. \"hc\" stands for \"halfcheetah\", \"hp\" stands for \"hopper\", \"wl\" stands for \"walker2d\", \"r\"stands for \"random\", \"m\" stands for \"medium\", \"e\" stands for \"expert\", \"me\" stands for \"medium-expert\", \"mr\"stands for \"medium-replay\", \"fr\" stands for \"full-replay\". denote stds across random seeds. MSE denotesoriginal algorithm implementation, CE denotes the replacement of MSE with cross-entropy, CE+AT denotescross-entropy with tuned algorithm parameters, CE+CT denotes cross-entropy with tuned classificationparameters2. ReBRAC original scores are taken from Tarasov et al. (2024a).",
  "Algorithms hyperparameters search with classification": "The subsequent step in our investigation involved determining whether better performance could be achievedby tuning the hyperparameters of the original algorithms. The evaluation of the best hyperparameter setscan be found under the CE+AT columns in , , and (see also Appendix I). In Gym-MuJoCo tasks, these hyperparameter adjustments yielded slight improvements in ReBRACs averageperformance, although random datasets remained problematic. Similarly, in Adroit tasks, performance saw",
  "Avg76.889.488.390.160.816.516.217.03.06.86.09.5": ": Average normalized score over the final evaluation and ten (four for LB-SAC) unseen random seedson Adroit tasks. \"ham\" stands for \"hammer\", \"rel\" stands for \"relocate\", \"h\" stands for \"human\", \"c\" stands for\"cloned\", \"e\" stands for \"expert\". denote stds across random seeds. ReBRAC original scores are taken fromTarasov et al. (2024a).",
  "Avg58.657.856.559.037.150.358.342.531.620.423.420.9": "some improvement but still fell short of the original algorithms performance. Notably, for AntMaze tasks, thehyperparameter adjustments did not improve performance in this domain when compared to just pluggingcross-entropy. For IQL and LB-SAC, tuning the algorithms hyperparameters with a cross-entropy objective helped alleviateunderperformance in Gym-MuJoCo tasks, although random datasets remained a common issue. However, thisapproach did not yield significant improvements for AntMaze tasks, and only marginally improved averageperformance in Adroit. Following the methodology outlined by Kurenkov & Kolesnikov (2022), we investigated whether algorithmsutilizing classification offered superior hyperparameter search under uniform policy selection on D4RL tasks,using Expected Online Performance (EOP). The results are presented in under the CE+AT rows. Itis evident that for ReBRAC, classification facilitated much better hyperparameter search in the AntMazedomain, marginally improved search for Gym-MuJoCo, and showed slight degradation for Adroit tasks. IQLbenefited only in the Adroit domain, while LB-SAC showed a slight improvement in AntMaze tasks.",
  "What is the impact of classification parameters?": "In Farebrother et al. (2024), the authors conducted a limited study on the influence of specific classificationhyperparameters, only examining if / = 0.75 was suitable across varying numbers of bins m, using onlineRL tasks. Drawing inspiration from their work, we selected a set of m values: 21, 51, 101, 201, 401, and a setof / values: 0.55, 0.65, 0.75, 0.85. Subsequently, we conducted experiments using all possible pairs of theseparameters, while maintaining the parameters of the original algorithms.",
  "Gym-MuJoCo": "ReBRAC62.0 17.170.6 9.973.3 5.574.8 2.175.6 0.875.8 0.675.9 0.676.0 0.5ReBRAC+CE+AT64.1 15.471.7 8.774.0 4.875.4 1.976.2 1.176.6 1.076.7 0.976.8 0.8ReBRAC+CE+CT78.4 1.479.2 1.279.7 1.080.1 0.780.5 0.580.6 0.480.7 0.480.7 0.3ReBRAC+CE+MT62.0 15.570.1 10.573.3 6.675.3 2.976.4 0.976.6 0.576.7 0.3- IQL62.3 9.867.6 6.069.5 3.870.9 1.971.6 0.771.8 0.471.9 0.371.9 0.3IQL+CE+AT55.7 4.258.1 3.659.3 2.860.5 1.861.4 0.961.7 0.761.8 0.661.8 0.6IQL+CE+CT58.4 7.262.5 5.664.4 4.366.1 2.667.2 1.367.6 1.067.8 0.967.9 0.9IQL+CE+MT65.2 3.667.2 2.768.1 2.068.9 1.269.5 0.669.7 0.469.7 0.3- LB-SAC52.5 13.859.5 8.362.0 5.564.1 3.8----LB-SAC+CE+AT48.9 11.354.4 7.056.4 5.258.3 4.1----LB-SAC+CE+CT63.4 3.765.4 2.466.2 1.766.9 1.067.3 0.6---LB-SAC+CE+MT44.4 11.150.5 7.953.0 5.455.0 2.956.3 1.556.8 1.257.0 1.0-",
  "AntMaze": "ReBRAC67.9 10.073.6 7.476.1 5.578.3 3.479.9 1.780.4 1.1--ReBRAC+CE+AT84.8 4.987.5 3.588.7 2.589.7 1.590.4 0.890.7 0.5--ReBRAC+CE+CT83.8 10.388.3 4.789.4 2.290.0 0.990.4 0.690.6 0.490.7 0.490.7 0.3ReBRAC+CE+MT63.5 33.079.8 22.385.8 13.789.2 5.290.8 1.491.2 0.991.3 0.7- IQL21.1 5.424.1 4.725.7 4.227.5 3.629.5 2.730.4 2.030.7 1.730.9 1.5IQL+CE+AT15.6 0.916.1 0.816.4 0.716.7 0.717.0 0.617.2 0.517.3 0.417.3 0.4IQL+CE+CT16.2 0.716.6 0.716.8 0.717.1 0.717.5 0.617.7 0.617.8 0.517.8 0.5IQL+CE+MT23.3 9.328.1 9.931.2 9.635.1 8.339.2 5.140.6 3.041.0 2.3-",
  "Adroit": "ReBRAC44.1 18.453.2 10.956.1 6.157.8 2.358.6 0.958.9 0.759.0 0.759.1 0.6ReBRAC+CE+AT43.9 17.152.7 10.355.6 6.057.5 2.558.5 0.958.7 0.658.8 0.658.9 0.5ReBRAC+CE+CT56.9 1.757.9 1.358.4 1.058.8 0.759.2 0.459.3 0.359.4 0.359.4 0.2ReBRAC+CE+MT40.8 17.049.8 12.053.5 8.156.3 4.157.8 1.458.0 0.658.1 0.4- IQL33.9 2.335.2 1.735.8 1.536.4 1.237.1 0.937.4 0.637.5 0.637.5 0.5IQL+CE+AT53.0 3.555.0 3.156.1 2.757.2 1.958.1 0.858.3 0.458.4 0.358.4 0.3IQL+CE+CT49.6 1.350.4 0.950.7 0.751.0 0.651.3 0.451.4 0.451.5 0.351.5 0.3IQL+CE+MT54.2 3.155.9 2.356.7 1.757.4 1.057.8 0.357.8 0.157.8 0.1- LB-SAC15.7 14.123.4 12.827.7 10.531.7 6.6----LB-SAC+CE+AT12.7 8.017.1 5.919.1 4.320.6 2.3----LB-SAC+CE+CT22.0 2.723.4 2.624.2 2.725.2 2.626.4 2.2---LB-SAC+CE+MT12.5 8.817.3 6.519.4 4.621.1 2.622.3 1.522.8 1.223.0 1.1- We present algorithms performance heatmaps for the parameter grid averaged over domains in , withdetailed scores per dataset available in Appendix J. Additionally, in Appendix F, we provide further insightsby fixing one parameter and averaging over the second. Our analysis indicates that setting m = 101 and/ = 0.75 may serve as a reasonable starting point and produce results better than the average. Notably,",
  ": Heatmaps for the impact of the classification parameters averaged over the domains. SeeAppendix J and Appendix F for more results": "Farebrother et al. (2024) did not investigate the impact of the vmin and vmax choices, nor did they propose amethod for selecting these when the reward function is unknown. In our previous experiments, we computedthese values by taking the minimum and maximum return across all possible sub-trajectories in the dataset.However, this approach may not be optimal because offline RL algorithms often introduce pessimism into theQ function. Additionally, setting these limits based on extreme values may cause the values on the edges ofthe support to differ from other values from the models perspective, potentially impacting the final results. To evaluate the effect of these parameters, we computed the support size as vmax vmin from the datasetand multiplied this size by a parameter vexpand. We then extended the support in two ways: Subtractingvexpand(vmax vmin) from vmin (referred to as min), and simultaneously subtracting from vmin and addingto vmax the term vexpand(vmax vmin)/2 (referred to as both)4. The experimental results, shown in ,demonstrate that our initial choice of support parameters was sub-optimal and increasing the support rangemay strongly benefit. Surprisingly, in some cases, reducing the support was beneficial. The both strategygenerally yielded better performance. Our observations support the assumption about efficiency of cross-entropy for offline RL algorithms withpolicy regularization compared to other types. Notably, ReBRAC exhibited less sensitivity to the classificationparameters.",
  "Do MLPs scale better with classification?": "In Farebrother et al. (2024), authors emphasized the enhanced scalability of models when regression isreplaced with classification, particularly when using Transformers or ResNets. In this subsection, we delveinto whether similar scalability benefits apply to Multilayer Perceptrons (MLPs), which are commonly utilizedin the development of novel RL approaches. Experimental results are presented in . Surprisingly,our findings suggest that there is no consistent improvement in terms of scaling when MSE is replaced withcross-entropy. This discrepancy from the results reported by Farebrother et al. (2024) could be attributed to the fundamentalarchitectural differences between MLPs and models like Transformers or ResNets.Unlike these latterarchitectures, vanilla MLPs typically lack residual connections (He et al., 2016), which have been identifiedas a crucial factor in enabling effective scaling with depth. Consequently, the absence of such connections inMLPs may limit their ability to capitalize on the benefits offered by classification over regression. Number of additional critic layers Average score Gym-MuJoCo ReBRACReBRAC+CEIQLIQL+CE Number of additional critic layers Average score AntMaze ReBRACReBRAC+CEIQLIQL+CE Number of additional critic layers Average score",
  "Combining the findings": "We investigated whether a mixed tuning strategy for hyperparameters could enhance classification performance,building on our previous findings. For each algorithm, we tuned the following parameters: m from the set{201, 401}, vexpand from the set of {0.05, 0.05, 0.1} using the both strategy, and three values for one ofalgorithm-specific parameter: 1 for ReBRAC, IQL for IQL, and N critics for LB-SAC. For ReBRAC andIQL, the second parameter was held constant across all tasks (see subsection A.1). / was set to 0.75. Toenhance readability, we provide detailed per-dataset results in a separate tables in Appendix H and EOPresults in under the +CE+MT rows (see also Appendix I). Per-dataset results indicate that the proposed strategy yields better performance in most cases, performingon par in others, with the exception of LB-SAC on Gym-MuJoCo. It makes this approach the best choicewhen no prior knowledge on optimal algorithm-specific parameters is available. EOP results reveal that this tuning strategy is particularly effective for IQL, especially under a low fine-tuningbudget. For ReBRAC, benefits are apparent only under a high fine-tuning budget. Conversely, this strategydid not perform well for LB-SAC. The suboptimal performance of LB-SAC can be attributed to the strongdependence on ensemble size; due to computational constraints, we were unable to use an ensemble size of 50,which is optimal for many datasets.",
  "Learned Q-functions analysis": "Training Step / 1000 Q-value ReBRAC antmaze-umaze-v2 ReBRACReBRAC+CEReBRAC+CE+ATReBRAC+CE+CTReBRAC+CE+MT Training Step / 1000 Q-value ReBRAC antmaze-medium-play-v2 ReBRACReBRAC+CEReBRAC+CE+ATReBRAC+CE+CTReBRAC+CE+MT Training Step / 1000 Q-value ReBRAC antmaze-large-play-v2 ReBRACReBRAC+CEReBRAC+CE+ATReBRAC+CE+CTReBRAC+CE+MT Training Step / 1000 Q-value ReBRAC antmaze-umaze-diverse-v2 ReBRACReBRAC+CEReBRAC+CE+ATReBRAC+CE+CTReBRAC+CE+MT Training Step / 1000 Q-value ReBRAC antmaze-medium-diverse-v2 ReBRACReBRAC+CEReBRAC+CE+ATReBRAC+CE+CTReBRAC+CE+MT Training Step / 1000 Q-value ReBRAC antmaze-large-diverse-v2 ReBRACReBRAC+CEReBRAC+CE+ATReBRAC+CE+CTReBRAC+CE+MT Training Step / 1000 Q-value IQL antmaze-umaze-v2 IQLIQL+CEIQL+CE+ATIQL+CE+CTIQL+CE+MT Training Step / 1000 Q-value IQL antmaze-medium-play-v2 IQLIQL+CEIQL+CE+ATIQL+CE+CTIQL+CE+MT Training Step / 1000 Q-value IQL antmaze-large-play-v2 IQLIQL+CEIQL+CE+ATIQL+CE+CTIQL+CE+MT Training Step / 1000 Q-value IQL antmaze-umaze-diverse-v2 IQLIQL+CEIQL+CE+ATIQL+CE+CTIQL+CE+MT Training Step / 1000 Q-value IQL antmaze-medium-diverse-v2 IQLIQL+CEIQL+CE+ATIQL+CE+CTIQL+CE+MT Training Step / 1000 Q-value IQL antmaze-large-diverse-v2 IQLIQL+CEIQL+CE+ATIQL+CE+CTIQL+CE+MT",
  ": Q-value functions behaviour for ReBRAC and IQL on AntMaze tasks. Shaded area demonstratesstandard deviation across ten random seeds": "Finally, in we present the dynamics of the Q-function learning for ReBRAC and IQL on AntMazedatasets, as they are of the most interest based on our results. For ReBRAC, modified versions have lowerQ-function values on average, but the learning pattern remains similar, except for CE+MT, where theQ-values are notably lower, and the shape of the curve is different. However, these differences in the Q-functionbetween CE+MT and CE+AT/CE+CT do not result in significant differences in average performance.For IQL, the plots demonstrate that the usage of classification leads to more optimistic Q-functions, with asignificant gap between MSE and others. Unfortunately, there is no consistent pattern. We cannot claim thata more optimistic Q-function is the core issue for IQL because, for example, CE+MT is sometimes aboveCE+CT, but this has no correlation with task performance.",
  "Limitations and Future Work": "Looking ahead, several avenues for future research emerge. Firstly, one promising direction involves examininghow classification affects the performance of offline algorithms in offline-to-online RL setup. Understandinghow classification objectives impact the transferability of learned policies to online settings could providevaluable insights into the practical applicability of classification usage. One limitation here, however, is thatdataset-specific finite return ranges may restrict policy generalization when transitioning to online tuning.This could require new design choices to handle potentially expanded return ranges in online scenarios. Second, our approach applies a relatively straightforward replacement of regression with classification, andfuture studies could explore more refined adaptations. Although this naive approach fits naturally forReBRAC and LB-SAC, for IQL we replaced the MSE with cross-entropy only in the Q-network, retainingthe expectile loss for the V-function. Expectile loss, as a more general form of MSE, is integral to IQLsperformance, and investigating whether it could be effectively adapted to a classification-based objective is anexciting direction. However, such a replacement is mathematically complex and requires additional research.For algorithms like ReBRAC and LB-SAC, which incorporate pessimism in the Q-function, it may also beworth exploring whether discretization should occur before or after the pessimism adjustment, rather thanafterward as in our study. Our findings also offer practical insights for the development of new offline RL algorithms. For example, ourresults with ReBRAC suggest that classification-based objectives could mitigate Q-function divergence, arecurring challenge in off-policy and offline RL (Van Hasselt et al., 2018). Moreover, the guidance we provideon hyperparameter choices for classification losses could assist RL practitioners and researchers in fine-tuningthese algorithms for better performance if they decide to incorporate classification loss. For instance, arecent modification of ReBRAC by Tarasov et al. (2024c) is based on our modification of it, leading tonear-resolution of the AntMaze domain in the D4RL benchmark suite. Another theoretically promising direction is leveraging classification-specifics for uncertainty estimation inoffline RL. For example, the entropy provided by the Q function could be used to incorporate pessimism intooffline RL algorithms.",
  "Conclusion": "In this study, we explored the impact of integrating classification objectives into offline RL algorithms.Our findings provide nuanced insights into the efficacy of classification in improving offline RL algorithmperformance. Initially, we examined whether classification objectives could be seamlessly integrated intoexisting algorithms without altering other aspects. While some algorithm with policy regularization (ReBRAC)demonstrated promising results across various tasks , other algorithms with implicit regularization (IQL) andalgorithm with Q function regularization (LB-SAC) faced challenges. And the only case when classificationhave high chances to bring improvement without much effort is the divergence of Q function with MSE loss. Next, we explored the impact of different hyperparameter tuning with classification objectives. Notably, weobserved performance improvements for ReBRAC, when tuning classification hyperparameters over algorithm-specific ones. Other results underscore the importance of carefully selecting both types of hyperparameters:algorithm-specific and classification-specific, when employing classification. Furthermore, our investigation into the scalability of MLPs with classification revealed mixed results.Contrary to previous findings with architectures like Transformers and ResNets, we did not observe consistentimprovements in scaling when using classification objectives with MLPs. This highlights the importance ofconsidering the architectural nuances of different models when assessing the potential benefits of classificationobjectives. In conclusion, our study underscores the need for a nuanced understanding of the interplay between algorithmdesign, task characteristics, and the integration of classification objectives in RL. While classificationholds promise in certain contexts, its efficacy is highly dependent on factors such as algorithm design andhyperparameter selection. Future research could further explore these nuances and develop approaches thatleverage classification objectives optimally across a diverse range of RL tasks and algorithms. Rishabh Agarwal, Max Schwarzer, Pablo Samuel Castro, Aaron C Courville, and Marc Bellemare. Deepreinforcement learning at the edge of the statistical precipice. Advances in neural information processingsystems, 34:2930429320, 2021. Dmitriy Akimov, Vladislav Kurenkov, Alexander Nikulin, Denis Tarasov, and Sergey Kolesnikov. Let offline rlflow: Training conservative agents in the latent space of normalizing flows. arXiv preprint arXiv:2211.11096,2022. Gaon An, Seungyong Moon, Jang-Hyun Kim, and Hyun Oh Song. Uncertainty-based offline reinforcementlearning with diversified q-ensemble. Advances in neural information processing systems, 34:74367447,2021.",
  "Marc G Bellemare, Will Dabney, and Rmi Munos. A distributional perspective on reinforcement learning.In International conference on machine learning, pp. 449458. PMLR, 2017": "Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter Abbeel, AravindSrinivas, and Igor Mordatch. Decision transformer: Reinforcement learning via sequence modeling. Advancesin neural information processing systems, 34:1508415097, 2021. Xi Chen, Ali Ghadirzadeh, Tianhe Yu, Yuan Gao, Jianhao Wang, Wenzhe Li, Bin Liang, Chelsea Finn, andChongjie Zhang. Latent-variable advantage-weighted policy optimization for offline rl. arXiv preprintarXiv:2203.08949, 2022. Jesse Farebrother, Jordi Orbay, Quan Vuong, Adrien Ali Taga, Yevgen Chebotar, Ted Xiao, Alex Irpan,Sergey Levine, Pablo Samuel Castro, Aleksandra Faust, et al. Stop regressing: Training value functions viaclassification for scalable deep rl. arXiv preprint arXiv:2403.03950, 2024.",
  "Alexander Nikulin, Vladislav Kurenkov, Denis Tarasov, and Sergey Kolesnikov. Anti-exploration by randomnetwork distillation. arXiv preprint arXiv:2301.13616, 2023": "Gregory Rogez, Philippe Weinzaepfel, and Cordelia Schmid. Lcr-net: Localization-classification-regressionfor human pose. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.34333441, 2017. Gregory Rogez, Philippe Weinzaepfel, and Cordelia Schmid. Lcr-net++: Multi-person 2d and 3d posedetection in natural images. IEEE transactions on pattern analysis and machine intelligence, 42(5):11461161, 2019.",
  "Rasmus Rothe, Radu Timofte, and Luc Van Gool. Deep expectation of real and apparent age from a singleimage without facial landmarks. International Journal of Computer Vision, 126(2):144157, 2018": "Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt,Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, et al. Mastering atari, go, chess and shogiby planning with a learned model. Nature, 588(7839):604609, 2020. David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, ThomasHubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go without humanknowledge. nature, 550(7676):354359, 2017. Jost Tobias Springenberg, Abbas Abdolmaleki, Jingwei Zhang, Oliver Groth, Michael Bloesch, ThomasLampe, Philemon Brakel, Sarah Bechtle, Steven Kapturowski, Roland Hafner, et al. Offline actor-criticreinforcement learning scales to large models. arXiv preprint arXiv:2402.05546, 2024. Lawrence Stewart, Francis Bach, Quentin Berthet, and Jean-Philippe Vert. Regression as classification:Influence of task formulation on neural network features.In International Conference on ArtificialIntelligence and Statistics, pp. 1156311582. PMLR, 2023. Denis Tarasov, Vladislav Kurenkov, Alexander Nikulin, and Sergey Kolesnikov. Revisiting the minimalistapproach to offline reinforcement learning. Advances in Neural Information Processing Systems, 36, 2024a. Denis Tarasov, Alexander Nikulin, Dmitry Akimov, Vladislav Kurenkov, and Sergey Kolesnikov. Corl:Research-oriented deep offline reinforcement learning library. Advances in Neural Information ProcessingSystems, 36, 2024b.",
  "Hado Van Hasselt, Yotam Doron, Florian Strub, Matteo Hessel, Nicolas Sonnerat, and Joseph Modayil. Deepreinforcement learning and the deadly triad. arXiv preprint arXiv:1812.02648, 2018": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ukasz Kaiser,and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30,2017. Rui Yang, Chenjia Bai, Xiaoteng Ma, Zhaoran Wang, Chongjie Zhang, and Lei Han. Rorl: Robust offlinereinforcement learning via conservative smoothing. Advances in Neural Information Processing Systems,35:2385123866, 2022.",
  "AExperimental Details": "For results with original algorithms and algorithms hyperparameters search in and ,we conducted a hyperparameter search and selected the best results from the final evaluations for each dataset.We used the JAX implementation of ReBRAC from the Clean Offline RL (CORL) library (Tarasov et al.,2024b) and used the same code template for IQL and LB-SAC. Algorithmic part of IQL implementation isbased on the original codebase from Kostrikov et al. (2021) and in case of LB-SAC we have adapted SAC-Nimplementation from",
  "The experiments were conducted on RTX Titan, Quadro RTX 6000 and Quadro RTX 8000": "Our study utilized the v2 version of datasets for Gym-MuJoCo and AntMaze, and v1 for Adroit. The agentswere trained for one million steps in all domains and evaluated over ten episodes for Gym-MuJoCo andAdroit and over one hundred episodes for AntMaze. Following Chen et al. (2022), AntMaze reward functionis multiplied by 100. For ReBRAC, we fine-tuned the 1 parameter with 0.001, 0.01, 0.05, 0.1 values and the 2 parameterwith 0, 0.001, 0.01, 0.1, 0.5 values for Gym-MuJoCo and Adroit. For AntMaze the corresponding rangesare 0.0005, 0.001, 0.002, 0.003 and 0, 0.0001, 0.0005, 0.001. When replacing regression with classification forAntMaze we also set batch size to 1024 and learning rates to 0.001 as it is was done for Gym-MuJoCo inoriginal algorithm which slightly improves the performance."
}