{
  "Abstract": "Invariant causal prediction (ICP) is a popular technique for finding causal parents (directcauses) of a target via exploiting distribution shifts and invariance testing (Peters et al.,2016). However, since ICP needs to run an exponential number of tests and fails to identifyparents when distribution shifts only affect a few variables, applying ICP to practical largescale problems is challenging. We propose MMSE-ICP and fastICP, two approaches whichemploy an error inequality to address the identifiability problem of ICP. The inequalitystates that the minimum prediction error of the predictor using causal parents is the smallestamong all predictors which do not use descendants. fastICP is an efficient approximationtailored for large problems as it exploits the inequality and a heuristic to run fewer tests.MMSE-ICP and fastICP not only outperform competitive baselines in many simulationsbut also achieve state-of-the-art result on a large scale real data benchmark.",
  "Introduction": "Causal discovery (CD) can offer insights into systems dynamics (Pearl, 2018) which are helpful for influencingsome outcomes (e.g. having diseases or not) or creating robust ML models. For example, a model basedon a targets causal parents can robustly predict the target despite various distribution shifts. CD can beglobal or local: global CD searches for the complete causal graph while local CD only searches for causalrelations surrounding a specific target Y . Thus, global CD is often intractable as the search domain growsexponentially with the number of variables.Local CD is more tractable and is sufficient if the goal isto change Y through interventions or to build domain-invariant ML models of Y . Recently, many workshave built on local CD to improve ML models out-of-distribution generalization (Rojas-Carulla et al., 2018;Magliacane et al., 2018; Arjovsky et al., 2019; Christiansen et al., 2021). Invariant causal prediction (ICP) is a local CD method that finds the causal parents of Y using the invarianceproperty: the distribution of Y conditioned on all of its causal parents will be invariant under interventionson variables other than Y (Peters et al., 2016). Specifically, ICP tests all subsets of variables for invarianceand outputs the intersection (denoted as SICP) of all invariant subsets. As the number of subsets growsexponentially, applying ICP to large problems (> 20 variables) is difficult. Besides, ICP is guaranteed toidentify all causal parents only when every variable in the system except Y is perturbed (Peters et al.,2016). However, in many practical problems (e.g. problems in cell biology with 20,000 variables), it is almostimpossible to perturb all variables (Uhler, 2024). With limited perturbation/intervention, ICP may fail toidentify causal parents (Rosenfeld et al., 2021; Mogensen et al., 2022), as disjoint subsets can be invariant,yielding an empty intersection (see ). Invariant ancestry search (IAS) addresses ICPs failure to identify causal parents by outputting the causalancestors of Y instead (Mogensen et al., 2022).The causal ancestors identified is the union (denoted",
  "EX2X1Y": ": Consider this directed acyclic graph (DAG) representing a noisy causal structural model as amotivating example, where no causal mechanism is noise-free and thus there are no duplicate variables. LetE denote the variable capturing the environment (or context) (Mooij et al., 2020). Its children are directlyaffected by intervention. This reflect distribution shifts between contexts. Y is the target variable. SICP = because the invariant sets are {X1}, {X2}, and {X1, X2}. In contrast, SIAS = {X1, X2}. Our methodsoutput {X1} since the prediction error of YM(X1) is less than YM(X2)s. as SIAS) of all minimally invariant subsets. A set is minimally invariant if none of its proper subsets isinvariant (Mogensen et al., 2022). Like ICP, SIAS can also be used to construct a stable predictor of Yacross environments even though it is not the most compact stable predictor. Besides, intervening on anancestor may not be as effective in influencing the value of Y as intervening on a parent. IAS can be spedup by only testing subsets up to a certain size at the cost of potentially finding fewer causal ancestors, thustrading accuracy for speed. We propose two approaches that employ an error inequality to address the identifiability and scalabilityproblem of ICP. The error inequality implies that the prediction error of the predictor using Y s causalparents is the smallest among all predictors which do not use Y s descendants. Thus, the first approach,MMSE-ICP (short for minimum mean squared error ICP), finds invariant sets that do not contain Y sdescendants and outputs the invariant set with the smallest error for predicting Y . The second approach isfastICP, an efficient approximation that relies on a heuristic and constrained search scheme (see .4).fastICP also exploits the error inequality to reduce the number of tests for invariance to identify the causalparents. Consequently, fastICP can handle large-scale problems with numerous variables. Experiments onsimulated and real data show MMSE-ICP and fastICP outperforming competitive baselines.",
  "Background": "In different environments/settings, data are generated using different generative mechanisms. An interven-tion refers to perturbation of generative mechanism of a specific variable (Pearl, 2009). Observational settingindicates no perturbation. Interventional setting indicates one or more variables are perturbed. Experimen-tal data belong to interventional setting in which the interventions are known (which variables are perturbedand how are they perturbed). Most naturally occurring distribution-shift data also fall under interventionalsetting, however the interventions are unknown. Many CD methods use solely observational data.Constraint-based (Spirtes et al., 2000) and score-based (Chickering, 2002) methods can identify up to the Markov equivalent class (MEC) of the true graph,leaving some edges with unresolved direction. With additional assumptions (e.g. non-Gaussianity (Shimizuet al., 2006), nonlinearity (Hyvrinen & Pajunen, 1999; Hoyer et al., 2008; Zhang & Hyvrinen, 2009; Peterset al., 2014; Zhang et al., 2015; Rolland et al., 2022), or independent causal mechanisms (Janzing et al.,2012)), methods are able to resolve more edges (Glymour et al., 2019). Optimization-base methods (Zhenget al., 2018; Geffner et al.) have become popular recently despite the lack of identifiability guarantees (Mo-gensen et al., 2022). Having data from multiple settings can improve identifiability. When interventions are known, undirectededges in the MEC first estimated using observational data can be resolved using interventional data (He &Geng, 2008). Other approaches jointly model interventional and observational data (Hauser & Bhlmann,2012; 2015; Wang et al., 2017). There are also optimization-based methods that leverage both types ofdata (Lorch et al., 2021; Lippe et al., 2022). In contrast, UT-IGSP (Squires et al., 2020), SDI (Ke et al., 2023),and DCDI (Brouillard et al., 2020) are methods that can work with unknown interventions. ICP (Peterset al., 2016; Pfister et al., 2019; Gamella & Heinze-Deml, 2020; Martinet et al., 2022) and IAS (Mogensenet al., 2022) both assume unknown interventions and only depends on the invariance property to identifycausal structures. Our approaches also assume unknown interventions.",
  "Published in Transactions on Machine Learning Research (09/2024)": "Guillaume G Martinet, Alexander Strzalkowski, and Barbara Engelhardt. Variance minimization in thewasserstein space for invariant causal prediction. In Proceedings of AISTATS, pp. 88038851. PMLR,2022. Nicolai Meinshausen, Alain Hauser, Joris M Mooij, Jonas Peters, Philip Versteeg, and Peter Bhlmann.Methods for causal inference from gene perturbation experiments and validation.Proceedings of theNational Academy of Sciences, 113(27):73617368, 2016.",
  "Definitions and assumptions": "We represent causal relations between variables using a Directed Acyclic Graph (DAG), where each nodeis a variable and directed edges between nodes represent direct causal influence (Pearl, 2009). We use theusual notations from graphical models (Lauritzen, 1996). Specifically, PA(Y ), CH(Y ), AN(Y ), DE(Y ), andND(Y ) denote the parents, children, ancestors, descendants, and non-descendants of Y respectively. Thus,PA(Y ) AN(Y ) ND(Y ) and CH(Y ) DE(Y ). There is an additional node E in the graph to denotethe different environments (or contexts) (Mooij et al., 2020). A set of variables/predictors S is invariantif Y E|S. As is common in the causal inference literature, we assume (1) no hidden confounder, (2) nointervention on Y , (3) no feedback between variables, (4) independence of mechanisms, and (5) independentadditive noise structural causal models (SCMs), with no noise-free mechanisms and duplicate variables. Wefurther assume that (6) the additive noise variables in the SCM have unbounded support (e.g. Gaussiannoise). Like prior work (Mogensen et al., 2022; Mooij et al., 2020), we assume that (7) E is exogenous (i.e.,E has no parent).",
  "S MI S. Although Peters et al. (2016) proposed two": "different tests for invariance, H0,S, the approximate test based on residuals of a predictor (Method II) isusually used because of its speed. Specifically, this statistical test checks whether the distribution (meansand variances) of the predictors errors across environments E are the same. The predictor is fitted usingthe data from all environments (see Appendix A for more details).",
  "Finding causal parents by minimizing error": "Instead of taking the intersection or union of invariant subsets, we rely on the concept of minimum meansquared error predictor to find the causal parents. The mean squared error (MSE) of a predictior Y (X)with input variables X of target Y is EXY (Y Y (X))2, where E denotes expectation. Let the minimumMSE achieved by an optimal predictor YM(X) with input variables X be MMSE(X).Note that this isalways with respect to predicting a target variable Y . It is well-known that, if the minimization is uncon-strained, the MMSE(X) is the conditional expectation EY |X(Y |X), and the corresponding MMSE is equalto EXVarY |X(Y |X) (Leon-Garcia, 1994), where Var denotes variance. In practice, we will assume that a",
  "MMSE(S) > MMSE(PA(Y )), if and only if PA(Y ) S": "Proof. The first inequality is an immediate corollary of Lemma 3.1. Since S ND(Y ), it follows from theCausal Markov condition that Y S|PA(Y ). By Lemma 3.1, MMSE(S PA(Y ))MMSE(S). Furthermore,also by Lemma 3.1, MMSE(PA(Y ) S)=MMSE(PA(Y )). Hence, MMSE(S)MMSE(PA(Y )). It is easy to show that, in the assumed DAG, MMSE(PA(Y )) = 2Y , where 2Y is the variance of theindependent additive noise for Y .Furthermore, for any S which is a subset of ND(Y ), MMSE(S) =ESVarY |S(fY (PA(Y ))|S) + 2Y , where fY (PA(Y )) is the functional mapping from Y s parents to Y in theSCM. It can be shown that in the assumed noisy SCM where the noise distribution has unbounded support(e.g. Gaussian noise), VarY |S(fY (PA(Y ))|S) > 0 if and only if PA(Y ) S and VarY |S(fY (PA(Y ))|S) = 0 ifand only if PA(Y ) S.",
  "Combining MMSE and invariance test": "Algorithm 1 combines the error inequality with the invariance test to find causal parents. Theorem 3.4shows that Algorithm 1 can identify all causal parents of Y as long as PA(Y ) DE(E). This implies thatAlgorithm 1 usually requires fewer interventions than the number of variables for complete identification. Forexample, a single intervention at an upstream variable that affects all causal parents of Y is sufficient. This ismuch more favorable than ICP, which may need as many interventions at as the number of variables (Peterset al., 2016).",
  ". X is not on any blocking path so S1 \\ {X} still blocks all paths between E and Y": "2. X is on a blocking path between E and Y . Since E has no parent (see .1), X DE(E) DE(Y ). Thus, this blocking path is out-going from Y . Removing all nodes on this path will keepthe path blocked. The removed nodes are descendants of Y so they are not in DE(E) ND(Y ).",
  "Theorem 3.4 (Identifiability). Given that all tests of invariance return the correct results and PA(Y ) DE(E), Algorithm 1 will always find the set of causal parents PA(Y )": "Theorem 3.4 follows from Corollary 3.2 and Lemma 3.3. The loop in Algorithm 1 finds invariant subsets ofDE(E) ND(Y ) which include PA(Y ) when PA(Y ) DE(E). Amongst these subsets, PA(Y ) has minimumMSE and is the most compact. The solution is unique (no other subset has the same MSE and cardinality)because other subsets with minimum MSE are all strict supersets of PA(Y ) so they have larger cardinalitythan PA(Y ). Lemma 3.3 implies that for any invariant set S1 that is not a subset of DE(E) ND(Y ), there exists aninvariant set S2 that is a subset of S1 (having smaller cardinality than S1s) and of DE(E) ND(Y ). SinceAlgorithm 1 iterates through the sets of variables in increasing cardinality order, it will first find S2 and addS2 to the list. It will later exclude S1 because S2 is a subset of S1. Thus, all invariant sets that are notsubsets of DE(E) ND(Y ) will be excluded. PA(Y ) will always be in the list because no subset of PA(Y ) isinvariant.",
  ": Performance when Nint = d = 6 (, No. 1). Linear simulation. Reference set: PA(Y )": "Stage 1 aims to find the largest invariant set S that includes PA(Y ). Starting with the set of all variables,it removes potential colliders and their descendants, which might be unblocking paths from E to Y , usingthe heuristic presented in Cheng et al. (1998). The heuristic assumes that blocking a path by removingsome nodes from the conditioning set decreases the statistical dependency between E and Y (Cheng et al.,1998). In Algorithm 2, MaxDepth is a hyper-parameter for the maximum number of nodes to be removedat one time. The statistical dependency can be measured using the invariance test of Peters et al. (2016)where the lower the statistical dependency, the higher the probability of being invariant. Stage 2, based onProposition 3.5, removes variables from S one-by-one while maintaining invariance. The algorithm terminateswhen S = PA(Y ). Proposition 3.5. We are given that PA(Y ) DE(E), S is invariant and has the lowest MMSE amongstsubsets with the same cardinality, and PA(Y ) S. For each Z S, let SZ := S \\ {Z}. If no subset SZof S is invariant, then S = PA(Y ). Otherwise, PA(Y ) arg mininvariant SZ MMSE(SZ). Proof. If no subset SZ of S is invariant, all nodes of S must be on blocking paths because removing anynode Z from S leads to loss of invariance (unblocks a path from E to Y ). If multiple nodes are on the sameblocking path, one node (e.g. W) can be removed and SW is still invariant which is contradictory. Thus,Z PA(Y ) because otherwise, exchanging Z with the parent in the same blocking path would result in a",
  "subset with the same cardinality but with smaller MMSE, which is contradictory. Since Z PA(Y ), Z S,then S = PA(Y )": "SupposethereexistsaZSwhereSZisinvariant.LetsassumethatPA(Y )SZ:= arg mininvariant SZ MMSE(SZ).Since PA(Y ) SZ but PA(Y ) S so Z PA(Y ). Besides, SZ is invariant but Z / SZ, so theremust be another node T SZ that is on the same blocking path as Z.Let S:=S \\ {Z, T}. Consequently, SZ = S \\ {Z} = S {T} and ST = S \\ {T} = S {Z}.Since T and Z are on the same blocking path, ST is also invariant. In addition, because Z PA(Y ), Tmust precede Z, i.e. Y T|S {Z} Y T|ST . As a result, by Lemma 3.1 and Corollary 3.2,",
  "Complexity Analysis": "For a problem with V variables, the complexity of ICP and MMSE-ICP is O(2V ) as the number of invariancetests run is exponential. Stage 1 of fastICP is O(V 2MaxDepth) because of the number of candidates checkedgrows exponentially with MaxDepth. Stage 2 is O(V 2). Thus, the complexity of fastICP is O(V 2MaxDepth +V 2). When MaxDepth is less than V , fastICP is faster than ICP.",
  "Data": "We apply our methods to a real-world large-scale yeast gene expression dataset with 6170 variables (Kem-meren et al., 2014). There are 160 observational samples and 1479 interventional samples. Each interven-tional sample corresponds to an experiment where single Xk has been perturbed. Following Peters et al.(2016); Mogensen et al. (2022), we assume that a direct causal effect XY exists (true positive) if the",
  "Baselines and implementation details": "ICP and IAS are used as baselines as they give confidence estimates for individual predicted par-ents/ancestors.The same linear invariance test is used although the threshold is set at = 0.01, fol-lowing Peters et al. (2016). L2-boosting (Friedman, 2001; Bhlmann & Yu, 2003; Hothorn et al., 2010) isused to estimate the MB of size 10 for ICP and MMSE-ICP. Even though fastICP can search exhaustivelyin simulations of 100 variables, it would be too slow for this problem. Hence, we restrict the fastICP searchscope to an estimated MB of 100 variables. ICP-CORTH is excluded because it takes more than 3 daysfor one gene so obtaining the result for 6170 genes would take more than 18000 days. Instead scoring themethods output at fixed thresholds, we ranked the set of predicted causal relations and score the mostconfident predicted relations.",
  "Results from linear simulations": "When, Nint = d = 6, invariance-based algorithms should be able to discover all parents. showstheir results in this setting.With sufficient samples, invariance-based algorithms outperform fGES-MB(observational constraint-based CD) and UT-IGSP. When the number of interventions is limited (i.e. Nint <d, , No. 26), identifying all direct parents using invariance is more challenging since some invariancesubsets may be strict subset of the parents, i.e. |S| < |PA(Y )|.Due to the strict inequality, we onlybenchmark our approaches against invariance-based algorithms. For perfect interventions, MMSE-ICP andfastICP achieve similar performance and outperform the baselines in both Jaccard similarity and F1-score().The recall of our approaches is the same as IASs and higher than ICPs. shows",
  "Computation time analysis": "To analyze the runtime complexity of invariance-based algorithms, we recorded the time each algorithm tookwhen the number of nodes (d) varies. The data were generated from linear SCMs as outlined in .1,but with varying d and N (number of samples) fixed at 1000. reports the numbers of seconds elapsedwhen executing on an AMD EPYC 7642 CPU core (@ 2.3GHz). For this benchmark, since the official ICPimplementation was in R, we employed a re-implementation in Python for a fair comparison. The algorithmssearch through the full set of covariates (i.e. no pre-selection of variables using Markov Blanket estimation).",
  "Results from nonlinear simulations": "For nonlinear simulation, our approaches are still better than ICP ( and Appendix C.4). As predictedby Heinze-Deml et al. (2018), it is sometime possible to use the linear invariance test (invariance test withlinear regression) to find causal parents in nonlinear simulations. However, it is preferable to use a nonlinearinvariance test to obtain more accurate results if runtime is not a constraint.",
  "expression level of gene Y after intervening on gene X lies in the 1% lower or upper tail of the observationaldistribution of gene Y": "Since neither the ground-truth graph nor a separate set of validation data is available, we must use the samedata for validation. Hence, we employ the same cross-validation scheme used in the original ICP paper toprevent information leakage (Peters et al., 2016). Specifically, when predicting whether Xk is a parent of Y ,we do not include the sample when intervening on Xk (if the sample exists) in the data used for inference.The interventional samples are split into 3 folds. In each fold, two thirds of the interventional samples notcontaining Xk are used as interventional data, and remaining interventional data are used for validation.Thus, for each target, we need to run the algorithms the same number of times s the number of folds.Additionally, when looking for potential causes of Y , we exclude samples corresponding to intervention onY (if it exists).",
  "Results": "shows how the number of true positives vary as the methods are allowed to make more predictions.When the number of allowed predictions is lower than 8, the performance of the methods are very similar.However, when the number of predictions is more than 10, MMSE-ICP is generally more accurate than ICP.Moreover, fastICP is the best approach for this task as it can afford to search more widely for the parents of",
  "Discussion": "While using only observational data for causal discovery (CD) is common, interventional data with mech-anism changes are very valuable for inferring causal relations. In fact, changes (e.g. different equipments,locations, demographics, weathers) often arise naturally in distribution-shift data. Given their abundance,harnessing changes for CD would reveal insights useful for understanding and manipulating complex sys-tems. However, exploiting changes in general is difficult as they are often imperfect interventions (merelyaltering the mechanism generating a variable) affecting some unknown variables. In contrast, controlledexperiments are perfect interventions that fix designated variables to predetermined values (Tian & Pearl,2001; Eberhardt & Scheines, 2007). As such, invariance-based CD methods are appealing because they canwork with unknown interventions. Like other invariance-based CD methods, ours also make no assumption about where interventions are andwhat precisely the effect of interventions may be. We only assume that there is no intervention on thetarget. This make our approaches appealing in many problems whereby specifying what an intervention orchange of environment actually means is difficult. In addition, unlike ICP which needs a sufficient numberof interventions to identify all parents (Rosenfeld et al., 2021; Mogensen et al., 2022), our approaches canidentify all parents even if they are only indirectly effected by interventions. Thus, our methods can beapplied to problems where it is logistically challenging to exhaustively intervene on many variables (Uhler,2024). Besides, our work may have implications for robust representation learning. Building ML models withinvariant representations has become popular recently since they can generalize better to distribution-shiftdata (Arjovsky et al., 2019; Rosenfeld et al., 2021; Nguyen et al., 2024c;b). It may be possible to build evenmore accurate invariant representations by identifying causal variables (Subbaswamy et al., 2019). Thus, asan efficient way to find causal variables from distribution-shift data, fastICP can be a key to more adaptiveML models. In this work, we proposed two algorithms: MMSE-ICP which has similar runtime but better recall thanICP; and fastICP which is more scalable. Both approaches outperform multiple baselines in simulations.fastICP also achieves SOTA result on the large-scale Kemmeren et al. (2014)s benchmark. MMSE-ICP andfastICP are based on general theoretical results are orthogonal to the implementations of invariance test andMSE estimation. There are several unaddressed questions that are left as future work. First, despite its speed, fastICP still has worst-case exponential complexity that is controlled by theMaxDepth parameter.The larger the MaxDepth, the closer fastICP is to ICP (i.e. testing exponentialnumber of subsets). Hence, setting a smaller MaxDepth will result in greater speed-up, with an increasedrisk for inaccurate results. In our experiments, MaxDepth of 2 seems to be a good trade-off between accuracyand speed on the simulations and the real data. Of course, there may be adversarial case where MaxDepthparameter needs to be increased to yield correct results. However, these adversarial cases seem rare in realdata. Although additional speed-up can be achieved by testing multiple subsets at once using amortizationtechniques such as (Nguyen et al., 2024a), whether there is a general invariance-based polynomial-complexityalgorithm remains an open question. Second, our algorithms assume accurate invariance test and MMSE estimation. The theoretical analysisof the proposed algorithms performance under imperfect invariance test is challenging since unlike ICP,they are greedy algorithms that do not exhaustively test all subsets. Thus, the performance bounds areproblem-specific (dependent on the ground-truth graph) as the invariance test might fail along some greedyoptimization path even with perfect MMSE estimation. In practice, MMSE estimation may be inaccurateas well so we need to account for the probability of making a consequential error in ordering the estimatedMMSE values. MMSE estimation may be imperfect due to insufficient samples or misspecified model class.Ensuring good MMSE estimation is an empirical exercise that will require careful experimentation. Werecommend users to account for the uncertainty in the predicted MMSE via approaches like bootstrappingon the test data (Raschka, 2022) or double ML (Chernozhukov et al., 2018).",
  "A Hyvrinen and Petteri Pajunen. Nonlinear independent component analysis: Existence and uniquenessresults. Neural networks, 12(3):429439, 1999": "Dominik Janzing, Joris Mooij, Kun Zhang, Jan Lemeire, Jakob Zscheischler, Povilas Daniuis, BastianSteudel, and Bernhard Schlkopf. Information-geometric approach to inferring causal directions. ArtificialIntelligence, 182:131, 2012. Nan Rosemary Ke, Olexa Bilaniuk, Anirudh Goyal, Stefan Bauer, Hugo Larochelle, Bernhard Schlkopf,Michael C Mozer, Chris Pal, and Yoshua Bengio. Neural Causal Structure Discovery from Interventions.TMLR, 2023. Patrick Kemmeren, Katrin Sameith, Loes AL Van De Pasch, Joris J Benschop, Tineke L Lenstra, ThanasisMargaritis, Eoghan ODuibhir, Eva Apweiler, Sake van Wageningen, Cheuk W Ko, et al. Large-scalegenetic perturbations reveal regulatory networks and an abundance of gene-specific repressors. Cell, 157(3):740752, 2014.",
  "Shuai Yang, Hao Wang, Kui Yu, Fuyuan Cao, and Xindong Wu. Towards efficient local causal structurelearning. IEEE Transactions on Big Data, 8(6):15921609, 2021": "K Zhang and A Hyvrinen.Causality discovery with additive disturbances: An information-theoreticalperspective. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases,pp. 570585, 2009. K Zhang, Zhikun Wang, Jiji Zhang, and B Schlkopf. On estimation of functional causal models: generalresults and application to the post-nonlinear causal model. ACM Transactions on Intelligent Systems andTechnology (TIST), 7(2):122, 2015."
}