{
  "Chronos: Learning the Language of Time Series": "Abdul Fatir Ansari1, Lorenzo Stella1, Caner Turkmen1, Xiyuan Zhang3, Pedro Mercado1,Huibin Shen1, Oleksandr Shchur1, Syama Sundar Rangapuram1, Sebastian Pineda Arango4,Shubham Kapoor1, Jasper Zschiegner, Danielle C. Maddix1, Hao Wang1,5, Michael W.Mahoney2,6, Kari Torkkola2, Andrew Gordon Wilson2,7, Michael Bohlke-Schneider1, YuyangWang1{ansarnd, stellalo}@amazon.com1AWS AI Labs, 2Amazon Supply Chain Optimization Technologies, 3UC San Diego, 4University of Freiburg, 5RutgersUniversity, 6UC Berkeley, 7New York University",
  "Abstract": "We introduce Chronos, a simple yet effective framework for pretrained probabilistic timeseries models. Chronos tokenizes time series values using scaling and quantization intoa fixed vocabulary and trains existing transformer-based language model architectures onthese tokenized time series via the cross-entropy loss.We pretrained Chronos modelsbased on the T5 family (ranging from 20M to 710M parameters) on a large collection ofpublicly available datasets, complemented by a synthetic dataset that we generated viaGaussian processes to improve generalization. In a comprehensive benchmark consisting of42 datasets, and comprising both classical local models and deep learning methods, we showthat Chronos models: (a) significantly outperform other methods on datasets that werepart of the training corpus; and (b) have comparable and occasionally superior zero-shotperformance on new datasets, relative to methods that were trained specifically on them.Our results demonstrate that Chronos models can leverage time series data from diversedomains to improve zero-shot accuracy on unseen forecasting tasks, positioning pretrainedmodels as a viable tool to greatly simplify forecasting pipelines.",
  "Introduction": "Time series forecasting is an essential component of decision-making across various domains, including retail,energy, finance, healthcare, climate science, among others. Traditionally, forecasting has been dominated bystatistical models such as ARIMA and ETS. These have served as reliable tools, at least until the recent shifttowards deep learning techniques (Hyndman & Athanasopoulos, 2018; Benidis et al., 2022). This shift can beattributed to the availability of large and diverse time series data sources, and the emergence of operationalforecasting problems (Kolassa & Januschowski, 2019) that play to the strengths of deep forecasters, i.e., theability to extract patterns out of a large collection of time series. Despite their impressive performance, deepforecasters still operate in the standard regime of training and prediction on the same dataset. While therehave been works dedicated to transfer learning (Ye & Dai, 2018) and domain adaptation (Jin et al., 2022)for forecasting, the field has yet to converge on a unified, general-purpose forecasting model, a goal thatremains a beacon for time series researchers. The emergence of large language models (LLMs) with zero-shot learning capabilities has ignited interestin developing foundation models for time series. In the context of LLMs, this interest has been pursuedthrough two main avenues: directly prompting pretrained LLMs in natural language (Gruver et al., 2023; Equal contribution.Xiyuan Zhang and Sebastian Pineda Arango contributed to this work during their internships at AWS. Hao Wang, MichaelW. Mahoney, and Andrew Gordon Wilson hold concurrent appointments at Amazon and their corresponding universities, andthis paper describes work performed at Amazon.",
  "Published in Transactions on Machine Learning Research (10/2024)": ": MASE scores of different models for datasets in Benchmark II, comprising 27 datasets not seen by Chronosmodels during training. Models achieving the first, second, and third best scores have been highlighted. Scoresfor Chronos and task-specific models have been averaged over 3 random seeds. The aggregated relative score wascomputed as described in .4.",
  "Time Series TokenizationTrainingInference": ": High-level depiction of Chronos. (Left) The input time series is scaled and quantized to obtain a sequenceof tokens. (Center) The tokens are fed into a language model which may either be an encoder-decoder or a decoder-only model. The model is trained using the cross-entropy loss. (Right) During inference, we autoregressively sampletokens from the model and map them back to numerical values.Multiple trajectories are sampled to obtain apredictive distribution. For the development of a useful general-purpose time series forecasting model, the scarcity of publiclyavailable time series datasets, both in quantity and quality, is arguably more critical than the modelingframework. In addition to the comprehensive collection of public datasets we used to train Chronos, acentral aspect of our approach is the integration of data augmentation strategies, including TSMixup andKernelSynth. TSMixup randomly samples a set of base time series from different training datasets, andgenerates new time series based on a convex combination of them; KernelSynth uses Gaussian processesto generate synthetic time series by randomly composing kernel functions. These techniques address theinherent limitations of small training datasets in time series forecasting, enhancing model robustness andgeneralization. Our comprehensive evaluation across 42 datasets establishes Chronos as a benchmark for both in-domainand zero-shot forecasting, surpassing both traditional models and task-specific deep learning approaches.",
  "Background and Related Work": "Time series forecasting concerns using historical data from a quantity of interest (typically real-valued)to predict their future values. Formally, given a uniformly-spaced time series x1:C = [x1, . . . , xC], we areinterested in predicting the joint distribution of the next H steps, p(xC+1:C+H|x1:C). In this work, we focuson univariate forecasting, where the observations are scalars, i.e., xi R for all i. Time series forecasting can be addressed with a variety of different methods which can be broadly categorizedinto classical forecasting methods and deep learning methods. Classical forecasting methods such as ETS,ARIMA (Hyndman et al., 2008), Theta (Assimakopoulos & Nikolopoulos, 2000) fit a separate model to eachtime series independently (hence referred to as local models). In contrast, deep learning forecasting modelslearn across time series in a given dataset (and are called global models). These methods leverage advancesin deep learning, such as RNNs which are used by DeepState (Rangapuram et al., 2018), DeepFactor (Wanget al., 2019), DeepAR (Salinas et al., 2020), TimeGrad (Rasul et al., 2021), and transformers which areused by TFT (Lim et al., 2021) and PatchTST (Nie et al., 2023). Apart from the choice of architecture,these approaches differ in the way they model the target, with some modeling the density function whileothers directly predicting a set of quantiles (Wen et al., 2017; Gasthaus et al., 2019; Park et al., 2022).Nevertheless, not all models produce probabilistic forecasts: notably, models such as Informer (Zhou et al.,2021) and DLinear (Zeng et al., 2023) only produce point forecasts. Large language models (LLMs) have demonstrated impressive performance on various natural languageprocessing tasks (Brown et al., 2020; Chung et al., 2022; Touvron et al., 2023). Given a sequence of input to-kens, w1:k = [w1, . . . , wk], language models aim to predict the next token, wk+1, by modeling the conditionaldistribution, p(wk+1|w1:k). The tokens belong to a vocabulary, V, and may be characters, subwords (Sennrichet al., 2015), or words, depending on the tokenization scheme used. Most modern LLMs (Brown et al., 2020; Chung et al., 2022; Touvron et al., 2023) are based on the transformerarchitecture (Vaswani et al., 2017).The original transformer architecture is an encoder-decoder modeldesigned for machine translation. The encoder maps an input sentence of some language to a continuousrepresentation, and the decoder generates the translation token-by-token using the input representationand previously decoded tokens. Many popular language models, such as BART (Lewis et al., 2019) andT5 (Raffel et al., 2020; Chung et al., 2022), belong to this family. Another popular architecture for LLMs isdecoder-only, used in GPT-3 (Brown et al., 2020) and Llama 2 (Touvron et al., 2023), where the model onlyattends to tokens up to the current token. LLMs are typically trained on a very large corpus of text withtheir number of parameters ranging from millions (Raffel et al., 2020) to hundreds of billions (Chowdheryet al., 2023). We refer the reader to Zhao et al. (2023) for a recent survey on this area of research. LLM-based forecasters.Inspired by the success of pretrained LLMs, recent work has shown that LLMsare general pattern recognizers (Mirchandani et al., 2023) and several methods adapting LLMs to the timeseries domain have been developed. One line of work treats numerical time series data as raw text and directlyuses the pretrained LLMs with minimal or no fine tuning to forecast unseen time series. PromptCast (Xue &Salim, 2023) leverages pretrained LLMs for forecasting by transforming the time series data into text-based",
  "Chronos: A Language Modeling Framework for Time Series": "In this section we introduce Chronos, a framework adapting existing language model architectures andtraining procedures to probabilistic time series forecasting. While both language and time series are sequen-tial in nature, they differ in terms of their representation natural language consists of words from a finitevocabulary, while time series are real-valued. This distinction necessitates specific modifications to existinglanguage modeling frameworks, especially concerning tokenization, to make them applicable to time seriesdata. Nevertheless, since existing transformer models have excelled on language tasks, our design philosophyinvolves making minimal changes to the model architectures and training procedure.",
  "Time Series Tokenization": "Consider a time series x1:C+H = [x1, . . . , xC+H], where the first C time steps constitute the historicalcontext, and the remaining H represent the forecast horizon. Language models operate on tokens from afinite vocabulary, so using them for time series data requires mapping the observations xi R to a finite setof tokens. To this end, we first scale and then quantize observations into a fixed number of bins. Scaling.The scale of time series can differ significantly even within a single dataset. This poses optimiza-tion challenges for deep learning models. Therefore, individual time series are normalized to facilitate betteroptimization. In the case of Chronos, the goal of normalization is to map the time series values into asuitable range for quantization. A common normalization technique involves applying an affine transforma-tion to the time series, i.e., xi = (xi m)/s. Several popular normalization schemes, such as mean scaling,standard scaling and min-max scaling, can be obtained by appropriately choosing m and s. We opt for meanscaling, a method that has proven effective in deep learning models commonly used for practical time seriesapplications (Salinas et al., 2020; Rabanser et al., 2020), but other approaches are viable and only requireminimal changes. An attractive feature of mean scaling is that it preserves zero values in the time series,which are often semantically meaningful, such as zero sales for a product or zero solar energy generation atnight. Mean scaling normalizes individual entries of the time series by the mean of the absolute values inthe historical context. Specifically, this involves setting m = 0 and s = 1",
  "CCi=1 |xi|": "Quantization.The scaled time series x1:C+H = [x1, . . . , xC, . . . , xC+H], is still real-valued and cannotbe processed directly by language models. To convert these real values into discrete tokens, we employquantization. Formally, we select B bin centers c1 < . . . < cB on the real line, and B 1 edges bi separatingthem, ci < bi < ci+1, for i {1, . . . , B 1}.The quantization function q : R {1, 2, . . . , B}, anddequantization d : {1, 2, . . . , B} R, are then defined as",
  "andd(j) = cj,(1)": "respectively. The positioning of bin centers and edges can either be data-dependent or uniform (Rabanseret al., 2020).Quantile binning, a type of data-dependent binning, exploits the cumulative distributionfunction (CDF) of the training datapoints to construct bins such that approximately equal number of dat-apoints are assigned to each bin. In contrast, uniform binning selects uniformly-spaced bin centers withinthe interval [c1, cB] and the bin edges fall mid-way between the successive bin centers, i.e., bi = ci+ci+1 2fori {1, . . . , B 1}. Since the distribution of values for unseen downstream datasets can differ significantlyfrom the training distribution, we opt for uniform binning in our experiments, but other quantization tech-niques can be used. We refer the reader to Rabanser et al. (2020) for a detailed discussion on quantizationschemes for time series. A potential limitation of this approach is that the prediction range is restrictedbetween [c1, cB], making it theoretically infeasible to model time series with a strong trend. We explore thisfurther in a practical setting in .7. Apart from the time series tokens {1, 2, . . . , B}, we include two special tokens, commonly used in languagemodels, into the time series vocabulary, Vts: PAD and EOS. The PAD token is used to pad time series of different",
  "Objective Function": "As typical in language models, we use the categorical distribution over the elements of Vts as the outputdistribution, p(zC+h+1|z1:C+h) where z1:C+h is the tokenized time series. Chronos is trained to minimizethe cross entropy between the distribution of the quantized ground truth label and the predicted distribution.Formally, the loss function for a single tokenized time series (also accounting for EOS tokens) is given by,",
  "where p(zC+h+1 = i|z1:C+h) denotes the categorical distribution predicted by the model parameterized by. In practice, the loss is averaged over a batch of time series during training": "Note that the categorical cross entropy loss (Eq. 2) is not a distance-aware objective function, i.e., it does notexplicitly recognize that bin i is closer to bin i + 1 than to i + 2. Instead, the model is expected to associatenearby bins together, based on the distribution of bin indices in the training dataset.In other words,Chronos performs regression via classification (Torgo & Gama, 1997; Stewart et al., 2023). This is unliketypical probabilistic time series forecasting models, which either use parametric continuous distributionssuch as Gaussian and Students-t (Salinas et al., 2020) or perform quantile regression (Wen et al., 2017; Limet al., 2021). Opting for a categorical output distribution offers two key advantages. Firstly, it requires no modificationto the language model architecture or training objective, enabling the use of popular language modelinglibraries and the utilities they provide out of the box (Wolf et al., 2020). Secondly, it imposes no restrictionson the structure of the output distribution, allowing the model to learn arbitrary distributions, includingmultimodal ones. This flexibility proves especially valuable for a pretrained model, as time series datasetsfrom diverse domains may follow distinct output distribution patterns. Arguably, modeling the output as an ordinal variable would be more appropriate, since the output domain isobtained by discretizing the real line. In fact, regression models for ordinal variables have been extensivelystudied in the literature (McCullagh, 1980; Winship & Mare, 1984), including for neural networks andtransformer models (Cheng et al., 2008; Hu et al., 2021). Imposing the ordinal nature of the classes on topof the models, in similar ways to the mentioned literature, could be an interesting extension of this work.",
  "Forecasting": "Chronos models are probabilistic by design and multiple realizations of the future can be obtained byautoregressively sampling from the predicted distribution, p(zC+h+1|z1:C+h), for h {1, 2, . . . , H}. Thesesample paths come in the form of token IDs that need to be mapped back to real values and then unscaled",
  "Data Augmentation": "The quality and quantity of public time series data pales in comparison to the natural language processing(NLP) domain, which benefits from ample high-quality text datasets such as WikiText-103 (Merity et al.,2016), C4 (Raffel et al., 2020), and The Pile (Gao et al., 2020). This poses challenges for training modelsintended for zero-shot forecasting, which rely on large-scale time series data with diverse patterns. To addressthis issue, we propose enhancing the diversity of training data by generating mixup augmentations from realdatasets and supplementing training with synthetic data.",
  "TSMixup: Time Series Mixup": "Mixup (Zhang et al., 2017) is a data augmentation scheme proposed in the context of image classi-fication.It generates convex combinations of random image pairs and their labels from the trainingdataset, which alleviates issues such as memorization and overfitting in deep learning models.Exist-ing works (Carmona et al., 2021; Zhou et al., 2023b) have extended Mixup to the time series domain.",
  "i=1i x(i)1:l,(3)": "where x(i)1:l denotes the i-th scaled time series. The time seriesare scaled before mixing to ensure that time series with smalland large values are given equal importance in the mixing pro-cess. The combination weights, [1, . . . , k], are sampled from asymmetric Dirichlet distribution, Dir(), parameterized by thescalar concentration parameter . The complete pseudocodeof TSMixup can be found in Algorithm 1 in Appendix A. Intu-itively, TSMixup enhances the diversity of data by combiningpatterns from different time series. shows exampleaugmentations generated by TSMixup and illustrates how different patterns are mixed.",
  "KernelSynth: Synthetic Data Generation using Gaussian Processes": "While TSMixup improves pattern diversity, it may still prove insufficient for training a generalist timeseries model, especially when real data is limited. To further supplement the training dataset, we proposeKernelSynth, a method to generate synthetic time series using Gaussian processes (GPs). KernelSynth isinspired by the Automatic Statistician (Duvenaud et al., 2013), where a compositional search over a spaceof GP kernels is performed to explain the structure of a time series. We use the inverse of this process randomly compose GP kernels to generate new time series. GPs are distributions over functions defined by the mean function, m(t), and the positive definite kernel,(t, t), where t R is the domain.The kernel specifies a covariance function which defines the jointvariability of the function values at an arbitrary pair of points, (t, t), in the input domain. Diverse patternscan be generated by appropriately selecting the kernel. We constructed a kernel bank, K, of basis kernelsdefining fundamental time series patterns. These include linear kernels for trend, RBF kernels for smoothlocal variation, and periodic kernels for seasonalities found in typical time series frequencies. The final kernel,",
  "(a) KernelSynth(b) Synthetic samples from KernelSynth": ": (a) An illustration of KernelSynth, a Gaussian process (GP)-based synthetic time series generation method.Kernels are sampled from a kernel bank and then randomly combined using a binary operator ( or +). The resultantkernel is used in a GP prior to generate synthetic time series. Random samples from kernels at each step are shownin red and blue colors. (b) Example synthetic time series generated by KernelSynth. (t, t), is constructed by sampling j U{1, J} kernels from K with replacement and combining these kernelsvia random binary operations, + or . A synthetic time series is generated by drawing a sample of lengthlsyn from the GP prior, GP(m(t) = 0, (t, t)); see Algorithm 2 in Appendix A for details. depictsthis generative process used in KernelSynth, illustrating how time series with intricate patterns can arisefrom the composition of simple basis kernels.",
  "In this section, we present empirical results on commonly used benchmark datasets.First, we give anoverview of the datasets, training strategy, baselines, and evaluation metrics (.1-5.4)": "provides a high-level summary of the datasets and baselines used in our experiments. We then (a) evaluatethe performance of Chronos models in the in-domain and zero-shot settings against local models andtask-specific deep learning models (.5); (b) analyze the effect of various design choices such asmodel size, initialization, synthetic data proportion, context length, and vocabulary size on the performanceof Chronos models (.6); and (c) analyze the qualitative performance of Chronos models andhighlight their limitations (.7). We discuss our key findings in this section and relegate specificexperiment details to the appendices.",
  "Training Corpus and Protocols": "We selected T5 (Raffel et al., 2020) as the main architecture for Chronos in our experiments, since it isavailable in a variety of sizes, ranging from 16M (Tiny) to 11B (XXL) parameters (Tay et al., 2021). Wealso conducted experiments with the decoder-only GPT-2 model to demonstrate the applicability of theChronos framework to decoder-only models. In the following, we discuss the training configurations usedfor our main results (.5) and explore alternatives for some of the hyperparameters in .6. We trained T5 models of 4 sizes,2 namely, Mini (20M), Small (46M), Base (200M) and Large (710M), and theGPT-2 base model (90M), on 10M TSMixup augmentations (see .1) generated from the 28 trainingdatasets, with K = 3 in Algorithm 1, and 1M synthetic time series generated using Gaussian processes (see.2). Note that with this setup, original time series are adequately represented since they are includedin the TSMixup augmentations with probability 1/3.We sampled time series from the augmentationsand synthetic data in the ratio 9:1 during training. Each model is trained with an effective batch size of256 sequences, using distributed data parallelism and gradient accumulation, whenever necessary. Thesesequences were constructed by slicing random windows from the time series, and then scaling and quantizingthem into equal-sized bins within the interval [c1= 15, cB= + 15], as described in .1. We set thevocabulary size, Vts, to 4096, including the special tokens (PAD and EOS). The context length of the sequenceswas set to 512, the default for T5 models, and the prediction length was set to 64, a value greater than theprediction lengths of all tasks we consider in our evaluation. The models were optimized for 200K steps using the AdamW optimizer with a weight decay of 0.01. Thelearning rate was annealed linearly from its initial value of 0.001 to 0 over the training steps. The other modeland training hyperparameters were set to their defaults used in the transformers library (Wolf et al., 2020).We used an AWS EC2 instance with 8 A100 (40GB) GPUs to train all Chronos models, and we employedfaster floating point formats (TF32) and model compilation to speed up training. in Appendix Ereports the training time and the approximate cost of training Chronos models of different sizes.",
  "Evaluation Metrics": "Whenever possible,3 we evaluated models both in terms of their probabilistic and point forecast performance.We used the weighted quantile loss (WQL) to assess the quality of the probabilistic forecasts: the WQL isrelated to the continuous ranked probability score (CRPS, Gneiting & Raftery (2007))4 and is commonlyused to evaluate probabilistic forecasts (Gasthaus et al., 2019; Shchur et al., 2023). The WQL measuresthe compatibility between the predictive distribution and the ground-truth observation at a uniformly-spaced grid of quantile levels; we compute the WQL on 9 uniformly-spaced quantile levels {0.1, 0.2, . . . , 0.9}.Quantile forecasters such as TFT were directly trained on these quantile levels.For methods requiringsampling, we estimated the quantiles using 20 sample forecast paths. We used the mean absolute scalederror (MASE, Hyndman & Koehler (2006)) to evaluate the point forecast performance. The MASE is definedas the absolute error of the forecast scaled by the historical seasonal error of the time series, and was selecteddue to its favorable properties over other point forecasting metrics (Hyndman & Koehler, 2006). We used themedian forecast (0.5-quantile) for computing the MASE for the probabilistic forecasters. See Appendix Dfor a detailed discussion on the evaluation metrics. Since the magnitude of the evaluation metrics can vary across datasets, we adopt a different approach toaggregate scores than naive averaging. For each dataset, we compute the relative score of each model asthe models score divided by the score of a baseline model (here, Seasonal Naive). The relative scores areaggregated across all datasets using the geometric mean. The choice of the geometric mean is deliberate Fleming & Wallace (1986) show that the arithmetic mean can yield misleading conclusions in this context,and the geometric mean is provably the only meaningful way to aggregate such relative scores. Furthermore,the geometric mean is also not sensitive to the choice of the baseline, and the model ordering stays intactif another baseline is selected instead. We used Seasonal Naive due to its simplicity and popularity as aforecasting baseline. For models that failed or could not finish evaluation within the allotted time on certaindatasets, we used a relative score of 1, i.e., the baseline relative score, when aggregating the results. Weassign equal weights to all tasks during aggregation, reflecting real-world scenarios where datasets may havedifferent numbers of time series, frequencies, history and prediction lengths.",
  "Main Results": "In this section, we present our main results on 42 datasets, which comprise Benchmark I (15 datasets)and Benchmark II (27 datasets). Chronos models surpass classical statistical baselines, task-specific deeplearning models, and other pretrained models on the in-domain datasets (Benchmark I; see .5.1). Onthe zero-shot datasets (Benchmark II; .5.2), Chronos models comfortably outperform statisticalbaselines and other pretrained models, while performing on par with the best deep learning models trainedon these tasks. With an inexpensive fine-tuning regimen, our Chronos-T5 (Small) model achieves the topspot on Benchmark II, significantly outperforming all baselines. 3Some models (GPT4TS and ForecastPFN) only generate point forecasts and we only evaluate those.4Many existing works (Ansari et al., 2021; Rasul et al., 2023; Kollovieh et al., 2023) use CRPS and WQL synonymously.",
  "Model": "0.838 0.875 0.908 0.953 1.000 1.188 0.810 0.830 0.835 0.8430.847 0.8940.895 0.951 0.823 0.832 0.841 0.8500.852 0.962 0.876 0.907 1.291 Local ModelsTask Specific ModelsPretrained Models (Zero Shot)Pretrained Models (Other) : Performance of different models on Benchmark II, comprising 27 datasets not seen by Chronos modelsduring training. This benchmark provides insights into the zero-shot performance of Chronos models against localstatistical models, which fit parameters individually for each time series, task-specific models trained on each task,and pretrained models trained on a large corpus of time series data. Pretrained Models (Other) indicates that thezero-shot setting does not apply to these models as they were pretrained on some datasets in Benchmark II. Theprobabilistic (WQL) and point (MASE) forecasting metrics (lower is better) were normalized using the scores ofthe Seasonal Naive baseline and aggregated through a geometric mean to obtain the aggregated relative WQL andMASE, respectively. Results for Chronos and task-specific models (except GPT4TS) have been averaged over 3random seeds. Models producing point-forecasts (GPT4TS and ForecastPFN) are only compared based on MASE. Benchmark II consists of 27 datasets that were not used during Chronos models training (see in appendix B), i.e., this benchmark evaluates the zero-shot performance of these models. These datasetsbelong to diverse domains and frequencies, some of which are not even part of the training data, makingthis a challenging benchmark for Chronos.5 summarizes the results on Benchmark II in terms ofthe aggregated relative scores. This benchmark is clearly more challenging than Benchmark I (), asthe best models tend to offer lower improvements relative to the baseline. Nevertheless, despite never having seen these datasets during training, Chronos models significantly outper-form standalone local statistical models. On probabilistic forecasting (aggregate relative WQL), Chronosmodels achieve the 2nd to 4th spots, performing better than most task-specific models that have been trainedon these tasks. In terms of the point forecasting performance, Chronos-T5 (Large) places 2nd, surpass-ing most baselines, including the strong SCUM ensemble. Chronos models also significantly outperformother pretrained models such as Moirai-1.0-R, Lag-Llama, LLMTime, and ForecastPFN, and even GPT4TS,which fine-tunes a pretrained GPT-2 model on each dataset. Moirai-1.0-R obtains the best performance afterChronos, although the evaluation setup may have been advantageous for Moirai-1.0-R as many datasets inBenchmark II were part of its pretraining corpus. The raw WQL and MASE values for individual datasetssummarized in can be found in Tables 9 and 10 in Appendix E. 5From a rigorous standpoint, to prevent information leakage, the start time of any dataset within this category must beafter the timestamp of the last observation from the pretraining dataset and Benchmark I. Nevertheless, we consider the riskto be minimal given that the datsets bear no overlap beyond high-level conceptual categorization.",
  "Fine tuning.Motivated by the remarkable zero-shot perfor-mance of Chronos models, we conducted a preliminary in-vestigation into fine-tuning Chronos models individually ondatasets from Benchmark II": "We selected the Chronos-T5 (Small) model for this experi-ment due to its good zero-shot performance with a relativelylow training cost.We fine-tuned the model in a dataset-agnostic fashion with an initial learning rate of 0.001, annealedlinearly to 0 over 1000 steps. shows that fine-tuningsignificantly improves the aggregate performance of the modelon Benchmark II. The fine-tuned Chronos-T5 (Small) modelnow takes the top spot on Benchmark II overall, overtakingboth larger (zero shot) Chronos models and the best task-specific models. Notably, Chronos-T5 (Small) is not even themost accurate variant of Chronos on Benchmark II in thezero shot setting, suggesting that further improvements may be obtained by fine-tuning larger Chronos-T5variants.",
  "Analysis of Hyperparameters": "Here, we explore the effect of different design choices on the downstream model performance, beginningwith a comparison of different model sizes and initializations. We then analyze the effect of training steps,synthetic data proportion, context length, and vocabulary size, on the performance of Chronos-T5 (Small).We only vary the parameter of interest, keeping everything else fixed to the value used in the main results. Model size.We experimented with four model sizes ranging from 20M to 710M parameters.6 Unsurpris-ingly, the training loss improves with the model capacity, as shown in a. We also observe this trendin the downstream model performance it improves with the model size for both in-domain and zero-shotbenchmarks, as shown in b. These trends suggest that even larger models may improve performancefurther. However, we did not explore larger models due to slow inference times which would render themimpractical for real-world applications. Initialization.We investigated whether initializing Chronos models to the corresponding T5 languagemodels pretrained by Tay et al. (2021) on the C4 dataset (Raffel et al., 2020) has any impact on the trainingdynamics or the downstream performance. shows the training loss curve for models initializedrandomly and those initialized with language model weights. Notably, models initialized randomly tend toconverge to a lower training loss compared to their counterparts initialized with language model weights.For the larger models (Base and Large), models initialized with language model weights initially exhibit afaster decrease in training loss, but they ultimately converge to a higher final loss. Overall, these observations suggest that language model weights are not particularly remarkable in thecontext of time series forecasting and offer no improvement over random initialization. These conclusionsare further reinforced by which shows the downstream performance of models initialized withlanguage model weights against three randomly-initialized models of each size. Across all model sizes, theperformance of models initialized with language model weights either overlaps with or slightly underperformscompared to randomly initialized models. These results suggest that LLM initialization offers relatively littleadvantage in the context of time series forecasting, and instead random initialization may be the preferablechoice.",
  "Model Size": "0.55 0.60 0.65 0.70 0.75 0.80 0.85 0.90 0.95 Agg. Relative Score Benchmark (Metric) In Domain (WQL) Zero Shot (WQL) In Domain (MASE) Zero Shot (MASE) Random Init.Language Model Init. :Comparison of the in-domain andzero-shot performance (lower is better) of mod-els initialized with language model weights(marked as star) and three randomly initial-ized models (marked as circles) across differentmodel sizes. TSMixup augmentations.As described in .2, wetrained Chronos models on TSMixup augmentations ratherthan directly on the original time series. In this experiment,we investigate whether using TSMixup augmentations is ad-vantageous for downstream performance. a comparesthe performance of Chronos-T5 (Small, 46M) models trainedwith and without TSMixup augmentations. The model trainedon TSMixup augmentations obtains similar in-domain perfor-mance to the model trained without augmentations. However,the zero-shot performance improves when using TSMixup aug-mentations.This suggests that TSMixup enchances the di-versity of training data which leads to improved performanceon unseen datasets. a also shows that the zero-shotperformance obtains an additional boost with the inclusion ofsynthetic data. We investigate this further in the next experi-ment. Synthetic data proportion.We systematically exploredthe impact of KernelSynth on downstream model performance.We trained Chronos-T5 (Small, 46M)models with time series sampled from TSMixup augmentations and KernelSynth data in different ratios,ranging from 0% (i.e., trained solely on TSMixup augmentations) to 100% synthetic data.",
  "(b)": ": Loss of precision due to scaling and quantization. In (a), data consists of unit spikes every n = 10, 20, 50observations (top to bottom): the scale here is 1/n, hence the maximum representable value is 15/n. When 1 > 15/nthen the model cannot possibly capture the spikes appropriately (all but the top case), since their value is notrepresented accurately by tokens. In (b), data is a sine wave shifted up by = 1, 10, 50: the scale here is , and asthe variance of the signal becomes smaller and smaller relative to , the tokens precision decreases. some observations will fall out of the representable range. An example of this behaviour is with sparseseries, and as shown in a. On the other hand, very large values of s compared to the varianceresult in loss of precision: in the original space, tokens are spaced 30s/(B 1) from each other, where Bis the number of bins (we used B = 4094 in our experiments); values closer than that to each other maybe mapped to the same token, with an apparent loss of precision. An example of this behaviour is given inb. An inference-time heuristic solution to this problem is to preprocess the time series using analternative normalization scheme, such as standardization, for time series with large scale and small variance.Improving the tokenization to overcome these edge cases without heuristics is subject for future work, butthe results from .5 suggest that the Chronos models performs well on real-world data despite thelimitations.",
  ": In-domain and zero-shot performance of a Chronos-T5 (Small) models varying over (a) the number oftraining steps, (b) the training context length, and (c) the vocabulary size": "Training steps.We trained a Chronos-T5 (Small, 46M) for 1M training steps to study the effect oflonger training on model performance. a shows that the downstream model performance improvesover the course of training, both on in-domain and zero-shot benchmarks. This suggests that performanceof the larger models (Base and Large) can potentially be improved by training them for longer. Context length.We studied the effect of the context length on downstream performance by trainingChronos-T5 (Small, 46M) models with four distinct context lengths. b shows how the performancevaries with increasing context length. We observe improvements on both in-domain and zero-shot metrics as",
  "(d)": ": Forecasts generated by Chronos-T5 (Base) on synthetically generated patterns. (a) Noise: Chronosgenerates reasonable forecasts for Gaussian noise with the 80% prediction interval matching the interval of theunderlying distribution (shown by the horizontal dashed blue line). (b) Trend: Chronos forecasts a linear trend(top) correctly but struggles with an exponential trend (bottom). (c) Seasonality: Chronos accurately modelsseasonal patterns of varying degrees of complexity (single seasonality at the top and three seasonalities at the bottom).(d) Combined Patterns: Chronos forecasts time series generated by the additive (top) or multiplicative (bottom)combination of trend and seasonal patterns accurately.",
  "i=1iXti + t,": "where t N(0, 1) and 1, . . . , p are the parameters of the model. We generated time series from stationaryAR processes of different orders ranging from 1 to 4, and we compared the forecasts generated by Chronos-T5 (Base) against those generated by three models: (a) the ground truth AR model that was used togenerate the time series; (b) an AR model with the correct order (p) fitted to the time series; and (c) anAutoARIMA model fitted to the time series. shows the results for the AR(1) and AR(4) processes,and (Appendix E) shows the results for AR(2) and AR(3). We observe that Chronos-T5 (Base)generates plausible forecasts across all four AR processes.The simpler AR(1) and AR(2) processes areeasier for the correctly-specified AR model and AutoARIMA model to fit, resulting in a better MSE thanChronos-T5 (Base). However, with increasing complexity in AR(3) and AR(4) processes, Chronos-T5(Base) not only outperforms the AutoARIMA model (which belongs the same family as the ground truthmodel) but also performs on par with the fitted AR model with correct order. These results highlight thatChronos models can recognize fundamental patterns present in time series data. Flexible predictive distributions.Using a categorical distribution to encode predictions gives Chronosflexibility in producing predictive distributions of different shapes. This is shown in , illustratingkernel density estimate (KDE) plots of token IDs sampled from a Chronos model, for the first five timesteps in the forecast horizon, across three datasets. Despite the fact that cross-entropy is not distance-aware, Chronos outputs predictive distributions over a contiguous set of tokens, and with different shapes,",
  "(b) AR(4)": ": Forecasts generated by Chronos-T5 (Base) for time series generated from AR(1) and AR(4) processescompared against forecasts generated by the ground truth AR model, a fitted AR model of the correct order, andan AutoARIMA model. Chronos-T5 (Base) generates plausible forecasts and prediction intervals in both cases. AllAR models fit the simpler AR(1) process correctly and obtain better MSE than Chronos-T5 (Base); however, withthe increased complexity in the AR(4) process, Chronos-T5 (Base) performs second best after the ground truth ARmodel. h = 1 h = 2 h = 3 h = 4 Token ID h = 5",
  "(c) Hospital": ": Forecast distributions from a Chronos model on series from the NN5 (Daily), Traffic, and Hospital datasetsrespectively. Each plot shows the predictive distribution for five prediction steps (h = 1, . . . , 5): the densities wereobtained via kernel density estimation from sample forecasts. Even though the cross entropy is not distance-aware,the model learns to estimate distributions over neighboring tokens, and of diverse shapes, including multimodal ones. including multi-modal ones. Although Chronos learns the topology of the space directly from the data, wehypothesize that providing explicit topological information to the model during training may expedite theprocess and make the model robust for tokens where fewer datapoints are available. A potential method toinject topological information into the cross-entropy loss is through a type of label smoothing assigningnon-zero probability mass to tokens (i.e., bins) in the neighborhood of the the correct token. Farebrotheret al. (2024) have obtained promising results with such a distance-aware regression-via-classification objectivein the context of reinforcement learning. An in-depth theoretical and empirical analysis of the regression-via-classification paradigm in the context of time series forecasting would constitute interesting future research. Overflow and loss of precision.One limitation of Chronos comes from the proposed tokenizationapproach (see .1). Specifically, the tokens we select represent bin centers in the range [15, +15],which ultimately represent original time series values in the range [15s, 15s], where s is the scale of thetime series (mean absolute value). If s is very small compared to the range of values in the series, then",
  "Beyond Zero-shot Univariate Forecasting": "In our experiments, we evaluated Chronos in a zero-shot manner for most datasets.Such a setuphighlights the competitiveness of zero-shot Chronos models against task-specific baselines. We expect thatboth in-domain and zero-shot results could be enhanced further through fine-tuning, an avenue we brieflyexplored in .5.2. This can be done using any parameter-efficient fine-tuning methods such as thosebased on low-rank adapters (LoRA) (Hu et al., 2022; Zhang et al., 2023). Alternatively, Chronos can becalibrated for a specific task with conformal methods (Romano et al., 2019; Stankeviciute et al., 2021; Xu& Xie, 2021). Chronos is especially attractive in the context of conformal prediction since it requires notraining set, so all available data can be used for calibration. In this work, we have focused on univariate forecasting of uniformly-spaced time series since it constitutesthe most common of real-world time series use-cases. Nevertheless, practical forecasting tasks often involveexogenous information that must be taken into account or may require modeling of irregularly-sampled time",
  ": Inference time of different models for forecastinga single time series, averaged across datasets. The computerequirements of individual models have been highlighted": "A potential limitation of the larger Chronos mod-els is their inference speed compared to task-specificdeep learning models. illustrates theinference time of generating forecasts for a singletime series, averaged across datasets. The inferencespeed of the larger Chronos models is comparableto some statistical local models.Moreover, whileChronos models are slower than task-specific mod-els, they are not too large to be prohibitively slow.Furthermore, task-specific models need to be trainedfor each task individually, which requires additionaltime and compute. In contrast, Chronos modelscan be deployed for datasets with diverse historylengths, frequencies, prediction horizons, and con-text lengths. This makes model deployment signif-icantly easier and drastically simplifies forecastingpipelines, obviating the need for task-specific training. By leveraging a language modeling framework for time series, we make developments in the NLP commu-nity immediately transferable to Chronos models. For instance, inference speed can be improved by usingCUDA kernels optimized for modern Ampere GPUs, quantization (Dettmers et al., 2022), and faster de-coding techniques, including speculative (Leviathan et al., 2023) and lookahead (Fu et al., 2023) decoding.Developments in long-context language models (Sun et al., 2022; Dao, 2023) may help improve Chronosmodels applicability to high-frequency datasets that require longer contexts to capture seasonal patterns.Other techniques popularly used for text language models, such as temperature tuning, beam search (Freitag& Al-Onaizan, 2017), Top-K sampling (Fan et al., 2018), nucleus sampling (Holtzman et al., 2019), couldenhance the quality of forecasts. These may particularly be helpful in improving the speed and quality ofpoint forecasts, which currently require aggregation over multiple samples.",
  "Data": "Our findings underscore that training larger models on a large corpus of time series data yields excellentin-domain and zero-shot performance.Nevertheless, in contrast to NLP, high-quality public time seriesdata remains limited. This poses a dilemma when training models on a large corpus of diverse datasets selecting more datasets for training leaves fewer for zero-shot evaluation. The time series community wouldbenefit greatly from the availability of larger time series datasets that could be used to develop and improve",
  "Conclusion": "In this work, we approach the problem of developing generalist pretrained forecasting models from thelens of a minimalist.We adapt existing language model architectures and training procedures for timeseries forecasting, challenging the notion that time-series-specific features or architectures are necessary forforecasting. This results in Chronos, a language modeling framework for time series that is, paradoxically,agnostic to time. The defining characteristic of Chronos is its compatibility with any language modelarchitecture, only requiring minimal modifications tokenization though scaling and quantization. Ourpretrained models significantly outperform existing local models and task-specific deep learning baselines interms of their in-domain performance. More remarkably, Chronos models obtain excellent results on unseendatasets (zero-shot performance), performing competitively with the best deep-learning baselines trained onthese datasets, while showing promising evidence of further improvements through fine-tuning. Our contributions are significant in two key aspects. First, we show that existing language model architec-tures are capable of performing forecasting without time-series-specific customizations. This paves the wayfor accelerated progress by leveraging developments in the area of LLMs and through better data strategies.Second, on a practical level, the strong performance of Chronos models suggests that large (by forecastingstandards) pretrained language models can greatly simplify forecasting pipelines without sacrificing accuracy,offering an inference-only alternative to the conventional approach involving training and tuning a model onindividual tasks.",
  "Acknowledgements": "We are indebted to Stefano Soatto for challenging us to think about the fundamental question regardinglanguage models and time series modeling, ultimately leading to the creation of the present work.Weare grateful to our fellow researchers who have contributed to this work with insightful discussions andvaluable feedback, including but not limited to George Karypis, Huzefa Rangwala, Devamanyu Hazarika,Imry Kissos, Laurent Callot, Baris Kurt, Valentin Flunkert, David Salinas, Boran Han, Xiaoyong Jin, LukeHuan, Youngsuk Park, Gaurav Gupta, Karthick Gopalswamy, Tim Januschowski, Jan Gasthaus, Bing Xiang,Kashif Rasul, Juba Nait Saada, Matthias Karlbauer, Hugo Senetaire, Mononito Goswami and Gerald Woo. Alexander Alexandrov, Konstantinos Benidis, Michael Bohlke-Schneider, Valentin Flunkert, Jan Gasthaus,Tim Januschowski, Danielle C Maddix, Syama Rangapuram, David Salinas, Jasper Schulz, et al. GluonTS:Probabilistic and Neural Time Series Modeling in Python. The Journal of Machine Learning Research,21(1):46294634, 2020. 33 Abdul Fatir Ansari, Konstantinos Benidis, Richard Kurle, Ali Caner Turkmen, Harold Soh, Alexander JSmola, Bernie Wang, and Tim Januschowski. Deep Explicit Duration Switching Models for Time Series.Advances in Neural Information Processing Systems, 34, 2021. 10 Abdul Fatir Ansari, Alvin Heng, Andre Lim, and Harold Soh. Neural continuous-discrete state space modelsfor irregularly-sampled time series. In International Conference on Machine Learning, pp. 926951. PMLR,2023. 20",
  "Oliver Borchert, David Salinas, Valentin Flunkert, Tim Januschowski, and Stephan Gnnemann. Multi-objective model selection for time series forecasting. arXiv preprint arXiv:2202.08485, 2022. 21": "Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss,Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, ClemensWinter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, JackClark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Languagemodels are few-shot learners. In Advances in Neural Information Processing Systems, 2020. 2, 3, 4",
  "Chris U Carmona, Franois-Xavier Aubet, Valentin Flunkert, and Jan Gasthaus. Neural Contextual AnomalyDetection for Time Series. arXiv:2107.07702, 2021. 7": "Cristian Challu, Kin G Olivares, Boris N Oreshkin, Federico Garza Ramirez, Max Mergenthaler Canseco, andArtur Dubrawski. N-HiTS: Neural Hierarchical Interpolation for Time Series Forecasting. In Proceedingsof the AAAI Conference on Artificial Intelligence, volume 37, 2023. 10, 33 Jianlin Cheng, Zheng Wang, and Gianluca Pollastri. A neural network approach to ordinal regression. In2008 IEEE international joint conference on neural networks (IEEE world congress on computationalintelligence), pp. 12791284. IEEE, 2008. 6 Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts,Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. PaLM: Scaling LanguageModeling with Pathways. Journal of Machine Learning Research, 24(240):1113, 2023. 3 Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, XuezhiWang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling Instruction-Finetuned Language Models.arXiv:2210.11416, 2022. 3",
  "YichaoFu,PeterBailis,IonStoica,andHaoZhang.BreakingtheSequentialDependencyof LLM Inference Using Lookahead Decoding, November 2023.URL 20": "Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, HoraceHe, Anish Thite, Noa Nabeshima, et al. The Pile: An 800GB Dataset of Diverse Text for LanguageModeling. arXiv:2101.00027, 2020. 7 Federico Garza, Max Mergenthaler Canseco, Cristian Chall, and Kin G. Olivares. StatsForecast: Lightningfast forecasting with statistical and econometric models. PyCon Salt Lake City, Utah, US 2022, 2022.URL 33 Jan Gasthaus, Konstantinos Benidis, Yuyang Wang, Syama Sundar Rangapuram, David Salinas, ValentinFlunkert, and Tim Januschowski. Probabilistic Forecasting with Spline Quantile Function RNNs. In Pro-ceedings of the Twenty-Second International Conference on Artificial Intelligence and Statistics, volume 89of Proceedings of Machine Learning Research, pp. 19011910. PMLR, 2019. 3, 10, 35",
  "Rob J Hyndman and Anne B Koehler. Another look at measures of forecast accuracy. International journalof forecasting, 22(4):679688, 2006. 10, 34": "Hassan Ismail Fawaz, Germain Forestier, Jonathan Weber, Lhassane Idoumghar, and Pierre-Alain Muller.Deep learning for time series classification: a review. Data mining and knowledge discovery, 33(4):917963,2019. 20 Ming Jin, Shiyu Wang, Lintao Ma, Zhixuan Chu, James Y. Zhang, Xiaoming Shi, Pin-Yu Chen, YuxuanLiang, Yuan-Fang Li, Shirui Pan, and Qingsong Wen. Time-LLM: Time series forecasting by reprogram-ming large language models. In The Twelfth International Conference on Learning Representations, 2024.2, 4 Xiaoyong Jin, Youngsuk Park, Danielle Maddix, Hao Wang, and Yuyang Wang. Domain adaptation for timeseries forecasting via attention sharing. In International Conference on Machine Learning, pp. 1028010297. PMLR, 2022. 1, 4 Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, and Tie-Yan Liu.LightGBM: A Highly Efficient Gradient Boosting Decision Tree. Advances in neural information processingsystems, 30, 2017. 20",
  "Yaniv Leviathan, Matan Kalman, and Yossi Matias. Fast inference from transformers via speculative decod-ing. In International Conference on Machine Learning, pp. 1927419286. PMLR, 2023. 20": "Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, VesStoyanov, and Luke Zettlemoyer. BART: Denoising Sequence-to-Sequence Pre-training for Natural Lan-guage Generation, Translation, and Comprehension. arXiv:1910.13461, 2019. 3 Bryan Lim, Sercan Ark, Nicolas Loeff, and Tomas Pfister. Temporal fusion transformers for interpretablemulti-horizon time series forecasting. International Journal of Forecasting, 37(4):17481764, 2021. 3, 6,10, 33 Xu Liu, Yutong Xia, Yuxuan Liang, Junfeng Hu, Yiwei Wang, Lei Bai, Chao Huang, Zhenguang Liu,Bryan Hooi, and Roger Zimmermann. Largest: A benchmark dataset for large-scale traffic forecasting.arXiv:2306.08259, 2023. 21",
  "Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models.arXiv:1609.07843, 2016. 7": "Suvir Mirchandani, Fei Xia, Pete Florence, Brian Ichter, Danny Driess, Montserrat Gonzalez Arenas, Kan-ishka Rao, Dorsa Sadigh, and Andy Zeng.Large language models as general pattern machines.InProceedings of The 7th Conference on Robot Learning, volume 229 of Proceedings of Machine LearningResearch, pp. 24982518. PMLR, 2023. 3 Yuqi Nie, Nam H. Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam. A time series is worth 64 words:Long-term forecasting with transformers. In International Conference on Learning Representations, 2023.3, 4, 10, 33 Kin G. Olivares, Cristian Chall, Federico Garza, Max Mergenthaler Canseco, and Artur Dubrawski. Neu-ralForecast: User friendly state-of-the-art neural forecasting models. PyCon Salt Lake City, Utah, US2022, 2022. URL 33 Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, NalKalchbrenner, Andrew Senior, and Koray Kavukcuoglu. Wavenet: A generative model for raw audio.arXiv:1609.03499, 2016. 10, 33 Boris N. Oreshkin, Dmitri Carpov, Nicolas Chapados, and Yoshua Bengio. N-BEATS: Neural basis expansionanalysis for interpretable time series forecasting. In International Conference on Learning Representations,2020. 10, 33 Boris N. Oreshkin, Dmitri Carpov, Nicolas Chapados, and Yoshua Bengio. Meta-learning framework withapplications to zero-shot time-series forecasting.In Proceedings of the AAAI Conference on ArtificialIntelligence (AAAI), 2021. 4 Bernardo Prez Orozco and Stephen J. Roberts. Zero-shot and few-shot time series forecasting with or-dinal regression recurrent neural networks. In 28th European Symposium on Artificial Neural Networks,Computational Intelligence and Machine Learning, pp. 503508, 2020. 4 Youngsuk Park, Danielle Maddix, Franois-Xavier Aubet, Kelvin Kan, Jan Gasthaus, and Yuyang Wang.Learning quantile functions without quantile crossing for distribution-free time series forecasting.InInternational Conference on Artificial Intelligence and Statistics, pp. 81278150. PMLR, 2022. 3",
  "Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language modelsare unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. 4, 6": "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou,Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer.The Journal of Machine Learning Research, 21(1):54855551, 2020. 3, 6, 7, 9, 13 Wasifur Rahman, Md Kamrul Hasan, Sangwu Lee, Amir Zadeh, Chengfeng Mao, Louis-Philippe Morency,and Ehsan Hoque. Integrating multimodal information in large pretrained transformers. In Proceedings ofthe conference. Association for Computational Linguistics. Meeting, volume 2020, pp. 2359. NIH PublicAccess, 2020. 20",
  "Luis Torgo and Joao Gama. Regression using Classification Algorithms. Intelligent Data Analysis, 1(4):275292, 1997. 6": "Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bash-lykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Fer-rer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, HakanInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit SinghKoura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao,",
  "Christopher Winship and Robert D Mare. Regression models with ordinal variables. American sociologicalreview, pp. 512525, 1984. 6": "Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pier-ric Cistac, Tim Rault, Rmi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen,Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame,Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-the-art natural language processing.In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: SystemDemonstrations, pp. 3845. Association for Computational Linguistics, 2020. 6, 9",
  "BDatasets": "The complete list of datasets used for our empirical evaluation is provided in . The table is dividedinto three sections, representing how the datasets were used for Chronos models: in total, 55 datasetswhere used for experiments, 13 of which for pretraining only, 15 for in-domain evaluation, and 27 for zero-shot evaluation (see also ). In the following, we provide a brief description of each dataset, organizedby its domain.",
  "Solar (5 Min., Hourly) contains data about solar power generation in the US in 2006. The original datahas 5 minute frequency and was obtained from thehourly version was obtained via mean aggregation": "Spanish Energy and Weather contains 4 years of electricity consumption, generation, pricing, andweather data for Spain.Electricity data is for all of Spain, weather data is provided for each of 5major Spanish cities.The data was obtained from Wind Farms (Hourly, Daily) (Godahewa et al., 2021) contains energy production data from wind farmsin Australia. Original data was collected at 1 minute frequencey, which we aggregated to hourly and dailyusing the mean.",
  "Weather (Godahewa et al., 2021) contains daily time series of four weather variables (rain, mintemp,maxtemp and solar radiation) measured at weather stations in Australia": "Weatherbench (Hourly, Daily, Weekly) contains WeatherBench data at the spatial resolution of 5.625(3264 grid points). WeatherBench is a comprehensive benchmark dataset for weather prediction researchand contains hourly values of the many weather-related variables over 40 years from 1979 to 2018 (includingtemperature, humidity, wind, precipitations). The original data has hourly frequency and was obtained from we aggregated it to daily and weekly using mean, exceptfor total precipitation which was aggregated by sum.",
  "B.6Mobility and transport": "Mexico City Bikes contains hourly usage statistics for 494 bike stations in Mexico City from 2010 to 2022.Each value in the time series corresponds to the number of bikes returned at the given station at the givenhour of the day. Data was obtained from Time series thatcontain less than 50 non-zero observations were removed.",
  "B.7Various": "M1 (Monthly to Yearly) (Makridakis et al., 1979; Godahewa et al., 2021) contains the time time seriesused in the M1 forecasting competition. Data spans micro-/macroeconomics, industry, and demographics. M3 (Monthly to Yearly) (Makridakis & Hibon, 2000; Godahewa et al., 2021) contains the time timeseries used in the M1 forecasting competition. Data spans micro-/macroeconomics, industry, finance anddemographics. M4 (Hourly to Yearly) (Makridakis et al., 2020; Godahewa et al., 2021) contains data from variousdomains, at different sampling periods, used for the M4 forecasting competition. Domains include micro-/macroeconomics, demographic, industry, and finance. M5 (Makridakis et al., 2022) contains products sales data, used for the M5 forecasting competition. Thedata includes sales up to the end of the validation set (end of public leaderboard), but not values for thetest set (private leaderboard).",
  "CBaselines": "We considered a total of 17 baseline methods for benchmarking Chronos. Local statistical baselines wereAutoETS, AutoARIMA, Naive, Seasonal Naive, and AutoTheta (Assimakopoulos & Nikolopoulos, 2000); forthese, we relied on implementations in the StatsForecast library (Garza et al., 2022). For task-specific deeplearning architectures, DeepAR (Salinas et al., 2020), PatchTST (Nie et al., 2023), TFT (Lim et al., 2021),DLinear (Zeng et al., 2023), and WaveNet (Oord et al., 2016), we based evaluations on the implementations inGluonTS (Alexandrov et al., 2020). However, N-BEATS (Oreshkin et al., 2020) and N-HiTS (Challu et al.,2023), experiments were based on implementations in the NeuralForecast (Olivares et al., 2022) library.Finally, we used reference implementations of ForecastPFN8 (Dooley et al., 2023), GPT4TS9 (One-Fits-All)(Zhou et al., 2023a), LLMTime10 (Gruver et al., 2023), Lag-Llama11 (Rasul et al., 2023), and Moirai-1.0-R12 (Woo et al., 2024). WaveNet and GPT4TS models were trained on AWS EC2 p3.2xlarge instances which have 1 NVIDIA V100GPUs with 16GB VRAM. All other baselines were trained on the CPU on Intel-based EC2 instances. Task-specific deep learning baselines not based on large language models (DeepAR, PatchTST, TFT, DLinear,WaveNet, N-BEATS, and N-HiTS) were trained and evaluated three times and their performance averagedin order to account for high variance inherent in their optimization. For inference, we used EC2 CPU instances for local models, N-HiTS, and N-BEATS. The p3.2xlarge instance(1 V100 16GB) was used for inference for other task-specific deep learning models and pretrained modelssuch as Lag-Llama, Moirai-1.0-R, and ForecastPFN. Since LLMTime uses a Llama-2 70B model which hassignificantly larger compute requirements, LLMTime inference was performed on the p3dn.24xlarge AWSEC2 instance with 8 NVIDIA V100 32GB GPUs.",
  "15min2030min101H101D or 1B101W101M1.53M or 1Q1.51Y1.5": "Statistical baselines (AutoETS, AutoARIMA, AutoTheta and SeasonalNaive) were used with their defaulthyperparameters in StatsForecast, but with season lengths implied by their frequencies. For example, dailyfrequency data had season length set to 7, hourly data 24, and so on. For this heuristic, we used the helperfunction get_seasonality from GluonTS. Unless otherwise specified, the default hyperparameter configurations provided in baseline implementationswere kept as is, and no dataset specific or global hyperparameter tuning was performed. GluonTS-basedimplementations were optimized with a batch size of 128, for a time limit of 4 hours and early stoppingpatience of 200 epochs. In PatchTST and DLinear, we experimented with two loss functions: original lossesaimed at point forecasting (L1 or L2 loss) as well as default probabilistic forecasting heads used in theirGluonTS implementations, where the loss is set to the negative Students-t log likelihood of the forecasthorizon. Due to the consistently superior performance, our final results include the probabilistic versions ofPatchTST and DLinear only. For GPT4TS, we set the context length equal to a multiple of the predictionlength, with the multiplier depending on the frequency of the dataset (). We used the MASE lossfunction for fine-tuning in GPT4TS due to its superior performance. For LLMTime, we experimented only with the Llama-2 70B due to the prohibitively high costs of running thebenchmark through OpenAI APIs. We used the same hyperparameters as used in the Monash experimentin the original paper (Gruver et al., 2023) with a few notable differences. We set the context length to512, same as for Chronos models, instead of 500. During our experiments, we observed that the defaulthyperparameters may lead to a significant drop in the scale of the last prediction on some datasets. Toalleviate this issue, we set the STEP_MULTIPLIER to 1.4 (instead of 1.2) and increased the prediction lengthby 1 (this extra prediction is removed before computing the metrics). The inference time for LLMTime(Llama-2 70B) is 0.8 seconds per observation on p3dn.24xlarge. As an example, this will take 92 hoursto generate all the predictions on the Traffic dataset (862 time series, 24 as prediction length, 20 samples).Due to the very high compute cost, we skip the evaluation of LLMTime on some large datasets.",
  "DEvaluation Metrics": "In what follows, we consider a dataset of N time series {xi = [xi,1, . . . , xi,C+H]}Ni=1, each spanning both thecontext length C and prediction horizon H. We are interested in evaluating the accuracy of predictions forxi,C+1:C+H, for all i {1, . . . , N}, which can be either point forecasts or probabilistic ones. A point forecast for xi is denoted as as xi = [xi,C+1, . . . , xi,C+H]. To evaluate point forecasts, we use themean absolute scaled error (MASE, Hyndman & Koehler (2006)). For each series, this is simply the meanabsolute error (MAE) divided by the empirical error of a seasonal nave model:",
  "Moirai-1.0-RPretrainedReferenceYesC = 1024, Patch length: selected by dataset-specific validation": "Probabilistic forecasts are given in terms of predicted quantiles q()i= [q()i,C+1, . . . , q()i,C+H] at levels (0, 1).To evaluate the quality of such predicted quantiles, we use the weighted quantile loss (WQL): this is anaggregation of the quantile loss (Koenker & Hallock, 2001), which is defined for the predicted -quantile qof a real observation x, as",
  "j=1WQLj": "In all experiments, we use quantiles at level {0.1, 0.2, . . . , 0.9} to compute WQL, so that K = 9. Notethat, being a weighted average of the quantile loss at different levels, WQL approximates (a weighted averageof) the continuous ranked probability score (CRPS), a commonly used metric for evaluating probabilisticpredictions (Gneiting & Raftery, 2007; Gasthaus et al., 2019). Unlike for MASE, where errors are scaled bya term proportional to the scale of each series, WQL aggregates absolute errors: as such, its value is affectedby the relative scale of all series in the dataset.",
  "EAdditional Results": "This section complements .5 by providing additional details to the experimental results. reports the training time and cost of Chronos-T5 models on a p4d.24xlarge EC2 instance. Tables 7 and 8report the raw WQL and MASE scores together with the aggregate relative score and average rank obtainedby all models on the datasets in Benchmark I. Similarly, Tables 9 and 10 report these scores on Benchmark II.Figures 18 and 19 show the average ranks obtained by different models on Benchmark I and II, respectively. illustrates the zero-shot performance of Chronos-T5-Synth (Small), a model trained solely onsynthetic data generated using KernelSynth, against various baselines.",
  "Chronos-T5 (Mini)7.68252Chronos-T5 (Small)7.73253Chronos-T5 (Base)17.96588Chronos-T5 (Large)63.052066": ": WQL scores of different models for datasets in Benchmark I, comprising 15 datasets also included in thetraining data of Chronos models. Models achieving the first, second, and third best scores have been highlighted.Scores for Chronos and task-specific models have been averaged over 3 random seeds. The aggregated relative scorewas computed as described in .4.",
  "Avg. Rank3.4004.6676.2006.0677.53314.53311.1339.1336.3339.53310.73310.40010.4678.2008.53317.36717.20015.33316.56718.00019.667": ": MASE scores of different models for datasets in Benchmark I, comprising 15 datasets also included in thetraining data of Chronos models. Models achieving the first, second, and third best scores have been highlighted.Scores for Chronos and task-specific models have been averaged over 3 random seeds. The aggregated relative scorewas computed as described in .4.",
  "Avg. Rank3.3334.7336.0676.4676.93314.20011.5339.4675.73310.86712.13313.93311.8009.6679.40012.13316.50016.66717.33317.56716.66719.867": ": WQL scores of different models for datasets in Benchmark II, comprising 27 datasets not seen by Chronosmodels during training. Models achieving the first, second, and third best scores have been highlighted. Scoresfor Chronos and task-specific models have been averaged over 3 random seeds. The aggregated relative score wascomputed as described in .4.",
  "(b) AR(3)": ": Forecasts generated by Chronos-T5 (Base) for time series generated from AR(2) and AR(3) processescompared against forecasts generated by the ground truth AR model, a fitted AR model of the correct order, andan AutoARIMA model. Chronos-T5 (Base) generates plausible forecasts and prediction intervals in both cases. AllAR models fit the simpler AR(2) process well and obtain better MSE than Chronos-T5 (Base); however, with theincreased complexity in the AR(3) process, Chronos-T5 (Base) performs better than other models.",
  "Dominick": "08-27 1208-28 0008-28 1208-29 0008-29 1208-30 0008-30 1208-31 00 ERCOT Load 06-25 0306-25 0606-25 0906-25 1206-25 1506-25 1806-25 2106-26 00 ETT (15 Min.) 06-23 0006-23 1206-24 0006-24 1206-25 0006-25 1206-26 0006-26 12 ETT (Hourly) 11-05 0611-05 0911-05 1211-05 1511-05 1811-05 2111-06 0011-06 0311-06 06 Electricity (15 Min.) 12-28 0012-28 1212-29 0012-29 1212-30 0012-30 1212-31 0012-31 1201-01 00 Electricity (Hourly) 2014-062014-072014-082014-092014-102014-112014-12 Electricity (Weekly) 2018-092018-102018-112018-122019-012019-02 0.98 1.00 1.02 1.04"
}