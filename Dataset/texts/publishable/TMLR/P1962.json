{
  "Abstract": "Despite the widespread use of unsupervised Graph Neural Networks (GNNs), their post-hocexplainability remains underexplored. Current graph explanation methods typically focus onexplaining a single dimension of the final output. However, unsupervised and self-supervisedGNNs produce d-dimensional representation vectors whose individual elements lack clear,disentangled semantic meaning. To tackle this issue, we draw inspiration from the success ofscore-based graph explainers in supervised GNNs and propose a novel framework, grXAI, forgraph representation explainability. grXAI generalizes existing score-based graph explainers toidentify the subgraph most responsible for constructing the latent representation of the inputgraph. This framework can be easily and efficiently implemented as a wrapper around existingmethods, enabling the explanation of graph representations through connected subgraphs,which are more human-intelligible. Extensive qualitative and quantitative experimentsdemonstrate grXAIs strong ability to identify subgraphs that effectively explain learnedgraph representations across various unsupervised tasks and learning algorithms.",
  "Introduction": "Graph neural networks (GNNs) have become increasingly useful in a wide range of applications, such asdrug discovery (Lu et al., 2024; Hajiramezanali et al., 2020), large-scale social networks (Hajiramezanaliet al., 2019), and recommender systems (Hasanzadeh et al., 2019). However, due to their complexity andopacity, understanding GNNs can be challenging for human users. To tackle this issue, several tools havebeen developed to explain supervised GNNs in terms of their predictions. In the supervised setting, a GNNmodel learns to map input graphs to labels (f() : G Y), and explanations shed light on the modelsprediction of a specific label. The interpretability of model-agnostic GNN explanation methods stems fromthe fact that the particular label1 of interest, such as a chemical property, is meaningful to humans (,left). 1The term label used in the context of explainability refers to the predicted label for the test input graph by the GNN modelbeing explained, which is distinct from the ground truth label used in the context of supervised ML (Ying et al., 2019).",
  "d-dimensional vector-based scoring function": ": (Left) Post-hoc graph explainability methods typically provide explanations based on a specificlabel of interest. (Right) Our framework for explaining graph representations provides explanations basedon a d-dimensional representation vector. The Score refers to a range of post-hoc explanation functionsproposed in the literature, including Shapley and Saliency. On the other hand, unsupervised GNNs map input graphs to representations (f() : G H) that cannot beeasily interpreted using existing supervised explanation methods. Although these methods can be used tounderstand individual elements in the representation space (hi H), such explanations are not meaningful tohumans unless the elements have a natural semantic meaning (Lin et al., 2023). Unfortunately, the meaningof individual elements in the representation space is generally unknown. Two possible solutions have beenproposed: the first approach enforces semantic meaning in the representations, but it requires concept labelsfor every training sample, which is often impractical (Koh et al., 2020). The second approach enforcesdisentangled representations and manually identifies semantically meaningful elements (Paige et al., 2017),but this approach is not model-agnostic and requires potentially unwanted changes to the modeling andtraining process (Lin et al., 2023). Another possible approach to solve the aforementioned issue may involve utilizing the existing methodsto explain each element of the representation vector independently and then averaging over them (Crabb& van der Schaar, 2022). However, this solution is not sufficient to provide a global understanding of thelatent representation as a whole and does not necessarily generate connected subgraphs as explanations,which are important for human understanding and to improve consistency (Hajiramezanali et al., 2023).Additionally, many existing post-hoc explanation methods, such as gradient- or perturbation-based methods,rely on multiple perturbations or backpropagations of the GNN model to generate explanations for eachinput graph, making them time-consuming and impractical when applied to each element of high-dimensionalrepresentations (Chuang et al., 2023). For instance, Shapley-based graph explainers that utilize the GNNarchitecture to approximate Shapley values are computationally expensive (Xie et al., 2022). This costescalates linearly with the dimension of the representation vector, rendering them impractical for explainingunsupervised GNNs. Addressing this issue would require non-trivial extensions. Our main contributions are: (i) We introduce a general framework, grXAI, that adapts model-agnosticscore-based graph explainability methods to explain (representation) vectors. We do this by defining anauxiliary cosine similarity function as a wrapper around the unsupervised GNN to interpret. (ii) We applyour framework to different types of graph explainability approaches, including SubgraphX, and introducethe first graph explainability method for unsupervised settings that can explain the model using connectedsubgraphs. (iii) While motivated by the explanation of purely unsupervised graph representation vectors,the proposed approach is beneficial even for supervised multi-class classifications, generating more robustand accurate explanations for high-uncertainty cases. (iv) Our experiments show that our methods achievestate-of-the-art results, both qualitatively and quantitatively, on various self-supervised tasks, including graphrepresentation and node embedding. Motivation. Learning meaningful representations of graph-structured data with GNNs is important in manyfields, particularly when labeled data are scarce due to costly and time-consuming experiments(Hasanzadehet al., 2021). To this end, applications of GNNs in a wide range of domains, such as drug discovery andmolecular biology (Hasanzadeh et al., 2022), have inspired recent unsupervised and self-supervised strategiesto learn from massive corpora of unlabeled structured data (Sun et al., 2019; You et al., 2020; Hu et al., 2019).The ability to explain unsupervised GNNs can provide valuable insights into the learned representation,",
  "This method is designed to handle probability vectors, such as the outputs of a softmax layer, and is not capable of handlingembedding vectors learned through unsupervised settings": "help researchers understand and compare graph representation learning methods, as well as assist users ineffectively monitoring and debugging these models during deployment. Our proposed framework enablesscore-based explainability in unsupervised settings in a scalable way, making it feasible and effective forreal-world applications. Furthermore, our framework offers benefits even in supervised settings, provingparticularly helpful when the models predictions are uncertain.",
  "Related work": "With many recent advances in GNNs and their numerous applications across different fields, explainabilitymethods have become critical for providing insight into their predictions. To this end, many approaches havebeen proposed to explain the predictions of supervised GNN models (Baldassarre & Azizpour, 2019; Yinget al., 2019; Luo et al., 2020; Xie et al., 2022; Yuan et al., 2021; Zhang et al., 2022; Ye et al., 2024; Huanget al., 2024; Hasanzadeh et al., 2020). These methods can be divided into four main categories (Yuan et al., 2020): gradient-, perturbation-,decomposition-, and surrogate-based methods.In this paper, we mainly focus on gradient-based andperturbation-based methods. The gradient-based methods are generally fast and easy to implement, butthey may not be able to capture more complex relationships in the data (Yuan et al., 2021; 2020). On theother hand, perturbation-based methods can be more computationally expensive, but they generally achievestate-of-the-art performance in terms of explanation quality (Xie et al., 2022). The common characteristic ofmost of these methods is that they require labels to specify which element of the GNNs output to explain. Instead of explaining individual elements in the representation, recent approaches have focused on explainingmulti-dimensional representation vectors as a whole, which is critical to explaining unsupervised models.These include TAGE (Xie et al., 2022) in the context of GNNs, and methods such as COCOA (Lin et al.,2023), RELAX (Wickstrm et al., 2023), Label-Free XAI (Crabb & van der Schaar, 2022), and CoRTX(Chuang et al., 2023) for other domains. TAGE (Xie et al., 2022) successfully addresses unsupervised GNN settings using a contrastive loss approach.However, it has two main limitations (see ). First, TAGE is a mutual information-based (MI-based)explainability method; however, it has been shown that score-based explainability methods outperformMI-based ones in supervised GNN settings (Yuan et al., 2020). Hence, an open problem is how to leverageexisting score-based methods to explain GNNs in unsupervised settings. Second, TAGE focuses on explainingthe importance of graph edges, but ignores the substructures of graphs. However, explanations consisting ofconnected subgraphs have not only been deemed more intuitive to humans in practice (Yuan et al., 2021;2020), but they are also more consistent in supervised GNN settings (Hajiramezanali et al., 2023). While the explainability of graph representations remains largely unexplored, several methods have beenproposed to explain unsupervised models for unstructured data. The RELAX and LFXAI approaches sharethe goal of identifying features in the sample being explained (i.e., the explicand) that, if removed, wouldcause the altered representation to diverge from the original representation of the explicand (Lin et al.,2023). Chuang et al. (2023) introduce the contrastive real-time explanation (CoRTX) framework, whichutilizes contrastive learning techniques. Unlike previous feature-based methods, Lin et al. (2023) introducecontrastive corpus attribution (COCOA), which allows users to choose corpus and foil samples in order to",
  "Published in Transactions on Machine Learning Research (10/2024)": "implementation of GraphCL, InfoGraph, and GRACE for pre-training GNNs in self-supervised settings (Liuet al., 2021), specifically using GIN (Xu et al., 2019) for GraphCL and InfoGraph, and GCN for GRACE. Wefollowed the hyperparameters outlined in the main papers for these methods, using 3-layer GINs with anembedding dimension of 32 for GraphCL, 4-layer GINs with an embedding dimension of 512 for InfoGraph,and 2-layer GCNs with a node embedding dimension of 128 for GRACE. For grSubgraphX, we modified theoriginal implementation of SubgraphX (Yuan et al., 2021) and followed the same hyperparameters of theoriginal paper. We have implemented grIG and grSaliency based on their original implementation in theCaptum (Kokhlikyan et al., 2020) package.",
  "Preliminary": "Our framework can generalize most of the existing score-based explainability methods to an unsupervisedsetting. Therefore, we first outline supervised score-based graph explainability and then overview two typesas proof-of-concept examples. We will show how our framework can generalize them to explain (unsupervised)graph representations in . Specifically, we will discuss SubgraphX, which is currently the state-of-the-art perturbation-based model (Yuan et al., 2020; Hajiramezanali et al., 2023), followed by two computationallyefficient gradient-based methods (Integrated Gradients and Saliency). Additional preliminaries less related tothe proposed method, including graph self-supervised learning and TAGE, can be found in Appendix B. Notation. Let G = (V, E) denote a graph on nodes V and edges E with the adjacency matrix A andM-dimensional node attributes X RNM, where N = |V|. We are given a trained GNN model f(),which is optimized on all graphs (or nodes) in the training set and is then used for predictions. For graphclassification, f() : G Y maps each input graph G G to an output yG = f(G) Y. For nodeclassification, f() : G Y maps each input node v G to an output yv = f(v) Y. Please note thatY Rdy and dy > 1.",
  "Score-based graph explainability in supervised settings": "Given a pre-trained GNN model f() : G Y, and assuming {G(i)}ni=1 is the set of subgraphs of the inputgraph G G, where n is the total number of subgraphs, these methods assign a score to each subgraphG(i) G based on its importance to the GNN prediction fj(G). Here, fj(G) denotes the j-th element of theGNN output. Intuitively, the score functions aim to identify the most important subgraphs (G) that, whenremoved, decrease the predicted probability of a class of interest the most. Therefore, the main component ofsuch models is the scoring function Score(fj(), G, G(i)), which calculates the importance of each possiblesubgraph with respect to a specific label of interest (, left). A straightforward way to obtain G is to enumerate all possible subgraphs in G (i.e. {G(i)}ni=1), calculatetheir scores using the scoring function, and select the most important one as the explanation. However, thisbrute-force approach is an intractable combinatorial problem, as the number of potential candidates increasesexponentially. Different approaches address this issue in various ways. Some methods incorporate searchalgorithms to efficiently explore the (connected) subgraphs, as seen in SubgraphX, while others calculate theimportance of each node separately, similar to the gradient-based methods. The latter approach leads todisconnected subgraphs.",
  "SubgraphX": "Key properties of graphs can often be attributed to important and localized structural information. Thegoal of SubgraphX (Yuan et al., 2021) is to identify the most important connected subgraph within the inputgraph that contributes to the GNNs predictions. To achieve this, SubgraphX uses a scoring function toevaluate the importance of different subgraphs based on their interactions with the trained GNN.",
  "Gradient-based methods": "Gradient-based methods, including Integrated Gradients (Sundararajan et al., 2017) and Saliency (Simonyanet al., 2013), address the problem of attributing the prediction of a black-box, here a GNN, to its inputfeatures. Saliency is a simple approach for computing input attribution, returning the gradient of the outputwith respect to the input features. This approach can be understood as taking a first-order Taylor expansionof the network at the input, and the gradients are simply the coefficients of each feature in the linearrepresentation of the model (Simonyan et al., 2013). The value of these coefficients can be interpreted asthe importance of the features (nodes/edges) in explaining the output of the black-box model. IntegratedGradients (IG) (Sundararajan et al., 2017) computes the integral of the gradients with respect to the inputsalong the path from a given vector. Similar to Saliency, an attribution at input x relative to a baseline vectorxb represents the contribution of each individual input feature (nodes/edges) to the model prediction. Formally, given an input graph G and a GNN classifier f(), these methods rank the nodes vi G basedon their influence on the predicted label fj(G). Specifically, the importance of each node vi to the GNNoutput can be written as Score(fj(), G, vi), where {vi}|V|i=1 are the nodes of G. We can then explain a GNNprediction based on a disconnected subgraph using the top-ranked nodes as G = {vs}Vs=1. Here, V is thetotal number of important nodes.",
  "Method": "The grXAI framework is designed to provide explainability for unsupervised GNN models by identifyingimportant subgraphs that can explain the d-dimensional representation vector generated by these models.However, current graph explainability methods have limited capability in handling d-dimensional embeddings,as their score functions are only applicable to a single dimension of the output (Xie et al., 2022). Thislimitation makes it challenging to determine which models output(s) to interpret in unsupervised settings,as representation dimensions do not generally correspond to any meaningful quantity. To overcome thislimitation, we first outline score-based graph explanations for d-dimensional outputs of GNNs in a supervisedsetting, gaining insights that will enable us to extend the framework to unsupervised GNN settings.",
  "Explaining d-dimensional GNNs with score-based methods": "Score-based graph explainability methods aim to explain the predictions of GNN models by computingan importance score Score(f(), G, G(i)) for each subgraph G(i) of G. However, as mentioned in ,existing importance score functions are designed to be applied to individual elements of the softmax output,represented by fj(.) R for some j 1, . . . , dy, and cannot handle high-dimensional probability vectors. Asa result, they approximate Score(f(), G, G(i)) as Score(fj(), G, G(i)), where j = arg maxk1,...,dy fk(). Thatis, the importance score is solely based on the predicted class with the highest probability. This approach limits the comprehensiveness of the graph explanation. Indeed, it does not account for theinherent uncertainty in the models predictions, particularly in multi-class problems. Thus, it hinders acomplete understanding of the models behavior. One possible solution to address this issue is to computethe importance scores for each output component separately and then combine them into a weighted sum",
  "k=1fk(G) Score(fk(), G, G(i)).(2)": "This formulation considers the contribution of each class and is therefore accurate when class probabilitiesare more balanced, unlike the standard definition, which is only accurate for low-uncertainty predictions (i.e.,when fj(G) 1). However, using equation 2 has important limitations. Primarily, searching for (connected)important subgraphs can be computationally expensive given the high number of output dimensions. Specifi-cally, this approach requires calculating the score function for each input graph dy times (one for each outputdimension), resulting in a total of dy n importance scores per input graph. Additionally, while using equation 2 to find connected subgraphs is theoretically possibleby plugging thenaive score function from equation 2 into equation 1 to examine all subgraphs and select the one with thehighest scorethis approach is intractable due to the additional computational complexity of searchingfor connected subgraphs. In the case of SubgraphX, to manage this additional complexity, we identifyimportant subgraphs for each output dimension separately using the standard implementation and then mergethem. That is, instead of directly computing G = arg max|G(i)|Nmindyk=1 fk(G) Score(fk(), G, G(i)),we identify the important connected subgraphs for each output dimension k individually using Gk =arg max|G(i)| Nmin Score Shapley(fk(), G, G(i)), and then construct G = dyk=1 Gk, where Gk is the importantsubgraph of k-th element of GNN output. While this approach can reduce the computational load (given that Nmin, can be smaller than Nmin to achievethe same level of sparsity), it may result in a disconnected subgraph even when a connected subgraph isdesired, which could limit the effectiveness and comprehensiveness of the graph explanation. To address the aforementioned issues, we propose to leverage the linearity property of importance scores withrespect to the GNN. This property is common among score functions used for graph explainability, includingShapley, Saliency, and IG (Crabb & van der Schaar, 2022). Therefore, we propose to rewrite the weightedimportance score in equation 2 as follows:",
  "k=1{fk(G) fk()}, G, G(i)),(3)": "where Scoresup refers to the scoring function for d-dimensional GNNs output in the supervised settings. Theimportance score can be computed efficiently through the auxiliary function (G(i)) = dyk=1 fk(G) fk(G(i))for all subgraphs G(i) G. This approach avoids the need to compute the importance scores for each outputcomponent individually, thus significantly reducing the computational complexity. Furthermore, it guaranteesexplanations consisting of connected subgraphs when applied to a score-based method that returns connectedsubgraphs. Remark 1. The approximation in equation 3 is applicable to most supervised score-based graph explanationmethods that compute linear importance scores, including SubgraphX (Yuan et al., 2021). Rather thaninterpreting individual elements of the argmax output, as normally done in SubgraphX, one can use equation 3to explain supervised GNNs based on all dimensions of their softmax output. Therefore, we can incorporatethis approximation into equation 1 by modifying it as follows:",
  "G = arg max|G(i)|NminScore Shapley((G(i)), G, G(i)).(4)": "Remark 2. It is noteworthy that equation 3 can also be easily extended to gradient-based models such asSaliency (and IG). In particular, we can calculate the node importance with respect to the GNN output asScore Sal(f(), G, vi) = Score Sal(dyk=1{fk(G) fk()}, G, vi), where Score Sal is the Saliency value.",
  "Explanation for graph representations": "Notation. In an unsupervised setting, the trained GNN encoder f() maps graphs (or nodes) to a latent space.For graph representation learning, f() : G H maps each input graph G G to the latent representationhG = f(G) H Rdh, where 1 dh. For node embedding, f() : G H maps each input node v G toa latent vector hv = f(v) H. The goal is to identify important subgraphs that explain the behavior of theGNN and contribute to the latent representation of the graph or node. Ideally, the importance score shouldreflect the contribution of subgraph G(i) to the representation hG = f(G) or hv = f(v). However, in thissetting, there is no principled way to select a particular output component fk() for some k 1, . . . , dh to beutilized in the current explainability methods (, right). Equation 3 proposes a framework that offers the benefit of approximating the importance scores of the entired-dimensional output probability vector, eliminating the need to select a specific output class, which is acommon requirement in many existing score-based graph explainability methods. This enables the extensionof the same approach to explain graph representations, which is the focus of this paper. However, it isessential to note that while fk(G) in equation 3 corresponds to the class probability and",
  "defined for all subgraphs G(i) G using (G(i)) =dhk=1 fk(G)fk(G(i))": "|f(G)| |f(G(i))|, where dh is the dimensionality of thelatent space H. It is important to note that this importance score can also be modified for node representationlearning, where the input graph G would be replaced with a single node v G, and the subgraph G(i) wouldbe a subset of the nodes and edges connected to v. Extending SubgraphX for graph representation explainability using grXAI (grSubgraphX). Ourframework is applicable to most existing score-based graph explainability methods. In the following, we focuson the generalization of SubgraphX (Yuan et al., 2021). The main reasons for our choice are as follows: 1)SubgraphX is currently SOTA for graph explainability (Yuan et al., 2020; Hajiramezanali et al., 2023), 2) itsexplanations consist of connected subgraphs, and 3) due to its computational complexity, using it for graphrepresentations necessitates a non-trivial extension. In SubgraphX, the effectiveness of both the MCTS process and the selection of explanations rely heavily onthe accuracy of the chosen scoring function. Therefore, it is crucial to precisely approximate the importance ofvarious subgraphs. To achieve this, we propose to modify SubgraphX using the importance score in equation 5for explaining graph representations. Let f() : G H represent a GNN encoder, where G = (V, E) G is agiven graph with node set V = {v1, . . . , vN}. Let G(s) = (V(s), E(s)) be the target subgraph with Ns nodes,where V(s) = {v(s)1 , . . . , v(s)Ns} is the set of nodes in G(s), and V(o) = V \\ V(s) = {v(o)1 , . . . , v(o)No} is the set of all other nodes not in V(s), and No = N Ns. We define the set of players P as P = {G(s), v(o)1 , . . . , v(o)No},where we consider the entire subgraph G(s) as a single player. In the grXAI version of SubgraphX, named 2The latent space is not tied to any fixed or predetermined labels on each axis. This means that multiple latent spaces withpermuted dimensions can be equivalent to one another. Therefore, the set of transformations that preserve the geometry of thelatent space, as determined by cosine similarity, is the set of orthogonal transformations.",
  "with : G R such that for all G(s) G,(G(s)) = SCf(G) , f(G(s)),(6)": "where R is a possible coalition set of players, G(s) is a connected graph, and SC is cosine similarity. Algorithm1 in Appendix G includes the computation steps for grSubgraphX. The proposed approximation in equation 6is both fair and accurate, as it considers all possible coalitions and adheres to the four fundamental axiomsintroduced by Lundberg & Lee (2017): efficiency, symmetry, linearity, and the dummy axiom. These desirableaxioms guarantee the validity and impartiality of the explanations for graph representations. grSaliency. Given an input graph G (with N nodes and M-dimensional node attributes X RNM), and aGNN encoder f() : G H, we calculate the Saliency score for each node as follows. First, we calculate thederivative wi = vi RM by backpropagation, where is cosine similarity define in equation 6. After that,to derive a single Saliency score for each node, we take the magnitude of the average of wi across all nodefeatures: Score Sal(, G, vi) = |mean(wi)|. Finally, we sort the nodes based on their scores and select thenodes with the highest Saliency values as the subgraph. Formally, G = {v(s)}Vs=1, where is a permutationof nodes {vi}Ni=1 such that Score Sal(, G, v(1)) Score Sal(, G, v(2)) Score Sal(, G, v(N)), and Vis the size of desired subgraph. grIG. Similar to grSaliency, we calculate the IG score for each node individually and select nodes with thehighest IG score for the explanation. Specifically, we first calculate wi = vi 1=0vi ( vi) d. Then, weobtain the IG score for each node as Score IG(, G, vi) = |mean(wi)|. Similarly, the identified subgraph is G ={v(s)}Vs=1, where is a permutation of nodes {vi}Ni=1 such that Score IG(, G, v(1)) Score IG(, G, v(2)) Score IG(, G, v(N)).",
  "Experiments": "To evaluate the effectiveness of the proposed method, we performed experiments on various datasets, graphlearning models, and explainability methods. We evaluated our grXAI framework on seven datasets in bothunsupervised and supervised settings, covering synthetic, biological, citation network, and text data. Thedetails of the experimental settings are included in Appendix F. MUTAG (Debnath et al., 1991), BBBP (Wu et al., 2018), BACE (Wu et al., 2018), and NCI1 (Youet al., 2020) are molecular datasets for graph representation learning. Each graph in these datasets representsa molecule, with nodes representing atoms and edges representing bonds. The labels for these datasetscorrespond to molecular properties and biological activities. BA-Shapes (Yuan et al., 2020; 2021) is asynthetic node classification dataset with 4 unique node labels. Each graph includes a base Barabasi-Albert(BA) graph with embedded five-node house-like motifs. The labels for each node are determined by whetherit belongs to the base graph or different parts of the motif. The Graph-Twitter (Yuan et al., 2020) datasetis a sentiment graph classification dataset with 3 labels. Yuan et al. (2020) convert each tweet sequence intoa graph, where each node represents a word and edges are the relationships between words. Cora (Sen et al.,2008) is a citation network for node embedding tasks.",
  "Evaluation metrics": "As the considered real-world datasets do not provide ground truth for explanations, we follow previous studies(Pope et al., 2019; Xie et al., 2022; Yuan et al., 2020) and adopt Fidelity and Sparsity scores to quantitativelyevaluate the explanations. Fidelity. This metric is the main available metric for evaluating post-hoc graph explanations. It assesseswhether the input subgraphs identified by the explanation method are important (Yuan et al., 2020). If so,then removing these subgraphs should result in a significant change in the models outputs. In the supervisedsetting, Fidelity is calculated as the difference in predicted probability between the original predictions and",
  ": Quantitative results for various explanation techniques, with a preference for higher FidelityCS inhigher Sparsity levels": "the new predictions after masking out important subgraphs (Pope et al., 2019; Yuan et al., 2021). Formally,given a graph G, the softmax output of the predicted class fc(), and its explanation as the subgraph G, wedefine Go = G \\ G as the other graph, i.e. the graph of all possible nodes that are not in G. The Fidelityscore can be computed as Fidelity(G) = fc(G) fc(Go). Building on the original definition, we use three types of Fidelity for evaluating explanations for graphrepresentations: (i) FidelityCS, which measures the negative cosine similarity between the original em-bedding and the new embedding after masking out the important subgraph. Formally, this is calculatedas Sc(f(G), f(G(o))); (ii) Fidelityprob, which evaluates the relative importance of the subgraphs on adownstream task. Specifically, it measures the difference between the predicted probability of the originalembedding and the new embedding after masking out the important subgraph. The predicted probability isgiven by a logistic regression model trained on the original embeddings. Sparsity. Effective explanations should be sparse, which means they should capture the most importantsubgraphs and ignore the irrelevant ones (Yuan et al., 2020). Post-hoc graph explainability methods, includinggrXAI, can control the size of the identified subgraph through various hyperparameters, and the Sparsitymetric measures the proportion of structures identified as important by the explanation method. It isimportant to note that accounting for Sparsity promotes a fair comparison between different methods. Indeed,larger subgraphs generally improve Fidelity, and therefore explanations with different sizes are not directlycomparable. By comparing Fidelity at the same Sparsity, we compare explanations with the same size.",
  "Results and discussion": "Quantitative studies. We evaluate the quality of graph representations in terms of Fidelity and Sparsityscores. Specifically, we assess the Fidelity of both raw embedding vectors (FidelityCS) and their downstreamtasks (Fidelityprob) by comparing our grXAI-based methods (grSubgraphX, grIG, and grSaliency) with theonly available baseline, TAGE (Xie et al., 2022). Please refer to Appendix B.2 for a detailed overview ofTAGE. The naive approach (equation 2) is not included due to its significant time cost on real-world datasets.We trained GNN encoders using two different algorithms for self-supervised graph representation learning,InfoGraph (Sun et al., 2019) and GraphCL (You et al., 2020). The performance of the grXAI framework is demonstrated in , which shows better Fidelity (higherFidelityCS) consistently across all levels of Sparsity. This conclusion is also supported by (a higherFidelityprob corresponds to better performance). Notably, our gradient-based extensions, particularly grIG,perform exceptionally well on multiple datasets, despite their lower computational complexity compared togrSubgraphX. The grSubgraphX approach, the only method that explains graph representation based on connected subgraphs,outperforms TAGE on all datasets except Graph-Twitter. This is likely due to the nature of molecules, wherelocalized functional groups significantly affect global molecular properties. Hence, there is a preference for aninductive bias toward explaining based on connected subgraphs. However, sentiment analysis requires a more",
  "Mutagenic": ": (Left) The explainability of graph representations for six molecules from the MUTAG dataset.Carbon, Oxygen, and Nitrogen atoms are highlighted in yellow, red, and blue, respectively. The top rowdisplays the grSubgraphX explanation, which is a connected subgraph that is more easily interpretable formolecular graphs. In particular, the grSubgraphX explanation often identifies a carbon ring for mutageniccompounds, which is consistent with the prior knowledge of the underlying chemistry.(Right) Theexplainability of graph representations for five molecules with minor structural changes from the MUTAGdataset. The grSubgraphX method generates highly stable explanations, while TAGE is not as stable, i.e.,small changes to the molecular graph produce substantially different explanations. flexible approach to handling disconnected subgraphs. Our gradient-based techniques, grIG and grSaliency,outperform grSubgraphX in such cases since they do not have any constraints on providing connectedsubgraphs. This flexibility allows them to perform better in scenarios where disconnected subgraphs play avital role in the task. In addition to FidelityCS, we also compare grSubgraphX to TAGE based on the InfidelityCS metric, whereInfidelityCS = Scf(G), f(G()). While FidelityCS measures how much the representation changes whenimportant subgraphs are masked, InfidelityCS directly measures how much the important subgraphs contributeto the representation. In contrast to FidelityCS, lower InfidelityCS is desired, meaning that the representationof the identified important subgraph is close to the original graph representation. Figure A1 shows the resultson this metric. As shown, grXAI outperforms the baseline in terms of InfidelityCS, particularly in regions ofhigher sparsity. In most cases, the performance is significantly better across a range of sparsity levels. This ismostly because according to equations 1 and 6, our models optimization to find the important subgraph isbased on minimizing the Infidelity metric. Qualitative studies. To determine the effectiveness of explaining by connected subgraphs in real-worldmolecule datasets, we conducted further investigations using the MUTAG dataset, for which a deeperunderstanding of the underlying mechanism connecting structural features to the property is available (Yuanet al., 2021). Specifically, it is known that carbon rings tend to be mutagenic (Debnath et al., 1991). We studywhether the explanations provided by different methods can identify the carbon rings characterizing the positiveclass. Our results, presented in (left), demonstrate that 1) our grSubgraphX successfully and preciselyidentifies the carbon rings as important subgraphs for the mutagenic class; and 2) grSubgraphX providesmore localized and interpretable explanations for both classes. This is a critical factor in understandingmolecular properties and aiding scientific decision-making, making grSubgraphX a valuable tool for the field.",
  "Time> 11 hours106.32 sec": "shows that the naive calculation of SubgraphX(equation 2) to explain the embedding of InfoGraph re-quires approximately 11 hours per graph. In contrast, ourproposed grSubgraphX method accomplishes the sametask in less than 2 minutes per graph. Consequently, uti-lizing the naive approach would take a minimum of 916days to explain a small molecule dataset comprising 2k samples; however, grXAI archives it within 2 days.",
  "grSubgraphXTAGE": ": Explanations for a specific node in the Coradataset. The plots show a part of the Cora graph, high-lighting the target node and its 2-hops neighboring nodes.The target node is shown as a larger green circle, withdifferent colors denoting node labels. The edges connect-ing the most important nodes are bold. Explanation for node embedding.Althoughwe have focused on graph-level learning tasks, ourgrXAI framework also applies to node embeddingtasks. shows a GNN trained by theGRACE (Zhu et al., 2020) algorithm on the Coradataset (Sen et al., 2008), with explanations givenby grSubgraphX (top) and TAGE (bottom). Qual-itatively, we observe that grSubgraphX identifiesneighbor nodes that are densely connected andhave the same class as the target node (i.e., thenode we are explaining). In contrast, TAGE ex-planations are more disconnected, scattered, anddifficult to interpret. Quantitatively, grSubgraphXachieves better FidelityCS scores across varyingSparsity values (Figure A2), demonstrating thatour framework also improves the performance innode embedding tasks.",
  ".1grXAI in supervised settings": "Although grXAI has been primarily designed to improve performance and efficiency in explaining unsupervisedGNNs, a natural question is whether it maintains the same performance in supervised settings. In thefollowing, we show that not only grXAI versions of the explainability methods achieve identical performanceto their original counterparts with little additional computational overhead, but they also surpass them incases with high-uncertainty predictions. Specifically, we compare our grXAI modifications (grIG, grSaliency,and grSubgraphX) to their original counterparts on a variety of datasets and tasks, including BBBP, MUTAG,and Twitter datasets for graph classification and BA-shape for node classification. As expected, our grXAIresults achieve equivalent performance in this supervised setting (Figure A3, Appendix). This is mainlybecause, in most cases, the GNNs generate highly confident predictions, i.e., fc(Gi) 1 (or fc(vi) 1 in",
  "Tweet II (grIG)Tweet II (IG: Neutral p=0.47) Tweet II (IG: Positive p=0.50)": ": grXAI in supervised settings. (Left) Explanation results on the BA-Shape dataset. The targetnode is shown in a larger size. Different colors denote node labels. Each graph contains a base graph (yellow)and a house-like five-node motif. grSubgraphX (similar to SubgraphX) precisely identifies the motifs asthe explanations. (Right) Explanation of a GCN classifier for the Graph-Twitter dataset. The blue nodesrepresent words. Important subgraphs (edges and nodes) are highlighted in yellow. In the first tweet, weremoved the words as the tweet was an opinion about a specific person. node classification). Node classification results on BA-Shape dataset are shown in (left), with theimportant substructures highlighted in bold. We observe that the grSubgraphX identifies motifs that areconsistent with the ground truth. To investigate when grXAI outperforms the original counterparts in supervised settings, we examined specificexamples from the Twitter dataset with high uncertainty predictions, i.e., fc(Gi) 0.5. (right)shows IG and grIG explanations for two such examples. grIG simultaneously explains both predictions withtwo subgraphs that correspond to both labels with a similar probability. Instead, IG explainability changesdepending on whether the true class or the predicted class is used. This illustrates that traditional methodsbased on a single output may not always accurately and reliably explain the GNN, particularly in cases ofhigh uncertainty, whereas our d-dimensional vector-based approach can provide explanations that directlyreflect a soft distribution over multiple classes.",
  "Discussion": "In this paper, we introduced grXAI, a novel framework to extend score-based graph explainability methodsto unsupervised GNN settings. We analyzed our framework and demonstrated its compatibility with manyscore-based methods, thus highlighting its flexibility. We validated grXAI applied to different self-supervisedgraph representation learning, self-supervised node embedding, and supervised models across several datasets,both qualitatively and quantitatively. Overall, grXAI leads to accurate and stable explanations which canbe computed efficiently and outperform previously-proposed method. We also investigated the impact ofdifferent augmentation strategies on the explainability of graph representations (Appendix D). This newperspective can help domain experts to identify which augmentation technique is more closely related tothe main molecular property of interest, leading to semantically meaningful design choices, and ultimatelyenhancing the effectiveness of self-supervised graph representation learning methods. While our framework has demonstrated success in explaining graph representations in an efficient way,it is important to acknowledge its limitations. The grXAI framework identifies subgraphs that increasethe similarity to the original graph in the representation space from any direction. However, in practicalapplications, some of these directions may not be meaningful or equally important.Instead, the goalmay be to identify subgraphs that increase the similarity to a target graph with desired properties, whilesimultaneously separating them from a set of graphs with undesirable properties in the representation space,such as non-mutagenic molecules. This is similar to how human perception operates (Lin et al., 2023). Webelieve grXAI opens the door to several exciting avenues for future research, such as developing example-basedexplainability of graph representation by examining similarity to other graphs representations (Lin et al.,2023), and extending grXAI to the structure-aware scoring functions, for instance, by adapting GStarX(Zhang et al., 2022).",
  "Julius Adebayo, Justin Gilmer, Michael Muelly, Ian Goodfellow, Moritz Hardt, and Been Kim. Sanity checksfor saliency maps. Advances in neural information processing systems, 31, 2018": "Federico Baldassarre and Hossein Azizpour. Explainability techniques for graph convolutional networks.In International Conference on Machine Learning (ICML) Workshops, 2019 Workshop on Learning andReasoning with Graph-Structured Representations, 2019. Yu-Neng Chuang, Guanchu Wang, Fan Yang, Quan Zhou, Pushkar Tripathi, Xuanting Cai, and Xia Hu.CoRTX: Contrastive framework for real-time explanation. In The Eleventh International Conference onLearning Representations, 2023. URL Jonathan Crabb and Mihaela van der Schaar. Label-free explainability for unsupervised models. In KamalikaChaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato (eds.), Proceedingsof the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine LearningResearch, pp. 43914420. PMLR, 1723 Jul 2022. Asim Kumar Debnath, Rosa L Lopez de Compadre, Gargi Debnath, Alan J Shusterman, and Corwin Hansch.Structure-activity relationship of mutagenic aromatic and heteroaromatic nitro compounds. correlationwith molecular orbital energies and hydrophobicity. Journal of medicinal chemistry, 34(2):786797, 1991.",
  "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectionaltransformers for language understanding. arXiv preprint arXiv:1810.04805, 2018": "Matt Gardner, Joel Grus, Mark Neumann, Oyvind Tafjord, Pradeep Dasigi, Nelson Liu, Matthew Peters,Michael Schmitz, and Luke Zettlemoyer. Allennlp: A deep semantic natural language processing platform.arXiv preprint arXiv:1803.07640, 2018. Ehsan Hajiramezanali, Arman Hasanzadeh, Krishna Narayanan, Nick Duffield, Mingyuan Zhou, and XiaoningQian. Variational graph recurrent neural networks. Advances in neural information processing systems, 32,2019. Ehsan Hajiramezanali, Arman Hasanzadeh, Nick Duffield, Krishna Narayanan, and Xiaoning Qian. Bayrel:Bayesian relational learning for multi-omics data integration. Advances in Neural Information ProcessingSystems, 33:1925119263, 2020. Ehsan Hajiramezanali, Sepideh Maleki, Alex Tseng, Aicha BenTaieb, Gabriele Scalia, and Tommaso Biancalani.On the consistency of gnn explainability methods. In XAI in Action: Past, Present, and Future Applications,2023. Arman Hasanzadeh, Ehsan Hajiramezanali, Krishna Narayanan, Nick Duffield, Mingyuan Zhou, and XiaoningQian. Semi-implicit graph variational auto-encoders. Advances in neural information processing systems,32, 2019. Arman Hasanzadeh, Ehsan Hajiramezanali, Shahin Boluki, Mingyuan Zhou, Nick Duffield, Krishna Narayanan,and Xiaoning Qian. Bayesian graph neural networks with adaptive connection sampling. In Internationalconference on machine learning, pp. 40944104. PMLR, 2020.",
  "Wengong Jin, Regina Barzilay, and Tommi Jaakkola. Multi-objective molecule generation using interpretablesubstructures. In International conference on machine learning, pp. 48494859. PMLR, 2020": "Pang Wei Koh, Thao Nguyen, Yew Siang Tang, Stephen Mussmann, Emma Pierson, Been Kim, and PercyLiang. Concept bottleneck models. In International Conference on Machine Learning, pp. 53385348.PMLR, 2020. Narine Kokhlikyan, Vivek Miglani, Miguel Martin, Edward Wang, Bilal Alsallakh, Jonathan Reynolds,Alexander Melnikov, Natalia Kliushkina, Carlos Araya, Siqi Yan, and Orion Reblitz-Richardson. Captum:A unified and generic model interpretability library for pytorch, 2020.",
  "Chris Lin, Hugh Chen, Chanwoo Kim, and Su-In Lee.Contrastive corpus attribution for explainingrepresentations. In The Eleventh International Conference on Learning Representations, 2023. URL": "Meng Liu, Youzhi Luo, Limei Wang, Yaochen Xie, Hao Yuan, Shurui Gui, Haiyang Yu, Zhao Xu, JingtunZhang, Yi Liu, Keqiang Yan, Haoran Liu, Cong Fu, Bora M Oztekin, Xuan Zhang, and Shuiwang Ji. DIG:A turnkey library for diving into graph deep learning research. Journal of Machine Learning Research, 22(240):19, 2021. URL Stephen Zhewen Lu, Ziqing Lu, Ehsan Hajiramezanali, Tommaso Biancalani, Yoshua Bengio, Gabriele Scalia,and Micha Koziarski. Cell morphology-guided small molecule generation with gflownets. In ICML 2024Workshop on Structured Probabilistic Inference {\\&} Generative Modeling, 2024.",
  "Dongsheng Luo, Wei Cheng, Dongkuan Xu, Wenchao Yu, Bo Zong, Haifeng Chen, and Xiang Zhang.Parameterized explainer for graph neural network, 2020. URL": "Brooks Paige, Jan-Willem van de Meent, Alban Desmaison, Noah Goodman, Pushmeet Kohli, Frank Wood,Philip Torr, et al. Learning disentangled representations with semi-supervised deep generative models.Advances in neural information processing systems, 30, 2017. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen,Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, ZacharyDeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, andSoumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In Advances inNeural Information Processing Systems 32, pp. 80248035. Curran Associates, Inc., 2019. Phillip E Pope, Soheil Kolouri, Mohammad Rostami, Charles E Martin, and Heiko Hoffmann. Explainabilitymethods for graph convolutional neural networks. In Proceedings of the IEEE Conference on ComputerVision and Pattern Recognition, pp. 1077210781, 2019.",
  "Mukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic attribution for deep networks. In Internationalconference on machine learning, pp. 33193328. PMLR, 2017": "Kristoffer K Wickstrm, Daniel J Trosten, Sigurd Lkse, Ahcne Boubekki, Karl yvind Mikalsen, Michael CKampffmeyer, and Robert Jenssen. Relax: Representation learning explainability. International Journal ofComputer Vision, pp. 127, 2023. Zhenqin Wu, Bharath Ramsundar, Evan N Feinberg, Joseph Gomes, Caleb Geniesse, Aneesh S Pappu, KarlLeswing, and Vijay Pande. Moleculenet: a benchmark for molecular machine learning. Chemical science, 9(2):513530, 2018. Yaochen Xie, Sumeet Katariya, Xianfeng Tang, Edward Huang, Nikhil Rao, Karthik Subbian, and ShuiwangJi. Task-agnostic graph explanations. In Advances in Neural Information Processing Systems, volume 35,pp. 1202712039. Curran Associates, Inc., 2022.",
  "Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? InInternational Conference on Learning Representations, 2019. URL": "Ziyuan Ye, Rihan Huang, Qilin Wu, and Quanying Liu. Same: Uncovering gnn black box with structure-awareshapley-based multipiece explanations. Advances in Neural Information Processing Systems, 36, 2024. Zhitao Ying, Dylan Bourgeois, Jiaxuan You, Marinka Zitnik, and Jure Leskovec. Gnnexplainer: Generatingexplanations for graph neural networks. Advances in neural information processing systems, 32, 2019.",
  "ADetails of datasets": "We evaluated our grXAI framework on seven datasets in both unsupervised and supervised settings, coveringsynthetic, biological, citation network, and text data. The datasets are summarized as follows: Graph-Twitter (Yuan et al., 2020) dataset is a sentiment graph classification dataset with 3 labels. Yuanet al. (2020) convert each tweet sequence into a graph, where each node represents a word and edges representthe relationships between words. Biaffine parser (Gardner et al., 2018) has been used to extract worddependencies, and a pre-trained 12-layer BERT (Devlin et al., 2018) model has been used to extract a768-dimensional feature vector for each word. BA-Shape (Yuan et al., 2020) is a synthetic node classification dataset with 4 labels. Each graph contains abase graph (300 nodes) and a house-like five-node motif. The base graph is obtained by the Barabsi-Albert(BA) model, which can generate random scale-free networks with a preferential attachment mechanism. Themotif is attached to the base graph while random edges are added. Each node is labeled based on whether itbelongs to the base graph or to different spatial locations of the motif. Molecule dataset. Molecular datasets such as MUTAG, BBBP, BACE, and NCI1 are widely used inexplanation and graph representation learning tasks (You et al., 2020; Yuan et al., 2021; Sun, 2022). Eachgraph in such datasets corresponds to a molecule where nodes represent atoms and edges are the chemicalbonds. The labels of molecular graphs are generally determined by the chemical functionalities or propertiesof the molecules. In particular, in the MUTAG dataset, molecular graphs are labeled based on their mutageniceffects on a bacterium. It is known that carbon rings and NO2 chemical groups may lead to mutagenic effects(Yuan et al., 2021; Zhang et al., 2022).",
  "We applied our explainer to two self-supervised graph-level representation learning models:": "GraphCL (You et al., 2020). GraphCL is a contrastive learning framework for learning unsupervisedrepresentations of graph data. GraphCl proposes several graph data augmentations as well as a novel graphcontrastive learning framework for GNN pre-training. InfoGraph (Sun et al., 2019).Infograph maximizes the mutual information between the graph-levelrepresentation and the representations of different components with different scales (e.g., nodes, edges,triangles). By doing so, the graph-level representations encode aspects of the data that are shared acrossdifferent scales of substructures. We also used GRACE (Zhu et al., 2020) for pre-training GNN encoders in node-level self-supervised settings.GRACE is an unsupervised graph representation learning framework that leverages a contrastive objective atthe node level. Specifically, it generates two graph views by corruption and learns node representations bymaximizing the agreement of node representations in these two views.",
  "B.2Task-agnostic GNN explainer": "TAGE, introduced by Xie et al. (2022), decomposes a supervised GNN into an encoder model and a downstreammodel, designing separate explainers for each component. The embedding explainer part, which is the onlyavailable baseline for explaining graph representations, is trained using a self-supervised training framework. TAGE aims to maximize the mutual information (MI) between two embeddings: one from the input graph Gand one from the corresponding important subgraph G induced by the explainer. It introduces a maskingvector p to indicate specific dimensions of the embeddings to maximize the MI. During training, TAGEsamples the masking vector p from a multivariate Laplace distribution to exploit sparse gradients, ensuringthat only a few dimensions are of high importance (Xie et al., 2022). This assumes that embeddings from",
  "maxEp [MI (p f(G), p f((p, G)))] ,": "where MI(, ) computes the mutual information between two random vectors, p denotes the random maskingvector sampled from a certain distribution, (p, G) identifies the subgraph of high importance, and denotesthe element-wise multiplication, which applies masking to the embeddings f(). Intuitively, given an inputgraph G and the desired embedding dimensions to be explained, the explainer predicts the subgraphwhose embedding shares the maximum mutual information with the original embedding on the desireddimensions (Xie et al., 2022). To efficiently compute the mutual information, TAGE uses a contrastive loss such as InfoNCE. Additionally,to restrict the size of the subgraphs provided by the explainer, TAGE includes a size regularization term (Xieet al., 2022). TAGE employs a multilayer perceptron (MLP) as the base architecture of the explainer ,which predicts the importance score for each edge, leading to disconnected subgraphs.",
  "CAdditional experiments": "Explanation for graph representations.Figure A1 compares grSubgraphX and TAGE based on theInfidelityCS metric, where InfidelityCS = Scf(G), f(G()). InfidelityCS directly measures how much theimportant subgraphs contribute to the representation; therefore, lower InfidelityCS means the representation ofthe identified important subgraph is close to the original graph representation. As shown, grXAI outperformsTAGE in terms of InfidelityCS in most cases. This is mostly because according to equations 1 and 6,grSubgraphX optimization is based on the Infidelity metric. 0.500.550.600.650.700.750.800.85",
  "Figure A1: Quantitative results for various explanation techniques, with a preference for lower InfidelityCS inhigher Sparsity levels": "Explanation for node embedding.As we stated in the main paper, grSubgraphX has a better explanationperformance in terms of both FidelityCS and visualization. For evaluating this, we trained a GNN usingGRACE (Zhu et al., 2020) on the citation network Cora. Then, we compare the performance of grSubgraphXto the baseline (TAGE) using test nodes that have at least 50 nodes in their 2-hop subnetworks. Figure A2also shows that grSubgraphX outperforms TAGE in terms of FidelityCS for different Sparsity levels. Pleasenote that due to the higher number of classes in this dataset and the fact that the node embedding hasbeen calculated as the output of a two-layer GNN, the change in FidelityCS is lower here compared to otherexperiments. grXAI in supervised settings.We compare our grXAI-based methods, which employ d-dimensionalsoftmax outputs (grIG, grSaliency, and grSubgraphX), with their original counterparts that rely on explainingthe predicted class using the BBBP dataset (as shown in Figure A3). Our proposed grXAI frameworkhas identical performance to its original counterparts but is more efficient, especially for high-confidencepredictions. This is because supervised GNNs are often overconfident in their predicted class, with fc(Gi) 1,and therefore taking into account all the outputs does not significantly affect the performance. Effect of the wrapper function.In unsupervised settings, the individual dimensions of a latent vectordo not necessarily correspond to probability vectors. To address this, we replaced the weighted sum inEquation 3 with a cosine similarity, which can be seen as a method of normalization that is invariant withrespect to latent symmetries. We also experimentally evaluate this selection compared to the dot-product.",
  "Figure A3: Fidelity comparisons of our proposed grXAI-based methods with their original implementation onsupervised task for BBBP dataset": "Our results show that the wrapper function has little effect on the performance of the grSubgraphX. In somedatasets, cosine similarity yields slightly better average results, as seen in Figure A4. This might be due tothe fact that the latent vector typically corresponds to the activation functions of the neurons in the GNN(e.g. ReLU or sigmoid), which reduce the weight of inactivated components on the score function (Crabb &van der Schaar, 2022). Stability study.As we stated in the main paper, there have been arguments that post-hoc (graph)explainability methods may lack stability (Slack et al., 2021; Adebayo et al., 2018), as even slight changes toan instance can lead to significantly different explanations. In addition to (right) in the main paper,our results in Figure A5 show that grSubgraphX is able to provide very stable explanations for moleculeswith minor structural differences, while TAGE generates completely different explanations.",
  "DAugmentation design": "In self-supervised settings, understanding the augmentation strategies can inform the design of novel learningtechniques that are not solely based on empirical approaches. For instance, the GraphCL (You et al.,2020) framework introduced various types of graph augmentations, each of which incorporates specific priorknowledge about graph data. Results such as those shown in Figure A6 and Table A1, demonstrate that",
  "Molecule Set IMolecule Set II": "Figure A5: The explainability of graph representations for two sets of molecules with minor structural changesfrom the MUTAG dataset. The grSubgraphX generates highly stable explanations, while TAGE is not stable,i.e. small changes to the sample produce substantially different explanations. different augmentations used in GraphCL training correspond to different explanation subgraphs, and havedifferent Fidelity, meaning that perturbing them will affect the representation learning differently. Thisinsight can aid in understanding the effectiveness of self-supervised learning on graph data and provide abasis for comparing different pre-training methods beyond their predicted performance.",
  "EComputational complexity": "We further compare the computational complexity of our grXAI-based methods to their traditional coun-terparts on various supervised node and graph classification datasets. As seen in Table A2, our proposedmethods have close computational complexity to their traditional counterparts and are significantly fasterthan averaging over all output components. We should also point out that providing a connected subgraph",
  "maskNpermEsubgraph": "Figure A6: The explanation for graph representations of two different MUTAG molecules learned byGraphCL. Each column corresponds to a different augmentation technique to train GraphCL. Using differentaugmentation pushes the model to learn the representation based on different substructures of input molecules,and some of them might be more aligned with prior knowledge. to explain GNN encoders using grSubgraphX comes at the cost of higher computational complexity. In caseswhere computational complexity is a bottleneck, we suggest applying our gradient-based extensions, i.e. grIGand grSaliency.",
  "FExperimental settings": "We used PyTorch (Paszke et al., 2019) to develop our grXAIe framework and conducted experiments on anNvidia A100 with 80 GB of memory. For supervised experiments, we used GNN checkpoints from DIG (Liuet al., 2021) as part of Yuan et al. (2020) and did not train any new models. We also utilized the DIG packages"
}