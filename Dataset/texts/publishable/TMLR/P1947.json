{
  "Abstract": "The need for abundant labelled data in supervised Adversarial Training (AT) has promptedthe use of Self-Supervised Learning (SSL) techniques with AT. However, the direct applicationof existing SSL methods to adversarial training has been sub-optimal due to the increasedtraining complexity of combining SSL with AT. A recent approach DeACL (Zhang et al.,2022) mitigates this by utilizing supervision from a standard SSL teacher in a distillationsetting, to mimic supervised AT. However, we find that there is still a large performance gapwhen compared to supervised adversarial training, specifically on larger models. In this work,investigate the key reason for this gap and propose Projected Feature Adversarial Training(ProFeAT) to bridge the same. We show that the sub-optimal distillation performance is aresult of mismatch in training objectives of the teacher and student, and propose to use aprojection head at the student, that allows it to leverage weak supervision from the teacherwhile also being able to learn adversarially robust representations that are distinct fromthe teacher. We further propose appropriate attack and defense losses at the feature andprojector, alongside a combination of weak and strong augmentations for the teacher andstudent respectively, to improve the training data diversity without increasing the trainingcomplexity. Through extensive experiments on several benchmark datasets and models, wedemonstrate significant improvements in both clean and robust accuracy when compared toexisting SSL-AT methods, setting a new state-of-the-art. We further report on-par/ improvedperformance when compared to TRADES, a popular supervised-AT method. Our code isavailable at:",
  "Introduction": "Deep Neural Networks are known to be vulnerable to crafted imperceptible input-space perturbations knownas Adversarial attacks (Szegedy et al., 2013), which can be used to fool classification networks into predictingany desired output, leading to disastrous consequences. Amongst the diverse attempts at improving theadversarial robustness of Deep Networks, Adversarial Training (AT) (Madry et al., 2018; Zhang et al., 2019)has been the most successful. This involves the generation of adversarial attacks by maximizing the trainingloss, and further minimizing the loss on the generated attacks for training. While adversarial training basedmethods have proved to be robust against various attacks developed over time (Carlini et al., 2019; Croce &Hein, 2020; Sriramanan et al., 2020), they require significantly more training data when compared to standardtraining (Schmidt et al., 2018), incurring a large annotation cost. This motivates the need for self-supervisedlearning (SSL) of robust representations, followed by lightweight standard training of the classification head.Motivated by the success of contrastive learning for standard self-supervised learning (Van den Oord et al.,2018; Chen et al., 2020b; He et al., 2020), several works have attempted to use contrastive learning forself-supervised adversarial training as well (Jiang et al., 2020; Kim et al., 2020; Fan et al., 2021). While thisstrategy works well in a full network fine-tuning setting, the performance is sub-optimal when the robustly",
  "Published in Transactions on Machine Learning Research (10/2024)": "order to ensure consistency across different SSL methods, we use a random trainable projector (2-layer MLPwith both hidden and output dimensions of 640) for training the student and do not employ any projectionhead for the pretrained frozen teacher. While the default teacher trained using SimCLR was finetuned acrosshyperparameters, we utilize the default hyperparameters from the solo-learn Github repository for thistable, and thus present SimCLR also without tuning for a fair comparison. For uniformity, we report allresults with = 8 (the robustness-accuracy trade-off parameter). From , we note that in mostcases, the clean accuracy of the student increases as the accuracy of the teacher improves, while the robustaccuracy does not change much. We note that this table merely shows that the proposed approach can beeffectively integrated with several base self-supervised learning algorithms for the teacher model. However,it does not present a fair comparison across different SSL pretraining algorithms, since the ranking on thefinal performance of the student would change if the pretraining SSL algorithms were used with appropriatehyperparameter tuning.",
  "Preliminaries": "We consider the problem of self-supervised learning of robust representations, where a self-supervised standardtrained teacher model T is used to provide supervision to a student model S. The feature, projection andlinear probing layers of the teacher are denoted as Tf, Tp and Tl, respectively. We will use projection layerand projector interchangeably in the rest of the paper. A composition of the feature extractor followed bythe projector of the teacher is denoted as Tpf = Tp Tf, and a composition of the feature extractor followedby the linear probing layer is denoted as Tlf = Tl Tf. An analogous notation is followed for the student aswell. The dataset used for self-supervised pretraining D consists of images xi where i N. An adversarial",
  "Related Works": "Self Supervised Learning (SSL): Early works on SSL had focused on designing well-posed tasks calledpretext tasks, such as rotation prediction (Gidaris et al., 2018) and solving jigsaw puzzles (Noroozi &Favaro, 2016), to provide a meaningful supervisory signal in the absence of true labels. However, the designof these hand-crafted tasks involves manual effort, and is generally specific to the dataset and trainingtask. To alleviate this problem, contrastive learning based SSL approaches have emerged as a promisingdirection (Van den Oord et al., 2018; Chen et al., 2020b; He et al., 2020), where different augmentations of agiven anchor image form the positives, and augmentations of other images in the batch form the negatives.The training objective involves pulling the representations of the positives together, and pushing away therepresentations of negatives. Strong augmentations like random cropping and color jitter are applied to makethe learning task sufficiently hard, while also enabling the learning of invariant representations. Self Supervised Adversarial Training: To alleviate the large sample complexity and training cost ofadversarial training, there have been several works that have attempted self-supervised learning of adversariallyrobust representations. Chen et al. (2020a) propose AP-DPE, an ensemble adversarial pretraining frameworkwhere several pretext tasks like Jigsaw puzzles (Noroozi & Favaro, 2016), rotation prediction (Gidaris et al.,2018) and Selfie (Trinh et al., 2019) are combined to learn robust representations without task labels. Jianget al. (2020) propose ACL, that combines the popular contrastive SSL method - SimCLR (Chen et al.,2020b) with adversarial training, using Dual Batch normalization layers for the student model - one for thestandard branch and another for the adversarial branch. RoCL (Kim et al., 2020) follows a similar approachto ACL by combining the contrastive objective with adversarial training to learn robust representations. Fanet al. (2021) propose AdvCL, that uses high-frequency components in data as augmentations in contrastivelearning, performs attacks on unaugmented images, and uses a pseudo label based loss for training tominimize the cross-task robustness transferability. Luo et al. (2023) study the role of augmentation strength inself-supervised contrastive adversarial training, and propose DynACL, that uses a strong-to-weak\" annealingschedule on augmentations. Additionally, motivated by Kumar et al. (2022), they propose DynACL++ thatobtains pseudo-labels via k-means clustering on the clean branch of the DynACL pretrained network, andperforms linear-probing (LP) using these pseudo-labels followed by adversarial full-finetuning (AFT) of thebackbone. This is a generic strategy that can be integrated with several algorithms including ours. Decoupled Adversarial Contrastive Learning (DeACL): While most self-supervised adversarial trainingmethods aimed at integrating contrastive learning methods with adversarial training, Zhang et al. (2022)showed that combining the two is a complex optimization problem due to their conflicting requirements. Theauthors propose DeACL, where a teacher model is first trained using existing self-supervised training methodssuch as SimCLR (Chen et al., 2020b), followed by the adversarial training of a student model using thesupervision from the earlier-trained teacher in a distillation framework, as shown in in Appendix A.1.The loss used during distillation is a combination of the cosine similarity between the representations ofthe clean image at the output of the teacher and student, along with a smoothness loss at the output ofthe student, as shown in Equation (5). The attack used during training is generated by a minimizing thecosine similarity between the teacher and student representations at the feature output (see Equation (6)).We present a more detailed description of DeACL in Appendix A.1. The robust student model is used forthe downstream tasks. While existing methods used 1000 epochs for contrastive adversarial training, thecompute requirement for DeACL is much lesser since the first stage does not involve adversarial training,and the second stage is similar in complexity to supervised adversarial training (details in Appendix E). Inthis work, we investigate and address the shortcomings of the distillation framework presented in DeACL byintroducing a projection layer during the distillation process, along with well-designed training and attacklosses, and an appropriate augmentation scheme for better attack diversity, when compared to DeACL.",
  "Projection Layer in Self-supervised Distillation": "In this work, we follow the setting proposed by Zhang et al. (2022), where a standard self-supervisedpretrained teacher provides supervision for self-supervised adversarial training of the student model. This isdifferent from a standard distillation setting (Hinton et al., 2015) because the representations of standardand adversarially trained models are known to be inherently different (Engstrom et al., 2019). Ilyas et al.(2019) attribute the adversarial vulnerability of models to the presence of non-robust features which canbe disentangled from robust features that are learned by adversarially trained models. The differences inrepresentations of standard and adversarially trained models can also be justified by the fact that linearprobing of standard trained models using adversarial training cannot produce robust models as shown in. Similarly, standard full finetuning of adversarially trained models destroys the robust featureslearned (Chen et al., 2020a; Kim et al., 2020; Fan et al., 2021), yielding 0% robustness, as shown in the table.Due to the inherently diverse representations of these models, the ideal goal of the student in the considereddistillation setting is not to merely follow the teacher, but to be able to take weak supervision from it whilebeing able to differ considerably. In order to achieve this, we take inspiration from standard self-supervisedlearning literature (Van den Oord et al., 2018; Chen et al., 2020b; He et al., 2020; Navaneet et al., 2022; Gaoet al., 2022; Gupta et al., 2022) and propose to utilize a projection layer following the student backbone,so as to isolate the impact of the enforced loss on the learned representations. Bordes et al. (2022) showthat in standard supervised and self-supervised training, a projector is useful when there is a misalignmentbetween the pretraining and downstream tasks, and aligning them can eliminate the need for the same. Xueet al. (2024) further shows theoretically that lower layers represent more features evenly resulting in bettergeneralizability of the learned representations. Motivated by this, we hypothesize the following for the settingof self-supervised distillation:",
  ". Pretraining and linear probe training objectives of the student": "Here, the ideal goal of the student depends on the downstream task, which is clean (or standard) accuracy instandard training, and clean and robust accuracy in adversarial training. On the other hand, the trainingobjective of the standard self-supervised trained teacher is to achieve invariance to augmentations of the sameimage when compared to augmentations of other images. We now explain the intuition behind the above hypotheses and empirically justify the same by consideringseveral distillation settings involving standard and adversarial, supervised and self-supervised trained teachermodels in Tables 2 and 3. The results are presented on CIFAR-100 (Krizhevsky et al., 2009) with WideResNet-34-10 (Zagoruyko & Komodakis, 2016) architecture for both teacher and student. The standard self-supervisedmodel is trained using SimCLR (Chen et al., 2020b). Contrary to a typical knowledge distillation settingwhere a cross-entropy loss is also used (Hinton et al., 2015), all the experiments presented involve the use ofonly self-supervised losses for distillation (cosine similarity between representations), and labels are used onlyduring linear probing. Adversarial self-supervised distillation in is performed using a combination of",
  "Exp #TeachertrainingTeacheracc (%)ProjectorLP LossStudent accuracy after linear probecos(T , S)Feature space (%)Projector space (%)Feature spaceProjector space": "S1Self-supervised70.85AbsentCE64.90-0.94-S2Self-supervised70.85Absentcos(T , S)68.49-0.94-S3Supervised80.86AbsentCE80.40-0.94-S4Supervised69.96AbsentCE71.73-0.98S5Self-supervised70.85PresentCE73.1464.670.190.92 : Role of projector in self-supervised adversarial distillation (CIFAR-100, WRN-34-10):Student performance after linear probe at feature space is reported. The drop in standard accuracy (SA) ofthe student (S) w.r.t. the teacher (T ), and the robust accuracy (RA-G) of the student improve by matchingthe training objective of the teacher with ideal goals of the student (A3 vs. A1), and by using similar lossesfor pretraining and linear probing (LP) (A2 vs. A1). Using a projector improves performance in case ofmismatch in the above (A4 vs. A1).",
  "Exp #Teacher trainingTeacher accuracyProjectorLP LossStudent accuracycos(T , S)SA (%)RA-G (%)SA (%)RA-G (%)": "A1Self-supervised (standard training)70.850AbsentCE50.7124.630.78A2 (DeACL)Self-supervised (standard training)70.850Absentcos(T , S)54.4823.200.78A3Supervised (TRADES adversarial training)59.8825.89AbsentCE54.8627.170.94A4Self-supervised (standard training)70.850PresentCE57.5124.100.18 distillation loss on natural samples and smoothness loss on adversarial samples as shown in Equation (2)(Zhang et al., 2022). A randomly initialized trainable projector is used at the output of student backbone inS5 of and A4 of . Here, the training loss is considered in the projection space of the student(Sp) rather than the feature space (Sf). 1. Matching the training objectives of teacher with the ideal goals of the student: Considertask-A to be the teachers training task, and task-B to be the students downstream task or its ideal goal.The representations in deeper layers (last few layers) of the teacher are more tuned to its training objective,and the early layers contain a lot more information than what is needed for this task (Bordes et al., 2022).Thus, features specific to task-A are dominant or replicated in the final feature layer, and other features thatmay be relevant to task-B are sparse. When a similarity based distillation loss is enforced on such features,higher importance is given to matching the task-A dominated features, and the sparse features which maybe important for task-B are suppressed further in the student (Addepalli et al., 2023). On the other hand,when the students task matches with the teachers task, a similarity based distillation loss is very effectivein transferring the necessary representations to the student, since they are predominant in the final featurelayer. Thus, matching the training objective of the teacher with the ideal goals of the student should improvedownstream performance. To test this hypothesis, we first consider the standard training of a student model, using either a self-supervisedor supervised teacher, in . One can note that in the absence of a projector, the drop in studentaccuracy w.r.t. the respective teacher accuracy is 6% with a self-supervised teacher (S1), and < 0.5% with asupervised teacher (S3). To ensure that our observations are not a result of the 10% difference in teacheraccuracy between S1 and S3, we present results and similar observations with a supervised sub-optimallytrained teacher in S4. Thus, a supervised teacher is significantly better than a self-supervised teacher fordistilling representations specific to a given task. This justifies the hypothesis that, student performanceimproves by matching the training objectives of the teacher and the ideal goals of the student.",
  "ProFeAT: Projected Feature Adversarial Training": "We now present our proposed approach ProFeAT which is illustrated in . ProFeAT training happensin two stages. Firstly, a teacher model T is trained using a standard self-supervised learning algorithmsuch as SimCLR (Chen et al., 2020b), whose weights are used as an initialization for the student S forbetter convergence in the next stage. Next, the student model is trained robustly in a distillation frameworkwhere the teacher model supervises the student model (see ). Both the student and the teacher aretrained with a projection layer on top of their feature backbone. The projection layer is kept frozen andis initialized using the learned weights from the earlier teacher training. We use well-designed attack anddefense losses along with appropriate augmentations to ensure the student model is trained robustly whiletaking appropriate supervision from the teacher model. We now describe in detail the individual designcomponents of the proposed approach ProFeAT: Projection Layer: As discussed in .1, to overcome the impact of the inherent misalignment betweenthe training objective of the teacher and the ideal goals of the student, and to mitigate the mismatch betweenthe pretraining and linear probing objectives, we propose to use a projection head at the output of both theteacher and the student backbone. Most self-supervised pretraining methods (Chen et al., 2020b; He et al.,2020; Grill et al., 2020; Chen & He, 2021; Zbontar et al., 2021) use similarity based losses at the output ofa projection layer for standard training, resulting in a projected space where similarity has been enforcedduring pretraining, thus giving higher importance to the key dimensions. Defense Loss: As is common in adversarial training literature (Zhang et al., 2019), we use a combinationof (1) loss on clean samples and (2) a smoothness loss, to enforce adversarial robustness in the studentmodel. Since the loss on clean samples utilizes supervision from the self-supervised pretrained teacher, it isenforced at the outputs of the respective projectors of the teacher and student. The goal of the smoothnessloss is to enforce local smoothness in the loss surface of the student backbone (Zhang et al., 2019), and is",
  "PGD attack": ": Proposed approach (ProFeAT): The student is trained using a distillation loss on clean samplesusing supervision from an SSL pretrained teacher, and a smoothness loss to enforce adversarial robustness(details of exact loss formulation is presented in ). A frozen pretrained projection layer is used at theteacher and student to prevent overfitting to the clean distillation loss. The use of strong augmentations at thestudent increases attack diversity, while weak augmentations at the teacher reduce the training complexity. ideally enforced at the feature space of the student network, since these representations are directly used fordownstream applications. While the ideal locations for the clean and adversarial losses are the projectedand feature spaces respectively, we find that such a loss formulation is hard to optimize, resulting in either anon-robust model, or collapsed representations (shown in ). We therefore use a complimentary lossas a regularizer in the respective spaces. This results in a combination of losses at the feature and projectionspaces as shown below:",
  "xi =arg minxi:||xixi||cosTpf(xi), Spf(xi)+ cosSf(xi), Sf(xi)(4)": "Here, Lpf and Lf are the defense losses enforced at the projection and feature spaces, respectively. S = S(x)is the student representation for the adversarial input x (see for notations). The first terms inEquations (1) and (2) represents the Distillation loss (see ) involving only the clean input xi, whereasthe second terms correspond to the Smoothness loss at the respective layers of the student, involving bothclean and adversarial input xi, and is weighted by a hyperparameter that controls the robustness-accuracytrade-off in the downstream model. The overall loss LProFeAT in Equation (3) is minimized during training. Attack generation: The attack used during training is generated by a maximizing a combination of lossesat both projection and feature spaces as shown in Equation (4). Since the projection space is primarily usedfor enforcing similarity with the teacher, we minimize the cosine similarity between the teacher and studentrepresentations for attack generation. Since the feature space is primarily used for enforcing local smoothnessin the loss surface of the student, we utilize the unsupervised formulation that minimizes similarity betweenrepresentations of clean and adversarial samples at the student. Augmentations: Standard supervised and self-supervised training approaches are known to benefit from theuse of strong data augmentations such as AutoAugment (Cubuk et al., 2018). However, such augmentations,which distort the low-level features of images, are known to deteriorate the performance of adversarial training(Rice et al., 2020; Gowal et al., 2020). Addepalli et al. (2022) attribute the poor performance to the largerdomain shift between the augmented train and unaugmented test set images, in addition to the increasedcomplexity of the adversarial training task, which overpower the superior generalization attained due to theuse of diverse augmentations. Although these factors influence adversarial training in the self-supervisedregime as well, we hypothesize that the need for better generalization is higher in self-supervised training,",
  "DeACL83.83 0.2057.09 0.0648.85 0.1152.92 0.3532.66 0.0823.82 0.07ProFeAT (Ours)87.62 0.1354.50 0.1751.95 0.1961.08 0.1831.96 0.0826.81 0.11": "since the pretraining task is not aligned with the ideal goals of the student, making it important to usestrong augmentations. However, it is also important to ensure that the training task is not too complex. Wethus propose to use a combination of weak and strong augmentations as inputs to the teacher and studentrespectively, as shown in . From we note that, the use of strong augmentations results inthe generation of more diverse attacks, resulting in a larger drop when differently augmented images areused across different restarts of a PGD 5-step attack. The use of weak augmentations at the teacher impartsbetter supervision to the student, reducing the training complexity.",
  "Experiments and Results": "We first present an empirical evaluation of the proposed method, followed by several ablation experiments tounderstand the role of each component individually. Due to lack of space, details on the training, datasetsand compute are presented in Appendices C and D. For evaluation, we compare the proposed approach ProFeAT with several existing self-supervised adversarialtraining approaches (Chen et al., 2020a; Kim et al., 2020; Jiang et al., 2020; Fan et al., 2021; Zhang et al.,2022; 2019) by performing linear probing (LP) using cross-entropy loss on clean samples. To ensure a faircomparison, the same is done for the supervised AT method TRADES (Zhang et al., 2019) as well. Theresults are presented on CIFAR-10 and CIFAR-100 datasets Krizhevsky et al. (2009), and on ResNet-18He et al. (2016) and WideResNet-34-10 (WRN-34-10) Zagoruyko & Komodakis (2016) architectures, asis common in the adversarial research community. The results of existing methods on ResNet-18 are asreported by Zhang et al. (2022). Since DeACL (Zhang et al., 2022) also uses a teacher-student architecture,we reproduce their results using the same teacher as used in our method, and report it as DeACL (OurTeacher). Since existing methods do not report results on larger architectures like WideResNet-34-10, wecompare our results only with the best performing method (DeACL) and a recent approach DynACL (Luoet al., 2023). As the results with larger architectures are not reported in these papers, we run them usingtheir official code.",
  "DeACL52.9024.6655.0522.0456.8231.26ProFeAT (Ours)61.0527.4163.8126.1058.0932.26": "The Robust Accuracy (RA) in the SOTA comparison table is presented against AutoAttack (RA-AA) (Croce& Hein, 2020) which is widely used as a reliable benchmark for robustness evaluation (Croce et al., 2021). Inother tables, we also present robust accuracy against the GAMA attack (RA-G) (Sriramanan et al., 2020)which is known to be a competent and a reliable estimate of AutoAttack while being significantly faster toevaluate. We additionally present results against a 20-step PGD attack (RA-PGD20) (Madry et al., 2018)in the SOTA comparison table (), although it is a significantly weaker attack. A larger differencebetween RA-PGD20 and RA-AA indicates that the loss surface is more convoluted due to which weakerattacks are unsuccessful, yielding a false sense of robustness (Athalye et al., 2018). Thus this difference servesas a check for verifying the extent of gradient masking (Papernot et al., 2017; Tramr et al., 2018). Therefore,in order to compare true robustness between any two defenses, accuracy against RA-AA or RA-G should beconsidered, while RA-PGD20 should not be considered. The accuracy on clean or natural samples is denotedas SA, which stands for Standard (Clean) Accuracy.",
  "Comparison with the state-of-the-art": "presents the standard linear probing results of the proposed method ProFeAT in comparison to severalSSL-AT baseline approaches. The proposed approach obtains superior robustness-accuracy trade-off whencompared to the best performing baseline method DeACL, with 33.5% gains in both robust and cleanaccuracy on CIFAR-10 dataset and similar gains in robustness on CIFAR-100 dataset with WideResNet-34-10architecture. We obtain significant gains of 8% on the clean accuracy on CIFAR-100. With ResNet-18architecture, ProFeAT achieves competent robustness-accuracy trade-off when compared to DeACL on CIFAR-10 dataset, and obtains 2% higher clean accuracy alongside improved robustness on CIFAR-100 dataset.We notice significantly less gradient masking, indicated by a lower value of (RA-PGD20 RA-AA), for theproposed approach compared to all other baselines across all settings, indicating a reliable attack generationeven in the absence of ground truth labels. Overall, the proposed approach significantly outperforms allthe existing baselines, especially for larger model capacities (WRN-34-10), with improved results on smallermodels (ResNet-18). Additionally, we obtain superior results when compared to the supervised AT methodTRADES as well, at higher model capacities. Performance comparison with other evaluation methods: We also present results of additionalevaluation methods to evaluate the performance of the pretrained backbone in . We note that theproposed method achieves improvements over the baseline across all evaluation methods. Since the trainingof classifier head in LP and MLP is done using standard training and not adversarial training, the robustaccuracy reduces as the number of layers increases (from linear to 2-layers), and the standard accuracyimproves. The standard accuracy of KNN is better than the standard accuracy of LP for the baseline,indicating that the representations are not linearly separable. Whereas, as is standard, for the proposedapproach, LP standard accuracy is higher than that obtained using KNN. The adversarial attack used forevaluating the robust accuracy using KNN is generated using GAMA attack on a linear classifier. The attack",
  "Supervised62.4639.4064.9741.02DeACL62.6539.1861.0139.09ProFeAT66.1142.1264.1641.25": ": Efficiency of Self-supervised adversarial training (CIFAR-100, WRN-34-10): Performance(%) using lesser number of attack steps (2 steps) when compared to the standard case (5 steps) duringadversarial training. Clean/ Standard Accuracy (SA) and robust accuracy against GAMA attack (RA-G)and AutoAttack(RA-AA) are reported. The proposed approach is stable at lower attack steps as well, whilebeing better than both TRADES (Zhang et al., 2019) and DeACL (Zhang et al., 2022).",
  "is suboptimal since it is not generated by using the evaluation process (KNN), and thus the robust accuracyagainst such an attack is higher": "Transfer learning: To evaluate the transferrability of the learned robust representations, we comparethe proposed approach with the best baseline DeACL in under standard linear probing (LP). Weconsider transfer from CIFAR-10/100 to STL-10 (Coates et al., 2011). When compared to DeACL, the cleanaccuracy is 4 10% higher on CIFAR-10 and 1.7 6% higher on CIFAR-100. We also obtain 3 5%higher robust accuracy when compared to DeACL on CIFAR-100, and higher improvements over TRADES.We also present transfer learning results using lightweight adversarial full finetuning (AFF) to STL-10 andCaltech-101 (Li et al., 2022) in . We defer the details on the process of adversarial full-finetuning andthe selection criteria for the transfer datasets to Appendix D. The transfer is performed on WRN-34-10 modelthat is pretrained on CIFAR-10/100. As shown in , the proposed method outperforms DeACL by asignificant margin. Note that by using merely 25 epochs of adversarial full-finetuning, the proposed methodachieves improvements of around 4% on CIFAR-10 and 11% on CIFAR-100 when compared to the linearprobing accuracy presented in , highlighting the practical utility of the proposed method. The AFFperformance of the proposed approach is better than that of a supervised TRADES pretrained model as well. Efficiency of self-supervised adversarial training Similar to prior works (Zhang et al., 2022), theproposed approach uses 5-step PGD based optimization for attack generation during adversarial training. In, we present results with lesser optimization steps (2 steps). The proposed approach is stable andobtains similar results even by using 2-step attack. Even in this case, the clean and robust accuracy of theproposed approach is significantly better than the baseline approach DeACL (Zhang et al., 2022), and alsooutperforms the supervised TRADES model (Zhang et al., 2019). We compare the computational aspects ofthe existing baselines and the proposed method in more detail in Appendix E.",
  "Effect of each component of the proposed approach: We study the impact of each component of theProFeAT in Table-9, and make the following observations based on the results:": "Projector: A key component of the proposed method is the introduction of the projector. We observesignificant gains in clean accuracy ( 5%) by introducing the projector along with defense losses at thefeature and projection spaces (E1 vs. E2). The importance of the projector is also evident by the fact thatremoving the projector from the proposed defense results in a large drop (5.7%) in clean accuracy (E9 vs.E5). We observe a substantial improvement of 9.2% in clean accuracy when the projector is introduced inthe presence of the proposed augmentation strategy (E3 vs. E7), which is significantly higher than thegains obtained by introducing the same in the baseline DeACL (4.76%, E1 vs. E2). Further ablations onthe projector are provided in Appendix F.1. Augmentations: The proposed augmentation strategy improves robustness across all settings. Introducingthe proposed strategy in the baseline improves its robust accuracy by 2.47% (E1 vs. E3). Moreover, theimportance of the proposed strategy is also evident from the fact that in the absence of the same, there isa 4.48% drop in SA and 2% drop in RA-G (E9 vs. E6). Further, when combined with other componentsas well, the proposed augmentation strategy shows good improvements (E4 vs. E5, E2 vs. E7, E6 vs. E9).Detailed ablation on the respective augmentations used for the teacher and student model can be found inAppendix F.2. Attack loss: The proposed attack objective is designed to be consistent with the proposed defense strategy,where the goal is to enforce smoothness at the student in the feature space and similarity with the teacherin the projector space. The impact of the attack loss in feature space can be seen in combination with theproposed augmentations, where we observe an improvement of 2.5% in clean accuracy alongside notableimprovements in robust accuracy (E3 vs. E5). However, in presence of projector, the attack results inonly marginal robustness gains, possibly because the clean accuracy is already high (E9 vs. E7). Moredetailed ablations on the attack loss in provided in Appendix F.3. Defense loss: We do not introduce a separate column for defense loss as it is applicable only in the presenceof the projector. We show the impact of the proposed defense losses in the last two rows (E8 vs. E9). Theproposed defense loss improves the clean accuracy by 1.4% and robust accuracy marginally. Appendix F.4provides further insights on various defense loss formulations and their impact on the proposed method.",
  ": Performance of ProFeAT when comparedto DeACL (Zhang et al., 2022) across variation in therobustness-accuracy trade-off parameter on CIFAR-100 dataset with WRN-34-10 architecture": "1.000.750.500.250.00 Weight on feature loss ( ) 52.5 55.0 57.5 60.0 Clean Accuracy (%) 1.000.750.500.250.00 Weight on feature loss ( ) 26.0 26.5 27.0 27.5 28.0 Robust Accuracy (%) : Performance (%) of ProFeAT by varying the weight between the defense losses at the feature and projector. Theperformance is stable across the range [0.25, 0.75]. We thusfix the value of to 0.5 in the proposed approach. : Performance across different architecture types and sizes: Standard Linear Probingperformance (%) of DeACL (Baseline) and ProFeAT (Ours) across different architectures on CIFAR-100.ViT-B/16 uses Imagenet-1K trained SSL teacher for training, while the SSL teacher in all other cases istrained on the CIFAR-100. SA: Standard Accuracy, RA-AA: Robust Accuracy against AutoAttack.",
  "ResNet-1811.2751.5321.9153.4722.61ResNet-5023.5053.3023.0059.3425.86WideResNet-34-1046.2852.9223.8261.0826.81ViT-B/1685.7961.3417.4965.0821.52": "et al., 2021) on the CIFAR-100 dataset in . ProFeAT consistently outperforms DeACL in both cleanand robust accuracy across various model architectures. An explanation behind the mechanism for successfulscaling to larger datasets can be found in Appendix B. Robustness-Accuracy trade-off: We present results across variation in the robustness-accuracy trade-offparameter (Equations (1) and (2)) in . Both robustness and accuracy of the proposed method aresignificantly better than DeACL across all values of . Weighting of defense losses at the feature and projector: In the proposed approach, the defense lossesare equally weighted between the feature and projector layers as shown in Equation (3). In , wepresent results by varying the weighting between the defense losses at the feature (Lf) and projector (Lpf)layers: LProFeAT = Lf + (1 ) Lpf, where Lpf and Lf are given by Equations (1) and (2), respectively.It can be noted that the two extreme cases of = 0 and = 1 result in a drop in clean accuracy, with alarger drop in the case where the loss is enforced only at the feature layer. The robust accuracy shows lesservariation across different values of . Thus, the overall performance is stable over the range [0.25, 0.75],making the default setting of = 0.5 a suitable option.",
  "Conclusion": "In this work, we bridge the performance gap between supervised and self-supervised adversarial trainingapproaches, specifically for large capacity models. We utilize a teacher-student setting (Zhang et al., 2022)where a standard self-supervised trained teacher is used to provide supervision to the student. Due to theinherent misalignment between the teacher training objective and the ideal goals of the student, we propose touse a projection layer to prevent the network from overfitting to the standard SSL trained teacher. We presenta detailed analysis on the use of projection layer in distillation to justify our method. We additionally proposeappropriate attack and defense losses in the feature and projector spaces alongside the use of weak and strongaugmentations for the teacher and student respectively, to improve the attack diversity while maintaining lowtraining complexity. The proposed approach obtains significant gains over existing self-supervised adversarialtraining methods, specifically for large models, demonstrating its scalability. While we improve the scalabilityof self-supervised AT methods to larger capacity models, we limit the datasets to CIFAR scale similar toprior works in this field due to the large computational costs. We hope future works explore the applicationof such SSL-AT methods to large scale datasets such as ImageNet.",
  "Jacob Buckman, Aurko Roy, Colin Raffel, and Ian Goodfellow. Thermometer encoding: One hot way to resistadversarial examples. In International Conference on Learning Representations (ICLR), 2018": "Nicholas Carlini, Anish Athalye, Nicolas Papernot, Wieland Brendel, Jonas Rauber, Dimitris Tsipras, IanGoodfellow, and Aleksander Madry. On evaluating adversarial robustness. arXiv preprint arXiv:1902.06705,2019. Jinghui Chen and Quanquan Gu. Rays: A ray searching method for hard-label adversarial attack. InProceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining,pp. 17391747, 2020. Tianlong Chen, Sijia Liu, Shiyu Chang, Yu Cheng, Lisa Amini, and Zhangyang Wang. Adversarial robustness:From self-supervised pre-training to fine-tuning. In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition, pp. 699708, 2020a. Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastivelearning of visual representations. In International conference on machine learning, pp. 15971607. PMLR,2020b.",
  "Spyros Gidaris, Praveer Singh, and Nikos Komodakis. Unsupervised representation learning by predictingimage rotations. arXiv preprint arXiv:1803.07728, 2018": "Sven Gowal, Chongli Qin, Jonathan Uesato, Timothy Mann, and Pushmeet Kohli. Uncovering the limits ofadversarial training against norm-bounded adversarial examples. arXiv preprint arXiv:2010.03593, 2020. Jean-Bastien Grill, Florian Strub, Florent Altch, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, CarlDoersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, Bilal Piot, koray kavukcuoglu,Remi Munos, and Michal Valko. Bootstrap your own latent - a new approach to self-supervised learning.In Advances in Neural Information Processing Systems (NeurIPS), 2020.",
  "Leslie Rice, Eric Wong, and J. Zico Kolter. Overfitting in adversarially robust deep learning. In InternationalConference on Machine Learning (ICML), 2020": "Ludwig Schmidt, Shibani Santurkar, Dimitris Tsipras, Kunal Talwar, and Aleksander Madry. Adversariallyrobust generalization requires more data. Advances in neural information processing systems, 31, 2018. Yang Song, Taesup Kim, Sebastian Nowozin, Stefano Ermon, and Nate Kushman. Pixeldefend: Leveraginggenerative models to understand and defend against adversarial examples. In International Conference onLearning Representations (ICLR), 2018. Gaurang Sriramanan, Sravanti Addepalli, Arya Baburaj, and R Venkatesh Babu. Guided Adversarial Attackfor Evaluating and Enhancing Adversarial Defenses. In Advances in Neural Information Processing Systems(NeurIPS), 2020. Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian J. Goodfellow,and Rob Fergus. Intriguing properties of neural networks. In International Conference on LearningRepresentations (ICLR), 2013. Florian Tramr, Alexey Kurakin, Nicolas Papernot, Ian Goodfellow, Dan Boneh, and Patrick McDaniel. En-semble adversarial training: Attacks and defenses. In International Conference on Learning Representations(ICLR), 2018. Florian Tramr, Jens Behrmann, Nicholas Carlini, Nicolas Papernot, and Jrn-Henrik Jacobsen. Fundamentaltradeoffs between invariance and sensitivity to adversarial perturbations. In International Conference onMachine Learning (ICML), 2020.",
  "Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146, 2016": "Jure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and Stphane Deny. Barlow twins: Self-supervised learningvia redundancy reduction. In International Conference on Machine Learning, pp. 1231012320. PMLR,2021. Chaoning Zhang, Kang Zhang, Chenshuang Zhang, Axi Niu, Jiu Feng, Chang D Yoo, and In So Kweon.Decoupled adversarial contrastive learning for self-supervised adversarial robustness. In Computer VisionECCV 2022: 17th European Conference, Tel Aviv, Israel, October 2327, 2022, Proceedings, Part XXX,pp. 725742. Springer, 2022. Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric Xing, Laurent El Ghaoui, and Michael I Jordan. Theoreticallyprincipled trade-off between robustness and accuracy. In International Conference on Machine Learning(ICML), 2019.",
  "A.1Decoupled Adversarial Contrastive Learning (DeACL)": "DeACL introduces a teacher-student distillation framework to train robust models in a self-supervised settingwithout the need of labeled training data. While most self-supervised adversarial training methods aimed atintegrating contrastive learning methods with adversarial training, the authors (Zhang et al., 2022) showedthat combining the two is a complex optimization problem due to their conflicting training requirements. Thetraining is thus decoupled to two stages as discussed below. The first stage trains a standard self-supervisedmodel using any of the existing Self-supervised Learning (SSL) techniques such as SimCLR, BYOL, orBarlowTwins. This is followed by the second stage that involves adversarial training of a student model usingthe supervision from the earlier-trained teacher in a distillation framework as shown in . The lossused during distillation is a combination of the cosine similarity between the representations of the cleanimage at the output of the teacher and student, along with a smoothness loss at the output of the student asshown in Equation (5).",
  "xi =arg minxi:||xixi||cosTf(xi), Sf(xi)(6)": "In Equation (5), the first term is the Distillation loss on the feature backbone Tf of the teacher T , andSf of the student S for a clean input xi. x is the adversarial input. The second term corresponds to theSmoothness loss at the student to enforce similar representations for the clean and adversarial inputs, andis weighted by a hyperparameter that controls the robustness-accuracy trade-off in the downstream model.The loss LDeACL (Equation (5)) is minimized during training. The attack used during training is generatedby a minimizing the cosine similarity between the teacher and student representations at the feature outputas shown in Equation (6).",
  "A.2Supervised Adversarial Defenses": "Following the demonstration of adversarial attacks by Szegedy et al. (2013), there have been several attemptsof defending Deep Networks against them. Early defenses proposed intuitive methods that introducednon-differentiable or randomized components in the network to thwart gradient-based attacks (Buckman et al.,2018; Ma et al., 2018; Dhillon et al., 2018; Xie et al., 2018; Song et al., 2018). While these methods were efficient",
  "BMechanism behind Scaling to Larger Datasets": "For a sufficiently complex task, a scalable approach results in better performance on larger models givenenough data. Although the task complexity of adversarial self-supervised learning is high, the gains in priorapproaches are marginal with an increase in model size, while the proposed method results in significantlyimproved performance on larger capacity models (). We discuss the key factors that result in betterscalability below: As discussed in .1, a mismatch between training objectives of the teacher and ideal goals ofthe student causes a drop in student performance. This primarily happens because of the overfittingto the teacher training task. As model size increases, the extent of overfitting increases. The use of aprojection layer during distillation alleviates the impact of this overfitting and allows the student toretain more generic features that are useful for the downstream robust classification objective. Thus,a projection layer is more important for larger model capacities where the extent of overfitting ishigher. Secondly, as the model size increases, there is a need for higher amount of training data for achievingbetter generalization. The proposed method has better data diversity as it enables the use ofmore complex data augmentations in adversarial training by leveraging supervision from weakaugmentations at the teacher.",
  "CDetails on Datasets": "We compare the performance of the proposed approach ProFeAT with existing methods on the benchmarkdatasets CIFAR-10 and CIFAR-100 (Krizhevsky et al., 2009), that are commonly used for evaluating theadversarial robustness of models (Croce et al., 2021). Both datasets consist of RGB images of dimension32 32. CIFAR-10 consists of 50,000 images in the training set and 10,000 images in the test set, with theimages being divided equally into 10 classes - airplane, automobile, bird, cat, deer, dog, frog, horse, shipand truck. CIFAR-100 dataset is of the same size as CIFAR-10, with images being divided equally into 100classes. Due to the larger number of classes, there are only 500 images per class in CIFAR-100, making it amore challenging dataset when compared to CIFAR-10.",
  "DDetails on Training and Compute": "Model Architecture: We report the key comparisons with existing methods on two of the commonlyconsidered model architectures in the literature of adversarial robustness (Pang et al., 2021; Zhang et al.,2019; Rice et al., 2020; Croce et al., 2021) - ResNet-18 (He et al., 2016) and WideResNet-34-10 (Zagoruyko &Komodakis, 2016). Although most existing methods for self-supervised adversarial training report results",
  "ATTeacher modelTotal": "Supervised (TRADES)110101210-1210AP-DPE450104950-4950RoCL100056000-6000ACL1000512000-12000AdvCL1000512000100013000DynACL1000512000-12000DynACL++1025512300-12300DeACL100570010001700ProFeAT (Ours)100580010001800 only on ResNet-18 (Zhang et al., 2022; Fan et al., 2021), we additionally consider the WideResNet-34-10architecture to demonstrate the scalability of the proposed approach to larger model architectures. Weperform the ablation experiments on the CIFAR-100 dataset with WideResNet-34-10 architecture, which isa very challenging setting in self-supervised adversarial training, to be able to better distinguish betweendifferent variations adopted during training. Training Details: The self-supervised training of the teacher model is performed for 1000 epochs with theSimCLR algorithm (Chen et al., 2020b) similar to prior work (Zhang et al., 2022). We utilize the solo-learn1 GitHub repository for this purpose. For the SimCLR SSL training, we tune and use a learning rate of 1.5with SGD optimizer, a cosine schedule with warmup, weight decay of 1e5 and train the backbone for 1000epochs with other hyperparameters kept as default as in the repository. The self-supervised adversarial training of the feature extractor using the proposed approach is performedfor 100 epochs using SGD optimizer with a weight decay of 3e4, cosine learning rate with 10 epochs ofwarm-up, and a maximum learning rate of 0.5. We consider the standard based threat model Zhang et al.(2022); Fan et al. (2021) for attack generation during both training and evaluation, i.e., ||xi xi|| ,where x is the adversarial version of the clean sample xi. The value of is set to 8/255 for both CIFAR-10and CIFAR-100 datasets, as is standard in literature (Madry et al., 2018; Zhang et al., 2019). A 5-stepPGD attack is used for attack generation during training with a step size of 2/255. We fix the value of ,the robustness-accuracy trade-off parameter (Ref: Equations (1) and (2) in the main paper) to 8 in all ourexperiments, unless specified otherwise. Details on configurations in Tables 2 and 3: In these tables, we present the experiments showing theimportance of the projector in self-supervised distillation, first in a standard setting () and then in anadversarial setting (). In , the linear probing performance of the student model is shown atboth the feature and projection space when linear probed using two different probing methods (column LPLoss): standard CE loss (S1, S3-S5), and cosine similarity loss between the student and the teacher (denotedas cos(T , S), S2). Experiments S1-S4 refer to settings where there is no projector involved during distillation,while S5 includes the projector network. S1 and S2 considers a self-supervised trained teacher for distillation,whereas S3 and S4 uses a standard teacher trained using label supervision. In the absence of a projector,when the training objectives of teacher matches with the student (S3/S4 vs. S1), or when similar losses areused for pretraining and linear probing (S2 vs. S1), we see an improvement in the downstream performanceof the student. On the other hand, in the event of misalignment between the student and teacher objectives,one can employ a projector network (see S5 vs. S1) to alleviate the impact of the mismatch and gain back theperformance of the student. In that case, the similarity between the teacher and the student is significantlyhigher at the projector space when compared to the feature space (S5).",
  "% increase0.0560.004.350.0090.001.25": "Likewise, we show a similar behavior in the setting of adversarial self-supervised distillation in . Letsagain consider the case when theres no projector. When using CE loss for linear probing (A3 vs. A1),using a supervised AT teacher for adversarial distillation (A3) which is also trained using CE loss, results inmuch better student performance compared to a self-supervised teacher (A1) which has been trained using asimilarity-based objective, like a contrastive loss. On the other hand, when the linear probing objective ischanged instead to a similarity-based loss (A2: cos(T , S)), the mismatch of the student with a self-supervisedteacher vanishes, and we recover the student performance (note similar clean performance of A2 and A3,w.r.t. A1). In the case of using a projector network, we see an improvement in the performance of the studentmodel even when distilled SSL teacher for distillation and linear probed using CE loss (A4 vs. A1). Details on Linear Probing: To evaluate the performance of the learned representations, we performstandard linear probing by freezing the adversarially pretrained backbone as discussed in of themain paper. We use a class-balanced validation split consisting of 1000 images from the train set and performearly-stopping during training based on the performance on the validation set. The training is performed for25 epochs with a step learning rate schedule where the maximum learning rate is decayed by a factor of 10 atepoch 15 and 20. The learning rate is chosen amongst the following settings {0.1, 0.05, 0.1, 0.5, 1, 5}, withSGD optimizer, and the weight decay is fixed to 2e4. The same evaluation protocol is used for the bestbaseline DeACL (Zhang et al., 2022) as well as the proposed approach ProFeAT, for both in-domain andtransfer learning settings. Details on Transfer Learning: To perform transfer learning using Adversarial Full-Finetuning, a robustlypretrained base model is used as an initialization, which is then adversarially finetuned using TRADES Zhanget al. (2019) for 25 epochs. After finetuning, the model is evaluated using linear probing on the same transferdataset. We consider STL-10 Coates et al. (2011) and Caltech-101 Li et al. (2022) as the transfer datasets.Caltech-101 contains 101 object classes and 1 background class, with 2416 samples in the train set and 6728samples in the test set. The number of samples per class range from 17 to 30, and thus is a suitable datasetto highlight the practical importance of lightweight finetuning of an adversarial self-supervised pretrainedrepresentations for low-data regime. Compute: The following Nvidia GPUs have been used for performing the experiments reported in thiswork - V100, A100, and A6000. Each of the experiments are run either on a single GPU, or across 2 GPUsbased on the complexity of the run and GPU availability. Uncertainty estimates in the main SOTA table( of the main paper) were obtained on the same machine and with same GPU configuration to ensurereproducibility. For 100 epochs of single-precision (FP32) training with a batch size of 256, the proposedapproach takes 8 hours and 16GB of GPU memory on a single A100 GPU for WideResNet-34-10 modelon CIFAR-100.",
  "EComputational Complexity": "In terms of compute, both the proposed method ProFeAT and DeACL (Zhang et al., 2022) lower the overallcomputational cost when compared to prior approaches. This is because self-supervised training in generalrequires larger number of epochs (1000) to converge (Chen et al., 2020b; He et al., 2020) when compared tosupervised learning (<100). Prior approaches like RoCL (Kim et al., 2020), ACL (Jiang et al., 2020) and",
  "AblationStudent proj.Proj. init. (Student)Teacher projProj. init. (Teacher)SARA-G": "AP1Absent-Absent-55.3527.86AP2TrainableRandomAbsent-63.0726.57AP3FrozenPretrainedAbsent-40.4322.23AP4TrainablePretrainedAbsent-62.8926.57AP5TrainableRandom (common)TrainableRandom (common)53.4327.23AP6TrainablePretrained (common)TrainablePretrained (common)54.6027.41AP7TrainablePretrainedFrozenPretrained58.1827.73OursFrozenPretrainedFrozenPretrained61.0527.41 : Ablation on Projector architecture (CIFAR-100, WRN-34-10):Performance (%) obtainedby varying the projector configuration (config.) and architecture (arch.). A non-linear projector effectivelyreduces the gap in clean accuracy between the teacher and student. A bottleneck architecture for the projectoris worse than other variants. SA: Standard Accuracy, RA-G: Robust Accuracy against GAMA attack.",
  "AblationProjector config.Projector arch.Teacher SAStudent SADrop in SA% Drop in SARA-G": "APA1No projector-70.8555.3515.5021.8827.86APA2Linear layer640-25668.0853.3514.7321.6427.47Ours2 Layer MLP640-640-25670.8561.059.8013.8327.41APA33 Layer MLP640-640-640-25670.7160.3710.3414.6227.37APA42 Layer MLP640-640-64069.8861.248.6412.3627.36APA52 Layer MLP640-2048-64070.9661.769.2012.9726.66APA62 Layer MLP640-256-64069.3757.8711.5016.5827.56 AdvCL (Fan et al., 2021) combine the contrastive training objective of SSL approaches and the adversarialtraining objective. Thus, these methods require larger number of training epochs (1000) for the adversarialtraining task, which is already computationally expensive due to the requirement of generating multi-stepattacks during training. ProFeAT and DeACL use a SSL teacher for training and thus, the adversarialtraining is more similar to supervised training, requiring only 100 epochs. In , we present theapproximate number of forward and backward propagations for each algorithm, considering both pretrainingof the auxiliary network used and the training of the main network. It can be noted that the distillation basedapproaches - ProFeAT and DeACL require significantly lesser compute when compared to prior methods, andare only more expensive than supervised adversarial training. In , we present the FLOPS requiredduring training for the proposed approach and DeACL. One can observe that there is a negligible increase inFLOPS compared to the baseline approach.",
  "F.1More ablations on the Projector": "Training configuration of the Projector: We present ablations using different configurations of theprojection layer in . As also discussed in .1 of the main paper, we observe a large boost inclean accuracy when a random (or pretrained) trainable projection layer is introduced to the student (AP2/AP4 vs. AP1 in ). While the use of pretrained frozen projection head only for the student degradesperformance considerably (AP3), the use of the same for both teacher and student (Ours) yields a optimalrobustness-accuracy trade-off across all variations. The use of a common trainable projection head for bothteacher and student results in collapsed representations at the projector output (AP5, AP6), yielding resultssimilar to the case where projector is not used for both teacher and student (AP1). This issue is overcomewhen the pretrained projector is trainable only for the student (AP7).",
  ":Robust accuracy of a supervisedTRADES model across random restarts of PGD5-step attack (CIFAR-100, WRN-34-10)": "Architecture of the Projector: In the proposed approach, we use the following 2-layer MLP projectorarchitecture for both SSL pretraining of the teacher and adversarial training of the student: (1) ResNet-18:512-512-256, and (2) WideResNet-34-10: 640-640-256. In , we present results using differentconfigurations and architectures of the projector. Firstly, the use of a linear projector (APA2) is similar tothe case where projector is not used for student training (APA1), with 21% drop in clean accuracy of thestudent with respect to the teacher. This improves to 12 17% when a non-linear projector is introduced(APA3-APA6 and Ours). The use of a 2-layer MLP (Ours) is marginally better than the use of a 3-layerMLP (APA3) in terms of clean accuracy of the student. The accuracy of the student is stable across differentarchitectures of the projector (Ours, APA4, APA5). However, the use of a bottleneck architecture (APA6)results in a higher drop in clean accuracy of the student.",
  "F.2Augmentation for the Student and Teacher": "We present ablation experiments to understand the impact of different augmentations used for the teacherand student separately in . The base method (AG1) uses common Pad and Crop (PC) augmentationfor both teacher and student. By using more complex augmentations AutoAugment followed by Pad andCrop (denoted as AuAu in the table), there is a significant improvement in both clean and robust accuracy.By using separate augmentations for the teacher and student, there is an improvement in the case of PC(AG3), but a drop in clean accuracy accompanied by better robustness in case of AuAu. Finally by using amix of both AuAu and PC at the student and teacher respectively (Ours), we obtain improvements in bothclean and robust accuracy, since the former improves attack diversity (shown in ), while the lattermakes the training task easier.",
  "F.3Attack Loss": "For performing adversarial training using the proposed approach, attacks are generated by minimizing acombination of cosine similarity based losses as shown in Equation (4) of the main paper. This includes anunsupervised loss at the feature representations of the student and another loss between the representationsof the teacher and student at the projector. As shown in , we obtain a better robustness-accuracytrade-off by using a combination of both losses rather than by using only one of the two losses, due tobetter diversity and strength of attack. These results also demonstrate that the proposed method is not verysensitive to different choices of attack losses.",
  "F.4Defense Loss": "We present ablation experiments across variations in training loss at the feature space and the projectionhead in . In the proposed approach (Ours), we introduce a combination of clean and robust lossesat both feature and projector layers, as shown in Equation (3) of the main paper. By introducing the lossonly at the features (AD1), there is a considerable drop in clean accuracy as seen earlier, which can berecovered by introducing the clean loss at the projection layer (AD3). Instead, when only the robust loss is",
  "AblationAttack @ feat.Attack @ proj.SARA-G": "AT1cos(T , S)cos(T , S)60.8426.78AT2cos(S, S)cos(S, S)61.3026.75AT3cos(T , S)cos(S, S)60.6927.44AT4cos(S, S)-61.6226.62AT5-cos(S, S)61.0927.00AT6cos(T , S)-62.0126.89AT7-cos(T , S)61.1827.24Ourscos(S, S)cos(T , S)61.0527.41 : Ablation on Defense Loss (CIFAR-100, WRN-34-10):Performance (%) with variationsin training loss at feature (feat.) and projector (proj.). clean\" denotes the cosine similarity betweenrepresentations of teacher and student on clean samples.adv\" denotes the cosine similarity betweenrepresentations of the corresponding clean and adversarial samples either at the output of student (S, S) orbetween the teacher and student (T , S). SA: Standard Accuracy, RA-G: Robust accuracy against GAMA.",
  "AblationLoss @ feat.Loss @ proj.SARA-G": "AD1clean + adv(S, S)-55.3527.86AD2-clean + adv(S, S)59.6526.90AD3clean + adv(S, S)clean61.6926.40AD4clean + adv(S, S)adv(S, S)49.5925.35AD5adv(S, S)clean59.721.38AD6adv(S, S)clean + adv(S, S)59.2226.50AD7cleanclean + adv(S, S)62.2425.97AD8clean + adv(S, S)clean + adv(T , S)63.8523.91AD9clean + adv(T , S)clean + adv(T , S)65.3422.40Oursclean + adv(S, S)clean + adv(S, S)61.0527.41 : Failure of AD5 (refer ) defense loss for SSL-AT training of WRN-34-10 modelon CIFAR-100: Using clean and adversarial loss exclusively at projector and feature space respectively,results in an unstable optimization problem, giving either a non-robust model or collapsed representations atthe end of training. As shown below, a lower value of (the robustness-accuracy trade-off parameter) resultsin a non-robust model, while higher results in the learning of collapsed representations.",
  "SimCLR67.9862.2026.13SimCLR (tuned)70.8563.0726.57BYOL72.9763.1926.82Barlow Twins67.7460.6924.48SimSiam68.6063.4626.69MoCoV372.4865.5726.65DINO68.7560.6124.80": "introduced at the projection layer (AD4), there is a large drop in clean accuracy confirming that the need forprojection layer is mainly enforcing the clean loss. When the combined loss is enforced only at the projectionhead (AD2), the accuracy is close to that of the proposed approach, with marginally lower clean and robustaccuracy. Enforcing only adversarial loss in the feature space, and only clean loss in the projector space is ahard optimization problem, and this results in a non-robust model (AD5). As shown in , even byincreasing in AD5, we do not obtain a robust model, rather, there is a representation collapse. Thus, asdiscussed in .1 of the main paper, it is important to introduce the adversarial loss as a regularizer inthe projector space as well (AD6). Enforcing only one of the two losses at the feature space (AD6 and AD7)also results in either inferior clean accuracy or robustness. Finally from AD8 and AD9 we note that therobustness loss is better when implemented as a smoothness constraint on the representations of the student,rather than by matching representations between the teacher and student. Overall, the proposed approach(Ours) results in the best robustness-accuracy trade-off.",
  "F.5Accuracy of the self-supervised teacher model": "The self-supervised teacher model is obtained using 1000 epochs of SimCLR (Chen et al., 2020b) training inall our experiments. We now study the impact of training the teacher model for lesser number of epochs. Asshown in , as the number of teacher training epochs reduces, there is a drop in the accuracy of theteacher, resulting in a corresponding drop in the clean and robust accuracy of the student model. Thus, theperformance of the teacher is crucial for training a better student model."
}