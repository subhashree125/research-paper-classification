{
  "Abstract": "We study a variant of the bandit problem where side information in the form of boundson the mean of each arm is provided. We prove that these translate to tighter estimatesof subgaussian factors and develop novel algorithms that exploit these estimates. In thelinear setting, we present the Restricted-set OFUL (R-OFUL) algorithm that additionallyuses the geometric properties of the problem to (potentially) restrict the set of arms beingplayed and reduce exploration rates for suboptimal arms. In the stochastic case, we proposethe non-optimistic Global Under-Explore (GLUE) algorithm which employs the inferredsubgaussian estimates to adapt the rate of exploration for the arms. We analyze the regretof R-OFUL and GLUE, showing that our regret upper bounds are never worse than that ofthe standard OFUL and UCB algorithms respectively. Further, we also consider a practicallymotivated setting of learning from confounded logs where mean bounds appear naturally.",
  "Introduction": "We study the problem of bandits with mean bounds and bounded rewards where an agent is presented with aset of K arms, along with side-information in the form of upper and lower bounds on the average reward foreach of the arms. The agent is then asked to successively choose arms based on previous observations inorder to maximize the cumulative reward. As is standard in MABs, the agents performance is compared tothat of a genie that always chooses the arm that gives the largest reward in expectation. We consider this inthe commonly studied frameworks of stochastic Multi-Armed Bandits (MABs) and the Linear Bandit. In theformer, rewards of each arm are drawn independently from its associated distribution and are uncorrelatedwith rewards of other arms. The linear setting couples the rewards from all arms through an unknown, butxed latent vector that is used to paramterize the mean rewards. We seek to design arm selection policies thateciently utilize the provided mean bounds in oering both improve regret performance and computationalcomplexity. Our setting is motivated by the problem of inferring ecacy of interventions from confounded logs. As aconcrete example, consider the following healthcare example: Interavenous tissue plasminogen (tPA) activatorsare known to be highly eective in treating acute ischemic strokes if administered within 3 hours of the onsetof symptoms. Otherwise, a less-eective medical therapy is recommended, as tPA causes higher chances ofadverse side eects and hemorrhages Powers et al. (2019). If a log is then generated without recording thetime since the onset of symptoms, it would present tPA to be better than the alternative. Naively using sucha log to infer tPA to be the best intervention could lead to unfavourable outcomes in areas with poor access",
  "Published in Transactions on Machine Learning Research (11/2024)": "After the above, the Occupation attribute is treated as the visible one, age and gender are hidden. We formmeta-users by averaging all the users according to the tuple of attributes (gender,age). The reward matrix isalso modied to now have each row represent a particular (gender, age) realization (for example, (M, 0-17)).These reward matrices are separately computed for each of the 8 occupations. We then sample a randomcollection of 15 movies for each occupation to be considered as arms. Theorem 7 is then employed in order toform the confounded logs for each occupation class. The realized upper and lower bounds after this processare shown in in the appendix. In , we provide the online learning behavior for two of theseinstances.",
  "Contribution 1: Improved Subgaussian Factors with Mean Bounds: We provide a characterizationof the subgaussian factor 1": "2 of any random variable bounded in when upper and lower bounds onits mean are known. Specically, in Theorem 3 and Corollary 3.1, we show that when the mean is known tobe towards either half of this interval, one can infer factors that are strictly less than 0.5. These immediatelyimply tighter concentration bounds for bounded random variables. This result could be of independentinterest, however, we study the eects of such information in bandit learning. Contribution 2: Linear Bandits: We present the Restricted-set Optimism in the Face of Uncertainty forLinear bandits (R-OFUL) algorithm for bounded rewards. R-OFUL rst uses the structure imposed by thelinear rewards to rene the given side information and produces the tightest possible mean bounds for eacharm in the action set before online interaction. Then, at each time, it leverages the geometry of the problemto restrict the set of arms to be considered. Finally, it reduces exploration by using the sharp estimates ofsubgaussian factors above for arms in the restricted set and chooses actions much like the standard OFULalgorithm of Abbasi-Yadkori et al. (2011). We show that the lower bounds are key to our improvements in the online phase of the algorithm. First, therestricted set is constructed as a cone around the arm with the largest lower bound. Combining the lowerbounds of the arms that remain with the boundedness of rewards then leads to our reduced explorationrates. Our analysis in Theorem 4 shows that using side information, R-OFUL can improve over the regretguarantees of standard OFUL by a constant factor. It also improves the computational cost due to therestriction of arms. To the best of our knowledge, this is the rst investigation on Linear Bandits with Meanbounds. Contribution 3: Stochastic MABs: We develop GLobal Under-Explore (GLUE)an index based policywhich, unlike the UCB Auer et al. (2002) and kl-UCB Capp et al. (2013) algorithms, is not optimistic; i.e.the indices do not serve as high-probability upper bounds to true means of each arm. We use the fact thatviolating the upper bound property for the indices of suboptimal arms does not adversely aect the regret aslong as the property is maintained for the (unknown) best arm. In particular, our indices for an arm areformed using a quantity that can be strictly lesser than the true subgaussian factor of the arm only if thearm is sub-optimal. This causes the sub-optimal arms to be under-explored which leads to improvement inregret performance.",
  "Related Work": "Bandit problems have seen a lot of interest over the past few decades (see Bubeck et al. (2012); Lattimore& Szepesvri (2020) for comprehensive surveys). A vein of generalization for the same has seen numerousadvances in incorporating several forms of side information to induce further structure into this setting.Notable among them are graph-structures information (Buccapatnam et al., 2014; Valko et al., 2014; Aminet al., 2012), latent models (Li et al., 2010; Bareinboim et al., 2015; Lattimore et al., 2016; Sen et al., 2017),expert models (Auer et al., 1995; Mannor & Shamir, 2011), smoothness of the search space (Kleinberg et al.,2008; Srinivas et al., 2010; Bubeck et al., 2011), among several others. We assume side information in theform of mean bounds and study how such information aects the decision making process. In the stochastic setting, our bandit problem has connections to the works by Zhang & Bareinboim (2017)and Combes et al. (2017). An in-depth comparison with these can be found in .3. Along anotherthread, Bubeck et al. (2013) provide algorithms with bounded regret if the mean of the best arm and a lowerbound on the minimum suboptimality gap is known. These techniques, however, do not apply in our settingas the side information we consider does not allow us to extract such quantities in general. In the linearsetting, with time-varying action sets, the works of Li et al. (2010); Abbasi-Yadkori et al. (2011) are inspiredby the upper condence bound-type arguments of Auer et al. (2002) for the stochastic case. When action setsremain xed over time, arm-elimination type algorithms like ones in Valko et al. (2014) improve dependenceon the dimension of the arms. We study the novel setting of linear bandits with mean bounds and varyingaction sets in this work. The extraction of mean bounds from confounded logs has been studied in the context of estimating treatmenteects in the presence of confounders. Here, actions are treatments, and the rewards capture the eects ofthis choice. A line of existing work performs sensitivity analysis by varying a model on the latents, measuredvariables, treatments and outcomes in a way that is consistent with the observed data (Robins et al., 2000;Brumback et al., 2004; Richardson et al., 2014). Recently in (Yadlowsky et al., 2022; Zhao et al., 2019), auniversal bound on the ratio of selection bias due to the unobserved confounder is assumed. This means that",
  "Bandits with Mean Bounds": "We consider the round-based interaction of a learning agent with a stochastic environment through a set ofK0 actions (or arms). At each round, the agent chooses one of the provided arms and observes a stochasticreward. To aid its decision making, the agent is also provided with side information in the form of bounds onthe mean reward of each arm. The goal of the agent is to choose arms such that the cumulative reward iscompetitive with respect that obtained by a genie that only chooses the best arm in each round. In this work, we concentrate on two commonly studied formulations of this problem: Stochastic Multi-armedBandits and Linear Bandits. We detail the notations, structure of rewards, and notions of the best arm foreach of these below. Stochastic Multi-armed Bandit: The agent is provided with K0 arms indexed by the set [K0] = {1, 2, ...K0},with each arm being associated with a xed and unknown reward distribution supported over the interval. The mean reward of arm k [K0] is denoted by k. For each arm k [K0], the side information isgiven by a tuple (lk(t), uk(t)) such that k [lk(t), uk(t)] and lk(t), uk(t) . These tuples thus specifyupper and lower bounds on the mean reward for each of the arms and can be dierent for each arm. With thisknowledge, at round t, the agent chooses an arm At and observes a reward Yt sampled from the distributionassociated with the chosen arm. We dene k = arg maxk[K0] k be the (unique) best arm with k =",
  "and the genie chooses this arm at each round": "The agent thus aims to minimize its average cumulative regret, which at round T is given by RT =Tt=1 E[Yt]. The expectation here is over the randomness of the rewards and the choice of arms ofthe agent. If we have that for all k [K0], lk = 0, uk = 1, then our setting matches that of the standardMulti-armed Bandit with bounded rewards. Linear Bandit: In this case, in each round t, the agent is provided a (possibly dierent) set of action At Asampled from a (possibly innite) set of actions A such that At = K0 . Each arm a A is a vector in Rd.At round t, the agent chooses an arm At At and observes a reward Yt = At, + t where Rd is axed unknown vector. The noise t is a conditionally (At)subgaussian random variable with respect tothe ltration Ft = (A1, Y1, ..., At1, Yt1, At}). This arm-specic subguassian factor is not revealed to theagent. As in the case above, for all rounds t, Yt and the agent is provided with tuples (la, ua) for eacha A such that T a [la, ua] and la, ua . We also assume that [m, M] and that a = 1 ( is the Euclidean norm) for a A. As before, the agent aims to maximize its cumulative reward in order to remain competitive with a geniethat chooses the arm with highest mean reward at each round. Equivalently, the agent aims to minimize theregret, which at round T is given by RT = Tt=1 maxaAta At, . In contrast to the setting above, RTis a random variable due to the randomness in At. With la = 0, ua = 1 for a A, our setting becomes thatof the standard Linear Bandit with bounded rewards.",
  ")estimate of the subgaussian factor?. This is important because a tighter subgaussian parameter would leadto faster concentrations of the random variable": "Suppose now that the random variable X has mean m that is known. Since it is bounded, the varianceof this random variable is bounded by that of a Bernoulli random variable with the same mean. Further, thesquare of the subgaussian factor for any random variable is an upper bound on its variance (See Lemma 5.4in Lattimore & Szepesvri (2020)). Therefore, we have that var(X) m(1 m) sg(Bernoulli(m))2 1",
  "The lower bound of": "m(1 m) on implies that it increases in (0, 0.5) and decreases in (0.5, 1). In , we plot the values of 2 and m(1 m), the variance of a Bernoulli with mean m, for dierent values of min (0, 1) to verify this trend. This leads to the following corollary:",
  "Restricted-set Optimism under Uncertainty for Linear Bandits with Mean Bounds": "Now that we know the implications of non-trivial mean bounds on the concentration behavior of a randomvariable, we move on to studying the eects of such information on the regret of a linear bandit agent. Recallthat at each round t, the agent is given an action set At of K0 vectors in Rd sampled from a set A andmust learn to choose actions At At. Observing Yt = At, + t, where is an unknown vector and t aconditionally (At)-subgaussian random variable with respect to the ltration generated by all observationsup to t 1 and the choice of arm At, the agent competes with a genie that always picks the arm in the",
  ": end for": "provided set with largest mean reward. That is, the agent aims to minimize its regret at round T given byRT = Tt=1 maxaAta At, . As is standard in Linear Bandits, this regret RT is random due to therandomness in the choice of AT and thus, we seek to minimize this regret with high probability. Specically,we use to be the user-specied failure probability and develop a policy that minimizes regret with probabilityat least 1 . The agent is also provided with tuples (la, ua) for each arm a A which can be used to aid its decision making.Further, it is known that [m, M] and that for all a A, a = 1. We propose the Restricted-setOptimism under Uncertainty for Linear Bandits (R-OFUL) algorithm (summarized in Algorithm 1) that usesthese mean bounds and the linear structure of rewards to minimize regret. At each round, the agent performsthree steps: a) Tightening the mean bounds, b) Restricting the instantaneous action set, and c) Invoking theupdate similar to the OFUL algorithm of Abbasi-Yadkori et al. (2011) with potentially improved subgaussianfactors that leads to reduced exploration. We describe each of these steps below. Tightening the Mean Bounds: Since all rewards are obtained using the same parameter , bounds onthe mean of some action give us non-trivial information about the mean reward of all other actions. Thisfact is used to tighten the provided upper and lower bounds on arm means. Formally, dene Cb = { : [m, M], a A, a, [la, ua]} be the set of feasible parameter vectors. Then, the tight bounds la, ua (weabuse notation by representing both the provided and tighter bounds by the same variables) are computedfor each arm a A as",
  "After these tight bounds are computed, the set of feasible vectors Cb is recomputed with these updatedversions of the mean bounds": "Restricting instantaneous action sets: This phase is carried out after receiving the set At at time tand chooses a subset of arms to be considered at each round. First the agent prunes away deterministicallysuboptimal arms used lmax(t) = maxaAt la. This restricts the set of arms in consideration to Ap(t) = {a :ua lmax(t)}. Now we denote by at = arg maxaAt la the most promising arm at time t and use ang(a, a) =cos1 (a,a/aa) as the angle between the vectors a and a. With t = cos1 (lmax(t)/M), the restrictedset of actions at time t is given by",
  "At =arg maxaAr(t),Cta, .(6)": "This selection rule is similar to that of vanilla OFUL in Abbasi-Yadkori et al. (2011). The dierence here, werestrict a) the set Ct with the feasible set Cb, b) the set of arms that can be played at time t with Ar(t) andc) the condence width t() using the tighter subgaussian estimate of .",
  "Key Ideas": "1. Sharper subgaussian factors: As we saw in , bounds on the mean of a random variable (here,the arm rewards), provide tighter estimates of its subgausssian factor (Corollary 3.1). This is reected in ourdenition of k in Equation 7. 2. Underexploring suboptimal arms using k: Consider a standard MAB instance with K arms whereit is known that arm k provides rewards with a subgaussian factor of 2k. The standard UCB algorithm wouldthen explore arm k at the rate dictated by the corresponding subgaussian factor. Now suppose additionally,that it was known that the (unknown) best arm produced rewards with subgaussian factor 2best. We constructthe algorithm A which sets the index of arm k analogous to UCB, but with exploration rate dictated bymin{2k, 2best}. As a result, A potentially assumes incorrect s.g-factors for suboptimal arms. Consequently,the indices of A are no longer upper condence bounds to the arm means, i.e., A is not optimistic. Thekey dierence in the dynamics of UCB and A is that the latter potentially explores suboptimal arms lessfrequently than the former. This would lead to A incurring lower regret than standard UCB. In GLUE, min{2k, 2best} is analogous to k. Specically, when lmax > 0.5, the side information allows usto estimate an upper bound on the subgaussian factor of the best arm. We note that this upper bound iscomputed using the arm kmax; the identity of the best arm is still unknown. Thus, the regret minimizationproblem remains non-trivial in this case.",
  "(c) 15 Arms per round": ": Comparing R-OFUL (Algorithm 1) with vanilla OFUL with arms in R10 and bounded rewards. Results areaveraged over 200 runs and error bars for one standard deviation are displayed. R-OFUL restricts arms and onlychooses between 2-3 arms per round on average and thus, its average regret is comparable over all three gures, whileOFUL suers regret that grows with the number of arms per round. We also observe that R-OFUL computes armupdates 6 6.7 faster on average. Using this lemma, we generalize the concentration arguments of Abbasi-Yadkori et al. (2011) to prove thatwith probability at least 1 , the parameter vector satises Cb. The result then follows using thestandard arguments for bounding regret in linear bandits.",
  "dtmax(d + log(1/))with": "max = (ba)/2 as the universal upper bound on the s.g-factor of the arms in the set A. If no informativebounds are present (the worst case) our R-OFUL algorithm matches the performance of OFUL exactly.However, richer side information can lead to a constant factor improvement in the nite-time regret guaranteesas max. Further, in cases where Ar(t) = 1, the instance-dependent regret of R-OFUL is 0, while anylinear bandit algorithm that does not perform this arm restriction will suer non-zero regret on average. We also improve on the computation complexity of OFUL. Specically, to compute At (Equation 6), theusual practice is to solve the optimization problem for all a individually for all arms and then choose onewith the maximum objective value. If the mean bounds are such that |Ar(t)| < |At|, that is, if the set ofarms is restricted by R-OFUL, then, so is the number of optimization sub-problems to solve, which speeds upcomputation. Optimality: We study a generalization of OFUL which is known to be optimal up to logarithmic factors inthe worst case. However, it is known that optimality in general linear bandit problems is achieved by complexnon-optimistic policies. The study of regret lower bounds and policies that match them are left open. Empirical Evaluations: We compare the regret performance of R-OFUL with the vanilla OFUL algorithmempirically in . For this, we sample a random set of 100 arms in R10 and sample 5, 10, 15 of these ineach round. We set as the vector0, 1",
  ", ... 9": "10and normalize it. To generate rewards, for each arma At, we rst sample a Gaussian random variable with mean a, and variance 0.8, and clip this to be in () if a, > 0( 0) to form our bounded rewards. The upper and lower bounds on the rewardsare generated separately to be away from the mean by a uniform random variable in [0, 0.5] and these arealso clipped the same way as the rewards. That is, arms with positive (non-positive) mean reward are forcedto have bounds in (). The subgaussian upper bound for OFUL is provided as 0.5, the worst-casesubgaussian factor for any random variable bounded in either or while R-OFUL computes tightersubgaussian factors based on the mean bounds and uses as in Equation 5 to choose arms. We also usethe norm bounds on to m = 0, M = 1. We average our results over 200 independent runs. We see thatR-OFUL consistently achieves much lower regret than the vanilla variant. Further, we also observe that",
  "Global Under-exploration for Stochastic Multi-armed Bandits with Mean Bounds": "Now we move on to the stochastic Multi-armed Bandit (MAB) setting. We recall that in this case, there isno linear structure on the rewards. At each time, the agent picks one of K0 arms, and observes a rewardthat is randomly sampled from the distribution that is associated with this arm. The agent can also accessthe tuples (lk, uk) for all arms k [K0] to inform its choices. At round T, the agent aims to minimize theexpected cumulative regret RT = Tt=1 E[ Yt]. Prior work in the Zhang & Bareinboim (2017); Combes et al. (2017) have investigated this problem before,the for Bernoulli rewards, and the latter as a structured bandit problem. The B-KL-UCB algorithm of theformer sets arms indices as the clipped version (using the provided bounds) of the the standard KL-UCBindices from Capp et al. (2013). The latter proposes OSSB for general reward distributions where the agentdecides arms at each round by solving a distribution-dependent, semi-innite optimization problem. Theauthors also prove that OSSB achieves asymptotically optimal regret for Bernoulli and Gaussian rewardswhen reward distributions are known apriori. In the special case of Bernoulli rewards, OSSSB and B-KL-UCBare equivalent (see Appendix D), and are thus optimal. However, it is often not practical to assume that the reward distributions are known apriori. Further, evenwith this knowledge, it is unclear how computationally tractable the per-round optimization problem ofOSSB is. While the Bernoulli version in B-KL-UCB is still applicable in this case, the KL-UCB indices foreach arm in each round are themselves optimization problems with no known closed form solution. As thenumber of arms grow, computing these indices eciently poses a challenge. The UCB algorithm of Auer et al. (2002) provides an index-based solution that is inexpensive to compute,albeit at the cost of the optimal regret performance. Specically, it is well known that in the vanilla MABsetting, KL-UCB always outperforms UCB for bounded reward distributions, with larger gains when themean rewards are closer to the extremities. This is mainly due to the fact that UCB chooses the worst-caseexploration rate for any bounded distribution, while KL-UCB adapts its rate according to the (unknown)value of the mean rewards. Works such as Liu et al. (2018) address the computation issue by using a set ofsemi-distance functions to boost the UCB index. This leads to a trade-o between optimal performance andecient compute. We propose Global Under-Exploration (GLUE) for Stochastic MABs with Mean Bounds in Algorithm 2. Itdraws inspiration from KL-UCB, adapting its exploration rate to the location of the arm means using theprovided mean bounds but like UCB, provides arm indices that are inexpensive to compute. We note thatunlike B-KL-UCB, GLUE is not the clipped version of vanilla UCB. In particular, GLUE is not optimistic,i.e, its arm indices are not high-probability upper bounds to the true mean of the arm. The algorithm worksin two phases:",
  "with f(t) = 1 + t log2(t).(8)": "We observe two key things here. First, for each arm k, Uk(t) is set using the quantity k rather than the truesubgaussian factor 2k. When lmax > 0.5, k for all arms is set as the subgaussian factor of the arm with thelargest lower bound kmax which is strictly lower than 2k for k = kmax. Thus in general, k 2k. Second,arm indices are clipped at the respective upper bounds, which is sensible as these indices are a representationof our belief of true arm means. We also note that using k = 0.25 and setting Uk(t) without clipping at ukrecovers the standard Upper Condence Bound (UCB) algorithm in Auer et al. (2002).",
  "Regret Upper Bounds for GLUE": "In this section, we study the regret performance of GLUE. We regard improvements from pruning as trivialand analyze regret for the set of arms that remain after pruning. We assume without loss of generalitythat after pruning the arms are indexed in non-decreasing order of mean, i.e., = 1 > 2 ... K,where K K0. We also dene the sub-optimality gap of an arm k as k = k. The following theorembounds the regret of GLUE, and also presents its asymptotic regret scaling which is an upper bound tolim supnRn",
  "(d) Non-informative bounds; Clipping only": ": Empirical validation for Stochastic MABs with Mean Bounds under Clipped Uniform and Bernoulli rewards:Each row corresponds to a dierent specication of arm means and mean bounds shown in the left subplot. Regretperformance under clipped uniform and Bernoulli rewards are shown in the latter two. The regret is averaged over 200runs and error bars of one standard deviation are shown. In a, for each arm, k = 1. In b, the boundsreveal no non-trivial information, however, Arm 2 is meta-pruned. In c, we set k = k since lmax < 0.5 andthus, ImprovedUCB and GLUE coincide. In d, the bounds do not provide non-trivial information aboutsubgaussian factors and are only used to clip rewards. B-UCB, UCBImproved and GLUE compute arm choices 11faster than B-KL-UCB on average.",
  "k .(9)": "Proof Sketch for Theorem 6. As is usual in the regret analysis of stochastic MABs, we upper bound thenumber of times a suboptimal arm is played, which translates to a bound on the cumulative regret. Incontrast however, we can not rely on the UCB property of the indices any longer. Recall that we only need to consider the set of K arms that remain after pruning. Theorem 3 and Corollary3.1 gives us that k is an upper bound on the true s.g-factor for arm k. We rst show that the best arm k isalways ksubgaussian. Then, we establish that the asymptotic contribution to regret of any arm in K1() isa constant. We deem these arms to be meta-pruned. For K2(), we show that the use of pseudo-variancesis sucient to explore these arms at a logarithmic rate. The theorem then follows by combining these tworesults, with the full proof in Appendix C.",
  "Comparison and Discussions": "Regret Upper Bounds: GLUE vs Existing Algorithms: We compare the regret upper bound of GLUEto a slew of baselines in . Here, B-UCB is the clipped version of the vanilla UCB algorithm of Aueret al. (2002). It is equivalent to using GLUE with k = 0.25, the worst-case subgaussian factor for each armk. Further, d(p, q) = p log (p/q) + (1 p) log ((1p)/(1q)) is the Bernoulli Kullback-Liebler divergence.",
  "k": "The vanilla UCB and KL-UCB algorithms do not display meta-pruning as they do not use the mean bounds.The regret bound of OSSB in Combes et al. (2017) matches that of B-KL-UCB for Bernoulli rewards (seeAppendix D) and is optimal. The UCB, B-UCB and GLUE algorithms require O(K0) compute per iteration in the worst-case (when noarms are pruned). KL-UCB, B-KL-UCB (and OSSB for Bernoulli rewards) require O(optK0) computationalcomplexity, with opt the cost of solving the index optimization problem for each arm. Uncertain mean bounds: Our focus in this work has been the case when the provided mean bounds holdwith probability 1. The immediate follow-up would be the case where these bounds only hold for each arm kwith some probability pk < 1. Practically, such settings occur when we are given historical data about playsfrom each arm (this is the setting studied in Shivaswamy & Joachims (2012)). The following corollary gives ahorizon-dependent regret bound for GLUE in this case.",
  "If we know the horizon to be T and each arm is observed O(log T) times in the provided history, usingstandard Cherno-type arguments, we easily see that": "k[K] pk = O(1/T). Thus, the additional regretincurred by time T is simply a constant. However, the cases when the number of samples for an arm aresub-logarithmic in the horizon and that with an unknown horizon are left open. Lack of global under-exploration in R-OFUL: In the stochastic MAB with bounds, we used GLUE toexplore all arms at rate 2k the subgaussian factor of the best arm (when lmax > 0.5). This was possiblebecause the estimate of the best arm remained a 2k-subgaussian random variable, and was thus explored atthe correct rate. In the linear case, the estimates for each arm uses all samples collected so far in t, the ridgeregression estimate of . The estimate of the best arm is thus no longer 2k-subgaussian and under-exploringarms in the set Ar(t) using (lmax, b), for example, would lead to poor estimates even for the best arm andthus larger regret. Empirical Comparisons: We compare GLUE with B-UCB, ImprovedUCB (GLUE with k instead of kin Uk(t)) and B-KL-UCB for clipped uniform distributions (drawn from a uniform distribution in an intervalaround the mean that is fully contained in ) and Bernoulli rewards. We chose not to compare with OSSBsince it is unclear how the associated optimization problem can be solved for this distributions. We dropUCB and KL-UCB since the clipped variants in B-UCB and B-KL-UCB, respectively, are never worse thenthe vanilla counterprats. All plots are averaged over 200 independent runs. When lmax is close to 1, we see that GLUE outperforms the optimistic UCB-based baselines (B-UCB andImprovedUCB) and is comparable to B KL UCB. We note that this is the case where KL variants enjoymaximum improvements over UCB variants, but GLUE competes with the optimal in this case. In the casewhen all upper bounds are lesser than 0.5, GLUE matches the performance of ImprovedUCB, thus improvingover B-UCB. However, since there is no information about the best arm in this case, B-KL-UCB outperforms",
  "A Use Case: Learning from Confounded Logs": "Up to this point, we have assumed that the learning agent has access to a tuple of mean bounds for each armbefore the start of the online learning process. In this section, we describe a practically motivated scenario inwhich such mean bounds arise naturally. Specically, we consider the task of extracting non-trivial boundson means from partially confounded data from an optimal oracle. We show that under mild assumptions,this problem leads to the stochastic MAB problem (and a Linear Bandit) with mean bounds.",
  "Confounded Logs for Stochastic Multi-armed Bandits": "The Oracle Environment: We consider a contextual environment, where nature samples a context vector(z, u) C = Z U from an unknown but xed distribution P. We assume that sets Z and U are both discrete.At each time any of the K0 actions from the set A = {1, 2, ..., K0} can be taken. The reward of each armk A for a context (z, u) C has mean k,z,u and support (with appropriate shifting and scaling, thiscan be generalized for any nite interval [a, b]). For each context (z, u) C, let there be a unique best armkz,u and let z,u be the mean of this arm. Confounded Logs: The oracle observes the complete context (zt, ut) C at each time t and also knowsthe optimal arm kzt,ut for this context. She picks this arm and observes an independently sampled rewardyt with mean value kzt,ut,zt,ut = zt,ut. She logs the information in a data set while omitting the partialcontext ut. In particular, she creates the data set D = {(zt, kt, yt) : t N}. The Agent Environment: A new agent is provided with the oracles log. In this paper, we consider theinnite data setting. The agent makes sequential decisions about the choice of arms having observed thecontext zt Z at each time t = {1, 2, ...}, while the part of the context u U is hidden from this agent. Letat be the arm that is chosen, zt be the context, and Yt be the reward at time t. Dene the average reward ofarm k A under the observed context z Z as k,z =",
  "Transferring Knowledge through Bounds": "The agent is interested in the quantities k,z, the mean reward of arm k under the partial context z, for allarms k A and contexts z Z, to minimize its cumulative regret. As the oracle only plays an optimal armafter seeing the hidden context u, the log provided by the oracle is biased and thus k,z can not be recoveredfrom the log in general. However, it is possible to extract non-trivial upper and lower bounds on the averagek,z. In a binary reward and action setting, similar observations have been made in Zhang & Bareinboim(2017). Alternative approaches to this problem, that include assuming bounds on the inverse probabilityweighting among others, are discussed in .1. Our assumption below is dierent, and in a setting where there are more than two arms. We specify that thelogs have been collected using a policy that plays an optimal arm for each (z, u) C, but do not explicitlyimpose conditions on the distributions. Instead, in Assumption 1, we impose a separation condition on themeans of the arms conditioned on the full context. We dene:Denition 1. Let us dene z, z for each z Z as follows:",
  "Remarks on Model and Assumption 1:": "Relation to Gap in Agent Space: We note that these gaps in the latent space do not allow us to infer thegap in the agent space. However, due to optimal play by oracle, these gaps help in obtaining mean bounds forall arms, even those which have not been recorded in the log (Theorem 7). Interpretation: The gap z gives us information about the hardest arm to dierentiate in the latent spacewhen the full context was observed (z analogously is for the easiest arm to discredit). We also note that ourapproach can still be applied when the trivial bound z = 1, or universal bounds not depending on context z, z are used, leading to bounds that are not as tight. Motivating Healthcare Example: Often, hospital medical records contain information zt about the patient,the treatment given kt, and the corresponding reward yt that is obtained (in terms of patients health outcome).However, a good doctor looks at some other information ut (that is not recorded) during consultation andprescribes the best action kt under the full context (zt, ut). If one is now tasked with developing a machinelearning algorithm (an agent) to automate prescriptions given the medical record z, this agent algorithmneeds to nd the best treatment k(z) on average over P(u|z). Furthermore, the gaps on treatments eectscan potentially be inferred from other data sets like placebo-controlled trials. Existing alternate assumptions: Studies in Yadlowsky et al. (2022); Zhao et al. (2019) impose conditions onbounded sensitivity of the eects with respect to the hidden/unrecorded context (eectively, that this contextdoes not signicantly alter the eect). In our work, we explore the other alternative where the treatment hasbeen chosen optimally with respect to the hidden/unrecorded context, and allows strong dependence on thishidden context.",
  ". K>(k, z): Finally, we identify the set of arms with \"large rewards\" K>(k, z) = {k [K0] : k = k, z(k) >zpz(k)} for each k [K] and z Z": "To see that these quantities can indeed be inferred, assume that the logs are given as an innite table withcolumns Z, A, Y . Now, we collapse rows that share Z = z, A = k into a single row with Y now being themean reward of all such rows in the original table.",
  "z(k) zpz(k)": "Note that these bounds can be provided for all arms k [K0] and contexts z Z. Specically, bounds canalso be extracted for the arms that are never played in the log. This comes as a result of the gaps dened inDenition 1. Proving this results requires a careful use of the total probability theorem which can be foundin Appendix E Remarks:1. Finite Log: While we study the case of infnite logs, our approach can be readily extended to nite logsby replacing the quantities z, z(k), pz(k) with empirical estimates and using standard Cherno bounds toaugment the bounds in Theorem 7 to hold with a probability which is a function of the size of the log. Then,the performance of the learning algorithm is as in Corollary 6.1 and the discussion that follows.2. Distribution shifts: Our results can also be used in the case where under a xed visible context, theconditional distribution of the hidden contexts changes from oracle to the learner. In particular, for somez Z, suppose that z maxuU |Po(u|z) Pl(u|z)|, where Po and Pl are used to dierentiate the oracleand learner environments respectively. Using z, we can readily modify our results in Theorem 7 to produceupper and lower bounds for arm rewards under the context z when the distributions are no longer the same. Now, we show the existence of instances for which our bounds are tight. We say an instance is admissible ifand only if it satises all the statistics generated by the log data. The following proposition shows all thebounds dened in Theorem 7 are partially tight. Proposition 8 (Tightness of Transfer). For any log with uk,z, lk,z as dened in Theorem 7 the followingstatements hold:1. There exists an admissible instance where upper bounds k,z = uk,z, for all k [K0] and z Z.2. For each k [K0], there exists a (separate) admissible instance such that k,z = lk,z for each z Z.",
  "Confounded Logs for Linear Bandits": "The Oracle Environment: Consider a linear bandit environment with K arms ak = (ak,z, ak,u) for i [K]and (random) latent vector = (z, Tu) such that z is xed. Suppose a, Rp, and for all k [K],au,k, Tu Rd Further, we assume that Tu is such that each entry of the vector is always between [m, M], andak = 1 for all k [K] (the equality can be generalized to bounds on the norm). At each round t, Tu(t)is drawn independently from some distribution C and the full vector (t) = (z, Tu(t)) is revealed to theoracle, based on which the arm At = arg maxk[K]ak, (t) is played. The oracle then observes the rewardYt = At, (t) + t where t is the conditionally (At) subgaussian bounded random variable with respectto the ltration generated by the reward observations up to time t 1 and arm choices up to time t. Confounded Logs: We dene k(T) to be the index of the best arm when the random realization of Tu wasT, i.e., k(T) = arg maxk[K],Tu=T ak, . The oracle records the tuple (k(Tu(t)), z, Yt) at the end of eachround and omits the explicit information about Tu(t). These tuples are collected over an innite horizon toform the dataset D = {(k(Tu(t)), z, Yt) : t N}. The Agent Environment: The agent is provided with the innite log generated by the oracle and needsto interact with the same environment with no other information about being revealed at each time.Specically, at each round t, the agent chooses an arm At {ak : k [K]} and observes Yt = At, + twith Tu(t) being drawn independently according to C and t the conditionally (At)-subgaussian noise asbefore. Since the latent vector is hidden and is random at each round, the agent seeks to minimize theaverage regret given by",
  "t=1maxk[K]ak,z At,z, z + ak,u At,u, u": "In the above, u us the average value of the vector Tu under the distribution C and At,z, At,u are the parts ofthe arm At corresponding to parameters z, u respectively. Note that the rst term in the sum is known fullyto the agent as z is recorded in the oracle logs and thus, the learning problem of the agent now reduces to aLinear Bandit over d dimensions (the dimension of u) using the pseudo-rewards Y t = Yt At, z for thechosen arm At. The genie policy in this case is to always play the arm k = arg maxk[K]ak,z, z+ak,u, u.We note that this denition averages out the randomness in Tu and in general can not be inferred by knowingk(Tu) from the logs. Mapping these back to our running healthcare example, the seen context z are the known eects of eachintervention in {ak : k [K]} on the patient that have been established using medical trials, while Turepresents the responses patient to the intervention based on biomarkers that have not been explored in thetrials. Thus, u is the average of these unknown eects over the population of patients.",
  "Confounded Logs for Stochastic MABs:": "We present a recommendation systems example where an agent is tasked with learning the best movie torecommend to users in an online manner: the agent observes the users occupation at each round and solvesan independent bandit instance for each occupation. To assist in its learning, logs collected from an oracleare provided to this agent. The oracle has access to more information about users and observes occupation,age and gender in order to recommend movies, but only records the occupation for each user. We assumethat environments do not change between the oracle and the agent, i.e., users are drawn randomly from thesame population in both cases. For this set of experiments, we use the Movielens 1M dataset of Harper & Konstan (2015). This data setconsists of 6040 users, from whom over 1 million ratings of 3952 movies are collected. Each user is associatedwith a gender, age, occupation and zip-code. In this work, we ignore the zip-code. The ratings (or rewards) liein the interval . These are normalized to through normal shifting and scaling. The reward matrix ofsize 6040 3952 is then completed using the SoftImpute algorithm from Mazumder et al. (2010). Post matrixcompletion, we delete all users whos total reward across all movies is below 1500. We also further clusterthe age attribute to have 4 classes: 0 17, 18 25, 25 49, 50+. We also combine the occupation attributesappropriately to have 8 classes namely: Student, Academic, Scientic, Oce, Arts, Law, Retired and Others.",
  "(b) Setting 2": ": Online Learning Behavior of two instances from the synthetic Linear Bandit setup. In each of the gures,the left gure summarizes the inferred bounds on u, ak for each k . The right gure displays the onlineexperiment. Results are averaged over 200 independent runs and one standard deviation error bars are displayed.In the rst setting, the bounds do not provide any improved subgaussian estimates, however, the restriction helpsimprove regret. In the second setting, the bounds provide improved exploration rates. Confounded Logs for Linear Bandits: For this set of experiments, we use a synthetic setup: We rstx z, u = (1, 2, 3, 4, 5)T R5. We normalize z to have norm 1, and u to have norm 0.9. For the arms,for each of the 12 arms, we draw ak,z to from a Folded Normal Distribution (|X| is folded normal if X isnormally distributed) in R5. We set a1,u to be in a ball of radius 0.1 around u at random and for all other k,ak,u is set to be a normalized 2-sparse vector in R5. With these, we generate logs by uniformly sampling Tu(t)in a ball of radius 0.1 around u independently for each t, then picking the best arm for this Tu(t). Thenwe sample a noisy latent reward uniformly at random from a symmetric interval around Tu(t), ak(Tu(t)),u,ensuring that it is in . The reward at each time is given by the sum of this noisy latent reward andz, ak(Tu(t)),u. We collect logs of size 105 to approximate the innite logs. We invoke Theorem 9 in order to infer upper and lower bounds on each of the 12 arms. Then, we spawnvariants of the R-OFUL (Algorithm 1 and vanilla OFUL from Abbasi-Yadkori et al. (2011) and run an onlinelinear bandit algorithm on our setting for 5000 iterations. The results are averaged over 200 independent runs.In , we present out results. We see that when the bounds do no help in restricting the arms underconsideration, R-OFUL matches vanilla OFUL (improvements here are from clipping the optimisitc indicesfor each arm). When non-trivial information can be gathered from the mean bounds, R-OFUL signicatntlyoutperforms the vanilla variant.",
  "Conclusion": "In this work, we treated the problem of bandit learning with mean bounds in the linear and stochastic settings.Beginning with the study of how mean bounds lead to inference of sharp subgaussian factors when rewardsare bounded, we present the R-OFUL algorithm for the linear setting and the GLUE algorithm for stochasticbandits that use these sharper factors to reduce exploration. In the linear case, by restricting the set of armsbeing considered at each round, we show that R-OFUL enjoys not only improved regret, but also increasedcompute eciency in comparison to vanilla OFUL. In the stochastic setting, GLUE can oer comparableperformance to the optimal B-KL-UCB algorithm when rich side information is available, and is never worsethan the vanilla UCB policy. Further, we studied the practical use case of learning from (innite) confoundedlogs where such mean bounds on rewards of actions can be inferred under mild assumptions on the latentenvironment. Several avenues of future work were discussed over the course of exposition, chief among which is the questionof lower bounds for the linear setting. While we present a compute-ecient baseline for the case of linearbandits with mean bounds, drawing from intuition in standard linear bandits, the lower bound is likelyachieved by a complex, non-optimistic algorithm that further leverages the side information. Studyingthe optimality would also help characterize the environments in which the mean bounds improve regretperformance. Further, the case of learning from nitely many confounded observations, which manifests as aproblem of learning from probabilistic mean bounds remains open. In Corollary 6.1, we provide a naturala regret upper bound to the performance of GLUE in this case (which can also be extended to R-OFULfor linear bandits) which serves as a baseline. Finally, our assumption of bounded rewards led to tightersubgaussian factors and regret improvements. Identifying other use cases and modes of side information witharbitrary, potentially unbounded, reward distributions from which such improved factors can be computedalso serves an an interesting direction.",
  "Acknowledgements": "We sincerely thank all the reviewers and editors for the feedback that helped improve the quality of thepaper. This research was partially supported by NSF Grants 1826320, 2019844, 2107037 and 2112471, AROgrant W911NF-17-1-0359, US DOD grant H98230-18-D-0007, ONR Grant N00014-19-1-2566 and the USDoT supported D-STOP Tier 1 University Transportation Center.",
  "Sbastien Bubeck, Vianney Perchet, and Philippe Rigollet. Bounded regret in stochastic multi-armed bandits.In Conference on Learning Theory, pp. 122134, 2013": "Swapna Buccapatnam, Atilla Eryilmaz, and Ness B Shro. Stochastic bandits with side observations onnetworks. In The 2014 ACM international conference on Measurement and modeling of computer systems,pp. 289300, 2014. Olivier Capp, Aurlien Garivier, Odalric-Ambrym Maillard, Rmi Munos, Gilles Stoltz, et al. Kullbackleiblerupper condence bounds for optimal sequential allocation. The Annals of Statistics, 41(3):15161541, 2013.",
  "Rahul Mazumder, Trevor Hastie, and Robert Tibshirani. Spectral regularization algorithms for learning largeincomplete matrices. Journal of machine learning research, 11(Aug):22872322, 2010": "William J Powers, Alejandro A Rabinstein, Teri Ackerson, Opeolu M Adeoye, Nicholas C Bambakidis, KyraBecker, Jos Biller, Michael Brown, Bart M Demaerschalk, Brian Hoh, et al. Guidelines for the earlymanagement of patients with acute ischemic stroke: 2019 update to the 2018 guidelines for the earlymanagement of acute ischemic stroke: A guideline for healthcare professionals from the american heartassociation/american stroke association. Stroke, 50(12):e344e418, 2019. Amy Richardson, Michael G Hudgens, Peter B Gilbert, and Jason P Fine. Nonparametric bounds andsensitivity analysis of treatment eects. Statistical science: a review journal of the Institute of MathematicalStatistics, 29(4):596, 2014. James M Robins, Andrea Rotnitzky, and Daniel O Scharfstein. Sensitivity analysis for selection bias andunmeasured confounding in missing data and causal inference models. In Statistical models in epidemiology,the environment, and clinical trials, pp. 194. Springer, 2000. Rajat Sen, Karthikeyan Shanmugam, Murat Kocaoglu, Alex Dimakis, and Sanjay Shakkottai. Contextualbandits with latent confounders: An nmf approach. In Proceedings of the 20th International Conference onArticial Intelligence and Statistics, pp. 518527, 2017.",
  "Proof of Theorem 3. Subgaussianity": "Let gm(x) =2x2 log (fm(x)). For m < 0.5, part b of Lemma 1 and Theorem 14 imply that the maximum ofgm(x) is not achieved at any negative x. Further, Theorem 16 implies that there exists x > 0 : gm(x) >m(1m). Additionally, since gm(x) < m(1m) for x > 2",
  "BLinear Bandits with Mean Bounds": "Now we concentrate on the setting of and provide proofs for the result of Theorem 4. First, wewill show that the tight bounds we propose in Equation 2 are valid and tight. Then, we prove that the bestinstantaneous arm at each time is always contained in the restricted set at each time. Next, we show thatthe quantity t is a valid upper bound on the subgaussian factor for all arms in the restricted set. Thisculminates in our nal regret result. We begin with the bounds.",
  "Claim 1. The solutions to the optimization problems in Equation 2 is tight, that is, Cb such that forany a A, a, < la or a, > ua": "Proof. First we note that the constraint set Cb is non-empty since Cb and thus the quantities la, uaare well-dened for each a A. Suppose Cb with a, < la for some action a. This implies thatla = minCba, which is a contradiction. A similar argument for the upper bound completes the proof.",
  "Proof. From Corollary 3.1, we have that (a) (la, ua) for any arm a Ar(t). Therefore, it follows that(a) maxaAr(t) (la, ua)": "We are only left to prove that (a) m(3t). For this, we note that the arm in Ar(t) with lowestmean reward is at most 3t away from . This happens when is at an angle t away from at andthere exists some arm abad at an angle t on the opposite side of at. For this ctitious arm abad, we haveabad, = abad cos(3t) m cos(3t). Now, there are two cases:1. If m cos(3t) < 0.2178a+0.7822b: Then, (m cos(3t), b) = (ba)2 4and a Ar(t), (a) (m cos(3t), b)follows by denition.2. If m cos(3t) 0.2178a + 0.7822b: Then any arm a Ar(t), a, abad, m cos(3t). Using thisas a lower bound for the arm a, we can write (a) (m cos(3t), ua) = (m cos(3t), b).This completes the proof.",
  "DOSSB for MABs with mean bounds and Bernoulli Rewards": "Here, we derive closed form expressions for the semi-innite optimization problem in Combes et al. (2017) inthe case of Bernoulli rewards and bandits with mean bounds. In this section, we follow the notation in theirpaper. An instance is represented by a vector of arm means (1, 2, ...K). We say an instance is feasibleif for each k [K], we have that k [lk, uk]. Let be the set of all feasible instances. With as above,(k) Bernoulli(k) and the mapping k, (k, ) as (k, ) = k, our setting of Bernoulli bandits withmean bounds can hence be viewed as a Structured Bandit. Dene for any , , D(, , k) = d(k, k) where d(, ) is the Bernoulli kl-divergence function. Let() = maxk[K](k, ). Then, the optimization problem that determines the regret lower bound in ourcase can be given as:",
  "The inequality follows since the logs are assumed to be collected under an optimal policy. This completes theproof of the upper bound": "To prove the lower bound, we x an arbitratry k [K0] and z Z. Recall that K>(k, z) = {k : k =k, z(k) > zpz(k)} and let K(k, z) := [K0]\\K>(k, z) = {k : k = k, z(k) zpz(k)}. We note thatthese sets can be identied from the logged data because z(k) and pz(k) can be derived for any k and z.Now we dene the sets U>(k, z), U(k, z) as U>(k, z) = {u U : kz,u K>(k, z)}, U(k, z) = {u U : kz,u K(k, z)}",
  "k,z,u = z,u z u U : k = kz,u": "If the above equality holds for all k [K0] and all z Z, we have that k,z = uk,z. This instance is admissiblesince it maintains the means of the best arms are unchanged, and thus do not aect the quantities z andpz(k) for any k. Thus, there exists an admissible instance where the upper bounds for each arm is tight.",
  "Lower Bound Tightness:": "Fix an arm k, and a partial context z. For the tightness of the lower bound for this particular arm k andpartial context z, we present an instance now. We assign k,z,u = 0 for all u U(k, z), and k,z,u = z,u zfor all u U(k, z). For this particular choice, it is easy to see that the above lower bound is tight. We nowneed to prove that this instance is admissible.",
  ". We know that (z,u k,z,u) z by construction for all partial contexts u U>(k, z), and due to thefact that z,u z and k,z,u = 0 for all partial contexts u U(k, z)": "3. We know that (z,u k,z,u) z by construction for all partial contexts u U>(k, z), and due to thefact that z,u z and k,z,u = 0 for all partial contexts u U(k, z). Here, z,u z due to non-negativityof the mean-rewards and by denition of z (the smallest gaps). 4. Finally, k is never optimal for any partial contexts u U(k, z) U>(k, z). Indeed, for all the partialcontexts u U(k, z), we have k,z,u = 0 and z,u z > 0. For all the partial contexts u U>(k, z) wehave k,z,u = (z,u z) < z,u. Therefore, for this instance we never observe k for any partial contextsu U(k, z) U>(k, z), which ensures the log statistics does not change."
}