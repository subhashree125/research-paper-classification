{
  "Abstract": "Numerous benchmarks for Few-Shot Learning have been proposed in the last decade. How-ever all of these benchmarks focus on performance averaged over many tasks, and the ques-tion of how to reliably evaluate and tune models trained for individual few-shot tasks hasnot been addressed. This paper presents the first investigation into task-level validationafundamental step when deploying a model. We measure the accuracy of performance es-timators in the few-shot setting, consider strategies for model selection, and examine thereasons for the failure of evaluators usually thought of as being robust. We conclude thatcross-validation with a low number of folds is the best choice for directly estimating the per-formance of a model, whereas using bootstrapping or cross validation with a large numberof folds is better for model selection purposes. Overall, we find that with current methods,benchmarks, and validation strategies, one can not get a reliable picture of how effectivelymethods perform on individual tasks. However, we find that existing methods already pro-vide enough information to enable selection of few-shot learners on a task-level basis.",
  "Introduction": "Deep learning excels on many tasks where large-scale datasets are available for training (LeCun et al., 2015).However, many standard deep learning techniques struggle to construct high accuracy models when only avery small number of training examples are available. This is a serious impediment to the broader uptakeof machine learning in domains where web-scale data are not available. In many domains, such as medicineand security, it is common to suffer from data scarcity issues due to a multitude of resource constraintsand the rarity of the events being modelled. The Few Shot-Learning (FSL) paradigm, which focuses onenabling models to generalise well with little data through the use of transferred prior knowledge, has gainedrelevance in an attempt to overcome these challenges. A significant amount of attention has been given toFSL and related meta-learning research in the last decade (Wang et al., 2020; Hospedales et al., 2021), witha large number of methods and benchmarks proposed in application domains ranging from visual recognitionsystems for robots to identifying therapeutic properties of molecules (Xie et al., 2018; Stanley et al., 2021). Even though many learning algorithms have been developed in this area, and great efforts have been directedtowards improving model performance in FSL scenarios (Finn et al., 2017; Snell et al., 2017; Hospedaleset al., 2021), the best practices for how to evaluate models and design benchmarks for this paradigm remain",
  "Bootstrapping": ": Scatter plots of model accuracy on query set versus accuracy estimated using only the supportset. The ideal estimator would have the points almost co-linear and lying approximately on the diagonal line(green). All estimators have very high bias (points are off diagonal) and variance (no co-linearity), indicatingthey provide poor estimates of performance. relatively unexplored. In typical academic benchmark setups, performance estimation often relies on theexistence of test (query) sets that are several times larger than training (support) setsa desideratumthat is clearly not satisfied in realistic FSL scenarios. Moreover, performance is averaged over a large numberof learning problems, leading to accuracy estimates that cannot be used as a validation score for a singleproblem (episode) of interest. While these experimental design decisions make sense when assessing thegeneral efficacy of a learning algorithm, they are not helpful when considering how one might validate orselect a model for a real FSL problem with a single few-shot support set. Even in the case where a goodestimate of model performance is not required, little attention has been given to robust methods for selectingthe best model from some prospective pool of models when little data is available. The shortage of modelselection procedures for FSL settings means that one cannot reliably perform hyperparameter optimisationor select which FSL algorithm to use. We hypothesise that existing model evaluation procedures are not effective in the few-shot learning regimemuch like how conventional learning algorithms are not well-suited for direct application to few-shot learning.We experimentally investigate this hypothesis by analysing the behaviour of commonly used model evaluationprocesses. In addition to studying the accuracy of the performance estimates, we also consider how well thesepotentially noisy estimates can be utilised in model selection pipelines where one cares only about the relativeperformance of different models. We analyse failure modes of these methods and show that, should evaluationmethods be improved, one could see both a risk reduction for FSL deployments and better models, due tothe ability to do model selection reliably.",
  "We answer several concrete questions about the state of methods currently available for evaluating modelstrained in a FSL setting1:": "Q1How accurately can we estimate the performance of task-level models trained in the FSL regime? Thereare no combinations of learning algorithms and evaluators that are able to produce reliable performanceestimates, but we find that 5-fold cross-validation is the best of the bad options. Q2Are rankings produced by existing evaluators well-correlated with the true performance rankings ofmodels, thus allowing for effective model selection? Current model evaluation procedures do not providereliable rankings at the per-episode level, but methods based on resampling with a large number of iterationsare most reliable. Q3By how much could performance in FSL be improved by incorporating accurate model selection proce-dures? Our results show that there is still a lot of room for improvement in the case of model selection, asevidenced by the large gap between performance obtained via current model selection methods and perfor-mance from using an oracle evaluator to perform model selection.",
  "Related Work": "Current practices for evaluating FSL arose from the work on developing new learning algorithms (Lake et al.,2015; Ravi & Larochelle, 2017; Ren et al., 2018; Triantafillou et al., 2020; Ullah et al., 2022), and the aim hasconsistently been to determine the general efficacy of new learning algorithms. The experimental setup usedby these works employ a large number of downstream FSL tasks (drawn from a so-called meta-test set), eachof which has a test set several times larger than the associated training set. The endpoint measured andcompared when making conclusions is the average accuracy across the episodes in the meta-test set. Whilesuch an experimental setup is sensible when assessing the average-case performance of an FSL method, inthis work we address the more classic model validation and selection problem encountered when deployingmachine learning models, which is currently neglected in existing FSL benchmarks. I.e., Given a singlespecific few-shot learning task defined by a small training/support set, how can we estimate the performanceon the unseen test/query set? This is crucial in order to perform model selection, and to validate whetherpredictions of the few-shot learner are safe to act upon or not.Recent work from application domainsthat require few-shot learning indicate that the lack of reliable evaluation procedures is a major blocker fordeployment (Varoquaux & Cheplygina, 2022). Some prior work on studying the failure modes of FSL has identified that the variance of accuracy acrossepisodes is typically very high (Dhillon et al., 2020; Agarwal et al., 2021; Basu et al., 2023).Work inthis area has focused on identifying hard episodes by constructing support sets that lead to pathologicalperformance on a given query set. The focus of this existing work has been on discovering trends in thetypes of downstream tasks where FSL methods do not work, whereas our motivation is to explore: i) whatprocesses can be followed to determine whether a particular model will perform as required; and ii) how canwe reliably select the best model from some set of potential models. These specific questions are currentlyunaddressed by existing literature.",
  "Few-Shot Model Validation and Selection": "Modern few-shot learning research considers a two-level data generating process. The top level correspondsto a distribution over FSL tasks, and the bottom level represents the data generation process for an indi-vidual task. More precisely, consider a top level environment distribution, E, from which one can samplea distributions, Di, associated with a task indexed by i. One can then sample data from Di to generateexamples for task i. We first outline the existing assumptions about data availability and evaluation proce-dures used in the literature, which we refer to as Aggregated Evaluation (AE). We then discuss how this canbe augmented to better match real-world FSL scenarios, and then outline the Task-Level Evaluation (TLE)and Task-Level Model Selection (TLMS) protocols that we experimentally investigate. Aggregated Evaluation (AE)In this setting, one samples a collection of distributions, D = {Di E}ti=1, and then for each of these distributions one constructs a meta-test episode consisting of a supportset, Si = {(xj, yj) Di}nj=1, and a query set, Qi = {(xj, yj) Di}mj=1. It is typical for the size of the queryset, m, to be several times larger than the size of the support set, n. We observe that in a true FSL settingthis is an unrealistic assumption. Standard practice when evaluating FSL methods is to sample t episodes.For each episode, a model is trained using the support set and then a performance estimate is computedusing the query set, Di = 1",
  "tti=1 Di": "Using E to evaluate the performance of a FSL algorithm makes sense if the downstream application involvesa large number of different FSL problems, and the success of the overall system is dependent on being accurateon average. Examples of such applications include recommender systems and personalised content tagging,where each episode corresponds to a single user session. The success of such personalisation systems dependson being accurate for most user sessions, but a poor experience for a small number of sessions (i.e., episodes)is acceptable. In contrast, applications of FSL in medicine or security are not well-suited to the aggregatedevaluation provided by E. In these settings, each episode might correspond to recognising the presence of aspecific pathology or security threat, and poor performance on such episodes would translate to systematic",
  "misdiagnosis and critical security vulnerabilities. Such outcomes would be considered a catastrophic systemfailure": "Task-Level Evaluation (TLE)We investigate Task-Level Evaluation, which serves a purpose distinctfrom AE. While AE is typically undertaken to assess the general efficacy of a FSL algorithm, the purposeof TLE is to determine the performance of a model trained for a particular episode. In many real-worldapplications of FSL, one must have accurate estimates of the performance of models trained for each episode.Moreover, in realistic situations there is no labelled query set for evaluating a model, so both the modelfitting and model evaluation must be done with the support set. Just as FSL requires specialised learningalgorithms, we argue that TLE of FSL requires specialised performance estimators.",
  "We investigate three common approaches to evaluating machine learning models, which we summarise below": "Hold-out (Witten et al., 2016) The Hold-out evaluation method, which is commonly used in the deep learningliterature, requires a portion of the data to be set aside so it can be used to test the model. In particular,one sets aside n samples from the support set, trains a model on the m n remaining samples, and uses then held-out data points to estimate performance. Cross-validation (CV) (Stone, 1974) The cross-validation method splits the support set in k folds and repeatsmodel training k time. For each repetition, a different fold is used for testing the performance of the model,and the other k 1 folds are used to train the model. The mean performance across all iterations is used asa performance estimate for a model trained on all the data. When k is set to the number of data points thismethod is referred to as cross-validation leave-one-out (CV-LOO). Bootstrapping (Efron, 1979) Bootstrap sampling is a technique which consists of randomly drawing sampledata with replacement from a given set, resulting in a new sample with the same size as the original set,and an out-of-bag set composed of the data points that where not chosen during the sampling process. Thismethod is used in our bootstrapping evaluator to create a new support sample from the support set anda new query set composed by the out-of-bag data points. The process is repeated and the result of eachrepetition averaged to give a final performance estimate.",
  "In practice, the Bias and MAE are estimated using episodes from the meta-test set": "Task-Level Model SelectionOne of the common use-cases for performance estimators is in the modelselection process, where they are used to rank the relative performance of a set of candidate models, eitherfor deployment or as part of the hyperparameter optimisation process. We observe that accurate estimatesof the performance of models may not be needed for model selection, as long as similar types of errors asmade for all the prospective models. For example, if a performance estimator is systematically biased, butdoes not exhibit much variance, then the resulting rankings should be reliable, even though the estimate ofthe performance is not.",
  "Experiments": "We conduct experiments to determine which evaluation techniques are most reliable for: i) estimating theperformance of a model; and ii) selecting the best performing model from some pool of prospective models.Wealso investigate the underlying reasons why evaluators traditionally thought of as being quite robust can failin the FSL setting, and the extent to which better evaluators could enable improved performance via morereliable model selection.",
  "Baseline21.509.3911.3214.50Baseline++20.299.3212.2214.91ProtoNet15.789.069.7710.58MAML27.389.7535.5017.67R2D216.739.5011.6614.32": "We conduct experiments on datasets and learners popular in both the meta-learning (Hospedales et al., 2021)and vision & language transfer learning (Radford et al., 2021) family of approaches to few-shot learning.Specifically, we use: miniImageNetThis is a scaled down version of ImageNet consisting of 100 classes, each with 600 totalexamples and image scaled down to 84 84 pixels. We make use of the standard meta-train/validaiton/testsplit proposed by (Ravi & Larochelle, 2017).",
  "Published in Transactions on Machine Learning Research (11/2024)": "Meta-AlbumThis is a recently proposed (Ullah et al., 2022) dataset that has a focus on providingdata from a broad range of application domains, accomplished by pooling data from 30 different existingdatasets. We use the Mini version of the dataset, which provides 10 different domains (three source datasetsper domain) and 40 examples per class. Three splits are present, each containing one source dataset fromeach domain.Each split is used once each as meta-train, meta-validation, and meta-test, and the finalevaluation results are obtained by computing the mean of these three runs. CLIP Few-ShotFor experiments with CLIP-like few-shot learners, we also use three datasets (EuroSat,Flowers, and Food101) selected from the CLIP few-shot suite (Radford et al., 2021). In this case we follow theCLIP few-shot learning protocol, drawing from 1-16 shots from all categories in each dataset, and evaluatingon the full test sets. Implementation DetailsWe primarily study the classic FSL methods from the meta-learning communityProtoNet (Snell et al., 2017), MAML (Finn et al., 2017), R2D2 (Bertinetto et al., 2019), and Baseline(++)(Chen et al., 2019), using the implementations provided by the LibFewShot package.2. We use the meta-training hyperparameters suggested in the documentation of this implementation, which were tuned usingthe meta-validation set of miniImageNet. We use the standard Conv4 architecture for miniImageNet andCIFAR-FS, ResNet18 for meta-album. From the vision & language community, we also study CLIP (Radfordet al., 2021) , using the VIT-B32 vision encoder architecture.",
  "Performance Estimation": "Experimental SetupFor each meta-test episode, all evaluators are given only the support set andrequired to produce an estimate of the generalisation error for models trained using each of the FSL algorithmswe consider. We additionally construct a query set that is several times larger than the support set, and isused as an oracle estimate for the performance of each model. For each evaluator we construct a sample ofperformance estimates, where each data point corresponds to a meta-test episode. We report the mean ofthe accuracy across episodes, and also the mean absolute difference in accuracy between each estimator andthe oracle accuracy. ResultsFor the 5-way 5-shot setting, compares FSL accuracy as observed by the oracle withthe four standard estimators considered. We can see that the estimators tend to under-estimate the actualaccuracy of the oracle, to varying degrees. This is due to the performance estimators having to sacrificesome of the training data in order to estimate performance, meaning the model fit is poorer than a modelthat is trained on all available data. This issue also exists in the many-shot setting, but the effect is usuallynegligible in that case because of the diminishing returns of training on more data. presents the mean absolute difference between each estimator and the oracle. We can see that 5-foldCV tends to have the lowest difference, i.e., it provides the best estimate of the oracle accuracy. We note,however, that it is still likely to be too high to be practically useful in many applications. To investigatethis on a more fine-grained level, we visualise the distribution of absolute differences in . From thisfigure we can see that the distribution of errors is quite dispersed for all combinations of learning algorithmand evaluator: for all potential choices there is at least a 50% chance the validation accuracy would be wrongby more than 10%, in absolute terms. This means that there is no pipeline that one can use to reliably trainand validate models to ensure they are safe to deploy in situations where one must take into account therisks involved in deploying their models. We next analyse how the error between oracle and estimator varies with number of shots. From the results in, we can see that the error of the estimator decreases with shot number as expected, but tends to besubstantial in the 20 shot regime typically considered by FSL. Together, , , and illustrate our point that there is no good existing solution to task-level FSL performance estimation. FSL with Vision & Language Foundation ModelsIn we plot the number of shots versusthe MAE and accuracy, measured on logistic regression models trained on features extracted from CLIP. Theissues seen in the meta-learning setting are less pronounced hereparticularly in the Food101 and Flower102datasets. Measuring accuracy can be thought of as sampling from a Bernoulli distribution with the aim of",
  "Baseline": "Hold OutCV 5-FoldCV LOOBootstrapping Baseline++ Hold OutCV 5-FoldCV LOOBootstrapping ProtoNet Hold OutCV 5-FoldCV LOOBootstrapping MAE Frequency R2D2 Hold OutCV 5-FoldCV LOOBootstrapping MAML : Box plots showing the distribution of absolute differences between the estimated accuracy andoracle accuracy on the meta-test episodes of miniImagenet. Distributions should be ideally concentrated asclose to zero as possible, but we can see that a substantial proportion of the mass is far away from zero.This indicates that many of the performance estimates are unreliable.",
  ": Dependence of estimator-oracle error on shot number. Estimator error is substantial in the few-shot regime. The error bars indicate the standard error": "measuring the success parameter, p. An accurate model will have p close to one. The variance of theBernoulli distribution is p(1p), and the variance of the Bernoulli distribution directly determines the valueof the MAE. Therefore, estimating performance of models with high accuracy will generally be more reliable.Paradoxically, we measure the performance of models to determine if they have high accuracy.",
  "Performance Estimation on Imbalanced Problems": "Standard FSL evaluation protocols assume perfectly balanced class labels, but this is rarely true in practice.We conduct additional experiments on imbalanced data using the linear imbalance protocol of Ochal et al.(2023), where the support set for each episode contains 25 examples. Note that we do note consider theextreme class imbalance sometimes addressed in the more general machine learning literature; our focus ison weakening the assumption of perfect balance. In the linear imbalance setting, each episode is composedof examples from five different classes, but the counts of examples for classes one through five are (4, 4, 5,6, 6). Baseline (i.e., logistic regressions (Chen et al., 2019)) models are trained and the MAE between eachperformance estimator and the performance measured on the query set, which has the same class distributionas the support set but contains 100 examples. The results are reported in , with box plots showingthe distribution of absolute differences across episodes in . The conclusions remain unchanged fromthose identified in the balanced classes experiments: 5-fold CV still performs the best, but still has an MAEof at least 10 for half the episodes.",
  "Further Analysis of Cross-Validation": "We have shown that 5-fold CV is the least bad existing estimator for task-level FSL performance estimation.This might seem surprising, as LOO-CV is generally considered preferred from a statistical bias and variancepoint of view (Witten et al., 2016). To analyse this in more detail, we plot the error as a function of folds",
  "Evaluator": "MetaAlbumMini : Box plots showing the distribution of absolute differences across each episode. Ideally these shouldbe concentrated close to zero, but we observe that there is significant probability density far away from zero. in (left). The results of this show that increasing the number of foldsand therefore reducing thebias introduced by fitting each fold on a smaller training datasetis counteracted by another phenomenonthat causes LOO-CV to have low quality performance estimates. We find that 5-fold CV presents the idealtrade-off between these two effects. We hypothesise that the negative effect that causes CV with a large number of folds to become less accurateis related to an induced distribution shift between the training folds and the validation fold.Considerthe standard balanced few-shot learning experimental evaluation setting.In the case of LOO-CV, theconsequence of holding out one example for evaluation will mean that the training data contains one classwith fewer examples than the other classes. Moreover, the test example will come from this minority class,which will result in LOO-CV evaluating the model in the pathological case where all test examples belongto the minority class encountered during training. To investigate this hypothesis, we vary the number of ways while keeping the size of the support set fixed.As the number of classes is reduced, the number of training examples per class is increased, and classimbalance is less prevalent. (right) shows the result. Using the number of ways as a proxy forclass imbalance in the LOO-CV setting, we see there is an association between class imbalance and MAE ofLOO-CV performance estimation.",
  "Model Selection": "An alternative to accurate performance estimates that is acceptable in some situations is to instead rank a setof prospective models and select the one that performs the best according to some performance estimator. Inthis setting, the estimator can exhibit some types of biases without having an impact on the final rankings.For example, the performance underestimation bias of the estimators may not have a significant impact onthe final rankings, as long as all models are similarly affected by this bias. However, the ranking procedureis still sensitive to variance of the estimators. To this end, we investigate how well the rankings produced by the different estimators are correlated (inthe Spearman Rank correlation sense) with the rankings produced by the oracle estimator. Rankings areproduced for each episode in the meta-test sets, and the mean Spearman rank correlation (w.r.t. the oracle)is computed for each performance estimator.",
  "R2D2": ": Further analysis of the estimation accuracy of cross validation performed on the miniImageNetdataset. Left: MAE between k-fold CV estimates and the oracle estimates, as a function of k. Right: MAEof LOO-CV, relative to the oracle, as the number of ways (and therefore class imbalance) is increased. Theerror bars indicate the standard error.",
  "ModelCIFAR-FSminiImageNetMeta-Album": "Oracle80.11 0.49571.40 0.46463.53 0.932Hold-Out71.65 0.74962.79 0.71056.30 1.0485-Fold CV72.38 0.72663.73 0.73458.58 1.005LOO-CV73.29 0.73864.34 0.71758.53 1.014Bootstrapping73.44 0.72464.05 0.73758.62 1.011 ResultsThe results of this experiment are shown in . It is evident from these plots that noevaluator produces rankings that are highly correlated with the oracle rankings, with a maximum achievedcorrelation of around only 0.36 for 5-fold CV on the meta-album benchmark, and resampling methods witha large number of iterations generally being the most reliable approach. However, these results indicate thatwe are unlikely to be able to do effective model selection using currently available evaluation procedures.",
  "How can we improve FSL in practice?": "We further investigate how existing model selection methods applied at the task-level can be used to im-prove performance. Experiments are conducted on both types of model selection seen in machine learning:hyperparameter optimisation, and algorithm selection. Hyperparameter OptimisationAs a compromise between computational cost and obtaining the bestmodel rank correlation, we propose a new baseline for FSL that uses 5-fold cross validation to tune thehyperparameters of different learners. We consider the ridge regularisation parameter of a Baseline (i.e.,logistic regression) model (Chen et al., 2019) and the number of fine-tuning iterations in MAML. The resultsfor this experiment are shown in for the meta-learning setting, and for the CLIP setting.From these results we can see that using 5-fold CV can provide a noticeable improvement over the aggregatedaccuracy of the standard Baseline, but tuning the number of iterations for MAML is unreliable. Algorithm SelectionBy using each estimator to rank the performance of each FSL algorithm on a per-episode basis, we can try to select which algorithm should be used in each episode to maximise performance.",
  "Accuracy": "Food101 N=5N=101 (all)N=10 : We tune the Ridge regression hyper-parameter on a task-level basis using 5-fold cross-validationfor different combinations of ways and shots. We report Accuracy vs K for Baseline (without tuning markedwith o) and Baseline + HPO (after tuning marked with x). It is observed that while hyper-parametertuning for each fold benefits performance, this effect is more pronounced for low values of K. E.g., MAML might perform best in one episode and Prototypical Networks in another. showsaggregated accuracy results when using each estimator to do this type of model selection. These resultsdemonstrate that LOO-CV and Bootstrapping are the best approaches to use for model selection, which isconsistent with the rank correlation results from .",
  "Discussion": "We investigate the effectiveness of methods for evaluating and selecting models on a task-level basis in few-shot learning. The results in provide several concrete takeaways that can be used to inform bestpractices and show the limitations of the state-of-the-art. Revisiting the questions asked in , weprovide three main insights. Performance EstimationIt is demonstrated that existing evaluation approaches are not able to provideaccurate estimates of model performance in the few-shot learning setting. For most combinations of learningalgorithm and evaluation method, the performance estimates obtained for individual episodes are more than10% away from the true accuracy, in absolute terms, for the majority of the episodes we generate. Even thebest combination is only below this threshold approximately 50% of the time. Recommendations for PractitionersThe fact that performance estimates for few-shot learning areunreliable means that there is currently no way to validate specific few-shot learning tasks. Practitionersshould not deploy any FSL system that requires being able to estimate in advance whether a particularattempt at learning has been successful there is unacceptable risk of performance being substantiallydifferent than estimated by any validation procedure. However, this risk can be mitigated by noting thatsome performance estimators such as LOO-CV and Bootstrapping consistently underestimate the accuracy.Such pessimistic estimators may be acceptable in some scenarios. Model SelectionWe investigate how well these inaccurate performance estimates can be used to rankmodels. Accurate rankings allow practitioners to select the best possible model, even if the exact performanceis not known. It would also enable a better degree of hyperparameter optimisation than is currently employedin many FSL settings. Our findings in this area show that there is only a mild correlation between the bestmodel ranking approaches and the oracle rankings provided by larger held-out datasets that are not typically",
  "available in the few-shot setting. Despite the only mild correlation, we do achieve a small degree of successin using these rankings for hyperparameter tuning": "What makes a good performance estimator?We argue that the stability of the underlying learningalgorithm is an important factor when estimating performance. Many of the existing estimators rely onconstructing a model using different subsets of the available training data and then aggregating severaldifferent performance estimates. Algorithmic stability has been shown to reduce the dependence on datawhen fitting models (Bousquet & Elisseeff, 2002), and more stable algorithms have also been shown to providemore reliable performance estimates when used in conjunction with cross validation (Aghbalou et al., 2022).This line of reasoning is congruent with our empirical results, where we see that the most stable algorithm(Prototypical Networks) consistently has low MAE (relative to other approaches) when coupled with CVestimator. Recommendations for Future WorkJust as few-shot learning requires specialised algorithms thattake advantage of learned prior knowledge, we propose that future work should design specialised evaluationprocedures. This could take the form of Bayesian estimation procedures that are informed by performanceon the meta-training and meta-validation episodes, or they could leverage other side-information to reducevariance in estimates.",
  "Mervyn Stone. Cross-validatory choice and assessment of statistical predictions. Journal of the royal statis-tical society: Series B (Methodological), 36(2):111133, 1974": "Eleni Triantafillou, Tyler Zhu, Vincent Dumoulin, Pascal Lamblin, Utku Evci, Kelvin Xu, Ross Goroshin,Carles Gelada, Kevin Swersky, Pierre-Antoine Manzagol, et al. Meta-dataset: A dataset of datasets forlearning to learn from few examples. In International Conference on Learning Representations, 2020. Ihsan Ullah, Dustin Carrin-Ojeda, Sergio Escalera, Isabelle Guyon, Mike Huisman, Felix Mohr, Jan N vanRijn, Haozhe Sun, Joaquin Vanschoren, and Phan Anh Vu. Meta-album: Multi-domain meta-dataset forfew-shot image classification. Advances in Neural Information Processing Systems, 35:32323247, 2022."
}