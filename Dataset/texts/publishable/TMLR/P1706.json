{
  "Abstract": "Can emergent language models faithfully model the intelligence of decision-making agents?Though modern language models already exhibit some reasoning ability, and theoreticallycan potentially express any probable distribution over tokens, it remains underexplored howthe world knowledge these pre-trained models have memorized can be utilised to compre-hend an agents behaviour in the physical world. This paper empirically examines, for thefirst time, how well large language models (LLMs) can build a mental model of reinforce-ment learning (RL) agents, termed agent mental modelling, by reasoning about an agentsbehaviour and its effect on states from agent interaction history. This research attemptsto unveil the potential of leveraging LLMs for elucidating RL agent behaviour, addressinga key challenge in explainable RL. To this end, we propose specific evaluation metrics andtest them on selected RL task datasets of varying complexity, reporting findings on agentmental model establishment. Our results disclose that LLMs are not yet capable of fully re-alising the mental modelling of agents through inference alone without further innovations.This work thus provides new insights into the capabilities and limitations of modern LLMs,highlighting that while they show promise in understanding agents with a longer historycontext, preexisting beliefs within LLMs about behavioural optimum and state complexitylimit their ability to fully comprehend an agents behaviour and action effects.",
  "Introduction": "Large language models (LLMs) perform surprisingly well in some types of reasoning due to their common-sense knowledge (Li et al., 2022b), including math, symbolic, and spatial reasoning (Kojima et al., 2022;Yamada et al., 2023; Momennejad et al., 2023; Zhao et al., 2024b). Still, most reasoning experiments focuson human-written text corpora (Cobbe et al., 2021; Lu et al., 2022), rather than real or simulated sequentialand temporal data, such as interactions of reinforcement learning (RL) agents with physical simulators. Thelatter scenario unveils the potential of leveraging LLMs for elucidating RL agent behaviour, with whichwe may further facilitate human understanding of such behavioura long-standing challenge in explainableRL (Milani et al., 2024; Lu et al., 2024). Leveraging LLMs for this purpose is tempting as they can provide",
  "Published in Transactions on Machine Learning Research (12/2024)": "Exercise caution.In general, LLMs are proneto hallucination, and their reasoning should only betrusted after expert auditing to ensure a high ratingof reliability/trustworthiness. It is crucial to exploreethical metrics to quantify trustworthiness. Consider multimodal input. While our evalua-tion used only textual input, practitioners may con-sider incorporating multimodal environmental data(e.g., visual or sensory inputs), which could poten-tially enhance LLMs understanding of behaviour anddynamics, leading to more trustworthy reasoning. Incorporate domain-specific RL data.LLMscould benefit from action- and dynamics-related data(e.g., sequential/temporal trajectories) during pre-training or post-training stages to improve their abil-ity to model RL agents. Practitioners may considerthis practice when LLMs fall short of their expecta-tion in mental modelling for tasks in their domain. Explore new learning paradigms. Theoreticiansmay consider developing new learning paradigms thatenables LLMs to learn and align with cause-effect re-lationships in input-output data, which could helpLLMs perform agent mental modelling in a moretrustworthy manner for a wider range of complex andunseen tasks.",
  ": A conception of LLMs approximating an agents mental model where language models reason overnon-transparent agent rationale into more interpretable artefacts for end-users": "There is an ongoing debate about whether the next-token prediction paradigm of modern LLMs can modelhuman-like intelligence (Merrill & Sabharwal, 2023; Bachmann & Nagarajan, 2024). While next-token pre-dictors can theoretically express any conceivable token distribution, it remains underexplored how the worldknowledge these models have memorised during the pre-training phase (Roberts et al., 2020) can be utilisedto comprehend an agents behaviour in the real or simulated physical world. In this work, we conduct thefirst empirical study to examine whether LLMs can build a mental model (Johnson-Laird, 1983; Bansalet al., 2019) of agents (), termed agent mental modelling, by reasoning about an agents behaviourand the consequences from its interaction history. Understanding LLMs ability to interpret agent behaviourcould guide the development of agent-oriented LLMs that plan and generate sequences of embodied actions.Though recent studies (Li et al., 2022a; Huang et al., 2023) show that LLMs can aid in planning for em-bodied tasks, they merely demonstrate a limited understanding of the physical world, often constrained bybasic, low-level control mechanisms. Further, this agent understanding could also inform the use of LLMs ascommunication mediators between black-box agents and various stakeholders (see Sec. 5.3 for a discussion). Understanding RL agent behaviour is more complex for LLMs than solving traditional reasoning tasks, whichoften involve the procedure of plugging different values into equations (Razeghi et al., 2022). In our study,we formalize the process of agent mental modelling, requiring LLMs to not only comprehend the actionstaken by the agent but also perceive the resulting state changes (details in Sec. 3). The contributions of this paper include: 1) we shed light on evaluating LLMs ability to build a mentalmodel of RL agents, including both agent behaviour and environmental dynamics, conducting quantitativeand qualitative analyses of their capabilities and limitations; 2) we present empirical evaluation results inRL tasks, offering a well-designed testbed for this research with proposed evaluation metrics, and discussthe broader implications of enabling agent mental modelling.",
  "Related Work": "Mental Modelling for Explaining Agents. With rapid advancements in RL techniques, understandingthe behaviour of black-box RL agents has become increasingly critical yet challenging (Qing et al., 2022;Milani et al., 2024). Various methods have been proposed to improve human comprehension of agent deci-sions, including visual explanations that highlight salient state features (Greydanus et al., 2018; Iyer et al.,2018), building surrogate models like decision trees to approximate original complex agents (Bastani et al.,2018), and learning interpretable agents through causal reasoning (Lu et al., 2024). From a psychologicalperspective, the explainability of RL agents relates to mental modelling, i.e., understanding how learningsystems function (Johnson-Laird, 1983; Bansal et al., 2019). Previous studies often focus on isolated aspects",
  "LLM-Xavier Evaluation Framework": "Our work studies the capability of LLMs to understand and interpret RL agents, i.e., agent mental modellingin the context of Markov Decision Process (MDP) M (Puterman, 2014), including policies : S A andtransition function T : S A S, where S represents the state space and A represents the action space.See for an overview of the LLM-Xavier1 evaluation framework.",
  "In-Context Prompting": "The evaluation is carried out in the context of an RL task T which can be viewed as the instantiation ofan MDP M. For each T , we compile a dataset of interactions between the agent and the task environment,consisting of traversed state-action-reward tuples, denoted as ET := {(si, ai, ri)}iL, where L indicates thetask episode length. Further, the subset of the interaction history with a time window (history size) Hending at time t is denoted as Et,H := {(si, ai, ri)}tH1it, i.e., capturing the most recent H tuples up totime t L 1. The in-context learning prompts we constructed consist of task-specific background information, agent be-haviour history, and evaluation question prompts (see Appendix B for example instantiated prompts):",
  "Evaluation Metrics": "Evaluating the extent to which LLMs can develop a mental model requires examining their understanding ofboth the dynamics (mechanics) of environments that RL agents interact with and the rationale behind theagents chosen actions. Mental modelling implies a comprehensive grasp of agents, which is difficult to fullycapture. However, we believe that predictability is a key component of understanding. To systematicallyevaluate these aspects, we designed a series of targeted evaluation questions focused on predictability.",
  "Post-processing of LLMs Predictions in Practice": "The evaluation prompts are tailored to the RL tasks action or state space. For discrete actions, LLMs predicta single integer within the action range. For continuous actions, we explore two options: (1) predicting whichbin (from a manually divided set of 10) the next action will fall into, and (2) directly predicting the absoluteaction value within the valid range for each action dimension. For continuous state prediction, we adoptpredicting relative changes (e.g., increase, decrease, unchange) instead of exact state values. This approachassesses the LLMs ability to sense state transitions (s), e.g., changes in physical properties in physicstasks. Detailed evaluation prompts are provided in Appendix B. We extract predictions by post-processing the generations y LLM(|xquery, Et,H) with regular expressionsand compute performance by comparing them to the ground truth from the dataset. Accuracy/matchingrate for predicting states and actions is calculated as the number of correct predictions divided by the querylength (L Et,H). Note that if LLMs predict absolute action values, both predicted and ground truth valuesare quantised into 10 bins, and matching is based on bin alignment. Detailed prediction post-processing isdiscussed in Appendix C.",
  "Experimental Setup": "We empirically evaluate contemporary open-source and proprietary LLMs on their understanding of theagents mental model, including Llama3-8B2, Llama3-70B, GPT-3.53, and GPT-4o4 models5. All languagemodels are prompted with the Chain-of-Thought (CoT) strategy (Wei et al., 2022b), explicitly encouragedto provide reasoning with explanations before jumping to the answer. Offline RL Datasets. To benchmark LLMs ability to build a mental model of an agents behaviour, weselected a variety of tasks featuring different state spaces, action spaces, and reward spaces, resulting in adataset comprising seven tasks (Brockman et al., 2016) with approximately 2000 query samples, representedas (st, at, rt) tuples. Four of the seven tasks are classic physical control tasks of increasing complexity, whilethe other three are from the Fetch environment (Plappert et al., 2018), which includes a 7-DoF arm with atwo-fingered parallel gripper. Brief descriptions of each task are provided below, with corresponding visualsin . See Appendix A.2 for detailed descriptions and in Appendix A.1 for task statistics.",
  ": Visual representations of the seven tasks utilised in the evaluation experiments": ": Comparative plots of LLMs performance across various tasks with different history sizes (utilisingindexed history in prompts, i.e., Et,H presented with indices as prefixes). The Pendulum task evaluatescontinuous action prediction. First two columns show agent understanding; last two columns show dynamicsunderstanding. A description of these tasks can be found in Appendix A.2.",
  "LLMs can utilise agent history to build mental model": "The experimental results indicate that LLMs can predict agent behaviours to a certain extent, for examplein MountainCar, where they achieve over 75% accuracy, far surpassing the random guess baseline (1/3chance for three action choices). However, performance declines with more challenging tasks like Acrobotand FetchPickAndPlace, as illustrated in (or in Appendix F.5 for averaged accuracy) and, which feature larger state and action spaces. We hypothesise that complex tasks require morespecialized knowledge, whereas common-sense knowledge about cars and hills aids LLMs predictions in theMountainCar task. Longer histories enhance understanding but can degrade with excess. We study the impact ofthe size of history provided in the context. As expected, as is shown in , providing a longer historygenerally improves LLMs understanding of agent behaviours. However, the benefits of including more historysaturate and may even degrade, as seen in action prediction across different models, particularly Llama3-70b. This indicates that current LLMs, despite their long context length, struggle to handle excessive data incontext. In this case, more data may hinder the ability to model the agents behaviour, which is in contrastwith a typical learning scenario where model performance rapidly increases as learning samples increase. The issue of performance decline due to an excessively long history becomes more pronounced for dynamicspredictions, as evidenced in the MountainCar results (refer to for extensive details). However,as task complexity increases, the detrimental effects of redundant history may diminish (as observed inAcrobot results in and Fetch-series results in ), primarily because of the challenges posedby complex state and action spaces. On the whole, across different tasks, smaller models show significant variations in optimal history sizes forboth behaviour and dynamics predictions (see or in Appendix F.6 for extracted values).In contrast, larger models like GPT-3.5 are more consistent, achieving the highest accuracy with a historysize of 5 (GPT-4o with a size of 1). This indicates that larger models handle contextual information moreconsistently in inference to an extent, while smaller models fail to utilise history statistics effectively. Tofurther explore beyond basic accuracy, we offer a statistical analysis on how gradually increasing history sizeaffects previous predictions (improve/worsen) in Appendix F.7. Regressing on absolute action values is easier than predicting action bins. Surprisingly, LLMsperform better at predicting absolute action values than at predicting the bins into which the estimated actionfalls (refer to Appendix B.5 for differences in prompts). At most, LLama3-8b can allocate the numbers intocategories with a mere 10.87% accuracy for the Pendulum task (GPT-3.5 achieves 39.19%), but performsbetter in predicting numeric values with an accuracy of up to 47.73% (GPT-3.5 scores 56.82%). A detailedcomparison of the averaged accuracy across LLMs is depicted in . We hypothesise that predictingbins requires additional math ability to categorise values using context information. Refer to andAppendix F.6.1 for the illustrative discrepancy.",
  "Inferring the dynamics in a simulated world for different tasks can be challenging in many aspects, such asreasoning on a high-dimension state, computing physics consequences, and so on": "To investigate LLMs potential of understanding dynamics, first, we investigate the impact of providingdynamics principles, which turns out to improve both behaviour and dynamics prediction when the dynamicscontext is informed to LLMs (see for details). Further, we explicitly examined prediction performance across state components for each dimension. Asdepicted in , LLMs find it relatively easier to sense car position (element 0) than velocity (element1) for the MountainCar task; in contrast, for the Acrobot task, LLMs exhibit nearly uniform predictionaccuracy across all state elements due to the difficulty in sensing state changes (see Appendix F.2 for",
  ": Dynamics of LLMs performance on predicting individual state element for the MountainCar task(with indexed history in prompts)": "State span is statistically influential in LLMs dynamics understanding. Based on the observationsabove, we examined the factors that statistically influence LLMs perception of state changes. demonstrates a decent linear relationship between the span of state elements and LLMs ability to detectthese changes in tasks like MountainCar, Acrobot, and LunarLander, especially with GPT-3.5. For Acrobot,we observe a low slope with a moderate fit, while LunarLander shows a positive slope with a moderate",
  "Understanding error occurs from various aspects": "With the anticipation that LLMs explanatory reasoning (elicited via CoT) can benefit the human un-derstanding of agent behaviour, in addition to the existing quantitative results, we further examined thereasoning error types across LLMs by manually reviewing their judgments on the rationale of actions taken. shows an examination of the MountainCar task, highlighting that LLama3-8b displays the mosterrors.Meanwhile, GPT-3.5, despite having superior task comprehension (e.g., referring to momentumstrategies), is less effective at retaining task descriptions in memory compared to Llama3-70b. Notably,GPT-4os responses are of higher quality overall. While GPT-4o is occasionally inconsistent with the op-timal momentum strategy (i.e., initially accelerating left), it displayed a significantly better grasp of thisstrategy than other models. Furthermore, it avoids fundamental mistakes such as misinterpreting numbersor the presented information. Detailed error type reports are available in Appendix G.1.",
  "GPT-3.560%67%Llama3-8b40%52%Llama3-70b67%65%GPT-4o85%81%": "Pre-training biases may affect understanding despite history length. Interestingly, we found thatLLMs prediction errors may be influenced by pre-training biases towards cautious actions. inAppendix F.7 shows that increased history size does not affect late-episode steps where RL agents consistentlyaccelerate right to finish. Instead, LLMs mentally suggest conservative actions, like accelerating left toavoid overshooting, regardless of history context size. We hypothesise that these LLMs prediction errorsmay not be solely due to difficulty in processing extensive histories but also stem from inherent biases aboutagent behaviour and environment context, consistent with our earlier observations on confirmation bias.",
  "Data format influences understanding": "Prompting format generally has an impact on LLMs reasoning performance. In the context of agent un-derstanding, we do an ablation study to investigate the robustness of prompts on the history format andprovided information. We find that: 1) Excluding the sequential indices from the history context in prompts for LLMs generally negativelyimpacts their performance in most tasks, indicating that LLMs still struggle to process raw dataand indexing helps. The resulting performance variations are reported in . 2) Task description, despite not being directly relevant to numerical value regression as in statistics, isessential for a better understanding of both agent behaviour and dynamics, which brings the promiseof utilising LLMs to digest additional information beyond mere numerical regression when mentalmodelling agents. The ablation results can be found in Appendix F.8.2.",
  "Conclusion": "This work studies an underexplored aspect of next-token predictors, with a focus on whether LLMs can builda mental model of agents. We proposed specific prompts to evaluate this capability. Quantitative evaluationresults disclose that LLMs can establish agent mental models to some extent only since their understandingof state changes may diminish with increasing task complexity (e.g., high-dim spaces); their interpretationof agent behaviours may tumble for tasks with continuous actions. Analysis of evaluation prompts revealsthat their content and structure, such as history size, task instructions, and data format are crucial for theeffective establishment, indicating areas for future improvement. A further review of LLMs error responses(elicited via CoT prompting) highlights qualitative differences in LLMs understanding performance, withmodels like GPT-4o showing superior comprehension and fewer errors compared to the small Llama3 model.Additionally, it reveals that LLM responses may be influenced by preexisting beliefs about optimal behaviour,which can be a hurdle or a catalyzer for understanding. These findings suggest the potential and limitationsof in-context mental modelling of agents within MDP frameworks and underscore the possible role of LLMsas communication mediators between black-box agents and stakeholders, pointing to future research avenues.",
  "Potential Direction": "It remains unclear whether LLMs can benefit from thousands of agent trajectories compared to the lim-ited number of examples studied in this paper.We hypothesise that large amounts of demonstrations(state-action-reward tuples) in the prompt could enhance the capacity that LLMs have already developed.Additionally, fine-tuning LLMs with demonstrations (Lin et al., 2023; Wang et al., 2024) from specific do-mains may further improve their understanding capacity in these domains. Further analysis on this aspectis left for future work. At this stage, LLMs lack several key aspects required for true understanding, often due to reasoning errors andlimitations in predictability. Relying on LLMs for mental modelling without oversight could be problematic.Therefore, our key takeaway is that rather than using a fully automated, LLM-driven evaluation system foragent mental modelling, domain experts are expected to be involved upfront to audit LLMs reasoning andensure reasonable predictability and reliability before presenting the results to non-expert practitioners. Afew concrete suggestions for utilising LLMs in agent mental modelling are provided in Appendix H. We recognise that the issue of hallucination may exist. To increase the robustness and reliability of usingLLMs for explaining an agents behaviour, a detailed analysis of this behaviour is necessary before beingdeployed to a setting where they directly interact with humans. Also, our evaluation results underscore theneed for developing methods to mitigate hallucinations.",
  "Future-Proofing the Approach for LLM-Based Agents": "While our current framework focuses on evaluating LLMs capability of mental modelling traditional RLagents, it remains equally relevant as RL agents evolve into LLM-based counterparts that can providehuman-readable justifications for their decisions. One might argue that these self-generated explanationsreduce the need for external evaluation. However, these internal justifications may not reflect a truly objectivemental model as they can be shaped by the agents underlying optimisation processes, biases, and emergentbehaviours. To address this, when evaluating LLM-based agents, we can extend our framework to comparetheir self-reported rationales against the mental model formed by an independent evaluator LLM. Furthermore, as LLM-driven agents rapidly adapt policies in response to dialogue histories, external instruc-tions, or internal reasoning steps, we can integrate temporal tracking of these shifts in real-time. This enablesthe evaluator LLM to detect subtle policy shifts and predict future actions based on evolving rationales. AsLLM-based agents expand into multi-domain environments with non-traditional state, action, and rewardstructuressuch as language-based tasksour framework can similarly evolve, challenging the evaluatorLLM to generalise beyond conventional RL dynamics. Finally, as we leveraged human input for error-typeannotation and analysis, future-proofing involves integrating semi-automated tools to handle intricate LLM-agent interactions at scale. Human evaluators may focus on higher-level interpretation and strategic insights,while automated metrics track the consistency and coherence of evolving mental models over time.",
  "Broader Impact Statement": "We do not anticipate any immediate ethical or societal implications from our research. However, since weexplore LLMs applications for enhancing human understanding of agents, it is important to be cautiousabout the potential for fabricated or inaccurate claims in LLMs explanatory responses, which may arisefrom misinformation and hallucinations inherent to the LLMs employed.It is recommended to use ourproposed evaluation prompts and task dataset with care and mindfulness.",
  "Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and WojciechZaremba. Openai gym, 2016. URL": "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.Advances in neural information processing systems, 33:18771901, 2020. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, MatthiasPlappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math wordproblems. arXiv preprint arXiv:2110.14168, 2021. Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid,Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. Palm-e: An embodied multimodal language model.In International Conference on Machine Learning, pp. 84698488. PMLR, 2023. Shivam Garg, Dimitris Tsipras, Percy S Liang, and Gregory Valiant.What can transformers learn in-context? a case study of simple function classes. Advances in Neural Information Processing Systems, 35:3058330598, 2022.",
  "Samuel Greydanus, Anurag Koul, Jonathan Dodge, and Alan Fern. Visualizing and understanding atariagents. In International Conference on Machine Learning (ICML), pp. 17921801. PMLR, 2018": "Hado van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double q-learning. InProceedings of the Thirtieth AAAI Conference on Artificial Intelligence, AAAI16, pp. 20942100. AAAIPress, 2016. Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tomp-son, Igor Mordatch, Yevgen Chebotar, et al. Inner monologue: Embodied reasoning through planningwith language models. In Conference on Robot Learning, pp. 17691782. PMLR, 2023. Rahul Iyer, Yuezhang Li, Huao Li, Michael Lewis, Ramitha Sundar, and Katia Sycara. Transparency andexplanation in deep reinforcement learning neural networks.In Proceedings of the 2018 AAAI/ACMConference on AI, Ethics, and Society, pp. 144150, 2018.",
  "Brenden M Lake, Tomer D Ullman, Joshua B Tenenbaum, and Samuel J Gershman. Building machines thatlearn and think like people. Behavioral and brain sciences, 40:e253, 2017": "Michael Laskin, Luyu Wang, Junhyuk Oh, Emilio Parisotto, Stephen Spencer, Richie Steigerwald,DJ Strouse, Steven Hansen, Angelos Filos, Ethan Brooks, et al. In-context reinforcement learning withalgorithm distillation. arXiv preprint arXiv:2210.14215, 2022. Teven Le Scao and Alexander M Rush. How many data points is a prompt worth? In Proceedings of the2021 Conference of the North American Chapter of the Association for Computational Linguistics: HumanLanguage Technologies, pp. 26272636, 2021. Jonathan Lee, Annie Xie, Aldo Pacchiano, Yash Chandak, Chelsea Finn, Ofir Nachum, and Emma Brun-skill. Supervised pretraining can learn in-context reinforcement learning. Advances in Neural InformationProcessing Systems, 36, 2023. Shuang Li, Xavier Puig, Chris Paxton, Yilun Du, Clinton Wang, Linxi Fan, Tao Chen, De-An Huang,Ekin Akyrek, Anima Anandkumar, et al. Pre-trained language models for interactive decision-making.Advances in Neural Information Processing Systems, 35:3119931212, 2022a. Xiang Lorraine Li, Adhiguna Kuncoro, Jordan Hoffmann, Cyprien de Masson dAutume, Phil Blunsom, andAida Nematzadeh. A systematic investigation of commonsense knowledge in large language models. InProceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 1183811855, 2022b. Yingcong Li, Muhammed Emrullah Ildiz, Dimitris Papailiopoulos, and Samet Oymak.Transformers asalgorithms: Generalization and stability in in-context learning. In International Conference on MachineLearning, pp. 1956519594. PMLR, 2023. Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, DavidSilver, and Daan Wierstra.Continuous control with deep reinforcement learning.arXiv preprintarXiv:1509.02971, 2015. Licong Lin, Yu Bai, and Song Mei. Transformers as decision makers: Provable in-context reinforcementlearning via supervised pretraining. In The Twelfth International Conference on Learning Representations,2023. Ruibo Liu, Jason Wei, Shixiang Shane Gu, Te-Yen Wu, Soroush Vosoughi, Claire Cui, Denny Zhou, andAndrew M Dai. Minds eye: Grounded language model reasoning through simulation. arXiv preprintarXiv:2210.05359, 2022. Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, PeterClark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science questionanswering. Advances in Neural Information Processing Systems, 35:25072521, 2022. Wenhao Lu, Xufeng Zhao, Thilo Fryen, Jae Hee Lee, Mengdi Li, Sven Magg, and Stefan Wermter. Causalstate distillation for explainable reinforcement learning. In Causal Learning and Reasoning, pp. 106142.PMLR, 2024.",
  "Yunpeng Qing, Shunyu Liu, Jie Song, Huiqiong Wang, and Mingli Song. A survey on explainable reinforce-ment learning: Concepts, algorithms, challenges. arXiv preprint arXiv:2211.06665, 2022": "Yasaman Razeghi, Robert L Logan IV, Matt Gardner, and Sameer Singh.Impact of pretraining termfrequencies on few-shot numerical reasoning. Findings of the Association for Computational Linguistics:EMNLP 2022, 2022. Adam Roberts, Colin Raffel, and Noam Shazeer. How much knowledge can you pack into the parametersof a language model? In Proceedings of the 2020 Conference on Empirical Methods in Natural LanguageProcessing (EMNLP), pp. 54185426, 2020.",
  "Jiuqi Wang, Ethan Blaser, Hadi Daneshmand, and Shangtong Zhang. Transformers learn temporal differencemethods for in-context reinforcement learning. arXiv preprint arXiv:2405.13861, 2024": "Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama,Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, PercyLiang, Jeff Dean, and William Fedus. Emergent abilities of large language models. Transactions on Ma-chine Learning Research, 2022a. ISSN 2835-8856. URL Certification. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al.Chain-of-thought prompting elicits reasoning in large language models. Advances in neural informationprocessing systems, 35:2482424837, 2022b. Jiannan Xiang, Tianhua Tao, Yi Gu, Tianmin Shu, Zirui Wang, Zichao Yang, and Zhiting Hu. Languagemodels meet world models: Embodied experiences enhance language models. Advances in neural infor-mation processing systems, 36, 2023. Mengdi Xu, Yikang Shen, Shun Zhang, Yuchen Lu, Ding Zhao, Joshua Tenenbaum, and Chuang Gan.Prompting decision transformer for few-shot policy generalization. In international conference on machinelearning, pp. 2463124645. PMLR, 2022.",
  "Ruiqi Zhang, Spencer Frei, and Peter Bartlett. Trained transformers learn linear models in-context. InR0-FoMo: Robustness of Few-shot and Zero-shot Learning in Large Foundation Models, 2023": "Haiyan Zhao, Hanjie Chen, Fan Yang, Ninghao Liu, Huiqi Deng, Hengyi Cai, Shuaiqiang Wang, Dawei Yin,and Mengnan Du. Explainability for large language models: A survey. ACM Transactions on IntelligentSystems and Technology, 15(2):138, 2024a. Xufeng Zhao, Mengdi Li, Wenhao Lu, Cornelius Weber, Jae Hee Lee, Kun Chu, and Stefan Wermter.Enhancing Zero-Shot Chain-of-Thought Reasoning in Large Language Models through Logic. In 2024Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), May 2024b. URL",
  "A.1Data Collection": "The dataset of interaction histories (episodes) is collected by running RL agents in each task. Unlike Liu et al.(2022), whose physics alignment dataset contains text-based physical reasoning questions resembling physicstextbooks, our dataset comprises interactions of RL agents with various physics engines (environments). Foreach task, episodic histories are collected by running single-task RL algorithms (Lillicrap et al., 2015; Hasseltet al., 2016; Schulman et al., 2017) to solve that task. An overview of the task dataset statistics is providedin .",
  "A.2A Full Task Description": "Below, we provide a complete description of the MountainCar task (at the last of the section), including itsMDP components. For the remaining tasks, only the task descriptions are provided. Most of the texts arecredited to Acrobot Task Description. The Acrobot environment is based on Suttons work in Generalization inReinforcement Learning: Successful Examples Using Sparse Coarse Coding and Sutton and Bartos book.The system consists of two links connected linearly to form a chain, with one end of the chain fixed. Thejoint between the two links is actuated. The goal is to apply torques on the actuated joint to swing the freeend of the outer-link above a given height while starting from the initial state of hanging downwards. Pendulum Task Description. The inverted pendulum swingup problem is based on the classic problemin control theory. The system consists of a pendulum attached at one end to a fixed point, and the other endbeing free. The pendulum starts in a random position and the goal is to apply torque on the free end to swingit into an upright position, with its center of gravity right above the fixed point. LunarLander Task Description. This environment is a classic rocket trajectory optimization problem.According to Pontryagins maximum principle, it is optimal to fire the engine at full throttle or turn it off.This is the reason why this environment has discrete actions: engine on or off. The landing pad is alwaysat coordinates (0,0). The coordinates are the first two numbers in the state vector. Landing outside of thelanding pad is possible. Fuel is infinite, so an agent can learn to fly and then land on its first attempt. FetchPickAndPlace Task Description. The task in the environment is for a manipulator to move ablock to a target position on top of a table or in mid-air. The robot is a 7-DoF Fetch Mobile Manipulatorwith a two-fingered parallel gripper (i.e., end effector). The robot is controlled by small displacements ofthe gripper in Cartesian coordinates and the inverse kinematics are computed internally by the MuJoCoframework. The gripper can be opened or closed in order to perform the graspping operation of pick andplace. The task is also continuing which means that the robot has to maintain the block in the target positionfor an indefinite period of time. FetchSlide Task Description. The task in the environment is for a manipulator to hit a puck in orderto reach a target position on top of a long and slippery table. The table has a low friction coefficient in",
  "MountainCar Task Prompt": "task_description =The Mountain Car MDP is a deterministic MDP that consists of a car placed stochastically atthe bottom of a sinusoidal valley, with the only possible actions being theaccelerations that can be applied to the car in either direction. The goal of the MDP isto strategically accelerate the car to reach the goal state on top of the right hill. observation_space =The observation is a ndarray with shape (2,) where the elements correspond to the following:position of the car along the x-axis (range from -1.2 to 0.6), velocity of the car (rangefrom -0.07 to 0.07)",
  "reward_space =The goal is to reach the flag placed on top of the right hill as quickly as possible, assuch the agent is penalised with a reward of -1 for each timestep": "transition_dynamics =Given an action, the mountain car follows the following transition dynamics,velocity_t+1 = velocity_t + (action - 1) * force - cos(3 * position_t) * gravityposition_t+1 = position_t + velocity_t+1where force = 0.001 and gravity = 0.0025. The collisions at either end are inelastic withthe velocity set to 0 upon collision with the wall.",
  "In next step {i} (indexed from 0), the agent transitioned to the state s{i} =": "{state}. In this state s{i}, the agent chose to take action a{i} = {action}.Based on your observation and understanding of the agent's behaviour up to thispoint, critically evaluate the rationale behind the agent's choice of actiona{i} in state s{i}. You're encouraged to scrutinize the correctness of theaction a{i}, especially if your analysis suggests that the action might beflawed or suboptimal.",
  "B.5Evaluation Prompts in Practice": "The evaluation prompts (parts b, c in Sec. 3.1) are adapted based on the nature of the RL tasks, specificallythe type of action or state space (discrete or continuous).For tasks with discrete action spaces, LLMsare prompted to output a single integer within the action range. For tasks with continuous actions, weevaluate two options:",
  "CPost-processing LLMs Predictions": "We evaluate LLMs using metrics that require predicting states and actions. We extract LLMs responsesthrough pattern matching and compute evaluation results by comparing them with the ground truth state-action pairs from the episodes on which the LLMs are evaluated. For predicting discrete actions, we compute the matching rate of the LLMs predicted actions with theground truth. For predicting continuous actions, if LLMs are prompted to predict bins, we compute thematching rate as we did for discrete actions, with the ground truth represented by the bin index to whichit belongs. However, if LLMs are queried to directly predict absolute action values, we quantize both thepredicted and ground truth values into bins (by dividing the original action range into 10 bins) and thenmeasure whether they fall into the same bin. For predicting continuous states, we evaluate if LLMs correctly predict the change in state, s, categorisingincreases as 1, decreases as 0, and unchanged as 2. We then compute the accuracy classification score fortheir predictions. We also record the accuracy of predicting changes in individual state elements, si.",
  "Llama3-8b replies": "The action of not accelerating might delay the agents arrival at the goal position, especially whenit is very close to the goal. It is crucial for the agent to maintain its momentum and continueaccelerating towards the goal to minimize the time taken to reach the flag. Analysis: The car needs to accelerate to the left to get to a position from which it can build enoughmomentum towards the right to overcome the right hill. The LLM is missing the information aboutthe environment that would allow it to understand this behaviour.",
  "Llama3-70b replies": "Given the agents behaviour of accelerating to the right when the car is far from the right hilland not accelerating when it is closer, the choice of action 1 (no acceleration) in state s18 seemsreasonable. This action allows the car to conserve momentum and potentially reach the goal positionmore efficiently.",
  "GPT-4o replies": "Given the cars current state, taking action 0 (accelerate to the left) might not be the most optimalchoice. The car is already moving left with a very small velocity, and further accelerating to the leftmight not significantly help in building the necessary momentum to move rightward. Instead, it mightbe more beneficial to start accelerating to the right (action 2) to begin the process of climbing theright hill.",
  "F.2Average State Element Prediction Accuracy": "In addition to reporting the dynamics of prediction accuracy for individual state elements, we report theaveraged prediction accuracy for state elements in the MountainCar task (), the Pendulum task(), the Acrobot task (), the LunarLander task (), the FetchPickAndPlace task(), the FetchPush task (), and the FetchSlide task (). Refer to for themeanings of the state element indices in Fetch-series tasks. We find that LLMs are slightly more sensitive to changes in angular velocity than angle, as shown by thePendulum and Acrobot results. In addition, LLMs show greater sensitivity to changes in position than linearvelocity, as demonstrated by MountainCar and LunarLander results.",
  "IndicesState Elements Meanings": "0End effector x position1End effector y position2End effector z position3Block x position4Block y position5Block z position6Relative block x position with respect to gripper x position7Relative block y position with respect to gripper y position8Relative block z position with respect to gripper z position9Joint displacement of the right gripper finger10Joint displacement of the left gripper finger11Global x rotation of the block in a XY Z Euler frame rotation12Global y rotation of the block in a XY Z Euler frame rotation13Global z rotation of the block in a XY Z Euler frame rotation14Relative block velocity in x direction with respect to the gripper15Relative block velocity in y direction with respect to the gripper16Relative block velocity in z direction with respect to the gripper17Block angular velocity along the x axis18Block angular velocity along the y axis19Block angular velocity along the z axis20End effector velocity in x direction21End effector velocity in y direction22End effector velocity in z direction23Right gripper finger velocity24Left gripper finger velocity",
  "F.3Influential Factors in State Element Prediction": "shows the linear relationship between state span and prediction accuracy, while showsthe corresponding residuals-fitted plot. illustrates the relationship between state variance andprediction accuracy. Detailed state span (min-max) statistics for each state element across all tasks aredetailed in . For each task in and , we used the raw state span during analysis,as LLMs process the unscaled state values. The linear analysis was conducted between the raw state spanand prediction accuracy. However, to compare relationships across tasks, we used a 0-1 dummy scale forplotting purposes, as shown in . The linear relationship analysis for different models and tasks reveals key insights. For GPT-3.5 on theMountainCar task, the positive slope indicates that accuracy improves with span, and the perfect r2 of1 shows a flawless fit, meaning the models predictions align perfectly with the data.For GPT-3.5 onthe Acrobot task, the moderate slope and r2 of 0.58 suggest a decent correlation with some unexplainedvariability. In contrast, the Llama3-70b model on the Pendulum task shows a low or near-zero slope and anr2 of 0, indicating the model does not effectively explain the variability in the data for this task. State element variance also influences dynamics prediction, being slightly more significant than state spanin tasks like Pendulum and LunarLander, as shown in . This offers a different perspective onhow LLMs dynamics understanding can be affected and generalized. Additionally, the linear relationship isoften less significant for smaller models, suggesting that larger models are more sensitive to these changes.In general, from a statistical point of view, larger spans and greater state variance facilitate the LLMsperception of changes due to larger value discrepancies. : The linear relationship between state span and state element prediction accuracy across all LLMsand tasks. Each blue dot represents the average prediction accuracy for the corresponding state element.MC stands for MountainCar, AC for Acrobot, Pen for Pedulum, LL for LunarLander, FetchPNPfor FetchPickAndPlace.",
  "F.6Dynamic Performance of All Evaluation Metrics": "The dynamics of LLMs understanding performance (with and without indexed history in prompts) withincreasing history size for the MountainCar task (), the Acrobot task (), the Pendulumtask ( and ), and the LunarLander task (). Among all results, it is observed that models understanding of agent behaviour improves significantly withsmaller history sizes but does not increase further with larger histories. In some cases, like with Llama3-70b,",
  "F.6.1Comparative performance of models on predicting discretized actions": "Continuing from the plot of LLMs performance on the Pendulum task with continuous actions (third row of in the main text), presents a comparative plot of LLMs performance on the Pendulumtask with discretized actions. The results show that LLMs are generally less effective at classifying actionbins compared to regressing on absolute action values.",
  "F.7Relative Improvement and Worsening Rates with Increased History Size": "When increasing history size in prompts for LLMs, we observed that previously incorrect predictions atcertain time steps may become correct, and vice versa (see for an example with the MountainCartask). We define the relative improvement rate for a history size H > 0 compared to a fixed history size 0as #(incorrectcorrect)0H",
  "#correct0": "As shown in , increasing history size does not impact steps near the episode end, where RL agentsconsistently accelerate to the right to cross the finish line. In contrast, LLMs, likely influenced by pre-trainingon human preferences, suggest more cautious actions, such as accelerating left to avoid overshooting. Thissafety bias, ingrained during pre-training, significantly affects their action predictions regardless of historylength. Thus, LLM prediction failures are not solely due to difficulty processing extensive histories; theirinherent beliefs about agent behaviour and environment context can also impair prediction accuracy incertain tasks. Further, by examining step-wise mismatches in the Acrobot task, it is observed that LLMs often justify theirpredictions mainly on the rewards received in previous steps, neglecting history patterns, especially whenthe history is short. and illustrate how these improvement/worsening rates change as history size increases.Analysing the impact of longer history on correcting previous incorrect predictions when H is fixed at 0 showsthat increasing history size improves LLMs understanding of agent behaviour initially, but further increasesmay degrade it. Conversely, increasing history size has a negligible influence on dynamics predictions. : Step-wise mismatches with increasing history size for the GPT-3.5 model on the MountainCartask (the same mismatch pattern is observed across different episodes). Each bar (corresponding to a differenthistory size) at each step index indicates a mismatch between the LLMs prediction (based on that historysize) and the action taken by the RL agent at that step. At certain steps (e.g., around 40), the model with h = 0 fails, while the model with h > 1 succeeds. Incontrast, at other steps (e.g., at the last steps), increasing history size minimally affects the LLMspredictions.",
  "F.8.2Comparison of models without using task instructions": "Akin to prior works by Mishra et al. (2022); Le Scao & Rush (2021), which show that task framing in promptinfluences language models, we observe a similar effect. When removing task instruction from evaluationprompts, models understanding performance across the majority of evaluation metrics is significantly de-grading, as demonstrated in MountainCar () and Acrobot () tasks; despite the historycontext (i.e., sequence of numerical values) remaining unchanged. We hypothesise that LLMs ability tomental model agents is enhanced by a more informative context.",
  "GLLMs Erroneous Responses inMountainCar Task": "Explanations of Various Error Types in LLMsReasoning. A manual review of the MountainCartask across three LLMsGPT-3.5, Llama3-8b, andLlama3-70brevealed significant differences in theirexplanations that were not necessarily anticipatedfrom the quantitative analysis. provides anoverview of the types and for counts of errorsfound in each model. During the evaluation, a singleresponse could contain multiple error types. DespiteLlama3-8b producing the shortest responses, it alsohad the highest error count. (1) The first type of error, understanding thetask, appeared frequently when the LLMshad to evaluate a proposed action, such asno acceleration in the MountainCar task. Allthree models tended to be concerned aboutovershooting the goal of reaching a positionof >= 0.5. However, in this task, overshoot-ing is irrelevant since the goal is to surpass0.5.Similar replies across models suggestthis mistake stems from a shared common-sense notion. Additionally, Llama3-8b oftenfailed to recognise the presence of a hill onthe left side. (2) Logical mistakes were noted in GPT-3.5 andLlama3-70b when the LLMs justified movingleft without recognising the need for oscilla-tion to gain momentum, leading to paradox-ical replies. These types of errors were moreprevalent in Llama3-8b.",
  "task. Even the RL agent occasionally makessuch mistakes": "Aside from the errors, GPT-3.5 demonstrated a bet-ter understanding of the task, often referring to theneed to accelerate left to gain momentum for climbingthe right hill. This was rarely mentioned by Llama3-70b and never by Llama3-8b, indicating GPT-3.5ssuperior task comprehension and explanatory ability.Llama3-70b, however, had an advantage in maintain-ing coherence, as it was less likely to contradict itsarguments, unlike GPT-3.5, which occasionally ar-gued against an action before ultimately support-ing it. Both GPT-3.5 and Llama3-8b also displayedmisunderstandings of the actions, such as incorrectlydefining action 0 (no acceleration). This suggestsa common-sense bias toward interpreting 0 as no ac-tion.Llama3-70b was better at retaining the taskdescription in memory.",
  "G.1A Compact Analysis of Error Types": "shows a quantitative analysis of the frequencyof different error types committed by the LLMs forthe MountainCar task.The evaluation highlightedvarious types of errors (see in the Appendix),with Llama3-8b displaying the most errors despite itsshorter responses. A common error among all mod-els was misinterpreting the goal of the task, reflect-ing a shared common sense misunderstanding. Logi-cal errors, particularly in oscillation movements, wereprevalent in GPT-3.5 and Llama3-70b, while Llama3-8b frequently produced paradoxical replies. Misun-derstanding the task history and physical principleswas rare but present. Mathematical errors, especiallydisregarding the minus sign, occasionally impactedreasoning.Notably, GPT-3.5 demonstrated a bet-ter task understanding by referring to momentumstrategies in the task, an insight less frequently ornever mentioned by Llama3-70b and Llama3-8b, re-spectively. Llama3-70b did have one other advantageover other models as it was less often confused by itsargument and excelled in maintaining task descrip-tions. Despite occasional errors in defining actions,GPT-3.5s superior comprehension of the task con-tributed to its higher-quality explanations."
}