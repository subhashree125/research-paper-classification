{
  "Abstract": "Although neural networks are conventionally optimized towards zero training loss, it hasbeen recently learned that targeting a non-zero training loss threshold, referred to as aflood level, often enables better test time generalization.Current approaches, however,apply the same constant flood level to all training samples, which inherently assumes allthe samples have the same difficulty. We present AdaFlood, a novel flood regularizationmethod that adapts the flood level of each training sample according to the difficulty of thesample. Intuitively, since training samples are not equal in difficulty, the target trainingloss should be conditioned on the instance. Experiments on datasets covering four diverseinput modalitiestext, images, asynchronous event sequences, and tabulardemonstratethe versatility of AdaFlood across data domains and noise levels.",
  "Introduction": "Preventing overfitting is an important problem of great practical interest in training deep neural networks,which often have the capacity to memorize entire training sets, even ones with incorrect labels (Neyshaburet al., 2015; Zhang et al., 2021). Common strategies to reduce overfitting and improve generalization per-formance include weight regularization (Krogh & Hertz, 1991; Tibshirani, 1996; Liu & Ye, 2010), dropout(Wager et al., 2013; Srivastava et al., 2014; Liang et al., 2021), label smoothing (Yuan et al., 2020), and dataaugmentation (Balestriero et al., 2022). Although neural networks are conventionally optimized towards zero training loss, it has recently beenshown that targeting a non-zero training loss threshold, referred to as a flood level, provides a surprisinglysimple yet effective strategy to reduce overfitting (Ishida et al., 2020; Xie et al., 2022). The original Floodregularizer (Ishida et al., 2020) drives the mean training loss towards a constant, non-zero flood level, whilethe state-of-the-art iFlood regularizer (Xie et al., 2022) applies a constant, non-zero flood level to eachtraining instance. Training samples are, however, not uniformly difficult: some instances have more irreducible uncertaintythan others (i.e. heteroskedastic noise), while some instances are simply easier to fit than others. It may",
  "Published in Transactions on Machine Learning Research (08/2024)": "We set = 0.5 following one of the experiments in Hinton et al. (2014) so that all the methods have onlyone hyperparameter to tune. We tune the temperature scale with a grid search on {1, 2, 3, , 9, 10}. ResultsThe results are presented in . We report the means and standard errors of accuracies overthree runs. We can observe that KD and flooding methods, including AdaFlood, are not significantly betterthan the unregularized baseline on SVHN. However, AdaFlood noticeably improves the performance overthe other methods on harder datasets like CIFAR10 and CIFAR100, whereas iFlood is not obviously betterthan the baseline and Flood is worse than the baseline on CIFAR100. The gap between AdaFlood and KDis more noticeable on CIFAR100, particularly with L2 regularization. DiscussionWhile iFlood is closely related to label smoothing, AdaFlood shares similarities with KD asboth utilize auxiliary networks. However, a motivation behind two algorithms are fundamentally different.KD relies on predictions made on already-seen training examples, whereas AdaFlood leverages predictions onintentionally forgotten (or unseen) examples. Since the predictions of teacher networks in KD are based onalready-seen examples, they do not serve as meaningful measures of uncertainty. In contrast, the predictionsfrom an auxiliary network in AdaFlood can effectively measure uncertainty, and flood levels computed fromthese predictions can function as uncertainty regularizations. A disadvantage of AdaFlood, however, is theadditional fine-tuning step required to forget already-seen examples, which is not necessary in KD.",
  "Related Work": "Regularization techniques have been broadly explored in the machine learning community to improve thegeneralization ability of neural networks. Regularizers augment or modify the training objective and aretypically compatible with different model architectures, base loss functions, and optimizers. They can beused to achieve diverse purposes including reducing overfitting (Hanson & Pratt, 1988; Ioffe & Szegedy, 2015;Krogh & Hertz, 1991; Liang et al., 2021; Lim et al., 2022; Srivastava et al., 2014; Szegedy et al., 2016; Vermaet al., 2019; Yuan et al., 2020; Zhang et al., 2018), addressing data imbalance (Cao et al., 2019; Gong et al.,2022), and compressing models (Ding et al., 2019; Li et al., 2020; Zhuang et al., 2020). AdaFlood is a regularization technique for reducing overfitting. Commonly adopted techniques for reducingoverfitting include weight decay (Hanson & Pratt, 1988; Krogh & Hertz, 1991), dropout (Liang et al., 2021;Srivastava et al., 2014), batch normalization (Ioffe & Szegedy, 2015), label smoothing (Szegedy et al., 2016;Yuan et al., 2020), and data augmentation (Lim et al., 2022; Verma et al., 2019; Zhang et al., 2018). Inspiredby work on double descent (Belkin et al., 2019; Nakkiran et al., 2021), Ishida et al. (2020); Xie et al. (2022)proposed Flood and iFlood, respectively, to prevent the training loss from reaching zero by maintaining asmall constant value. In contrast to the original flood regularizer, which encourages the overall training losstowards a constant target, iFlood drives each training samples loss towards some constant b. AdaFlood instead uses an auxiliary model trained on a heldout dataset to assign an adaptive flood level toeach training sample. Using a heldout dataset to condition the training of the primary model is an effectivestrategy in machine learning, and is regularly seen in meta-learning (Bertinetto et al., 2019; Franceschi et al.,2018), batch or data selection (Fan et al., 2018; Mindermann et al., 2022), and neural architecture search(Liu et al., 2019; Wang et al., 2021), among other areas.",
  "Problem Statement": "BackgroundGiven a labeled training dataset D = {(xi, yi)}Ni=1, where xi X are data samples andyi Y are labels, we train a neural network f : X Y by minimizing a training loss : Y Y R. Insupervised learning we usually have 0, but in settings such as density estimation it may be negative.",
  "(b) Training dynamics by difficulty": ": (a) Illustration of how difficulties of examples are dispersed with and without label noise (wherethe relevant portion of examples have their label switched to a random other label). (b) Comparison oftraining dynamics on some examples between iFlood and AdaFlood. The Hard example is labeled horse,but models usually predict cow; the Wrong example is incorrectly labeled in the dataset as cat (there isno rat class).",
  "While conventional training procedures attempt to minimize the average training loss, this can lead tooverfitting on training samples": "The original flood regularizer (Ishida et al., 2020) defines a global flood level for the average trainingloss, attempting to reduce the incentive to overfit.Denote the average training loss by L(f, B) =1BBi=1 (yi, f(xi)), where f(xi) denotes the model prediction and B = {(xi, yi)}Bi=1 is a mini-batch withsize of B. Instead of minimizing L, Flood (Ishida et al., 2020) regularizes the training by minimizing",
  "|(yi, f(xi)) b| + b.(2)": "MotivationTraining samples are, however, not uniformly difficult: some are inherently easier to fit thanothers. a shows the dispersion of difficulty on CIFAR10 and 100 with various levels of added labelnoise, as measured by the heldout cross-entropy loss from cross-validated models. Although difficulties onCIFAR10 without noise are concentrated around difficulty 0.5, as the noise increases, they vastly spreadout. CIFAR100 has a wide spread in difficulty, even without noise. A constant flood level as used in iFloodmay be reasonable for un-noised CIFAR10, but it seems less appropriate for CIFAR100 or noisy-label cases. Moreover, it may not be beneficial to aggressively drive the training loss for training samples that are outliers,noisy, or mislabeled. In b, we show training dynamics on an easy, wrong, and a hard example fromthe training set of CIFAR10. With iFlood, each examples loss converges to the pre-determined flood level(0.03); with AdaFlood, the easy example converges towards zero loss, while the wrong and hard examplesmaintain higher loss.",
  "Proposed Method: AdaFlood": "Many advances in efficient neural network training and inference, such as batch or data selection (Colemanet al., 2020; Fan et al., 2018; Mindermann et al., 2022) and dynamic neural networks (Li et al., 2021; Verelst& Tuytelaars, 2020), stem from efforts to address the differences in per-sample difficulty. AdaFlood connectsthis observation to flooding. Intuitively, easy training samples (e.g. a correctly-labeled image of a cat in",
  "!\"#,&": ": AdaFlood for settings where training data is limited and acquiring additional data is impractical.In the first stage, we partition the training set into two halves and train two auxiliary networks f aux,1 andf aux,2: one on each half. In the second stage, we use each auxiliary network to set the adaptive flood levelof training samples from the half it has not seen, via equation 4. The main network f is then trained onthe entire training set, minimizing the AdaFlood-regularized loss, equation 3. Note that the flood levels arefixed over the course of training f and need to be pre-computed once only. a typical pose) can be driven more aggressively to zero training loss without overfitting the model, whiledoing so for noisy, outlier, or incorrectly-labeled training samples may cause overfitting. These types of datapoints behave differently during training (Ren et al., 2022), and so should probably not be treated the same.AdaFlood differentiates training samples by setting a sample-specific flood level = {i}Bi=1 in its objective:",
  "i = (yi, (f aux,i(xi), yi)),(4)": "where f aux,i is an auxiliary model trained with cross-validation such that xi is in its heldout set, and ()is a correction function explained in a moment. illustrates the training process using equation 3,.5 gives further motivation, and .4 provides further theoretical support. The flood targets i are fixed over the course of training the main network f, and can be pre-computed foreach training sample prior to the first epoch of training f. We typically use five-fold cross-validation as areasonable trade-off between computational expense and good-enough models to estimate i, but see furtherdiscussion in .3. The cost of this pre-processing step can be further amortized over many trainingruns of the main network f since different variations and configurations of f can reuse the adaptive floodlevels. Correction function.Unfortunately, the predictions from auxiliary models are not always correct evenwhen trained on most of the training setif they were, our model would be perfect already. In particular,the adaptive flood levels i can be arbitrarily large for any difficult examples where the auxiliary model isincorrect; this could lead to strange behavior when we encourage the primary model f to be very incorrect.We thus correct the predictions with the correction function , which mixes between the datasets labeland the heldout models signal.",
  "Here = 0 fully trusts the auxiliary models (no correction), while = 1 disables flooding": "For K-way classification tasks, f(xi) RK is a vector of output probabilities (following a softmax layer)and the label is yi K, usually considered as a one-hot vector. Cross-entropy loss is then computedas: (yi, f(xi)) = Kk=1 yi,k log f(xi)k. Similar to the regression tasks, we define the correction function(f aux(xi), yi) for classification tasks as a linear interpolation between the predictions and labels as:",
  "Efficiently Training Auxiliary Networks": "Although the losses from auxiliary networks can often be good measures for the difficulties of samples, thisis only true when the number of folds n is reasonably large; otherwise the training set of size about n1 n |D|may be too much smaller than D for the model to have comparable performance. The computational costscales roughly linearly with n, however, since we must train n auxiliary networks: if we do this in parallel itrequires n times the computational resources, or if we do it sequentially it takes n times as long as traininga single model. To alleviate the computational overhead for training auxiliary networks, we sometimes instead approximatethe process by fine-tuning a single auxiliary network. More specifically, we first train a single base modelf aux on the entire training set D. We then train each of the n auxiliary models by randomly re-initializingthe last few layers, then re-training with the relevant fold held out. The overall process is illustrated inAlgorithm 1 and n = 2 case is described in . Although this means that xi does slightly influence the final prediction f aux,i(xi) (training on the test set),it is worth remembering that we use i only as a parameter in our model, not to evaluate its performance:xi is in fact a training data point for the overall model f being trained. This procedure is justified byrecent understanding in the field that in typical settings, a single data point only loosely influence the earlylayers of a network. In highly over-parameterized settings (the kernel regime) where neural tangent kerneltheory is a good approximation to the training of f aux (Jacot et al., 2018), re-initializing the last layer wouldcompletely remove the effect of xi on the model. Even in more realistic settings, although the mechanism isnot yet fully understood, last layer re-training seems to do an excellent job at retaining core features andremoving spurious ones that are more specific to individual data points (Kirichenko et al., 2023; LaBonteet al., 2023). For smaller models with fewer than a million parameters, we use 2- or 5-fold cross-validation, since trainingmultiple auxiliary models is not much of a computational burden. For larger models such as ResNet18,however, we use the fine-tuning method. This substantially reduces training time, since each fine-tuninggradient step is less expensive and the models converge much faster given strong features from lower levelsthan they do starting from scratch; .6 gives a comparison.",
  "Fine-tune": ": Efficient fine-tuning method for training a auxiliary network when held-out split is n = 2. First, asingle model f aux is trained on the entire training set D. Then, the last few layers of each of the n auxiliarymodels are randomly re-initialized and re-trained with the relevant fold held out. To validate the quality of the flood levels from the fine-tuned auxiliary network, we compare them to theflood levels from n = 50 auxiliary models using ResNet18 (He et al., 2016) on CIFAR10 (Krizhevsky et al.,2009); with n = 50, each model is being trained on 98% of the full dataset, and thus should be a goodapproximation to the best that this kind of method can achieve. The Spearman rank correlation betweenthe flood levels i from the fine-tuned method and the full cross-validation is 0.63, a healthy indication thatthis method provides substantial signal for the correct i. Our experimental results also reinforce that thisprocedure chooses a reasonable set of parameters.",
  "Theoretical Intuition": "For a deeper understanding of AdaFloods advantages, we now examine a somewhat stylized supervisedlearning setting: an overparameterized regime where the i are nonetheless optimal.Proposition 1. Let F be a set of candidate models, and suppose there exists an optimal model fopt arg minfF Ex,y(y, f(x)), where is a nonnegative loss function. Given a dataset D = {(xi, yi)}Ni=1, letfemp denote a minimizer of the empirical loss L(f, D) =1NNi=1 (yi, f(xi)); suppose that, as in an over-parameterized setting, L(femp, D) = 0. Also, let fada be a minimizer of the AdaFlood loss equation 3 usingperfect flood levels = {i}Ni=1 where i = (yi, fopt(xi)). Then we have that",
  "LAdaFlood(femp, D, ) = 2L(fopt, D) L(fopt, D) = LAdaFlood(fopt, D, ) = LAdaFlood(fada, D, ).(8)": "Proof. We know that L(fopt, D) will be approximately the Bayes risk, the irreducible distributional errorachieved by fopt; this holds for instance by the law of large numbers, since fopt is independent of therandom sample D. Thus, if the Bayes risk is nonzero and the i are optimal, we can see that empirical riskminimization of overparametrized models will find femp, and disallow fopt; minimizing LAdaFlood, on theother hand, will allow the solution fopt and disallow the empirical risk minimizer femp.",
  "|(yi, f(xi)) (yi, fopt(xi))| + (yi, fopt(xi))": "Since | | is nonnegative, we have LAdaFlood(f, D, ) L(fopt, D) for any f, and LAdaFlood(fopt, D, ) =L(fopt, D); this establishes that fopt minimizes LAdaFlood, and that any minimizer fada must achieve(yi, fada(xi)) = i for each i, so L(fada, D) = L(fopt, D). Using that (yi, femp(xi)) = 0 for each i, asis necessary for 0 when L(femp, D) = 0, shows LAdaFlood(femp, D, ) = 1",
  "Discussion: Why We Calculate Using Held-out Data": "In .2, we estimate i for each training sample using the output of an auxiliary network f aux(xi) thatis trained on a held-out dataset. In fact, this adaptive flood level i can be considered as the sample difficultywhen training the main network. Hence, it is reasonable to consider existing difficulty measurements basedon learning dynamics, like C-score (Jiang et al., 2021) or forgetting score (Maini et al., 2022). However, wefind these methods are not robust when wrong labels exist in the training data, because the network willlearn to remember the wrong label of xi, and hence provide a low i for the wrong sample, which is harmfulto our method. That is why we propose to split the whole training set into n parts and train f aux(xi) for ntimes (each with different n 1 parts). Dataset and implementationTo verify this, we conduct experiments on a toy Gaussian dataset, asillustrated in the first panel in . Assume we have N samples, each sample in 2-tuple (x, y). To drawa sample, we first select the label y = k following a uniform distribution over all K classes. After that, wesample the input signal x | (y = k) N(k, 2I), where is the noise level for all the samples. k is themean vector for all the samples in class k. Each k is a 10-dim vector, in which each dimension is randomlyselected from {, 0, }. Such a process is similar to selecting 10 different features for each class. Weconsider 3 types of samples for each class: regular samples, the typical or easy samples in our training set,have a small ; irregular samples have a larger ; mislabeled samples have a small , but with a flippedlabel. We generate two datasets following this same procedure (call them datasets A and B). Then, werandomly initialize a 2-layer MLP with ReLU layers and train it on dataset A. At the end of every epoch,we record the loss of each sample in dataset A. ResultThe learning paths are illustrated in the second panel in .The model is clearly ableremember all the wrong labels, as all the curves converge to a small value. If we calculate i in this way, alli would have similar values. However, if we instead train the model using dataset B, which comes from thesame distribution but is different from dataset A, the learning curves of samples in dataset A will behavelike the last panel in . The mislabeled and some irregular samples can be clearly identified from thefigure. Calculating i in this way gives different samples more distinct flood values, which makes our methodmore robust to sample noise, as our experiments on various scenarios show.",
  "Experiments": "We demonstrate the effectiveness of AdaFlood on three tasks (probability density estimation, classificationand regression) in four domains (asynchronous event sequences, image, text and tabular).We compareflooding methods on asynchronous event time in .1 and image classification tasks in .2.We also demonstrate that AdaFlood is more robust to various noisy settings in .3, and that it yieldsbetter-calibrated models in .4. Some ablation studies are provided in Sections 4.5 and 4.6.",
  "Results on Asynchronous Event Sequences": "We compare flooding methods on asynchronous event sequence datasets of which goal is to estimate theprobability distribution of the next event time given the previous event times. Each event may have a classlabel. Asynchronous event sequences are often modeled as temporal point processes (TPPs). DatasetsWe use two popular benchmark datasets, Stack Overflow (predicting the times at which usersreceive badges) and Reddit (predicting posting times). Following Bae et al. (2023), we also benchmark ourmethod on a dataset with stronger periodic patterns: Uber (predicting pick-up times). We split each trainingdataset into train (80%) and validation (20%) sets. Details are provided in Appendix A. Following the literature in TPPs, we use two metrics to evaluate models: root mean squared error (RMSE)and negative log-likelihood (NLL). As NLL can be misleadingly low if the probability density is mostly focusedon the correct event time, RMSE is also considered a complementary metric. However, RMSE has its ownlimitation: if a baseline is directly trained on the ground truth event times as point estimation, the stochasticcomponents of TPPs are ignored. Therefore, we train our TPP models on NLL and use RMSE at test timeto ensure that we do not rely too heavily on RMSE scores and account for the stochastic nature of TPPs.When class labels for events are available, we also report the accuracy of class predictions. ImplementationFor TPP models to predict the asynchronous event times, we employ Intensity-free mod-els (Shchur et al., 2020) based on GRU (Chung et al., 2014), and Transformer Hawkes Processes (THP) (Zuoet al., 2020) based on Transformer (Vaswani et al., 2017). THP predicts intensities to compute log-likelihoodand expected event times, but this approach can be computationally expensive due to the need to computeintegrals, particularly double integrals to calculate the expected event times. To overcome this challenge",
  "while maintaining performance, we follow Bae et al. (2023) in using a mixture of log-normal distributions,proposed in Shchur et al. (2020), for the decoder; we call this THP+": "For each dataset, we conduct hyper-parameter tuning for learning rate and the weight for L2 regularizationwith the unregularized baseline (we still apply early stopping and L2 regularization by default).Oncelearning rate and weight decay parameters are fixed, we search for the optimal flood levels. The optimalflood levels are selected via a grid search on {50, 45, 40 . . . , 0, 5}{4, 3 . . . , 3, 4} for Flood and iFlood,and optimal on {0.0, 0.1 . . . , 0.9} for AdaFlood using the validation set. We use five auxiliary models. ResultsIn order to evaluate the effectiveness of various regularization methods, we present the results ofour experiments in (showing means and standard errors from three runs). This is the first time weknow of where flooding methods have been applied in this domain; we see that all flooding methods improvethe generalization performance here, sometimes substantially. Furthermore, AdaFlood often outperformsother flooding methods on various datasets, suggesting that instance-wise flooding level adaptation usingauxiliary models can effectively enhance the generalization capabilities of TPP models. However, there areinstances where AdaFloods performance is comparable to or slightly worse than other methods, indicatingthat its effectiveness may vary depending on the specific context. Despite this variability, AdaFlood generallyappears to be the best choice for training TPP models.",
  "Results on Image Classification": "DatasetsWe use SVHN (Netzer et al., 2011), CIFAR-10, and 100 (Krizhevsky et al., 2009) for imageclassification with random crop and horizontal flip as augmentation. Unlike Xie et al. (2022), we split eachtraining dataset into train (80%) and validation (20%) sets for hyperparameter search; thus our numbers aregenerally somewhat worse than what they reported, as we do not directly tune on the test set. ImplementationFollowing Ishida et al. (2020) and similar to Xie et al. (2022), we consider trainingResNet18 (He et al., 2016) with and without L2 regularization (with a weight of 104). All methods aretrained with SGD for 300 epochs, with early stopping. We use a multi-step learning rate scheduler with aninitial learning rate of 0.1 and decay coefficient of 0.2, applied at every 60 epochs. The optimal flood levelsare selected based on validation performance with a grid search on {0.01, 0.02 . . . , 0.1, 0.15, 0.2 . . . , 1.0} forFlood and iFlood, and {0.05, 0.1 . . . , 0.95} for AdaFlood. We use a single ResNet18 auxiliary network whereits layer 3 and 4 are randomly initialized and fine-tuned on held-out sets with n = 10 splits.",
  "Noisy Labels": "DatasetsIn addition to CIFAR10 for image classification, we also use the tabular datasets BrazilianHouses and Wine Quality from OpenML (Vanschoren et al., 2013), following Grinsztajn et al. (2022), forregression tasks. We further employ Stanford Sentiment Treebank (SST-2) for the text classification task,following Xie et al. (2022). Details of datasets are provided in Appendix A.",
  "We report accuracy for classification tasks. For regression tasks, we report mean squared error (MSE) in themain body, as well as mean absolute error (MAE) and R2 score in (Appendix C)": "ImplementationWe inject noise for both image and text classification by changing the label to a uniformlyrandomly selected wrong class, following Xie et al. (2022). More specifically, for % of the training data,we change the label to a uniformly random class other than the original label. For the regression tasks, weadd errors sampled from a skewed normal distribution, with skewness parameter ranging from 0.0 to 3.0.Similar to the previous experiments, we tune learning rate and the weight for L2 regularization with theunregularized baseline (with early stopping and L2 regularization by default except for c). Then,we tune the flood levels with the fixed learning rate and L2 regularization. Results compares the flooding methods for noisy settings. We report the mean and standarderror over three runs for CIFAR10, and five and seven runs for tabular datasets and SST-2, respectively.We provide Acc (%) for CIFAR10 and SST-2 compared to the unregularized model: that is, we plot theaccuracy of each method minus the accuracy of the unregularized method, to display the gaps betweenmethods more clearly. The mean accuracies of the unregularized method are displayed below the zero line.",
  "Calibration": "Datasets and implementationMiscalibrationneural networks being over or under-confidenthasbeen a well-known issue in deep learning. We thus evaluate the quality of calibration with different floodingmethods on CIFAR100, as measured by the Expected Calibration Error (ECE) metric. ( does thesame for CIFAR10, but since model predictions are usually quite confident, this becomes difficult to mea-sure.) We use a ResNet18 with L2 regularization with the optimal hyperparameters for the baseline andflooding methods. The optimal hyperparameter varies by seed for each run. Result provides the calibration quality in ECE metric as well as a visualization over three runs,compared to perfect calibration (dotted red lines). We can observe that AdaFlood significantly improves thecalibration, both in ECE and visually. Note that iFlood significantly miscalibrates at the bins correspondingto high probability e.g. bin 0.7, compared to the other methods, and also has high standard errors. Thisbehavior is expected, since iFlood encourages the model not to predict higher than a probability of exp(b),where b denotes the flood level used in iFlood.",
  "Ablation study: Relationship with Other Regularization": "In this ablation study, we design an experiment that shows how different regularization methods interact withflooding methods. We conduct the experiment on CIFAR100 with ResNet18, gradually adding regularizationmethods in the order of early stopping, L2 regularization, dropout and CutMix (Yun et al., 2019), a populardata augmentation method, as shown in . Please note that the second row with early stopping andthe third row with early stopping + L2 regularization are the same as what we report in . Similar to the results in , Flood is comparable to or slightly worse than the unregularized baselinefor the case with dropout and with both dropout and CutMix. Although iFlood is generally better than the",
  ": Comparison of aux. training": "compares training of ten ResNet18 auxiliary net-works (original proposal) to the single fine-tuned auxiliarynetwork (efficient variant) in terms of wall-clock time fortraining the auxiliary network(s), and performance of thecorresponding main model, on the test set of CIFAR10.For the efficient variant, we fine-tune different layers toshow insensitivity to the choice of layers: Layer3, 4 + FC,Layer3+FC, and FC, where Layer3 and 4 are the 3rd, 4thlayers in ResNet18 and FC denotes the last fully connectedlayer. For example, Layer4 + FC means we only fine-tuneLayer4 and FC layers, freezing all the previous layers. Results show that training multiple auxiliary net-works yields the same-quality model as fine-tuning, though training time is 3 to 4 times longer. There is alsolittle difference in performance between different fine-tuning methods: it seems that fine-tuning only the FClayer is sufficient to forget the samples, with early-stopping regularizing well enough for similar generalizationability. We also compare AdaFlood with various architectures for auxiliary networks in Appendix F.",
  "Conclusion": "In this paper, we introduced the Adaptive Flooding (AdaFlood) regularizer, a novel reguralization techniquethat adaptively regularizes a loss for each sample based on the difficulty of the sample. Each flood levelis computed only once through an auxiliary training procedure with held-out splitting, which we can makemore efficient by fine-tuning the last few layers on held-out sets. Experimental results on various domainsand tasks: density estimation for asynchronous event sequences, image and text classification tasks as wellas regression tasks on tabular datasets, with and without noise, demonstrated that our approach is morerobustly applicable to a varied range of tasks including calibration. LimitationAlthough AdaFlood is a robust and effective regularizer on many different tasks, particularlyin high-noise settings, an open question that we leave for future work is how to best apply AdaFlood inlong-tailed learning. For long-tailed data, it is expected that samples from the rare classes will tend to havehigher losses. During the training of the main model, AdaFlood will direct the model to keep the higherlosses for rare classes and lower losses for common classes, which may not be desirable. One potential solutioncould be to adaptively adjust for different classes. Alternatively, imbalanced learning techniques such asresampling, reweighting, or two-stage training could be adopted. ReproducibilityFor each experiment, we listed implementation details such as model, regularization, andsearch space for hyperparameters. We also specified datasets we used for each experiment, and how theywere split and augmented, along with the description of metrics. The code is released with the final version.",
  "Mohamad Amin Mohamadi, Wonho Bae, and Danica J Sutherland. A fast, well-founded approximation tothe empirical neural tangent kernel. In ICML, 2023": "Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya Sutskever.Deepdouble descent: Where bigger models and more data hurt. Journal of Statistical Mechanics: Theory andExperiment, 2021(12):124003, 2021. Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading digits innatural images with unsupervised feature learning. In NIPS Workshop on Deep Learning and UnsupervisedFeature Learning, 2011.",
  "ADetails about Datasets": "Stack OverflowIt contains 6,633 sequences with 480,414 events where an event is the acquisition ofbadges received by users. The maximum number of sequence length is 736 and the number of marks is 22.The dataset is provided by Du et al. (2016); we use the first folder, following Shchur et al. (2020) and Baeet al. (2023). RedditIt contains 10,000 sequences with 532,026 events where an event is posting in Reddit. The maxi-mum number of sequence length is 736 and the number of marks is 22. Marks represent sub-reddit categories. UberIt contains 791 sequences with 701,579 events where an event is pick-up of customers. The maximumnumber of sequence length is 2,977 and there is no marks. It is processed and provided by Bae et al. (2023). Brazilian HousesIt contains information of 10,962 houses to rent in Brazil in 2020 with 13 features. Thetarget is the rent price for each house in Brazilian Real. According to OpenML (Vanschoren et al., 2013)where we obtained this dataset, since the data is web-scrapped, there are some values in the dataset thatcan be considered outliers.",
  "DatasetsWe use ImageNet100 (Tian et al., 2020) for image classification with random crop, horizontalflip, and color jitter as augmentation. We also add 30% of label noise as done in .3": "ImplementationWe train ResNet34 (He et al., 2016) on the dataset with L2 regularization (with aweight of 0.0001). All methods are trained for 200 epochs with early stopping using SGD. We use a multi-step learning rate scheduler with an initial learning rate of 0.1 and decay coefficient of 0.5, applied at every25 epochs. The optimal flood levels are selected based on validation performance with a grid search on{0.01, 0.02..., 0.1, 0.15, 0.2..., 0.3} for Flood and iFlood, and {0.05, 0.1..., 0.95} for AdaFlood. We use a singleResNet34 auxiliary network where its last FC layer is randomly initialized and fine-tuned on held-out setswith n = 10 splits. Results (Left) compares flooding methods on ImageNet100 dataset with and without 30% oflabel noise. We report test accuracies along with expected calibration error (ECE) on the right. AlthoughFlood and iFlood do not improve the performance over the unregularized model, AdaFlood improves theperformance by about 0.80% over the unregularized baseline. Given the size of the dataset, the gap is notmarginal. This gap is even larger than that we observed in SVHN and CIFAR datasets . We conjectureit is because ImageNet contains more noisy samples. It is well-known that there are many ImageNet imagescontaining multiple objects although the label says there is only one object (Beyer et al., 2020).",
  "CAdditional Results on Tabular Regression": "DatasetsWe use NYC Taxi Tip dataset from OpenML (Vanschoren et al., 2013), one of the largest tabulardataset used in Grinsztajn et al. (2022), for regression tasks. NYC Taxi Tip dataset contains 581, 835 rowsand 9 features. As the name of the dataset implies the target variable is tip amount\". To increase theimportance of other features, the creator of the dataset deliberately ignores fare amount\" or trip distance\".",
  ": Additional results in various metrics on tabular datasets with noise and bias": "ImplementationAs with .3, we use a model tailored for tabular dataset proposed by (Grinsztajnet al., 2022) and add errors sampled from a skewed normal distribution, with skewness parameter rangingfrom 0.0 to 3.0. Results (Right) compares flooding methods on NYC Taxi Tip dataset (Grinsztajn et al., 2022)with and without noises. We report mean square error (MSE) and R2 score on the right. Note that R2 scoreis usually in between 0 and 1 but when predictions are bad, it can go below 0. From the table, we can observe that all flooding methods perform similar to the unregularized baselinewhen there is no noise. Although it continues for Flood and iFlood even under noisy settings, AdaFloodsignificantly outperforms (lower MSE and higher R2 scores) the other methods when noise level is 1.5 and3.0. In particular, while R2 scores of other methods go below 0, it does not happen with AdaFlood, whichdemonstrates the robustness of AdaFlood even for the large-scale dataset like NYC Taxi Tip. It is consistentwith the results we provided in .3.",
  "EInitialization of the Main Model using an Auxiliary Network": "Even though we efficiently fine-tune an auxiliary network to compute flood levels, it may be still too expensiveto train both the auxiliary network f aux and main model f. To reduce computation further, we may utilize apre-trained auxiliary network when we use the same architecture for the auxiliary and main model. Instead ofrandomly initialize the main model, we can initialize the parameters of the main model using the parametersfrom the pre-trained auxiliary network (before fine-tuning).",
  ": Comparison between random initialization and initialization with the pre-trained auxiliary network.ECE metrics are reported on the right": "We conduct an experiment to validate if it actually saves computation without hurting performance onCIFAR10 with ResNet18. We first trained an auxiliary network on the whole training set. The pre-trainedauxiliary network is then used for both computing through the fine-tuning step described in .3and initializing the main model. When we fine-tune the main model, we freeze the first k layers (amongfour layers of ResNet18) to save computation, and randomly initialize the last fully connected layer to havethe model forget some information. shows that if we initialize the parameters of the main model with those of the pre-trained auxiliarymodel without freezing any layers, it performs better than random initialization. As we freeze more layersto save computation, the performance gradually goes down compared to that of random initialization but itis still comparable up to k = 2 case.",
  ": Comparison of various architectures for auxiliary networks in AdaFlood": "To investigate the robustness of AdaFlood in terms of the choice of architectures for auxiliary networks, weconduct an ablation study on AdaFlood with various architectures for auxiliary networks: VGG11, VGG19,ResNet18 small and ResNet18. Here, we use a ResNet18 for the main model, and utilize the efficient fine-tuning method to train auxiliary networks. We report the mean and standard error of three runs in . With VGG11 where its number of parameters is slightly less than ResNet18, the mean test accuracy of themain model is lower but the gap is marginal. With VGG19 which is larger than ResNet18, the performanceslightly improves. We also try with a smaller variant of ResNet18 for the auxiliary network where its numberof parameters is a quarter of the original ResNet18. Even with this significantly smaller architecture, themean test accuracy of the main model is only slightly worse than much larger models e.g. VGG11 andResNet18, which implies that what is important from the auxiliary network is the relative magnitude oflosses (or flood levels) not the absolute values of losses.",
  "j=1jeNTK(xi, xj)(11)": "where eNTK stands for empirical neural tangent kernel (NTK) following (Mohamadi et al., 2023) and{xj}nj=1 denotes data from a training set. Equation (11) says we can approximate a prediction on xi as aninterpolation of eNTK(xi, ) with some weights . Suppose f(x) = V (x) where (x) Rh denotes a feature from the penultimate layer and V Rkh denotesthe weights of the last fully connected layer (k being the number of classes), consisting of vj Rh for j-throw. If vj,i, i-th entry of vj, is from N(0, 2), then Mohamadi et al. (2023) haven shown that,",
  "HChoice of Flood Levels": "In and , we report the choice of flood levels for Flood and iFlood, and for AdaFlood(recall that is the hyperparameter for correct function adjusting interpolation level) on TPP and imageclassification tasks with three different random seeds. One interesting observation is that flood levels for iFlood on CIFAR100 is significantly larger than on theother datasets. It is because CIFAR100 is particularly harder than the other two datasets. However, eventhough a flood level is sometimes high e.g. 0.65, it is still a reasonable choice because 0.65 flood level impliesthat the models highest predicted probabilities do not deviate much from 0.52 exp(0.65). With thishigh regularization, the model is not too overconfident for its predictions."
}