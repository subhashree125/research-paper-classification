{
  "Abstract": "The ability to detect out-of-distribution (OOD) inputs is critical to guarantee the reliabilityof classification models deployed in an open environment. A fundamental challenge in OODdetection is that a discriminative classifier is typically trained to estimate the posteriorprobability p(y|z) for class y given an input z, but lacks the explicit likelihood estimationof p(z) ideally needed for OOD detection. While numerous OOD scoring functions havebeen proposed for classification models, these estimate scores are often heuristic-driven andcannot be rigorously interpreted as likelihood. To bridge the gap, we propose IntrinsicLikelihood (INK), which offers rigorous likelihood interpretation to modern discriminative-based classifiers. Specifically, our proposed INK score operates on the constrained latentembeddings of a discriminative classifier, which are modeled as a mixture of hypersphericalembeddings with constant norm. We draw a novel connection between the hypersphericaldistribution and the intrinsic likelihood, which can be effectively optimized in modern neuralnetworks. Extensive experiments on the OpenOOD benchmark empirically demonstrate thatINK establishes a new state-of-the-art in a variety of OOD detection setups, including bothfar-OOD and near-OOD.",
  "Introduction": "The problem of out-of-distribution (OOD) detection has garnered significant attention in reliable machinelearning. OOD detection refers to the task of identifying test samples that do not belong to the samedistribution as the training data, which can occur due to various factors such as the emergence of unseenclasses in the models operating environment. The ability to detect OOD samples is crucial for ensuringthe reliability and safety of machine learning models (Amodei et al., 2016), especially in applications wherethe consequences of misidentifying OOD samples can be severe. To facilitate the separation between in-distribution (ID) and OOD data, a rich line of works (Yang et al., 2021b) have developed OOD indicatorfunctions, which, at the core, compute a scalar score for each input signifying the degree of OOD-ness. Thedesign of these scoring functions has emerged as a critical cornerstone in achieving effective OOD detection. By definition, OOD data inherently diverges from ID data by means of their data density distributions,rendering likelihood or log-likelihood an ideal scoring function for detection. However, a fundamental dilemmain OOD detection is that a discriminative classifier is typically trained to estimate the posterior probabilityp(y|z) for class y given an input z, but lacks the explicit likelihood estimation of p(z) needed for OODdetection. While many OOD scoring functions have been proposed for classification models, they are eitherheuristic-driven and cannot be rigorously interpreted as log-likelihood, or impose strong assumption on thedensity function that can fail to hold in test time (Lee et al., 2018b). Against this backdrop, the key researchquestion of this paper thus centers on:",
  "RQ: How to design an OOD scoring function that can offer rigorous likelihood interpretation to moderndiscriminative-based classifiers without resorting to separate generative models?": "To bridge the gap, we introduce a novel framework INtrinsic liKelihood (INK) for classifier-based OODdetection, which can be theoretically reasoned from a log-likelihood perspective. The key insight is thatwe need to incorporate explicit probabilistic modeling of the data distribution p(z) directly into optimizingthe posterior p(y|z), which is effectively achieved by our framework. Specifically, our proposed INK scoreoperates on the constrained latent embeddings of a discriminative classifier, which are modeled as a mixtureof hyperspherical embeddings with constant norm (see ). Under our probabilistic model, each classhas a mean vector in the hypersphere, where the class-conditional density function decays from the centerexponentially by the similarity between an embedding to the center. The proposed INK score is formalizedby drawing a novel connection between the hyperspherical distribution and the density function under ourprobabilistic model. Theoretically, we show that the INK score is equivalent to the log-likelihood score log p(z)(up to some constant difference), and thus can act rigorously as a density-based score for OOD detection.Though simple and elegant in hindsight, establishing this connection was non-trivial. We lay out the fulltheoretical derivation in .1, along with how to effectively optimize for the intrinsic likelihood in thecontext of discriminative neural networks in .2. We show that our theoretical guarantee indeed translates into competitive empirical performance ().We extensively evaluate our method on the latest OpenOOD benchmarks (Zhang et al., 2023a), containingCIFAR and ImageNet-1k as ID datasets.Our method exhibits significant performance improvementswhen compared to the state-of-the-art method ASH (Djurisic et al., 2023), which employs a heuristic-drivenactivation shaping approach and lacks a rigorous likelihood interpretation. Moreover, we show that our methodis effective under both far-OOD and near-OOD scenarios and generalizes to different model architectures,including ViT (Dosovitskiy et al., 2021). The overall evaluation demonstrates notable strengths of ourapproach in both empirical performance and theoretical underpinnings.",
  "Preliminaries": "Let X and Y = {1, . . . , C} represent the input space and ID label space, respectively. The joint ID distribution,represented as PXIYI, is a joint distribution defined over X Y. During testing time, there are some unknownOOD joint distributions DXOYO defined over X Yc, where Yc is the complementary set of Y. We also denotep(x) as the density of the ID marginal distribution PXI. According to Fang et al. (2022), OOD detection canbe formally defined as follows:",
  "Published in Transactions on Machine Learning Research (12/2024)": "ViM generates an additional logit from the residual of the feature against the principal space. Weset the dimension of principal space to D = 256 for ResNet-18 and ResNet-34 and D = 1024 forResNet-50. ReAct improves the energy score by rectifying activations at an upper limit, which is set based onthe p-th percentile of the activations estimated on the in-distribution (ID) data. We set p to thedefault value of p = 90. DICE utilizes logit sparsification to enhance the vanilla energy score, which is set based on the p-thpercentile of the unit contributions estimated on the ID data. We set the sparsity parameter p = 0.7. SHE stores the mean direction of the penultimate layer features from correctly classified trainingsamples. During inference, the Hopfield energy score is calculated as the dot product between thesample embedding and the class mean of the predicted class. KNN+ and CIDER utilize the KNN score, which requires selecting a number of nearest neighborsk. Following the settings in Ming et al. (2023), we set k to 300 and 1,000 for CIFAR-100 and theImageNet benchmark, respectively. ASH proposes three heuristic-based approaches for activation shaping: Pruning, Binarizing, andScaling. In line with the OpenOOD framework, we adopt the Binarizing method (ASH-B), specificallywith a parameter setting of p = 90.",
  "g (x) = ID, if p(x) ; otherwise, g (x) = OOD.(1)": "The performance of OOD detection thus heavily relies on the estimation of data density p(x). However, insupervised learning, the classifiers directly estimate the posterior probability p(y|x) instead of the likelihoodp(x), which makes likelihood-based OOD detection non-trivial. Our focus on discriminative models differsfrom unsupervised OOD detection using generative-based models, which do not offer classification capability.For completeness, we review these studies extensively in the related work ().",
  "Methodology": "In this section, we introduce our proposed framework, INtrinsic liKelihood (INK), for OOD detection. Wefirst provide an overview of the key challenge and motivation. Then, we introduce the notion of intrinsiclikelihood and theoretically justify its feasibility as an indicator function of OOD in .1. Finally, in.2, we discuss how to optimize neural networks to achieve the desired intrinsic likelihood for OODdetection.",
  "p(z)=p(z | y)Cj=1 p(z | y = j).(3)": "Thus, one can define S(z) = log Cj=1 p(z | y = j) as a measure of how well a given embedding matches thedistribution of ID samples, which is proportional to the log-likelihood log p(z). Despite the connection, thechallenge lies in how to concretely define the class-conditional latent distribution p(z | y), as well as how toefficiently optimize neural networks to achieve such latent distribution while training a classifier. In whatfollows, we address these two challenges.",
  "Intrinsic Likelihood": "To address the challenge of defining class-conditional latent distributions, we model the representations bythe von Mises-Fisher (vMF) distribution in the hypersphere. A hypersphere is a topological space that isequivalent to a standard (d 1)-sphere, which represents the set of points in d-dimensional Euclidean spacethat are equidistant from the center (see ). The unit hypersphere is a special sphere with a unitradius and is denoted as Sd1 := {z Rd|z2 = 1} in the (d 1)-dimensional space. The formal definitionof vMF distribution is given below:",
  "where c Rd denotes the mean direction of the class c, 0 denotes the concentration of the distributionaround c, and Zd() denotes the normalization factor": "Benefits of the probabilistic model.We model the latent representations as vMF distribution becauseit is a simple and expressive probability distribution in directional statistics. Compared with multivariateGaussian distribution, vMF distribution avoids estimating large covariance matrices for high-dimensionaldata that is shown to be costly and unstable in literature (Chen et al., 2017). Meanwhile, choosing the vMFdistribution allows us to have the features live on the unit hypersphere, which leads to several benefits. Forexample, fixed-norm vectors are known to improve training stability in modern machine learning, wheredot products are ubiquitous and are beneficial for learning a good representation space. For this reason,hyperspherical representations have been popularly adopted in recent contrastive learning literature (Chenet al., 2020a; Khosla et al., 2020).",
  "Theorem 3.1 shows that our scoring function in Eq. 6 can be functionally equivalent to the log-likelihoodscore, for OOD detection purposes": "Theorem 3.1. Under uniform class prior, the intrinsic likelihood score S(z) is a logarithmic function of thedensity p(z), with a constant difference. The two measurements return the same level set for OOD detection. Proof. The likelihood p(z) is the probability of observing a given embedding z under the vMF distribution.p(z) can be calculated as a summation of class-conditional likelihood p(z | y = j), weighted by the class prior:",
  "where samples with higher scores are classified as ID, and vice versa. The threshold is chosen based on theID score at a certain percentile (e.g., 95%)": "Generalization to class-imbalanced datasets.In Theorem 3.1, we adopt the commonly assumedscenario prevalent in OOD detection literature, where datasets are class-balanced with a uniform priordistribution over classes, i.e., p(y = c) = 1/C. However, we acknowledge that this assumption may not alwayshold in real-world scenarios. To further demonstrate the generality and effectiveness of our method, we extend it to account for non-uniformclass priors. We perform standard maximum likelihood estimation on the training dataset {(xi, yi)}Ni=1:",
  "So far, we have established that intrinsic likelihood derived under the vMF distribution is desirable for OODdetection. Now, we discuss how to optimize neural networks for intrinsic likelihood": "Maximum likelihood estimation. We aim to train the neural network, such that the hypersphericalembedding conforms to a mixture of vMF distributions defined in Eq. 5. Specifically, we consider a deepneural network h : X Rd which encodes an input x X to a normalized embedding z := h(x). To optimizefor the vMF distribution, one can directly perform standard maximum likelihood estimation on the trainingdataset {(xi, yi)}Ni=1:",
  "i=1p(y = yi | zi; {, j}Cj=1),(13)": "where i is the index of the sample, j is the index of class, and N is the size of the training set. In effect, thisloss encourages each ID sample to have a high probability assigned to the correct class in the mixtures ofthe vMF distributions. While hyperspherical learning algorithms have been studied (Mettes et al., 2019; Duet al., 2022a; Ming et al., 2023), none of the works explored its principled connection to intrinsic likelihoodfor test-time OOD detection, which is our distinct focus. We discuss the differences further in .3.Moreover, we show new insights below that minimizing this loss function effectively shapes the intrinsiclikelihood to aid OOD detection.",
  ",(15)": "where s(zi, y) = y zi. During optimization, the loss reduces s(zi, y) for the correct class while increasingit for other classes. This can be justified by analyzing the gradient of the loss function with respect to themodel parameters , for an embedding z and its associated class label y:",
  "(17)": "The sample-wise intrinsic likelihood S(z) of ID data is S(z) = log Cc=1 exp(s(z, c)/), which is dominatedby the term s(z, y) with ground truth label. Hence, the training overall induces a higher intrinsic likelihoodfor ID data. By further connecting to our Theorem 3.1, this higher intrinsic likelihood can directly translateinto high () log p(z), for training data distribution.",
  "Differences w.r.t. Existing Approaches": "summarizes the key distinctions among prevalent hyperspherical methodologies, specifically in termsof their training-time loss functions and test-time scoring functions. Both SSD+ (Sehwag et al., 2021) andKNN+ (Sun et al., 2022b) employ the SupCon (Khosla et al., 2020) loss during training, which does notexplicitly model the latent representations as vMF distributions. Instead of promoting instance-to-prototypesimilarity, SupCon promotes instance-to-instance similarity among positive pairs. SupCons loss formulationthus does not directly correspond to the vMF distribution. Geometrically speaking, our framework directlyoperates on the vMF distribution, a key property that enables log-likelihood interpretation. Our approach also differs significantly from CIDER (Ming et al., 2023) and SIREN (Du et al., 2022a) in termsof OOD scoring function, which employ either KNN distance or maximum conditional probability max p(y|z).In contrast, our method establishes the novel connection between the hyperspherical representations andintrinsic likelihood for OOD detection, which enjoys rigorous theoretical interpretation from a densityperspective. We show empirically in that our proposed OOD score achieves competitive performanceon different OOD detection benchmarks, and is much more computationally efficient by eliminating the needfor time-consuming KNN searches.",
  "Setup": "Benchmarks. Our evaluation encompasses both far-OOD and near-OOD detection performance, followingthe same setup as the latest OpenOOD benchmark1 (Zhang et al., 2023a).The benchmark includesCIFAR (Krizhevsky, 2009) and ImageNet-1k (Deng et al., 2009) as ID datasets. Due to space constraints, weprimarily focus on the most challenging ImageNet benchmark in the main paper, and report the full resultsfor CIFAR datasets in the Appendix E. For the ImageNet benchmark, iNaturalist (Van Horn et al., 2018),Textures (Cimpoi et al., 2014), and OpenImage-O (Wang et al., 2022) are employed as far-OOD datasets. Inaddition, we also evaluate on the latest NINCO (Bitterwolf et al., 2023) as the near-OOD dataset, whichcircumvents the issue of contaminated classes found in other near-OOD datasets (Vaze et al., 2022). Weadhere to the same data split used in OpenOOD (Zhang et al., 2023a), which involves the removal of imageswith semantic overlap. Evaluation metrics and implementation details.We utilize the following metrics, which are widelyused to assess the efficacy of OOD detection methods: (1) false positive rate (FPR@95) of OOD sampleswhen the true positive rate of ID samples is set at 95%, (2) area under the receiver operating characteristiccurve (AUROC), and (3) ID classification accuracy (ID ACC). Due to the space limit, we provide theimplementation details in the Appendix B and ID accuracy in Appendix D. For INK score, we use test-timetemperature test = 0.05 based on validation (see Appendix C). We further show the effect of the temperaturein our ablation study (.3).",
  "Main Results": "Intrinsic likelihood score establishes state-of-the-art performance.We extensively compare theperformance of our approach with state-of-the-art methods. The compared methods are categorized as eitherhyperspherical-based or non-hyperspherical-based. All the non-hyperspherical-based methods, includingMSP (Hendrycks & Gimpel, 2017), ODIN (Liang et al., 2018), Mahalanobis (Lee et al., 2018b), Energy (Liuet al., 2020), ViM (Wang et al., 2022), ReAct (Sun et al., 2021), DICE (Sun & Li, 2022), SHE (Zhanget al., 2023b), and ASH (Djurisic et al., 2023) use the softmax cross-entropy (CE) loss for model training.Hyperspherical-based methods, on the other hand, include SSD+ (Sehwag et al., 2021), KNN+ (Sun et al.,2022b), and CIDER (Ming et al., 2023).It is worth noting that we have adopted the recommendedconfigurations proposed by prior works. Further details of each baseline method are included in Appendix B. As shown in , our method INK displays competitive OOD detection performance compared to existingones on the far-OOD group of the ImageNet benchmark. Here we highlight two observations: (1) INK alsooutperforms the current state-of-the-art cross-entropy loss method ASH (Djurisic et al., 2023), which employsa heuristic-driven activation shaping approach and lacks a rigorous likelihood interpretation. In contrast,INK is derived from a more principled likelihood-based perspective while being empirically strong. (2) INKscore outperforms the current state-of-the-art hyperspherical-based methods CIDER by 8.97% and KNN+ by6.60% in FPR@95. Compared to CIDER, our method enjoys a significant performance boost by leveragingthe likelihood interpretation within hyperspherical space, and a drastic reduction in computational cost (morein .3).",
  "KNN+ or CIDER3.5619 0.26465.4886 0.18543.6157 0.1186336.1780 4.3871INK (ours)0.0076 0.00030.0157 0.00070.2745 0.01450.2745 0.0145": "Near-OOD detection.We also evaluate the performance of our method in the more challenging near-OODscenario. As shown in , INK demonstrates a competitive performance, outperforming CIDER by9.27% in FPR@95. This improvement underscores the versatility of the INK score, proving its effectivenessin near-OOD detection scenarios where out-of-distribution samples bear a close semantic resemblance to theID samples. Moreover, our method shows an enhanced performance over ASH in near-OOD detection, withan improvement of 4.84% in FPR@95. This advancement also signifies progress in bridging the performancegap between hyperspherical-based methods and cross-entropy loss methods. CIFAR benchmark.We evaluate the performance of our method on the CIFAR benchmark. Consistentwith the results observed in the ImageNet benchmark, the INK score displays competitive performance inhandling both far-OOD and near-OOD groups, compared to the current state-of-the-art methods. Due tospace constraints, further details are included in Appendix E.",
  "Additional Abalations and Discussions": "Computational cost.The current state-of-the-art hyperspherical representation methods, such as KNN+and CIDER, rely on a non-parametric KNN score, which requires a nearest neighbor search in the embeddingspace. Although recent studies on approximated nearest neighbor search have shown promise (Aumlleret al., 2020), the computation required grows with the size of the embedding pool N. In contrast, INK onlyincurs complexity O(C), where the number of classes C N is independent of ID sample size. Thus, ourmethod can significantly reduce the computation overhead. To demonstrate this, we conduct an experimentto compare the average compute time between the KNN score and the INK score. In this experiment, weassess the performance of the model on CIFAR-10, CIFAR-100, and ImageNet-1k benchmarks using defaultvalues of k in Sun et al. (2022b). In particular, we measure the time taken to calculate the score per sample,excluding the time to extract the sample embedding from the neural network. KNN-based methods utilizethe Faiss library (Johnson et al., 2019), which is optimized for efficient nearest-neighbor search. The size ofthe embedding pool is 50,000 for both CIFAR-10 and CIFAR-100, 12K for ImageNet ( = 1%), and 1.2M for",
  "Method": "EnergyOurs Greedy (w=0)ARGS-greedy (k=10, w: Comparison of Energy vs. INK on Ima-geNet (ID) with ResNet-50, using OpenOOD bench-mark Zhang et al. (2023a). INK consistently outper-forms Energy on far-OOD and near-OOD datasets. Relationship with energy score.We discussthe relationship of INK with prior work by Liu etal. Liu et al. (2020), which attempted to interpretthe energy score derived from a multi-class classifierfrom a likelihood-based view. The key distinctionbetween the two methods lies in the geometry of theembedding space, from which the score is derived.Specifically, an energy score is defined as the negativelog of the denominator in the softmax function:",
  "j=1exp(f j(x)/),": "where f j(x) denotes the logit corresponding to theclass j.Unfortunately, the energy score derivedfrom the Euclidean space logits f(x) can be un-constrained, and does not necessarily reflect the log-likelihood (see Theorem A.1 and proof in Appendix).In contrast, our framework explicitly models the la-tent representations in the discriminative classifieras constrained hyperspherical space, which enables a rigorous interpretation from a log-likelihood perspective(c.f. Theorem 3.1). In our formulation (Eq. 5), the classifiers logit f j(x) is equivalent to j z, where z isthe penultimate layer embedding of input x and j can be interpreted as the weight vector connecting theembedding and output logit. Different from Liu et al. (2020), the embedding z and the weight connection jare explicitly constrained together to form a meaningful probabilistic distribution during our optimization. 0.00.20.40.60.81.0 Test temperature Average FPR@95 (%) 0.00 85.5 86.0 86.5 87.0 87.5 88.0 88.5 Average AUROC (%)",
  ": Ablation on test-time temperatures. Theresults are averaged across the far-OOD test sets and 3random training runs, based on ResNet-34 trained onCIFAR-100": "illustrates a comparison of the performanceon the ImageNet benchmark. Our method reducesthe average FPR@95 by 23.12%, which validatesthat the proposed intrinsic likelihood score is sig-nificantly more effective for OOD detection. Theperformance improvement can also be reasoned the-oretically, as we show in Theorem 3.1. Overall, ourmethod enjoys both strong empirical performanceand theoretical justification. Additionally, for a moredetailed understanding, we have included a visualiza-tion of ID-OOD score distributions in . Weillustrate a comparison between the distributions ofenergy and INK scores for ImageNet (ID) vs. Tex-tures (OOD). The noticeable separability of usingINK score enhances the performance in OOD detec-tion. Ablation study on test-time temperature .We conduct an ablation study on the test-time temperature parameter to investigate its impact on OODdetection performance, as shown in . The curve highlights that the optimal value of roughlymatches the training-time temperature. This also matches our theory, where INK under the same temperature in testing allows recovering the log-likelihood log p(z) (up to some constant) learned in training time.",
  ": Density plots illustrating the distribution of energy score (left) and INK (right). Lighterand darker blue shades represent the distributions of scores for the ID and OOD datasets, respectively": "shown in in Appendix, our INK score maintains competitive performance with the ViT model. Thisunderscores the adaptability of our method across various model architectures. Further details are includedin Appendix F. Ablation study on class-imbalance scenarios.We further verify this class-imbalanced setting empiricallyby eliminating half of the training examples from the first 5 classes in CIFAR-10 and the first 10 classes inCIFAR-100. compares the FPR@95 between ASH and the generalized intrinsic likelihood, which showsthat our method significantly outperforms the state-of-the-art in both far-OOD and near-OOD detectiontasks.",
  "Conclusion": "In this paper, we introduce intrinsic likelihood for detecting out-of-distribution samples. Our approachis based on density estimation in the hyperspherical space, which enjoys a sound interpretation from alog-likelihood perspective. Through extensive experiments on common OOD detection benchmarks, wedemonstrate the state-of-the-art performance of our approach for both far-OOD and near-OOD detection.Moreover, our method outperforms KNN-based approaches in terms of computational efficiency. Overall,our proposed intrinsic likelihood score provides a promising solution for both effective and efficient OODdetection.",
  "Broader Impacts": "OOD detection can be used to improve the reliability of machine learning models, which can have a significantpositive impact on society. In situations where saving features is a concern, the use of the KNN score maynot be feasible as it requires access to a certain amount of labeled data. In contrast, our INK score does notrequire access to a pool of ID data. This makes it a valuable tool in various applications and industries wheresensitive information needs to be protected. It is important to note that the usefulness and effectiveness ofOOD detection may vary across different applications and domains, and careful evaluation and validation arenecessary before its deployment. We gratefully acknowledge the support from the AFOSR Young Investigator Program under award numberFA9550-23-1-0184, National Science Foundation (NSF) Award No. IIS-2237037 & IIS-2331669, Office ofNaval Research under grant number N00014-23-1-2643, Philanthropic Fund from SFF, and faculty researchawards/gifts from Google and Meta. Davide Abati, Angelo Porrello, Simone Calderara, and Rita Cucchiara. Latent Space Autoregression forNovelty Detection. In Proceedings of the IEEE/CVF International Conference on Computer Vision andPattern Recognition, 2019.",
  "Mu Cai and Yixuan Li. Out-of-distribution detection via frequency-regularized generative models. InProceedings of IEEE/CVF Winter Conference on Applications of Computer Vision, 2023": "Jinggang Chen, Junjie Li, Xiaoyang Qu, Jianzong Wang, Jiguang Wan, and Jing Xiao. GAIA: Delving intogradient-based attribution abnormality for out-of-distribution detection. In Thirty-seventh Conference onNeural Information Processing Systems, 2023. Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastivelearning of visual representations. In International conference on machine learning, pp. 15971607. PMLR,2020a.",
  "Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi. Describingtextures in the wild. In IEEE Conference on Computer Vision and Pattern Recognition, 2014": "Pierre Colombo, Eduardo Dadalto Cmara Gomes, Guillaume Staerman, Nathan Noiry, and Pablo Piantanida.Beyond mahalanobis distance for textual OOD detection. In Advances in Neural Information ProcessingSystems, 2022. Jean-Rmy Conti, Nathan Noiry, Stephan Clemencon, Vincent Despiegel, and Stphane Gentric. Mitigatinggender bias in face recognition using the von mises-Fisher mixture model. In Proceedings of the 39thInternational Conference on Machine Learning, 2022.",
  "Li Deng. The mnist database of handwritten digit images for machine learning research [best of the web].Signal Processing Magazine, IEEE, 29:141142, 11 2012. doi: 10.1109/MSP.2012.2211477": "Andrija Djurisic, Nebojsa Bozanic, Arjun Ashok, and Rosanne Liu. Extremely simple activation shaping forout-of-distribution detection. In The Eleventh International Conference on Learning Representations, 2023. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby.An image is worth 16x16 words: Transformers for image recognition at scale. In International Conferenceon Learning Representations, 2021.",
  "Hariprasath Govindarajan, Per Sidn, Jacob Roll, and Fredrik Lindsten. DINO as a von mises-fisher mixturemodel. In The Eleventh International Conference on Learning Representations, 2023": "Md. Abul Hasnat, Julien Bohn, Jonathan Milgram, Stphane Gentric, and Liming Chen. von mises-fishermixture model-based deep learning: Application to face verification. arXiv preprint arXiv:1706.04264,2017. Matthias Hein, Maksym Andriushchenko, and Julian Bitterwolf. Why relu networks yield high-confidencepredictions far away from the training data and how to mitigate the problem. In 2019 IEEE/CVFConference on Computer Vision and Pattern Recognition (CVPR), 2019.",
  "Wenjian Huang, Hao Wang, Jiahao Xia, Chengyan Wang, and Jianguo Zhang. Density-driven regularizationfor out-of-distribution detection. In Advances in Neural Information Processing Systems, 2022": "Jyh-Jing Hwang, Stella X Yu, Jianbo Shi, Maxwell D Collins, Tien-Ju Yang, Xiao Zhang, and Liang-ChiehChen. Segsort: Segmentation by discriminative sorting of segments. In Proceedings of the IEEE InternationalConference on Computer Vision, 2019. Xue Jiang, Feng Liu, Zhen Fang, Hong Chen, Tongliang Liu, Feng Zheng, and Bo Han. Detecting out-of-distribution data through in-distribution class prior. In Proceedings of the 40th International Conferenceon Machine Learning, 2023.",
  "Pascal Mettes, Elise van der Pol, and Cees Snoek. Hyperspherical prototype networks. Advances in NeuralInformation Processing Systems, 32, 2019": "Yifei Ming, Ziyang Cai, Jiuxiang Gu, Yiyou Sun, Wei Li, and Yixuan Li. Delving into out-of-distributiondetection with vision-language representations. In Advances in Neural Information Processing Systems,2022. Yifei Ming, Yiyou Sun, Ousmane Dia, and Yixuan Li. How to exploit hyperspherical embeddings forout-of-distribution detection? In The Eleventh International Conference on Learning Representations,2023.",
  "Peyman Morteza and Yixuan Li.Provable guarantees for understanding out-of-distribution detection.Proceedings of the AAAI Conference on Artificial Intelligence, 36, 2022": "Eric Nalisnick, Akihiro Matsukawa, Yee Whye Teh, Dilan Gorur, and Balaji Lakshminarayanan. Do deepgenerative models know what they dont know? In International Conference on Learning Representations,2019. Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y. Ng. Reading digits innatural images with unsupervised feature learning. In NIPS Workshop on Deep Learning and UnsupervisedFeature Learning 2011, 2011. Bo Peng, Yadan Luo, Yonggang Zhang, Yixuan Li, and Zhen Fang. Conjnorm: Tractable density estimationfor out-of-distribution detection. In The Twelfth International Conference on Learning Representations,2024. Jie Ren, Peter J. Liu, Emily Fertig, Jasper Snoek, Ryan Poplin, Mark Depristo, Joshua Dillon, and BalajiLakshminarayanan. Likelihood ratios for out-of-distribution detection. In Advances in Neural InformationProcessing Systems, 2019.",
  "Jie Ren, Stanislav Fort, Jeremiah Liu, Abhijit Guha Roy, Shreyas Padhy, and Balaji Lakshminarayanan. Asimple fix to mahalanobis distance for improving near-ood detection, 2021": "Jie Ren, Jiaming Luo, Yao Zhao, Kundan Krishna, Mohammad Saleh, Balaji Lakshminarayanan, and Peter JLiu. Out-of-distribution detection and selective generation for conditional language models. In The EleventhInternational Conference on Learning Representations, 2023. Chandramouli Shama Sastry and Sageev Oore. Detecting out-of-distribution examples with Gram matrices.In Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings ofMachine Learning Research, 2020. Robin Schirrmeister, Yuxuan Zhou, Tonio Ball, and Dan Zhang. Understanding anomaly detection with deepinvertible networks through hierarchies of distributions and features. In Advances in Neural InformationProcessing Systems, 2020. T. R. Scott, A. C. Gallagher, and M. C. Mozer. von misesfisher loss: An exploration of embedding geometriesfor supervised learning. In 2021 IEEE/CVF International Conference on Computer Vision (ICCV), 2021.",
  "Vikash Sehwag, Mung Chiang, and Prateek Mittal. Ssd: A unified framework for self-supervised outlierdetection. In International Conference on Learning Representations, 2021": "Joan Serr, David lvarez, Vicen Gmez, Olga Slizovskaia, Jos F. Nez, and Jordi Luque. Input complexityand out-of-distribution detection with likelihood-based generative models. In International Conference onLearning Representations, 2020. Yihong Sun, Adam Kortylewski, and Alan Yuille.Amodal segmentation through out-of-task and out-of-distribution generalization with a bayesian model. In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition (CVPR), 2022a.",
  "Leitian Tao, Xuefeng Du, Jerry Zhu, and Yixuan Li. Non-parametric outlier synthesis. In The EleventhInternational Conference on Learning Representations, 2023": "Joost Van Amersfoort, Lewis Smith, Yee Whye Teh, and Yarin Gal. Uncertainty estimation using a single deepdeterministic neural network. In Proceedings of the 37th International Conference on Machine Learning,volume 119 of Proceedings of Machine Learning Research, 2020. Grant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui, Chen Sun, Alex Shepard, Hartwig Adam, PietroPerona, and Serge Belongie. The inaturalist species classification and detection dataset. In 2018 IEEE/CVFConference on Computer Vision and Pattern Recognition, pp. 87698778, 2018. doi: 10.1109/CVPR.2018.00914.",
  "Jingkang Yang, Kaiyang Zhou, Yixuan Li, and Ziwei Liu. Generalized out-of-distribution detection: A survey.arXiv preprint arXiv:2110.11334, 2021b": "Shuyang Yu, Junyuan Hong, Haotao Wang, Zhangyang Wang, and Jiayu Zhou. Turning the curse ofheterogeneity in federated learning into a blessing for out-of-distribution detection. In The EleventhInternational Conference on Learning Representations, 2023. Jingyang Zhang, Jingkang Yang, Pengyun Wang, Haoqi Wang, Yueqian Lin, Haoran Zhang, Yiyou Sun,Xuefeng Du, Kaiyang Zhou, Wayne Zhang, Yixuan Li, Ziwei Liu, Yiran Chen, and Hai Li. Openood v1.5:Enhanced benchmark for out-of-distribution detection. arXiv preprint arXiv:2306.09301, 2023a. Jinsong Zhang, Qiang Fu, Xu Chen, Lun Du, Zelin Li, Gang Wang, xiaoguang Liu, Shi Han, and DongmeiZhang. Out-of-distribution detection based on in-distribution data patterns memorization with modernhopfield energy. In The Eleventh International Conference on Learning Representations, 2023b.",
  "E(x; f) log p (x)": "To prove this, one can rewrite the posterior probability p(y = c|x) = p(x, y = c)/p(x).Note thatone can always rescale the logits f(x) by a function log (x), without changing the posterior probability:p(y = c|x) = p(x, y = c)(x)/p(x)(x). In this case, the negative log of the denominator in softmax (i.e.,the energy score) is log[p(x)(x)], which is no longer truthfully reflecting the log p(x).",
  "BExperimental Details": "Software and hardware. We conduct our experiments on NVIDIA RTX A6000 GPUs (48GB VRAM). Weuse Ubuntu 22.04.2 LTS as the operating system and install the NVIDIA CUDA Toolkit version 11.6 andcuDNN 8.9. All experiments are implemented in Python 3.8 using the PyTorch 1.8.1 framework. Training cross-entropy models. For methods using cross-entropy loss, such as MSP (Hendrycks & Gimpel,2017), ODIN (Liang et al., 2018), Mahalanobis (Lee et al., 2018b), Energy (Liu et al., 2020), ViM (Wanget al., 2022), ReAct (Sun et al., 2021), DICE (Sun & Li, 2022), SHE (Zhang et al., 2023b), and ASH (Djurisicet al., 2023), the models are trained using stochastic gradient descent with momentum 0.9 and weight decay104. The initial learning rate is 0.1 and decays by a factor of 10 at epochs 100, 150, and 180. We trainthe models for 200 epochs on CIFAR. For the ImageNet benchmark, we adopt Torchvisions pre-trainedResNet-50 model with ImageNet-1k weights. Training contrastive models. For methods using SupCon loss (Khosla et al., 2020), such as KNN+ (Khoslaet al., 2020) and SSD+ (Sehwag et al., 2021), we adopt the same training scheme as in (Ming et al., 2023).For CIFAR, we use stochastic gradient descent with a momentum of 0.9 and a weight decay of 104. Theinitial learning rate to 0.5 and follows a cosine annealing schedule. We train the models for 500 epochs. Thetraining-time temperature is set to be 0.1. For the ImageNet model, we use the checkpoint provided in Sunet al. (2022b). For methods using vMF loss, we train on CIFAR using stochastic gradient descent with momentum 0.9 andweight decay 104. The initial learning rate is 0.5 and follows a cosine annealing schedule. We use a batchsize of 512 and train the model for 500 epochs. The training-time temperature is set to be 0.1. We adoptthe exponential-moving-average (EMA) for the prototype update Li et al. (2020), with a momentum of 0.5for CIFAR-100. For ImageNet-1k, we fine-tune a pre-trained ResNet-50 model in Khosla et al. (2020) for 100epochs, with an initial learning rate of 0.01 and cosine annealing schedule.",
  "CValidation Method for Selecting Test-time Temperature": "To select the test-time temperature for our proposed INK score, we follow the validation method outlinedin Hendrycks et al. (2019). We generate a validation distribution by corrupting in-distribution data withspeckle noise, creating speckle-noised anomalies that simulate out-of-distribution data. After that, we computethe performance of INK at different test-time temperatures on the validation set and select the one thatachieves the highest AUROC.",
  "DID Classification Accuracy": "presents the in-distribution classification accuracy for each training dataset.We evaluate theclassification accuracy by performing linear probing on normalized features, following the approach in Khoslaet al. (2020). Using vMF loss shows competitive ID classification accuracy compared to standard cross-entropyloss, indicating it does not compromise the models capability to distinguish samples between in-distributionclasses.",
  "EResults on CIFAR Benchmark": "In this section, we present additional results and analysis on the CIFAR-100 benchmarks. Specifically, weutilize CIFAR-10 and TIN as the near-OOD datasets, while MNIST (Deng, 2012), SVHN (Netzer et al.,2011), Textures (Cimpoi et al., 2014), and Places (Zhou et al., 2016) serve as our far-OOD datasets. Asshown in , INK displays competitive performance compared to existing state-of-the-art methods. INKachieves average FPR@95 values of 44.51% and 61.83% for the far-OOD and near-OOD groups. As shown in, INK shows strong performance on the far-OOD benchmark (average FPR 44.51%), outperforming",
  "FResults on Vision Transformer architecture": "This section details the performance evaluation of different methods for out-of-distribution (OOD) detectionusing the Vision Transformer (ViT) architecture, focusing on the ImageNet-1k dataset. Our approach involvesfine-tuning a pre-trained ViT-B-16 model over 100 epochs. The initial learning rate is set at 0.01, and weemploy a cosine annealing schedule for optimization. The performance of various methods is comparedin . INK demonstrates competitive performance in both far-OOD and near-OOD groups, with anFPR@95 rate of 38.64% and 48.80%. This result highlights the robustness and versatility of our proposedmethod when applied to diverse architectural models. As a note, while INK achieves competitive results, itdoes not consistently outperform all methods.",
  "HLimitation": "A key strength of our method is its grounded interpretation from a likelihood perspective. This approach,however, necessitates training the model under the vMF distribution. While our method achieves accuracycomparable to state-of-the-art models in certain scenarios, its important to note that it is less common thanmodels trained using traditional cross-entropy loss. This may limit its immediate applicability in environmentswhere standard cross-entropy loss models are the norm. However, we believe that the trustworthiness of themodels is worth the effort of training them in a certain way."
}