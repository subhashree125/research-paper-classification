{
  "Abstract": "Graph lottery tickets (GLTs), comprising a sparse graph neural network (GNN) and a sparseinput graph adjacency matrix, can significantly reduce the computing footprint of inferencetasks compared to their dense counterparts. However, their performance against adversarialattacks remains to be fully explored. In this paper, we first investigate the resilience ofGLTs against different poisoning attacks based on structure perturbations and observe thatthey are vulnerable and show a large drop in classification accuracy. We then present anadversarially robust graph sparsification (ARGS) framework that prunes the adjacency matrixand the GNN weights by minimizing a novel loss function capturing the graph homophilyproperty and information associated with the true labels of the train nodes and the pseudolabels of the test nodes. By iteratively applying ARGS to prune both the perturbed graphadjacency matrix and the GNN model weights, we can find graph lottery tickets that arehighly sparse yet achieve competitive performance under different training-time (poisoning)structure-perturbation attacks. Evaluations conducted on various benchmarks, consideringattacks such as PGD, MetaAttack, PR-BCD, GR-BCD, and adaptive attack, demonstratethat ARGS can significantly improve the robustness of the generated GLTs, even whensubjected to high levels of sparsity.",
  "Introduction": "Graph neural networks (GNNs) (Hamilton et al., 2017; Kipf & Welling, 2017; Velikovi et al., 2018; Zhouet al., 2020; Zhang et al., 2020) achieve state-of-the-art performance on various graph-based tasks like semi-supervised node classification (Kipf & Welling, 2017; Hamilton et al., 2017; Velikovi et al., 2018; Chowdhuryet al., 2021), link prediction (Zhang & Chen, 2018; Chowdhury et al., 2023b), and graph classification (Yinget al., 2018). The success of GNNs is attributed to the neural message-passing scheme in which each nodeupdates its feature by recursively aggregating and transforming the features of its neighbors. However, the",
  "Published in Transactions on Machine Learning Research (September/2024)": "Graph Sparsity (%) 75Perturbation rate 5% ARGSUGSRandom PruningBaseline (STRG)ARGLT Graph Sparsity (%) 75Perturbation rate 10% ARGSUGSRandom PruningBaseline (STRG)ARGLT Graph Sparsity (%) 74Perturbation rate 15% ARGSUGSRandom PruningBaseline (STRG)ARGLT Accuracy (GAT citeseer) MetaAttack Graph Sparsity (%) 78Perturbation rate 5% ARGSUGSRandom PruningBaseline (STRG)ARGLT Graph Sparsity (%) 77Perturbation rate 10% ARGSUGSRandom PruningBaseline (STRG)ARGLT Graph Sparsity (%) 77Perturbation rate 15% ARGSUGSRandom PruningBaseline (STRG)ARGLT Accuracy (GAT citeseer) PGD Attack : Node classification performance over achieved graph sparsity levels for GAT on the Citeseer datasetattacked by the PGD attack and the MetaAttack. The perturbation rates are 5%, 10%, and 15%. Red stars indicatethe ARGLTs which achieve similar performance with high sparsity. STRG is used as the baseline. Graph Sparsity (%) 86Perturbation rate 5% ARGSUGSBaseline (STRG)ARGLT Graph Sparsity (%) 84Perturbation rate 10% ARGSUGSBaseline (STRG)ARGLT Graph Sparsity (%) 86Perturbation rate 15% ARGSUGSBaseline (STRG)ARGLT Graph Sparsity (%) 86Perturbation rate 20% ARGSUGSBaseline (STRG)ARGLT Graph Sparsity (%) ARGSUGSBaseline (STRG)ARGLT Graph Sparsity (%) ARGSUGSBaseline (STRG)ARGLT Graph Sparsity (%) ARGSUGSBaseline (STRG)ARGLT Graph Sparsity (%) ARGSUGSBaseline (STRG)ARGLT Accuracy (pubmed) PGD Attack Accuracy (pubmed) MetaAttack GCN : Node classification performance over achieved graph sparsity levels for GCN on the PubMed datasetattacked by the PGD attack and the MetaAttack. The perturbation rates are 5%, 10%, 15%, and 20%. Red stars indicate the ARGLTs which achieve similar performance with high sparsity. STRG is used as the baseline.",
  ": Comparison of different graph sparsifi-cation techniques in accuracy vs. graph sparsity.ARGS achieves similar accuracy (see red star)with 35% more sparsity for the Cora datasetunder the PGD attack": "We pursue this objective by first investigating empiricallythe resilience of GLTs identified by UGS against differentstructure-perturbation attacks (Zgner & Gnnemann, 2019;Liu et al., 2019; Mujkanovic et al., 2022; Chowdhury et al.,2024; 2023a) and showing that they are vulnerable. We thenpresent ARGS (Adversarially Robust Graph Sparsification),an optimization framework that, given an adversarially per-turbed graph, iteratively prunes the graph adjacency matrixand the GNN model weights to generate an adversariallyrobust graph lottery ticket (ARGLT) which achieves com-petitive node classification accuracy while exhibiting highlevels of sparsity. Adversarial attacks like the projected gradient descent (PGD)attack (Wu et al., 2019), the meta-learning-based graphattack (MetaAttack) (Zgner & Gnnemann, 2019), theprojected randomized block coordinate descent (PR-BCD)attack (Geisler et al., 2021), and the greedy randomizedblock coordinate descent (GR-BCD) attack (Geisler et al.,2021), poison the graph structure by adding new edges ordeleting existing edges, resulting in changes in the propertiesof the graph. In the case of homophilic graphs, connected nodes generally have similar features and oftenbelong to the same class, while for heterophilic graphs, linked nodes have dissimilar features and differentclasses. Our analysis shows that the PGD attack and the MetaAttack introduce most edge modificationsaround the train nodes (Li et al., 2023b) while the local structure of the test nodes is less affected. Moreover,for homophilic graphs, adversarial edges are often introduced between nodes with dissimilar features. Incontrast, for heterophilic graphs, adversarial edges are introduced between nodes with dissimilar neighborhoodstructures. We leverage this information to formulate a new loss function that better guides the pruningof the adversarial edges in the graph and the GNN weights. Additionally, we use self-learning Amini et al.(2022) to train the pruned GNNs on sparse graph structures, which improves the classification accuracy ofthe GLTs. To the best of our knowledge, this is the first study on the adversarial robustness of GLTs. Our proposal is evaluated across various GNN architectures on both homophilic (Cora, citeseer, PubMed,OGBN-ArXiv, OGBN-Products) and heterophilic (Chameleon, Squirrel) graphs attacked by the PGD attack,the MetaAttack, the PR-BCD attack (Geisler et al., 2021), and the GR-BCD attack, for the node classificationtask. We also evaluate the proposed technique for an adaptive attack, i.e., a stronger form of attack specifically",
  "Graph Lottery Ticket Hypothesis": "The lottery ticket hypothesis (LTH) (Frankle & Carbin, 2019) conjectures that there exist small sub-networks,dubbed as lottery tickets (LTs), within a dense randomly initialized neural network, that can be trainedin isolation to achieve comparable accuracy to that of their dense counterparts. Unified graph sparsificatio(UGS) made it possible to extend the LTH to GNNs (Chen et al., 2021), showing the existence of GLTsthat can make GNN inference efficient. A GNN sub-network along with a sparse input graph is definedas a GLT if the sub-network with the original initialization, trained on the sparsified graph, has a testaccuracy that matches the one of the original, unpruned GNN trained on the full graph. Specifically, duringtraining, UGS applies two differentiable binary mask tensors to the adjacency matrix and the GNN modelweights, respectively. After training, the lowest-magnitude elements are removed and the corresponding masklocation is updated to 0, eliminating the low-scored edges and weights from the adjacency matrix and theGNN, respectively. The sparse GNN weight parameters are then rewound to their original initialization. Toidentify the GLTs, the UGS algorithm is applied in an iterative fashion until pre-defined graph and weightsparsity levels are reached. Experimental results show that UGS can significantly trim down the inferencecomputational cost without compromising predictive accuracy. In this work, we aim to find GLTs for datasetsthat have been adversarially perturbed. When we apply the UGS algorithm directly to the perturbed graphs,the accuracy performance of the GLTs is substantially lower than the one of their clean counterparts, callingfor new methods to find adversarially robust GLTs.",
  "Adversarial Attacks on Graphs": "Adversarial attacks on graphs can be classified as poisoning attacks, perturbing the graph at train time, andevasion attacks, perturbing the graph at test time. Both poisoning and evasion attacks can be targeted or globalattacks (Liu et al., 2019). A targeted attack deceives the model to misclassify a specific node (Zgner et al.,2018; Bojchevski & Gnnemann, 2019). A global attack degrades the overall performance of the model (Zgner& Gnnemann, 2019; Wu et al., 2019). Depending on the amount of information available, the existingattacks can further be categorized into white-box attacks, gray-box attacks, and black-box attacks (Zgneret al., 2018; Chang et al., 2020). Finally, an attacker can modify the node features, the discrete graphstructure, or both. Different attacks show that structure perturbation is often more effective when comparedto modifying the node features (Zhu et al., 2021a). Examples of global poisoning attacks include the MetaAttack (Zgner & Gnnemann, 2019), PGD attack (Wuet al., 2019), PR-BCD attack (Geisler et al., 2021), and GR-BCD attack (Geisler et al., 2021). Gradient-based attacks like PGD and MetaAttack treat the adjacency matrix as a parameter tensor and modify itvia scaled gradient-based perturbations that aim to maximize the loss, thus resulting in degradation ofthe GNN prediction accuracy. PR-BCD and GR-BCD (Geisler et al., 2021) are more scalable, first-orderoptimization-based attacks that can scale up to large datasets like OGBN-ArXiv and OGBN-Products (Huet al., 2020), respectively. Global poisoning attacks are highly effective in reducing the classification accuracyof multiple GNN models and are typically more challenging to counter since they modify the graph structurebefore training (Zhu et al., 2021a). In this work, we consider global graph-structure poisoning attacks.",
  "Defenses on Graphs": "Several approaches have been developed to combat adversarial attacks on graphs (Tang et al., 2020; Entezariet al., 2020; Zhu et al., 2019; Jin et al., 2020; Zhang & Zitnik, 2020; Wu et al., 2019; Deng et al., 2022;Zhou et al., 2023). Many of these techniques try to improve the classification accuracy by preprocessingthe graph structure, i.e., they detect the potential adversarial edges and assign lower weights to theseedges, or even remove them. Jaccard-GCN (Wu et al., 2019) removes all the edges between nodes whosefeatures exhibit a Jaccard similarity below a certain threshold. SVD-GCN (Entezari et al., 2020) replaces theadjacency matrix with a low-rank approximation since many real-world graphs are low-rank and attacks tendto disproportionately affect the high-frequency spectrum of the adjacency matrix. ProGNN (Jin et al., 2020)leverages low-rank, sparsity, and feature smoothness properties of graphs to clean the perturbed adjacencymatrix. GARNET (Deng et al., 2022) combines spectral graph embedding with probabilistic graphicalmodels to recover the original graph topology from the perturbed graph. GNNGuard (Zhang & Zitnik, 2020)learns weights for the edges in each message passing aggregation step via cosine similarity and penalizes theadversarial edges either by filtering them out or by assigning less weight to them. STABLE (Li et al., 2022)preprocesses the graph structure by leveraging unsupervised learning. Instead of using the node features likethe above mentioned techniques, STABLE leverages contrastive learning to learn new node representations,which are then used to refine the graph structure. Other techniques try to improve the GNN performanceby enhancing model training through data augmentation (Li et al., 2022; Feng et al., 2020), adversarialtraining (Wu et al., 2019), self-learning (Li et al., 2023b), robust aggregate functions (Geisler et al., 2021; Liet al., 2022), or by developing novel GNN layers (Zhu et al., 2019). Differently from the approaches above, GCN-LFR (Chang et al., 2021), a spectral-based method, leveragesthe fact that some low-frequency components in the graph spectrum are more robust to edge perturbationsand regularizes the training process of a given GCN with robust information from an auxiliary regularizationnetwork to improve its adversarial performance. Overall, graph preprocessing tends to remove only a smallfraction of edges from the adjacency matrix. Additionally, none of these defenses reduces the number ofparameters in the GNN model, which results in unchanged computational footprints. We instead aim toimprove the robustness of sparse GNNs with sparse adjacency matrices to achieve computation efficiency. Asrobustness generally requires more non-zero parameters, yielding parameter-efficient robust GLTs remains achallenge.",
  "Preliminaries": "Notation.Let G = {V, E} represent an undirected graph with |V| nodes and |E| edges. The topology of thegraph can be represented with an adjacency matrix A R|V||V|, where Aij = 1 if there is an edge ei,j Ebetween nodes vi and vj, while Aij = 0 otherwise. Each node vi V has an attribute feature vector xi RF ,where F is the number of node features. Let X R|V|F and Y R|V|C denote the feature matrix andthe labels of all nodes in the graph, respectively, where C is the number of classes in the dataset. With aslight abuse of notation, in this paper, we will also represent a graph as a pair {A, X}. We call the nodesin the train set as train nodes, whereas those within the test set are denoted as test nodes. In the case ofmessage-passing GNN, the representation of a node vi is iteratively updated by aggregating and transformingthe representations of its neighbors. As an example, a two-layer GCN(Kipf & Welling, 2017) can be specifiedas",
  ": Classification accuracy of GLTs generated using UGS for Cora and Citeseer datasets attacked by the PGDattack and the MetaAttack. The baseline refers to accuracy on the clean graph": "where YT L is the set of train node indices, C is the total number of classes, and Yl is the one-hot encodedlabel of node vl. In the transductive SSNC task, for a given graph, the labels of the train nodes are knownand the goal is to predict the labels of the remaining one, i.e., the test nodes. Graph Lottery Tickets.A GLT consists of a sparsified graph, obtained by pruning some edges inG, and a GNN sub-network, with the original initialization, that can be trained to achieve comparableperformance to the original GNN trained on the full graph, where performance is measured in terms oftest accuracy. Given a GNN f(, ) and a graph G = {A, X}, the associated GNN sub-network and thesparsified graph can be represented as f(, m ) and Gs = {mg A, X}, respectively, where mg and mare differentiable masks applied to the adjacency matrix A and the model weights , respectively, and isthe element-wise product. UGS (Chen et al., 2021) finds the two masks mg and m by minimizing the lossfunction LUGS = L0(f({mg A, X}, m )) + 1||mg||1 + 2||m||1, such that the GNN sub-networkf(, m ) along with the sparsified graph Gs can be trained to a similar accuracy as f(, ) on G, where1, 2 are the l1-norm sparsity regularizers of mg, m. Poisoning Attack on Graphs.In this work, we primarily investigate the robustness of GLTs under globalpoisoning attacks that modify the structure of the graph. In the case of a poisoning attack, GNNs are trainedon a graph that has been maliciously modified by the attacker. The aim of the attacker is to find an optimalperturbed A that fools the GNN into making incorrect predictions. This can be formulated as a bi-leveloptimization problem (Zgner et al., 2018; Zgner & Gnnemann, 2019):",
  "Graph Sparsification Under Adversarial Attacks": "We perform the MetaAttack (Zgner & Gnnemann, 2019) and the PGD attack (Wu et al., 2019) on theCora and Citeseer datasets with different perturbation rates. We use the same setup as Xu et al. (2019),Zhang & Zitnik (2020), and Mujkanovic et al. (2022) for performing the poisoning attacks on the datasets.Then, we apply UGS on these perturbed graphs to find the GLTs. As shown in , the classificationaccuracy of the GLTs identified by UGS is lower than the clean graph accuracy. The difference increasessubstantially when the perturbation rate increases. For example, in the PGD attack, when the graph sparsityis 30%, at 5% perturbation, the accuracy drop is 6%. This drop increases to 25% when the perturbation rateis 20%. Moreover, for 20% perturbation rate, even with 0% sparsity, the accuracy of the GNN is around 20%",
  "(a)(b)(c)": ": Impact of adversarial attacks on graph properties. (a), (b) Density distribution of attribute featuredifferences of connected nodes in perturbed homophilic (Citeseer) and heterophilic (Chameleon) graphs. (c) Densitydistribution of positional feature differences of connected nodes in perturbed heterophilic graphs. lower than that of the clean graph accuracy. While UGS removes edges from the perturbed adjacency matrix,as shown in , it may not effectively remove the adversarially perturbed edges. A nave applicationof UGS may not be sufficient to improve the adversarial robustness of the GLTs. Consequently, there is aneed for an adversarially robust UGS technique that can efficiently remove the edges affected via adversarialperturbations while pruning the adjacency matrix and the associated GNN, along with improved adversarialtraining, allowing the dual benefits of improved robustness and inference efficiency. Analyzing the Impact of Adversarial Attacks on Graph Properties.Adversarial attacks like theMetaAttack, PGD, PR-BCD, and GR-BCD poison the graph structure by either introducing new edges ordeleting existing edges, resulting in changes in the original graph properties. We analyze the difference inthe attribute features of the nodes that are connected by the clean and adversarial edges. a and bdepict the density distribution of the attribute feature difference between connected nodes in homophilic andheterophilic graph datasets attacked by the PGD attack. In homophilic graphs, the attack tends to connectnodes with large attribute feature differences. A defense technique can potentially leverage this informationto differentiate between the benign and adversarial edges in the graph(Chen et al., 2022). However, this isnot the case for heterophilic graphs (Zhu et al., 2022). For heterophilic graphs, we resort, instead, to thepositional features of the nodes, using positional encoding techniques like DeepWalk (Perozzi et al., 2014). Aswe observe from c, in heterophilic graphs, attacks tend to connect nodes with large positional featuredifferences. ARGS uses these graph properties to iteratively prune the adversarial edges from homophilic andheterophilic graphs.",
  "Adversarially Robust Graph Sparsification": "We present ARGS, a sparsification technique that simultaneously reduces edges in G and GNN parameters in under adversarial attack conditions to effectively accelerate GNN inference yet maintain robust classificationaccuracy. ARGS reformulates the loss function to include (a) a CE loss term on the train nodes, (b) a CEloss term on a set of test nodes, and (c) a square loss term on all edges. Pruning the edges based on thiscombined loss function results in the removal of adversarial as well as less-important non-adversarial edgesfrom the graph. Removing Edges Around the Train Nodes.Poisoning attacks like the MetaAttack and the PGDattack tend to modify more the local structure around the train nodes than that around the test nodes (Liet al., 2023b). Specifically, a large portion of the modifications is introduced to the edges connecting a trainnode to a test node or a train node to another train node. We include a CE loss term associated with thetrain nodes, as defined in equation 2 in our objective function to account for the edges surrounding the trainnodes. These edges include both adversarial and non-adversarial edges.",
  "i,j=1Aij(yi yj)2,(5)": "where yi, yj RP are the positional features of nodes i, j, obtained by running the DeepWalk algorithm (Per-ozzi et al., 2014) on the input graph G, P is the number of node positional features, and (yi yj)2 measuresthe positional feature distance. Removing Edges Around the Test Nodes.Removal of edges tends to be random in later iterations ofUGS (Hui et al., 2023) since only a fraction of edges in G is related to the train nodes and directly impactsthe corresponding CE loss. To better guide the edge removal around the test nodes, we also introduce a CEloss term for these nodes. However, the labels of the test nodes are unknown. We can then leverage the factthat structure poisoning attacks only modify the structure surrounding the train nodes, while their featuresand labels remain clean. Therefore, we first train a simple multi-layer perceptron (MLP) with 2 layers onthe train nodes. MLPs only use the node features for training. We then use the trained MLP to predict thelabels for the test nodes. We call these labels pseudo-labels. Finally, we use the test nodes for which the MLPhas high prediction confidence for computing the test node CE loss term. Let YP L be the set of test nodesfor which the MLP prediction confidence is above a threshold and Ymlp be the prediction by the MLP. TheCE loss is given by",
  "Pruning Iterations": "No. of Adversarial Edges Train-TestTrain-TrainTest-Test : Evolution of adversarial edges inCora dataset (attacked by PGD, 20% pertur-bation) as we apply ARGS to prune the graph.Train-Train edges connect two nodes from thetrain set. Train-Test edges connect a node fromthe train set with one from the test set. Test-Test edges connect two nodes from the test set. where , 1, and 2 are hyperparameters and the value of and is set to 1. 1 and 2 are the l1-norm regularizers ofmg and m, respectively. After the training is complete, theelements with the smallest values in mg and m, representingthe lowest percentages pg and p, are set to 0.Then, theupdated masks are applied to prune A and , and the weightsof the GNN are rewound to their original initialization valueto generate the ARGLT. We apply these steps iteratively untilwe reach the desired sparsity sg and s. Algorithm 1 illustratesour iterative pruning process. As shown in , most ofthe adversarial perturbation edges are between train and testnodes (Li et al., 2023b). Moreover, our proposed sparsificationtechnique successfully removes many of the adversarial edges. Inparticular, after applying our technique for 20 iterations, whereeach iteration removes 5% of the graph edges, the number oftrain-train, train-test, and test-test adversarial edges reducesby 68.13%, 47.3%, and 14.3%, respectively.",
  ":Set percentage p of the lowest-scored values in m to 0 and set others to 1": "Training Sparse ARGLTs.Structure poisoning attacks do not modify the labels of the nodes. In thecase of attacks like PGD and MetaAttack, the locality structure of the test nodes is less contaminated (Liet al., 2023b), implying that the train node labels and the local structure of the test nodes contain relativelyclean information. We leverage this insight and train the GNN sub-network using both train nodes and testnodes. We use a CE loss term for both the train (L0) and test (L1) nodes. Since the true labels of the testnodes are not available, we train an MLP on the train nodes and then use it to predict the labels for the testnodes (Li et al., 2018; 2023b). To compute the CE loss, we use only those test nodes for which the MLPhas high prediction confidence. The loss function used for training the sparse GNN on the sparse adjacencymatrix generated by ARGS is",
  "minL0(f({mg A, X}, m )) + L1(f({mg A, X}, m )),(8)": "where m and mg are the masks evaluated by ARGS that are kept fixed throughout training, and is set to1. In the early pruning iterations, when graph sparsity is low, the test nodes are more useful in improving themodels adversarial performance because the train nodes localities are adversarially perturbed and there existdistribution shifts between the train and test nodes. However, as the graph sparsity increases, adversarialedges associated with the train nodes are gradually removed by ARGS, thus reducing the distribution shiftand making the contribution of the train nodes more important in the adversarial training.",
  "Evaluation": "We evaluate the effectiveness of ARGS and assess the existence of ARGLTs across diverse datasets and GNNmodels under different adversarial attacks and perturbation rates. In particular, we evaluate our sparsificationmethod on both homophilic and heterophilic graph datasets which are attacked by two structure poisoningattacks, namely, PGD (Wu et al., 2019) and MetaAttack (Zgner & Gnnemann, 2019). We consider threeGNN models, namely, graph convolution networks (GCNs) (Kipf & Welling, 2017), graph isomorphismnetworks (GINs) (Xu et al., 2019), and graph attention networks (Velikovi et al., 2018). We also evaluateARGS on larger datasets, namely, OGBN-ArXiv and OGBN-Products (Hu et al., 2020), attacked by thePR-BCD and GR-BCD attacks, respectively, for the DeeperGCN model. Finally, we evaluate the robustnessof ARGS against adaptive attacks (Mujkanovic et al., 2022). We use DeepRobust, an adversarial attack library (Li et al., 2020), to perform the PGD attack and theMetaAttack and generate the perturbed graph adjacency matrix A. When performing these attacks, weuse surrogate models which have the same type and architecture of the GNN models being attacked. Forexample, when attacking ARGS on a 2-layer GCN, the surrogate model is also a 2-layer GCN. We usePytorch-Geometric (Fey & Lenssen, 2019) to perform the PR-BCD and GR-BCD attacks on the OGBN-ArXiv and OGBN-Products datasets, respectively. We compare our method with UGS (Chen et al., 2021),random pruning, and other state-of-the-art adversarial defense methods, namely, STRG (Li et al., 2023b),",
  "GCN-GARNET66.661.101684.972.971.203898.21GCN-ARGS77.531.1578.7873.190.78425.81": "GARNET (Deng et al., 2022), GNNGuard (Zhang & Zitnik, 2020), ProGNN (Jin et al., 2021), and SoftMedian (Geisler et al., 2021). Only UGS and random pruning techniques prune both the graph adjacencymatrix and the GNN model parameters no other existing defense techniques prune the GNN modelparameters. We set pg = 5%, p = 20%, similarly to the parameters used by UGS. More details on the datasetstatistics, model configurations, and hyperparameters in ARGS can be found in Appendix A.1.",
  "Defense on Homophilic Graphs": "We first evaluate the performance of ARGS on homophilic graphs against PGD and MetaAttack. Due tospace limitations, we show the results for a 20% perturbation rate for the Cora and Citeseer datasets. Resultsfor PubMed and other perturbation rates are shown in Appendix A.4. shows the results for the GCN, GIN, and GAT architectures on the Cora and Citeseer datasetsattacked by PGD and MetaAttack, respectively, where the average accuracy of the ARGLTs is reportedacross 5 runs. ARGLTs at a range of graph sparsity from 30% to 60% with similar performance as the STRGbaseline can be identified across the different GNN backbones. The ARGLTs significantly reduce the MACoperation count for GCN, GIN, and GAT by 95%, 97%, and 83%, respectively, for the Cora dataset.For the Citeseer dataset, the inference MACs reduce by 98% for all the backbone GNNs. Comparison with Other Defense Techniques. We compare the performance of ARGS with the oneof GNNGuard, GARNET, and ProGNN, which are all defense methods. Differently from ARGS, none ofthese methods prunes the weights of the GNN model. We compare these methods in terms of accuracyand inference MAC and we consider GCN as the backbone. For the different baselines, the GLT which hassimilar accuracy as the baseline with maximum graph and model sparsity is identified as the ARGLT byARGS and reported in . For the Cora dataset, the ARGLT identified by ARGS for the PGD attack",
  ": Node classification performance versus weight sparsity levels for GPRGNN on Chameleon and Squirreldataset attacked by the PGD Attack and MetaAttack. indicate the ARGLTs": "(20% perturbation rate) with maximum sparsity levels (model sparsity: 98.9%, graph sparsity: 64.1%) hasa classification accuracy of 77.53%. The three different defense techniques, namely, ProGNN, GNNGuard,and GARNET have a classification accuracy of 63.43%, 73.19%, and 66.66%, respectively, which are all lessthan the classification accuracy of the ARGLT identified by ARGS. For the Citeseer dataset attacked byMetaAttack with a 20% perturbation rate, the most sparse ARGLT has a classification accuracy of 70.2%.The defense technique ProGNN has a classification accuracy of 61.02%, which is less than the classificationaccuracy of the most sparse ARGLT. The defense techniques GNNGuard and GARNET have a classificationaccuracy of 71.62% and 72.97%, respectively. In these cases, the GLT with the same classification accuracyas the defense technique is reported in .",
  "Defense on Heterophilic Graphs": "We report the classification accuracy of ARGS on heterophilic graphs in . We use GPRGNN (Chienet al., 2020) as the GNN model for the heterophilic graph datasets Chameleon and Squirrel (McCallum et al.,2000). GPRGNN performs better than GCN, GIN, and GAT for heterophilic graphs (Deng et al., 2022). Weuse GARNET as the baseline since it achieves state-of-the-art adversarial classification accuracy compared toother defense techniques for heterophilic graphs. As shown in , ARGS is able to identify GLTs thatachieve similar classification accuracy as GARNET for the Chameleon and Squirrel datasets attacked by PGDand MetaAttack with 85% to 97% weight sparsity, resulting in a substantial reduction in inference MACs.",
  ": Node classification performance ver-sus graph sparsity levels and inference MACs forDeeper-GCN on OGBN-ArXiv dataset attackedby PR-BCD. indicate the ARGLTs": "We evaluate the robustness of ARGS on the large-scale datasetsOGBN-ArXiv and OGBN-Products. OGBN-ArXiv has 170,000nodes and 1.16 million edges while OGBN-Products has 2.5million nodes and 61 million edges.We use the PR-BCDattack for perturbing the OGBN-ArXiv dataset. Attemptingthe PR-BCD attack on the OGBN-Products dataset resultedin out-of-memory errors. We then conducted a more scalableGR-BCD attack (Geisler et al., 2021) on the OGBN-Productsdataset, employing a perturbation rate of 50%. The referenceGNN model is Deeper-GCN (Li et al., 2023a). For both the PR-BCD and GR-BCD attacks, the adversarial edges are uniformlydistributed among the train and test nodes. Hence, we set thevalue of to be 0. The PGD attack or MetaAttack faces timeoutdue to memory for these large graphs. shows thatARGS with 28-layer DeeperGCN can identify ARGLTs that havehigher model and graph sparsity compared to UGS. We takeGARNET as the baseline since it achieves better adversarialrobustness than other defense techniques for the OGBN-ArXivdataset. GARNET uses 28-layer DeeperGCN as the backbone",
  "Adaptive Attack83.090.0683.750.0980.780.1283.040.09": "GNN. The model sparsity and graph sparsity of the ARGLT are 94.50% and 48.67%, respectively, for the10% perturbed dataset, and 94.50% and 48.70%, respectively, for the 15% perturbed dataset. Results onthe OGBN-Products dataset are reported in . Our baselines include GCN, GARNET, GNNGuard,and Soft Median GDC. For the different baselines, the GLT which has similar accuracy as the baseline withmaximum graph and model sparsity is identified as the ARGLT by ARGS and reported in . Asevident from , ARGS can identify lottery tickets with higher graph and model sparsity than UGS.",
  "Defense Against Adaptive Attacks": "Recently, adaptive attacks (Mujkanovic et al., 2022) have been developed, which are stronger attacks sincethey are specifically tailored for a given defense technique. Because all the components in the loss function ofARGS are differentiable, ARGS can be directly attacked by an adaptive attack. Specifically, we evaluateARGS on a gradient-based adaptive attack, called Meta-PGD (Mujkanovic et al., 2022) which iterativelyperturbs the adjacency matrix. compares the performance of ARGS against the PGD attack and theadaptive attack, with GCN as the GNN backbone for Cora and CiteSeer. For a 5% perturbation rate, theaccuracy of the ARGLT identified by ARGS reduces by only 1.7% for Cora while for CiteSeer it reducesby only 1.5%. For a 10% perturbation rate, the reduction in classification accuracy is 2.9% for theCora dataset and 2.5% for the Citeseer dataset, showing that the performance degradation of ARGS fromadaptive attacks is minimal. We also perform the adaptive attack on OGBN-ArXiv and the results arehighlighted in .",
  "Analysis Under Structure and Node Feature Attacks": "In addition to structural attacks, we also evaluate the performance of ARGS against an attack that modifiesboth the graph structure and the node features simultaneously. Mettack (Zgner & Gnnemann, 2019) canbe modified for this purpose. For a given perturbation budget, this attack performs a structure perturbationand a feature perturbation at each iteration, and between the two perturbations, it chooses the one thatresults in a higher attack loss. This iterative process is repeated until the perturbation budget is exhausted.We attack the Cora and Citeseer datasets with 5%, 10%, and 15% perturbation rates, and use the STRGdefense technique as the baseline. ARGS can find highly sparse GLTs that achieve similar classificationaccuracy as the baseline for different graph datasets perturbed with different perturbation rates using thisattack. For example, for a 5% perturbation rate, ARGS finds GLTs that have 53.75% graph sparsity and96.55% model sparsity for the Cora dataset and 58.31% graph sparsity and 97.92% model sparsity for theCiteseer dataset. We also include the performance of UGS for comparison. shows that, for thesame sparsity levels, GLTs identified by ARGS achieve much higher classification accuracy when compared toGLTs identified by UGS. We observe that the attacked graph contains more edge perturbations than featureperturbations since modifying the graph structure results in higher attack loss than modifying the nodefeatures. This result shows that ARGS can find highly sparse GLTs for graphs attacked by both structureand node feature perturbations.",
  "Ablation Study": "We evaluate the effectiveness of each component of the proposed loss function for the sparsification algorithmby performing an ablation study, as shown in . We consider the Cora dataset under the PGD attackwith 10% and 20% perturbation rates. Configuration 1 corresponds to ARGS with all the loss components in(7). Configuration 2 does not use the feature smoothness component in (4) while configuration 3 skips theCE loss associated with the predicted test nodes in (6). Configuration 4 skips both the smoothness and CEloss on predicted test nodes. shows that both configurations 2 and 3 improve the final performancewhen compared to that of configuration 4, highlighting the importance of the losses introduced in (4) and (6).More importantly, at both high and low target sparsity, we yield the best classification performance withconfiguration 1, showcasing the importance of the unified loss function in (7). Further ablation studies areprovided in Appendix A.6.",
  "Conclusion": "In this paper, we first empirically observed that the performance of GLTs collapses against structure-perturbation poisoning attacks.To address this issue, we presented a new adversarially robust graphsparsification technique, ARGS, that prunes the perturbed adjacency matrix and the GNN weights byminimizing a novel loss function. By iteratively applying ARGS, we found ARGLTs that are highly sparseyet achieve competitive performance under different structure poisoning attacks. Our evaluation showed thesuperiority of our method over UGS at both high and low sparsity regimes.",
  "Broader Impact": "Graphs are universally used to capture the structure of real-world complex systems. Empowering deeplearning for reasoning and making predictions over graph-structured data is of broad interest in a wide rangeof applications, such as recommendation systems, neural architecture search, and drug discovery. However,scaling up GNNs to large datasets is often difficult due to the computational costs. Besides, GNNs arehighly vulnerable to adversarial attacks. This work aims to find GNN models that reduce the computationalfootprint yet maintain adversarial robustness. Such robust models will enable efficient and reliable deploymentof GNNs.",
  "Aleksandar Bojchevski and Stephan Gnnemann. Adversarial attacks on node embeddings via graph poisoning.In International Conference on Machine Learning, pp. 695704. PMLR, 2019": "Heng Chang, Yu Rong, Tingyang Xu, Wenbing Huang, Honglei Zhang, Peng Cui, Wenwu Zhu, and JunzhouHuang. A restricted black-box adversarial framework towards attacking graph embedding models. InProceedings of the AAAI Conference on Artificial Intelligence, volume 34, pp. 33893396, 2020. Heng Chang, Yu Rong, Tingyang Xu, Yatao Bian, Shiji Zhou, Xin Wang, Junzhou Huang, and WenwuZhu. Not all low-pass filters are robust in graph convolutional networks. Advances in Neural InformationProcessing Systems, 34:2505825071, 2021. Tianlong Chen, Yongduo Sui, Xuxi Chen, Aston Zhang, and Zhangyang Wang. A unified lottery tickethypothesis for graph neural networks. In International Conference on Machine Learning, pp. 16951706.PMLR, 2021.",
  "Subhajit Dutta Chowdhury, Zhiyu Ni, Qingyuan Peng, Souvik Kundu, and Pierluigi Nuzzo. Sparse butstrong: Crafting adversarially robust graph lottery tickets. arXiv preprint arXiv:2312.06568, 2023a": "Subhajit Dutta Chowdhury, Kaixin Yang, and Pierluigi Nuzzo. Simll: Similarity-based logic locking againstmachine learning attacks. In 2023 60th ACM/IEEE Design Automation Conference (DAC), pp. 16. IEEE,2023b. Subhajit Dutta Chowdhury, Zhiyu Ni, Qingyuan Peng, Souvik Kundu, and Pierluigi Nuzzo. Analyzingadversarial vulnerabilities of graph lottery tickets. In ICASSP 2024-2024 IEEE International Conferenceon Acoustics, Speech and Signal Processing (ICASSP), pp. 78307834. IEEE, 2024.",
  "Chenhui Deng, Xiuyu Li, Zhuo Feng, and Zhiru Zhang. GARNET: Reduced-rank topology learning for robustand scalable graph neural networks. In Learning on Graphs Conference, pp. 31. PMLR, 2022": "Negin Entezari, Saba A Al-Sayouri, Amirali Darvishzadeh, and Evangelos E Papalexakis. All you need is low(rank) defending against adversarial attacks on graphs. In Proceedings of the 13th International Conferenceon Web Search and Data Mining, pp. 169177, 2020. Wenzheng Feng, Jie Zhang, Yuxiao Dong, Yu Han, Huanbo Luan, Qian Xu, Qiang Yang, Evgeny Kharlamov,and Jie Tang. Graph random neural networks for semi-supervised learning on graphs. Advances in neuralinformation processing systems, 33:2209222103, 2020.",
  "Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. InInternational Conference on Learning Representations, 2017": "Guohao Li, Chenxin Xiong, Guocheng Qian, Ali Thabet, and Bernard Ghanem. DeeperGCN: Trainingdeeper GCNs with generalized aggregation functions. IEEE Transactions on Pattern Analysis and MachineIntelligence, 45(11):1302413034, 2023a. doi: 10.1109/TPAMI.2023.3306930. Kuan Li, Yang Liu, Xiang Ao, Jianfeng Chi, Jinghua Feng, Hao Yang, and Qing He. Reliable representationsmake a stronger defender: Unsupervised structure refinement for robust gnn. In Proceedings of the 28thACM SIGKDD conference on knowledge discovery and data mining, pp. 925935, 2022. Kuan Li, Yang Liu, Xiang Ao, and Qing He. Revisiting graph adversarial attack and defense from a datadistribution perspective. In The Eleventh International Conference on Learning Representations, 2023b. Qimai Li, Zhichao Han, and Xiao-Ming Wu. Deeper insights into graph convolutional networks for semi-supervised learning. In Proceedings of the AAAI conference on artificial intelligence, volume 32, 2018.",
  "Felix Mujkanovic, Simon Geisler, Stephan Gnnemann, and Aleksandar Bojchevski. Are defenses for graphneural networks robust? Advances in Neural Information Processing Systems 35 (NeurIPS 2022), 2022": "Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. DeepWalk: Online learning of social representations. InProceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining,pp. 701710, 2014. Xianfeng Tang, Yandong Li, Yiwei Sun, Huaxiu Yao, Prasenjit Mitra, and Suhang Wang. Transferringrobustness for graph neural network against poisoning attacks. In Proceedings of the 13th internationalconference on web search and data mining, pp. 600608, 2020.",
  "Ziwei Zhang, Peng Cui, and Wenwu Zhu. Deep learning on graphs: A survey. IEEE Transactions onKnowledge and Data Engineering, 34(1):249270, 2020": "Bingxin Zhou, Yuanhong Jiang, Yuguang Wang, Jingwei Liang, Junbin Gao, Shirui Pan, and XiaoqunZhang. Robust graph representation learning for local corruption recovery. In Proceedings of the ACM WebConference 2023, pp. 438448, 2023. Jie Zhou, Ganqu Cui, Shengding Hu, Zhengyan Zhang, Cheng Yang, Zhiyuan Liu, Lifeng Wang, ChangchengLi, and Maosong Sun. Graph neural networks: A review of methods and applications. AI open, 1:5781,2020. Dingyuan Zhu, Ziwei Zhang, Peng Cui, and Wenwu Zhu. Robust graph convolutional networks againstadversarial attacks. In Proceedings of the 25th ACM SIGKDD international conference on knowledgediscovery & data mining, pp. 13991407, 2019.",
  "A.1Dataset Details": "We use seven benchmark datasets, namely, Cora, Citeseer, PubMed, OGBN-arXiv, OGBN-Produts,Chameleon, and Squirrel, to evaluate the efficacy of ARGS. Details about the datasets are summarized in. In the case of Cora, Citeseer, and PubMed, 10% of the data constitutes the train set, 10% of thedata constitutes the validation set, while the test set is the remaining 80%. For Chameleon and Squirrel, wekeep the same data split settings as Chien et al. (2020). For OGBN-ArXiv and OGBN-Products we followthe data split setting of the Open Graph Benchmark (OGB) (Hu et al., 2020).",
  "A.2Poisoning Attacks on GNNs": "This section gives further details about the poisoning attacks introduced in the paper. We consider differentstructure poisoning attacks to perturb the input graph. We recall the attackers objective is to find an optimalperturbed A which results in degradation in the performance of the GNN model on the test data. Poisoningattacks can be formulated as bi-level optimization problems, as shown in (3). The MetaAttack tackles the",
  "DatasetsType#Nodes#EdgesClassesFeatures": "CoraHomophilic2485506971433CiteseerHomophilic2110366863703PubMedHomophilic19717443383500OGBN-ArXivHomophilic169,3431,166,24340128OGBN-ProductsHomophilic2,449,02961,859,14047100ChameleonHeterophilic22776279252325SquirrelHeterophilic520139684652089 bi-level problem using meta-gradients (Bojchevski & Gnnemann, 2019). It treats the graph adjacencymatrix as a hyperparameter which is to be optimized such that Latk increases. The PGD attack (Xu et al.,2019) relaxes the discrete adjacency matrix A to a continuous matrix in {0, 1}nn during the gradient-basedoptimization and optimizes its entries. The resulting adjustments to the matrix entries reflect the probabilityof an edge getting flipped. After each gradient update, the entries in the adjacency matrix are projected backsuch that the perturbations are within . Out of the different perturbed adjacency matrices in (A), the onewhich results in maximum Latk is chosen as the perturbed graph A. Attacks like PGD and MetaAttack facescalability issues when extended to large graph datasets like OGBN-ArXiv and OGBN-Products. Inspiredby the randomized block coordinate descent (RBCD), more scalable attacks have recently been developed,namely, the PR-BCD and GR-BCD attacks Geisler et al. (2021), where only a subset of variables is optimizedat a time, and only the gradients of those variables are computed, resulting in lower memory requirements.",
  "A.3ARGS Implementation Details": "We follow the setup used by UGS as our default setting (Chen et al., 2021) for ARGS. For Cora, Citeseer,and PubMed, we conduct all our experiments on two-layer GCN, GIN, and GAT networks with 512 hiddenunits. The graph sparsity pg and model sparsity p are 5% and 20% unless otherwise stated. The valueof is chosen from {0.01, 0.1, 1, 10} while the value of , , , and is 1 by default. We use the Adamoptimizer for training the GNNs. In each pruning round, the number of epochs to update the masks is bydefault 200, using early stopping. The 2-layer MLP used for predicting the pseudo-labels of the test nodeshas by default hidden dimension of 1024 unless otherwise mentioned. We use DeepRobust, an adversarialattack library (Li et al., 2020), to implement the PGD attack and the MetaAttack on the different datasets.We use Pytorch-Geometric (Fey & Lenssen, 2019) to perform the PR-BCD and GR-BCD attacks on theOGBN-ArXiv and OGBN-Products datasets, respectively. All the experiments are conducted on an NVIDIATesla A100 (80-GB GPU). For GCN, the values of 1 and 2 are both 102 for Cora and Citeseer, while for PubMed they are 106 and103, respectively. The value of the learning rate is 8 103 and that of the weight decay is 8 105 for theCora dataset. For Citeseer and PubMed, the learning rate is 102 and the weight decay is 5 104. For thedifferent datasets, we use a dropout of 0.5. In the case of GIN, for the Cora dataset, the learning rate is 8 103, the weight decay is 8 105, 1 is103, and 2 is 103. For Citeseer, the learning rate is 102, the weight decay is 5 104, 1 is 105, and 2is 105. For GAT, in the case of the Cora dataset, the learning rate is 8 103, the weight decay is 8 105,1 is 103, 2 is 103, and dropout is 0.6. Finally, for the Citeseer dataset, the learning rate is 102, theweight decay is 5 104, 1 is 105, 2 is 105, and the dropout is 0.6. We use DeeperGCN models for the OGBN-ArXiv and OGBN-Products datasets. In the case of OGBN-ArXiv,we use a 28-layer DeeperGCN model; for OGBN-Products, we use a 14-layer DeeperGCN model. ForOGBN-ArXiv, the learning rate is 102, 1 is 106, and 2 is 106. For OGBN-Products, the learning rateis 103, 1 is 104, and 2 is 106. Droput is 0.5 for both the datasets. In the case of heterophilic graphs,we use GPRGNN as the backbone GNN. For the Chameleon dataset, the learning rate is 5 102, 1 is103, 2 is 103, and dropout is 0.4. For the Squirrel dataset, the learning rate is 5 102, 1 is 106, 2 is102, and dropout is 0.4.",
  "A.4Performance Evaluation of ARGS: Additional Results": "We evaluate the performance of ARGS on homophilic graphs perturbed by the PGD attack and MetaAttack.We show the results for a 5%, 10%, and 15% perturbation rate for the Cora and Citeseer datasets in Figures 9and 10. Figures 11 and 12 show the performance of ARGS on the Cora and Citeseer datasets with GIN as thebackbone GNN. Finally, shows the performance of ARGS on Citeseer with GAT as the backboneGNN. Finally, we evaluate the performance of ARGS on the PubMed dataset, as shown in .",
  "A.5Comparing the Impact of Node Attribute Features and Positional Features on ARGLTs forHomophilic Graphs": "For homophilic graphs, we considered node attribute features for removing the adversarial edges, while nodepositional features were considered for heterophilic graphs. We then perform a set of experiments, where weinstead consider positional features of the nodes in homophilic graphs. We observe that the overlap betweenthe density distribution of positional feature differences for clean edges and that for adversarial edges is higherwhen compared to the overlap between the density distributions of attribute features. However, the twodensity distributions of positional feature differences are still separable. The results are reported in .",
  "A.6Further Ablation Study": "We perform an ablation study to verify the effectiveness of each component of the proposed loss function usedfor the sparsification algorithm. Part of this study is already included in the main sections of the paper 5. Wereport the rest of the results in . In particular, we present the analysis performed on the Cora datasetfor all the 3 different attacks with all the 4 perturbation rates. We recall that configuration 1 corresponds toARGS with all the loss components. As shown in , at both high and low target sparsity, we yield thebest classification performance with configuration 1, showcasing the importance of all the components of theproposed loss function.",
  "A.7Performance Evaluation of ARGS on Clean Graphs": "We finally evaluate the performance of ARGS on clean graphs. As shown in , ARGS can still findhighly sparse GLTs in clean graphs. The lottery tickets found by ARGS achieve similar model and graphsparsity level when compared to UGS for the same classification accuracy on Cora and Citeseer datasetsacross three different GNN models. We assume the accuracy of UGS at 0% graph and 0% model sparsity asthe baseline accuracy."
}