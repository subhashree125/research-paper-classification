{
  "Abstract": "Imitation Learning (IL) is one of the most widely used methods in machine learning. Yet,many works find it is often unable to fully recover the underlying expert behavior (Wen et al.,2020; Jacob et al., 2022), even in constrained environments like single-agent games (De Haanet al., 2019; Hambro et al., 2022b). However, none of these works deeply investigate therole of scaling up the model and data size. Inspired by recent work in Natural LanguageProcessing (NLP) (Kaplan et al., 2020; Hoffmann et al., 2022) where scaling up hasresulted in increasingly more capable LLMs, we investigate whether carefully scaling upmodel and data size can bring similar improvements in the imitation learning setting forsingle-agent games. We first demonstrate our findings on a variety of Atari games, andthereafter focus on the extremely challenging game of NetHack. In all games, we find thatIL loss and mean return scale smoothly with the compute budget (FLOPs) and are stronglycorrelated, resulting in power laws (and variations of them) for training compute-optimalIL agents. Finally, we forecast and train several NetHack agents with IL and find our bestagent outperforms the prior state-of-the-art by 1.7x in the offline setting. Our work bothdemonstrates the scaling behavior of imitation learning in a variety of single-agent games, aswell as helps narrow the gap between the learner and the expert in NetHack, a game thatremains elusively hard for current AI systems.1",
  "Introduction": "While conceptually simple, imitation learning has powered some of the most impressive feats of AI in recentyears. AlphaGo (Silver et al., 2016) used imitation on human Go games to bootstrap its ReinforcementLearning (RL) policy. Cicero, an agent that can play the challenging game of Diplomacy, used an IL-basedpolicy as an anchor to guide planning (Jacob et al., 2022). Go-Explore, a method for hard-explorationproblems which solved all previously unsolved Atari games, used self-imitation learning in its robustificationphase (Ecoffet et al., 2021). Despite its prevalence, several works have pointed out some of the limitations of IL. De Haan et al. (2019) andWen et al. (2020) call out the issue of causal confusion, where the IL policy relies on spurious correlations toachieve high training and held-out accuracy, but performs far worse than the data-generating policy, even insingle-agent Atari games. Jacob et al. (2022) have mentioned similar issues for policies learning from humangames: they consistently underperform the data-generating policy. However, in many of these works, the roleof model and data size is not deeply investigated. This is especially striking considering the increasinglyimpressive capabilities that recent language models have exhibited, mostly as a consequence of scale. In aseries of papers trying to characterize these improvements with scale starting with Hestness et al. (2017)and Rosenfeld et al. (2019), it has been shown language modeling loss (i.e. cross-entropy) scales smoothlywith model size and number of training tokens (Kaplan et al., 2020; Hoffmann et al., 2022). If we thinkof language models as essentially performing imitation learning\" on text, then a natural next question is",
  "(d) Optimal samples vs. FLOPs": ": BC return scaling. We train a wide range of model sizes across several orders of magnitudesof FLOP budgets (same models as in a) and plot their average return in the environment (a). Wethen regress the optimal returns (b), the return-optimal number of parameters (c), and the return-optimalnumber of samples (d) on their corresponding FLOP budgets. We find mostly clear power law trends forNethack (left), Battle Zone (middle), and Breakout (right). Full Atari results can be found in Appendix J. fitted parameters. Note that sometimes we set the b variables to 0 since they model the threshold of thepower laws, which we dont always observe. For many of the loss power laws however, we do observe itand hence fit bL as well (e.g. see left plot of b). We refer to the legends of c, d,",
  "Published in Transactions on Machine Learning Research (12/2024)": "menu skipping. We also experimented with the NetHackChallenge-v0 environment but found the resultstoo noisy at the FLOP budgets we were able to run. However, we expect similar results will hold for thisenvironment at larger FLOP budgets. IsoFLOP profiles.We train 9 different model sizes ranging from 100k to 50M using IMPALA (Espeholtet al., 2018), each with a FLOP budget ranging from 1e13 to 1e17. For each of these models, we evaluatethe model at the end of training by rolling it out 1k times in the environment and reporting the averagereturn. While learning curves in RL tend to have high variance, we generally still find that compute-optimalmodels should increase both the number of parameters and number of environment interactions as the FLOPbudgets are scaled up (see ). We also find that the NetHack game score varies smoothly with FLOPsand hence can be seen as a natural performance metric (Hilton et al., 2023), as discussed in more detailin section 6. We again follow a similar procedure as in subsection 4.1 resulting in power laws as listedin Equation 6. We find = 0.43, = 0.56, and = 0.32. Parametric fitWe take the functional form in Equation 3, and replace loss with mean return. We canthen solve the same constrained optimization problem resulting in the exact same expressions as foundin Equation 5 (the denominator of 6 is replaced with 8 due to a slight difference in FLOP counting for RL,see Appendix E). After fitting, we find = 0.6 and = 0.4. Note we dropped the low flop budgets whenperforming this regression, as we found this greatly improved the fit. Forecasting human performance.Hambro et al. (2022b) report that average human performanceis around 127k. Based on the two approaches discussed above, we forecast the compute requirements fortraining an RL agent from scratch to achieve human-level performance on NetHack, listed in . Forthe isoFLOP profile approach, we first use b to solve for C127k. Then we plug this into the powerlaws from c and d. For the parametric fit, we instead plug C127k into the power lawsfrom Equation 5 with the correct and from above, where the denominator of 6 is replaced with 8 asmentioned earlier. In , we find the parametric fit to put significantly more emphasis on model size,which could be possible due to dropping of the low FLOP budgets (optimal model size tends to shift moreclearly in larger FLOP budgets). Due to computational constraints, we leave testing the limits of thisprediction to future work. RL with pretraining.All our scaling law results on the RL side in this paper are with policies trainedfrom scratch. However, some of the most promising neural methods for NetHack and other domains leveragea pre-trained (e.g. through imitation learning) policy that is then finetuned with RL. It would be veryinteresting to analyze the scaling behaviors for these kind of kickstarted policies, and see whether they scaledifferently than the ones trained from scratch. We leave this to future work.",
  "Preliminaries": "We now introduce the formal setup for behavioral cloning. We assume the environment can be described by aPartially Observable Markov Decision Process (POMDP) S, T, A, O, R, , with states S, transition functionT, action set A, possible observation emissions O, reward function R(s, a), and discount factor . In the behavioral cloning setup, we dont assume access to the rewards but instead assume access to a datasetD consisting of trajectories = (s0, a0, s1, a1, . . .) of states and actions. These trajectories can be generatedby multiple (possibly sub-optimal) demonstrators acting in the environment. However, in this work, they areassumed to all come from the same expert policy . The goal is to recover this expert policy. To do this, alearner will optimize the following cross-entropy loss:",
  "Experimental setup": "We analyze the scaling behavior of agents trained with BC in two domains: (1) Atari and (2) NetHack. Theformer serves to test the validity of the scaling laws in a range of games, while the latter tests the performancegains of scaling in an extremely challenging and unsolved game. Whenever we report FLOP or parameter counts, we are referring to their effective counts, which we define asonly including the parts of the network that are being scaled, similar to Hilton et al. (2023) (see Appendix Efor full details). Please see Appendix G for details on all hyperparameters.",
  "Atari": "We chose the following set of 8 Atari games: Battle Zone, Q*bert, Name This Game, Phoenix, Space Invaders,Bank Heist, Boxing, and Breakout. For a detailed discussion on how these games were selected, please referto Appendix F. We then perform the following steps for each game. First, we train a CNN-based agentwith PPO (Schulman et al., 2017) in order to get an expert agent. Second, we gather a dataset of about1B samples consisting of rollouts of the expert agent. We then train a family of CNN-based agents on thisdataset using BC, varying the width of the core CNN and the final linear layer (see Appendix E). The totalnumber of parameters ranged from 1k to 5M.",
  "NetHack": "We train Transformer-based agents on the NLD-AA dataset (Hambro et al., 2022b), varying both the widthand depth (i.e. number of layers) of the model (see Appendix E). The total number of parameters rangedfrom 200k to 200M. While the original NLD-AA dataset already contains around 3B samples, we extendedthe dataset to around 55B samples (NLD-AA-L) by collecting more rollouts from AutoAscend (i.e. thedata-generating policy).",
  "Scaling up imitation learning": "This section is structured as follows. We first investigate the role of model size and number of samples withrespect to cross-entropy loss (subsection 4.1). While intuitively it feels like a lower loss should result in abetter agent, we verify this by directly investigating the role of model size and number of samples with respectto the environment return (subsection 4.2), and relating these results to the loss results. Finally, we alsoshow a possible extension of our analysis to the RL setting (subsection 4.3).",
  "To investigate the role of model size and number of samples with respect to cross-entropy loss, we follow twoapproaches that are similar to the ones used in Hoffmann et al. (2022)": "Approach #1: isoFLOP profiles.For Atari, we train up to 12 different model sizes, ranging from 1kto 5M. For NetHack, we train 10 different model sizes, ranging from 200k to 200M. For all domains, wetrain FLOP budgets as low as 1e14 and up to 5e18. In we plot the loss evaluated on a held-outset of about 100 (for Atari) and 10k (for NetHack) trajectories against the parameter count for each FLOPbudget. Similarly to Hoffmann et al. (2022), we observe clear concave-up parabolas with well-defined minimaat the optimal model size for a given compute budget in all games. We also find that loss decreases withincreasing FLOP budgets. We take these loss-optimal data points to fit three regressions: one that regressesthe parameters on the FLOPs, another that regresses the samples on the FLOPs, and a final one that regressesthe loss on the FLOPs. These regressions give rise to the following power laws plus a constant:",
  ". BC Loss0.61 (0.52, 0.71)0.39 (0.29, 0.48)0.590 (0.587, 0.593)0.410 (0.407, 0.413)2. BC Return0.51 (0.43, 0.59)0.49 (0.41, 0.57)0.605 (0.601, 0.610)0.395 (0.390, 0.399)": "and b for sample values of the power law exponents , , and , respectively. These figures indicatethat as we increase FLOPs, we can expect loss to decrease according to a power law when appropriatelyincreasing the size of the model and the size of the data according to their own power laws as well.",
  "If we only look at the linear terms here, we notice that this loss has the form of a Cobb-Douglas productionfunction:L(N, D) = exp(0) N N DD,(4)": "where we can think of parameters N and samples D as inputs that affect how much output (i.e. loss) getsproduced. We then take the functional form in Equation 3 and minimize the loss subject to the constraintthat FLOPs(N, D) 6ND4. To do this, we used the method of Lagrange multipliers to get the followingfunctional forms for Nopt and Dopt (see Appendix A for full derivation):",
  "Scaling laws for BC return": "Note that the analysis in the previous section was all in terms of cross-entropy loss. However, in the imitationlearning setting, we almost never care directly about this quantity. Instead, we care about the average returnof the resulting agent in the environment. To investigate how this quantity scales, we roll out every modelfrom a in the corresponding Atari or NetHack5 environment and average their score across 100rollouts in Atari, and across at least 500 rollouts in NetHack. The results in a show we get a mirroredimage of the loss case: we observe clear concave-down parabolas with well-defined maxima at the optimalmodel size for a given compute budget in all games. We also see that when FLOPs are increased, the returns 4Note that this FLOPs equation is only valid for our NetHack experiments, since the model there is Transformer-based. Tocarry out a similar analysis for Atari, where the models are CNN-based, this formula needs to be adjusted. We only perform theanalysis for NetHack due to the simplicity of the FLOPs equation.5While past work has pointed out the NetHack score is not necessarily aligned with winning the game (Kttler et al., 2020),they still recommend using it as a proxy to measure progress in the game.",
  "Nopt = aNC + bNDopt = aDC + bDRopt = (aRC + bR)1 ,(6)": "where Nopt indicates the return-optimal model size, Dopt the return-optimal data size, Ropt the maximalreturn, and C the compute budget in FLOPs. We refer to the legends of c, d, and bfor sample values of , , and , respectively. These figures indicate that as we increase FLOPs, the policyreturns improve according to a power law when appropriately increasing the size of the model and the sizeof the data according to their own power laws as well. Note that the functional form for Ropt is simplythe inverse of a power law plus constant. When looking at b, we find that for the Atari games thescaling laws hold all the way until expert performance. For NetHack, we find more FLOPs (and maybe otherimprovements, see the end of section 5 for a discussion on this) will be required to reach the expert score of17k. Additionally, we can take the functional form in Equation 3 and simply replace loss with mean return. Wecan then solve the same constrained optimization problem resulting in the exact same expressions as foundin Equation 5. We list the resulting coefficients for NetHack in . Unlike was the case for BC loss, wefind the scaling exponents for the two methods to somewhat differ for BC return. While the parametric fitindicates scaling model and data size similarly to the loss case, the isoFLOP profiles indicate scaling modeland data size equally instead. In general, we recommend performing a rolling cross-validation and pickingthe method with the lowest RMSE. To investigate the relationship between loss and mean return, we regress the loss-optimal returns on thecorresponding loss values. We find the relationship can be well described by the inverse of a power law plus aconstant, i.e. R = (aRL (1/Lopt) + bRL)1, as shown in . The fit in the figure shows optimal lossand mean return are highly correlated in all games, indicating we can expect return to increase smoothly aswe make improvements in loss, rather than showing sudden jumps.",
  "Extension to reinforcement learning": "Given the stark trends we found for BC in the previous sections, we investigate whether similar trends canbe found for RL. We explore this briefly for the game of NetHack since several works in the past years haveattempted RL-based approaches for NetHack (Kttler et al., 2020; Hambro et al., 2022b) without comingclose to solving the game, unlike is the case for Atari. We investigate the role of model size and environmentinteractions using approaches 1 and 2 from subsection 4.1 applied to IMPALA (Espeholt et al., 2018). While learning curves in RL tend to have high variance, suggests that compute-optimal agentsshould increase both the number of parameters and number of environment interactions as the FLOP budgetsare scaled up. We also find that the NetHack game score varies smoothly with FLOPs and hence can be seenas a natural performance metric (Hilton et al., 2023). We provide complete details of our setup and resultsin Appendix I.",
  "(d) Samples vs. FLOPs": ": RL return scaling. We train a wide range of model sizes across several orders of magnitude ofFLOP budgets and plot the average return when rolled out in the environment at the end of training (a). Wethen regress the return-optimal average returns (b), parameters (c), and samples (d) on their correspondingFLOP budgets. We train 1 seed per point on the isoFLOP profile.",
  "Forecasting compute-optimal BC agents": "The isoFLOP profiles and scaling laws shown in and allow us to predict the loss or returnwe can expect to achieve for a given compute budget, when allocating FLOPs optimally between model anddata size. For all our Atari games (except for Space Invaders), we already trained agents that reached theexpert return (see the black lines in b), and we dont expect the agents to improve further with morecompute, so these games are not very suitable to test our scaling laws for extrapolation. Hence, we will turnto NetHack to test the predictive power of our scaling laws, using the following extrapolation tests:",
  ". Loss @ 40B: Given 40B samples, predict the loss-optimal model size, and check if the resultingagent achieves a loss predicted by our loss scaling law": "2. Return @ 40B & 55B: Similar to the the loss prediction, we predict the return-optimal modelsize for both 40B samples and 55B samples. Then, we check if the resulting agents achieve a returnpredicted by our return scaling law. While the predictions above start with a dataset size and calculate the optimal FLOPs and model size fromthere, one could just as well have started from a desired loss or return to achieve, a set model size, or a givenFLOPs budget. Given any one of the aforementioned, one can compute any of the others (assuming we wantto run in the compute optimal regime). To compute the optimal model size for the predictions mentioned earlier, we use the scaling laws derivedfrom the isoFLOP profiles6 as follows. We first plug in D = 40B into the regression in d (for loss)or d (for return) to solve for C40B, the FLOP budget corresponding to a dataset size of 40B samples.Then, we plug C40B into the regression in c (for loss) or c (for return) to get N40B, thecompute-optimal model size when using 40B samples. This way, we find that the model size for the lossprediction @40B, the return prediction @40B, and the return prediction @55B should be 138M, 32M, and45M, respectively. We trained the three forecasting settings above which took 5 days for the @40B models, and 6.5 daysfor the @55B models. The results can be found in . For the loss prediction, we find our scalinglaw matches the actual loss almost perfectly. Somewhat surprisingly, the return models performed a bitbetter than predicted by our scaling law. This could be due to using a cosine learning rate schedule for thepredictions, while using a constant learning rate for the isoFLOP profiles from which the scaling laws arederived (see section 6 for a bit more discussion on this). In , we compare our best agent with offlineRL methods (DQN-Offline, CQL, and IQL) as well as other past BC methods (CDGPT5, Transformer, anddiff History LM). For completeness, we also list methods that use additional RL fine-tuning on top of offline",
  "(b) Return predictions": ": Forecasting Results. In , we compare our models to prior work and find it sets a newSOTA in the offline setting. In (a), we show the loss-optimal model matches the prediction. In (b), we showthe return-optimal models are close to our predictions. training. Our agent gets a score of 7784 on the Human Monk role in NetHack - well beyond the next bestoffline method of 4504, which additionally relied on a pretrained initialization. This indicates BC performancecan be boosted significantly with scale. DiscussionWhile our return power law initially continues to improve with scale, we find it will plateauwell before expert performance in NetHack (but not in Atari). There could be various reasons for this earlyplateauing of the power law, one of which is partial observability, which we study further in section 7.",
  "Limitations": "Natural performance metrics.There is no reason in general to expect game scores to scale smoothly. Ifthey do, Hilton et al. (2023) define them as natural performance metrics. Hence, one way of viewing ourresults is as a confirmation of the score functions for NetHack and Atari as natural performance metrics forIL. We expect that for any game score to be a natural performance metric, it needs to be at least somewhatdense so it tracks learning progress, which is why we focused on environments with relatively dense rewardsin this paper7. Its possible our results extend to highly sparse reward settings as well, but one may need tointroduce alternative proxy metrics (e.g. intrinsic performance (Hilton et al., 2023)) in that case.",
  "(b) Effect of missing features": ": Effects of partial observability. In (a) we observe context lengths up to 4096 give significantimprovements in loss, suggesting long contexts may be needed to fully model the expert. In (b) we show thatincluding the inventory, which has been commonly left out in past work but upon which the expert relies,further improves the loss. Experimental setup.Previous works have pointed to the importance of tuning hyperparameters forevery run on the isoFLOP profile. In particular, Hoffmann et al. (2022) recommend using a separate cosinelearning rate schedule for every FLOP budget on the isoFLOP profile. However, to limit computational cost,we used a constant learning rate for every model size so we could leverage snapshots\" of the same run toevaluate different FLOP budgets for the same model size. While we did tune this constant learning ratepretty carefully (see Appendix G), there will nevertheless be some uncertainty in the exact values of all ourpower law coefficients. However, we expect the overall trends to still hold. Availability of data.While some game environments might already come with large datasets of trajec-tories (Hambro et al., 2022b), this is generally not the case. Hence, depending on the game, the expert,and the scaling law, we might find ourselves in a data-constrained setting. If the game has a fast simulatorand a computationally cheap expert (as is the case in this paper), then data availability may be less of aproblem since we can simply collect more data by rolling out many trajectories in parallel using the expertpolicy. However, if the game simulator itself is slow or the expert is expensive (e.g. a human), then dataavailability may become a bottleneck. Finally, the specific scaling law for a game dictates how much data weactually need (assuming we want to run in the compute-optimal regime). Hence, if the scaling law indicateswell need trillions of data points, we might be data-constrained somewhat irrespective of the computationalrequirements of running the game and the expert. The opposite is also true, however: for simple games (likeAtari), the scaling laws seem to usually indicate we dont need that much data (< 1B), which means we mightstill be able to get away with slightly more computationally demanding game simulators and experts. Oneinteresting direction for future work could be extending our results to the data-constrained setting, similarto Muennighoff et al. (2024). Comparing across environments and architectures.While we find the functional form of the scalinglaws to be the same across all environments (Atari and NetHack) and architectures (Transformer and CNN),we did not investigate the influence of different environment properties or architectures on the scalingcoefficients. We leave such cross-comparisons of different environments and architectures to future work.",
  "Related work": "NetHack.Work on NetHack has been somewhat limited so far, with early work establishing the NLEbenchmark (Kttler et al., 2020), evaluating symbolic vs. neural agents (Hambro et al., 2022a), and creatinglarge-scale datasets based off of rule-based and human playthroughs for methods aiming to learn fromdemonstrations (Hambro et al., 2022b). Later work has either focused on better reward signal supervisionand sample efficiency through proxy metrics and contrastive pre-training (Mazoure et al., 2023; Bruce et al.,2023) or leveraged dynamics models with language descriptions in order to improve sample efficiency andgeneralization (Mu et al., 2022). Piterbarg et al. (2023b) also investigates the gap between neural methodsand AutoAscend, but focuses on leveraging an action hierarchy, improvements in architecture, and fine-tuningwith RL. More recent work includes building long-context language agents (Piterbarg et al., 2023a) andinvestigating RL finetuning techniques to boost the performance of pretrained models (Wolczyk et al., 2024). Scaling laws.Hestness et al. (2017) and Rosenfeld et al. (2019) are one of the earliest works that tryto characterize empirical scaling laws for deep learning. Kaplan et al. (2020) and Hoffmann et al. (2022)specifically focus on training compute-optimal language models, finding similar trends as presented in thispaper. While in the imitation learning setting, our agents also minimize cross-entropy loss, we additionallyshow that the eventual performance of the agent as measured by the average return in the environmentscales smoothly with the loss. Other works focus more broadly on generative modeling (Henighan et al.,2020), or analyze specific use cases such as acoustic modeling (Droppo & Elibol, 2021). Clark et al. (2022)investigate scaling laws for routing networks, and Hernandez et al. (2021) study scaling laws for transfer,finding the effective data transferred (the amount of extra data required to match a pre-trained model fromscratch) follows a power-law in the low-data regime. More recent works have also tried to extend these scalinglaw results to multi-modal learning (Cherti et al., 2022; Aghajanyan et al., 2023). Caballero et al. (2022)introduce broken neural scaling laws, which allow modeling of double descent and sharp inflection points. Perhaps the closest work to our paper is that of Hilton et al. (2023), who characterize scaling laws in RL.However, they dont consider IL, and they do not evaluate on Atari or NetHack, the latter of which weconsider an especially interesting environment because of its challenging nature.",
  "Discussion": "Beyond single-agent games.We have shown that in the imitation learning setting (and to some extendin the RL setting), scaling up model and data size provides predictable improvements, as demonstrated ina variety of Atari games and in the full game of NetHack. While we do not extend our analysis beyondsingle-agent games in this paper, we believe there are several key takeaways to take from our work whenperforming BC on a new domain. First, our results show that the same scaling laws show up across gameswith various properties (stochasticity, partial observability, pixel-based, etc.), suggesting they may show upfor other domains as well. Second, poor performance (relative to the expert) in a new domain might beexplained at least in part by scale. This is powerful since one might be able to stick with BC as a methodinstead of turning to alternatives as long as one is careful about the model and data size. Third, one has tobe careful about partial observability. A gap in context length or information parity between the learner andthe expert could potentially hurt BC performance. Leveraging human data.In this work, we did not consider analyzing the scaling relationships whenusing human trajectories (e.g. from NLD-NAO (Hambro et al., 2022b)) instead of those from AutoAscend(NLD-AA (Hambro et al., 2022b)). This is because extra care must be taken to handle the lack of actionsin the human dataset, requiring techniques such as BCO (Torabi et al., 2018). Investigating scaling lawshere could be especially interesting since: (1) the human dataset is more diverse, containing trajectoriesfrom many different players with varying level of skill, and (2) it contains many examples of trajectoriesthat ascend (i.e. win the game). (1) could shed perspective on the role of scaling when the data includesmany different and potentially suboptimal demonstrations, similar to Beliaev et al. (2022). (2) could provideinsight into the viability of methods such as Video PreTraining (Baker et al., 2022) since these rely heavilyon being able to clone the expert data well. The effect of expert quality.The experts used in our paper are of different qualities for different domains.For example, while AutoAscend is pretty poor compared to a human expert, the expert we use for Breakoutis superhuman (Schwarzer et al., 2021). Nevertheless, we observe scaling laws in all cases, suggesting that thequality of the expert does not affect the existence of our scaling law. Second, what we do find to vary basedon the quality of the expert is the upper bound or ceiling that the scaling laws converge to. Specifically, forall Atari games, we find that the scaling laws converge exactly to the mean return of the expert. Hence, weexpect that as the expert quality improves or deteriorates for a particular environment, the correspondingscaling law will change to reflect this shift in the ceiling (i.e. it will now plateau at a different mean return).",
  "Conclusion": "In this work, we find that imitation learning loss and mean return follow clear scaling laws with respect toFLOPs, as demonstrated in Atari and in the challenging game of NetHack. In addition, we find loss andmean return to be highly correlated, meaning improvements in loss translate in improved performance inthe environment. Using the found power laws, we forecast the compute requirements (in terms of modeland data size) to train compute-optimal agents aimed at recovering the underlying expert. In NetHack, wefind the performance improves substantially, surpassing prior SOTA by 1.7x in the offline setting. We alsobriefly extend our results to the reinforcement learning setting, and find similar power laws for model sizeand number of interactions in NetHack. Our results demonstrate that scaling up model and data size canprovide big boosts to imitation learning performance in single-agent games. More broadly, they also call forwork in the larger imitation learning and reinforcement learning community to more carefully consider andstudy the role of scaling laws, which could provide large improvements in many other domains.",
  "Acknowledgements": "We thank Alexander Wettig, Ameet Deshpande, Dan Friedman, Howard Chen, Jane Pan, Mengzhou Xia,Khanh Nguyen, Shunyu Yao, and Vishvak Murahari from the Princeton NLP group for valuable feedback,comments, and discussions. We are also grateful to Riccardo Savorgnan, Sohrab Andaz, Tessa Childers-Day,Carson Eisenach, Kenny Shirley, and others from the Amazon SCOT Forecasting team for helpful discussionsand encouragement. We thank Kurtland Chua for helpful feedback. Finally, we give special thanks to EricHambro for answering any questions we had about the NetHack environment and datasets throughout theproject. Sham Kakade acknowledges funding from the Office of Naval Research under award N00014-22-1-2377and the National Science Foundation Grant under award #CCF-2212841. JT and KN acknowledge supportfrom the National Science Foundation under Grant No. 2107048. Any opinions, findings, and conclusionsor recommendations expressed in this material are those of the author(s) and do not necessarily reflect theviews of the National Science Foundation. Armen Aghajanyan, Lili Yu, Alexis Conneau, Wei-Ning Hsu, Karen Hambardzumyan, Susan Zhang, StephenRoller, Naman Goyal, Omer Levy, and Luke Zettlemoyer. Scaling laws for generative mixed-modal languagemodels. arXiv preprint arXiv:2301.03728, 2023.",
  "Matthew Aitchison, Penny Sweetser, and Marcus Hutter. Atari-5: Distilling the arcade learning environmentdown to five games. In International Conference on Machine Learning, pp. 421438. PMLR, 2023": "Bowen Baker, Ilge Akkaya, Peter Zhokov, Joost Huizinga, Jie Tang, Adrien Ecoffet, Brandon Houghton, RaulSampedro, and Jeff Clune. Video pretraining (vpt): Learning to act by watching unlabeled online videos.Advances in Neural Information Processing Systems, 35:2463924654, 2022. Mark Beliaev, Andy Shih, Stefano Ermon, Dorsa Sadigh, and Ramtin Pedarsani. Imitation learning byestimating expertise of demonstrators. In International Conference on Machine Learning, pp. 17321748.PMLR, 2022.",
  "Ethan Caballero, Kshitij Gupta, Irina Rish, and David Krueger. Broken neural scaling laws. arXiv preprintarXiv:2210.14891, 2022": "Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell Wortsman, Gabriel Ilharco, Cade Gordon,Christoph Schuhmann, Ludwig Schmidt, and Jenia Jitsev. Reproducible scaling laws for contrastivelanguage-image learning. arXiv preprint arXiv:2212.07143, 2022. Aidan Clark, Diego de Las Casas, Aurelia Guy, Arthur Mensch, Michela Paganini, Jordan Hoffmann, BogdanDamoc, Blake A. Hechtman, Trevor Cai, Sebastian Borgeaud, George van den Driessche, Eliza Rutherford,T. W. Hennigan, Matthew G. Johnson, Katie Millican, Albin Cassirer, Chris Jones, Elena Buchatskaya,David Budden, L. Sifre, Simon Osindero, Oriol Vinyals, Jack W. Rae, Erich Elsen, Koray Kavukcuoglu,and Karen Simonyan. Unified scaling laws for routed language models. In International Conference onMachine Learning, 2022.",
  "Jacob Hilton, Jie Tang, and John Schulman. Scaling laws for single-agent reinforcement learning. arXivpreprint arXiv:2301.13442, 2023": "Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford,Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimallarge language models. arXiv preprint arXiv:2203.15556, 2022. Athul Paul Jacob, David J Wu, Gabriele Farina, Adam Lerer, Hengyuan Hu, Anton Bakhtin, Jacob Andreas,and Noam Brown. Modeling strong and human-like gameplay with kl-regularized search. In InternationalConference on Machine Learning, pp. 96959728. PMLR, 2022. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray,Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprintarXiv:2001.08361, 2020.",
  "Diederik P Kingma and Jimmy Ba.Adam:A method for stochastic optimization.arXiv preprintarXiv:1412.6980, 2014": "Martin Klissarov, Pierluca DOro, Shagun Sodhani, Roberta Raileanu, Pierre-Luc Bacon, Pascal Vincent,Amy Zhang, and Mikael Henaff. Motif: Intrinsic motivation from artificial intelligence feedback. arXivpreprint arXiv:2310.00166, 2023. Heinrich Kttler, Nantas Nardelli, Alexander Miller, Roberta Raileanu, Marco Selvatici, Edward Grefenstette,and Tim Rocktschel. The nethack learning environment. Advances in Neural Information ProcessingSystems, 33:76717684, 2020.",
  "John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimizationalgorithms. arXiv preprint arXiv:1707.06347, 2017": "Max Schwarzer, Ankesh Anand, Rishab Goel, R Devon Hjelm, Aaron Courville, and Philip Bachman.Data-efficient reinforcement learning with self-predictive representations. In International Conference onLearning Representations, 2021. David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den Driessche, JulianSchrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, Sander Dieleman, Dominik Grewe,John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy Lillicrap, Madeleine Leach, Koray Kavukcuoglu,Thore Graepel, and Demis Hassabis. Mastering the game of go with deep neural networks and treesearch. Nature, 529(7587):484489, Jan 2016. ISSN 1476-4687. doi: 10.1038/nature16961. URL",
  "Faraz Torabi, Garrett Warnell, and Peter Stone. Behavioral cloning from observation. In Proceedings of the27th International Joint Conference on Artificial Intelligence, pp. 49504957, 2018": "Chuan Wen, Jierui Lin, Trevor Darrell, Dinesh Jayaraman, and Yang Gao. Fighting copycat agents inbehavioral cloning from observation histories. Advances in Neural Information Processing Systems, 33:25642575, 2020. Maciej Wolczyk, Bartomiej Cupia, Mateusz Ostaszewski, Micha Bortkiewicz, Micha Zajc, Razvan Pascanu,ukasz Kuciski, and Piotr Mio. Fine-tuning reinforcement learning models is secretly a forgettingmitigation problem. In Forty-first International Conference on Machine Learning, 2024.",
  "We use two main architectures for all our experiments, one for the BC experiments and another for the RLexperiments": "BC architecture.The NLD-AA dataset (Hambro et al., 2022b) is comprised of ttyrec-formatted trajectories,which are 24 80 ASCII character and color grids (one for each) along with the cursor position. To furtherreduce partial observability, we regenerate the dataset to also store the dungeon and inventory glyphs. Toencode the ASCII characters and colors along with the glyphs, we modify the architecture used in Hambroet al. (2022b), resulting in the following: Dungeon encoder. This component encodes the main observation in the game, which is a 21 80grid per time step. Note the top row and bottom two rows are cut off as those are fed into themessage and bottom line statistics encoder, respectively. We embed each character, color, and glyphin an embedding lookup table, concatenate them together, and feed them through one linear layer,after which we put them in their respective positions in the grid. We then feed this embedded gridinto a ResNet, which consists of two identical modules, each using one convolutional layer followed",
  "by a max pooling layer and two residual blocks (of two convolutional layers each), for a total of 10convolutional layers, closely following the setup in Espeholt et al. (2018)": "Message encoder. The message encoder takes the top row of the grid, converts all ASCII charactersinto a one-hot vector, and concatenates these, resulting in a 80 256 = 20,480 dimensional vectorrepresenting the message. This vector is then fed into a two-layer MLP, resulting in the messagerepresentation. Bottom line statistics. To encode the bottom line statistics, we flatten the bottom two rows ofthe grid and create a character-normalized\" (subtract 32 and divide by 96) and digits-normalized\"(subtract 47 and divide by 10, mask out ASCII characters smaller than 45 or larger than 58) inputrepresentation, which we then stack, resulting in a 160 2 dimensional input. This closely followsthe Sample Factory8 model used in Hambro et al. (2022b). Inventory. Unlike prior work (Hambro et al., 2022a), we also include the inventory glyphs in ournetwork to increase information parity with respect to the expert (see section 7). We use the sameglyph embedding table as used for the dungeon encoder followed by a linear projection. Then, weconcatenate all inventory items along the hidden dimension and feed them through a two-layer MLP.Note that we only use these inventory features for IL, not for RL. After the components above are encoded, we concatenate all of them together.Additionally, we alsoconcatenate the previous frames action representation (coming from an embedding lookup table), and a croprepresentation (a 9 9 crop around the player, processed by a five-layer CNN). We then feed this combinedrepresentation into a two-layer MLP, after which a Transformer (in case of IL) or LSTM (in case of RL)processes the representation further. Finally, we have two linear heads on top of the sequence model, one forthe policy and one for the value (not used for BC). RL architecture.We modify the architecture from Kttler et al. (2020) to also include a five-layerone-dimensional CNN that processes the message, as well as another five-layer two-dimensional CNN thatprocesses a 9 9 crop of the dungeon grid around the player.",
  "E.1Atari": "The model network consists of three convolutional layers interleaved with ReLUs (see Appendix D for details),followed by a linear layer (i.e. the policy head). We simply scale the width of all layers. This means we scalethe channels of the convolutional network and the width of the linear head on top. Since we scale the whole network, we count the FLOPs from all convolutional and linear layers. We computethe forward FLOPs of a convolutional layer as 2 hout wout cout p, where hout is the height of the outputshape, wout is the width of the output shape, cout are the number of output channels, and p indicates thenumber of parameters in one filter of the current layer (without counting bias). Hence, p = k2 cin, wherek is the kernel size and cin is the number of input channels. Following prior work (Hilton et al., 2023), weassume the backward pass takes about twice the number of FLOPs from the forward pass.",
  "The input size of the two linear layers for the actor and critic respectively (i.e. the policy and valueheads)": "Similar to prior work (Kaplan et al., 2020; Hoffmann et al., 2022), we found 6ND to be a good approximation(for both the Transformer and the LSTM) for the number of FLOPs used based on model size N and numberof samples D. This is because we found there to be about 2ND FLOPs in the forward pass, and we assumethe backward pass takes about the twice the number of FLOPs from the forward pass. For the RL experiments, there is a slight change in the way we count FLOPs, which is that we count everyforward pass number of FLOPs from the learner twice, since there is a corresponding forward pass from anactor. Hence, for RL our formula becomes 8ND.",
  "FGame Selection for Atari Games": "We chose the following set of 8 Atari games: Battle Zone, Q*bert, Name This Game, Phoenix, Space Invaders,Bank Heist, Boxing, and Breakout. The first 4 games were picked from the Atari-5 subset (Aitchison et al.,2023), which tries to condense the full set of Atari games to a subset of 5 representative games. We hadsome trouble training a good (i.e. substantially better than random) expert for Double Dunk (the 5th ofthe Atari-5 subset), so instead we added 4 other games, chosen at random from the full set of Atari games(though excluding Montezumas Revenge due to its extremely sparse rewards). We didnt include more than8 games due to constraints on both compute and experimenter time, but we strongly suspect our resultswill generalize to most games in the Atari set. Note that the results could even hold true for Montezumasrevenge as well, but since we didnt specifically seek out very sparse reward games, we do not make thatclaim and instead convey this as a potential limitation.",
  "G.1Atari": "We list the hyperparameters for all our BC experiments in Atari in a and the hyperparameters totrain expert policies for each Atari game in b. To train these expert policies, we used the StableBaselines3 (Raffin et al., 2021) implementation of PPO (Schulman et al., 2017). We use Adam (Kingma &Ba, 2014) as our optimizer for both settings. All training experiments were done on NVIDIA GPUs (a mixof GeForce RTX 3090, GeForce RTX 2080 Ti, RTX A5000, and RTX A6000) and took about 1 - 2 daysdepending on the game and FLOP budget.",
  "G.2NetHack": "We use AdamW (Loshchilov & Hutter, 2019) as our optimizer for all BC experiments. For RL, we useRMSprop. Please find all hyperparameters for both settings in , all of which were manually tuned ortaken from prior work. All NetHack BC experiments were run on NVIDIA H100 80GB GPUs. All Atari BCexperiments were run on a mixture of NVIDIA A5000 and A6000 GPUs. The RL experiments were run onV100 32GB GPUs. All experiments took anywhere from a few hours to 4 days to run. The one exception tothis is the forecasted BC models which took up to 6.5 days to run.",
  "The NLD-AA dataset (Hambro et al., 2022b) is released under the NetHack General Public License and canbe found at": "In a, we investigate the optimal learning rate for both cosine and constant learning rate schedules inNetHack. We find 0.0005 works best for the constant schedule and hence use this for generating the isoFLOPprofiles in the paper. For the predictions, we use a cosine schedule since a shows it works better. Inaddition, in b we test width vs. depth scaling and find that keeping the aspect ratio (i.e. hiddendimension divided by the number of layers) is best kept at a little over 100, which we stick to when generatingour isoFLOP profiles. : Hyperparameters for all experiments in NetHack. We list the hyperparameters for all our BCexperiments (a) as well as the ones for our RL experiments (b). For BC, we always use a constant learningrate except for the predictions in section 5, for which we use a cosine schedule with warmup.",
  "(b) Scaling depth vs. width": ": IsoFLOP learning rate & aspect ratio in NetHack. In (a), we test both cosine and constantlearning rate schedules with varying warmup periods (specified in number of gradient steps). We find 0.0005to work best when using a constant learning rate. In (b), we investigate the optimal aspect ratio and find itremains constant across model sizes.",
  "JFull results for Atari": ", , , and list the full set of Atari results with respect to cross entropyloss. , , , and list the full set of Atari results with respect to environmentreturn. Finally, lists the full set of Atari results relating environment return and optimal loss. Notethat for the return results, we can see that Space Invaders is the only Atari game where didnt run highenough FLOP budgets to reach expert performance."
}