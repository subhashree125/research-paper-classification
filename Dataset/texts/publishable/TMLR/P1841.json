{
  "Abstract": "This report aims to verify the findings and expand upon the evaluation and training meth-ods from the paper LICO: Explainable Models with Language-Image COnsistency.Themain claims from the original paper are that LICO (i) enhances interpretability by produc-ing more explainable saliency maps in conjunction with a post-hoc explainability methodand (ii) improves image classification performance without computational overhead duringinference. We have reproduced the key experiments conducted by Lei et al.; however, theobtained results do not support the original claims. Additionally, we identify a limitation inthe papers evaluation method, which favors non-robust models, and propose robust exper-imental setups for more comprehensive quantitative analysis. Furthermore, we undertakeadditional studies on LICOs training methodology to enhance its interpretability. Our codeis available at",
  "Introduction": "Despite deep neural networks showing state-of-the-art performance in numerous computer vision tasks (Chaiet al., 2021), their application in safety-critical tasks like medical diagnosis is limited due to their black-boxnature, making it hard to understand how decisions are made. While contemporary post-hoc explanationmethods like Grad-CAM (Selvaraju et al., 2019) or RISE (Petsiuk et al., 2018) can increase the transparencyin these systems, they cannot be used to improve the decision-making process underneath to make it moreconsistent with human intuition. Previous model-based methods proposed improving the models by enforcing consistency constraints on thepost-hoc explanations generated during training (Pillai et al., 2022; Pillai & Pirsiavash, 2021). In the reviewedstudy, Lei et al. (2023) propose Language-Image COnsistency (LICO), a training framework for enhancingmodel interpretability. Unlike the previous methods, LICO does not utilize post-hoc explanations duringtraining. Instead, the loss enforces consistency between the manifold of visual features and the manifold ofclass-aware semantic information from a text encoder of a pretrained vision-language model (VLM), such asCLIP (Radford et al., 2021). By doing so, the authors claim to achieve both improved interpretability andclassification performance.",
  "Published in Transactions on Machine Learning Research (10/2024)": "Tao Han, Wei-Wei Tu, and Yu-Feng Li. Explanation consistency training: Facilitating consistency-basedsemi-supervised learning with interpretability. Proceedings of the AAAI Conference on Artificial Intelli-gence, 35(9):76397646, May 2021. doi: 10.1609/aaai.v35i9.16934. URL Jos Armando Hernndez and Miguel Colom. Repeatability, reproducibility, replicability, reusability (4r)in journals policies and software/data management in scientific publications: A survey, discussion, andperspectives. arXiv preprint arXiv:2312.11028, 2023. Lisanne Koning, Johan van Riel, and Leon Sebek. Enteric methane emission of the dutch dairy herd: Averageand variation of enteric methane emission among the dutch dairy herd. Technical report, WageningenLivestock Research, 2020.",
  "Scope of Reproducibility": "A central idea in model-based methods comparable with LICO is enforcing consistency with an additionalhuman prior during training (Han et al., 2021; Pillai & Pirsiavash, 2021; Pillai et al., 2022). In LICO, thisprior takes the form of semantic information encoded within a pretrained text encoder, which should allowfor training more interpretable models without compromising the quality of classification. In this work, wewill focus on the following claims by Lei et al.: 1. Enhanced Interpretability: By matching the manifolds of visual and text embeddings, the se-mantic information from the pretrained text encoder can guide the image classification model tofocus on distinguishing features of objects. As a result, the explanations for model decisions aremore consistent with human expectations, which is reflected in both quantitative and qualitativeanalyses. 2. Improved Classification Performance: As opposed to the previous methods increasing the inter-pretability of image models, LICO does not bring a deterioration to the classification performanceand, in some cases, may even result in increased accuracy, when compared with baseline modelstrained without LICO. 3. Necessity of both Manifold Matching and Optimal Transport Loss: LICO introduces atraining method that supplements the traditional cross-entropy loss with two additional loss com-ponents (described in section 3.1), both with a goal of bringing the manifolds of text and visualfeatures closer to each other. Although these two components serve similar functions, the mutualpresence of both is beneficial to the quality of the resulting models.",
  "Methodology": "This section describes the approach taken to reproduce the work. We start with an overview of the LICOalgorithm, and then continue by describing the datasets and hyperparameters used. We then discuss theexperimental setup and code - specifically, which parts of the authors implementation were missing orincomplete, the parts we re-implemented based on the paper, and the resources that we used. Lastly, weintroduce the quantitative metrics that we use for the evaluation.",
  "The LICO Algorithm": "LICO is a training framework for supervised image classification models that utilises a pre-trained textencoder (e.g.from CLIP models) to guide the training process of the image classifier such as ResNet-18. This is achieved by supplementing the the standard Cross-Entropy (CE) loss used to train the modelby adding Manifold Matching (MM) and Optimal Transport (OT) loss components, weighted by and parameters respectively. A general description of both is provided in the following paragraphs. The idea behind LICO is presented in fig. 1 with the most important concepts to consider being imagemanifold and language prompt manifold. The image manifold is defined by feature maps f(xi) from theproduced by CNN within the image classifier, and the language prompt manifold is defined by projectedtext embeddings h(g(ti))) obtained by passing a prompt through a pretrained Vision-Language Model(VLM) and a trainable MLP (which matches the dimensionality of the visual feature maps and the textembeddings). This prompt consists of the ground truth class label and several trainable tokens. The MM and OT losses aim to align these two manifolds, such that the image manifold becomes enhancedwith the semantic structure allegedly present in the language prompt manifold due to the multi-modaltraining of the VLM from which the embeddings are extracted. The goal of MM loss is to provide coarsealignments between the two manifolds based on the similarities between pairs of samples within each mini-batch, while OT loss is responsible for establishing a correlation between text tokens and feature maps forindividual data points. Other than images and their ground truth class labels, no additional information is needed to make use ofLICO. After training, everything except the image model is discarded. This means that a model trainedusing LICO only differs from a non-LICO model by its parameters, and LICO therefore does not increasethe computational cost at inference.",
  "Datasets": "We train and evaluate the presented models on two image classification datasets. Following the originalexperiments, we use CIFAR-100 (Krizhevsky et al., 2009), which provides 50000 training and 10000 vali-dation images divided into 100 classes. Additionally, we use ImageNet-S50 (Gao et al., 2022), consisting of64431 training images and 752 validation images with segmentation masks and bounding box informationthat we use for extended evaluation. Both datasets are balanced, containing approximately the same amountof samples of each class. The preprocessing performed during training is described in appendix B. The selection of these datasets, and not full ImageNet-1k, has been dictated mostly by the computationallimitations we have encountered during our work. However, we believe those to be sufficient to show whetherthe main claims of the LICO paper hold. As the original authors have used CIFAR-100 in their work, weshould be able to see the same trends present in our results as they have. However, we acknowledge the factthat limited resolution of images within CIFAR-100 may limit the ability of the method to connect the visualcues with semantic information from VLM. For this reason, we have chosen to conduct additional experimentsusing ImageNet-S50 dataset, a subset of the original ImageNet-1k limited to 50 classes. The higher resolutionwith images remove the possibility of LICOs impact being limited by the lack of distinguishable features in",
  "Hyperparameters": "We use the original values for the hyperparameters that were specified by Lei et al. (2023): SGD optimizerwith learning rate = 0.03, momentum = 0.9, weight decay = 0.0001, and cosine rate decay schedule i.e. = 0 cos 7k 16K, where 0 denotes the initial learning rate, k is the index of training step, and K is thetotal amount of training steps. The LICO-specific parameters are also used unchanged: = 10, = 1, andthe hidden dimension of the text projection MLP is 512. We use 100 epochs for all tested datasets and theResNet-18 architecture unless otherwise stated.",
  "Experimental Setup and Code": "Some code for LICO has been made publicly available by the authors via GitHub1. However, as of the time ofwriting, the published codebase does not include a significant portion of the experimental and method-relatedcode. While the currently available README file provides some guidance, additional details would greatlydecrease the effort required for an accurate reproduction of the original experiments. Implementations ofexplainability methods, such as GradCam and the metrics used for evaluation, like insertion and deletionscore, were not provided in the available codebase. Because of that, we use the CGC GitHub repository2 forimplementations of GradCam, insertion score, and deletion score. Consequently, we re-implemented most of the method based on the paper, except for computing the OTloss, which is present in the authors codebase. In our work, we follow the paper as closely as possible.Our codebase includes the complete LICO method, scripts for experiments, thorough documentation, andinstructions how to reproduce the results of our extended evaluation. To reduce the amount of code neededfor the implementation, and to increase readability, we use the PyTorch Lightning framework (Falcon & ThePyTorch Lightning team, 2019). Our code diverges from the formulas by Lei et al. in the way the temperature parameter of the MM loss isimplemented. Inspired by the implementation of CLIP (Radford et al., 2021), we multiply by the exponentof the trainable value instead. The range of valid values in this case is the same, but the multiplicative factoris naturally in the interval (0, ), avoiding a possible division by zero or negative temperature. Further inline with CLIP, we bound the trainable parameter to [0, log(100)]. Not doing so allows a trivial solution forthe MM loss: minimize the temperature to and achieve a feature-independent uniform softmax output.",
  "Quantitative Analysis of Salience Maps": "First, we show that the metrics used in the original paper, insertion and deletion scores, are applied ina context for which they were not originally intended. Then, we propose an extended experimental setupto quantitatively assess the interpretability of LICO and analyze the consistency of its explanations withthe human prior. Our approach involves the use of three metrics: Salience Equivariance Similarity (SES),Segmentation Content Heatmap (SCH), and the Multi Object Salience Uniformity (MOSU). Insertion and deletion: Initially proposed by Petsiuk et al., these metrics were introduced for the as-sessment of post-hoc interpretablity methods that produce saliency maps, not the models.The metricswork as follows: given a picture, a saliency map for the target class is extracted from the model. Then,for insertion, the blurred version of the image is taken, and the parts that have high saliency values aregradually unblurred. At every step, the output probability of the model for the target class is recorded. Thevalue of the insertion metric is the area under the curve (AUC) of the graph with share unblurred (thepercentage of pixels within the image being unblurred in the order of decreasing salience) as the x-axis andoutput probability of the target class as the y-axis. The deletion acts similarly, but instead, the evaluationstarts with a fully complete image, and the high-saliency regions are greyed out. The insertion needs tobe maximized, while deletion - minimized. As such, a good metric score for a combination of a model and an explanation method merely demonstratesthat the explanation method has done well. As demonstrated in appendix A, even a model that focuses onsmall regions and lacks robustness can perform well on these metrics, highlighting their limitations in thiscontext.: removing a tiny part completely confuses the model, and adding that part immediately makes themodel predict the correct class. The LICO method aims to change the model itself in order to make it more interpretable. Therefore, thecomparison between the insertion and deletion scores of a baseline model and a model trained with LICOis not sufficient to claim that one model is more interpretable than the other. Salience Equivariance Similarity: One intuitively desirable property of saliency maps is for them to benear-equivariant under certain transformations. This idea has resulted in the previously mentioned CGCmethod, created by Pillai et al. (2022), which uses a contrastive loss in order to encourage saliency mapsthat are both equivariant (under translation and cropping), and distinct from saliency maps of other images. Because LICO does not explicitly encourage equivariance, yet claims to outperform CGC, it is of interest toinvestigate whether CGC retains an advantage in this respect. We compute the SES metric, used by (Pillaiet al., 2022), as follows for an image I:",
  "map for any image. It must therefore be considered in combination with other metrics, such as the one wediscuss next": "Segmentation Content Heatmap: One of the obstacles in producing models consistent with humanintuition is the reliance of image classification models on spurious features that are associated with thetarget object, but are not causally related to it. Hence, our second evaluation concerns the intuition thatthe features used by the model to classify the object ought to lie within the boundaries of the object. Usinga specific variant of the Content Heatmap (CH) metric proposed by Pillai & Pirsiavash (2021), we considera dataset where segmentation masks for the main object are provided, and take the weighted share of thesaliency heatmap that lies within the segmented class. More precisely, we compute the SCH for some imageas:",
  "i,j Hij(2)": "Where H is a matrix with salience values Hij for each pixel, and M is a matrix representing a binarysegmentation mask obtained from the dataset, with Mij {0, 1} being 1 if the pixel belongs to the targetobject, and 0 otherwise. Such a metric shows how much importance the model assigns to the target object.It does not penalize if only part of the object is required to perform the classification, but it discouragesattending to features that do not identify the class in a way consistent with human expectations.",
  "MOSU(varea, vsalience) = DKL(varea || vsalience)(3)": "Where vsalience Rn denotes the probability vector corresponding to the distribution of salience lyingwithin the bounding box encompassing each of the n instances of the class in an image with its elementsequal to vsalience[i] =SalienceInBoundingBox[i]SalienceInBoundingBox with SalienceInBoundingBox[i] being a sum of salience within the bounding box of the object i. In a similar manner, varea Rn can be defined as a vector such thatvarea[i] =BoundingBoxAreas[i]BoundingBoxAreas where i corresponds to each of object instances in the image. To our knowledge, MOSU is a novel metric. Since this measure does not take into account salience attributed to regions outsideof objects, it should be used in tandem with the SCH metric or its variant utilizing bounding box information(Pillai et al., 2022).",
  "Computational Requirements": "Aiming to provide a more robust evaluation, we decided to run the experiments with multiple seeds, ratherthan once like the original authors do. To accommodate that, we needed to focus on datasets smaller thanthe full ImageNet-1k due to limited computational resources. For that reason, we have decided to limitour reproduction to CIFAR-100 and ImageNet-S50 datasets as described in 3.2. This allowed us reduce thehardware requirements of our reproduction compared to Lei et al. (2023). For the experiments, we use 2machines with the following GPUs: NVIDIA GeForce RTX 4090 (Machine 1), and NVIDIA A100-SXM4-40GB (Machine 2). The time taken per training epoch in different configurations is provided fully in table 1. We generallyobserve that the LICO and CGC take significantly more time and memory to train. We also notice that thetime increase depends on the model size and batch size. The table does not provide standard deviations asthey are all near 0.",
  "Claim 1. Enhanced Interpretability": "Insertion and Deletion Tests: The experiments in based on Insertion and Deletion metricsused by Lei et al. show a deterioration over the baseline, which is not in line with the authorss claims.However, due to reasons described in section section 3.4.1 this may not be sufficient evidence to judge theinterpretability of models; hence, the extended quantitative evaluation is conducted in section 4.2.1.",
  "+ MM & OT (LICO).706 .012.263 .008.443 .019": "Visual Analysis of Saliency Maps: To qualitatively investigate the interpretability of LICO, saliencymaps were also analyzed visually. Several images were sampled randomly from the CIFAR-100 and ImageNet-S50 datasets. The samples can be seen in fig. 3. Ideally, we would like to see that GradCam saliency maps ofLICO models are less often focused on spurious features. However, the opposite can generally be observed,indicating that the saliency maps tend to be less interpretable. We also noted that saliency concentrates onthe extreme edges of images for LICO models notably more often than for the baseline. This can be seen infig. 3 for classes \"snail\", \"bottle\", and \"kuvasz\".",
  "Claim 2. Improved Classification Performance": "Although LICO trained on CIFAR-100 outperforms CGC model on top-1 and top-5 accuracy, it does notimprove upon a baseline ResNet-18 as seen in . Likewise, the experiments conducted on ImageNet-S50 dataset in do not support the claim that the classification accuracy of baseline models can beimproved by training with LICO.",
  "decrease and suggests the lack of positive interaction between MM and OT losses, contrary to the claims ofLei et al.. This will be further investigated in section 4.2.1": "Classification Performance: shows that full LICO obtains lower top-1 accuracy on ImageNet-S50compared to model trained using only OT or only MM loss. It should be noted that while in terms of top-5accuracy LICO slightly outperforms a model trained with only MM loss or only OT loss, it is still worsethan the baseline. Overall, these results do not support the claims of LICO authors, who argue that usingMM and OT losses jointly is beneficial to the result.",
  "For the extended quantitative analysis of LICOs interpretability, we follow an experimental setup that hasbeen described in detail in section 3.4.1": "Salience Equivariance Similarity: LICO trained on CIFAR-100 shows a small improvement over thebaseline in terms of SES metric as seen in . It should be noted that CGC (Pillai et al., 2022) vastlyoutperforms both LICO and the baseline, which should be expected given that a similar objective is usedduring its training. During the experiments on ImageNet-S50, the full LICO achieves lower SES than the baseline as shown in. Since the SES metric measures the robustness of post-hoc explanations produced for the modelto different image transforms, this suggests that saliency maps for LICO models are less stable given basicgeometric transformation of the input compared to the baseline ResNet-18 model. However, it should benoted that models trained using only one of the OT and MM losses slightly outperform the baseline, whichshows the negative effect of combining two losses. Segmentation Content Heatmap: The results for the experiments on ImageNet-S50 in showthat the explanations generated for baseline models tend to be more localized and contained within theboundaries of the relevant objects compared to LICO models as measured by SCH. Although the modelstrained in the ablation study on loss components perform worse than the baseline on SCH metric, they againoutperform LICO models. Multi Object Salience Uniformity: The results for MOSU in show that the baseline models arebetter at distributing the salience between all relevant instances of a considered class proportionally to theirsize compared to the LICO model. The full LICO also achieves worse results than the model trained usingonly MM or only OT loss. As shown by these three metrics, the explanations produced for LICO-trained models are less consistentwith human intuition compared to a ResNet-18 baseline as well as models trained using only one of theadditional loss components. This further confirms that there is little support for Claim 1 regarding improvedinterpretability of LICO models and Claim 3 regarding the beneficial effect of using MM and OT lossesjointly.",
  "LICO Extensions for Improved Interpretability": "In our attempt to reproduce the experiments, we noticed that the text features for the context tokens encodedby CLIP were the same for all classes, which limits the chances of identifying meaningful features in alignedvisual feature maps. This happens due to the CLIPs text transformer encoding the text features based only",
  "Discussion": "In this work, we were unable to reproduce the claims made by Lei et al. (2023). We conducted severalexperiments, some of which replicated the original authors evaluations, while others expanded upon theevaluation methods and the LICO model itself. The first claim of enhanced interpretability was not reproduced. Qualitatively, produced saliency maps onImageNet-S50 tend to cover spurious features more often than for the baseline. For CIFAR-100, this is evenmore the case. In some instances, LICO resulted in artifacts appearing around the borders of the saliencymaps, which were not present in maps for baseline models. Quantitatively, LICO saliency maps result ininsertion, deletion, and combined scores that are not significantly better than baseline scores. Likewise, we did not observe significant improvements in classification performance compared to the baselineas stated in the second claim we strove to reproduce. For both CIFAR-100 and ImageNet-S50, LICO modelsresult in lower or similar top-1 and top-5 accuracy than the baseline model. Finally, we were also unable to confirm the third claim regarding the necessity of both MM and OT loss.Ablating individual loss components did not lead to a decrease in either classification performance or modelinterpretability for any of the evaluations. In most cases, it rather showed an improvement over the fullLICO approach. Our extended analysis highlighted additional areas where the performance of LICO may be limited. Thesaliency maps generated for LICO models appear to be less equivariant to translations and crops comparedto the baseline and also appear to cover more of the regions outside the boundaries of the target objects.Moreover, the explanations for LICO are worse at highlighting all instances of the relevant class comparedto the baseline model. Furthermore, we observed improved accuracy and interpretability when incorporating class-specific trainablecontext and positioning class labels at the front compared to utilising original shared prompts. This suggeststhat more relevant and descriptive text features may lead to improvements in the quality of the aligned featuremaps. Future work: We believe the idea of incorporating the text features into an image classifier is an interestingway of improving model interpretability. It might be able to hint at human understanding of the image,",
  "especially if longer descriptions are used. Thus, we propose to explore using image captions to provide morerelevant signals for aligning image features": "Limitations: Our results may be limited by several factors. Computational constraints prevented us fromconducting experiments using the ImageNet1k dataset, on which the authors claim their method performswell. That being said, LICO was also reported to perform well on CIFAR-100, and this did not turn outto be the case in our reproduction. The differences in results might have also arisen due to the potentialuse of regularisation or normalization methods not mentioned in the paper during the original experiments.The question regarding that was part of our attempted communication with the authors; however, we hadto implement the method without their response.",
  "What was Easy": "From a technical perspective, most of the content of the paper was approachable. The explanations in thepaper are quite clear, except for a few instances for example, the cost function c in eq. (4) not beingdefined. Hyperparameters were included where relevant, which made it quite straightforward to set up theexperiments replicating the authors claims. Moreover, in addition to working on ImageNet-1k, Lei et al.also conducted experiments on smaller datasets like CIFAR-100, which allows for attempts at reproductioneven with a limited computational budget.",
  "What was Difficult": "The most time-consuming part of our reproduction study was the full re-implementation of LICO based onthe papers description, as the provided codebase was partial. At the time of writing, the code on the LICOrepository contains only the implementation of Sinkhorn distance used in OT loss and the partial code forthe calculation of MM loss. However, the latter deviates from the text of the paper the main differencebeing the calculation of softmax on distances rather than negated distances as described in the paper. Thischoice was not justified by comments in the codebase. There is also no code with a complete implementationof the training loop or any of the conducted experiments.",
  "Communication with Original Authors": "As described in the previous sections, the official LICO code published by Lei et al. is not sufficient toreproduce the paper without reimplementing the majority of the method. Moreover, it should be noted thatthere was no code publicly available in the repository before the 12th of January 2024, despite the paperhaving been published in the autumn of 2023. To clarify certain implementation details not fully specified in the paper or available code, we attempted tocontact the authors via email and GitHub. As we did not receive a response during our study period, ourimplementation decisions were based on our interpretation of the published paper and referenced literature.",
  "Vipin Pillai, Soroush Abbasi Koohpayegani, Ashley Ouligian, Dennis Fong, and Hamed Pirsiavash. Consis-tent Explanations by Contrastive Learning, April 2022": "Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, GirishSastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. LearningTransferable Visual Models From Natural Language Supervision, February 2021. Ramprasaath R. Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, andDhruv Batra.Grad-cam:Visual explanations from deep networks via gradient-based localization.International Journal of Computer Vision, 128(2):336359, October 2019.ISSN 1573-1405.doi:10.1007/s11263-019-01228-7. URL",
  "AAdversarial Dataset": "In order to demonstrate that deletion and insertion metrics favor non-robust models, we construct theAdversarial dataset based on ImageNet-S50. The only difference from the base dataset is that all images forone of the classes in a dataset are marked with a small red dot of fixed size at the top left corner. With this,we aim to induce non-robustness in the model, as it is likely to start using the red dot as the sole predictorof the modified class, rather than the actual object in the picture. We train a ResNet-18 with CE loss from scratch on this dataset and observe that high importance is assignedto the location of the red dot for the class that has it. The overall deletion score is 0.129, which is comparableto models trained on ImageNet-S50. Meanwhile, deletion specifically for the class with the red dot is 0.026. For the deletion metric smaller is claimed to be better, so this experiment shows that the deletion metricfavors non-robust models that focus on relatively small, but perhaps irrelevant, details. It thus should notbe used to assess the explainability potential of an image classification model.",
  "BData Preprocessing": "During our training process, we use minimal data augmentations.Namely, RandomResizedCrop andRandomHorizontalFlip for the training set, following the available code of Pillai et al. (2022) and noaugmentation for the validation and testing sets. For the Adversarial dataset (see appendix A), we removeboth augmentations to ensure the red dot is always visible and always at the same spot. Instead, we justresize the images to (224, 224).",
  "CEnsuring Reproducibility": "To ensure reproducibility, we make all of our experiments deterministic by setting the random seed. So thatno cherry-picking is involved, we fix the seeds to be: 1, 2, and 3. Due to computational constraints, wetrained only the most important models on ImageNet50 dataset on all three seeds the remaining modelswere trained on seed 2.",
  "DEnvironmental Impact": "The GPUs used for the training, RTX 4090 and Tesla A100, consume about 200W during training. Tothat, we can add another 200W for the CPU, cooling, etc. One training run of LICO on CIFAR-100 orImageNet-S50 takes approximately 10000 seconds in our case. During the reproduction efforts, we estimatethat an equivalent of 100 full runs were performed. Therefore, 0.4kW 10000s 100 3600s/h = 111kWh of power was consumed.According to the onlineresource Nowtricity, in the Netherlands, an equivalent of 421 grams of CO2 is emitted per kWh produced.The reproduction study thus contributed an equivalent of 111421 = 46.7 kilograms of CO2 this is roughlyequal to the CO2-equivalent emissions of 3 livestock cows in the Netherlands within a single day (Koninget al., 2020)."
}