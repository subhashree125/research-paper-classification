{
  "Abstract": "Language model agents (LMA) recently emerged as a promising paradigm on muti-stepdecision making tasks, often outperforming humans and other reinforcement learning agents.Despite the promise, their performance on real-world applications that often involve combi-nations of tasks is still underexplored. In this work, we introduce a new benchmark, calledCompWoB 50 new compositional web automation tasks reecting more realistic assump-tions. We show that while existing prompted LMAs (gpt-3.5-turbo or gpt-4) achieve94.0% average success rate on base tasks, their performance degrades to 24.9% success rateon compositional tasks. On the other hand, transferred LMAs (netuned only on base tasks)show less generalization gap, dropping from 85.4% to 54.8%. By balancing data distributionacross tasks, we train a new model, HTML-T5++, that surpasses human-level performance(95.2%) on MiniWoB, and achieves the best zero-shot performance on CompWoB (61.5%).While these highlight the promise of small-scale netuned and transferred models for taskcompositionality, their performance further degrades under dierent instruction compositionschanging combinational order. In contrast to the recent remarkable success of LMA, ourbenchmark and detailed analysis emphasize the necessity of building LMAs that are robustand generalizable to task compositionality for real-world deployment.",
  "Introduction": "Based on the exceptional capability of large language models (LLMs) (OpenAI, 2023; Anil et al., 2023;Touvron et al., 2023) in commonsense understanding (Brown et al., 2020; Chowdhery et al., 2022), multi-step reasoning (Wei et al., 2022; Kojima et al., 2022), program synthesis (Chen et al., 2021) and self-improvement (Shinn et al., 2023; Madaan et al., 2023; To et al., 2023), language model agents (LMAs) haverecently emerged to tackle various decision making problems, such as robotics (Huang et al., 2022a; Ahnet al., 2022), information retrieval (Nakano et al., 2021; Yao et al., 2022b), and external tool use (Wu et al.,2023; Shen et al., 2023). Especially, web automation (Shi et al., 2017) has attracted attention a lot as anpromising application of LMAs, because LMAs with prompting (Kim et al., 2023; Sun et al., 2023; Zhenget al., 2023) outperform humans and other learning-based agents, such as imitation (Furuta et al., 2023) orreinforcement learning (Humphreys et al., 2022). Despite their human-level prociency in MiniWoB (Shi et al., 2017), a standard web automation benchmark,it is still unclear whether LMAs could deal with challenges in the real world; such as complex observation (Guret al., 2023), domain generalization (Deng et al., 2023), and ambiguity of instructions (Zhou et al., 2023b).These challenges are exacerbated due to the open-ended nature of real-world tasks, making it infeasible for theagent system to prepare exemplars and prompts for any unseen task in advance. Moreover, it is reasonable andpractically important to assume the zero-shot evaluation of the o-the-shelf agents at deployment.This is an orthogonal assumption to the recent progress in LMAs learnable from execution/self-feedback (Shinn",
  "Zero-shotEvaluation": ": (Left) We design CompWoB, as a novel decision making benchmark for LMAs, by leveraging the highcomposability of simulated web environments. We rst select base tasks from the original MiniWoB (Shi et al., 2017)based on the brute-force task complexity averaged among existing LMAs. While considering the feasibility and reality,we systematically combine them into a sequential single task (e.g. click-checkboxes-transfer + enter-password +click-dialog click-checkboxes-transfer_enter-password_click-dialog). The instructions of base tasks arestitched with and then. LMAs are asked to satisfy the given instructions sequentially (e.g. satisfying the successcriteria of task A B C). We also implement reverse-order instruction settings, where the instructions areprovided upside down (e.g. solve task B, and solve task C, after solving task A). These complex yet controllablestrategies make the analysis of LMAs behaviors easy, while maintaining complex and ambiguous aspects of real-worldweb environments. (Right) In this paper, we assume the zero-shot evaluation of the o-the-shelf agents at deployment,which is practically important because (1) it is infeasible to cover all the possible prompts/demonstrations for userrequests in advance and (2) iterative trial-and-error to adjust system prompts hurts user experiences. CompWoBworks as a hold-out test environment accompanying with MiniWoB as a train environment. et al., 2023; Ma et al., 2023) during the deployment. Considering the agents are deployed as a service, (1) itis infeasible to cover all the possible prompts/demonstrations for user requests in advance and (2) iterativetrial-and-error to adjust system prompts hurts user experiences. In this work, we extensively study the transferability of LMAs to more realistic sequential task compositions.We rst design a new controlled test bed, called CompWoB, with 50 compositional tasks by combining a setof base tasks based on their diculty (, right). CompWoB works as a hold-out test environmentaccompanying with MiniWoB as a train environment (, left). Each compositional task is implementedfrom 2 to 8 base tasks in a single-page or multi-page environment with instructions linked together usingsimple connectors such as and then. Only providing the knowledge about base tasks, we investigate thegeneralization performance of existing SoTA prompted LMAs (Kim et al., 2023; Sun et al., 2023; Zhenget al., 2023) with planning, self-improvement, program synthesis, and structured prompts that are supportedby gpt-3.5-turbo and gpt-4. Our ndings indicate that their performance drops signicantly, from 94.0%success on base tasks to 24.9% success on compositional tasks. In contrast, small-scale LMAs netuned onlyon base tasks and zero-shot-transferred to compositional settings (i.e. transferred LMAs), deal with unknowntask compositionality better, achieving 54.8% success rate on average. By rebalancing the data distribution,we train a new model, HTML-T5++, that achieves human-level performance on MiniWoB and performsthe best among all the LMAs on compositional tasks. In contrast, small-scale LMAs netuned only on basetasks and zero-shot-transferred to compositional settings (i.e. transferred LMAs), deal with unknown taskcompositionality better, achieving 54.8% success rate on average. By rebalancing the data distribution, wealso train a new model, HTML-T5++, that achieves human-level performance on MiniWoB and performsthe best among all the LMAs on compositional tasks. We further point out that LMAs struggle to handlecomplex instruction compositions permuting the order of sub-instructions, where prompted agents are morerobust to the dierence in the order of compositions compared to transferred agents (6.9% vs 23.8% drop inperformance). Finally, we illustrate that instruction length and observation complexity are useful indicatorsof compositional task performance. In contrast to the recent notable success of LMAs, our benchmark and detailed analysis highlight buildingrobust and generalizable LMAs to be safely deployed in the real world. In summary, our key contributionsare: We empirically show that (1) prompted LMAs even with gpt-4 suer from generalizing tocompositional web automation tasks much more than transferred LMAs, and (2) LMAs arehighly sensitive to the order of instructions.",
  "We develop CompWoB 1 , simulated web environments for LMAs to measure the generalization to therealistic task compositionality and complex instructions": "We explore a new data mixture heuristic for netuning LMAs, where we nd that it is eective toadd demonstrations of the tasks that suer from data shortage and then gradually reduce the ratio ofeasier tasks to focus more on challenging tasks. HTML-T5++, trained on our novel heuristic, achieveshuman-level performance on MiniWoB (95.2%) and the best zero-shot transfer to CompWoB (61.5%).",
  "Related Works": "Web Automation Although prior works have worked on imitation learning and reinforcement learning (Liuet al., 2018; Gur et al., 2019; Jia et al., 2019; Humphreys et al., 2022), web automation has become a populardomain as an application of LMAs (Gur et al., 2022; Kim et al., 2023). In earlier work, netuned LMAs,based on at most 3-billion parameters, amortize the training costs with the strong prior knowledge on webenvironments (Gur et al., 2022; Furuta et al., 2023; Shaw et al., 2023), but they often result in sub-optimalperformances due to the insucient data coverage. Recently, by leveraging capable private LLMs (Brownet al., 2020; Ouyang et al., 2022) with self-renement (Kim et al., 2023), program synthesis (Sun et al., 2023),structured instruction-state translation (Zheng et al., 2023), or hierarchical prompts (Sridhar et al., 2023;Ma et al., 2023), prompted LMAs with few-shot exemplars have outperformed netuned LMAs and shownsuperior performance to humans and RL-netuned agents. In contrast, our work discusses the transferabilityand robustness of those LMAs in zero-shot web automation with a set of compositional tasks, and resolvesthe sub-optimality of netuned LMAs via data-rebalancing. In addition to MiniWoB (Shi et al., 2017), a representative web simulator, several works have conductedreal-world evaluation (Gur et al., 2023) and proposed novel benchmarks reecting real-world assumptions,such as a simulated e-commerce site (Yao et al., 2022a), sand-boxed real-world websites (Zhou et al., 2023b;Drouin et al., 2024), an adaptive sim-to-real bridge with unsupervised auto-curricula (Gur et al., 2021), andlarge-scale web interaction dataset curated by human annotators (Deng et al., 2023). However, real-worldweb automation may make the analysis challenging because it often faces miscellaneous obstacles, such ascomplex HTML observations, domain gaps between websites, and ambiguous instructions. In this work, wedesign CompWoB under realistic assumptions while controlling task diculty and ambiguity of instructions,and investigate what may prevent the generalization capability of LMAs in compositional tasks. Language Model AgentsBeyond the common NLP tasks, LLMs could act as autonomous agents (Wanget al., 2023b; Qin et al., 2023a) to solve the given instruction-following tasks, by considering the contextin the prompt as states (Ahn et al., 2022; Yao et al., 2022b; Hsieh et al., 2023) and sequentially planningand manipulating external tools or actuators, such as calculators (Parisi et al., 2022), retrievers (Schicket al., 2023; Hao et al., 2023), APIs (Qin et al., 2023b; Tang et al., 2023a), programs (Gao et al., 2023; Wanget al., 2023a; Liang et al., 2023; Song et al., 2023; Cai et al., 2023), robotic commands (Huang et al., 2022a;b;Tang et al., 2023b), computer game (Nottingham et al., 2023; Wang et al., 2023c), or other foundationmodels (Lu et al., 2023; Hsieh et al., 2023; Wu et al., 2023; Shen et al., 2023; Yang et al., 2023). Those priorworks have worked on proposing novel benchmarks (Li et al., 2023; Xu et al., 2023; Patil et al., 2023) andcomparing backbone LLMs (e.g. open-sourced v.s. private) (Ruan et al., 2023; Liu et al., 2023b;a). Despitetheir success, it is still unclear how such LMAs designed for specic tasks can generalize out-of-domainproblems, which should be an important perspective since we may not prepare prompts and exemplars forall the possible problems in the real world. In this work, we measure the transferability and robustness tounseen compositionality and instructions in a realistic web automation scenario. Large Language Models for Compositional TasksSeveral works have investigated compositionalnatural language problems with LLMs, such as semantic parsing (Furrer et al., 2021; Shaw et al., 2021;Zhou et al., 2023a), logic grid puzzles (Dziri et al., 2023), mathematical reasoning (Chen et al., 2023),programming (Zelikman et al., 2023), and planning (Brahman et al., 2023), which shows that dynamical",
  "Published in Transactions on Machine Learning Research (12/2024)": "Long Ouyang, Je Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang,Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller,Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Traininglanguage models to follow instructions with human feedback. arXiv preprint arxiv:2203.02155, 2022.",
  "Preliminaries": "Web automation could be described as a deterministic sequential decision making problem, which consistsof a state space S, action space A, deterministic transition function T : S A S, a set of instructionsG, a set of contextual information (i.e. prompts for LLM) C, and episodic reward function (i.e. successcriteria) r : S G A {0, 1}. At each time step t, the language model agent infers the action conditionedon the prompt, instruction, current state, and previous actions : S AtCG A, and moves to the nextstate: st+1 = T(st, at). When the agent reaches the terminal state (e.g. Login button is clicked) or the maxtime step is exceeded, the episode is marked as a success if the instruction g is satised (i.e. r(st, g, at) = 1).The state st S is a raw HTML, and we assume the programmatic action space: function(selector,text). function is either click, move or type, selector is an integer index or XPath that can uniquelyspecify the element, and text is a text input for type function. Task Compositionality Web automation tasks can be decomposed into a set of primitive base tasks. Forinstance, (1) clicking several checkboxes, (2) fullling the password form, and (3) closing the dialog window.Such a combination could be open-ended and have some dependencies. In this work, we assume that the task is characterized by a corresponding subtree of HTML (S S) and instructions (G G), and can bedependently combined each other as long as the compositional task is executable.",
  "Language Model Agents for Web Automation": "To be self-contained, we here review the representative LMAs, which are selected based on their superiorperformance and novelty in using LLMs for web automation; RCI (.1), AdaPlanner (.2),Synapse (.3). These are characterized with dierent base LLMs, prompting and pipeline. Toclarify their methodological dierence, we provide the pseudo code in Appendix B. Moreover, we resolve thesub-optimal performance of transferred LMAs (.4) with data-rebalanced netuning (.5).",
  "RCI": "The agent with Recursive Criticism and Improvement (RCI) prompting (Kim et al., 2023) rst generates anopen-loop plan to follow a given instruction using few-shot demonstrations. Next, it uses a prompt-basedcritic to identify the errors in the plan and improves it by reecting on self-criticized outputs, which is referredas an explicit RCI (ERCI) loop. After ERCI, the agent follows the self-improved plan step-by-step. Beforeexecuting the action at each step, the agent grounds the action to the current state (i.e. HTML, open-loopplan, and previous actions) and renes its formatting to be parsable, which increases the feasibility andreduces hallucinations. These nal steps are referred as an implicit RCI (IRCI) loop without the self-criticism.All of those play complementary roles to achieve human-level prociency. While 1 ERCI and 3 IRCI loopsare recommended, we observe that the optimal number of self-improvement iterations may dier across thetasks (see Appendix C.1).",
  "AdaPlanner": "In contrast to other LMAs, AdaPlanner (Sun et al., 2023) leverages the capability of program synthesis inLLMs to mitigate the hallucination in a plan. Conditioning on the instruction, the description of permissibleactions in the web environments, and few-shot demonstrations, the agent rst generates an open-loop plan ina Python function, where each snippet corresponds to the action. Once the agent receives environmentalfeedback at each step, such as assertion errors in the code, other functional errors, or ask action to LLMs,it adaptively re-generates the plan for the remaining steps in a closed-loop manner. LLMs more capable ofcode have performed better, such as text-davinci-003 than gpt-3.5-turbo.",
  "RCI (Kim et al., 2023)90.6%AdaPlanner (Sun et al., 2023)92.9%Human93.5%CC-Net (Humphreys et al., 2022)93.5%RCI (gpt-4) (Kim et al., 2023)94.0%Synapse (Zheng et al., 2023)98.5%": ":Average success rate of netunedLMAs in 56 tasks on MiniWoB. Adding 77Kepisodes and reducing redundant thousands ofepisodes, HTML-T5++ achieves competitiveperformance to prompted LMAs, RL-netunedagents, and humans, while improving the suc-cess rate from 85.6% to 95.2%. Synapse (Zheng et al., 2023) argues that LMAs perform betterif well-designed structured prompts are provided, even withoutself-improvement or program synthesis. The structured prompt-ing is formed by two pre-processing strategies: state lteringand task reformulation. State ltering gradually transformsraw HTML into simple formatted text, such as a Pythonic listor dictionary, in a multi-step manner, which may improve thestate understanding of LMAs. Task reformulation translatesgiven instructions or raw HTML into decomposed queries: forinstance, translating select 12/03/2016 as the date and hitsubmit into select the datepicker at step 1, click Prev 7times at step 2-8 (May is 7 months before December), click thedate 12 at step 9, and nally submit at step 10 (translatedinstruction), or mapping proper noun into corresponding XPath(translated HTML). While detailed structured prompts haveled to strong performances, those should be specialized for eachprimitive task in MiniWoB by leveraging 7 dierent types ofreformulation. See Appendix C.3 for further details.",
  "Finetuned and Transferred Language Model Agents": "In addition to the prompted LMAs, LMAs netuned on base tasks have also been developed (Gur et al., 2022;Furuta et al., 2023; Shaw et al., 2023), which are built on pre-trained language models, such as T5 (Raelet al., 2020), Flan-T5 (Chung et al., 2022), HTML-T5 (Gur et al., 2023), or Pix2Struct (Lee et al., 2023),with web automation demonstrations. Those LMAs take HTML (or screenshots) and previous actionsas inputs and predict the text-format next actions in a closed-loop manner. Since pre-trained languagemodels have a sucient inductive bias for web environments and instruction-following, netuned LMAscan data-eciently achieve competitive performance to the RL-netuned agents trained from scratch withdomain-specic architectures (Humphreys et al., 2022; Liu et al., 2018). Compared to the prompted LMAsrelying on private LLM API, it is possible to build on-premise agents based on tractable-size models (at most3 billion parameters), which may reduce inference time and costs. However, prior works have pointed out thatnetuned LMAs struggle to the sub-optimal performance (Zheng et al., 2023) and they require demonstrationson the order of hundreds of thousands while prompted LMAs just need hundreds of episodes (Kim et al.,2023). In this paper, we extensively evaluate such netuned LMAs in zero-shot transfer settings; LMAsare netuned only with base task demonstrations and should deal with unseen compositional tasks. We callthose transferred LMAs in the later sections.",
  "Data-Rebalancing Improves Finetuned Language Model Agents": "Furuta et al. (2023) utilized agent-driven data collection instead of humans to improve the performanceof netuned LMAs further. For each task, 10k demonstrations are collected and ltered based on tasksuccess, which resulted in challenging tasks having much less than 10k demonstrations due to the sub-optimalperformance of LMA on these tasks. We identify that by xing the data-imbalance problem, the performanceof netuned LMAs can be signicantly improved, achieving super-human performance on MiniWoB. We rstrun Synapse (Zheng et al., 2023) on MiniWoB and collect 77K additional demonstrations across 16 tasks ontop of 347K demonstrations (Furuta et al., 2023) to compensate for the lack of data in specic tasks. Wethen estimate the brute-force task diculty averaging success rates for representative web automationagents. Based on those proximal measures, we classify 65 base tasks into three categories, such as easy (0.8 -1.0), medium (0.6 - 0.8), and hard (0.0 - 0.6) (see Appendix D). We then balance the number of episodesbased on the task diculty, where we gradually reduce the ratio of easier tasks to focus more on challengingtasks. For instance, we remove X% episodes from top-k tasks in easy group. We heuristically design thefollowing data-mixing strategies:",
  "See Appendix E for further details. The automation of data mixture design (Xie et al., 2023) would be animportant future direction": "We netune HTML-T5-XL (Gur et al., 2023), a pre-trained language model with local and global attentionin the encoder and a mixture of long-span denoising, on these rebalanced datasets. shows that all thedata-rebalance strategies improve the success rate, and reducing 50% episodes from easy tasks (nally 282Kepisodes in total) is the most eective rebalancing strategy. This suggests that netuned LMAs can be ascapable as prompted LMAs in decision making tasks. Moreover, due to the inherent compositional nature inchallenging web automation tasks, data rebalancing may improve the capability of dealing with unseen taskcompositions. We include HTML-T5++ as a baseline in the later sections.",
  "Designing CompWoB Controlling Task Complexity and Compositionality": "Even though MiniWoB includes a spectrum of simulated environments, they have still focused on narrow andsingle-task instances. We need more advanced environments to measure the generalization to the variousfunctional challenges in real-world web automation, such as complex observation (Deng et al., 2023), instruc-tion (Zhou et al., 2023b), and task compositionality (Gur et al., 2021). We design CompWoB (i.e. compositionalMiniWoB), as a general test bed for LMAs, by leveraging the high composability of simulated web environments(). In CompWoB, we systematically combine several base tasks (from 2 to 8) in the original MiniWoB,which LMAs can already solve, into a single task (e.g. click-checkboxes-transfer + enter-password+ click-dialog click-checkboxes-transfer_enter-password_click-dialog). This allows us to con-trol the complexity of HTML and instructions, ensuring novel tasks are solvable to some extent. Moreover,CompWoB can work as a hold-out test environment accompanying with MiniWoB as a train environment.While tasks are visually simplied, CompWoB reects a broader spectrum of functionalities in real-worldwebsites without sacricing controllability. For instance, the task in sketches the structure of a loginform with agreement checkboxes and a pop-up window, like Internet banking. We also designed tasks withpage transitions, such as from the login form to the email browser.",
  "Details of Task Design": "As mentioned in .5, we rst calculate the average success rates among representative web automationagents as brute-force task complexity, and classify 65 primitive tasks in MiniWoB into 3 categories (easy,medium, hard) based on those scores. The details of classication are described in Appendix D. We randomlyselect base tasks from easy group, lter those combinations by their feasibility and reality, and make 50compositional tasks. We divide those into ve categories: two-way tasks (20), three-way tasks (10), n-way",
  "Number of Tasks": "MiniWoB has around 104 base tasks in total; all the two-way and three-way combinations of these taskswould give 185,460 compositional tasks.It is infeasible to manually curate all the two-way/three-waycombinations.Unfortunately, it is also nontrivial to automate this process due to the locality of theenvironment implementations in MiniWoB. Each environment is mostly self-contained, making it nontrivialto modularize the whole benchmark/codebase so that every combination can be automatically generated.Alternatively, we outline a set of design principles solvability, feasibility, and reality, which we follow tomanually curate 50 compositional tasks and 50 reverse-order instruction tasks that could cover a wide variety ofdiculty, and compositionality. We believe these guiding principles would be applicable to other compositionalgeneralization problems such as robotic navigation. Based on these design principles, our compositionalbenchmark can easily be extended in the future to study even more challenging and compositional webautomation problems. We would also like to highlight that, because previous works were tested around 50- 60 MiniWoB tasks (Kim et al., 2023; Sun et al., 2023; Zheng et al., 2023; Gur et al., 2022; Furuta et al.,2023), 50 compositional tasks is a decent number of tasks to evaluate the agents. Our addition of a hold-outcompositional test set doubles the number of tasks for the community to study.",
  "Inherent Compositionality and Sub-Task Dependency in Web Automation": "While many of the real-world tasks have inherent compositionality to some degree, these tasks are notexplicitly designed for compositionality, making it dicult to systematically investigate the generalizationgap. This can be analyzed with our CompWoB by separating the diculty of the base tasks themselves andthe diculty of the task compositions. Real-world websites have dependencies between sub-tasks. For instance, an email client requires a successfullogin to proceed to writing an email or reading social media posts of a specic user requires navigating to thepersonal page of that user. In CompWoB, we have implemented these kinds of dependencies in the rewardfunction using logical AND operations. For instance, while we allowed agents to continue to write an emailsub-task without successful login, the episode is agged as a failure (i.e. zero reward) even if the write anemail sub-task is successful. We inform agents of these dependencies using connectors in instructions suchas solve B after A or solve A then B. Our analysis in .4 also suggests that, in addition to taskcompositionality, long instructions and deep HTML sub-tree can lead to challenging tasks.",
  "Comparison to Other Web Automation Benchmarks": "We design CompWoB on top of MiniWoB, which enables us to test the human-level LMAs (Kim et al., 2023;Sun et al., 2023; Zheng et al., 2023; Gur et al., 2023) as strong baselines. Because the success of in-domaintasks is ensured, we could focus on the analysis of generalization to unknown task compositions and theirfunctionality in a controllable way. In contrast, other benchmarks copying real websites fail to obtain decentagents (for instance, the episode success rate of Mind2Web (Deng et al., 2023) is around 5-10% at most;around 15% success at most in WebArena (Zhou et al., 2023b)) and it seems to be harder to identify theattribution of errors due to the complex nature of real websites. Moreover, while realistic benchmarks (Denget al., 2023; Zhou et al., 2023b) often provide deterministic instructions and observations, CompWoB hasrandomized instructions generated from a set of templates, as well as element-randomized HTML. For the",
  "Prompted": "(Ave.) Avg. Success Rate (%) 46.3 56.661.5 29.129.734.3 (-17.2)(-26.9)(-27.2) 17.922.525.828.7 15.717.018.119.2 (-2.2)(-5.5)(-7.7)(-9.5)20.618.9 (-1.7)17.815.4 25.4 9.812.116.0 (-8.0)(-3.3)(-9.4) 54.8 31.0(-23.8)24.918.0 (-6.9) CompWoBReverse-Order Instructions :Average success rate of language model agents in reverse-order instruction settings.We usegpt-3.5-turbo as the backbone LLM for prompted LMAs (RCI, AdaPlanner, Synapse), and transferred LMAswith 3-billion parameters. Notably, most LMAs signicantly degrade the success rate when reverse-order instructionsare provided (the performance gap is highlighted with a red number). This trend is more remarkable in transferredLMA (54.8% 31.0% on average) than prompted LMA (24.9% 18.0% on average), which suggests that any kindof LMAs are susceptible to the order of compositional instructions. The capability as general language models mightbe important to parse semantically complex instructions into the correct plan. See Appendix K for further details. In contrast, prompted LMAs degrade their performance drastically; eventhe best RCI with few-shot combination exemplars (comb) just degrades the success rate to 28.7% from90.6% in base MiniWoB. These results indicate that LMAs suer from generalization to task compositionality,and transferred LMAs can relatively deal with that better than prompted LMAs, which is an oppositetrend to base MiniWoB performance. Among prompted LMAs, RCI performs better than AdaPlanner andSynapse, which suggests that multiple iterations of self-criticism and improvement might be more robustto out-of-domain decision making from the exemplars than program synthesis with feedback or structuredprompting with state translations. In the failure episodes (), LMAs often miss necessary steps, common to all the prompted LMAs. Sincethe instructions get long in compositional settings, LMAs may skip important intermediate steps to satisfythe instructions. In addition, they predict incorrect action types and XPath: for instance, hallucination inXPath (RCI) and mixing up click and type action (Synapse). Without oracle exemplars, LMAs often struggleto parse compositional task instructions and understand complex HTML. Appendix H also provides averagesuccess rates per category.",
  "Results": "Evaluation Methodology We evaluate both transferred and prompted LMAs with base MiniWoB demon-strations on the unseen compositional tasks in a zero-shot manner; i.e.we do not provide anydemonstrations on the compositional tasks for the training corpus and exemplars to measurethe generalization. We test 50 compositional tasks and run 100 episodes per task. We adopt gpt-3.5-turboas a backbone LLM, unless otherwise mentioned. We assume the optimal exemplar retriever throughoutexperiments and always provide the pre-dened prompts to LMAs. We borrow hyper-parameters and prompttemplates from respective papers with minimal change to respect our zero-shot transfer setting. As a sanitycheck for the quality of environments and baseline agents, we also test LMAs with oracle demonstrations inAppendix I. RCI We test 4 prompting strategies: (1) zero-shot (without any exemplars), few-shot with (2) rst-taskexemplars, (3) second-task exemplars, and (4) combination exemplars (i.e. both rst and second tasks). Forconsistency and limited context length, we always consider the rst two tasks even if the number of primitivetasks is more than two, and x the number of self-improvement iterations to 1 explicit RCI and 3 implicitRCI as recommended in the original paper. The exemplars we use are provided by Kim et al. (2023). AdaPlanner Following the original code, we use the same exemplars provided by Sun et al. (2023) for taskswhere those base tasks are included, such as enter-text and click-widget (see Appendix C.2). Otherwise,the agents are prompted in a zero-shot manner. Synapse We test 3 prompting strategies: few-shot with (1) rst-task, (2) second-task exemplars, and (3)best exemplars (i.e. maximum score between (1) and (2)). Because prompts and modules are quite dierentamong tasks, we do not merge the prompts and use proper hyper-parameters corresponding to the givenexemplars designed by Zheng et al. (2023).",
  "Language Model Agents Struggle to Handle Task Compositionality": "shows that, in CompWoB, all the LMAs face performance degradation. Among those, transferredLMAs achieve better success rate (54.8%) than prompted LMAs (24.9%) on average. In particular, HTML-T5++ achieves the best generalization, suppressing the performance drop from 95.2% to 61.5%. This mightbe because netuned LLMs are better at natural language compositional tasks prompted LLMs in general.",
  "Reverse-Order Instructions Degrade Language Model Agents": "As shown in , all the LMAs signicantly degrade the success rate when reverse-order instructions areprovided. This trend is more remarkable in transferred LMAs dropping from 54.8% to 31.0% than promptedLMAs from 24.9% to 18.0% on average, which suggests that any kind of LMAs is susceptible to the order ofcompositional instructions and that transferred LMAs may not generalize well to diverse instructions beyondthe dataset distribution. As opposed to .1, the performance dierences among prompted LMAs aremarginal, which, however, implies that existing prompting methods, even with self-improvement, may nothandle complex task instructions enough. The stronger capability as general-purpose language models orother prompting methods might be important to parse semantically complex instructions into the executablesequential plan.",
  ". click //button[@id=\"subbtn1\"]1. type //button[@id=\"subbtn2\"]1. click //*[@id=\"password\"]1. click //*[text()=\"yE\"]/input": "2. click //button[@id=\"subbtn2\"]2. click //*[text()=\"whX\"]/input2. type UBKR1. type UBKR2. click //input[@id=\"tt\"]1. click //input[@id=\"tt\"]3. click //*[text()=\"whX\"]/input3. click //*[text()=\"1Nk\"]/input3. click //*[@id=\"verify\"]2. type yE 4. click //*[text()=\"1Nk\"]/input4. click //*[text()=\"fUK3\"]/input4. type UBKR2. type UBKR3. type Juan3. type Juan5. click //*[text()=\"fUK3\"]/input5. click //*[text()=\"gSm\"]/input5. click //input[@id=\"ch0\"]3. click //input[@id=\"ch0\"]4. click //*[@id=\"subbtn\"]4. click //*[@id=\"subbtn\"]6. click //*[@id=\"subbtn\"]6. click //*[@id=\"subbtn\"]6. click //*[@id=\"subbtn\"]4. click //*[@id=\"subbtn\"]",
  "Do Advanced LLMs Solve Compositional Tasks?": "presents the results when we adopt other advanced LLMs, than gpt-3.5-turbo, as a backboneof each LMA. The more capable models, such as gpt-4 in a generalist aspect and text-davinci-003 ina code generation, can improve the success rate of all the prompted LMAs. However, even gpt-4 is stillfar from the generalization in compositional tasks (from 28.7% to 56.0% by RCI) or from dealing withreverse-order instructions (from 19.2% to 43.5% by RCI). This indicates that we need much better LLMsto realize deployable systems in complex real-world decision-making tasks. We provide failure examples inAppendix G.",
  "What Determines Task Complexity on the Web?": "visualizes the correlation between the success rate averaged across WebGUM, HTML-T5, RCI,AdaPlanner, and Synapse (y-axis) and each statistic of compositional tasks (x-axis), such as synthesizedsuccess rate a product of base task success rates among compositional tasks the number of instructiontokens, and max depth of HTML subtrees. Synthesized success rate positively correlates with an averagesuccess rate (R = 0.691), indicating that compositional task diculty takes over base task diculties. Inaddition, the number of instruction tokens (R = 0.579) and the max depth of HTML subtrees (R = 0.433)show negative correlations. All those are statistically signicant in paired t-test with p < 0.01. In contrast,other task statistics, such as synthesized success rate with human performance, the number of HTML tokens,and elements in HTML, just show relatively weaker correlations (see Appendix F for the details). Thisanalysis suggests that HTML with larger depth and long instructions make generalizing compositional taskschallenging. The complexity of HTML is determined by its depth rather than its length or the number of",
  "Instruction Tokens": "0.0 0.2 0.4 0.6 0.8 R =0.579, p < 0.01 3.03.54.04.55.05.56.06.57.0 Max HTML Subtree Depth 0.0 0.2 0.4 0.6 0.8 R =0.433, p < 0.01 : 2D-scatter plots between success rate averaged among LMAs (y-axis) and each statistic of compositionaltask (x-axis), such as success rate synthesized with a product of base task success rate, the number of instructiontokens, and max depth of HTML subtrees. Synthesized success rate positively correlates with an average success rate(R = 0.691, statistically signicant in paired t-test with p < 0.01), indicating that base task diculty may determinecompositional task diculty. In addition, the number of instruction tokens (R = 0.579; p < 0.01) and the maxdepth of HTML subtrees (R = 0.433; p < 0.01) show negative correlations, which suggests the high complexity ofobservation and long instructions make the compositional tasks hard to resolve. elements. This might come from the hierarchical nature of HTML; in deeper HTML subtrees, the elementsnear the root tend to be distant from each other after the traversal. Such sparsity may cause confusion duringplanning. We also report the individual characteristics of each agent in Appendix J.",
  "Discussion": "Generalizable Prompting MethodsThe results of Synapse and RCI in imply that thoseprompted LMAs have over-tting trends to the base MiniWoB tasks. While the robustness across theprompts has been investigated in natural language tasks (Wei et al., 2022; Kojima et al., 2022), it is not wellunderstood in decision making. Because we will not be able to prepare the optimal self-improvement iterationsor decomposed prompts for all the possible instructions and task compositions, even if using optimal exemplarretrievers, we should care more about the generalization of prompting methods for the agent systems. Agent-Specialized Large Language Models As shown in , the more capable LLMs, such asgpt-4, can improve the performance of LMAs in CompWoB. However, it has not reached the base MiniWoByet (e.g. from 90.6% to 56.0% in RCI, and from 98.5% to 41.4% in Synapse). Similarly, as described in.4, transferred LMAs can perform better if the training dataset has a good balance and coverage,but it is far from sucient generalization to compositionality and instructions. The current pre-trained LLMsmay still not be sucient to generalize to complex decision making tasks, and then, in addition to promptingmethods, the development of agent-specialized LLMs with enhanced reasoning and generalization throughthe instruction-tuning (Zeng et al., 2023) or RLHF/AIF (Furuta et al., 2024) would be expected. Parsing Complex Instructions to Executable Plan .2 highlights that LMAs are fragile whenwe increase the complexity of instruction even by the most straightforward reverse-order instructions. Thismay not be preferable for the real-world application since the instructions might not be easy-to-parse and theusers should carefully and concisely tell what they would like to do, which hinders the users experience. Itwould be an interesting future direction to investigate better planning modules that could parse complexinstructions to correct and executable plans.",
  "Conclusion": "The robustness and generalization of LMAs are important aspects for real-world deployment. We extensivelyexamine how much existing LMAs, via transferring and prompting, can deal with a set of compositional webautomation environments, CompWoB, that consists of easily-resolvable base primitive tasks. Our evaluationimplies the contrary conclusion to the prior works (); the prompted LMAs are strong solver forprimitive web automation tasks but signicantly drop their performance in unknown task compositionality.The transferred LMAs often show sub-optimal performance in basic tasks but can deal with compositionalproblems much better. Our detailed analysis also highlights that LMAs also face catastrophic degradationwhen they receive complex, even in the simplest reversed-order instructions, and that the challenges incompositional tasks might come from instruction length and the depth of HTML subtree. We hope this",
  "Ethics Statements": "This paper systematically evaluates the performance of existing language model agents for web automation inrealistic compositional tasks, and unveils remaining challenges towards broader generalization and real-worlddeployments. This technique could realize capable AI assistant tools on digital devices (e.g. computers orsmartphones), and improve productivity and accessibility for society. While we anticipate the positive aspects of autonomous agents, for responsible development, we should alsoconsider the potential harmful applications and unintended consequences. The misuse of web automationwould threaten cyber security, and the users may get scammed. To reduce these risks, it is essential forresearchers, policymakers, and industry to discuss concrete guidelines and regulations.",
  "We thank Mustafa Safdari, Yingjie Miao, Yusuke Iwasawa, Heiga Zen, and Doina Precup for the help on thiswork. HF was supported by JSPS KAKENHI Grant Number JP22J21582": "Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn,Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Daniel Ho, Jasmine Hsu, JulianIbarz, Brian Ichter, Alex Irpan, Eric Jang, Rosario Jauregui Ruano, Kyle Jerey, Sally Jesmonth, Nikhil JJoshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Kuang-Huei Lee, Sergey Levine, Yao Lu, LindaLuu, Carolina Parada, Peter Pastor, Jornell Quiambao, Kanishka Rao, Jarek Rettinghouse, Diego Reyes,Pierre Sermanet, Nicolas Sievers, Clayton Tan, Alexander Toshev, Vincent Vanhoucke, Fei Xia, Ted Xiao,Peng Xu, Sichun Xu, Mengyuan Yan, and Andy Zeng. Do as i can, not as i say: Grounding language inrobotic aordances. arXiv preprint arxiv:2204.01691, 2022. Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, SiamakShakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, Eric Chu, Jonathan H. Clark, Laurent El Shafey,Yanping Huang, Kathy Meier-Hellstern, Gaurav Mishra, Erica Moreira, Mark Omernick, Kevin Robinson,Sebastian Ruder, Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang, Gustavo Hernandez Abrego, JunwhanAhn, Jacob Austin, Paul Barham, Jan Botha, James Bradbury, Siddhartha Brahma, Kevin Brooks, MicheleCatasta, Yong Cheng, Colin Cherry, Christopher A. Choquette-Choo, Aakanksha Chowdhery, ClmentCrepy, Shachi Dave, Mostafa Dehghani, Sunipa Dev, Jacob Devlin, Mark Daz, Nan Du, Ethan Dyer,Vlad Feinberg, Fangxiaoyu Feng, Vlad Fienber, Markus Freitag, Xavier Garcia, Sebastian Gehrmann,Lucas Gonzalez, Guy Gur-Ari, Steven Hand, Hadi Hashemi, Le Hou, Joshua Howland, Andrea Hu, JereyHui, Jeremy Hurwitz, Michael Isard, Abe Ittycheriah, Matthew Jagielski, Wenhao Jia, Kathleen Kenealy,Maxim Krikun, Sneha Kudugunta, Chang Lan, Katherine Lee, Benjamin Lee, Eric Li, Music Li, WeiLi, YaGuang Li, Jian Li, Hyeontaek Lim, Hanzhao Lin, Zhongtao Liu, Frederick Liu, Marcello Maggioni,Aroma Mahendru, Joshua Maynez, Vedant Misra, Maysam Moussalem, Zachary Nado, John Nham, EricNi, Andrew Nystrom, Alicia Parrish, Marie Pellat, Martin Polacek, Alex Polozov, Reiner Pope, SiyuanQiao, Emily Reif, Bryan Richter, Parker Riley, Alex Castro Ros, Aurko Roy, Brennan Saeta, RajkumarSamuel, Renee Shelby, Ambrose Slone, Daniel Smilkov, David R. So, Daniel Sohn, Simon Tokumine, DashaValter, Vijay Vasudevan, Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, Tao Wang, JohnWieting, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Qiao Zhang, Steven",
  "Zheng, Ce Zheng, Weikang Zhou, Denny Zhou, Slav Petrov, and Yonghui Wu. Palm 2 technical report.arXiv preprint arXiv:2305.10403, 2023": "Faeze Brahman, Chandra Bhagavatula, Valentina Pyatkin, Jena D. Hwang, Xiang Lorraine Li, Hirona J.Arai, Soumya Sanyal, Keisuke Sakaguchi, Xiang Ren, and Yejin Choi. Plasma: Making small languagemodels better procedural knowledge models for (counterfactual) planning, 2023. Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, GretchenKrueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jerey Wu, Clemens Winter,Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark,Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language modelsare few-shot learners. arXiv preprint arXiv:2005.14165, 2020.",
  "Tianle Cai, Xuezhi Wang, Tengyu Ma, Xinyun Chen, and Denny Zhou. Large language models as tool makers.arXiv preprint arXiv:2305.17126, 2023": "Jiaao Chen, Xiaoman Pan, Dian Yu, Kaiqiang Song, Xiaoyang Wang, Dong Yu, and Jianshu Chen. Skills-in-context prompting: Unlocking compositionality in large language models. arXiv preprint arXiv:2308.00304,2023. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan,Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger,Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder,Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet,Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, ArielHerbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin,Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, JoshAchiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati,Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and WojciechZaremba. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts,Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi,Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, VinodkumarPrabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, MichaelIsard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, HenrykMichalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito,David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, ShivaniAgrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, AitorLewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang,Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck,Je Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling language modeling with pathways. arXiv preprintarXiv:2204.02311, 2022. Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang,Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun,Xinyun Chen, Aakanksha Chowdhery, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, YanpingHuang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Je Dean, Jacob Devlin, Adam Roberts,Denny Zhou, Quoc V. Le, and Jason Wei. Scaling instruction-netuned language models. arXiv preprintarxiv:2210.11416, 2022.",
  "Workarena: How capable are web agents at solving common knowledge work tasks?arXiv preprintarxiv:2403.07718, 2024": "Andrew Drozdov, Nathanael Schrli, Ekin Akyrek, Nathan Scales, Xinying Song, Xinyun Chen, OlivierBousquet, and Denny Zhou. Compositional semantic parsing with large language models. In InternationalConference on Learning Representations, 2023. Nouha Dziri, Ximing Lu, Melanie Sclar, Xiang Lorraine Li, Liwei Jiang, Bill Yuchen Lin, Peter West, ChandraBhagavatula, Ronan Le Bras, Jena D. Hwang, Soumya Sanyal, Sean Welleck, Xiang Ren, Allyson Ettinger,Zaid Harchaoui, and Yejin Choi. Faith and fate: Limits of transformers on compositionality. arXiv preprintarxiv:2305.18654, 2023.",
  "Daniel Furrer, Marc van Zee, Nathan Scales, and Nathanael Schrli. Compositional generalization in semanticparsing: Pre-training vs. specialized architectures. arXiv preprint arXiv:2007.08970, 2021": "Hiroki Furuta, Tatsuya Matsushima, Tadashi Kozuno, Yutaka Matsuo, Sergey Levine, Or Nachum, andShixiang Shane Gu. Policy information capacity: Information-theoretic measure for task complexity indeep reinforcement learning. In International Conference on Machine Learning, 2021. Hiroki Furuta, Or Nachum, Kuang-Huei Lee, Yutaka Matsuo, Shixiang Shane Gu, and Izzeddin Gur.Multimodal web navigation with instruction-netuned foundation models. arXiv preprint arxiv:2305.11854,2023. Hiroki Furuta, Kuang-Huei Lee, Shixiang Shane Gu, Yutaka Matsuo, Aleksandra Faust, Heiga Zen, andIzzeddin Gur. Geometric-averaged preference optimization for soft preference labels. arXiv preprintarxiv:2409.06691, 2024.",
  "Izzeddin Gur, Ulrich Rueckert, Aleksandra Faust, and Dilek Hakkani-Tur. Learning to navigate the web. InInternational Conference on Learning Representations, 2019": "Izzeddin Gur, Natasha Jaques, Yingjie Miao, Jongwook Choi, Manoj Tiwari, Honglak Lee, and AleksandraFaust. Environment generation for zero-shot compositional reinforcement learning. In Advances in neuralinformation processing systems, 2021. Izzeddin Gur, Or Nachum, Yingjie Miao, Mustafa Safdari, Austin Huang, Aakanksha Chowdhery, SharanNarang, Noah Fiedel, and Aleksandra Faust. Understanding html with large language models. arXivpreprint arxiv:2210.03945, 2022. Izzeddin Gur, Hiroki Furuta, Austin Huang, Mustafa Safdari, Yutaka Matsuo, Douglas Eck, and AleksandraFaust. A real-world webagent with planning, long context understanding, and program synthesis. arXivpreprint arxiv:2307.12856, 2023.",
  "Evan Zheran Liu, Kelvin Guu, Panupong Pasupat, and Percy Liang. Reinforcement learning on web interfacesusing workow-guided exploration. In International Conference on Learning Representations, 2018": "Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, KaiwenMen, Kejuan Yang, Shudan Zhang, Xiang Deng, Aohan Zeng, Zhengxiao Du, Chenhui Zhang, Sheng Shen,Tianjun Zhang, Yu Su, Huan Sun, Minlie Huang, Yuxiao Dong, and Jie Tang. Agentbench: Evaluatingllms as agents. arXiv preprint arxiv:2308.03688, 2023a. Zhiwei Liu, Weiran Yao, Jianguo Zhang, Le Xue, Shelby Heinecke, Rithesh Murthy, Yihao Feng, ZeyuanChen, Juan Carlos Niebles, Devansh Arpit, Ran Xu, Phil Mui, Huan Wang, Caiming Xiong, and SilvioSavarese. Bolaa: Benchmarking and orchestrating llm-augmented autonomous agents. arXiv preprintarXiv:2308.05960, 2023b. Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, andJianfeng Gao. Chameleon: Plug-and-play compositional reasoning with large language models. arXivpreprint arXiv:2304.09842, 2023.",
  "Kaixin Ma, Hongming Zhang, Hongwei Wang, Xiaoman Pan, and Dong Yu. Laser: Llm agent with state-spaceexploration for web navigation. arXiv preprint arxiv:2309.08172, 2023": "Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegree, Uri Alon, NouhaDziri, Shrimai Prabhumoye, Yiming Yang, Sean Welleck, Bodhisattwa Prasad Majumder, Shashank Gupta,Amir Yazdanbakhsh, and Peter Clark. Self-rene: Iterative renement with self-feedback. arXiv preprintarxiv:2303.17651, 2023. Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Je Wu, Long Ouyang, Christina Kim, Christopher Hesse,Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou, GretchenKrueger, Kevin Button, Matthew Knight, Benjamin Chess, and John Schulman. Webgpt: Browser-assistedquestion-answering with human feedback. arXiv preprint arXiv:2112.09332, 2021. Kolby Nottingham, Prithviraj Ammanabrolu, Alane Suhr, Yejin Choi, Hannaneh Hajishirzi, Sameer Singh,and Roy Fox. Do embodied agents dream of pixelated sheep: Embodied decision making using languageguided world modelling. In International Conference on Machine Learning, 2023.",
  "Shishir G. Patil, Tianjun Zhang, Xin Wang, and Joseph E. Gonzalez. Gorilla: Large language model connectedwith massive apis. arXiv preprint arXiv:2305.15334, 2023": "Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu Cui, Zheni Zeng, Yufei Huang, ChaojunXiao, Chi Han, Yi Ren Fung, Yusheng Su, Huadong Wang, Cheng Qian, Runchu Tian, Kunlun Zhu, ShihaoLiang, Xingyu Shen, Bokai Xu, Zhen Zhang, Yining Ye, Bowen Li, Ziwei Tang, Jing Yi, Yuzhang Zhu,Zhenning Dai, Lan Yan, Xin Cong, Yaxi Lu, Weilin Zhao, Yuxiang Huang, Junxi Yan, Xu Han, Xian Sun,Dahai Li, Jason Phang, Cheng Yang, Tongshuang Wu, Heng Ji, Zhiyuan Liu, and Maosong Sun. Toollearning with foundation models. arXiv preprint arXiv:2304.08354, 2023a. Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang,Bill Qian, Sihan Zhao, Runchu Tian, Ruobing Xie, Jie Zhou, Mark Gerstein, Dahai Li, Zhiyuan Liu, andMaosong Sun. Toolllm: Facilitating large language models to master 16000+ real-world apis. arXiv preprintarXiv:2307.16789, 2023b. Linlu Qiu, Peter Shaw, Panupong Pasupat, Tianze Shi, Jonathan Herzig, Emily Pitler, Fei Sha, and KristinaToutanova. Evaluating the impact of model scale for compositional generalization in semantic parsing.arXiv preprint arXiv:2205.12253, 2022. Colin Rael, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou,Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unied text-to-text transformer.Journal of Machine Learning Research, 21(140):167, 2020. Jingqing Ruan, Yihong Chen, Bin Zhang, Zhiwei Xu, Tianpeng Bao, Guoqing Du, Shiwei Shi, Hangyu Mao,Xingyu Zeng, and Rui Zhao. Tptu: Task planning and tool usage of large language model-based ai agents.arXiv preprint arXiv:2308.03427, 2023. Timo Schick, Jane Dwivedi-Yu, Roberto Dess, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, NicolaCancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. arXivpreprint arXiv:2302.04761, 2023. Peter Shaw, Ming-Wei Chang, Panupong Pasupat, and Kristina Toutanova. Compositional generalization andnatural language variation: Can a semantic parsing approach handle both? arXiv preprint arXiv:2010.12725,2021. Peter Shaw, Mandar Joshi, James Cohan, Jonathan Berant, Panupong Pasupat, Hexiang Hu, UrvashiKhandelwal, Kenton Lee, and Kristina Toutanova. From pixels to ui actions: Learning to follow instructionsvia graphical user interfaces. arXiv preprint arXiv:2306.00245, 2023.",
  "Hung Quoc To, Nghi D. Q. Bui, Jin Guo, and Tien N. Nguyen. Better language models of code throughself-improvement. arXiv preprint arxiv:2304.01228, 2023": "Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothe Lacroix,Baptiste Rozire, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, EdouardGrave, and Guillaume Lample. Llama: Open and ecient foundation language models. arXiv preprintarxiv:2302.13971, 2023. Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and AnimaAnandkumar. Voyager: An open-ended embodied agent with large language models. arXiv preprintarXiv:2305.16291, 2023a. Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang,Xu Chen, Yankai Lin, Wayne Xin Zhao, Zhewei Wei, and Ji-Rong Wen. A survey on large language modelbased autonomous agents. arXiv preprint arXiv:2308.11432, 2023b. Zihao Wang, Shaofei Cai, Anji Liu, Xiaojian Ma, and Yitao Liang. Describe, explain, plan and select:Interactive planning with large language models enables open-world multi-task agents. In InternationalConference on Machine Learning, 2023c. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, andDenny Zhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprintarXiv:2201.11903, 2022.",
  "Longtao Zheng, Rundong Wang, and Bo An. Synapse: Leveraging few-shot exemplars for human-levelcomputer control. arXiv preprint arXiv:2306.07863, 2023": "Denny Zhou, Nathanael Schrli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, ClaireCui, Olivier Bousquet, Quoc V Le, and Ed H. Chi. Least-to-most prompting enables complex reasoning inlarge language models. In International Conference on Learning Representations, 2023a. Shuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, YonatanBisk, Daniel Fried, Uri Alon, and Graham Neubig. Webarena: A realistic web environment for buildingautonomous agents. arXiv preprint arxiv:2307.13854, 2023b.",
  "BPseudo Code for Prompted Language Model Agents": "We explicitly distinguish LLM itself and LMA (combination of LLM, prompting and pipeline) in this paper.We provide the pseudo code for prompted LMAs to clarify their methodological dierence. RCI (Kim et al.,2023) is the rst to use prompting in a self-renement loop, outperforming imitation- or reinforcement-learnedagents on MiniWoB benchmark that requires millions of demonstrations to work. AdaPlanner (Sun et al.,2023) and Synapse (Zheng et al., 2023) are the follow-up works outperforming RCI via code generation fromenvironmental feedback or via well-designed decomposed prompts with retrieval.",
  "C.1RCI": "As we described in .1, RCI has two important hyper-parameters to control the number of self-improvement iterations. In Explicit RCI (ERCI) loop, LLMs criticize their own generated plans to identifythe problem and then improve it, reecting self-criticism. In Implicit RCI (IRCI) loop, LLMs ground theaction to the current state (i.e. HTML) and rene its formatting to be parsable without self-criticism, whichmay reduce hallucinations or tiny errors. We here test how many self-improvement loops RCI requires (IRCI:1-4, ERCI: 0-2). shows that the optimal number of inference loops is dierent among tasks, while therecommendations are ERCI = 1 and IRCI = 3. These two hyper-parameters might need to be adjusted foreach task.",
  "As we explained in .3, Synapse has several hyper-parameters to construct optimal structured promptsper task to specify whether LLMs translate the instruction or HTML": "summarizes the type of reformulation into 7 categories and claries which transformed inputs are usedfor predicting open-loop plans. For instance, Task only requires translated instructions (and few-shot planningexemplars), although Obs takes raw instruction, HTML, and translated HTML as inputs. For the tasks thatrequire temporal abstraction, it also employs state-conditional decomposition, which factorizes demonstrationsinto a set of exemplars conditioned on the environmental states, and can reduce error accumulation over thetime step. provides the detailed values for state-ltering and task reformulation, which is quite dierent acrossthe tasks. These well-designed structured prompts could be the source of the best performance in baseMiniWoB. However, in compositional settings, it is challenging to modify them for any combinations. Instead,we assume the optimal retriever always picks up the exemplars for one of the base tasks, and we compute themaximum score among the results with given prompts.",
  "DRanking Base MiniWoB Tasks": "To ensure the solvability of CompWoB to some extent and to identify the data-redundant tasks for netunedLMAs, we estimate the brute-force task diculty (Furuta et al., 2021) (). We compute the averagesuccess rate for each task across representative previous web automation agents, such as CC-Net (SL,SL+RL) (Humphreys et al., 2022), WGE (Liu et al., 2018), WebN-T5 (Gur et al., 2022), WebGUM (Furutaet al., 2023), HTML-T5 (Gur et al., 2023), RCI (Kim et al., 2023), AdaPlanner (Sun et al., 2023), Pix2Act(BC, RL) (Shaw et al., 2023), and Synapse (Zheng et al., 2023). Based on those proxy diculty measures, weclassify 65 tasks into three categories (Kim et al., 2023): easy (from 0.8 to 1.0), medium (from 0.6 to 0.8),and hard (from 0.0 to 0.6).",
  "EDetails of MiniWoB Dataset": "To resolve the data-imbalance problem, we rst run Synapse (Zheng et al., 2023) on MiniWoB and collect 77Kadditional demonstrations across 16 tasks on top of 347K demonstrations (Furuta et al., 2023) to compensatefor the lack of data in specic tasks (Strategy A). We use PaLM 2-L (Anil et al., 2023) as a backbone LLMfor Synapse. We then reduce the number of demonstrations for the tasks the agents can solve to focus onmore challenging tasks. Based on brute-force task complexity (Appendix D), we consider the following fourstrategies as shown in :",
  "Removing 80% episodes from top-15 easy tasks and removing 50% episodes from other 11 easy tasks(Strategy E; -183K)": "We hold out 21K episodes (5% of 347K + 77K = 424K) as a validation split, and after the convergence, wechoose the top-5 checkpoints that achieve higher oine validation accuracy, run those checkpoints online onMiniWoB benchmarks, and then report the best success rate. Through the empirical evaluations (.4),we nd that Strategy D realizes a well-balanced dataset to improve the performance. Except for thesedataset mixtures, we follow Gur et al. (2023) for other training hyper-parameters of HTML-T5++. We haveused cloud TPU-v3, which has a 32 GiB HBM memory space, with 128 cores. Each netuning experimenttakes about 1-2 days.",
  "Average Success Rate": "R =0.369, p < 0.01 3.03.54.04.55.05.56.06.57.0 Max HTML Subtree Depth 0.0 0.2 0.4 0.6 0.8 1.0 R =0.236, p = 0.100 Number of Elements in HTML 0.0 0.2 0.4 0.6 0.8 1.0 R =0.194, p = 0.177 HTML Tokens 0.0 0.2 0.4 0.6 0.8 1.0 R =0.183, p = 0.202 HTML-T5+ +RCIAdaPlannerSynapse : 2D-scatter plots between the success rate for each LMA (y-axis) and each statistic of compositional task(x-axis), such as the number of instruction tokens, max depth of HTML subtrees, the number of elements in HTML,and the number of HTML tokens. The results imply that while all the language model agents (HTML-T5++, RCI,AdaPlanner, Synapse) show negative correlations between the success rate and instruction tokens with statisticalsignicance, the trends for other statistics may dier among the methods.",
  "GFailure Examples with Advanced Language Models": "We provide several failure episodes with advanced language models such as gpt-4 and text-davinci-003 in. The left columns have correct plans, and the right columns have failure plans. Qualitatively, LMAsbased on advanced models manage to reduce the ratio of failure modes in gpt-3.5-turbo (), such asskipping necessary intermediate steps and hallucination of unnecessary actions. However, they tend to suerfrom tiny errors more such as capitalization (RCI, AdaPlanner) or attributes in HTML (Synapse).",
  "IPrompted Language Model Agents with Oracle Exemplars": "In this paper, we assume, as a realistic and important constraint, that: (1) prompted or netuned LMAsare deployed as a service in the real world; (2) users are only allowed to interact with theagents via providing task instruction, and (3) not allowed to intervene the backend system orprompts. Providing exemplars or preparing netuning demonstrations for every compositional problemis infeasible given the huge space of web automation problems. Moreover, it signicantly hinders the userexperience that the agents would be good at some specic format of instructions and that their violation ofthose aects the performance even if semantically the same. As a sanity check of the environments and baseline agents, we here demonstrate that prompted LMAscan change their behaviors adaptively by modifying their prompts and demonstrations. We evaluate theperformance of RCI with gpt-4 and oracle exemplars on 20 two-way tasks in CompWoB. We providetwo demonstrations per task in the prompts. shows that RCI with gpt-4 and oracle exemplarsachieves 82.9% success rate, which is the best among the baselines, such as HTML-T5++ (73.9%), RCI withgpt-3.5-turbo and oracle exemplars (56.8%), RCI with combination exemplars (gpt-3.5-turbo: 46.9%,gpt-4: 71.5%). See for other baselines. This ensures that the tasks are feasible, and that if aprompt includes how to perform on compositional tasks, the performance gets better. In contrast, whileoracle demonstrations help improve performance, they could not fully resolve the issues from reverse-orderinstructions. Through the experiments in , we have intended to shed light on their zero-shot generalizationcapabilities of dealing with unknown sequential-task compositions. For reliable and robust web automationagents, we might need to leverage both prompting and netuning approaches at the dierent developmentstages. For instance, prompted LMAs might work well as adaptive automated data collectors to the noveldomains and netuned LMAs as deployed agents for service. HTML-T5+ +RCI (comb)RCI (oracle)",
  "Reverse-Order": "gpt-3.5-turbogpt-4w/ oracle exemplars : Average success rate of LMAs in 20 two-way tasks from CompWoB. RCI with gpt-4 achieves the bestperformance when the oracle exemplars are provided (82.9%) in the prompt. While oracle demonstrations help improveperformance, they could not fully resolve the issues from reverse-order instructions (for instance, 82.9% 71.8% inRCI with gpt-4).",
  "JTask Complexity Analysis with Language Model Agents": "Following the recent work in deep reinforcement learning literature (Furuta et al., 2021), we measure averageperformance as a proxy of oracle task solvability. If many kinds of agents perform poorly, such tasks areregarded as challenging. This can shed light ontask or environment themselves, rather than languagemodel agents or prompting methods, while such an analysis has been overlooked so far. Additionally,while the trend with average might be the same, distributional characteristics for each language model agentcould be dierent. We extend our analysis by reporting the individual performances of each agent in .Our results still indicate that while all the language model agents (HTML-T5++, RCI, AdaPlanner, Synapse)often show negative correlations between the success rate and instruction tokens or max subtree depth withstatistical signicance, the trends for other statistics may dier among the methods. 0.0 0.2 0.4 0.6 0.8 1.0",
  "KFinetuning May Generally Outperform Prompting in Compositional Tasks": "In this section, we examine the capability of netuned LLMs and prompted LLMs for compositionalgeneralization in natural language reasoning tasks, rather than agentic tasks.We employ Last LetterConcatenation (Wei et al., 2022; Zhou et al., 2023a) as a benchmark, where if LLMs take hans, structureas a question, LLMs are asked to answer it as The last letter of hans is s. The last letter of structure ise. Considering s, e leads to se. So, hans, structure outputs se. This reasoning problem exhibits acompositional nature as we increase the number of words provided as a question. For netuned LLMs, weadopt 11B-parameter Flan-T5-XXL (Chung et al., 2022; Rael et al., 2020) as a base model, and prepare2000 examples per n-letter for a training dataset. For prompted LLMs, we employ Flan-PaLM-8B, 62B, and540B (Chung et al., 2022; Chowdhery et al., 2022), and prepare randomized 4-shot CoT prompts from thesame training dataset. We randomly use one of them during inference. Furthermore, we use up to 8 letters asa training dataset and few-shot exemplars and also use 9-12 letters as a hold-out test dataset to investigatethe compositional generalization in natural language reasoning tasks. shows that netuned LLM (Flan-T5-XXL) almost always outperforms prompted LLM (Flan-U-PaLM-540B) on average (76% v.s. 44% in an exact match) and also in both in-distribution (2-8 letters)and out-of-distribution (9-12 letters) settings. This implies that netuned LLMs may generally outperformprompted LLMs in compositional tasks, which can also lead to the better performance of netuned LMAsthan prompted LMAs in sequential web automation tasks, as shown in the main text.",
  "Flan-T5-XXL (11B; Finetuned)1.000.980.990.980.980.960.960.830.470.200.050.76": ": The performance of netuning (Flan-T5-XXL (Chung et al., 2022; Rael et al., 2020), 11B parameters) andprompting (Flan-PaLM-8B/62B/540B (Chung et al., 2022; Chowdhery et al., 2022)) in Last Letter Concatenation (Weiet al., 2022; Zhou et al., 2023a). We measure the performance with an exact match. We use up to 8 letters as a trainingdataset and few-shot exemplars and also use 9-12 letters as a hold-out test dataset to investigate the compositionalgeneralization in natural language reasoning tasks. The results show that netuned LLM (Flan-T5-XXL) almostalways outperforms prompted LLM (Flan-U-PaLM-540B) in both in-distribution (2-8 letters) and out-of-distribution(9-12 letters) settings."
}