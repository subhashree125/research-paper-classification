{
  "Abstract": "Fourier Neural Operators (FNO) offer a principled approach to solving challenging partialdifferential equations (PDE) such as turbulent flows. At the core of FNO is a spectral layerthat leverages a discretization-convergent representation in the Fourier domain, and learnsweights over a fixed set of frequencies.However, training FNO presents two significantchallenges, particularly in large-scale, high-resolution applications: (i) Computing Fouriertransform on high-resolution inputs is computationally intensive but necessary since fine-scale details are needed for solving many PDEs, such as fluid flows, (ii) selecting the relevantset of frequencies in the spectral layers is challenging, and too many modes can lead tooverfitting, while too few can lead to underfitting. To address these issues, we introducethe Incremental Fourier Neural Operator (iFNO), which progressively increases both thenumber of frequency modes used by the model as well as the resolution of the training data.We empirically show that iFNO reduces total training time while maintaining or improvinggeneralization performance across various datasets. Our method demonstrates a 38% lowertesting error, using 20% fewer frequency modes compared to the existing FNO, while alsoachieving up to 46% faster training and a 2.8x reduction in model size.",
  "Introduction": "Recently, deep learning has shown promise in solving partial differential equations (PDEs) significantlyfaster than traditional numerical methods in many domains. Among them, Fourier neural operator (FNO),proposed by Li et al. (a), is a family of neural operators that achieves state-of-the-art performance in solvingPDEs Azizzadenesheli et al. (2024), in applications modeling turbulent flows, weather forecasting Pathaket al. (2022), material plasticity Liu et al. (2022), and carbon dioxide storage Wen et al. (2022).FNOis specifically designed to learn mesh-independent solution operators for PDEs. Unlike traditional neuralnetworks that learn point-wise mappings, FNO learns a functional mapping between infinite-dimensional",
  "K lowest modes": ": Top: Fourier convolution operator in FNO. After the Fourier transform F, the layer first truncatesthe full set of frequencies to the K lowest ones using a dynamically set truncation before applying a learnable lineartransformation (blue) and finally mapping the frequencies back to the linear space using the inverse FFT F 1. Theprevious method (Li et al., a) picks a fixed K throughout the entire training. The FNO architecture does not includethe incremental algorithm and resolution. Bottom: full iFNO architecture. Our model takes as input functionsat different resolutions (discretizations). The operator consists of a lifting layer, followed by a series of iFNO blocks.The loss is used by our method to dynamically update the input resolution and the number of modes K in theSpectral Convolutions. The incremental algorithm is detailed in section 4 and algorithm 1.",
  "function spaces. It approximates the action of the solution operator on any input function, allowing it togeneralize across different discretizations and resolutions": "Unlike conventional neural networks, FNO possesses a property termed discretization-convergence, meaningit can output predictions at different resolutions, and those predictions converge to a unique solution uponmesh refinement. The discretization-convergence property relies on kernel integration, which, in the caseof FNO, is realized using the Fourier transform. A series of such spectral layers, along with normalizationlayers and non-linear activations, compose the core of the FNO architecture. For many PDE systems, likefluid flows, there is a decay of energy with frequency, i.e., low-frequency components usually have largermagnitudes than high-frequency ones. Therefore, as an inductive bias and a regularization procedure toavoid overfitting, FNO consists of a frequency truncation function TK in each layer that only allows thelowest K Fourier modes to propagate to the next layer, and the other modes are zeroed out, as shown in. It is thus crucial to select the correct number of frequency modes K in various spectral layers in FNO toachieve optimal generalization performance. Too few number of modes leads to underfitting, while too manycan result in overfitting. Further, we need to train on high-enough resolution data to capture all the physicaldynamics and approximate the ground-truth solution operator faithfully. Our approach: We propose the Incremental Fourier neural operator (iFNO) to address the above chal-lenges. iFNO incrementally and automatically increases the number of frequency modes K and resolution Rduring training. It leverages the explained ratio as a metric to dynamically increase the number of frequencymodes. The explained ratio characterizes the amount of information in the underlying spectrum that thecurrent set of modes can explain. A small explained ratio (i.e., below a fixed threshold) indicates that thecurrent modes are insufficient to represent the target spectrum and that more high-frequency modes shouldbe added, as illustrated in . Simultaneously, iFNO also starts by first training on low-resolution dataand progressively increases the resolution, akin to the principle of curriculum learning, thereby increasingtraining efficiency.",
  "Related Works": "The applications of neural networks on partial differential equations have a rich history (Lagaris et al., 1998;Dissanayake & Phan-Thien, 1994). These deep learning methods can be classified into three categories:(1) ML-enhanced numerical solvers such as learned finite element, finite difference, and multigrid solvers(Kochkov et al., 2021; Pathak et al., 2021; Greenfeld et al., 2019) ; (2) Network network-based solvers suchas Physics-Informed Neural Networks (PINNs), Deep Galerkin Method, and Deep Ritz Method (Raissi et al.,2019; Sirignano & Spiliopoulos, 2018; Weinan & Yu, 2018); and (3) the data-driven surrogate models suchas (Guo et al., 2016; Zhu & Zabaras, 2018; Bhatnagar et al., 2019). Among them, the machine learningsurrogate models directly parameterized the target mapping based on the dataset. They do not require apriori knowledge of the governing system and usually enjoy a light-fast inference speed. Recently, a novelclass of surrogate models called neural operators was developed (Li et al., b; Lu et al., 2021; Kissas et al.,2022).Neural operators parameterize the PDEs solution operator in the function spaces and leverageits mathematical structures in their architectures. Consequentially, neural operators usually have a betterempirical performance (Takamoto et al., 2022) and theoretical guarantees Kovachki et al. (2021) combinedwith conventional deep learning models. The concept of implicit spectral bias was first proposed by Rahaman et al. as a possible explanation for thegeneralization capabilities of deep neural networks. There have been various results towards theoreticallyunderstanding this phenomenon Cao et al.; Basri et al.. Fridovich-Keil et al. further propose methodologiesto measure the implicit spectral bias on practical image classification tasks. Despite a wealth of theoreticalresults and empirical observations, there has been no prior research connecting the implicit spectral biaswith FNO to explain its good generalization across different resolutions. The notion of incremental learning has previously been applied to the training of PINNs Krishnapriyanet al. (2021); Huang & Alkhalifah. However, these focus on a single PDE instance and apply incrementallearning to the complexity of the underlying PDE, e.g., starting with low-frequency wavefields first andusing high-frequency ones later Huang & Alkhalifah. Our work is orthogonal to this direction, and ratherthan modifying the underlying PDE, we directly incrementally increase the models capacity and the datasresolution.",
  "Fourier Neural Operator": "Fourier Neural Operators belong to the family of neural operators, which are formulated as a generalizationof standard deep neural networks to operator setting (Li et al., b). A neural operator learns a mappingbetween two infinite dimensional spaces from a finite collection of observed input-output pairs. Let D be abounded, open set and A and U be separable Banach spaces of functions of inputs and outputs. We want to learn a neural operator G : A U that maps any initial condition a A to its solutionu U. The neural operator G composes linear integral operator K with pointwise non-linear activationfunction to approximate highly non-linear operators.",
  "G := Q (WL + KL) (W1 + K1) P,": "where P, Q are the pointwise neural networks that encode the lower dimension function into higher dimen-sional space and decode the higher dimension function back to the lower dimensional space. The model stackL layers of (Wl + Kl) where Wl are pointwise linear operators (matrices), Kl are integral kernel operators,and are fixed activation functions. The parameters consist of all the parameters in P, Q, Wl, Kl.",
  "Published in Transactions on Machine Learning Research (09/2024)": "2. Incremental resolution: iTFNO also adopts the incremental resolution technique from iFNO,where the model is trained on progressively higher resolutions of the input data.This enablesiTFNO to capture fine-grained details and scale to higher resolutions more efficiently. 3. Factorization schemes and ranks: In addition to the incremental techniques, we explore differentfactorization schemes and ranks for the tensor decomposition of the weights in iTFNO. This allows usto investigate the impact of the tensor structure on the performance and efficiency of the incrementalapproach.",
  "where F and F1 are the Fourier transform and its inverse, R is a learnable transformation and TK is afixed truncation that restricts the input to lowest K Fourier modes": "FNO is discretization-convergent, such that the model can produce a consistent solution for any query points,potentially not in the training grid. In other words, FNO can be trained on low resolution and evaluated athigh resolution. This property is highly desirable as it allows a transfer of solutions between different gridresolutions and discretizations. Frequency truncation.To ensure discretization-convergence, FNO truncates the Fourier series Fv at amaximal number of modes K using TK. In this case, for any discrete frequency mode k {1, ..., K}, wehave Fv(k) CC and R(k) CCC, where C is the channel dimension of the input v. The size of linearparameterization R depends on K. For example, R has the shape of KdCC for a d-dimensional problem.We also denote the modes 1, ..., K as the effective frequency modes. In the standard FNO, Li et al. (a) viewK as an additional hyperparameter to be tuned for each problem.",
  "where Sk denotes the strength of the k-th frequency mode. A smaller Sk indicates that the k-th frequencymode is less important for the output": "Implicit spectral bias in neural networksIt has been well studied that neural networks implicitlylearn low-frequency components first, and then learn high-frequency components in a later stage Rahamanet al.; Xu et al. (2019). This phenomenon is known as implicit spectral bias, and it helps explain the excellentgeneralization ability of overparameterized neural networks. Since FNO performs linear transformation R in the frequency domain, the frequency strength Sk of eachfrequency mode i is directly related to the spectrum of the resulting model. As FNO is a neural network andtrained by first-order learning algorithms, it follows the implicit spectral bias such that the lower frequencymodes have larger strength in R. This explains why FNO chooses to preserve a set containing the lowestfrequency modes, instead of any arbitrary subset of frequencies.",
  ": The spectrum of Kolmogorov flowdecays exponentially with the Kolmogorovscale of 5/3 in the inverse cascade range": "Low-frequency components in PDEsLearning frequencymodes is important as large-scale,low-frequency compo-nents usually have larger magnitudes than small-scale, high-frequency components in PDEs. For dissipative systems (withdiffusion terms) such as the viscous Burgers equation and in-compressible Navier-Stokes equation, the energy cascade in-volves the transfer of energy from large scales of motion to thesmall scales, which leads to the Kolmogorov spectrum with theslope of k5/3 in the inverse cascade range (), and k3 in the direct-cascade range (Boffetta et al., 2012). The smallestscales in turbulent flow is called the Kolmogorov microscales.Therefore, one should choose the model frequencies with re-spect to the underlying equation frequencies when designingmachine learning models.It would be a challenge to selectthe correct model frequencies in advance, without knowing theproperties of the underlying PDEs.",
  "Incremental frequency FNO": "While frequency truncation TK ensures the discretization-invariance property of FNO, it is still challengingto select an appropriate number of effective modes K, as it is task-dependent and requires careful hyperpa-rameter tuning. Inappropriate selection of K can lead to severe performance degradation. Setting K too small will result intoo few frequency modes, such that it doesnt approximate the solution operator well, resulting in underfit-ting. As shown in , in the prediction of Darcy flow, FNO requires higher frequency modes to capturesmall-scale structures in the fluid for better performance. On the other hand, a larger K with too manyeffective frequency modes may encourage FNO to interpolate the noise in the high-frequency components,leading to overfitting. Previously, people have chosen the number of effective modes in FNO by estimating the underlying datafrequencies or using heuristics borrowed from pseudo-spectral solvers such as the 2/3 dealiaising rule (Hou &Li, 2007), i.e., picking the max Fourier modes as 2/3 of the training data resolution. However, its usually noteasy to estimate the underlying frequency, and the resolution of data may vary during training and testing.",
  "!\"#$%&$'()*+),-": ":Frequency evolution of the Fourier convolution operators in FNO with and without weight decay. Top:with weight decay; bottom: without weight decay. When training with weight decay, the strength of each modeconverges around 100 training iterations. And the weights of lower frequency tend to have high strength. On theother hand, the weight continues growing throughout the training without the weight decay. In this case, higherfrequencies can also have high strength. b compares FNO (90) and iFNO (Freq) over the training on Kolmogorov flow. It shows that iFNOrequires much fewer frequency modes during training. As the linear transformation R dominates the numberof parameters and FLOPs in FNO when K is large, iFNO (Freq) can significantly reduce the computationalcost and memory usage of training FNO. iFNO achieves better generalization and even requires fewer modesat the end of training. In a, we compare iFNO (Freq) with FNO with a particular K that achievesthe best performance in each dataset. The results suggest that the trained iFNO (Freq) consistently requiresfewer frequency modes than its FNO counterpart, making the iFNO inference more efficient.This alsoindicates that iFNO can automatically determine the optimal number of frequency modes K during trainingwithout predetermining K as an inductive bias that could potentially hurt the performance. To furtherdemonstrate the adaptability and broader applicability of our incremental approach, we conducted a case",
  "g(K, sp) =Kk=1 Skpk=1 Sk.(3)": "We define a threshold , which is used to determine if the current modes can well explain the underlyingspectrum.If g(K, sp) > , it indicates that the current modes are sufficient to capture the importantinformation in the spectrum, and thus no additional modes need to be included in FNO. Otherwise, morefrequency modes will be added into the model until g(K, sp) > is satisfied. Although sp reflects the entire spectrum when p is sufficiently large, maintaining a large p is unnecessary andcomputationally expensive. Instead, we only maintain a truncated spectrum sK+b RK+b, which consistsof K effective modes and b buffer modes. The buffer modes contain all mode candidacies that potentiallywould be included as effective modes in the later stage of training. In practice, at iteration t with Kt effective modes, we construct a transformation Rt C(Kt+b)CC for 1Dproblems. After updating Rt at iteration t + 1, we will find Kt+1 that satisfies g(Kt+1, st) > . In thisway, we can gradually include more high-frequency modes when their evolution becomes more significant, asillustrated in . We denote this variant as iFNO (Freq). We also describe how iFNO (Freq) is trainedfor solving 1D problems in Algorithm 1. iFNO is extremely efficient as the training only requires a part offrequency modes. In practice, it also converges to a model requiring fewer modes than the baseline FNOwithout losing any performance, making the model efficient for inference. Notably, the cost of computing the",
  "Regularization in FNO training": "As discussed in the previous section, learning low-frequency modes is essential for the successful training ofFNO. However, the standard regularization techniques used in training neural networks are not capable ofexplicitly promoting the learning of low-frequency modes. As shown in , although FNO with well-tuned weight decay strength successfully regularizes thehigh-frequency modes, its regularization can be too strong for certain layers (such as the fourth operatorin the figure). This leads to the instability of the frequency evolution, which damages the generalizationperformance, as shown in . On the other hand, insufficient decay strength cannot regularize thehigh-frequency modes and can even lead to overfitting the associated high-frequency noise. Dynamic Spectral Regularization:However, we find that iFNO can be served as a dynamic spectralregularization process. As shown in , iFNO properly regularizes the frequency evolution withoutcausing instability or overfitting high-frequency modes. As we will present in the experiment section, thedynamic spectral regularization gives iFNO a significant generalization improvement.",
  "Finally, we introduce the incremental algorithm on the resolution part of our approach": "Computational cost in training large-scale FNOTraining the FNO on large-scale, high-resolutiondata to solve PDEs with high-frequency information, a large effective mode number K is necessary tocapture the solution operator. However, this requires constructing a large-scale transformation R, whichdominates the computational cost of all the operations in FNO. This makes the training and inference morecomputationally expensive. For example, the Forecastnet (Pathak et al., 2022) for global weather forecastbased on FNO is trained on 64 NVIDIA Tesla A100 GPUs, only covering a few key variables. To simulateall the weather variables, it potentially needs the parallel FNO, which scales to 768 GPUs (Grady II et al.,2022). Incremental Resolution:To mitigate the computational cost of training FNO on high-resolution data,we incorporate curriculum learning to improve our model (Soviany et al. (2022)). This approach involvestraining the models in a logical sequence, where easier samples are trained first and gradually progressingto more challenging ones. In our case, we start with lower-resolution images, allowing the model to capturelarge-scale features first, potentially accelerating learning and reducing computational costs in the early stagesof training, and then moving on to higher-resolution images. This approach allows the model to initiallyfocus on learning low-frequency components of the solution using small data, avoiding wasted computationand overfitting. Then, the model progressively adapts to capture higher frequency components as the dataresolution increases. This model combines both incremental frequency (specifically the iFNO Freq)) andincremental resolution and is denoted by iFNO. Algorithm 1 showcases the joint implementation.",
  "MethodBurgersDarcy FlowNavier Stokes (FNO2D)Train (1e 3)Test (1e 2)Train (1e 3)Test (1e-2)Train (1e 2)Test (1)": "iFNO0.418 0.0020.139 0.0062.466 0.8550.129 0.0283.533 0.0030.106 0.011iFNO (Freq)0.422 0.0010.141 0.0011.491 0.2980.215 0.0101.548 0.0290.177 0.008iFNO (Loss)0.430 0.0010.140 0.0072.262 0.9850.198 0.0292.466 0.0920.202 0.020iFNO (Res)0.423 0.0010.146 0.0061.410 0.0200.123 0.0013.576 0.0210.105 0.001 FNO (10)1.135 0.0020.270 0.0061.471 0.2230.162 0.0035.133 0.1820.166 0.027FNO (30)0.455 0.0010.156 0.0021.462 0.1020.140 0.0113.262 0.0010.154 0.001FNO (60)0.421 0.0010.153 0.0061.468 0.2120.130 0.007- - FNO (90)0.423 0.0010.146 0.0061.425 0.2120.127 0.004",
  "Experimental Setup": "We evaluate iFNO and its variants on five different datasets of increasing difficulty. We retain the structureof the original FNO, which consists of stacking four Fourier convolution operator layers with the ReLUactivation and batch normalization. More specifically, for the time-dependent problems, we use an RNNstructure in time that directly learns in space-time1. The initial and coefficient conditions are sampled fromGaussian random fields (Nelsen & Stuart, 2021) for all the datasets.",
  "w(x, 0) = w0(x), t (0, T]": "where w = u is the vorticity, w0 is the initial vorticity, R+is the viscosity coefficient, and f(x) =0.1sin(2(x1 + x2)) + cos(2(x1 + x2))is the forcing function. We are interested in learning the operatormapping the vorticity up to time 10 to the vorticity up to some later time. We experiment with the hardestviscosity task, i.e., = 1e 5. The equivalent Reynolds number is around 500 normalized by forcing anddomain size. We set the final time T = 20 as the dynamic becomes chaotic. The resolution is fixed to be64 64. We consider solving this equation using the 2D FNO with an RNN structure in time or using the 3D FNOto do space-time convolution, similar to the benchmark in Li et al. (a). As 3D FNO requires significantlymore parameters, evaluating 3D FNO with more than 30 modes is computationally intractable due to thehardware limitation. Kolmogorov Flow:We evaluate the models on a very challenging dataset of high-frequency KolmogorovFlow. The Re5000 problem is the most challenging dataset we consider. It is governed by the 2D Navier-Stokes equation, with forcing f(x) = sin(nx2)x1, where x1 is the unit vector and a larger domain size ,as studied in (Li et al., 2021; Kochkov et al., 2021). More specifically, the Kolmogorov flow (a form of theNavier-Stokes equations) for a viscous, incompressible fluid,",
  "on 2 (0, )": "with initial condition u(, 0) = u0 where u denotes the velocity, p the pressure, and Re > 0 is the Reynoldsnumber. It has an extremely high Reynolds number with Re = 5000. The resolution of this dataset is128 128. We consider learning the evolution operator from the previous time step to the next based on2D FNO. We also consider a variation of the dataset with a lower Re value, which we call the KolmogorovFlow problem. This resolution is fixed to be 256 256 to show high-frequency details. MethodsWe present our results for each dataset where we consider four standard FNO baselines withdifferent numbers of frequency modes K = 10, 30, 60, 90. For each dataset, all methods are trained withthe same hyperparameters, including the learning rate, weight decay, and the number of epochs. Unlessotherwise specified, we use the Adam optimizer to train for 500 epochs with an initial learning rate of0.001, which is halved every 100 epochs, and weight decay is set to be 0.0005. For the re5000 dataset, wetrain for 50 epochs, and experiment details are explained in the Appendix. All experiments are run usingNVIDIA Tesla V100/A100 GPUs. We also test our incremental methods on Tensorized Factorized NeuralOperators Kossaifi et al. (2023) by sweeping over different factorization schemes and ranks for the weights.",
  "Experiment details for Re5000 dataset - iFNO": "Regarding setting the scheduler, the incremental resolution approach significantly alters the loss landscapeand distribution of the training data each time the resolution is increased. As noted by Smith (2017), cycliclearning rates oscillating between higher and lower values during training can be highly beneficial whendealing with such changing loss landscapes. Therefore, our incremental resolution training methodologyadopts a combination of the cyclic learning rate and step learning rate scheme. We see that this combina-tion provides improved and more stable convergence throughout the curriculum of incremental resolutionincreases, contributing to strong empirical results for both the incremental resolution model and the iFNOmodel. A triangular cyclic learning rate allows the model to first explore with larger learning rates to findnew optimal parameters after a resolution increase before narrowing down with smaller rates to converge.This prevents the model from getting stuck in sub-optimal solutions. Our experiments utilize 3 differentmodes, i.e., triangular, triangular with decreased height, and exponential decay. We also sweep over variousstep sizes and maximum and minimum bounds. The cyclic period is set to e epochs. Although the cyclicLRmethod is indeed suitable, we find that after we switch to the highest resolution, using stepLR converges to abetter minimum compared to cyclicLR. This makes sense, as once we start training on the highest resolution,instead of changing our learning rates in an oscillatory manner, we should stabilize and take smaller stepsfor convergence, like how the baseline FNO is trained. Our algorithm increases resolution every e epoch from 32 to 64 and then 128. We use powers of 2 for optimalFast Fourier Transform speed. The hyperparameter e can be tuned, and r is a list determining how low wesub-sample data. In the re5000 dataset case, we have set e = 10 and r = , which results in startingfrom resolution 32 all the way up to the highest resolution 128.",
  "MethodBurgersDarcy FlowNavier Stokes (FNO 2D)Train (1e 3)Test (1e 3)Train (1e 3)Test (1e-3)Train (1e 1)Test (1e-1)": "iFNO0.891 0.0220.760 0.0394.580 0.8550.264 0.0201.022 0.0430.824 0.016iFNO (Freq)0.898 0.0220.778 0.0052.453 0.2600.387 0.0091.102 0.1321.265 0.044iFNO (Loss)0.919 0.0330.779 0.0343.113 0.8870.322 0.0681.135 0.0031.375 0.224iFNO (Res)0.979 0.0530.854 0.0344.463 0.9200.271 0.0101.112 0.0800.737 0.227 FNO (10)2.523 0.0281.063 0.0252.991 0.3020.481 0.04710.66 0.7361.286 0.178FNO (30)1.032 0.0170.868 0.0332.981 0.0460.412 0.0291.223 0.0451.184 0.107FNO (60)0.966 0.0230.845 0.0162.596 0.4330.466 0.0131.112 0.0291.178 0.047FNO (90)0.973 0.0420.851 0.0292.521 0.0460.379 0.0431.155 0.0011.577 0.002",
  "FNO (10)1.035 0.0270.296 0.0165.462 0.0042.227 0.012FNO (30)0.977 0.0010.587 0.0013.571 0.0021.312 0.012FNO (60)2.751 0.0011.015 0.001FNO (90)2.330 0.0021.042 0.002": "First, we study the performance of our model on the large-scale Re5000 dataset, where we use 36000 trainingsamples and 9000 testing samples.The results in show that iFNO achieves significantly bettergeneralization performance than just incrementally increasing either frequency or resolution alone. Thishighlights the advantages of the iFNO while gaining around a 30% efficiency and much better accuracythan the stand-alone Incremental methods. The training time is reduced due to starting with a very smallmodel and input data. There is one important note to mention that, in fact, for the Re5000 dataset, sincethe problem is highly challenging, i.e., having a high Reynolds number, we need to reach all 128 modes tocapture even the high-frequency components. Next, we evaluate all methods in the low-data regime across different datasets, where the models only haveaccess to a few data samples. This is common in many real-world PDE problems where accurately labeleddata is expensive to obtain. In this regime, the ability to generalize well is highly desirable. We consider asmall training set (5 to 50 samples) for Burgers, Darcy, and Navier-Stokes datasets. As the data trajectoriesare already limited in the Kolmogorov dataset, we choose 8x larger time step (sampling rate) for eachtrajectory, resulting in fewer time frames. As shown in , iFNO consistently outperforms the FNO baselines across all datasets, regardless ofthe number of modes K. It also shows that FNO with larger K achieves lower training loss but overfitsthe training data and performs poorly on the testing set. On the other hand, iFNO (Freq) achieves slightlyhigher training loss but generalizes well to the testing set, which demonstrates the effectiveness of the dynamicspectral regularization. We also notice that although iFNO (Loss) has significant fluctuations, especially inthe training results, it does achieve better generalization on most datasets against the baselines.",
  "Standard Regularization in training FNO": "We also visualize the frequency evolution of FNO and iFNO. As shown in , the weight decay used inthe optimization has a significant effect on the learned weights. When the weight decay is active, the strengthof weights will converge around the first 100 iterations of training. Further, the strength of frequencies isordered with respect to the wave numbers where the lower frequencies have high strength and the higherfrequencies have lower strength. On the other hand, when training without weight decay, the strength of allfrequency modes will continuously grow and mix up with each other.",
  "Conclusion": "In this work, we proposed the Incremental Fourier Neural Operator (iFNO) that dynamically selects fre-quency modes and increases the resolution during training. Our results show that iFNO achieves bettergeneralization while requiring less number of parameters (frequency modes) compared to the standard FNO.We believe iFNO opens many new possibilities, such as solving PDEs with very high resolution and withlimited labeled data and training computationally efficient neural operators. In the future, we plan to applyiFNO on more complex PDEs and extend it to the training of physics-informed neural operators.",
  "Impact statement": "This paper presents work with the goal of advancing the field of machine learning and PDEs. FNOs haveshown promise to solve challenging physics and engineering problems accurately and efficiently. Our researchhas made FNOs more optimized through incremental learning and, hence, could lead to FNOs being appliedto solve larger real-world problems, allowing for advances in fields like engineering, weather forecasting, andclimate science while reducing training times with fewer computing resources. There might also be potentialnegative societal consequences of our work, but none of them are immediate to be specifically highlightedhere.",
  "Jean Kossaifi, Nikola Kovachki, Kamyar Azizzadenesheli, and Anima Anandkumar. Multi-grid tensorizedfourier neural operator for high-resolution pdes. arXiv preprint arXiv:2310.00120, 2023": "Nikola Kovachki, Zongyi Li, Burigede Liu, Kamyar Azizzadenesheli, Kaushik Bhattacharya, Andrew Stuart,and Anima Anandkumar.Neural operator: Learning maps between function spaces.arXiv preprintarXiv:2108.08481, 2021. Aditi Krishnapriyan, Amir Gholami, Shandian Zhe, Robert Kirby, and Michael W Mahoney. Characterizingpossible failure modes in physics-informed neural networks. In M. Ranzato, A. Beygelzimer, Y. Dauphin,P.S. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems, vol-ume 34, pp. 2654826560. Curran Associates, Inc., 2021.URL Isaac E Lagaris, Aristidis Likas, and Dimitrios I Fotiadis. Artificial neural networks for solving ordinary andpartial differential equations. IEEE transactions on neural networks, 9(5):9871000, 1998. Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew Stuart,and Anima Anandkumar. Fourier neural operator for parametric partial differential equations, a. URL type: article. Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew Stuart,and Anima Anandkumar. Neural operator: Graph kernel network for partial differential equations, b.URL Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew Stu-art, and Anima Anandkumar.Markov neural operators for learning chaotic systems.arXiv preprintarXiv:2106.06898, 2021.",
  "Nicholas H Nelsen and Andrew M Stuart. The random feature model for input-output maps between banachspaces. SIAM Journal on Scientific Computing, 43(5):A3212A3243, 2021": "Jaideep Pathak, Mustafa Mustafa, Karthik Kashinath, Emmanuel Motheau, Thorsten Kurth, and MarcusDay. Ml-pde: A framework for a machine learning enhanced pde solver. Bulletin of the American PhysicalSociety, 2021. Jaideep Pathak, Shashank Subramanian, Peter Harrington, Sanjeev Raja, Ashesh Chattopadhyay, MortezaMardani, Thorsten Kurth, David Hall, Zongyi Li, Kamyar Azizzadenesheli, et al.Fourcastnet:Aglobal data-driven high-resolution weather model using adaptive fourier neural operators. arXiv preprintarXiv:2202.11214, 2022.",
  "A.1Frequency mode evolution during training": "As shown in , the effective modes K adaptively increase during the training phase on the Burgers,Darcy, Navier-Stokes (2D & 3D formulation), and Kolmogorov Flows. For smooth problems such as viscousBurgers equation and Darcy Flow, the mode converges in the early epoch, while for problem with highfrequency structures such as the Kolmogorov Flows, the mode will continuously increase throughout thetraining phase.",
  "B.1Selection of Hyperparameters": "In Tables 4 and 5 we present the results of our hyperparameter search for in iFNO (Freq) and in iFNO(Loss) respectively, across various tasks. These hyperparameters control the threshold for adding new modesin our incremental approaches.For iFNO (Freq), represents the cumulative energy threshold in theExplanation Ratio criterion.We explored values ranging from 0.6 to 0.9999, covering a wide spectrumfrom more aggressive (lower ) to more conservative (higher ) mode addition strategies. For iFNO (Loss), represents the threshold for the relative decrease in loss that triggers the addition of new modes. Weinvestigated values from 0.1 to 0.00001, encompassing both rapid (higher ) and gradual (lower ) modeaddition schemes. For most tasks, moderate values of (around 0.99) and (around 0.001) tend to performwell, balancing between adding sufficient modes for expressiveness and avoiding overfitting. However, sometasks like Kolmogorov Flow benefit from more aggressive mode addition (lower or higher ), likely due tothe complexity of the underlying dynamics.",
  "Generalization: iFNO (Resolution) improves testing error by 3 Efficiency: iFNO is 46 Model size: iFNO achieves a 2.3x reduction (471 million vs. 1.061 billion parameters)": "These results demonstrate that iFNO consistently outperforms FNO (90 modes) in terms of generalization,efficiency, and model size across various tasks. We chose to compare primarily with FNO (90 modes) as itrepresents the most challenging baseline. Other iFNO variants, while potentially larger than the base iFNOmodel, still maintain efficiency advantages over FNO.",
  "iTFNO incorporates our incremental mode selection and incremental resolution techniques into the TFNOframework. Specifically, we apply the following modifications to TFNOs:": "1. Incremental mode selection: Similar to iFNO, iTFNO starts with a small number of modesand gradually increases the number of modes during training. We experiment with both frequency-based (iFNO (Freq)) and loss-based (iFNO (Loss)) criteria for determining when to add modes. Thisallows iTFNO to adaptively adjust the model capacity and focus on the most relevant frequencycomponents for the given task.",
  "B.7Different Losses": "We have also conducted our models on the H1 Loss. We acknowledge that the L2 loss, while commonly used,may not fully capture the desired properties and behavior of the learned operators. The L2 loss treats allparts of the signal equally and may not prioritize important structural or geometric features. Our proposediFNO(freq) and iFNO(loss) methods are closely tied to controlling the L2 loss, either directly or indirectly. We have conducted additional experiments using the Sobolev loss (H1). The Sobolev loss considers notonly the pointwise differences between the predicted and target signals but also the differences in theirderivatives or gradients. This loss is more sensitive to the smoothness and regularity of the learned operatorsand can provide insights into their ability to capture important spatial or temporal structures. We haveevaluated our proposed methods, including iFNO, iFNO(freq), iFNO(loss), and iFNO(res), as well as thebaseline FNO and TFNO models, on three datasets: Re5000, Navier-Stokes 2D, and Burgers. The resultsare presented in the tables above.For the Re5000 dataset, we observe that iTFNO outperforms bothiTFNO(freq) and the baseline TFNO in terms of H1 loss, indicating that the incremental approach combinedwith the tensor factorization can lead to improved accuracy in capturing the spatial regularity of the solutions.The performance gap is consistent across different tensor ranks (5 and 25).",
  "iTFNO0.23960.2984iTFNO (Freq)0.28890.3321TFNO0.27250.3496": "In the Navier-Stokes 2D dataset, iFNO and its variants (iFNO(freq), iFNO(loss), iFNO(res)) generallyachieve lower H1 loss compared to the baseline FNO. This suggests that the incremental mode selectionand resolution techniques can help learn operators that better preserve the smoothness and continuity of thefluid dynamics. Similarly, for the Burgers dataset, iFNO and its variants demonstrate competitive or slightlyimproved performance in terms of H1 loss compared to the baseline FNO. The iFNO(res) variant, whichfocuses on incremental resolution, achieves the lowest H1 loss, highlighting the importance of adaptivelyrefining the spatial resolution for this specific problem. Finally, we include one more comparison on the NS2D dataset where we present results for training on L2 lossand evaluating on both L2 and H1 losses, as well as training on H1 loss and evaluating on both L2 and H1",
  "iFNO0.02913iFNO Freq0.02892iFNO Loss0.03005iFNO Res0.02675FNO0.03160": "losses. The incremental techniques in iFNO and its variants can lead to improved performance in L2 and H1losses, demonstrating their effectiveness in capturing both pointwise accuracy and spatial regularities. TheiFNO variants show different behaviors when trained on L2 versus H1 loss, suggesting that the incrementaltechniques interact differently with these loss functions.",
  "ModelTrain (L2)Eval (L2)Eval (H1)Train (H1)Eval (L2)Eval (H1)": "iFNO0.05240.03770.12840.09720.08310.1543iFNO Freq0.05140.03690.12590.09630.12440.1476iFNO Loss0.05040.03590.12420.11440.11810.1606iFNO Res0.05460.03790.13160.11520.08810.1507FNO0.05170.03600.12580.11030.15840.1619 These results provide reassurance that our proposed incremental techniques not only improve the L2 loss butalso lead to better performance in terms of the Sobolev loss, which is more sensitive to the spatial or temporalregularity of the learned operators. However, we acknowledge that the Sobolev loss is just one example ofan alternative metric, and some many other relevant losses and distances could be considered, such as L1loss, Wasserstein distance, or problem-specific metrics. We believe that a comprehensive evaluation acrossmultiple losses and datasets is crucial for understanding the strengths and limitations of operator learningmethods. We will investigate this in future work."
}