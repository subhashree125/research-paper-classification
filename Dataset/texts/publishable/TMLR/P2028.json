{
  "Abstract": "We study the problem of building reasoning agents that are able to generalize in an effectivemanner. Towards this goal, we propose an end-to-end approach for building model-basedreinforcement learning agents that dynamically focus their reasoning to the relevant aspectsof the environment: after automatically identifying the distinct aspects of the environment,these agents dynamically filter out the relevant ones and then pass them to their simulator toperform partial reasoning. Unlike existing approaches, our approach works with pixel-basedinputs and it allows for interpreting the focal points of the agent. Our quantitative analysesshow that the proposed approach allows for effective generalization in high-dimensionaldomains with raw observational inputs. We also perform ablation analyses to validate ourdesign choices.Finally, we demonstrate through qualitative analyses that our approachactually allows for building agents that focus their reasoning on the relevant aspects of theenvironment.",
  "Introduction": "The goal in model-based reinforcement learning (RL) is to build reasoning agents that maximize long-term cumulative reward through trial and error interaction with the environment. In the last decade, theemployment of deep neural networks as function approximators have allowed model-based RL agents toachieve significant successes across a wide range of areas, ranging from challenging board and video games(Silver et al., 2017b; 2018; Ha & Schmidhuber, 2018; Hafner et al., 2020; 2021; 2023; Schrittwieser et al.,2020) to discovering efficient algorithms (Fawzi et al., 2022; Mandhane et al., 2022; Mankowitz et al., 2023).However, despite these successes, these agents are typically designed to improve sample efficiency in regularRL settings and thus lack the inductive biases that are necessary for achieving effective performance ingeneralization settings. One of the promising lines of research for tackling this shortcoming considers the use of partial models in thereasoning process of the agent (see e.g. Talvitie & Singh, 2008; Khetarpal et al., 2020; 2021; Zhao et al., 2021;Alver & Precup, 2023). Inspired by studies in cognitive science, these approaches aim for building agentsthat mimic the reasoning process in humans, in which planning is hypothesized to be performed over a fewabstract aspects of the environment (Bengio, 2017; Goyal & Bengio, 2022): e.g. when planning a route frompoint A to point B humans typically focus their reasoning to the points in between and discard the rest. Morespecifically, rather than building plain reasoning agents that plan over every aspect of the environment, theseapproaches advocate for building partial reasoning agents that focus the planning on the relevant aspects ofthe environment. Among these studies, the study of Zhao et al. (2021) has empirically demonstrated thatthis type of planning allows for effective generalization in scenarios where the details of the environment keepchanging but the overall task remains the same. However, this study has two main shortcomings: (i) it failsin providing an approach for building partial reasoning agents that works with pixel-based inputs, i.e. an",
  "Iterative Attention": ": Our overall architecture for building partial reasoning agents. This architecture consists of threemain modules: (Box 1) the aspect identification module, (Box 2) the permutation-invariant filtrationmodule, and (Box 3) the value-equivalent simulation module. The details of these modules are provided inthe main text. The q, k and v blocks represent a linear mapping to obtain the query, key and value matrices,respectively, and the attention blocks represent a soft attention operation. The GRU block represents a GRUcell. Lastly, the black arrows indicate the direction of information flow. More details on this architecturecan be found in App. A. approach that works beyond low-dimensional environments in which the distinct aspects of the environmentis hand-provided via symbolic inputs, and (ii) it lacks detailed qualitative analyses demonstrating that theirproposed approach actually allows for building agents that focus their reasoning on the relevant aspects ofthe environment. In this paper, we overcome these shortcomings by proposing an approach for building partial reasoning agentsthat works with pixel-based inputs and allows for interpreting the focal points of the agent. Unlike priorwork, our end-to-end approach allows for building reasoning agents that automatically identify the distinctaspects of the environment and then dynamically attend to the relevant ones, which makes it possible forthem to work on high-dimensional domains with raw observational inputs. Importantly, the built agentsalso allow for further inspection on what they actually incorporate into their model throughout the course ofinteraction with the environment. After laying out its details (Sec. 3), we demonstrate through quantitativeanalyses that our approach allows for effective generalization in high-dimensional domains (Sec. 4.3). Inaddition to the quantitative analyses, we also perform ablation analyses to validate our design choices (Sec.4.4). Finally, we perform several qualitative analyses with our approach and show that it actually allows forbuilding agents that dynamically focus their reasoning on the relevant aspects of the environment (Sec. 4.5).We hope that our study will bring the RL community a step closer to building scalable and interpretablereasoning agents that are able to effectively generalize to novel situations throughout their interaction withthe environment. Key Contributions. The key contributions of this study are as follows: (i) We propose an approach forbuilding partial reasoning agents that works with pixel-based inputs and allows for interpreting the focalpoints of the agent. (ii) We demonstrate, through quantitative analyses, that our end-to-end approach allowsfor effective generalization in high-dimensional domains with raw observational inputs. (iii) We demonstrate,through qualitative analyses, that our approach allows for building agents that actually focus their reasoningon the relevant aspects of the environment.",
  "Background": "Reinforcement Learning. In RL (Sutton & Barto, 2018), an agent interacts with its environment througha sequence of actions to maximize its long-term cumulative reward. The environment is usually modeled asa Markov decision process (MDP) M (S, A, P, R, d0, ) where S and A are the (finite) set of states andactions, P : S A Dist(S) is the transition distribution, R : S A S R is the reward function,d0 : S Dist(S) is the initial state distribution and [0, 1) is the discount factor. At each time step t,after taking an action at A, the environments state transitions from st S to st+1 S; and the agentreceives an observation ot+1 O (reflecting st+1) and an immediate reward rt. The goal of the agent is tolearn a policy : O Dist(A) that maximizes E,P [t=0 tR(St, At, St+1)|S0 = s0 d0], where E,P []denotes the expectation over trajectories induced by and P. Model-Based RL. One way of achieving this goal is through model-based RL (Moerland et al., 2023) inwhich there are two alternating phases: (i) the learning and (ii) planning phases. In the learning phase, thegathered experience is mainly used in learning a model m, and in the planning phase, the learned modelm is then used for simulating experience either to be used alongside real experience in improving the valuepredictions or just to be used in selecting actions at decision time (Alver & Precup, 2024). The Attention Mechanism. In its general form, the soft attention mechanism (Vaswani et al., 2017) isdescribed as y = (QK)V where Q Rmdq, K Rndq and V Rndv are the query, key and valuematrices, respectively, and () is the softmax function. Here, m and n denote the number of query andkey / value vectors, respectively; and, dq and dv denote the dimensionality of the query / key and valuevectors. More explicitly, the attention output y is computed in three steps: (i) first, each of the m dq-dimensional query vectors are dotted with a database of n dq-dimensional key vectors to compute the scorevalues, (ii) then these values are passed through a softmax function (), along the axis of the key vectors,to compute the attention weights, and (iii) finally, the output y is computed by taking a weighted sum ofthe n dv-dimensional value vectors with the weights. Self-Attention and Permutation-Invariance. A particular form of attention that is of interest in thisstudy is self-attention. In self-attention, the Q, K, V matrices are a function of the input x, e.g., Q = q(x),K = k(x) and V = v(x) where q(), k() and v() are linear mappings. By default, the self-attention operationis not permutation-invariant, i.e., permuting the input also results in a permutation in the output. However,a clever way to enable permutation-invariance in self-attention is to make Q a function of a set of fixedpositional embeddings, rather than the input x (see Lee et al., 2019; Tang & Ha, 2021).",
  "An Attentive Approach for Building Partial Reasoning Agents from Pixels": "In this section, we present our approach for building partial reasoning agents that works with pixel-basedinputs and allows for interpreting the focal points of the agent. Our approach consists of an end-to-endarchitecture and an accompanying training procedure. The architecture, depicted in , consists of threeseparate modules: (i) the aspect identification module, (ii) the permutation-invariant filtration module and(iii) the value-equivalent simulation module. The details of these modules and the procedure for collectivelytraining them are presented in the following sections.",
  "Aspect Identification Module": "At a high level, at each time step t in the environment, the aspect identification module (see Box 1 in ) takes in the raw observational input ot and binds its distinct aspects into a set of n distinct slots {sit}ni=1.The main function of this module is to automatically identify the distinct aspects of the environment, e.g.the distinct objects or the distinct regions in the observational input. Internally, this module resemblesan autoencoder architecture (Goodfellow et al., 2016) and it consists of (i) an encoder network, (ii) a slotattention module1 and (iii) a decoder network. 1We prefer slot attention over other techniques as it allows for better generalization (Locatello et al., 2020). We also note thateven though Locatello et al. (2020) introduced slot attention and used it for the purposes of object discovery and set predictionin the context of computer vision, in this study, we are using slot attention for the purpose of building partial reasoning agents.",
  "Published in Transactions on Machine Learning Research (08/2024)": "MF Agent. The MF agent is a model-free counterpart of the MZ agent, i.e. a version of the MZ agent inwhich no MCTS is performed: action selection is done solely with the value predictions from the root state.This agent shares the same architecture and hyperparameters with the MZ agent. PR-NoPIF Agent. The PR-NoPIF agent is a version of the PR agent with no PI filtration module, i.e. aversion in which the aspect identifying slots are directly passed to the value-equivalent simulation module.This agent shares the same hyperparameters with the PR agent. PR-SH Agent. The PR-SH agent is a version of the PR agent which makes use of top-k semi-hard attention(rather than soft attention) in the implementation of its PI filtration module (see Algorithm 3). For thisagent, we have experimented with different values of k Z+, ranging between 2 to 7, and ended up withsetting it to 3 as it gave the best results (note that this also what was done in Zhao et al. (2021)). Thisagent shares the same hyperparameters with the PR agent.",
  "Permutation-Invariant Filtration Module": "The permutation-invariant (PI) filtration module (see Box 2 in ) takes as input the aspect identifyingslots {sit}ni=1, i.e. the output of the slot attention module, and performs a soft filtration operation overthese slots to distill out the relevant information. The main purpose of this module is to filter out the relevantinformation from the slots that are likely to be useful in decision-making, which is later to be used for partialreasoning. Here, it is important to note that the filtration is dynamic, i.e. throughout the agent-environmentinteraction, the PI filtration module can filter out different slots at different time steps. Internally, the slotsare weighted by a PI soft attention mechanism (Lee et al., 2019). Here, the use of a PI attention mechanismis critical, as the aspect identifying slots form a set with permutation symmetry, i.e. the order of them canchange at each time step. More on the details of this module can be found in App. A.2. It is also important to note that the PI filtration module has some analogues to the consciousness-inspiredbottleneck that was proposed in Zhao et al. (2021). However, there is an important difference: ratherthan using a top-k semi-hard attention mechanism (Ke et al., 2018), we use a relatively straightforwardsoft attention mechanism (Vaswani et al., 2017). This difference makes our approach much simpler andit also mitigates the burden of tuning for the optimal k value. Experiments in the following sections alsodemonstrate that the use of top-k semi-hard attention does not bring any performance advantages.",
  "Value-Equivalent Simulation Module": "The value-equivalent simulation module (see Box 3 in ) takes the filtered aspect identifying slots asinput and uses them as the root state x0t for performing value-equivalent simulation (Grimm et al., 2020;2021; Schrittwieser et al., 2020). The main function of this module is to perform partial reasoning, i.e.to perform planning with the relevant aspects of the environment.More specifically, at each time stept, for each of the hypothetical k = 1, . . . , K steps, the simulator takes in the root state x0t and predictsthree future quantities: (i) the policy pkt (at+k+1|o1, . . . , ot, at+1, . . . , at+k), (ii) the value function vkt E[ut+k+1 + ut+k+2 + . . . |o1, . . . , ot, at+1, . . . , at+k], and (iii) the immediate reward rkt ut+k, where u isthe true reward, is the policy used to select real actions, and is the discount factor. Internally, thesimulator is represented by a combination of a dynamics and prediction network. The dynamics networkm recurrently predicts, at each hypothetical step k, an internal state xkt and an immediate reward rkt , i.e.",
  ",(1)": "where Lp, Lv, and Lr are the loss functions for the policy, value function and reward, respectively, constitutingthe simulation loss Lsim. While Lrecon optimizes for minimizing the error between input observation ot andthe reconstructed observation ot, Lp, Lv and Lr optimize for minimizing (i) the error between the searchpolicy t+k and predicted policy pkt , (ii) the error between the improved value target zt+k = ut+1 + ut+2 + + n1ut+n + nvt+n and the predicted value vkt , and (iii) the error between the observed reward ut+kand predicted reward rkt , respectively. In Lv, the improved value targets zt+k are generated by collectingtrajectories in the environment, and in Lp, the improved policy targets t+k are generated by performingMCTS. More details on individual losses and the weighting between them can be found in App. B.",
  ": Sample frames from the(top row) DynObs-Pass, DynObs-Collect, (bottom row) CoinRunand BigFish environments": "Environment Descriptions. We perform our experiments on the (i)MiniGrid (Chevalier-Boisvert et al., 2018) and (ii) Procgen domains(Cobbe et al., 2020). We choose these domains as the former one al-lows for designing controlled experiments and the latter is helpful indemonstrating the scalability of our approach to more complex do-mains. Here, it is important to note that in these domains the agentonly receives raw observational inputs, i.e. 64x64 RGB images, and notsymbolic inputs as in Zhao et al. (2021).2 In the MiniGrid domain, we consider two customized environments:(i) DynamicObstacles-Pass and (ii) DynamicObstacles-Collect (abbre-viated as DynObs-Pass and DynObs-Collect, respectively, see thetop row of ). In both of these environments, the agent (red square)is randomly spawned at the leftmost column of the grid and has to passthrough a region with randomly oscillating obstacles to reach the goal(green square), which is randomly placed at the rightmost column ofthe grid. At each time step the agent is able to move to one of its 2Zhao et al. (2021) perform experiments on a customized MiniGrid environment with symbolic inputs. The symbolic inputis in the form of a triple (x, y, idx), where x and y are the coordinates on the grid, and idx is the id of the object.",
  "In our experiments, we compare the following agents:": "Partial Reasoning (PR) Agent. A partial reasoning agent that was built by our proposed approach inSec. 3.MuZero (MZ) Agent. A MuZero (Schrittwieser et al., 2020) agent. Unlike the PR agent, the MZ agentperforms regular reasoning. The main difference between the MZ agent and the PR agent is in the procedurefor constructing the root state x0t: while the PR agent constructs it by the use of an aspect identificationand PI filtration module (Sec. 3), the MZ agent constructs it by the use of a plain convolutional and residualarchitecture (He et al., 2016). We use this agent as a baseline to demonstrate the impact of partial reasoningin achieving better generalization.Model-Free (MF) Agent. A version of the MZ agent that does not perform any planning. This agenttakes actions by skipping the search process and just following its value predictions (which makes it amodel-free agent). We use this agent as a baseline to demonstrate the impact of planning in achievingbetter generalization.PR-NoPIF Agent. A version of the PR agent with no PI filtration module.PR-SH Agent. A version of the PR agent in which the PI filtration module is implemented with top-ksemi-hard attention. The purposes of this agent and the PR-NoPIF agent will be unfolded in the ablationanalyses section (Sec. 4.4).",
  "Quantitative Analyses": "Our approach allows for effective generalization in high-dimensional domains.The study ofZhao et al. (2021) demonstrated that partial reasoning can allow for effective generalization over regularreasoning. It also demonstrated that planning plays an important role for better generalization. However,these demonstrations were mainly performed on low-dimensional gridworlds with symbolic inputs. One of themain goals of our approach is to generalize these successes to high-dimensional domains. Towards this goal,we compare the generalization performances of the PR, MZ and MF agents across two image-based MiniGridand Procgen domains (Sec. 4.1). Performance curves in consistently show that, the PR agent displaysa better generalization performance than the MZ agent, both (i) highlighting the role of partial reasoning inachieving effective generalization and (ii) corroborating the findings of Zhao et al. (2021). We can also see",
  "(h) BigFish (Test)": ": The generalization performances of the PR and baseline agents on the (a, b) DynObs-Pass, (c, d)DynObs-Collect, (e, f) CoinRun and (g, h) BigFish environments. Here, the plots are obtained by evaluatingthe agent on a set of test levels (referred to as Test) as it gets trained on a set of training levels (referredto as Train) of the same environment. In MiniGrid and Procgen experiments, the training sets consist of 16and 500 randomly-sampled game levels, respectively. In both experiments, the test sets consist of all of thepossible game levels. The means and the confidence intervals are computed over 15 independent runs.",
  "Ablation Analyses": "The PI filtration module plays a critical role in the performance of our approach. One of thecrucial components of our proposed agent architecture is the PI filtration module (Sec. 3.2). This moduleis particularly important as it acts as a soft filter to what the agent would be modeling. To quantify itsimportance, we compare the PR agent with the PR-NoPIF agent (Sec. 4.2). Recall that in the PR-NoPIFagent the aspect identifying slots are directly passed, without any filtration, to the value-equivalent simulationmodule. Results in show that the removal of the PI filtration module significantly deteriorates thegeneralization performance of the PR agent, making it on par with the MZ agent. These results indicatethat the PI filtration module indeed plays a critical role in the performance of our approach. The use of top-k semi-hard attention in the PI filtration module does not bring any per-formance advantages. As expressed previously, in this study, we have implemented the PR agents PIfiltration module with soft attention, in which the attention weights can take any value between zero and one.However, another way to implement this module could have been with top-k semi-hard attention, which wasused in the consciousness-inspired bottleneck of Zhao et al. (2021) (Sec. 3.2). In top-k semi-hard attention,only the top-k attention weights are kept and all others are set to zero. If used in the PI filtration module,this type of attention would allow for a hard filtration of only the k aspect identifying slots with the largestattention weights. To investigate the impact of this type of attention, we compare the PR agent with thePR-SH agent (Sec. 4.2). The comparison results, shown in , indicate that the use of top-k semi-hardattention does not bring any advantages in the generalization performance of the PR agent; in fact, the useof it slightly decreases the performance. These results suggest that the use of the more simple soft attentionmechanism can be a better choice in the implementation of the PI filtration module.",
  "Qualitative Analyses": "The study of Zhao et al. (2021) claimed the learning of partial reasoning agents, but lacked in providingdetailed qualitative analyses that demonstrates that the learned agents actually focus their reasoning onthe relevant aspects of the environment. In this section, we perform qualitative analyses with the PR agentand demonstrate that our approach actually allows for building reasoning agents that focus on the relevantaspects of the environment. Our approach allows for building reasoning agents that automatically identify the distinctaspects of the environment. Before the dynamic attention illustrations, we first start by presentingvisualizations to demonstrate that the PR agent is actually able to automatically identify the distinct aspectsof the environment.Recall from Sec. 3.1 that the automatic identification of the distinct aspects fallsunder the role of the aspect identification module.For this purpose, in we visualize the maskedreconstructions of each of the individual aspect identifying slots across different environments. Inspectingthis figure, we see that the slots indeed bind to the distinct aspects of the environment, corresponding eitherto different objects or to different regions in the observational input. This demonstrates that our proposedapproach is indeed successful in automatically identifying the distinct aspects of the environment. Our approach allows for building reasoning agents that dynamically attend to the relevantaspects of the environment. We now present illustrations to demonstrate that the PR agent is actuallyable to dynamically attend to the relevant aspects of the environment. Recall that the dynamic attentionto the relevant aspects falls under the role of the PI filtration module (Sec. 3.2). To see if the PI filtrationmodule is actually successful in this task, in , we present the attention maps of the PR agent throughoutits course of interaction with different environments. Examining this figure, we can see that the PI filtrationmodule indeed dynamically attends to the relevant aspects of different environments (see the caption of for the interpretations), which demonstrates that our proposed approach indeed allows for buildingreasoning agents that dynamically focus on the relevant aspects of the environment.",
  "Related Work": "Partial Reasoning Agents. The use of partial models for building partial reasoning agents has previouslybeen investigated in the model-based RL literature (Talvitie & Singh, 2008; Khetarpal et al., 2020; 2021;Zhao et al., 2021; Alver & Precup, 2023; Alver et al., 2024). Among these studies, the study of Zhao et al.(2021) is closest to our work in that it proposes an approach for building partial reasoning agents and",
  "Conclusion": "To summarize, in this study, we have presented a novel approach for building partial reasoning agents whichare agents that dynamically focus their reasoning on the relevant aspects of the environment. Unlike existingapproaches, our approach works with pixel-based inputs and it allows for interpreting the focal points of theagent. Our empirical results (i) demonstrate that the proposed approach allows for effective generalization inhigh-dimensional domains with raw observational inputs, and (ii) show that it allows for building reasoningagents that indeed dynamically focus their reasoning on the relevant aspects of the environment throughoutthe agent-environment interaction. We believe that our approach can be an important step towards buildingscalable and interpretable reasoning agents that are able to effectively generalize to novel situations. Beforeclosing, it is important to note that even though we have made use of particular attentive architectures inthe modules, our overall architecture, presented in , is a general architecture and its modules can beimplemented by using different attentive architectures. Finally, we note that in this study, similar to Zhaoet al. (2021), we have only considered environments in which the details keep changing but the overall taskremains the same. In future work, we hope to extend our work and tackle environments in which the overalltask also changes over time.",
  "Maxime Chevalier-Boisvert, Lucas Willems, and Suman Pal. Minimalistic gridworld environment for openaigym. 2018": "Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, HolgerSchwenk, and Yoshua Bengio. Learning phrase representations using rnn encoderdecoder for statisticalmachine translation. In Proceedings of the 2014 Conference on Empirical Methods in Natural LanguageProcessing (EMNLP), pp. 1724. Association for Computational Linguistics, 2014. Karl Cobbe, Chris Hesse, Jacob Hilton, and John Schulman. Leveraging procedural generation to benchmarkreinforcement learning. In International conference on machine learning, pp. 20482056. PMLR, 2020.",
  "Stanislas Dehaene, Hakwan Lau, and Sid Kouider. What is consciousness, and could machines have it?Robotics, AI, and Humanity: Science, Ethics, and Policy, pp. 4356, 2021": "Quentin Delfosse, Jannis Blml, Bjarne Gregori, Sebastian Sztwiertnia, and Kristian Kersting. Ocatari:Object-centric atari 2600 reinforcement learning environments. arXiv preprint arXiv:2306.08649, 2023. Quentin Delfosse, Hikaru Shindo, Devendra Dhami, and Kristian Kersting. Interpretable and explainablelogical policies via neurally guided symbolic abstraction.Advances in Neural Information ProcessingSystems, 36, 2024a. Quentin Delfosse, Sebastian Sztwiertnia, Wolfgang Stammer, Mark Rothermel, and Kristian Kersting. In-terpretable concept bottlenecks to align reinforcement learning agents. arXiv preprint arXiv:2401.05821,2024b. Lauro Langosco Di Langosco, Jack Koch, Lee D Sharkey, Jacob Pfau, and David Krueger. Goal misgeneral-ization in deep reinforcement learning. In International Conference on Machine Learning, pp. 1200412019.PMLR, 2022. Jeff Druce, Michael Harradon, and James Tittle. Explainable artificial intelligence (xai) for increasing usertrust in deep reinforcement learning driven autonomous systems. arXiv preprint arXiv:2106.03775, 2021. Gregory Farquhar, Tim Rocktschel, Maximilian Igl, and Shimon Whiteson. Treeqn and atreec: Differen-tiable tree-structured models for deep reinforcement learning. arXiv preprint arXiv:1710.11417, 2017. Alhussein Fawzi, Matej Balog, Aja Huang, Thomas Hubert, Bernardino Romera-Paredes, MohammadaminBarekatain, Alexander Novikov, Francisco J R Ruiz, Julian Schrittwieser, Grzegorz Swirszcz, et al. Dis-covering faster matrix multiplication algorithms with reinforcement learning. Nature, 610(7930):4753,2022. Claire Glanois, Paul Weng, Matthieu Zimmer, Dong Li, Jianye Hao, Tianpei Yang, and Wulong Liu. Asurvey on interpretable reinforcement learning. IEEE Transactions on Neural Networks and LearningSystems, 2021.",
  "Anirudh Goyal and Yoshua Bengio. Inductive biases for deep learning of higher-level cognition. Proceedingsof the Royal Society A, 478(2266):20210068, 2022": "Klaus Greff, Raphal Lopez Kaufman, Rishabh Kabra, Nick Watters, Christopher Burgess, Daniel Zoran,Loic Matthey, Matthew Botvinick, and Alexander Lerchner. Multi-object representation learning withiterative variational inference. In International Conference on Machine Learning, pp. 24242433. PMLR,2019. Christopher Grimm, Andre Barreto, Satinder Singh, and David Silver.The value equivalence princi-ple for model-based reinforcement learning.In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Bal-can, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 55415552. Curran Associates, Inc., 2020.URL Christopher Grimm, Andre Barreto, Gregory Farquhar, David Silver, and Satinder Singh. Proper valueequivalence. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan (eds.), Advances inNeural Information Processing Systems, 2021. URL",
  "Zhengyao Jiang and Shan Luo. Neural logic reinforcement learning. In International conference on machinelearning, pp. 31103119. PMLR, 2019": "Lukasz Kaiser, Mohammad Babaeizadeh, Piotr Milos, Blazej Osinski, Roy H Campbell, Konrad Czechowski,Dumitru Erhan, Chelsea Finn, Piotr Kozakowski, Sergey Levine, et al. Model-based reinforcement learningfor atari. arXiv preprint arXiv:1903.00374, 2019. Nan Rosemary Ke, Anirudh Goyal, Olexa Bilaniuk, Jonathan Binas, Michael C Mozer, Chris Pal, andYoshua Bengio. Sparse attentive backtracking: Temporal credit assignment through reminding. Advancesin neural information processing systems, 31, 2018. Khimya Khetarpal, Zafarali Ahmed, Gheorghe Comanici, David Abel, and Doina Precup. What can i dohere? a theory of affordances in reinforcement learning. In International Conference on Machine Learning,pp. 52435253. PMLR, 2020.",
  "Iou-Jen Liu, Raymond A Yeh, and Alexander G Schwing. Pic: permutation invariant critic for multi-agentdeep reinforcement learning. In Conference on Robot Learning, pp. 590602. PMLR, 2020": "Francesco Locatello, Dirk Weissenborn, Thomas Unterthiner, Aravindh Mahendran, Georg Heigold, JakobUszkoreit, Alexey Dosovitskiy, and Thomas Kipf. Object-centric learning with slot attention. Advancesin Neural Information Processing Systems, 33:1152511538, 2020. Lirui Luo, Guoxi Zhang, Hongming Xu, Yaodong Yang, Cong Fang, and Qing Li. Insight: End-to-end neuro-symbolic visual reinforcement learning with language explanations. arXiv preprint arXiv:2403.12451, 2024. Amol Mandhane, Anton Zhernov, Maribeth Rauh, Chenjie Gu, Miaosen Wang, Flora Xue, Wendy Shang,Derek Pang, Rene Claus, Ching-Han Chiang, et al. Muzero with self-competition for rate control in vp9video compression. arXiv preprint arXiv:2202.06626, 2022. Daniel J Mankowitz, Andrea Michi, Anton Zhernov, Marco Gelmi, Marco Selvi, Cosmin Paduraru, EdouardLeurent, Shariq Iqbal, Jean-Baptiste Lespiau, Alex Ahern, et al. Faster sorting algorithms discoveredusing deep reinforcement learning. Nature, 618(7964):257263, 2023.",
  "Thomas M Moerland, Joost Broekens, Aske Plaat, Catholijn M Jonker, et al. Model-based reinforcementlearning: A survey. Foundations and Trends in Machine Learning, 16(1):1118, 2023": "Alexander Mott, Daniel Zoran, Mike Chrzanowski, Daan Wierstra, and Danilo Jimenez Rezende. Towardsinterpretable reinforcement learning using attention augmented agents. Advances in neural informationprocessing systems, 32, 2019. Yazhe Niu, Yuan Pu, Zhenjie Yang, Xueyan Li, Tong Zhou, Jiyuan Ren, Shuai Hu, Hongsheng Li, andYu Liu. Lightzero: A unified benchmark for monte carlo tree search in general sequential decision scenarios.Advances in Neural Information Processing Systems, 36, 2024.",
  "Junhyuk Oh, Satinder Singh, and Honglak Lee. Value prediction network. Advances in neural informationprocessing systems, 30, 2017": "Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt,Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, et al. Mastering atari, go, chess andshogi by planning with a learned model. Nature, 588(7839):604609, 2020. David Silver, Hado Hasselt, Matteo Hessel, Tom Schaul, Arthur Guez, Tim Harley, Gabriel Dulac-Arnold,David Reichert, Neil Rabinowitz, Andre Barreto, et al. The predictron: End-to-end learning and planning.In International Conference on Machine Learning, pp. 31913199. PMLR, 2017a. David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, ThomasHubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al.Mastering the game of go without humanknowledge. nature, 550(7676):354359, 2017b. David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, MarcLanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, et al.A general reinforcement learningalgorithm that masters chess, shogi, and go through self-play. Science, 362(6419):11401144, 2018.",
  "Niklas Wahlstrm, Thomas B Schn, and Marc Peter Deisenroth. From pixels to torques: Policy learningwith deep dynamical models. arXiv preprint arXiv:1502.02251, 2015": "Manuel Watter, Jost Springenberg, Joschka Boedecker, and Martin Riedmiller. Embed to control: A locallylinear latent dynamics model for control from raw images. Advances in neural information processingsystems, 28, 2015. Nicholas Watters, Loic Matthey, Christopher P Burgess, and Alexander Lerchner. Spatial broadcast decoder:A simple architecture for learning disentangled representations in vaes. arXiv preprint arXiv:1901.07017,2019. Vinicius Zambaldi, David Raposo, Adam Santoro, Victor Bapst, Yujia Li, Igor Babuschkin, Karl Tuyls,David Reichert, Timothy Lillicrap, Edward Lockhart, et al. Deep reinforcement learning with relationalinductive biases. In International conference on learning representations, 2018. Mingde Zhao, Zhen Liu, Sitao Luan, Shuyuan Zhang, Doina Precup, and Yoshua Bengio. A consciousness-inspired planning agent for model-based reinforcement learning. Advances in Neural Information Process-ing Systems, 34, 2021.",
  "A.1Aspect Identification Module": "As explained in the main text, the aspect identification module consists of (i) an encoder network, (ii) a slotattention module and (i) a decoder network. The architectural details of the encoder and decoder networksare summarized in & 7, respectively. Conv2d (filters=16, kernel_size=5, stride=1, padding=2) ReLU ReLU Conv2d (filters=16, kernel_size=5, stride=1, padding=2) Conv2d (filters=16, kernel_size=5, stride=1, padding=2) ReLU Conv2d (filters=32, kernel_size=5, stride=1, padding=2) ReLU SoftPosEmbed (hidden_size=32, resolution=64)",
  ": The architecture of the decoder network. The black arrows indicate the direction of informationflow": "For the implementation of the slot attention module, we have followed the pseudocode in Locatello et al.(2020), presented in Algorithm 1, and have used the hyperparameters in . In our experiments, wehave experimented with 4, 6, 8, 10, 16 slots and all of them resulted around the same performance (score).",
  "BDetails of the Training Procedure": "As explained in the main text, we train our proposed architecture (see ), in an end-to-end manner,with the loss function Ltotal depicted in Eq. 1, which is a weighted sum of the reconstruction loss Lrecon andthe simulation loss Lsim = Lp + Lv + Lr. In our experiments, we ended up in using an equal weighting forthe losses as the different combinations of and did not lead to a significant increase ordecrease in the generalization performances, i.e. we set and to 0.5.",
  "Lr(u, r) = (u) log r,(5)": "where () refers to a transformation from a scalar representation to a categorical one, and the p, v andr indicate the categorical outputs of the dynamics and prediction networks. Note that, in all of the lossfunctions, the loss is over batches of data and the time subscripts are omitted for simple presentation.",
  "CImplementation Details of the Agents": "PR Agent. The PR agent is a partial reasoning agent that was built by our proposed approach in Sec.3.This agent is implemented by using the MCTS simulation framework of Niu et al. (2024).3Morespecifically, we have built upon the available MuZero implementation by replacing the internals of theRepresentationNetwork class in lzero/model/common.py with the internals of the aspect identification",
  "Dirichlet noise weight0.25": "MZ Agent. The MZ agent is a MuZero (Schrittwieser et al., 2020) agent. For this agent, we have used thealready available MuZero implementation in Niu et al. (2024) which directly follows the implementation ofSchrittwieser et al. (2020). The architecture of its representation network is depicted in . For a faircomparison, we use the same hyperparameters with the PR agent, i.e. the hyperparameters in . Conv2d (filters=32, kernel_size=3, stride=2, padding=1) ReLU ResBlock (in_channels=32, actiovation=ReLU, norm=batchnorm) BatchNorm2d (num_features=32) ResBlock (in_channels=32, out_channels=64, actiovation=ReLU, norm=batchnorm, type=downsample) ResBlock (in_channels=64, actiovation=ReLU, norm=batchnorm) AvgPool2d (kernel_size=3, stride=2, padding=1) ResBlock (in_channels=64, actiovation=ReLU, norm=batchnorm) AvgPool2d (kernel_size=3, stride=2, padding=1)",
  "(b) Heist (Test)": ": The generalization performances of the PR and baseline agents on the Heist environment. Theplots are obtained again by evaluating the agent on a set of test levels (referred to as Test) as it gets trainedon a set of training levels (referred to as Train) of the same environment. The training set consists of 500randomly-sampled game levels. The test set consists of all of the possible game levels. The means and theconfidence intervals are again computed over 15 independent runs. To strengthen our quantitative and ablation results, we also compare the generalization performances of thePR and baseline agents on the Heist environment. In this environment, the agent (a stealer) must stealthe gem that is behind a network of locks. Similar to every Procgen environment, a new game level isprocedurally generated after every episode. For more details on the Heist environment, we refer the readerto the study of Cobbe et al. (2020). We see that the performance curves in corroborate the findingsof Sec. 4.3 & 4.4."
}