{
  "Reviewed on OpenReview: https: // openreview. net/ forum? id= oG65SjZNIF": ": An illustration of a hypergraph of recipes. The nodes are the ingredients and the hyperedges arethe recipes. The task of higher order link prediction is to predict hyperedges in the hypergraph. A negativehyperedge sample would be the dotted hyperedge.The Asian ingredient nodes () and the Europeaningredient nodes () form two separate isomorphism classes. However, GWL-1 cannot distinguish betweenthese classes and will predict a false positive for the negative sample.",
  "Abstract": "A hypergraph consists of a set of nodes along with a collection of subsets of the nodes calledhyperedges. Higher order link prediction is the task of predicting the existence of a miss-ing hyperedge in a hypergraph. A hyperedge representation learned for higher order linkprediction is fully expressive when it does not lose distinguishing power up to an isomor-phism. Many existing hypergraph representation learners, are bounded in expressive powerby the Generalized Weisfeiler Lehman-1 (GWL-1) algorithm, a generalization of the Weis-feiler Lehman-1 (WL-1) algorithm. The WL-1 algorithm can approximately decide whethertwo graphs are isomorphic. However, GWL-1 has limited expressive power. In fact, GWL-1can only view the hypergraph as a collection of trees rooted at each of the nodes in thehypergraph. Furthermore, message passing on hypergraphs can already be computationallyexpensive, particularly with limited GPU device memory. To address these limitations, wedevise a preprocessing algorithm that can identify certain regular subhypergraphs exhibit-ing symmetry with respect to GWL-1. Our preprocessing algorithm runs once with thetime complexity linear in the size of the input hypergraph. During training, we randomly",
  "Introduction": "Hypergraphs can model complex relationships in real-world networks, extending beyond the pairwise con-nections captured by traditional graphs. is an example hypergraph consisting of recipes of twodifferent types of dishes, which are largely determined by the ingredients to be used. In this hypergraph,the hyperedges are the recipes, and the nodes are the ingredients used in each recipe. The Asian recipesare presented in the right part of the figure, which consist of combinations of the ingredients of soy sauce,scallions, rice, peppers, chicken, ginger, curry and onions. The European recipes are presented in the leftpart of the figure, which consist of combinations of the ingredients of Parmesan cheese, mozzarella cheese,salami, butter, eggs, and pasta. Hypergraphs have found applications in diverse fields such as recommender systems L et al. (2012), visualclassification Feng et al. (2019), and social networks Li et al. (2013). Higher-order link prediction is the taskof predicting missing hyperedges in a hypergraph. For this task, when the hypergraph is unattributed, it isimportant to respect the hypergraphs symmetries, or automorphism group. This brings about challengesto learning an expressive view of the hypergraph. A hypergraph neural network (hyperGNN) is any neural network that learns on a hypergraph. This is inanalogy to a graph neural network (GNN), which is a neural network that learns on a graph. Many existinghyperGNN architectures follow a computational message passing model called the Generalized WeisfeilerLehman-1 (GWL-1) algorithm Huang & Yang (2021), a hypergraph isomorphism testing approximationscheme. GWL-1 is a generalization of the message passing algorithm called Weisfeiler Lehman-1 (WL-1)algorithm Weisfeiler & Leman (1968) used for graph isomorphism testing. GWL-1, like WL-1 on graphs, is limited in how well it can express its input. Specifically, by viewing ahypergraph as a collection of rooted trees, GWL-1 loses topological information of its input hypergraphand thus cannot fully recognize the symmetries, or automorphism group, of the hypergraph. In fact, thehyperGNN views the hypergraph as having false-positive symmetries. For a task like transductive hyperlinkprediction, this can result in predicting false-positive hyperlinks as shown in . Furthermore, suchan issue can become even worse during test time since the automorphism group of the hypergraph mightchange. It is thus important to find a way to improve the expressivity of existing hyperGNNs. Let a hypergraph H = (V, E) denote a pair where V is a set of nodes and E 2V, a collection of subsets of V,indexes a set of hyperedges. GWL-1 views a hypergraph as a collection of trees rooted at the nodes. Theserooted trees are formed by viewing each node-hyperedge incidence as an edge and recursively expandingabout the nodes and hyperedges alternately. As an example of the GWL-1 algorithm, one step of GWL-1would output a collection of depth 1 trees rooted at each node where leaves are the incident hyperedges ofeach node. Two steps of GWL-1 would output a collection of depth 2 trees where the depth 1 trees of onestep of GWL-1 have their leaves expanded with the incident nodes of the hyperedges the new leaves. Thisleaf expansion process can be repeated, alternating nodes and hyperedges. This is the recursive expansionof the GWL-1 algorithm. We can recover hyperGNNs by expressing the computation of GWL-1 as a matrix equation. Parameterizingthe expression with learnable weights, GWL-1 becomes a neural network, called a hypergraph neural network(hyperGNN), similar to graph neural networks (GNN)s. In practice this is implemented through repeatedsparse matrix multiplication.",
  "grow exponentially with the number of nodes of the hypergraph. Thus, a computationally more expensivemessage passing scheme over GWL-1 based hyperGNNs may bring difficulties": "In order to address the issue of the expressivity of hyperGNNS for the task of hyperlink prediction while alsorespecting the computational complexity of computing on a hypergraph, we devise a method that selectivelybreaks the symmetry of the hypergraph topology itself coming from the limitations of the hyperGNN archi-tecture.Our method is designed as an efficient preprocessing algorithm that can improve the expressivepower of GWL-1 for higher order link prediction. Since the preprocessing only runs once with complexitylinear in the input, we do not increase the computational complexity of training. Similar to a substructure counting algorithm Bouritsas et al. (2022), we identify certain symmetries in in-duced subhypergraphs. However, unlike in existing work where node attributes are modified, such as randomnoise being appended to the node attributes Sato et al. (2021), we directly target and modify the symmetriesin the topology. This limits the space for augmentation, which can prevent extreme perturbations of thedata.The algorithm identifies a cover of the hypergraph by disjoint connected components whose nodesare indistinguishable by GWL-1. During training, we randomly replace the hyperedges of the identified sym-metric regular induced subhypergraphs with single hyperedges that cover the nodes of each subhypergraph.We show that our method of hyperedge hallucination to break symmetry can increase the expressivity ofexisting hypergraph neural networks both theoretically and experimentally. Contributions.In the context of hypergraph representation learning and hyperlink prediction, we have amethod that can break the symmetries introduced by conventional hypergraph neural networks. Conventionalhypergraph neural networks are based on the GWL-1 algorithm on hypergraphs.However, the GWL-1algorithm on hypergraphs views the hypergraph as a collection of rooted trees. This introduces false positivesymmetries. We summarize our contributions in this work as follows: Provide a formal analysis of GWL-1 on hypergraphs from the perspective of algebraic topol-ogy. Our analysis offers a novel characterization of the expressive power and limitations of GWL-1.By leveraging concepts from algebraic topology, we establish a precise connection between GWL-1and the universal covers of hypergraphs, providing deeper insights into the algorithms behavior oncomplex hypergraph structures. Devise an efficient hypergraph preprocessing algorithm to identify false positive symmetriesof GWL-1. We propose a linear time preprocessing algorithm to identify specific regular subhyper-graphs that exhibit symmetry with respect to GWL-1, which are potential sources of expressivitylimitations. Introduce a data augmentation scheme that leverages the preprocessing algorithms output toimprove GWL-1s expressivity. During training, we randomly modify the hyperedges of the iden-tified symmetric subhypergraphs, effectively breaking symmetries that GWL-1 cannot distinguish.This approach enhances the models ability to capture fine-grained structural information withoutsignificantly increasing computational complexity. Provide formal analysis and performance guarantees for our method. We rigorously provehow our approach improves the expressivity of GWL-1 under certain conditions. These theoreticalresults offer valuable insights into the circumstances under which our method can enhance hyper-graph representation learning, providing a solid foundation for its practical application. Perform extensive experiments on real-world datasets to demonstrate the effectiveness of ourapproach. Our comprehensive evaluation spans various hypergraph and graph datasets, showcasingconsistent improvements across different hypergraph neural network architectures for higher-orderlink prediction tasks. These empirical results validate the practical utility of our method and itsability to enhance existing models with minimal computational overhead.",
  "Let N {0, 1, ...}, Z {..., 1, 0, 1, ...}, Z+ {1, ...}, and R denote the naturals, integers, positive integers,and real numbers respectively. Let [n] {1, ..., n} Z+ denote the integers from 1 to n Z+": "For a set A, letAkdenote the set of all subsets of A of size k. Given the set A, a multiset is definedby A (A, m), m : A Z+. A set is also a multiset with m = 1. A submultiset B A is defined byB (B, m) with B A and m(e) m(e), e B. A multiset with its elements explicitly enumerated withthe double curly brace notation: {{a, a, a, ...}}. The cardinality of a multiset A, is defined as | A|",
  "eA m(e).For two multisets A = (A, mA), B = (B, mB), we may define their multiset sum by A B (AB, mAB =mA + mB)": "For two pairs of multisets A = ( A1, A2), B = ( B1, B2), denote their multiset sum by their elementwisemultiset sum:A B ( A1 B1, A2 B2). Similarly, for two pairs of sets A = (A1, A2), B = (B1, B2),denote their union by their elementwise union: A B (A1 B1, A2 B2). Let Pt P(; t) denote a probability distribution parameterized by t R. The distribution Pt has somedomain D, which we denote by dom(Pt). The notation supp(Pt) {x dom(Pt) : Pt(x) > 0} denotes thesupport of a distribution Pt.",
  ". Inverse Element: For each a G, there exists an element b G such that a b = b a = e (suchelement b is unique for a and is often denoted as a1.)": "Permutation GroupsA permutation group is a group where the elements are permutations of a set, andthe group operation is the composition of these permutations. Permutations are bijective functions thatrearrange the elements of a set. Group Isomorphism and AutomorphismTwo groups G and H are called isomorphic if there existsa bijective function : G H such that for all a, b G, (a b) = (a) (b). This means that G andH have the same group structure, even if their elements are different. Such bijective funtions are calledisomorphisms.If G = H, an isomorphism to a group itself is called an automorphism.The set of allautomorphisms of a group G forms a group, with group operations given by compositions. Such group iscalled the automorphism group of G, denoted as Aut(G).",
  "eF e, F)": "For a given hypergraph H, we also use VH and EH to denote the sets of vertices and hyperedges of Hrespectively. According to the definition, a hyperedge is a nonempty subset of the vertices. A hypergraphwith all hyperedges the same size d is called a d-uniform hypergraph. A 2-uniform hypergraph is an undirectedgraph, or just graph.",
  "When viewed combinatorially, a hypergraph can include some symmetries that are captured by isomorphisms.These isomorphisms are defined by bijective structure preserving maps": "Definition 2.3. For two hypergraphs H and D, a structure preserving map : H D is a pair of maps =(V : VH VD, E : EH ED) such that e EH, E(e) {V(vi) | vi e} ED. A hypergraph isomorphismis a structure preserving map = (V, E) such that both V and E are bijective. Two hypergraphs are saidto be isomorphic, denoted as H = D, if there exists an isomorphism between them.When H = D, anisomorphism is called an automorphism on H. All the automorphisms form a group, which we denote asAut(H).",
  "A graph isomorphism is the special case of a hypergraph isomorphism between 2-uniform hypergraphsaccording to Definition 2.3": "A neighborhood N(v) (ve e, {e : v e}) of a node v V of a hypergraph H = (V, E) is the subhypergraphof H induced by the set of all hyperedges incident to v. The degree of v is denoted deg(v) = |EN(v)|. Adegree vector of a node v of H is defined as: degvecH(v) = (|{e : e v, |e| = k}|)nk=1",
  "Definition 2.4. A neighborhood-regular hypergraph is a hypergraph where all neighborhoods of each nodeare isomorphic to each other": "A d-uniform neighborhood of v is the set of all hyperedges of size d in the neighborhood of v. Thus, in aneighborhood-regular hypergraph, all nodes have their d-uniform neighborhoods of the same cardinality forall d N. Representing Higher Order Structures as Tensors : There are many data stuctures one can define ona higher order structure like a hypergraph. An n-order tensor Maron et al. (2018), as a generalization of",
  "Published in Transactions on Machine Learning Research (11/2024)": "Wen et al. (2016)JF17K: The full Freebase data in RDF format was downloaded. Entities involvedin very few triples and the triples involving String, Enumeration Type and Numbers were removed.A fact representation was recovered from the remaining triples. Facts from meta-relations havingonly a single role were removed. From each meta-relation containing more than 10,000 facts, 10,000facts were randomly selected.",
  "In practice, as data to machine learning algorithms, the matrix H is sparsely represented by its nonzeroentries": "To study the symmetries of a given hypergraph H = (V, E), we consider the permutation group on the verticesV, denoted as Sym(V), which acts jointly on the rows and columns of star expansion adjacency matrices.We assume the rows and columns of a star expansion adjacency matrix have some canonical ordering, saylexicographic ordering, given by some prefixed ordering of the vertices. Therefore, each hypergraph H has aunique canonical matrix representation H.",
  "Definition 2.7. For a given hypergraph H with star expansion matrix H, two k-node sets S, T V arecalled isomorphic, denoted as S T, if Stab(H), (S) = T": "Such isomorphism is an equivalance relation on k-node sets. When k = 1, we have isomorphic nodes, denotedu =H v for u, v V. Node isomorphism is also studied as the so-called structural equivalence in Lorrain &White (1971).Furthermore, when S T we can then say that there is a matching due to the graph of the map of the form {(s, (s)) : s S}. This matching is between the node sets S and T so that matchednodes are isomorphic.",
  "Invariance and Expressivity": "For a given hypergraph H = (V, E), we want to do hyperedge prediction on H, which is to predict missinghyperedges from k-node sets for k 2. Let |V| = n, |E| = m, and H Zn2n2be the star expansion adjacencymatrix of H. To do hyperedge prediction, we study k-node representations g :Vk Zn2n2 Rd that mapk-node sets of hypergraphs to d-dimensional Euclidean space. Ideally, we want a most-expressive k-noderepresentation for hyperedge prediction, which is intuitively a k-node representation that is injective on k-node set isomorphism classes from H. We break up the definition of most-expressive k-node representationinto possessing two properties, as follows: Definition 2.8. Let g :Vk Zn2n2 Rd be a k-node representation on a hypergraph H. Let H Zn2n2be the star expansion adjacency matrix of H for n nodes. The representation g is k-node most expressive ifS, S V, |S| = |S| = k, the following two conditions are satisfied:",
  ". g is k-node expressive Stab(H), (S) = S = g(S, H) = g(S, H)": "The first condition of a most expressive k-node representation states that the representation must be welldefined on the k nodes up to isomorphism. The second condition requires the injectivity of our representation.These two conditions mean that the representation does not lose any information when doing prediction formissing k-sized hyperedges on a set of k nodes.",
  "(5)": "This is slightly different from the algorithm presented in Huang & Yang (2021) at the f i+1eupdate step. Ourupdate step involves an edge representation f ie, which is not present in their version. Thus our version ofGWL-1 is more expressive than that in Huang & Yang (2021). However, they both possess some of the sameissues that we identify. We denote f ie(H) and hiv(H) as the hyperedge and node ith iteration GWL-1, calledi-GWL-1, values on an unattributed hypergraph H with star expansion H. If GWL-1 is run to convergencethen we omit the iteration number i. We also mean this when we say i = . For a hypergraph H with star expansion matrix H, GWL-1 is strictly more expressive than WL-1 onA = H D1e HT with De = diag(HT 1n), the node to node adjacency matrix, also called the cliqueexpansion of H. This follows since a triangle with its 3-cycle boundary: T and a 3-cycle C3 have exactly thesame clique expansions. Thus WL-1 will give the same node values for both T and C3. GWL-1 on the starexpansions HT and HC3, on the other hand, will identify the triangle as different from its bounding edges.",
  "X(l+1)V= (HX(l)E W (l)V+ b(l)V )(8)": "where is a nonlinearity, XV , XE are vector representations of hi(H) and f i(H) respectively, andWE, WV , bE, bV are learnable weight matrices. Setting b(l)E = 0, b(l)V = 0, we maintain permutation equivari-ance as in Proposition 2.2. Furthermore, the HNHN equations of Equation 8 become in direct analogy tothe steps of GWL-1 from Equation 5.",
  "Related Work and Existing Issues": "There are many hyperlink prediction methods. Most message passing based methods for hypergraphs arebased on the GWL-1 algorithm. These include Huang & Yang (2021); Yadati et al. (2019); Feng et al.(2019); Gao et al. (2022); Dong et al. (2020); Srinivasan et al. (2021); Chien et al. (2022); Zhang et al. (2018).Examples of message passing based approaches that incorporate positional encodings on hypergraphs includeSNALS Wan et al. (2021). The paper Zhang et al. (2019) uses a pair-wise node attention mechanism to dohigher order link prediction. For a survey on hyperlink prediction, see Chen & Liu (2022). Various methods have been proposed to improve the expressive power of GNNs due to symmetries in graphs.In Papp & Wattenhofer (2022), substructure labeling is formally analyzed. One of the methods analyzedincludes labeling fixed radius ego-graphs as in You et al. (2021); Zhang & Li (2021). Other methods includeappending random node features Sato et al. (2021), labeling breadth-first and depth-first search trees Li et al.(2023b) and encoding substructures Zeng et al. (2023); Wijesinghe & Wang (2021). All of the previouslymentioned methods depend on a fixed subgraph radius size. This prevents capturing symmetries that spanlong ranges across the graph. Zhang et al. (2023) proposes to add metric information of each node relativeto all other nodes to improve WL-1. This would be very computationally expensive on hypergraphs. Cycles are a common symmetric substructure. There are many methods that identify this symmetry. Cy2CChoi et al. is a method that encodes cycles to cliques. It has the issue that if the the cycle-basis algorithm isnot permutation invariant, isomorphic graphs could get different cycle bases and thus get encoded by Cy2Cdifferently, violating the invariance of WL-1. Similarly, the CW Network Bodnar et al. (2021) is a methodthat attaches cells to cycles to improve upon the distinguishing power of WL-1 for graph classification.However, inflating the input topology with cells as in Bodnar et al. (2021) would not work for link predictingsince it will shift the hyperedge distribution to become much denser. Other works include cell attentionnetworks Giusti et al. (2022) and cycle basis based methods Zhang et al. (2022). For more related work, seethe Appendix. Data augmentation is a commonly used approach to improve robustness to distribution shifts Yao et al.(2022b), recognize symmetries Chen et al. (2020b), and handle data imbalance Chawla et al. (2002). Inthe graph domain, a priori knowledge of the data distribution can inform rule-based data augmentations.In molecular classification, prior knowledge of the physical meaning of the data can be used to augmentgraphs Sun et al. (2021). Data augmentations can also be learned through data generation methods. Forexample a link prediction neural network GAE Kipf & Welling (2016b) can be used to propose edges to toimprove node classification Zhao et al. (2021). For a survey on graph data augmentation, see Zhao et al.(2022).",
  "A Characterization of GWL-1": "A hypergraph can be represented by a bipartite graph BV,E from V to E where there is an edge (v, e) inthe bipartite graph iff node v is incident to hyperedge e. This bipartite graph is called the star expansionbipartite graph. We introduce a more structured version of graph isomorphism called a 2-color isomorphism to characterizehypergraphs. It is a map on 2-colored graphs, which are graphs that can be colored with two colors so thatno two nodes in any graph with the same color are connected by an edge. We define a 2-colored isomorphismformally here:",
  "A 2-colored isomorphism from a graph G to itself is called a 2-colored automorphism. The set of all 2-coloredautomorphisms on G is denoted Autc(G)": "A bipartite graph always has a 2-coloring. In this paper, we canonically fix a 2-coloring on all star expansionbipartite graphs by assigning red to all the nodes in the node partition and and blue to all the nodes in thehyperedge partition. See (a) as an example. We let BV, BE be the red and blue colored nodes inBV,E respectively.",
  ". G is simply connected (a tree)": "A covering graph is a graph that satisfies property 1 but not necessarily property 2 in Definition B.9. It isknown that a universal covering G covers all the graph covers of the graph G. Let T rx denote a tree with rootx where every node has depth r. Furthermore, define a rooted isomorphism Gx = Hy as an isomorphismbetween graphs G and H that maps x to y and vice versa. We will use the following result to prove acharacterization of GWL-1: Lemma B.6 (Krebs & Verbitsky (2015)). Let T and S be trees and x V (T) and y V (S) be their verticesof the same degree with neighborhoods N(x) = {x1, ..., xk} and N(y) = {y1, ..., yk}. Let r 1. Suppose thatT r1x= Sr1yand T rxi = Sryi for all i k. Then T r+1x= Sr+1y.",
  "with f i, hi the ith GWL-1 values for the hyperedges and nodes respectively where e1 = pBV1,E1(e1), x1 =pBV1,E1(x1), e2 = pBV2,E2(e2), x2 = pBV2,E2(x2)": "Theorem 4.3 states that a 2-colored isomorphism is maintained during each step of the GWL-1 algorithm.Thus we can view the GWL-1 algorithm on a hypergraph as equivalent to computing a universal cover ofthe star expansion bipartite graph up to a 2-colored isomorphism. We can thus deduce from Theorems 4.3,4.2 that GWL-1 reduces to computing WL-1 on the bipartite graph up to the 2-colored isomorphism. Corollary 1. Let H1 = (V1, E1) and H2 = (V2, E2) be two connected hypergraphs. Let BV1,E1 and BV2,E2 betwo canonically colored bipartite graphs for H1 and H2 (vertices colored red and hyperedges colored blue).Let pBV1,E1 : BV1,E1 BV1,E1, pBV2,E2 : BV2,E2 BV2,E2 be the universal coverings of BV1,E1 and BV2,E2respectively.For any i Z+,",
  ". The last group isomorphism follows by the equivalence between L-GWL-1 and the universal cover ofthe star expansion bipartite graph BV,E up to 2L-hops": "Since there are more automorphisms over the GWL-1 view of the hypergraph, many false positive symmetriesmight exist. Consider the following example. For two neighborhood-regular hypergraphs C1 and C2, thered/blue colored universal covers BC1, BC2 of the star expansions of C1 and C2 are isomorphic, with thesame GWL-1 values on all nodes. However, two neighborhood-regular hypergraphs of different order becomedistinguishable if a single hyperedge covering all the nodes of each neighborhood-regular hypergraph isadded. Furthermore, deleting the original hyperedges, does not change the node isomorphism classes ofeach hypergraph. Referring to , consider the hypergraph C = C34 C35, the hypergraph with two3-regular hypergraphs C34 and C35 acting as two connected components of C. As shown in , the noderepresentations of the two hypergraphs are identical due to Theorem 4.3.",
  "h((V, Ete), e) predicts whether e (Ete)gt \\ Ete, e 2V": "We will assume that the unobservable hyperedges are of the same size k so that we only need to predict onk-node sets. In order to preserve the most information while still respecting topological structure, we aimto start with an invariant multi-node representation to predict hyperedges and increase its expressiveness,as defined in Definition 2.8. For input hypergraph H and its matrix representation H, to do the predictionof a missing hyperedge on node subsets, we use a multi-node representation h(S, H) for S V(H) as inEquation 7 due to its simplicity, guaranteed invariance, and improve its expressivity. We aim to not affectthe computational complexity since message passing on hypergraphs is already quite expensive, especiallyon GPU memory.",
  "guarantee of improving expressivity, see Lemma 5.4 and Theorems 5.5, 5.6. For an illustration of the dataaugmentation, see": "Alternatively, downstream training using the output of Algorithm 1 can be done. Similar to subgraph NNs,this is done by applying an ensemble of models Alsentzer et al. (2020); Papp et al. (2021); Tan et al. (2023),with each model trained on transformations of H with its symmetric subhypergraphs randomly replaced.This, however, is computationally expensive. Illustration:In an illustration of Algorithm 1 is shown. For the hypergraph shown in (a), two graph cycles are glued at a single node while a separate hypergraph is glued at that same node.With 1-GWL-1, there is a large class of nodes labeled \"a\" that are indistinguishable. Each of the connectedcomponents of these 1-GWL-1 node classes is covered by a single hyperedge (light blue box) to form a multi-hypergraph. We show in .1 that in this multihypergraph, the nodes express more of the originalhypergraph. The data augmentation procedure during downstream training can then be applied to the blueboxes and the original hyperedges within each blue box separately.",
  "Prediction Guarantees:": "In order to guarantee that the GWL-1 symmetric components Rc,i found by Algorithm 12 carry additionalinformation, there needs to be a separation between them to prevent an intersection between the rootedtrees computed by GWL-1. We redefine from the main paper what it means for two node subsets to besufficiently separated via the shortest hyperedge path distance between nodes in V as follows:",
  "A collection of node subsets C 2V is sufficiently L-separated if all pairs of node subsets are sufficientlyL-separated": "Our definition of sufficiently L-separated is similar in nature to that of well separation between point setsCallahan & Kosaraju (1995) in Euclidean space. Assuming that the CcL are sufficiently L-separated fromeach other, intuitively meaning that no two nodes from two separate VRcL,i RV are within L hyperedgesaway, then the cardinality of each component |VRcL,i| is recognizable. This is stated in the following lemma:",
  "(n2k) many pairs of k-node sets S1 S2 such that (hLu(H))uS1 = (hLvS2(H)) = C, as ordered k-tuples,while h(S1, HL) = h(S2, HL) also by L steps of GWL-1": "The conditions of Theorem 5.5 assume an arbitrarily large hypergraph that is sparse and for every cL GLhas CcL sufficiently L-separated, has a bounded set of node set sizes from ScL and a controlled growth foreach node set size from ScL. The idea behind the proof of Theorem 5.5 is that under these conditions, thereare enough isomorphic rooted trees computed by L-GWL-1. It can then be shown that over all pairs ofk-sets of nodes with elementwise isomorphic rooted trees, that they can be distinguished by the componentsize they belong in. We give a simple example hypergraph that illustrates the condition of Theorem 5.5. Example:A simple example of a hypergraph that statisfies the conditions of Theorem 5.5 is a unionof many disconnected hypergraphs H = iHi = (V, E) with |VHi| S where S < is a small constantindependent of n = |V| S. Such a hypergraph could be a social network where the nodes are user instancesand the hyperedges are private groups. The disconnected hypergraphs represent disconnected communitieswhere a user can only belong to a single community.",
  "This follows by the fact that the identity augmentation is in the support of the distribution of randomaugmentations and that Sym(hL( HL)) = Autc( B2LV,E( H)) by Theorem 4.4": "Since hyperGNNs represent each node v V by message passing through the neighbors in the rooted tree( BV,E)v at v. If probabilities are assigned between nodes, then T layers of a hyperGNN can be viewed ascomputing the random walk probability of ending on any node starting from some uniformly chosen node.We define these terms in the following:",
  "deg(u)|e|, where (e) : E issome discrete probability distribution on the hyperedges. When not specified, this is the constant 1 function": "Assuming H is connected, let Xt V denote the state of the Markov chain at step t with P(X0 = v) =1|V|, v V. Letting t , this probability converges to the stationary distribution on the nodes V, whichis independent of the time. This is expressed in the following definition:",
  "is a connected (multi) hypergraph": "For the downstream training, we show that there are Bernoulli hyperedge drop/attachment probabilitiesp, qi respectively for each RcL,i so that the stationary distribution doesnt change. This shows that our dataaugmentation can still preserve the low frequency random walk signal. Proposition 5.8. For a connected hypergraph H = (V, E), let (RV , RE) be the output of Algorithm 1 on H.Then there are Bernoulli probabilities p, qi for i = 1, ..., |RV | for attaching a covering hyperedge so that isan unbiased estimator of . The intuition for Proposition 5.8 is that if a hyperedge is added to cover a connected subhypergraph RcL,icontaining at least one hyperedge, then allowing any of the hyperedges in RcL,i to drop is enough to keepthe estimated stationary distribution unbiased.",
  "(f) cat-edge-madison-restaurants": "We evaluate our method on higher order link prediction with many of the standard hypergraph neu-ral network methods.Due to potential class imbalance, we measure the PR-AUC of higher orderlink prediction on the hypergraph datasets.These datasets are: cat-edge-DAWN, cat-edge-music-blues-reviews, contact-high-school, contact-primary-school, email-Eu, cat-edge-madison-restaurants.These datasets range from representing social interactions as they develop over time tocollections of reviews to drug combinations before overdose. We also evaluate on the amherst41 dataset,which is a graph dataset. All of our datasets are unattributed hypergraphs/graphs. Data Splitting:For the hypergraph datasets, each hyperedge in it is paired with a timestamp (a realnumber).These timestamps are a physical time for which a higher order interaction, represented by ahyperedge, occurs. We form a train-val-test split by letting the train be the hyperedges associated with the80th percentile of timestamps, the validation be the hyperedges associated with the timestamps in betweenthe 80th and 85th percentiles. The test hyperedges are the remaining hyperedges. The train validation andtest datasets thus form a partition of the nodes. We do the task of hyperedge prediction for sets of nodesof size 3, also known as triangle prediction. Half of the size 3 hyperedges in each of train, validation andtest are used as positive examples. For each split, we select random subsets of nodes of size 3 that do notform hyperedges for negative sampling. We maintain positive/negative class balance by sampling the samenumber of negative samples as positive samples. Since the test distribution comes from later time stampsthan those in training, there is a possibility that certain datasets are out-of-distribution if the hyperedgedistribution changes. For the graph dataset, the single graph is deterministically split into 80/5/15 for train/val/test. We remove10% of the edges in training and let them be positive examples Ptr to predict. For validation and test, weremove 50% of the edges from both validation and test to set as the positive examples Pval, Pte to predict.For train, validation, and test, we sample |Ptr|, |Pval|, |Pte| negative link samples from the links of train,validation and test.",
  "hyperGNN Baseln.+edrop0.61 0.030.61 0.030.61 0.090.71 0.060.71 0.020.69 0.050.73 0.090.73 0.04": "APPNP0.42 0.070.42 0.070.42 0.070.42 0.070.42 0.070.42 0.070.42 0.070.42 0.07APPNP+edrop0.42 0.030.42 0.030.42 0.030.42 0.030.42 0.030.42 0.030.42 0.030.42 0.03GAT0.49 0.060.49 0.060.49 0.060.49 0.060.49 0.060.49 0.060.49 0.060.49 0.06GAT+edrop0.49 0.060.49 0.060.49 0.060.49 0.060.49 0.060.49 0.060.49 0.060.49 0.06GCN20.56 0.120.56 0.120.56 0.120.56 0.120.56 0.120.56 0.120.56 0.120.56 0.12GCN2+edrop0.54 0.020.54 0.020.54 0.020.54 0.020.54 0.020.54 0.020.54 0.020.54 0.02GCN0.40 0.030.40 0.030.40 0.030.40 0.030.40 0.030.40 0.030.40 0.030.40 0.03GCN+edrop0.65 0.040.65 0.040.65 0.040.65 0.040.65 0.040.65 0.040.65 0.040.65 0.04GIN0.73 0.100.73 0.100.73 0.100.73 0.100.73 0.100.73 0.100.73 0.100.73 0.10",
  "Architecture and Training": "Our algorithm serves as a preprocessing step for selective data augmentation.Given a single traininghypergraph H, the Algorithm 1 is applied and during training, the identified hyperedges of the symmetricinduced subhypergraphs of H are randomly replaced with single hyperedges that cover all the nodes of eachinduced subhypergraph. Each symmetric subhypergraph has a p = 0.5 probability of being selected. To geta large set of symmetric subhypergraphs, we run 2 iterations of GWL-1. We implement h(S, H) from Equation 7 as follows.Upon extracting the node representations from thehypergraph neural network, we use a multi-layer-perceptron (MLP) on each node representation, sum acrosssuch compositions, then apply a final MLP layer after the aggregation. We use the binary cross entropy losson this multi-node representation for training. We always use 5 layers of hyperGNN convolutions, a hiddendimension of 1024, and a learning rate of 0.01.",
  "Higher Order Link Prediction Results": "We show in the comparison of PR-AUC scores amongst the baseline methods of HGNN, HGNNP,HNHN, HyperGCN, UniGIN, UniGAT, UniSAGE, their hyperedge dropped versions, and \"Our\" method,which preprocesses the hypergraph to break symmetry during training. For the hyperedge drop baselines,there is a uniform 50% chance of dropping any hyperedge. We use the Laplacian eigenmap Belkin & Niyogi(2003) positional encoding on the clique expansion of the input hypergraph. This is common practice in(hyper)link prediction and required for using a hypergraph neural network on an unattributed hypergraph. We show in the PR-AUC scores on the amhrest41. Along with hyperGNN architectures we use forthe hypergraph experiments, we also compare with standard GNN architectures: APPNP Gasteiger et al.(2018), GAT Velikovi et al. (2017), GCN2 Chen et al. (2020a), GCN Kipf & Welling (2016a), GIN Xuet al. (2018), and GraphSAGE Hamilton et al. (2017). For every hyperGNN/GNN architecture, we alsoapply drop-edge Rong et al. (2019) to the input graph and use this also as baseline. The number of layers ofeach GNN is set to 5 and the hidden dimension at 1024. For APPNP and GCN2, one MLP is used on theinitial node positional encodings. Overall, our method performs well across a diverse range of higher order network datasets. We observethat our method can often outperform the baseline of not performing any data perturbations as well asthe same baseline with uniformly random hyperedge dropping. Our method has an added advantage ofbeing explainable since our algorithm works at the data level. There was also not much of a concern forcomputational time since our algorithm runs in time O(nnz(H) + n + m), which is optimal since it is thesize of the input.",
  "Empirical Observations on the Components Discovered by the Algorithm": "According to Proposition B.14, we know that the symmetry finding algorithm always covers the hypergraphand thus that we can generate on the order of O(2|V|) counterfactual multi-hypergraphs. It is known thata large set of data augmentations during learning improves learner generalization. The cover by GWL-1symmetric components follows a distribution depending on the data. We show in the distributions for the component sizes over all GWL-1 symmetric connected compo-nents for samples from the Hy-MMSBM model Ruggeri et al. (2023). This is a model to sample hypergraphswith community structure. In a we sample hypergraphs with 3 isolated communities, meaning thatthere is 0 chance of any interconnections between any two communities. In b we sample hypergraphswith 3 communities where every node in a community has a weight of 1 to stay in its community and aweight of 0.2 to move out to any other community. We plot the boxplots as a function of increasing num-ber of nodes. We notice that the more communication there is between communities for more nodes thereis more spread in possible connected component sizes. Isolated communities should make for predictableclusters/connected components.",
  "Discussion": "Our proposed data augmentation method uses symmetry breaking to handle the symmetries induced byGWL-1 based hyperGNNs. Symmetry breaking provides some guarantees that the symmetries induced bythe hyperGNN are brought closer to the symmetries of the training hypergraph. These include hyperlinkprediction guarantees. In addition, symmetry breaking prepares the hyperGNN for an automorphism groupdifferent from the symmetries it views during training. This brings about an important question regardingthe invariant automorphism group across training and testing, namely:",
  "A temporal shift from training to testing means that the testing hyperedges have a temporal future relation-ship with the training hyperedges": "We formalize a hypergraph in terms of a physical system that can change over time with the followingassumptions.Assumption 7.1. If we view a hypergraph as a physical system of particles where only the nodes have mass,then the task of transductive higher order link prediction is on a closed system Landau & Lifshitz (1960).This means no mass can enter or leave this system.",
  "(20)": "where nv(H) |{u V : u =H v}| is the number of nodes u V isomorphic to node v, including v itself,(See Definition 2.7).Assumption 7.2. (Second Law of Thermodynamics ): The second law of thermodynamics Carnot(1978) states that the entropy of a closed system must increase over time. This is denoted by the followingequation:S > 0(21)",
  "This law can be used in terms of the hypergraph topological entropy as defined in Assumption 7.1": "We also assume the following random model on the hypergraph we predict on. Let = tr, te representtemporally ordered training and testing distributions where (E)gt = (E)gt \\ E are the hyperedges thatcomplete H to (H)gt and are predicted from E:Assumption 7.3. For each node v V, let Xv be independent Bernoulli random variables of some probabilityqv . Let |(Ete)gt| be the number of testing hyperedges of H. It is a random variable defined by:",
  "Nodes rarely increase their isomorphism class size:": "For the complementary case, nv((Htr)gt) nv((Hte)gt), v U V, we show that under Assumption 7.3,node isomorphism class cardinality growth occurs with low probability due to an anti-concentration boundon multivariate polynomials Fox et al. (2021). In this complementary case, we must have that the nodes in U V increased the number of nodes isomorphicto them. This implies that each of these nodes v U must have changed their degree vector upon changing(Htr)gt to (Hte)gt. This change requires that some other node u V, u = v obtains a degree vector equal tothe degree vector of v.",
  "for any s > 0 which may depend on n and any x R. Setting x := M, s := D, gives the last inequality": "Theorem 7.1 states that the ground truth node isomorphism classes for the nodes must shrink with probabilityon order 1 O( 1n). Thus, for large n >> 0, we have that with high probability that the nodes v U V, U = have nv((Htr)gt) > nv((Hte)gt). We can recognize this property of (Hte)gt by shrinking the automorphism group that the hypergraph encoderrecognizes from the training hypergraph.According to Proposition 5.7, symmetry breaking as given inEquation 18, does this. Our method breaks the symmetry of a GWL-1 based hyperGNN. This, of course, is not of importance if thesymmetry group of the GWL-1 based hyperGNN on some training hypergraph is already the trivial group.Nonetheless, our symmetry breaking method is theoretically beneficial. Our method can also be used withinother downstream learning methods such as feature averaging Lyle et al. (2020), and ensemble methods, asmentioned in .",
  "Conclusion": "Many existing hyperGNN architectures are based on the GWL-1 algorithm, which is a hypergraph isomor-phism testing algorithm. We have characterized and identified the limitations of GWL-1. GWL-1 views thehypergraph as a collection of rooted trees. This means that hyperGNNs recognize more symmetries than thenatural automorphisms of the training hypergraph. In fact, maximally connected subsets of nodes that sharethe same value of GWL-1, which act like regular hypergraphs, are indistinguishable. To address this issuewhile respecting the structure of a hypergraph, we have devised a preprocessing algorithm that identifiesall such connected components. These components cover the hypergraph and allow for downstream dataaugmentation of symmetry breaking by training on a random multi-hypergraph. We show that this approachimproves the expressivity of a hyperGNN learner, including in the case of hyperlink prediction. We performextensive experiments to evaluate the effectiveness of our approach and make empirical observations aboutthe output of the algorithm on hypergraph data.",
  "Mikhail Belkin and Partha Niyogi. Laplacian eigenmaps for dimensionality reduction and data representation.Neural computation, 15(6):13731396, 2003": "Austin R. Benson, Rediet Abebe, Michael T. Schaub, Ali Jadbabaie, and Jon Kleinberg. Simplicial closureand higher-order link prediction. Proceedings of the National Academy of Sciences, 2018a. ISSN 0027-8424.doi: 10.1073/pnas.1800683115. Austin R Benson, Rediet Abebe, Michael T Schaub, Ali Jadbabaie, and Jon Kleinberg. Simplicial closure andhigher-order link prediction. Proceedings of the National Academy of Sciences, 115(48):E11221E11230,2018b.",
  "Garrett Birkhoff. Lattice theory, volume 25. American Mathematical Soc., 1940": "Cristian Bodnar, Fabrizio Frasca, Nina Otter, Yuguang Wang, Pietro Lio, Guido F Montufar, and MichaelBronstein. Weisfeiler and lehman go cellular: Cw networks. Advances in Neural Information ProcessingSystems, 34:26252640, 2021. Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana Yakhnenko. Translat-ing embeddings for modeling multi-relational data. In C.J. Burges, L. Bottou, M. Welling, Z. Ghahra-mani, and K.Q. Weinberger (eds.), Advances in Neural Information Processing Systems, volume 26. Cur-ran Associates, Inc., 2013. URL Giorgos Bouritsas, Fabrizio Frasca, Stefanos Zafeiriou, and Michael M Bronstein. Improving graph neuralnetwork expressivity via subgraph isomorphism counting. IEEE Transactions on Pattern Analysis andMachine Intelligence, 45(1):657668, 2022. Derun Cai, Moxian Song, Chenxi Sun, Baofeng Zhang, Shenda Hong, and Hongyan Li. Hypergraph structurelearning for hypergraph neural networks. In Proceedings of the Thirty-First International Joint Conferenceon Artificial Intelligence, IJCAI-22, pp. 19231929, 2022. Paul B. Callahan and S. Rao Kosaraju. A decomposition of multidimensional point sets with applicationsto k-nearest-neighbors and n-body potential fields. J. ACM, 42(1):6790, jan 1995. ISSN 0004-5411. doi:10.1145/200836.200853. URL",
  "Eli Chien, Chao Pan, Jianhao Peng, and Olgica Milenkovic. You are allset: A multiset function frameworkfor hypergraph neural networks. arXiv preprint arXiv:2106.13264, 2021": "Eli Chien, Chao Pan, Jianhao Peng, and Olgica Milenkovic. You are allset: A multiset function frameworkfor hypergraph neural networks. In International Conference on Learning Representations, 2022. URL Uthsav Chitra and Benjamin Raphael. Random walks on hypergraphs with edge-dependent vertex weights.In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th International Conferenceon Machine Learning, volume 97 of Proceedings of Machine Learning Research, pp. 11721181. PMLR,0915 Jun 2019. URL Yun Young Choi, Sun Woo Park, Youngho Woo, and U Jin Choi.Cycle to clique (cy2c) graph neuralnetwork: A sight to see beyond neighborhood aggregation. In The Eleventh International Conference onLearning Representations. Nicolas A Crossley, Andrea Mechelli, Petra E Vrtes, Toby T Winton-Brown, Ameera X Patel, Cedric EGinestet, Philip McGuire, and Edward T Bullmore. Cognitive relevance of the community structure ofthe human brain functional coactivation network. Proceedings of the National Academy of Sciences, 110(28):1158311588, 2013.",
  "Ryoma Sato, Makoto Yamada, and Hisashi Kashima. Random features strengthen graph neural networks.In Proceedings of the 2021 SIAM international conference on data mining (SDM), pp. 333341. SIAM,2021": "Sameer Singh, Amarnag Subramanya, Fernando Pereira, and Andrew McCallum. Wikilinks: A large-scalecross-document coreference corpus labeled via links to Wikipedia. Technical Report UM-CS-2012-015,2012. Arnab Sinha, Zhihong Shen, Yang Song, Hao Ma, Darrin Eide, Bo-June (Paul) Hsu, and Kuansan Wang. Anoverview of microsoft academic service (MAS) and applications. In Proceedings of the 24th InternationalConference on World Wide Web. ACM Press, 2015. doi: 10.1145/2740908.2742839. URL",
  "Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks?arXiv preprint arXiv:1810.00826, 2018": "Naganand Yadati, Madhav Nimishakavi, Prateek Yadav, Vikram Nitin, Anand Louis, and Partha Talukdar.Hypergcn: A new method for training graph convolutional networks on hypergraphs. Advances in neuralinformation processing systems, 32, 2019. Huaxiu Yao, Caroline Choi, Bochuan Cao, Yoonho Lee, Pang Wei W Koh, and Chelsea Finn. Wild-time: Abenchmark of in-the-wild distribution shift over time. Advances in Neural Information Processing Systems,35:1030910324, 2022a. Huaxiu Yao, Yu Wang, Sai Li, Linjun Zhang, Weixin Liang, James Zou, and Chelsea Finn. Improving out-of-distribution robustness via selective augmentation. In International Conference on Machine Learning,pp. 2540725437. PMLR, 2022b. Hao Yin, Austin R. Benson, Jure Leskovec, and David F. Gleich.Local higher-order graph clustering.In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and DataMining. ACM Press, 2017. doi: 10.1145/3097983.3098069. URL",
  "Bohang Zhang, Shengjie Luo, Liwei Wang, and Di He. Rethinking the expressive power of gnns via graphbiconnectivity. arXiv preprint arXiv:2301.09505, 2023": "Muhan Zhang and Yixin Chen. Weisfeiler-lehman neural machine for link prediction. In Proceedings of the23rd ACM SIGKDD international conference on knowledge discovery and data mining, pp. 575583, 2017. Muhan Zhang and Pan Li. Nested graph neural networks. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S.Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems, volume 34,pp. 1573415747. Curran Associates, Inc., 2021. URL Muhan Zhang, Zhicheng Cui, Shali Jiang, and Yixin Chen. Beyond link prediction: Predicting hyperlinksin adjacency space. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 32, 2018. Muhan Zhang, Pan Li, Yinglong Xia, Kai Wang, and Long Jin. Labeling trick: A theory of using graph neuralnetworks for multi-node representation learning. Advances in Neural Information Processing Systems, 34:90619073, 2021.",
  "h0v Xv, v VGhi+1v {{(hiv, hiu)}}uNbrA(v), v VG(36)": "The algorithm terminates after the vertex labels converge. For graph isomorphism testing, the concatenationof the histograms of vertex labels for each iteration is output as the graph representation. Since we are onlyconcerned with node isomorphism classes, we ignore this step and just consider the node labels hiv for everyv VC. The WL-1 isomorphism test can be characterized in terms of rooted tree isomorphisms between the universalcovers for connected graphs Krebs & Verbitsky (2015). There have also been characterizations of WL-1 interms of counting homomorphisms Knill (2013) as well as the Wasserstein Distance Chen et al. (2022) andMarkov chains Chen et al. (2023). A graph neural network (GNN) is a message passing based node representation learner modeled after theWL-1 algorithm. It has the important inductive bias of being equivariant to node indices. As a neuralmodel of the WL-1 algorithm, it learns neural weights common across all nodes in order to obtain a vectorrepresentation for each node. A GNN must use some initial node attributes in order to update its neuralweights. There are many variations on GNNs, including those that improve the distinguishing power beyondWL-1. For two surveys on the GNNs and their applications, see Zhou et al. (2020); Wu et al. (2020).",
  "A.2Link Prediction": "The task of link prediction on graphs involves the prediction of the existence of links. There are two kinds oflink prediction. There is transductive link prediction where the same nodes are used for all of train validationand testing. There is also inductive link prediction where the test validation and training nodes can all bedisjoint. Some existing works on link prediction include Zhang & Chen (2017). Higher order link predictionis a generalization of link prediction to hypergraph data. A common way to do link prediction is to compute a node-based GNN and for a pair of nodes, aggregate,similar to in graph auto encoders Kipf & Welling (2016b), the node representations in any target pair inorder to obtain a 2-node representation. Such aggregations are of the form:",
  "A.3More Related Work": "The work of Wei et al. (2022) also does a data augmentation scheme. It considers randomly dropping edgesand generating data through a generative model on hypergraphs.The work of Lee & Shin (2022) alsoperforms data augmentation on a hypergraph so that homophilic relationships are maintained. It does thisthrough contrastive losses at the node to node, hyperedge to hyperedge and intra hyperedge level. Neitherof these methods provide guarantees for their data augmentations. As mentioned in the main text, an ensemble of neural networks can be used with a drop-out Baldi & Sadowski(2014) like method on the output of the Algorithm. Subgraph neural networks Alsentzer et al. (2020); Tanet al. (2023) are ensembles of models on subgraphs of the input graph.",
  "We recall the definition of an isomorphism between hypergraphs:": "Definition B.3. For two hypergraphs H and D, a structure preserving map : H D is a pair ofmaps = (V : VH VD, E : EH ED) such that e EH, E(e) {V(vi) | vi e} ED.Ahypergraph isomorphism is a structure preserving map = (V, E) such that both V and E are bijective.Two hypergraphs are said to be isomorphic, denoted as H = D, if there exists an isomorphism between them.When H = D, an isomorphism is called an automorphism on H. All the automorphisms form a group,which we denote as Aut(H).",
  "Proposition B.1. Aut(H) = Stab(H) are equivalent as isomorphic groups": "Proof. Consider Aut(H), define the map : := |V(H). The group element Sym(V) acts as astabilizer of H since for any entry (v, e) in H, H1(v),1(e) = ( H)v,e = 1 iff 1(e) EH iff e EH iffHv,e = 1 = H1(v),1(e). Since (v, e) was arbitrary, preserves the positions of the nonzeros. We can check that is a well defined injective homorphism as a restriction map. Furthermore it is surjectivesince for any Stab(H), we must have Hv,e = 1 iff ( H)v,e = H1(v),1(e) = 1 which is equivalent tov e E iff (v) (e) E which implies e E iff (e) E. Thus is a group isomorphism from Aut(H)to Stab(H) In other words, to study the symmetries of a given hypergraph H, we can equivalently study the auto-morphisms Aut(H) and the stabilizer permutations Stab(H) on its star expansion adjacency matrix H.Intuitively, the stabilizer group 0 Stab(H) Sym(V) characterizes the symmetries in a graph. Whenthe graph has rich symmetries, say a complete graph, Stab(H) = Sym(V) can be as large as the wholepermutaion group.",
  "Definition B.4. For a given hypergraph H with star expansion matrix H, two k-node sets S, T V arecalled isomorphic, denoted as S T, if Stab(H), (S) = T and (T) = S": "When k = 1, we have isomorphic nodes, denoted u =H v for u, v V. Node isomorphism is also studied asthe so-called structural equivalence in Lorrain & White (1971).Furthermore, when S T we can then saythat there is a matching due to the graph of the map of the form {(s, (s)) : s S}. This matching isbetween the node sets S and T so that matched nodes are isomorphic.",
  "These follow by the definition of multiset equality and since there is no loss of information upon factoringout a constant tuple entry of each pair in the multisets": "PropositionB.3. The update steps of GWL-1:f i(H)[f ie1(H), , f iem(H)] and hi(H)[hiv1(H), , hivn(H)], are permutation equivariant; in other words, For any Sym(V), let f i(H) [f i1(e1)(H), , f i1(em)(H)] and hi(H) [hi1(v1)(H), , hi1(vn)(H)], we have i N, f i(H) =f i( H) and hi(H) = hi( H)",
  "(48)": "Definition B.7. Let h : [V]k Zn2n2 Rd be a k-node representation on a hypergraph H. Let H Zn2n2be the star expansion adjacency matrix of H for n nodes. The representation h is k-node most expressive ifS, S V, |S| = |S| = k, the following two conditions are satisfied:",
  "The converse, that h(S, H) is k-node expressive, is not necessarily true since we cannot guarantee h(S, H) =h(S, H) implies the existence of a permutation that maps S to S (see Zhang et al. (2021))": "A hypergraph can be represented by a bipartite graph BV,E from V to E where there is an edge (v, e) in thebipartite graph iff node v is incident to hyperedge e. This bipartite graph BV,E is called the star expansionbipartite graph. We introduce a more structured version of graph isomorphism called a 2-color isomorphism to characterizehypergraphs. It is a map on 2-colored graphs, which are graphs that can be colored with two colors so thatno two nodes in any graph with the same color are connected by an edge. We define a 2-colored isomorphismformally here: Definition B.8. A 2-colored isomorphism is a graph isomorphism on two 2-colored graphs that preservesnode colors. In particular, between two graphs G1 and G2 the vertices of one color in G1 must map tovertices of the same color in G2. It is denoted by =c. A bipartite graph must always have a 2-coloring. In fact, the 2-coloring with all the nodes in the nodebipartition colored red and all the nodes in the hyperedge bipartition colored blue forms a canonical 2-coloring of BV,E. Assume that all star expansion bipartite graphs are canonically 2-colored.",
  "has the structure preserving property that (u1, ..., uk) E1 iff ((u1), ..., (uk)) E2": "We may induce a 2-colored isomorphism : V(BV1,E1) V(BV1,E1) so that |L(BV1,E1) = where equalityhere means that |L(BV1,E1) acts on L(BV1,E1) the same way that does on V1. Furthermore has theproperty that |R(BV1,E1)(u1, ..., uk) = ((u1), ..., (uk)), (u1, ..., uk) E1, following the structure preserv-ing property of isomorphism .",
  "The map is also a 2-colored map since it maps L(BV1,E1) to L(BV2,E2) and R(BV1,E1) to R(BV2,E2)": "We can also check that the map is structure preserving and thus a 2-colored isomorphism since(ui, (u1, ..., ui, ..., uk)) E(BV1,E1), i = 1, ..., k iff (ui V1 and (u1, ..., ui, ..., uk) E1) iff (ui) V2and ((u1), ..., (ui), ..., (uk)) E2 iff ((ui), ((u1, ..., ui, ..., uk)) E(BV2,E2), i = 1, ..., k. This followsfrom being structure preserving and the definition of .",
  "In fact, Lemma B.6 holds for 2-colored isomorphisms, which we show below:": "Lemma B.7. Let T and S be 2-colored trees and x V (T) and y V (S) be their vertices of the same degreewith neighborhoods N(x) = {x1, ..., xk} and N(y) = {y1, ..., yk}. Let r 1. Suppose that T r1x=c Sr1yandT rxi =c Sryi for all i k. Then T r+1x=c Sr+1y.",
  "Proof. Certainly 2-colored isomorphisms are rooted isomorphisms on 2-colored trees. The converse is trueif the roots match in color since recursively all descendants of the root must match in color": "If T r1x=c Sr1yand T rxi =c Sryi for all i k and N(x) = {x1, ..., xk}, N(y) = {y1..yk}, the roots x and ymust match in color. The neighborhoods N(x) and N(y) then must both be of the opposing color. Sincerooted colored isomorphisms are rooted isomorphisms, we must have T r1x= Sr1yand T rxi = Sryi for alli k. By Lemma B.6, we have T r+1x= Sr+1y. Once the roots match in color, a rooted tree isomorphism is thesame as a rooted 2-colored tree isomorphism. Thus, since x and y share the same color, T r+1x=c Sr+1y Theorem B.8. Let H1 = (V1, E1) and H2 = (V2, E2) be two connected hypergraphs. Let BV1,E1 and BV2,E2be two canonically colored bipartite graphs for H1 and H2 (vertices colored red and hyperedges colored blue)",
  "Proof. We prove by induction:": "Let T ke1 := ( BkV1,E1)e1 where e1 is a pullback of a hyperedge, meaning pBV1,E2(e1) = e1.Similarly, letT ke2 := ( BkV2,E2)e2, T kx1 := ( BkV1,E1)x1, T kx2 := ( BkV2,E2)x2, k N, where e1, e2, x1, x2 are the respectivepullbacks of e1, e2, x1, x2. Define an (2-colored) isomorphism of multisets of graphs to mean that there exists a bijection between thetwo multisets so that each graph in one multiset is (2-colored) isomorphic with exactly one other element inthe other multiset.",
  "We write here the theorem characterizing the WL-1 algorithm on a graph by the graphs universal cover": "Theorem B.9. [Krebs & Verbitsky (2015)] Let G and H be two connected graphs. Let pG : G G, pH :H H be the universal covering maps of G and H respectively. For any i N, for any two nodes x Gand y H: Gix = Giy iff the WL-1 algorithm assigns the same value to nodes x = pG(x) and y = pH(y).",
  "It follows immediately from Theorem B.9 that the WL-1 algorithm on colored star expansion bipartite graphsof two hypergraphs corresponds to constructing their universal covers": "Corollary 2. Let H1 = (V1, E1) and H2 = (V2, E2) be two connected hypergraphs. Let BV1,E1 and BV2,E2 betwo canonically colored bipartite graphs for H1 and H2 (vertices colored red and hyperedges colored blue).Let pBV1,E1 : BV1,E1 BV1,E1, pBV2,E2 : BV2,E2 BV2,E2 be the universal coverings of BV1,E1 and BV2,E2respectively.For any i Z+,",
  "The successive two equivalences follow by Theorem B.8 and the first equivalence": "Observation 3. If the node values for nodes x and y from GWL-1 for i iterations on two hypergraphs H1and H2 are the same, then for all j with 0 j i, the node values for GWL-1 for j iterations on x and yalso agree. In particular deg(x) = deg(y).",
  "hi+1v 1(hiv, hi+1e) = Wv H hi+1e(65b)": "for constant We and Wv weight matrices, is equivalent to GWL-1 provided that 1 and 2 are both injectiveas functions. Without injectivity, we can only guarantee that if UniGCN distinguishes H1, H2 then GWL-1distinguishes H1, H2. In fact, each matrix power of order n in Equation 63 corresponds to hnv so long as wesatisfy the following constraints:",
  "Proof.1. First if and only if :": "By Theorem B.8, GWL-1 symmetric hypergraph H = (V, E) means that for every pair of nodes u, v V, ( BV,E)u =c ( BV,E)v.This implies that for any L 1, ( B2LV,E)u =c ( B2LV,E)v by restricting the rootedisomorphism to 2L-hop rooted subtrees, which means that hLu(H) = hLv (H). The converse is true since L isarbitrary. If there are no cycles, we can just take the isomorphism for the largest. Otherwise, an isomorphismcan be constructed for L = by infinite extension.",
  "Let pBV,E be the universal covering map for BV,E. Denote v, u by the lift of some nodes v, u V by pBV,E": "Let ( N(u))u be the rooted bipartite lift of (N(u))u. If H is L-GWL-1 symmetric for all L 1 then withL = 1, ( B2V,E)u =c ( N(u))u =c ( N(v))v =c ( B2V,E)v, iff (N(u))u = (N(v))v, u, v V since N(u) and N(v)are cycle-less for any u, v V. For the converse, assume all nodes v V have (N(v))v = (N 1)x for some1-hop rooted tree (N 1)x rooted at node x, independent of any v V. We prove by induction that for allL 1 and for all v V, ( B2LV,E)v =c ( N 2L)x for a 2L-hop tree ( N 2L)x rooted at node x.",
  "Base case: L = 1 is by assumption": "Inductive step: If ( B2LV,E)v =c (N 2L)x, we can form ( B2L+2V,E)v by attaching ( N(u))u to each node u in the2L-th layer of ( B2LV,E)v =c ( N 2L)x.Each ( N(u))u is independent of the root v since every u V has( N(u))u =c ( N 2)x iff (N(u))u = (N 1)x for an x independent of u V. This means ( B2L+2V,E)v =c ( N 2L+2)xfor the same root node x where ( N 2L+2)x is constructed in the same manner as ( B2LV,E)v, v V.",
  "B.3.1Algorithm Guarantees": "Continuing with the notation, as before, let H = (V, E) be a hypergraph with star expansion matrix Hand let (RV, RE) be the output of Algorithm 1 on H for L Z+. Denote CcL as the set of all connectedcomponents of HcL:CcL {CcL : conn. comp. CcL of HcL}(72)",
  "Proof. Let pBV,E be the universal covering map for BV,E.Denote v, u, v, u by the lift of some nodesv, u, v, u V by pBV,E": "Let L = and let Hc = (Vc, Ec). For any i, since u, v Vc, ( BV,E)u =c ( BV,E)v for all u, v VRc,i.Since Rc,i is maximally connected we know that every neighborhood NHc(u) for u Vc induced by Hc hasNHc(u) = N(u) Hc. Since L = we have that NHc(u) = NHc(v), u, v VRc,i since otherwise WLOGthere are u, v VRc,i with NHc(u) = NHc(v) then WLOG there is some hyperedge e ENHc(u) withsome w e, w = u where e cannot be in isomorphism with any e ENHc(v). For two hyperedges to be inisomorphism means that their constituent nodes can be bijectively mapped to each other by a restriction ofan isomorphism between NHc(u), NHc(v) to one of the hyperedges. This means that ( BV\\{u},E)w is therooted universal covering subtree centered about w not passing through u that is connected to u ( BV,E)uby e. However, v has no e and thus cannot have a Tx for x V( N(v))v satisfying Tx =c ( BV\\{u},E)w withx connected to v by a hyperedge e isomorphic to e in its neighborhood in ( BV,E)v. This contradicts that( BV,E)u =c ( BV,E)v.",
  "By the definition of RV, since every connected component is size atleast 1 and every node is considered, wemust have": "vV RV = V. Since each connected component of a given GWL-1 value is maximal, meaningthere is no superset of nodes that is connected, no two connected components can intersect through eithernodes or hyperedges. Furthermore, a single node can belong to only one GWL-1 value, thus the values forma partition of V. This proves V = V RVV .",
  "cL,i{VRcL,i}) be the hypergraphformed by attaching a hyperedge to each VRcL,i": "For any cL, a L-GWL-1 node class, let RcL,i, i = 1, ..., |CcL| be a connected component subhypergraph ofHcL. Over all (cL, i) pairs, all the RcL,i are disconnected from each other and for each cL each RcL,i ismaximally connected on HcL. Upon covering all the nodes VRcL,i of each induced connected component subhypergraph RcL,i with a singlehyperedge e = VRcL,i of size s = |VRcL,i|, we claim that every node of class cL becomes cL,s, a L-GWL-1node class depending on the original L-GWL-1 node class cL and the size of the hyperedge s.",
  "( B2LV, E)v =c (( B2LV,E)v (v,e) Te)v, v VRcL,i(85)": "The notation (( B2LV,E)v (v,e) Te)v denotes a tree rooted at v that is the attachment of the tree Te rooted ate to the node v by the edge (v, e). As is usual, we assume v, e are the lifts of v V, e E respectively. Weonly need to consider the single e since L was chosen small enough so that the 2L-hop tree ( B2LV, E)v does notcontain a node u satisfying pBV,E(u) = u with u VRcL,j for all j = 1, ..., |CcL|, j = i.",
  "We will need the following definition to prove the next lemma": "Definition B.14. A partial universal cover of hypergraph H = (V, E) with an unexpanded induced subhy-pergraph R, denoted U(H, R)V,E is a graph cover of BV,E where we freeze BVR,ER BV,E as an inducedsubgraph. A l-hop rooted partial universal cover of hypergraph H = (V, E) with an unexpanded induced subhypergraphR, denoted (U l(H, R)V,E)u for u V or (U l(H, R)V,E)e for e E, where v, e are lifts of v, e, is a rootedgraph cover of BV,E where we freeze BVR,ER BV,E as an induced subgraph. Lemma B.16. Assuming the same conditions as Lemma B.15, where H = (V, E) is a hypergraph andfor all L-GWL-1 node classes cL with connected components RcL,i, as discovered by Algorithm 1, so thatL diam(RcL,i). Instead of only adding the hyperedges {VRcL,i}cL,i to E as stated in the main paper, letH (V, (E \\ RE) RV ), meaning H with each RcL,i for i = 1, ..., |CcL| having all of its hyperedges droppedand with a single hyperedge that covers VRcL,i and let H = (V, E RV ) then:",
  "(U 2L(H, R)u \\ R(B(VR, ER)))u =c (U 2L(H, R)v \\ R(B(VR, ER)))v(88)": "by Proposition B.13, where U 2L(H, R)v \\ R(B(VR, ER)) denotes removing the nodes R(B(VR, ER)) fromU 2L(H, R)v. This follows since removing R(B(VR, ER)) removes an isomorphic neighborhood of hyperedgesfrom each node in VR. This requires assuming maximal connectedness of each RcL,i. Upon adding thehyperedgeecL,i VRcL,i(89) covering all of VRcL,i after the deletion of ERcL,i for every (cL, i) pair, we see that any node u VRcL,i isconnected to any other node v VRcL,i through ecL,i in the same way for all nodes u, v VRcL,i. In fact,we claim that all the nodes in VRcL,i still have the same GWL-1 class.",
  "We can write the multi-hypergraph H equivalently as (V,": "cL,i(E \\ E(RcL,i) {{ecL,i}})), which is the multi-hypergraph formed by the algorithm.The replacement operation on H can be viewed in the universalcovering space BV,E as taking U(H, R) and replacing the frozen subgraph BVR,ER with the star graphs(N H(ecL,i))ecL,i of root node ecL,i determined by hyperedge ecL,i for each connected component indexed by(cL, i). Since the star graphs (N H(ecL,i))ecL,i are cycle-less, we have that:",
  "We thus have ( B2LV H,E H)u =c ( B2LV H,E H)v for every u, v VRcL,i with u, v being the lifts of u, v by pBV,E,": "since (U 2L(H, R)u\\R(B(VR, ER)))u =c (U 2L(H, R)v \\R(B(VR, ER)))v for every u, v VRcL,i as in Equation88. These rooted universal covers now depend on a new hyperedge ecL,i and thus depend on its size s. This proves the claim that all the nodes in VRcL,i retain the same L-GWL-1 node class by changing H to Hand that this new class is distinguishable by s = |VRcL,i|. In otherwords, the new class can be determinedby cs. Furthermore, cL,s on the hyperedge ecL,i cannot become the same class as an existing class due tothe algorithm.",
  "n)) = o(n1).Therefore, due tothe pigeonhole principle, there existn": "o(n1) = (n) many nodes v whose ( BLV,E)v are isomorphic to eachother. Denote GL as the set of all L-GWL-1 values. Denote the set of these nodes as VcL, which consist ofnodes whose L-GWL-1 values are all the same value cL GL after L iterations of GWL-1 by Theorem B.8.For a fixed L, the sets VcL form a partition of V, in other words,",
  "For any cL GL, there is a partition VcL =": "s VcL,s where VcL,s is the set of nodes all of which have L-GWL-1 class cL and that belong to a connected component of size s in HcL. Let ScL {|VRcL,j| : RcL,j CcL}denote the set of sizes s 1 of connected component node sets of HcL. We know that |ScL| S where S isindependent of n.",
  ". Computing the lower bound:": "Let Y denote the number of pairs of k-node sets S1 S2 such that (hLu(H))uS1 = (hLv (H))vS2 = C =(c(L,1), ..., c(L,k)), as ordered tuples, from L-steps of GWL-1. Since if any pair of nodes u, v have the same L-GWL-1 values cL, then they become distinguishable by the size of the connected component in HcL that theybelong to. We can lower bound Y by counting over all pairs of k tuples of nodes ((u1, ..., uk), (v1, ..., vk)) (ki=1 Vc(L,i)) (ki=1 Vc(L,i)) that both have L-GWL-1 values (c(L,1), ..., c(L,k)) where there is atleast onei {1, .., k} where ui and vi belong to different sized connected components si, si Sc(L,i) with si = si. Wehave:",
  "Proof.1. Expressivity:": "Let L Z+ be arbitrary. We first prove that L-GWL-1 enhanced by Algorithm 1 is strictly more expressivefor node distinguishing than L-GWL-1 on some hypergraph(s). Let C34 and C35 be two 3-regular hypergraphsfrom . Let H = C34 C35 be the disjoint union of the two regular hypergraphs. L iterations of GWL-1will assign the same node class to all of VH. These two subhypergraphs can be distinguished by L-GWL-1for L 1 after editing the hypergraph H from the output of Algorithm 1 and becoming H = C34 C35. Thisis all shown in . Since L was arbitrary, this is true for L = .",
  "a. Case 1 (node u V has its class c changed to class cs):": "Let L Z+ be arbitrary. For any node u with L-GWL-1 class c changed to cs in H, if u =H v for anyv V, then the GWL-1 class of v must also be cs. In otherwords, both u and v belong to s-sized connectedcomponents in Hc We prove this by contradiction.",
  "i.Say v is originally of L-GWL-1 class c and changes to L-GWL-1 cs for s < s on H, WLOG": "If this is the case then v belongs to a L-GWL-1 symmetric induced subhypergraph S with |VS| = s. Sincethere is a Aut(H) with (u) = v and since s < s, by the pigeonhole principle some node w VS musthave (w) / VS. Since S and S are maximally connected, (w) cannot share the same L-GWL-1 class asw. Thus, it must be that ( B2LV,E) (w) =c ( B2LV,E) w where w, (w) are the lifts of w, (w) by universal covering",
  "ii. Say node v V has its class c unchanged": "The argument for when v does not change its class c after the algorithm, follows by noticing that since c isthe GWL-1 node class of u, cs is the GWL-1 node class of v and c = cs. Thus we must have u =H v onceagain by the contrapositive of Theorem B.8. This also gives a contradiciton.",
  "b. Case 2 (node u V has its class c unchanged):": "Now assume L = . Let pBV,E be the universal covering map of BV,E. For all other nodes u =H v foru, v V unaffected by the replacement, meaning they do not belong to any Rc,i discovered by the algorithm,if the rooted universal covering tree rooted at node u connects to any node w in l hops in ( BlV,E)u wherepBV,E(u) = u, pBV,E( w) = w and where w has any class c in H, then v must also connect to a node z inl hops in ( BlV,E)u where pBV,E(z) = z and w =H z. Furthermore, if w becomes class cs in H due to thealgorithm, then z also becomes class cs in H. This will follow by the previous result on isomorphic w and zboth of class c with w becoming class cs in H. Since L = : For any w V connected by some path of hyperedges to u V, consider the smallest l forwhich ( BlV,E)u, the l-hop universal covering tree of H rooted at u, the lift of u, contains the lifted w ofw V with GWL-1 node class c at layer l. Since u =H v by . We can use to find some z = (w). We claim that z is l hops away from v. Since u =H v due to some Aut(H) with (u) = v, usingProposition B.2 for singleton nodes and by Theorem B.8 we must have ( BlV,E)u =c ( BlV,E)v as isomorphicrooted universal covering trees due to an induced isomorphism of where we define an induced isomorphism : ( BV,E)u ( BV,E)v between rooted universal covers ( BV,E)u and ( BV,E)v for u, v V( BV,E) as (a) = bif (a) = b a, b V(BV,E) connected to u and v respectively and pBV,E(a) = a, pBV,E(b) = b. Since l is theshortest path distance from u to w, there must exist some shortest (as defined by the path length in BV,E)",
  "vV deg(v) and m is the number of hyperedges": "Proof. Computing Edeg, which requires computing the degrees of all the nodes in each hyperedge takes timeO(nnz(H)). The set Edeg can be stored as a hashset datastructure. Constructing this takes O(nnz(H)).Computing GWL-1 takes O(Lnnz(H)) time assuming a constant L number of iterations. Constructing thebipartite graphs for H takes time O(nnz(H) + n + m) since it is an information preserving data structurechange. Define for each c C, nc := |Vc|, mc := |Ec|. Since the classes partition V, we must have:",
  "cCnnz(Hc)(102)": "where Hc is the star expansion matrix of Hc. Extracting the subgraphs can be implemented as a maskingoperation on the nodes taking time O(nc) to form Vc followed by searching over the neighbors of Vc in timeO(mc) to construct Ec. Computing the connected components for Hc for a predicted node class c takestime O(nc + mc + nnz(Hc)). Iterating over each connected component for a given c and extracting theirnodes and hyperedges takes time O(nci +mci) where nc =",
  "i nci, mc =": "i mci. Checking that a connectedcomponent has size at least 3 takes O(1) time. Computing the degree on H for all nodes in the connectedcomponent takes time O(nci) since computing degree takes O(1) time. Checking that the set of node degreesof the connected component doesnt belong to Edeg can be implemented as a check that the hash of the setof degrees is not in the hashset datastructure for Edeg.",
  "= O(nnz(H) + n + m)(103c)": "Proposition B.21. For a connected hypergraph H, let (RV , RE) be the output of Algorithm 1 on H. Thenthere are Bernoulli probabilities p, qi for i = 1, ..., |RV | for attaching a covering hyperedge so that is anunbiased estimator of . Proof. Let CcL = {RcL,i}i be the maximally connected components induced by the vertices with L-GWL-1values cL. The set of vertex sets {V(RcL,i)} and the set of all hyperedges i{E(RcL,i)} over all the connectedcomponents RcL,i for i = 1, ..., CcL form the pair (RV , RE).",
  "(b) Average size of components of size atleast3 from Algorithm 1": "As we are primarily concerned with symmetries in a hypergraph, we empirically measure the size andfrequency of the components found by the Algorithm for real-world datasets. For the real-world datasetslisted in Appendix D, in a, we plot the fraction of connected components of the same L-GWL-1value (L = 2) that are atleast 3 in cardinality from Algorithm 1 as a function of the number of nodes of thehypergraph. For these datasets, it is much more common for the connected components to be of sizes 1 and2. On the right, in b we show the distribution of the sizes of the connected components found byAlgorithm 1. We see that, on average, the connected components are at least an order of magnitude smallercompared to the total number of nodes. Common to both plots, the graph datasets appear to have morenodes and a consistent fraction and size of components, while the hypergraph datasets have higher variancein the fraction of components, which is expected since there are more possibilities for the connections in ahypergraph.",
  "HGNNP": "0.550.600.650.700.750.80 baseline (0.57)drop (0.64)(0.79) Ours HGNN 0.550.600.650.700.75 drop (0.56)baseline (0.69)(0.76) Ours HNHN 0.550.600.650.700.750.800.85 Ours (0.54)drop (0.62)(0.83) baseline HyperGCN 0.500.550.600.650.700.75 baseline (0.5)drop (0.73)(0.77) Ours UniGAT 0.50.60.70.8 drop (0.49)baseline (0.7)(0.81) Ours UniGCN 0.650.660.670.680.690.70 Ours (0.66)drop (0.66)(0.69) baseline UniGIN 0.550.600.650.700.75 baseline (0.57)drop (0.66)(0.78) Ours UniSAGE Critical difference diagram of average score ranks for cat-edge-vegas-bars-reviews",
  "C.2Experiments on Graph Data": "We show in Tables 4, 5, 6 the PR-AUC test scores for link prediction on some nonattributed graph datasets.The train-val-test splits are predefined for FB15k-237 and for the other graph datasets a single graph isdeterministically split into 80/5/15 for train/val/test.We remove 10% of the edges in training and letthem be positive examples Ptr to predict. For validation and test, we remove 50% of the edges from bothvalidation and test to set as the positive examples Pval, Pte to predict. For train, validation, and test, wesample 1.2|Ptr|, 1.2|Pval|, 1.2|Pte| negative link samples from the links of train, validation and test. Alongwith hyperGNN architectures we use for the hypergraph experiments, we also compare with standardGNN architectures: APPNP Gasteiger et al. (2018), GAT Velikovi et al. (2017), GCN2 Chen et al.(2020a), GCN Kipf & Welling (2016a), GIN Xu et al. (2018), and GraphSAGE Hamilton et al. (2017).For every hyperGNN/GNN architecture, we also apply drop-edge Rong et al. (2019) to the input graphand use this also as baseline. The number of layers of each GNN is set to 5 and the hidden dimension at1024. For APPNP and GCN2, one MLP is used on the initial node positional encodings. Since graphsdo not have any hyperedges beyond size 2, graph neural networks fit the inductive bias of the graph datamore easily and thus may perform better than hypergraph neural network baselines more often than expected.",
  "Ours0.71 0.040.71 0.090.69 0.090.75 0.140.75 0.090.74 0.090.65 0.080.65 0.07": "hyperGNN Baseline0.68 0.000.69 0.060.67 0.020.75 0.040.74 0.020.74 0.000.65 0.050.64 0.08hyperGNN Baseln.+edrop0.67 0.020.70 0.070.66 0.000.75 0.030.73 0.080.74 0.050.63 0.010.64 0.03APPNP0.40 0.030.40 0.030.40 0.030.40 0.030.40 0.030.40 0.030.40 0.030.40 0.03APPNP+edrop0.40 0.130.40 0.130.40 0.130.40 0.130.40 0.130.40 0.130.40 0.130.40 0.13GAT0.49 0.030.49 0.030.49 0.030.49 0.030.49 0.030.49 0.030.49 0.030.49 0.03GAT+edrop0.51 0.050.51 0.050.51 0.050.51 0.050.51 0.050.51 0.050.51 0.050.51 0.05GCN20.50 0.090.50 0.090.50 0.090.50 0.090.50 0.090.50 0.090.50 0.090.50 0.09GCN2+edrop0.56 0.070.56 0.070.56 0.070.56 0.070.56 0.070.56 0.070.56 0.070.56 0.07GCN0.73 0.020.73 0.020.73 0.020.73 0.020.73 0.020.73 0.020.73 0.020.73 0.02",
  "GraphSAGE+edrop0.73 0.090.73 0.090.73 0.090.73 0.090.73 0.090.73 0.090.73 0.090.73 0.09": ": PR-AUC on graph dataset FB15k-237. Each column is a comparison of the baseline PR-AUCscores against the PR-AUC score for our method (first row) applied to a standard hyperGNN architecture.Red color denotes the highest average score in the column. Orange color denotes a two-way tie in the column,and brown color denotes a three-or-more-way tie in the column.",
  "hyperGNN Baseline0.65 0.060.65 0.060.65 0.040.82 0.090.74 0.040.74 0.050.75 0.030.77 0.01": "hyperGNN Baseln.+edrop0.65 0.090.65 0.000.64 0.050.82 0.000.72 0.000.74 0.070.73 0.030.72 0.07APPNP0.72 0.100.72 0.100.72 0.100.72 0.100.72 0.100.72 0.100.72 0.100.72 0.10APPNP+edrop0.71 0.050.71 0.050.71 0.050.71 0.050.71 0.050.71 0.050.71 0.050.71 0.05GAT0.64 0.060.64 0.060.64 0.060.64 0.060.64 0.060.64 0.060.64 0.060.64 0.06GAT+edrop0.61 0.090.61 0.090.61 0.090.61 0.090.61 0.090.61 0.090.61 0.090.61 0.09GCN20.66 0.030.66 0.030.66 0.030.66 0.030.66 0.030.66 0.030.66 0.030.66 0.03GCN2+edrop0.65 0.100.65 0.100.65 0.100.65 0.100.65 0.100.65 0.100.65 0.100.65 0.10GCN0.69 0.030.69 0.030.69 0.030.69 0.030.69 0.030.69 0.030.69 0.030.69 0.03GCN+edrop0.71 0.060.71 0.060.71 0.060.71 0.060.71 0.060.71 0.060.71 0.060.71 0.06GIN0.73 0.030.73 0.030.73 0.030.73 0.030.73 0.030.73 0.030.73 0.030.73 0.03GIN+edrop0.56 0.070.56 0.070.56 0.070.56 0.070.56 0.070.56 0.070.56 0.070.56 0.07GraphSAGE0.46 0.150.46 0.150.46 0.150.46 0.150.46 0.150.46 0.150.46 0.150.46 0.15GraphSAGE+edrop0.47 0.010.47 0.010.47 0.010.47 0.010.47 0.010.47 0.010.47 0.010.47 0.01 : PR-AUC on graph dataset AIFB. Each column is a comparison of the baseline PR-AUC scoresagainst the PR-AUC score for our method (first row) applied to a standard hyperGNN architecture. Redcolor denotes the highest average score in the column. Orange color denotes a two-way tie in the column,and brown color denotes a three-or-more-way tie in the column.",
  "Ours0.79 0.110.73 0.100.73 0.020.85 0.070.75 0.100.84 0.090.72 0.030.72 0.12": "hyperGNN Baseline0.72 0.070.72 0.070.72 0.060.85 0.050.75 0.090.84 0.050.72 0.070.72 0.06hyperGNN Baseln.+edrop0.72 0.050.72 0.080.72 0.060.85 0.070.73 0.090.84 0.060.72 0.030.72 0.07APPNP0.81 0.120.81 0.120.81 0.120.81 0.120.81 0.120.81 0.120.81 0.120.81 0.12APPNP+edrop0.80 0.050.80 0.050.80 0.050.80 0.050.80 0.050.80 0.050.80 0.050.80 0.05GAT0.50 0.020.50 0.020.50 0.020.50 0.020.50 0.020.50 0.020.50 0.020.50 0.02GAT+edrop0.33 0.020.33 0.020.33 0.020.33 0.020.33 0.020.33 0.020.33 0.020.33 0.02GCN20.83 0.050.83 0.050.83 0.050.83 0.050.83 0.050.83 0.050.83 0.050.83 0.05 GCN2+edrop0.78 0.040.78 0.040.78 0.040.78 0.040.78 0.040.78 0.040.78 0.040.78 0.04GCN0.73 0.140.73 0.140.73 0.140.73 0.140.73 0.140.73 0.140.73 0.140.73 0.14GCN+edrop0.75 0.080.75 0.080.75 0.080.75 0.080.75 0.080.75 0.080.75 0.080.75 0.08GIN0.73 0.000.73 0.000.73 0.000.73 0.000.73 0.000.73 0.000.73 0.000.73 0.00GIN+edrop0.73 0.100.73 0.100.73 0.100.73 0.100.73 0.100.73 0.100.73 0.100.73 0.10GraphSAGE0.46 0.150.46 0.150.46 0.150.46 0.150.46 0.150.46 0.150.46 0.150.46 0.15GraphSAGE+edrop0.47 0.010.47 0.010.47 0.010.47 0.010.47 0.010.47 0.010.47 0.010.47 0.01",
  "DDataset and Hyperparameters": "lists the datasets and hyperparameters used in our experiments. All datasets are originally from Ben-son et al. (2018b) or are general hypergraph datasets provided in Sinha et al. (2015); Amburg et al. (2020a).We list the total number of hyperedges |E|, the total number of vertices |V|, the positive to negative labelratios for train/val/test, and the percentage of the connected components searched over by our algorithmthat are size atleast 3. A node isomorphism class is determined by our isomorphism testing algorithm. ByProposition B.2 we can guarantee that if two nodes are in separate isomorphism classes by our isomorphismtester, then they are actually nonisomorphic.",
  "v 2X(l)W (l))(121)": "where Dv Rnn is the diagonal node degree matrix, De Rmm is the diagonal hyperedgedegree matrix, H Rnm is the star incidence matrix, W is the diagonal hyperedge weight matrix,X(l) Rnd is a node signal matrix, W (l) Rdd is a weight matrix, and is a nonlinear activation.Following the matrix products, as a message passing neural network, HGNN is GWL-1 based sincethe nodes pass to the hyperedges and back. HGNNP Feng et al. (2023) is an improved version of HGNN where asymmetry is introduced into themessage passing weightings to distinguish the vertices from the hyperedges. This is also a GWL-1based message passing neural network. It is described by the following node signal update equation:",
  "Lim et al. (2021)johnshopkins55: Non-homophilous graph datasets from the facebook100 dataset": "Ristoski & Paulheim (2016)AIFB: The AIFB dataset describes the AIFB research institute in termsof its staff, research groups, and publications. The dataset was first used to predict the affiliation(i.e., research group) for people in the dataset. The dataset contains 178 members of five researchgroups, however, the smallest group contains only four people, which is removed from the dataset,leaving four classes.",
  "Lim et al. (2021)amherst41:Non-homophilous graph datasets from the facebook100 dataset": "Bordes et al. (2013)FB15k-237:A subset of entities that are also present in the Wikilinks databaseSingh et al. (2012) and that also have at least 100 mentions in Freebase (for both entities andrelationships). Relationships like !/people/person/nationality which just reverses the head and tailcompared to the relationship /people/person/nationality are removed. This resulted in 592,213triplets with 14,951 entities and 1,345 relationships which were randomly split. Guan et al. (2019)WikiPeople-0bi:The Wikidata dump was downloaded and the facts concerningentities of type human were extracted.These facts are denoised.Subsequently, the subsets ofelements which have at least 30 mentions were selected. And the facts related to these elementswere kept. Further, each fact was parsed into a set of its role-value pairs. The remaining facts wererandomly split into training set, validation set and test set by a percentage of 80%:10%:10%. Allbinary relations are removed for simplicity. This modifies WikiPeople to WikiPeople-0bi.",
  "D.1Timings": "We perform experiments on a cluster of machines equipped with AMD MI100s GPUs and 112 shared AMDEPYC 7453 28-Core Processors with 2.6 PB shared RAM. We show here the times for computing eachmethod. The timings may vary heavily for different machines as the memory we used is shared and duringpeak usage there is a lot of paging. We notice that although our data preprocessing algorithm involvesseemingly costly steps such as GWL-1, connected connected components etc. The complexity of the entirepreprocessing algorithm is linear in the size of the input as shown in Proposition B.20. Thus these operationsare actually very efficient in practice as shown by Tables 9 and 10 for the hypergraph and graph datasetsrespectively. The preprocessing algorithm is run on CPU while the training is run on GPU for 2000 epochs."
}