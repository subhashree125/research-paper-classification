{
  "Abstract": "Document segmentation, the process of dividing a document into coherent and signifi-cant regions, plays a crucial role for diverse applications that require parsing, retrieval,and categorization.However, most existing methods rely on supervised learning, whichrequires large-scale labeled datasets that are costly and time-consuming to obtain.Inthis work, we propose a novel self-supervised framework for document segmentation thatdoes not require labeled data.Our framework consists of two components: (1) an un-supervised isothetic covers based pseudo mask generator which approximately segmentsdocument objects, and (2) an encoder-decoder network that learns to refine the pseudomasks and segments the document objects accurately.Our approach can handle di-verse and intricate document layouts by leveraging the rich information from unlabeleddatasets. We demonstrate the effectiveness of our approach on several benchmarks, whereit outperforms state-of-the-art document segmentation methods. Our code is available at",
  "Introduction": "Document digitization transforms paper-based documents into machine-readable formats that can be ac-cessed and processed by various devices and applications. Document layout analysis (DLA) is a crucial stepin digitization, as it aims to extract the information and structural components of a document, such as text,images, tables and graphs (Namboodiri & Jain, 2007; Lee et al., 2019). DLA enables key-value informationextraction and localization, which are essential for tasks such as text recognition, retrieval, segmentation,",
  "Published in Transactions on Machine Learning Research (01/2025)": "(a)(b) : Result of a typical magazine image from PRImA dataset using our learning technique. (a) Thereal annotation provided for the document. (b) The predicted mask generated using our proposed learningtechnique with AutoDocSegmenter-MNV3sm. Our proposed method have been able to outline the complexboundary of the image while it is kept non-annotated in the given dataset.",
  "Heuristic Rule-based Methods": "Rule-based methods are typically classical approaches, which perform document segmentation by using pixel-level information. Based on the sequence in which segmentation is performed, these can be categorized astop-down, bottom-up, or hybrid. In top-down methods (Kise et al., 1998; Journet et al., 2005), the entiredocument is successively split into various components at each step and further into definite regions depictingsimilar entities. Bottom-up techniques (Saabni & El-Sana, 2011; Asi et al., 2015) are usually more effectivethan top-down methods.They begin by considering every pixel as a separate cluster and at successiveiterations, similar pixels are grouped together until homogeneous regions are formed.Hybrid technique(Tran et al., 2015) combine both the above approaches to obtain fast and generalized segmentation methods.",
  "MethodTextImagePseudo Self-supervised Supervised Self-supervisedfeatures featuresmaskspre-trainingfine-tuningtraining": "Layout Parser (Shen et al., 2021)DocSegTr (Biswas et al., 2022)LayoutLMv3 (Huang et al., 2022)UDoc (Gu et al., 2021)DiTBASE (Li et al., 2022)MaskRCNN (He et al., 2017b)FasterRCNN (Ren et al., 2015)SwinDocSegmenter (Banerjee et al., 2023)BYOL (Grill et al., 2020)SelfDocSeg (Maity et al., 2023)AutoDocSegmenter (ours) visual features. Several CNN based segmentation approaches have focused on specific document types suchas historical documents (Chen et al., 2015; 2017; Oliveira et al., 2018), newspaper articles (Almutairi &Almashan, 2019), and scientific publications (Biswas et al., 2021; Yang & Hsu, 2021). Researchers have alsodeveloped methods on recognizing and segmenting specific objects. For instance, DeepDeSRT (Schreiberet al., 2017) and CascadeTabNet (Prasad et al., 2020) aimed at detecting and segmenting tables. Lin etal. (Lin et al., 2021) developed a character (text region) detection technique based on RetinaNet (Lin et al.,2017) and transfer learning. Saha et al. (2019) used transfer learning on Faster-RCNN backbone to segmentgraphical objects in documents. To account for the domain shift in the documents present in training andtest datasets, Li et al. (Li et al., 2020a) proposed cross-domain document object detection. LayoutParser(Shen et al., 2021) is a repository of pre-trained CNN models for layout detection, character recognition,and various other document processing tasks. Recently, Maity et al. (2023) developed SelfDocSeg which performs self-supervised pre-training for document-image segmentation by combining Bootstrap Your Own Latent (BYOL) method (Grill et al., 2020) and focalloss on the pseudo masks. The pseudo masks are generated using morphological operations such as erosion.However, erosion-based masks potentially have drawbacks such as: (i) they may not not cover the documentobjects entirely; (ii) they may distort the structure and shape of the document objects; and importantly (iii)they are sensitive to the resolution of the image and the size and shape of the erosion kernel. To overcomesuch issues, SelfDocSeg finally fine-tune the network with labeled datasets.",
  "Using Transformer Architecture": "Transformer based models gained popularity in computer vision after the success of Vision Transformers(ViT) (Dosovitskiy et al., 2021). Li et al. (Li et al., 2022) proposed self-supervised document image trans-formers DiT and showed its effectiveness in various downstream tasks like document classification, layoutanalysis, table detection, and text detection (OCR). However, the pre-training for DiT is done on large-scale unlabelled document images and is not applicable to small scale magazine datasets (e.g., PRImA)with dataset-specific attributes (Banerjee et al., 2023). StrucTexT (Li et al., 2021b) employed multi-modaltransformers for understanding structured text. Although StrucTexT obtains good performance at bothsegment and token levels, it gets confused when the textual contents are semantically related and closelyplaced. Yang & Hsu (2022) employ OCR based text extraction to perform segmentation on PubLayNetdataset. LayoutLM (Xu et al., 2020) and LayoutLMv3 (Huang et al., 2022) perform joint learning of layout,visual, and text features for visual document understanding tasks. Biswas et al. (Biswas et al., 2022) proposed DocSegTr using ResNet-FPN backbone on transformer. SinceDocSegTr employs self attention mechanism, it is able to achieve faster convergence on small-scale datasetsand obtain competitive performance. SwinDocSegmenter (Banerjee et al., 2023) is state-of-the-art instancesegmentation method on complex layout document images. It employs a supervised SwinTransformer fea-ture extractor backbone. This is integrated with a transformer-based encoder-decoder architecture which",
  "Decoder": "Multi-scaled features Isothetic coversSegmentation block Isothetic masks Segmentation masks Five Rules of generating isothetic covers Fixed FPN Decoder combined with any Encoder backbone detect detect detect Segmentation model Unsupervised mask generation : Schematic diagram of our framework, AutoDocSegmenter. The isothetic covers are generatedusing five types of rules .1 and the isothetic masks extracted are used as reference to train anencoder-decoder model. Different encoder backbones (e.g., MNV3sm, MiT-B0, etc.) may be employed. Wefix the decoder network to feature pyramid network (FPN). fine-tunes on the downsampled features from the backbone using the annotations in a supervised learningparadigm. Some approaches (Li et al., 2021a; Kim et al., 2022) use cross-modality encoder during self-supervised pre-training phase to recognise the types of documents and segment entities. Recent techniques(Appalaraju et al., 2021; Kim et al., 2022; Gu et al., 2021; 2022) have also focused on general purpose featureswhich are generated using unified pre-training on multiple downstream tasks. However, such methods haveinherent biases towards classes with more number of samples and fail to perform during domain shift.",
  "Distinguishing AutoDocSegmenter with Existing Methods": "The existing (partially) self-supervised methods (Huang et al., 2022; Maity et al., 2023; Banerjee et al., 2023)mainly employ self-supervision for pre-training and subsequently perform a supervised fine-tuning step fordataset specific segmentation. The supervised dataset is typically annotated using rectangular boundarieswhich vaguely distinguishes different objects present in the image. However, for a detailed document analysis,it is essential to capture the structure of these entities. On the other hand, our fully self-supervised approach,AutoDocSegmenter, is able to annotate as well as segment the document images, using polygonal boundariesto outline different objects present in the document.Empirically, AutoDocSegmenter generalize well tovarious document layouts without additional fine-tuning. We enlist the distinctions of our method comparedto the existing deep-learning based document segmentation approaches in . We note that all themethods in except our AutoDocSegmenter have a supervised fine-tuning step.",
  "Robust isothetic covers using local and global information": "Isothetic covers are axis-aligned polygons that encloses the objects in binary images. They capture objectstructure and geometry with low cost. Variation in document type and binarization quality may renderthe isothetic covers noisy. In particular, binarization is challenging because a single/global (binarization)threshold may not preserve all document objects, while multiple local thresholds may introduce noise arti-",
  ". A grid point is not a vertex and is skipped if none of the four grids are occupied": "2. A grid point is a vertex of type 1 with a 90 angle and a value of t = 1 if exactly one of the four gridsis occupied. The method turns to the occupied grid and continues along the same row or column( (a)). 3. A grid point is a vertex of type 0 with a 180 angle and a value of t = 0 if two adjacent grids areoccupied. The method follows the edge between the occupied grids and moves to the next row orcolumn ( (b)). 4. A grid point is a vertex of type -1 with a 270 angle and a value of t = 1 if two diagonal grids areoccupied. The method turns to the unoccupied grid and advances along the same row or column( (c)). 5. A grid point is also a vertex of type -1 with a 270 angle and a value of t = 1 if three of the fourgrids are occupied. The method turns to the unoccupied grid and proceeds along the same row orcolumn ( (d)).",
  ". A grid point is not a vertex and is skipped if all four grids are occupied": "The detailed algorithm in discussed in the Appendix A.2. The isothetic covers are generated without anybacktracking and in linear time with respect to the perimeter of the polygon. Pseudo masks are generatedby filling these polygons. We refer to this stage of training pipeline with the conventional isothetic covers(Biswas et al., 2010) as AutoDocSegmenter-I. Binarization and Merging polygons: Binarization facilitates the extraction of isothetic polygonal coversfrom document images, by enabling a clear distinction between foreground and background regions. However,the quality of binarization depends on the choice of the threshold value, which can affect the preservation ofinformation or the exclusion of noise. Different document objects may have different gray levels, making itchallenging to find a single global threshold that can separate them from the background. On the other hand,applying multiple local thresholds on small image patches may introduce noise artifacts such as bleed-through,speckles, or uneven illumination. We introduce a hybrid method for obtaining the isothetic covers from adocument image, which combines the advantages of global and local thresholding techniques. Importantly,our method maintains the shape and orientation of the document elements, and produces isothetic polygonsthat are aligned with the axes and independent of the resolution. This facilitates effective polygon mergingwith minimal loss or distortion of information.",
  "produce multi-scale feature maps from a single input image. FPN is suitable for our task as it can captureboth fine and coarse details of the document layout": "Encoder block: Our self-supervised method does not rely on any specific encoder architecture or pre-training scheme, as it only uses conventional learning of isothetic masks. Therefore, we can integrate ourmethod with different types of encoder backbones, ranging from lightweight to heavyweight models, such asEfficientNets (Tan & Le, 2019), MobileNetV2 (Sandler et al., 2018), MobileNetV3 (Howard et al., 2019),Mix Transformer (Xie et al., 2021), ResNeSt (Zhang et al., 2022), MobileOne (Vasu et al., 2023), ResNeXt(Xie et al., 2017), or RegNet (Xu et al., 2022). These encoder backbones extract feature maps at differentlevels of resolution and semantic richness from the input image. Decoder block: Document segmentation is a challenging task that requires capturing both fine-grainand coarse features of diverse real-world documents. A single architectural choice for the encoder-decodernetwork may compromise the precision of the segmentation masks. To overcome this limitation, we adoptthe Feature Pyramid Network (FPN) decoder, which produces multi-scale feature maps by fusing low-leveland high-level features from the encoder backbone. The FPN decoder leverages the spatial resolution ofthe low-level features and the semantic information of the high-level features to enhance the accuracy androbustness of the document segmentation. In particular, the FPN decoder fuses the multi-scale featuresfrom the encoder backbone through a top-down pathway and lateral connections. The top-down pathwayupsamples the highest level encoder feature map progressively by nearest neighbour interpolation and addsit element-wise to the corresponding encoder feature map, which is first reduced to the same number ofchannels by a 1 1 convolution. This fusion process is repeated for all encoder levels, creating a set ofmerged feature maps. To reduce the anti-aliasing effect of upsampling at each level a 3 3 convolution isapplied. Finally, the refined feature maps are passed through a common classification layer to obtain thefinal segmentation mask. Loss function: Our model aims to segment document images using polygonal masks where the referenceannotation is the isothetic covers generated in an unsupervised manner. We employ the Dice loss betweenthe generated and reference masks. Let y represents the pseudo annotations generated using isothetic coversand p represents the predicted segmentation generated by the model. Then, the Dice loss is computed as",
  "Experiments": "We evaluate the proposed AutoDocSegmenter approach on different benchmarks datasets by (a) comparingit against state-of-the-art document segmentation methods (.2) and (b) performing ablation studieswith varying backbone feature extractors (.3.1), polygon merging thresholds (.3.2), bina-rization techniques (.3.3), grid size of isothetic covers (.3.4), image sizes (.3.5),and document object types (.3.6). We begin by detailing our experimental setup.",
  "Experimental Details": "Datasets: We evaluate our method on four popular document segmentation datasets: PRImA (Antona-copoulos et al., 2009), DocLayNet (Pfitzmann et al., 2022), PubLayNet (Zhong et al., 2019), and M 6-Doc(Cheng et al., 2023). PRImA has 382 and 96 images for training and testing, respectively, with polygonal an-notations for each entity. DocLayNet and PubLayNet have rectangular annotations and contain 69 375/6489and 335 703/11 245 images for training/testing, respectively. M 6-Doc is a recent dataset with only a test setof 2724 images. Please refer to Appendix A.1 for more details. Evaluation Metric: We use the mean average precision (mAP) metric to compare segmentation models,which averages the intersection over union (IoU) scores of predicted and groundtruth masks across documents",
  "and IoU thresholds from 0.5 to 0.95 in steps of 0.05. This follows the Microsoft COCO benchmark protocol(Banerjee et al., 2023)": "AutoDocSegmenter training and evaluation: As discussed in .1, the first stage of our pipelinegenerate pseudo masks for each object without using any supervision. For both isothetic covers as well as theproposed modified isothetic covers (Algorithm 1), we employ Otsus thresholding technique (Otsu, 1979) forglobal and local thresholding, as it can automatically find the optimal threshold that minimizes the intra-class variance of the pixel intensities. The model is trained with the Adam optimizer and a learning rate of0.001 for 50 epochs, and the learning rate is lowered by a factor of 10 every 10 epochs. All document imagesare resized to 256 pixels. We do not utilize any annotations from the datasets during training. We assessthe models segmentation quality by comparing its results with the groundtruth labels from the datasets.",
  "Comparison of pseudo masks": "To evaluate the effectiveness of our dual binarization and polygon merging algorithm, we first compare themasks generated by AutoDocSegmenter-U with the existing isothetic covers algorithm (AutoDocSegmenter-I)(Biswas et al., 2010) and erosion-based SelfDocSeg Maity et al. (2023) masks in . (a) shows that AutoDocSegmenter-I fails to segment the bold text at the bottom of the image, asit relies on gray level intensities that vary across the document.The erosion-based masks produced bySelfDocSeg (Maity et al., 2023) create a binary version of the original image, where text elements are mergedtogether. This does not provide useful segmentation masks for training models, as it loses the distinctionbetween different text regions. While the erosion window size of SelfDocSeg can be adjusted to improve the",
  "MethodPRImA DocLayNet M 6-Doc": "LayoutLMv3 (Huang et al., 2022)4.352.305.30SwinDocSegmenter (Banerjee et al., 2023)43.8010.0033.40AutoDocSegmenter-I-MiT-B0 (ours)89.5783.2175.22AutoDocSegmenter-I-MNV3sm (ours)85.4683.4374.26AutoDocSegmenter-U-MiT-B0 (ours)89.6583.8075.56AutoDocSegmenter-U-MNV3sm (ours)87.7683.6675.24 text segmentation, it does not affect the segmentation of other objects. In contrast, AutoDocSegmenter-Usegments the document by tracing the boundaries of the objects, resulting in well-defined masks for each textregion. The difference in the masks is more evident in figures, tables and graphs, where AutoDocSegmenter-U groups each object as a single unit, while SelfDocSeg and AutoDocSegmenter-I split them into multipleparts, as seen in (b). We also evaluate the pseudo masks from SelfDocSeg (Maity et al., 2023) and AutoDocSegmenter-U methodswith respect to the given annotations in . We observe that AutoDocSegmenter-U outperforms SelfDoc-Seg by extracting higher quality pseudo-masks on all the three datasets. Overall, our AutoDocSegmenter-Upreserves the structure and alignment of the document objects while ensuring that they remain axis-paralleland resolution-independent. This enables precise capturing of the structure of the objects with minimalinformation loss or distortion. On the other hand, morphological operation-based SelfDocSeg Maity et al.(2023) does not capture the shape of the object precisely and distorts with varying intensities.",
  "Comparison against existing approaches": "We next compare AutoDocSegmenter against recent document segmentation approaches in two settings: (1)methods are trained on a large unannotated corpus (PubLayNet) and evaluated on unseen datasets (PRImA,DocLayNet, and M 6-Doc), and (2) training and test sets belong to the same dataset. In this section, wereport results of AutoDocSegmenter with lightweight encoders (MiT-B0 and MobileNetV3-small) which offersignificant advantages in terms of parameter and computational efficiency. These are important prerequisitesfor mobile applications (e.g., Microsofts M365 and Office Lens apps, Adobe Scan app, etc.) that demand fastand reliable document layout analysis. Thus, such variants of AutoDocSegmenter enhance its applicabilityin real-world scenarios. In .3, we also evaluate the performance of AutoDocSegmenter with variousfeature extractors. Setting 1. We evaluate the generalization ability of our method and the baselines in the scenario wherethe training and test sets come from different sources. This is a realistic situation where one has access toa large amount of unlabeled images for training and wants to apply the segmentation model to a specificdataset (e.g., new user data) that may have different characteristics. The baselines we compare with areLayoutLMv3 (Huang et al., 2022) and SwinDocSegmenter (Banerjee et al., 2023), whose pre-trained models(on PubLayNet dataset) are publicly available. shows the performance of different methods whenthey are trained on PubLayNet and tested on PRImA, DocLayNet or M 6-Doc.",
  "MethodM PRImA DocLayNet PubLayNet": "Layout Parser (Shen et al., 2021)-64.70-86.70DocSegTr (Biswas et al., 2022)-42.50-90.40LayoutLMv3 (Huang et al., 2022)13340.30-95.10UDoc (Gu et al., 2021)272--93.90DiTBASE (Li et al., 2022)87--93.50MaskRCNN (He et al., 2017b)63.7-73.5091.00FasterRCNN (Ren et al., 2015)19-73.4090.20SwinDocSegmenter (Banerjee et al., 2023)22354.3976.8593.72BYOL (Grill et al., 2020)9428.7063.5079.00SelfDocSeg (Maity et al., 2023)-52.1074.3089.20AutoDocSegmenter-I-MiT-B0 (ours)579.1287.2084.50AutoDocSegmenter-I-MNV3sm (ours)4.180.7287.1485.10AutoDocSegmenter-U-MiT-B0 (ours)580.5182.5085.20AutoDocSegmenter-U-MNV3sm (ours)4.181.4785.9785.20 approach generalizes well to different types of documents and captures the layout information effectively.Both LayoutLMv3 and SwinDocSegmenter are large-scale transformer-based models. We note that both theabove baselines struggle to adapt to the diverse domains and styles of the documents, and may suffer fromoverfitting to the PubLayNet dataset. We also note from that AutoDocSegmenter-MiT-B0 has a slight edge over AutoDocSegmenter-MNV3sm. MiT-B0 is based on the transformer architecture, which can capture the long-range dependenciesand the spatial relations in the documents, while MNV3sm is based on the mobile network architecture,which can reduce the computational cost and the runtime memory usage. Setting 2. We next evaluate the models when the training and test sets belong to the same dataset in. In addition to LayoutLMv3 and SwinDocSegmenter, we also report the results obtained by LayoutParser (Shen et al., 2021), SelfDocSeg (Maity et al., 2023), DocSegTr (Biswas et al., 2022), MaskRCNN (Heet al., 2017b), and FasterRCNN (Ren et al., 2015). For LayoutLMv3 and SwinDocSegmenter, we obtain theresults using the pre-trained weights provided by the authors while for other baselines, we report the resultsas published in (Shen et al., 2021; Biswas et al., 2022; Zhong et al., 2019). Overall, AutoDocSegmenterdemonstrates superior performance across datasets. In particular, we make the following observations: PRImA dataset contains complex and diverse document layouts with non-rectangular annotationboundaries. The results in show that AutoDocSegmenter is able to handle the variationsand challenges of different document shapes and structures. DocLayNet dataset consists of document images with rectangular annotation boundaries and variousdocument structures. AutoDocSegmenter outperforms the baselines on this dataset, indicating itsrobustness and adaptability to different layout styles. PubLayNet is a large dataset, consisting of research document images with non-overlapping rect-angular annotation boundaries and relatively simple and regular layouts. LayoutLMv3 is the bestperforming method on this dataset. In the model training phase with unsupervised annotations, we observe two kinds of outcomes: (a) the modelmimics some of the annotation errors, such as careful outlining of in-line images, in the segmentation masks,and (b) the model deviates from some of the annotation errors, and thus discards some of the wrongly labeledareas. These outcomes indicate both the advantages and disadvantages of using an unsupervised annota-tion process. However, as shown in , AutoDocSegmenter can generalize effectively to various unseen",
  "Performance on various backbone feature extractors": "We begin by evaluating the performance of AutoDocSegmenter-U using different CNN and transformer basedbackbone feature extractors (combined with the FPN decoder). We train the models using image size of256. compares the performance of AutoDocSegmenter-U on various backbone feature extractors ontwo datasets: PRImA and DocLayNet. In addition to mAP metric, we also report intersection over union(IoU) scores. We observe from that:",
  "The transformer-based MiT-B4 performs worse than the much lighter MiT-B0 backbone or theCNN-based backbones. This may indicate that it requires more data and fine-tuning to achievecomparable results": "Among the CNN-based backbones, AutoDocSegmenter-U-EfficientNet variants are among the bestmethods on both datasets. However, these backbones also have the largest number of parameters,ranging from 5.3M to 30M, which may limit their applicability on resource-constrained scenarios. AutoDocSegmenter-U-MobileNet variants, with MobileNetV2 and MobileNetV3 sm backbones, showcompetitive performance on both datasets. The number of parameters in these backbones range from2.5M to 3.4M, making them efficient and lightweight for mobile scenarios.",
  "IoUmAPIoUmAP": "EfficientNet-B0 (Tan & Le, 2019)5.30.6584.110.8397.56EfficientNet-B3 (Tan & Le, 2019)120.6586.200.8498.80EfficientNet-B5 (Tan & Le, 2019)300.6982.420.8799.20MobileNetV2 (Sandler et al., 2018)3.40.6580.010.8696.37MobileNetV3 sm (Howard et al., 2019)2.50.6781.470.8496.30MobileOne-S0 (Vasu et al., 2023)2.10.4242.700.7070.20RegNetX-200 (Xu et al., 2022)2.70.4242.700.7070.20ResNeSt-14 (Zhang et al., 2022)80.6784.080.8496.87ResNeSt-26 (Zhang et al., 2022)150.6375.730.8397.10ResNeSt-50 (Zhang et al., 2022)250.6479.260.8898.20ResNeSt-101 (Zhang et al., 2022)460.6282.830.8998.62ResNeXt-50 (Xie et al., 2017)220.6575.700.8998.30MiT-B0 (Xie et al., 2021)30.6780.510.8193.64MiT-B4 (Xie et al., 2021)600.6680.330.7070.20",
  "Performance with different polygon merging thresholds": "AutoDocSegmenter-U uses a polygon merging technique to mitigate the influence of binarization thresh-olds on the overall quality of pseudo masks. The polygons generated using local and global thresholdingare merged if there is an overlap of more than a given threshold. analyses the performance ofAutoDocSegmenter-U using different overlapping threshold values on PRImA and DocLayNet datasets. Weobserve that the default threshold value of 50% is a robust choice across datasets. Setting it too low or toohigh may result in over- or under-segmentation, as observed in the PRImA dataset.",
  "Performance with different binarization techniques": "Document images often have regions with different gray levels, which pose a challenge for foreground-background separation using a single threshold value. For instance, documents with text and images, varyingbackground colors, or noise effects such as shadows and low lighting, require different thresholds for differentparts of the image to preserve the object shapes and details. A global threshold may fail to capture thecontrast or brightness variations across the image Yan et al. (2005), while a local threshold based on a fixedwindow size may miss some objects or introduce artifacts due to the local background variation Wanas et al.",
  "(a)(b)(c)(d)": ": Pseudo masks generated for (a) the input image after binarisation using (b) Otsus global thresh-olding, (c) Otsus local thresholding and (d) Otsus local+global thresholding. In (b), we observe that theglobal thresholding approach fail to differentiate between the foreground and the background in the rightpart of the document because of different grey-level of the background. In (c), we observe that the localthresholding approach unnecessarily captures very fined grained details, which would easily be influenced bynoise. It also fails to capture objects near the bottom right of the document. In (d), the joint global andlocal approach combines the advantages of both the approaches and gives the best result. (2006). To overcome these limitations, we explore an approach (Otsus global + local thresholding) thatadapts the threshold value to the global and local characteristics of the image. shows the robustperformance of this method over global or local thresholding alone on complex document images. In , we compare the binarization performance of Otsus global + local thresholding technique with thefollowing baselines: (a) Global thresholding technique, where we set the threshold value to 147; (b) Otsusglobal thresholding method (Otsu, 1979); (c) Adaptive mean thresholding, which is a local thresholdingalgorithm with window size fixed at 150; and (d) Otsus local thresholding technique, where the thresholdfor every window is computed using Otsus algorithm. In , we observe that Otsus global + localthresholding performs competitively against the baselines while retaining the benefits of both local andglobal thresholding approaches.We note that our datasets contain limited number of complex images,which reduces the advantage of combining global and local thresholding techniques that can handle diversedocument types. Therefore, we observe similar results from global thresholding and the combined method.",
  "Varying grid size of isothetic covers": "The quality of the isothetic masks is influenced by the grid size used for generating the isothetic covers. Asuitable grid size should correspond to the scale and the variety of the document layout, preventing over-segmentation or under-segmentation of the layout elements. To measure the influence of the grid size on thesegmentation performance, we calculate the intersection over union (IoU) and the mean average precision(mAP) of the isothetic masks with respect to the groundtruth masks.",
  "MNV3sm10240.6780.3179.2080.905120.6677.5376.2078.102560.6575.8074.6076.50": "shows the results for the PRImA dataset using the MiT-B0 and MNV3sm backbones. We observethat a grid size of 18 achieves the best performance for both encoders, attaining the highest IoU and mAPscores. Smaller grid sizes result in lower IoU and mAP scores due to excessive splitting of the isotheticcovers. Likewise, we find an appropriate grid size for DocLayNet and PubLayNet as 8 and 10, respectively,by following the same procedure. The proposed method relies on selecting an appropriate grid size for each dataset, which may vary dependingon the characteristics of the document images, such as resolution, density, and diversity of layout elements.Therefore, we conduct hyperparameter tuning to determine the grid size that optimizes performance onthe validation set. Empirically, we have observed that the optimal grid size is approximately1",
  "Varying image size": "We next evaluate the effect of different image sizes on the segmentation performance of AutoDocSegmenterusing the PRImA dataset. We use MiT-B0 and MNV3sm as encoder networks to extract features fromthe input images, and compare the results using four metrics: IoU, mAP, AP@50, and AP@75. summarizes our findings. reveals that the segmentation performance is influenced by the imagesize, and that the optimal size varies depending on the encoder network. For both encoders, the highestresolution of 1024 1024 pixels achieves the best results for most metrics, indicating that more details andfeatures of the document layout are captured and exploited by the encoders. However, this also increases thememory and computational demands of the model. In this work, we consider image resolution of 256 256for our experiments (except in ) as this setting provides a reasonable trade-off between generalizationperformance and efficiency for resource constrained scenarios (e.g., mobile applications).",
  "AutoDocSegmenter-U82.2072.90AutoDocSegmenter-U-MiT-B089.6575.56AutoDocSegmenter-U-MNV3sm87.7675.24": "our model predicts only the segmentation mask for each document image without labeling the underlyingobjects. The PRImA dataset, on the other hand, has labels corresponding to each object in the ground truthannotation. Hence, we perform connected component analysis on the masks generated by our approach andlabel each component based on the maximum overlap of the component with the ground truth. We report theaverage mIoU scores obtained for each object in . We observe that the mIoU scores for paragraphsare high followed by tables and figures. This is because the mask of figures in PRImA dataset have diverseand complex shapes compared to paragraphs and tables. Moreover, for some complex document images, theground truth does not cover all the objects while our model is able to detect them (e.g., shown in A.3).",
  "Comparing variants of AutoDocSegmenter": "To study the effectiveness of Algorithm 1 and the self-supervised training stage, we evaluate the performanceof the unsupervised algorithms AutoDocSegmenter-I and AutoDocSegmenter-U and their self-supervisedvariants with MiT-B0 and MNV3sm backbones on PRImA and M 6-Doc. These two datasets closely re-semble real world documents consisting of complex layouts with varying background textures. The self-supervised learning methods are trained PubLayNet (i.e., we follow the generalized setting 1 discussed in.2.2). reports the performance of all the six methods. We observe that AutoDocSegmenter-U achieves comparable or better performance than AutoDocSegmenter-I. Similarly, the self-supervisedAutoDocSegmenter-U-MiT-B0 and AutoDocSegmenter-U-MNV3sm retain this advantage over their self-supervised AutoDocSegmenter-I counterparts.",
  "Conclusions": "The scarcity of large-scale datasets with precise polygonal annotations of document objects remains a sig-nificant obstacle in developing document segmentation models. Our work introduces AutoDocSegmenter,a self-supervised approach utilizing isothetic covers as pseudo masks to train an encoder-decoder model.AutoDocSegmenter effectively handles diverse and complex document layouts, producing accurate segmen-tation masks and generalizing well to unseen datasets. Additionally, it supports lightweight encoder ar-chitectures, offering notable parameter and computational efficiency.Overall, we develop an end-to-endself-supervised document segmentation method for real-world applications. Limitations and Future Works: Although AutoDocSegmenter can handle complex document layouts,it only segregates the foreground information from the background and segments various objects presentin the document as binary masks. As it is a self-supervised approach without any supervised fine-tuning,our method cannot identify or classify the objects into different categories.We plan to investigate thisunsupervised classification problem as a future work.",
  "Broader Impact Statement": "We present a self-supervised technique of document segmentation which can handle complex layouts effi-ciently and can be integrated with both lightweight and heavyweight encoder architectures based on theavailable resources. Overall, our work reduces the dependency on human annotations for segmentation task,which is a tedious task. To the best of our knowledge, this work does not pose any negative societal impact.",
  "Abdullah Almutairi and Meshal Almashan. Instance segmentation of newspaper elements using mask r-cnn.In IEEE International Conference On Machine Learning And Applications, pp. 13711375, 2019": "Apostolos Antonacopoulos, David Bridson, Christos Papadopoulos, and Stefan Pletschacher. A realisticdataset for performance evaluation of document layout analysis. In International Conference on DocumentAnalysis and Recognition, pp. 296300, 2009. Srikar Appalaraju, Bhavan Jasani, Bhargava Urala Kota, Yusheng Xie, and R Manmatha. Docformer: End-to-end transformer for document understanding. In IEEE/CVF International Conference on ComputerVision, pp. 9931003, 2021.",
  "Abedelkadir Asi, Rafi Cohen, Klara Kedem, and Jihad El-Sana.Simplifying the reading of historicalmanuscripts. In International Conference on Document Analysis and Recognition, pp. 826830, 2015": "Ayan Banerjee, Sanket Biswas, Josep Llads, and Umapada Pal. Swindocsegmenter: An end-to-end unifieddomain adaptive transformer for document instance segmentation. In Document Analysis and Recognition,pp. 307325, 2023. Arindam Biswas, Partha Bhowmick, and Bhargab B Bhattacharya. Construction of isothetic covers of adigital object: A combinatorial approach. Journal of Visual Communication and Image Representation,21(4):295310, 2010. Sanket Biswas, Pau Riba, Josep Llads, and Umapada Pal. Beyond document object detection: instance-level segmentation of complex layouts. International Journal on Document Analysis and Recognition, 24(3):269281, 2021.",
  "Sanket Biswas, Ayan Banerjee, Josep Llads, and Umapada Pal. Docsegtr: An instance-level end-to-enddocument image segmentation transformer. arXiv preprint arXiv:2201.11438, 2022": "Kai Chen, Mathias Seuret, Marcus Liwicki, Jean Hennebert, and Rolf Ingold. Page segmentation of historicaldocument images with convolutional autoencoders. In International Conference on Document Analysisand Recognition, pp. 10111015, 2015. Kai Chen, Mathias Seuret, Jean Hennebert, and Rolf Ingold. Convolutional neural networks for page segmen-tation of historical document images. In International Conference on Document Analysis and Recognition,volume 1, pp. 965970, 2017. Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastivelearning of visual representations.In International Conference on Machine Learning, pp. 15971607.PMLR, 2020. Hiuyi Cheng, Peirong Zhang, Sihang Wu, Jiaxin Zhang, Qiyuan Zhu, Zecheng Xie, Jing Li, Kai Ding,and Lianwen Jin. M6doc: A large-scale multi-format, multi-type, multi-layout, multi-language, multi-annotation category dataset for modern document layout analysis. In IEEE/CVF Conference on ComputerVision and Pattern Recognition, pp. 1513815147, June 2023. Christian Clausner, Apostolos Antonacopoulos, Tom Derrick, and Stefan Pletschacher. Icdar2019 competi-tion on recognition of early indian printed documentsreid2019. In International Conference on DocumentAnalysis and Recognition, pp. 15271532, 2019.",
  "Kaiming He, Georgia Gkioxari, Piotr Dollr, and Ross Girshick. Mask r-cnn. In IEEE international confer-ence on computer vision, pp. 29612969, 2017b": "Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen, Mingxing Tan, Weijun Wang,Yukun Zhu, Ruoming Pang, Vijay Vasudevan, et al. Searching for mobilenetv3. In IEEE/CVF Interna-tional Conference on Computer Vision, pp. 13141324, 2019. Yupan Huang, Tengchao Lv, Lei Cui, Yutong Lu, and Furu Wei. Layoutlmv3: Pre-training for document aiwith unified text and image masking. In ACM International Conference on Multimedia, pp. 40834091,2022. Nicholas Journet, Vronique Eglin, Jean-Yves Ramel, and Rmy Mullot. Text/graphic labelling of ancientprinted documents. In International Conference on Document Analysis and Recognition, pp. 10101014,2005. Geewook Kim, Teakgyu Hong, Moonbin Yim, JeongYeon Nam, Jinyoung Park, Jinyeong Yim, WonseokHwang, Sangdoo Yun, Dongyoon Han, and Seunghyun Park. Ocr-free document understanding trans-former. In European Conference on Computer Vision, pp. 498517, 2022.",
  "Koichi Kise, Akinori Sato, and Motoi Iwata. Segmentation of page images using the area voronoi diagram.Computer Vision and Image Understanding, 70(3):370382, 1998": "Joonho Lee, Hideaki Hayashi, Wataru Ohyama, and Seiichi Uchida. Page segmentation using a convolutionalneural network with trainable co-occurrence features. In International Conference on Document Analysisand Recognition, pp. 10231028, 2019. Junlong Li, Yiheng Xu, Tengchao Lv, Lei Cui, Cha Zhang, and Furu Wei. Dit: Self-supervised pre-trainingfor document image transformer. In ACM International Conference on Multimedia, pp. 35303539, 2022. Kai Li, Curtis Wigington, Chris Tensmeyer, Handong Zhao, Nikolaos Barmpalios, Vlad I Morariu, VarunManjunatha, Tong Sun, and Yun Fu. Cross-domain document object detection: Benchmark suite andmethod. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1291512924, 2020a.",
  "Nobuyuki Otsu. A threshold selection method from gray-level histograms. IEEE Transactions on Systems,Man, and Cybernetics, 9(1):6266, 1979": "Birgit Pfitzmann, Christoph Auer, Michele Dolfi, Ahmed S Nassar, and Peter Staar. Doclaynet: A largehuman-annotated dataset for document-layout segmentation. In ACM SIGKDD Conference on KnowledgeDiscovery and Data Mining, pp. 37433751, 2022. Devashish Prasad, Ayan Gadpal, Kshitij Kapadni, Manish Visave, and Kavita Sultanpure. Cascadetabnet:An approach for end to end table detection and structure recognition from image-based documents. InIEEE/CVF Conference on Computer Vision and Pattern Recognition workshops, pp. 572573, 2020.",
  "Ranajit Saha, Ajoy Mondal, and CV Jawahar. Graphical object detection in document images. In Interna-tional Conference on Document Analysis and Recognition, pp. 5158, 2019": "Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Mobilenetv2:Inverted residuals and linear bottlenecks. In IEEE Conference on Computer Vision and Pattern Recogni-tion, pp. 45104520, 2018. Sebastian Schreiber, Stefan Agne, Ivo Wolf, Andreas Dengel, and Sheraz Ahmed. Deepdesrt: Deep learningfor detection and structure recognition of tables in document images. In International Conference onDocument Analysis and Recognition, volume 1, pp. 11621167, 2017. Zejiang Shen, Ruochen Zhang, Melissa Dell, Benjamin Charles Germain Lee, Jacob Carlson, and WeiningLi. Layoutparser: A unified toolkit for deep learning based document image analysis. In InternationalConference on Document Analysis and Recognition, pp. 131146, 2021.",
  "Mingxing Tan and Quoc Le. Efficientnet: Rethinking model scaling for convolutional neural networks. InInternational Conference on Machine Learning, pp. 61056114. PMLR, 2019": "Tuan Anh Tran, In-Seop Na, and Soo-Hyung Kim. Hybrid page segmentation using multilevel homogeneitystructure. In International Conference on Ubiquitous Information Management and Communication, pp.16, 2015. Pavan Kumar Anasosalu Vasu, James Gabriel, Jeff Zhu, Oncel Tuzel, and Anurag Ranjan. Mobileone: Animproved one millisecond mobile backbone. In IEEE/CVF Conference on Computer Vision and PatternRecognition, pp. 79077917, 2023.",
  "Nayer M Wanas, Dina A Said, Nadia H Hegazy, and Nevin M Darwish.A study of local and globalthresholding techniques in text categorization. In Australasian Data Mining Conference, 2006": "Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M Alvarez, and Ping Luo. Segformer:Simple and efficient design for semantic segmentation with transformers. Advances in Neural InformationProcessing Systems, 34:1207712090, 2021. Saining Xie, Ross Girshick, Piotr Dollr, Zhuowen Tu, and Kaiming He. Aggregated residual transformationsfor deep neural networks. In IEEE Conference on Computer Vision and Pattern Recognition, pp. 14921500, 2017.",
  "Huichen Yang and William Hsu. Transformer-based approach for document layout understanding. In 2022IEEE International Conference on Image Processing, pp. 40434047, 2022": "Huichen Yang and William H Hsu. Vision-based layout detection from scientific literature using recurrentconvolutional neural networks. In International Conference on Pattern Recognition, pp. 64556462, 2021. Hang Zhang, Chongruo Wu, Zhongyue Zhang, Yi Zhu, Haibin Lin, Zhi Zhang, Yue Sun, Tong He, JonasMueller, R Manmatha, et al. Resnest: Split-attention networks. In IEEE/CVF Conference on ComputerVision and Pattern Recognition, pp. 27362746, 2022.",
  "A.1Data Description": "We evaluate our method on four popular document segmentation datasets: PRImA (Antonacopoulos et al.,2009), DocLayNet (Pfitzmann et al., 2022), PubLayNet (Zhong et al., 2019), and M 6-Doc (Cheng et al.,2023). PRImA dataset (Antonacopoulos et al., 2009) has 382 training and 96 testing samples, respectively withthe image dimensions varying between 2500 to 3500. The annotations of this dataset provide polygonalboundaries for each entity present in the document.",
  "else if (g1&g2&g3&g4) || !(g1&g2&g3&g4) thencontinueend if": "The type of grid and angle associated with it depends on the occupancy of the four adjacent grids. Wheneither one or three of the grids are occupied with text, image, etc., we denote the type, t = 1 and ta = 90.Therefore, the direction of movement takes a 90 turn and traverses along the boundary of the occupied grid.Similarly, the direction of movement remains the same when two adjacent grids are occupied. There is noeffect on the traversal when all or none of the grids are empty. We show more examples of AutoDocSegmenter-U from DocLayNet dataset in .",
  "A.3Comparing AutoDocSegmenter-U with ground truth masks": "We show the difference between groundtruth masks and the masks generated using AutoDocSegmenter-U in for PubLayNet dataset.PubLayNet consists of clean images with simple layout. Hence, it is easierto compare the masks generated using human annotation with the isothetic covers. We observe that for documents with text-only format or sequential paragraphs, the AutoDocSegmenter-Umasks are identical to that of the given annotations.However, for tables, instead of a rectangular boxdenoting the entire table, AutoDocSegmenter-U precisely outline each row and column using polygonalboundaries. Similar results are observed for images as well.",
  ": Comparing the ground truth annotations with our AutoDocSegmenter-U masks for PubLayNetdataset": "As our proposed learning method is trained using AutoDocSegmenter-U masks, the model learns to mimicthe characteristics of polygonal covers and generates similar predicted masks for various objects present inthe document. Most complex layout images are annotated using rectangular boxes or left non-annotated insome cases. As AutoDocSegmenter-U is independent of the given groundtruth, it is able to detect complexshaped objects present in the documents. shows an example of our method applied to a magazineimage, where we can see that our network can segment the figure with reasonable precision, even though thetraining dataset does not contain any tight annotations for the document objects."
}