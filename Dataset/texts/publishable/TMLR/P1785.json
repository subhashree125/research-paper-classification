{
  "Abstract": "Compositional learning, mastering the ability to combine basic concepts and construct moreintricate ones, is crucial for human cognition, especially in human language comprehensionand visual perception. This notion is tightly connected to generalization over unobservedsituations. Despite its integral role in intelligence, there is a lack of systematic theoreticaland experimental research methodologies, making it dicult to analyze the compositionallearning abilities of computational models. In this paper, we survey the literature on com-positional learning of AI models and the connections made to cognitive studies. We identifyabstract concepts of compositionality in cognitive and linguistic studies and connect theseto the computational challenges faced by language and vision models in compositional rea-soning. We overview the formal denitions, tasks, evaluation benchmarks, various computa-tional models, and theoretical ndings. Our primary focus is on linguistic benchmarks andcombining language and vision, though there is a large amount of research on compositionalconcept learning in the computer vision community alone. We cover modern studies onlarge language models to provide a deeper understanding of the cutting-edge compositionalcapabilities exhibited by state-of-the-art AI models and pinpoint important directions forfuture research.",
  "Introduction": "The compositional learning and reasoning of an intelligent agent refers to the ability to understand andmanipulate complex structures by decomposing them into simpler parts and composing parts to form newcomplex concepts with a coherent understanding. This ability is a key factor in generalizing learning tounobserved situations (Hupkes et al., 2023). Compositional learning in intelligent systems is cognitively mo-tivated since humans learn compositionally (Lake et al., 2019). Researchers have examined this phenomenonfrom cognitive, linguistic, and psychological perspectives (Shepard, 1987; Frankland & Greene, 2020). The formal notion of compositionality originated from natural language and semantics, with various theoriesand arguments that elaborate on this concept. The principle of compositionality (Partee, 2004; Janssen& Partee, 1997; Montague, 1974) is dened as The meaning of a whole is a function of the meanings ofthe parts and of the way they are syntactically combined with three general methods- new meanings, newbasic parts, and new constructions. One of the earliest formalizations of compositionality was groundedin grammar trees when cognitive scientists proposed an information processing approach to create a model",
  "Published in Transactions on Machine Learning Research (11/2024)": "Shuofei Qiao, Yixin Ou, Ningyu Zhang, Xiang Chen, Yunzhi Yao, Shumin Deng, Chuanqi Tan, Fei Huang,and Huajun Chen.Reasoning with language model prompting:A survey.In Anna Rogers, Jor-dan Boyd-Graber, and Naoaki Okazaki (eds.), Proceedings of the 61st Annual Meeting of the Associ-ation for Computational Linguistics (Volume 1: Long Papers), pp. 53685393, Toronto, Canada, July2023. Association for Computational Linguistics.doi:10.18653/v1/2023.acl-long.294.URL Linlu Qiu, Hexiang Hu, Bowen Zhang, Peter Shaw, and Fei Sha. Systematic generalization on gSCAN:What is nearly solved and what is next?In Proceedings of the 2021 Conference on Empirical Methodsin Natural Language Processing, pp. 21802188, Online and Punta Cana, Dominican Republic, November2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.166. URL Hossein Rajaby Faghihi, Quan Guo, Andrzej Uszok, Aliakbar Nafar, and Parisa Kordjamshidi. DomiKnowS:A library for integration of symbolic domain knowledge in deep learning. In Heike Adel and Shuming Shi(eds.), Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: SystemDemonstrations, pp. 231241, Online and Punta Cana, Dominican Republic, November 2021. Associationfor Computational Linguistics. doi: 10.18653/v1/2021.emnlp-demo.27. URL Parikshit Ram, Tim Klinger, and Alexander G. Gray. How compositional is a model? In International JointConference on Articial Intelligence 2023 Workshop on Knowledge-Based Compositional Generalization,2023. URL Parikshit Ram, Tim Klinger, and Alexander G. Gray. What makes models compositional? a theoreticalview. In Kate Larson (ed.), Proceedings of the Thirty-Third International Joint Conference on ArticialIntelligence, IJCAI-24, pp. 48244832. International Joint Conferences on Articial Intelligence Organi-zation, 8 2024. doi: 10.24963/ijcai.2024/533. URL MainTrack.",
  "(Sec.2.4)": ": Outline of covered concepts in this survey, related to the structure of the paper. We structureour study of compositional learning by dividing it into four parts of compositional learning facets, datasets,compositional learning models, and evaluation methods from both empirical and theoretical perspectives.The topics have the respective sections associated with them. The main areas of required research and futuredirection are included in the descriptions in the Evaluation boxes, which are further discussed in . in compositionality. A commonly used task categorization, derived from reformulated theoretically groundedtests from Hupkes et al. (2020), denes ve main metrics of compositionality: systematicity, productivity,substitutivity, localism, and overgeneralization (Dankers et al., 2021).",
  "Measures of Compositionality": "Hupkes et al. (2020) introduces theoretically grounded tests for compositionality of models based on dierentinterpretations of compositionality (Fodor & Pylyshyn, 1988; Chomsky, 1956). Some of these tasks existedin earlier research under dierent terms, such as productivity, which builds on a substantial body of priorresearch on length generalization. These tasks are becoming widely accepted tasks for compositional learning,and there are current datasets that use them for their evaluation splits. We describe these measures in detailbelow.",
  "Systematicity or Novel Composition": "CREPE. This is a Compositional REPresentation Evaluation benchmark (CREPE) (Ma et al., 2022). Thedataset is synthesized and includes multiple splits, one of which relates to Systematicity. The main tasksetting is that, given an image, the model needs to identify an appropriate text caption describing it amongmultiple given choices. This systematicity challenge tests whether the model can systematically generate newcombinations of seen atomic concepts during training. For example, Crepe on a skillet is never observedin the training while Crepe and skillet are observed separately in dierent contexts. SCAN. The task is to navigate in a two-dimensional grid world based on natural language instructions. Itis the Simplied version of the CommAI Navigation tasks (SCAN) (Lake & Baroni, 2017; Mikolov et al.,2018). One of the proposed experiments in SCAN evaluates the models compositional generalization acrossprimitive commands. Specic compounds are excluded from training where the model has seen the primitivesand similar compound structures. Then, these unseen compounds are tested during testing.",
  "Substitutivity or Synonymity": "Substitutivity is another form of evaluation dened in Dankers et al. (2021), which concerns the modelperformance on the introduction of synonyms in expressions. For example, testing a model on the translationof the same sentence, when switching between synonymous words such as donut to doughnut or aubergineto eggplant the translation would not change. This is one of the less explored axioms of compositionality.",
  "Localism": "Another nuance of compositionality is the notion of global versus local composition.According to theprinciple of compositionality, the locality of the composition operator can vary. The meaning of a complexexpression can depend solely on the meaning of its immediate parts (local composition) or the global structureof the context. Localism can be tested by analyzing the meaning a model assigns to a standalone compoundversus when that compound is part of a larger expression. For example, sentences X and Y have the sametruth values, but if the same context is added, their composition with the new context might lead to dierenttruth values. For instance, when we add the context, Peter thinks and obtain Peter thinks X and Peterthinks Y, these two new sentences can have dierent truth values (Hupkes et al., 2020; Carnap, 1947). Thelocal interpretation of compositionality says that these new phrases will have the same truth values whichmight not be the case anymore as Peter might be aware of X and not Y. In other words, considering thephrase that X and Y are a part of, changes the meaning. This is another one of the less explored axioms ofcompositionality.",
  "Overgeneralization": "Overgeneralization, as dened in Dankers et al. (2021), evaluates how much a model prefers an exceptionversus a rule. The term is originally used in language acquisition literature, also known as overregularization.A well-known example of this is the past-tense debate (Marcus et al., 1992) in language, which is about therule that English past-tense verbs can be formed by appending -ed to the stem of the verb in most caseswhile there are some exceptions. This property can be evaluated by testing a model on exceptions of ausual rule in the training data and seeing if the model has over-tted the training samples (Hupkes et al.,2020). Another example of this task is translating idioms where the meaning of sentences is exceptionsto usual rules. For example, when translating the idiom its raining cats and dogs, a literal translationdoes not make sense as the phrase is an exception and has a specic meaning dierent from the usual literalinterpretation. In this scenario, a model can achieve better performance by considering sentences in a globalsense -that is looking at the bigger picture, such as context from placement in a compound- instead of tryingto evaluate the meaning locally, such as by isolating the phrase. This is yet another less evaluated axiom ofcompositionality.",
  "Compositionality as Function Properties": "In Ram et al. (2024), compositional functions are dened with several components with a computationdirected acyclic graph (cDAG) at the core. Their formal denitions facilitate the evaluation of compositionalproperties of the learning models (i.e. compositional functions). They relate their dened structure to thelearning models expressivity and sample complexity. In this system, Systematicity can be thought of asthe expressivity of a compositional function as a low entropy program (for example, decision tree) versusa high entropy program (for example, transformer). Productivity, in simple terms, can be interpretedas whether a compositional function is recursive. Substitutivity tests whether a compositional functionrespects important abstractions and can be factored over them.Localism measures the stability of acompositional function against local changes, where the structure of the functions elements aects theimportance of the level of locality respected. Overgeneralization is the extent of compression of a function,where a function might have a general rule but have exceptions to those in special cases.",
  "Compositionality and Continual Learning": "Compositionality is an important aspect of continual learning, also known as lifelong learning (Mendez &Eaton, 2021). Continual learning (Wang et al., 2024) is the concept of learning new tasks while retainingknowledge from previously learned tasks. In continual learning, compositionality is particularly crucial toprevent catastrophic forgetting, where earlier tasks are forgotten over time due to learning of new tasks (Liaoet al., 2023). Since knowledge about novel tasks is compositional, the existing information can be combinedin novel ways and used for future tasks. This enables forward transfer of knowledge rather than catastrophicforgetting (Mendez & Eaton, 2023). Both compositional and continual learning share the key challengeof nding reusable knowledge and further connecting those for transfer learning and dealing with complexunobserved situations.",
  "Compositionality and Emergent Intelligence": "The term emergent behavior has been used across various science-related elds, rooted in More Is Dierentby Nobel Prize-winning physicist P.W. Anderson (Anderson, 1972). Its introduction to the language model-ing community, specically in the context of the large language models, begins with Wei et al. (2022). Theydened emergent ability as the ability that appears only in large models and is not observed in any smallermodels. The emergent abilities demonstrated in their study include performing unseen tasks by followinginstructions (Ouyang et al., 2024) and demonstrating multi-hop reasoning skills through Chain-of-Thoughtprompting (Wei et al., 2024). These abilities reect the models capacity for generalizing to unobservedsituations, which can further extend to the models compositional learning ability. Therefore, the composi-tional learning ability of the models can be associated with the emergent intelligence of language models.In the same paper of Wei et al. (2022), creating new compositional learning benchmarks is proposed as onedirection for evaluating and understanding LLMs emergent abilities.",
  "Abstract Tasks and Datasets": "In this section, we categorize the existing datasets proposed for the evaluation of compositional learning.Our categorization is based on the type of compositionality facet explained in . points to alist of important datasets we have surveyed. In general, there are more common evaluation benchmarks forSystematicity and Productivity. Systematicity focuses on the novel composition of seen atomic concepts andthere are several benchmarks established for its evaluation, although some of those works do not explicitlyuse the term systematicity. The productivity measure was often referred to as length generalization beforethe term became commonly used. However, despite the abundance of datasets for these tasks, most rely onsynthetic data, which poses a risk to their reliability in capturing the variation and complexity of real-worldproblems. While some datasets in the computer vision community for compositional learning utilize realisticimages, they address fewer aspects of compositionality (e.g.object-attribute combination) compared tosynthesized linguistic corpora designed for the same purpose. We describe some of the existing datasets andtasks in this section.",
  "Productivity or Length Generalization": "CREPE. The productivity split of CREPE (Ma et al., 2022) evaluates if a model can perform the trainedtask of longer sets of expressions.In this task, there are variations of complexity for n-subgraphs withn {4, 5, ....12} and variations in the type of hard negatives used in the generation of text options. Thereare three types of hard negatives used- atomic hard negatives, swap hard negatives, and negation hardnegatives. PCFG SET. For the Productivity test (Hupkes et al., 2020), the data is split based on sequence lengths.The model is tested on sequences longer than the ones observed during training. For example, in a grammarcontext, if the model has been trained on the syntax entity-relation-entity, it will be tested on a longer,nested version of this concept, entity-relation-(entity-relation-entity). CFQ. This is a dataset of Compositional Freebase Questions (CFQ). It is a natural language question-answering task(Keysers et al., 2020), focusing on semantic parsing, with a SPARQL query against theFreebase knowledge base. Questions are generated at varying levels of complexity. Various splits are availablebased on input length, output length, input pattern, or output pattern. All these splits aim to maximizecompound divergence while minimizing atom divergence. This task appears to test both systematicity andproductivity to some extent, although not explicitly. However, it cannot identify specic areas where amodels compositional behavior may be decient. COGS. While some splits of this dataset are explained in .1, it also contains a split for lengthgeneralization. Category 3 (i.e. Deeper Recursion), one of the splits of this dataset, (Kim & Linzen, 2020)is a test for length generalization by increasing the length of the input sequence recursively during testing.Input sequences are generated using nesting of phrases that are longer than those seen during training.",
  "Other Generalization Criteria": "To the best of our knowledge, PCFG SET (Hupkes et al., 2020) is the only benchmark that evaluates theother three additional criteria. The Substitutivity or synonymity test uses an input sequence with anatomic unit replaced by a synonymous atomic unit to evaluate how the model prediction changes. Localismis tested by using input sequences composed of smaller sequences A and B. The model is used to translatethe full sequences rst and then forced to process A and B separately. The outputs of these two experimentsare compared to evaluate how local versus global the model is in its compositional reasoning. Overgener-alization test evaluates the models results on input sequences that do not conform to the general rules ofthe dataset, that is, input sequences that are exceptions to the dataset rules. For example, the acquisitionof past-tense forms such as the common ed ending (open-opened) versus more uncommon forms such asbreak to broke.",
  "Empirical Findings: Compositional Learning Models": "Traditional symbolic AI models naturally support compositional reasoning using classical logic applied toformal language (Szab, 2022), formal verication (Giannakopoulou et al., 2018), and more. First-order logiccan express objects, their properties, and complex compositional relations. Logical operations like conjunc-tion, disjunction, and implication can express compositional structures on which inference rules are applied,supporting complex compositional reasoning (Porto, 2002). Another classical symbolic formalism includesgrammars (Chomsky, 1965), which can express and generate complex compositional structures. However,dealing with noisy and uncertain data is hard with pure symbolic AI. Moreover, probabilistic augmenta-tions and structured output prediction models have been able to explicitly model structural dependenciesand support compositional reasoning based on their learned complex patterns from the data (Pearl, 1988).Nevertheless, scalability becomes a challenge for training and inference as the structural dependencies andthe number of correlated variables increase. Given these long-lasting challenges of traditional models ofcompositionality, current neural models have shown success in both scalability and dealing with noisy andsensory data (OpenAI, 2024). Especially in modern large language models, complex compositional patternscan be memorized and resemble compositional reasoning. In the rest of this section, we overview the researchfocused on the development, design, and empirical evaluation of dierent types of neural models for compo-sitional reasoning. We relate the type of empirical evaluations to the cognitive aspects of compositionalitythat they are testing. While some of these models utilize tasks and datasets covered in , othershave their own tasks, specic to the problem of their choice.",
  "Basic Neural Models": "In Hupkes et al. (2020), dierent neural models are tested on a set of compositional learning tasks. Theyevaluate Long short-term memory (LSTM) Networks (Hochreiter & Schmidhuber, 1997), ConvolutionalNeural Networks (CNN) (LeCun et al., 1989), and Transformers for sequence-to-sequence language processingtasks on their proposed PCFG SET tasks including systematicity, productivity, substitutivity, localism andovergeneralization. On average, Transformer outperformed the other two models, but within the two classicneural models, the convolutional model performs better than the LSTM counterpart. In the reported results,two specic architectures, called LSTMS2S and ConvS2S were used. LSTMS2S is a recurrent, bidirectionalencoder-decoder model with attention where the encoder and decoder are LSTMs, from the OpenMT-pyframework. ConvS2S is a convolution-based sequence-to-sequence model as used in Gehring et al. (2017).",
  "Transformer-based Architectures": "The compositional capability of large language models is currently a controversial topic. They have beenevaluated on general tasks such as arithmetic, logic, and dynamic programming that are compositional bynature Nafar et al. (2024a;b). Some of these evaluation eorts conducted in Dziri et al. (2023) concluded thatGPT (OpenAI, 2024) family models, solve these tasks by reducing them to linearized subgraph matching,without developing true compositional reasoning skills. Moreover, it is shown that, asymptotically, theyhave architectural limitations in solving highly complex compositional tasks with novel patterns due to errorpropagation of the composition of erroneous building block functions. There is a substantial gap in the per-formance of Transformers on in-domain and low-complexity compositional examples versus out-of-domaininstances. The tested tasks were 1) multi-digit multiplication (Hiebert, 2013), 2) Einsteins puzzle, which isa constraint satisfaction problem (Prosser, 1993), and 3) NP-complete maximum weighted independent setproblem (Kleinberg & Tardos, 2005). These tasks are mostly aimed at testing systematicity and productivity.Their results indicate that transformers make predictions on shallow reasoning and memorization of similarsubgraph patterns seen during training as opposed to reasoning holistically based on true compositionalreasoning. This hypothesis also aligns with the ndings presented in Chang & Bisk (2024). They conductexperiments with counting problems, a basic form of length generalization tasks. While transformers cancount in observed cases, they dramatically fail to perform out-of-domain for the same task, indicating thattransformers rely on memorizing observations. The results on transformers often extend to transformer-basedlanguage models. As shown in this work (Anil et al., 2022), ne-tuned transformer-based large languagemodels lack generalization capabilities irrespective of the model size, when tested on other length generaliza-tion tasks such as parity and boolean variable assignment. This is due to the transformers tendency to learnnon-sequential patterns that do not apply to longer sequences. However, combining a pretrained LLMs in-context learning ability with scratchpad prompting signicantly improves performance on longer sequences.This also implies that despite having access to an innite data pool, LLMs can potentially learn some tasksbetter through in-context learning than netuning, conrmed by theoretical work on LLMs explained in. While a series of research papers focused on evaluating the compositional generalization of Transformers (De-hghani et al., 2019; Hahn, 2020; Feng et al., 2023) and complex reasoning Mirzaee et al. (2021); Mirzaee& Kordjamshidi (2022), some recent research investigated specic architectural factors that can impact theperformance of Transformers on compositional tasks, following the claim that Transformers cannot reasoncompositionally (Dziri et al., 2023). In Ontanon et al. (2022), ve congurations were evaluated on sev-eral dierent datasets and benchmarks, by varying ve properties of Transformers including, 1) type ofpositional encoding, 2) use of copy decoders, 3) model size, 4) weight sharing, and 5) use of intermediaterepresentations for prediction. The employed tasks were Addition, AdditionNegatives, Reversing, Dupli-cation, Cartesian, Intersection, SCAN-length and SCAN-add-jump, PCFG productivity and systematicity,COGS, and CFQ-mcd1. This work concluded that relative positional encodings usually help, but usingembeddings is necessary, and merely relative position biases are not sucient. Tasks like SCAN and CFQwere not aected by positional embeddings. Tasks like Duplication or PCFG benet from a copy decoderbecause it can learn a type of symmetry like learning a certain position of the input. As for model size, it wasfound that for algorithmic tasks, large models did not make a dierence. However, for PCFG, large modelsseemed to outperform their smaller variants. Weight sharing across transformer layers seems to improveaccuracy in most tasks. Intermediate representations also improved performance by creating new levels ofabstraction that make reasoning easier for solving the end task. Specically, using intermediate represen-tations achieved state-of-the-art performance on COGS by converting the task from seq2seq to sequencetagging. Using intermediate representation on CFQ, eliminating the need to perform Cartesian products",
  "Neurosymbolic Architectures": "A rising trend in cutting-edge research on modeling intelligent systems is neurosymbolic modeling. As theneed for general-purpose AI models grows, there is a need for highly compositional models that can reasonbased on previously trained simpler tasks to do novel and complex ones. Although not explicitly mentionedin this research, they mainly address systematicity and productivity. One approach in this direction is to usenatural language explanations to generate formal specications that explicitly lay out a compositional task interms of required simpler steps. The formal specications then are passed to appropriate engines to solve theproblem. A prominent vision understanding model that follows this approach is VisProg (Gupta & Kembhavi,2023). VisProg is a modular neurosymbolic model that can solve various compositional visual reasoning tasksgiven natural language instruction relying merely on the in-context learning of large language models. Itproduces modular programs in Python to obtain the solution.This approach provides an interpretablereasoning for how the model derives the solution. These modular programs use built-in modules supportedby VisProg such as o-the-shelf neural computer vision models, image preprocessing modules, or Pythonsubroutines, and solve complex tasks without any task-specic training. Another example in this line ofwork is to generate a formal logical specication of the problem from natural language explanations and passthe logical form to a logical reasoner engine (Poesia et al., 2023). This work uses large language models suchas GPT-3 or GPT-3.5 Turbo, for producing guides to solving complex compositional tasks by breakingthose down into smaller steps based on a reasoning chain.Similar to this, many recent works focusedon dierent prompting strategies that can be used to solve complex compositional tasks with a modularapproach. Examples include Decomposed Prompting (Khot et al., 2023), which uses a modular approach",
  "Theoretical Findings: Mathematical Formulations of Compositionality": "Theoretical analysis is fundamental for deepening our understanding of the compositionality of learningmodels.It can reveal intriguing and previously uncovered information that experimental analysis mayoverlook. Many research works have proposed diverse approaches for investigating the compositionality oflearning models. We highlight three dierent approaches, including a mathematical framework for deningcompositionality (Ram et al., 2023), exploring the upper-bounds of expressivity that relate to composition-ality (Merrill & Sabharwal, 2023), and analyzing error-bounds to demonstrate the models limitations insolving compositional learning problems (Dziri et al., 2023). In the rest of this section, we provide a detailedoverview of these cases and explain the theoretical results on compositional generalization of classical neuralnetworks, transformers, and modern language models. We also relate the mentioned techniques to aspectsof compositionality when applicable.",
  "Classical Neural Network": "Ram et al. (2023) provides a mathematical denition of compositionality for learning models and connectstheir expressively to computational complexity. They frame the existing well-known models, such as vari-ations of RNN and CovNets, with the provided formal denition to explain properties related to theircompositional generalization. Hewitt et al. (2020) further investigates the RNNs ability to generate naturallanguage with a certain nesting depth. They claim that RNNs with optimal memory and O(m log k) hiddenunits can generate a natural language of well-nested brackets of k types and m bounded nesting depth. Withthe rise of LLMs, compositional generalization has recently become more critical. Due to their large-scaleparameters and training data, LLMs perform empirically well on many tasks. However, the empirical per-formance measures are now less reliable, as the high performance on test data can not be interpreted ascompositional generalization anymore. This issue is due to the nature of internet-scale training of LLMsand data contamination. Consequently, there is more urgency for theoretical studies to understand theirlimitations and measure their reliability in unobserved situations. However, Ahn et al. (2023) argued thatstudying the smaller models at the single neuron level potentially leads to a better understanding of thelarge/deep models learning behavior, which is related to explaining the Systematicity of the model.Theyalso establish a connection between the Edge of Stabilityidentied by the learning rate of the gradientdescent approach for non-convex optimization and the emergent abilities in learning. This result remainslimited to the scope of a single neuron and has not yet been extended to large models.",
  "Transformers": "To dene the limitations of LLMs, it is essential to investigate the limitations of transformers and their un-derlying architectural component. In this work (Merrill & Sabharwal, 2023), the authors assume a specictransformer type, suggesting that their arithmetic precision is logarithmic in the number of input tokens.Based on this assumption, they demonstrate that transformers cannot accurately solve linear equalities orcheck membership in an arbitrary context-free grammar with empty productions. The studies of transformerprecision have been explored before in Dehghani et al. (2019). They claim that standard transformers havelimited precision, implying that they cannot handle an innite input length. This conclusion indicates thelimitations in of the compositionally of the transformers in terms of the Productivity aspect. Another notabletheoretical investigations focus on the activation functions to explain the limitation of the transformer (De-hghani et al., 2019; Hahn, 2020). Hahn (2020) analyzes both hard-attention and soft-attention transformers.For hard attention, they prove that the transformer ignores most of the input information diagnosed bythe specic modications applied to the input. According to their analysis, transformers with hard atten-tion will be unable to solve problems that require processing the entire input, such as PARITY and logicalformula problems. However, this conclusion contradicts older papers that state transformers are Turingcomplete (Prez et al., 2021). They utilize the strong assumption that all input information is accessibleusing hard attention to prove Turing completeness. This leads to a dierent conclusion, stating that the",
  "Large Language Models": "In addition to inconclusive theoretical studies on transformer limitations, there are controversial results onlarge language models. The most noteworthy study is on the emerging abilities and capabilities claimedto be unique in the large models. The emerging abilities relate to the generalization to new and complextasks in LLMs. This kind of ability is also a feature of models compositional learning ability, allowing themto perform in novel compositional situations (Yu et al., 2024). Multiple works have shown the existenceof emergent abilities of LLMs (Wei et al., 2022).The recent work of Arora & Goyal (2023) provides amathematical framework for identifying complex skills in language models. They use the LLM Scaling Ruleto argue that emergent skills are the results of reducing excessive loss. This excessive loss enables the modelto learn how to utilize and combine skills from downstream tasks during training. Their claims are basedon the assumption that language inherently contains a random mix of complex skills. Although severalexperiments reveal these emerging capabilities, at least two papers disclaim their existence. The rst groupprovides a theoretical proof based on a mathematical framework. They illustrate that the emerging abilityappears due to the selected evaluation metrics that are nonlinear and discontinuous (Schaeer et al., 2023).They show as an artifact of the evaluation metrics, even simple models such as CNNs can show emergingabilities. Therefore, they conclude that emerging abilities may not be a fundamental property of the largemodels. Moreover, Lu et al. (2024) provides an extensive empirical study with 1000 experiments on 22 taskswith dierent LLMs. However, given the inconsistency in some results and the unpredictability of emergingabilities, they do not nd any strong evidence of how they emerge. They associate the performance within-context learning techniques, memorization, and data contamination. However, a recent study presentsa positive theoretical analysis of reasoning capabilities by investigating the chain of thought (CoT) (Weiet al., 2024) and draws a dierent conclusion. They argue that the log-precision transformer can performfundamental operations such as multiplication and a look-up table. Consequently, it can solve linear equationsand other reasoning problems if it stores all the input information. However, the architecture alone struggleswith storing the entire input, as observed in Dehghani et al. (2019); Merrill & Sabharwal (2023). They showthat the model addresses this limitation by repeatedly referring to the input by enabling CoT (Feng et al.,2023). Therefore, with the right number of CoT examples, LLMs can overcome the transformers weaknessin solving mathematical reasoning. This is aligned with previous empirical results of in-context learningdiscussed in .2.",
  ": Summary of computational models with compositional learning ability from the theoretical per-spective and an example from the experimental perspective": "1965; 2002). However, from the AI and machine learning perspective, ideas are borrowed from both cognitiveand linguistics, and computational tasks and models are designed focusing on narrow aspects of composi-tionality (Hupkes et al., 2020). Our investigation of AI models indicates several challenges regarding thedesigned tasks, benchmarks, and theoretical frameworks that make the evaluation of computational modelsproblematic. shows the connections between the main topics identied and discussed in this survey. Among theve identied types of compositional learning facets, only systematicity and productivity are well-researchedand have clear connections to evaluation benchmarks. Given that these are the ve main metrics of com-positionality, we should aim to expand our evaluation capabilities by developing tests for the other three aswell. Empirical evaluations are comparatively more well-studied compared to theoretical analyses. Theoret-ical evaluations are either lacking or do not follow a consistent methodology. There is a lack of connectionbetween the theoretical methods and the cognitive aspects, making these results hard to use to guide betterarchitectural design. For Models, dierent types of architectures have been designed. However, evaluatingLLMs and making fundamental design decisions for compositional generalization present new challenges. Weoutline some of these challenges in detail below. Less Explored Facets of Compositionality.Only Systematicity and Productivity have been well-researched and have established connections to evaluation benchmarks. While the other three were intro-duced as fundamental types of compositionality, they have received less attention, as they appear to be lesscommonly occurring aspects of compositionality. However, in the era of LLMs and the emergent in-contextlearning, Substitutivity and Localism are potential bottlenecks in the performance of LLMs for attending theappropriate context for solving problems. Moreover, Overgeneralization can be associated with hallucina-tion and generating unfounded and incorrect information by making up new unrealistic abstractions. Whilehallucination is a broader concept than overgeneralization, this compositional learning facet can highlight animportant aspect to be addressed to prevent hallucination (Huang et al., 2023). Therefore, directing atten-tion to the other three types of measures can help establish new formal evaluation benchmarks, contributingto the development of more robust systems and addressing the challenges of the LLMs. Synthetic and Unrealistic Evaluations. One issue in current evaluations is that controlled and cleantests of compositonality are mostly synthesized (Wu et al., 2021; Ruis et al., 2020).This is evidencedby our examples in . Even in rare cases that claim to work with realistic data (Keysers et al.,2020), synthesized questions are used to query knowledge graphs. However, more recent studies on languagemodels evaluation of compositionality focus on more challenging problems such as multi-hop question an-swering (Press et al., 2023; Liu et al., 2022b; Okawa et al., 2023; Mirzaee et al., 2021) as well as complexpuzzles with combinatorial search solutions or compositional mathematical reasoning (Dziri et al., 2023).Although the benchmarks are designed for evaluating specic tasks, the reliance on mostly synthesized datarisks the eectiveness of generalization to real-world data, which is often more complex. Misalignment of Performance and Compositional Learning (LLM Evaluation Challenge). Thesecond challenge that mostly applies to LLMs is data contamination.Though the recent research com-pares language models to the specialized architectures and indicates their outperformance in compositionaltasks (Furrer et al., 2021), this result does not necessarily mean these models have better generalizations in",
  "Limitations": "Despite the comprehensive nature of the survey and our eorts to cover and connect most research relatingto compositional learning, we would like to acknowledge some limitations. The scope of this survey coversa broad spectrum of topics and tries to capture both theoretical and experimental frameworks, but theremight be some relevant papers that are missed. Compositional learning is an interdisciplinary topic acrossComputer Science, Linguistics, Cognitive Science, etc. Although we have included insights and connectionsfrom across these elds, our work has a more in-depth focus on Computer Science literature, especiallyNatural language processing. While we tried to provide the overall picture of the related research and builda coherent story, we might not capture the detailed nuances of each denition and application.",
  "P. W. Anderson. More is dierent. Science, 177(4047):393396, 1972. doi: 10.1126/science.177.4047.393.URL": "Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein. Neural module networks. In 2016 IEEEConference on Computer Vision and Pattern Recognition (CVPR), pp. 3948, 2016. doi: 10.1109/CVPR.2016.12. Cem Anil, Yuhuai Wu, Anders Johan Andreassen, Aitor Lewkowycz, Vedant Misra, Vinay Venkatesh Ra-masesh, Ambrose Slone, Guy Gur-Ari, Ethan Dyer, and Behnam Neyshabur. Exploring length gener-alization in large language models. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and KyunghyunCho (eds.), Advances in Neural Information Processing Systems, 2022. URL",
  "Wentao Bao, Lichang Chen, Heng Huang, and Yu Kong. Prompting language-informed distribution forcompositional zero-shot learning, 2024. URL": "David Barrett, Felix Hill, Adam Santoro, Ari Morcos, and Timothy Lillicrap. Measuring abstract reasoningin neural networks.In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th InternationalConference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp. 511520.PMLR, 1015 Jul 2018. URL Emily M. Bender and Alexander Koller. Climbing towards NLU: On meaning, form, and understanding inthe age of data. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault (eds.), Proceedingsof the 58th Annual Meeting of the Association for Computational Linguistics, pp. 51855198, Online,July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.463. URL Samuel R. Bowman, Christopher D. Manning, and Christopher Potts. Tree-structured composition in neuralnetworks without tree-structured architectures. In Proceedings of the 2015th International Conference onCognitive Computation: Integrating Neural and Symbolic Approaches - Volume 1583, COCO15, pp. 3742,Aachen, DEU, 2015. CEUR-WS.org. Shulin Cao, Jiaxin Shi, Liangming Pan, Lunyiu Nie, Yutong Xiang, Lei Hou, Juanzi Li, Bin He, and HanwangZhang. KQA pro: A dataset with explicit compositional programs for complex question answering overknowledge base. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio (eds.), Proceedings ofthe 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp.61016119, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.422. URL",
  "Noam Chomsky.Backmatter, pp. 115118.De Gruyter Mouton, Berlin, New York, 2002.ISBN9783110218329. doi: doi:10.1515/9783110218329.bm. URL": "Rbert Csords, Kazuki Irie, and Juergen Schmidhuber. The devil is in the detail: Simple tricks improvesystematic generalization of transformers. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, andScott Wen-tau Yih (eds.), Proceedings of the 2021 Conference on Empirical Methods in Natural LanguageProcessing, pp. 619634, Online and Punta Cana, Dominican Republic, November 2021. Association forComputational Linguistics. doi: 10.18653/v1/2021.emnlp-main.49. URL Vanessa DAmario, Tomotake Sasaki, and Xavier Boix. How modular should neural module networks be forsystematic generalization?In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan (eds.),Advances in Neural Information Processing Systems, 2021. URL",
  "Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Lukasz Kaiser. Universal trans-formers. In International Conference on Learning Representations, 2019. URL": "Roberto Dess and Marco Baroni. CNNs found to jump around more skillfully than RNNs: Compositionalgeneralization in seq2seq convolutional networks. In Anna Korhonen, David Traum, and Llus Mrquez(eds.), Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 39193923, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1381.URL Nouha Dziri, Ximing Lu, Melanie Sclar, Xiang (Lorraine) Li, Liwei Jiang, Bill Yuchen Lin, SeanWelleck, Peter West, Chandra Bhagavatula, Ronan Le Bras, Jena Hwang, Soumya Sanyal, Xi-ang Ren, Allyson Ettinger, Zaid Harchaoui, and Yejin Choi.Faith and fate:Limits of trans-formers on compositionality.In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, andS. Levine (eds.), Advances in Neural Information Processing Systems, volume 36, pp. 7029370332. Cur-ran Associates, Inc., 2023. URL Guhao Feng, Bohang Zhang, Yuntian Gu, Haotian Ye, Di He, and Liwei Wang.Towards revealing themystery behind chain of thought: A theoretical perspective.In Thirty-seventh Conference on NeuralInformation Processing Systems, 2023. URL",
  "Daniel Furrer, Marc van Zee, Nathan Scales, and Nathanael Schrli. Compositional generalization in semanticparsing: Pre-training vs. specialized architectures, 2021": "Tong Gao, Qi Huang, and Raymond Mooney. Systematic generalization on gSCAN with language conditionedembedding. In Kam-Fai Wong, Kevin Knight, and Hua Wu (eds.), Proceedings of the 1st Conference of theAsia-Pacic Chapter of the Association for Computational Linguistics and the 10th International JointConference on Natural Language Processing, pp. 491503, Suzhou, China, December 2020. Association forComputational Linguistics. URL Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolutional sequenceto sequence learning. In Proceedings of the 34th International Conference on Machine Learning - Volume70, ICML17, pp. 12431252. JMLR.org, 2017. Robert Geirhos, Jrn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel, Wieland Brendel, MatthiasBethge, and Felix A. Wichmann.Shortcut learning in deep neural networks.Nature Machine In-telligence, 2(11):665673, November 2020.ISSN 2522-5839.doi: 10.1038/s42256-020-00257-z.URL Dimitra Giannakopoulou, Kedar S. Namjoshi, and Corina S. Psreanu.Compositional Reasoning, pp.345383.Springer International Publishing, Cham, 2018.ISBN 978-3-319-10575-8.doi:10.1007/978-3-319-10575-8_12. URL Ross Girshick, Pedro Felzenszwalb, and David McAllester.Object detection with grammar models.InJ. Shawe-Taylor, R. Zemel, P. Bartlett, F. Pereira, and K.Q. Weinberger (eds.), Advances in NeuralInformation Processing Systems, volume 24. Curran Associates, Inc., 2011. URL Nicolas Gontier, Koustuv Sinha, Siva Reddy, and Chris Pal. Measuring systematic generalization in neu-ral proof generation with transformers.In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, andH. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 2223122242. Cur-ran Associates, Inc., 2020. URL Liangke Gui, Yingshan Chang, Qiuyuan Huang, Subhojit Som, Alexander G Hauptmann, Jianfeng Gao, andYonatan Bisk. Training vision-language transformers from captions. Transactions on Machine LearningResearch, 2023. ISSN 2835-8856. URL Tanmay Gupta and Aniruddha Kembhavi. Visual programming: Compositional visual reasoning withouttraining.In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition(CVPR), pp. 1495314962, June 2023. Michael Hahn.Theoretical limitations of self-attention in neural sequence models.Transactions of theAssociation for Computational Linguistics, 8:156171, December 2020. ISSN 2307-387X. doi: 10.1162/tacl_a_00306. URL Monica Haurilet, Alina Roitberg, and Rainer Stiefelhagen. Its not about the journey; its about the destina-tion: Following soft paths under question-guidance for visual reasoning. In 2019 IEEE/CVF Conference onComputer Vision and Pattern Recognition (CVPR), pp. 19301939, 2019. doi: 10.1109/CVPR.2019.00203.",
  "Kaiming He, X. Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. 2016IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 770778, 2015. URL": "Ruining He and Julian McAuley. Ups and downs: Modeling the visual evolution of fashion trends withone-class collaborative ltering. In Proceedings of the 25th International Conference on World Wide Web,WWW 16, pp. 507517, Republic and Canton of Geneva, CHE, 2016. International World Wide WebConferences Steering Committee. ISBN 9781450341431. doi: 10.1145/2872427.2883037. URL",
  "Matthias Hofer, Tuan Anh Le, Roger Levy, and Josh Tenenbaum. Learning evolved combinatorial symbolswith a neuro-symbolic generative model, 2021": "Yining Hong, Qing Li, Song-Chun Zhu, and Siyuan Huang. Vlgrammar: Grounded grammar inductionof vision and language. In Proceedings of the IEEE/CVF International Conference on Computer Vision(ICCV), pp. 16651674, October 2021. Joy Hsu, Jiayuan Mao, Joshua B. Tenenbaum, and Jiajun Wu. Whats left? concept grounding with logic-enhanced foundation models. In Thirty-seventh Conference on Neural Information Processing Systems,2023. URL Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen,Weihua Peng, Xiaocheng Feng, Bing Qin, and Ting Liu. A survey on hallucination in large language models:Principles, taxonomy, challenges, and open questions, 2023. URL Dieuwke Hupkes, Sara Veldhoen, and Willem Zuidema. Visualisation and diagnostic classiers reveal howrecurrent and recursive neural networks process hierarchical structure. J. Artif. Int. Res., 61(1):907926,January 2018. ISSN 1076-9757. Dieuwke Hupkes, Verna Dankers, Mathijs Mul, and Elia Bruni. Compositionality decomposed: How doneural networks generalise? (extended abstract). In Christian Bessiere (ed.), Proceedings of the Twenty-Ninth International Joint Conference on Articial Intelligence, IJCAI-20, pp. 50655069. InternationalJoint Conferences on Articial Intelligence Organization, 7 2020.doi: 10.24963/ijcai.2020/708.URL Journal track. Dieuwke Hupkes, Mario Giulianelli, Verna Dankers, Mikel Artetxe, Yanai Elazar, Tiago Pimentel, ChristosChristodoulopoulos, Karim Lasri, Naomi Saphra, Arabella Sinclair, Dennis Ulmer, Florian Schottmann,Khuyagbaatar Batsuren, Kaiser Sun, Koustuv Sinha, Leila Khalatbari, Maria Ryskina, Rita Frieske, RyanCotterell, and Zhijing Jin. A taxonomy and review of generalization research in nlp. Nature MachineIntelligence, 5(10):11611174, 2023. ISSN 2522-5839. doi: 10.1038/s42256-023-00729-y. URL",
  "Phillip Isola, Joseph J. Lim, and Edward H. Adelson.Discovering states and transformations in imagecollections. In CVPR, 2015": "Takuya Ito, Tim Klinger, Doug Schultz, John Murray, Michael Cole, and Mattia Rigotti.Com-positionalgeneralizationthroughabstractrepresentationsinhumanandarticialneuralnet-works.In S. Koyejo,S. Mohamed,A. Agarwal,D. Belgrave,K. Cho,and A. Oh (eds.),Advances in Neural Information Processing Systems,volume 35,pp. 3222532239. Curran As-sociates,Inc.,2022.URL Theo M.V. Janssen and Barbara H. Partee.Chapter 7 - compositionality.In Johan van Benthem andAlice ter Meulen (eds.), Handbook of Logic and Language, pp. 417473. North-Holland, Amsterdam, 1997.ISBN 978-0-444-81714-3. doi: URL Anya Ji, Noriyuki Kojima, Noah Rush, Alane Suhr, Wai Keen Vong, Robert Hawkins, and Yoav Artzi.Abstract visual reasoning with tangram shapes. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang(eds.), Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp.582601, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics.doi: 10.18653/v1/2022.emnlp-main.38. URL Zhijing Jin*, Yuen Chen*, Felix Leeb*, Luigi Gresele*, Ojasv Kamal, Zhiheng Lyu, Kevin Blin, FernandoGonzalez, Max Kleiman-Weiner, Mrinmaya Sachan, and Bernhard Schlkopf. Cladder: Assessing causalreasoning in language models.In NeurIPS, 2023.URL Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Li Fei-Fei, C. Lawrence Zitnick, and Ross B.Girshick. Clevr: A diagnostic dataset for compositional language and elementary visual reasoning. 2017IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 19881997, 2016.URL Danial Kamali and Parisa Kordjamshidi. Syntax-guided transformers: Elevating compositional generalizationand grounding in multimodal environments. In Dieuwke Hupkes, Verna Dankers, Khuyagbaatar Batsuren,Koustuv Sinha, Amirhossein Kazemnejad, Christos Christodoulopoulos, Ryan Cotterell, and Elia Bruni(eds.), Proceedings of the 1st GenBench Workshop on (Benchmarking) Generalisation in NLP, pp. 130142,Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.genbench-1.10. URL Amirhossein Kazemnejad, Inkit Padhi, Karthikeyan Natesan, Payel Das, and Siva Reddy. The impact ofpositional encoding on length generalization in transformers.In Thirty-seventh Conference on NeuralInformation Processing Systems, 2023. URL Daniel Keysers, Nathanael Schrli, Nathan Scales, Hylke Buisman, Daniel Furrer, Sergii Kashubin, NikolaMomchev, Danila Sinopalnikov, Lukasz Staniak, Tibor Tihon, Dmitry Tsarkov, Xiao Wang, Marc vanZee, and Olivier Bousquet. Measuring compositional generalization: A comprehensive method on realisticdata. In International Conference on Learning Representations, 2020. URL Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, and Ashish Sabhar-wal. Decomposed prompting: A modular approach for solving complex tasks. In The Eleventh InternationalConference on Learning Representations, 2023. URL",
  "Noriyuki Kojima, Hadar Averbuch-Elor, and Yoav Artzi.A joint study of phrase grounding and taskperformance in vision and language models.Trans. Mach. Learn. Res., 2024, 2023.URL": "Kris Korrel, Dieuwke Hupkes, Verna Dankers, and Elia Bruni. Transcoding compositionally: Using attentionto nd more generalizable solutions. In Tal Linzen, Grzegorz Chrupaa, Yonatan Belinkov, and DieuwkeHupkes (eds.), Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting NeuralNetworks for NLP, pp. 111, Florence, Italy, August 2019. Association for Computational Linguistics. doi:10.18653/v1/W19-4801. URL",
  "Yen-Ling Kuo, Boris Katz, and Andrei Barbu. Compositional networks enable systematic generalization forgrounded language understanding, 2020. URL": "Brenden M. Lake and Marco Baroni. Generalization without systematicity: On the compositional skills ofsequence-to-sequence recurrent networks. In International Conference on Machine Learning, 2017. URL Brenden M. Lake, Tomer D. Ullman, Joshua B. Tenenbaum, and Samuel J. Gershman. Building machinesthat learn and think like people. Behavioral and Brain Sciences, 40, 2017. doi: 10.1017/s0140525x16001837.",
  "Brenden M. Lake, Tal Linzen, and Marco Baroni. Human few-shot learning of compositional instructions.In Annual Meeting of the Cognitive Science Society, 2019. URL": "Guillaume Lample, Sandeep Subramanian, Eric Smith, Ludovic Denoyer, MarcAurelio Ranzato, and Y-LanBoureau. Multiple-attribute text rewriting. In International Conference on Learning Representations,2019. URL Yann LeCun, Bernhard Boser, John Denker, Donnie Henderson, R. Howard, Wayne Hubbard, and LawrenceJackel. Handwritten digit recognition with a back-propagation network. In D. Touretzky (ed.), Advances inNeural Information Processing Systems, volume 2. Morgan-Kaufmann, 1989. URL Michael A. Lepori, Thomas Serre, and Ellie Pavlick. Break it down: Evidence for structural compositionalityin neural networks. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL Yafu Li, Yongjing Yin, Yulong Chen, and Yue Zhang.On compositional generalization of neural ma-chine translation. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (eds.), Proceedings ofthe 59th Annual Meeting of the Association for Computational Linguistics and the 11th InternationalJoint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 47674780, Online,August 2021. Association for Computational Linguistics.doi:10.18653/v1/2021.acl-long.368.URL",
  "Zhiqiu Lin, Xinyue Chen, Deepak Pathak, Pengchuan Zhang, and Deva Ramanan. Revisiting the role oflanguage priors in vision-language models. In ICML, 2024. URL": "Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. Program induction by rationale generation:Learning to solve and explain algebraic word problems. In Regina Barzilay and Min-Yen Kan (eds.),Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: LongPapers), pp. 158167, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi:10.18653/v1/P17-1015. URL Chenyao Liu, Shengnan An, Zeqi Lin, Qian Liu, Bei Chen, Jian-Guang Lou, Lijie Wen, Nanning Zheng, andDongmei Zhang. Learning algebraic recombination for compositional generalization. In Chengqing Zong,Fei Xia, Wenjie Li, and Roberto Navigli (eds.), Findings of the Association for Computational Linguistics:ACL-IJCNLP 2021, pp. 11291144, Online, August 2021. Association for Computational Linguistics. doi:10.18653/v1/2021.ndings-acl.97. URL Guisheng Liu, Yi Li, Yanqing Guo, Xiangyang Luo, and Bo Wang. Multi-attribute controlled text generationwith contrastive-generator and external-discriminator. In Nicoletta Calzolari, Chu-Ren Huang, HansaemKim, James Pustejovsky, Leo Wanner, Key-Sun Choi, Pum-Mo Ryu, Hsin-Hsi Chen, Lucia Donatelli,Heng Ji, Sadao Kurohashi, Patrizia Paggio, Nianwen Xue, Seokhwan Kim, Younggyun Hahm, ZhongHe, Tony Kyungil Lee, Enrico Santus, Francis Bond, and Seung-Hoon Na (eds.), Proceedings of the 29thInternational Conference on Computational Linguistics, pp. 59045913, Gyeongju, Republic of Korea,October 2022a. International Committee on Computational Linguistics. URL Nan Liu, Shuang Li, Yilun Du, Antonio Torralba, and Joshua B. Tenenbaum. Compositional visual gen-eration with composable diusion models.In Computer Vision ECCV 2022: 17th European Con-ference, Tel Aviv, Israel, October 2327, 2022, Proceedings, Part XVII, pp. 423439, Berlin, Heidel-berg, 2022b. Springer-Verlag.ISBN 978-3-031-19789-5.doi:10.1007/978-3-031-19790-1_26.URL Sheng Lu, Irina Bigoulaeva, Rachneet Sachdeva, Harish Tayyar Madabushi, and Iryna Gurevych.Areemergent abilities in large language models just in-context learning?In Lun-Wei Ku, Andre Martins,and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers), pp. 50985139, Bangkok, Thailand, August 2024. Association forComputational Linguistics. doi: 10.18653/v1/2024.acl-long.279. URL Zixian Ma, Jerry Hong, Mustafa Omer Gul, Mona Gandhi, Irena Gao, and Ranjay Krishna. @ crepe: Canvision-language foundation models reason compositionally?2023 IEEE/CVF Conference on ComputerVision and Pattern Recognition (CVPR), pp. 1091010921, 2022. URL",
  "Gary Marcus. Deep learning: A critical appraisal, 2018": "Gary F. Marcus, Steven Pinker, Michael Ullman, Michelle Hollander, T. John Rosen, Fei Xu, and HaraldClahsen. Overregularization in language acquisition. Monographs of the Society for Research in Child De-velopment, 57(4):i178, 1992. ISSN 0037976X, 15405834. URL Marco Marelli, Stefano Menini, Marco Baroni, Luisa Bentivogli, Raaella Bernardi, and Roberto Zamparelli.A SICK cure for the evaluation of compositional distributional semantic models. In Nicoletta Calzolari,Khalid Choukri, Thierry Declerck, Hrafn Loftsson, Bente Maegaard, Joseph Mariani, Asuncion Moreno,",
  "Jorge A. Mendez and Eric Eaton. How to reuse and compose knowledge for a lifetime of tasks: A survey oncontinual learning and functional composition, 2023. URL": "William Merrill and Ashish Sabharwal. The parallelism tradeo: Limitations of log-precision transformers.Transactions of the Association for Computational Linguistics, 11:531545, 2023. doi: 10.1162/tacl_a_00562. URL Tomas Mikolov, Armand Joulin, and Marco Baroni. A roadmap towards machine intelligence. In AlexanderGelbukh (ed.), Computational Linguistics and Intelligent Text Processing, pp. 2961, Cham, 2018. SpringerInternational Publishing. ISBN 978-3-319-75477-2. Pasquale Minervini, Sebastian Riedel, Pontus Stenetorp, Edward Grefenstette, and Tim Rocktschel. Learn-ing reasoning strategies in end-to-end dierentiable proving.In Proceedings of the 37th InternationalConference on Machine Learning, ICML20. JMLR.org, 2020. Roshanak Mirzaee and Parisa Kordjamshidi. Transfer learning with synthetic corpora for spatial role labelingand reasoning. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (eds.), Proceedings of the 2022 Con-ference on Empirical Methods in Natural Language Processing, pp. 61486165, Abu Dhabi, United ArabEmirates, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.emnlp-main.413. URL Roshanak Mirzaee, Hossein Rajaby Faghihi, Qiang Ning, and Parisa Kordjamshidi.SPARTQA: A tex-tual question answering benchmark for spatial reasoning.In Kristina Toutanova, Anna Rumshisky,Luke Zettlemoyer, Dilek Hakkani-Tur, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty,and Yichao Zhou (eds.), Proceedings of the 2021 Conference of the North American Chapter of the As-sociation for Computational Linguistics: Human Language Technologies, pp. 45824598, Online, June2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.364. URL",
  "Richard Montague. Formal Philosophy: Selected Papers of Richard Montague. Yale University Press, NewHaven 1974": "Shikhar Murty, Pratyusha Sharma, Jacob Andreas, and Christopher Manning. Pushdown layers: Encodingrecursive structure in transformer language models.In Houda Bouamor, Juan Pino, and Kalika Bali(eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp.32333247, Singapore, December 2023. Association for Computational Linguistics.doi: 10.18653/v1/2023.emnlp-main.195. URL Muhammad Ferjad Naeem, Yongqin Xian, Federico Tombari, and Zeynep Akata. Learning graph embeddingsfor compositional zero-shot learning. 2021 IEEE/CVF Conference on Computer Vision and Pattern Recog-nition (CVPR), pp. 953962, 2021. URL Aliakbar Nafar, K. Brent Venable, and Parisa Kordjamshidi. Teaching probabilistic logical reasoning totransformers. In Yvette Graham and Matthew Purver (eds.), Findings of the Association for ComputationalLinguistics: EACL 2024, pp. 16151632, St. Julians, Malta, March 2024a. Association for ComputationalLinguistics. URL",
  "OpenAI. GPT-4 Technical Report, 2024": "Long Ouyang, Je Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang,Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller,Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Traininglanguage models to follow instructions with human feedback. In Proceedings of the 36th InternationalConference on Neural Information Processing Systems, NIPS 22, Red Hook, NY, USA, 2024. CurranAssociates Inc. ISBN 9781713871088.",
  "Laura Ruis, Jacob Andreas, Marco Baroni, Diane Bouchacourt, and Brenden M. Lake. A benchmark forsystematic generalization in grounded language understanding, 2020.URL": "Anian Ruoss, Grgoire Deltang, Tim Genewein, Jordi Grau-Moya, Rbert Csords, Mehdi Bennani, ShaneLegg, and Joel Veness. Randomized positional encodings boost length generalization of transformers. InAnna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.), Proceedings of the 61st Annual Meetingof the Association for Computational Linguistics (Volume 2: Short Papers), pp. 18891903, Toronto,Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-short.161. URL",
  "Jenny Saran, Seth Pollak, Rebecca Seibel, and Anna Shkolnik. Dog is a dog is a dog: Infant rule learningis not specic to language. Cognition, 105:66980, 12 2007. doi: 10.1016/j.cognition.2006.11.004": "Oscar Sainz, Jon Campos, Iker Garca-Ferrero, Julen Etxaniz, Oier Lopez de Lacalle, and Eneko Agirre. NLPevaluation in trouble: On the need to measure LLM data contamination for each benchmark. In HoudaBouamor, Juan Pino, and Kalika Bali (eds.), Findings of the Association for Computational Linguistics:EMNLP 2023, pp. 1077610787, Singapore, December 2023. Association for Computational Linguistics.doi: 10.18653/v1/2023.ndings-emnlp.722.URL",
  "H.T. Siegelmann and E.D. Sontag. On the computational power of neural nets. Journal of Computer andSystem Sciences, 50(1):132150, 1995. ISSN 0022-0000. doi: URL": "Ankur Sikarwar, Arkil Patel, and Navin Goyal.When can transformers ground and compose: Insightsfrom compositional generalization benchmarks. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang(eds.), Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp.648669, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics.doi: 10.18653/v1/2022.emnlp-main.41. URL Harman Singh, Pengchuan Zhang, Qifan Wang, Mengjiao Wang, Wenhan Xiong, Jingfei Du, and Yu Chen.Coarse-to-ne contrastive learning in image-text-graph space for improved vision-language composition-ality.In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Proceedings of the 2023 Conference onEmpirical Methods in Natural Language Processing, pp. 869893, Singapore, December 2023. Associationfor Computational Linguistics.doi: 10.18653/v1/2023.emnlp-main.56.URL Koustuv Sinha, Shagun Sodhani, Jin Dong, Joelle Pineau, and William L. Hamilton. CLUTRR: A diagnosticbenchmark for inductive reasoning from text. In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan(eds.), Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing andthe 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 45064515, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1458. URL",
  "Paul Smolensky and Graldine Legendre. The Harmonic Mind: From Neural Computation to Optimality-Theoretic GrammarVolume I: Cognitive Architecture (Bradford Books).The MIT Press, 2006.ISBN0262195267": "Paul Smolensky, R. Thomas McCoy, Roland Fernandez, Matthew Goldrick, and Jianfeng Gao.Neu-rocompositional computing in human and machine intelligence: A tutorial.Technical Report MSR-TR-2022-5, Microsoft, May 2022a. URL 52 pages maintext, 78 pages total, 11 gures, 2 Appendices, 239 references. For a short presentation of some of this ma-terial, see (to appear in AI Magazine). Paul Smolensky, Richard McCoy, Roland Fernandez, Matthew Goldrick, and Jianfeng Gao. Neurocomposi-tional computing: From the central paradox of cognition to a new generation of ai systems. AI Magazine,43(3):308322, Sep. 2022b. doi: 10.1002/aaai.12065. URL Sam Spilsbury, Pekka Marttinen, and Alexander Ilin. Generating demonstrations for in-context composi-tional generalization in grounded language learning. In Proceedings of the 2024 Conference on EmpiricalMethods in Natural Language Processing, Miami, FL, USA, November 2024. Association for ComputationalLinguistics.",
  "Paul Thagard. Cognitive Science. In Edward N. Zalta and Uri Nodelman (eds.), The Stanford Encyclopediaof Philosophy. Metaphysics Research Lab, Stanford University, Winter 2023 edition, 2023": "Pavel Tokmakov, Yu-Xiong Wang, and Martial Hebert. Learning compositional representations for few-shotrecognition. In 2019 IEEE/CVF International Conference on Computer Vision (ICCV), pp. 63716380,2019. doi: 10.1109/ICCV.2019.00647. Lazar Valkov, Dipak Chaudhari, Akash Srivastava, Charles Sutton, and Swarat Chaudhuri. Houdini: lifelonglearning as program synthesis. In Proceedings of the 32nd International Conference on Neural InformationProcessing Systems, NIPS18, pp. 87018712, Red Hook, NY, USA, 2018. Curran Associates Inc. Josef Valvoda, Naomi Saphra, Jonathan Rawski, Adina Williams, and Ryan Cotterell. Benchmarking com-positionality with formal languages. In Nicoletta Calzolari, Chu-Ren Huang, Hansaem Kim, James Puste-jovsky, Leo Wanner, Key-Sun Choi, Pum-Mo Ryu, Hsin-Hsi Chen, Lucia Donatelli, Heng Ji, Sadao Kuro-hashi, Patrizia Paggio, Nianwen Xue, Seokhwan Kim, Younggyun Hahm, Zhong He, Tony Kyungil Lee,Enrico Santus, Francis Bond, and Seung-Hoon Na (eds.), Proceedings of the 29th International Conferenceon Computational Linguistics, pp. 60076018, Gyeongju, Republic of Korea, October 2022. InternationalCommittee on Computational Linguistics. URL Liyuan Wang, Xingxing Zhang, Hang Su, and Jun Zhu.A comprehensive survey of continual learning:Theory, method and application. IEEE Transactions on Pattern Analysis and Machine Intelligence, 46(8):53625383, 2024. doi: 10.1109/TPAMI.2024.3367329. Jason Wei, Yi Tay, Rishi Bommasani, Colin Rael, Barret Zoph, Sebastian Borgeaud, Dani Yogatama,Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, PercyLiang, Je Dean, and William Fedus. Emergent abilities of large language models. Transactions on Ma-chine Learning Research, 2022. ISSN 2835-8856. URL Certication. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le,and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. In Proceedingsof the 36th International Conference on Neural Information Processing Systems, NIPS 22, Red Hook,NY, USA, 2024. Curran Associates Inc. ISBN 9781713871088.",
  "Zhengxuan Wu, Elisa Kreiss, Desmond C. Ong, and Christopher Potts. ReaSCAN: Compositional reasoningin language grounding. NeurIPS 2021 Datasets and Benchmarks Track, 2021. URL": "Zhengxuan Wu, Christopher D. Manning, and Christopher Potts.ReCOGS: How incidental detailsof a logical form overshadow an evaluation of semantic interpretation.Transactions of the Associ-ation for Computational Linguistics, 11:17191733, 2023.doi: 10.1162/tacl_a_00623.URL Guangyue Xu, Joyce Chai, and Parisa Kordjamshidi. GIPCOL: Graph-injected soft prompting for com-positional zero-shot learning. 2024 IEEE/CVF Winter Conference on Applications of Computer Vision(WACV), pp. 57625771, 2023a. URL Guangyue Xu, Parisa Kordjamshidi, and Joyce Chai. MetaReVision: Meta-learning with retrieval for visuallygrounded compositional concept acquisition.In Houda Bouamor, Juan Pino, and Kalika Bali (eds.),Findings of the Association for Computational Linguistics: EMNLP 2023, pp. 1222412236, Singapore,December 2023b. Association for Computational Linguistics. doi: 10.18653/v1/2023.ndings-emnlp.818.URL",
  "Aron Yu and Kristen Grauman. Fine-grained visual comparisons with local learning. In 2014 IEEE Con-ference on Computer Vision and Pattern Recognition, pp. 192199, 2014. doi: 10.1109/CVPR.2014.32": "Aron Yu and Kristen Grauman. Semantic jitter: Dense supervision for visual comparisons via syntheticimages. In 2017 IEEE International Conference on Computer Vision (ICCV). IEEE, October 2017. doi:10.1109/iccv.2017.594. URL Dingli Yu, Simran Kaur, Arushi Gupta, Jonah Brown-Cohen, Anirudh Goyal, and Sanjeev Arora. SKILL-MIX: a exible and expandable family of evaluations for AI models. In The Twelfth International Con-ference on Learning Representations, 2024. URL Hao Zheng and Mirella Lapata.Compositional generalization via semantic tagging.In Marie-FrancineMoens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih (eds.), Findings of the Association forComputational Linguistics: EMNLP 2021, pp. 10221032, Punta Cana, Dominican Republic, November2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.ndings-emnlp.88. URL Hao Zheng and Mirella Lapata. Disentangled sequence to sequence learning for compositional generalization.In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio (eds.), Proceedings of the 60th AnnualMeeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 42564268, Dublin,Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.293. URL Tianqi Zhong, Zhaoyi Li, Quan Wang, Linqi Song, Ying Wei, Defu Lian, and Zhendong Mao. Benchmarkingand improving compositional generalization of multi-aspect controllable text generation. In Lun-Wei Ku,Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Associationfor Computational Linguistics (Volume 1: Long Papers), pp. 64866517, Bangkok, Thailand, August2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.351. URL"
}