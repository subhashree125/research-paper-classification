{
  "Abstract": "Combining reinforcement learning with language grounding is challenging as the agent needsto explore the environment while simultaneously learning multiple language-conditionedtasks. To address this, we introduce a novel method: the compositionally-enabled rein-forcement learning language agent (CERLLA). Our method reduces the sample complexityof tasks specified with language by leveraging compositional policy representations and asemantic parser trained using reinforcement learning and in-context learning. We evaluateour approach in an environment requiring function approximation and demonstrate com-positional generalization to novel tasks. Our method significantly outperforms the previousbest non-compositional baseline in terms of sample complexity on 162 tasks designed totest compositional generalization. Our model attains a higher success rate and learns infewer steps than the non-compositional baseline. It reaches a success rate equal to an oraclepolicys upper-bound performance of 92%. With the same number of environment steps,the baseline only reaches a success rate of 80%.",
  "World Value Functions": ": Pipeline diagram of the learning process for the CERLLA agent. The agent takes in a BabyAIlanguage mission command and a set of 10 in-context examples that are selected using the BM25 searchretrieval algorithm (Robertson et al., 2009). The agent produces 10 candidate Boolean expressions. Theseexpressions specify the composition of the base compositional value functions. Each compositional valuefunction is instantiated in the environment and the policy it defines is evaluated over 100 rollouts. If thesuccess rate in reaching the goal is greater than 92%, the expression is considered a valid parse of the languageinstruction and is added to the set of in-context examples. While previous works have attempted to use natural language to specify tasks for RL agents (Ahn et al.,2023; Blukis et al., 2020), here we exploit the compositional nature of language along with compositionalpolicy representations to demonstrate improvements in sample complexity and generalization in solving noveltasks. Previous approaches to mapping language to behaviors use policies learned using imitation learning (Silvaet al., 2021; Ahn et al., 2023; Blukis et al., 2020). In this work, we instead focus on the setting where theagent does not have access to supervised demonstrations and instead learns to ground language to specifiedbehaviors with RL. The challenge in this approach is the significantly higher sample complexity of RL-based methods when grounding behaviors. Agents must map a variety of potential language instructions tounknown corresponding behaviors. Pretraining and transfer learning offers one possible solution. In naturallanguage processing, pretraining language models such as BERT (Devlin et al., 2019) and GPT-4 (OpenAI,2023) have enabled substantial reductions in sample complexity of solving novel NLP tasks. However, in RLthere is a lack of pretrained policy representations that can be fine-tuned using novel examples in few-shotsettings. In CERLLA RL policy learning, pretraining instead involves learning representations that can be com-posed to solve novel tasks (Todorov, 2009; Nangue Tasse et al., 2020). For instance, Nangue Tasse et al.(2020) demonstrate zero-shot task solving using compositional value functions and Boolean task algebra.The value functions are composed using Boolean operators to produce new complex behaviors. But thesemethods require manual specification of the Boolean expressions that describe value function composition,thus limiting their application to novel tasks. We overcome this and use RL to learn to compose the valuefunctions, given a task description in natural language. Our method uses these compositional value functions and pretrained language models to solve a large numberof tasks using RL while not relying on curricula, demonstrations, or other external aids to solve noveltasks. Leveraging compositionality is essential to solving large numbers of tasks with shared structure. Thesample complexity of learning a large number of tasks using RL is often prohibitive unless methods leveragecompositional structure (Mendez-Mendez & Eaton, 2023). This work builds on the Boolean compositional value function representations of Nangue Tasse et al. (2020)to construct a system for learning compositional policies for following language instructions. Our insight isthat language commands reflect the compositional structure of the environment, but without compositionalRL representations, this structure cannot be used effectively. Language, therefore, unlocks the utility of",
  "Published in Transactions on Machine Learning Research (12/2024)": "Jorge Mendez-Mendez and Eric Eaton. How to reuse and compose knowledge for a lifetime of tasks: Asurvey on continual learning and functional composition. Transactions on Machine Learning Research(TMLR), 2023. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, AlexGraves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control throughdeep reinforcement learning. nature, 518(7540):529533, 2015.",
  "Our primary contributions are as follows:": "1. We present a novel approach for solving RL tasks specified using language. The policies for thetasks are represented as conjunctions, disjunctions, and negations of pretrained compositional valuefunctions. 2. We combine in-context learning with feedback from environment rollouts to improve the semanticparsing capabilities of an LLM. To our knowledge, our method is the first to learn a semantic parserusing only in-context learning with feedback from environment rollouts.",
  "BabyAI Domain": "Because we build on the compositional value function representations of Nangue Tasse et al. (2020), ourmethod is applicable to any environment with goal-reaching tasks, the ability to learn value functions throughRL, and language instructions. To evaluate our method, we select the BabyAI MiniGrid domain (Chevalier-Boisvert et al., 2019), an easily extensible test-bed for compositional language-RL tasks used in many recentlanguage-RL works including (Carta et al., 2022; Li et al., 2022). It has language commands, image-stateobservations, a discrete action space, and objects with both color and type attributes. We augment BabyAIwith 162 compositional tasks specified using intersection, disjunction, and negation. The appendix providesa full list of task attributes available in the environment and the grammar of the Boolean expressions. provides examples of the types of tasks our method learns. The environment is initialized with one or moregoal objects and distractor objects that are randomly placed. For each episode in the BabyAI environment, the agent is provided with two forms of input as observations:a task instruction formulated in natural language and a 56 56 3 RGB image representing the state of theenvironment at each time-step. The objective for the agent is to correctly identify and pick up a specific goalobject, which is described by the task instruction. The goal objects are defined by their attributes: threeshapes and six colors. The combination of these attributes results in 18 unique goal objects that the agentcan pick up. In each episode, the environment contains at least one correct goal object and four additionaldistractor objects, which are sampled randomly. We consider the case of an agent required to solve a series of related tasks. Each task is formalized as aMarkov decision process (MDP) S, A, p, r, where S is the state space and A is the set of actions availableto the agent. The transition dynamics p(s|s, a) specify the probability of the agent entering state s afterexecuting action a in state s, while r(s, a, s) is the reward for executing a in s. We further assume that r isbounded by [rMIN, rMAX]. We focus here on goal-reaching tasks, where an agent is required to reach a set ofterminal goal states G S. Tasks are related in that they differ only in their reward functions. Specifically, we first define a backgroundMDP M0 = S0, A0, p0, r0. Then, any new task is characterized by a task-specific reward function r that",
  "is non-zero only for transitions entering g in G. Consequently, the reward function for the resulting MDP isgiven by r0 + r": "We implement this reward function with a penalty of r0 = 0.1 for each step taken. If the agent chooses thepickup action upon reaching an object, it observes the picked object, and the episode ends. If the selectedobject matches the correct goal as per the task instruction, the agent receives a reward of r = +2. The agent aims to learn an optimal policy , which specifies the probability of executing an action in a givenstate. The value function of policy is given by V (s) = E [t=0 r(st, at)] and represents the expectedreturn after executing from s. Given this, the optimal policy is that which obtains the greatest expectedreturn at each state: V (s) = V (s) = max V (s) for all s S. Closely related is the action-value function,Q(s, a), which represents the expected return obtained by executing a from s, and thereafter following .Similarly, the optimal action-value function is given by Q(s, a) = max Q(s, a) for all (s, a) S A.",
  "Logical Composition of Tasks using World Value Functions (WVFs)": "Recent work (Nangue Tasse et al., 2020; 2022) has demonstrated how logical operators such as conjunction(), disjunction () and negation () can be applied to value functions to solve semantically meaningfultasks compositionally with no further learning. To achieve this, the reward function is extended to penalisethe agent for attaining goals it did not intend to:",
  "rMINif g = s Gr(s, a)otherwise,(1)": "where rMIN is a large negative penalty. The agent receives the unmodified reward r(s, a) for all steps exceptwhere it reaches a different goal state than intended: g = s G.Given r, the related value function,termed world value function (WVF), can be written as:Q(s, g, a) = r(s, g, a) +",
  "S V (s, g)p(s|s, a)ds,where V (s, g) = E [t=0 r(st, g, at)]": "These value functions are intrinsically compositional since if a task can be written as a logical expressionover previous tasks, then the optimal value function can be similarly derived by composing the learnedWVFs.For example, consider the PickUpObject environment shown in .Assume the agenthas separately learned the task of collecting red objects (task R) and keys (task K). Using these valuefunctions, the agent can immediately solve the tasks defined by their union (R K), intersection (R K),and negation (R) as follows: QRK = QR QK := max{ QR, QK}, QRK = QR QK := min{ QR, QK},and QR = QR := QMAX + QMIN QR, where QMAX and QMIN are the world value functions for themaximum and minimum tasks respectively.1",
  "Methods": "We propose a two-step process for training an RL agent to solve Boolean compositional tasks with language.During an initial pretraining phase, a set of WVFs are learned which can later be composed to solve newtasks in the environment. This set forms a task basis that can express any task which can be written as aBoolean algebraic expression using the WVFs. In a second phase, an LLM learns to semantically parse language instructions into the Boolean compositionsof WVFs using RL and in-context learning. The parser learns this mapping from abstract symbols to WVFsusing RL by observing language instructions and interacting with the environment. Notably, our methoddoes not require the semantic parser to have access to any knowledge of the underlying basis tasks that theWVFs represent, and instead regards the WVFs as abstract symbols which can be composed to solve tasks.Since the semantic parser does not have access to any information about what task a WVF represents, ourmethod can be applied in principle to any basis of tasks. Tasks like pickup the red key can be represented by taking the intersection of the WVFs for picking upred objects and key objects: red key. Our method also supports negation and disjunctionwe canspecify tasks like pick up a red object that is not a ball: red ball. We augment this domain withadditional tasks. For further examples of tasks, see , which lists the complete set of tasks createdusing the attributes yellow and key. We implement the model from Chevalier-Boisvert et al. (2019) as anon-compositional baseline. This model does not have a pretraining phase for its RL representations, andin our experiments we account for this discrepancy in training steps by penalizing our agent by the numberof training steps needed to learn the WVFs.",
  "Pretraining World Value Functions": "During pretraining, a set of WVFs is learned which can later be composed to solve any task in the BabyAIenvironment. Each WVF takes as input a 56 56 3 RGB image observation of the environment andoutputs |G| |A| = 18 7 values for accomplishing one of the basis tasks (by maximising over the goal-action values). As there are nine object attributes (three object type attributes and six color attributesas listed in the appendix) we train nine WVFs. Each WVF is a value function for the policy of pickingup objects that match one of the nine attributes. However, our method does not require knowledge of theunderlying semantics of the value functions (i.e. the names of the underlying task attributes). We thereforeassign each WVF a random identifier, denoted as Symbol_0 through Symbol_8. While we refer to theWVFs by their color and object type in the paper, our model does not have access to this information andonly represents the WVFs by their unique identifiers. Each WVF is implemented using |G| = 18 CNN-DQN (Mnih et al., 2015) architectures. The WVF pre-training takes nineteen million steps.This is done by first training QMIN(s, g, a) for one million stepsand QMAX(s, g, a) for eighteen million steps (one million steps per goal in the environment).Each ba-sis WVF QB(s, g, a) is then generated from QMIN(s, g, a) and QMAX(s, g, a) by computing QB(s, g, a) =QMAX(s, g, a) if rB(g, a) = rMAX else QMIN(s, g, a). This yielded a 98% success rate for each basis WVF.For more details on WVF pretraining see Sec. 2 and Nangue Tasse et al. (2022), and see the appendix for afull list of hyperparameters used in WVF training.",
  "Compositionally Enabled RL Language Agent (CERLLA)": "We assume the downstream tasks are distinct from the basis tasks. During downstream task learning, thepretrained WVFs are composed to solve novel tasks specified in language. To solve BabyAI language-specifiedtasks, the agent must interpret the input language command and pick up an object of an allowed type. Toaccomplish this, the semantic parser maps from language to a Boolean expression specifying the compositionof WVFs. These Boolean expressions are then composed using a fixed pipeline that takes as input the setof WVFs and the Boolean expression. This pipeline parses the Boolean expression and returns a composedWVF. The agent then acts in the environment under the policy of the WVF by taking the action with thegreatest value at each step. If the Boolean expression is not syntactically correct, it cannot be instantiatedas a WVF and the episode terminates unsuccessfully.",
  "Language InstructionGround Truth Boolean Expression": "pick up a yellow boxyellow & boxpick up a box that is not yellow yellow & boxpick up a yellow object that is not a boxyellow & boxpick up an object that is not yellow and not a box yellow & boxpick up a box or a yellow objectyellow | boxpick up a box or an object that is not yellow yellow | boxpick up a yellow object or not a boxyellow | boxpick up an object that is not yellow or not a box yellow | boxpick up a boxboxpick up an object that is not a box boxpick up a yellow objectyellowpick up an object that is not yellow yellow",
  "RoleContent": "SystemWe are going to map sentences to Boolean expressions. The Boolean expression variableSymbols are numbered 0 to 8, e.g. Symbol_0, Symbol_1... The operators are and : &, or: |, not : ~. I will now give a new sentence and you will come up with an expression. Nowwait for a new sentence command. Respond with a list of 10 candidate Boolean expressions.Respond only with the list of Boolean expressions. Never say anything else.",
  "User (Command)pick up a red object that is not a ballAssistantSymbol_0 & Symbol_1 & Symbol_2Symbol_3 & Symbol_4Symbol_5 & Symbol_6 & Symbol_7[Additional candidate expressions]": "To implement the semantic parser, we utilize state-of-the-art large language models: GPT 4 (OpenAI, 2023)and GPT 3.5.2 Our method builds on the work of Shin et al. (2021) which builds a semantic parser usingLLMs and few-shot learning, and Toolformer (Schick et al., 2023) which learns an LLM semantic parserfrom weak supervision. Our method is distinct from these approaches in that it utilizes in-context examplescombined with an environment rollout RL signal. At the start of learning, the agent has no in-contextexamples of valid mappings from language to Boolean expressions (see ). At each episode, our agentis prompted with general instructions defining the semantic parsing task, the input language command, andup to 10 previously-acquired in-context examples selected using the BM25 retrieval algorithm (Robertsonet al., 2009). During training, the LLM is sampled with a temperature of 1.0 and produces a beam of 10 semantic parses(Boolean expressions) of the input language command. Together the temperature and beam width controlthe exploitation-exploration trade-off of the semantic parsing model. Each candidate Boolean expression isparsed using a fixed pipeline and instantiated as a WVF. The policy defined by the WVF is evaluated inthe environment over 100 episode rollouts. If the success rate across these episodes in reaching the specified",
  "Baselines": "The baseline is a joint language and vision model which learns a single action-value function for all tasksbased on the architecture used in the original BabyAI paper (Chevalier-Boisvert et al., 2019). We explore twobaseline models: an LM Baseline that utilizes pretrained language representations for embedding missioncommands from a frozen all-mpnet-base-v2 model from the SentenceTransformers library (Reimers &Gurevych, 2019) based on the MPNet model (Song et al., 2020) and an ablated Baseline which does notuse pretrained language representations. This pretrained sentence embedding model is trained on diversecorpora of sentence embedding tasks.",
  "Results": ": Results for learning all 162 tasks simultaneously. The mean episode success rate is plotted againstthe number of environment steps. Learning curves are presented for CERLLA and the non-compositionalbaseline agents. The Oracle agent is given the ground-truth Boolean expressions and upper bounds theattainable success rate in the environment, denoted by the dashed line at 92%. Our method is initialized at19 million steps to reflect the number of training steps used in pretraining the compositional value functions.Note the change in steps scale at 19 million steps. Means and 95% confidence intervals are reported over 10trials, 5 trials for the LM Baseline. We conduct experiments across four agent types and two settings. The first experiment evaluates samplecomplexity (). We learn all 162 tasks simultaneously and plot the mean success rate against thenumber of environment steps. The second experiment divides the task set in half, and measures the abilityof the agents to generalize to held-out novel tasks while learning from a fixed set of tasks (). We evaluate our method implemented using both GPT-4 and GPT-3.5. We compare our method to thebaseline agents, but penalize our method by the number of environment steps required to learn the pretrainedWVFs. As an upper limit on the performance of our method, we compare to an Oracle Agent which hasa perfect semantic parsing module. It has access to the ground-truth mappings from language to Booleanexpressions and its performance is limited only by the accuracy of the pretrained policies and randomnessin the environment.",
  "Simultaneous Learning of 162 Tasks": "In this experiment, at each episode a random task is sampled from the set of 162 language tasks. Thebaseline agents learn for 21 million steps, and CERLLA learns for 2 million steps.Because our agentpretrains the WVFs, we penalize our agent by starting it at 19 million steps (). Note that thisdisadvantages our method, as the WVF pretraining phase does not include language information and itsonly exposure to language-task data is over the following two million steps. Our method therefore has accessto less information about the tasks structure than the baseline agents during the first 19 million steps. ForCERLLA, due to the latency and cost of invoking the LLM, we only evaluate on one randomly selectedtask every 5, 000 environment steps, computing the average performance over 100 episodes. For the baselineagents we evaluate all 162 tasks every 50, 000 timesteps. This results in higher variance for our method inthe plots. We also plot the number of in-context training examples added to CERLLAs set in .This isequivalent to the number of training tasks successfully solved at that step. The Oracle Agent solves theoverwhelming majority of tasks during their first occurrence and is limited only by the small amount of noisein the policies and environment.",
  "Held-out Task Generalization": "This experiment () measures the generalization performance of each method on held-out tasks. Wecompare the performance of CERLLA to the baseline agents. In this setting, the set of tasks is randomlysplit into two halves at the start of training. At each episode, a random task from the first set is selected.During evaluation of our agent, one random task from each set is selected and the agent is evaluated over100 episodes. The baseline agents are evaluated over all 81 tasks in each set. : The mean number of tasks solved is plotted against the number of environment steps.Thisquantity is equal to the total number of in-context examples present in CERLLAs in-context example setat that step. Because the Oracle Agent has access to the ground truth Boolean expressions for each task,it solves tasks immediately. The population of tasks remains constant, so the number of unsolved tasksdecreases over time, leading to a logistic learning curve and an exponential decay in the rate at which newtasks are solved for the Oracle Agent. Means and 95% confidence intervals are reported over 10 trials.",
  "Discussion": "In both experiments CERLLA attains a significantly higher success rate in fewer total samples than thebaselines. shows our method attains a 92% success rate (matching the performance upper boundof the Oracle Agent) after only 600k environment steps, a small fraction of the steps of the baseline. Thebaseline agents are not able to generalize to all 162 tasks and only reach a success rate of 80% after 21million steps. Note that while the WVF pretraining for our method requires 19 million steps, the pretraining",
  "Related Work": "Our work is situated within the paradigm of RL, where novel tasks are specified using language and the agentis required to solve the task in the fewest possible steps. BabyAI (Chevalier-Boisvert et al., 2019) explores alarge number of language-RL tasks, however it learns far fewer tasks simultaneously and their tasks do notinvolve negation. Another compositional RL benchmark CompoSuite (Mendez et al., 2022) does not includelanguage, and has fewer tasks than our 162 task benchmark when accounting for the number of unique goalconditions that could be specified in language. Previous approaches have solved this problem using end-to-end architectures that are learned or improvedusing RL and a set of demonstrations(Anderson et al., 2018; Blukis et al., 2020; Chaplot et al., 2018).A problem with such approaches is a lack of compositionality in the learned representations. For example,learning to navigate to a red ball provides no information to the agent for the task of navigating to a blueball. Moreover, demonstrations are hard to collect especially when users cannot perform the desired behavior.Some approaches demonstrate compositionality by mapping to a symbolic representation and then planningover the symbols (Dzifcak et al., 2009; Williams et al., 2018; Gopalan et al., 2018). However, these works donot learn the semantics of these symbols or the policies to solve the tasks. Compositional representation learning has been demonstrated in the computer vision and language process-ing tasks using Neural Module Networks (NMN) (Andreas et al., 2016; Hu et al., 2018), but we explicitlydesire compositional representations both for the RL policies and the language command. Kuo et al. (2021)demonstrate compositional representations for policies, but they depend on a pre-trained parser and demon-strations to learn this representation. On the other hand, we use large language models (Raffel et al., 2020)and compositional policy representations to demonstrate compositionality in our representations and theability to solve novel unseen instruction combinations. Compositional policy representations have been developed using value function compositions, as first demon-strated by Todorov (2007) using the linearly solvable MDP framework.Moreover, zero-shot disjunc-tion (Van Niekerk et al., 2019) and approximate conjunction (Haarnoja et al., 2018; Van Niekerk et al.,2019; Hunt et al., 2019) have been shown using compositional value functions. Nangue Tasse et al. (2020)demonstrate zero-shot optimal composition for all three logical operatorsdisjunction, conjunction, andnegationin the stochastic shortest path problems. These composed value functions are interpretable be-cause we can inspect intermediate Boolean expressions that specify their composition. Our approach extendsideas from Nangue Tasse et al. (2020) to solve novel commands specified using language. Recent works like SayCan use language models and pretrained language-conditioned value functions to solvelanguage specified tasks using few-shot and zero-shot learning (Ahn et al., 2023). Shridhar et al. (2021)use pretrained image-text representations to perform robotic pick-and-place tasks. Other work incorporateslearning from demonstration and language with large-scale pretraining to solve robotics tasks (Driess et al.,2023; Brohan et al., 2022).However, these works use learning from demonstration as opposed to RL.Furthermore, these approaches do not support negations of pre-trained value functions that our methodallows. More importantly, their methodology is unsuitable for continual learning settings where both the RLvalue functions and language embeddings are improved over time as novel tasks are introduced. Shin et al. (2021) utilize LLMs to learn semantic parsers using few-shot learning with in-context examplesand Schick et al. (2023) uses an LLM to learn a semantic parser in a weakly supervised setting. Our methodis distinct as we use policy rollouts in an environment as the supervision with in-context learning.",
  "Conclusion": "We introduce a method that integrates pretraining of compositional value functions with large languagemodels to solve language tasks using RL. Our method rapidly solves a large space of RL tasks specified inlanguage more effectively than previous approaches. Demonstrating efficacy across 162 tasks with reducedsample requirements, our findings also further differentiate the capabilities of more powerful language modelsfrom weaker ones in the task of semantic parsing using an environment policy rollout signal. The combinationof compositional RL with language models provides a novel framework for reducing the sample complexityof learning RL tasks specified in language.",
  "Acknowledgements": "This material is based upon work supported by the DARPAs Perceptually-enabled Task Guidance (PTG)program under Contract No. HR001122C007. Any opinions, findings and conclusions or recommendationsexpressed in this material are those of the author(s) and do not necessarily reflect the views of the DARPA.This material is also based upon work supported by the Air Force Office of Scientific Research under awardnumber FA9550-24-1-0239. Any opinions, findings, and conclusions or recommendations expressed in thismaterial are those of the author(s) and do not necessarily reflect the views of the United States Air Force.This work was also supported by ONR grant #N00014-23-1-2887 and #N00014-19-2076. Michael Ahn, Anthony Brohan, Yevgen Chebotar, Chelsea Finn, Karol Hausman, Alexander Herzog, DanielHo, Julian Ibarz, Alex Irpan, Eric Jang, Ryan Julian, et al. Do as I can, not as I say: Grounding languagein robotic affordances. In Conference on Robot Learning, pp. 287318. PMLR, 2023. Peter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark Johnson, Niko Snderhauf, Ian Reid, StephenGould, and Anton Van Den Hengel. Vision-and-language navigation: Interpreting visually-grounded navi-gation instructions in real environments. In Proceedings of the IEEE Conference on Computer Vision andPattern Recognition, pp. 36743683, 2018.",
  "Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein. Neural module networks. In Proceedingsof the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3948, 2016": "Valts Blukis, Yannick Terme, Eyvind Niklasson, Ross A Knepper, and Yoav Artzi. Learning to map naturallanguage instructions to physical quadcopter control using simulated flight.In Conference on RobotLearning, pp. 14151438. PMLR, 2020. Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, KeerthanaGopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, et al. Rt-1: Robotics transformer for real-world control at scale. arXiv preprint arXiv:2212.06817, 2022.",
  "Nakul Gopalan, Dilip Arumugam, Lawson Wong, and Stefanie Tellex.Sequence-to-sequence languagegrounding of non-markovian task specifications. Robotics: Science and Systems XIV, 2018": "Tuomas Haarnoja, Vitchyr Pong, Aurick Zhou, Murtaza Dalal, Pieter Abbeel, and Sergey Levine. Com-posable deep reinforcement learning for robotic manipulation. In 2018 IEEE International Conference onRobotics and Automation, pp. 62446251. IEEE, 2018. Ronghang Hu, Jacob Andreas, Trevor Darrell, and Kate Saenko. Explainable neural computation via stackneural module networks. In Proceedings of the European Conference on Computer Vision, pp. 5369, 2018.",
  "OpenAI. Gpt-4 technical report, 2023": "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou,Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer.Journal of Machine Learning Research 21, 2020. Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-networks. InProceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. Associationfor Computational Linguistics, 11 2019. URL",
  "pick up an object that Symbol_4 & Symbol_0is not grey and not a ball": ": Subset of the in-context examples that the GPT-3.5 agent has accumulated at the end of 2 millionsteps of learning the 162 tasks.As shown, many of these expressions are not consistent or needlesslycomplicated. This helps to explain the relatively poorer performance of the GPT-3.5 agent which producesmuch noisier semantic parses and thus has much higher variance and lower performance than the GPT-4agent. Reducing the sampling temperature during learning leads to better expressions, but at the cost ofslower exploration and learning."
}