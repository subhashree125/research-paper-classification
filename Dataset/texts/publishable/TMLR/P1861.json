{
  "Abstract": "Symmetries of input and latent vectors have provided valuable insights for disentanglementlearning in VAEs. However, only a few works were proposed as an unsupervised method,and even these works require known factor information in the training data. We propose anovel method, Composite Factor-Aligned Symmetry Learning (CFASL), which is integratedinto VAEs for learning symmetry-based disentanglement in unsupervised learning withoutany knowledge of the dataset factor information. CFASL incorporates three novel featuresfor learning symmetry-based disentanglement: 1) Injecting inductive bias to align latentvector dimensions to factor-aligned symmetries within an explicit learnable symmetry code-book 2) Learning a composite symmetry to express unknown factors change between tworandom samples by learning factor-aligned symmetries within the codebook 3) Inducing agroup equivariant encoder and decoder in training VAEs with the two conditions. In ad-dition, we propose an extended evaluation metric for multi-factor changes in comparisonto disentanglement evaluation in VAEs. In quantitative and in-depth qualitative analysis,CFASL demonstrates a significant improvement of disentanglement in single-factor change,and multi-factor change conditions compared to state-of-the-art methods.",
  "Introduction": "Disentangling representations by intrinsic factors of datasets is a crucial issue in machine learning liter-ature (Bengio et al., 2013).In the Variational Autoencoder (VAE) frameworks, a prevalent method tohandle the issue is to factorize latent vector dimensions to encapsulate specific factor information (Kingma& Welling, 2013; Higgins et al., 2017; Chen et al., 2018; Kim & Mnih, 2018; Jeong & Song, 2019; Shao et al.,2020; 2022). Although their effective disentanglement learning methods, Locatello et al. (2019) raises theserious difficulty of disentanglement without sufficient inductive bias. In VAE literature, recent works using group theory offer a possible solution to inject such inductive biasby decomposing group symmetries (Higgins et al., 2018) in the latent vector space. To implement groupequivariant VAE, Winter et al. (2022); Nasiri & Bepler (2022) achieve the translation and rotation equivariantVAE. The other branch implements the group equivariant function (Yang et al., 2022; Keller & Welling,2021b) over the pre-defined group elements.All of the methods effectively improve disentanglement by",
  "Published in Transactions on Machine Learning Research (11/2024)": "T. Anderson Keller and Max Welling. Topographic VAEs learn equivariant capsules. In A. Beygelzimer,Y. Dauphin, P. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Sys-tems, 2021b. URL Hyunjik Kim and Andriy Mnih. Disentangling by factorising. In Jennifer Dy and Andreas Krause (eds.), Pro-ceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of MachineLearning Research, pp. 26492658. PMLR, 1015 Jul 2018. URL",
  "GroupCodebookGGroupGCodebookGL(n, R)General Lie groupGiith section of codebookgGroup element of Group Ggijjth element of Gi": "gl(n, R)Lie algebra of GL(n, R)|S|size of sectiongLie algebra Rnn|SS|size of subsection(, )group actiongijjth symmetry of ith section of codebook,equal to exp(gij)Setgccomposite symmetryXdatasetXsubset of datasetOthersFSet of factors {f1, . . . fk}RReal numberfiith index value of fstandard deviation (scalar)fivalues of f except fistandard deviation (vector)ZSet of latent vectorsmean (vector)zlatent vectorX|B|Random samples with |B| batch size, dot productFunction| |2L2 normGen()F Xorthogonalexpmatrix exponential||parallel",
  "Equivariant map:Let G be a group and both X1 and X2 are G-set with corresponding group action ofG in each sets, where g G. Then a function f : X1 X2 is equivariant if f(g X1) = g f(X1)": "Lie Group and Lie algebra:Lie group is defined as a group that simultaneously functions as a differen-tiable manifold, with the operations of group multiplication and inversion being smooth and differentiable.In this paper, we consider the matrix Lie group, which is a Lie group realized as a subgroup contained inGL(n, R), the group of n n invertible matrices over R. Lie algebra gl(n, R) is a tangent space of the Lie group at the identity element. The Lie algebra covers aLie group by the matrix exponential of elements of gl(n, R) as exp : gl(n, R) GL(n, R), where exp(X) =I + X + 1",
  "Disentanglement Learning": "Variational Auto-Encoder(VAE)We take VAE (Kingma & Welling, 2013) as a base framework. VAEoptimizes the tractable evidence lower bound (ELBO) instead of performing intractable maximum likelihoodestimation as:ELBO = Eq(z|x) log p(x|z) DKL(q(z|x)||p(z)),(1) where q(z|x) is the approximated posterior distribution, q(z|x) N(, I), the prior p(z) N(0, I),the first term of Equation 1 is a reconstruction error between input and outputs from the decoder p(x|z),and second term of Equation 1 is a Kullback-Leibler term to reduce the approximated posterior and priordistance. FactorVAE Metric (FVM)FVM metric (Kim & Mnih, 2018) is one of the disentanglement learningmetrics, which evaluates the consistency of dimension variation as a single factor is fixed. First, the fixedfactor f i is selected, and other factors f i are randomly selected, then generate the subset of the dataset X,which corresponds to factor f, where X = {x1, x2, . . . , xi}, xj = Gen(f ij, f ij ), and Gen() is a generator.The second, latent vector from the encoder of VAE is normalized by the standard deviation () of the dataset(zi / ), then selects the dth dimension as d = arg mind V ar(zdj ), V ar() is a variance of each dimension.The FMV metric is the accuracy of the majority vote classifier with data points (d, i).",
  "Symmetries for Inductive Bias": "Disentangled Representation Based on the GroupHiggins et al. (2018) shows the definition ofdisentangled representation through the group. First, the generator Gen : F X and the inference processh : X Z are defined. Then a group G of symmetries acts on F via a group action : G F F. Lastly,symmetries decompose as a direct product G = G1 . . . Gn. Then the disentangled representation isdefined with 1) group action : G Z Z, 2) composition b : F Z is an equivariant map, and 3) Zi isfixed by the action of all Gj, and affected only by Gi, where decomposition Z = Z1 . . . Zn. Implimentation of Equivariant ModelTo implement an equivariant map for learning symmetries,previous works utilize the Variational Auto-Encoder with an additional objective function defined as q(gx xx) = gz z q(x), where q : x z is an encoder, gx, gz is a symmetry on input space X and latent vectorspace Z, respectively. Yang et al. (2022); Winter et al. (2022), represent the gz in the latent vector space",
  "(g) Ideal": ": Distribution of latent vectors for dimensions responsible for Shape, X-pos, and Y-pos factors in the dSpritesdataset. The groupified-VAE method is applied to -TCVAE because this model shows a better evaluation score.The results show disentanglement for shape from the combination of the other two factors by coloring three shapes(square, ellipse, and heart) as red, blue, and green color, respectively. Each 3D plot shows the whole distribution.We fix Scale and Orientation factor values, and plot randomly sampled 640 inputs (20.8% of all possible observations(32 32 3 = 3, 072)). We select the dimensions responsible for the factors by selecting the largest value of theKullback-Leibler divergence between the prior and the posterior. Cont.-VAE is a Control-VAE.",
  "Related Work": "Disentanglement LearningDiverse works for unsupervised disentanglement learning have elaboratedin the machine learning field. The VAE based approaches have factorized latent vector dimensions withweighted hyper-parameters or controllable weighted values to penalize Kullback-Leibler divergence (KL di-vergence) (Higgins et al., 2017; Shao et al., 2020; 2022).Extended works penalize total correlation forfactorizing latent vector dimensions with divided KL divergence (Chen et al., 2018) and discriminator (Kim& Mnih, 2018). Differently, we induce disentanglement learning with group equivariant VAE for inductivebias. Group Theory-Based Approaches for Disentangled RepresentationIn recent periods, variousunsupervised disentanglement learning research proposes different approaches with another definition ofdisentanglement, which is based on the group theory (Higgins et al., 2018). To learn the equivariant function,Topographic VAE (Keller & Welling, 2021a) proposes the sequentially permuted activations on the latentvector space called shifting temporal coherence, and Groupified VAE (Yang et al., 2022) method proposesthat inputs pass the encoder and decoder two times to implement permutation group equivariant VAEmodels. Also, Commutative Lie Group VAE (CLG-VAE) (Zhu et al., 2021; Mercatali et al., 2022) mapslatent vectors into Lie algebra with one-parameter subgroup decomposition for inductive bias to learn thegroup structure from abstract canonical point to inputs. Differently, we propose the trainable symmetriesthat are extracted between two samples directly on the latent space while maintaining the equivariancefunction between input and latent vector space. Symmetry Learning with Equivariant ModelLie group equivariant CNN (Dehmamy et al., 2021;Finzi et al., 2020) construct the Lie algebra convolutional network to discover the symmetries automatically.In the other literature, several works extract symmetries, which consist of matrices, between two inputs orobjects. Miyato et al. (2022) extracts the symmetries between sequential or sequentially augmented inputsby penalizing the transformation of difference of the same time interval. Other work extracts the symmetriesby comparing two inputs, in which the differentiated factor is a rotation or translation, and implementssymmetries with block diagonal matrices (Bouchacourt et al., 2021). Furthermore, Marchetti et al. (2023)decomposes the class and pose factor simultaneously by invariant and equivariant loss function with weaklysupervised learning. The unsupervised learning work (Winter et al., 2022) achieves class invariant and groupequivariant function in less constraint conditions. Differently, we generally extend the a class invariant andgroup equivariant model in the more complex disparity condition without any knowledge of the factors ofdatasets.",
  "Limits of Disentanglement Learning of VAE": "By the definition of disentangled representation (Bengio et al., 2013; Higgins et al., 2018), the disentangledrepresentations are distributed on the flattened surface as shown in g because each change of the factoronly affects a single dimension of latent vector. However, the previous methods (Higgins et al., 2017; Chenet al., 2018; Shao et al., 2020; Zhu et al., 2021; Yang et al., 2022) show the entangled representations ontheir latent vector space as shown in a-1c. Even though the group theory-based methods improvethe disentanglement performance (Zhu et al., 2021; Yang et al., 2022), these still struggle with the sameproblem as shown in d and 1e. In addition, symmetries are represented on the latent vector spacefor disentangled representations. In current works (Miyato et al., 2022; Keller & Welling, 2021b; Quessardet al., 2020), the sequential observation is considered in unsupervised learning. However, these works needthe knowledge of sequential changes of images to set up inputs manually, as summarized in .",
  "Methods": "Our work is mainly focused on how to 1) define the inputs and symmetry, 2) optimize the symmetry, 3)represent the composite symmetry, and 4) inject the symmetry as an inductive bias. We first define the 1)inputs as a pair of two samples, 2) group, group action, and G-set, and 3) codebook in section 5.1. In thenext, we optimize the codebook for disentangled representation in section 5.2, then extract the compositesymmetry to represent transformation between two inputs in section 5.3. Lastly, we introduce the objectiveloss to inject an inductive bias for disentangled representations in section 5.4.",
  "(c) Two-step attention": ": The overall architecture of the proposed method. refers to a loss function. 1) A pair of images (e.g.,differences between two images are in the x- and y-position) is given, and the goal of the model is to represent thedifferences on the latent vector space, called the composite symmetry gc for disentangled representations. 2) Thecodebook is designed to represent the composite symmetry gc. 3) Each section of the codebook is separated to affecta single factor e.g., the ith section affects the x-position, and the jth section affects the y-position of images. 4) Eachsection consists of Lie algebra to provide diversity of symmetries. 5) As shown in (b), each loss optimizes the codebookto guarantee the 3) as follows: i) symmetries from the same section affect the same factor through the parallel lossLpl (e.g., symmetries from ith section exp(gik) only affects the x-position), ii) each section affects different factorsby the perpendicular loss Lpd (e.g., symmetry from ith and jth section gic and gjc affect x-position and y-positionrespectively), and iii) each section changes a single dimension of latent vectors for disentangled representation by thesparsity loss Ls. 6) attn ensures diversity in symmetries representation, and ps predicts the activated section, in thiscase, ith and jth sections for x- and y-position differences. 7) The model then represents the composite symmetrygc. 8) Lastly, model optimizes the Lee to match gcz1(= z2) and z2, and Lde to match the x1 and p(gc q(x2)) toinject the inductive bias. Codebook:Explicit and Learnable Symmetry Representation for GL(n)To allow the directinjection of inductive bias into symmetries, we implement an explicit and trainable codebook for symmetryrepresentation. we consider the symmetry group on the latent vector space as a subgroup of the general liegroup GL(n) under a matrix multiplication. The codebook G = {G1, G2, . . . , Gk} is composed of sectionsGi, which affect to a different single factor, where k {1, 2, . . . |S|}, and |S| is the number of sections. Thesection Gi is composed of Lie algebra {gi1, gi2, . . . , gil}, where gij R|D||D|, l {1, 2, . . . , |SS|}, |SS| is thenumber of elements in each section, and |D| is a dimension size of latent z. We assume that each Lie algebraconsists of linearly independent bases B = {Bi|Bi Rnn,",
  "i iBi = 0, i = 0}: gij =": "b i,jb Bb, whereb {1, 2, . . . , kl}. Then, the dimension of the element of the codebook is equal to |B|, and the dimension ofthe Lie group composed of the codebook element is also |B|. To utilize previously studied effective expressionof symmetry for disentanglement, we set the symmetry to be continuous (Higgins et al., 2022) and invertiblevia matrix exponential form (Xiao & Liu, 2020) as gij = egij = k=01k!(gij)k to construct the Lie group (Hall,2015).",
  "(d) (c) + sparsity loss": ": Roles of parallel, perpendicular, and sparsity loss on symmetries in the codebook for adjusting representationchange. Parallel loss is for symmetries of the same section, and perpendicular loss is for different sections. Each axis(x and y) only affects a single factor. 2) perpendicular loss Lpd that match each symmetry to a single factor changes. Each loss optimizes thesymmetries to affect the same and different factors, respectively. Then we add 3) sparsity loss Ls to thelosses for disentangled representations as shown in . It aligns a single factor change to an axis of latentvector space. Also, we implement the 4) commutative loss Lc to reduce the computational costs for matrixexponential multiplication. Inductive Bias: Group Elements of the Same Section Impacts on the Same Factor ChangesAs we design the symmetries from the same section of the codebook to affect the same factor shown in (example), we add a bias that the latent vector is changed by symmetries of the same section to be parallel(z gijz z gilz for ith section) as shown in a. We define a loss function to make them parallel as:",
  "where gij = egij, , is a dot product, and | |2 is a L2 norm": "Inductive Bias:Group Elements of the Different Section Impacts on the Different FactorChangesAs shown in example of perpendicular loss to enforce each section affects different factor,we inject another bias that changes the latent vector through symmetries of the each sections to be orthogonalfor different factors change impacts ( zgijz zgkl z for different ith and kth sections) as shown in c.The loss for inducing the orthogonality is",
  "|z gijz|2 |z gkl z|2.(3)": "Due to the expensive computational cost for Eq. 3 (O(|S|2 |SS|2)), we randomly select a (j, l) pair ofsymmetries of each section. This random selection still holds the orthogonality, because if all elements in thesame section satisfy Equation 2 and a pair of elements from a different section (Gi, Gj) satisfies Equation 3,then any pair of the element (gi Gi, gj Gj) satisfies the Equation 3. More details are in the Appendix B. Inductive Bias:Align Each Factor Changes to the Axis of Latent Space for DisentangledRepresentationsFactorizing latent dimensions to represent the change of independent factors is an at-tribute of disentanglement defined in Bengio et al. (2013) and derived by ELBO term in VAE trainingframeworks (Chen et al., 2018; Kim & Mnih, 2018). However, the parallel and the perpendicular loss do notfactorize latent dimension as shown in a, 3c. To guide symmetries to hold the attribute, we enforcethe change ijz = z gijz to be a parallel shift to a unit vector as b, 3d via sparsity loss defined as",
  "Composition of Factor-Aligned Symmetries via Two-Step Attention for Unsupervised Learning": "In this section, we introduce how the model extracts the symmetries between two inputs. We propose the atwo-steps process as follows: 1) the model extracts the Lie algebra of each section gic to represent the factor-aligned symmetries between two inputs in the first step, then 2) predicts which section of the codebook isactivated in the second step process as shown in example.",
  "where attnij = softmax([M; ]W ic + bic) [M; ] = [1; 1; 2; 2], W ic R4|D||SS| and bic R|SS| arelearnable parameters, i {1, 2, . . . |S|}, and softmax() is a softmax function": "Second Step: Section SelectionIn the second step of our proposed model, we enforce the prediction offactors that have undergone changes. We assume that if some factor of two inputs is equal, then the varianceof the corresponding latent vector dimension value is smaller compared to others. Based on this assumption,we define the target (T) for factor prediction: if z1,i z2,i > threshold, then we set Ti as 1 and 0 otherwise,where Ti is a ith dimension value of T R|D|, zj,i is an ith dimension value of zj, and we set the thresholdas a hyper-parameter. For section prediction, we utilize the cross-entropy loss is defined as follows:",
  "Equivariance Induction of Composite Symmetries": "Motivated by the implementations of equivariant mapping in prior studies (Yang et al., 2022; Miyato et al.,2022) for disentanglement learning, we implement an equivariant encoder and decoder that satisfies q(c xi) = gc q(xi) and p(gc zi) = c p(zi) respectively, where q is an encoder, p is the decoder,c xi = xj, and gc zi = zj. As shown in example, we propose 1) the encoder equivariant loss Leeto match the zi and gc zj on the latent vector space, and 2) the decoder equivalent loss Lde to match theinput xi and generated image from the transformed vector p(x|gc zj). Equivariance Loss for Encoder and DecoderIn the notation, c and gi are group elements of thegroup (, ) and (G, ) respectively, and both groups are isomorphic. Each group acts on the input andlatent vector space with group action , and , respectively. We specify the form of symmetry gc and as aninvertible matrix, and group action as matrix multiplication on the latent vector space. Then, we optimizethe encoder and decoder equivariant function as:",
  "Extended Evaluation Metric: m-FVM Metric for Disentanglement in Multi-Factor Change": "We define the multi-factor change condition as simultaneously altering more than two factors in the transfor-mation between two samples or representations. To the best of our knowledge, there is no evaluation metricfor disentanglement in multi-factor change, so we propose the extended version of the Factor-VAE metric(FVM) score called as multi-FVM score (m-FVMk), where factor F = F1 . . . Fn, k {2, 3, . . . , n 1},and |Fi| is a number of factors. Similar to FVM, 1) we randomly choose the factor f = (f (i,...,j), f (i,...,j)).2) Then we fix the corresponding factor dimension value in the mini-batch. 3) Subsequently, we estimate thestandard deviation (std.) of each dimension to find the number of k lowest std. dimension (zl1, zl2, . . .) in oneepoch. 4) We then count each pair of selected dimensions by std. values (the number of (zl1, zl2, . . .), whichare corresponded to fixed factors). 5) In the last, we add the maximum value of the number of (zl1, zl2, . . .)on all fixed factor cases, and divide with epoch.",
  "Experiments": "DeviceWe set the below settings for all experiments in a single Galaxy 2080Ti GPU for 3D Cars andsmallNORB, and a single Galaxy 3090 for dSprites 3D Shapes and CelebA. More details are in README.mdfile. DatasetsThe dSprites dataset Matthey et al. (2017) consists of 737,280 binary 64 64 images with fiveindependent ground truth factors (number of values), i.e. x-position (32), y-position (32), orientation (40),",
  "CFASL1.001.431.131.131.17": "shape (3), and scale (6). Any composite transformation of x- and y-position, orientation (2D rotation), scale,and shape is commutative. The 3D Cars Reed et al. (2015) dataset consists of 17,568 RGB 64643 imageswith three independent ground truth factors: elevations(4), azimuth directions(24), and car models(183).Any composite transformation of elevations(x-axis 3D rotation), azimuth directions (y-axis 3D rotation),and models are commutative. The smallNORB LeCun et al. (2004) dataset consists of total 96 96 24,300grayscale images with four factors, which are category(10), elevation(9), azimuth(18), light(6) and we resizethe input as 64 64. Any composite transformation of elevations(x-axis 3D rotation), azimuth (y-axis 3Drotation), light, and category is commutative. 4) The 3D Shapes dataset Burgess & Kim (2018) consists of480,000 RGB 64 64 3 images with six independent ground truth factors: orientation(15), shape(4), floorcolor(10), scale(8), object color(10), and wall color(10). 5) The CelebA dataset Liu et al. (2015) consists of202,599 images, and we crop the center 128 128 area and then, resize to 64 64 images. Evaluation SettingsWe set prune_dims.threshold as 0.06, 100 samples to evaluate global empiricalvariance in each dimension, and run it a total of 800 times to estimate the FVM score introduced in Kim& Mnih (2018). For the other metrics, we follow default values introduced in Michlo (2021), training andevaluation 10,000 and 5,000 times with 64 mini-batches, respectively Cao et al. (2022). Model Hyper-parameter TuningWe implement -VAE Higgins et al. (2017), -TCVAE Chen et al.(2018), control-VAE Shao et al. (2020), Commutative Lie Group VAE (CLG-VAE) Zhu et al. (2021),and Groupified-VAE (G-VAE) Yang et al. (2022) for baseline. For common settings to baselines, we set",
  "batch size 64, learning rate 1e-4, and random seed from {1, 2, . . . , 10} without weight decay.We trainfor 3 105 iterations on dSprites smallNORB and 3D Cars, 6 105 iterations on 3D Shapes, and 106": "iterations on CelebA. We set hyper-parameter {1.0, 2.0, 4.0, 6.0} for -VAE and -TCVAE, fix the, for -TCVAE as 1 Chen et al. (2018).We follow the ControlVAE settings Shao et al. (2020), thedesired value C {10.0, 12.0, 14.0, 16.0}, and fix the Kp = 0.01, Ki = 0.001. For CLG-VAE, we also fol-low the settings Zhu et al. (2021) as hessian = 40.0, decomp = 20.0, p = 0.2, and balancing parameter oflossrec group {0.1, 0.2, 0.5, 0.7}. For G-VAE, we follow the official settings Yang et al. (2022) with -TCVAE( {10, 20, 30}), because applying this method to the -TCVAE model usually shows higher performancethan other models Yang et al. (2022). Then we select the best case of models. We run the proposed modelon the -VAE and -TCVAE because these methods have no inductive bias to symmetries. We set the samehyper-parameters of baselines with {0.1, 0.01}, threshold {0.2, 0.5}, |S| = |SS| = |D|, where |D| is alatent vector dimension. More details for experimental settings.",
  "Quantitative Analysis Results and Discussion": "Disentanglement Performance in Single and Multi-Factor ChangeWe evaluate four commondisentanglement metrics: FVM (Kim & Mnih, 2018), MIG (Chen et al., 2018), SAP (Kumar et al., 2018), andDCI (Eastwood & Williams, 2018). As shown in , our method gradually improves the disentanglementlearning in dSprites, 3D Cars, 3D Shapes, and smallNORB datasets in most metrics. To show the quantitativescore of the disentanglement in multi-factor change, we evaluate the m-FVMk, where max(k) is 2, 3, and4 in 3D Cars, smallNORB, and dSprites datasets respectively. These results also show that our methodpositively affects single and multi-factor change conditions. As shown in , the proposed methodshows a statistically significant improvement, as indicated by the higher average rank of dataset metricscompared to other approaches.",
  "-VAE": "88.19(4.60)6.82(2.93)0.63(0.33)20.45(3.93)42.36(7.16)88.57(6.68)7.18(2.52)1.85(1.04)18.39(4.80)48.23(5.51)88.56(7.78)7.27(4.16)1.31(0.70)19.58(4.45)42.63(4.21)86.95(5.96)7.11(3.49)1.09(0.40)18.35(3.32)41.90(7.80)85.42(7.89)7.30(3.73)1.15(0.70)21.69(4.70)41.90(6.07)89.34(5.18)9.44(2.91)1.26(0.40)23.14(5.51)51.37(9.29)90.71(5.75)9.29(3.74)1.07(0.65)22.74(5.06)45.84(7.71)91.91(3.45)9.51(2.74)1.42(0.52)20.72(3.65)55.47(10.09) Comparison of Plug-in MethodsTo compare our method with a wider range of approaches, we evaluatethe disentanglement performance of the plug-in style method, G-VAE Yang et al. (2022) and apply bothmethods to -TCVAE. As shown in , our method shows statistically significant improvements indisentanglement learning although hyper-parameter of CFASL is smaller than G-VAE. Ablation Study shows the ablation study to evaluate the impact of each component of ourmethod for disentanglement learning. In the case of w/o Lpl, the extraction of the composite symmetry gcbecomes challenging due to the lack of unified roles among individual sections. Also, the coverage of codew/o Lpd is limited due to the absence of assurance that each section aligns with distinct factors. In the caseof w/o Ls, each section assigns a different role and the elements of each section align on the same factor,and w/o Ls case is better than w/o Lpl and w/o Lpd. These results imply that the no-differentiated role ofeach section struggles with constructing adequate composite symmetry gc. Also, it shows that dividing thesymmetry information in each section (Lpl, Lpd) is more important than Ls for disentangled representation.To compare factor-aligned losses (w/o Lpl, w/o Lpd, w/o Ls, and w/o Lpl + Lpd + Ls), the best of amongfour cases is the w/o Lpl + Lpd + Ls and it implies that these losses are interrelated. Also, constructing thesymmetries without the equivariant model is meaningless because the model does not satisfy Equation 10-12. The w/o Lequiv naturally shows the lowest results compared to other cases except w/o Lpd and Lpl.Moreover, the w/o Lp case shows the impact of the section selection method (section 5.3) for unsupervisedlearning. Above all, each group exhibits a positive influence on disentanglement when compared to the basemodel (-VAE). When combining all loss functions, our method consistently outperforms the others acrossthe majority of evaluation metrics.",
  ": Loss curves: 1) HT: hyper-parameter tuning ( {0.01, 0.1, 1.0}) with -TCVAE based CFASL. 2) AB:ablation study with -VAE based CFASL": "Impact of Hyper-Parameter TuningWe operate a grid search of the hyper-parameter . As shown ina, the Kullback-Leibler divergence convergences to the highest value, when is large ( = 1.0) andit shows less stable results. It implies that the CFASL with larger struggles with disentanglement learning,as shown in a. Also, the Lee in b is larger than other cases, which implies that the modelstruggles with extracting adequate composite symmetry because its encoder is far from the equivariant modeland it is also shown in a. Even though = 0.01 case shows the lowest value in the most loss, Lde inc is higher than others and it also implies the model struggles with learning symmetries, as shownin a because the model does not close to the equivariant model compare to = 0.1 case.",
  ":Heatmaps of Eigenvec-tors for latent vector representa-tions": "Posterior of CFASLThe symmetry codebook and composite symme-try are linear transformations of latent vectors. Intuitively, they enforcethe posterior to be far from prior as q(z|x) N(gc, gcgc ), where and are close to zero vectors and identity matrix, respectively. However,as shown in d, Kullbeck Leibler divergence is lower than VAE. Itrepresents the ability of CFASL to preserve Gaussian normal distribution,which is similar to VAE. Impact of Factor-Aligned Symmetry SizeWe set the codebook sizeas 100, and 10 to validate the robustness of our method. In b, thelarger size shows better results than the smaller one and is more stableby showing a low standard deviation. : Generated images by composite symmetry and its factor-aligned symmetries. Image 1 and 2 are inputs,and image 3 is an output from image 1 (p(z1)). Image 5 is a output of group element gc acted on z1 (p(gcz1)).Images 4 are outputs of decomposed composite symmetry gc acted on z1 sequentially.",
  "Qualitative Analysis Results and Discussion": "Is Latent Vector Space Close to Disentangled Space?The previous result as shown in isa clear example of of whether the latent vector space closely approximates a disentangled space. The latentvector space of previous works (a-1e) are far from disentangled space (g) but CFASL showsthe closest disentangled space compare to other methods. Alignment of Latent Dimensions to Factor-Aligned SymmetryIn the principal component analysisof latent vectors shown in , the eigenvectors V = [v1, v2, . . . , v|D|] are close to one-hot vectorscompared to the baseline, and the dominating dimension of the one-hot vectors are all different. This resultimplies that the representation (factor) changes are aligned to latent dimensions. Factor Aligned SymmetriesTo verify the representation of learnable codebook over composite symme-tries and factor-aligned symmetries, we randomly select a sample pair as shown in . The resultsimply that gc generated from the codebook represents the composite symmetries between two images ( 1and 2) because the image 2 and the generated image 5 by symmetry gc are similar (p(z2) p(gcz1)).Also, each factor-aligned symmetry (gi) generated from codebook section affects a single factor changes asshown in images 4. in . Factor Aligned Latent DimensionTo analyze each factor changes aligned to each dimension of latentvector space, we set the qualitative analysis as shown in . We select the baseline and our model,which are the highest disentanglement performance scores. We select two random samples (x1, x2), generatelatent vectors z1 and z2, and select the largest Kullback-Leibler divergence (KLD) value dimension fromtheir posterior. Then, replacing the dimension value of z1 to the value of z2 one by one sequentially. As aresult, the shape and color factors are changed when a single dimension value is replaced within the baseline",
  "Conclusion": "This work tackles the difficulty of disentanglement learning of VAEs in unknown factors change conditions.We propose a novel framework to learn composite symmetries from explicit factor-aligned symmetries bycodebook to directly represent the multi-factor change of a pair of samples in unsupervised learning. Theframework enhances disentanglement by learning an explicit symmetry codebook, injecting three inductivebiases on the symmetries aligned to unknown factors, and inducing a group equivariant VAE model. Wequantitatively evaluate disentanglement in the condition by a novel metric (m-FVMk) extended from acommon metric for a single factor change condition.This method significantly improved in the multi-factor change and gradually improved in the single factor change condition compared to state-of-the-artdisentanglement methods of VAEs. Also, training process does not need the knowledge of factor informationof datasets. This work can be easily plugged into VAEs and extends disentanglement to more general factorconditions of complex datasets.",
  "Limitation and Future Work": "In the real-world dataset, the variance of the factor is much more complex and has more combinations thanthe used datasets in this paper (the maximum number of factors is five). Although our method shows theadvance of disentanglement learning in multi-factor change conditions, it remains the generalization in realworld datasets or larger dataset. We consider extending the learning of composite symmetries in generalconditions. Another drawback is to use of six loss functions, which require more hyper-parameter tuning. Asthe statistically learned group methods reduce hyper-parameters (Winter et al., 2022), we consider a morecomputationally efficient loss function. This work was supported by the National Research Foundation of Korea (NRF) grant funded by the Koreagovernment (MSIT) (No.2022R1A2C2012054, Development of AI for Canonicalized Expression of TrainedHypotheses by Resolving Ambiguity in Various Relation Levels of Representation Learning).",
  "Chris Burgess and Hyunjik Kim. 3d shapes dataset. 2018": "Jinkun Cao, Ruiqian Nai, Qing Yang, Jialei Huang, and Yang Gao. An empirical study on disentanglementof negative-free contrastive learning. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and KyunghyunCho (eds.), Advances in Neural Information Processing Systems, 2022. URL Ricky T. Q. Chen, Xuechen Li, Roger B Grosse, and David K Duvenaud.Isolating sources of dis-entanglement in variational autoencoders.In S. Bengio, H. Wallach, H. Larochelle, K. Grauman,N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing Systems, vol-ume 31. Curran Associates, Inc., 2018.URL Nima Dehmamy, Robin Walters, Yanchen Liu, Dashun Wang, and Rose Yu. Automatic symmetry discoverywith lie algebra convolutional network. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan(eds.), Advances in Neural Information Processing Systems, 2021. URL",
  "B. Hall.Lie Groups, Lie Algebras, and Representations: An Elementary Introduction.Graduate Textsin Mathematics. Springer International Publishing, 2015. ISBN 9783319134673. URL": "Irina Higgins, Loc Matthey, Arka Pal, Christopher P. Burgess, Xavier Glorot, Matthew M. Botvinick,Shakir Mohamed, and Alexander Lerchner. beta-vae: Learning basic visual concepts with a constrainedvariational framework. In ICLR, 2017. Irina Higgins, David Amos, David Pfau, Sbastien Racanire, Loc Matthey, Danilo J. Rezende, and Alexan-der Lerchner. Towards a definition of disentangled representations. CoRR, abs/1812.02230, 2018. URL",
  "Diederik P Kingma and Max Welling. Auto-encoding variational bayes, 2013. URL": "Abhishek Kumar, Prasanna Sattigeri, and Avinash Balakrishnan. VARIATIONAL INFERENCE OF DISEN-TANGLED LATENT CONCEPTS FROM UNLABELED OBSERVATIONS. In International Conferenceon Learning Representations, 2018. URL Yann LeCun, Fu Jie Huang, and Lon Bottou. Learning methods for generic object recognition with invari-ance to pose and lighting. In Proceedings of the 2004 IEEE Computer Society Conference on ComputerVision and Pattern Recognition, CVPR04, pp. 97104, USA, 2004. IEEE Computer Society.",
  "Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. 2015IEEE International Conference on Computer Vision (ICCV), pp. 37303738, 2015": "Francesco Locatello, Stefan Bauer, Mario Lucic, Gunnar Raetsch, Sylvain Gelly, Bernhard Schlkopf, andOlivier Bachem. Challenging common assumptions in the unsupervised learning of disentangled represen-tations. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th InternationalConference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pp. 41144124.PMLR, 0915 Jun 2019. URL Giovanni Luca Marchetti, Gustaf Tegnr, Anastasiia Varava, and Danica Kragic. Equivariant representationlearning via class-pose decomposition. In Francisco Ruiz, Jennifer Dy, and Jan-Willem van de Meent(eds.), Proceedings of The 26th International Conference on Artificial Intelligence and Statistics, volume206 of Proceedings of Machine Learning Research, pp. 47454756. PMLR, 2527 Apr 2023. URL",
  "Nathan Juraj Michlo. Disent - a modular disentangled representation learning framework for pytorch. Github,2021. URL": "Takeru Miyato, Masanori Koyama, and Kenji Fukumizu. Unsupervised learning of equivariant structure fromsequences. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances inNeural Information Processing Systems, 2022. URL Alireza Nasiri and Tristan Bepler. Unsupervised object representation learning using translation and rotationgroup equivariant VAE. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.),Advances in Neural Information Processing Systems, 2022. URL",
  "H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 1972719737. Cur-ran Associates, Inc., 2020. URL": "Scott E Reed, Yi Zhang, Yuting Zhang, and Honglak Lee.Deep visual analogy-making.In C. Cortes,N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett (eds.), Advances in Neural Information ProcessingSystems, volume 28. Curran Associates, Inc., 2015.URL Huajie Shao, Shuochao Yao, Dachun Sun, Aston Zhang, Shengzhong Liu, Dongxin Liu, Jun Wang, andTarek Abdelzaher. ControlVAE: Controllable variational autoencoder. In Hal Daum III and Aarti Singh(eds.), Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedingsof Machine Learning Research, pp. 86558664. PMLR, 1318 Jul 2020. URL Huajie Shao, Yifei Yang, Haohong Lin, Longzhong Lin, Yizhuo Chen, Qinmin Yang, and Han Zhao. Re-thinking controllable variational autoencoders. In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition (CVPR), pp. 1925019259, June 2022. Robin Winter, Marco Bertolini, Tuan Le, Frank Noe, and Djork-Arn Clevert. Unsupervised learning ofgroup invariant and equivariant representations.In Alice H. Oh, Alekh Agarwal, Danielle Belgrave,and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022.URL Changyi Xiao and Ligang Liu. Generative flows with matrix exponential. In Hal Daum III and Aarti Singh(eds.), Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedingsof Machine Learning Research, pp. 1045210461. PMLR, 1318 Jul 2020. URL Tao Yang, Xuanchi Ren, Yuwang Wang, Wenjun Zeng, and Nanning Zheng. Towards building a group-based unsupervised representation disentanglement framework. In International Conference on LearningRepresentations, 2022. URL",
  "C.2Ablation Studies": "How Commutative Lie Group Improves Disetanglement Learning?The Lie group is not com-mutative, however most factors of the used datasets are commutative. For example, 3D Shapes datasetfactors consist of the azimuth (x-axis), yaw (z-axis), coloring, scale, and shape. Their 3D rotations are allcommutative. Also, other composite symmetries as coloring and scale are commutative. Even though werestrict the Lie group to be commutative, our model shows better results than baselines as shown in .",
  ": Complexity": "Impact of Commutative Loss on Computational ComplexityAsshown in , our methods reduce the composite symmetries computation.Matrix exponential is based on the Taylor series and it needs high computationcost though its approximation is lighter than the Taylor series. We need one matrix exponential computationfor composite symmetries with commutative loss, in contrast, the other case needs the number of symmetrycodebook elements |S| |SS| for the matrix exponential and also |S| |SS| 1 time matrix multiplication.",
  "4th row: ours disentangles the all factors when the 1st and 2nd dimension values are changed": "3D ShapesAs shown in c, the CFASL shows better results than the baseline. In the 1st, 3rd, and5th rows, our model clearly disentangles the factors while the baseline struggles with disentangling multi-factors. Even though our model does not clearly disentangle the factors, compared to the baseline, which istoo poor for disentanglement learning, ours improves the performance."
}