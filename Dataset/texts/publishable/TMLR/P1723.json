{
  "Abstract": "Understanding the interactions of atoms such as forces in 3D atomistic systems is fundamentalto many applications like molecular dynamics and catalyst design. However, simulatingthese interactions requires compute-intensive ab initio calculations and thus results inlimited data for training neural networks.In this paper, we propose to use denoisingnon-equilibrium structures (DeNS) as an auxiliary task to better leverage training data andimprove performance. For training with DeNS, we first corrupt a 3D structure by addingnoise to its 3D coordinates and then predict the noise. Different from previous works ondenoising, which are limited to equilibrium structures, the proposed method generalizesdenoising to a much larger set of non-equilibrium structures. The main difference is thata non-equilibrium structure does not correspond to local energy minima and has non-zeroforces, and therefore it can have many possible atomic positions compared to an equilibriumstructure. This makes denoising non-equilibrium structures an ill-posed problem since thetarget of denoising is not uniquely defined. Our key insight is to additionally encode theforces of the original non-equilibrium structure to specify which non-equilibrium structurewe are denoising. Concretely, given a corrupted non-equilibrium structure and the forcesof the original one, we predict the non-equilibrium structure satisfying the input forcesinstead of any arbitrary structures. Since DeNS requires encoding forces, DeNS favorsequivariant networks, which can easily incorporate forces and other higher-order tensors innode embeddings. We study the effectiveness of training equivariant networks with DeNS onOC20, OC22 and MD17 datasets and demonstrate that DeNS can achieve new state-of-the-artresults on OC20 and OC22 and significantly improve training efficiency on MD17.",
  "Published in Transactions on Machine Learning Research (12/2024)": ":Visualization of corrupted structures in MD17 dataset.We add noise of different scalesto original structures (column 1).For each row,we sample iNp0, I3q,multiply iwith 0.01 (column 2), 0.03 (column 3), 0.05 (column 4) and 0.07 (column 5), and add the scaled noise to the original struc-tures. For columns 2, 3, 4 and 5, the lighter colors denote the atomic positions of the original structures. Here we addnoise to all the atoms in a structure for better visual effects.",
  "Zaidi et al., 2023; Liu et al., 2023; Feng et al., 2023b;a) end up being a special case where the forces of originalstructures are close to zero as in (e)": "Based on the insight, in this paper, we propose to use denoising non-equilibrium structures (DeNS) as anauxiliary task to better leverage atomistic data. For training DeNS, we first corrupt a structure by addingnoise to its 3D atomic coordinates and then reconstruct the original uncorrupted structure by predicting thenoise. For noise prediction, a model is given the forces of the original uncorrupted structure as inputs to makethe transformation from a corrupted non-equilibrium structure to an uncorrupted non-equilibrium structuretractable. When used along with the original tasks like predicting the energy and forces of non-equilibriumstructures, DeNS improves the performance on the original tasks with a marginal increase in training cost. Wefurther discuss how DeNS can better leverage training data to improve performance and show the connectionto self-supervised learning methods in other domains. Because DeNS requires encoding forces, it favors equivariant networks, which build up equivariant featuresat each node with vector spaces of irreducible representations (irreps) and have interactions or messagepassing between nodes with equivariant operations like tensor products. Since forces can be projected tovector spaces of irreps with spherical harmonics, equivariant networks can easily incorporate forces in nodeembeddings. Moreover, with the reduced complexity of equivariant operations (Passaro & Zitnick, 2023) andincorporation of the Transformer network design (Liao & Smidt, 2023; Liao et al., 2023) from NLP (Vaswaniet al., 2017) and CV (Dosovitskiy et al., 2021), equivariant networks have become the state-of-the-art methodson large-scale atomistic datasets. We mainly focus on how DeNS can improve equivariant networks and conduct extensive experiments onOC20 (Chanussot* et al., 2021), OC22 (Tran* et al., 2022) and MD17 (Chmiela et al., 2017; Schtt et al.,2017; Chmiela et al., 2018) datasets. On OC20 S2EF-2M dataset, EquiformerV2 (Liao et al., 2023) trainedwith DeNS can achieve better energy and force results and save 2.3 training time compared to trainingwithout DeNS (.1.1). On OC20 S2EF-All+MD dataset, EquiformerV2 trained with DeNS achievesnew state-of-the-art results on Structure to Energy and Forces (S2EF) and Initial Structure to RelaxedEnergy (IS2RE) tasks (.1.2). Similarly, EquiformerV2 trained with DeNS sets new state-of-the-artresults on OC22 dataset, improving energy by up to 15%, forces by up to 12%, and IS2RE by up to 15%(.2). On MD17 dataset, Equiformer (Lmax 2) (Liao & Smidt, 2023) trained with DeNS achievesbetter results and saves 3.1 training time compared to Equiformer (Lmax 3) without DeNS (.3),where Lmax denotes the maximum degree. Additionally, DeNS can improve other equivariant networks likeeSCN (Passaro & Zitnick, 2023) on OC20 and SEGNN-like networks (Brandstetter et al., 2022) on MD17.",
  "Related Works": "Denoising 3D Atomistic Structures.Denoising structures have been used to boost the performance ofGNNs on 3D atomistic datasets (Godwin et al., 2022; Jiao et al., 2022; Zaidi et al., 2023; Liu et al., 2023;Feng et al., 2023b; Wang et al., 2023; Feng et al., 2023a). The approach is to first corrupt data by addingnoise and then train a denoising autoencoder to reconstruct the original data by predicting the noise, and themotivation is that learning to reconstruct data enables learning generalizable representations (Devlin et al.,2019; He et al., 2022; Godwin et al., 2022; Zaidi et al., 2023). Since denoising equilibrium structures do notrequire labels and is self-supervised, similar to BERT (Devlin et al., 2019) and MAE (He et al., 2022), it iscommon to pre-train via denoising on a large dataset of equilibrium structures like PCQM4Mv2 (Nakata &Shimazaki, 2017) and then fine-tune with supervised learning on smaller downstream datasets. Besides, thework of Noisy Nodes (Godwin et al., 2022) uses denoising equilibrium structures as an auxiliary task alongwith original tasks without pre-training on another larger dataset. However, most of the previous works arelimited to equilibrium structures, which occupy a much smaller amount of data than non-equilibrium ones.In contrast, the proposed DeNS generalizes denoising to non-equilibrium structures with force encoding sothat we can improve the performance on the larger set of non-equilibrium structures. We provide a detailedcomparison to previous works on denoising in Section A.1.",
  "Problem Setup": "Calculating quantum mechanical properties like energy and forces of 3D atomistic systems is fundamentalto many applications. An atomistic system can be one or more molecules, a crystalline material and so on.Specifically, each system S is an example in a dataset and can be described as S tpzi, piq | i P t1, ..., |S|uu,where zi P N denotes the atomic number of the i-th atom and pi P R3 denotes the 3D atomic position. Theenergy of S is denoted as EpSq P R, and the atom-wise forces are denoted as FpSq fi P R3 | i P t1, ..., |S|u(,where fi is the force acting on the i-th atom. In this paper, we define a system to be an equilibrium structureif all of its atom-wise forces are close to zero. Otherwise, we refer to it as a non-equilibrium structure. Sincenon-equilibrium structures have non-zero atomic forces and thus are not at an energy minimum, they havemore degrees of freedom and constitute a much larger set of possible structures than those at equilibrium. In this work, we focus on the task of predicting energy and forces given non-equilibrium structures. Specifically,given a non-equilibrium structure Snon-eq, GNNs predict energy EpSnon-eqq and atom-wise forces FpSnon-eqq !fipSnon-eqq P R3 | i P t1, ..., |Snon-eq|u)and minimize the following loss function:",
  "where E and F are energy and force coefficients controlling the relative importance between energy and forcepredictions. E1pSnon-eqq EpSnon-eqqE": "Eis the normalized ground truth energy obtained by first subtractingthe original energy EpSnon-eqq by the mean of energy labels in the training set E and then dividing by thestandard deviation of energy labels E. Similarly, f 1i fiF is the normalized atom-wise force. For forceprediction, we can either use direct methods, which is to directly predict forces from latent representationslike node embeddings, as commonly used for OC20 and OC22 datasets or gradient methods, which is to takethe negative gradients of predicted energy with respect to atomic positions, for datasets like MD17.",
  "Formulation of Denoising": "Denoising structures has been used to improve the performance of GNNs on 3D atomistic datasets (Godwinet al., 2022; Zaidi et al., 2023; Feng et al., 2023b; Wang et al., 2023). We first corrupt data by adding noiseand then train a denoising autoencoder to reconstruct the original data by predicting the noise. Specifically,given a 3D atomistic system S tpzi, piq | i P t1, ..., |S|uu, we create a corrupted structure S by addingGaussian noise with a tuneable standard deviation to the atomic positions pi of the original structure S:",
  "Force Encoding": "The reason that denoising non-equilibrium structures can be ill-posed is because we do not provide sufficientinformation to specify the properties of the target structures. Concretely, given an original non-equilibriumstructure Snon-eq and its corrupted counterpart Snon-eq, some structures interpolated between Snon-eq andSnon-eq could be in the same data distribution and therefore be the potential target structures of denoising.In contrast, when denoising equilibrium structures as shown in (c), we implicitly provide the extrainformation that the target structure should be at equilibrium with near-zero forces, and this thereforelimits the possibility of the target of denoising. Motivated by the assumption that the forces of the originalstructures are close to zeros when denoising equilibrium ones, we propose to encode the forces of originalnon-equilibrium structures when denoising non-equilibrium ones as illustrated in (d). Specifically,when training denoising non-equilibrium structures (DeNS), GNNs take both a corrupted non-equilibriumstructure Snon-eq and the forces FpSnon-eqq of the original non-equilibrium structure Snon-eq as inputs, outputatom-wise noise prediction ` Snon-eq, FpSnon-eqq",
  "fl(4)": "Equation 4 is more general and reduces to Equation 3 when the target of denoising becomes equilib-rium structures with near-zero forces. Since we train GNNs with Snon-eq and FpSnon-eqq as inputs andNoisepSnon-eq, Snon-eqq as outputs, they implicitly learn to leverage FpSnon-eqq to reconstruct Snon-eq insteadof predicting any arbitrary non-equilibrium structures. Empirically, we find that force encoding is critical tothe effectiveness of DeNS on OC20 S2EF-2M dataset (Index 2 and Index 3 in (b)) and MD17 dataset(Index 2 and Index 3 in ). Since DeNS requires encoding forces, DeNS favors equivariant networks, which can easily incorporate forces aswell as other higher-degree tensors like stress into their node embeddings. Specifically, the node embeddingsof equivariant networks are equivariant irreps features built from vectors spaces of irreducible representations(irreps) and contain CL channels of type-L vectors with degree L being in the range from 0 to maximumdegree Lmax. CL and Lmax are architectural hyper-parameters of equivariant networks. To obtain theforce embedding xf from the input force f, we first project f into type-L vectors with spherical harmonicsY pLq f",
  "(5)": "xpLqfdenotes the channels of type-L vectors in force embedding xf, and SO3_LinearpLq denotes the SOp3qlinear operation on type-L vectors. Since we normalize the input force when using spherical harmonics,we multiply Y pLq f ||f||with the norm of input force ||f|| to recover the information of force magnitude.After computing force embeddings for all atom-wise forces, we simply add the force embeddings to initialnode embeddings to encode forces in equivariant networks. The pseudocode for encoding forces into nodeembeddings can be found in Section E. On the other hand, it might not be that intuitive to encode forces in invariant networks since their internallatent representations such as node embeddings and edge embeddings are scalars instead of type-L vectors",
  "Training with DeNS": "Auxiliary Task.We propose to use DeNS as an auxiliary task along with the original task of predictingenergy and forces and summarize the training process in . Specifically, given a batch of structures,for each structure, we decide whether we optimize the objective of DeNS ((b) or (c)) orthe objective of the original task ((a)). This introduces an additional hyper-parameter pDeNS,the probability of optimizing DeNS. We use an additional noise head for noise prediction, which slightlyincreases training time. Additionally, when training DeNS, similar to Noisy Nodes (Godwin et al., 2022),we also leverage energy labels and predict the energy of original structures. Therefore, given an originalnon-equilibrium structure Snon-eq and the corrupted counterpart Snon-eq, training DeNS corresponds tominimizing the following loss function:",
  "E LE ` DeNS LDeNS E E1pSnon-eqq E` Snon-eq, FpSnon-eqq ` DeNS LDeNS(6)": "where LDeNS denotes the loss function of denoising as defined in Equation 4. We note that we also encodeforces as discussed in .2.2 to predict the energy of Snon-eq, and we share the energy prediction headacross Equation 1 and Equation 6. The loss function introduces another hyper-parameter DeNS, DeNScoefficient, controlling the relative importance of the auxiliary task. Besides, the process of corruptingstructures also results in another hyper-parameter as shown in Equation 2. We provide the pseudocode inSection F. We note that the force encoding in DeNS only relies on force labels on the training set and we donot use any force labels on the validation or testing sets. Partially Corrupted Structures.Empirically, we find that adding noise to all atoms in a structure cansometimes lead to limited performance gain of DeNS. We surmise adding noise to all atoms makes denoisingtoo difficult and potentially not well-defined and that there are still several structures satisfying input forces.To address this issue, we use partially corrupted structures, where we only add noise to and denoise a randomsubset of atoms as shown in (c). Specifically, for corrupted atoms, we encode their atom-wise forcesand predict the noise. For other uncorrupted atoms, we do not encode forces and train for the original taskof predicting forces. We also predict the energy of the original structures given partially corrupted structures.This introduces an additional hyper-parameter, corruption ratio rDeNS, which is the ratio of the number ofcorrupted atoms to that of all atoms.",
  "Discussion": "Why Does DeNS Help.DeNS can better leverage training data to improve the performance in thefollowing two ways. First, DeNS adds noise to non-equilibrium structures to generate structures with newgeometries and therefore naturally achieves data augmentation (Godwin et al., 2022). Second, training DeNSenourages learning a different yet highly correlated interaction. Since we encode forces as inputs and predictthe original structures in terms of noise corrections, DeNS enables learning the interaction of transformingforces into structures, which is the inverse of predicting forces. As demonstrated in the works of NoisyNodes (Godwin et al., 2022) and UL2 (Tay et al., 2023), training a single model with multiple correlatedobjectives to learn different interactions can help the performance on original tasks.",
  "(b) DeNS without partially corrupted structures. All the atoms in a structure are corrupted with Gaussian noise. Weencode all the atom-wise forces to predict noise": "(c) DeNS with partially corrupted structures. We add noise to and denoise a random subset of atoms. For corruptedatoms, we encode their forces and predict the noise. For other uncorrupted atoms, we instead predict their forces.Here only the top two white atoms are corrupted. : Training process when incorporating DeNS as an auxiliary task. The pseudocode for training with DeNScan be found in Section F. Equivariant GNN, energy head, force head and noise head are shared across (a),(b) and (c). For each batch of structures, we use the original task (a) for some structures and DeNS ((b) or (c)) forthe others. Using partially corrupted structures as in (c) is empirically better than (b). We note that the force labelFpSnon-eqq and energy label EpSnon-eqq used in (b) and (c) are the same as those in (a) and that training with DeNSdoes not require any additional data. Connection to Self-Supervised Learning.DeNS shares similar intuitions to self-supervised learningmethods like BERT (Devlin et al., 2019) and MAE (He et al., 2022) and other denoising methods (Vincentet al., 2008; 2010; Godwin et al., 2022; Zaidi et al., 2023) they remove or corrupt a portion of input dataand then learn to predict the original data. Learning to reconstruct data can help learning generalizablerepresentations, and therefore these methods can use the task of reconstruction to improve the performanceon downstream tasks. However, unlike those self-supervised learning methods (Devlin et al., 2019; He et al.,2022; Zaidi et al., 2023), DeNS requires force labels for encoding, and therefore, we propose to use DeNS as an",
  "(b) Design Choices": ": Ablation results of training with DeNS on OC20 S2EF-2M dataset. We report mean absolute errors for forcesin meV/ and energy in meV, and lower is better. Errors are averaged over the four validation sub-splits of OC20. (a)We train EquiformerV2 and eSCN and compare the results of training with and without DeNS. The training time ismeasured on V100 GPUs. (b) We train EquiformerV2 for 12 epochs to verify the design choices of DeNS. auxiliary task optimized along with original tasks and do not follow the previous practice of first pre-trainingand then fine-tuning. Additionally, we note that before obtaining a single equilibrium structure, we need torun relaxations and generate many intermediate non-equilibrium ones ((a)), which is the labelingprocess as well.We hope that the ability to leverage more from non-equilibrium structures as proposedin this work can encourage researchers to release data containing intermediate non-equilibrium structuresin addition to final equilibrium ones. Moreover, we note that DeNS can also be used in fine-tuning. Forexample, we can first pre-train models on PCQM4Mv2 dataset and then fine-tune them on the smaller MD17dataset with both the original task and DeNS. Marginal Increase in Training Time.Since we use an additional noise head for denoising, training withDeNS marginally increases the time of each training iteration. We optimize DeNS for some structures andthe original task for the others for each training iteration, and we demonstrate that DeNS can improve theperformance given the same amount of training iterations. Therefore, training with DeNS only marginallyincrease the overall training time.",
  "OC20 Dataset": "Dataset and Tasks.We start with experiments on the large and diverse Open Catalyst 2020 dataset(OC20) (Chanussot* et al., 2021), which consists of about 1.2M Density Functional Theory (DFT) relaxationtrajectories. Each DFT trajectory in OC20 starts from an initial structure of an adsorbate molecule placed on acatalyst surface, which is then relaxed with the revised Perdew-Burke-Ernzerhof (RPBE) functional (Hammeret al., 1999) calculations to a local energy minimum. Relevant to DeNS, all the intermediate structures froma relaxation trajectory, except the relaxed structure, are considered non-equilibrium structures. The relaxedor equilibrium structure has forces close to zero. The primary task in OC20 is Structure to Energy and Forces (S2EF), which is to predict the energy andper-atom forces given an equilibrium or non-equilibrium structure from any point in the trajectory. Thesepredictions are evaluated on energy and force mean absolute error (MAE). Most of the previous worksuse direct methods to predict forces on OC20, and we follow this practice and investigate how DeNS canimprove direct methods for force prediction. Once a model is trained for S2EF, it is used to run structuralrelaxations from an initial structure using the predicted forces till a local energy minimum is found. Theenergy predictions of these relaxed structures are evaluated on the Initial Structure to Relaxed Energy(IS2RE) task.",
  "We use OC20 S2EF-2M dataset to compare the results of training with and without DeNS and verify thedesign choices of DeNS": "Comparison between Training with and without DeNS.(a) summarizes the results of trainingwith and without DeNS. For EquiformerV2, incorporating DeNS as an auxiliary task boosts the performanceon energy and force predictions and only increases training time by 7.4% and the number of parameters from83M to 89M. Particularly, EquiformerV2 trained with DeNS for 12 epochs outperforms EquiformerV2 trainedwithout DeNS for 30 epochs and saves 2.3 training time. This suggests that using data augmentation andlearning an auxiliary task are more efficient to improve performance compared to simply training for longer.Additionally, we show that DeNS can be applicable to other equivariant networks like eSCN (Passaro &Zitnick, 2023). Training eSCN with DeNS for 20 epochs results in better energy and force MAE compared toEquiformerV2 trained without DeNS for 30 epochs while requiring 1.9 less training time. DeNS slightlyincreases training time of eSCN by 1.5%, and the different amounts of increase in training time betweenEquiformerV2 and eSCN are because they use different modules for noise prediction. Design Choices.We train EquiformerV2 for 12 epochs with DeNS to verify the design choices of DeNSand summarize the results in (b). Comparing Index 2 and Index 3, we show that encoding forcesFpSnon-eqq in Equations 4 and 6 enables denoising non-equilibrium structures to significantly improve bothenergy and force MAE. Moreover, DeNS without force encoding (Index 3) results in clearly worse force MAEcompared to training without DeNS (Index 1). These results verify our claim that denoising non-equilibriumstructures naively can be ill-posed and potentially harmful to performance and that force encoding is criticalto leverage the gain of denoising. We note that force encoding only relies on force labels in the trainingset and does not require any additional data. We also compare DeNS with and without force encoding onMD17 in .3 and have similar observations. Comparing Index 2 and Index 4, we demonstrate thatpredicting energy of original structures given corrupted ones can improve energy MAE by 5.6% while slightlyincreasing force MAE by 1.2%. The increase in force MAE is because predicting energy given corruptedstructures is equivalent to using E 0 in Equation 6 and implicitly decreases the relative importance offorce prediction. Since the decrease in energy MAE is greater than the increase in force MAE, we adopt thepractice of predicting energy given corrupted structures. Finally, the comparison between Index 2 and Index5 shows that partially corrupted structures can further improve the performance gain of DeNS.",
  "Main Results": "We train EquiformerV2 (160M) with DeNS on OC20 S2EF-All+MD dataset. The model follows the sameconfiguration as EquiformerV2 (153M) trained without DeNS, and the additional parameters are due to forceencoding and one additional block of equivariant graph attention for noise prediction. We report resultsin . All test results are computed via the EvalAI evaluation server1. EquiformerV2 trained withDeNS achieves new state-of-the-art results on both S2EF and IS2RE tasks. On the S2EF validation split,EquiformerV2 trained with DeNS improves energy MAE by 5meV and force MAE by 1.0meV/, which iscomparable to the gain brought by increasing the size of EquiformerV2 from 31M to 153M (energy MAEimproved by 5meV and force MAE improved by 1.3meV/). On the S2EF test split, the improvement inenergy and force predictions is smaller, which is probably because of different splits. On the IS2RE test split,training EquiformerV2 with DeNS reduces MAE by 16meV, achieving similar performance gain of goingfrom eSCN (Passaro & Zitnick, 2023) to EquiformerV2 (Liao et al., 2023) (IS2RE MAE improved by 14meV)and demonstrating the effectiveness of training with DeNS. We note that the improvement might not be assignificant as that on OC20 S2EF-2M (.1.1), OC22 (.2) and MD17 (.3) datasetssince the OC20 S2EF-All+MD training set contains much more structures along relaxation trajectories,making new 3D geometries generated by DeNS less helpful. However, DeNS is still valuable because mostdatasets are not as large as OC20 S2EF-All+MD dataset (about 172M structures in the training set) buthave sizes closer to OC20 S2EF-2M (2M structures), OC22 (8.2M structures), and MD17 (950 structures)datasets.",
  "OC22 Dataset": "Dataset and Tasks.The Open Catalyst 2022 (OC22) dataset (Tran* et al., 2022) focuses on oxideelectrocatalysis and consists of about 62k DFT relaxations obtained with Perdew-Burke-Ernzerhof (PBE)functional calculations. One crucial difference between OC22 and OC20 is that the energy targets in OC22are DFT total energies. DFT total energies are harder to predict but are the most general and closest to aDFT surrogate, offering the flexibility to study property prediction beyond adsorption energies. Analogous tothe task definitions in OC20, the primary tasks in OC22 are S2EF-Total and IS2RE-Total. We train modelson the OC22 S2EF-Total dataset, which has 8.2M structures, and evaluate them on energy and force MAEon the S2EF-total validation and test splits. Same as OC20, we use direct methods to predict forces here.Then, we use these models to perform relaxations starting from initial structures in the IS2RE-Total testsplit and evaluate the predicted relaxed energies on energy MAE.",
  "Training Details.Please refer to Section C.1 for details on architectures, hyper-parameters and trainingtime": "Results.We use EquiformerV2 with energy coefficient E 4 and force coefficient F 100 to demonstratehow DeNS can further improve state-of-the-art results and summarize the comparison in . Comparedto EquiformerV2 (E 4, F 100) trained without DeNS, EquiformerV2 trained with DeNS consistentlyachieves better energy and force MAE across all the S2EF-Total validation and test splits, with 9.6% to 15.3%improvement in energy MAE and 7.1% to 11.7% improvement in force MAE. For IS2RE-Total, EquiformerV2trained with DeNS achieves the best energy MAE results. The improvement in IS2RE-Total from trainingwith DeNS on only OC22 is greater than that of training on the much larger OC20 and OC22 datasetsreported in previous works. Specifically, training GemNet-OC on OC20 and OC22 datasets (about 134M+ 8.2M structures) improves IS2RE-Total energy MAE ID by 129meV and OOD by 50meV compared totraining GemNet-OC on only OC22 dataset (8.2M structures). Compared to training without DeNS, trainingEquiformerV2 with DeNS improves ID by 168meV and OOD by 158meV. Thus, training with DeNS clearlyimproves data efficiency and performance on OC22.",
  "MD17 Dataset": "Dataset.The MD17 dataset (Chmiela et al., 2017; Schtt et al., 2017; Chmiela et al., 2018) consists ofmolecular dynamics simulations of small organic molecules. The task is to predict the energy and forces ofthese non-equilibrium molecules. Following previous works, we adopt gradient methods for force prediction.We use 950 and 50 different configurations for training and validation sets and the rest for the testing set.",
  "Training Details.Please refer to Section D.2 for additinoal implementation details of DeNS, hyper-parameters and training time": "Main Results.We train Equiformer (Lmax 2) (Liao & Smidt, 2023) and Equiformer (Lmax 3)with DeNS based on their official implementation, where Lmax denotes the maximum degree of equivariantrepresentations. As shown in , DeNS improves the results on all molecules. Along with the results onOC20 and OC22 datasets, DeNS can generally improve the performance on force predictions with both direct(i.e., OC20 and OC22) and gradient (i.e., MD17) methods. Particularly, Equiformer (Lmax 2) trainedwith DeNS acheives better results on all the tasks and requires 3.1 less training time than Equiformer(Lmax 3) trained without DeNS. This demonstrates that for this small dataset, training an auxiliary taskand using data augmentation are more efficient and result in larger performance gain than increasing Lmaxfrom 2 to 3. Additionally, we find that the gains from training DeNS as an auxiliary task are comparable topre-training. For example, Zaidi et al. (2023) uses TorchMD-NET (Thlke & Fabritiis, 2022) pre-trained onthe PCQM4Mv2 dataset and reports results on Aspirin. Their improvement in force MAE is about 17.2%( in Zaidi et al. (2023)). Training Equiformer (Lmax 2) with DeNS results in 20.8% improvement inforce MAE without relying on another dataset. Note that we only increase training time by 10.5% while theirmethod takes much more time since PCQM4Mv2 dataset is more than 3000 larger than the training set of",
  "Conclusion": "In this paper, we propose to use denoising non-equilibrium structures (DeNS) as an auxiliary task to betterleverage training data and improve performance on original tasks of energy and force predictions. Denoisingnon-equilibrium structures can be an ill-posed problem since there are many possible target structures. Toaddress the issue, we propose force encoding and take the forces of original structures as inputs to specifywhich non-equilibrium structures we are denoising. With force encoding, DeNS successfully improves theperformance on original tasks when it is used as an auxiliary task. We conduct extensive experiments onOC20, OC22 and MD17 datasets to demonstrate that DeNS can boost the performance on energy and forcepredictions across datasets of various scales with minimal increase in training cost and is applicable to manyequivariant networks. Finally, we note that the proposed DeNS is general and can be directly applied toother atomistic datasets containing non-equilibrium structures. Take Materials Project (Jain et al., 2013) forexample. Similar to OC20 and OC22 datasets, for each entry in the Materials Project database, the MaterialsProject Trajectory (MPtrj) dataset (Deng et al., 2023) contains the corresponding relaxation trajectorybetween initial and relaxed structures. Most structures in MPtrj are non-equilibrium and have energy andforce labels. Therefore, we can apply DeNS to MPtrj dataset in the same way as OC20 and OC22 datasets.",
  "Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton.Layer normalization.arxiv preprintarxiv:1607.06450, 2016": "Ilyes Batatia, David Peter Kovacs, Gregor N. C. Simm, Christoph Ortner, and Gabor Csanyi. MACE: Higherorder equivariant message passing neural networks for fast and accurate force fields. In Advances in NeuralInformation Processing Systems (NeurIPS), 2022. Simon Batzner, Albert Musaelian, Lixin Sun, Mario Geiger, Jonathan P. Mailoa, Mordechai Kornbluth, NicolaMolinari, Tess E. Smidt, and Boris Kozinsky. E(3)-equivariant graph neural networks for data-efficient andaccurate interatomic potentials. Nature Communications, 13(1), May 2022. doi: 10.1038/s41467-022-29939-5.URL Johannes Brandstetter, Rob Hesselink, Elise van der Pol, Erik J Bekkers, and Max Welling. Geometric andphysical quantities improve e(3) equivariant message passing. In International Conference on LearningRepresentations (ICLR), 2022. URL Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, GretchenKrueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, ChrisHesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, ChristopherBerner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shotlearners. In Advances in Neural Information Processing Systems (NeurIPS), 2020. Lowik Chanussot*, Abhishek Das*, Siddharth Goyal*, Thibaut Lavril*, Muhammed Shuaibi*, MorganeRiviere, Kevin Tran, Javier Heras-Domingo, Caleb Ho, Weihua Hu, Aini Palizhati, Anuroop Sriram,Brandon Wood, Junwoong Yoon, Devi Parikh, C. Lawrence Zitnick, and Zachary Ulissi. Open catalyst2020 (oc20) dataset and community challenges. ACS Catalysis, 2021. doi: 10.1021/acscatal.0c04525. Stefan Chmiela, Alexandre Tkatchenko, Huziel E. Sauceda, Igor Poltavsky, Kristof T. Schtt, and Klaus-Robert Mller. Machine learning of accurate energy-conserving molecular force fields. Science Advances,3(5):e1603015, 2017. doi: 10.1126/sciadv.1603015. URL Stefan Chmiela, Huziel E. Sauceda, Klaus-Robert Mller, and Alexandre Tkatchenko. Towards exact moleculardynamics simulations with machine-learned force fields. Nature Communications, 9(1), sep 2018. doi:10.1038/s41467-018-06169-2.",
  "Taco S. Cohen, Mario Geiger, Jonas Khler, and Max Welling. Spherical CNNs. In International Conferenceon Learning Representations (ICLR), 2018. URL": "Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin Gilmer, AndreasSteiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin, Rodolphe Jenatton, Lucas Beyer,Michael Tschannen, Anurag Arnab, Xiao Wang, Carlos Riquelme, Matthias Minderer, Joan Puigcerver,Utku Evci, Manoj Kumar, Sjoerd van Steenkiste, Gamaleldin F. Elsayed, Aravindh Mahendran, Fisher Yu,Avital Oliver, Fantine Huot, Jasmijn Bastings, Mark Patrick Collier, Alexey Gritsenko, Vighnesh Birodkar,Cristina Vasconcelos, Yi Tay, Thomas Mensink, Alexander Kolesnikov, Filip Paveti, Dustin Tran, ThomasKipf, Mario Lui, Xiaohua Zhai, Daniel Keysers, Jeremiah Harmsen, and Neil Houlsby. Scaling visiontransformers to 22 billion parameters. arXiv preprint arXiv:2302.05442, 2023. Bowen Deng, Peichen Zhong, KyuJung Jun, Janosh Riebesell, Kevin Han, Christopher J. Bartel, and GerbrandCeder. Chgnet as a pretrained universal neural network potential for charge-informed atomistic modelling.Nature Machine Intelligence, 2023.",
  "An image is worth 16x16 words: Transformers for image recognition at scale. In International Conferenceon Learning Representations (ICLR), 2021. URL": "Alexandre Duval, Simon V. Mathis, Chaitanya K. Joshi, Victor Schmidt, Santiago Miret, Fragkiskos D.Malliaros, Taco Cohen, Pietro Li, Yoshua Bengio, and Michael Bronstein. A hitchhikers guide to geometricgnns for 3d atomic systems. arXiv preprint arXiv:2312.07511, 2024. Alexandre Agm Duval, Victor Schmidt, Alex Hernndez-Garca, Santiago Miret, Fragkiskos D. Malliaros,Yoshua Bengio, and David Rolnick. FAENet: Frame averaging equivariant GNN for materials modeling. InInternational Conference on Machine Learning (ICML), 2023. Rui Feng, Qi Zhu, Huan Tran, Binghong Chen, Aubrey Toland, Rampi Ramprasad, and Chao Zhang. Maythe force be with you: Unified force-centric pre-training for 3d molecular conformations. arXiv preprintarXiv:2308.14759, 2023a.",
  "Shikun Feng, Yuyan Ni, Yanyan Lan, Zhi-Ming Ma, and Wei-Ying Ma. Fractional denoising for 3D molecularpre-training. In International Conference on Machine Learning (ICML), 2023b": "Xiang Fu, Zhenghao Wu, Wujie Wang, Tian Xie, Sinan Keten, Rafael Gomez-Bombarelli, and TommiJaakkola. Forces are not enough: Benchmark and critical evaluation for machine learning force fields withmolecular simulations. Transactions on Machine Learning Research (TMLR), 2023. Fabian Fuchs, Daniel E. Worrall, Volker Fischer, and Max Welling. Se(3)-transformers: 3d roto-translationequivariant attention networks. In Advances in Neural Information Processing Systems (NeurIPS), 2020.",
  "Rui Jiao, Jiaqi Han, Wenbing Huang, Yu Rong, and Yang Liu. Energy-motivated equivariant pretraining for3d molecular graphs. arXiv preprint arXiv:2207.08824, 2022": "Bowen Jing, Stephan Eismann, Patricia Suriana, Raphael John Lamarre Townshend, and Ron Dror. Learningfrom protein structure with geometric vector perceptrons.In International Conference on LearningRepresentations (ICLR), 2021. URL Risi Kondor, Zhen Lin, and Shubhendu Trivedi.Clebschgordan nets: a fully fourier space sphericalconvolutional neural network. In Advances in Neural Information Processing Systems 32, pp. 1011710126,2018. Janice Lan, Aini Palizhati, Muhammed Shuaibi, Brandon M Wood, Brook Wander, Abhishek Das, MattUyttendaele, C Lawrence Zitnick, and Zachary W Ulissi. AdsorbML: Accelerating adsorption energycalculations with machine learning. arXiv preprint arXiv:2211.16486, 2022.",
  "Benjamin Kurt Miller, Mario Geiger, Tess E. Smidt, and Frank No. Relevance of rotationally equivariantconvolutions for predicting molecular properties. arxiv preprint arxiv:2008.08461, 2020": "Albert Musaelian, Simon Batzner, Anders Johansson, Lixin Sun, Cameron J. Owen, Mordechai Kornbluth,and Boris Kozinsky. Learning local equivariant representations for large-scale atomistic dynamics. arxivpreprint arxiv:2204.05249, 2022. Albert Musaelian, Anders Johansson, Simon Batzner, and Boris Kozinsky. Scaling the leading accuracy ofdeep equivariant models to biomolecular simulations of realistic size. arXiv preprint arXiv:2304.10061,2023. Maho Nakata and Tomomi Shimazaki. Pubchemqc project: A large-scale first-principles electronic structuredatabase for data-driven chemistry. Journal of chemical information and modeling, 57 6:13001308, 2017.",
  "Raghunathan Ramakrishnan, Pavlo O Dral, Matthias Rupp, and O Anatole von Lilienfeld. Quantumchemistry structures and properties of 134 kilo molecules. Scientific Data, 1, 2014": "Albert Reuther, Jeremy Kepner, Chansup Byun, Siddharth Samsi, William Arcand, David Bestor, BillBergeron, Vijay Gadepally, Michael Houle, Matthew Hubbell, Michael Jones, Anna Klein, Lauren Milechin,Julia Mullen, Andrew Prout, Antonio Rosa, Charles Yee, and Peter Michaleas. Interactive supercomputingon 40,000 cores for machine learning and data analysis. In 2018 IEEE High Performance extreme ComputingConference (HPEC), pp. 16. IEEE, 2018.",
  "Vctor Garcia Satorras, Emiel Hoogeboom, and Max Welling. E(n) equivariant graph neural networks. InInternational Conference on Machine Learning (ICML), 2021": "K. T. Schtt, P.-J. Kindermans, H. E. Sauceda, S. Chmiela, A. Tkatchenko, and K.-R. Mller. Schnet: Acontinuous-filter convolutional neural network for modeling quantum interactions. In Advances in NeuralInformation Processing Systems (NeurIPS), 2017. Kristof T. Schtt, Farhad Arbabzadah, Stefan Chmiela, Klaus R. Mller, and Alexandre Tkatchenko.Quantum-chemical insights from deep tensor neural networks. Nature Communications, 8(1), jan 2017. doi:10.1038/ncomms13890. Kristof T. Schtt, Oliver T. Unke, and Michael Gastegger. Equivariant message passing for the prediction oftensorial properties and molecular spectra. In International Conference on Machine Learning (ICML),2021.",
  "Justin S. Smith, Benjamin Tyler Nebgen, Nicholas Lubbers, Olexandr Isayev, and Adrian E. Roitberg. Lessis more: sampling chemical space with active learning. The Journal of chemical physics, 2018": "Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: Asimple way to prevent neural networks from overfitting. Journal of Machine Learning Research, 15(56):19291958, 2014. URL Yi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier Garcia, Jason Wei, Xuezhi Wang, Hyung Won Chung,Dara Bahri, Tal Schuster, Steven Zheng, Denny Zhou, Neil Houlsby, and Donald Metzler. UL2: Unifyinglanguage learning paradigms. In International Conference on Learning Representations (ICLR), 2023. URL",
  "Raphael J. L. Townshend, Brent Townshend, Stephan Eismann, and Ron O. Dror. Geometric prediction:Moving beyond scalars. arXiv preprint arXiv:2006.14163, 2020": "Richard Tran*, Janice Lan*, Muhammed Shuaibi*, Brandon Wood*, Siddharth Goyal*, Abhishek Das,Javier Heras-Domingo, Adeesh Kolluru, Ammar Rizvi, Nima Shoghi, Anuroop Sriram, Zachary Ulissi, andC. Lawrence Zitnick. The open catalyst 2022 (oc22) dataset and challenges for oxide electrocatalysis. arXivpreprint arXiv:2206.08917, 2022. Oliver Thorsten Unke, Mihail Bogojeski, Michael Gastegger, Mario Geiger, Tess Smidt, and Klaus RobertMuller. SE(3)-equivariant prediction of molecular wavefunctions and electronic densities. In A. Beygelzimer,Y. Dauphin, P. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems(NeurIPS), 2021. URL",
  "We discuss previous works on denoising (Godwin et al., 2022; Zaidi et al., 2023; Feng et al., 2023b; Wanget al., 2023) in chronological order and compare them with this work as below": "Godwin et al. (2022) first proposes the idea of adding noise to 3D coordinates and then using denoising as anauxiliary task. The auxiliary task is trained along with the original task without relying on another largedataset. Their approach requires known equilibrium structures and therefore is limited to QM9 (Ramakrishnanet al., 2014; Ruddigkeit et al., 2012) and OC20 IS2RE datasets and can not be applied to force predictionsuch as OC20 S2EF dataset. For QM9, all the structures are at equilibrium, and for OC20 IS2RE, thetarget of denoising is the relaxed, equilibrium structure. Denoising without force encoding is well-defined onboth QM9 and OC20 IS2RE datasets. In contrast, this work proposes using force encoding to generalizetheir approach to non-equilibrium structures, which have much larger datasets than equilibrium ones. Forceencoding can achieve better results on OC20 S2EF-2M dataset with little overhead (Index 2 and Index 3 in(b)) and is indispensable on MD17 dataset (.3). Zaidi et al. (2023) adopts the denoising approach proposed by Godwin et al. (2022) as a pre-training methodand therefore requires another large dataset containing unlabelled equilibrium structures for pre-training. Onthe other hand, Godwin et al. (2022) and this work use denoising along with the original task and do not useany additional unlabeled data. Feng et al. (2023b) follows the same practice of pre-training via denoising (Zaidi et al., 2023) and proposes adifferent manner of adding noise. Specifically, they separate noise into dihedral angle noise and coordinatenoise and only learn to predict coordinate noise. However, adding noise to dihedral angles requires tools likeRDKit (rdk) to obtain rotatable bonds and cannot be applied to other datasets like OC20 and OC22. Although Zaidi et al. (2023) and Feng et al. (2023b) report results of force prediction on MD17 dataset, theyfirst pre-train models on PCQM4Mv2 dataset (Nakata & Shimazaki, 2017) and then fine-tune the pre-trainedmodels on MD17 dataset. We note that their setting is different from ours since we do not use any datasetfor pre-training. As for fine-tuning on MD17 dataset, Zaidi et al. (2023) simply follows the same practice",
  "A.2SE(3)/E(3)-Equivariant Networks": "We first discuss the concept of equivariance and how equivariant networks achieve equivariance and thencompare previous works below. We note that most of the content is adapted from Equiformer (Liao & Smidt,2023) and EquiformerV2 (Liao et al., 2023). We refer readers to the works (Liao & Smidt, 2023; Liao et al.,2023) for more detailed background on equivariant networks and the work (Duval et al., 2024) for a broaderreview on geometric GNNs for modeling 3D atomistic systems. 3D atomistic systems are often described in 3D coordinate systems. We have the freedom to choose arbitrary3D coordinate systems since we can change between different coordinates via the symmetries of 3D space.The relevant 3D symmetris are rotation, translation and inversion. The Euclidean group Ep3q consists of 3Drotation, translation and inversion while the special Euclidean group SEp3q is comprised of 3D rotation andtranslation. The laws of physics remain the same regardless of the coordinate we use, and thus the properties of3D atomistic systems are equivariant to 3D symmetries. For instance, when a 3D atomistic system is rotated,quantities like energy will remain identical while others like forces will rotate accordingly. Mathematically,a function f mapping between vector spaces X and Y is equivariant to a group of transformations G iffor any input x P X, output y P Y and group element g P G, we have fpDXpgqxq DY pgqfpxq DY pgqy,where DXpgq and DY pgq are transformation matrices or group representations parametrized by g in X andY . Additionally, f is invariant when DY pgq is an identity matrix for any g P G. Incorporating equivariance into neural networks as inductive biases can improve data efficiency and generaliz-ability. Equivariant neural networks (Thomas et al., 2018; Kondor et al., 2018; Weiler et al., 2018; Fuchset al., 2020; Miller et al., 2020; Townshend et al., 2020; Batzner et al., 2022; Jing et al., 2021; Schtt et al.,2021; Satorras et al., 2021; Brandstetter et al., 2022; Thlke & Fabritiis, 2022; Le et al., 2022; Musaelianet al., 2022; Batatia et al., 2022; Liao & Smidt, 2023; Passaro & Zitnick, 2023; Liao et al., 2023) achieveequivariance to 3D rotation and optionally inversion by using vector spaces of irreducible representations(irreps) as equivariant features. The vector spaces of irreps are p2L ` 1q-dimensional, with degree L being anon-negative integer. L can be viewed as the angular frequency of vectors and determines how fast vectorschange with respect to a rotation of the coordinate system. Vectors of degree L are referred to as type-Lvectors. They are transformed with Wigner-D matrices DpLq when rotating the coordinate system, and DpLq",
  "of different L acts on independent vector spaces. Euclidean vectors r in R3 such as relative positions andforces can be projected into type-L vectors by using spherical harmonics Y pLqp r": "||r||q. We concatenate multipletype-L vectors to build an equivariant feature f. Given the maximum degree Lmax of equivariant features,f has CL type-L vectors, where 0 L Lmax and CL is the number of channels for type-L vectors. BothLmax and CL are architectural hyper-parameters of equivariant networks. Equivariant operations are appliedto equivariant features to preserve equivariance, and two examples are tensor products and SOp3q linearoperations. Tensor products are the fundamental operation in equivariant networks for interacting vectors ofdifferent L and are used to build convolutions and attention. On the other hand, SOp3q linear operationsapply separate linear operations to each group of type-L vectors. Relevant to the proposed method, weencode forces into equivariant features by first projecting forces to type-L vectors with spherical harmonicsand then expanding the number of channels to CL with an SOp3q linear operation. Previous works on equivariant networks mainly differ in which equivariant operations are used and thecombination of those operations.TFN (Thomas et al., 2018) and NequIP (Batzner et al., 2022) usetensor products for equivariant graph convolution with linear messages, with the latter utilizing extra gateactivation (Weiler et al., 2018). SEGNN (Brandstetter et al., 2022) applies gate activation to messages passing",
  "A.3Equiformer Series": "Equiformer (Liao & Smidt, 2023) is an SEp3q/Ep3q-equivariant graph neural network that combines theinductive biases of 3D-related equivariance with the strength of Transformers (Vaswani et al., 2017). Startingfrom Transformers, Equiformer introduces three architectural modifications.First, Equiformer adoptsequivariant features built from vector spaces of irreps as internal representations to incorporate equivariance.Second, equivariant operations are applied to the equivariant features. These operations include tensorproducts and the equivariant counterparts of the original operations in Transformers. The latter part consistsof equivariant linear operations (i.e., SOp3q linear operations), equivariant layer normalization (Ba et al.,2016) and gate activation (Weiler et al., 2018). Third, Equiformer proposes to apply non-linear functionsto both attention weights and message passing, which improves the expressivity of attention in standardTransformers. Although Equiformer demonstrates that Transformers generalize well to 3D atomistic systems, it is limited tosmall values of maximum degree Lmax because of the compute-intensive tensor product operations. Higherdegrees can better capture angular resolutions and directional information, and therefore lower Lmax canlimit the expressivity of Equiformer. To address this limitation, EquiformerV2 (Liao et al., 2023) adoptseSCN (Passaro & Zitnick, 2023) convolutions, which significantly reduce the computational complexity oftensor products, to incorporate higher-degree equivariant representations and proposes three architecturalimprovements to better leverage the power of higher degrees. First, attention re-normalization is proposedto introduce one additional layer normalization to the non-linear functions of attention weights.Thishelps stabilize attention and improves empirical performance. Second, separable S2 activation based on S2 activation (Cohen et al., 2018) is proposed to better mix the information of all degrees and stabilize training.Third, separable layer normalization is proposed to replace the original equivariant layer normalization inorder to preserve the relative importance of different degrees.",
  "B.1Training Details": "Since each structure in OC20 S2EF dataset has a pre-defined set of fixed and free atoms and we only predictforces of free atoms, we only apply DeNS to free atoms. When partially corrupted structures are used, we addnoise to and denoise a random subset of free atoms. When training EquiformerV2 on OC20 S2EF-All+MDdataset, we only apply DeNS to structures from the All split. For force prediction, we adopt direct methods following previous works for a fair comparison. We add anadditional block of equivariant graph attention to EquiformerV2 for noise prediction. We mainly follow thehyper-parameters of training EquiformerV2 without DeNS on OC20 S2EF-2M and S2EF-All+MD datasets.For training EquiformerV2 on OC20 S2EF-All+MD dataset, we increase the number of epochs from 1 to2 for better performance. This results in higher training time than other methods. However, we note thatwe already demonstrate training with DeNS can achieve better results given the same amount of trainingtime in (a). summarizes the hyper-parameters of training EquiformerV2 with DeNS for theablation studies on OC20 S2EF-2M dataset in .1.1 and for the main results on OC20 S2EF-All+MD",
  "104 for 20, 30 epochsBatch size64 for 12 epochs512128 for 20, 30 epochsNumber of epochs12, 20, 302Weight decay1 1031 103": "Dropout rate0.10.1Stochastic depth0.050.1Energy coefficient E24Force coefficient F100100Gradient clipping norm threshold100100Model EMA decay0.9990.999Cutoff radius ()1212Maximum number of neighbors2020Number of radial bases600600Dimension of hidden scalar features in radial functions dedgep0, 128qp0, 128qMaximum degree Lmax66Maximum order Mmax23Number of Transformer blocks1220Embedding dimension dembedp6, 128qp6, 128qf pLqijdimension dattn_hiddenp6, 64qp6, 64qNumber of attention heads h88f p0qijdimension dattn_alphap0, 64qp0, 64qValue dimension dattn_valuep6, 16qp6, 16qHidden dimension in feed forward networks dffnp6, 128qp6, 128qResolution of point samples R1818",
  ": Hyper-parameters of training EquiformerV2 with DeNS on OC20 S2EF-2M dataset and OC20 S2EF-All+MDdataset": "dataset in .1.2. Please refer to the work of EquiformerV2 (Liao et al., 2023) for details of thearchitecture. For training eSCN with DeNS, we use the same DeNS-related hyper-parameters as those fortraining EquiformerV2 for 20 epochs and the same module as force prediction to predict noise. V100 GPUs with 32GB are used to train models. We use 16 GPUs for training EquiformerV2 for 12epochs and eSCN on OC20 S2EF-2M dataset and use 32 GPUs for training EquiformerV2 for 20 and 30epochs. We train EquiformerV2 with 128 GPUs on OC20 S2EF-All+MD dataset. The training time and thenumbers of parameters of different models on OC20 S2EF-2M dataset can be found in (a). For OC20S2EF-All+MD dataset, the training time is 88982 GPU-hours and the number of parameters is 160M.",
  "C.1Training Details": "Different from OC20, all the atoms in a structure in OC22 are free, and we apply DeNS to all the freeatoms. We add an additional block of equivariant graph attention to EquiformerV2 for noise prediction.We follow the same practice as on OC20 and use direct methods for force prediction. We follow the samehyper-parameters not relevant to DeNS, and summarizes the hyper-parameters for the results onOC22 in . We use 32 V100 GPUs (32GB) for training. The training time is 5082 GPU-hours, and thenumber of parameters is 127M.",
  "Batch size128Number of epochs6Weight decay1 103": "Dropout rate0.1Stochastic depth0.1Energy coefficient E4Force coefficient F100Gradient clipping norm threshold50Model EMA decay0.999Cutoff radius ()12Maximum number of neighbors20Number of radial bases600Dimension of hidden scalar features in radial functions dedgep0, 128qMaximum degree Lmax6Maximum order Mmax2Number of Transformer blocks18Embedding dimension dembedp6, 128qf pLqijdimension dattn_hiddenp6, 64qNumber of attention heads h8f p0qijdimension dattn_alphap0, 64qValue dimension dattn_valuep6, 16qHidden dimension in feed forward networks dffnp6, 128qResolution of point samples R18",
  "D.1Additional Details of DeNS": "It is necessary that gradients consider both the original task and DeNS when updating learnable parameters,and this affects how we sample structures for DeNS when only a single GPU is used for training modelson the MD17 dataset. We zero out forces corresponding to structures used for the original task so thata single forward-backward propagation can consider both DeNS and the original task. In contrast, if weswitch between DeNS and the original task for different iterations, gradients only consider either DeNS or theoriginal task, and we find that this does not result in better performance on the MD17 dataset than trainingwithout DeNS.",
  "D.2Training Details": "We use the official implementation of Equiformer (Liao & Smidt, 2023) for experiments on the MD17 datasetand follow most of the original hyper-parameters for training with DeNS. Gradient methods are used topredict forces. For training DeNS, we use an additional block of equivariant graph attention for noiseprediction, which slightly increases training time and the number of parameters. The hyper-parametersintroduced by training DeNS and the values of energy coefficient E and force coefficient F on differentmolecules can be found in . Empirically, we find that linearly decaying DeNS coefficient DeNS to0 thoughout the training can result in better performance. For the Equiformer variant without attentionand layer normalization, we find that using normal distributions to initialize weights can result in trainingdivergence and therefore we use uniform distributions. For some molecules, we find training Equiformervariant without attention and layer normalization with DeNS is unstable and therefore reduce the learningrate to 3 104.",
  "D.3Additional Simulation-Based Results": "Following the work (Fu et al., 2023), we run simulations on the four molecules (i.e., Aspirin, Ethanol,Naphthalene and Salicylic Acid) and compare the two simulation-based metrics, which are stability anddistribution of interatomic distances hprq. We use the previously trained Equiformer for the simulations. Wenote that Equiformer models are trained on 950 examples for each molecule instead of 9, 500 as in Fu et al.(2023). The results are summarized in . Training with DeNS helps energy and forces MAE as wellas stability. The results of hprq are similar. For Naphthalene, training with DeNS improves stability from133.8{300 to 157.2{300. Both models become unstable when running simulations of Naphthalene, and wesurmise that is because we only use 950 examples for training instead of 9, 500. For others, the performancegain in simulation-based metrics is not significant since the original Equiformer trained on 950 examplesalready achieves similar results to other top-performing models trained on 9, 500 examples, suggesting thatthe potential room for improvement would be quite limited.",
  "EPseudocode for Force Encoding": "We provide the pseudocode for force encoding in Algorithm 1. Here the original node embedding xi containsonly the atom embedding xi,z. Note that the type-L vectors in xi,z are all zeros for L 0 since xi,z isobtained by applying an SOp3q linear layer to type-0 vectors. We directly add the force embedding xi,f tothe original node embedding to encode forces."
}