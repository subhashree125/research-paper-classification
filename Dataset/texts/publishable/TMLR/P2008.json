{
  "Abstract": "The world offers unprecedented amounts of data in real-world domains, from which we candevelop successful decision-making systems. It is possible for reinforcement learning (RL)to learn control policies offline from such data but challenging to deploy an agent duringlearning in safety-critical domains. Offline RL learns from historical data without accessto an environment. Therefore, we need a methodology for estimating how a newly-learnedagent will perform when deployed in the real environment before actually deploying it. Toachieve this, we propose a framework for conservative evaluation of offline policy learning(CEOPL). We focus on being conservative so that the probability that our agent performsbelow a baseline is approximately , where specifies how much risk we are willing toaccept. In our setting, we assume access to a data stream, split into a train-set to learn anoffline policy, and a test-set to estimate a lower-bound on the offline policy using off-policyevaluation with bootstrap confidence intervals. A lower-bound estimate allows us to decidewhen to deploy our learned policy with minimal risk of overestimation. We demonstrateCEOPL on a range of tasks as well as real-world medical data.",
  "Introduction": "Suppose someone else is controlling a sequential decision making task for you. This could be a person tradingstocks, a hand-coded controller for a chemical plant, or even a PID controller for temperature regulation.Offline reinforcement learning (RL) allows us to learn policies from historical data collected by some othercontroller. But when would one want to switch from the existing controller to the new policy? This decisionmay depend on the cost for continued data collection from the existing controller, your risk appetite, andyour confidence in the performance of the policy you have learned. This paper takes a critical step towardsthe question: how to (confidently) select the right time to deploy when learning offline, if we donot have access to the environment nor the policy generating the data? Offline reinforcement learning is a way to train off-policy algorithms using existing data. It presents a greatopportunity for learning data-driven policies without environment interaction. In safety-critical applicationssuch as in healthcare or autonomous driving, there is a large amount of data that we can use to learn RLpolicies and hence use for decision making (Gottesman et al., 2019). Learning with offline data is challengingbecause of the distribution mismatch between the data collected by the behavior policy that collected thedata, and the offline agent, which learns from data (Levine et al., 2020). What is even more challengingis evaluating offline agents in the offline RL setting if we assume no access to the environment; in some",
  "Published in Transactions on Machine Learning Research (9/2024)": "control (Fujimoto et al., 2019a), coupled with an auto-encoder for better state representation learning.Another important decision is to decide how often we will run our conservative evaluation methods becauseof its computational cost. For this, we train the policy for 200k epochs, where we evaluate the policy every10k epochs. To estimate the behavior policy b, which is referred to as the clinicians policy, we rely onbehavior cloning (Hanna et al., 2021) as done in other experiments on simulated environments.",
  "Data": "source :Setupforcontinualsafety-evaluation of offline RL: this figure illustratesthe loop of interaction between an offline agent anddata while the agent is trained, evaluated and de-ployed if it passes the safety test. Our target setup is summarized in : we havesome source of data, from which we request data sam-ples. At each step, data is split into training and test-ing. After each offline learning step, we test the pol-icy using conservative evaluation. A policy passes thesafety test if a lower-bound on the policys value is bet-ter than the value of the data distribution. We con-tinue the process of training/testing for a few itera-tions until the testing shows the policy can outperformthe existing controller with a confidence level . Wedynamically receive samples, continuously perform RLupdates, and continuously monitor a confidence intervalon the changing policy until it reaches a sufficient levelfor deployment.We hypothesize that our conserva-tive off-policy evaluation (COPE) is preferred over off-policy evaluation (OPE) because OPE may be prone tooverestimation, which is problematic for safety-criticalproblems. CEOPL acts as a workflow for offline learn-ing and evaluation in safety-critical domains. Contri-butions of this article are summarized as follows:",
  "use a direct model to lower IS variance. A few other methods improve upon this approach, such as weighteddoubly-robust (WDR) (Thomas & Brunskill, 2016), and MAGIC (Thomas & Brunskill, 2016)": "Gap in The LiteratureIn the offline RL literature, there is no standard way to evaluate a policy whilelearning offline; most of the literature still evaluates the performance of offline algorithms in the environmentor in a simulator, by running a policy online. A workflow for offline RL (Kumar et al., 2021) proposed a setof metrics to indicate how the algorithm can be further improved, such as detecting over-fitting and under-fitting, providing no clear workflow for estimating the policy performance. At the intersection of offline RLand OPE, there has been recent work combining these two areas. One paper discussed the applicability ofFQE (Le et al., 2019) to test the performance of offline policies under different hyper-parameters and selectthe best performance (Paine et al., 2020). Active offline policy selection (Konyushova et al., 2021) uses OPEto warm-start the evaluation process and choose which policy to evaluate online when given a limited budgetof online interactions.In benchmarks for deep off-policy evaluation (Fu et al., 2021), authors evaluateoffline policies learned on continuous control tasks using various OPE methods ranging from importancesampling, model-based methods, and doubly-robust estimators. They show how challenging OPE can be asan evaluation method (Fu et al., 2021). Even considering the previous work combining offline RL with OPE (Paine et al., 2020) (Fu et al., 2021),none discussed the feasibility of evaluating offline RL agents as we learn , and how much we can trust OPEas an approach for testing. The setup we are studying is quite different from the current literature becauseprevious work for off-policy evaluation assumed a behavior policy b and a target policy , where bothpolicies are fixed and may be related. For example, in a study for high-confidence OPE methods (Thomaset al., 2015a), the target policy is initialised as a subset of the behavior policy such that they are close toeach other. In another study for safe improvement (Thomas et al., 2015b), the Daedulus algorithm learnsa safe target policy as a continuous improvement over the behavior policy. This is quite relevant to ourwork; however, it is not purely offline learning because their algorithm uses data from an older version of thepolicy it currently improves to perform both the improvement step and safety tests. With the doubly-robustestimator, results are presented with different versions of such that is always a mixture of b and with different degrees to ensure their relevance (Jiang & Li, 2016).",
  ": Proposed workflow for CEOPL: thisfigure shows the methodology to follow, when givena source of data, to both learn a policy offline andestimate its value confidently": "Recent work on offline RL focuses on continuouscontrol in near-complex domains and environments(Levine et al., 2020). As detailed above, OPE liter-ature focuses on much simpler domains in discreteand continuous control (Voloshin et al., 2019). Noneof the previous work discussed the feasibility of us-ing conservative OPE to evaluate offline RL policies.We believe the setup we are tackling is under-studiedin the literature where the target policy is not nec-essarily similar to the data collection policy, whichis the case for offline RL.",
  "Methodology": "In this section, we introduce our framework CEOPLin detail with the methods used for both learningand evaluation of an offline policy. CEOPL couplesoffline RL algorithms with conservative off-policyevaluation as a feasible evaluation method for of-fline agents in safety-critical domains.Conserva-tive evaluation is defined by any off-policy evalu-ation method which seeks a high-probability lowerbound on the policys estimate. In CEOPL, we sam-ple data from a buffer of data dynamically at eachiteration, perform policy updates, and continuously",
  ": end while9: return , v()": "We refer to each offline policy learning method with and each off-policy evaluation method with .CEOPL is further explained in Algorithm 1. The inputs are a dataset of trajectories D, a confidence level appropriate for the problem in hand, offline policy learning method , and an off-policy evaluation method to combine with bootstrap confidence intervals, shown in Algorithm 2 in Appendix A.3. The output willbe a policy trained offline and a confidence lower-bound estimate of its return v(). Since the stoppingcondition in Algorithm 1 is not always satisfied if no evaluation method can detect an improved policy, weset a maximum number of iterations for training and evaluation in our experiments.",
  "Methodology: Offline Reinforcement Learning": "Is it possible to learn a good policy offline given data and no environment interaction? Earlier work (Agarwalet al., 2020) tried to answer this question, and showed how capable RL algorithms are in the offline setting,since they were able to learn a successful DQN agent totally offline when provided with diverse data. However,a benchmark on batch RL (Fujimoto et al., 2019a) showed how RL without correction can fail in the offlinesetting given ordinary data.",
  "Discrete ControlContinuous Control": "Off-policyDouble DQN (Hasselt et al., 2016)SAC (Haarnoja et al., 2018)Imitation LearningBehavioral Cloning (Bain & Sammut, 1999)Behavioral CloningOfflineBCQ (Fujimoto et al., 2019b)BCQ & BEAR (Kumar et al., 2019) In CEOPL, we investigate different techniques for learning from offline data, ranging from normal off-policyRL algorithms (off-policy), imitation learning techniques (imitation learning), and policy constraint methodsspecific for offline learning (offline). Imitation learning is a form of supervised learning and is good at learningfrom expert data or demonstrations. Policy constraint methods are designed to mitigate the challenges fornormal off-policy to learn offline, such as bootstrapping error (Levine et al., 2020). This categorization isinspired by the offline RL tutorial (Levine et al., 2020) and is shown in . Details about each algorithmare further explained in Appendix A.1. Given the data samples used from a data source, we aim to learn apolicy totally offline without environment interaction or access to the policy generating the data. This offlinepolicy is expected to be at least as good as the behavior policy in case of imitation learning and better inthe case of other algorithms, after a few iterations.",
  "Off-Policy Evaluation": "While a policy is being improved offline, we are interested in evaluating such a policy. Off-policy evaluationis a promising technique to evaluate a policy that is continuously learning given access to fixed data. Anoff-policy estimator is a method for computing an estimate v() for the true value of the target policyv() using trajectories D collected while following another policy b, which is what OPE methods do.When evaluating, we use new samples for testing each time to avoid the multi-comparison problem2. Thissampling is also to ensure that we do not over-fit our OPE estimates or tune training parameters to reducethe estimate error. shows the three categories of OPE methods, with examples under each category.The methods in bold are the ones we use for evaluation. For instance, we use Weighted Importance Sampling(WIS) (Mahmood et al., 2014), Direct Model-based (MB) (Hanna et al., 2017), and Weighted Doubly-Robust(WDR) Thomas & Brunskill (2016) estimators as shown. More details are further shown in Appendix A.2. : Different OPE methods categorized: This table shows the three main categories of OPEmethods, importance sampling, direct methods and hybrid methods with examples with the methods usedin our experimentation.",
  "Bootstrapping": "To conservatively evaluate in CEOPL, each OPE method is combined with bootstrapping to approximatea confidence lower bound of v() on v() such that v() v() with probability at least 1 . Inprinciple, bootstrapping can be replaced by any other method that provides tight lower bounds. Considera data sample D of n random variables Hj for j = 1, 2, ..., n where we can sample Hj from some i.i.d.",
  "Experiments": "This section discusses the experiments and the results of CEOPL on simulated setups in discrete and con-tinuous control tasks. In our experiments, two simulated environments (MountainCar-v0 and Pendulum-v0)were used to demonstrate CEOPL in discrete and continuous control, respectively. This choice shows howthe complexity of the environment, state and action dimensions, and horizon are affecting offline evaluation.To mimic the setup where there is a source of trajectories without access to the environment itself, we simu-late a source of data in each environment as a partially-trained policy that collects data of medium quality.Such data resulting from a policy that is not optimal nor random resembles real-world data and leaves roomfor improvement when training offline RL policies. For each environment, a policy is optimized offline usingthree different offline learning methods and evaluated simultaneously using three different off-policy evalua-tion methods. Following Algorithm 1, in each iteration, we get m trajectories from our data source which wesplit between training and testing. We use more data for testing than training since CEOPL requires largeamounts of data to estimate tight bounds. For each improvement method, we load the train split into thetraining buffer and do k training epochs where k varies among offline RL algorithms. Since we exceptionallyhave access to the true environment in these simulated domains, we compute the true value of the offlinepolicy by testing it in the actual environment and use it for comparison. For bootstrapping, we use = 0.05to get a 95% confidence lower-bound using B = 2000 bootstrap estimates, as recommended by practitioners(Efron, 1979). In weighted doubly-robust with bootstrapping 3.2, we use a value of B = 224 for the numberof bootstrap estimates to avoid heavy computation; this can still get us a good approximation as suggestedby MAGIC (Thomas & Brunskill, 2016). Since we have no access to b, we estimate b given the test data(Hanna et al., 2021) with a behavior cloning model that is improved gradually with more test data. Results on MountainCar-v0: For each iteration, 300 trajectories are sampled, where 20 trajectories areused into the training buffer and 280 trajectories are used for evaluation. This totals to 3000 trajectoriesover the 10 iterations. Each evaluation iteration includes performing k training epochs to the offline policy;a training epoch is a single optimization step of the policy over a batch of data. k is set to be 29 for BCQand Double DQN, while BC does 26 policy updates in one iteration (since BC was faster to optimize). shows how different offline policy improvement methods perform given medium-quality data withconservative evaluation, indicating which method can tell when v() > v(b). While the x-axis shows thenumber of evaluation iterations, the y-axis shows the value estimate of a policy across different iterations.Behavior cloning, as an offline policy improvement method, can only perform as well as b. Double DQNand BCQ were able to outperform b. The true value of a target policy is calculated as the average returnwhen running the policy in the actual environment (not possible in practice) for 1000 episodes. All reportedresults are an average of 40 runs, while the shaded area shows the standard error. The value of the behavioralpolicy, v(b), is the sum of undiscounted rewards of all trajectories in the data set.",
  "Evaluation Iterations": "v( ) : Bootstrapping Error Accumulation Reduction (BEAR) : CEOPL on Pendulum-v0: this figure shows the performance of the target policy as it istraining offline over iterations (x-axis).Weighted importance sampling (WIS) failed to detect that theoffline policy outperforms b. The model-based estimator (MB) and weighted doubly-robust (WDR) wereable to determine when an offline agent outperforms b. While the y-axis shows the estimated policy valuev(), we show the empirical estimate v(b) for the behavioral policy (given the dataset), and the conservativeoff-policy estimate v() across different estimators.",
  "Overestimation of OPE vs. Conservative OPE:": "It is important to question whether conservative evaluation in our proposed framework is worth the addedcomplexity and computation of bootstrapping over the vanilla off-policy evaluation (OPE). Although bothOPE and conservative OPE can provide value estimates of an offline target policy, conservative OPE lower-bound estimates underestimate the value of the offline policy which serves our purpose of ensuring safety.Moreover, most existing OPE estimators are prone to overestimating the value of the policy Thomas et al.(2015a). Empirical results on simulated environments showed how OPE estimates can overestimate the truevalue of the offline policy, as shown in . Even if not all OPE estimators are overestimating the truevalue of the offline policy in , there are no guarantees on the performance of the reported OPEestimators. On the other hand, lower bounds in CEOPL are guaranteed to only overestimate the true valueof the offline policy within the allowable 5% error rate for a 95% confidence, as shown in in AppendixB.2. Bootstrapping has strong guarantees as the size of test data goes to , but it lacks guarantees for finitesamples (Hanna et al., 2017); this is because it assumes that the bootstrap distribution is representative ofthe true distribution of the statistic of interest. As a result of this assumption, bootstrapping is considered",
  "WDR6.18 1.4517.642 2.7912.81 2.42": ": Mean error between CEOPL estimates and true value of on MountainCar-v0: thistable shows the mean error of conservative OPE estimates, v(), over all training iterations and multipleruns given each offline improvement method. Lowest mean errors for each method are shown in bold., andstandard error over 40 runs is reported.",
  "WDR161.1 13.47307.11 37.354.75 10.43": ": Mean error between CEOPL estimates and true value of on Pendulum-v0: this tableshows the mean error of conservative OPE estimates, v(), over all training iterations and multiple runsgiven each offline learning method. Lowest mean errors for each method are shown in bold, and standarderror over 40 runs is reported. SAC is not reported as it failed to learn a good policy. semi-safe because the assumption it makes may be false. Prior work Kostrikov & Nachum (2020) identifiedconditions, such as sufficient data size and sufficient coverage, under which statistical bootstrapping isguaranteed to yield correct confidence intervals. Other work (Morrison et al., 2007) shows that bootstrappingis still safe enough for high-risk predictions with a known record of producing accurate confidence intervals.Therefore, it is better to rely on CEOPL for conservative evaluation in safety-critical applications.",
  "What affects CEOPL?": "If we are to apply CEOPL to any problem, OPE methods and offline RL methods are different factorsaffecting the performance.To investigate this, we do further analysis and report total variation (TV)distance, a measure of similarity between two probability distributions; this is to check how the learningmethod of an offline policy is affecting TV distance between b and and hence affecting the off-policyestimates. Results are shown in in Appendix B.3, where the total variation distance between theoffline policy and the estimated behavior policy b across the different offline learning methods is reported.For discrete control, behavioral cloning (imitation learning) can achieve a much lower distance than otherimprovement methods (batch-constrained Q-learning and double DQN) whether they are constrained ornon-constrained. This is because behavioral cloning forces its target policy to be close to the behavior policywhile other methods do not. The same applies to continuous control in Pendulum-v0; behavior cloningachieves the lowest error between b and . This insight on divergence explains why weighted importancesampling achieves the lowest error between the estimate and the true value in the case of behavioral cloning,as shown in Tables 3 and 4; the error grows for other methods that do not constrain the policy to be closeto the data distribution. This analysis suggests that the choice of a OPE method is dependant on the offlinemethod chosen to optimize the policy. Other factors affecting the performance include the horizon of thetrajectory and the environment dynamics, which we discuss further in Appendix B.3. Takeaways: the performance of conservative off-policy evaluation, and OPE in general can be affected bythe following factors: a) divergence between b and as discussed earlier in .1, b) the horizonof the trajectory, c) the environment dynamics. The horizon of the trajectory affects importance samplingsuch that it suffers from high variance with longer horizons (Liu et al., 2020). The environment dynamicsalso affect direct and hybrid methods. The MB estimator performs well when it is possible to model theenvironment dynamics such as the case for both MountainCar-v0 and Pendulum-v0 environments.TheWDR estimator also performs well because q-values can get more accurate when it is easy to learn a modelof the environment. However, the WDR estimator is still affected by the divergence between b and as",
  "In short-horizon environments, we should rely on importance sampling methods when the offlinepolicy algorithm is constrained to be similar to b": "Based on our empirical analysis in subsections 5.1 and 5.2, we conclude that conservative off-policy estimatorsare considered a safe evaluation method for offline learning with enough confidence that minimizes the riskof overestimating the true performance of an offline policy. In contrast, off-policy evaluation suffers fromoverestimation to the true values so it makes sense to rely on conservative evaluation in CEOPL instead,for domains where safety is essential. Further, our ability to accurately detect an improved policy withCEOPL is a function of different factors: the offline learning algorithm, the environment dynamics and thetrajectories horizon.",
  "Problem: Sepsis Treatment": "Problems in healthcare can be considered a form ofsequential decision making, such as clinical physi-cians making decisions about the best next stepto take in care (Ghassemi et al., 2019).Currentadvances in AI can provide personalized care thatis equal to or better than humans by finding anoptimal decision making policy given clinical data(Ghassemi et al., 2019). Sepsis, a severe infectionwith organ failure, is a leading cause of mortality inintensive care units (ICUs) in hospitals (Sakr et al.,2018). There are continuous efforts in research tounderstand and cure sepsis and, hence, increase thesurvival rates for ICU patients. RL for sepsis treat-ment is preferred over supervised learning becausethe ground truth of good treatment strategy is un-clear in the current medical literature (Marik, 2015). Sepsis data is a form of offline data, from which wecan learn an offline policy given no access to an environment. The challenge lies in evaluating such policieswhile learning to determine when it is good enough to be deployed. We use the well-known MIMIC III data(Johnson et al., 2016) for sepsis treatment. Current literature in RL for health (Raghu et al., 2017a;b) findsit challenging to evaluate policies learned from such data given no agent-environment interaction. For this problem, given no further data collection, it is better to rely on offline RL methods (rather thanonline RL) to learn a policy from a fixed dataset. A recent work (Killian et al., 2020) explores differentrepresentation learning techniques to learn a good policy offline with discrete BCQ (Fujimoto et al., 2019a).To evaluate their policies, they rely on Weighted Importance Sampling (Mahmood et al., 2014) to comparethe different representation learning techniques. The problem at hand is widely researched but, only thisprevious work (Killian et al., 2020) relied on offline learning with off-policy evaluation. While relying onimportance sampling for evaluation is good when comparing different approaches, we believe it is essential",
  "Experimental Design": "We would like to perform conservative evaluation for offline policy learning and see if we can trust our RLpolicy to make decisions in the real-world. To apply CEOPL, there are a few design decisions to makefirst. These decisions include: how much data to use for training/testing, bootstrapping size, confidencelevel, and choice of algorithm to optimize a policy offline. Given that the available data is fixed, we splitthe dataset once in the beginning into a train set, used to train the offline policy, and a test set, used forevaluating the policy performance during training. Note that this fixed batch setup is different from whatwe followed in where we assumed a growing dataset. With stratified sampling (Killian et al., 2020),we use a 70/30 train/test split, which maintains the same proportions of each terminal outcome (survival ormortality). Data description and experimental details are shown in Appendix C.1. Unlike simulated environments in , we do not have a way in this setup to evaluate how a policyperforms except for using off-policy estimators. With CEOPL, we use weighted importance sampling (WIS),model-based estimator (MB), and weighted doubly-robust estimator (WDR) with bootstrapping to providelower-bound estimates v() for . We also report v() using the same estimators without bootstrapping,as the OPE estimate. Data is split between training and evaluation. Training data is used to optimize aBCQ agent while testing data is used for evaluation with conservative OPE methods.3 With evaluation, wecheck if the agent is good enough to deploy in the real-world, and if not, we need to continue improving ouroffline agent before deploying. For this problem, we hypothesize that importance sampling (Precup et al.,2000) will provide good estimates given the short horizon and the offline policy constraint method used forimprovement, unlike the MB estimator which will find transition dynamics for this problem challenging.",
  "Results": "For sepsis data, there is no ground-truth to compare against, as opposed to the simulated environments wepresented in . While learning our offline policy, we would like to see how well it performs over timeand when it outperforms the behavior policy so we can deploy it into the real world. shows the results of applying CEOPL to train and evaluate an offline RL policy, as an average of10 runs with the standard error indicated. The value of the behavior policy, v(b), is the average returnof the data in hand. With WIS, the conservative estimate v() and the mean estimate v() show theimprovement of the policy over training iterations. The mean OPE estimate of WIS slightly outperforms thebehavior policy but this can be an overestimate of the true value. We focus on our conservative estimate,which did not show that the target policy outperforms the behavior policy. The conservative estimates bythe model-based estimator (MB) failed to detect the improvement of the policy over time. This has todo with failing to model the complex dynamics of the dataset; as a result, it failed to estimate the truevalue of the target policy. In a model-based estimator, we model all the environment dynamics, the nextstate, reward, and terminal state. In MountainCar-v0, we only modeled the next state since the reward andterminal state can be computed deterministically from the current state. In Pendulum-v0, the environmentalways terminates at the maximum horizon, so we only modeled the next state and reward. With sepsisdata, it was hard enough to model the next state because the state dimension is large; it is also hard tomodel the terminal state indicating when a patient dies and whether they survive or not. With weighted doubly-robust estimator, values of v() and v() can show the improvement of the offlinepolicy as it is training, until both estimates outperform v().WDR combines per-decision weightedimportance sampling with an approximate model; the approximate model only fits q-values so it does notsuffer from the modeling complexity as the MB estimator. However, we cannot rely on the mean estimatesv() as they may overestimate. For real-world sepsis data, we suggest that relying on OPE only is notenough for such critical application, and it is better to rely on CEOPL for conservative lower-bound estimates 3For this case study, we face the multi-testing problem in this experiment as we used the same data split for each evaluationiteration; this is because the size of the test data is too small to split among iterations.",
  ": Results of CEOPL on sepsis data: this figure shows results of evaluating the performance ofthe offline policy. We show both the conservative off-policy estimate v(), and mean OPE estimate v()": "because it will be less likely to overestimate. As for the choice of an evaluation method, WIS can be reliablefor this problem given the short horizon but WDR can provide better estimates as a hybrid method thatimproves upon importance sampling. It is challenging for the MB estimator to detect an improved policygiven the complex environment dynamics.",
  "Conclusion": "This paper proposed a framework for evaluating offline RL methods in a conservative manner (CEOPL),combining OPE with bootstrap confidence intervals. First, we studied the feasibility of different categoriesof conservative off-policy estimators and how they are affected by offline learning algorithms. We suggestthat conservative evaluation is better for safety-critical applications since off-policy evaluation may sufferfrom overestimation. We tested CEOPL on real-world medical data for sepsis treatment. While dynamicallyreceiving data, we optimize offline RL agents and run safety tests to estimate a lower-bound on the valueof the offline policy and control the risk of overestimating its true value. This is essential for safety-criticalapplications to be able to tell when it is safe enough to deploy a new policy. In conclusion, conservativeevaluation with CEOPL proved to be a reliable evaluation method for offline agents that do not have accessto a simulated environment. When combined with bootstrapping, direct and hybrid OPE methods can betrusted for evaluating offline agents. The horizon of the trajectory and the environment dynamics affectthe choice for the off-policy estimator to rely on. In future work, we plan to tackle learning offline frommultiple data sources with different qualities and explore how the quality of the data affects offline learningand conservative evaluation. In addition, we can explore other OPE methods such fitted Q-evaluation (Leet al., 2019) to eliminate the challenges of direct model-based evaluation, and explore recent approachesfor conservative evaluation such as HAMBO (Rothfuss et al., 2023) which are more reliable in continuousstate-action spaces.",
  "Broader Impact Statement": "We do not foresee any potential negative impacts of this research that users need to be aware of. Additionally,our framework provides a safe way to perform evaluation of offline RL methods. We have showcased a real-world application of our proposed framework in healthcare, where offline RL can be utilized safely, and hencehave a positive impact. Part of this work has taken place in the Intelligent Robot Learning (IRL) Lab at the University of Alberta,which is supported in part by research grants from the Alberta Machine Intelligence Institute (Amii); aCanada CIFAR AI Chair, Amii; Digital Research Alliance of Canada; Huawei; Mitacs; and NSERC.",
  "Scott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without exploration.In International conference on machine learning, pp. 20522062. PMLR, 2019b": "Marzyeh Ghassemi, Tristan Naumann, Peter Schulam, Andrew Beam, Irene Chen, and Rajesh Ranganath.Practical guidance on artificial intelligence for health-care data. The Lancet Digital Health, 1:e157e159,08 2019. doi: 10.1016/S2589-7500(19)30084-6. Omer Gottesman, Fredrik Johansson, Matthieu Komorowski, Aldo Faisal, David Sontag, Finale Doshi-Velez,and Leo Celi. Guidelines for reinforcement learning in healthcare. Nature Medicine, 25:1618, 2019. Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximumentropy deep reinforcement learning with a stochastic actor. In Jennifer Dy and Andreas Krause (eds.),Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Ma-chine Learning Research, pp. 18611870. PMLR, 1015 Jul 2018. URL Josiah P. Hanna, Peter Stone, and Scott Niekum. Bootstrapping with models: Confidence intervals for off-policy evaluation. In Proceedings of the 16th Conference on Autonomous Agents and MultiAgent Systems,AAMAS 17, pp. 538546, Richland, SC, 2017. International Foundation for Autonomous Agents andMultiagent Systems.",
  "Josiah P Hanna, Scott Niekum, and Peter Stone. Importance sampling in reinforcement learning with anestimated behavior policy. Machine Learning, 110(6):12671317, 2021": "Hado van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double q-learning. InProceedings of the Thirtieth AAAI Conference on Artificial Intelligence, AAAI16, pp. 20942100. AAAIPress, 2016. Natasha Jaques, Asma Ghandeharioun, Judy Hanwen Shen, Craig Ferguson, gata Lapedriza, Noah Jones,Shixiang Gu, and Rosalind W. Picard. Way off-policy batch deep reinforcement learning of implicit humanpreferences in dialog. CoRR, abs/1907.00456, 2019. URL",
  "Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114,2013": "Matthieu Komorowski, Leo Celi, Omar Badawi, Anthony Gordon, and Aldo Faisal. The artificial intelligenceclinician learns optimal treatment strategies for sepsis in intensive care. Nature Medicine, 24, 11 2018.doi: 10.1038/s41591-018-0213-5. Ksenia Konyushova, Yutian Chen, Thomas Paine, Caglar Gulcehre, Cosmin Paduraru, Daniel J Mankowitz,Misha Denil, and Nando de Freitas.Active offline policy selection.Advances in Neural InformationProcessing Systems, 34:2463124644, 2021.",
  "Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning: Tutorial, review,and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020": "Yao Liu, Pierre-Luc Bacon, and Emma Brunskill. Understanding the curse of horizon in off-policy evaluationvia conditional importance sampling. In International Conference on Machine Learning, pp. 61846193.PMLR, 2020. A. Rupam Mahmood, Hado P van Hasselt, and Richard S Sutton.Weighted importance sampling foroff-policy learning with linear function approximation.In Z. Ghahramani, M. Welling, C. Cortes,N. Lawrence, and K. Q. Weinberger (eds.), Advances in Neural Information Processing Systems, vol-ume 27. Curran Associates, Inc., 2014.URL",
  "P E Marik. The demise of early goal-directed therapy for severe sepsis and septic shock. Acta AnaesthesiolScand, 59(5):561567, February 2015": "Alanna C. Morrison, Lance A. Bare, Lloyd E. Chambless, Stephen G. Ellis, Mary Malloy, John P. Kane,James S. Pankow, James J. Devlin, James T. Willerson, and Eric Boerwinkle. Prediction of CoronaryHeart Disease Risk using a Genetic Risk Score: The Atherosclerosis Risk in Communities Study. AmericanJournal of Epidemiology, 166(1):2835, 04 2007.ISSN 0002-9262.doi: 10.1093/aje/kwm060.URL Ofir Nachum, Yinlam Chow, Bo Dai, and Lihong Li.Dualdice:Behavior-agnostic estimation of dis-counted stationary distribution corrections. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 32.Curran Associates, Inc., 2019a.URL",
  "Aniruddh Raghu, Matthieu Komorowski, Imran Ahmed, Leo Celi, Peter Szolovits, and Marzyeh Ghassemi.Deep reinforcement learning for sepsis treatment. arXiv preprint arXiv:1711.09602, 2017a": "Aniruddh Raghu, Matthieu Komorowski, Leo Anthony Celi, Peter Szolovits, and Marzyeh Ghassemi. Contin-uous state-space models for optimal sepsis treatment: a deep reinforcement learning approach. In MachineLearning for Healthcare Conference, pp. 147163. PMLR, 2017b. Martin Riedmiller. Neural fitted q iteration first experiences with a data efficient neural reinforcementlearning method. In Joo Gama, Rui Camacho, Pavel B. Brazdil, Alpio Mrio Jorge, and Lus Torgo(eds.), Machine Learning: ECML 2005, pp. 317328, Berlin, Heidelberg, 2005. Springer Berlin Heidelberg.ISBN 978-3-540-31692-3. Jonas Rothfuss, Bhavya Sukhija, Tobias Birchler, Parnian Kassraie, and Andreas Krause.Hallucinatedadversarial control for conservative offline policy evaluation.In Robin J. Evans and Ilya Shpitser(eds.), Proceedings of the Thirty-Ninth Conference on Uncertainty in Artificial Intelligence, volume216 of Proceedings of Machine Learning Research, pp. 17741784. PMLR, 31 Jul04 Aug 2023.URL Yasser Sakr, Ulrich Jaschinski, Xavier Wittebole, Tamas Szakmany, Jeffrey Lipman, Silvio A amendysSilva, Ignacio Martin-Loeches, Marc Leone, Mary-Nicoleta Lupu, Jean-Louis Vincent, and ICON Inves-tigators. Sepsis in Intensive Care Unit Patients: Worldwide Data From the Intensive Care over NationsAudit. Open Forum Infectious Diseases, 5(12), 11 2018. ISSN 2328-8957. doi: 10.1093/ofid/ofy313. URL ofy313.",
  "Philip Thomas and Emma Brunskill. Data-efficient off-policy policy evaluation for reinforcement learning.In International Conference on Machine Learning, pp. 21392148. PMLR, 2016": "Philip Thomas, Georgios Theocharous, and Mohammad Ghavamzadeh.High-confidence off-policy eval-uation.Proceedings of the AAAI Conference on Artificial Intelligence, 29(1), Feb. 2015a.doi:10.1609/aaai.v29i1.9541. URL Philip Thomas, Georgios Theocharous, and Mohammad Ghavamzadeh. High confidence policy improvement.In Francis Bach and David Blei (eds.), Proceedings of the 32nd International Conference on MachineLearning, volume 37 of Proceedings of Machine Learning Research, pp. 23802388, Lille, France, 0709 Jul2015b. PMLR. URL",
  "Yifan Wu, George Tucker, and Ofir Nachum. Behavior regularized offline reinforcement learning. arXivpreprint arXiv:1911.11361, 2019": "Mengjiao Yang, Ofir Nachum, Bo Dai,Lihong Li,and Dale Schuurmans.Off-policy evaluationvia the regularized lagrangian.In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, andH. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 65516561. Cur-ran Associates, Inc., 2020. URL Shangtong Zhang, Bo Liu, and Shimon Whiteson. GradientDICE: Rethinking generalized offline estimationof stationary values.In Hal Daum III and Aarti Singh (eds.), Proceedings of the 37th InternationalConference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pp. 1119411203. PMLR, 1318 Jul 2020. URL",
  "A.1Offline Reinforcement Learning": "As mentioned in .1, we categorized learning methods into three categories: normal off-policy RLalgorithms (off-policy), imitation learning techniques (imitation learning), and policy constraint methodsspecific for offline learning (offline). For continuous control, we describe the approaches in the three cate-gories. For the off-policy category, we used soft actor-critic (SAC) (Haarnoja et al., 2018); this means a SACagent is optimized offline using a fixed buffer of data without interaction with the environment or controlover the buffer. For the imitation learning category, we study Behavioral Cloning (BC) (Bain & Sammut,1999); BC is a supervised learning algorithm where the model learns how to predict actions from the currentstate. In the offline category, we study batch-constrained Q-learning (BCQ) (Fujimoto et al., 2019b) and bootstrap-ping error accumulation reduction (BEAR) (Kumar et al., 2019). Batch-constrained Q-learning (Fujimotoet al., 2019b) argue that off-policy algorithms often fail in the offline setting due to the extrapolation errorsince unseen state-action pairs will have unrealistic values (Fujimoto et al., 2019b). This is a result of themismatch between the state-action visitation of the current policy and the state-action pairs in the offlinedataset. To solve this, the policy should induce a similar state-action visitation to the offline data. BCQ(Fujimoto et al., 2019b) proposed the use of a state-conditioned generative model to produce only likely ac-tions to the current offline data. Then, this generative model, effectively a variational auto-encoder (Kingma& Welling, 2013), is combined with a network which aims to optimally perturb the generated actions in asmall range. When this is combined with the Q-network, the network will only select the highest valuedactions similar to the data in the batch (Fujimoto et al., 2019b). BEAR (Kumar et al., 2019) argues that the source of instability for off-policy algorithms learning offline isthe bootstrapping error. This error results from bootstrapping with actions that lie outside of the trainingdata distribution, and it accumulates via the bellman backup operator (Kumar et al., 2019).BEAR isbuilt on top of any actor-critic algorithm, such as SAC (Haarnoja et al., 2018), by modifying the policyimprovement step to use a distribution-constrained backup. To apply the constraint, the sampled version ofmaximum-mean discrepancy is used between the unknown behavior policy b and the current actor toconstrain the distribution of the actor to the support of the behavior policy (Kumar et al., 2019). Hence,BEAR allows the actor to maximize the Q-function while being constrained to remain in the valid supportspace of the behavior policy defined by the data samples (Kumar et al., 2019). For discrete control, we use Double DQN (Hasselt et al., 2016) as a normal off-policy RL algorithm; similarto SAC, Double DQN is used to learn solely from offline data. For the imitation learning category, we usebehavioral cloning (BC) (Watkins & Dayan, 1992) in a supervised learning manner with cross-entropy loss.In the offline category, we experiment with the discrete version of BCQ (Fujimoto et al., 2019a). DiscreteBCQ is much simpler than its continuous version, since it trains Q-learning with a constrained argmaxoperator. This only allows actions in the backup with probability, given by the generative model, abovesome threshold . The generative model is a behavioral cloning network trained in standard supervisedlearning with a cross-entropy loss (Fujimoto et al., 2019a). BCQ is also based on Double DQN, and we usethe threshold to be = 0.3 as reported in their original paper. If the threshold is 1, the algorithm returnsan imitator of all the actions in the dataset, while a threshold = 0 returns the Q-learning objective.",
  "A.2Off-Policy Evaluation": "Importance Sampling(Precup et al., 2000) is a method for handling mismatch between distributionsand hence presented as a consistent and unbiased off-policy estimator. For a trajectory H b of length L,as H = s1, a1, r1, .., sL, aL, rL, we can define the importance sampling up to time t for policy as follows:",
  "iL1mj=1 jL1g(Hi)(2)": "Direct Model-Based Estimatoris another off-policy estimator that falls under direct methods. Themodel-based off-policy estimator (MB) computes v() by building a model using all the available trajectoriesD to build a model M = (S, A, P, r, , d0) where P and d0 are estimated with trajectories sampled from b.Then, MB will compute v() as the average return of trajectories simulated in the estimated model M whilefollowing . Despite having lower variance than IS methods, MB is a biased estimator for: 1) we lack datafor particular state-action pairs, so we assume their transition probabilities, 2) we assume that the modelclass includes the true model for the transitions. As a result, as n , the model estimates may convergeto a value different from the true v(). This effect is due to its dependency on the modeling assumptionswe make whether we assume a linear or a non-linear model. Therefore, the model-based estimator is combined with bootstrapping, as discussed in .2, for con-servative off-policy evaluation. To clarify, as mentioned in Section A.1, we rely mainly on model-free policyoptimization algorithms for learning a policy offline. However, we use model-based estimation to evaluate apolicy, which is independent from the policy optimization part. Weighted Doubly-Robust Estimator(Thomas & Brunskill, 2016) is a hybrid method for off-policyestimation, presented as an extension to the doubly-robust (DR) method (Jiang & Li, 2016). DR is anunbiased estimator of v() that uses an approximate model of the MDP to reduce the variance of importancesampling (Jiang & Li, 2016). Although biased, WDR is based on per-decision weighted importance sampling(PDWIS) and improves upon the DR method as it balances the bias-variance trade-off. In addition, theapproximate model value functions act as a control variate for PDWIS.",
  "t=0t(witq(Sit, Ait) wit1vpi(Sit))(4)": "Similar to direct model-based methods, we use bootstrapping with WDR to provide a confidence lower-bound estimate on v(). WDR with bootstrapping is guaranteed to converge to the correct estimate asn increases, given the statistical consistency of PDWIS (Hanna et al., 2017). For the approximate model,a single model is estimated with the available trajectories D, then used to compute the value functions ofWDR for each bootstrap data. We choose the weighted doubly robust estimator to represent OPE hybridmethods in our study.",
  "B.1Simulated Environments": "MountainCar-v0: we use discrete MountainCar (Sutton & Barto, 2018) with a continuous state (velocityand position) and 3 possible discrete actions. At each time-step, the reward is 1, except for the terminalstate when it is 0. However, we used the modified version of Mountain-Car as described here (Thomas,2015). The horizon is shortened by holding an action at constant for 4 updates of the environment state.We also change the start state such that an episode starts with a random position in the range of (-1.2, 0.6)and random velocity in the range of (-0.07, 0.07) (Jiang & Li, 2016; Thomas, 2015). The data collector weuse for this environment is an online actor-critic (Sutton & Barto, 2018) agent that is partially trained withadded 30% randomization when generating data. This means when collecting the offline data, an agent takesa random action 30% of the time instead of following the online policy, to include exploratory transitions. Pendulum-v0: inverted pendulum swing-up problem is a classic problem in the control literature withone continuous action (Brockman et al., 2016). In this version of the problem, the pendulum starts in arandom position, and the goal is to swing it up to stay upright. We also shorten the horizon following thesame procedure as done in MountainCar (Thomas, 2015) by holding an action constant for 4 updates of theenvironment state. This limits the horizon of the environment to 50 instead of 200. The data source we usefor this environment is a soft actor-critic (Haarnoja et al., 2018) agent that is trained partially, with added30% randomization when generating the dataset. This means when collecting the offline data, an agent takesa random action 30% of the time instead of following the online policy to resemble real-world data.",
  "B.2Overestimation of CEOPL": "shows the overestimation of lower bounds compared to the true value of the offline policy; it isguaranteed to remain within the allowable = 5% error rate for a 95% confidence. We would like to pointout that the safety guarantee for the confidence bound specified is per-iteration (Thomas, 2015). If we wantto consider this guarantee over multiple iterations, the probability 1 decreases for the early iterations aswe perform more tests. As a result, we should only consider this guarantee for the last iteration of our loop,such that the error rate in the last iteration is at most .",
  "B.3Further Analysis": "To better understand how evaluation is affected by the offline learning method, multiple measures canshow the similarity between two probability distributions (as a policy is a distribution over actions). Onesimple measure is the total variation (TV) distance. TV distance measures the difference between actionprobabilities taken under two policies given the data set in hand. TV would be the sum of differences in",
  "(c) Batch-Constrained Q-learning (BCQ)": ": The empirical error rate on MountainCar-v0 with CEOPL: This figure shows the empiricalerror rate on MountainCar given different evaluation methods. The lower bound is computed z times foreach method (z = 40 for Mountain Car) and we count how many times the lower bound is above the truev() computed in the true environment. Given 200 trajectories only for evaluation, all methods correctlyapproximate the allowable 5% error rate for a 95% confidence lower bound. However, there are two instancesof WIS and WDR that slightly exceed 5%, which can be mitigated with more data. probabilities between the behavior policy b and the target policy for each state-action pair in the testdataset. Since b is not known, we estimate b given the test data (Hanna et al., 2021). TV distance iscorrelated to KL-divergence, showing how two policies are different from each other. As discussed in .2 and shown in , in MountainCar-v0 (discrete control), behavior cloning (BC) learns a policy thatis very similar to b. Discrete BCQ and double DQN do not limit the distance with the behavior policy, andtheir TV distance is quite large as opposed to BC. For Pendulum-v0 (continuous control), behavior cloning",
  "(b) Pendulum-v0 (continuous control)": ": Divergence between b & over training iterations: this figure shows the total variation(TV) distance between b & on MountainCar-v0 & Pendulum-v0 over all training iterations. For bothdiscrete (a) and continuous control (b), BC achieves the lowest distance since it tries to mimic the datadistribution. In (a), Double DQN and BCQ policies achieve similar distance, but higher compared to BC.(b), BEAR achieves a higher distance than BC, but much less than BCQ; this is because BEAR enforcesexplicit divergence minimization between b and .",
  "C.1Experimental Details": "To build a decision-making policy for the treatment of septic patients, we use data from the Medical Informa-tion Mart for Intensive Care (MIMIC-III) dataset (v1.4) (Johnson et al., 2016). We follow previous relevantwork (Komorowski et al., 2018) to extract and preprocess vital and lab measurements and build a cohort of19418 patients; this cohort has an observed mortality rate just above 9% (determined by death within 48hof the final observation). Data extraction, preprocessing, and splitting follow previous work (Killian et al.,2020). A dataset D is a collection of trajectories; a trajectory H of a non-fixed horizon L refers to a single patientthat ends by the survival or death of a patient, with a maximum horizon L of 19. A transition is composedof current observation Ot, the action taken by the clinician at to move to the next observation Ot+1 receivingreward of rt until a patient survives or dies. We treat the problem as a markov decision process (MDP). Dataconsists of a continuous state space of a dimension 38 and a discrete action space of 25 actions. Observedactions are the administration of fluids or vasopressors that can be given, categorized by volume, and putinto 5 discrete bins per action type resulting in 25 actions (Killian et al., 2020). The data has sparse rewardsuch that a trajectory has a terminal reward of +1 if a patient survived, -1 if a patient died, and 0 otherwise.The observations used as our state space are a combination of time-varying continuous features (33), suchas heart rate, glucose, etc., and demographic features (5), such as gender, age, weight, etc. (Killian et al.,2020). For the bootstrapping size, we use B = 2000 as in the experiments with simulated data. We also use aconfidence level = 0.05. We optimize a policy offline using batch-constrained Q-learning (BCQ) for discrete"
}