{
  "Abstract": "Data valuation plays a pivotal role in ensuring data quality and equitably compensatingdata contributors. Existing game-theoretic data valuation techniques mostly rely on theavailability of a high-quality validation set for their ecacy. However, the feasibility of ob-taining a clean validation set drawn from the test distribution may be limited in practice.In this work, we show that the choice of validation set can signicantly impact the naldata value scores. In order to mitigate this, we introduce a general paradigm that convertsa traditional validation-based game-theoretic data valuation method into a validation-freealternative. Specically, we utilize the cross-validation error as a surrogate for to evaluatethe models performance on a validation set. As computing the cross-validation error can becomputationally expensive, we propose using the cross-validation error of a kernel regressionmodel as an eective and ecient surrogate for the true performance score on the popula-tion. We compare the performance of the validation-free variant of existing data valuationtechniques with their original validation-based counterparts. Our results indicate that thevalidation-free variants generally match or often signicantly surpass the performance oftheir validation-based counterparts.",
  "Introduction": "Data valuation aims to measure the contribution of individual data instances to the training of machinelearning models. The task of data valuation is crucial not only in data marketplaces where it ensures equitablecompensation for data providers, but also in the realm of explainable machine learning - where it identiesinuential training data points that are responsible for certain model behavior. The importance of datavaluation research is underlined by legal eorts such as the DASHBOARD Act (Foster, 2019) and DataDividend Project (Project, 2020), mandating companies to assess the economic value of user data. The Shapley value is a well-known solution concept in cooperative game theory (CGT) that fairly dividestotal revenue among players. Originally proposed by Jia et al. (2019b) and Ghorbani & Zou (2019), theShapley value has become one of the most popular approaches for data valuation. Specically, each trainingdata point is considered a player in a coalitional game, and the Shapley value is used to fairly evaluate thecontribution of each player. The technique is often termed Data Shapley. Other CGT-based approacheshave been developed, including Beta Shapley (Kwon & Zou, 2021), Data Banzhaf (Wang & Jia, 2023a), etc.The primary reason for the success of Data Shapley and the other CGT-based approaches is their axiomaticformulations, which intuitively resonate with the fairness requirements inherent in data valuation. They havebeen adopted in various use cases such as improving dataset quality (Tang et al., 2021; Pandl et al., 2021),incentive mechanism design (Liu et al., 2020; Wei et al., 2020), data debugging (Karla et al., 2022). A key issue that is often overlooked in data valuation research is the dependency on a well-curated valida-tion data. The choice of validation data can often greatly impact the outcome of data valuation, potentially",
  ": Overview of our proposed approach vs. traditional data valuation approaches": "leading to varying results. Moreover, procuring a clean, representative validation set is often a challengingtask, and in many real-world scenarios, the size of the available validation set is limited. In addition, datavaluations sensitive nature, specically concerning prot sharing, introduces additional complexities. If cer-tain individuals possess insider knowledge about the validation set, they might strategically optimize theircontribution to the training data. This could potentially skew the prot distribution in their favor, therebyintroducing unfairness. In this paper, we uncover the risks of using validation data for data valuation, and propose a generalparadigm that converts the many existing validation-dependent data valuation approaches into validation-free approaches. Our technical contributions are listed as follows: Uncovering the potential vulnerabilities of validation-based data valuation techniques. Througha series of experiments, we demonstrate that the choice and quality of the validation set can signicantlyaect data values. The size of the validation set directly impacts the ranking of these values. Moreover, weshow that even minor class-imbalance within the validation set can substantially alter data values. Finally,we highlight that these valuation systems can be gamed and become unfair if a malicious data provider gainsaccess to information about the validation set. Our ndings underscore the need for caution when usingthese techniques, due to the potential for signicant variations and vulnerabilities.",
  "Avalidation-freeparadigmfordatavaluationusingLeave-One-OutCross-Validation": "(LOOCV). Recognizing the limitations of validation-based data valuation techniques, we propose a novelvalidation-free approach using Leave-One-Out Cross-Validation (LOOCV) to estimate performance scores onthe population. Cross-Validation (CV) is a classic technique for estimating model performance scores whena validation set is unavailable. LOOCVs deterministic nature makes it an ideal alternative utility functionin data valuation. To reduce the computational demands associated with LOOCV, we propose to use Reg-ularized Least Squares (RLS) as an ecient proxy model tailored for validation-free data valuation. This isdue to the special property of RLS in having a computational shortcut for LOOCV. While proxy modelsare frequently being used to overcome computational challenges in data valuation (Jia et al., 2019a; Kwon& Zou, 2023), the existing proxy models yield limited advantages in our context, primarily because theirassociated LOOCV computation is still inecient. Our novelty lies in developing an ecient validation-freedata valuation algorithm based on appropriate proxy models. We complement our method with an erroranalysis for using LOOCV as utility scores. shows the overview of the proposed paradigm. Empirical Evaluation. We demonstrate the eectiveness of LOOCV-based data valuation techniques onimportant downstream tasks. Compared with validation-based techniques, we show that LOOCV-based datavaluation techniques achieve comparable performance on the weighted accuracy task and (often) superiorperformance on noisy label detection task. We also show that RLS with Gaussian kernel as a proxy model isan eective proxy model for valuation, the computed data value scores have a better performance on these",
  "i=1 where each zi =": "(xi, yi) (X Y) for data space X and label space Y. The aim of data valuation is to assign an importancescore to each data point in a dataset, reecting its value in training the model. The contribution of a datapoint is typically assessed based on a utility function U :",
  "n=0(X Y)n R. This function assigns a score to": "any set of training data points, indicating its utility. Ideally, one would like to select U as the test performanceof the model trained on a given dataset and evaluated on the population distribution P. However, in practicalscenarios, the population might not be known at the time of training. Consequently, model performance iscommonly assessed on a validation set Dval that is drawn from P. Formally, for any dataset S, we deneUDval(S) := PerfDval(A(S)), where A is a learning algorithm that takes dataset S as input and returnsmodel trained on it, and PerfDval denotes the validation performance of the given model evaluated on thevalidation set Dval. We also denote UP(S) := PerfP(A(S)) as the model performance evaluated on thepopulation distribution. Given a dataset D = {zi}N",
  "i=1, the ultimate objective of data valuation is to compute": "a score vector (zi(UP))ziD, where each zi(UP) signies the importance of data point zi to the resultingmodel performance evaluated on the population distribution. In practice, however, one usually use UDval toapproximate UP and thus compute the score vector as (zi(UDval))ziD. Data Shapley. The Shapley value (Shapley, 1953) is a classic concept in cooperative game theory toattribute the total gains generated by the coalition of all players. At a high level, it appraises each pointbased on the (weighted) average utility change caused by adding the point into dierent subsets. Given autility function U() and a dataset D = {zi}N",
  "[U(S {i}) U(S)]": "Recently, Wang & Jia (2023a) discovered that the Banzhaf value is the most robust data value notion amongthe class of semivalues (Dubey et al., 1981), and thus is suitable for data valuation especially when theunderlying learning algorithm is stochastic. Related work. Multiple recent works have been carried out to relax the assumptions and stringent re-quirements of the Data Shapley framework. Wu et al. (2022); Just et al. (2023); Nohyun et al. (2022); Jiaet al. (2019a); Kwon & Zou (2023); Yoon et al. (2020) propose dierent data valuation paradigms that donot require training neural networks multiple times, sidestepping the often cumbersome model re-trainingprocesses. For instance, methods like KNN-Shapley (Jia et al., 2019a) and Data-OOB (Kwon & Zou, 2023)",
  "Published in Transactions on Machine Learning Research (09/2024)": "used is Data Shapley- since it can be easily extended to dataset valuation. To simulate gaming the validationset, we have selected a percentage of D-4 (in red) to match the validation set. When the validation data isindependently sourced (0% match with D-4), we see that dataset D-3 (which contributes the largest numberof data points) is rightly attributed the highest value. However, when 50% of the validation set matchesD-4, we see that D-3 has a very low value (suggesting that it should be discarded). Moreover, D-2 and D-5suddenly see a sharp increase in importance and are likely to be selected over D-3. When a higher percentage(75% and 100%) of D-4 matches with the validation set, we see that D-4 has a very high data value. This experiment is a naive eort to show that if a malicious data provider has information about thevalidation set, they can manipulate their data to ensure selection, which can adversely aect the expectedpayo to other providers. When validation data is limited and not necessarily private, it can risk gaming thevaluation framework.",
  "The Achilles heel of Validation-Based Data Valuation": "Validation-based data valuation approaches can provide fair and intrinsic value given access to a clean,unbiased, and relatively large validation set. However, this validation set needs to be sourced independentlyfrom the training data, which is often dicult in practice. A common strategy to deal with this issue is totake a piece of training set as the hold-out validation set. However, this can cause a large variance in thenal performance score due to the randomness in the partition. Moreover, it is hard to ensure all edge casesare fairly represented in this validation set. If the validation set is shifted from the population distribution,certain training points may unfairly get higher or lower values. Procurement of the validation data is notthe only challenge. Even when a clean and unbiased validation set is available, the choice of validation setcan be a big factor in determining the values obtained. Preparation of such a validation set for valuationcan become a bottleneck. In this section, we highlight situations when the choice of validation set aectsthe nature of values obtained. Specically, we study the eect of (1) Choice, (2) Size and (3) ClassDistribution of the underlying validation data on data valuation. We also study the eects of gamingthe valuation framework given access to validation data. For all experiments, we consider multiple datasets,presenting one in the main paper and deferring the rest to Appendix B.2 due to the similar nature of results. Choice of Validation Set. This experiment tries to show how the choice of a xed-size validation set canaect the quality of data values obtained. We attempt to value a 1000-sized subset of the Census Dataset",
  "Proposed Approach": "To address the challenges associated with the choice of validation data for data valuation, we propose ageneral paradigm that leverages cross-validation (CV), a widely adopted method in statistical machinelearning. CV is renowned for its ability to provide reliable estimates of model performance, even in theabsence of a dedicated validation set. By substituting the traditional validation accuracy with the CVerror, we eliminate the necessity for a clean, representative validation set. This eectively transforms theconventional validation-based game-theoretic data valuation framework into a validation-freealternative. By removing this dependence on the choice of the validation set, we avoid the aforementionedissues associated with the quality, size, and bias of the validation set. Moreover, it reduces the risks of havingmalicious players gaming with the validation set.",
  "Cross Validation as Utility Function": "Cross-validation (CV) is a widely-used technique in statistical machine learning for estimating the generaliz-ability of a trained model to the population distribution. In a K-fold CV, data is randomly partitioned intoK equal-sized subsets. The model is trained on K 1 subsets and tested on the remaining one, repeatingthis process K times and averaging the validation performance over the remaining subset. Leave-one-outcross-validation (LOOCV) is a special case of K-fold where K equals the total sample size. That is, it trainsthe model on all data points except one, and repeats this for each data point. Compared with other K-foldCV, LOOCV is often preferred due to its deterministic nature and the elimination of the need to tune K. Given LOOCVs ability to provide reliable estimates of model performance without the need for validationdata, it emerges as a natural choice for use as an alternative utility function in the context of data valuationwithout a dedicated validation set. Formally, for a pre-specied learning algorithm A, we dene the utilityfunction that uses LOOCV as ULOOCV(S) :=1|S|",
  "iS Perfzi(A({zj}jS\\i))": "Computational Challenge of using LOOCV as Utility Functions. The challenge of using (3) asthe utility function is the substantial computational overhead that the calculation of LOOCV introduces.While validation-based utility functions UDval(S) require training a single model on the input dataset S,each evaluation of ULOOCV(S) necessitates training a model for each subset of S \\ zi, for each i S. This iscomputationally prohibitive for modern learning algorithms such as neural networks.",
  "Ecient LOOCV Computation for Regularized Least Square (RLS)": "Proxy Model Approach for Data Valuation. A prevalent strategy in data valuation for improvingcomputational eciency is to utilize proxy models that allow ecient evaluation of utility functions. Aprominent example is the KNN-Shapley (Jia et al., 2019a), which uses K-Nearest Neighbors (KNN) as aproxy model with a closed-form solution for exact Shapley value computation. While justifying the use ofKNN as a proxy model for data valuation on theoretical grounds remains challenging, KNN-Shapley hasbeen recognized as one of the most practical data valuation techniques due to its computational eciencyand eectiveness in distinguishing data quality. Drawing inspiration from this, we turn to Regularized LeastSquares (RLS) as our proxy model, given its unique property of having an ecient formula for computingLOOCV (Pahikkala et al., 2006).",
  "k(1)": "where f : X Rp, > 0 is a regularization coecient, and k is a norm in a Reproducing Kernel HilbertSpace (RKHS) associated with the kernel k(, ). Let K RNN be the kernel matrix with Kij = k(xi, xj),and let Y = [y1, . . . , yN]T . Let G := (K + IN)1 where IN is the identity matrix. Because the kernelfunction from which the kernel matrix is generated is positive denite, the matrix K + IN is invertiblewhen > 0. The solution to this optimization problem can be expressed as f(x) = k(x)T GY where k(x) :=[k(x, x1), . . . , k(x, xN)]T . Ecient Computation of LOOCV for RLS. One of the well-known results of RLS is its short-cutformula for computing the leave-one-out model directly from the model trained on the full dataset. Thisresult leads to an ecient computation method for LOOCV. Specically, denote the matrix A := KG, andlet ai := [IiKGIT",
  "i ] be the i-th diagonal element of matrix A, where Ii is the i-th row of identity matrix": "IN. At a high level, the shortcut for RLSs LOOCV formula exists because of the combined eects of thelinear nature of RLSs predictions and the structure provided by the hat matrix, which together allows foreciently computing the change in predictions when a training data point is omitted and being evaluatedon.",
  "(2)": "Leveraging this ecient computation of the leave-one-out model, we can compute the LOOCV score forRLS with any utility metric. The main advantage we gain by using this LOOCV linear formulation isthat we can evaluate the utility of a set once and eciently obtain the leave-one-out models (and themarginal contributions) without the need for re-training. Hence, we eliminate the additional matrix inversionoperations that are otherwise necessary to train these leave-one-out models-making our method a desirablecandidate for many data valuation frameworks. Corollary 2 (LOOCV Formula for RLS for arbitrary utility metric (Pahikkala et al., 2006)). Let f denotethe model of RLS trained on dataset S. The LOOCV error for RLS on dataset S can be computed using thefollowing formula:",
  "where fi can be eciently computed via (2)": "This corollary allows us to eciently compute the LOOCV error, which is a key component of our proposedutility function for data valuation. Additionally, it opens the potential for parallel computation of fi for all ivia GPU operations, though we do not explore this setup in this work. This utility metric can be instantiatedfor both regression and classication tasks by choosing the appropriate (task-specic) performance metricfunction Perf. Extending f to classication tasks further involves a one-hot style label encoding using 1. Remark 3 (Computational eciency). When using ULOOCV as the alternative utility function for computingData Shapley and other CGT-based values (such as Data Banzhaf (Wang & Jia, 2023a)), the computationaloverhead with respect to the number of models trained (that is, the number of linear regressions to t) remainsidentical to the case when using regular utility functions. We refer the readers to Wang & Jia (2023b;a) forthe detailed sample complexity results for dierent types of Monte Carlo estimators. Additionally, once theregression model is t, the computation of LOOCV only requires model predictions, an operation which, interms of computational demand, is negligible compared with model tting. In Appendix B.7, we discuss theFLOPS analysis and runtime advantage for ULOOCV when compared to its re-training based alternatives.",
  "Error Analysis": "Recall that the objective in data valuation is to compute the data value score zi(UP), where UP() is theutility function dened by the models performance evaluated on the population distribution P. In thissection, we provide an error analysis for the use of UDval and ULOOCV as replacements for UP in computingthe celebrated Shapley value, specically for the case of linear regression models. That is, for a randomdataset D drawn from the population distribution P, we analyze the expected deviation from z1(UP) whenusing UDval or ULOOCV to compute the Shapley value of a data point z1 D. We use z1(UP; D) to stress theunderlying dataset for computing the data value score.",
  "for linear regression models (Tian et al., 2007). It tells us that, at least for the case of lin-": "ear regression, when the size of available validation data k is signicantly less than the training set size N,the Shapley value derived from ULOOCV can be closer to the ground truth compared with the Shapley valuederived from UDval. Remark 6 (Further discussion about Theorem 5). (1) Applicability. Although Theorem 5 is statedspecically for the case of linear regression, this bound is also applicable to other learning algorithms wherethere have been previous results on the error bound of LOOCV, such as Ridge regularized logistic regression(Rad et al., 2020). (2) Error of using RLS as proxy model. It is important to note that our erroranalysis does not account for using RLS as a proxy model for the primary learning algorithm. The closenessbetween the utility scores from dierent learning algorithms, however, is generally challenging to analyze andhas been noted in the literature (Coleman et al., 2019). (3) Error of sampling from an alternativedistribution. In our error analysis for Theorem 5, we have assumed that the training data D is drawnfrom P. However, in practical scenarios, there may be a distribution shift, i.e., D may be sampled from analternative distribution P. This scenario can be analyzed using theoretical results from domain adaptation,and such a distribution will introduce an additional error term characterized by the distance between P andP, which is independent of the size of the training data. See Appendix A for details.",
  "Evaluation": "We have evaluated our method to answer the following questions: (1) Can our Kernel Regression-basedLOOCV method transform existing game-theoretic (validation-based) valuation techniques into a validation-free framework, while retaining performance on standard valuation tasks? (2) How does our method compareto existing validation-free approaches or naive solutions? If our method performs at par with existing vali-dation methods, we also oer insight into why Kernel Regression is a suitable candidate for valuation.",
  "We summarize important experiment settings here, and additional details are available in Appendix B": "Data valuation frameworks. We apply LOOCV to existing CGT-based data valuation frameworks torender them validation-independent. Our goal is to understand how this process aects the usefulness of thederived data values. We consider two commonly used CGT-based frameworks - Data Shapley (Ghorbani &Zou, 2019) and Data Banzhaf (Wang & Jia, 2023a). We use the state-of-the-art approximation algorithmsfor these two frameworks (see Appendix B for details). We also include a comparison with Beta Shapley(Kwon & Zou, 2021) in Appendix B.3. Our method is not compatible with KNN-Shapley ( a special instanceof Data Shapley), hence we do not test it with our method. Implementation details. We evaluate data values over 9 classication datasets popularly used in datavaluation literature (refer Appendix B.1). For each comparison study, validation-based baselines use standardmodels (either binary MLP or logistic regression) that initially perform the best on a select held-out validationset. LOOCV calculation (outlined in Theorem 2) involves computing the ecient cross-validation accuracy(using Theorem 5) on an RLS model ( = 0.1) with a Gaussian Kernel. Additionally, we perform an ablationstudy on the eect of changing parameter in Appendix B.6. Baselines. A novel baseline that we explore is the usage of the whole training set as a substitute forthe validation set. This naive approach (henceforth mentioned as Self-Eval) helps us verify the superiorperformance that cross-validation can provide over simply using the whole training dataset as the validationset for attribution. The Self-Eval baseline also uses the RLS model. Our second baseline is the validation-dependent version of the same data valuation framework, where we set the size of the validation set thesame as the size of the training set. Note that this baseline assumes more knowledge than our approachand Self-Eval and thus their results are not directly comparable. Nevertheless, having this baseline helps usto evaluate whether game-theoretic validation-free valuation approaches can yield results as competitive asthose from validation-based approaches.",
  "Quality of Values Obtained": "We study the quality of data values obtained using two dierent criteria- the rst is performance (AUROC)on the noisy label detection task. In all experiments, labels have been randomly ipped with a xed poisonratio of 10%. A higher AUC of the detection rate suggests better detection performance, which in turnsupports the quality of values obtained. Our second criterion to assess quality is learning with weightedsamples. Comparison with a validation-based baseline will test if LOOCV can provide equally useful datavalues. In comparing with Self-Eval, we test how dierent designs of validation-free schemes aect utility ofderived values.",
  ": Comparison of AUROC on misla-beled detection task with dierent data val-uation techniques as the size of the datasetincreases": "Mislabeled Data Detection. shows the results formislabeled detection. We found that LOOCV can get compara-ble and often superior performance from traditional validation-based approaches. Noisy label detection is quite challengingand the strong performance reinforces that our data values cancompete with validation-based counterparts. shows how the AUROC varied when we increasedthe dataset size for the OpenML Phoneme speech-recognitiondataset. We see that for this dataset, LOOCV can achieve highperformance for larger sizes but self-eval performance notice-ably drops. We perform an ablation study varying the mislabeled ratioand evaluating AUROC scores on LOOCV, Self-Eval, andValidation-based methods in Appendix B.6. As per intuition,we nd that there is a slight decrease in AUROC scores as thenoisy label ratio increases to 20%- since our method relies ontraining data for evaluation. However, our AUROC scores werealways higher than those obtained from validation-based coun-terparts. Additional evaluation (using Beta Shapley frameworkin ) and settings can be found in Appendix B. Weighted Accuracy. We further conduct a weighted training experiment in . We weigh the datasetwith the data value obtained from Data Shapley and Data Banzhaf. After obtaining data values, we test theperformance on an independent test set that was not used during valuation. We observe that test accuraciesobtained using LOOCV closely track their validation-based counterparts.",
  "Comparison with Existing Validation-free Frameworks": "Comparison with Volume-based data valuation (Xu et al., 2021). In Appendix B.4 we conduct acomparison with Volume (Xu et al., 2021), a label-agnostic and validation-free valuation method. We ndthat it is unable to perform well on mislabeled-detection tasks (due to its label-agnostic nature) and ourweighted accuracies usually perform better than Volume on the standard datasets. We note that Volume isdesigned for Dataset Valuation and we conduct an experiment for comparing LOOCV and Volume in that",
  ": Data Removal Experiment- Test Accuracies of Logistic Regression models when data points areremoved from the highest-valued, according to data values from LOOCV, Data-OOB and a Random Baseline": "setting. We consider experiments on three datasets- Adult Census data, Fraud Dataset and the PhonemeDataset. These datasets were split into 8 datasets of size 50 each and the highest valued dataset was addedrst, following the experimental setup in Xu et al. (2021). The results from indicate that LOOCVperformed better at this selection task. We include additional settings and setup in Appendix B.4. Comparison with Data-OOB valuation framework (Kwon & Zou, 2023): Data-OOB (Out-Of-Bag) valuation framework proposes a valuation strategy that utilizes the out of bag error estimate from abagging model (random forest). We conduct both mislabeled detection and weighted accuracy experimentson 7 datasets in . Our choice of label noise is 20% and we choose Data Banzhaf as the valuationframework for LOOCV. Data-OOB was proposed as a strong candidate for identifying outliers and noisypoints present in the training set, and we nd that it indeed performs better than LOOCV at assigning lowvalues to these points. It is important to note that LOOCV based detection is quite strong on its own, as seenin . Interestingly, we nd that LOOCV results in better weighted accuracies compared to Data-OOB,suggesting that LOOCV assigned higher weights to important points. In order to solidify this observation, weconduct a data removal experiment where a new logistic regression model is t to the resulting dataset each",
  ": Heatmap of data values. x-axis: Data points with index 0-14 are valued using RLS (left) and logisticregression (right). y-axis: Individual test points": "time a data point is removed. We remove the highest valued data point rst. It is expected that on removalof high-valued points the accuracy should decrease. We observe in that LOOCV performs betterthan Data-OOB in reducing the accuracy across all 6 datasets. Data-OOB performance is slightly worsethan Random baseline in 3 of the datasets. This conrms that LOOCV is procient in nding pivotal points,while also demonstrating a strong performance in detecting noisy points. We also conduct an ablation studyfor the data-removal experiment in Appendix B.5 where we assume clean label information and observe asimilar trend in . Additional settings and experimental setup is outline in Appendix B.5.",
  "Why the choice of kernel regression is a powerful one?": "We provide potential reasons for why RLS with Gaussian kernel is a strong proxy model for valuation.Logistic Regression is frequently used in past literature for model-based data valuation (Ghorbani & Zou,2019; Kwon & Zou, 2021). Meanwhile, Kernel Regression is rarely used in this setting. Our values indicatestrong performance by kernel regression, often superior (on noisy-label detection task) than values obtainedfrom validation-based methods using Logistic Regression. We conduct a data valuation experiment on theCensus Dataset using individual test points one at a time. We perform data valuation using Data Banzhafframework for each test point using RLS and logistic regression. 15 training data points are selected atrandom from the whole set. We visualize a heatmap of their value estimates in . We observe thatthe values for each training point are much more sensitive for the RLS-based model than for their logisticregression-based equivalent. A possible reason for this behavior is the use of Gaussian Kernels in RLS whichare more sensitive to localized change (or similarity) in the distance between two points - resulting in betterdetection of mislabeled samples as seen in . Meanwhile, models like logistic regression on averagewere not as sensitive to individual test points.",
  "Conclusion & Limitations": "In this work, we tackled the challenge of data valuation in the absence of a representative validation set. Weexamined the current validation-based data valuation techniques and identied their limitations. In response,we proposed a novel validation-free data valuation approach using LOOCV, and we propose to use RLS asproxy model. Our work oers a potential solution for scenarios where a clean, representative validation setis not available. Limitations. Although we leverage RLS to make LOOCV computationally ecient, computing the exactData Shapley (or other CGT-based approaches) is still computationally prohibitive as it requires computingLOOCV for all 2N data subsets. An interesting future work is to explore potential learning algorithms wherethe exact Shapley value can be computed eciently based on the performance scores from LOOCV (analogueto the famous KNN-Shapley (Jia et al., 2019a; Wang & Jia, 2023c))."
}