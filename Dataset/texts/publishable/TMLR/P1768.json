{
  "Abstract": "Contrastive learning has emerged as a powerful unsupervised learning technique for extractingmeaningful representations from unlabeled data by pulling similar data points closer in therepresentation space and pushing dissimilar ones apart. However, its vulnerability to adver-sarial attacks remains a critical challenge. To address this, adversarial contrastive learning incorporating adversarial training into contrastive loss has emerged as a promisingapproach to achieving robust representations that can withstand various adversarial attacks.While empirical evidence highlights its effectiveness, a comprehensive theoretical frameworkhas been lacking. In this paper, we fill this gap by introducing generalization bounds foradversarial contrastive learning, offering key theoretical insights. Leveraging the Lipschitzcontinuity of loss functions, we derive generalization bounds that scale logarithmically withthe number of negative samples, K, and apply to both linear and non-linear representations,including those obtained from deep neural networks (DNNs). Our theoretical results aresupported by experiments on real-world datasets.",
  "Introduction": "Learning meaningful representations from unlabeled data plays a crucial role in enhancing the performanceof machine learning models. Representation learning has shown great success in fields such as computervision (Chen et al., 2020b; He et al., 2020; Caron et al., 2020) and natural language processing (Brownet al., 2020; Gao et al., 2021; Radford et al., 2021). Among various representation learning techniques,self-supervised contrastive learning (CL), popularized by the SimCLR framework (Chen et al., 2020b),stands out. The core idea behind contrastive learning is to bring similar pairs (x, x+) closer together inthe embedding space while pushing apart negative samples from x, (denoted as (x, x1 , , xK)). Theselearned representations can then be leveraged for downstream tasks, such as a classification task, whether",
  "Published in Transactions on Machine Learning Research (11/2024)": "To generate adversarial perturbations, we employ the PGD algorithm (Madry, 2017) with a step size of/255, where represents the maximum allowable perturbation. Afterward, a PGD attack is performed toevaluate the generalization error. The generalization error is calculated as the difference between the trainingaccuracy and testing accuracy, a standard method commonly used in the literature Yin et al. (2019). We calcu-late the generalization error for different values of = {2/255, 4/255, 8/255, 16/255, 32/255, 64/255, 128/255},with varying negative samples, K = {63, 127, 511, 1023}.",
  "Related work": "Contrastive LearningOur work is primarily related to the theoretical analysis of contrastive learning byArora et al. (2019) and Lei et al. (2023). Arora et al. (2019) provided generalization bounds for contrastivelearning by analyzing the Rademacher complexity of the representation function class and examining theperformance of linear classifiers trained on the learned representations. They showed that the classification",
  "Contrastive Representation Learning": "In the contrastive learning setting, we aim to learn representations by contrasting similar and dissimilardata points. Let X be an input space (e.g., a set of input images). Given an anchor sample x, we use apositive sample x+, which is drawn from a distribution of similar data Dsim, and multiple negative samplesx1 , x2 , , xK, which are drawn from a negative distribution Dneg. The goal is to learn a representationwhere the anchor and positive pair are pulled closer together, while the anchor and negative samples arepushed apart in the feature space. In this setup, a single positive sample is paired with multiple negativesamples, creating an inherent asymmetry that is standard in both theoretical (Arora et al., 2019; Lei et al.,",
  "Dneg(x) = Ec[Dc(x)]": "That is, Dsim(x, x+) measures the probability of drawing x and x+ from the same class c , which meansthat x and x+ are conditionally dependent, given c, while Dneg(x) measures the probability of drawing x that is independent of x and x+, coming from other latent classes. The objective of CL is to select a featuremap f : X Rd from a class of representation functions F = {f : f()1 R}, for some R > 0, where 1denotes the 1-norm, and d N represents the dimensionality of the feature space. This is achieved using thetraining set",
  "S = {(x1, x+1 , x11, , x1K), (x2, x+2 , x21, , x2K), , (xn, x+n , xn1, , xnK)},": "where (xj, x+j ) Dsim and (xj1, , xjK) DKneg, with j [n] := {1, , n} and K indicating thenumber of negative samples. However, the specific distributions Dsim and Dneg are abstracted out oncewe are dealing with a fixed dataset S. The quality of the representation f is evaluated using the loss{f(x)T (f(x+) f(xk )}Kk=1, where : RK [0, B] is some loss function and f(x)T is the transpose off(x). The population and empirical risks are then defined as follows.",
  "Adversarial Contrastive Representation Learning": "In this paper, we examine adversarial settings where an attacker employs a noise function A : X B X,with B being a noise set, to subtly introduce noise B to an input x X in order to maximize the loss.For example, in the Lp-additive attack, A(x, ) = x + and B is the p-ball { : p }. The attackersobjective is to select B that maximizes the loss:",
  "i=1maxB ({f(A(xi, ))T (f(x+i ) f(xik))}Kk=1)]": "In our definition of adversarial contrastive risk, the adversary is restricted to perturbing only the anchorsample x, while the positive sample x+ and negative samples x remain clean. This is consistent withstandard practices in both theoretical (Zou & Liu, 2023) and practical applications of ACL (Kim et al., 2020;Ho & Nvasconcelos, 2020; Jiang et al., 2020), where adversarial perturbations are typically applied to theanchor sample alone. This setup allows us to evaluate the robustness of the learned representation withoutdisrupting the fundamental contrastive structure between positive and negative samples.",
  "Generalization Error Bounds": "In this section, we establish a generalization bound for ACL. Our technique relies on the concept of coveringnumbers of the adversarial contrastive loss class. Covering numbers measure the complexity of a functionclass F by counting the minimum number of balls needed to cover all functions in F, where each ballrepresents a region of approximation with a specified level of accuracy. Definition 4.1 (Covering number). Let F := {(f(x1), , f(xn))} be a real-valued function class, thatmaps X Rd defined over a vector space V, and let S := {x1, , xn} X n be a dataset. For any > 0,the p-norm covering number, denoted as Np(, F, S), is defined as the size of the smallest set of vectorsv1, , vm that covers F. Specifically, it satisfies:",
  "p ,": "where v1, , vm forms the (, p)-cover of F with respect to S. Moreover, when the p-norm is taken as the-norm, we denote the covering number as N(, F, S). Finally, the worst-case covering number is definedas Np(, F, n) = maxSX n Np(, F, S), where the maximum is taken over all possible datasets S X n ofsize n. To analyze the complexity of a function class involving a nonlinear loss function (e.g., hinge loss), we useLipschitz continuity to simplify the analysis by reducing the complexity to that of a function class withoutthe loss function . We consider a general Lipschitz continuity w.r.t. a p-norm, defined as follows:",
  ") such that": "21 . We now introduce our first lemma, which establishes a relationshipbetween the covering number of Gadv on S and that of H on SH.Lemma 4.1. Let ({f(A(x, ))T (f(x+) f(xk ))}Kk=1) be 1-Lipschitz and be 2-Lipschitz withrespect to the -norm, for all (x, x+, x1 , . . . , xK) X K+2 and f F. Then, we have:",
  ", H, SH": "The lemma shows that we can upper-bound the -covering number of the ACL function class by that ofthe class H with the extended training set SH. Notably, the class H does not include the maxB operator,significantly simplifying the analysis. Furthermore, it shifts the dependence on the number of negativesamples from the dimensionality of the function classs output to the size of the training set. For most classes,the dependence of the covering numbers on the size of the training set is only logarithmic (Zhang, 2002).Consequently, our bound will lead to a generalization bound that only has a logarithmic dependence on thenumber of negative samples K. The proof of Lemma 4.1 begins by eliminating the maxB-operator, anapproach based on Mustafa et al. (2022). Details of the proof are provided in the appendix.Remark. The Lipschitz condition on the function ({f(A(x, ))T (f(x+) f(xk ))}Kk=1) is crucial becauseit is a mild yet standard assumption that most adversarial attacks in the literature satisfy (Engstrom et al.,2019; Awasthi et al., 2021; Madry et al., 2017). This condition ensures that the loss function behaves smoothlyand predictably, which is essential for the success of gradient-based adversarial attacks. Additionally, thisLipschitzness allows us to bound the covering number of the adversarial class Gadv.Remark. The size of the extended training set SH grows linearly with the size of the cover CB(",
  ") can grow exponentially with the dimensionality of the perturbation set B, its important": "to note that the dependence of the generalization performance is typically of the order O(log1/2(|SH|)), asshown in prior work (Bartlett et al., 2017; Zhang, 2002; Mustafa et al., 2021). Thus, the generalizationbounds will exhibit a square-root dependency on the dimensionality of B, leading to manageable bounds evenin the presence of large perturbation sets. While Lemma 4.1 provides an upper bound on the ACL class Gadv in terms of the non-adversarial class H,the class H is not directly the representation function class F. This makes it challenging to utilize existing",
  "R2, F, S F": "The proof of this lemma is provided in the appendix. The lemma upper-bounds the covering number of H bythe covering number of the class F. Notably, F is a class of scalar-valued functions of the same form as therepresentation function class F. This simplifies the analysis by (1) reducing the form of the functions in Hto that of the representation class, and (2) simplifying the analysis from vector-valued functions to scalarfunctions of the same form. Note that the number of dimensions contributes only through the size of thedataset S F, and for many typical function classes, this contribution is only logarithmic. This achieves thebest known rate for vector-valued functions (Lei et al., 2019). Combining Lemmas 4.2 and 4.1 with Dudleysentropy integral (Boucheron et al., 2003; Bartlett et al., 2017; Ledent et al., 2021a; Srebro et al., 2010) givesour main result. Theorem 4.1. Let (0, 1), and F = {f : f()1 R}, for some R > 0, where 1 denotes the 1-norm,and d N represents the dimensionality of the feature space and F =(x, j) fj(x) : f F, x X, j [d].With probability at least 1 over the randomness of the training data S with size n, we have for all f F:",
  "R2, F, S Fd": "The theorem demonstrates that we can control the generalization error of ACL by controlling the coveringnumber of the class F.The covering numbers of many classes F (e.g., linear models (Zhang, 2002),MLPs (Bartlett et al., 2017), CNNs (Ledent et al., 2021b), and structured learning models (Mustafa et al.,2021)) can be directly applied here to derive generalization bounds for ACL across a large family of models.",
  "({f(A(x, ))T (f(x+) f(xk ))}Kk=1) B,f F": "This assumption is valid because we can impose constraints on the norms of the models weights and inputs,ensuring the loss function remains bounded.In this section, we instantiate our bound (Theorem 4.1) for two models: linear and DNN-based features.Throughout the section, we consider feature extractors of the form x Uv(x), where U Rdd is atransformation matrix, and v : X Rd is a map from the original data x X to some intermediateembedding space in Rd. We consider the linear feature extractor in .1, while in .2, weexplore features from a DNN.",
  "Nonlinear Features": "Now, we consider the covering numbers for learning the nonlinear features by DNNs. We say an activationfunction : R R is positive-homogeneous if (ax) = a(x) for a > 0, and is contracting if |(x) (x)| |x x|. The ReLU activation function (x) = max{x, 0} is both positive-homogeneous and contractive. Nowassume the DNN feature map is defined as (removing matrix U for now),",
  "Let V V be the weight of the network. Suppose that V is such that, for all V V, Vl2 al and Vl slfor all l [L 1]. Further, suppose that, for all V V, VL2 aL, VL2, sL and V11, s1": "We now consider the -additive perturbation applied to the DNN. As with the linear case, we first establishthe Lipschitzness of the function ((Uv(x + ))T (Uv(x+) Uv(xk ))Kk=1) w.r.t. -norm. Thefollowing lemma establishes the Lipschitz continuity of the loss as a function of . Lemma 5.3. Consider the function gUV (x, ) = ((Uv(x + ))T (Uv(x+) Uv(x))Kk=1) and assumeU,2 1 and v() is the neural network. Then, for any x X and V V, the function gUV v(x, )is -Lipschitz with constant 221s1w1x2Ll=1 slLl=2 sl. Now, we can get the upper bound of the covering number of the neural network scalar-valued feature classw.r.t. .-norm.Lemma 5.4. Let F be the DNN (nonlinear) feature class on the extended dataset S F, defined as before.Let B := { : }. Assume the previous assumptions on the weights of DNN, and x2 . Then,for S F and > 0, we have",
  "(log(n) + log(B))": "Remark. Similar to the results for the linear case, our analysis reveals a dependency on the square root of theinput dimension. This issue can be resolved if we assume the 2 attack, where B = { : 2 }. As in thelinear case, our results maintain a logarithmic dependence on the negative samples, K.",
  "Experiments": "We evaluate our theoretical results to two widely used benchmark datasets from the image domain: CIFAR-10and CIFAR-100 (Krizhevsky, 2009). CIFAR-10 and CIFAR-100 consist of 50,000 training images and 10,000testing images, organized into 10 and 100 classes, respectively. Experimental SetupFor linear features, we use a one-layer neural network, while for nonlinear features,we implement a four-layer neural network with ReLU activation. Both models are trained using the Adamoptimizer, with a learning rate of 1e 3. We perform adversarial training on the following objective function:",
  "Results": "We analyze how the generalization error varies with respect to different parameters by examining the effect ofstep size () and the number of negative samples (K). The results are presented in for CIFAR-10and CIFAR-100. As expected, the generalization error increases as the number of negative samples (K) grows.While this behavior is consistent with our theoretical bounds, it contrasts with findings in prior work showingthat increasing K can enhance downstream classification performance (Wang et al., 2022; Bao et al., 2022;Awasthi et al., 2022). This apparent discrepancy arises from the different objectives in these two types ofanalyses. The generalization error increases with K because the function class becomes more complex, as theloss function must handle a greater number of comparisons. Furthermore, a larger K can make the modelmore prone to overfitting to the negative examples, thereby amplifying the generalization gapparticularlyif the loss function becomes overly sensitive to the contrast between positive and negative pairs. Thesetwo phenomena are not inherently contradictory but rather highlight the distinction between task-specificperformance and the broader ability of the learned representations to generalize across diverse tasks. : Generalization error with varying numbers of negative samples. On the left, the features are learnedusing a linear model, and on the right, the features are learned using a nonlinear model (4-layer neuralnetwork). As the number of negative samples (K) increases, the generalization error rises.",
  "Proofs of Results in .1": "In this subsection, we present the omitted proofs of section 5.1 when the features are linear. Our approachrelies on -covering numbers for the feature classes. First, we show that the loss function is -Lipschtizwith respect to the noise parameter . Next, we derive a bound on the size of the set CB(/21). Finally, weestablish a bound on the -covering number of the feature class F on the extended data set S F. Initially, we prove the bounds of -additive attacks applied to linear models as stated in Lemma 5.1. Ourfirst step is to derive the -Lipschitz constant of the function ({(U(x + ))T (U(x+) U(xk ))}Kk=1).The following is the proof of Lemma 5.1.",
  "Proofs of Results in .2": "In this subsection, we provide the omitted proofs from section 5.2 for the case when the features are non-linear.As in the linear case, we begin by showing that the loss function is -Lipschitz with respect to the noiseparameter . We then establish a bound on the set CB(/21) and apply the -covering number results ofthe non-linear feature class F on the extended dataset S F. First, we prove the bounds of the -additive attacks applied to non-linear models as stated in Lemma 5.3. Thefirst step is to derive the -Lipschitz constant of the function ({(Uv(x +))T (Uv(x+)Uv(xk ))}Kk=1).The following is the proof of Lemma 5.3.",
  "We now review the upper bounds on the -covering numbers of norm-bounded neural networks (non-linear)function classes": "Lemma 7.3 (Ledent et al. 2021b). Let V be the class of neural networks, that is, V = {x v(x)}, whereV = (V 1, . . . , V L) are a set of weights the DNN v() and is defined as above. Suppose that V l2,1 aland V l sl for all l [L 1], V L2 aL, V L2, sL, x2 b, and wl is the width of the lthlayer. Then given a data set S with n elements and > 0, we have",
  "Conclusion": "We conducted a generalization analysis of ACL, showing that the generalization error is bounded by thecovering number of the feature class. Our results leverage the Lipschitz continuity and boundedness of thehinge loss as our unsupervised loss function, given the constraints on the models weights and inputs. Weapplied this bound on both linear and non-linear features, subject to -additive attacks. Unlike previouswork, such as Zou & Liu (2023), our bounds are directly applied to the adversarial contrastive loss, avoidingthe use of surrogate losses. Moreover, our bounds scale logarithmically with the number of negative samplesK, with a complexity of O(log K). Although these are algorithm-independent bounds, they could be extendedto algorithm-dependent bounds to understand how the optimization process affects the generalization error.In this paper, we have applied only -additive attacks; nonetheless, other types of adversarial attacks canalso be tested, especially non-additive attacks.",
  "Stphane Boucheron, Gbor Lugosi, and Olivier Bousquet. Concentration inequalities. In Summer school onmachine learning, pp. 208240. Springer, 2003": "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.Advances in neural information processing systems, 33:18771901, 2020. Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervisedlearning of visual features by contrasting cluster assignments. Advances in neural information processingsystems, 33:99129924, 2020. Kejiang Chen, Yuefeng Chen, Hang Zhou, Xiaofeng Mao, Yuhong Li, Yuan He, Hui Xue, Weiming Zhang, andNenghai Yu. Self-supervised adversarial training. In ICASSP 2020-2020 IEEE International Conferenceon Acoustics, Speech and Signal Processing (ICASSP), pp. 22182222. IEEE, 2020a. Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastivelearning of visual representations. In International conference on machine learning, pp. 15971607. PMLR,2020b.",
  "Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical Report, 2009": "Antoine Ledent, Rodrigo Alves, Yunwen Lei, and Marius Kloft. Fine-grained generalization analysis ofinductive matrix completion. In Advances in Neural Information Processing Systems, volume 34, pp.2554025552, 2021a. Antoine Ledent, Waleed Mustafa, Yunwen Lei, and Marius Kloft. Norm-based generalisation bounds for deepmulti-class convolutional neural networks. In Proceedings of the AAAI Conference on Artificial Intelligence,volume 35, pp. 82798287, 2021b.",
  "N(/2, Gadv, S) N(/(22), H, SH)": "The observation here is that we can construct a cover for the function class Gadv on the training set S fromthe elements of the cover of the function class H. Additionally, from the Definition 4.1, we know that thecovering number of a set is the cardinality of the smallest cover for a set.For any f, define hf ashf(x, x+, x, ) = f(A(x, ))T (f(x+) f(x)).",
  "A.2Proof of Theorem 4.1": "To prove Theorem 4.1, we define the Rademacher complexity and review the theorem A.1 from Mohri et al.(2018), which controls generalization of learning algorithms by Rademacher complexity of function classes.Definition A.1 (Rademacher complexity). Given a class of real-valued functions F and dataset S = {zi}mi=1drawn from the distribution D over a space Z, the empirical Rademacher complexity of F w.r.t. S isdefined as RS = E[supfF1mi[m]if(zi)], where each i is an independent Rademacher variable, uniformlydistributed over {+1, 1}m. The worse-case Rademacher complexity is then RZ,m = supSZ:|S|=m[RS(F)].Theorem A.1 (Mohri et al. 2018). Let S = {zi}mi=1 be i.i.d. random sample from a distribution D definedover Z. Further let F Z be a loss class. Then for all (0, 1), we have with probability at least 1 over the draw of the sample S, for all f F that",
  "2n": "Our approach relies, however, on another complexity measure, namely -covering numbers. The followingclassical result of Dudleys entropy integral (Boucheron et al., 2003; Bartlett et al., 2017; Ledent et al., 2021a;Srebro et al., 2010) gives a relationship between the Rademacher complexity and -covering number. Weapply the version by Srebro et al. (2010).Theorem A.2 (Srebro et al. 2010). Let F be a class of functions mapping from a space Z and taking valuesin [0, b], and assume that 0 F. Let S be a finite sample of size m and E[f(z)2] := 1"
}