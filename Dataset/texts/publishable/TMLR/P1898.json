{
  "Abstract": "Federated semi-supervised learning (FedSemi) refers to scenarios where there may be clientswith fully labeled data, clients with partially labeled, and even fully unlabeled clientswhile preserving data privacy. However, challenges arise from client drift due to undefinedheterogeneous class distributions and erroneous pseudo-labels. Existing FedSemi methodstypically fail to aggregate models from unlabeled clients due to their inherent unreliability,thus overlooking unique information from their heterogeneous data distribution, leading to sub-optimal results. In this paper, we enable unlabeled client aggregation through SemiAnAgg, anovel Semi-supervised Anchor-Based federated Aggregation. SemiAnAgg learns unlabeledclient contributions via an anchor model, effectively harnessing their informative value.Our key idea is that by feeding local client data to the same global model and the sameconsistently initialized anchor model (i.e., random model), we can measure the importanceof each unlabeled client accordingly. Extensive experiments demonstrate that SemiAnAggachieves new state-of-the-art results on four widely used FedSemi benchmarks, leading tosubstantial performance improvements: a 9% increase in accuracy on CIFAR-100 and a 7.6%improvement in recall on the medical dataset ISIC-18, compared with prior state-of-the-art.Code is available at:",
  "Introduction": "Federated learning has emerged as a promising solution for learning in decentralized environments, wheredata centralization is often infeasible due to privacy concerns. Federated learning has gained considerableattention in machine learning tasks in natural image domains (McMahan et al., 2017) as well as medicalimage domains (Liu et al., 2021; Saha et al., 2023; Jiang et al., 2023). Due to the challenges of data and labelheterogeneity, multiple methods such as MOON (Li et al., 2021a), FedDisco (Ye et al., 2023), FedFed (Yanget al., 2023), and FedRoD (Chen & Chao, 2022) have been developed, utilizing FedAvg (McMahan et al., 2017)as their baseline. While these methods have shown promise, they often assume that all clients have exhaustiveand expert-level annotations, which is a requirement that is both time-consuming and labor-intensive. Thisrenders them impractical in real-world cross-silo federated settings, such as those found in hospitals. For",
  "Published in Transactions on Machine Learning Research (10/2024)": "Zhiqin Yang, Yonggang Zhang, Yu Zheng, Xinmei Tian, Hao Peng, Tongliang Liu, and Bo Han. Fedfed:Feature distillation against data heterogeneity in federated learning. In Thirty-seventh Conference onNeural Information Processing Systems, 2023. URL 1 Rui Ye, Mingkai Xu, Jianyu Wang, Chenxin Xu, Siheng Chen, and Yanfeng Wang. Feddisco: federatedlearning with discrepancy-aware collaboration. In Proceedings of the 40th International Conference onMachine Learning, ICML23. JMLR.org, 2023. 1, 4 Jure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and Stphane Deny. Barlow twins: Self-supervised learningvia redundancy reduction. In International Conference on Machine Learning, Proceedings of MachineLearning Research, pp. 1231012320. ML Research Press, 2021. 17, 19 Bowen Zhang, Yidong Wang, Wenxin Hou, Hao Wu, Jindong Wang, Manabu Okumura, and TakahiroShinozaki.Flexmatch:Boosting semi-supervised learning with curriculum pseudo labeling.InA. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan (eds.), NeurIPS, 2021.URL",
  "Data Size": ": Upper: FedAvg (McMahan et al.,2017) fails miserably. Yet, disentangling theaggregation (FedAvg-Semi) achieves perfor-mance comparable to the state-of-the-art Fed-Semi method, CBAFed (Li et al., 2023). Bypromoting diversity among unlabeled clientsour SemiAnAgg achieves a new SOTA onfour FedSemi benchmarks. Lower: Leave oneunlabeled client out. The bar refers to the Error (%), and the line refers to the DataSize. In the leave-one-out experiment setting,one unlabeled client is excluded during train-ing. The results indicate that the decrease inaccuracy (represented by the bar) does notcorrespond proportionally with the data size(represented by the line). Despite the effectiveness of these methods, existing approacheshave two crucial limitations. First, their aggregation typicallyfollows the standard FedAvg (McMahan et al., 2017), which aggre-gates clients based on the volume of local data, failing to accountfor the fact that labeled clients can generate more accurate in-formation than unlabeled clients, regardless of data volume. Forexample, when the number of unlabeled clients is dominant, theglobal model becomes heavily influenced by the unlabeled clients,leading to limited performance primarily due to errors introducedby the pseudo-labels assigned to the unlabeled data. Second,existing methods treat the divergence of unlabeled clients, calcu-lated in the gradient space, as noise due to the unreliability ofthe pseudo-labels used in training. It is worth mentioning thatclient divergence may reflect the presence of minority classes andunique attributes within the dataset, rather than mere noise. To address the first limitation, we first show that aggregationbased on the local dataset size, as in FedAvg (McMahan et al.,2017), falls short in the FedSemi setting. At first glance, this maynot be surprising: an unlabeled client with a large amount of databut incorrect pseudo-labels should not dominate the global opti-mization. Recall that traditional semi-supervised learning (SSL)typically seeks a balance between optimizing the empirical risksassociated with both labeled and unlabeled data to avoid skewingtoward potentially incorrect pseudo-labels (Tarvainen & Valpola,2017; Sohn et al., 2020; Zhang et al., 2021; Chen et al., 2023; Wanget al., 2023). Therefore, by balancing the empirical risk of labeledand unlabeled clients during global model aggregation, we presenta stronger baseline for FedSemi termed FedAvg-Semi. shows that our simple baseline, FedAvg-Semi, can achieve com-parable performance to SOTA of FedSemi, CBAFed (Liet al., 2023). Nevertheless, treating unlabeled clients based ontheir local dataset size is suboptimal: an unlabeled client with asmall dataset size may contain underrepresented attributes andminority classes that are not present in others. The second limitation of current methodologies is their failure toaccount for the diverse contributions of unlabeled clients, partic-ularly in reflecting the presence of minority classes and uniqueattributes within majority classes. To validate the importance of unlabeled clients diverse contributions, weconducted an extensive empirical evaluation using the popular leave-one-out valuation strategy (Ghorbani& Zou, 2019). As shown in , the leave-one-out valuation strategy reveals significant differences in",
  "We provide a new insight for FedSemi that was neglected in prior work: measuring the importance ofunlabeled clients": "Unlike previous FedSemi approaches that treat the divergence of unlabeled clients as noise, we proposeSemiAnAgg, a novel aggregation method that effectively aggregates the most informative unlabeledclients, significantly harnessing their unique information. Our method consistently outperforms prior state-of-the-art methods on four widely used benchmarks,surpassing SOTA CBAFed by 9% in accuracy on CIFAR-100, 9.5% on its highly imbalanced version,CIFAR-100LT, and achieving a 7.65% recall improvement on the medical dataset, ISIC-18.",
  "Semi-Supervised Learning": "Semi-Supervised Learning (SSL) leverages a large amount of unlabeled data along with labeled data tolearn a generalizable model. Semi-supervised methods include consistency regularization (ensuring consistencybetween two distorted unlabeled images) (Li et al., 2021b; Tarvainen & Valpola, 2017), generating pseudolabels through supervised objectives (Zhang et al., 2021; Chen et al., 2023; Wang et al., 2023), or self-supervisedclustering objectives (Fini et al., 2023). Prominent methods incorporating adaptive pseudo labeling areFlexMatch (Zhang et al., 2021), SoftMatch (Chen et al., 2023), and FreeMatch (Wang et al., 2023). However,these strategies assume uniform and identical class distribution of labeled and unlabeled data, which is oftennot the case in real-world applications. Semi-Supervised Imbalanced Learning (SSIL) aims to learn SSL models in scenarios where there is aclass imbalance distribution in labeled and unlabeled data. Several works address this issue by amplifyingpseudo labels for minority classes through resampling (Wei et al., 2021), re-weighting (Wei et al., 2022; Wang& Li, 2023a;b), and classifier blending (Oh et al., 2022). However, these methods assume the unlabeled classimbalanced distributions follow the labeled ones. In the non-IID FedSemi setting, a more realistic scenarioarises where there is a class distribution mismatch between labeled and unlabeled data. This setting, whichhas not been extensively explored, is addressed by the state-of-the-art method proposed by ACR (Wei &Gan, 2023). ACR builds upon FixMatch (Sohn et al., 2020) with a dual branch and introduces adaptive",
  "Federated Semi-Supervised Learning": "Federated Semi-Supervised Learning (FedSemi) addresses the decentralized learning of both unlabeled andlabeled data while preserving privacy. FedSemi has been explored in three different settings. In the firsttwo settings, labeled data is available on the server, as in FedMatch (Jeong et al., 2021), Semi-FL (Diaoet al., 2022), and FedLID (Psaltis et al., 2023), or clients have partially labeled data, as seen in SemiFed (Linet al., 2021) and FedLabel (Cho et al., 2023). The third setting, which we consider more realistic, involvesfully unlabeled clients, while other clients can be either fully labeled or partially labeled. For the thirdsetting, recent works have shifted from addressing clients with independent and identically distributed (IID)data, such as FedConsist (Wang et al., 2020) and FedIRM (Liu et al., 2021), to non-IID data, includingRSCFed (Liang et al., 2022), IsoFed (Saha et al., 2023), and CBAFed (Li et al., 2023). This work focuses onthe third setting, characterized by most clients with fully unlabeled and potentially non-IID data.",
  "Federated Model Aggregation": "Federated aggregation aims to improve the global model through proper aggregation across various clients.Given the heterogeneity of client data, clients with unique information could benefit the global model moreduring optimization, rendering the control of clients contributions a crucial concern. This direction hasshown great promise in supervised federated learning settings (Jiang et al., 2023; Elbatel et al., 2023; Yeet al., 2023). For instance, FedCE (Jiang et al., 2023) measures contribution in both the gradient andsample space. Using the sample space involves prediction error calculation, relying on labeled validation datawithin clients. FedMAS (Elbatel et al., 2023) proposes leveraging labeled data for estimating inter-clientintra-class variation to emphasize the contribution of clients with minority classes. However, these methodsrely on label information, making them inapplicable to unlabeled clients in FedSemi. In unlabeled clients, thenon-IID distribution results in noisy pseudo-label estimation, rendering the surrogation with pseudo-labels onunlabeled clients less effective for aggregation measurement. Existing FedSemi (Liang et al., 2022; Li et al., 2023) methods regard non-robust clients or data as outliersand opt to minimize their influence. RSCFed (Liang et al., 2022) proposes to leverage unlabeled gradientdivergence to eliminate non-robust (noisy) clients through average consensus inspired by RANSAC (Fischler& Bolles, 1981). However, RSCFed (Liang et al., 2022) does not consider cases when unlabeled clientsare diverse due to unique information (i.e. underrepresented classes or attributes). IsoFed (Saha et al.,2023) proposes alternating model training between labeled and unlabeled clients in each round, whichmight result in significant catastrophic forgetting after each round. CBAFed (Li et al., 2023) introducedfederated class adaptive pseudo labeling, which can be seen as curriculum-paced pseudo learning from aglobal perspective (Zhang et al., 2021). It uses hard temporal ensembling (Rasmus et al., 2015; Tarvainen& Valpola, 2017; Laine & Aila, 2017) to address the stochastic variability in the global model and simplyaggregates models based on reliable data amount. These strategies overlook the informative unlabeled clientswith heterogeneous data, leading to suboptimal optimization. Therefore, a more comprehensive methodfor quantifying unlabeled clients is needed. In this work, we fill this gap by introducing SemiAnAgg, ananchor-based aggregation strategy in FedSemi, which harnesses the heterogeneity of unlabeled clients duringvaluation.",
  "i=1": ". Here, N L and N U correspond to the number of labeled andunlabeled data instances, respectively. The objective is to derive a robust global model, glob, which effectivelyleverages all the available data across all clients. The most rigorous setting, as defined in (Liang et al., 2022;Li et al., 2023), is characterized by the majority of clients possessing fully unlabeled non-IID data, withN L = 0. Federated Warm-up. To ensure reliable pseudo-labels for unlabeled clients in initial stages, previousFedSemi approaches (Li et al., 2023; Liang et al., 2022) have employed a warm-up phase based on labeledclients using traditional FedAvg (McMahan et al., 2017). In this paper, we follow the generic FedSemibenchmark (Liang et al., 2022; Li et al., 2023) in label-dependent federated warmup for all reported results,yet we present additional experiments in Appendix A with federated self-supervised warmup (Zhang et al.,2020; Lubana et al., 2022; Zhuang et al., 2022; Kim et al., 2023a) highlighting their effectiveness in case ofworking with limited and highly imbalanced datasets (Yang & Xu, 2020; Yan et al., 2022). Local Training. Each client C performs E local steps based on the local dataset Dclient. Given our proposedaggregation method is perpendicular to self-training regimes (Tarvainen & Valpola, 2017; Chen et al., 2023;Wang et al., 2023), we adopt the canonical supervised training and self-training regimes on labeled andunlabeled data, respectively. Specifically, we utilize FlexMatch (Zhang et al., 2021) as our baseline to minimizea local objective function:",
  "FedAvg-Semi: A Strong Aggregation Baseline for Fedsemi": "Previous FedSemi approaches (Cho et al., 2023; Liang et al., 2022; Li et al., 2023) show that assigninga higher weight to labeled clients on the server produces better performance than traditional FedAvgaggregation (McMahan et al., 2017). Notably, semi-supervised learning necessitates a weighted combinationof supervised and unsupervised loss (Equation 1), with prior semi-supervised regimes (Sohn et al., 2020;Wang et al., 2023; Zhang et al., 2021) equally weighting Lsup and Lunsup to avoid bias towards potentiallyincorrect pseudo-labels. Consequently, it is crucial to re-adjust the global optimization objective infederated semi-supervised learning. Unlike existing FedSemi approaches (Cho et al., 2023; Liang et al.,2022; Li et al., 2023) that aggregate clients on the server based on traditional FedAvg (McMahan et al., 2017),we disentangle the aggregation based on labeled and unlabeled data on the server to write a more genericform, termed FedAvg-Semi. Given K clients, with number of labeled samples, N L, and the selected number of unlabeled samples contribut-ing for Lunsup in each client local training, N U, FedAvg-Semi dynamically disentangles FedAvg (McMahanet al., 2017) to ensure a global optimization objective consistent with traditional semi-supervised optimizationas follows:",
  "k=1N Uk ,(2)": "where 1 + 2 = 1 to ensure a normalized mean, and control the optimization of the labeled and unlabeleddata consistent with semi-supervised regimes. Setting 1 = 1 reduces to supervised warm-up on labeled clients.For simplicity, we set 1 = 2 = 0.5, noting that ideally these values could be ramped during federation.",
  "SemiAnAgg: Semi-Supervised Anchor-Based Aggregation": "Unlabeled clients in semi-supervised learning can provide valuable and diverse information, but their localmodels are unreliable due to erroneous pseudo-labels. In FedSemi, aggregation strategies typically aim toidentify and treat unlabeled clients local gradient divergence as noise. Nevertheless, this approach overlooksthat such divergence could be due to informative, underrepresented attributes and classes in the data.Therefore, it is crucial to develop aggregation strategies that can leverage the full potential of unlabeledclients data. To this end, we propose a novel aggregation strategy to re-think the valuation of unlabeled clients in FedSemi,capturing their diversion without relying on labeled samples. Specifically, we propose to measure thediversity in the feature space based on a reference embedding from a consistently initialized anchormodel across clients.",
  "nary of feature representations as: {(qi)}NU": "i=1, whereqi = encanch(Xi), and encanch represents a Kaiming ini-tialized random encoder with the same seedacross clients (He et al., 2015). The motivationfor using random encoders stems from the realm ofgenerative model evaluation, where they have beenshown to provide macroscopic views on distributionaldiscrepancies (Naeem et al., 2020). The storage foot-print of the feature dictionary varies, from 968KB fora client with 484 samples to 5.01MB for a client with2567 samples, totaling 15.6MB across all clients forthe ISIC-18 dataset-a mere 0.006 of the datasets2.5GB size. During the federated process, the client receives theaveraged global model glob from the server to up-date local. From this, we can extract the featurerepresentation qi = encglob(Xi), where encglob refers to the global models encoder applied to the client data Xi.Additionally, the pseudo label is generated as yi = fcglob(qi), where fcglob represents the global models fullyconnected layer used for classification. Note that qi and qi for each client are computed using the same encgloband encanch, rendering client data, Dclient as the only variable. We extract the features and compute thenecessary pseudo-labels simultaneously, maintaining the computational complexity of O(BE) per client perround, where B represents the batch count and E denotes the local epoch count.",
  "qi qi,(3)": "where q and q are normalized along the feature dimension so they lie on the unit sphere. Note q representsthe indexed features from the dictionary computed with the same initial weight distribution across all clients.Notably, the distance between q and q can serve as a consistent measure for assessing divergence. A high wc indicates that the class representation of the client is close to random. Consequently, clients withlower wc are more likely to approximate the global optimum. Given that all clients adhere to the sameanchor and global model, a client with a lower wc demonstrates greater diversity in its class representation.Therefore, rc = 1 wc reflects the class importance of each client based on diversity measurement, which",
  "Experimental Setup": "Datasets, Models, and Settings. We adhere to existing FedSemi benchmarks in all experimental settings,as established by (Li et al., 2023) and (Liang et al., 2022), while additionally expanding on multiplescenarios. Specifically, we utilize four datasets to assess the effectiveness of our approach: SVHN, CIFAR-100 (Krizhevsky, 2009) (both the standard and its long-tailed imbalanced variant with an imbalance factor of100), and the skin-lesion classification dataset, ISIC-18. Note that for imbalanced datasets, the imbalanceis global, existing in both labeled and unlabeled data with class distribution mismatch between the labeland unlabeled data. For all datasets, we reproduce the reported results of RSCFed (Liang et al., 2022) andCBAFed (Li et al., 2023) on the same non-IID federated partitioning publicly available, Dir() = 0.8. WhileCBAFed utilize ResNet-18 ImageNet version detailed in (He et al., 2016) and RSCFed use a simple CNN, wefind their lower-bound is not comprehensive. Thus, we reproduce their results by using the ResNet-18 CIFARversion detailed (He et al., 2016) for datasets with small spatial input dimensions (SVHN, CIFAR100), andthe traditional ImageNet ResNet-18 in (He et al., 2016) for ISIC-18. This results in improved reproductionof all baselines than reported in CBAFed (Li et al., 2023). To demonstrate the generalization performance, we evaluate the global model on the standard balanced testset for all datasets and report accuracy, following the conventions of previous FedSemi methods (Li et al.,2023; Liang et al., 2022). It should be noted that the ISIC-18 test set exhibits imbalanced class distribution,prompting us to provide a more comprehensive evaluation for this particular dataset. Implementation Details (A detailed version in Appendix C.) To ensure a fair comparison, we maintainconsistency in the training protocol, architecture, exact federated partitioning checkpoint, and other exper-imental settings across all methods. The same warmup model is utilized for initialization in all reportedtables unless otherwise stated, which is trained for 250 rounds for SVHN, 250 rounds for ISIC-18, and 500rounds for CIFAR-100. This is followed by FedSemi learning for an additional 500 rounds for SVHN, 500rounds for ISIC-18, and 1000 rounds for CIFAR-100 until convergence. Notably, unlike RSCFed (Liang et al.,2022), and SemiAnAgg (ours), CBAFed (Li et al., 2023) benefit from temporal ensembling and a higherlower-bound. Consequently, we initialize CBAFed (Li et al., 2023) with a temporal ensembled model tomaintain consistency. Baselines.In our evaluation, we compare our results with state-of-the-art reproducible methods:RSCFed (Liang et al., 2022), IsoFed (Saha et al., 2023), and CBAFed (Li et al., 2023).To providea more competitive baseline, we present in Appendix D and Appendix E additional baselines ablating localtraining strategies, specifically involving ACR (Wei & Gan, 2023) and SoftMatch (Chen et al., 2023) withinthe FedSemi framework.",
  "SemiAnAgg (ours)72.2448.77": "alongside nine unlabeled clients. Additionally, we expand our experiments to include multiple scenarios andablation studies. reports the quantitative results on four benchmarks, including two balanced datasetsSVHN and CIFAR-100, and two imbalanced datasets CIFAR-100-LT and ISIC-18 (skin-lesion). Results on the Balanced Global Setting. As shown in , SemiAnAgg achieves the best accuracyon two balanced datasets, SVHN and CIFAR-100, outperforming the compared methods in terms of accuracy.Specifically, SemiAnAgg surpasses RSCFed, IsoFed, and CBAFed by 9.85%, 9.21%, and 0.12%, respectively,on SVHN, which is a relatively simple dataset. On a more challenging dataset, CIFAR-100, SemiAnAggshowcases a substantial improvement of 17.25%, 16.74%, and 9.03% respectively. Results with Imbalanced Global Setting. Previous FedSemi methods did not consider the imbalancedglobal setting, which is critical in non-IID FedSemi, where class distribution mismatch between the label andunlabeled clients exists. To this end, we report the results on two imbalanced datasets (CIFAR-100LT andISIC-18) while all baselines adopting logit-adjustments (Ren et al., 2020) to account for the class-imbalance.Our SemiAnAgg outperforms the SOTA baseline, CBAFed (Li et al., 2023) with 9.52% and 2.25% onCIFAR-100-LT and ISIC-18 respectively, achieving the best performance. Results with Partially Labeled Clients. In , we compare the generic FedSemi approaches inthe scenario of having partially labeled clients on the SVHN and ISIC dataset, a subset of the setting. Inthis partially labeled setting, all clients possess 10% of their data as labeled, while the remaining 90% isconsidered unlabeled. Our SemiAnAgg method demonstrates higher performance over CBAFed (Li et al.,2023), achieving a 1.63% increase in accuracy on the SVHN dataset and a 4.41% improvement in B-Acc onISIC. Further experiments and insights are provided in Appendix C.",
  "Ablation studies": "Effectiveness of SemiAnAgg. The results of employing different aggregation strategies are presentedin . Adopting FedAvg-Semi, which aligns with the principles of traditional semi-supervised optimizationregimes, increases accuracy by 5.63% and B-Acc by 8.48% compared to the FedAvg baseline. FedAvg-Semi disentangles the aggregation of labeled and unlabeled clients, thereby avoiding the skewing of global",
  ": Ablation using FedAvg-Semi andSemiAnAgg with different random anchors.Upper: SVHN. Lower: ISIC-18": "Effects of Random Anchor Initialization. presentsthe results of our ablation study using different random seeds forthe anchor model in SemiAnAgg compared to our simple baseline,FedAvg-Semi. The upper plot illustrates the performance on thebalanced dataset, SVHN, while the lower plot shows results on theimbalanced dataset, ISIC-18. SemiAnAgg with different anchor seeds(denoted as anch and anch,2) consistently outperforms FedAvg-Semi with consistent and faster convergence. Notably, the resultsdemonstrate that the choice of anchor seed has a minimal impact onthe final performance, indicating the stability of SemiAnAgg. It isworth mentioning that the random anchor model does not introduceadditional information during local client optimization. SemiAnAgg Convergence. We analyze the convergence behaviorof SemiAnAgg on the SVHN dataset in . Given that SVHNis a relatively simple task, the pseudo-labels become highly reliable( 99%) in (a). Consequently, SemiAnAgg converges toalmost equal contribution from all clients (1/9 0.111) as shownin (b) . Notably, in the early rounds, SemiAnAgg valuesclients 9, 7, and 6 the most due to their respective distances from theanchor model. SemiAnAgg behavior is supported by the leave-one-out in (c) indicating these clients are the most significant(their removal results in the highest error rate). Conversely, clients2, 3, and 8 are weighted less in SemiAnAgg, supported by the factthat their removal does not significantly impact performance (theirremoval results in the lowest error rate). To this end, SemiAnAggsnovel setup of using a randomly initialized anchor model can measurenot only the learned probably (most distant from the random anchor) but also consider the inter-clientdivergence (relative client distance). Unlike other FedSemi approaches (Liang et al., 2022; Li et al., 2023),SemiAnAggs novel setup enables diverse unlabeled client aggregation.",
  "(b)(c)": ": SemiAnAgg convergence analysis on the SVHN dataset. Note that as training progresses, theunlabeled client has reliable pseudo labels in (a) ( 99%) given digit classification is relatively a simpletask. SemiAnAgg converges to the clients contributing almost equally 1/9 0.1111. (c) Client Importanceby leaving one unlabeled client out (Each bar corresponds to the performance drop in FedSemi where anunlabeled client is removed).",
  "SemiAnAgg (ours)39.9065.3646.0686.3848.7786.9848.8688.63": "Effects of Client Numbers. Expanding the experimental scenarios, we consider a broader range of clients.Following the rigorous tests of (Liang et al., 2022; Li et al., 2023), we conduct an ablation study with 5, 10,25, and 50 clients, where only one client is labeled and the others are unlabeled. Our SemiAnAgg outperformsthe SOTA FedSemi approach, CBAFed (Li et al., 2023), with improvements of 4.56% in B-Acc and 1.69%in AUC on the ISIC dataset when federating 50 clients as shown in . Effects of Different Heterogeneous Levels. expands the ablations with scenarios of differentheterogeneity and a broader number of unlabeled clients. Specifically, we examine the impact of clientheterogeneity by employing a Dirichlet distribution Dir() with varying values. A smaller indicatesgreater heterogeneity. The results demonstrate that the SemiAnAgg method outperforms the state-of-the-art,CBAFed (Li et al., 2023), particularly under the most challenging conditions of heterogeneity with = 0.1,achieving improvements of 6.78% in B-Acc and 5.11% in AUC on the ISIC-18 dataset. Effects of Labeled Clients. presents an ablation study on the impact of increas-ing the number of labeled clients on the ISIC dataset.Notably,all methods show improve-ments when the number of labeled clients is increased to two.Our SemiAnAgg method particu-larly outperforms the state-of-the-art CBAFed (Li et al., 2023) under the two-labeled-clients setting. One LabeledTwo Labeled 65.25 67.25 69.25 71.25 73.25 75.25 77.25 Accuracy (%) lower-boundRSCFed CBAFedSemiAnAgg (ours) 65.25 67.25 69.25 71.25 73.25 75.25 77.25",
  "Detailed metrics in Appendix C reveal a remarkable 12%increase in precision and a substantial 4.5% in B-Acc": "Privacy Implications of FedSemi. While suscepti-bility to attacks within the FedSemi framework has notbeen studied previously due to pseudo-labels adding anextra layer of stochasticity, SemiAnAgg is more privacy-preserving than the SOTA FedSemi, CBAFed (Li et al.,2023), given the latter sharing the pseudo-class count ofeach unlabeled client, while SemiAnAgg opts for sharingpseudo-diversity scalars instead. Sharing pseudo-diversityscalars adds a layer of ambiguity against attempts to deci-pher the class distribution of unlabeled clients, stemmingfrom the pseudo-diversity scalars being influenced not onlyby class count but also by the presence of minority at-tributes within a majority class, or conversely, a majorityattribute within a minority class.",
  "Limitations and Future Directions": "In the absence of labeled data from unlabeled clients, SemiAnAgg approximates their contribution bycomparing their data distribution through two consistently initialized models across clients. SemiAnAggadds a layer of feature dictionary saving, nevertheless, it is negligible compared to the dataset size and theimprovements obtained. Cosine similarity does not accurately represent the direction of the divergence givenit shares only scalars. A more comprehensive approach, potentially involving multiple anchor models orfeature representations, could better bound the similarity within the unit sphere. Nevertheless, sharing moreindependent features from clients might raise privacy concerns, which would need to be carefully addressed.Despite its simple design, SemiAnAgg achieves state-of-the-art on four different benchmarks in FedSemi,compared to state-of-the-art federated semi-supervised learning (FedSemi). FedSemi is widely addressed inclassification, whereas extending it to imbalanced regression (Yang et al., 2021; Wang & Wang, 2023) remainsa challenging future task. SemiAnAggs lack of theoretical formulations is rather due to FedSemi complexitythat has not been previously studied theoretically given the stochasticity of pseudo-labels highly dependenton initialization. We hope that our findings and insights inspire future approaches addressing the non-iidclass mismatch and imbalanced problems in both centralized and decentralized settings.",
  "Broader Impact Statement": "This paper introduces a novel federated learning aggregation, SemiAnAgg, whose goal is to enhance theutilization of unlabeled data across distributed networks, improving collaborative learning and ensuringdata confidentiality. Uniquely, SemiAnAgg promotes the diversity of unlabeled clients, an aspect previouslyunexplored, by establishing a more equitable framework. While SemiAnAgg shares model weights anddistance-derived scalars, these scalars do not reveal sensitive information due to their irreversible nature. Weacknowledge the potential societal impacts, yet no specific issues must be highlighted here. This work was partially supported by grants from the National Natural Science Foundation of China (GrantNo. 62306254) and the Hetao Shenzhen-Hong Kong Science and Technology Innovation Cooperation Zone(Project No. HZQB-KCZYB-2020083). Hao Chen, Ran Tao, Yue Fan, Yidong Wang, Jindong Wang, Bernt Schiele, Xing Xie, Bhiksha Raj, andMarios Savvides. Softmatch: Addressing the quantity-quality tradeoff in semi-supervised learning. InICLR, 2023. URL 2, 3, 5, 7, 16, 18, 22, 23",
  "Wonyong Jeong, Jaehong Yoon, Eunho Yang, and Sung Ju Hwang. Federated semi-supervised learning withinter-client consistency & disjoint learning. In ICLR, 2021. URL 2, 4": "Meirui Jiang, Holger R Roth, Wenqi Li, Dong Yang, Can Zhao, Vishwesh Nath, Daguang Xu, Qi Dou, andZiyue Xu. Fair federated medical image segmentation via client contribution estimation. In CVPR, 2023.1, 4 Hansol Kim, Youngjun Kwak, Minyoung Jung, Jinho Shin, Youngsung Kim, and Changick Kim. Protofl:Unsupervised federated learning via prototypical distillation. In ICCV, pp. 64706479, October 2023a. 5,17 Taehyeon Kim, Eric Lin, Junu Lee, Christian Lau, and Vaikkunth Mugunthan. Navigating data heterogeneityin federated learning: A semi-supervised approach for object detection. In Thirty-seventh Conference onNeural Information Processing Systems, 2023b. URL 2",
  "Haowen Lin, Jian Lou, Li Xiong, and Cyrus Shahabi. Semifed: Semi-supervised federated learning withconsistency and pseudo-labeling. ArXiv, abs/2108.09412, 2021. URL 2, 4": "Quande Liu, Hongzheng Yang, Qi Dou, and Pheng-Ann Heng. Federated semi-supervised medical imageclassification via inter-client relation matching. In Marleen de Bruijne, Philippe C. Cattin, Stphane Cotin,Nicolas Padoy, Stefanie Speidel, Yefeng Zheng, and Caroline Essert (eds.), Medical Image Computingand Computer Assisted Intervention MICCAI 2021, pp. 325335, Cham, 2021. Springer InternationalPublishing. ISBN 978-3-030-87199-4. 1, 2, 4 Ekdeep Lubana, Chi Ian Tang, Fahim Kawsar, Robert Dick, and Akhil Mathur. Orchestra: Unsupervisedfederated learning via globally consistent clustering. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song,Csaba Szepesvari, Gang Niu, and Sivan Sabato (eds.), International Conference on Machine Learning,volume 162 of Proceedings of Machine Learning Research, pp. 1446114484. PMLR, 1723 Jul 2022. URL 5, 17, 18",
  "H. B. McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Agera y Arcas. Communication-efficient learning of deep networks from decentralized data. In AISTATS, 2017. 1, 2, 5, 17, 20, 21,23": "Muhammad Ferjad Naeem, Seong Joon Oh, Youngjung Uh, Yunjey Choi, and Jaejun Yoo. Reliable fidelityand diversity metrics for generative models. In Hal Daum III and Aarti Singh (eds.), Proceedings of the 37thInternational Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research,pp. 71767185. PMLR, 1318 Jul 2020. URL 6 Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y. Ng.Readingdigits in natural images with unsupervised feature learning. In NIPS Workshop on Deep Learning andUnsupervised Feature Learning 2011, 2011. URL 16 Youngtaek Oh, Dong-Jin Kim, and In So Kweon. Daso: Distribution-aware semantics-oriented pseudo-labelfor imbalanced semi-supervised learning. In Proceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition (CVPR), pp. 97869796, 2022. 3 Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen,Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, ZacharyDeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, andSoumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In H. Wallach,H. Larochelle, A. Beygelzimer, F. d'Alch-Buc, E. Fox, and R. Garnett (eds.), NeurIPS, pp. 80248035.Curran Associates, Inc., 2019. 19",
  "Jiawei Ren, Cunjun Yu, Shunan Sheng, Xiao Ma, Haiyu Zhao, Shuai Yi, and Hongsheng Li. Balancedmeta-softmax for long-tailed visual recognition. In NeurIPS, Dec 2020. 8, 20, 21, 22": "Pramit Saha, Divyanshu Mishra, and J. Alison Noble. Rethinking semi-supervised federated learning: Howto co-train fully-labeled and fully-unlabeled client imaging data. In Hayit Greenspan, Anant Madabhushi,Parvin Mousavi, Septimiu Salcudean, James Duncan, Tanveer Syeda-Mahmood, and Russell Taylor (eds.),Medical Image Computing and Computer Assisted Intervention MICCAI 2023, pp. 414424, Cham, 2023.Springer Nature Switzerland. ISBN 978-3-031-43895-0. 1, 2, 4, 5, 7, 8",
  "Hongyi Wang, Mikhail Yurochkin, Yuekai Sun, Dimitris Papailiopoulos, and Yasaman Khazaeni. Federatedlearning with matched averaging. In ICLR, 2020. URL 4": "Yidong Wang, Hao Chen, Qiang Heng, Wenxin Hou, Yue Fan, Zhen Wu, Jindong Wang, Marios Savvides,Takahiro Shinozaki, Bhiksha Raj, Bernt Schiele, and Xing Xie. Freematch: Self-adaptive thresholding forsemi-supervised learning. In ICLR, 2023. URL 2, 3,5, 22 Ziyan Wang and Hao Wang. Variational imbalanced regression: Fair uncertainty quantification via probabilisticsmoothing. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL 11 Chen Wei, Kihyuk Sohn, Clayton Mellina, Alan Loddon Yuille, and Fan Yang. Crest: A class-rebalancingself-training framework for imbalanced semi-supervised learning. 2021 IEEE/CVF Conference on ComputerVision and Pattern Recognition (CVPR), pp. 1085210861, 2021. 3",
  "Tong Wei and Kai Gan. Towards realistic long-tailed semi-supervised learning: Consistency is all you need.In CVPR, pp. 34693478, June 2023. 3, 4, 7, 16, 18, 22": "Xiu-Shen Wei, He-Yang Xu, Faen Zhang, Yuxin Peng, and Wei Zhou. An embarrassingly simple approach tosemi-supervised few-shot learning. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho(eds.), NeurIPS, 2022. URL 3 Rui Yan, Liangqiong Qu, Qingyue Wei, Shih-Cheng Huang, Liyue Shen, D. Rubin, Lei Xing, and Yuyin Zhou.Label-efficient self-supervised federated learning for tackling data heterogeneity in medical imaging. IEEETransactions on Medical Imaging, 42:19321943, 2022. 5, 17"
}