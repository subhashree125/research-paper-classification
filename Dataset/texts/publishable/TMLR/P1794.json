{
  "Abstract": "We present Unified PDE Solvers (UPS), a data- and compute-efficient approach to developingunified neural operators for diverse families of spatiotemporal PDEs from various domains,dimensions, and resolutions. UPS embeds different PDEs into a shared representationspace and processes them using a FNO-transformer architecture. Rather than trainingthe network from scratch, which is data-demanding and computationally expensive, wewarm-start the transformer from pretrained LLMs and perform explicit alignment to reducethe modality gap while improving data and compute efficiency. The cross-modal UPSachieves state-of-the-art results on a wide range of 1D and 2D PDE families from PDEBench,outperforming existing unified models using 4 times less data and 26 times less compute.Meanwhile, it is capable of few-shot transfer to unseen PDE families and coefficients.",
  "Introduction": "Partial Differential Equations (PDEs) play a pivotal role in modeling and understanding real-world phenomena,such as fluid dynamics and heat transfer. Although there exists a rich body of classical PDE solvers (Boyd,2001; LeVeque, 2007; Moukalled et al., 2016) that are effective and mathematically proven, these solversoften incur substantial computational costs when used in practice, as they need to be re-run every time acoefficient or boundary condition changes. This motivates the development of neural operators (Li et al.,2020a; Chen & Chen, 1995; Lu et al., 2019), which use neural networks to approximate a solution map for aPDE family and can generalize to different initial/boundary conditions or coefficients. While existing neuraloperators (Lippe et al., 2023; Hao et al., 2023a; Marwah et al., 2023) have demonstrated strong performanceon various practical benchmarks (Takamoto et al., 2022; Gupta & Brandstetter, 2022), most of them aredesigned to work with a single PDE family. Training a separate model for each PDE family remains costly. Several recent works, such as Subramanian et al. (2023), MPP (McCabe et al., 2023), and DPOT1 (Haoet al., 2024), have taken initial steps towards developing foundation models for PDE solving, learning unifiedoperators that transfer across PDE families. These models are pretrained from scratch using extensiveamounts of data and compute.For example, MPP is trained with over 80,000 PDE trajectories on 8NVIDIA H100 GPUs for 200,000 steps. Despite the development costs, the resulting models are limitedin generalization abilityall existing unified models focus on pretraining with 2D PDEs. Finally, as these",
  "Published in Transactions on Machine Learning Research (11/2024)": "Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, LukeZettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXivpreprint arXiv:1907.11692, 2019. Jiasen Lu, Christopher Clark, Sangho Lee, Zichen Zhang, Savya Khosla, Ryan Marten, Derek Hoiem, andAniruddha Kembhavi. Unified-io 2: Scaling autoregressive multimodal models with vision, language,audio, and action. ArXiv, abs/2312.17172, 2023. URL Kevin Lu, Aditya Grover, Pieter Abbeel, and Igor Mordatch. Frozen pretrained transformers as universalcomputation engines. Proceedings of the AAAI Conference on Artificial Intelligence, 36(7):76287636, Jun.2022. Lu Lu, Pengzhan Jin, and George Em Karniadakis. Deeponet: Learning nonlinear operators for identi-fying differential equations based on the universal approximation theorem of operators. arXiv preprintarXiv:1910.03193, 2019.",
  "models are developed using only PDE trajectories, they do not leverage meta information that could helpdistinguish between various PDE families, such as the name of the family and the coefficients": "We present Unified PDE Solvers (UPS), which learns unified neural operators for complex time-dependentPDEs with improved efficiency and generalization ability. Unlike existing efforts that train models fromscratch, we propose a novel method to adapt pretrained Large Language Models (LLMs) to PDE solving.This is inspired by the line of work that repurposes LLMs for scientific domains like mathematics (Lewkowyczet al., 2022), computational biology (Shen et al., 2023; Vinod et al., 2023; Joachimiak et al., 2023), andchemistry (Bran et al., 2023; Shen et al., 2024). These works not only show how LLMs utilize both text andnon-text information to solve scientific problems and transfer effectively to unseen tasks, but also providestrong evidence that the general-purpose pretraining and inductive biases of LLMs could substantially reducethe sample complexity needed for adaptation. Concretely, UPS adapts pretrained LLMs to time-evolution operators that map the current state of a PDEto its future state for general spatiotemporal PDEs (see equation 1 for definition) using two keydesigns (see also and ): 1. We propose a unified data representation scheme to align PDEs with varying dimensions and physicalquantities into the same feature space. Given the space and time discretization u = {ut(x)}Tt=0, wherex Rd is the spatial variable and ut(x) is the state variable, UPS homogenizes ut(x) from diversePDEs into a shared superspace RNndmax, where dmax is the maximum dimension of x among allPDEs considered, N is the superset of the physical quantities, and n is the resolution. This frameworkof embedding lower-dimensional PDEs in a higher dimension enables UPS to model cross-dimensionalPDEs simultaneously and distinguishes us from all existing unified operators, which do not consider lowdimensional PDEs in 1D. 2. We employ a unified network architecture to predict ut+1(x) based on ut(x). To leverage pretrainedLLMs, we design a three-way architecture that consists of (i) a FNO (Li et al., 2020a) based embeddingnetwork to convert PDE data into resolution-invariant, sequential features; (ii) an LLM body to processthe PDE features and the text embeddings of the PDE metadata; and (iii) a prediction network to generatethe final output. Inspired by previous cross-modality adaptation work (Shen et al., 2023), we employ atwo-stage align-then-refine process for model training. However, we improve the align stage by using ajoint loss that adds feature extraction on top of alignment to pretrain the embedding network. We improvethe refine stage by fine-tuning on a collection of PDE tasks rather than a single task. Our enhanced",
  "workflow outperforms naive transfer and previous cross-modal approaches, reducing both the data andcompute needed for training": "By design, UPS can handle diverse PDE families, data dimensions, channels, and resolutions. More crucially,by warm-starting with pretrained LLM weights and applying explicit alignment, UPS strikes a balancebetween effectiveness and efficiencyit achieves state-of-the-art performance across 9 datasets from PDEBench(Takamoto et al., 2022) (7 in-distribution, 2 out-of-distribution), using about 20,000 training trajectories, asingle A6000, 60,000 train steps, and under 100 GPU hours. This means that we achieve better results thanexisting unified models using 4 times less data and 26 times less compute. Beyond prediction accuracy, we confirm that UPS preserves key properties of neural operators, such as grid-and resolution-invariance. We also show that UPS is compatible with a variety of LLM backbones, includingRoBERTa (Liu et al., 2019), T5 (Raffel et al., 2020), and CLIP (Radford et al., 2021), and demonstratesbetter performance when scaled to larger backbones. We believe that the model-agnostic design of UPSoffers a systematic approach to harnessing the advancements in LLMs for PDE solving, and it takes a furtherstep towards building generalized foundation models for more complex physical systems efficiently. Code isavailable at",
  "Related Work": "Recent years has seen a variety of neural-network-based methods for approximating PDE solutions. Hybridsolvers (Hsieh et al., 2019; Bar-Sinai et al., 2019; Kochkov et al., 2021) apply classical solvers like finiteelement/volumn methods (LeVeque, 2007; Moukalled et al., 2016) to a low-resolution grid and use neuralnetworks to predict the correction terms. Others directly approximate the PDE solutions with neuralnetworks (Sirignano, 2017; Raissi et al., 2019; Khoo et al., 2021; Shen et al., 2022), using variational losses (Yuet al., 2018) or physical constraints defined by the PDE (Raissi et al., 2019; Bruna et al., 2024). Being mostlyequation-specific, these methods can solve one PDE at a time. The learned models do not apply to otherPDEs in the same family, let alone other families. A more general approach involves learning neural operators (Lu et al., 2019; Li et al., 2020a;b) whichapproximate an infinite-dimensional operator between two functional spaces. For time-dependent PDEs, aneural operator maps the current state of a PDE to the next state, with quantities like initial conditionsprovided as input. Neural operators can be implemented using any architecture. For example, Fourier neuraloperator (FNO) (Li et al., 2020a) uses convolution-based integral kernels evaluated in the Fourier space.Other works also use transformer models (Cao, 2021; Li et al., 2022; Hao et al., 2023a) or U-Net (Lippe et al.,2023). Learning neural operators enables solving an entire family of PDE and they can easily adapt to newparameterizations of a PDE without fine-tuning. However, the learned operators cannot extend to differentPDE families. To facilitate operator transfer across PDE families, two recent works develop large pretrained modelsfor multiple physical systems: Subramanian et al. (2023) train FNOs on steady-state linear PDEs withperiodic boundary conditions; McCabe et al. (2023) design a new transformer architecture based on theaxial attention (Ho et al., 2020) and train it using various 2D non-linear, time-dependent PDEs. Whilethese methods show that a unified operator can outperform single-family operators, they are limited in twoaspects. First, existing unified methods consider mainly 2D PDEs for pretraining and evaluation. In contrast,UPS leverages a unified representation scheme to tackle both 1D and 2D PDEs. This method can be alsoextended to any d-dimensional systems in theory. Second, existing methods pretrain large models fromscratch and necessitate extensive GPU resources and pretraining data, which can be prohibitive to collect forhigh-dimensional complex PDEs. However, by adapting from pretrained LLMs and closing the modality gapbetween text and PDE efficiently, UPS achieves competitive results using 4x less data and 26x less compute. Beyond the aforementioned works, DPOT (Hao et al., 2024) was developed concurrently with our work andpresents an auto-regressive denoising strategy for pretraining. While DPOT has shown better transferabilityto unseen PDE tasks than MPP, it shares the same limitations of focusing on 2D problems for pretrainingand requiring large amount of data and compute (8 A800 GPUs for 500,000 steps).",
  "Methodology": "Our goal is to train unified neural operators for spatiotemporal PDEs with varying domain dimensions,coefficients, initial and boundary conditions. These PDEs could model a range of quantities that evolveover time, from scalars (e.g., pressure, density) to vectors (e.g., velocity). To achieve this, we propose UPS,which consists of a unified way to represent the PDE and a LLM-based network to model them.",
  "(1)": "where x Rd is the spatial variable, u : [0, T] Rdu is a time-varying function defined overthe domain for finite time T. Here, L is a (possibly non-linear) operator which acts on u and multiplepartial derivatives of u w.r.t the spatial variable x. u0(x) : Rdu denotes PDEs initial condition,and the operator B defines the boundary condition where y is a point on domains boundary, andh : Rdu defines the given function over the boundary2. PDE families in this form include Navier-Stokesequations, Reaction-Diffusion equations, Burgers equations, and many others that describe phenomena likefluid dynamics and heat flow over time. They also constitute most PDE benchmarks in the field of machinelearning (Takamoto et al., 2022; Tu et al., 2022; Roberts et al., 2021). Consider a set of S spatiotemporal PDEs {us}Ss=1. Here, each us = {ust(x)}Tst=1 is a solution to a PDE ofthe form defined in equation 1 such that for all t [Ts], we have ust(x) Rdsu and x s Rds, whereds is the dimension of the PDE s. For each ust, we assume that we have an n-point discretization of thefunctions {ust}Tst=1 at points W sn = {xs1, xs2, , xsnds }, where each xsi Rds. That is, for each PDE s Sand t Ts, we have the realization of the function ust on a grid with each dimension divided into n parts,thus giving rise to nds points in the set. We assume that n is constant across PDE families. We note thatthis value n is a hyperparameter, and ideally should be the minimum number of points that work well forall the PDEs considered, for example, low-viscosity Navier-Stokes may require more discretization pointscompared to the high viscosity counterparts. Denote the set of N physical quantities considered for eachPDE as V = {v1, v2, vN}. Our goal is to learn an operator G which, for a given PDE s, predicts thestate of the PDE at time t + 1 based on its state at time t [Ts], i.e., ust+1(x) = G(ust(x)). We thus need aunified representation for the inputs so a model can handle different quantities at once. Unifying Dimension Let ds denote the dimension of the PDE s and d = maxsS ds. We want to representall datasets in Rd. Thus, for PDEs with ds < d, the final d ds coordinates of xsi W sn are set to zero. Inthis work, we mainly consider PDEs defined over one- and two-dimensional domains, i.e., ds {1, 2} s S.Hence, for PDEs with ds = 1, the point x s is represented with the 2D-coordinate (x, 0). Note that ourmethodology to unify the total number of dimensions in the PDE is general and can be adapted to PDEsdefined in higher-dimensional domains as well. In the following, we will denote ust(x) as the value of thefunction ust on all the points in W sn, unless stated otherwise. Unifying Physical Quantities We consider a fixed set V = {v1, v2, vN} of N physical quantities andtrain our model on the quantities that belong to V for each PDE. The quantities we consider in this paperare velocity (in both x and y directions), pressure, and density, and they are the superset of all quantities for",
  "the PDE families we evaluate. This leads to N = 4. If a dataset does not use a particular quantity, the entiredimension corresponding to it is set to 0": "With the above procedure, we lift every PDE to a unified space so us RTsNnd s S. To obtain thedatasets for forward prediction, we generate input-output pairs via autoregressive teacher-forcing: for eachtime step t [Ts], we use ust to predict ust+1, yielding Ts 1 pairs of data from a single trajectory. We appendthe coordinates of each xsi W sn to the input and maintain an output mask to mask out the zero-paddeddimensions when computing the loss.",
  "Unified Architecture": "Transformer models have demonstrated success in various domains like natural language (e.g., Touvron et al.,2023), vision (e.g., Dosovitskiy et al., 2021), and audio processing (e.g., Lu et al., 2023). In this work, weexplore the potential of transformers for PDE solving. We break down the UPS architecture into 3 parts:an embedding network that transforms the unified representation into sequence features; the model body,consisting of the pretrained LLM layers; and a predictor that generates the prediction (). FNO Embedding Network The embedding network plays two roles. First, it projects the PDE ust(x)into the LLMs sequential embedding space Rle, where l denotes the sequence length of the embeddedfeatures and e denotes the LLMs hidden dimension. Second, it should extract key features of the PDEinput to enable subsequent transformer layers to make predictions. Therefore, we design a PDE-specificembedding network with FNO layers for feature extraction, a linear layer for dimensionality matching, and aconcatenation operator for adding metadata (). We use FNO due to its strong empirical performance (Li et al., 2020a; Takamoto et al., 2022) and its abilityto extract resolution-invariant features. As we consider maximum two-dimensional PDEs in this work, we usea series of 2D FNO layers with l channels to obtain PDE features in Rlnd. Then, to map the FNO outputto the LLMs embedding dimension, we apply a pointwise convolution with input channel nd, output channele, kernel size 1, stride 1. This yields the desired sequential features hPDE Rle. Since UPS is intended to handle diverse data from various generating sources, we leverage the PDEs metadatain addition to the input dynamics. The motivation is that LLMs can use the textual information to betterunderstand the context and characteristics of different PDEs. To implement this, we specify the metadata inthe form [PDE family][coefficients] which is embedded into sequential features hmeta using the LLMstokenizer and text embedding layer. We then concatenate the meta features and the PDE features to gethmix := [hmeta, hPDE]. Finally, we apply positional encoding and layer norm to hmix. This will be the inputto the subsequent transformer layers. In .3, we perform various ablation studies on the embedding network. We investigate the effectdifferent hyperparameters, such as the channel dimension l in FNO, and show that incorporating metadataimproves both prediction performance and generalization ability of UPS. Utilizing Pretrained LLMs The main body of a UPS model consists of pretrained transformer layers froman LLM. Thus, we pass hmix to the pretrained transformer layers, which produce the hidden states h Rle.Since there is no causal structure in the spatial dimensions of a PDE, we do not apply autoregressive maskingto hmix and allow the embedded features to attend to each other. Our design provides flexibility for using different LLMs as the model body. We show experiment resultswith multiple LLMs in .3. While different LLMs have different performance, they are competitivewith existing baselines. We also show that adapting from pretrained weights outperforms training the samearchitecture from scratch, so UPS is especially useful for low-data regimes. Linear Predictor Finally, we define a prediction head to transform the hidden state of the LLM body h tothe predicted next step of the input ust+1(x) RNnd (we predict all the physical quantities in the set V ).This is achieved by averaging over the sequence dimension of h to get shape Re, applying a linear layer tomap it to RNnd, and reshaping the results to obtain the final prediction ust+1(x). The linear predictor isshared for all PDEs.",
  "Full Workflow and Training": "We train UPS in two stages. In the first stage, we train the embedding network to align hmix with the LLMsembedding space. This is because LLMs are trained for the text modality, which has distinct characteristicsand features from physical processes like fluid dynamics and heat flow. Stage 1 reduces the modality gap toprevent the distortion of pretrained weights. Next, we fine-tune the entire model on a dataset of multiplefamilies of spatiotemporal PDEs. Stage 1: Embedding Pretraining Intuitively, there is a modality gap between text data used to traingeneral-purpose LLMs and PDEs. Previous work has also shown that directly fine-tuning pretrained LLMson non-text inputs can result in suboptimal performance (Lu et al., 2022). To address this, Shen et al. (2023)introduced ORCA, which performs distribution matching before fine-tuning to enable cross-modal adaptation.That is, given a randomly initialized embedding network, we first pretrain it to minimize the distributiondistance between the embedding networks outputin our case hmixand the text embeddings of a externalreference NLP dataset, which we denote as hLM. This process makes the cross-modal distribution resemblethe text distribution that the LLM is pretrained on. Following the ORCA work, we use the CoNLL-2003dataset (Sang & Meulder, 2003) as the reference dataset for alignment. We propose several PDE-specific improvements to the alignment process. First, unlike ORCA which uses anoptimal transport (OT) based metric for measuring the distribution distance, we use the maximum meandiscrepancy (MMD) distance for UPS. This is because the OT-based metric requires discrete class labels tocompute, making it unsuitable for PDEs. In contrast, MMD acts directly on the features hmix and is morecomputationally efficient. Thus, we define",
  "Thus, the final objective for pretraining the embedding network is the joint loss Lemb = Lalign + Ltask. Weshow in .3 that both objectives are essential to the overall performance of UPS": "Stage 2: Multi-Task Fine-TuningIn contrast to most existing neural PDE solvers, which train aseparate model for each dataset, UPS is trained using one large dataset consisting of PDE data from multiplegenerating sources (all of S). Hence, after learning the embedding network, we fine-tune the entire model (theembedding network, the LLM body, and the linear predictor) using Ltask defined in equation 3. We evaluatethe performance of UPS in .1 and find it outperforms existing single-dataset neural operators. Wealso show that UPS generalizes to unseen PDE families and coefficients (.2)the zero-shot andfew-shot adaptation performance is competitive with models specifically trained on the entire target dataset.",
  "Experiments": "Data We train and evaluate our method using PDEBench (Takamoto et al., 2022). For training, we combine7 datasets from different PDE families: Burgers Equation (1D), Advection (1D), Diffusion-Sportion (1D),Shallow-Water (2D), compressible Navier-Stokes (1D and 2D), and incompressible Navier-Stokes (2D). Weexplicitly hold out two families, 1D and 2D Diffusion-Reaction, to evaluate the generalization ability of UPS.The dataset details can be found in Appendix A.1. We autoregressively generate the predictions and use the",
  "We preprocess all the PDEs by normalizing each dataset along the channel dimension to ensure the scale ofust(x) across datasets is similar3": "Baselines We compare against two sets of baselines: (i) single-family models trained on individual PDEdatasets, including the widely used U-Net (Ronneberger et al., 2015), FNO (Li et al., 2020b), the improvedversion FFNO (Tran et al., 2023), the transformer-based GNOT (Hao et al., 2023b) and OFormer (Li et al.,2023), as well as the cross-modal ORCA (Shen et al., 2023); (ii) unified models trained on multiple datasets,including MPP (McCabe et al., 2023), DPOT (Hao et al., 2024), and a unified FNO trained using datatransformed by our unified representation scheme. We note that MPP and DPOT focus on 2D PDEs andare pretrained on 2D Navier-Stokes, Shallow-Water, and Diffusion-Reaction from PDEBench. Subramanianet al. (2023) is not included as a baseline because its models are pretrained on different PDE families (e.g.,Poissons and Helmholtz equations) not present in PDEBench. Implementation Details As noted in , UPS is compatible with any pretrained LLM. We presentour main results using RoBERTa (Liu et al., 2019) and study other backbones in ablation studies ().We set the embedding FNO channel l to 32. Since the resolution of the 2D datasets in PDEBench is 128, weset the model resolution n to 128 and downsample datasets with higher resolutions. All of our experimentscan be run on a single NVIDIA A6000 GPU. See Appendix A.2 for training details. Due to computationalconstraints, results are based on a single run per network configuration.",
  "Achieving State-of-the-Art Results on PDEBench with Compute Efficiency": "We first study the in-distribution performance of UPS, i.e., we evaluate UPS on the test splits of the datasetsthat are used to train UPS, which consists of PDEs that share the same boundary conditions and coefficientswith the training samples, but have different initial conditions. The results are shown in . In general,UPS with RoBERTa ranks first on all 7 datasets and improves the state-of-the-art by an order of magnitudeon many 1D datasets. We analyze the results in more details below. 3We standardize the data by subtracting the mean and dividing by the standard deviation to ensure training stability whenusing pretrained model weights. For loss computation, we apply an inverse transformation to the outputs to revert them tothe original scale. Although data normalization may affect non-linear equations, it is essential to prevent loss explosion duringfine-tuning. We leave exploring alternative methods to minimize distortion in non-linear dynamics as future work.",
  "UPS-B 0.00030.0410.00080.01919K (Full)FNO0.00140.120.00310.098ORCA 0.00340.0820.0120.0287": "In this section, we investigate the generalization (out-of-distribution) performance of UPS under three sce-narios: (i) unseen PDE families, (ii) PDEs belongingto the training families but with different coefficients,and (iii) PDEs with higher-resolution grids. Unlessotherwise specified, UPS-B is used. Unseen PDE Families As mentioned earlier, wehold out the Diffusion-Reaction equations from de-veloping UPS. We first directly evaluate UPS onthese two tasks and report the zero-shot transferperformance. Then, we study few-shot transfer byrandomly sampling k {10, 100} trajectories fromthe training sets of the held-out tasks and use themto fine-tune UPS. Lastly, we report the fine-tuningresults with the full training dataset. The results are",
  "Advection (nRMSE)0.00570.00640.0068Incomp Navier-Stokes (nRMSE)0.1190.126-": "Unseen ResolutionsZero-shot resolution refers totraining the model on a lower resolution of the inputdata and evaluating them directly on a higher resolution.PDE solvers with this ability are better equipped tohandle real-world scenarios where input data may varyin resolution due to practical constraints or sensor-basedlimitations.Recall that UPS is trained with n-pointdiscretization W sn, and we set n = 128 because most2D datasets in PDEBench has resolution 128. Now, weevaluate the performance of UPS for n {256, 512, 1024}, increasing the resolution of the input PDE. Thisis achieved by downsampling the higher-resolution inputs to make them compatible with UPS and thenupsampling the output prediction to the desired resolution. We do not fine-tune the model at all. We report the resolution generalization performance for 1D Advection Equation and 2D incompressibleNavier-Stokes in . Although the nRMSEs for both PDEs slightly increase compared to the nRMSEfor the training resolution, they outperform all baselines in .Since the numbers are similar acrosscolumns, UPS generalizes to higher resolutions in a zero-shot manner.",
  "We perform five sets of studies to ablate various design decisions in UPS. S1-S4 demonstrate why adaptingfrom pretrained LLMs is beneficial, while S5 is related to the FNO embedding network": "S1: Pretrained LLMs vs. Training From Scratch Compared to existing single-family models likeFNO, UPS uses a transformer-based architecture with more parameters and reuses the pretrained LLMweights for the model body. To show that our results are not solely attributed to the model size and thatcross-modal adaptation is important, we evaluate the models performance when we train a transformermodel from scratch using the same PDE datasets without doing anything more complicated. As shown in, training from scratch results in much worse performance than UPS, showing the benefits of adaptinga pretrained LLM. S2: Cross-Modal AlignmentWe also test the importance of the two objectives used in stage 1, i.e.,alignment loss with MMD, and task loss with nRMSE. We study three settings: (i) using only Lalign for stage1 as in Shen et al. (2023); (ii) using only Ltask for stage 1; and (iii) removing stage 1 from our workflow entirely.As shown in , while removing any objective reduces the performance across all datasets, removingthe task loss has a more significant negative effect. Meanwhile, removing the entire stage of embeddingpretraining hurts prediction accuracy. This shows that simply fine-tuning the LLM without considering themodality gap or learning to extract PDE features is ineffective. S3: Incorporating Text-Form Metadata UPS leverages the PDEs metadata by combining its textembeddings with the learned PDE embeddings. To study whether incorporating such metadata is helpfuland identify an optimal approach, we compare our workflow with two alternatives: (i) we do not usemetadata, so hmix := hPDE; (ii) we use metadata, but instead of concatenating features from two modalities,we apply a cross-attention mechanism: hmix := softmax( QKT",
  "S5l = 320.00270.0399 0.00090.00560.00160.01530.0931l = 200.00240.04230.00090.00680.00220.01570.1043l = 80.00320.04290.00090.00710.00240.01950.1064": "and V = WV hmeta. To further investigate whether the pretrained text embeddings contribute beyondmerely labeling the PDE type, we also study alternative embedding strategies that do not leverage languagepretraining: (iii) we replace the pretrained text embeddings of the PDE meta information with one-hotencoded vectors representing each PDE type. This setting serve as a baseline to assess the impact of merelylabeling the PDE types without any semantic understanding; (iv) we also test a setting where PDE typeswere embedded using a randomly initialized embedding layer that was trained from scratch along with therest of the network, i.e., each new token represents a PDE family. The results are shown in . UPS outperforms the non-metadata baseline, demonstrating the effect ofincorporating metadata as a textual form of domain knowledge, which LLMs are able to understand. Theresults also suggest that feature concatenation is better than cross-modal attention, possibly because thelatter is harder to optimize. Lastly, the setting utilizing pretrained text embeddings consistently outperformsthe one-hot and embedding-from-scratch settings. In terms of learning dynamics, we also observe that usingpretrained embeddings demonstrated faster convergence compared to the alternative strategies. This suggeststhat the pretrained semantic knowledge in the LLM indeed contributes to processing the PDE data, not justin labeling PDE types but might also in understanding the underlying physical phenomena. However, weleave studying the the exact mechanism of cross-modal transfer and the optimal combination of metadataand PDE data as a future direction. S4: Other LLMs/VLMs To study whether UPS applies to other pretrained models, we further investigateFlan-T5 (Chung et al., 2022) and the vision language model CLIP (Radford et al., 2021). In particular,for CLIP, we use its text model to encode the metadata and its vision model to process the PDE data.The results are reported in . Since these models are trained using the same datasets and optimizerconfiguration as RoBERTa, the results are not fully optimized. Nonetheless, their performance is competitivewith existing baselines, and CLIP further outperforms RoBERTa on 3 tasks. This shows the compatibilityof UPS with diverse pretrained backbones. A future direction is to study whether optimizing the traininghyperparameters for each pretrained modelespecially VLMs like CLIP that are trained for an additionalvision modalitycould improve downstream performance. S5: FNO Embedder & Target Sequence Length As discussed in , the channel l of the FNOlayers in the embedding network determines the sequence length of the PDE features that will be fed intothe transformer layers. To study how this hyperparameter affects learning outcomes, we vary l {8, 20, 32}and report the results in . In general, increasing l improves the size and capacity of the embeddingnetwork, as well as the expressivity of the PDE features. This leads to lower prediction error. However, usingtoo many parameters for the embedding network may result in a trade-off between effectiveness and efficiency.For instance, we also experimented with l = 64 (Appendix A.3.3) and find that the longer sequence length",
  "Conclusion and Future Work": "In this paper, we present UPS, a method for adapting pretrained LLMs to unified time-evolution operators thatpredict the next state of a PDE from the current state. UPS applies to a diverse set of PDE families definedover one- and two-dimensional domains, with varying initial conditions, boundary conditions, coefficients,and resolutions. To train UPS, we develop a two-stage cross-modal adaptation protocol that first pretrains aFNO-based embedding network and aligns its hidden representations with the LLMs embedding space, andthen fine-tunes the entire model on a dataset containing diverse families of PDEs. Since UPS is adapted frompretrained models, it requires fewer training samples and compute than previous approaches for trainingunified PDE solvers from scratch. We show that UPS achieves state-of-the-art performance across multipledatasets from PDEBench and is capable of zero- and few-shot transfer to different PDE families, coefficients,and resolutions. We identify several future directions based on our work. First, we can validate our method on a broader rangeof PDEs with higher-order temporal derivatives or three-dimensional domains. Meanwhile, to seek a trulygeneral foundation model for PDE, we aim to extend the types of tasks that UPS can solve. Currently, UPSis only applicable to forward prediction. It is important to study inverse problems of parameter estimationfor different PDEs as well. For an impact statement, see Appendix A.5.",
  "Acknowledgement": "We thank Mikhail Khodak and Wenduo Cheng for providing useful feedback on the paper. This work wassupported in part by the National Science Foundation grants IIS1705121, IIS1838017, IIS2046613, IIS2112471,and funding from Meta, Morgan Stanley, Amazon, and Google. Any opinions, findings and conclusions orrecommendations expressed in this material are those of the author(s) and do not necessarily reflect the viewsof any of these funding agencies. Yohai Bar-Sinai, Stephan Hoyer, Jason Hickey, and Michael P Brenner. Learning data-driven discretizationsfor partial differential equations. Proceedings of the National Academy of Sciences, 116(31):1534415349,2019.",
  "Shuhao Cao. Choose a transformer: Fourier or galerkin. Advances in neural information processing systems,34:2492424940, 2021": "Tianping Chen and Hong Chen. Universal approximation to nonlinear operators by neural networks witharbitrary activation functions and its application to dynamical systems. IEEE Transactions on NeuralNetworks, 6(4):911917, 1995. Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, XuezhiWang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, MiracSuzgun, Xinyun Chen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pellat, Kevin Robinson, DashaValter, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, HongkunYu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and JasonWei. Scaling instruction-finetuned language models, 2022.",
  "Jayesh K Gupta and Johannes Brandstetter. Towards multi-spatiotemporal-scale generalized pde modeling.arXiv preprint arXiv:2209.15616, 2022": "Zhongkai Hao, Zhengyi Wang, Hang Su, Chengyang Ying, Yinpeng Dong, Songming Liu, Ze Cheng, JianSong, and Jun Zhu. Gnot: A general neural operator transformer for operator learning. In InternationalConference on Machine Learning, pp. 1255612569. PMLR, 2023a. Zhongkai Hao, Chengyang Ying, Zhengyi Wang, Hang Su, Yinpeng Dong, Songming Liu, Ze Cheng, JunZhu, and Jian Song. Gnot: A general neural operator transformer for operator learning. arXiv preprintarXiv:2302.14376, 2023b. Zhongkai Hao, Chang Su, Songming Liu, Julius Berner, Chengyang Ying, Hang Su, Anima Anandkumar, JianSong, and Jun Zhu. Dpot: Auto-regressive denoising operator transformer for large-scale pde pre-training,2024.",
  "Zijie Li, Kazem Meidani, and Amir Barati Farimani. Transformer for partial differential equations operatorlearning. Transactions on Machine Learning Research, 2023. ISSN 2835-8856. URL": "Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew Stuart,and Anima Anandkumar. Fourier neural operator for parametric partial differential equations. arXivpreprint arXiv:2010.08895, 2020a. Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew Stuart,and Anima Anandkumar. Neural operator: Graph kernel network for partial differential equations. arXivpreprint arXiv:2003.03485, 2020b. Phillip Lippe, Bastiaan S Veeling, Paris Perdikaris, Richard E Turner, and Johannes Brandstetter. Pde-refiner:Achieving accurate long rollouts with neural pde solvers. arXiv preprint arXiv:2308.05732, 2023.",
  "Fadl Moukalled, Luca Mangani, Marwan Darwish, F Moukalled, L Mangani, and M Darwish. The finitevolume method. Springer, 2016": "Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from naturallanguage supervision. In International conference on machine learning, pp. 87488763. PMLR, 2021. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou,Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer.The Journal of Machine Learning Research, 21(1):54855551, 2020. Maziar Raissi, Paris Perdikaris, and George E Karniadakis. Physics-informed neural networks: A deeplearning framework for solving forward and inverse problems involving nonlinear partial differential equations.Journal of Computational physics, 378:686707, 2019. Nicholas Roberts, Samuel Guo, Cong Xu, Ameet Talwalkar, David Lander, Lvfang Tao, Linhang Cai,Shuaicheng Niu, Jianyu Heng, Hongyang Qin, Minwen Deng, Johannes Hog, Alexander Pfefferle,Sushil Ammanaghatta Shivakumar, Arjun Krishnakumar, Yubo Wang, Rhea Sanjay Sukthanker, FrankHutter, Euxhen Hasanaj, Tien-Dung Le, Mikhail Khodak, Yuriy Nevmyvaka, Kashif Rasul, FredericSala, Anderson Schneider, Junhong Shen, and Evan R. Sparks.Automl decathlon: Diverse tasks,modern methods, and efficiency at scale.In Neural Information Processing Systems, 2021.URL",
  "K Spiliopoulos Sirignano, J and. A deep learning algorithm for solving partial differential equations. ArXive-prints, 2017": "Shashank Subramanian, Peter Harrington, Kurt Keutzer, Wahid Bhimji, Dmitriy Morozov, Michael Mahoney,and Amir Gholami. Towards foundation models for scientific machine learning: Characterizing scaling andtransfer behavior. arXiv preprint arXiv:2306.00258, 2023. Makoto Takamoto, Timothy Praditia, Raphael Leiteritz, Daniel MacKinlay, Francesco Alesiani, Dirk Pflger,and Mathias Niepert. Pdebench: An extensive benchmark for scientific machine learning. Advances inNeural Information Processing Systems, 35:15961611, 2022. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothe Lacroix,Baptiste Rozire, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin,Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. ArXiv,abs/2302.13971, 2023. URL",
  "A.1Datasets": "As mentioned in , we train our models using the datasets provided in the PDEBench (Takamotoet al., 2022). The time-dependent PDE families considered by our models are: Burgers Equation (1D),Diffusion-Sportion (1D), Shallow-Water (2D), compressible Navier-Stokes (1D and 2D), incompressibleNavier-Stokes (2D), and Diffusion-Reaction (1D and 2D). For each s S, the number of points in the n-pointdiscretization W sn is 128, i.e, n = 128. For PDEs where the PDEbench-provided grid has more than 128points in each dimension, we sample 128 equispaced points. In this section, we provide few key properties and considerations for the PDEs used in this paper. The initialconditions u(0, x) for most of the datasets are sampled from a superposition of sinusoidal waves. The set ofcoefficients and number of trajectories used per PDE are reported in Appendix . For full details onthe data generation process and the hyperparameters used to generate the PDE dataset, we refer the readerto Takamoto et al. (2022).",
  "A.1.7Shallow-Water Equations (2D)": "These are derived from Navier-Stokes and are a framework for modelling free-surface flow problems. Wedenote by u1(x), and u2(x) as the velocities in the horizontal and vertical directions and h as the height ofthe water and b defining the spatially varying bathymetry (the measurement of the depth of water in oceans,rivers, or lakes). The shallow-water equations are defined as follows:",
  "Num Params149M387M176M132MPer Epoch (s)3200760035003000Total (hrs)882119783": "We reported additional metrics such as FLOPs and the time required for predicting a single step for a PDEinstance in , assuming the input data is 2D with 4 channels and resolution 128. We mainly comparedwith unified models that have similar model sizes. Compared to these existing work, UPS has lower FLOPsand shorter inference time. This shows that our model is ideal for deployment in practical environmentswhere both computational efficiency and speed are critical.",
  "A.3.12D-Only UPS": ": Training UPS with all of the 2D datasets in PDEBench and compare with MPP and DPOT. Notethat beyond these PDEBench datasets, MPP is also pretrained on PDEArena (Gupta & Brandstetter, 2022)and DPOT is pretrained on PDEArena (Gupta & Brandstetter, 2022) as well as CFDBench (Yining et al.,2023). Baseline results taken from Hao et al. (2024). - means that the result is not available.",
  "A.3.3Ablation on Longer Sequence Length": "We studied the effect of embedding sequence length in .3 paragraph S5 of the main paper. Theresults show that among l = {8, 20, 32}, larger l indeed leads to better performance. However, since LLMscan support sequence lengths much longer than l = 32, we consider expanding the feature length (the numberof tokens) used to represent PDE data. See results below.",
  "l = 32 0.00270.03990.00090.00560.00160.01530.0931l = 64 0.00340.0380.00090.00540.00150.01620.0988": "While l = 64 performs slightly better on some tasks, increasing the sequence length means that (i) theembedding network is going to be larger (since l also corresponds to the width of the FNO layers), and (ii)the training time will increase as each sequence is longer. Both increase the training cost. Hence, we want toselect the l that achieves a balance between efficiency and effectiveness. Thats why we use l = 32 for ourmain experiments.",
  "A.3.4Long-Horizon Prediction": "As stated in , our method mainly focuses on predicting the next step from the current step, i.e.,ust+1(x) = G(ust(x)). However, we are also interested in the prediction capacity of our method over a longerperiod of time. Thus, we study an additional setting that predicts ust+10(x). We show non-autogressiveevaluation results since otherwise we will only have very few time steps for each test PDE trajectory. Thetable below shows that UPS is still effective for long-horizon prediction compared to baselines like FNO.Even though the prediction interval is longer, the error rates only slightly increase possible because we usenon-autoregressive evaluation, so the errors do not accumulate.",
  "We show Vx, Vy, density, and pressure": "In the prediction for 2D compressible Navier-Stokes we see a few artifacts in our generation. Furthermore,for quantities like pressure, our network often seems to generate an overly smoothened output. This couldbe because the 2D Navier-Stokes is the only PDE in our dataset that requires us to model pressure, andtherefore the network is biased towards predicting a uniform value, which in our case is 0. We believe this canbe avoided by adding more families of PDEs that model pressure, and is a fertile ground for future work.",
  "A.5Broader Impact": "This paper calls for ML communitys attention to take advantage of LLMs and apply them to a wider rangeof real-world problems beyond the NLP domains. This moves towards truly democratizing machine learningin real life. In terms of broader societal impact, our work can exert a positive influence as it contributesto reusing existing models and resources, reducing the computational burden of developing new large-scalemodels on massive data. However, lowering the barrier for applying LLMs to a wide range of tasks necessarilycomes with the risk of misuse. Hence, it is imperative to develop adaptation methods with better privacy,safety, and fairness guarantees."
}