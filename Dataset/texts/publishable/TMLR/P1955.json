{
  "Abstract": "Large neural networks pretrained on web-scale corpora are central to modern machine learn-ing. In this paradigm, the distribution of the large, heterogeneous pretraining data rarelymatches that of the application domain. This work considers modifying the pretrainingdistribution in the case where one has a small sample of data reflecting the targeted testconditions. We propose an algorithm motivated by a recent formulation of this setting asan online, bilevel optimization problem. With scalability in mind, our algorithm prioritizescomputing gradients at training points which are likely to most improve the loss on thetargeted distribution. Empirically, we show that in some cases this approach is beneficialover existing strategies from the domain adaptation literature but may not succeed in othercases. We propose a simple test to evaluate when our approach can be expected to workwell and point towards further research to address current limitations.",
  "Introduction": "Large models pretrained on massive, heterogeneous datasets have impacted various application do-mains (Bommasani et al., 2021), including natural language processing (Devlin et al., 2019), computervision (Mahajan et al., 2018), and audio processing (Schneider et al., 2019). These models are typicallytrained on two different distributions: a generic distribution for pretraining and a specific distribution forfine tuning. Only the specific distribution matches the test conditions while the generic distribution offersan abundant source of data with some similarities to the specific data. This novel paradigm builds uponearlier work in multitask learning (Caruana, 1997), transfer learning (Bennett et al., 2003), and domainadaptation (Moore & Lewis, 2010). For all of these methods, the accuracy of a model on the specific taskheavily depends on selecting an appropriate distribution over the generic auxiliary tasks and data. This work proposes a scalable online strategy for data selection along with a comprehensive and realisticempirical study1. We build upon a bilevel formulation of the generic re-weighting problem which allows forgradient-based optimization (Franceschi et al., 2018). Contributions First, we unify several gradient-based data selection methods into a common framework inwhich their similarities and distinctions are more easily understood. Second, we introduce a scalable, onlinealgorithm. This algorithm can train a large model while updating an inexpensive auxiliary data selectionmodel which tracks the distribution required to make fast progress on the targeted task. Our algorithmleverages the asymmetry in computational cost between the selection model and the large model by filteringexamples on the fly, ensuring that the majority of examples are not examined by the large model. Thisallows for much faster training on the specific distribution than pre-training on the generic distribution only. Third, we perform a comprehensive and realistic empirical comparison of data selection strategies.Wecompare several alternative strategies across different tasks and modalities including large scale language",
  "Related Work": "Prior work has proposed automatic methods to adjust the generic training distribution in order to improvemodel generalization on the specific task. The domain adaptation literature has explored variants of impor-tance sampling, which uses importance weights to emphasize or select some generic examples. These weightshave been determined via domain classifiers (Aharoni & Goldberg, 2020; Gururangan et al., 2020), via theestimation of the label distribution (Ngiam et al., 2018), or via gradient alignment and fine-tuning. Contrastive Data Selection, CDS (Moore & Lewis, 2010; van der Wees et al., 2017; Wang et al., 2018) falls intothis later category. This method has four phases: (i) an initial model is pre-trained on the generic dataset,(ii) this model is fine tuned on the specific data, (iii) the generic set is restricted to the generic data whoseloss improvement between the pre-trained model (i) and the fine tuned model (ii) is the greatest. Finally,(iv) the training of the pre-trained model (i) is resumed on the selected data from stage (iii). AlthoughCDS is a generic method applicable to any training objective, it enjoys additional properties when appliedto generative models trained to maximize the (conditional) training likelihood. It can both be considered animportance sampling method and an influence function based selection method (Grangier & Iter, 2022). Huang et al. (2006) cast weights estimation as a quadratic problem with a kernel function. Related to domainadaptation, the removal of label noise in the generic distribution has received attention with methods basedon influence functions (Koh & Liang, 2017; Pruthi et al., 2020; Schioppa et al., 2022), data models (Ilyaset al., 2022; Jain et al., 2022), and data Shapley values (Ghorbani & Zou, 2019; Karla et al., 2022). As an alternative to static weighting, the literature also explored dynamic weighting where the distributionover generic examples is adapted during training. The two primary strategies are reinforcement learning anddirect optimization. Reinforcement learning does not assume that the specific task loss can be differentiatedwith respect to the weighting parameters. Instead, a parameterized model of the generic distribution isadjusted through reinforcement learning: the current model proposes generic distributions, and their rewardis measured as the specific loss after a few steps of generic training over a proposal distribution (Kumar et al.,2019; Yoon et al., 2020; Zhu et al., 2020). On the other hand, direct optimization assumes a differentiablefunctional dependency between the weighting parameters and the specific training loss. This dependencycan be derived through meta learning by unfolding the generic update (Ren et al., 2018; Hu et al., 2019; Shuet al., 2019; Zhang & Pfister, 2021): one gradient update step minimizing the weighted generic loss dependson the weighting parameters. The impact of this update can be evaluated by computing the post-updatespecific loss which can then be differentiated with respect to the weighting parameters. As an alternativeto update unfolding, a bilevel formulation of the reweighting problem also allows for direct optimization(Franceschi et al., 2018). Our work builds upon this bilevel formulation. Other research areas intersect with sample reweighting. Ho et al. (2019); Lim et al. (2019); Zoph et al. (2020)considered learning a distribution over training data augmentations. Curriculum learning visits successivetraining distributions based on training instance difficulty (Bengio et al., 2009; Kumar et al., 2010; Jianget al., 2018; Saxena et al., 2019). Multi-task learning research has considered gradient projection to minimizenegative interactions between tasks (Yu et al., 2020; Dery et al., 2020; Liu et al., 2021). Importance samplingfor accelerated stochastic training (Zhao & Zhang, 2015; Katharopoulos & Fleuret, 2018) is also relevant.",
  "Problem Setting": "Classical machine learning assumes that the model is trained on data drawn from the distribution from whichthe test data will also be sampled from (Vapnik, 1999). Our setting is different and belongs to the field oftransfer learning (Caruana, 1993; Thrun & Pratt, 1998). We are given two training sets, a large generictraining set Dgeneric and small specific training set Dspecific. Only the latter set is representative of the testconditions. The large generic set can be leveraged as it might contain information related to the targetedspecific. Its large scale allows more reliable statistical estimation and allows training higher capacity models.",
  "Published in Transactions on Machine Learning Research (10/2024)": "Demszky, Chris Donahue, Moussa Doumbouya, Esin Durmus, Stefano Ermon, John Etchemendy, KawinEthayarajh, Li Fei-Fei, Chelsea Finn, Trevor Gale, Lauren Gillespie, Karan Goel, Noah Goodman, ShelbyGrossman, Neel Guha, Tatsunori Hashimoto, Peter Henderson, John Hewitt, Daniel E. Ho, Jenny Hong,Kyle Hsu, Jing Huang, Thomas Icard, Saahil Jain, Dan Jurafsky, Pratyusha Kalluri, Siddharth Karam-cheti, Geoff Keeling, Fereshte Khani, Omar Khattab, Pang Wei Koh, Mark Krass, Ranjay Krishna, RohithKuditipudi, Ananya Kumar, Faisal Ladhak, Mina Lee, Tony Lee, Jure Leskovec, Isabelle Levent, Xiang LisaLi, Xuechen Li, Tengyu Ma, Ali Malik, Christopher D. Manning, Suvir Mirchandani, Eric Mitchell, ZaneleMunyikwa, Suraj Nair, Avanika Narayan, Deepak Narayanan, Ben Newman, Allen Nie, Juan CarlosNiebles, Hamed Nilforoshan, Julian Nyarko, Giray Ogut, Laurel Orr, Isabel Papadimitriou, Joon SungPark, Chris Piech, Eva Portelance, Christopher Potts, Aditi Raghunathan, Rob Reich, Hongyu Ren, FriedaRong, Yusuf Roohani, Camilo Ruiz, Jack Ryan, Christopher R, Dorsa Sadigh, Shiori Sagawa, KeshavSanthanam, Andy Shih, Krishnan Srinivasan, Alex Tamkin, Rohan Taori, Armin W. Thomas, FlorianTramr, Rose E. Wang, William Wang, Bohan Wu, Jiajun Wu, Yuhuai Wu, Sang Michael Xie, MichihiroYasunaga, Jiaxuan You, Matei Zaharia, Michael Zhang, Tianyi Zhang, Xikun Zhang, Yuhui Zhang, LuciaZheng, Kaitlyn Zhou, and Percy Liang. On the opportunities and risks of foundation models, 2021.",
  "xDgenericw(x; )(x; )": "where w(x; ) denotes a smaller, secondary weighting neural network which defines a distribution overDgeneric, i.e.x, w(x; ) > 0 and xDgeneric w(x; ) = 1.The weighting network is parameterized bythe parameters . We propose to use a weighting network that is much smaller than the main model, so thedimension of is typically smaller than that of . We denote the solution to generic training problem as",
  "Data Selection as a Bilevel Optimization Problem": "The previous equations make it clear that finding the optimal weighting network is a bilevel optimizationproblem: with a fixed weighting network, the optimal parameters for the main model are found by minimizingthe weighted loss over the generic dataset, Lgeneric (Equation 1). The optimal main model parameters depends explicitly on the weighting network parameters : changing changes the optimization problemin Equation 1 and its solution. The selection of is driven by the specific set loss, Lspecific(Equation 2). Equation 1 and Equation 2 form a bilevel optimization problem (Franceschi et al., 2018): the outer problem(Equation 2) depends implicitly on through the solution to the inner problem (1). One of the strengthsof such a bilevel formulation is that the weighting network must adapt to the main model: the questionis to learn a weighting network such that the main model trained with that network leads to good specificperformance. This has the potential to go beyond a simple model-agnostic scheme that would, for instance,build w(x) based on the similarity between x and the specific set. While a large body of the literature isdevoted to solving bilevel problems where the inner problem Equation 1 is convex in (Ghadimi & Wang,",
  "i) The arg min in Equation 1 is not a single element since there are multiple minimizers. Therefore, thefunction () is not properly defined": "ii) In order to use gradient-based methods to find the optimal , we have to compute the approximateJacobian of (). This is usually done using the implicit function theorem, which only applies when theloss function in equation 1 is locally convex and such property is hard to check in practice. Furthermore, we want a method with a computational cost similar to the standard training of the mainmodel. In other words, we have enough budget to solve Equation 1 only once: learning and must becarried out synchronously. This has an important consequence: the bilevel methods that we study update based on the current main model state and not on the optimal solution (). Hence, this is a slightdeviation from the bilevel formalism. This also means that the weighting network adapts to the currentstate of the main model and, ideally, tries to up-weight generic data that is useful at the current state oflearning. We explore online algorithms to solve the bilevel problem when the main model is large. Thesealgorithms alternate and updates and leverage the asymmetry in computation cost between evaluatingthe large main model and the small auxiliary weighting network.",
  "To update the main model, we fix and do a step to minimize Equation 1.A first, natural ideawould be to take a mini-batch of generic data Bgeneric of size b, compute the corresponding gradientg =1b": "xBgeneric w(x; )(x; ) and then use it to update , either implementing SGD by doing g with > 0 a learning rate, or by using it into a more involved optimizer like Adam.However, the computation of g with the previous equation can be wasteful when a significant fraction of theexamples of Bgeneric are assigned small weights w(x; ). These examples do not contribute much to g whilestill requiring the expensive computation of their gradient (x; ). To accelerate the optimization of , we leverage the asymmetry between the cost of evaluating the weightingnetwork and the main model: computing w(x; ) only requires inference of a small network while computing(x; ) requires inference and back-propagation through a large network. We start by sampling a largebatch Bbiggeneric from the generic dataset and compute w(x; ) for each x in Bbiggeneric. From there we can takea smaller batch Bsmallgeneric from Bbiggeneric, either by sampling from the distribution defined by w(x; ) or bytaking the examples with the highest w(x; ). The first option is an unbiased solution corresponding to",
  "Updating the weighting model": "With scalability in mind, we only consider stochastic methods, i.e., that update the weighting networkparameters using only a mini-batch of specific data Bspecific and a mini-batch of generic data Bgeneric. Weconsider three alternatives to update the weighting model. Before describing alternative methods to update , we summarize our approach in Algorithm 1. We denotesample(D, n) the set resulting from sampling n times uniformly from a set D. We denote filter(D, , n) theresult from either (a) sampling n times from D i.i.d relying on the distribution induced by the weightingmodel at , or (b) selecting the top-n highest weighted examples from D. The batch sizes bsmall, blarge arehyper-parameters selected through validation.",
  "xBgenericw(x; )(x; ),(3)": "which corresponds to the value of the specific loss on the mini-batch Bspecific after a gradient descent stepfor on the generic mini-batch Bgeneric using the current weights. It is similar to (Wang et al., 2020). Theidea behind this method is that u(, ) is a reasonable approximation to (). This method requires back-propagating through a gradient descent step, which requires only a little overhead compared to a standardgradient computation. In the limit where the step size in the gradient update u(, ) goes to 0, we see thatL() gspecific, ggeneric, with gspecific =",
  "Stochastic Bilevel Algorithm (SOBA)": "We also implement the SOBA method of (Dagrou et al., 2022), which is a scalable method to solve the bilevelproblem, developed in a setting where the inner function (Equation 1) is convex. This algorithm approx-imates a gradient descent on h() = Lspecific(()). The chain rule gives h() = Lspecific(()).The optimum () satisfies the first order condition Lgeneric((), ) = 0.Under the assumptionthat the Hessian 2Lgeneric((), ) is invertible, the implicit function theorem applied to the previ-ous equation gives",
  "= 2Lgeneric((), )2Lgeneric((), )1, which overall yields h() =": "2Lgeneric((), )2Lgeneric((), )1 Lspecific(()).SOBA approximates this quantity intwo ways: first, () is replaced by the current iterate in the above gradient.Second, in additionto and , SOBA has an additional variable v of the same size as that keeps track of the quantity2Lgeneric(, )1 Lspecific().This is done using the stochastic iterations v v dv withdv =",
  "Lspecific(). We recall that the steepest normalised descent direction according to the Euclidean norm for the specific loss isnsd = arg minv {vLspecific() : v = 1}(4)": "This direction aligns with the opposite gradient when v is not further constrained. In our case, updatesshould correspond to gradient descent updates on the weighted generic loss. We therefore constraints updates to decompose as an afine combination of individual generic example gradients, i.e. ngi=1 ai(xgi , )where Dgeneric is denoted as {xgi }ngi=1 and ai > 0, i.Therefore, we need to solve Equation 4 with theconstraint v V, with V =ngi=1 ai(xgi , ), ai 0. This amounts to solving",
  "Lspecific()": "which itself is equivalent to solve minacosinengi=1 ai(xgi , ), Lspecific()We now param-eterizeaastheoutputoftheweightingnetworkandintroducetheloss,Lanograd(, )=cosine (Lgeneric(, ), Lspecific()) . The anograd method performs gradient descent on that loss to up-date . Like for DDS and Soba, we perform a single step before updating . For scalability we rely onstochastic (batch) estimates for both both terms in the cosine. Compared to DDS, the normalization inanograd reduces the benefit of up-weighting generic examples with high gradient norm.",
  "Evaluated Alternative Methods": "For our empirical comparison, we first consider two common, simple methods which do not rely on dataselection. We call baseline pretraining on the generic training set followed by fine tuning on the specificset. We call mixing pretraining on a mix of generic and specific data. Each training batch contains a fixedfraction of specific data. This fraction is selected by validation. Our first baseline is contrastive data selection, CDS, as described in section 2. As we do for all data selectionmethod, we consider further fine tuning the final CDS model on the specific training set. We also consider a domain classifier. In that case, a simple model is pretrained on a binary classificationproblem to distinguish between generic and specific training examples. The model has the same architectureas the weighting model we use with bilevel methods and it minimizes the binary cross entropy on batcheswith the same proportion of specific and generic data. This model can estimate the probability that anexample belongs to the specific set and is applied to restrict the generic set to the data with the highestestimates. We can train a model on this restricted set and later fine tuning on the specific data. Closer to our bilevel selection methods, we evaluate learning to re-weight, LTR (Ren et al., 2018) andmeta-weight net (Shu et al., 2019)). Learning to re-weight is similar to the DDS approach we presented in except it does not maintain a weighting model. Instead, at each step, the model considers a uniformdistribution over the generic batch. It then computes the gradient of this flat weighting as free parameterswith the outer update, Equation 2. This single step updates from uniform is then used to reweight thegeneric loss and update the main model, Equation 1. Compared to our work, this method does not persista weighting model across steps and does not allow learning complex distributions. The lack of weightingmodel is also less efficient since a generic example x cannot be discarded without evaluating the main modeland its gradient at x. Meta-weight net is a particular, simple case of DDS in which the weight model takes as input a single scalarfor each example: the example loss, i.e. w(x; ) = mlp((x; ); ). This parameterization is sensible for someapplications, e.g. loss based up-weighting is a common approach for active learning in classification problems",
  ": Model architectures": "Language ModelMain model: Transformer decoder with 12 layers, 8 attention heads, residual dimension of 256, feed-forward latent dimension of 1,024.Weight model: Convolutional network with 2 layers followed by mean pooling, latent dimension of 128. Translation ModelMain model: Transformer with 6 encoder layers and 6 decoder layers, 16 attention heads, residualdimension of 1,024, feed-forward latent dimension of 4,096.Weight model: Embedding layer of dimension 32 followed by an MLP with a latent dimension of 128. Image ClassifierMain model: Dual encoder clip model with ResNet 50 for images (224x224) and an multi-layer percep-tron (2 latent layers with dim. 768) on top of sentence BERT for text.Weight model: Convolutional network over 32x32 images with 4 layers of dimension 32, 32, 32 and 64. with little intrinsic uncertainty (Settles, 2009). Loss values can be indicative of an example difficulty andloss based up-weighting might accelerate learning. However, an example loss seems to be orthogonal to theexample value for domain transfer.",
  "Language Modeling": "Our language modeling (LM) experiments relies on two datasets, the C4 dataset (Raffel et al., 2019) is usedas the generic set and the RCV1 (Lewis et al., 2004) dataset is used as the specific set. C4 is a datasetof English language web pages from common crawl (Patel, 2020), while RCV1 consists of newswire storiesfrom Reuters. This setup is representative of a generic large corpus spanning different types of examples(c4) while the specific task contains an homogeneous set of examples from the same domain and from thesame source (RCV1). In our setup, we use 30m examples from C4 and 10k examples from RCV1.",
  "MethodPre-trainFine-tune": "Baseline1.198 0.0030.864 0.002Mixing0.861 0.0020.847 0.002CDS1.067 0.0050.867 0.002Domain classif.1.098 0.0020.894 0.003MetaWeightNet1.212 0.0040.868 0.003LTR1.156 0.0020.879 0.002Sparse DDS1.039 0.0042 0.824 0.003Sparse Anograd1.034 0.0022 0.824 0.002Sparse SOBA1.019 0.0031 0.820 0.002 : Language modeling: Log-perplexity (negative log-likelihood per byte) on specific (Reuters).Circled num-bers indicate the best results. Numbers after the are thestandard deviation between 5 runs. The adjective sparserefers to methods that use the sparse batch trick describedin Sec. 4.2. Our language model is a byte-level languagemodel based on the transformer decoder archi-tecture (Vaswani et al., 2017). Although sub-word language models are more common thanbyte-level ones (Sennrich et al., 2016; Al-Rfouet al., 2019), we rely on bytes to avoid our out-of-domain generalization results to be contam-inated by the mismatch between the evalua-tion data and the tokenizer training data (Rustet al., 2021). The weighting network is a smallconvolutional network. gives archi-tectural details. We also use the same archi-tecture for the domain classifier baseline. Wereport performance in terms of log-perplexity,i.e. negative log likelihood. Our implementa-tion is derived from the language model of theFlax library (Heek et al., 2020). reports the results of our languagemodeling experiments.In general, domainadaptation is beneficial in our setting. The only method trained exclusively on c4 (baseline without fine-tuning) is much worse than all alternatives except for MetaWeightNet. Before pretraining, mixing is theonly method which directly applies updates from the specific training data and it performs best. The othermethods only emphasize part of the generic set without applying specific updates during pretraining. This",
  "S. SOBA1.2101.5821.1241.2961.1841.1490.8031.1340.908+ ft.0.8721 0.8831 0.5792 0.4801 1.0352 1.1282 0.7793 0.9891 0.803": "emphasis already show a benefit at pretraining time. More importantly, this benefit is complementary tofine tuning (Iter & Grangier, 2021) and these methods yield better results that mixing+fine-tuning. Amongthem, bilevel methods perform best, with SOBA giving the highest held-out specific likelihood. We perform additional language modeling experiments with different domains. We take 9 domains from (Gaoet al., 2021) and rely on 10k specific document for each domain. The generic set (c4 dataset) and the modelarchitectures are the same as in the previous experiments. Results are given in . Data selectionmethods show that it is helpful to emphasize part of the generic set and that this emphasis is complementaryto the benefit of fine tuning. The benefit varies across domains. For instance openweb is similar to the genericset c4 and only modest gains are observed, while freelaw contains legal proceedings whose domain is surelyrelatively rare in c4. Among methods, CDS and classifier provides a strong benefit for some datasets, butonly SOBA consistently ranks among the best methods.",
  ": Machine translation: BLEU and loss on spe-cific (newstest2020)": "Our machine translation (MT) experiments learn atranslation model from English into German. Theyrely on two datasets: our generic set is the Paracrawldataset (Release 1 for WMT 2018) with 36m sen-tence pairs (Ban et al., 2020). Our specific set con-catenates the WMT newstest sets (20092019) withsource original English sentences, which amounts to10,015 sentence pairs (Akhbardeh et al., 2021). Weuse the 2020 newstest data (1,997 sentences) as ourvalidation set and leave the 2021 newstest data (1,418sentences) as our test set. Our generic set is there-fore a large crawled set with different types of textand varying translation quality while the specific setis a small set from a single domain with high qualitytranslation. Our translation system is a sub-word model based on the transformer encoder-decoder architecture. For theweighting network and the domain classifier we compose a shared embedding layer for source and target andapply a multi-layered perceptron on the contatenated averaged embeddings of the source and target sentences.",
  ": Image Classification: Accuracy on specific(ImageNet67)": "Our vision setup performs contrastive training overimage and captions CLIP (Radford et al., 2021) for generic training and image classification for spe-cific training. Specifically, contrastive learning shouldselect the correct caption within a large set of ran-dom captions. This approach also allows to performclassification by representing classes as captions ofthe form \"a photo of a <class name>\" and lettingthe model infer the most appropriate caption withinthat set.As datasets, we rely on yfcc15m (Rad-ford et al., 2021) for generic training (14.9m im-age/caption pairs) and ImageNet67 (Eshed, 2020)dataset for the specific task.Imagenet 67 consistsin 67 high level classes over Imagenet (Deng et al.,2009), e.g. building, person, fish, dog... Like for otherexperiments, we consider a setup with limited specificdata and take 2,010 specific examples, 30 per class,for training. Held-out evaluation is performed with 50 images per class. For our CLIP model, the image branch is a Resnet 50 (He et al., 2016) while the text branch applies an MLPover precomputed sentence embeddings from Sentence BERT (Reimers & Gurevych, 2019). Training appliescontrastive learning only over alternative captions: for the generic loss, we consider a buffer of past captionsas negatives; for the specific loss, we consider all the other class captions as negatives. Our weighting networkis a small convolutional network over low resolution images (32x32). gives architectural details. reports the results of our image classification experiments. Unlike for our text experiments, thebenefit of data selection is limited for this task.After fine-tuning, only the CDS and domain classifiermethods outperform the baseline model. The bilevel data selection methods do not outperform the baselinemethod. We shed light on the cause of this poor performance in our further analysis in .3.",
  "Learning a Distribution vs Learning a Curriculum": "Algorithm 1 produces a sequence of main models parameters t and weighting models parameters t, thatgo towards the solution of the bilevel problem (2). We investigate whether the weighting models parameterscorrespond to a curriculum: does the evolution of the weighting parameters t adapt to the particular dataneeded at each step, helping the model perform better than a fixed weighting scheme? We are in the LM task setup described in .2, except that we use a smaller large batch size Bbiggeneric(see .2). We run Algorithm 1 with SOBA to obtain a sequence t, t. We then compare this settingwith two new training runs with standard ERM using different data weighting:-Final weighting: a new main model is trained with fixed weighting from the weighting model T .-Shuffled weighting: a new main model is trained with a random permutation of the weights (t). shows that SOBAs curriculum is not beneficial compared to the fixed final weighting scheme on thistask. The lesser performance of shuffled weighting certainly highlight poor weighting from early t. Resultsreported in this section do not match .2 because of a smaller Bbiggeneric was used in this ablation.",
  "Big Batches: Importance Sampling vs Filtering": "In Algorithm 1, we denote filter(B, , n) the operation resulting in a smaller sub-batch of size n startingfrom the generic batch B using the weighting network parameterized by . To get an unbiased estimate ofthe re-weighted generic loss, one can apply importance sampling and sample (with replacement) from theweight distribution induced by on B. Alternatively one can instead sample without replacement from thatdistribution or restrict the batch B to its highest weighted elements. The last two alternative are biased.Nevertheless, our results in uses sampling without replacement. justifies this choice. Basically, we observe that the learned weighted distribution is concentrated alongfew examples which yield importance sampling batches to contain less diverse sets than when sampling withreplacement. Similarly, cutting the tail of the distribution (highest weights selection) drop lower weighted but still helpful examples. These experiments illustrate that gradient-based estimates fail to accountfor the long term benefit of a more diverse training set. Although sampling with replacement alleviates thisissue, more principled solutions should be investigated in future work.",
  "{anorm(x, Bspecific) > anorm(x, Bgeneric)}": "We call this measure the Specific Acceleration Rate, SAR. We would like this rate to be high, meaningthat, according to the Taylor approximation of the loss, updates collected from a batch of specific examplesshould improve the loss on a given specific examples faster than updates collected from a generic batch.Symmetrically, we define the Generic Acceleration Rate,",
  "{anorm(x, Bgeneric) > anorm(x, Bspecific)}": "It is also desirable that this rate is high. When SAR, GAR are close to chance (50%), there are two possibleexplanations, (i) either generic and specific batches have the same effect on the model, meaning that dataselection is unlikely to be helpful since training on generic is already as good as training on specific for thepurpose of minimizing the specific loss; (ii) alternatively, the linear approximation (order 1 Taylor expansion)does not help discriminating between the effect of generic and specific examples on the specific loss. In thatlater case, such a learning problem will be a challenge for bilevel optimization methods where gradientalignments indicates which part of the dataset to upweight. reports SAR and GAR for our results.These results are indicative of the empirical benefit ofbilevel optimization methods for data selection. Language modeling where DDS, Anograd and SOBA areadvantageous, has the highest SAR. Conversely, our image classification problem shows near random SAR,GAR in line with the poor performance on bilevel methods on this problem. We therefore consider thatmeasuring SAR/GAR can be a simple but informative diagnostic to assess the potential benefit of bilevelmethods on a new problem.",
  "Re-using Weighting Strategies with Larger Scale Models": "The weighting network is trained by solving the bilevel problem (2), where the loss function depends onthe models architecture. We investigate whether a weighting network learned with a small model can bere-used out-of-the-box to train a large model and get good performances on the specific set. The weightingnetwork is frozen: the large model is trained by solving min",
  "Sparse SOBABaseline": ": Specific loss as a function of time for the scaling experiment.Sparse-SOBA which learnsweights on the fly accelerates learning. Using frozen weights learned with a small model leads to evenfaster training because this removes the cost of learning the weights. The training acceleration is significant.The loss value of 1.3 is reached 8.5 (resp. 3.9) times faster by the frozen weights (resp. Sparse SOBA)method than the baseline. The loss value of 1.25 is reached 21 (resp. 5) times faster by the frozen weights(resp. Sparse SOBA) method than the baseline. Note that the Sparse-SOBA method eventually gets betterthan the frozen weights method, as reported in , but this happens after a longer training time.",
  "LargeWeights from Small1.034": "the language modeling task (.2), where the small model and large model architectures are specifiedin ; the large models architecture is the same as in .2, and it has about ten times moreparameters. We observe that the weighting network learned with the small model transfers to the largearchitecture and leads to a large decrease in the loss on the specific set, which is only slightly worse thanusing the Sparse SOBA method on the large model itself. The weighting network learned at a small scalecan seamlessly be used at a larger scale and lead to significant performance improvement on the specific set. In order to have a different perspective on the improvements provided by the reweighting methods, we displaythe training curves for that experiment in . We see that the proposed methods lead to significantlyfaster training on the specific set.",
  "Conclusions": "This work studies bilevel optimization for learning training distributions. We consider the setup where amodel is trained from two training sets, a large generic set and a small specific set, where only the later isrepresentative of the test conditions. We propose a scalable algorithm that learns a training distributionover the generic data such that the loss on the specific set is minimized. We showed that our formulationgathers independently-proposed gradient-based methods for data selection under a common framework. Weintroduced an algorithm that enables streaming through the generic dataset quickly by examining most ofthe generic samples with only an inexpensive small auxiliary model. This work reported a comprehensive andrealistic empirical comparison of data selection strategies across language modeling, machine translation andcomputer vision. We studied the conditions in which gradient-based data selection is effective and proposea diagnostic based on gradient alignment to efficiently assess these conditions.",
  "Michael Arbel and Julien Mairal. Amortized implicit differentiation for stochastic bilevel optimization. arXivpreprint arXiv:2111.14580, 2021": "Marta Ban, Pinzhen Chen, Barry Haddow, Kenneth Heafield, Hieu Hoang, Miquel Espl-Gomis, Mikel LForcada, Amir Kamran, Faheem Kirefu, Philipp Koehn, et al. Paracrawl: Web-scale acquisition of parallelcorpora. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp.45554567, 2020. Yoshua Bengio, Jrme Louradour, Ronan Collobert, and Jason Weston.Curriculum learning.In An-drea Pohoreckyj Danyluk, Lon Bottou, and Michael L. Littman (eds.), Proceedings of the 26th AnnualInternational Conference on Machine Learning, ICML 2009, Montreal, Quebec, Canada, June 14-18,2009, volume 382 of ACM International Conference Proceeding Series, pp. 4148. ACM, 2009.doi:10.1145/1553374.1553380. URL Paul N Bennett, Susan T Dumais, and Eric Horvitz. Inductive transfer for text classification using generalizedreliability indicators. In Proceedings of the ICML-2003 Workshop on The Continuum from Labeled toUnlabeled Data, 2003.",
  "Lucio M Dery, Yann Dauphin, and David Grangier. Auxiliary task update decomposition: The good, thebad and the neutral, 2020": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidi-rectional transformers for language understanding. In Proceedings of the 2019 Conference of the NorthAmerican Chapter of the Association for Computational Linguistics: Human Language Technologies, Vol-ume 1 (Long and Short Papers), pp. 41714186, Minneapolis, Minnesota, June 2019. Association forComputational Linguistics. doi: 10.18653/v1/N19-1423. URL",
  "Noam Eshed. Novelty detection and analysis in convolutional neural networks. Masters thesis, CornellUniversity, 2020": "Abolfazl Farahani, Sahar Voghoei, Khaled Rasheed, and Hamid R Arabnia.A brief review of domainadaptation. Advances in data science and information engineering: proceedings from ICDATA 2020 andIKE 2020, pp. 877894, 2021. Luca Franceschi, Paolo Frasconi, Saverio Salzo, Riccardo Grazzi, and Massimiliano Pontil.Bilevel pro-gramming for hyperparameter optimization and meta-learning. In International Conference on MachineLearning, pp. 15681577. PMLR, 2018. Yaroslav Ganin and Victor Lempitsky. Unsupervised domain adaptation by backpropagation. In Francis Bachand David Blei (eds.), Proceedings of the 32nd International Conference on Machine Learning, volume 37of Proceedings of Machine Learning Research, pp. 11801189, Lille, France, 0709 Jul 2015. PMLR. URL",
  "Amirata Ghorbani and James Zou. Data shapley: Equitable valuation of data for machine learning. InInternational Conference on Machine Learning, pp. 22422251. PMLR, 2019": "David Grangier and Dan Iter. The trade-offs of domain adaptation for neural language models. In SmarandaMuresan, Preslav Nakov, and Aline Villavicencio (eds.), Proceedings of the 60th Annual Meeting of theAssociation for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022, pp. 38023813. Association for Computational Linguistics, 2022. URL Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio Csar Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi,Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, et al. Textbooks are all you need.arXiv preprint arXiv:2306.11644, 2023.",
  "Saachi Jain, Hadi Salman, Alaa Khaddaj, Eric Wong, Sung Min Park, and Aleksander Madry. A data-basedperspective on transfer learning. arXiv preprint arXiv:2207.05739, 2022": "Lu Jiang, Zhengyuan Zhou, Thomas Leung, Li-Jia Li, and Li Fei-Fei. Mentornet: Learning data-drivencurriculum for very deep neural networks on corrupted labels. In International conference on machinelearning, pp. 23042313. PMLR, 2018. Bojan Karla, David Dao, Matteo Interlandi, Bo Li, Sebastian Schelter, Wentao Wu, and Ce Zhang.Data debugging with shapley importance over end-to-end machine learning pipelines.arXiv preprintarXiv:2204.11131, 2022.",
  "Pang Wei Koh and Percy Liang. Understanding black-box predictions via influence functions. In Internationalconference on machine learning, pp. 18851894. PMLR, 2017": "Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Balsubra-mani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, Tony Lee, Etienne David,Ian Stavness, Wei Guo, Berton Earnshaw, Imran Haque, Sara M Beery, Jure Leskovec, Anshul Kundaje,Emma Pierson, Sergey Levine, Chelsea Finn, and Percy Liang. Wilds: A benchmark of in-the-wild distri-bution shifts. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conferenceon Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp. 56375664. PMLR,1824 Jul 2021. URL",
  "Bo Liu, Xingchao Liu, Xiaojie Jin, Peter Stone, and Qiang Liu. Conflict-averse gradient descent for multi-tasklearning. Advances in Neural Information Processing Systems, 34:1887818890, 2021": "Dhruv Mahajan, Ross B. Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan Li, AshwinBharambe, and Laurens van der Maaten. Exploring the limits of weakly supervised pretraining. In VittorioFerrari, Martial Hebert, Cristian Sminchisescu, and Yair Weiss (eds.), Computer Vision - ECCV 2018 -15th European Conference, Munich, Germany, September 8-14, 2018, Proceedings, Part II, volume 11206of Lecture Notes in Computer Science, pp. 185201. Springer, 2018. Nada Matic, Isabelle Guyon, J Denker, and Vladimir Vapnik. Writer-adaptation for on-line handwrittencharacter recognition. In Proceedings of 2nd International Conference on Document Analysis and Recog-nition (ICDAR93), pp. 187191. IEEE, 1993. Robert C. Moore and William Lewis. Intelligent selection of language model training data. In Proceedingsof the ACL 2010 Conference Short Papers, pp. 220224, Uppsala, Sweden, July 2010. Association forComputational Linguistics. URL",
  "Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language modelsare unsupervised multitask learners. OpenAI blog, 1(8):9, 2019": "Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, GirishSastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models fromnatural language supervision. In International conference on machine learning, pp. 87488763. PMLR,2021. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou,Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer.arXiv e-prints, 2019. Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-networks. InProceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. Associationfor Computational Linguistics, 11 2019. URL",
  "Mengye Ren, Wenyuan Zeng, Bin Yang, and Raquel Urtasun. Learning to reweight examples for robust deeplearning. In International conference on machine learning, pp. 43344343. PMLR, 2018": "Phillip Rust, Jonas Pfeiffer, Ivan Vuli, Sebastian Ruder, and Iryna Gurevych. How good is your tokenizer?on the monolingual performance of multilingual language models. In Chengqing Zong, Fei Xia, Wenjie Li,and Roberto Navigli (eds.), Proceedings of the 59th Annual Meeting of the Association for ComputationalLinguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: LongPapers), pp. 31183135, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.243. URL Shreyas Saxena, Oncel Tuzel, and Dennis DeCoste.Data parameters:A new family of parametersfor learning a differentiable curriculum.In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems, vol-ume 32. Curran Associates, Inc., 2019.URL",
  "Steffen Schneider, Alexei Baevski, Ronan Collobert, and Michael Auli. wav2vec: Unsupervised pre-trainingfor speech recognition. arXiv preprint arXiv:1904.05862, 2019": "Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subwordunits. In Katrin Erk and Noah A. Smith (eds.), Proceedings of the 54th Annual Meeting of the Associationfor Computational Linguistics (Volume 1: Long Papers), pp. 17151725, Berlin, Germany, August 2016.Association for Computational Linguistics. doi: 10.18653/v1/P16-1162. URL",
  "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, LukaszKaiser, and Illia Polosukhin.Attention is all you need.CoRR, abs/1706.03762, 2017.URL": "Wei Wang, Taro Watanabe, Macduff Hughes, Tetsuji Nakagawa, and Ciprian Chelba. Denoising neuralmachine translation training with trusted data and online data selection. In Proceedings of the ThirdConference on Machine Translation: Research Papers, pp. 133143, Brussels, Belgium, October 2018.Association for Computational Linguistics. doi: 10.18653/v1/W18-6314. URL Xinyi Wang, Hieu Pham, Paul Michel, Antonios Anastasopoulos, Jaime Carbonell, and Graham Neubig.Optimizing data usage via differentiable rewards. In International Conference on Machine Learning, pp.99839995. PMLR, 2020.",
  "Jinsung Yoon, Sercan Arik, and Tomas Pfister. Data valuation using reinforcement learning. In InternationalConference on Machine Learning, pp. 1084210851. PMLR, 2020": "Tianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine, Karol Hausman, and Chelsea Finn. Gradientsurgery for multi-task learning. Advances in Neural Information Processing Systems, 33:58245836, 2020. Tianyi Zhang, Felix Wu, Arzoo Katiyar, Kilian Q. Weinberger, and Yoav Artzi. Revisiting few-sample BERTfine-tuning. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event,Austria, May 3-7, 2021. OpenReview.net, 2021. URL Zizhao Zhang and Tomas Pfister.Learning fast sample re-weighting without reward data.In 2021IEEE/CVF International Conference on Computer Vision, ICCV 2021, Montreal, QC, Canada, Oc-tober 10-17, 2021, pp. 705714. IEEE, 2021.doi:10.1109/ICCV48922.2021.00076.URL",
  "Peilin Zhao and Tong Zhang. Stochastic optimization with importance sampling for regularized loss mini-mization. In international conference on machine learning, pp. 19. PMLR, 2015": "Linchao Zhu, Sercan O Arik, Yi Yang, and Tomas Pfister. Learning to transfer learn: Reinforcement learning-based selection for adaptive transfer learning. In European Conference on Computer Vision, pp. 342358.Springer, 2020. Barret Zoph, Ekin D Cubuk, Golnaz Ghiasi, Tsung-Yi Lin, Jonathon Shlens, and Quoc V Le. Learning dataaugmentation strategies for object detection. In European conference on computer vision, pp. 566583.Springer, 2020.",
  "Domain Adaptation aims to improve accuracy on target distribution with insufficient labeleddata by leveraging a model trained on a different but related source distribution (Farahaniet al., 2021)": "Unsupervised Domain Adaptation considers the setting where labeled source domain data(x, y) are available for training, while only unlabeled (x) data from the target domain areavailable (Ganin & Lempitsky, 2015). Distribution Shift considers that the test distribution is different from the training distribution,usually in the context where the model cannot be retrained to adapt to the new testconditions (Koh et al., 2021).",
  "Label shift corresponds to a predictive setting where the class prior p(y) between train and testchanges but the conditional distribution p(y|x) is assumed identical (Garg et al., 2020)": "Fine-tuning is a specific domain adaptation technique which considers training a model on thetarget domain from an initial model trained on the source domain (Matic et al., 1993;Denevi et al., 2018; Zhang et al., 2021). Zero-Shot Task Transfer addressesnewtasksattesttimewithoutupdatingthemodel (Larochelle et al., 2008).It typically relies on a way to represent novel tasksin order to condition the model, e.g. text prompts (Radford et al., 2019; Srivastava et al.,2022). Few-Shot Task Transfer is similar to zero-shot transfer and does not update the model weights.As a difference, the task conditioning information provides few training instances alongwith the description of the tasks (Radford et al., 2019; Srivastava et al., 2022).",
  "BMulti-Task Extension": "We extend the proposed framework to the multi-task setting, when there are several downstream specifictasks for which one seeks a good model. We treat the outer loss as the average loss over the specific sets:let D1specific, . . . , DTspecific a set of T specific sets. In Equation 2, we simply use",
  "Baseline0.9660.9940.9420.960Classifier2 0.8960.9800.8150.892CDS1 0.8940.9470.8070.929SOBA0.9080.9450.8480.931": "of the algorithm is identical. We conduct such an experiment in the Language Modelling setting describedin subsection 5.2, where the specific sets consist of T = 3 datasets from the Pile dataset. We considertwo different subsets of 3 specific sets: dialogue sets {opensubtitles, openwebtext2, stackexchange}and academic sets {arxiv, pubmed, wikipedia}. In , we report results after fine-tuning on thespecific loss that is, models are fine-tuned on a mixture of the T = 3 datasets. We observe that the bilevelmethods and the classifier both improve over the baseline. As expected, the resulting models are good oneach dataset but are not as good as models pre-trained and fine-tuned on one dataset only, as shown insubsection 5.2."
}