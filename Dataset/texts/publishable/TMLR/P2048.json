{
  "Abstract": "Recently, program synthesis driven by large language models (LLMs) has become increas-ingly popular.However, program synthesis for machine learning (ML) tasks still posessignificant challenges.This paper explores a novel form of program synthesis, targetingML programs, by combining LLMs and automated machine learning (autoML). Specifically,our goal is to fully automate the generation and optimization of the code of the entire MLworkflow, from data preparation to modeling and post-processing, utilizing only textualdescriptions of the ML tasks. To manage the length and diversity of ML programs, wepropose to break each ML program into smaller, manageable parts. Each part is gener-ated separately by the LLM, with careful consideration of their compatibilities. To ensurecompatibilities, we design a testing technique for ML programs. Unlike traditional programsynthesis, which typically relies on binary evaluations (i.e., correct or incorrect), evaluat-ing ML programs necessitates more than just binary judgments. Our approach automatesthe numerical evaluation and optimization of these programs, selecting the best candidatesthrough autoML techniques. In experiments across various ML tasks, our method outper-forms existing methods in 10 out of 12 tasks for generating ML programs. In addition,autoML significantly improves the performance of the generated ML programs. In experi-ments, given the textual task description, our method, Text-to-ML, generates the completeand optimized ML program in a fully autonomous process.The implementation of ourmethod is available at",
  "Background, challenges, and motivation": ": One-click program synthesis for machine learning. This figure presents an example of our qualitativeresults based on the dataset Learning. Our method, Text-to-ML, transforms a textual task description intoan optimized program, encompassing data preparation, modeling, post processing, and hyperparameters.All modules are seamlessly integrated into a single executable program. The entire process, from the usersinput of a textual description to the creation of an end-to-end, optimized, and executable program, is fullyautomated. The code examples in the figure are folded and abbreviated for readability. Program synthesis is the automatic creation of computer programs that satisfy high-level specifications(Manna & Waldinger, 1971). In recent years, autoregressive large language models (LLMs) (Achiam et al.,2023; Touvron et al., 2023; Anil et al., 2023) have demonstrated impressive performance in program synthesistasks (Austin et al., 2021; Li et al., 2022; Shinn et al., 2023). However, existing LLM-based program synthesismethods primarily focus on traditional coding problems, such as interview questions for software engineeringpositions (Austin et al., 2021; Chen et al., 2023b; Madaan et al., 2023), while the synthesis of machinelearning (ML) programs remains largely unexplored (). The challenges associated with programsynthesis for ML stem from their length, diversity, and complexity in the testing phase: Length: The first step in our study is to generate an entire ML program, which consists of threetask-specific components: (1) data preparation, such as data loading, cleaning, encoding, and featureengineering; (2) modeling, which implements an ML model, such as BERT (Devlin et al., 2018); and(3) post-processing, such as performance evaluation and decoding the results ().Thesecomponents collectively demand a lengthy output from LLMs. For instance, the validly generatedcode for the ML task depicted in consists of an average of 11432 lines, in stark contrast tothe average 6.3 5.0 lines of the reference solutions in the HumanEval benchmark, which addressestraditional coding problems (Chen et al., 2021). For LLMs, the output length strongly correlateswith the difficulty of the generation process (Anil et al., 2022; Newman et al., 2020).",
  "Published in Transactions on Machine Learning Research (09/2024)": "Next, we present the prompts for the intermediate steps in Text-to-ML . For Figures 23 to 29, texts high-lighted in yellow represent prompts based on the common instruction prompt (), prompts basedon the user input (Figures 11 to 22), or prompts dynamically assembled from responses in previous steps viathe instruction constructors ({1, 2, . . . , J} in Algorithm 1. Texts highlighted in green represent promptsthat depend on the specific implementation of the pre-written modules (e.g., loss functions, optimizers, train-ing procedure, and testing procedure) and are subject to the constraints in the progenitor testing protocols( and Appendix C.1). Texts in red represent prompts configured in the background settings.",
  "Automatic code generation": "Recently, many studies have leveraged autoregressive LLMs for sequence generation tasks. For example,Chain-of-Thought (Wei et al., 2022) enhances reasoning performance by incorporating input-output exampleswithin the input sequence, while Reflexion (Shinn et al., 2023) utilizes a reinforcement learning strategy toboost generation performance by reflecting on feedback. Despite these advancements, these frameworks stillstruggle with generating ML programs (see ). To manage the length and diversity in ML programs, weintroduce a novel sequence generation framework for autoregressive LLMs, Contextual Modular Generation(Algorithm 1). As a form of compositional reasoning (Andreas et al., 2016), this framework breaks downthe programs into multiple smaller modules, with each module being generated in a separate process andrepresenting a distinct code component of the overall program (Figures 1 and 3). Unlike traditional compositional approaches (Andreas et al., 2016; Surs et al., 2023; Gupta & Kembhavi,2023; Lu et al., 2023), Contextual Modular Generation is distinctive for two features: (1) instead of hingingon a fixed inventory of pre-written modules (e.g., selecting and reusing code from a repository of pre-writtencodes for available feature engineering methods), the framework dynamically generates the essential modulestailored to each specific task. In our study, the generated modules include data preparation, modeling, andpost processing. (2) the generated modules are readily compatible without requiring additional sequencegeneration steps from LLMs for combination.This feature contributes to scalability by ensuring lineargrowth in generation complexity over program length (Theorem 3.8).",
  "Automatic unit testing": "With each module being generated in a separate process, Contextual Modular Generation requires that thegenerated modules be readily compatible with each other to reach the linear complexity in the generationprocess (Theorem 3.8). To ensure this compatibility, we devise a novel testing technique for ML programs.This technique verifies compatibility by examining that the modules exhibit consistency in data structures(such as tensor shapes) and contents (such as data features in each tensor dimension). The verification processutilizes automatically generated unit tests and synthetic data constructed from LLMs (see ). This technique not only ensures compatibility among newly generated modules but also between these mod-ules and pre-written, less-variant modules. Specifically, the technique maintains uniformity in certain mod-ules that should be identical across different program instances, such as the loss function, optimizer, trainingand testing strategies, evaluation procedure of prediction accuracy, and optimization strategy (). Contextual Modular Generation with the testing technique effectively generates syntactically valid ML pro-grams (see ). However, our findings align with studies such as Gu et al. (2022) and Kim et al. (2019),which suggest that unit tests are inadequate for ML applications (see ). Most notably, the semanticaccuracy (i.e., the prediction performance) is often far from optimal, which underlines the limited practicalutility of ML programs generated solely by LLMs and unit tests. To address this issue, we implement autoMLin pre-written modules to further evaluate and optimize the programs, capitalizing on the compatibility ofgenerated modules with pre-written modules.",
  "Automatic code optimization": ": The autoML step in our method.Each solution is a program that implements a candidatecombination of data preparation, modeling, and post-processing algorithms and associated hyperparameters.First, the optimization algorithm chooses a solution. This chosen solution is then trained and evaluated,which provides feedback to improve selections in subsequent iterations. The last step of our method is autoML ().AutoML is the process of automatically selectingalgorithms and hyperparameters for ML tasks (Thornton et al., 2013). For a task, an autoML process beginsby designating a solution and then proceeds to data processing and model training based on the solution.Subsequently, the solution is evaluated, providing feedback for future iterations to designate more promisingsolutions (). Typically, autoML utilizes optimization algorithms, such as evolutionary computationor reinforcement learning, to designate the solutions. In our study, the decision variables in the optimization(i.e., the elements of the solution) include choices and hyperparameters regarding data preparation methods,ML models, and post-processing methods implemented by the ML program. The objective function (i.e., thegoal of the solution) is the prediction performance of the ML program, measured by the numerical metrics. Due to the aforementioned complexity involved in testing ML programs, existing works in program synthesisfor ML often employ autoML methods (see .3). The autoML step in our method adapts the strategiesin ztrk et al. (2022) by incorporating zero-cost (ZC) proxies (Mellor et al., 2021) (see Appendix C.5).These proxies predict the performance of the trained neural networks using at most a single forward/backwardpropagation pass, which significantly reduces the cost from evaluating (training and testing) the models.",
  "illustrates the relation among the goals, challenges, and methods in our study. Our contributionsare summarized as follows:": "We explore the limits of program synthesis for ML. Qualitatively, for 12 supervised ML tasks overtraditional data, CV, and NLP, Text-to-ML generates the code of the end-to-end, optimized programin a fully automatic process, given textual descriptions of the tasks ( and .2). Inthe exploration, we offer a new perspective for combining LLMs and autoML. We identify a synergyin this combination that enhances program synthesis for ML (see .1). We propose a sequence generation framework Contextual Modular Generation for autoregressiveLLMs, with a theoretical analysis of scalability over sequence length (Theorem 3.5 and Theorem 3.8).To ensure compatibility among the modules, as demanded by the framework, we devise a noveltesting technique based on progenitor testing protocols (see ) and LLMs. Our approachgenerates codes that are natively compatible with optimization algorithms, removing the need forextra adaptation and facilitating the immediate deployment of code optimization. For 10 out of 12 ML tasks across modalities, Contextual Modular Generation outperforms existingmethods (.2). Furthermore, autoML substantially improves the performance of the gener-ated programs (.4). In addition, we find that ZC proxies speed up the search with bothpretrained CV and NLP models (.4).",
  "Large language models": "Let the input and output sequences be X = (x1, x2, . . . , xM) and Y = (y1, y2, . . . , yN), where each xm or ynrepresents a token in the respective sequence. Let the current state of the LLM (e.g., choice of LLM, parame-ters in the LLM, and choice of sampling method) be . The conditional probability distribution of generatingY given X is: p(Y |X) = Nn=1 p(yn|y<n, X). Throughout the paper, we denote (a1, a2, . . . , aN1) as a<N,where each an is a token (and a<N is a sequence) or each an is a sequence (and a<N is a concatenatedsequence).",
  "Automated machine learning": "AutoML methods typically involve an optimization algorithm and a search space (i.e., a set of candidatesolutions) (Zoph & Le, 2016; Liu et al., 2018). ztrk et al. (2022) explores the search space of pretraineddeep learning models. Our autoML method adapts the strategies in ztrk et al. (2022). In addition, weincorporate ZC proxies from Mellor et al. (2021) and Tanaka et al. (2020) in our method, which acceleratesthe optimization for deep learning tasks by predicting the performance of the models. Very recently, several works have explored the application of LLMs for autoML, however, none of the worksachieve automatic generation of the entire ML program. For a qualitative comparison, see Appendix A.2.",
  "Software engineering": "Program synthesis is a longstanding objective within the arctificial intelligence research community(Manna & Waldinger, 1971). Recently, there has been a surge of interest in ML for program synthesis,particularly with LLMs (Austin et al., 2021; Li et al., 2022). Conversely, program synthesis for ML is also anemerging topic and often involves autoML. Real et al. (2020) evolves programs for ML models. Saha et al.(2022); Cambronero & Rinard (2019) construct the programs for the modeling and part of data preparationfor tasks about tabular data. Leveraging LLMs, our research expands the scope of program synthesis forML by targeting the entire ML program across task modalities. Software testing techniques (Myers et al., 2004) can assist program synthesis in evaluating the programs.Software testing for ML programs is a challenging topic (Braiek & Khomh, 2020). Pei et al. (2017) andTian et al. (2018) address the challenge for computer vision tasks. Recently, Chen et al. (2022); Shinn et al.(2023) employ LLMs to automatically generate tests for traditional coding problems. Based on LLMs, thetesting technique in our study is designed for implementing the Contextual Modular Generation framework.Compared to existing works, our technique is uniquely applicable for testing ML programs at a modularlevel and across task modalities (traditional algorithms, CV, and NLP).",
  "Decomposition and compatibility": "Compared to typical sequence generation tasks, code generation demands a complex output sequence. Forexample, generating 10 lines of code to meet a certain requirement is often considerably more challenging thanan open-ended short writing of similar length. In addition, for an ML program, certain componentssuch asdata preparation, modeling, and post-processingrequire task-specific solutions. The components constitutea lengthy concatenated sequence, which can be many times longer than those found in traditional codingproblems.",
  "Formal definition": "Consider the scenario where a binary evaluation method, denoted as E, is available to test whether agenerated sequence meets a specified requirement (e.g., absence of detectable errors and presence of desiredfunctionality). For a given task, there exists a set of sequences that fulfill this requirement, representedas = (1, 2, . . . , C). Let E(Y ) denote that the sequence Y has been assessed by E and meets thespecification. To establish a fair comparison framework applicable to all sequence generation methods (see.1 and Appendix A.1), we focus on the task of repeatedly generating the target sequence until asatisfactory sequence is produced. This task involves an LLM with state and a set of input sequences Xi.In the ith iteration, the LLM generates an output sequence Yi using Xi. The process continues until theLLM produces a sequence Y , such that Y can be any sequence or any concatenation of sequences generatedin the current or previous iterations that meet the users requirements as verified by E. Algorithm 1 defines our sequence generation framework (). As a form of compositional reasoning,Algorithm 1 first divides the entire program into J modules. Then for each module j, the algorithm repeat-edly generates the code until success (lines 4 to 11). In the ith iteration, the LLM f first generates the codeY j using the instruction Xji (line 8). Then, the evaluation method for the module j, Ej, examines Y ji andproduce the feedback Ri (line 9) . In line 6, previous instructions Xj<i, generated code Y j<i, and feedbackR<i are assembled as the instruction Xji for the current iteration. A key premise of the framework is the availability of a set of evaluation methods as defined in Definition 3.1.These methods verify each modules individual functionality and compatibility with other modules.",
  ": Output: Y 1, Y 2, . . . , Y J": "Definition 3.1. For a task with desired sequences , suppose the output sequences in one decompositionare {Y 1, Y 2, . . . , Y J}. For a set of binary evaluation methods {E1, E2, . . . , EJ}. We call the set of binaryevaluation methods as contextual modular evaluations for the task, if that every Y j in {Y 1, Y 2, . . . , Y J} isverified as valid by the corresponding Ej in {E1, E2, . . . , EJ} guarantees that {Y 1, Y 2, . . . , Y J} form a validprgram (if ((j, Ej(Y j)) {Y 1, Y 2, . . . , Y J} )). In other words, if the generated modules are verified by the contextual modular evaluations, then the modulesare compatible with each other to form a desired program without any modification. Contextual modularevaluations eliminate the need for extra computation for adapting the generated modules into a sequence Ysuch that Y . In our study, the contextual modular evaluations verify individual components in the MLprogram to ensure consistency in data flow based on a novel testing technique ().",
  "Theoretical analysis": "Next, we theoretically analyze the scalability of Algorithm 1 over the length of complex output sequences.First, we define several metrics for comparing sequence generation methods. There are multiple ways todefine the output length of a sequence generation problem (Anil et al., 2022). In our study, we define theoutput length of the task as the length of the longest valid output sequence: = max{|Y ||Y }. Underthe comparison scheme defined in .2, suppose the process of repeated generation terminates andthe total number of iterations is G() (We count G() as if the iteration never terminates). Then theexpectation of generations per success ExP [G()] is a measurement of the generation efficiency. For a sequence a and an integer n, let (a, n) denote the set of sequences {s|sr (m (1 m ), |s| =) |(a, s, sr)| = m (a, s, sr) }, where sr is an arbitrary sequence (including the empty sequence) and|a| + n . Intuitively, in a sequence generation process, (a, n) represents all valid sub-sequences fromthe (|a| + 1)th position to the (|a| + n)th position in the output sequence, given that the previously generatedpart in the output sequence is a. If n = 1, then (a, n) represents all valid tokens at the (|a| + 1)th position.For example, in , let ti denotes the ith candidate token, then ((t2, t4, t6), 1) = {t1, t3}.",
  ",X () =p,X (i) | 1 i p,X (i) <": "Remark 3.3. p,X (i) represents the probability that the generation step is correct for the ith position of theoutput sequence or a correct sequence with a length shorter than i is generated. With this formulation, thegeneration process can be modeled as a Markov process with sequential transitions (successful token-levelgeneration steps) and an absorbing state (a sequence with an incorrectly generated token) (). ,X ()represents the number of tokens that are difficult to generate correctly (with a success rate less than ) and0 ,X () for any .",
  "Assumption 3.4. For the repeated generation task defined above, in every iteration, the token-level com-plexity at 1 is": "Theorem 3.5. For any sequence generation process, if there exists a step in the process that generates theentire output sequence and Assumption 3.4 holds for the step, then the best case complexity of ExP [G()]of the process is O(exp()). Remark 3.6. Assumption 3.4 posits that errors may occur for any token of the output sequence, and thevalidity of this assumption is contingent upon , , and X.Theorem 3.5 suggests that for traditionalsequence generation approaches based on autoregressive LLMs, if the task exceeds a certain complexitylevel, the generation difficulty escalates exponentially over the length of the problem. If the value of ,X ()is ascertainable or estimable for a given , regardless of the applicability of Assumption 3.4, deriving a morespecific lower bound of ExP [G()] might be possible (Appendix B).",
  "Definition 3.1) to ensure compatibility among the modules. The desiderata for the unit tests in our studyare:": "As a basic role of unit tests, for each module, the tests should ensure the module is free from errorsdetectable by the code execution environment (e.g., Python interpreter) and confirm the presenceof required functionalities, such as properties of the data processed by the module. Due to the high diversity in the code of the generated modules, the tests should be automaticallygenerated. For instance, the required tensor shapes for the modules vary significantly depending onthe ML tasks involved. Internal compatibility: In our method, for each module in the ML program, multiple code imple-mentations of different candidate methods are generated to construct multiple candidate programs.( and ). Verified candidate codes of any module should be compatible with thecandidate codes of other modules. This compatibility is crucial for the unit tests role as contextualmodular evaluations (as defined in Definition 3.1). Furthermore, for an ML program with N mod-ules, if M candidate codes for each module are generated, this compatibility increases the ratio ofgenerated program combinations over generation cost by a factor of up to M N1 (). External compatibility: Verified modules should also be compatible with pre-written modules, in-cluding those implementing a consistent evaluation procedure, training procedure, testing procedure,and autoML methods (). These criteria require a balance between variation and constraint: each test targets a task-specific modulewhile adhering to the principles of both internal and external compatibility. To address this unique challenge,we develop the testing technique Constrained Generative Unit Testing, as illustrated in . Given thatboth the inputs and outputs of all modules in ML programs are data-centric, the technique predominantlyensures consistency in the structure and content of the data flow. A detailed formulation of the technique isavailable in Algorithm 3 of Appendix C.1. : Constrained Generative Unit Testing for CV tasks. The yellow arrow represents the data flowbetween codes. The unit tests focus on verifying the structure and content of the data exchanged among thecodes. For each module, codes for different candidate methods are generated. With the pre-written modules,each pathway through the network forms an ML program. In the technique, the progenitor testing protocol is a suite of unit tests that serves as the foundationalbasis for the more specific unit tests generated by LLMs (Appendix C.1). This protocol may encompassone or several subsets of tests, each addressing a particular domain of tasks, such as NLP or CV. Eachsubset imposes minimum requirements that modules must meet for any task within the targeted domains.For instance, for typical CV tasks, the data processed by any modules should at least include one featuredimension for image height and another for image width. Besides addressing some of the requirements forinternal compatibility, the primary purpose of the protocol is to ensure external compatibilityany generatedmodules are compatible with the set of pre-written modules for the domain.",
  ": until resources are exhausted20: Output: The best ML program based on H": "As illustrates, a network of codes is generated to form multiple candidate ML programs. AutoMLthen selects the high-performance program (i.e., pathway within the network), along with associated hyper-parameter values. Leveraging the external compatibility (), autoML is implemented as pre-writtenmodules. Many autoML methods architect high-performance deep learning models from scratchsuch as determiningthe number of convolutional layers and initiating training from zero. Such a process is prohibitively expensivefor regular users under low resource constraints (ztrk et al., 2022). By comparison, the search space in ourstudy leverages the ability of LLMs to directly deploy pretrained models as candidate methods for modeling. As a similar search space is explored in ztrk et al. (2022), we employ a comparable set of hyperparametersas in ztrk et al. (2022) (Appendix C.7). By contrast, the search space in our study includes more candidatepretrained models and both CV and NLP models. To manage the expanded scope, our autoML methodincorporates four ZC proxies applicable with synthetic data for both CV and NLP (Appendix C.5), whichsignificantly mitigates the cost associated with training and testing ML programs ().",
  "Setup": "To explore the limits of program synthesis for ML, in experiments, we consider the synthesis of the entireML program, given textual descriptions of the ML task. For an ML task, let the set of valid programs (asdefined in .3) for the entire ML program of the task be the desired sequences . Given the textualdescription T, testing data Dtes, and evaluation metrics E, then the objective is to automatically generatethe program C that maximizes the evaluation score based on E and Dtes:",
  "arg maxE (C, Dtes)s.t. C": "In other words, at first, a precondition is that the generated programs are free of major syntax errors. Thenamong these programs, select the best program, by maximizing the performance score evaluated by themetrics. For other sequence generation methods, which are incompatible with autoML due to the lack ofexternal compatibility (see ), we only evaluate their efficiency in code generation (.2). For our method, we further perform code optimization (.4). We generate each candidate program Cin an iterative approach through LLMs and autoML. Given an LLM f, generation method : f T C, autoML method , training data Dtr, and training procedure H : C Dtr C, in the ithiteration, our method generates Ci:",
  "Ci = H (C, Dtr, (i)), where C = (f, T, (i))": "Setup of datasetsFor the novel problem described above, we curate a suite of datasets in our experimentsby carefully addressing the relevant factors.For each of the traditional algorithms, CV and NLP, thedatasets include 2 widely-recognized datasets for traditional studies (Appendix D.1) and 2 datasets fromthe Kaggle competitions (Appendix D.2). The widely recognized datasets can be less biased; however, thetraining corpus of the LLMs may encompass knowledge about these datasets. For the Kaggle datasets, weconduct a thorough review of all medal-awarding competitions held between October 2021 and July 2023(Appendix D.2), considering the knowledge cutoff dates of the LLMs.",
  "Generation of valid programs": "As outlined in the task of repeated generations (), we compare each sequence generation methodacross the datasets by repeatedly attempting to generate valid ML programs. This process is capped at amaximum of 100 attempts. If a method generates a valid program within these 100 attempts, the methodis reset and tasked with generating another valid program until all 100 attempts are exhausted. For allmethods, particularly Contextual Modular Generation, each sequence generation process (each time line 8in Algorithm 1 is executed) counts as an attempt. In , among the techniques, Reflexion and Contextual Modular Generation incorporate a self-reflectionmechanism, as outlined by Shinn (2023). This mechanism retains memories of previous trials, includingfeedback from the testing results of earlier iterations (see and Algorithm 1). Such feedback is usedas an input sequence to provide insights for the generation process. However, we observe that these twomethods often fall into repetitive cycles involving the same code-errors-feedback loop. To address this issue,we force a memory reset if the method fails to produce a valid program after 10 attempts, thus mitigatingthe effects of poor initiations. For CoT, we set the number of input-output examples as 3 for GPT-4 andGPT-3.5 () and 2 for PaLM 2 (). : Mean number of attempts per valid program with GPT-4.(1) CoT, Zero-shot, and Reflexionrefer to Wei et al. (2022), Kojima et al. (2022), and Shinn et al. (2023) respectively. (2) Self-Revision:the generation based on the replacement of the contextual modular evaluations in Algorithm 1 with theself-revision mechanism in Le et al. (2023). (3) for Contextual Modular Generation, w/o tests: Algorithm 1by removing the unit tests ( and Algorithm 3), w/o reflection: Algorithm 1 by removing the self-reflection mechanism in {j}, standard: Algorithm 1. (4) the datasets are divided into three groups in theorder of tasks for traditional algorithms, CV, and NLP. Details of each dataset are available in Appendix D.(5) -: the method is unable to generate any valid program for the dataset within 100 attempts.",
  "Unit tests and false positives": "In a strict sense, errors in ML programs are nearly impossible to eliminate compared to traditional codingproblems. Traditionally, software systems are constructed deductively by explicitly programming the rulesthat govern system behavior. In contrast, ML rules are inferred inductively from the data (Braiek & Khomh,2020). For example, the program may be error-free for one set of data points but buggy for another (Kimet al., 2019; Braiek & Khomh, 2020; Wang et al., 2021b). Moreover, many ML programs rely on high-levellibraries like PyTorch and TensorFlow, and third-party libraries such as the Intel Math Kernel Library, whichoften lack comprehensive specifications or even detailed source code documentation (Braiek & Khomh, 2020). Nevertheless, as in the practices of many human programmers, ML programs can still be free of majorerrors and function effectively in most daily scenarios. In our study, we define valid programs as those thatsatisfy two criteria: (1) the program executes over the datasets in the task without interruptions from errorsdetectable by the execution environment (the Python interpreter), and (2) for programs that meet the firstrequirement, we manually ensure that the programs form a complete program for the specified ML task.Specifically, we manually check whether the generated program includes a complete ML pipeline, includingdata preparation, modeling, and post processing, as specified by the LLM in Search Space Generation inAlgorithm 3 (see Appendix C.4). Similar to traditional coding problems (Li et al., 2022), an ML program can be a false positive, appearingvalid in unit tests but actually failing to meet the criteria mentioned above. In our study, each programis divided into modules, each verified by unit tests before assembly. However, modules verified as validindividually may not necessarily remain valid when combined. Therefore, the entire assembled programundergoes an additional simple test by the execution method to automatically verify the first criterion. In the experiment in .2, for our method (Contextual Modular Generation - standard ), weexamine the false positive (FP) rates of the generated programs both before and after this additional testingstep. As shows, the false positive rates for deep learning tasks are significantly higher than those oftraditional algorithms, but they are still within a reasonable range. The false positive rates of the programsafter meeting the first criterion are noticeably low, which suggests the reliability of ML programs that canbe created automatically (by employing the execution step as an additional testing step).",
  "Traditional867%0%CV7934%2%NLP9929%0%": "Notably, for the experiments in .2, we do not insist on the compatibility of generated moduleswith any pre-written module, which is more lenient towards the other methods, particularly those withouttesting, such as CoT and Zero-Shot. Achieving such external compatibility (), is infeasible formethods that do not include testing or rely on traditional testing techniques. However, in our method, thegenerated modules are also tested for compatibility, which is crucial for performing consistent training andtesting procedures and for a uniform evaluation of these modules.",
  "Improvements from optimization": "Our method ensures that generated programs are compatible with a consistent evaluation procedure. Lever-aging this unique advantage, Text-to-ML automatically searches the programs generated by ContextualModular Generation against metrics specified in the textual task descriptions. For each dataset, with GPT-4, we generate codes of up to 20 candidate algorithms, each for data preparation and modeling and at least 1code for post processing, forming up to 400 module combinations due to the internal connectivity ()provided by the testing technique (the network in ). From these modules, we first randomly sample and evaluate 200 configurations of module combinations(pathways in the network in ) and their hyperparameters. We employ two optimization strategies:random search and BOHB (Falkner et al., 2018) (Appendix C.3).For both strategies, we examine theidentified program at the cost of 25 and 100 full evaluations (i.e., the cost equivalent to the cost of fullytraining and testing 25 and 100 models). In the search, ZC proxies and BOHB prematurely terminate theevaluations for less promising configurations (Appendix C.3). Then, we rank the best program identifiedduring the search among the 200 sampled configurations. For deep learning tasks, we train each model on 1 of 4 NVIDIA A100 GPUs. For sampling the configurationsand random search, we train each model until it meets the early stopping criteria with a patience of 3 anda minimum delta of 0.0 (Prechelt, 2002). The programs generated by traditional sequence generation methods are incompatible with autoML; however,these methods do allow for the program to be specified by LLMs. As a simple baseline, we also evaluate theone ML program chosen by the LLM without any further optimization (No Search in ). We utilizethe prompt select the best configuration for the machine learning task. shows random search already yields significant improvements over the baseline, particularly withcost@25. In addition, ZC proxies effectively increase the search efficiency for both CV and NLP, improvingthe ranks of programs from the search with cost@25 by 31 12% and the search with cost@100 by 9 3%.",
  "Synergy between LLMs and autoML": "A major goal of autoML is to democratize ML by reducing the amount of effort required by human experts(Thornton et al., 2013; Wang et al., 2021a; Xin et al., 2021; Van der Blom et al., 2021). AutoML has achievedsubstantial progress in recent years on an algorithm level (Zoph & Le, 2016; Liu et al., 2018). However, inpractice, autoML methods are gradually becoming a tool for experts to potentially boost the performance",
  "2.14.0 1.54.2 0.73.8 1.0": "of the workflow (Tornede et al., 2023; Van der Blom et al., 2021), due to the following three issues: (1) mostautoML methods only select the solutions at the algorithm level. However, the programming required toimplement the algorithms can demand substantial human effort and expertise. (2) for deep learning, manyautoML methods select model architectures, such as the number of convolutional layers, from scratch andtrain the models anew, which is prohibitively expensive for regular users in practical scenarios under lowresource constraints (ztrk et al., 2022). (3) many autoML methods do not offer a comprehensive andunified solution applicable across task modalities (e.g., traditional algorithms, CV and NLP). In our study, autoML numerically evaluates and selects ML programs generated by LLMs. Conversely, LLMsoffer a solution to all three issues of autoML. As demonstrated in Figures 1 and 9 and discussed in ,LLMs can generate the programs for the entire ML workflow across task modalities, while directly utilizingpretrained models for deep learning. Without the code generation from LLMs, there would be no code forautoML to select and optimize in the first place. Our approach not only generates this code but also ensuresthat the code is natively compatible with optimization algorithms. The objectives of autoML and program synthesis for ML are closely aligned, with significant overlap in theirpurposes and methods (see .3). Nonetheless, strictly speaking, the goal of our study is on programsynthesis for ML, rather than autoML per se. This distinction arises because most autoML methods do notaddress ML tasks at the code level. In this context, autoML is employed as a step in our method (). For the goal of synthesizing ML programs, LLMs synergize with autoML: LLMs bridge the gap betweenalgorithm-centered autoML and autoML in practice, and autoML addresses the complex numerical evaluationsof the programs generated by LLMs. In this synergy, LLMs contribute extensive coding knowledge embeddedin their weights, facilitating code generation, while autoML brings robust mathematical rigor from numericaloptimization, enabling code optimization.",
  "Sequence generation and autonomous agent": "From a sequence generation perspective, our work explores the resolution of ML problems as a multi-step sequence to sequence (seq2seq) task (Sutskever et al., 2014). The input sequence is the textual taskdescription.The output sequence is the corresponding optimized program for the entire ML program.Intermediary steps include sub-seq2seq tasks based on LLMs and optimization via autoML. Our method, Text-to-ML, seamlessly integrates all intermediary steps into a single fully autonomous process( and Algorithm 2). Future research may explore such multi-step seq2seq tasks as autonomousagents. Another interesting future exploration involves incorporating text-to-text or image-to-text methodsto enhance task description generation, further automating the process.",
  "Limitations": "While the complexity of Contextual Modular Generation could increase only linearly over the problem length(Theorem 3.8), implementing this framework demands considerable effort (Appendix C.1). The practicalsignificance of the theoretical linear complexity should be more extensively tested for other types of sequencegeneration tasks. Furthermore, our approach still struggles with highly challenging ML tasks, particularlythose that involve highly complex forms of input and output data (Appendix D). Whether addressing thesetasks requires a more efficient implementation of the framework or a transition to a different frameworkremains an open problem. In particular, challenging tasks on the Kaggle platform are typically addressed through the collective efforts ofthousands of participants. The participants can freely exchange ideas and solutions, greatly enhancing theefficiency of developing high-performance solutions. Additionally, the highest-performing publicly sharedsolution often becomes the default baseline, as many participants replicate it in their solutions.Thesepractices make a meaningful comparison between the solutions from human participants and automatedsolutions infeasible. In practice, most ML tasks are tackled by only one or a few human programmers. Regardless, our work is not intended to outperform the collaborative efforts of thousands of competitionparticipants with automatic solutions. Instead, our focus is on reducing the amount of effort and knowledgerequired for writing ML programs in everyday scenarios. Qualitatively, our method achieves an unprece-dented degree of automation for ML tasks at the code level. Additionally, our work demonstrates significantperformance improvements in the generated ML programs from the automatic optimization process. To target performance enhancement in a competitive environment, a future direction could involve signif-icantly expanding the number of candidate solutions.For example, in our method, a modeling modulemight consist of a pretrained model and a custom architecture designated by the LLMs (Appendix C.2, and ). Future research could extend Text-to-ML with additional autoML techniques, suchas predictor-based methods (Dudziak et al., 2020) and weight-sharing methods (Pham et al., 2018), to searchfor the custom architecture. Whether and to what extent such a pursuit can coexist with applications forregular users under low resource constraints is a crucial consideration. Additionally, establishing a systematicbenchmark to compare automated solutions with human-generated ones is another significant challenge inthis direction.",
  "Conclusion": "This study extends the scope of program synthesis for ML. By combining LLMs and autoML, we achievethe automatic generation and optimization of the complete ML programs over 12 tasks across traditionalalgorithms, CV, and NLP. Experimentally, over 10 out of the 12 tasks, our generation framework ContextualModular Generation outperforms existing methods, particularly for deep learning tasks and tasks outsidethe training corpus of the LLMs. Theoretically, Contextual Modular Generation offers a linear best-casecomplexity class for generation difficulty over sequence length. The testing technique Constrained Generative Unit Testing is instrumental for the implementation of theframework.A notable advantage of our approach is the seamless compatibility between generated andpre-written modules, allowing for consistent numerical evaluation and native integration with optimizationalgorithms.This compatibility enables autoML-driven numerical optimization to substantially improveprogram performance. In addition, we demonstrate the effectiveness of ZC proxies for pretrained models inboth CV and NLP.",
  "Broader Impact Statement": "From the perspective of autoML, our approach aims to enhance the automation of the ML workflow, espe-cially for everyday scenarios with limited resources. Text-to-ML is a potential tool for democratizing MLby significantly reducing the required efforts, expertise, and computational resources. Users simply providea textual description of the task, and from there, the process is fully automatic, making ML in practiceaccessible to a broader audience.",
  "Accessible Machine Learning: Benefits and Risks": "By enabling a broader range of individuals and organizations to leverage ML, the Text-to-ML framework canfoster innovation, accelerate scientific discovery, and enhance decision-making across various fields. However,this increased accessibility also introduces potential risks, such as misuse by individuals with malicious intentor those lacking a proper understanding of ML principles. We list the guidelines for responsible use of Text-to-ML as below: Ethical Education: Users should complete a general ethics course in ML/AI before accessing Text-to-ML. This course would cover topics including fairness, bias, and risks in ML, and provide examplesof both responsible and irresponsible uses of ML technology. Bias and Toxicity in Data: Before applying Text-to-ML to a dataset, we recommend thoroughlychecking for common biases such as selection bias, label bias, and sampling bias to ensure thedataset represents diverse populations, avoids subjective influences, and uses appropriate samplingmethods (Mehrabi et al., 2021). Additionally, it is crucial to filter out toxicity in the data, includinghate speech, offensive content, and misinformation. Data Privacy: Users of Text-to-ML must adhere to relevant data privacy laws and regulations,such as the General Data Protection Regulation (GDPR) (Voigt & Von dem Bussche, 2017) in theEuropean Union and similar laws in other jurisdictions. For instance, using Text-to-ML on personaldata without consent or scraping data from unauthorized sources would be strictly prohibited. Application Restrictions: The use of Text-to-ML for irresponsible or harmful applications should beexplicitly prohibited. Examples include generating misleading or harmful content and developingsurveillance systems that infringe on individual privacy rights. Impact Assessment: Users should conduct an impact assessment for their ML programs using Text-to-ML, outlining the potential societal and ethical implications. For example, a project aimed atpredicting criminal behavior would need to consider the risks of reinforcing biases and stigmatizingindividuals or communities.",
  "Transparency and Accountability in the Automation of Machine Learning": "Automating the synthesis of ML programs streamlines the creation of ML models, but it can also obscurethe underlying decision-making process. This lack of transparency is particularly concerning in critical fieldssuch as healthcare, finance, criminal justice, job hiring, and college admissions. For instance, in finance,transparency is necessary to ensure fair lending practices and avoid biases in credit scoring.In collegeadmissions, understanding the rationale behind acceptance or rejection decisions is vital for maintainingfairness and avoiding biases that could disadvantage certain groups of applicants. We outline our measuresand guidelines to increase transparency and accountability for Text-to-ML as follows: Logging the Automatic Process: The implementation of Text-to-ML in our study features a com-prehensive logger that records the entire process of synthesizing the ML program. This includesdecisions regarding the search space construction (Appendix C.2), rationales behind the sugges-tions of candidate solutions (), successfully generated modules, and the performance of eachmodule combination being evaluated. We strongly recommend that other implementations of theText-to-ML framework also incorporate such a logger. This detailed logging information helps userstrace outcomes back to specific decisions and increases accountability. Tracing the Prompts and Responses of LLMs: Text-to-ML involves a set of dynamically constructedprompts (Appendix E) in intermediate steps through the instruction constructors ({1, 2, . . . , J}in Algorithm 1). The implementation of Text-to-ML in our study transparently displays the input",
  "Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein. Neural module networks. In Proceedingsof the IEEE conference on computer vision and pattern recognition, pp. 3948, 2016": "Cem Anil, Yuhuai Wu, Anders Andreassen, Aitor Lewkowycz, Vedant Misra, Vinay Ramasesh, AmbroseSlone, Guy Gur-Ari, Ethan Dyer, and Behnam Neyshabur.Exploring length generalization in largelanguage models. Advances in Neural Information Processing Systems, 35:3854638556, 2022. Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, SiamakShakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report. arXiv preprintarXiv:2305.10403, 2023. Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, EllenJiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXivpreprint arXiv:2108.07732, 2021.",
  "Stefan Falkner, Aaron Klein, and Frank Hutter. Bohb: Robust and efficient hyperparameter optimizationat scale. In International conference on machine learning, pp. 14371446. PMLR, 2018": "Jiazhen Gu, Xuchuan Luo, Yangfan Zhou, and Xin Wang. Muffin: Testing deep learning libraries via neuralarchitecture fuzzing. In Proceedings of the 44th International Conference on Software Engineering, pp.14181430, 2022. Tanmay Gupta and Aniruddha Kembhavi. Visual programming: Compositional visual reasoning withouttraining. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.1495314962, 2023.",
  "Robert R Hoffman, Shane T Mueller, Gary Klein, and Jordan Litman. Metrics for explainable ai: Challengesand prospects. arXiv preprint arXiv:1812.04608, 2018": "Noah Hollmann, Samuel Mller, and Frank Hutter. Large language models for automated data science:Introducing caafe for context-aware automated feature engineering.In Thirty-seventh Conference onNeural Information Processing Systems, 2023. Mojan Javaheripi, Gustavo de Rosa, Subhabrata Mukherjee, Shital Shah, Tomasz Religa, Caio CesarTeodoro Mendes, Sebastien Bubeck, Farinaz Koushanfar, and Debadeepta Dey. Litetransformersearch:Training-free neural architecture search for efficient language models. Advances in Neural InformationProcessing Systems, 35:2425424267, 2022.",
  "Kaggle and The Learning Agency Lab. Feedback prize - english language learning competition data. 2023": "Jinhan Kim, Robert Feldt, and Shin Yoo. Guiding deep learning system testing using surprise adequacy. In2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE), pp. 10391049. IEEE,2019. Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large languagemodels are zero-shot reasoners. Advances in neural information processing systems, 35:2219922213, 2022. Arjun Krishnakumar, Colin White, Arber Zela, Renbo Tu, Mahmoud Safari, and Frank Hutter. Nas-bench-suite-zero: Accelerating research on zero cost proxies. Advances in Neural Information Processing Systems,35:2803728051, 2022.",
  "UCI Machine Learning. Iris species. 2023": "Chengshu Li, Jacky Liang, Andy Zeng, Xinyun Chen, Karol Hausman, Dorsa Sadigh, Sergey Levine, Li Fei-Fei, Fei Xia, and Brian Ichter. Chain of code: Reasoning with a language model-augmented code emulator.arXiv preprint arXiv:2312.04474, 2023. Lisha Li, Kevin Jamieson, Giulia DeSalvo, Afshin Rostamizadeh, and Ameet Talwalkar. Hyperband: Anovel bandit-based approach to hyperparameter optimization. Journal of Machine Learning Research, 18(185):152, 2018. Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rmi Leblond, Tom Eccles,James Keeling, Felix Gimeno, Agustin Dal Lago, et al. Competition-level code generation with alphacode.Science, 378(6624):10921097, 2022.",
  "Hanxiao Liu, Karen Simonyan, and Yiming Yang. Darts: Differentiable architecture search. arXiv preprintarXiv:1806.09055, 2018": "Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, andJianfeng Gao. Chameleon: Plug-and-play compositional reasoning with large language models. arXivpreprint arXiv:2304.09842, 2023. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, NouhaDziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement with self-feedback. arXivpreprint arXiv:2303.17651, 2023.",
  "Benjamin Newman, John Hewitt, Percy Liang, and Christopher D Manning. The eos decision and lengthextrapolation. arXiv preprint arXiv:2010.07174, 2020": "Ekrem ztrk, Fabio Ferreira, Hadi Jomaa, Lars Schmidt-Thieme, Josif Grabocka, and Frank Hutter. Zero-shot automl with pretrained models. In International Conference on Machine Learning, pp. 1713817155.PMLR, 2022. Kexin Pei, Yinzhi Cao, Junfeng Yang, and Suman Jana. Deepxplore: Automated whitebox testing of deeplearning systems. In proceedings of the 26th Symposium on Operating Systems Principles, pp. 118, 2017.",
  "Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks. Advancesin neural information processing systems, 27, 2014": "Hidenori Tanaka, Daniel Kunin, Daniel L Yamins, and Surya Ganguli. Pruning neural networks withoutany data by iteratively conserving synaptic flow. Advances in neural information processing systems, 33:63776389, 2020. Chris Thornton, Frank Hutter, Holger H Hoos, and Kevin Leyton-Brown. Auto-weka: Combined selectionand hyperparameter optimization of classification algorithms. In Proceedings of the 19th ACM SIGKDDinternational conference on Knowledge discovery and data mining, pp. 847855, 2013. Yuchi Tian, Kexin Pei, Suman Jana, and Baishakhi Ray.Deeptest: Automated testing of deep-neural-network-driven autonomous cars. In Proceedings of the 40th international conference on software engi-neering, pp. 303314, 2018. Alexander Tornede, Difan Deng, Theresa Eimer, Joseph Giovanelli, Aditya Mohan, Tim Ruhkopf, SarahSegel, Daphne Theodorakopoulos, Tanja Tornede, Henning Wachsmuth, et al. Automl in the age of largelanguage models: Current challenges, future opportunities and risks. arXiv preprint arXiv:2306.08107,2023. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothe Lacroix,Baptiste Rozire, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundationlanguage models. arXiv preprint arXiv:2302.13971, 2023.",
  "Mingkai Zheng, Xiu Su, Shan You, Fei Wang, Chen Qian, Chang Xu, and Samuel Albanie. Can gpt-4perform neural architecture search? arXiv preprint arXiv:2304.10970, 2023": "Denny Zhou, Nathanael Schrli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, ClaireCui, Olivier Bousquet, Quoc Le, et al.Least-to-most prompting enables complex reasoning in largelanguage models. arXiv preprint arXiv:2205.10625, 2022a. Qinqin Zhou, Kekai Sheng, Xiawu Zheng, Ke Li, Xing Sun, Yonghong Tian, Jie Chen, and Rongrong Ji.Training-free transformer architecture search. In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition, pp. 1089410903, 2022b.",
  "A.1Large Language Models": "Iterative methods repeatedly generate the output sequences in order to improve the output sequencebased on a sequence evaluation procedure.Let the initial input sequence be X0, the sequence genera-tion process be f, and the evaluation procedure be E. Then the output sequence in the ith iteration isYi = f(i(Y<i, E(Y<i), X0)), where i represents a mechanism that assembles the input sequence for theiteration by integrating the output sequences, their evaluations, and the initial input sequence from preced-ing iterations. Recent works along this line include Yao et al. (2022); Madaan et al. (2023); Cobbe et al.(2021); Shinn et al. (2023); Chen et al. (2023b). In a reinforcement learning context, each output sequence",
  "A.2Automated Machine learning": "compares our method with existing methods that employ LLMs for autoML. A detailed analysis ofrecent advancements in this domain can be found in Tornede et al. (2023). To the best of our knowledge,Text-to-ML is the first method that automatically generates the optimized program of the entire ML programacross task modalities. While recent autoML techniques also include predictor-based methods (Dudziak et al., 2020) and weight-sharing methods (Pham et al., 2018), these techniques are less suitable for the desiderata in our study:applications in low resource constraints scenarios and search space with diverse pretrained deep learningmodels. : Comparison with existing methods in terms of qualitative features. (1) Code generation: whetherthe method automatically generates programs (2) Deep learning: whether the method automatically gener-ates (part of) programs for deep learning tasks. (3) Data preparation: whether the method automaticallychooses the strategy for data preparation (4) Modeling: whether the method automatically chooses the strat-egy for modeling (5) Optimization: whether the method optimizes the programs (components). * Althoughthe method can automatically choose the strategy for modeling, the method does not automatically generatethe code for modeling. ** The method uses the LLM as a pseudo-optimization model instead of a dedicatedoptimization algorithm (e.g., evolutionary computation and reinforcement learning.)",
  ",X,Y j() =p,X,Y j(i) | 1 i p,X,Y j(i) <": "If Assumption 3.7 holds, then in the best case, for each Y j, for each i, exists an min such that min p,X,Y j(i) (1 j J, 1 i I and I = ). Let Zj represent the number of tokens in module j (Zj M,where M is the maximum number of tokens across all modules). Then by equation (1), in the best case, for each Y j, P(Y j j) = Zjmin Mmin (1 j J). In ContextualModular Generation, each module is iteratively generated until being evaluated as valid. Therefore, we have",
  "C.1Unit Testing Details": "Regardless of the specific task and modules, the generated modules should be compatible with a pre-writtenset of modules for evaluation, training, and testing. Progenitor testing protocols first ensure the externalconnectivity by testing the form of the input and output data in the ML task, since all modules, includingdata preparation, modeling, post processing, optimizer, and loss functions, center around the input andoutput data in the ML task. In this context, distinguishing between the input/output in the ML task and for the modules is crucial.For instance, the input to the data preparation module consists of raw data, while its output comprisesthe processed input data (features) and output data (targets) in the ML task. The unit tests describedin Algorithm 3 focus on the ML tasks input and output data by scrutinizing the output produced byeach module when provided with its respective input. For all modules except data preparation, syntheticinput/output data are used for testing in the ML task. Besides the requirements for external connectivity, internal connectivity demands additional tests for eachspecific task. Therefore, the LLM in Algorithm 3 first generates a plan specifying the suitable form andcontent of the input and output data for the task. The specified form of the input and output data shouldbe a subset of the form in the progenitor testing protocols. The progenitor testing protocols also ensure thatthe synthetic data meet the form specification in the plan. In our experiments, we utilize the Python libraries Pytorch, PyTorch Lightning, and Transformers for deeplearning tasks and Scikit-learn, XGBoost, CatBoost, and LightGBM for tasks involving tabular data. Fordeep learning, the specifications in the plan include (1) the tensor types for each tensor. (2) the dimensionalityof each tensor. (3) the meaning of each dimension. (4) the size of each dimension. (5) isomorphic dimensions:dimensions that should share the same size over different settings of the data preparation process andmodeling configuration.(6) suitable ranges of the size for each dimension.For tabular data tasks, thespecifications are limited to the dimensionality of arrays for each of input and output in the ML task.",
  "C.2Search Space Generation": "We leverage the LLM to generate the search space given the textual task description. Initially, the LLM cat-egorizes the task among tabular data tasks, CV tasks, and NLP tasks. It further distinguishes among varioustask categories among the options: binary classification, multi-class classification, multi-label classification,single-output regression, multi-output regression, sequence-to-sequence, and others. For classification tasks,the LLM also determines the most appropriate output format, choosing between an integer representationof class labels and a probability representation of class labels. Following the classifications, the LLM recommends candidate methods for each module and identifies suitablehyperparameter ranges. In terms of model recommendations, a single suggestion may encompass multiplemodels. . shows an example of an LLM-generated model recommendation for NLP tasks: Search Space Generation in Algorithm 2 is intertwined with the generation of the plan for high-level require-ments of the modules in Algorithm 3. With the classification of the task and the textual task description,the LLM also decides the suitable form and content of the input and output data in the task. This decisionnot only guides the search space definition but also informs the planning for unit tests, as constraints on theform of input and output data inherently limit the program space.",
  "C.3Search Strategy": "In our study, we employ two search strategies: random search and BOHB (Falkner et al., 2018). Randomsearch is a powerful baseline in hyperparameter optimization due to its wide applicability and high searchefficiency for tasks with low effective dimensionality (Bergstra & Bengio, 2012). BOHB combines BayesianOptimization (BO) and Hyperband (HB) (Li et al., 2018) to optimize hyperparameters efficiently. BOHBinherits HBs robustness and flexibility, and BOs ability to ensure strong performance. BOHB starts byrapidly identifying promising configurations like HB and refines them using BO to achieve optimal solutions",
  "C.5ZC proxies details": "ZC proxies are an autoML technique that allows for the prediction of a fully trained neural networksperformance with small computational expenses. In this study, we utilize four ZC proxies: flops, params,naswot, and synflow.The term flops stands for Floating Point Operations Per Second, indicating thecomputational power of the network. Params denotes the total number of parameters within the network,reflecting its complexity. Although flops and params are fundamental concepts within deep learning, theyhave been validated as powerful baseline ZC proxies (Krishnakumar et al., 2022). naswot (Mellor et al., 2021) leverages the overlap of activations between data points in untrained networks toderive a measure indicative of the networks future trained performance. This approach allows for rapid andefficient searching of powerful networks without the extensive computational costs typically associated withtraining each candidate network during the search process. Originally designed for network pruning, synflowidentifies sparse trainable subnetworks within untrained neural networks. The key principle of synflow is tomaintain a balance of synaptic strengths through the network, thus preserving the networks ability to betrained post-pruning. All ZC proxies, except for params, require data to be passed to the models for evaluation. To investigate the3 ZC proxies applicability with synthetic data, we first generate the program for producing synthetic data,then sample 50 versions of synthetic data based on different random seeds with a fixed shape of the data.We observe that performance variations reported by each proxy are minimal. Notably, synflow exhibits themost significant variation, with a Mean Relative Deviation (MRD) of 0.76%, indicating the suitability of theZC proxies for synthetic data.",
  "C.6ZC Proxies Evaluation": "For deep learning tasks, after the modeling modules specified by Search Strategy are generated, the modulesare evaluated with ZC proxies. ZC Proxies Evaluation ranks each module based on the average relativeranks over the 4 ZC proxies. Subsequently, the modules that fall below a certain threshold, defined by afraction denoted as are removed from the search space. In our experiments, we set as 0.5.",
  "C.7Hyperparameters for finetuning": "For deep learning, our research focuses on an unusual search space with pretrained models at the architecturallevel. ztrk et al. (2022) also explored a similar search space. Consequently, we utilize a comparable set ofhyperparameters for fine-tuning the models (). To tailor this approach to our specific requirements,we have reduced the minimum batch size from 16 to 2 to accommodate the varying GPU memory demandsacross different tasks.",
  "D.1Widely-recognized datasets": "Boston The Boston dataset is a widely used regression dataset that contains information about housing inthe suburbs of Boston. It consists of 506 samples, each representing a suburb, with 13 features describingvarious aspects of the housing environment, such as crime rate, average number of rooms, and accessibilityto highways. The target variable is the median value of owner-occupied homes in thousands of dollars. Thisdataset is often used for regression tasks to predict housing prices based on the given features. We obtainthe files of the dataset from Altavish (2023). Iris The Iris dataset is a classic classification dataset used to demonstrate various machine learning algorithmsand techniques. It contains 150 samples of iris flowers from three different species: setosa, versicolor, andvirginica. There are four features for each flower: sepal length, sepal width, petal length, and petal width.The goal is to classify the flowers into their respective species based on these features, making it a populardataset for teaching and practicing classification algorithms. We obtain the files of the dataset from Learning(2023). CIFAR-10 The CIFAR-10 dataset is a well-known benchmark for image classification tasks. It consists of60,000 32x32 color images across 10 different classes, with 6,000 images per class. The classes include objectslike airplanes, cars, birds, cats, and more. The dataset is divided into a training set of 50,000 images and atest set of 10,000 images, making it suitable for evaluating the performance of various image classificationalgorithms. We obtain the files of the dataset from Krizhevsky et al. (2023). CIFAR-100 Similar to CIFAR-10, the CIFAR-100 dataset is also used for image classification tasks. How-ever, CIFAR-100 is more challenging as it contains 100 classes, each with 600 images. These classes arefurther grouped into 20 superclasses, making it a more fine-grained classification task. The dataset is de-signed to test the models ability to handle a larger number of classes and fine distinctions between them.We obtain the files of the dataset from Krizhevsky et al. (2023). IMDb Reviews The IMDB dataset is often used for sentiment analysis and text classification tasks. Itconsists of movie reviews labeled with sentiment labels (positive or negative). The reviews are representedas text and the goal is to predict the sentiment of a given review. This dataset is useful for exploring naturallanguage processing techniques and building models that can understand and classify textual data based onsentiment. We obtain the files of the dataset from Lakshmi25npathi (2023). AG News The AG News dataset is commonly used for text classification tasks, particularly for newscategorization. It contains news articles from four different categories: World, Sports, Business, and Sci-ence/Technology. The dataset is often used to develop models that can classify news articles into theirrespective categories based on their content. Its a relatively simple text classification task that serves as agood starting point for experimenting with various natural language processing algorithms. We obtain thefiles of the dataset from Rai (2023).",
  "The dataset is from competitions held between October 2021 and July 2023. The knowledge cutoffdates of the LLMs are September 2021 for GPT-4 and GPT-3.5 and mid-2021 for PaLM 2": "We have identified a total of 2 datasets for tabular data tasks, 9 datasets for NLP tasks, and 13 datasetsfor CV tasks. From these, we chose the 2 simplest datasets for each category, based on the diversity of filetypes present in the datasets and the task types. For CV, tasks that involve image segmentation, objectdetection, or 3D reconstruction are significantly more demanding than image classification tasks. For NLP,tasks that require named entity recognition and sequence matching present greater challenges comparedto sequence regression and sequence classification tasks. We find that all the methods we tested in ourexperiments, including Text-to-ML, are unable to successfully generate even a single program for the mostchallenging ones in the NLP and CV datasets. However, as datasets from medal-awarding competitions, eventhe simplest ones among the datasets still pose considerable challenges compared to the widely-recognizeddatasets. The selected Kaggle datasets are listed as follows: Age The ICR - Identifying Age-Related Conditions competition (Kaggle & InVitro Cell Research, 2023)involves the development of machine learning models capable of detecting various age-related conditionsfrom medical imaging data. Participants are provided with a dataset containing images labeled with differentage-related conditions, and the task is to create models that can accurately classify these conditions. Thecompetition aims to advance research in medical imaging by leveraging AI to improve the diagnosis andunderstanding of age-related diseases. Default The American Express - Default Prediction competition (Kaggle & Express, 2023) challenges par-ticipants to predict the likelihood of credit default by American Express cardholders. The dataset providedincludes a wide range of anonymized features spanning transactional data, account information, and userbehavior over time, designed to simulate real-world conditions. The goal is to enhance credit risk modelsand improve financial inclusion by accurately identifying at-risk customers. Whale The Happywhale - Whale and Dolphin Identification competition (Kaggle & Happywhale, 2023)focuses on the identification of individual whales and dolphins from images. Participants are tasked withdeveloping algorithms that can recognize and match cetaceans based on unique physical features capturedin photographs, contributing to marine biology research and conservation efforts. The dataset comprisesimages labeled with species and individual IDs, making it a challenge in both classification and individualidentification. Strip The Mayo Clinic - STRIP AI competition (Kaggle & Clinic, 2023) is aimed at predicting strokeand other thrombotic events by analyzing electroencephalogram (EEG) data. This competition provides aplatform for the development of AI models that can assist clinicians in early detection and intervention forstroke patients, leveraging a dataset of EEG recordings with associated clinical outcomes. This challengestands at the intersection of AI and healthcare, with the potential to significantly impact patient care andoutcomes in neurology. Exam The Kaggle - LLM Science Exam competition (Kaggle, 2023) challenges participants to create lan-guage models that can solve science exam questions. The focus is on evaluating the ability of these modelsto understand and apply scientific knowledge accurately. This competition is part of Kaggles efforts to",
  "benchmark the performance of language models in educational contexts, providing a unique opportunity toadvance AI in the field of automated question answering": "Learning The Feedback Prize - English Language Learning competition (Kaggle & Lab, 2023) aims todevelop AI models that can provide feedback on written English tasks by students learning English as aforeign language. The challenge involves analyzing student-written essays to identify areas for improvement,thus aiding in language learning and teaching. This competition seeks to leverage AI to enhance Englishlanguage education by providing scalable, personalized feedback."
}