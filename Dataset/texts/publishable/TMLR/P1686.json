{
  "Abstract": "Deep generative models, particularly diffusion models, are a significant family within deep learning. This studyprovides a precise upper limit for the Wasserstein distance between a learned distribution by a diffusion modeland the target distribution. In contrast to earlier research, this analysis does not rely on presumptions regardingthe learned score function. Furthermore, the findings are applicable to any data-generating distributions withinrestricted instance spaces, even those lacking a density relative to the Lebesgue measure, and the upper limit is notexponentially dependent on the ambient space dimension. The primary finding expands upon recent research byMbacke et al. (2023), and the proofs presented are fundamental.",
  "Introduction": "Diffusion models, alongside generative adversarial networks and variational autoencoders (VAEs), are among the most influentialfamilies of deep generative models. These models have demonstrated remarkable empirical results in generating images and audio,as well as in various other applications. Two primary methods exist for diffusion models: denoising diffusion probabilistic models (DDPMs) and score-based generativemodels (SGMs). DDPMs incrementally convert samples from the desired distribution into noise via a forward process, whilesimultaneously training a backward process to reverse this transformation, enabling the creation of new samples. Conversely, SGMsemploy score-matching methods to approximate the score function of the data-generating distribution, subsequently generating newsamples through Langevin dynamics. Recognizing that real-world distributions might lack a defined score function, adding varyingnoise levels to training samples to encompass the entire instance space and training a neural network to concurrently learn the scorefunction for all noise levels has been proposed. Although DDPMs and SGMs may initially seem distinct, it has been demonstrated that DDPMs implicitly approximate the scorefunction, with the sampling process resembling Langevin dynamics. Moreover, a unified perspective of both methods using stochasticdifferential equations (SDEs) has been derived. The SGM can be viewed as a discretization of Brownian motion, and the DDPM as adiscretization of an Ornstein-Uhlenbeck process. Consequently, both DDPMs and SGMs are commonly referred to as SGMs in theliterature. This explains why prior research investigating the theoretical aspects of diffusion models has adopted the score-basedframework, necessitating assumptions about the effectiveness of the learned score function. In this research, a different strategy is employed, applying methods created for VAEs to DDPMs, which can be viewed as hierarchicalVAEs with fixed encoders. This method enables the derivation of quantitative, Wasserstein-based upper bounds without makingassumptions about the data distribution or the learned score function, and with simple proofs that do not need the SDE toolkit.Furthermore, the bounds presented here do not involve any complex discretization steps, as the forward and backward processes areconsidered discrete-time from the beginning, rather than being viewed as discretizations of continuous-time processes.",
  "Related Works": "There has been an increasing amount of research aimed at providing theoretical findings on the convergence of SGMs. However,these studies frequently depend on restrictive assumptions regarding the data-generating distribution, produce non-quantitative upperbounds, or exhibit exponential dependencies on certain parameters. This work successfully circumvents all three of these limitations.Some bounds are based on very restrictive assumptions about the data-generating distribution, such as log-Sobolev inequalities,which are unrealistic for real-world data distributions. Furthermore, some studies establish upper bounds on the Kullback-Leibler(KL) divergence or the total variation (TV) distance between the data-generating distribution and the distribution learned by thediffusion model; however, unless strong assumptions are made about the support of the data-generating distribution, KL and TVreach their maximum values. Such assumptions arguably do not hold for real-world data-generating distributions, which are widelybelieved to satisfy the manifold hypothesis. Other work establishes conditions under which the support of the input distributionis equal to the support of the learned distribution, and generalizes the bound to all f-divergences. Assuming L2 accurate score estimation, some establish Wasserstein distance upper bounds under weaker assumptions on the data-generating distribution, buttheir Wasserstein-based bounds are not quantitative. Quantitative Wasserstein distance upper bounds under the manifold hypothesishave been derived, but these bounds exhibit exponential dependencies on some of the problem parameters.",
  "Our contributions": "In this study, strong assumptions about the data-generating distribution are avoided, and a quantitative upper bound on the Wassersteindistance is established without exponential dependencies on problem parameters, including the ambient space dimension. Moreover,a common aspect of the aforementioned studies is that their bounds are contingent on the error of the score estimator. According tosome, providing precise guarantees for the estimation of the score function is challenging, as it necessitates an understanding of thenon-convex training dynamics of neural network optimization, which is currently beyond reach. Therefore, upper bounds are derivedwithout making assumptions about the learned score function. Instead, the bound presented here is dependent on a reconstructionloss calculated over a finite independent and identically distributed (i.i.d.) sample. Intuitively, a loss function is defined, whichquantifies the average Euclidean distance between a sample from the data-generating distribution and the reconstruction obtained bysampling noise and passing it through the backward process (parameterized by 03b8). This method is inspired by previous work onVAEs. This approach offers numerous benefits: it does not impose restrictive assumptions on the data-generating distribution, avoidsexponential dependencies on the dimension, and provides a quantitative upper bound based on the Wasserstein distance. Furthermore,this method benefits from utilizing very straightforward and basic proofs.",
  "Preliminaries": "Throughout this paper, lowercase letters are used to represent both probability measures and their densities with respect to theLebesgue measure, and variables are added in parentheses to enhance readability (e.g., q(xt|xt1) to denote a time-dependentconditional distribution). An instance space X, which is a subset of RD with the Euclidean distance as the underlying metric, anda target data-generating distribution M +1 (X) are considered. Note that it is not assumed that has a density with respect tothe Lebesgue measure. Additionally, || || represents the Euclidean (L2) norm, and Ep(x) is used as shorthand for Exp(x). Givenprobability measures p, q M +1 (X) and a real number k > 1, the Wasserstein distance of order k is defined as (Villani, 2009):",
  "XX||x y||kd(x, y)1/k,": "where (p, q) denotes the set of couplings of p and q, meaning the set of joint distributions on X X with respective marginals pand q. The product measure p q is referred to as the trivial coupling, and the Wasserstein distance of order 1 is simply referred toas the Wasserstein distance.",
  "Denoising Diffusion Models": "Instead of employing the SDE framework, diffusion models are presented using the DDPM formulation with discrete-time processes.A diffusion model consists of two discrete-time stochastic processes: a forward process and a backward process. Both processes areindexed by time 0 t T, where the number of time steps T is a predetermined choice. **The forward process.** The forward process transforms a data point x0 into a noise distribution q(xT |x0) through a sequenceof conditional distributions q(xt|xt1) for 1 t T. It is assumed that the forward process is defined such that for sufficientlylarge T, the distribution q(xT |x0) is close to a simple noise distribution p(xT ), which is referred to as the prior distribution. Forinstance, p(xT ) = N(xT ; 0, I), the standard multivariate normal distribution, has been chosen in previous work. **The backward process.** The backward process is a Markov process with parametric transition kernels. The objective of thebackward process is to perform the reverse operation of the forward process: transforming noise samples into (approximate) samplesfrom the distribution . Following previous work, it is assumed that the backward process is defined by Gaussian distributionsp(xt1|xt) for 2 t T as",
  "andp(x0|x1) = g1(x1),": "where the variance parameters 2t R0 are defined by a fixed schedule, the mean functions gt : RD RD are learned using aneural network (with parameters ) for 2 t T, and g1 : RD X is a separate function dependent on 1. In practice, the samenetwork has been used for the functions gt for 2 t T, and a separate discrete decoder for g1.",
  "W1(, ()) W1(, n) + W1(n, ()),": "the distance W1(, ()) can be upper-bounded by upper-bounding the two expressions on the right-hand side separately. Theupper bound on W1(, n) is obtained using a straightforward adaptation of a proof. First, W1(, n) is upper-bounded using theexpectation of the loss function l, then the resulting expression is upper-bounded using a PAC-Bayesian-style expression dependenton the empirical risk and the prior-matching term. The upper bound on the second term W1(n, ()) uses the definition of n. Intuitively, the difference between (|xi0) and ()is determined by the corresponding initial distributions: q(xT |xi0) and p(xT ) for (). Hence, if the two initial distributions areclose, and if the steps of the backward process are smooth (see Assumption 1), then (|xi0) and () are close to each other.",
  "We are now ready to present the main result: a quantitative upper bound on the Wasserstein distance between the data-generatingdistribution and the learned distribution ()": "**Theorem 3.1.** Assume the instance space X has finite diameter = supx,xX ||x x|| < , and let > 0 and (0, 1) bereal numbers. Using the definitions and assumptions of the previous section, the following inequality holds with probability at least1 over the random draw of S = {x10, . . . , xn0} i.i.d. :",
  "**Remark 3.1.** Before presenting the proof, let us discuss Theorem 3.1": "* Because the right-hand side of the equation depends on a quantity computed using a finite i.i.d. sample S, the bound holds withhigh probability with respect to the randomness of S. This is the price we pay for having a quantitative upper bound with noexponential dependencies on problem parameters and no assumptions on the data-generating distribution . * The first term of theright-hand side is the average reconstruction loss computed over the sample S = {x10, . . . , xn0}. Note that for each 1 i n, theexpectation of l(xT |xi0) is only computed with respect to the noise distribution q(xT |xi0) defined by xi0 itself. Hence, this termmeasures how well a noise vector xT q(xT |xi0) recovers the original sample xi0 using the backward process, and averages overthe set S = {x10, . . . , xn0}. * If the Lipschitz constants satisfy Kt < 1 for all 1 t T, then the larger T is, the smaller the upperbound gets. This is because the product of Kt s then converges to 0. In Remark 3.2 below, we show that the assumption that Kt < 1for all t is a quite reasonable one. * The hyperparameter controls the trade-off between the prior-matching (KL) term and thediameter term 2. If Kt < 1 for all 1 t T and T , then the convergence of the bound largely depends on the choice of .In that case, n1/2 leads to faster convergence, while n leads to slower convergence to a smaller quantity. This is becausethe bound stems from PAC-Bayesian theory, where this trade-off is common. * The last term of the equation does not depend on thesample size n. Hence, the upper bound given by Theorem 3.1 does not converge to 0 as n . However, if the Lipschitz factors(Kt )1tT are all less than 1, then this term can be very small, especially in low-dimensional spaces.",
  "Now, if t < 1 for all 1 t T, then we have 1 t > 1 t1, which implies Kt < 1 for all 1 t T": "Remark 3.2 shows that the Lipschitz norm of the mean function qt(, x0) does not depend on x0. Indeed, looking at the previousequation, we can see that for any initial x0, the Lipschitz norm Kt =t(1t1) 1tonly depends on the noise schedule, not x0 itself.Since gt (, x0) is optimized to match qt(, x0) for each x0 in the training set, and all the functions qt(, x0) have the same Lipschitznorm Kt, we believe it is reasonable to assume gt is Lipschitz continuous as well. This is the intuition behind Assumption 1."
}