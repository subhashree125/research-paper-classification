{
  "Abstract": "Code-mixing and script-mixing are prevalent across online social networks and multilingualsocieties. However, a users preference toward code-mixing depends on the socioeconomicstatus, demographics of the user, and the local context, which existing generative modelstend to ignore while generating code-mixed texts. In this work, we make a pioneering at-tempt to develop a persona-aware generative model to generate texts resembling real-lifecode-mixed texts of individuals. We propose PARADOX, a persona-aware generative modelfor code-mixed text generation, which is a novel Transformer-based encoder-decoder modelthat encodes an utterance conditioned on a users persona and generates code-mixed textswithout monolingual reference data. We propose an alignment module that re-calibrates thegenerated sequence to resemble real-life code-mixed texts. PARADOX generates code-mixedtexts that are semantically more meaningful and linguistically more valid. To evaluate thepersonification capabilities of PARADOX, we propose four new metrics CM BLEU, CMRouge-1, CM Rouge-L and CM KS. On average, PARADOX achieves 1.6% better CM BLEU,57% better perplexity and 32% better semantic coherence than the non-persona-based coun-terparts. The source code is available at:",
  "Introduction": "Code-mixing (aka code-switching) appears when two or more languages are used interchangeably in a singleutterance. It is common in multilingual societies like India, where more than 24% of the population speaksin more than one language (Sengupta et al., 2021). Code-mixing is even more prevalent on social media.Informal usage of code-mixed languages on social media platforms like Twitter, Facebook, YouTube, andother online social networks gives rise to script-mixing, in which a user can use a single script (e.g., Roman)or multiple scripts (e.g., Devanagari for Hindi and Roman for English) within the same text (Srivastavaet al., 2020). Recent literature has made significant efforts to understand syntactic structure and semanticsfrom code-mixed texts (Singh et al., 2018a;b; Sengupta et al., 2022b). Similar attempts have been made forpragmatic tasks humour, sarcasm and hate detection in the code-mixed regime (Sengupta et al., 2022a;Bansal et al., 2020).",
  "Published in Transactions on Machine Learning Research (10/2024)": "[INST]<<SYS>> You are a helpful assistant that generate a Hindi-English text based on a users previous message and a seed word. Make sure that you understand the users linguistic pattern from the previous message and generate a text that starts with the seed word.<</SYS>> User Previous Message: {user_message}Seed Word: {seed_word}[/INST]",
  "Probability": "Average text length 0.000.150.300.450.60 Standard deviation of CMI Probability (log scale) Standard deviation of text length TwitterYouTube : User-specific distribution of Code-Mixing In-dex (CMI) and text lengths across different platforms.CMI is calculated as the fraction of minority languagewords in a text. For instance, the CMI of the text Idont want your nautanki (I dont want your gim-mick) is 1 5 = 0.2, the fraction of Hindi (minority lan-guage in this example) words in the text. Texts skewedtoward monolingualism, i.e., having an unequal pro-portion of words between different languages, tend tohave lower CMI than multilingual texts. This motivates us to develop PARADOX, a novelpersona-aware generative model for code-mixed textgeneration. It aims to generate personalized code-mixed texts by leveraging users historical utterances.It uses a Transformer-based encoder-decoder archi-tecture to learn the semantics of code-mixed genera-tion. The model utilizes a novel persona encoder toencode a user persona from their behavioural prefer-ences. Instead of projecting the users persona ontoa static space, PARADOX projects it onto a proba-bilistic latent space and captures the contextual per-sona based on their historical persona. Additionally,PARADOX uses an alignment module to re-align de-coder outputs to generate coherent texts. We evaluate PARADOX against the vanilla Transformerin terms of both the quality and coherence of gener-ated texts. To quantify the extent of personificationin the code-mixed generation, we propose four met-rics CM BLEU, CM Rouge-1, CM Rouge-L, andCM KS (here CM stands for code-mixing). On aver-age, PARADOX achieves 1.6% better CM BLEU thanthe non-persona counterpart. We also conduct a de-tailed human evaluation, concluding that PARADOX-generated code-mixed texts are 32% more semanti-cally coherent than that of the vanilla Transformer model. PARADOX can imitate a users linguistic preference4% better than the non-persona-based Transformer model. Our empirical analyses also highlight the effec-tiveness of PARADOX over pre-trained large language models. On average, PARADOX achieves 4.2% better CMBLEU, 11% better CM Rouge-1, and 9.6% better CM Rouge-L than the pre-trained Llama 2 (Touvron et al.,2023) and GPT-4 (Achiam et al., 2023) models.",
  "We design an alignment module to automatically induce alignments between different subwords. Empiricalresults show that it improves the coherence of generated texts": "We propose three metrics influenced by supervised machine translation CM BLEU, CM Rouge-1, andCM Rouge-L for evaluating the personification of code-mixed generation models. We also propose CMKS as a distance measure to evaluate code-mixing generation models. Finally, we collect two large-scale longitudinal datasets from Twitter and YouTube, primarily monolingualHindi and Hindi-English code-mixed texts. The datasets will be valuable for code-mixing research.",
  "Related Works": "Rule-based and Linguistic Approaches.Code-mixed text generation has garnered much interest inrecent times. Pratapa et al. (2018) explored equivalence constraint (EC) theory to generate Spanish-Englishcode-mixed texts from monolingual corpora. Their linguistic theory-based approach showed superiority overrecurrent neural networks in complex code-mixed text generation. Rizvi et al. (2021) developed GCM, atoolkit that utilizes different linguistic theories to generate code-mixed texts. Motivated by embedding ma-trix theory, Srivastava & Singh (2021) proposed rule-based methods to generate Hindi-English code-mixedtexts. Santy et al. (2021) utilized parse tree structures within the monolingual texts for generating code-mixed texts. Alternative approaches use generative models generative adversarial networks (Goodfellowet al., 2020), or variational autoencoder (VAE) (Kingma & Welling, 2014). Towards this, Garg et al. (2018)explored recurrent neural networks with SeqGAN pre-training for generating Mandarin-English code-mixeddata. Samanta et al. (2019) developed a VAE-based method to generate realistic and coherent Hindi-Englishcode-mixed texts. Other classes of code-mixed text generation models explore alignment within parallelcorpora for code-mixed generation. Notably, Winata et al. (2019); Tan & Joty (2021) explored word align-ments and candidate selection from parallel corpora for generating synthetic code-mixed texts. Amin et al.(2023) explored word alignments for generating Marathi-English code-mixed text generation. On a similarattempt, Dowlagar & Mamidi (2021) explored gated convolutional encoder-decoder models to identify thecompositional structure and translate English texts to Hinglish. Pre-trained Models.With the inception of self-attention (Vaswani et al., 2017), several attempts havebeen made to develop large pre-trained models showing exceptional performances in semantic and gener-ative tasks. Among these methods, multilingual models, such as XLM (Conneau & Lample, 2019), XLM-RoBERTa (Conneau et al., 2020), and mBART (Liu et al., 2020) have shown noticeable performance evenon low-resource languages. Recently, MuRIL (Khanuja et al., 2021) was proposed, superseding the perfor-mances of multilingual-BERT (Devlin et al., 2019) on different syntactic and semantic tasks on a diverseset of low-resource languages. Gupta et al. (2020) devised a semi-supervised approach to transfer knowl-edge from XLM to generate synthetic Hindi-English code-mixed texts. Gautam et al. (2021) explored apre-trained mBART model for generating Hindi-English code-mixed texts. Jawahar et al. (2021) exploredmultilingual text-to-text models with curriculum learning for generating Hindi-English code-mixed texts.They pre-trained an encoder-decoder model on synthetic code-mixed texts, which improved the generationquality on the gold code-mixed dataset. In a recent study, Yong et al. (2023) explored multilingual largelanguage models (LLMs) for generating code-mixed texts in a zero-shot setting. They explored InstructGPT,ChatGPT (Ouyang et al., 2022), BLOOMZ (Muennighoff et al., 2022) and Flan-T5-XXL (Chung et al., 2022)for generating code-mixed texts in Indonesian, Malay, Chinese, Tagalog, Vietnamese, Tamil, and Singlish.They further emphasized the importance of better prompt templates and language pairing for generatingmore coherent and natural code-mixed texts. Personification of Code-mixing Languages.Language, as a mode of communication, is often person-alized for different users (King & Cook, 2020). The personification of language arises naturally, dependingon the context, socio-economic and demographic background of the user and their audience (Reichelt et al.,2014). Particularly in multilingual societies, the complex dynamics between different languages play a cru-cial role in the personification of language. Sengupta et al. (2024) highlighted the personalization aspects of",
  "Hindi-English code-mixed language, highlighting the importance of different sociological aspects behind theevolution of personalized code-mixed languages": "Major Limitations of Existing Studies.Despite their popularity in several generative applications,personalization remains neglected in the code-mixed generation. The existing code-mixed generation modelsutilize parallel corpora to understand the switching patterns and generate code-mixed texts synthetically.These limitations motivate us to develop a code-mixed language generation model that can automaticallylearn the languages semantics and capture the linguistic preferences of users while generating texts. Ourproposed method, PARADOX, improves the quality of generated texts and preserves the real-life phenomenonof code-mixing among users.",
  "Personalized Code-mixing": "As highlighted in , the study of the personalization of a language is crucial to develop conversationalgenerative models. Typically, a personalized generative model captures the users preferential behaviors dur-ing generation. Instead, our work delves into the personalization of users linguistic preferences in terms ofmixing multiple languages in an utterance. We define the personalization of code mixing as a behaviorallanguage that captures a users writing style given the historical trends, social context, targeted demograph-ics, and topical relevance. The proposed method, PARADOX, aims to capture these behavioral and contextual",
  "PARADOX Architecture": "PARADOX, as shown in , consists of four components (a) an encoder, (b) a contextual personaencoder, (c) a decoder, and (d) an alignment module. PARADOX utilizes a persona encoder to implicitly encodea users persona based on his/her historical utterances. It further projects the persona onto a probabilisticlatent space to capture the users contextual persona. Finally, PARADOX employs an alignment module tore-calibrate the output sequences that help our model understand the language of code-mixing and enablethe model to generate coherent texts.",
  "The Encoder with FAME": "The encoder is used to jointly learn the semantics of a code-mixed text and a users global persona basedon the previous comments/tweets. A comment/tweet Xu by user u is first tokenized using byte-pair encod-ings (Sennrich et al., 2016) into x1, x2, ..., xn. The initial contextual embedding of a token xi is conditionedwith the user persona as",
  "Emb(xi,u) = Embxi + PEi + Embu(1)": "where Embxi is the initial token embedding, PEi is the positional encoding at position i, and Embu is theusers global persona embedding captured through an embedding layer. Embu is computed with a uniqueidentifier for each user. This unique user ID is important to encode the temporal evolution of a user persona. Inorder to calculate the global persona for cold-start users (users present in the development set but not duringmodel training), we use a learnable [UNK] token embedding as Embu. We use a stacked encoder, in whicheach encoder consists of multi-headed fused attention (FAME), followed by a residual, layer-normalization,and pointwise feed-forward layers. Sengupta et al. (2021) introduced FAME by combining scaled dot-productattention (Vaswani et al., 2017) and outer-product attention (Le et al., 2020) and showed to be effective incapturing both semantics and morphology of code-mixed texts.",
  "Contextual Persona Encoder": "We project the static persona embedding onto a probabilistic latent space for generating the contextualpersona embedding for each user in a given context. We hypothesize that each user has a static (global)persona and a contextual (local) persona. The motivation behind projecting the persona embedding to alatent space is to capture the contextual perturbations in the user persona. For instance, highlightsa user who predominantly uses monolingual English for raising political opinions (CMI 0.0), however, atoccasions switches between English and Hindi to create stronger narrative.",
  "User IDGenerated TextCMI": "CM: This school student is a passionate supporter of clean politics.0.0Eng: this school student is a passionate supporter of clean politics.CM: once again vendetta politics will fail. Satyamev jayate.0.21Eng: once again vendetta politics will fail. Truth will triumph.CM: This so called party with difference has links with brastacharis.0.12Eng: This so called party with difference has links with corrupts. : Examples of user linguistic behavioral changes in different contexts. Usage of Hindi words arehighlighted with blue. The user predominantly uses monolingual English for expressing political opinions.However, at different instances the user uses Hindi interchangeably for referring to contextual popular nar-ratives for strengthening their expression.",
  "Following the reparameterization trick (Kingma & Welling, 2014), we define the final generated personaencoding as:Embu = u + u u(3)": "where u is the random noise, independently drawn from N(0, 1). We refer to this method randomizedpersona encoder. In another ablation, we use only the linear projection Embu = u = Embu.W forrepresenting the contextual persona embedding, which we call linear persona encoder. We obtain thehidden representation of token xi conditioned on the contextual persona encoding as",
  "The Decoder": "We adopt the Transformer decoder conditioned on the contextual user persona. Similar to the originalTransformer decoder, we use a stacked decoder initialized with the encoded output sequence. Drawing themotivation from autoregressive generative language models like GPT2 (Radford et al., 2019), the decodersobjective is to predict the next token, conditioned on all the previous tokens. The input to the decoder is theencoded input sequence added with positional encoding. Each decoder block consists of masked multi-headedFAME, a residual connection, and a normalization layer. We also deploy multi-headed FAME to attend toeach decoder token with the encoded input tokens h(xi,u). For each decoder input position j, we generate a",
  "The Alignment Module": "The final layer of PARADOX is an alignment module that learns the latent alignment matrix and re-aligns theoutputs generated by the decoder. The primary objective behind using alignments in generative models isexplicitly learning the global semantic similarity between different tokens. We use two projection matrices,W Q and W K, to project the decoder token embedding matrix Emb(dec) into two different subspaces. Thealignment matrix is defined as,",
  "(5)": "where Q = Emb(dec) W Q, K = Emb(dec) W K, and d is the hidden size of the decoder. This operationresembles the scaled dot-product attention mechanism (Vaswani et al., 2017). However, as opposed to atten-tion, we compute the global context by considering the original embedding space of all tokens. Finally, there-aligned hidden representation is derived as,",
  "h(j,u)(dec)= h(dec)(j,u) A + h(dec)(j,u)(6)": "This hidden representation is finally fed to a softmax layer to convert the outputs into probabilities. It isimportant to notice that the alignment matrix A is not intended to capture the semantic similarity betweencontextual tokens but rather to learn their similarities at a global level. By doing so, the alignment module cancapture the associations between all the tokens and recalibrate their probabilities during decoding. Moreover,as the dot-product is computed over the embedding matrix, the autoregressive rule of text generation is notviolated here. For the sake of simplicity, we denote the combination of text encoder and persona encoder as encoderand the combination of Transformer decoder and the alignment module as decoder throughout this paper.The generative model is trained w.r.t. the decoder reconstruction cross-entropy loss. The contextual personaencoder gives rise to a variational KL-divergence loss. We use a variational hyperparameter to assign itsweightage in the final computed loss.",
  "Code-Mixed Generation": "We adopt an autoregressive generation technique to generate new code-mixed texts for different users. Toencode the users historical persona, we use the users last utterance (comment/tweet) in the encoder. AsPARADOX is trained autoregressively on all historical utterances for different users, it can capture the entireconversational history of a user from the last utterance without passing the entire history during generation. Atypical text generation model starts decoding with a placeholder [CLS] token. Instead, we pass an additionalseed word as input to the decoder for a more guided generation. We formally report the text generationprocess in Algorithm 1.",
  "Datasets": "To the best of our knowledge, no existing longitudinal dataset is available for Hindi-English code-mixed.A longitudinal dataset is required to study the temporal evolution of a language. Although some datasetsin the literature consist of Hindi-English code-mixed texts collected from various online social networks,none of them contain user-specific information, making them unsuitable for our study. To overcome this, wecollected code-mixed texts from the two most popular mediums where Indians are engaged Twitter andYouTube. From Twitter, we collected over 0.8 million in tweets starting from the year 2011 till date, fromwhich we filtered only tweets originating from Mumbai and Delhi metropolitan regions, two cities with thelargest Hindi population. We used Twitter API for academic research with full archival access1. Further, forrelevance, we restricted ourselves to tweets related to Cricket, Bollywood, Politics, and Government.Starting at 2014, Twitter automatically tags the language of a tweet. We selected tweets with only non-emptylanguage tags. This gives us a total of 226, 480 tweets from 19, 782 users. From YouTube, we chose two channels NishaMadhulika2 (a popular chef based out of India with morethan 12.7 million followers), and T-Series3 (a popular Hindi music record channel started in 1983 havingmore than 200 million followers). We selected 42 videos from the NishaMadhulika channel and 69 from theT-Series that were first posted in 2011. We scraped all comments corresponding to these videos, accountingfor 144, 822 comments from 99, 998 users. For both datasets, we use a pre-trained language model open-sourced with Huggingface4, that was fine-tunedon Hindi-English parts-of-speech (PoS) and language identification (LID) tasks. Using this model, we labeleach token in each text with the corresponding language (Hindi or English) and their associated PoS. Wetag a text as code-mixed only when the text contains at least one Hindi verb written in either Devanagarior Roman script. We select users who have at least three utterances in their entire timeline. Finally, we areleft with 18, 126 tweets (from 2, 241 users) and 8, 957 YouTube comments (from 1, 349 users). We remove all the HTML tags, URLs, emoticons, user mentions (starting with @), and hashtags (startingwith #). For simplicity, we remove all numeric values from texts, as well. Finally, we convert all texts tolowercase. We highlight the key statistics of the datasets in . We use a 75-25 split for training andvalidation with stratified sampling. Therefore, we can ensure at least one training and validation sample foreach user. We choose the first word for each validation sample as the seed word for generating the code-mixed text. Finally, the generated code-mixed text is evaluated against the original validation text using theproposed personalization evaluation metrics.",
  "We adopt intrinsic and extrinsic evaluation metricsto evaluate our model in terms of semantic under-standing of code-mixing language and the ability topersonify code-mixing for different users": "For the intrinsic evaluation, we use perplexity, ametric that measures the predictive power of a lan-guage model, compared against ground truth. We cal-culate perplexity as eloss, with loss being the cross-entropy reconstruction loss on the validation data. Alower perplexity score indicates better reconstructibility and ability to learn the semantics of a generativemodel. Unlike Gupta et al. (2020), we do not have any labelled gold data for evaluating our generative model.Therefore, traditional supervised evaluation metrics BLEU (Papineni et al., 2002), Rouge (Lin, 2004) cannot be used directly to evaluate the personification aspects of code-mixed generation models. Similarly, otherextrinsic evaluation measures such as Multilingual index (M Index) (Barnett et al., 2000), Burstiness and",
  "(-) FAME864.98583.09": ": Intrinsic evaluation of the competing models based onperplexity (: lower value indicates better performance). For mod-els highlighted with *, perplexity is calculated with word-levelgeneration. Bold indicates the best results among all the mod-els. We report the intrinsic evaluation re-sults in . PARADOX achieves 56%better perplexity on the Twitter datasetthan the vanilla Transformer. On theYouTube dataset, the margin is evenhigher (59%). A lower validation perplex-ity shows PARADOXs strong ability to un-derstand code-mixing semantics and gen-erate texts of different linguistic varia-tions. PARADOX achieves 18% better per-plexity on the Twitter dataset than thebest non-transformer baseline VACS. Onthe YouTube dataset, the margin is signif-icantly higher (64%). highlights the extrinsic measuresacross all the generative models. On theTwitter dataset, PARADOX achieves 2.4%better CM BLEU than the Transformermodel. Similarly, on the YouTube dataset,PARADOX performs the best among allthe baselines and outperforms the Trans-former model with a margin of 2.0%. In terms of the Rouge measures, PARADOX performs consistently betterthan the non-persona counterpart with an average margin of 1.9%. In terms of distance-based measures,PARADOX performs significantly better than the Transformer model on both datasets. Overall, PARADOXachieves 11% lower CM KS distance than Transformer. Lower KS distance indicates the importance ofutilizing user persona in generating user-specific code-mixed texts. Among the pre-trained language models, fine-tuned Llama 2 and GPT-4 are the most competitive. Interest-ingly, even with a single example in the prompt (1-shot), CM BLEU increases by 9.8% for Llama 2. Similarperformance improvements are also observed with other extrinsic metrics. However, both Transformer andPARADOX perform significantly better than the pre-trained language models in terms of personalized code-mixed text generation. On average, PARADOX achieves 7.6% better CM BLEU and 12.5% better CM Rouge-1than Llama 2. PARADOX even achieves 3.9% better CM BLEU than the fine-tuned Llama model. Similarperformance improvements are observed with CM Rouge-1 and CM Rouge-L metrics. PARADOX achieves4.1% better CM Rouge-1 and 3.8% better CM Rouge-L than the GPT-4 model. Even with CM KS, ourmodel outperforms GPT-4 with a wide margin of 14%. Interestingly, with few-shot in-context learning,GPT-4 model demonstrates stronger performance than the existing baselines. On Twitter dataset, 1-shot",
  "(-) FAME18.4928.2954.0160.6747.6857.100.360.37": ": Extrinsic evaluation of pre-trained language models, Transformer and PARADOX in terms of preservinguser-level switching patterns ( (resp. ): higher (resp. lower) value indicates better performance). Boldindicates the best results among all the models. GPT-4 model achieves higher CM BLEU and CM Rouge scores than PARADOX. This superior performancecan be justified with the facts that tweets are generally shorter and more monolingual (lower CMI), there-fore, capturing the corpus-level trends are easier for robust LLMs like GPT-4 with even a single examplein the prompt. However, it is worth noting that 1-shot GPT-4 still underperforms than PARADOX in termsof the personalization metric CM KS. On YouTube dataset however, PARADOX achieves 0.8% higher CMBLEU and 1.5% higher CM Rouge scores than the 1-shot GPT-4 model demonstrating its superiority inunderstanding and generating personalized code-mixed texts. 20.0 22.5 25.0 27.5 30.0",
  ": Performances of PARADOX under linear and randomizedpersona encoder": "Our ablation study in shows theeffectiveness of fused attention, contextualpersona module, and the alignment mod-ule in PARADOX. Additionally, we empir-ically highlight the necessity of speakerID encoding for personalized code-mixedtext generation. Adding FAME improvesvalidation perplexity by 66%. Similarly,the contextual user persona and align-ment modules improve validation perplex-ity by 21% and 28%, respectively, justify-ing their contributions to modelling low-resource language. Modeling the user per-sona with an additional identifier (speakerID) improves the validation perplexity by48%, highlighting the importance of themodule for personalized text generation.Ablation results in also suggestthe importance of different modules ofPARADOX for personalized code-mixed textgeneration. Removing the contextual per-sona encoder, leads to an average performance drop by 0.52% in regards to CM BLEU and CM Rouge scores.Moreover, an one-sided t-test concludes the statistical significance (p-value < 0.001) of the performance dropacross different extrinsic measures and datasets. highlights the distribution of intrinsic and extrinsicmetrics for linear and randomized speaker contextualization modules. Not only does having a randomizedspeaker contextual module improve the performance, but it also improves the robustness of the generation",
  ": Human evaluation of the models": "We perform a human evaluation study to evalu-ate the code-mixed texts generated by PARADOXand the vanilla Transformer. In the previous sec-tion, we discussed our methodology for evaluat-ing the personification capabilities of PARADOXusing the intrinsic and proposed extrinsic met-rics. With human evaluation, we aim to assess the generative models in terms of linguistic correctness andcontextual relevance. Towards that, we randomly sample 24 examples from each of these models and ask30 human evaluators6 to rate these examples based on Semantic coherence and Linguistic quality. Semanticcoherence measures the meaningfulness of the code-mixed texts, whereas, with linguistic quality, we measuretheir structural validity. Both the scores ranged between 1-5, 1 being the lowest, and 5 being the highest. presents the average semantic coherence and linguistic quality scores, along with FleisssKappa (Fleiss, 1971) scores among the annotators. We observe that PARADOX displays a better semanticcoherence (32% better), as well as better linguistic quality (29% better) than the Transformer model. Weobserve fair agreement (Kappa 0.13 for semantic coherence and Kappa 0.14 for linguistic quality) among theannotators for both models.",
  ": Comparison of different text generation models": "We further study the quality of code-mixedgenerationandcomparethemagainst other baselines. We analyze thedistribution of length and CMI of textsgenerated by different generative mod-els and report in . Trained onthe Twitter dataset, PARADOX generatestexts with a median length of 16, 25%higher than the other generative baselines.A similar trend can also be observed inthe YouTube dataset. Similarly, the me-dian value of CMI on texts generated byPARADOX is 0.27 and 0.16, respectively, forthe Twitter and YouTube datasets, whichare significantly lower than the medianCMI achieved by other baselines (0.29 and0.28, respectively). A lower median CMIindicates that the texts generated by PARADOX are more monolingual at a corpus level, acknowledging thepopulation level trend shown in . (a) shows the distribution of top Hindi verbs and nouns from the Twitter dataset. Being more inclinedtowards monolingual, PARADOX assigns more probability to these Hindi words, irrespective of the parts ofspeech. (b) shows the distribution of top Hindi verbs and nouns from the Twitter dataset for differentablations of PARADOX. Interestingly, PARADOX with the alignment module can replicate the word distributionobserved in the real dataset. On the other hand, without the alignment module, the generative model couldhallucinate and unrealistically use common phrases in incorrect contexts. This highlights the effectiveness ofthe alignment module in recalibrating output tokens and generating semantically meaningful texts. Althoughthe dimension of the alignment matrix is |V ||V |, with |V | being the decoder vocabulary size, the learnable",
  ": Distribution of top Hindi verbs and nouns for Tweets. w.o. alignment is the ablation of PARADOXwithout the alignment module": "parameters are of order O(d2), where d is the hidden dimension. The total number of additional parametersintroduced by the alignment module is 0.025% of the entire network, which is insignificant compared to othermodules. We further analyze a few sample texts generated by the generative models. We report examples of textsgenerated by PARADOX and the Transformer, along with the average semantic coherence and linguisticquality scores annotated by the annotators in of Appendix B. Transformer usually picks top nounsin the corpus and generates texts around them without considering the syntax of the code-mixed language,resulting in incoherent texts in many cases. On the other hand, PARADOX preserves the grammar of code-mixed texts with a more human-like switching pattern. It shows that PARADOX maintains the grammar ofthe base language (Hindi in this case), attributing to a more coherent and reliable generation. The exampleshighlight the key differences between the texts generated by persona-based PARADOX and non-persona-basedTransformer models regarding text quality. further shows the personalized generation by our model.With the same prompt (e.g., salman in the first example), PARADOX can understand different personas ofusers and can generate texts suited for different users. The high similarity between the historical average ofthe user CMIs and the generated CMIs indicates the models ability to understand the linguistic preferencesof users. of Appendix B highlights code-mixed texts generated by the Llama 2 model with zero-shot and 1-shot. PARADOX exhibits better capability in mimicking the code-mixing linguistic traits than Llama. For userID 2226, who has had more monolingual usage in the past (average CMI 0.04), the text generated by Llamais more code-mixed than monolingual. Similarly, for user ID 3, the Llama model reverses the linguisticpreference of the user while generating the code-mixed texts. Albeit demonstrating superior performanceacross various natural language understanding and reasoning tasks with zero and few-shot in-context learning,pre-trained large language models such as Llama fail to understand the linguistic complexities of informallanguages such as code-mixed Hinglish. It is imperative to notice that the Llama model not only fails tocapture the historical linguistic preferences of users but also fails to impersonate the semantic structureof code-mixed texts. On the other hand, PARADOX demonstrates better code-mixed language understandingcapabilities, captures the linguistic preferences of the user from their historical utterances and preserves theinformation for future generations. This highlights the effectiveness of personification in code-mixed textgeneration and the importance of building more robust language understanding models for understandingthe linguistic nitty-gritty of low-resource languages.",
  "Conclusion": "This paper described a personalized code-mixed generation model for generating human-alike code-mixedtexts. We highlighted the need for a personalized generation under the pretext of code-mixing. Toward this, wedevised a novel persona-aware encoder-decoder model coupled with a novel alignment module for generatingmore realistic and coherent Hindi-English code-mixed texts, the first attempt toward personalized code-mixed generation. Empirical analyses would benefit the research community in developing robust and reliablelanguage models for low-resource languages. Although our current work explored PARADOX for Hindi-Englishgeneration, the proposed generative framework can be further extended to generate code-mixed texts in otherpersonalized code-mixing and derived languages, such as Spanish-English and Chinese-English. Although ourempirical study has shown the effectiveness of persona-attributed text generation, currently PARADOX capturesonly contextual persona, ignoring other explicit factors. Not only does this restrict our model in cold-startgeneration (text generation for new users without any history), but it also fails to consider the co-associationamong users. In conversational settings, particularly, this can be deemed essential. Another limitation ofPARADOX is the inability to determine the temporal evolution of a users persona driven by external factors.PARADOX captures the user persona and its evolution solely from contextual information. At the same time,a users linguistic preferences can also be driven by other external socio-demographic and economic factorsvarying over time, which our model currently undermines.",
  "Broader Impact Statement": "Our work highlights the need for personalized generation models for conversational languages like code-mixing. We release our curated datasets to encourage research on personalized code-mixed text generation.Persona-aware code-mixed generation models can aid in building data-driven solutions in low-resource lan-guages and can be expanded to broader demographics. We do not collect user-specific features or determinantsthat could reveal user-specific sensitive information. We remove all the user IDs, mentions and delimitersfrom the user texts to maintain user privacy. Moreover, no user-specific attributes are used to train thegenerative model except the texts written by the users. Although we do not anticipate any immediate neg-ative impact of our work, over-personalization can lead to targeted spamming and negative misuse of userpersona. We ask the researchers to be aware of the potential misuse and use the shared artefacts judiciouslyto prevent unwarranted events. Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, DiogoAlmeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al.Gpt-4 technical report.arXivpreprint arXiv:2303.08774, 2023.",
  "Alexis Conneau and Guillaume Lample.Cross-lingual language model pretraining.Advances in neuralinformation processing systems, 32, 2019": "Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, FranciscoGuzmn, douard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. Unsupervised cross-lingualrepresentation learning at scale. In Proceedings of the 58th Annual Meeting of the Association for Com-putational Linguistics, pp. 84408451, 2020. Amitava Das and Bjrn Gambck. Identifying languages at the word level in code-mixed Indian social mediatext. In Proceedings of the 11th International Conference on Natural Language Processing, pp. 378387,Goa, India, December 2014. NLP Association of India. URL Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of Deep Bidirec-tional Transformers for Language Understanding, May 2019. URL arXiv:1810.04805 arXiv:1810.04805 [cs]. Suman Dowlagar and Radhika Mamidi. Gated convolutional sequence to sequence based learning for english-hingilsh code-switched machine translation. In Proceedings of the Fifth Workshop on Computational Ap-proaches to Linguistic Code-Switching, pp. 2630, 2021.",
  "Joseph L Fleiss. Measuring nominal scale agreement among many raters. Psychological bulletin, 76(5):378,1971": "Saurabh Garg, Tanmay Parekh, and Preethi Jyothi. Code-switched Language Models Using Dual RNNsand Same-Source Pretraining. In Proceedings of the 2018 Conference on Empirical Methods in NaturalLanguage Processing, pp. 30783083, Brussels, Belgium, 2018. Association for Computational Linguistics.doi: 10.18653/v1/D18-1346. URL Devansh Gautam, Prashant Kodali, Kshitij Gupta, Anmol Goel, Manish Shrivastava, and PonnurangamKumaraguru. Comet: Towards code-mixed translation using parallel monolingual sentences. In Proceedingsof the Fifth Workshop on Computational Approaches to Linguistic Code-Switching, pp. 4755, 2021.",
  "Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In Text summarization branchesout, pp. 7481, 2004": "Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike Lewis, andLuke Zettlemoyer. Multilingual denoising pre-training for neural machine translation. Transactions of theAssociation for Computational Linguistics, 8:726742, 2020. Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Scao,M Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey Schoelkopf, et al. Crosslingual generalization throughmultitask finetuning. arXiv preprint arXiv:2211.01786, 2022. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang,Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions withhuman feedback. Advances in Neural Information Processing Systems, 35:2773027744, 2022. Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluationof machine translation. In Proceedings of the 40th annual meeting of the Association for ComputationalLinguistics, pp. 311318, 2002. Rana D. Parshad, Suman Bhowmick, Vineeta Chand, Nitu Kumari, and Neha Sinha. What is India speaking?Exploring the Hinglish invasion. Physica A: Statistical Mechanics and its Applications, 449:375389,May 2016. ISSN 03784371. doi: 10.1016/j.physa.2016.01.015. URL",
  "Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language modelsare unsupervised multitask learners. OpenAI blog, 1(8):9, 2019": "Maria Reichelt, Frauke Kmmerer, Helmut M Niegemann, and Steffi Zander. Talk to me personally: Per-sonalization of language style in computer-based learning. Computers in Human behavior, 35:199210,2014. Mohd Sanad Zaki Rizvi, Anirudh Srinivasan, Tanuja Ganu, Monojit Choudhury, and Sunayana Sitaram.GCM: A Toolkit for Generating Synthetic Code-mixed Text. In Proceedings of the 16th Conference ofthe European Chapter of the Association for Computational Linguistics: System Demonstrations, pp. 205211, Online, 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.eacl-demos.24. URL Koustav Rudra, Shruti Rijhwani, Rafiya Begum, Kalika Bali, Monojit Choudhury, and Niloy Ganguly.Understanding Language Preference for Expression of Opinion and Sentiment: What do Hindi-EnglishSpeakers do on Twitter? In Proceedings of the 2016 Conference on Empirical Methods in Natural Lan-guage Processing, pp. 11311141, Austin, Texas, 2016. Association for Computational Linguistics. doi:10.18653/v1/D16-1121. URL Bidisha Samanta, Sharmila Reddy, Hussain Jagirdar, Niloy Ganguly, and Soumen Chakrabarti. A DeepGenerative Model for Code Switched Text. In Proceedings of the Twenty-Eighth International Joint Con-ference on Artificial Intelligence, pp. 51755181, Macao, China, August 2019. International Joint Confer-ences on Artificial Intelligence Organization. ISBN 978-0-9992411-4-1. doi: 10.24963/ijcai.2019/719. URL Sebastin Santy, Anirudh Srinivasan, and Monojit Choudhury. Bertologicomix: How does code-mixing interactwith multilingual bert? In Proceedings of the Second Workshop on Domain Adaptation for NLP, pp. 111121, 2021. Ayan Sengupta, Sourabh Kumar Bhattacharjee, Tanmoy Chakraborty, and Md Shad Akhtar. Hit-a hierar-chically fused deep attention network for robust code-mixed language representation. In Findings of theAssociation for Computational Linguistics: ACL-IJCNLP 2021, pp. 46254639, 2021. Ayan Sengupta, Sourabh Kumar Bhattacharjee, Md. Shad Akhtar, and Tanmoy Chakraborty. Does ag-gression lead to hate? Detecting and reasoning offensive traits in hinglish code-mixed texts.Neu-rocomputing, 488:598617, June 2022a.ISSN 0925-2312.doi: 10.1016/j.neucom.2021.11.053.URL Ayan Sengupta, Tharun Suresh, Md Shad Akhtar, and Tanmoy Chakraborty.A Comprehensive Un-derstanding of Code-mixed Language Semantics using Hierarchical Transformer, April 2022b.URL Number: arXiv:2204.12753 arXiv:2204.12753 [cs]. Ayan Sengupta, Soham Das, Md Shad Akhtar, and Tanmoy Chakraborty. Social, economic, and demographicfactors drive the emergence of hinglish code-mixing on social media.Humanities and Social SciencesCommunications, 11(1):112, 2024. Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural Machine Translation of Rare Words with Sub-word Units. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics(Volume 1: Long Papers), pp. 17151725, Berlin, Germany, 2016. Association for Computational Linguis-tics. doi: 10.18653/v1/P16-1162. URL",
  "Samson Tan and Shafiq Joty.Code-mixing on sesame street: Dawn of the adversarial polyglots. arXivpreprint arXiv:2103.09593, 2021": "Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bash-lykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Fer-rer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, HakanInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit SinghKoura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, XavierMartinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizen-stein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian,Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan,Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, RobertStojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models,2023. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ukasz Kaiser,and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30,2017. Zihao Wang, Ming Jiang, and Junli Wang. A Speaker-aware Parallel Hierarchical Attentive Encoder-DecoderModel for Multi-turn Dialogue Generation, October 2021.URL arXiv:2110.06823 arXiv:2110.06823 [cs].",
  "Genta Indra Winata, Andrea Madotto, Chien-Sheng Wu, and Pascale Fung. Code-switched language modelsusing neural based synthetic data from parallel sentences. arXiv preprint arXiv:1909.08582, 2019": "BigScience Workshop, :, Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili, DanielHesslow, Roman Castagn, Alexandra Sasha Luccioni, Franois Yvon, Matthias Gall, Jonathan Tow,Alexander M. Rush, Stella Biderman, Albert Webson, Pawan Sasanka Ammanamanchi, Thomas Wang,Benot Sagot, Niklas Muennighoff, Albert Villanova del Moral, Olatunji Ruwase, Rachel Bawden, StasBekman, Angelina McMillan-Major, Iz Beltagy, Huu Nguyen, Lucile Saulnier, Samson Tan, Pedro OrtizSuarez, Victor Sanh, Hugo Laurenon, Yacine Jernite, Julien Launay, Margaret Mitchell, Colin Raffel,Aaron Gokaslan, Adi Simhi, Aitor Soroa, Alham Fikri Aji, Amit Alfassy, Anna Rogers, Ariel KreisbergNitzav, Canwen Xu, Chenghao Mou, Chris Emezue, Christopher Klamm, Colin Leong, Daniel van Strien,",
  "A.1Evaluation Metrics": "We compute Pearson correlation (c.f. ) between the proposed metrics to understand their rela-tionships. We observe strong positive correlations among the metrics CM BLEU, CM Rouge-1 and CMRouge-L. Similarly, all of these metrics have a strong negative correlation with perplexity. This highlightsthe strong linear associations between the evaluation metrics that measure the semantical behavior of thecode-switching patterns. On the other hand, the correlations between the CM KS measure and other metricsare meagre, indicating that the linguistic preferences of users have very minimal impact on the switchingpatterns. In other words, the switching patterns of a user who prefers multilingual could be more erraticthan those of a user who prefers monolinguals. PerplexityCM BLEUCM Rouge-1 CM Rouge-LCM KS",
  "A.2Training Details": "For all the models across all the experiments, we use a maximum text length of 40. PARADOX consists ofsix encoder and decoder layers, with hidden sizes of 768 in all the layers. For multi-headed FAME andmasked multi-headed FAME blocks, we use a total of eight heads with Dropout probabilities set as 0.1. Thetotal number of parameters is 296M. We use six encoder and six decoder layers in the Transformer model,with eight heads in each multi-headed attention block. For training PARADOX, We set the persona encodingvariational weight = 0.5. All the models are trained for 50 epochs with an early stopping condition onvalidation loss with the patience of 10. We set batch_size = 4 in all experiments during training andvalidation. We use Adam optimizer with a learning rate of 4e 4 and 1 = 0.9, 2 = 0.98 for both PARADOXand Transformer. We fine-tune the MuRIL and BLOOMZ models on autoregressive language modeling tasksfor 10 epochs with learning rates 3e5 and 3e6, respectively. The Llama-2 baseline is used in zero-shot and1-shot settings with the prompt shown in . We use one Tesla P100 and one Tesla V100 GPU to runall our experiments. For PARADOX, each training and validation iteration takes 0.18 and 0.12 seconds,respectively. Strubell et al. (2019) proposed estimation of power usage and carbon emission behind runningdeep learning experiments. Following those guidelines, we estimate a total power usage of 23.56 kWh and anequivalent CO2 emission of 22.46 pounds.",
  "BAnalysis of Code-Mixed Generation": "We highlight few examples of texts generated by PARADOX and the vanilla Transformer model in ,along with their semantic coherence and linguistic quality scores annotated by the annotators. As highlightedin , the semantic coherence of the texts generated by PARADOX is much higher than the non-persona-based counterpart. Moreover, texts generated by PARADOX are grammatically more valid than those generatedby the Transformer model. This analysis highlights the importance of incorporating persona for an enhancedunderstanding of conversational code-mixed languages. highlights the texts generated by the persona-based PARADOX and the Llama model for two differentusers. The margin between the historical average CMI and the generated CMI highlights our methodssuperiority in capturing users linguistic patterns. A large pre-trained language model such as Llama, evenwith few-shot in-context learning, may not be superior to a persona-based small language model fine-tunedon low-resource code-mixed texts."
}