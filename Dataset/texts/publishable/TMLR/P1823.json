{
  "Abstract": "This short note introduces the harmonic indel distance (HID), a new distance between stringswhere the cost of an insertion or deletion is inversely proportional to the string length. Wepresent a closed-form formula and show that the HID is a proper distance metric. Thenwe perform an experimental comparison of HID to normalized and unnormalized versionsof the indel distance on benchmark tasks for biomedical sequence data. We finally showplanar embeddings of the benchmark datasets to provide some insights into the geometryof the metric spaces associated with the different distance metrics.",
  "Introduction and Setting": "In this paper, we introduce the harmonic indel distance (HID). The HID is a distance metric between stringsthat is normalized in the sense that two long strings that differ by a single symbol are closer to each otherthan two short strings that differ by a single symbol. Our main technical contribution is Theorem 3.1 whichproves the triangle inequality for HID and shows that it indeed defines a distance metric between strings.",
  "d(A, B) = 2H|A|+|B|| lcs(A,B)| H|A| H|B|(1.1)": "where | | denotes string length, Hn = ni=1 1/i is the harmonic series and lcs(A, B) denotes the longestcommon subsequence (LCS) of A and B. We will also use the notation scs(A, B) for the shortest commonsupersequence (SCS) of two strings. The paper is structured as follows: in the remainder of this section we provide some additional intuitionon the definition of the HID, give an overview of related work and briefly comment on the computationalcomplexity. proves the triangle inequality. In section 4 we first show that HID can be applied tosupervised machine learning tasks by two experiments that apply HID to a classification and a regression task.Next, we show that HID is applicable to unsupervised learning by giving an example of a data visualizationtask. We additionally compare HID to alternative string distances and show that it differs from alternativedistances on some of the tasks. wraps up the paper and provides additional suggestions for usecases of the HID.",
  "1j(1.2)": "using that | scs(A, B)| = |A| + |B| | lcs(A, B)|. The interpretation is as follows: First we insert charactersto transform A into scs(A, B), where the cost of each insertion is inversely proportional to the length of theintermediate string (i in the formula) at that step. Then we delete characters to transform scs(A, B) intoB, with the cost of a deletion again being inversely proportional to the length of the intermediate string (jin the formula) on which it is performed.",
  "Published in Transactions on Machine Learning Research (11/2024)": "All benchmark experiments used support vector machines with radial basis function kernels based on thestring metrics described above. The SVM margin as well as the RBF variance hyperparameters were opti-mized using the Tree-structured Parzen Estimator algorithm implemented in the Optuna software (Akibaet al., 2019). We used the SVM implementation from Scikit-Learn (Pedregosa et al., 2011). The hyperpa-rameters used are included in in the appendix.",
  "Background and related work": "The idea behind edit distances it to define the distance between two strings A and B to be the total costof transforming A into B through a sequence of operations such as insertions, deletions and substitutionsof characters. In this work, we consider so-called indel string distances, which are a particular case of editdistances where the possible operations are restricted to insertions and deletions (indels). In particular, acharacter substitution corresponds to a deletion followed by an insertion. The most fundamental indel string distance is simply known as the indel distance (ID), see Deza & Deza(2013). Like the HID, the ID quantifies the total cost of transforming a string A into a string B using theoperations of inserting and deleting characters. The cost of each operation for ID is 1, whereas for HID thecost is inversely proportional to the length of the intermediate string. Comparing HID and ID thereforeallows to isolate the impact of normalizing the cost. The ID can be computed from the LCS using theformuladID(A, B) = |A| + |B| 2| lcs(A, B)|.",
  "|A| + |B| + dID(A, B)": "Denoting the empty string, note that dSTID(A, ) = 1 for any string A (since dID(A, ) = |A|) and thatdSTID(A, B) 1 for any A, B. This shows in particular that dSTID embeds the space of strings of arbitrarylength into a sphere of radius 1. The STID is normalized in the sense that if A is a subsequence of B thendID(A, B) = |B| |A| and dSTID(A, B) = |B||A|",
  "|B|. In contrast for the HID we have dSTID(A, ) = H|A| sothat the space of all strings equipped with the STID distance has infinite radius": "The HID is inspired by the contextualized normalized edit distance from de la Higuera & Mico (2008) whichrequires the computation of shortest paths over all possible edit operations, implemented using a customdynamic programming algorithm. The contextualized normalized edit distance is then the sum of the costsover the shortest path, where the cost at each step in the path is normalized by the inverse string length atthat step. From (1.2) it is clear that the HID is identical to the contextualized normalized edit distance restricted toinsertions and deletions (there is only a single shortest path). Our closed-form formula allows us to reduce",
  "d(A, B) = (H| scs(A,B)| H|A|) + (H| scs(A,B)| H|B|)": "so that d(A, B) = 0 implies | scs(A, B)| = |A| = |B| and A = B. To prove the triangle inequality, we use thenotation and property scs(A, B, C) = scs(A, scs(B, C)) = scs(scs(A, B), C) and apply in turn Lemmas 3.2,3.4, 3.3 twice and 3.2 again to obtain",
  "Experiments": "The purpose of the experiments in this section is to compare the HID to other string distances when appliedto machine learning tasks: the indel distance (ID) and the Steinhaus transform indel distance (STID). Notethat we do not compare against the contextualized normalized edit distance since it is identical to the HID ifwe restrict the operations to insertions and deletions as we do in this paper. In addition, the cubic complexityof the normalized edit distance would be prohibitive for our experiments. We perform experiments on two benchmark tasks for biological sequence regression and classification re-spectively. We also present planar embeddings for each dataset using t-SNE to gain some insight into thegeometries of the spaces associated to the different distances. presents statistics on the datasetsused. As our main goal is to evaluate the differences between HID, ID and STID, we do not aim to beatstate-of-the-art deep learning models but we do include standard baselines to put our results into perspec-tive. We do not claim either that HID is inherently superior to any of the other distances. Each distanceembeds the data in a metric space with a different geometry, and the most appropriate geometry dependson the task at hand. Our experiments do show that the different metrics considered result in differencesin performance for some but not all tasks and that normalization by string length is beneficial for the twosupervised learning tasks considered.",
  "Classification": "The classification task involves the classification of sequences of non-coding RNA according to their typeand uses the Dataset2 dataset from the benchmark paper Creux et al. (2024). This dataset was chosenbecause non-coding RNA are some of the shortest biological sequences, and we expect the benefit of thenormalization in HID to be higher for shorter sequences. The dataset provides training and test splits, anda validation set was generated by random splitting of the provided training set. The results are shown in . We see that the SVMs with HID and STID kernels are competitive withthe strongest baseline, which uses a recurrent neural network architecture, whereas the SVM with ID kernelunderperforms 4 out of the 6 baselines whereas .",
  "Regression": "We evaluate the regression performance on the thermostability prediction task from the FLIP benchmark forprotein sequences (Dallago et al., 2021). This is a challenging benchmark which includes a carefully selectedtrain-validation-test split based on biological considerations. The metric adopted by the benchmark is theSpearman correlation coefficient. The baselines include language models pre-trained on a large corpus ofsequence data (ESM), the same models trained only on the benchmark training set (ESM-untrained) as wellas a CNN and a ridge regression model.",
  "Metric Embedding": "To give some more insights into the different geometries entailed by the HID, STID and ID, we providet-SNE plots of the training datasets for ncRNA () and FLIP (). The objective of t-SNEis to embed a dataset into the plane while keeping the distances in the embedding as consistent as possiblewith the distances in the original space. Each of the distance metrics defines a metric space of strings, andwe expect the t-SNE embedding to reflect as much as possible the geometry of the dataset, viewed as pointsin the string space entailed by the respective distance metric. For the ncRNA dataset, a visual inspection of suggests that all distances result in a similar geometry,with the normalized distances leading to a slightly sharper separation for example between red and greenclasses. On the other hand, for the FLIP datasets () the different distances lead to clearly different embed-dings. The HID leads to a dataset geometry that can be embedded as a crescent shape, with lower-valuedpoints concentrating in one end and higher-valued points concentrating in the other end, especially for theHuman and Human-Cell datasets. The embedding for STID still shows concentration of high- and low-value points in different regions of theembedding space and recovers the same local structures as the HID embedding. However, it does not showany non-trivial global structures, which might be due to the geometry of the STID metric space being lesscompatible with Euclidean plane geometry than the HID metric space (recall that the STID space is a sphereof radius 1 where all elements are at distance at most 1 from each other). This interpretation is supportedby the STID embedding having significantly higher KL divergence than the HID embedding in the t-SNEobjective ( in the appendix). Finally, the ID produces a radially symmetric embedding with regularly spaced patterns that do not showany obvious relation to the target value. The regular spacing could be caused by the large number of pointsthat each are at the same integer distance from each other.",
  "Discussion": "We introduced the harmonic indel distance and showed that it defines a distance metric. We showed that theharmonic indel distance outperforms the unnormalized indel distance on two biomedical sequence regressionand classification tasks while showing comparable performance to a normalized version of the indel distance.Our experiments on planar embeddings with t-SNE show that HID and STID can in some, but not all,cases result in different planar embeddings.The original motivation for the development of the HID wasclassification of web browsing histories, which involves significantly shorter data with more variation in thelength of sequences, where the lack of normalization in ID is expected to have an even larger impact. It isstriking that there are still such large differences in performance on datasets where the median length is inthe hundreds of characters, and it could be interesting to evaluate the HID on shorter sequences, for examplewithin social sequence classification. Unfortunately, as of the time of submitting this manuscript, we werenot aware of any suitable benchmark datasets with baselines containing short sequences with high variabilityin sequence length.",
  "The author acknowledges funding received under European Unions Horizon Europe Research and Innovationprogramme under grant agreement No. 101070408": "Amir Abboud, Arturs Backurs, and Virginia Vassilevska Williams. Tight Hardness Results for LCS andOther Sequence Similarity Measures. In 2015 IEEE 56th Annual Symposium on Foundations of ComputerScience, pp. 5978, October 2015. doi: 10.1109/FOCS.2015.14. URL ISSN: 0272-5428. Takuya Akiba, Shotaro Sano, Toshihiko Yanase, Takeru Ohta, and Masanori Koyama. Optuna: A Next-generation Hyperparameter Optimization Framework. In Proceedings of the 25th ACM SIGKDD Interna-tional Conference on Knowledge Discovery and Data Mining, 2019. Karl Bringmann and Marvin Kunnemann. Quadratic Conditional Lower Bounds for String Problems andDynamic Time Warping. In 2015 IEEE 56th Annual Symposium on Foundations of Computer Science, pp.7997, Berkeley, CA, USA, October 2015. IEEE. ISBN 978-1-4673-8191-8. doi: 10.1109/FOCS.2015.15.URL Karl Bringmann, Vincent Cohen-Addad, and Debarati Das. A Linear-Time n0.4-Approximation for LongestCommon Subsequence. ACM Transactions on Algorithms, 19(1):9:19:24, February 2023. ISSN 1549-6325.doi: 10.1145/3568398. URL Kuan Cheng, Alireza Farhadi, MohammadTaghi Hajiaghayi, Zhengzhong Jin, Xin Li, Aviad Rubinstein,Saeed Seddighin, and Yu Zheng. Streaming and Small Space Approximation Algorithms for Edit Dis-tance and Longest Common Subsequence. In DROPS-IDN/v2/document/10.4230/LIPIcs.ICALP.2021.54.Schloss Dagstuhl Leibniz-Zentrum fr Informatik, 2021.doi: 10.4230/LIPIcs.ICALP.2021.54.URL Constance Creux, Farida Zehraoui, Franois Radvanyi, and Fariza Tahi. Comparison and benchmark ofdeep learning methods for non-coding RNA classification, April 2024. URL Pages: 2023.11.24.568536 Section: New Results. Christian Dallago, Jody Mou, Kadina E. Johnston, Bruce Wittmann, Nick Bhattacharya, Samuel Goldman,Ali Madani, and Kevin K. Yang.FLIP: Benchmark tasks in fitness landscape inference for proteins.In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track(Round 2), 2021. URL",
  "Muhammad Marwan Muhammad Fuad. Parameter-Free Extended Edit Distance. In International Confer-ence on Data Warehousing and Knowledge Discovery, pp. 465475. Springer, 2014": "F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer,R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay.Scikit-learn: Machine Learning in Python. Journal of Machine Learning Research, 12:28252830, 2011. Semih Yavuz, Chung-Cheng Chiu, Patrick Nguyen, and Yonghui Wu. CaLcs: Continuously ApproximatingLongest Common Subsequence for Sequence Level Optimization. In Ellen Riloff, David Chiang, JuliaHockenmaier, and Junichi Tsujii (eds.), Proceedings of the 2018 Conference on Empirical Methods in Nat-ural Language Processing, pp. 37083718, Brussels, Belgium, October 2018. Association for ComputationalLinguistics. doi: 10.18653/v1/D18-1406. URL",
  "A.1Effect of perplexity parameter on t-SNE plots": "To validate that the t-SNE plots in the main paper are representative, we here show the plots resulting froma parameter sweep of the perplexity parameter from 22 to 212 for the Human Cell dataset from FLIP. Theplots show that our observation that HID preserves more global structure in the t-SNE plots than STID isrobust to different parameter values. All embeddings were run to convergence."
}