{
  "Abstract": "A fundamental challenge in machine learning is training models that generalize well to dis-tributions different from the training distribution. Empirical Risk Minimization (ERM),which is the predominant learning principle, is known to under-perform in minority sub-populations and fail to generalize well in unseen test domains. In this work, we propose anovel learning principle called Uniform Risk Minimization (URM) to alleviate these issues.We first show theoretically that uniform training data distributions and feature represen-tations support robustness to distribution shifts.Motivated by this result, we proposean empirical method that trains deep neural networks to learn a uniformly distributedfeature representation in their final activation layer for improved robustness.Our ex-periments on multiple datasets for sub-population shifts and domain generalization showthat URM improves the generalization of deep neural networks without requiring knowl-edge of groups or domains during training.URM is competitive with the best exist-ing methods designed for these tasks and can also be easily combined with them forimproved performance.Our work sheds light on the importance of the distribution oflearned feature representations for model robustness and fairness.Code is available at",
  "Introduction": "ERM has been the predominant approach to train machine learning models. Consistently achieving highaccuracy on test distributions identical to the training data distribution was a challenge until the resurgentsuccess of deep learning. However, machine learning practitioners today realize that a major challenge isrobustness in test distributions different from the training data distribution. Hence, the assumption thattraining and test data distributions are identical no longer captures the challenge of modern machine learning. In this work, we consider the challenging problem of training models that perform well in distributionsdifferent from the training distribution without assuming knowledge of the test distribution during thelearning process. Specifically, we explore the question of what is the best data distribution to train classifierson for improved downstream generalization. We consider two downstream generalization tasks in our work,i.e., sub-population shifts and domain generalization. In the sub-population or group shift problem setup,we have input x X and labels y Y, with the objective to learn a function f : X Y. Moreover,there exist attributes of the data a1, ..., ai, ..., am, ai Ai, which may not be available during training.",
  "gG gpg, where |G|.The test distribution, which is not observedduring training, is: pte =": "gG gpg, where |G| and = .The objective for f is to performaccurately in the unobserved pte. Generally, models perform most poorly when the test distribution onlycomprises minority sub-populations of the training distribution (Hashimoto et al., 2018; Zhang et al., 2020).In sensitive applications of machine learning such as healthcare, this leads to fairness issues when modelsperform better for some groups of people than others. Multiple methods have been proposed to tackle thisproblem of robustness to sub-population shifts. A common approach so far has been identifying the sampleswhere models performs poorly and up-weighting these samples during training. This identify-and-emphasiseparadigm has been proposed in multiple works (Liu et al., 2021; Zhang & Sabuncu, 2018; Yaghoobzadehet al., 2021; Sanh et al., 2021; Utama et al., 2020). Many proposed methods also rely on the knowledgeof minority sub-population attributes for training or validation to achieve good performance (Gowda et al.,2021; Izmailov et al., 2022; Menon et al., 2021; Nam et al., 2022; Yao et al., 2022). However, knowledge ofrelevant sub-populations may not always be known, especially in novel settings, so this assumption may beimpractical. In domain generalization (DG), models are provided a set of training domains, each containing examplesabout the same task, with the goal that they must perform well on unseen target domains (Blanchardet al., 2011; Muandet et al., 2013). While challenging, DG captures real-world scenarios where unforeseendistribution shifts between training and test distributions can be encountered by models. Both scenarios,sub-population shifts and domain generalization, require novel solutions that enable generalization whilerequiring limited privileged information such as access to minority group annotations or access to samplesfrom the test domains. In this work, we present a novel learning principle called Uniform Risk Minimization (URM) to alleviatethese issues. We first propose a novel risk measure, uniform risk, which is defined as the expected risk of themodel as the test distribution varies over the support of the training distribution. Specifically, uniform riskis equal to the expected risk of a model over all possible test distributions under a uniform prior. Our maintheoretical result is as follows: training classifiers on uniformly distributed training data or learned featurerepresentations is an optimal choice to lower uniform risk and supports robustness and fairness. Motivated bythis result, we propose an empirical method to encourage deep neural networks to learn uniformly distributedfeature representations using an adversarial training objective. We show using experiments that encouragingmodels to have uniformly distributed feature representations significantly improves their robustness to sub-population shifts and domain generalization, without requiring knowledge of groups or domains.",
  ". We propose Uniform Risk Minimization (URM), a novel learning principle for out of distribu-tion (ood) robustness and fairness (.3)": "2. We first theoretically show that training classifiers on uniformly distributed input data orfeature representations in deep neural networks is optimal for lowering uniform risk andsupports robustness (Propositions 2.2, 2.3). 3. Motivated by our theoretical analysis, we propose a method to learn uniformly distributedfeature representations in the final activation layer of deep neural networks using an adversarialobjective (.6). We empirically demonstrate the efficacy of this method for sub-populationshifts and domain generalization, without requiring knowledge of task-specific knowledge such asgroup or domain labels ().",
  "Published in Transactions on Machine Learning Research (12/2024)": "digits rotated to different degrees (0, 15, 30, 45, 60, 75 and contains 10 classes. PACS (Li et al., 2017) containsfour diverse domains (art, cartoons, photos, sketches) and 7 classes. VLCS (Fang et al., 2013) contains fourphotographic domains (Caltech101, LabelMe, SUN09, VOC2007) and 5 classes. OfficeHome (Venkateswaraet al., 2017) also includes four domains (art, clipart, product, real) and 65 classes. TerraIncognita (Beeryet al., 2018) comprises photographs of animals taken by camera traps at four different locations.",
  "Notation and Problem Setup": "We describe the general setting of distribution shifts between the training and test distributions for theclassification problem. We denote the joint training distribution as ptr(x, y) and the test distribution aspte(x, y), where x denotes the input and y denotes the label.We assume these two distributions havethe same support sets X, Y. We denote a labeled training dataset of size Ntr sampled from the trainingdistribution (x(i)tr , y(i)tr )Ntri=1, where (x(i)tr , y(i)tr ) ptr(x, y), and a dataset of size Nte from the test distribution(x(i)te , y(i)te )Ntei=1, where (x(i)te , y(i)te ) pte(x).",
  "Uniform Risk": "In real-world deployments of machine learning, we do not always know which test distributions a model willencounter. Hence, we propose a novel risk measure that incorporates the expected risk over all possible testdistributions. As we assume that we do not know the test distribution, we wish to train models that performwell under various scenarios. So the expected risk is defined using an uninformative uniform prior over allpossible test distributions. A key fact enabling the mathematical definition of such a uniform prior is thatall computer systems represent data using finite-precision floating points, making all data representationsdiscrete in practice. This enables us to define the uniform prior over all test distributions using the Dirichletdistribution, which is the natural prior distribution for any discrete distribution. Let N be the size of thesupport set of the data distribution, i.e. the total number of possible inputs. For N-dimensional distributions,the uniform prior can be represented using the Dirichlet distribution such that pte(x) Dir( = 1). Note",
  ", we define uniform risk, RU, as the expectation of the test loss under a uniform prior over all possible testdistributions. We assume that training and test distributions have the same support": "1As suggested by (Nguyen et al., 2022), in a classification problem, this can be enforced easily by augmenting the outputsoftmax of the classifier such that each class probability is always at least exp (M). For example, if we choose M = 3 exp (M) 0.05, and if the output softmax is (p1, p2, ..., pC), we can augment it as (p1 K +0.05, p2 K +0.05, ..., pC K +0.05),where K = 1 0.05 C and C is the number of classes. This ensures the bound for the loss on a sample, while leaving theoutput prediction class unchanged.",
  "(4)": "Note that the bound depends on the training loss, the expected KL divergence between training and test dis-tributions, and a constant (Equation 4). Modern deep learning systems can minimize the training loss nearlyto zero as long as the task is well defined and realizable by modern architectures. Hence, the main componentof Eq. 4 to minimize would be the expected KL divergence between the training and test distributions. Thisleads us to our first key result:",
  "Proof. Proof in Appendix B": "Proposition 2.2 states that the training data distribution that minimises the upper bound on uniform riskis the uniform distribution, utr(x). This means that training on uniform data distributions may improverobustness to distribution shifts. However, most practical training datasets may not be uniformly distributed.While uniformity could in principle be achieved by re-weighting the data distribution i.e. by down-weightinghigh-density regions and up-weighting low-density regions of the distribution, this would require knowledgeof the data distribution or an accurate estimation of it, which is non-trivial in practice.Modern deeplearning systems are usually trained on high-dimensional image or audio datasets whose distributions arenot known and are difficult to estimate. Hence, we will consider a similar proposition using the learnedfeature representations of deep neural networks (.5).",
  "Revisiting Group-balanced and Class-balanced Training via URM": "Rather than considering the data distribution over the input space, we can also consider sub-group and classdistribution shifts i.e., shifts in p(g) and p(y), respectively. Similar to Proposition 2.2, we can show thattraining with uniform sub-group or class distributionsequivalent to group-balanced and class-balancedtrainingreduces uniform risk as defined in these scenarios (discussion and proof in Appendix E). Addition-ally, in these scenarios, uniform risk can be shown to be equivalent to the balanced risk across sub-groups orclasses, respectively (proof in Appendix E). Thus, minimizing uniform risk in these scenarios directly corre-sponds to minimizing the balanced risk across sub-groups or classes. In fact, prior work (Idrissi et al., 2022)has shown that group-balanced training is an effective approach for robustness to sub-population shifts whengroup-attributes of the training samples are known. Prior work (Buda et al., 2018) has also demonstrated theefficacy of class-balanced training. We provide further empirical evidence below that class-balanced trainingis effective for label-shifts in .3. Hence, URM provides a unified perspective on various algorithmsthat perform group or class balanced training.",
  "Proof. Follows proof of Proposition 2.2": "Hence, we have shown that uniformly distributed feature representations also lower uniform risk.Thismotivates our proposed method below that encourages deep neural networks to learn a feature representa-tion space that is uniformly distributed for improved robustness (.6). Note that the assumptionItr(z, y) = Itr(x, y) ensures that the representation z is useful for classification while being uniformly dis-tributed. In practice, this is ensured by the joint optimization of the classifier and our proposed regularizerobjectives.",
  ": Overview of adversarial distributionmatching to encourage the encoder to output uni-formly distributed representations": "Motivated by Proposition 2.3, we perform URM by learn-ing deep models with uniformly distributed feature rep-resentations.In order to encourage the distribution ofdeep feature vectors to match a uniform distribution, weuse an adversarial training approach. Adversarial train-ing is a useful approach to match distributions as it doesnot require significant modifications to the output of theencoder. Alternate approaches such as minimizing KL-divergence would require limiting the expressivity of themodel to output a parametric distribution in order to an-alytically compute its divergence from a target distribu-tion. We train a domain classifier (or discriminator) todistinguish between feature vectors produced by an en-coder and samples from a uniform noise distribution. Si-multaneously, the encoder is trained to fool the domainclassifier by generating feature vectors that resemble auniform distribution. This is achieved through adversar-ial training: the domain discriminator, D, learns to dif-ferentiate encoded feature vectors from random uniform noise, while the encoder, G, is optimized to bothconfuse D and learn useful representations for a downstream task, typically handled by a linear classifier T(see for an overview). Our method resembles Adversarial Autoencoders (Makhzani et al., 2016),which use adversarial training to align the latent space of autoencoders with desired prior distributions. During training, we want the discriminator Ds predictions for uniform random noise vectors to be accurate bymaximizing Ezpu[log D(z)]. On the other hand, given an encoded feature vector G(x) the discriminator mustoutput a probability, D(G(x)), close to zero by maximizing Expx[log(1 D(G(x)))]. The encoder must betrained to fool D to produce a high probability for feature vectors by minimizing Expx[log(1D(G(x)))] ormaximising Expx[log D(G(x))]. The combined objective function to update D and G represents a minimaxgame with the following loss function:",
  "minG maxD L(D, G) = Ezpu[log D(z)] + Ezpz[log(1 D(z)]": "where Ezpu[log D(z)] is not used to update G. Generative adversarial networks (GAN) (Goodfellow et al.,2014) theory shows that the above objective minimizes the JensenShannon Divergence between the encodedfeature distribution and the uniform distribution when the discriminator is optimal.Let the be theparameters of the task model, which includes the encoder G as its backbone and a task head T such as alinear classifier i.e. = {G, T}. LT is the task-specific loss function. We can summarize the overall URMtraining objective for and D as follows:",
  "minmaxD L(, D) = E(x,y)ptr[LT (T(G(x)), y)] + Exptr[log(1 D(G(x))] + Ezpu[log D(z)]": "The hyper-parameter determines the weight for training the encoder and discriminator to classify thefeature outputs z = G(x). A higher increases the strength of regularization at the expense of featurelearning for the downstream task and lower weakens regularization but allows the encoder to pay moreattention to the downstream task. The same is applied to both update the discriminator and encoderso that one does not overpower the other. The choice of activation function applied to the output of theencoder G can be changed for each task. We choose from either the hyperbolic Tangent (TanH) or ReLUactivations. In case of TanH activations, the uniform noise distribution is z U(1, 1) and in case of ReLUit is z U(0, 1). We use Leaky ReLUs in the discriminator to improve gradient flow to the generator. Wetrain the encoder G and discriminator D alternately.",
  "Experimental Settings and Datasets": "For group robustness, our experimental setup followed Yang et al. (2023). Specifically, we used the evaluationsetting where group attributes are unknown in the training set. The worst-class accuracy is used to performmodel selection, i.e., choice of the best checkpoint from each training run, as it was found to be an effectivealternative in the absence of group-attributes by Yang et al. (2023). For all methods, group attributes inthe validation set were used to select hyper-parameters as well as to fine-tune methods that require a group-balanced dataset e.g. DFR (Izmailov et al., 2022). We report worst-group and balanced (across classes)accuracies. All results were averaged across three random seeds. For domain generalization experiments, ourexperimental setup followed Gulrajani & Lopez-Paz (2021) using the leave-one-domain-out cross-validationscheme. We report the average accuracy across all test domains. For both tasks, we searched over sixteenrandom hyper-parameter combinations and applied adversarial distribution matching to the penultimatelayer before the linear classification head.Hyper-parameter search ranges for URM are included in theAppendix (). Adversarial training is often unstable in certain contexts, such as generative modeling.However, in our experiments, we did not encounter significant instability. This is likely because we arenot generating high-dimensional images, as in GANs, which involve a more complex distribution-matchingtask. Instead, we are generating lower-dimensional feature vectors to align with a uniform distribution. Forgroup robustness, we evaluate on Waterbirds (Wah et al., 2011), CelebA (Liu et al., 2015), CivilComments(Borkan et al., 2019) and MultiNLI (Williams et al., 2018). For Waterbirds and CelebA, we use the ResNet50(He et al., 2016) architecture pretrained on ImageNet1k (Russakovsky et al., 2015).For MultiNLI andCivilComments, we use BERT (Devlin et al., 2019) pretrained on Book Corpus and English Wikipedia data.For domain generalization (DG), we benchmark on ColoredMNIST (Arjovsky et al., 2020), RotatedMNIST(Ghifary et al., 2015), PACS (Li et al., 2017), VLCS (Fang et al., 2013), OfficeHome (Venkateswara et al.,2017) and TerraIncognita (Beery et al., 2018). For DG benchmarks, all methods used the ResNet50 (Heet al., 2016) architecture pretrained on ImageNet1k (Russakovsky et al., 2015). Other training and datasetparameters are based on (Gulrajani & Lopez-Paz, 2021). We further describe all benchmark datasets inAppendix F. All experiments were run on a computer with 8 A100 GPUs.",
  "Baselines": "We consider baselines including ERM and various methods proposed for sub-population shifts and domaingeneralization. For sub-population shifts, we consider GroupDRO, LfF, JTT, LISA, DFR, Mixup, IRM,CORAL, MMD, DANN, C-DANN, ReSample, ReWeight, Focal Loss, CBLoss (Class-balanced loss), LDAM,Balanced Softmax (BSoftmax), CRT and ReWeightCRT. We follow Yang et al. (2023) for implementation ofthese baselines. For domain generalization, we consider Inter-domain Mixup (Mixup), MLDF, MTL, ARM,SagNet, RSC, and VREx. We follow Gulrajani & Lopez-Paz (2021) for implementation of these baselines.For a description of these baselines, please refer to our Appendix G. We note that URM is a generic robustrepresentation learning method that does not require any knowledge of training domains or groups for bothsub-population shifts and domain generalization.",
  "URM Supports Class-balanced Training for Robustness under Label-shifts": "As discussed in .4, when considering shifts in the class-distribution (label-shifts) alone, we demon-strate that URM, which corresponds to class-balancing, produces the best balanced accuracy. We considerthe binary classification tasks in the Waterbirds and ColoredMNIST datasets. We trained models with differ-ent class-distributions and compute each models balanced accuracy, which is the average of the class-specificaccuracies on the test set. Balanced accuracy is a commonly used metric in imbalanced classification tasks.We train models with different values of p (p {0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9}), where p correspondsto the binary class distribution parameter (). Note that p = 0.5 corresponds to class-balanced train-ing, which corresponds to URM for label shift scenarios. The URM model (p = 0.5) achieves the highestbalanced accuracy on the test set (). Balanced accuracy deteriorates when the the training class dis-tribution is highly imbalanced e.g. p = 0.9 or 0.1. As discussed in .4, uniform risk corresponds to",
  "p = 0.185.755.3p = 0.286.068.0p = 0.385.376.5p = 0.486.176.9p = 0.5 (URM)86.876.9p = 0.685.776.5p = 0.786.276.3p = 0.886.475.7p = 0.985.766.6": "URM learns more sub-group robust represen-tations compared to ERM on multiple datasets().URM can also be easily com-bined with other methods as it is a genericrepresentation learning method.For exam-ple, we combined URM with Deep FeatureReweighting (DFR) as it involves fine-tuningthe classifier on a group balanced validationset, which URM does not require.Notably,when combined with DFR, URM achieved thebest worst-group accuracy on the Waterbirdsand MultiNLI datasets (URM (DFR), ). On MultiNLI, URM (DFR) achieved 4%higher worst-group accuracy than the existingbest methods. URM also achieved the high-est balanced accuracy on MultiNLI. The bet-ter worst-group accuracy achieved by URM(DFR) reflects the improved representation learned by URM in the absence of any group attribute in-formation. We note that URM is a generic robust representation learning method and is not specific tosub-group robustness like many of the other baseline methods.",
  "AlgorithmWaterbirdsCelebACivilCommentsMultiNLIworst (%)balanced (%)worst (%)balanced (%)worst (%)balanced (%)worst (%)balanced (%)": "ERM69.1 4.783.1 2.057.6 0.893.0 0.063.2 1.279.4 0.266.4 2.381.0 0.3Mixup77.5 0.788.9 0.257.8 0.893.1 0.165.8 1.579.7 0.066.8 0.381.7 0.1GroupDRO73.1 0.486.3 0.568.3 0.993.9 0.161.5 1.881.3 0.164.1 0.881.1 0.2CVaRDRO75.5 2.288.5 0.360.2 3.093.1 0.162.9 3.880.8 0.148.2 3.475.4 0.2JTT71.2 0.586.8 0.248.3 1.592.4 0.251.0 4.277.7 0.865.1 1.681.4 0.0LfF75.0 0.786.3 0.353.0 4.385.3 2.942.2 7.269.7 4.757.3 5.771.4 1.6LISA77.5 0.788.9 0.257.8 0.893.1 0.165.8 1.579.7 0.066.8 0.381.7 0.1ReSample70.0 1.085.0 0.274.1 2.293.8 0.161.0 0.680.7 0.166.8 0.581.5 0.0ReWeight71.9 0.686.2 0.169.6 0.294.0 0.159.3 1.181.3 0.064.2 1.979.4 0.2SqrtReWeight71.0 1.487.2 0.666.9 2.293.9 0.168.6 1.180.6 0.263.8 2.480.6 0.2CBLoss74.4 1.286.2 0.665.4 1.493.8 0.167.3 0.280.3 0.363.6 2.480.6 0.3Focal71.6 0.887.1 0.356.9 3.492.6 0.361.9 1.178.7 0.362.4 2.080.9 0.2LDAM70.9 1.786.0 0.257.0 4.193.2 0.228.4 7.769.5 3.265.5 0.880.9 0.1BSoftmax74.1 0.987.0 1.069.6 1.294.2 0.158.3 1.181.1 0.163.6 2.480.6 0.2DFR89.0 0.291.2 0.173.7 0.893.2 0.064.4 0.179.0 0.063.8 0.080.2 0.0CRT76.3 0.887.9 0.169.6 0.793.6 0.067.8 0.380.7 0.065.4 0.280.2 0.0ReWeightCRT76.3 0.288.0 0.270.7 0.693.6 0.064.7 0.280.7 0.065.2 0.280.2 0.0",
  "AlgorithmColoredMNISTRotatedMNISTVLCSPACSOfficeHomeTerraIncognita": "ERM36.7 0.197.7 0.077.2 0.483.0 0.765.7 0.541.4 1.4IRM40.3 4.297.0 0.276.3 0.681.5 0.864.3 1.541.2 3.6GroupDRO36.8 0.197.6 0.177.9 0.583.5 0.265.2 0.244.9 1.4Mixup33.4 4.797.8 0.077.7 0.683.2 0.467.0 0.248.7 0.4MLDG36.7 0.297.6 0.077.2 0.982.9 1.766.1 0.546.2 0.9CORAL39.7 2.897.8 0.178.7 0.482.6 0.568.5 0.246.3 1.7MMD36.8 0.197.8 0.177.3 0.583.2 0.260.2 5.246.5 1.5DANN40.7 2.397.6 0.276.9 0.481.0 1.164.9 1.244.4 1.1CDANN39.1 4.497.5 0.277.5 0.278.8 2.264.3 1.739.9 3.2MTL35.0 1.797.8 0.176.6 0.583.7 0.465.7 0.544.9 1.2SagNet36.5 0.194.0 3.077.5 0.382.3 0.167.6 0.347.2 0.9ARM36.8 0.098.1 0.176.6 0.581.7 0.264.4 0.242.6 2.7VREx36.9 0.393.6 3.476.7 1.081.3 0.964.9 1.337.3 3.0RSC36.5 0.297.6 0.177.5 0.582.6 0.765.8 0.740.0 0.8",
  "URM36.9 0.298.1 0.284.3 4.884.0 0.267.5 0.548.3 1.4URM (Mixup)32.4 3.696.7 0.477.1 0.287.2 3.468.9 0.649.3 0.9": "Inter-domain Mixup, URM achieved the best accuracy on the PACS dataset achieving 3.5% better accuracythan the best competing method, as well as the OfficeHome and Terraincognita benchmarks (). Thisdemonstrates that URM, a robust representation learning method, can be combined with an orthogonalmethod such as Mixup, a data augmentation method. URM does not use any knowledge of the domaingeneralization task and yet achieves competitive accuracy with methods that do. As URM is performed bylearning a feature representation that is uniformly distributed over a bounded range e.g. Z, where Z isthe dimensionality of the feature space, feature representations of test domain samples fall within the familiarfeature space of the URM model, reducing the likelihood of unexpected outputs. Hence, the URM modelsees the entire range of feature representations equally during training, improving domain generalization.",
  "Visualizing Uniformity of Learned Feature Representations": "Data distributions often have multiple peaks and troughs, and models trained on such distributions tendto be biased towards high-density regions (majority groups) since they contribute more to gradient updatesduring training. Consequently, the learned parameters are more optimized for these high-density regions,resulting in poorer performance on low-density regions (the troughs). URM addresses this by learning afeature space that is more uniformly distributed, effectively flattening the training feature distribution andreducing bias. This approach encourages the model to learn features that are more evenly distributed acrossboth high- and low-density regions. To demonstrate this, we visualized the feature representations learnedby URM compared to those learned by ERM using UMAP (McInnes et al., 2018) (). We extractedfeature vectors for test samples in the Waterbirds dataset using ERM and URM trained models. WhileUMAP does not provide a complete picture of the data due to significant dimensionality reduction, weobserved that URMs features were significantly more uniformly distributed than ERMs. This suggests thatURM achieves the training objective for the feature representations to be more uniformly distributed andcontributes to the robustness of the model.",
  "Related Work": "Group robustness: The topic of robustness to sub-population shifts has been increasingly studied inrecent years as machine learning models are being deployed in real-world applications where we wish toavoid downstream performance biases. Many such methods involve training on a group balanced datasetwith improved performance. As shown above, URM in fact supports this approach of training on uniform",
  ": UMAP (McInnes et al., 2018) visualization of feature vectors generated by ERM and URM onWaterbirds dataset. URM generates a more uniformly distributed feature representation space compared toERM": "group distributions. For example, one of the existing best methods, DFR, retrains the linear classifier head ona group balanced dataset. However, group-attributes may not available if many practical scenarios. Hence,a method such as URM that does not require group annotations would be useful. Moreover, as shown above,URM can also be combined with methods such as DFR for improved performance (.4). Uniform priors: We note that our work is closely related to the ideas presented in uniform priors fordata-efficient learning (Sinha et al., 2022). Compared to (Sinha et al., 2022), we proposed a novel learningprinciple and framework to motivate the use of uniform priors for robustness.In fact, our results mayexplain the empirical results of (Sinha et al., 2022). In other work, uniform priors have also been employedin self-supervised representation learning, both explicitly and implicitly (Assran et al., 2023). Robust Representation Learning: URM is also related to the idea of state reification (Lamb et al.,2019), which aims to project out-of-distribution hidden states back on to the manifold of familiar hiddenstates from the training distribution. Rather than training additional networks to map out-of-distributionhidden states back on to the training data manifold, URM learns bounded feature spaces that are uniformlydistributed. Hence, even out-of-distribution samples will fall in the familiar feature space of the model andis less likely to provide unexpected outputs. Other work has introduced input-space transformations thatmitigate the effect of irrelevant input features to improve representation learning (Taghanaki et al., 2021).",
  "Conclusion": "We presented a novel learning principle called Uniform Risk Minimization (URM) and showed theoreticallythat uniform training data distributions are an optimal choice to lower uniform risk, a novel risk measure forood robustness. Our results also provide a unified perspective on various existing algorithms that performclass-balanced or group-balanced training from the perspective of distribution shift. Finally, we proposedan empirical method to learn uniformly distributed feature representations in deep neural networks usingadversarial distribution matching. We empirically demonstrated the effectiveness of our method to improverobustness to sub-population shifts as well as domain generalization, without knowledge of any privilegedgroup or domain attribute information during training. Our work sheds light on the importance of the dis-tribution of learned feature representations for model robustness and fairness. We hope our work encouragesfurther research into uniform data distributions and priors for robust and fair representation learning.",
  "Martin Arjovsky, Lon Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant Risk Minimization, March2020. URL arXiv:1907.02893 [cs, stat]": "Mido Assran, Randall Balestriero, Quentin Duval, Florian Bordes, Ishan Misra, Piotr Bojanowski, PascalVincent, Michael Rabbat, and Nicolas Ballas. The hidden uniform cluster prior in self-supervised learning.In The Eleventh International Conference on Learning Representations, 2023. URL Sara Beery, Grant Van Horn, and Pietro Perona. Recognition in Terra Incognita. In Vittorio Ferrari, MartialHebert, Cristian Sminchisescu, and Yair Weiss (eds.), Computer Vision ECCV 2018, pp. 472489, Cham,2018. Springer International Publishing. ISBN 978-3-030-01270-0. Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer WortmanVaughan. A theory of learning from different domains. Machine Learning, 79(1):151175, May 2010. ISSN1573-0565. doi: 10.1007/s10994-009-5152-4. URL Gilles Blanchard, Gyemin Lee, and Clayton Scott.Generalizing from Several Related Classifica-tion Tasks to a New Unlabeled Sample.In J. Shawe-Taylor, R. Zemel, P. Bartlett, F. Pereira,and K. Q. Weinberger (eds.), Advances in Neural Information Processing Systems, volume 24. Cur-ran Associates, Inc., 2011. URL Gilles Blanchard, Aniket Anand Deshmukh, Urun Dogan, Gyemin Lee, and Clayton Scott. Domain gen-eralization by marginal transfer learning. J. Mach. Learn. Res., 22(1), January 2021. ISSN 1532-4435.Publisher: JMLR.org. Daniel Borkan, Lucas Dixon, Jeffrey Sorensen, Nithum Thain, and Lucy Vasserman. Nuanced Metrics forMeasuring Unintended Bias with Real Data for Text Classification. In Companion Proceedings of The2019 World Wide Web Conference, WWW 19, pp. 491500, New York, NY, USA, 2019. Association forComputing Machinery. ISBN 978-1-4503-6675-5. doi: 10.1145/3308560.3317593. URL event-place: San Francisco, USA. Mateusz Buda, Atsuto Maki, and Maciej A. Mazurowski. A systematic study of the class imbalance problemin convolutional neural networks. Neural Networks, 106:249259, October 2018. ISSN 08936080. doi:10.1016/j.neunet.2018.07.011. URL arXiv: 1710.05381. Kaidi Cao, Colin Wei, Adrien Gaidon, Nikos Arechiga, and Tengyu Ma. Learning Imbalanced Datasets withLabel-Distribution-Aware Margin Loss. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d Alch-Buc,E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 32. Cur-ran Associates, Inc., 2019. URL Y. Cui, M. Jia, T. Lin, Y. Song, and S. Belongie. Class-Balanced Loss Based on Effective Number of Samples.In 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 92609269,Los Alamitos, CA, USA, June 2019. IEEE Computer Society.doi: 10.1109/CVPR.2019.00949.URL Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of Deep Bidi-rectional Transformers for Language Understanding.In Jill Burstein, Christy Doran, and ThamarSolorio (eds.), Proceedings of the 2019 Conference of the North American Chapter of the Associationfor Computational Linguistics:Human Language Technologies, Volume 1 (Long and Short Papers),pp. 41714186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.doi:10.18653/v1/N19-1423. URL Chen Fang, Ye Xu, and Daniel N. Rockmore. Unbiased Metric Learning: On the Utilization of MultipleDatasets and Web Images for Softening Bias. In 2013 IEEE International Conference on Computer Vision,pp. 16571664, 2013. doi: 10.1109/ICCV.2013.208.",
  "Ishaan Gulrajani and David Lopez-Paz. In Search of Lost Domain Generalization. In International Confer-ence on Learning Representations, 2021. URL": "Tatsunori Hashimoto, Megha Srivastava, Hongseok Namkoong, and Percy Liang.Fairness Without De-mographics in Repeated Loss Minimization.In Proceedings of the 35th International Conference onMachine Learning, pp. 19291938. PMLR, July 2018.URL ISSN: 2640-3498. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep Residual Learning for Image Recognition.In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 770778, June 2016.doi: 10.1109/CVPR.2016.90. ISSN: 1063-6919. Zeyi Huang, Haohan Wang, Eric P. Xing, and Dong Huang. Self-challenging Improves Cross-Domain Gen-eralization.In Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm (eds.), Com-puter Vision ECCV 2020, volume 12347, pp. 124140. Springer International Publishing, Cham,2020.ISBN 978-3-030-58535-8 978-3-030-58536-5.doi: 10.1007/978-3-030-58536-5_8.URL Series Title: Lecture Notes in Computer Sci-ence. Badr Youbi Idrissi, Martin Arjovsky, Mohammad Pezeshki, and David Lopez-Paz. Simple data balancingachieves competitive worst-group-accuracy. In Proceedings of the First Conference on Causal Learning andReasoning, pp. 336351. PMLR, June 2022. URL ISSN: 2640-3498. Pavel Izmailov, Polina Kirichenko, Nate Gruver, and Andrew Gordon Wilson. On Feature Learning in thePresence of Spurious Correlations. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and KyunghyunCho (eds.), Advances in Neural Information Processing Systems, 2022. URL",
  "John C. Duchi and Hongseok Namkoong. Learning models with uniform performance via distributionallyrobust optimization. The Annals of Statistics, 49(3):13781406, June 2021. doi: 10.1214/20-AOS2004.URL": "Bingyi Kang, Saining Xie, Marcus Rohrbach, Zhicheng Yan, Albert Gordo, Jiashi Feng, and Yannis Kalan-tidis. Decoupling Representation and Classifier for Long-Tailed Recognition. In International Conferenceon Learning Representations, 2020. URL David Krueger, Ethan Caballero, Joern-Henrik Jacobsen, Amy Zhang, Jonathan Binas, Dinghuai Zhang,Remi Le Priol, and Aaron Courville. Out-of-Distribution Generalization via Risk Extrapolation (REx).In Proceedings of the 38th International Conference on Machine Learning, pp. 58155826. PMLR, July2021. URL ISSN: 2640-3498. Alex Lamb, Jonathan Binas, Anirudh Goyal, Sandeep Subramanian, Ioannis Mitliagkas, Yoshua Bengio,and Michael Mozer. State-Reification Networks: Improving Generalization by Modeling the Distributionof Hidden Representations. In Proceedings of the 36th International Conference on Machine Learning,pp. 36223631. PMLR, May 2019. URL ISSN:2640-3498. Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M. Hospedales.Deeper, Broader and Artier DomainGeneralization.In 2017 IEEE International Conference on Computer Vision (ICCV), pp. 55435551,Venice, October 2017. IEEE.ISBN 978-1-5386-1032-9.doi: 10.1109/ICCV.2017.591.URL Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M. Hospedales. Learning to generalize: meta-learning fordomain generalization. In Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligenceand Thirtieth Innovative Applications of Artificial Intelligence Conference and Eighth AAAI Symposiumon Educational Advances in Artificial Intelligence, AAAI18/IAAI18/EAAI18. AAAI Press, 2018a. ISBN978-1-57735-800-8. Place: New Orleans, Louisiana, USA. Haoliang Li, Sinno Jialin Pan, Shiqi Wang, and Alex C. Kot.Domain Generalization with AdversarialFeature Learning.In 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.54005409, 2018b. doi: 10.1109/CVPR.2018.00566. Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollr. Focal Loss for Dense ObjectDetection. IEEE Transactions on Pattern Analysis and Machine Intelligence, 42(2):318327, 2020. doi:10.1109/TPAMI.2018.2858826. Evan Z. Liu, Behzad Haghgoo, Annie S. Chen, Aditi Raghunathan, Pang Wei Koh, Shiori Sagawa, PercyLiang, and Chelsea Finn. Just Train Twice: Improving Group Robustness without Training Group Infor-mation. In Proceedings of the 38th International Conference on Machine Learning, pp. 67816792. PMLR,July 2021. URL ISSN: 2640-3498. Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep Learning Face Attributes in the Wild. In2015 IEEE International Conference on Computer Vision (ICCV), pp. 37303738, 2015. doi: 10.1109/ICCV.2015.425.",
  "A. Tuan Nguyen, Toan Tran, Yarin Gal, Philip Torr, and Atilim Gunes Baydin.KL Guided DomainAdaptation. In International Conference on Learning Representations, 2022. URL": "Jiawei Ren, Cunjun Yu, shunan sheng, Xiao Ma, Haiyu Zhao, Shuai Yi, and hongsheng Li. Balanced Meta-Softmax for Long-Tailed Visual Recognition.In Advances in Neural Information Processing Systems,volume 33, pp. 41754186. Curran Associates, Inc., 2020.URL Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang,Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei.ImageNetLarge Scale Visual Recognition Challenge. International Journal of Computer Vision, 115(3):211252,December 2015. ISSN 1573-1405. doi: 10.1007/s11263-015-0816-y. URL Shiori Sagawa, Pang Wei Koh, Tatsunori B. Hashimoto, and Percy Liang. Distributionally Robust Neu-ral Networks for Group Shifts: On the Importance of Regularization for Worst-Case Generalization.arXiv:1911.08731 [cs, stat], April 2020. URL arXiv: 1911.08731. Victor Sanh, Thomas Wolf, Yonatan Belinkov, and Alexander M. Rush. Learning from others mistakes:Avoiding dataset biases without modeling them. In International Conference on Learning Representations,2021. URL Samarth Sinha, Karsten Roth, Anirudh Goyal, Marzyeh Ghassemi, Zeynep Akata, Hugo Larochelle, andAnimesh Garg. Uniform Priors for Data-Efficient Learning. In 2022 IEEE/CVF Conference on ComputerVision and Pattern Recognition Workshops (CVPRW), pp. 40164027, New Orleans, LA, USA, June 2022.IEEE. ISBN 978-1-66548-739-9. doi: 10.1109/CVPRW56347.2022.00447. URL",
  "C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie. Caltech-UCSD. Technical Report CNS-TR-2011-001, California Institute of Technology, 2011": "Yufei Wang, Haoliang Li, and Alex C. Kot. Heterogeneous Domain Generalization Via Domain Mixup.In ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing(ICASSP), pp. 36223626, 2020. doi: 10.1109/ICASSP40776.2020.9053273. Adina Williams, Nikita Nangia, and Samuel Bowman. A Broad-Coverage Challenge Corpus for SentenceUnderstanding through Inference. In Marilyn Walker, Heng Ji, and Amanda Stent (eds.), Proceedings of the2018 Conference of the North American Chapter of the Association for Computational Linguistics: HumanLanguage Technologies, Volume 1 (Long Papers), pp. 11121122, New Orleans, Louisiana, June 2018.Association for Computational Linguistics. doi: 10.18653/v1/N18-1101. URL Yadollah Yaghoobzadeh, Soroush Mehri, Remi Tachet des Combes, T. J. Hazen, and Alessandro Sor-doni.Increasing Robustness to Spurious Correlations using Forgettable Examples.In Paola Merlo,Jorg Tiedemann, and Reut Tsarfaty (eds.), Proceedings of the 16th Conference of the European Chap-ter of the Association for Computational Linguistics:Main Volume, pp. 33193332, Online, April2021. Association for Computational Linguistics.doi: 10.18653/v1/2021.eacl-main.291.URL Yuzhe Yang, Haoran Zhang, Dina Katabi, and Marzyeh Ghassemi. Change is Hard: A Closer Look atSubpopulation Shift. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, SivanSabato, and Jonathan Scarlett (eds.), Proceedings of the 40th International Conference on Machine Learn-ing, volume 202 of Proceedings of Machine Learning Research, pp. 3958439622. PMLR, July 2023. URL Huaxiu Yao, Yu Wang, Sai Li, Linjun Zhang, Weixin Liang, James Zou, and Chelsea Finn. Improving Out-of-Distribution Robustness via Selective Augmentation. In Proceedings of the 39th International Conferenceon Machine Learning, pp. 2540725437. PMLR, June 2022.URL ISSN: 2640-3498. Haoran Zhang, Amy X. Lu, Mohamed Abdalla, Matthew McDermott, and Marzyeh Ghassemi.Hurt-ful Words: Quantifying Biases in Clinical Contextual Word Embeddings.In Proceedings of the ACMConference on Health, Inference, and Learning, CHIL 20, pp. 110120, New York, NY, USA, 2020.Association for Computing Machinery. ISBN 978-1-4503-7046-2. doi: 10.1145/3368555.3384448. URL event-place: Toronto, Ontario, Canada.",
  "{covariate-shift}(12)": "Using the covariate-shift assumption, the second KL divergence term in Eq. 11 involving the conditionalprobabilities p(y|x) equals 0 and can be dropped.This is a reasonable assumption as generalization todistributions different from the training distribution is challenging if the labeling mechanism changes (Ben-David et al., 2010). Hence, we can make the common assumption that KL[pte(y|x)|ptr(y|x)] = 0. We canthen focus on the divergence between the marginal distributions i.e. KL[pte(x)|ptr(x)].",
  "{target-shift}(30)": "In Eq. 29, we assume that the conditional distribution of p(x|y) does not change and that only the labeldistribution p(y) has shifted. This assumes that the generative process p(x|y) has not changed at test time.This is referred to target shift in the domain adaptation literature (Zhang et al., 2013). Hence, we can dropthe second term involving the conditional misalignment is zero and focus on the target shift in Eq. 30. Basedon Eq. 30, we can similarly show that the expected test loss under target shift is minimized by the uniformtraining class-distribution.",
  "DExtending Propositions 2.1 and 2.2 to Deep Feature Layers": "Here we extend Propositions 2.1 and 2.2 to feature spaces such as those learned by a deep neural network.This will be useful below when we wish to apply the above results to a method that modifies the featurespace to be uniformly distributed in order to improve robustness. Let G be a model parameterized by a deepneural network that outputs a latent variable z i.e. z = G(x), which can then be used for a downstream tasksuch as classification. To extend the propositions to the latent space, z, we make additional assumptionsabout the input data x, y and the latent variable, z:",
  "(31)": "This is a reasonable assumption that the latent variable z is sufficient to solve the task and contains thesame information about the output variable y as does the input variable x. As the latent representation z islearned while optimizing the task objective, it must learn to retain information about the label.",
  "gGRg(balanced risk)": "Thus, uniform risk is the balanced risk of a model over groups or classes (Eq. balanced risk) in the contextof sub-population or label shifts. However, in the sub-population shift literature, authors generally focus onthe worst-group performance. The limitations of over-emphasis on the worst-group accuracy metric has beenreported in prior work (.5 of Yang et al. (2023)). In contrast, uniform risk provides a balanced riskmeasure that equally weighs all groups or classes.",
  "E.1Class Balancing Lowers Uniform Risk": "Here we discuss the common practice of class-balancing datasets from the perspective of uniform risk androbustness to label shifts at test time.In Eq.7 of the Proof of Proposition 2.1, we can alternativelydecompose the KL-divergence between the joint training and test distributions p(x, y) using the marginalmisalignment in p(y) and the conditional misalignment in p(x|y). We can then similarly show that trainingon data with a uniform class or label distribution minimizes the upper bound on uniform risk, as pertainingto the label-shift problem.",
  "(34)": "We can now aim to minimize the upper bound on the expectation of test-loss under label-shift i.e. uniformrisk for label-shifts, by minimizing the expected KL-divergence between train and test class distributions.Let C be the number of classes in the classification task. The proof follows the same steps as the proof ofProposition 2.2 but for the class-distribution p(y) instead of p(x).",
  "We describe each dataset used in our group robustness benchmarks below": "Waterbirds (Wah et al., 2011). Waterbirds is a binary classification image dataset containingspurious correlations, constructed by placing images from the Caltech-UCSD Birds-200-2011 (CUB)dataset (Wah et al., 2011) over backgrounds from the Places dataset (Zhou et al., 2018). The task isto classify landbirds from waterbirds and the spurious attribute is the background (water or land).As most images of waterbirds have a water background and most images of landbirds have a landbackground, models may latch on to the spurious correlation between the background and the typeof bird. CelebA (Liu et al., 2015). CelebA is a binary classification image dataset. The task is to predicthair color from images of celebrity faces (blond vs. non-blond), where the spurious correlation isgender. As most blond haired people in this dataset are female and most non-blond haired peopleare male, this creates a spurious correlation between gender and hair color. CivilComments (Borkan et al., 2019). CivilComments is a binary classification text datasetwhere models must predict whether an internet comment contains toxic language. The spuriousattribute is the presence of references to eight demographic identities (male, female, LGBTQ, Chris-tian, Muslim, other religions, Black, and White). MultiNLI (Williams et al., 2018). MultiNLI is a text classification dataset with 3 classes andthe target is the natural language inference relationship between the premise and the hypothesis(neutral, contradiction, or entailment). The spurious attribute is the presence of negation in thetext, as negation is highly correlated with the contradiction label. Domain Generalization Datasets:We benchmarked on multiple challenging DG datasets. ColoredM-NIST (Arjovsky et al., 2020) is a variant of the MNIST digit recognition dataset where each domain containsa disjoint set of digits colored either red or blue. The binary label is a noisy function of digit and color, suchthat color is correlated with the label to varying degree in each domain and the digit bears correlation 0.75with the label. RotatedMNIST (Ghifary et al., 2015) is also a variant of MNIST where each domain contains",
  "GBaselines": "Vanilla Training:Empirical Risk Minimization (ERM, (Vapnik, 1998)), minimizes the errors across alltraining samples. Subgroup Robust Methods: Group distributionally robust optimization (GroupDRO)(Sagawa et al., 2020) performs ERM while emphasising sub-populations or domains with high losses duringtraining. CVaRDRO (John C. Duchi & Hongseok Namkoong, 2021) is a variant of GroupDRO that up-weights training samples with the highest losses.LfF (Nam et al., 2020) trains two models where thefirst model is biased and the second one is debiased using a re-weighted objective. Just train twice (JTT)(Liu et al., 2021) first trains a standard ERM model to identify minority sub-populations in the datasetand then trains another ERM model while up-weighting minority samples. LISA (Yao et al., 2022) trainsinvariant predictors using data interpolation within and across attributes. Deep feature re-weighting (DFR)(Izmailov et al., 2022) first trains an ERM model and then retrains only the last layer of the model using adataset balanced among different sub-populations. Data Augmentation Method: Mixup (Zhang et al.,2018) trains on linear interpolations of randomly sampled training data points and their labels. Domain-Invariant Representation Learning Methods: Invariant risk minimization (IRM) (Arjovsky et al., 2020)learns a feature space such that the optimal linear classifier is the same across domains. Deep correlationalignment (CORAL) (Sun & Saenko, 2016) matches the second order moments of the feature distributions.Maximum mean discrepancy (MMD) (Li et al., 2018b) matches the MMD (Gretton et al., 2012) of featuredistributions across domains. Domain Adversarial Neural Networks (DANN, (Ganin et al., 2016)) employsadversarial training to match feature distributions across domains. Class-conditional DANN (C-DANN, (Liet al., 2018b)) is similar to DANN as it matches the class-conditional feature distribution across domains.Imbalanced Learning Methods: ReSample (Japkowicz, 2000) and ReWeight (Japkowicz, 2000) simplyre-sample or re-weight the inputs according to the number of samples per class. Focal loss (Focal) (Linet al., 2020) reduces the relative loss for well-classified samples and focuses on difficult samples.Class-balanced loss (CBLoss) (Cui et al., 2019) proposes re-weighting by the inverse effective number of samples.The LDAM loss (LDAM) (Cao et al., 2019) employs a modified marginal loss that favors minority samplesmore. Balanced-Softmax (BSoftmax) (Ren et al., 2020) extends Softmax to an unbiased estimation thatconsiders the number of samples in each class. Classifier re-training (CRT) (Kang et al., 2020) decomposesthe representation and classifier learning into two stages, where it fine-tunes the classifier using class-balancedsampling with representation fixed in the second stage. ReWeightCRT (Kang et al., 2020) is a re-weightingvariant of CRT. Domain Generalization Baselines:Inter-domain Mixup (Mixup, (Zhang et al., 2018; Wang et al.,2020)) trains on linear iterpolations between samples from different domains. Meta-Learning for DomainGeneralization (MLDG, (Li et al., 2018a)) uses MAML to meta-learn generalizing across domains. MarginalTransfer Learning (MTL, (Blanchard et al., 2011; 2021)) estimates a mean embedding for each domainthat is passed as a second argument to the model.Adaptive Risk Minimization (ARM, (Zhang et al.,2021)) extends MTL using a separate embedding model. Style-Agnostic Networks (SagNet, (Nam et al.,2021)) trains models by keeping image content and randomizing style.Representation Self Challenging(RSC, (Huang et al., 2020)) learns robust models by iteratively pruning the most activated features. RiskExtrapolation (VREx, (Krueger et al., 2021)) approximates IRM using a variance penalty."
}