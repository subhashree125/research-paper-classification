{
  "Abstract": "Scientific processes are often modelled by sets of differential equations. As datasets grow,individually fitting these models and quantifying their uncertainties becomes a computa-tionally challenging task. Latent force models offer a mathematically-grounded balance be-tween data-driven and mechanistic inference in such dynamical systems, whilst accountingfor stochasticity in observations and parameters. However, the required derivation and com-putation of the posterior kernel terms over a low-dimensional latent force is rarely tractable,requiring approximations for complex scenarios such as nonlinear dynamics. In this paper,we overcome this issue by posing the problem as learning the solution operator itself to aclass of latent force models, thereby improving the performance and scalability of these mod-els. This is achieved by employing a deep kernel along with a meta-learned embedding ofthe output functions. Finally, we demonstrate the ability to extrapolate a solution operatortrained on simulations to real experimental datasets, as well as scaling to large datasets.",
  "Introduction": "Differential equations are mathematical models that describe the change of a function with respect to oneor more variables, such as time. They play a central role in the natural and social sciences, providing away to model and understand complex systems and phenomena, such as the growth and decline of pop-ulations (Burghes, 1975), morphogenesis (Turing, 1990), the dynamics of biochemical reactions (Thomaset al., 1976; Schoeberl et al., 2002; Barenco et al., 2006), and so on. They provide a rigorous, well-studied,and mathematically grounded method of making historic and future predictions in complex systems. Ina machine learning context, the modelling power of differential equations make them excellent inductivebiases. A popular method of incorporating these equations within a Bayesian machine learning setting isthe latent force model (LFM), introduced by Lawrence et al. (2006) to model a network of genes regulatedby a common protein. LFMs assume that the underlying dynamics of a system can be modelled in terms ofa low-dimensional latent force, typically with a Gaussian process prior, within a system of differential equa-tions. The involvement of a non-parametric Gaussian process within an interpretable parametric systemresults in a powerful framework for drawing mechanics-constrained inferences in noisy, high-dimensional,and nonlinear dynamics. It has since been extended to embryogenesis (Lpez-Lopera et al., 2019), where thelatent force represents mRNA concentration; patient interventions (Cheng et al., 2020), with latent forcescorresponding to different treatments; and movement segmentation (Alvarez et al., 2010b). However, thereare significant computational challenges hindering the usability of these models on large datasets.",
  "Published in Transactions on Machine Learning Research (10/2024)": "Further workDKLFM explicitly treats the uncertainty in the latent forces and output functions. In anactive learning context, we can query input points where the output function or latent forces have high-uncertainty.This enables experiment design, for example to determine an appropriate coarseness for atime-course experiment. Furthermore, we have currently considered one latent force per task. Multipleforces lead to identifiability issues, where many combinations of latent forces would solve the same LFM.",
  "f GP(m(x), (x, x))(1)": "is described by its mean function m(x) and its kernel function (x, x). The mean function is usually set to0 for standardised data. The kernel function may have a set of hyper-parameters , such as the lengthscalel in an RBF kernel, kRBF(x, x) = exp( l 2x x2). Under this prior, any finite collection of points f(X)for inputs X = [x1, x2, . . . , xN] is normally distributed: f N(m(X), (X, X)). In this paper, we denotethe covariance Kff = (X, X). With a Gaussian likelihood for observations y, meaning y N(f, 2), theposterior distribution for training data X, y is analytically tractable and given by",
  "p(y) = N(y | 0, (X, X) + 2I).(2)": "Deep Kernel LearningDeep kernel learning as presented by Wilson et al. (2016) constitutes an attemptto combine the representation learning capabilities of deep neural networks with the non-parametric natureof Gaussian processes. A neural network is used to map an input x into a latent space yielding a vectorNN(x) RD. This representation is then fed into a base kernel (, ) (such as an RBF kernel) to yield thecovariance between inputs (NN(x), NN(x)). Latent Force ModelsLFMs incorporate explicit dynamics of differential equations in the kernel functionsof Gaussian processes (GPs) in order to infer latent forcing terms (Lawrence et al., 2006; Alvarez et al.,2009). The latent force captures the underlying process and structure in the data, while being unobserved",
  "dx= gx, h, G(f(x)),(3)": "or equivalently for a PDE. The force can be transformed by some response function G(). A GP prior isassigned to the latent force, f GP(0, (x, x)), which naturally accounts for biological noise and enablesnon-linear expressivity through kernels. Some LFM literature considers multiple forces but we do not coverthis due to the identifiability issues they pose. An analytical expression for the covariance between outputs,Khh, is possible under the necessary condition that G is a linear operator. In these cases, maximum marginallikelihood yields the differential equation parameters and inference can be carried out with standard posteriorGP identities (see Rasmussen & Williams (2005)). Approximations are required where G is non-linear.",
  "Model Formulation": "The model setup is summarised in Figures 1 and 2, illustrating the generative process.Our input x istransformed by the latent force f, which is further manipulated by a set of differential equations to yield theoutput functions h. We aim to solve the inverse problem of inferring f given noisy observations of h. Westart by illustrating our approach with a model used later in this paper (Barenco et al., 2006). The timederivative of the mRNA, h(x), for P genes is related to its regulating transcription factor protein, f(x) by",
  "decay term c h(x) ,(4)": "where x R1 is time and b, s, c RP+ are the base transcription rates, sensitivities to the transcription factor,and decay rates of the P genes respectively. G is a function, for example a nonlinearity enforcing positivityor a saturation term enforcing limits on the latent force. The ODE parameters are thus = {b, s, c}.",
  ": Graphical model illustrating the generative process from latent forces to output functions": "At training time, we assume a simulated dataset of N tasks, each consisting of a different latent forcefunction and setting of parameter values. The n-th task is the set {Xn, Yn, fn}, where Xn RT D denotesT observed D-dimensional input points, e.g. temporal (D = 1) and spatio-temporal (D > 1). We havenoisy observations, Yn RT P , of the P-dimensional solutions to the differential equation at each of theseT input points. The solutions are assumed to follow the GP h with realisations hn where the P dimensionsare flattened. The observations Yn are similarly condensed into the blocked vector yn. Since we carry outtraining on a simulated dataset generated by the LFM, we also have access to latent force observations,fn RT . For clarity, we henceforth omit the subscript task index and it can be assumed that we are dealingwith each task independently. These tasks are split into train and test sets. At inference time, our objectiveis to infer the latent force and the model makes use only of the output function observations.",
  "where mf is the mean function of the GP prior for which we learn a constant output, and f is the kernel": "For each task, the distribution over output realisations h is implicitly determined via their joint distributionwith the latent function outputs, f. In practice we make a modelling assumption that this joint is multivariateGaussian. Inferences are then made using GP conditioning on the output observations for test tasks. Thisresults in the joint distributionfh",
  ",(joint)": "where the mean vectors and covariance matrices are obtained as described below. This assumption of amultivariate Gaussian joint distribution holds only for linear transformations of the latent force (i.e. whereG is a linear function). Nevertheless, we demonstrate in .3 that this approximation yields strongquantile similarities to the true empirical distribution even for nonlinear transformations. Deep kernelsIn an LFM, the kernel function is derived from solving a set of differential equations. Inthe case of a general non-linear equation, the kernel has no closed-form solution. We instead approximatethe kernel with a neural network of sufficient capacity, mapping the inputs to latent representation vectorsof size Ld before feeding them into a base kernel, : RLd RLd R (Wilson et al., 2016). This basekernel provides an additional inductive bias, for example ensuring smoothness in the latent space with anRBF kernel or periodicity with a periodic kernel. To map to this latent space, we construct two separatenetworks NNf : RD+Lr RLd, NNh : RD+Lr RLd for the latent and output functions respectively, whereLr is the size of the task representation, r RP Lr, discussed further in the next section. We also needa common network, NNc : RLd RLd, which maps both representations onto the same latent space. Thiscommon network helps to obtain an informative cross-covariance between latent force and output functions:we need to map both to a common latent space. For simplicity, we select a simple MLP for all networks.We found that incorporating skip connections greatly improved the performance whilst reducing overfitting.To illustrate for task n, the cross-function covariances, Kfh and Khh, are computed as follows. The deepkernel receives a concatenation of the task representation with the inputs, zn = rn xn,",
  "Task Representation": "The generative process relies on both the latent force and a set of task-specific parameters relating to thedynamics equations, such as reaction and decay rates. By tweaking these parameters, it is possible thatlatent forces from different tasks could produce the same or similar output functions. Moreover, tasks on thesame input mesh would result in identical covariance matrices, limiting cross-task utility. The GP thereforeneeds access to a task representation in order to generalise across tasks. We denote this rn = emb(xn, yn),defined by emb : RP T RP Lr, where the embedding size, Lr, is a hyper-parameter and each of the Poutput functions are treated independently. Note that it does not observe any latent force data. Instead,this representation is used by the deep kernel to learn the relationship between the output functions andthe latent force in its latent space. This separates inference of latent forces from the dynamics parameters,whilst enabling the task representation to be computed for tasks where no latent force observations exist. This embedding must contain the dynamics information usually captured by the differential equations andtheir parameters, and should be invariant to input resolution in order to maintain the flexibility of Gaussianprocesses. This also enables super-resolution inference; test cases can be at an arbitrary resolution higherthan the training data, as we demonstrate in .2. To that end, we explored two different encodersin our research: a Fourier neural operator (Li et al., 2020) and a Transformer (Vaswani et al., 2017). The Fourier neural operator is effectively an MLP in the Fourier domain, and was originally developedto solve partial differential equations (PDEs) due to the intricate relationship between differential calculusand Fourier analysis, their mesh-invariance, and fast solving speeds. However, a severe limitation is thatthe Fourier transform requires the input to be a regularly spaced. On the other hand, the Transformer isinvariant to resolution and regularity of observations. We implemented the decoder only consisting of a linearlayer followed by self-attention layers. Sinusoidal positional encoding enables the modelling of an irregularmesh. The weights are trained by maximising the marginal likelihood in Equation 6.",
  "Experiments": "In this section we investigate the performance of DKLFM on two ODE-based LFMs and one PDE-basedLFM. Given that this is the first multi-task model for latent force models, we analyse the performance onreal, experimentally-derived datasets not in the synthetic training distribution. We compare our approachwith two models from the literature on solving latent force problems. Alfi is a variational approximationwith a strong mechanistic prior, meaning the model is constrained by the differential equations. For linearg, Alfi resorts to the exact solution. The DeepLFM involves dynamics-informed random features which arecomposed with each layer. This results in only a mid-strength mechanistic prior, since the deep representationis not fully constrained by dynamics. Our approach has a weak mechanistic prior since it is not encodeddirectly into the model, but rather it is learnt.",
  "Nonlinear Ordinary Differential Equations": "The first ODE model is the similar to the original application of latent force models (Lawrence et al., 2006):the biological process of transcriptional regulation. We validate additionally on experimentally-derived datafrom Barenco et al. (2006), where cancer cells were subject to ionising radiation and the concentration ofmRNA was measured via microarray at different timepoints. The data contains transcript counts for fivetargets of the transcription factor p53 over seven timepoints and three replicates. We also consider the pairedLotka-Volterra equations, which govern predator-prey dynamics and exhibit periodic solutions. Latent Force SetupThe transcriptional regulation task is defined by the ODE in Eq. 4, where theexact solution is only tractable when the response function is the identity. In this case, we set G to thesoftplus function, G(f(x)) = log(1+exp(f(x))), to ensure positive protein abundance. We start by samplingparameters for Equation 4 from an empirical distribution of parameters learnt by running the Alfi (Moss",
  "dx= u(x)v(x) v(x),(11)": "where u(x) and v(x) are prey and predator populations respectively as a function of time, with growth rates and , and decay rates and . We use a periodic kernel for in Eq. 5 for this task in order to capture theperiodic nature of the Lotka-Volterra solutions, thus improving temporal extrapolation. We assume that weseek to infer the predator concentration from the abundance of prey; i.e., we take the predator population tobe the latent force. We simulated a dataset of Lotka-Volterra solutions corresponding to different sampledrates, , U(0.5, 1.5), using a 4th-order Runge-Kutta solver. This is a slightly different regime to thetranscriptional regulation model, where a Gaussian process was sampled for each datapoint. In this case, wevalidate our models ability to infer a latent force that was not explicitly generated by a Gaussian process. In both cases, Gaussian-distributed random noise is added to the latent forces. We generate 500 instancesand split into training, validation, and test tasks. demonstrates that DKLFM can infer distributionsover latent forces for the task of transcriptional regulation. We then apply the model trained on the simulateddataset to a real microarray dataset from Barenco et al. (2006), and show our inferred transcription factorconcentration alongside the unobserved ground truth in a. Next, we demonstrate the intra-taskextrapolation in b, where the input has been extended into the past and future. Finally, in ,we compare our results with closest models from the literature, finding lower errors and computation times.",
  "Partial Differential Equations": "PDE-based LFMs are the multivariate extension of ODEs and are significantly harder to solve. This isillustrated by the absence of a method capable of exactly solving all classes of PDEs. Numerical solverstypically operate on a mesh and thus suffer the curse of dimensionality. Here, demonstrating the flexibilityof DKLFM, we fit reaction diffusion equations with a very moderate dataset of 384 low-resolution tasks.The test tasks can then be inferred at a much higher resolution compared to training time.",
  "x2.(12)": "Here, s is the production rate of the driving mRNA, f(x, t) is the latent force, is the decay rate and d isthe diffusion rate. Notice that the latent force is 2-dimensional; DKLFM can take any multivariate input. In order to simulate a dataset from Equation 12, we implemented the Greens function approximation fromLpez-Lopera et al. (2019). This approximation gives the full covariance matrix, including cross-covariancesbetween latent force and outputs, and is faster in this direction than Alfi. Since the joint covariance matrix issingular due to repeated inputs, sampling is implemented using the eigendecomposition rather than Cholesky.We generate 448 tasks in this fashion, taking less than two hours on an AMD Ryzen 5600x, of which 384are used for training. From an empirical inspection of the gap gene dataset from Becker et al. (2013), weuniformly sampled production rates in the range [0.2, 1.0], decay rates in the range [0.01, 0.4], and diffusionrates in the range [0.001, 0.1]. For the latent force, we sampled the two lengthscales (corresponding to spatialand temporal dimensions) in the range [0.1, 0.4] since both dimensions are normalised to . We show that we can learn a general solution operator for PDE tasks in , invariant to input resolution.In , we show how our framework compares against single-instance models. Alfi tends to obtain veryaccurate results due to backpropagating the loss through the solver. However, DKLFM beats Alfi in outputMSE with a greatly reduced computational burden, and using a modest dataset of 384 tasks. DKLFM alsoachieves a competitive error for the latent function.",
  "Performance and Approximation Cost": "The utility of LFMs for large scientific datasets is limited by their lengthy training times. In genomics,a realistic scenario is where a bioinformatician will want to train an LFM on several thousand genes; forexample, RNA velocity (La Manno et al., 2018) solves a splicing kinetics ODE on the entire human genome.This limits the use of the available approximations. Moreover, the Gaussian joint distribution used in thiswork is a model assumption needing verification. We therefore qualitatively demonstrate the uncertainty inour approximated posterior predictive distribution using quantile-quantile plots. Our framework, however, solves many LFMs simultaneously rather than optimising a single instance. Ananalysis of the relationship between error and training set size is therefore key to finding the point our errorrate drops to the level of solving an individual instance. If this point is less than or similar to the numberof instances in a typical use-case, then it is computationally preferable to generate a simulated dataset ofthis size rather than to train individual LFMs. This is because the training dataset required to reach thisperformance becomes smaller than the evaluation set, making it more time-efficient to use the DKLFM. Forthis study, we compare against Alfi, an accurate nonlinear LFM approximation defined in Moss et al. (2021).We chose the ODE task, since the PDE solver in Alfi is too computationally intensive for this comparison.In , we confirm our hypothesis by plotting the MSE versus dataset size for our model, and horizontallines are the mean MSE for Alfi over a subset of 64 tasks. In order to analyse the posterior uncertainty of DKLFM, we use quantile-quantile plots showing samplesfrom our posterior compared with real, simulated samples. We do this for three different tasks: 1. thenonlinear transcriptional regulation ODE model in .1; 2) a logistic growth model with a sinusoidalnonlinearity; and 3) the reaction-diffusion experiment from .2. The logistic growth model measuresthe increase of a resource, for example a population or a plant, as it reaches its maximum value. The rateof increase in size or quantity of the resource at time x R1, h(x), is expressed with the ODE",
  "Test tasks": ": DKLFM trained on a synthetic reaction diffusion dataset. The first column is a training exampleand the next four are test cases, where the latent force is not observed. The embedding size was increased to96 to account for the increase in dimensionality. The model was trained with a 21 21 spatiotemporal grid.At prediction time, 40 40 grid was used to illustrate the super-resolution capability. Each pair of plotsvertically shares the same colorbar to enforce the same scale and accurately demonstrate inference accuracy. multi-modal distribution due to the sinusoidal response to temperature. The QQ-plots shown in demonstrate that our assumption that the output function can be distributed as a multivariate Gaussianleads to robust uncertainty quantification. Even for the edge case of the logistic growth model, the associatedQQ-plot exhibits a close fit to the unimodal Gaussian distribution.",
  "Related Work": "Differential equation-based inference in dynamical systems with Gaussian processes was introduced inLawrence et al. (2006) and Alvarez et al. (2009). These approaches derive the kernel functions by solv-ing the convolution integral of a base kernel with a linear operator corresponding to the ODE solution.When the dynamics becomes nonlinear, the Laplace approximation was used for the marginal likelihood.The benefit of this approach is that it is entirely non-parametric and enforces strong mechanistic behaviour.The primary issue is the requirement of analytically solving the specific ODE, which is not possible for someequations. Moreover, the first and second derivatives of the nonlinearity are also required. Also in line with",
  "TranscriptomicsGrowthReaction Diffusion": ": Quantile-quantile plots demonstrating the impact of our model approximation of a multivariateGaussian join distribution in .1. We plot sorted samples at various timepoints linearly spaced acrossthe input domain for three tasks. The transcriptomics task exhibits some deviation from an exact quantilefit due to the nonlinear transformation of the latent force. The reaction diffusion task involves only lineartransformations of a Gaussian process resulting in a very tight close quantile fit. The logistic growth modelcan result in a multimodal (and therefore non-Gaussian) distribution.",
  "this method is the switching LFM (Alvarez et al., 2010a), which segments time and switches between anumber of latent forces, with only a single force being active in each interval": "Filtering approaches have also been investigated (Hartikainen & Sarkka, 2012; Srkk et al., 2018; Wardet al., 2020). They typically employ a state-space model for approximating the posterior of a non-linearLFM. For example, Ward et al. (2020) use autoregressive flows to construct a joint density of the stateusing variational inference, bypassing complex kernel derivations. This results in a flexible model, scalableto multiple latent forces and output functions. However, the model exhibits the over-confidence prevalent insuch black-box variational approaches. Alfi (Moss et al., 2021) avoids the complex derivations of kernel functions by sampling the latent force fromthe GP prior and gradient matching to pre-estimate reasonable parameters. An ODE or PDE solver is thenused to fine-tune the parameters with the solution of the equations. Backpropagating through a solver is fartoo computationally intensive for a multi-task setting, rendering this approach impractical for large datasets.",
  "Conclusion": "We have introduced a novel framework for latent force models by leveraging the expressive power of deepkernels combined with a learned task representation. Where standard LFMs require an optimisation loopto find kernel parameters, our approach only requires GP conditioning on observations at prediction time,enabling extremely fast latent force inference.Specifically, this involves inverting a T T matrix withO(T 2) computational complexity. If the input size is too large, a technique such as variational inducingpoints reduces the computational complexity to O(TM 2) with M inducing points. DKLFM is therefore anexact inference probabilistic model: the first of its kind for learning the approximate solution operator for anarbitrary nonlinear LFM. We achieve this by learning a deep kernel corresponding to the differential equationby training on a simulated dataset of tasks. The embedding of each tasks observations are interpreted asthe task representation, containing information such as rate parameters. At test time, this representation iscombined with an arbitrary inputpossibly of higher resolution to the training datain order to computethe latent forces using the Gaussian process conditioned on observations. The inference performance of DKLFM is reliant on learning a good cross-covariance between latent forces andobservations. We hypothesise that this is why this model does not exhibit the tendency for over-confidenceaway from training data in predictions commonly found in deep kernel learning and related approaches. Sincewe learn the same deep kernel over a dataset of tasks, this has proved not to be an issue. We demonstratein two ablations the importance of both the common component of the deep kernel, NNc, as well as theFourier embeddings in learning a good representation of the dynamics. LimitationsSpeed is limited by generating the training dataset; however, this is easily parallelised. More-over, as in our PDE experiment, the generative direction, i.e. going from a parameterisation to an instanceis much faster than the inverse problem of inferring the latent force from the output functions. We envisagethese models being used analogously to large language models (Shanahan, 2022), where a user can fine-tunea pretrained DKLFM to make latent force inferences on their dataset. This would be particularly useful forcomputationally intensive simulations. The original LFM is strongly mechanistic, deriving the covariance function from strict dynamics equations.While this may be more robust in the presence of lots of noise, it is overly rigid for real-world tasks. Ourapproach is weakly mechanistic: dynamics are not imposed, but rather parametrically learnt from the dataand paired with a nonparametric GP to condition on unseen data. As with AlphaFold (Jumper et al., 2021),combining biophysical priors and data-driven approaches may be more appropriate for complex problems. Despite not being an issue for the examples covered in this paper, our model approximation can lead toincorrect confidence predictions. Due to modelling the joint as a multivariate Gaussian, and therefore h asa GP, the DKLFM theoretically cannot model transformations of the latent force that lead to non-Gaussiandistributions over the output, h, such as those with multiple modes. However, .3 shows we stillobtain close quantile fits even for a multimodal distribution.",
  "Ann C Babtie, Paul Kirk, and Michael PH Stumpf. Topological sensitivity analysis for systems biology.Proceedings of the National Academy of Sciences, 111(52):1850718512, 2014": "Martino Barenco, Daniela Tomescu, Daniel Brewer, Robin Callard, Jaroslav Stark, and Michael Hubank.Ranked prediction of p53 targets using hidden variable dynamic modeling. Genome biology, 7:118, 2006. Kolja Becker, Eva Balsa-Canto, Damjan Cicin-Sain, Astrid Hoermann, Hilde Janssens, Julio R Banga,and Johannes Jaeger.Reverse-engineering post-transcriptional regulation of gap genes in drosophilamelanogaster. PLoS computational biology, 9(10):e1003281, 2013.",
  "Jouni Hartikainen and Simo Sarkka.Sequential inference for latent force models.arXiv preprintarXiv:1202.3730, 2012": "John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, KathrynTunyasuvunakool, Russ Bates, Augustin dek, Anna Potapenko, et al. Highly accurate protein structureprediction with alphafold. Nature, 596(7873):583589, 2021. Gioele La Manno, Ruslan Soldatov, Amit Zeisel, Emelie Braun, Hannah Hochgerner, Viktor Petukhov, KatjaLidschreiber, Maria E Kastriti, Peter Lnnerberg, Alessandro Furlan, et al. Rna velocity of single cells.Nature, 560(7719):494498, 2018.",
  "Neil Lawrence, Guido Sanguinetti, and Magnus Rattray. Modelling transcriptional regulation using gaussianprocesses. Advances in Neural Information Processing Systems, 19, 2006": "Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew Stuart,and Anima Anandkumar. Fourier neural operator for parametric partial differential equations. arXivpreprint arXiv:2010.08895, 2020. Andrs F Lpez-Lopera, Nicolas Durrande, and Mauricio A Alvarez. Physically-inspired gaussian processmodels for post-transcriptional regulation in drosophila. IEEE/ACM transactions on computational biologyand bioinformatics, 18(2):656666, 2019.",
  "Tapesh Santra. Fitting mathematical models of biochemical pathways to steady state perturbation responsedata without simulating perturbation experiments. Scientific Reports, 8(1):11679, 2018": "Simo Srkk, Mauricio A Alvarez, and Neil D Lawrence. Gaussian process latent force models for learningand stochastic control of physical systems. IEEE Transactions on Automatic Control, 64(7):29532960,2018. Birgit Schoeberl, Claudia Eichler-Jonsson, Ernst Dieter Gilles, and Gertraud Mller. Computational model-ing of the dynamics of the map kinase cascade activated by surface and internalized egf receptors. Naturebiotechnology, 20(4):370375, 2002.",
  "Alan Mathison Turing. The chemical basis of morphogenesis. Bulletin of mathematical biology, 52:153197,1990": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ukasz Kaiser,and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30,2017. Wil Ward, Tom Ryder, Dennis Prangle, and Mauricio Alvarez. Black-box inference for non-linear latentforce models. In International Conference on Artificial Intelligence and Statistics, pp. 30883098. PMLR,2020. Andrew Gordon Wilson, Zhiting Hu, Ruslan Salakhutdinov, and Eric P. Xing. Deep kernel learning. InProceedings of the 19th International Conference on Artificial Intelligence and Statistics, volume 51, 2016."
}