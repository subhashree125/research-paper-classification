{
  "Abstract": "We propose a new framework for contextual multi-armed bandits based on tree ensem-bles. Our framework adapts two widely used bandit methods, Upper Confidence Bound andThompson Sampling, for both standard and combinatorial settings. As part of this frame-work, we propose a novel method of estimating the uncertainty in tree ensemble predictions.We further demonstrate the effectiveness of our framework via several experimental studies,employing XGBoost and random forests, two popular tree ensemble methods. Comparedto state-of-the-art methods based on decision trees and neural networks, our methods ex-hibit superior performance in terms of both regret minimization and computational runtime,when applied to benchmark datasets and the real-world application of navigation over roadnetworks.",
  "Introduction": "Stochastic multi-armed bandits (MABs) (see Slivkins, 2019) provide a principled framework for makingoptimal sequences of decisions under uncertainty. As discussed by Zhu & Van Roy (2023), MABs constitute avital component of modern recommendation systems for efficient exploration, among many other applications.An important variant known as the contextual multi-armed bandit incorporates additional contextual / sideinformation into the decision-making process, allowing for more personalized or adaptive action selection.Following the recent success of (deep) neural networks in solving various machine learning tasks, severalmethods building on such models have been suggested for finding functional relationships between the contextand outcomes of actions to aid the decision-making process (Zhou et al., 2020; Zhang et al., 2021; Zhu &Van Roy, 2023; Osband et al., 2021; Hoseini et al., 2022). Even though sophisticated, these methods can beimpractical, time-consuming, and computationally expensive. In light of these challenges, we propose bandit algorithms that utilize tree ensemble (TE) methods (Hastieet al., 2009), like gradient-boosted decision trees (GBDT) and random forests, to comprehend the contextualfeatures, combined with the most popular bandit methods Upper Confidence Bound (UCB) and ThompsonSampling (TS) as exploration strategies. Compared with other tree-based and neural network methods, ourmethods, called TEUCB and TETS, yield superior results on UCI benchmark datasets. Additionally, ourmethods benefit from more effective learning with less computational overhead on most problem instances,compared to existing methods that use similarly expressive machine learning models.We furthermore",
  "Published in Transactions on Machine Learning Research (10/2024)": "Yury Gorishniy, Ivan Rubachev, Valentin Khrulkov, and Artem Babenko. Revisiting deep learning models fortabular data. In Advances in Neural Information Processing Systems, pp. 1893218943. Curran Associates,Inc., 2021. Leo Grinsztajn, Edouard Oyallon, and Gael Varoquaux. Why do tree-based models still outperform deeplearning on typical tabular data?In Advances in Neural Information Processing Systems, pp. 507520.Curran Associates, Inc., 2022.",
  "Related Work": "The concept of applying decision trees to contextual bandit problems has been studied to a limited extent.Elmachtoub et al. (2017) suggest using a single decision tree for each arm available to the agent. Theirmethod is referred to as TreeBootstrap, as the concept of bootstrapping is employed in order to resemble theThompson Sampling process. One issue with their method is its limited ability to achieve the accuracy androbustness of tree ensemble methods, particularly when estimating complex reward functions. The possibilityof applying random forests in their framework is discussed, but no concrete methods or experimental resultsare presented. Furthermore, storing and training one tree model per arm does not scale well, especiallynot with large action spaces (e.g., in the combinatorial setting), and could potentially lead to excessiveexploration for dynamic arm sets, as it cannot attend to what it has learned from the contexts of otherarms. In contrast, Fraud et al. (2016) employ a tree ensemble but only use decision trees of unit depth, alsoknown as decision stumps. While tree ensembles with a large number of decision stumps tend to performwell regarding accuracy, lower variance, and increased robustness, restricting the depth in such an extremeway may not be adequate when addressing complex tasks (Hastie et al., 2009). The experimental studies byHastie et al. (2009) indicate that tree depths of 4 to 8 work well for boosting. Additionally, Fraud et al.(2016) only investigate the bandit algorithm known as successive elimination, which usually leads to lessefficient exploration compared to UCB and TS, and thus is used less commonly. Moreover, the method,called Bandit Forest, requires binary encoded contextual features. Comparing these two tree-based banditmethods, Elmachtoub et al. (2017) experimentally show that TreeBootstrap tends to outperform BanditForest in practice. It should be noted that neither Elmachtoub et al. (2017) nor Fraud et al. (2016) considercombinatorial contextual bandits. Apart from the two methods mentioned above, the problem has primarily been addressed using other ma-chine learning models. When the reward function is linear w.r.t. the context, LinUCB (Li et al., 2010;Abbasi-Yadkori et al., 2011) and Linear Thompson Sampling (LinTS) (Agrawal & Goyal, 2013) have demon-strated good performance. In more complicated cases, the linear assumption no longer holds, and thus moreexpressive models are needed. In recent years, neural contextual bandits have attracted a lot of attention. As the name suggests, thesemethods use (deep) neural networks to model the expected rewards for some observed contextual features.Out of several proposed neural contextual bandit algorithms, NeuralUCB (Zhou et al., 2020) and NeuralTS(Zhang et al., 2021), in particular, have been shown to perform well, while also providing theoretical regretbounds. On the negative side, due to how these methods estimate the uncertainty in predictions, they relyon inverting a matrix of size equal to the number of network parameters, which is usually computationallyexpensive and time-consuming, especially when the prediction tasks require large neural networks. Zhu & Van Roy (2023) provide a thorough review of different neural bandit methods and suggest a methodbased on epistemic neural networks (Osband et al., 2021) for more efficient uncertainty modeling, showing itsadvantages in terms of regret minimization and computational efficiency. Despite their promising results, weargue that neural networks may not necessarily be the most appropriate models for contextual bandits. Forinstance, even with epistemic neural networks, training is still resource-intensive. On the other hand, thereis a large body of work that shows the effectiveness of tree ensemble methods for regression and classificationtasks in the standard supervised learning setting (Borisov et al., 2022; Gorishniy et al., 2021; Grinsztajnet al., 2022), and therefore we believe that their extension to contextual bandits can have great potential. One less directly related method is Ensemble Sampling (Lu & Van Roy, 2017), where the ensembles aresolely used for uncertainty estimation and not for modeling the reward functions in bandits. Uncertaintyestimates in Ensemble Sampling and similar methods focus on uncertainties in the parameters of the modelsused to approximate the underlying processes being sampled. In contrast, we sample directly from estimated",
  "Multi-Armed Bandit Problem": "The multi-armed bandit (MAB) problem is a sequential decision-making problem under uncertainty, wherethe decision maker, i.e., the agent, interactively learns from the environment over a horizon of T time steps(interactions).At each time step t T, the agent is presented with a set of K actions A, commonlyreferred to as arms. Each action a A has an (initially) unknown reward distribution with expected valuea. The objective is to maximize the cumulative reward over the time horizon T, or more commonly (andequivalently) to minimize the cumulative regret, which is defined as",
  "t=1(a at),(1)": "where a is the expected reward of the optimal arm a, and at is the arm selected at time step t. It isimportant to note that the agent only receives feedback from the selected arm/action. More specifically,when selecting at the agent receives a reward rt,at which is sampled from the underlying reward distribution.As previously mentioned, the agents objective is to minimize the cumulative regret, not to learn the completereward function for each arm. Thus, the agent has to balance delicately between exploration and exploitation.",
  "Contextual Bandit Problem": "The contextual multi-armed bandit problem is an extension of the classical MAB problem described in.1. The agent is, at each time step t, presented with a context vector xt,a Rd for each actiona A. For example, a recommendation system could encode a combination of user-related and action-specific features into the context vector xt,a. Then, the expected reward of action a at time t is given byan (unknown) function of the context q : Rd R, such that E[rt,a] = q(xt,a). Learning to generalize therelationship between the contextual features and the expected reward is crucial for effective learning andminimizing cumulative regret.",
  "Combinatorial Bandits": "Combinatorial multi-armed bandits (CMAB) (Cesa-Bianchi & Lugosi, 2012) deal with problems where ateach time step t, a subset St (called a super arm) of the set of all base arms A is selected, instead of anindividual arm. The reward of a super arm S depends on its constituent base arms. When the reward for asuper arm is given as a function (e.g., sum) of feedback from its individual base arms (observable if and onlyif the super arm is selected), it is referred to as semi-bandit feedback. As previously, the evaluation metric isthe cumulative regret, and for the combinatorial semi-bandit setting (with the sum of base arm feedback assuper arm reward) it can be defined as",
  "Tree-Based Weak Learners": "The underlying concept of decision tree ensembles is to combine several weak learners. Each standalone treeis sufficiently expressive to learn simple relationships and patterns in the data, yet simple enough to preventoverfitting to noise. By combining the relatively poor predictions of a large number of weak learners, theerrors they all make individually tend to cancel out, while the true underlying signal is enhanced. In our notation, a tree ensemble regressor f is a collection of N decision trees {fn}, where the trees arecollectively fitted to a set of training samples {(x, r)}. Each sample consists of a context vector x and atarget value r. The fitting procedure can differ between different types of tree ensemble methods, which areoften divided into two categories based on the approach used, i.e., bagging or boosting. In both variants, theprediction of each tree is determined by assigning the training samples to distinct leaves, where all samplesin the same leaf resemble each other in some way, based on the features attended to in the splitting criteriaassociated with the ancestor nodes of the leaf. Hence, when fitted to the data, each tree fn receives an inputvector x which it assigns to one of its leaves. Every leaf is associated with three values that depend on the samples from the training data assigned tothat particular leaf. We denote the output of the nth tree by on, which is this trees contribution to the totalensemble prediction and depends on which leaf the input vector x is assigned to. The number of trainingsamples assigned to the same leaf as x in tree n is denoted by cn. Finally, the sample variance in the outputproposed by the individual training samples assigned to the leaf is denoted s2n. This value represents theuncertainty in the output on associated with the leaf and depends not only on the training samples butalso on the particular tree ensemble method used. We elaborate on the calculations of s2n for two types oftree ensembles in Sections 3.6 and 3.7. Note that, for the sample variance to exist, we must have at leasttwo samples. This is ensured by requiring the tree to be built such that no leaf has fewer than two trainingsamples assigned to it.",
  "i=1oi(x).(3)": "The outputs on, in turn, are averages of the cn outputs suggested by each training sample assigned tothe same leaf. These suggested outputs are based on the target values of the training samples, with thecalculation method depending on the specific type of tree ensemble used. We always consider the full treeensemble target value predictions as a sum of the individual tree predictions on to unify the notation of allkinds of tree ensembles. Hence, for averaging models, such as random forests, we assume that the leaf valueson are already weighted by 1/N.",
  "Uncertainty Modeling": "For bandit problems, keeping track of the uncertainty in the outcomes of different actions is crucial forguiding the decision-making process. In order to form estimates of the uncertainty in the final prediction ofthe tree ensembles we employ, we make a few assumptions. Firstly, we assume that the output on of the nth decision tree, given a context x, is an arithmetic averageof cn independent and identically distributed random variables with finite mean n and variance 2n. Bythis assumption, on is itself a random variable with mean n and variance 2n cn . Also, we can approximate nand 2n by the sample mean and variance respectively, based on the training samples assigned to the leaf.Moreover, the central limit theorem (CLT) (see e.g., Dodge, 2008) ensures that, as cn , the distribution ofon tends to a Gaussian distribution. See Appendix A.2 for more details about the independence assumption. Secondly, if we also assume the output of each one of the N trees in the ensemble to be independent ofevery other tree, we have that the total tree ensembles target value prediction, which is a sum of N randomvariables, is normally distributed with mean = Ni=1 i and variance 2 = Ni=1 2i . We acknowledge that these assumptions may not always be true in practice, but they act here as motivationfor the design of our proposed algorithms. Specifically, they entail a straightforward way of accumulatingthe variances of the individual tree contributions to obtain an uncertainty estimate in the total rewardprediction, allowing us to construct efficient exploration strategies. In the following two subsections, wepresent our approach to doing so with UCB and TS methods, respectively.",
  "Tree Ensemble Upper Confidence Bound": "UCB methods act under the principle of optimism in the face of uncertainty, and have been established assome of the most prominent approaches to handling exploration in bandit problems. A classic example is theUCB1 algorithm (Auer et al., 2002) which builds confidence bounds around the expected reward from eacharm depending on the fraction of times it has been played, and for which there are proven upper bounds onthe expected regret. One disadvantage of the method, however, is that it does not take the variance of theobserved rewards into account, which may lead to a sub-optimal exploration strategy in practice. In lightof this, Auer et al. (2002) further proposed the UCB1-Tuned and UCB1-Normal algorithms, which extendUCB1 by including variance estimates, and demonstrate better performance experimentally.The maindifference between the two extended versions is that UCB1-Tuned assumes Bernoulli-distributed rewards,while UCB1-Normal is constructed for Gaussian rewards. At each time step t, UCB1-Normal selects thearm a with the maximal upper confidence bound Ut,a, calculated as",
  "mt,a,(4)": "where mt,a is the number of times arm a has been played so far, and t,a and 2t,a are the sample mean andvariance of the corresponding observed rewards, respectively. For the sample variance to be defined for allarms, they must have been played at least twice first. By the assumptions we have made on the tree ensembles, the predictions of the expected rewards willbe approximately Gaussian. Therefore, we propose an algorithm called Tree Ensemble Upper ConfidenceBound (TEUCB) in Algorithm 1, which draws inspiration from UCB1-Normal, and is suitable for bothGaussian and Bernoulli bandits. As seen in lines 27 and 31 of Algorithm 1, the selection rule of TEUCBclosely resembles that of UCB1-Normal, but is constructed specifically for contextual MABs where the armsavailable in each time step are characterized by their context vectors. Therefore, TEUCB considers eachindividual contribution from all samples in a leaf as a sample of that leafs output distribution, which isdemonstrated in line 14. The total prediction for a context xt,a is subsequently computed as the sum ofsampled contributions from each leaf that xt,a is assigned to in the ensemble (lines 16, 17, 18). Beyond yielding better performance in experiments, there are additional benefits associated with consideringthe sample variances of rewards in the TEUCB method, as discussed regarding UCB1-Normal. Since eachtree in the ensemble may be given a different weight, certain trees can contribute more than others to the",
  "Tree Ensemble Thompson Sampling": "The way in which uncertainties are estimated by TEUCB can also be incorporated into Thompson Sampling(Thompson, 1933). In its traditional form, Thompson Sampling selects arms by sampling them from theposterior distribution describing each arms probability of being optimal, given some (known or assumed)prior distribution and previously observed rewards. This can be achieved by sampling mean rewards fromeach arms posterior distribution over expected rewards, and playing the arm with the largest sampled meanreward. Using a TS-based approach, we propose to estimate the uncertainty in the predicted reward given an armscontext in the same way as in the case with UCB. However, in line 23 of Algorithm 1, instead of constructingconfidence bounds, we sample mean rewards from the resulting distributions (here, interpreted as posteriordistributions). Hence, the main difference from TEUCB is how the uncertainty is used to guide exploration.Due to its similarity with standard Thompson Sampling, we call this algorithm Tree Ensemble ThompsonSampling (TETS). Regular Thomson Sampling is inherently a Bayesian approach. However, the framework may be extended toinclude the frequentist perspective as well, and the two views have been unified in a larger set of algorithmscalled Generalized Thompson Sampling (Li, 2013). It should be noted that TETS, as we present it here, isprior-free and should fall under the umbrella of Generalized TS. Although not the focus of this work, thealgorithm could be modified to explicitly incorporate and utilize prior beliefs, making it Bayesian in thetraditional sense.",
  "Extension to Combinatorial Bandits": "The framework outlined in Algorithm 1 is formulated to address the standard contextual MAB problem.However, TEUCB and TETS can easily be extended to the combinatorial semi-bandit setting. The maindifference is in the way arms are selected, i.e., super arms instead of individual arms. In Algorithm 1, thiscorresponds to modifying lines 27 to",
  "Adaptation to XGBoost": "Extreme gradient boosting (XGBoost) is a gradient boosting algorithm that utilizes ensembles of decisiontrees and techniques for computational efficiency (Chen & Guestrin, 2016). Gradient boosting creates newdecision trees based on the gradient of the loss function. The ensemble output is based on the cumulativecontribution of the decision trees within the ensemble. When using XGBoost regression models, we can extract on directly from the individual leaves in the con-structed ensemble. The sample variance s2n is not accessible directly, but we can easily calculate it. Asa GBDT method, XGBoost calculates its outputs during the training procedure as the average differencebetween the target value and the value predicted from the collection of preceding trees in the ensemble,multiplied with a learning rate. Hence, the trees do not have predetermined weights as in, e.g., randomforests. Instead, the relative size of the contributions of an assigned leaf to the final prediction depends onthe particular paths that are traversed through the other trees, which may be different for every sample weobserve in a leaf. Therefore, we cannot estimate s2n from the variance in target values directly. However, XGBoost is augmented with many useful features, one of them being staged predictions (XGBoostDevelopers, 2022b). This means that we can propagate the predicted outputs on sub-ensembles up to acertain tree on all data samples and cache these outputs. After recording these we include the next treein the prediction as well, without having to start over. By utilizing this technique, s2n is easily estimatedfrom the previously observed samples assigned to a particular leaf. Furthermore, cn is simply the numberof such samples. This gives us all the pieces of the puzzle needed to apply XGBoost to the TEUCB andTETS algorithms. In order to reduce bias when calculating on, s2n and cn, one may split the previouslyobserved samples into two distinct data sets; one which is used for building the trees, and a second for thevalue estimations.",
  "Adaptation to Random Forest": "Random forest (Ho, 1995) is a supervised machine learning algorithm that utilizes decision trees as baselearners together with bootstrapped aggregating (bagging).Bagging models randomly sample from thetraining data with replacementreducing the variance of the model by providing the base learners withdiverse subsets from the complete dataset. In addition to bagging, a random forest randomly samples asubset of the features (Hastie et al., 2009). For classification tasks, majority voting is commonly used tomake predictions, while in regression, the average or weighted average of predictions is a common method. Similar to XGBoost, random forests can also be incorporated into TEUCB and TETS. This is even lesscomplicated in the case of random forests as the trees in a random forest do not depend on the predictionsmade by any of the other trees. Therefore, s2n can be directly estimated from the variance in the targetvalues, taking the trees weight of 1/N into account. Hence, all the quantities of interest (i.e., on, s2n, and",
  "cn) are available from the trees of a random forest by looking at which observed samples are assigned to theterminal nodes": "As with XGBoost, one may consider dividing the previously observed samples into different subsets for fittingand value estimation. An alternative way for handling bias with random forests is to consider only the out-of-bag samples for computing on, s2n, and cn. Random forests employ the concept of baggingresamplingwith replacementwhen building trees, which means roughly one-third of the samples will be omitted inthe construction of any particular tree. Thus, they yield a natural way of providing an independent set ofdata samples for model evaluation without limiting the size of the training data set.",
  "Experiments": "In this section, we evaluate TEUCB and TETS and compare them against several well-known algorithmsfor solving the contextual bandit problem. For the implementations, we use the XGBoost library (Chen& Guestrin, 2016) to build gradient-boosted decision trees as our tree ensembles, as well as the version ofrandom forests found in the scikit-learn library (Pedregosa et al., 2011). We first evaluate the algorithmson benchmark datasets. We then study them in the combinatorial contextual setting for solving the real-world problem of navigation over stochastic road networks in Luxembourg. The agents cumulative regret iscalculated by comparing the path to an oracle agent, who knows the expected travel duration for each edgegiven the current time of day. All the algorithms are evaluated w.r.t. cumulative regret averaged over tenrandom seeds and we report the average results. The setup of the experiments is inspired by and adaptedfrom Russo et al. (2018).",
  "Setup": "We use the same procedure as Zhang et al. (2021) to transform a classification task into a contextual MABproblem. In each time step, the environment provides a context vector belonging to an unknown class. Forthe agent, each class corresponds to an arm, and the goal is to select the arm corresponding to the correctclass. This results in a reward of 1, while an incorrect prediction results in a reward of 0. Since only the context of a single (unknown) arm is provided, the agent must have a procedure for encodingit differently for each arm. This is generally done through positional encoding, where in the case of K armsand d data features, we form K context vectors (each of dimensionality Kd) such that: x1 = (x; 0; ; 0),x2 = (0; x; ; 0), , xK = (0; 0; ; x). As no features are shared between any of the arms, this setup isan instance of a disjoint model, as opposed to a hybrid model (Li et al., 2010). In the experiments in .2, we use the disjoint model for LinUCB, LinTS, NeuralUCB, and NeuralTS.However, due to the nature of discrete splitting by decision trees, we can encode the context more effectivelyusing a hybrid model for tree-based methods. There, we simply append the corresponding labels as a singlecharacter for each arm respectively at the beginning of the context: xk = (k, x), and mix numeric andcategorical features in the context vectors. For LinUCB, LinTS, NeuralUCB, and NeuralTS, categoricalfeatures are one-hot-encoded. In these experiments, we set the time horizon to 10,000 (except for the Mushroom dataset, where a horizonof 8,124 is sufficient to observe it entirely). For each of those time steps, the agents are presented with afeature vector x drawn randomly without replacement, which they encode for the different arms as describedabove. Subsequently, the agents predict the rewards for the individual arms according to the algorithmsused. TEUCB and TETS (Algorithm 1) are described in . The TreeBootstrap (Elmachtoub et al.,2017), NeuralUCB (Zhou et al., 2020), NeuralTS (Zhang et al., 2021), LinUCB (Li et al., 2010) and LinTS(Agrawal & Goyal, 2013) algorithms are implemented according to their respective references.",
  "Implementation": "We model the road network of Luxembourg via a graph G(V, E), with |V| = 2, 247 vertices and |E| = 5, 651edges. The vertices represent intersections in the road network, and the edges represent individual roadsegments connecting the intersections. In this scenario, edges correspond to base arms, and paths (i.e.,ordered sequences of edges) correspond to super arms. Each vertex has a coordinate consisting of longitude,latitude, and altitude values. For all edges e E, the contextual vector xe describes each road segment inthe network. The agent is presented with a vector containing contextual data according to . Edge traversal times have been collected using the Luxembourg SUMO Traffic (LuST) simulation scenario(Codeca et al., 2015). The recorded edge traversal times are used to form kernel density estimators (KDE)(Weglarczyk, 2018) for each edge. If an edge does not contain any recorded traversals, the expected traversal",
  "CPU0.080.090.040.08": "Initial test runs revealed that the agents are robust w.r.t. different choices of hyper-parameters for XGBoost(XGBoost Developers, 2022a), and consequently, we settle on the default values. The only exception is themaximum tree depth, which we set to 10. The same maximum tree depth is used for the implementation ofrandom forests. The number of initial random arm selections TI is set to 10 times the number of arms for alltree-based algorithms, and the exploration factor of TEUCB and TETS is set to = 1. As for the decisiontree (DT) algorithm used for TreeBootstrap, we use the scikit-learn library to employ CART (Breiman et al.,2017), which is free of tunable parameters. This is similar to the implementation presented in the originalwork on TreeBootstrap (Elmachtoub et al., 2017). Another point we noted during our initial test runs was that accurate predictions seem to be more importantthan less biased estimates obtained by splitting the previously observed samples into distinct data sets forensemble fitting and value calculations respectively. Therefore, we use all observed context-reward pairs forboth purposes in all experiments with TEUCB and TETS. In order to avoid unnecessary computations and speed up the runs, TEUCB and TETS do not build newtree ensembles from scratch at each time step. Instead, they only consider which leaves the latest observationassigned in each tree and update the corresponding on, s2n, and cn parameters. Initially, when newly observedsamples may have a relatively large effect on the optimal tree ensemble, re-building happens more frequently.However, as the effect that the samples are expected to have on the tree architectures degrades over time,less re-building takes place. More precisely, re-building happens when the function 8 ln(t) increases by onecompared to the previous time step. The experimental results are obtained using an NVIDIA A40 GPU for NeuralUCB and NeuralTS, and adesktop CPU for the other agents. To illustrate the differences in computational performance between theagents, we report the runtime of the neural agents on the same CPU as well.",
  "random forest, and yield significantly lower cumulative regrets. Furthermore, TEUCB and TETS tend toperform better than TreeBootstrap on the adult, magic, and mushroom data sets": "On the shuttle data set, with seven different arms, the TreeBootstrap agents exhibit comparable and evenslightly better average performance in terms of regret minimization. It appears that TreeBootstraps assign-ing of a separate tree-based model for each arm is beneficial for this problem. However, this comes at theexpense of having to fit multiple models in each time step, which is time-consuming and computationally de-manding. Furthermore, its inability to generalize the reward predictions of distinct but related arms may insome cases be disadvantageous, especially for larger arm sets. Comparing TEUCB vs. TETS, and XGBoostvs. random forest (i.e., the different design choices within our framework), there is no clear winner as they alltend to perform comparably (and very effectively) on different data sets. In addition to regret minimization,the CPU experiments indicate that TEUCB and TETS are significantly more efficient than their neuralcounterparts from a computational perspective. Notably, LinUCB, LinTS, and TreeBootstrap-DT are themost efficient methods in terms of computation. However, they do not minimize regret as effectively as mostother agents in our data sets.",
  "Combinatorial Contextual Bandits": "In this section, we investigate the combinatorial contextual bandit methods on a real-world application, wherewe study two scenarios corresponding to performing the most efficient navigation over the real-world roadnetwork of Luxembourg. This problem is crucial with the emergence of electric vehicles to mitigate the so-called range anxiety. Similar navigation problems have recently been studied from a CMAB perspective, butoften without contextual information (kerblom et al., 2023) or limited to neural bandit methods (Hoseiniet al., 2022). CMAB methods are well-suited to the navigation problem since the traversal time of each roadsegment can be highly stochastic and dependent on local factors (e.g., road works, traffic congestion, stoplights) about which knowledge may be gathered through sequential interactions with the environment.",
  "(z z)2Euclidean distance along z-axis.speed_limitMaximum speed limitstopA boolean if edge includes a stoptimeThe current time of day": "time is set to the length of the edge divided by the speed limit.At each time step t, the time of dayis randomly sampled, and used for updating the expected travel times of all edges and the correspondingKDEs. We generate edge-specific feedback by individually sampling travel times from the KDE of each edgeon the chosen path. As the graph contains more than 5,000 edges, the agents need to learn how the contextual features impact theexpected travel time. The road types in the graph are highways, arterial roads, and residential streets, witha total length of 955 km. In our experiments, we specifically study two problem instances (characterized bydifferent start and end nodes), referred to as problem instance 1 and problem instance 2. The paths selectedby the different agents during a single run are visualized for problem instance 1 in (in the Appendix). An agent predicts the expected travel time for each of the edges in the graph, and then solves the shortestpath problem using Dijkstras algorithm (Dijkstra, 1959). The agents are evaluated based on the sum of theexpected travel times for all edges forming the traversed path compared to the expected travel time of theoptimal path S. For this experiment, the neural agents utilize a neural network with two hidden layers, both containing 100fully connected neurons. A dropout probability of 0.2 is used and after a parameter search over the samesets of values as in .2.1, we set = 0.1 and = 0.001. Furthermore, the network is trained over 10epochs with the option of early stopping if the MSE loss does not tend to keep improving. We implementthe tree ensemble bandits utilizing XGBoost and random forest regressors with maximum tree depths of 10;all other hyper-parameters being set to their default values. The number of trees is set to N = 100 for allagents with tree ensembles, and the initial random selection is set to TI = 10 paths. An exploration factorof = 1 is used for TEUCB and TETS and the frequency of re-building their tree ensembles is the same asdescribed in .2.1.",
  "Experimental Results": "shows the results of the TEUCB and TETS methods along with the baselines. The road network andthe agents traversals are presented in (in the Appendix). The frequency by which an edge has beentraversed is indicated by the red saturation, where more traversals correspond to a higher saturation. It isworth noting that LinUCB, LinTS, and the TreeBootstrap require excessive exploration as their models areedge-specific. In contrast, TETS, TEUCB, NeuralTS, and NeuralUCB train one model with the possibility ofgeneralizing the expected travel time predictions over different edges. We observe that using both XGBoostand random forest, TEUCB and TETS significantly outperform the other methods. In this setting, if anagent can generalize well what it has learned from one arms reward distribution to the other arms, it canthen effectively avoid over-exploration, as indicated by both the regret plots in and the agents specificpath selections in . Comparing the two different tree ensemble methods, XGBoost seems to performbetter on the navigation task compared to random forests in the TEUCB and TETS frameworks.",
  "Conclusion": "We developed a novel framework for contextual multi-armed bandits using tree ensembles.Within thisframework, we adapted the two commonly used methods for handling the exploration-exploitation dilemma:UCB and Thompson Sampling. Furthermore, we extended the framework to handle combinatorial contextualbandits, enabling more complex action selection at each time step. To demonstrate the effectiveness of theframework, we conducted experiments on benchmark datasets using the XGBoost and random forest treeensemble methods. Additionally, we employed it for navigation over stochastic real-world road networks,modeled as a combinatorial contextual bandit problem. Across all problem instances, the introduced tree ensemble-based methods, TEUCB and TETS, consistentlydemonstrated effectiveness in minimizing regret while requiring relatively low computational resources. Inmany cases, these methods significantly outperformed neural network-based methods that are often consid-ered state-of-the-art for contextual bandits, in terms of both accuracy and computational cost. Hence, thiswork indicates that using tree ensembles offers several advantages over other machine-learning models forthese problems. Furthermore, compared to other tree-based methods, TEUCB and TETS exhibited similaror better capabilities in minimizing regret while maintaining computational efficiency, even for problems withlarge numbers of arms. These results indicate that using a single tree ensemble to predict the reward dis-tributions of all arms, particularly as implemented in TEUCB and TETS, facilitates effective generalizationbetween arms based on context, enabling more efficient learning with fewer samples. The study primarily focuses on the practical applicability of tree ensembles for bandits in various settings.Therefore, we did not provide a theoretical analysis of the respective regrets. For future work, exploringtheoretical regret bounds for the proposed tree ensemble methods could offer deeper insights and a betterunderstanding of the methods presented in this work. The computations and data handling were enabled by resources provided by the National Academic Infras-tructure for Supercomputing in Sweden (NAISS), partially funded by the Swedish Research Council throughgrant agreement no. 2022-06725. The work of Niklas kerblom was partially funded by the Strategic Ve-hicle Research and Innovation Programme (FFI) of Sweden, through the project EENE (reference number:2018-01937).",
  "Yasin Abbasi-Yadkori, Dvid Pl, and Csaba Szepesvari. Improved algorithms for linear stochastic bandits.In Neural Information Processing Systems, 2011. URL": "Shipra Agrawal and Navin Goyal. Thompson sampling for contextual bandits with linear payoffs. In Pro-ceedings of the 30th International Conference on Machine Learning, Proceedings of Machine LearningResearch, pp. 127135, Atlanta, Georgia, USA, 1719 Jun 2013. PMLR. Niklas kerblom, Yuxin Chen, and Morteza Haghir Chehreghani. Online learning of energy consumption fornavigation of electric vehicles. Artificial Intelligence, 317:103879, 2023. doi: 10.1016/j.artint.2023.103879.",
  "Leo Breiman, Jerome Friedman, Richard Olshen, and Charles Stone. Classification And Regression Trees.10 2017. ISBN 9781315139470. doi: 10.1201/9781315139470": "Nicol Cesa-Bianchi and Gbor Lugosi. Combinatorial bandits. Journal of Computer and System Sciences,78(5):14041422, 2012.ISSN 0022-0000.doi: 10.1016/j.jcss.2012.01.001.JCSS Special Issue: CloudComputing 2011. Olivier Chapelle and Lihong Li. An empirical evaluation of thompson sampling. In J. Shawe-Taylor, R. Zemel,P. Bartlett, F. Pereira, and K.Q. Weinberger (eds.), Advances in Neural Information Processing Systems,volume 24. Curran Associates, Inc., 2011. URL Tianqi Chen and Carlos Guestrin. Xgboost: A scalable tree boosting system. In Proceedings of the 22ndACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD 16. ACM,2016. doi: 10.1145/2939672.2939785. Lara Codeca, Raphael Frank, and Thomas Engel. Luxembourg sumo traffic (lust) scenario: 24 hours ofmobility for vehicular networking research. In 2015 IEEE Vehicular Networking Conference (VNC), pp.18, 2015. doi: 10.1109/VNC.2015.7385539.",
  "Weitong Zhang, Dongruo Zhou, Lihong Li, and Quanquan Gu. Neural thompson sampling. In InternationalConference on Learning Representations, 2021": "Dongruo Zhou, Lihong Li, and Quanquan Gu.Neural contextual bandits with UCB-based exploration.In Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings ofMachine Learning Research, pp. 1149211502. PMLR, 1318 Jul 2020. Zheqing Zhu and Benjamin Van Roy.Scalable neural contextual bandit for recommender systems.InProceedings of the 32nd ACM International Conference on Information and Knowledge Management,CIKM 23, pp. 36363646, New York, NY, USA, 2023. Association for Computing Machinery.ISBN9798400701245. doi: 10.1145/3583780.3615048.",
  "NotationDescription": "aArm/actionaOptimal armatArm selected at time step tAAction space, set of all armscnNumber of training samples assigned to a leaf in regression tree nfTree ensemble regressor consisting of N regression trees fitted in unisonfnnth individual tree of the ensemblemt,aNumber of times arm a has been selected up to time step tN(, 2)Normal distribution with mean and variance 2 onOutput value associated with a leaf in regression tree nqFunction of expected rewardrObserved rewardrEstimated rewardRRegrets2nSample variance of the output value associated with a leaf in regression tree nSSuper arm, a set of armsSOptimal super armStsuper arm selected at time step tttime stepUUpper confidence boundxContext/feature vectorTrue meanEstimated meanExploration factor2True variance2Estimated variance{xi}Ii=1List of I entries",
  "2ici2jcj.(10)": "Here, ij represents the correlation coefficient between the outputs of trees i and j. If the correlations ij arepositive, they increase the total variance of the ensembles prediction. However, if we assume independence(ij = 0), the second term vanishes, simplifying the variance to just the sum of individual variances, i.e.,",
  "2ici.(11)": "In the context of bagging-based ensembles like random forests, where trees are trained on different subsets ofdata and with random feature selection, the correlations ij are typically small, making this approximationreasonable. For boosting-based methods, which sequentially fit trees, the correlations might be stronger,implying that our approximation could be less accurate. However, we argue that this approximation often yields a lower bound for the actual variance, where thislower bound provides useful insights into the ensembles uncertainty, offering a simplified way to quantifyprediction variability.For this, we argue that negative correlations are less common than positive onesin ensemble learning primarily due to the nature of how individual models are trained and how ensemblemethods perform. In tree ensemble methods, individual models (e.g., decision trees) are trained on subsets of the same dataand tend to capture underlying patterns that are common across these subsets. This shared learning processoften leads to similar predictions, resulting in positive correlations. In boosting, in particular, trees are addedsequentially, with each new tree focused on correcting the errors (residuals) of the previous ensemble. As theboosting process continues, new trees are highly dependent on the earlier trees mistakes. Although boostingcan sometimes create predictions that pull in opposite directions (implying a negative correlation locally),the overall direction of learning is typically aligned to reduce residual error. This alignment contributesto positive correlations because all trees are ultimately working together to approximate the same targetfunction. Therefore, we can assume that in boosting, the overall correlation is positive. On the other hand,as mentioned, bagging methods such as random forests use randomness (bootstrapping data samples andfeature selection) to reduce the correlation between individual trees (i.e., the correlations can be discarded). In summary, we expect the independence assumption between the trees (i.e., ij = 0) to hold reasonablywell for bagging methods. On the other hand, this assumption usually leads to an underestimate of the totalvariance for boosting methods. This means that our calculated variance: Ni=12i ci serves as a lower boundon the true variance. Interestingly, some bandit algorithms, such as UCB, are known to suffer from over-exploration (Russo & Roy, 2014), and thus underestimating the variance could even be helpful to mitigatethis issue and lead to more efficient learning.",
  "(l) TreeBootstrap-DT": ": The road network of Luxembourg shows the trajectories of different agents for the experiment onproblem instance 1, where the corresponding cumulative regret is presented in a. The plots show allthe paths selected by the agents during a full run of the experiment, where a higher level of opacity indicatesthat a road segment was more frequently part of a traveled path.",
  "A.5Sensitivity Analysis": "Here, we investigate the robustness of TEUCB and TETS to changes in a number of key (hyper)parameters.To do so, we run the same experiments as presented in .2, but varying one parameter at a time. Inaddition, we study how robust the methods are to delays in reward feedback, and compare the performanceof TEUCB and TETS to TreeBootstrap.",
  "A.5.1Delayed Feedback": "As done by Zhang et al. (2021), in the experiments with delayed feedback presented in , the rewardsassociated with an agents actions are fed in batches of varying batch size, rather than one by one instantlyafter the action is taken. This setting appears to occur naturally in several real-world applications of multi-armed bandits, as discussed by Chapelle & Li (2011). Their findings suggest that Thompson Sampling scalesbetter with delay in rewards than UCB methods, which is also supported by the experiments of Zhang et al.(2021). It is not clear from our experiments whether this is the case for TEUCB, TETS, and TreeBootstrap(which, as discussed in .1, can be viewed as a Thompson Sampling method). Comparing with theresults of Zhang et al. (2021) for NeuralUCB and NeuralTS on the same datasets, however, it appears thatthe tree-based methods we study are more resilient to delays in the rewards.",
  "A.5.2Varied Tree Depth": "In , we study the robustness of TEUCB and TETS towards different depths for the regression trees inthe ensembles. We observe that a significant change in tree depth may influence the performance of TEUCBand TETS when used with both XGBoost and random forests. We note that random forests tend to benefitfrom deeper trees, while XGBoost performs better with shallower ones. This is particularly interesting asthe depth is only a maximum value for XGBoost, and the algorithm is designed to automatically prune treeswhere deemed beneficial for the reduction of overfitting. The extent to which this is utilized can be controlledby changing the XGBoost parameters and , though we use the default values. As a final note on theeffect of tree depth, we observe that a depth of 10, which is used in the comparison study in .2, issuitable.",
  "A.5.3Varied Ensemble Size": "In we study how the performance of TEUCB and TETS varies with different numbers of trees makingup the tree ensembles used to predict rewards. In this study, we see that initially going from a relatively smallensemble of trees to a larger one can greatly improve the performance, which makes sense. However, afterthis initial setup, the results stay stable when increasing the number of trees further. XGBoost appears tobe particularly sensitive to selecting too few trees. This issue may potentially be mitigated by adjusting thelearning rate parameter , though we have not investigated this. Further, we note that setting the numberof trees to 100, as in the comparison with other bandit methods in .2, is shown to be a good choice.",
  "A.5.4Varied Exploration Factor": "Finally, in we investigate the robustness of TEUCB and TETS to different exploration factors usedto control the level of exploration.The results demonstrate that the methods are robust to changes inthe exploration factor within a wide range of values. Only one observation deviates significantly from thepattern, in which an exploration factor of 100 is used for TEUCB with XGBoost. For both datasets, thisvalue results in a significantly larger average cumulative regret compared to all other evaluated methods andparameter values. For all other values and agents, the results are stable and consistent.",
  "(b) Cumulative regret with different levels of reward delays on the adult dataset": ": Comparison of TEUCB, TETS, and TreeBootstrap on the mushroom and the adult datasets withdifferent levels of delays. Cumulative regret for a single experiment is calculated over 10,000 time steps andrepeated 10 times. The results display the average cumulative regret plus/minus the standard deviation foreach respective agent. Hyperparameters are selected as in .2. All agents are evaluated on the samelevels of reward delays, but each agent is shifted slightly horizontally for visualization purposes.",
  "(b) Cumulative regret with varying tree depths on the adult dataset": ": Comparison of TEUCB and TETS on the mushroom and the adult datasets with different treedepths. Cumulative regret for a single experiment is calculated over 10,000 time steps and repeated 10 times.The results display the average cumulative regret plus/minus the standard deviation for each respective agent.Hyperparameters other than tree depth are selected as in .2. All agents are evaluated with thesame tree depths, but each agent is shifted slightly horizontally for visualization purposes.",
  "(b) Cumulative regret with varying ensemble sizes on the adult dataset": ": Comparison of TEUCB and TETS on the mushroom and the adult datasets with different numbersof trees in the ensembles. Cumulative regret for a single experiment is calculated over 10,000 time stepsand repeated 10 times. The results display the average cumulative regret plus/minus the standard deviationfor each respective agent. Hyperparameters other than the numbers of trees are selected as in .2.All agents are evaluated with the same number of trees, but each agent is shifted slightly horizontally forvisualization purposes.",
  "(b) Cumulative regret with varying exploration factors on the mushroom dataset": ": Comparison of TEUCB and TETS on the mushroom and the adult datasets with different explo-ration factors. Cumulative regret for a single experiment is calculated over 10,000 time steps and repeated10 times. The results display the average cumulative regret plus/minus the standard deviation for each re-spective agent. Hyperparameters other than the exploration factor are selected as in .2. All agentsare evaluated on the same exploration factors, but each agent is shifted slightly horizontally for visualizationpurposes."
}