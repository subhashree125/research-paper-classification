{
  "Abstract": "Efficient inference of Deep Neural Networks (DNNs) on resource-constrained edge devicesis essential.Quantization and sparsity are key techniques that translate to repetitionand sparsity respectively within tensors at the hardware-software interface.This paperintroduces the concept of repetition-sparsity trade-off that helps explain computationalefficiency during inference. We propose PLUM, a unified co-design framework that inte-grates DNN inference systems and quantization (forward and backward pass) to leveragerepetition-sparsity trade-off to improve inference efficiency.Our results demonstratethat PLUMs quantization method is more accurate than binary quantization with thesame number of non-zero weights.Detailed analysis indicates that signed binarizationgenerates a smaller distribution of effectual (non-zero) parameters nested within a largerdistribution of total parameters of latent full-precision weights for a DNN block. Finally,the proposed PLUM framework achieves a 26% speedup on real hardware, doubles energyefficiency, and reduces density by 2.8 compared to binary methods while retaining top-1accuracy when compared to prior-art methods for ResNets on ImageNet (by achieving66.2% top-1 accuracy), presenting an alternative solution for deploying efficient models inresource-limited environments. Code available at",
  "Introduction": "Despite significant strides in accuracy, the burgeoning complexity and resource demands of deep learningmodels pose challenges for their widespread adoption across a wide range of domains (He et al., 2016; Brownet al., 2020; Graves et al., 2013; Jain et al., 2022; Mandlekar et al., 2021; Jain et al., 2024). This requires thedevelopment of innovative techniques to enhance DNN efficiency during inference on edge devices. Two suchtechniques have been studied extensively: binarization and sparsity. Binarization, a form of quantization,results in weight repetition as only two values appear repeatedly in the weight tensor (Courbariaux et al.,2015).This approach significantly trims the memory footprint of the weight tensor, thereby decreasingmemory I/O during inference (Hegde et al., 2018). In contrast, sparsity leads to zero weight values. Sinceanything multiplied by zero is zero, weight sparsity leads to ineffectual multiplications (Wu et al., 2021).This approach reduces memory I/O during inference by not reading activations that would be multipliedby zero weights (Gong et al., 2020). Thus, both these techniques are geared towards reducing memory I/Oduring inference. DNN inference efficiency is usually achieved by leveraging either binarization or sparsity. Introducing spar-sity by adding a zero-valued weight in conjunction with binary weights is termed ternary quantization.Ternary was conceived with the reasonable assumption that transitioning from binary to ternary modelswould only minimally impact inference latency due to the effect of zero weights (Li et al., 2016). However,advances in the contemporary hardware-software systems have revealed a substantial increase in latencyduring such transitions (Prabhakar et al., 2021; Fu et al., 2022).This work aims to perform faster inferenceon contemporary hardware-software systems while retaining model accuracy.",
  "Published in Transactions on Machine Learning Research (10/2024)": "worse with moderate sparsity before improving under high sparsity conditions. This can be explained bythe repetition-sparsity trade-off. Signed-binary quantization consistently outperforms ternary due to higherweight repetition with equal sparsity and outperforms binary due to the presence of sparsity. The highlyefficient behavior of signed-binary under both very low and very high sparsity can be explained: withapproaching 0% sparsity, signed binary gets converted into monolithic filters containing one unique weightper filter, and with high sparsity, most operations become ineffectual, resulting in significant savings.",
  "We make the following contributions in our work:": "We present the concept of repetition-sparsity trade-off along with the PLUM framework. Whilethe former explains the inference inefficiency of binary and ternary weight quantization, the latterleverages (and thus proves) this insight to improve the efficiency of DNN inference. Compared to prior-art quantization schemes, we demonstrate that PLUMs signed-binary quantiza-tion scheme retains model accuracy (of 90.7% and 66.2% top-1 accuracy on CIFAR10 and ImageNet)while requiring significantly fewer effectual parameters. We offer detailed insights on co-dependentlearned features in the DNN when using PLUM by visualizing the distribution of quantized andlatent full-precision weights. We perform DNN inference on Intel CPUs to prove the repetition-sparsity trade-off and demonstratethat the PLUM framework leads to the fastest inference. We further demonstrate the benefits ofsparsity during inference when using PLUM via energy reduction experiment.",
  "Background": "QuantizationQuantization in deep learning involves mapping of real-valued numbers to a select set ofdiscrete values to streamline and optimize computations. The Binary Quantization technique assigns anyreal-valued number to either +1 or -1 (Courbariaux et al., 2015). The primary objective behind this isto simplify operations in DL hardware, turning multiply-and-accumulates (MACs) into mere accumulateoperations (Rastegari et al., 2016). Conversely, Ternary Quantization designates values to +1, -1, or 0 (Liet al., 2016). This determination is based on a threshold, . Values exceeding are assigned +1, thosebelow receive -1, while others become zero. For both quantization methods, the output values can undergoscaling using a factor, (Bulat & Tzimiropoulos, 2019; Bulat et al., 2019). There is a rich literature onimproving the accuracy of these methods (Qin et al., 2023; Zhang et al., 2018; Gong et al., 2019; Qin et al.,2020b; Hu et al., 2018; Pouransari et al., 2020; Liu et al., 2018).These methods are not aware of therepetition-sparsity trade-off. In PLUM framework, we combine the benefits of both binary and ternary tocreate efficient inference. Weight SparsityWeight Sparsity in DNN implies that there is repetition of weight with a value equal tozero. The basic idea is that since 0x = 0 for any real-valued scalar x, if the weight is zero, the multiplicationis ineffectual and should be skipped (Gong et al., 2020). Sparsity in weights is static during inference (Daiet al., 2020). Therefore, if the value of weight is zero, we can choose not to load activations correspondingto that weight (Qin et al., 2020a). This can lead to a reduction in data movement, memory accesses, andMACs thereby reducing computations and hence resulting in efficient DNN inference. This approach hasbeen effective on ASICs and general-purpose devices (Hegde et al., 2019; Wang et al., 2021; Dai et al., 2020;Gong et al., 2020; Nayak et al., 2023). PLUM exploits weight sparsity during inference to improve efficiency. Weight RepetitionQuantization of weights leads to the same value being repeated again and again inthe weight tensor. This phenomenon is known as weight repetition (Hegde et al., 2018; Sze et al., 2020).Since the weights are fixed during DNN inference (Dai et al., 2020), this leads to opportunities for improvingefficiency during inference with respect to time and energy by exploiting the repetition of weights andreducing memory accesses (Sze et al., 2020). This concept was first introduced in BNN (Courbariaux et al.,2016). They demonstrated that for a CNN, only 42% of filters are unique per layer on average which canlead to reducing the number of operations by 3x. UCNN (Hegde et al., 2018) demonstrated how to leverage widespread and abundant weight repetition presentacross a range of networks (like ResNet, GoogleNet) that are trained on a variety of datasets. It reordersthe weights and thus reorders activations, leading to reduced memory access and arithmetic operations,resulting in 4x more efficient inference. For example, if the filter weights are [a, b, a, a] and activations are[w, x, y, z], UCNN would reorder it as a (w + y + z) + b (x) for efficient inference (Sze et al., 2020)but does not exploit weight sparsity. SumMerge (Prabhakar et al., 2021) uses both weight repetition andweight sparsity for efficient DNN inference. For example, if b = 0 in the previous example, SumMerge would",
  "Leveraging Repetition-Sparsity Trade-off": "Conventional binary networks offer 2 unique weight choices per element, creating a dense setup. Conversely,ternary networks take an additional bit to represent 3 unique weight choices per element to create a sparsesetup. Continuing the 3 3 convolution filter illustration from the previous section, binary models can have29 unique filters. In contrast, ternary models employ 39 unique filters, inadvertently causing an exponentialdecrease in repetition by a factor of 38.4. These design decisions can be interpreted as follows: Binaryquantization prioritizing the maximization of weight repetition, overlooking the potential benefits of weightsparsity. In contrast, ternary quantization induces weight sparsity but compromises weight repetition. Thisdisparity places the two schemes at opposite ends of a spectrum, creating a repetition-sparsity trade-off(as",
  "Co-design Exploration": "Developing an efficient framework requires a redesign due to its distinct nature from existing methods, asa latent FP weight can be assigned at any of the two quantization functions. This leads to challenges: (1)retaining the ability to learn meaningful representations and (2) providing faster inference when using state-of-the-art inference systems. Our method addresses these concerns by revisiting design decisions on differentlayers of the stack with the goal of improving efficiency while retaining accuracy.For in-depth insightsinto our experimental setup and implementation details, please refer to the supplementary section C. Foradditional ablations, please see supplementary E F and G.",
  "DNN inference in PLUM": "The aim is to achieve faster inference by using two quantization functions for a DNN block. As explainedin (weight repetition paragraph), modern DNN inference systems enhance data locality duringinference by splitting the dot products of inputs and weights into smaller blocks through tiling. For instance,Prabhakar et al. (2021) divides every filter into smaller blocks by introducing a sub-dimension denoted asC, representing the tile size along the C dimension. We leverage this insight and make the design decisionof local binarization. This allows a single processing step during PLUM inference to see one signed binaryquantization function, leading to efficiency as shown in . We define the region to be sign-binarizedas RS Ct where Ct = max(C, kC) and k Z+. For a given , Wquant = U where W RRSCt andU {0, 1}RSCt. Hence, this results in global ternarization for a DNN block. Please refer to section 5.1for ablation on DNN inference for PLUM on Intel CPU.",
  "Quantization in PLUM: Signed Binary": "The challenges arise from the need to design signed-binary quantization functions, that enable the model tolearn meaningful representations. Local binarization techniques must accommodate a global ternarizationstrategy, which presents additional challenges such as determining suitable regions for local binarizationand assigning these regions to their designated signed binary quantization functions. In response to thesechallenges, we have designed our method, supported by systematic ablation studies conducted on ResNetstrained with the CIFAR-10 dataset (see supplementary I for additional ablations). Signed Binary Quant Functions: Our method involves the strategic use of two distinct quantizationfunctions (as shown in ). In this section, we intend to meticulously design these functions, charac-terized by the value sets {0,1} where the sign-factor 1 is equal to 1, and {0,-1} where the sign-factor 1 is-1. The scaling factor i mirrors i where i = 1. Following (Zhu et al., 2016), we define the threshold valueas = 0.05 max(|W|). To assess the efficacy and sensitivity of the chosen thresholds, we experiment withdifferent s using signed-binary quantization (see .2.2) to find stable performance across variousconfigurations, as depicted in . These are defined as:",
  "(4)": "Intra-Filter Signed-Binary QuantIn this section, we delve deeper into signed binary quantization byintroducing variations in Ct with respect to the constant value C, aiming to identify the optimal setting forCt. We design this approach to emphasize the changes in performance across different thresholds. assesses the impact on representation quality by adjusting the Ct values during the training. The resultshows that intra-filter signed binary co-design preserves a competitive level of representation quality evenwith a reduced Ct and setting of Ct = C works best.",
  "Backpropagation in PLUM": "In this section, we delve into tuning latent full-precision weights during training to enhance model accuracy.Traditional binary networks experience fluctuations in latent weight assignments at zero due to the use ofthe one quantization function, i.e., sign functionmanifesting as a peak of latent full-precision weights atzero (Bai et al., 2018; Qin et al., 2020b). This issue is exacerbated in PLUM. Employing two quantizationfunctions such that individual signed binary regions co-dependently learn diverse features, results in twodistinct peaks at non-zero values of as illustrated in b. This dual peak structure necessitatesa process to ensure stable and accurate weight updates. In response, we have adapted traditional binarysEDE (Qin et al., 2020b) that approximates the derivative of the sign function during backward propagationto improve gradient accuracy and update capability. PLUM stabilizes fluctuations in latent full-precisionweights around = 0.05 max(W) with EDE, thereby fostering improved representations as delineated in",
  "PLUM Pushes the Pareto-Frontier": "In this section, we evaluate the efficacy of PLUM in preserving the quality of learned representations whentransitioning from the conventional binarization technique.Initially, we train ResNets on the CIFAR10and ImageNet datasets using PLUM, and benchmark the results against existing state-of-the-art methodsin subsection 4.1. Additionally, subsection 4.3 examines the latent full-precision and quantized weights indetail to provide a holistic understanding of the retained representational capacity of the trained model.",
  "Comparison with other works": "To ascertain the true potential of PLUM codesign, we compare PLUMs signed-binary quantization withSOTA binary-quantization methods..We benchmark on CIFAR10 and ImageNet datasets by trainingResNet-{20,32} and ResNet-{18,34} models. Conversely, model parameters are of two types, effectual (ornon-zero valued) parameters and ineffectual (or zero valued) parameters (Wu et al., 2021; Nayak et al., 2023).Since only effectual parameters lead to computation during inference, we compare different binary methodson two axes: model accuracy on the Y-axis and effectual binary model parameters on the X-axis (refer tosupplementary D & C for baselines and setup). As shown in , PLUM exhibits Pareto optimality",
  "Comparing Against Full Precision Models on Additional Datasets": "We train VGG, AlexNet, and ResNet on CIFAR10, SVHN (svh, 2011), and TinyImageNet (Le & Yang)datasets, respectively. AlexNet* (ale, 2016) and VGG** (Cai et al., 2017) are derivative architectures basedon AlexNet (Krizhevsky et al., 2012) and VGG (Simonyan & Zisserman, 2015) for smaller datasets as usedin prior works (Prabhakar et al., 2021; Fu et al., 2022).",
  "(b) Latent Full Precision Weights": ": On the left: Quantized Weights, are depicted, illustrating a distribution reminiscent of ternarynetworks but with weights distinctively segregated across filters, enhancing weight-repetition and inferenceefficiency. On the right: Latent Full Precision Weights in a signed-binary conv block maintain a bluedistribution akin to binarys Laplacian. In sign-binary, total parameters can be divided between positiveand negative signed-binary filters. These parameters can be subdivided into non-zero valued effectual andzero-valued ineffectual ones. Notably, while the total parameters looks like binarys zero-mean Laplace dis-tribution, individual filters do not. The effectual and ineffectual parameters green-red and red-green distri-butions resemble Laplace and Gaussian distributions, respectively. Sign Binary reduces computations duringinference when compared to binary, as it reduces effectual parameters from blue to green-red distribution. pronounced sparsity introduced by signed binary quantization, the distribution of latent FP weights acrossan entire convolutional block resembles a zero-mean Laplacian distribution. Signed-binary filters are neitherzero mean, nor exhibit a Laplacian distribution (supplementary H). This shows that even if individual filtersdont align with a zero-mean or Laplacian distribution, collectively, the convolution block resembles thesecharacteristics. Moreover, the presence of four distinctive peaks overlaying Laplacian resembling distributionfor full-precision latent weights are due to: (a) Two peaks at the extremes appear because of clamped weightsat +1 and -1, and (b) two intermediate peaks arising from the employment of threshold functions defined at, analogous to zero-valued peak observed in binary network due to the use of sign quant function. Quantized weights.We investigate the distribution in quantized weights of a trained signed-binary modelby plotting the percentage distribution of quantized signed-binary convolutional blocks in ResNet18 trainedon Imagenet. reveals a roughly consistent, equal proportion of both positive and negative weights.This observation of a roughly equal proportion of positive and negative weights is also observed in ternaryquantization (Zhu et al., 2016). However, a crucial distinction arises in the distribution of positive andnegative weights within a convolutional layer. Like ternary, both positive and negative valued weights arepresent within a layer. However, signed binary co-design bifurcates them across different filters. This designdecision improves inference efficiency.",
  "PLUM improves Inference Efficiency": "This section shows that transitioning from conventional partitioned design for binary to PLUM co-designframework enhances inference efficiency for ResNet18 trained on ImageNet. We highlight PLUMs superiorityover conventional partitioned design using binary and ternary by considering the repetition-sparsity trade-off. Additionally, we explore how weight sparsity in DNNs, specifically in signed-binary ResNet18 trainedon ImageNet, benefits DNN inference.",
  "w/o sparsity support is aligned with blue distribution, matching binary performance. On the other hand,": "PLUM (w/ sparsity support) reduces computation to green-red distribution. Because negative and positivevalued quantized weights exist in separate regions of the network by design (see ), PLUM retainsrepetition, resulting in speedup. et al., 2021). Concurrently, exploiting weight repetition leads to diminishing arithmetic operations and datamovement (Hegde et al., 2018). However, traditional binary networks prioritize the maximization of weightrepetition, overlooking the potential benefits of weight sparsity, while ternary networks induce weight sparsitybut compromise on weight repetition. In contrast, we hypothesize that PLUM framework, being attuned tothis trade-off, promises enhanced efficiency in actual device inference. To validate our hypothesis, we deployquantized ResNet-18 models on Intel CPUs, rigorously measuring inference times under varying conditions. Experimental Setup and MethodologyWe use SumMerge (Prabhakar et al., 2021) for performinginference of quantized and sparse DNNs on Intel CPUs (details in supplementary A). All experiments areconducted under identical test environments and methodologies. Within our experiments utilizing Sum-Merge, we explore two distinct configurations: (1) with sparsity support deactivated, the software does notdistinguish between zero and non-zero weights, relying solely on weight repetition; (2) with sparsity supportactivated, the software additionally omits computations involving zero weights. We provide detailed analysisinto both per-layer speedups and the aggregate speedup across different quantization strategies relative tobinary quantization. Result and AnalysisPlease refer to Figures 4 and 7. We observe in that PLUM, i.e., signed-binary with kernel exploiting repetition and sparsity, is most efficient for every quantized layer and the modeloverall by 1.26x and 1.75x faster respectively when exploiting both repetition and sparsity. This result can be explained as follows: (A) When sparsity support is turned off : The software is onlyrelying on repeating values within the weight tensor for speedup. Because binary and signed-binary have twounique values per convolutional filter, they take similar time for DNN inference. Ternary is much slower asit has three unique values per convolution filter which makes extracting efficiency by using weight repetitionexponentially harder. (B) When sparsity support is turned on: The software not only cares about therepeating values in the weight tensor but also skips computations on zero weights to improve the runtime.Here we observe that ternary is slower than binary, because the reduction in work due to sparsity is not ableto compensate for the exponential decrease in weight repetition. On the other hand, our method, PLUM,does not suffer from this problem and can exploit weight repetition and weight sparsity to the fullest andis most efficient.Thus, PLUM framework that relies on repetition-sparsity aware-inference performs betterthan using prior-art inference with binary and ternary methods.",
  "Understanding benefits of Sparsity during inference": "In this section, we aim to understand the impact of weight sparsity given a fixed weight repetition in PLUMframework. We change the percentage of weight sparsity when using one bit quantization. To do this, wecount the number of quantized weights with zero values and divide it by the total number of quantizedweights to calculate the percentage of sparsity. We find that signed-binary ResNet-18 trained on ImageNethas 65% sparsity. Since density is (1 - sparsity) (Wu et al., 2021), ResNet-18 has 35% density (see a).if we switch from conventional binary to PLUMs signed-binary, we decrease the density from 100% to 35%.We would like to leverage the low density to reduce the amount of computation activity during inference: ThroughputIn a model with low density, represented by 1/x, there is one effectual multiplication forevery x total multiplications. By eliminating the ineffectual computations and the time associated withthem, there is the potential to improve throughput by a factor of x (Emer et al., 2021). Given that thePLUMs signed-binary has a 35% aggregate densitymeaning only 35% of the multiplications are effectualexploiting sparsity during inference can lead to a potential 2.86 increase in throughput compared to densebinary {1,-1}. In practice, the speedup we observe on real hardware is in the range 1.26x-1.75x as the supportfor unstructured sparsity on general-purpose devices is an active area of research (Wu et al., 2021). Thisspeedup is comparable to the speedup due to unstructured sparsity on general-purpose devices in recentpapers (Gong et al., 2020; Hegde et al., 2019). Energy ReductionTo estimate energy reduction due to the unstructured sparsity during inference, weuse the cycle-level micro-architectural simulator (Muoz-Matrnez et al., 2021) of a sparsity supportingASIC (Qin et al., 2020a). We take their publicly released code and use it under default configuration (detailsin supplementary A). We observe that decreasing density from 100% to 35% for one-bit ResNet 18 leads toa 2x reduction in energy during inference. Thus, switching from binary to PLUMs signed-binary wouldlead to significant improvements in power consumption on ASICs.",
  "Discussion and Future Work": "The paper introduces the concept of repetition-sparsity trade-off and proposes PLUM (PLUs-Minus) aquant-system co-design framework that leads to improvement of inference efficiency while retaining accuracy..PLUM exhibit Pareto optimality compared to prior conventional methods, improving accuracy per effectualparameter and enhancing computational efficiency during inference with respect to speed, energy, anddensity. However, PLUM requires training from scratch, which can be time-consuming and computationallyexpensive. PLUM requires the use of two quantization functions, i.e., with value sets of {1,0} and {-1,0}.Only using a single quantization function (just {1,0}) could lead to improving inference efficiency. Followingthe terminology from .1, while binary can be represented using at least R S C K bits,signed-binary requires one additional bit per filter to signify the corresponding signed-binary quantizationfunction, resulting in R S C K + K bits. Further, the tile size of the modern inference system shouldbe set such that a single processing step in PLUM should see only one signed binary quantization function.Additionally, the understudied repercussions of quantization and, consequently, signed binary quantizationon model bias warrant further exploration. Despite these limitations, PLUM presents a promising approachfor training efficient models that are both accurate and computationally efficient, particularly well-suitedfor applications where hardware resources are limited, such as mobile devices and embedded systems.",
  "Reproducibility Statement": "We provide all the hyperparameters for the key experiments including instructions on how to train the modelsin the supplementary C. Further, since our work is about quantization system co-design, we provide all thehyperparameters and configurations to reproduce our inference experiments in supplementary A. Finally,implementation details and baselines for our method can be found in supplementary C. Acknowledgements. We thank Tushar Krishna, Christopher Fletcher, Twinkle Kuhar and Paramvir Singhfor their insightful discussions, as well as the anonymous reviewers for their very helpful feedback. Wed alsolike to thank Francisco Muoz-Martnez and Raveesh Garg for helping with Stonne for our experiments.",
  "Y. Bai, Y. X. Wang, and E. Liberty. Proxquant: Quantized neural networks via proximal operators. InICLR19, arXiv (2018), 2018": "Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss,Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, ClemensWinter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, JackClark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Languagemodels are few-shot learners. CoRR, abs/2005.14165, 2020.",
  "Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducinginternal covariate shift. arXiv preprint arXiv:1502.03167, 2015": "Yash Jain, Chi Ian Tang, Chulhong Min, Fahim Kawsar, and Akhil Mathur. Collossl: Collaborative self-supervised learning for human activity recognition. Proceedings of the ACM on Interactive, Mobile, Wear-able and Ubiquitous Technologies, 6(1):128, 2022. Yash Jain, David Chan, Pranav Dheram, Aparna Khare, Olabanji Shonibare, Venkatesh Ravichandran, andShalini Ghosh. Multi-stage multi-modal pre-training for automatic speech recognition. arXiv preprintarXiv:2403.19822, 2024.",
  "Andrew L Maas, Awni Y Hannun, and Andrew Y Ng.Rectifier nonlinearities improve neural networkacoustic models. In ICML13, ICML13, 2013": "Ajay Mandlekar, Danfei Xu, Josiah Wong, Soroush Nasiriany, Chen Wang, Rohun Kulkarni, Li Fei-Fei,Silvio Savarese, Yuke Zhu, and Roberto Martn-Martn. What matters in learning from offline humandemonstrations for robot manipulation. In Conference on Robot Learning (CoRL), 2021. Francisco Muoz-Matrnez, Jos L. Abelln, Manuel E. Acacio, and Tushar Krishna. Stonne: Enabling cycle-level microarchitectural simulation for dnn inference accelerators. In 2021 IEEE International Symposiumon Workload Characterization (IISWC), 2021. Nandeeka Nayak, Toluwanimi O Odemuyiwa, Shubham Ugare, Christopher Fletcher, Michael Pellauer, andJoel Emer. Teaal: A declarative framework for modeling sparse tensor accelerators. In Proceedings of the56th Annual IEEE/ACM International Symposium on Microarchitecture, pp. 12551270, 2023.",
  "H. Pouransari, Z. Tu, and O. Tuzel. Least squares binary quantization of neural networks. In CVPRW20,2020": "Rohan Prabhakar, Sachit Kuhar, Rohit Agrawal, Christopher J Hughes, and Christopher W Fletcher. Sum-merge: an efficient algorithm and implementation for weight repetition-aware dnn inference. In Proceedingsof the ACM International Conference on Supercomputing, pp. 279290, 2021. Eric Qin, Ananda Samajdar, Hyoukjun Kwon, Vineet Nadella, Sudarshan Srinivasan, Dipankar Das, BharatKaul, and Tushar Krishna. Sigma: A sparse and irregular gemm accelerator with flexible interconnectsfor dnn training. In 2020 IEEE International Symposium on High Performance Computer Architecture(HPCA), pp. 5870. IEEE, 2020a. Haotong Qin, Ruihao Gong, Xianglong Liu, Mingzhu Shen, Ziran Wei, Fengwei Yu, and Jingkuan Song.Forward and backward information retention for accurate binary neural networks. In Proceedings of theIEEE/CVF conference on computer vision and pattern recognition, pp. 22502259, 2020b. Haotong Qin, Xiangguo Zhang, Ruihao Gong, Yifu Ding, Yi Xu, and Xianglong Liu. Distribution-sensitiveinformation retention for accurate binary neural network. International Journal of Computer Vision, 131(1):2647, 2023.",
  "Vivienne Sze, Yu-Hsin Chen, Tien-Ju Yang, and Joel S Emer. Efficient processing of deep neural networks.Synthesis Lectures on Computer Architecture, 15(2):1341, 2020": "Hanrui Wang, Zhekai Zhang, and Song Han.Spatten: Efficient sparse attention architecture with cas-cade token and head pruning. In 2021 IEEE International Symposium on High-Performance ComputerArchitecture (HPCA), pp. 97110. IEEE, 2021. Yannan Nellie Wu, Po-An Tsai, Angshuman Parashar, Vivienne Sze, and Joel S Emer. Sparseloop: Ananalytical, energy-focused design space exploration methodology for sparse tensor accelerators. In 2021IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS), pp. 232234.IEEE, 2021.",
  "AExperiment Setup Efficiency": "Deploying on CPUsWe use SumMerge Prabhakar et al. (2021) for this task. We run all experimentson Intel Xeon Gold 6226 CPU. In order to make our test environment as close as possible to the testenvironment of the authors of Prabhakar et al. (2021), we disable simultaneous multi-threading, and enable2MB huge pages and disable dynamic frequency scaling as well. The test methodology is exactly the sameas used by the authors of Prabhakar et al. (2021), i.e., each experiment is run 50 times when the machine isunloaded and the values for run with the lowest execution time are reported. All arithmetic operations are infloating-point. All DNN inference experiments are subject to identical test environments and methodology. ASICWe use STONNE Muoz-Matrnez et al. (2021), a cycle-level microarchitectural simulator for DNNInference Accelerator SIGMA Qin et al. (2020a) for this experiment. We use the docker image released by theauthors of Muoz-Matrnez et al. (2021). We use the standard configuration of SIGMA with 256 multiplierswitches, 256 read ports in SDMemory and 256 write ports in SDMemory. The reduction networks is set toASNETWORK and the memory controller is set to SIGMA_SPARSE_GEMM. We use SimulatedConv2dfunction in the PyTorch frontend version of STONNE. For a given convolutional layer, we run STONNEtwice, once with 0% sparsity and once with 65% sparsity. We calculate the reduction in energy consumptionby dividing the energy of the dense convolutional layer by the energy of the sparse convolutional layer. Sincethe weights precision (or bit-width) is a parameter of SIGMA, the reduction in energy due to sparsity whencompared to the dense model is not a function of the precision of the weights of the DNN.",
  "BVisualizing Exploitation of Repetition and Sparsity": ": Visualization of repetition and sparsity during inference in modern systems: (1) Input and twofilters that need to be multified (2) naive multiplication of weights and activations to create output 1 andoutput 2. (3) Re-ordering of weights and activations to simplify work within each filter (and exploit intrafilter repetition phenomenon (Hegde et al., 2018)) (3) Using partial sums to reduce the work across filters(and exploit intra filter repetition phenomenon (Hegde et al., 2018)) (4) exploiting sparsity (by acknowledgingrepetition of zero weights as a special case of weight repetition (Sze et al., 2020; Hegde et al., 2018; Prabhakaret al., 2021).",
  "DBaselines": "Baselines for Comparison with Prior work: shows comparison against prior-art methods.The numbers are reported from the literature: DIR-Net Qin et al. (2023) LQ-Net Zhang et al. (2018),DSQ Gong et al. (2019), BC Courbariaux et al. (2015), ProxQuant Bai et al. (2018), IR-Net Qin et al.(2020b), DoReFA Zhou et al. (2016), SQ-BWN Dong et al. (2017), BWN Rastegari et al. (2016), BWNH Huet al. (2018), ABC-Net Lin et al. (2017), BWHN Hu et al. (2018), LS Pouransari et al. (2020), Bi-Real Liuet al. (2018).",
  "ESigned-Binary vs Binary wrt Effectual parameters": "We would like to compare binary with signed-binary when the DNN has the same number of non-zeroparameters. Signed-Binary ResNet trained on CIFAR10 has slightly greater than 50% sparsity. If we reducethe total number of parameters of the binary ResNet by half, the resulting model would have comparablenumber of non-zero weights to signed-binary ResNet. This is done by reducing depth (see a) andby reducing width (see b). We train these models under identical conditions (setup details in theappendix). To clarify, row 1 & row 2 of have the same number of total parameters while row 1 &row 3 have a comparable number of non-zero parameters. Thus, signed-binary leads to a higher accuracythan binary when both methods have comparable number of effectual operations.",
  ": Additional Ablations on CIFAR10: Abations on batch size and non-linearity for SB": "We perform ablation on (1) batch sizes, (2) non-linearity on CIFAR10 (Krizhevsky & Hinton, 2009) datasetand report the numbers in a and 8b respectively. The setup is the same as mentioned above. Weobserve that for our method, there is a drop in accuracy with higher batch size, PReLU (Maas et al., 2013)works best and it is not sensitive to the choice of delta.",
  "GArithmetic Reduction Ablation": "The fewer arithmetic operations required during inference for a given layer, the more efficient the quantizationscheme is for both CPUs and GPUs (Hegde et al., 2018; Prabhakar et al., 2021; Fu et al., 2022). This can bemeasured using arithmetic reduction (Hegde et al., 2018; Prabhakar et al., 2021; Fu et al., 2022), which isdefined as the ratio of arithmetic operations required using naive dense computation (unaware of repetitionand sparsity) to the arithmetic operations taken during repetition-sparsity aware inference for a given DNNblock. This metric indicates inference efficiency at the algorithmic level (Hegde et al., 2018; Prabhakar et al.,2021; Fu et al., 2022). compares the arithmetic reduction across different quantization schemes. Theexperiment follows the original test settings and methodologies described by the authors of (Prabhakar et al.,2021), using synthetic, uniformly distributed weights in the DNN block. The results show that signed-binaryquantization is the most efficient, providing the highest arithmetic reduction across all conv layers. We extend this experiment by varying the sparsity percentage from 0% to 100%, with equal percentagesof positive and negative weights across all three quantization schemes. compares the arithmeticreduction (Y-axis) and the percentage of sparsity (X-axis) for a convolutional block of shape .Since binary quantization uses +1, -1 and does not leverage sparsity, its performance is represented by ahorizontal line. Ternary quantization initially behaves like binary when sparsity is negligible and performs",
  ": Dataset with Licenses: License and source of the datasets used": "Licenses of ImageNet (Deng et al., 2009) and CIFAR10 (Krizhevsky & Hinton, 2009) datasets used in thispaper are listed in .Every accuracy reported in this paper is on validation set of the dataset.ImageNet and CIFAR10 are standard publicly used datasets. Since they do not own their images, thereforethey do not have a release license. Actual images may have their own copyrights but ImageNet providesstipulations for using their dataset (for non-commercial use). We do not recommend using the resultingsigned-binary models trained on these datasets for any commercial use."
}