{
  "Abstract": "Recent studies evaluating various criteria for explainable articial intelligence (XAI) suggestthat delity, stability, and comprehensibility are among the most important metrics consid-ered by users of AI across a diverse collection of usage contexts. We consider these criteria asapplied to feature-based attribution methods, which are amongst the most prevalent in XAIliterature. Going beyond standard correlation, methods have been proposed that highlightwhat should be minimally sucient to justify the classication of an input (viz. pertinentpositives). While minimal suciency is an attractive property akin to comprehensibility,the resulting explanations are often too sparse for a human to understand and evaluate thelocal behavior of the model. To overcome these limitations, we incorporate the criteria ofstability and delity and propose a novel method called Path-Sucient Explanations Method(PSEM) that outputs a sequence of stable and sucient explanations for a given input ofstrictly decreasing size (or value) from original input to a minimally sucient explana-tion which can be thought to trace the local boundary of the model in a stable manner,thus providing better intuition about the local model behavior for the specic input. Wevalidate these claims, both qualitatively and quantitatively, with experiments that show thebenet of PSEM across three modalities (image, tabular and text) as well as versus otherpath explanations. A user study depicts the strength of the method in communicating thelocal behavior, where (many) users are able to correctly determine the prediction made bya model.",
  "Introduction": "Consider an approved loan applicant that asks their bank the following question: \"How can I change my proleand still remain in good standing?\" This is a common occurrence, for example, when someone wants to makea major purchase, e.g. buy a car, after their mortgage is approved but before the home purchase is ociallycompleted (see the article What is Considered a Big Purchase During Mortgage Underwriting\" (Sharkey,2022)), when a married couple divorces and each separate party needs to obtain a new mortgage whilenegotiating a settlement on their joint nances, or when parents want to relocate after a child graduates highschool while simultaneously paying for college; they all need to understand how the change to their nancesimpacts their credit. The applicant is essentially asking how much, if any, additional debt/risk can be takenwithout aecting the desired mortgage. One potential answer, called the minimally sucient explanation orpertinent positive (PP) (Dhurandhar et al., 2018; Feng et al., 2018), highlights the minimal set of features(and values) that are sucient to replicate a models decision, which is the minimal applicant prole thatwill cause the model to approve the loan1. In the college scenario, the parents want to compute how much",
  "Published in Transactions on Machine Learning Research (12/2024)": "it is worth noting that participants that preferred PSEM did much better on PSEM questions than those thatpreferred LIME or CEM-PP. By grouping participants according to explanation preference, showsthat only those that preferred PSEM performed particularly well, and furthermore only on PSEM questions.Those that preferred LIME performed best on LIME (however still not good performance). Those preferringCEM-PP did not perform best on any explanation type, while those preferring PSEM also performed beston CEM-PP. This implies that the extra information of PSEM does not overwhelm but rather signicantlyimproves understanding. A deeper look at the data also shows interesting cases such as preference for CEM-PP where the user scored 8/10 on PSEM and 6/10 on CEM-PP, and noted that he/she generally moved tothe last image of the PSEM explanation. So either subconciously the path helped this user or the PSEMpath actually did converge to better explanations than CEM-PP (at least in the view of this particular user).",
  "cash they can contribute towards tuition (which reduces any required college loans) while maintaining goodcredit standing for a new mortgage": "While such an explanation might answer the applicants question, the highlighted features are often too fewto act on in a practical manner (e.g., a true minimal solution does not contain enough explanatory/actionableinformation). Rather, a stable path of solutions will oer the applicant a better perspective as seen in which answers the question about remaining in good standing to two loan applicants from the Home EquityLine of Credit (HELOC) dataset. Our measure of stability, illustrated below in and formally denedin , connects each solution along the path, in eect forcing the applicant proles to remain realistic.The HELOC dataset FICO (2018) contains credit applicant data, describing various aspects of an applicantscredit history such as how many credit lines s/he has open and various features about delinquent payments,where the goal is to predict whether or not someone is a good applicant. The Original column is the applicants current prole, and the columns labeled PSEM 1-4 oer a stable pathof reducing prole feature values while maintaining an approval rating by the banks model. For example, itshows at what rate the applicant can close accounts while also reducing their estimate of being a safe applicant(External Safety Estimate) or reducing the percentage of accounts that have never been delinquent. If theapplicant was only given the perspective of the PP (termed CEM-PP in the last column), it would seem theapplicants must close many old accounts (reduce Age of Oldest Account), and open several new accounts(reduce Age of Newest Account to 0) in order to maintain the same number of original accounts, which isimpractical and against intuition. This example motivates the need for stable paths of sucient explanationsthat lead to better insight, which is the main contribution of this work. Utilization and impact of black-box models (e.g., neural networks) has signicantly grown creating the needfor new tools to help users understand and trust models (Gunning, 2017). Given this acute need, variousmethods have been proposed to explain local decisions of classiers. We seek to consider local explanationsfrom the perspectives of stability, delity, and comprehensibility - these are among the most importantcriteria considered by users of AI across a diverse collection of usage contexts (Liao et al., 2022). Mostsuch local methods highlight relevant features that determine the models decision, but we focus on methodsthat highlight the PP as described above that typically involve some type of control over the sparsity ofthe explanation. One could develop a path of explanations by varying such regularization, but we nd suchpaths often lack stability which is crucial for building trust in the explanation.",
  "Related Work": "Identifying important features of an input that are relevant to its classication has been an active researcharea (Zhou et al., 2021; Ribeiro et al., 2016; Lundberg & Lee, 2017; Simonyan et al., 2013; Lapuschkin et al.,2016). An attribution method (Schulz et al., 2020) inspired by the information bottleneck (N. Tishby &Bialek, 1999) quanties the amount of information each image attribute contributes to a prediction.Severalworks pursue explanations that require data to train neural networks for providing explanations. Situ et al.(2021) pursues stable explanations by learning a model that predicts the explanations given by an inputexplanation model such as (Ribeiro et al., 2016; Zeiler & Fergus, 2014), Yoon et al. (2019) trains predictor,baseline and selector neural networks to oer explanations from the selector network, Jethani et al. (2019)trains a global selector network to identify the important features, and Dabkowski & Gal (2017) focuses onimage classiers and trains a large neural network for inferring image masks. These explanation methods alldier from the scenario considered in this paper where we do not assume sucient data to train additionalneural networks. Another attribution method by Zhou et al. (2015) is tailored to explain convolutional neuralnetworks, particularly focuses on pooling operations, and is not a general explanation method. Lastly, Zhouet al. (2021) uses hypothesis testing to decide when more samples are necessary to induce stability. Hierarchical explanations (Singh et al., 2019; Tsang et al., 2018) oer a path of explanations that highlightwhich feature interactions are most meaningful to the prediction, i.e., which feature interactions ip theclassication to the predicted class from some other class. While informative, such explanations oer adierent type of information than considered here, more in line with counterfactuals (Guidotti et al., 2018;Le et al., 2020) or the pertinent negatives dened in Dhurandhar et al. (2018). Pertinent positives ratheroer a complimentary explanation to such methods. Most relevant to our paper are local suciency based methods (Dhurandhar et al., 2018; Feng et al., 2018;Ribeiro et al., 2018; Lei et al., 2016; Luss et al., 2021; Fong et al., 2019; Carter et al., 2019; 2021; Watsonet al., 2021) which highlight features that are sucient to obtain the same classication as the originalinput. The explanations provided by these methods may be too sparse to judge the quality of the model.Moreover, a higher sparsity penalty may not always lead to a sparser explanation. Masking and inllinghas been used in previous suciency-based explanations for colored images such as Fong & Vedaldi (2017)and Chang et al. (2019), where the main dierence is on how inlling is done. Luss et al. (2021) and Fonget al. (2019), which builds on Fong & Vedaldi (2017), consider smoother masks than these prior works;Fong et al. (2019) considers smooth masks over individual pixels while Luss et al. (2021) consider smoothmasks according to a prior image segmentation. These dierent perturbation-based methods are extendableto our framework, as will be demonstrated on a colored image dataset specically for the method of Lusset al. (2021). Lei et al. (2016) jointly trains a classier with an explanation generator for text classication;we explain xed models.Carter et al. (2019) is a form of input reduction (Feng et al., 2018), and thevery recent extension to Carter et al. (2021) uses gradients to select features rather than expensive greedysearch. Watson et al. (2021) oers a probabilistic measure of suciency by sampling and enumerates over allpossible sucient explanations and is impractical in high dimensions. Lastly are local sucient explanationmethods (also called abductive explanations or prime implicants) for logic-based models such as decisiongraphs (Ignatiev et al., 2021; Darwiche & Ji, 2022). While our focus is on complex dierentiable models(viz. deep neural networks), there is a large body of literature focused on these logic-based models, wherea sucient explanation seeks a minimal set of features that matter for a prediction, whereas our frameworkallows features to simply be reduced, such as in the loan approval example in the Introduction.",
  "Problem Statement and Method": "We start by introducing a formulation for learning a path of sucient explanations and relate it to ndingPPs in Dhurandhar et al. (2018). Let x0 denote an input with output label t0 and X the entire input space.For the dierent modalities considered, we assume X = n for text documents with n-word normalizeddictionaries or grayscale images with n pixels, and X Rn+ for tabular data with n features, i.e., tabulardata that has only non-negative features. Let Pred() denote a function that takes an input and outputsthe predicted probability vector for a classier. [Pred()]i denotes the ith component of this vector, i.e.,the probability of being classied in the ith class. A path of N explanations is denoted by 0, . . . , N Xand can be viewed as samples that lie in the same space as input sample x0.Dene the loss functionf(x0, ) = max{maxi=t0[Pred()]i [Pred()]t0, } with margin parameter 0 which applies a penaltyif is classied dierent from x0 (or has a similar score within the margin ). Our new formulation, whichwe call the Path-Sucient Explanation Problem, is as follows:",
  "i i122 ,i i1i [N](b)": "where N N is a hyperparameter dening the length of a path that must be tuned for a particulardataset. We use notation [N] = {1, . . . , N}. The rst term in the loss ensures that each explanation alongthe path maintains the same class as the input, i.e., delity. The second term in the loss penalizes thenumber of features in successive explanations. The constraint in (a) denes N variables i that representthe sucient explanations we learn along the path, where the rst explanation is the input sample x0 andthe last explanation is the remaining features that appear in the objectives loss function.The secondconstraint in (a) represents feasibility for the domain. The rst constraints in (b) enforce stability (eachsuccessive explanation in the path is close to the previous one); El Zini & Awad (2022) call such constraintsthe -connectedness of 0 to N which they used as a metric for faithfulness of counterfactuals. The secondconstraints in (b) enforce monotonicity of the explanations along the path; successive explanations highlightonly a subset of the features highlighted in prior explanations, where i i1 is dened component-wise.This gives comprehensibility to the path as features not highlighted before do not suddenly start showingup as important, which can be intuitively hard to understand. Taken together these constraints inform asto what it might take to remain in the particular class as we drop or reduce importance of features. The constraints in (b) are the main innovation over Dhurandhar et al. (2018), as given a PP solution fromDhurandhar et al. (2018), there is no guarantee that a exists with arg maxj[Pred()]j = arg maxj[Pred()]jsuch that 22 , X, and for some small > 0. Essentially, this means that there areno small additions within the boundaries of the original sample that could be made to the PP that wouldmaintain the same classication. In other words, it implies a lack of stability where judging the quality ofthe model just based on might be challenging, i.e., not comprehensible. This however does not imply thata stable path cannot exist, which motivates the Path-Sucient Explanation Problem (1). Hyperparametersc, , are to be tuned (see Appendix). Regarding feasibility, taking i = x0 for all i is a feasible, but clearly suboptimal, solution since the sparsityregularization in the objective could be reduced. There may be paths where the solution requires j = ifor all j i for some i along the path, meaning a sparser solution along the path at some point cannot befound, but such a solution resolves the stability issue lacking in Dhurandhar et al. (2018). One way to solve problem (1) is alternating minimization where each iteration executes the following steps:for i = 1 . . . , N, optimize problem (1) over i while j for j = i are xed. In this algorithm, i is constrainedto be within of both i1 and i+1 for 1 < i < N, but the constraint i i+122 can be removedbecause it will be enforced at iteration i + 1. Regularization is used to approximate the solution to theresulting constrained optimization.",
  ": end for6: Return 1, . . . , N": "This leads to an algorithm for learning a sequence of explanations, formalized in Algorithm 1, which we callthe Path-Sucient Explanations Method (PSEM). At each iteration i, a successive explanation is learnedby solving problem (2) in Algorithm 1. N iterations corresponds to executing one iteration of alternatingminimization to solve a regularized approximation of the Path-Sucient Explanation Problem (1). Problem(2) is solved via a prox-algorithm (Beck & Teboulle, 2009) that iteratively minimizes g() + i1 whereg() is a rst-order approximation to the remaining terms of the objective. Computationally, PSEM requiresexactly N times the cost of producing a single PP explanation (Dhurandhar et al., 2018; Luss et al., 2021),where N is the PSEM path length (typically < 10). In our setting, this means we are applying FISTA (Beck& Teboulle, 2009) to a nonconvex problem which is analyzed in Li et al. (2017). Note that parameter i isindexed as we want sparsity to increase through the sequence. In order to apply PSEM to color images, we adapt an extension of CEM-PP to color images (Luss et al., 2021)in a similar manner as described above. This extension of CEM-PP to color images notes that explanationsfor color images require a new perspective because, while adding or removing features from grayscale imagessimply reduces to increasing and decreasing pixel intensities, we cannot similarly optimize over pixel intensityin RGB space as there is no concept of background and the results would be unrealistic. Rather, the conceptof feature removal is done by masking superpixels (segments composed of multiple pixels) that result froman image segmentation. Hence, rather than optimize over an input space X, we obtain PPs for color imagesby optimizing over the space of images resulting from any possible binary mask applied to the input image.The binary masks are predetermined by a xed segmentation of image , and this space of masked imagesis denoted by M(). Let M denote the binary mask that when applied to input image x0 produces image. Then the PSEM algorithm for color images solves, at each iteration, problem (3) in Algorithm 1. In practice, the following steps are taken: 1) i1 is segmented, 2) an individual mask is created per segment,and 3) we optimize over binary variables (relaxed to lie in , constrained to be less than or equal to thosethat dene Mi1, and then thresholded) that turn individual masks on/o creating a mask in M(i1). It is also important to consider that the solution to problem (1) is not unique, as the solutions to thesubproblems (2) and (3) are not unique. In the event that multiple equivalent global minima exist, theinitial point will determine which minima is learned. With that in mind, it is also important to recall thatthe objective is non-convex with multiple local minima, and so even if there exist multiple global minima, alocal minima may be learned instead due to the initial point.",
  "PSEM100 (0.0)100 (0.0)100 (0.0)100 (0.0)": "paths. The l2 regularizations from CEM-PP are removed. One implementation caveat for explaining largemodels is to create a single object, referred to as AEADEN in CEM-PP code, where parameters that varyalong the PSEM path (i, i1, Mi1) are implemented with placeholders. This object can then be usedsequentially to solve the PSEM subproblems by passing appropriate parameters (and solutions of previousiterates) at each iteration to the attack function of CEM-PP. If one rather creates a new AEADEN objectat each iteration and calls the respective attack functions, the iterative creation of new AEADEN objects,which replicate the classication model, can quickly use up memory resources. We implemented IR. Allexperiments used 1 GPU and up to 16 GB RAM.",
  "Document 2 (Correct; sci.med, Model: rec.autos)": "I use a ZYGON Mind Machine as bought in the USA last year. Although its no wonder cure for what . . . supposeyoure tired and want to go to bed/sleep. BUT . . . I slip on the Zygon and select a soothing pattern of light& sound, and quickly I just cant concentrate on the previous stu. Your brains cache kinda gets ushed, and . . .ExplanationSelected Words",
  "Qualitative Evaluations": "We next illustrate the usefulness of PSEM on text and image datasets (the introduction already demonstratedPSEM on the tabular HELOC dataset). Each experiment compares PSEM with CEM-PP and LIME (andInput Reduction (IR) (Feng et al., 2018) on text).Among other notable methods, Anchors (Ribeiro et al.,2018) has been applied only to text and the information-theoretic attribution method (Schulz et al., 2020)only to images. Sucient input subsets (Carter et al., 2019) outputs identical solutions to input reduction.Implementation details, along with a hyperparameter discussion, can be found in the Appendix.",
  "Text data: 20 Newsgroups": "The 20 Newsgroups dataset contains text documents labelled as one of twenty categories from dierent topicsamong computers (graphics, hardware, etc.), recreation (autos, baseball, etc.), science (electronics, medicine,etc.), politics (guns, mideast, miscellaneous), and religion. Headers, signature blocks, and quotation blockshave been removed from each document, which is processed as a bag of words (all words are stemmed andstopwords removed) with TFIDF normalization. We trained a three layer neural network that achieves 64%test accuracy. Recall that our goal is to explain model predictions, not to build a state-of-the-art model. We compare PSEM, CEM-PP, LIME, and IR (Feng et al., 2018), a greedy method that removes words, oneby one, as long as the classication does not change. For LIME, we set the number of features to 10 for",
  "Color Image data: CelebA": "We next experiment on color images where the PSEM path is over superpixels rather than individual features(by solving Problem (3) as discussed in ). The CelebA (Liu et al., 2015) dataset contains imagesof celebrity faces along with 40 features describing things such as hair color, face shape, etc. We followthe setup of Luss et al. (2021) and explain an 8-class (by joining 3 binary classes) classier, a Resnet50model (He et al., 2016), that predicts the following three classes: sex (male/female), smiling (yes/no), andage (young/old). While, we apply PSEM here to Luss et al. (2021), we acknowledge that there are variousways to handle the masking of pixels, e.g., Fong & Vedaldi (2017) and Fong et al. (2019), but analyzingdierent stability penalties is outside the scope of this paper, where the goal is to derive paths of sucientexplanations which could be done for these works as well.",
  "Image data: MNIST Handwritten Digits": "The MNIST dataset is comprised of handwritten digit images 0-9. We trained a convolutional neural networkfrom Dhurandhar et al. (2018) which uses two blocks, each composed of two convolutions followed by a poolinglayer, followed by three fully-connected layers. Note that Dhurandhar et al. (2018) also include an additionalregularization of the form AE()22, where AE() is an autoencoder; this term ensures that the learnedPP lies near the true manifold, however Dhurandhar et al. (2018) only uses it on MNIST experiments wherea sucient autoencoder is learned. We similarly use an autoencoder only for MNIST experiments and thusleft it out of the formulations for clarity. The same autoencoder architecture from Dhurandhar et al. (2018)is used to keep PSEM explanations near the image manifold. Results are illustrated in . PSEM, CEM-PP, and LIME are compared across a single image eachfrom 0-9. PSEM outputs a path of ve images where PSEM-i labels the ith image along the path wherethe sucient number of pixels is decreasing from PSEM-1 to PSEM-5. For CEM-PP, the sparsity penalty inCEM-PP is set to the same sparsity penalty used in PSEM-3 in order to give a comparison with CEM-PPthat is not too sparse (sparsity levels dier because PSEM and CEM-PP have dierent regularizations). A key observation is that stability of the PSEM path results in sucient explanations that clearly distinguisheach of the digits. Better stability of PSEM is seen on the digit 1 where CEM-PP fails to nd a minimallysucient set. The dierent regularization of PSEM allows for better control to learn an intuitive path ofexplanations, where the sparser explanations even focus on dierent regions of the digit. CEM-PP has anintuitive explanation for the 0 (but very sparse for 2-7 and 9), and as already noted, if a user wanted a denserexplanation, there is no guarantee that adding pixels could remain in the same class, and simply decreasingthe sparsity regularization could lead to a completely dierent explanation which would be counterintuitive. PSEM oers a smooth path of explanations to the user. In particular, we see that PSEM converges to anotably dierent explanation than CEM-PP for 2, 4, 5, 6, 7, and 8. PSEM shows that the tail is distinguishingfor the 2, the shape for the 4, a full loop for the 6, the corner of the 7, and the x part of the 8, whichis clearly unique to that digit. LIME mostly nds the entire digit positively relevant for the prediction,which could build trust in the classier, but does not add additional useful information into how it might bemaking decisions. This does not mean that LIME isnt useful; indeed, LIME could be used to analyze whyit was not predicted something else since it allows for targeted explanations, unlike PSEM or CEM-PP.",
  "User Study": "We conducted a user study to investigate the information derived from PSEM in comparison with otherlocal explainability methods: CEM-PP and LIME. Our setup is in similar spirit to user studies conductedin Ribeiro et al. (2016) and Singh et al. (2019); for each explanation method, users were provided with twoexplanations, one for each of two dierent models, and had to select which model was more accurate.",
  "PSEM68.0 (6.7)": "& : Left: Accuracy of user study participants at distinguishing between two models based onPSEM, CEM-PP, and LIME. PSEM oers extra information that helps users better discriminate between predictionsin a manner better than the other explanations.Right: Results to exit question asking which explanation theuser found \"most useful for explaining why each model made their predictions.\" Participants that preferred PSEMperformed particularly well on PSEM questions; i.e., extra information is more meaningful to those that prefer moreinformation. Statistically signicant results are displayed in . Across three methods, CEM-PP, LIME, and PSEM,we see the percentage of correct, wrong, and cannot tell with error bars showing one standard error. Thepercentage correct for PSEM is 58.9%, 43.5% for LIME, and 41.1% for CEM-PP, with pairwise t-test p-values of 0.002 and 0.004 comparing PSEM to LIME and CEM-PP, respectively (i.e., benet of PSEM isstatistically signicant). This does not tell the complete picture as there are a signicant number of \"cannottell\", mostly for LIME, so those % correct numbers could have been dierent if participants were forced tochoose; however, in practical applications, an explanation where the user cannot discern information is onpar with a bad explanation. These results show that PSEM oers users useful information that they candisseminate. Whether users are right or wrong, it is interesting to note that users are much more comfortablemaking decisions based on the PSEM path. A nal exit question asked which explanation type the user found \"most useful for explaining why each modelmade their predictions,\" and the results were 40.5%, 35.1%, and 24.3% for PSEM, CEM-PP, and LIME,respectively. While one can debate whether this question is biased because PSEM oers more information,",
  "Discussion": "PSEM is more principled than individually learning pertinent positives at dierent sparsity levels or remov-ing least important features as in input reduction because the stability of the path removes those sucientsolutions that are likely random artifacts of the classier. Crucially, such stability, along with the compre-hensibility it creates, is required to build trust in the explanation and to make suciency-based explanationsmore widely used. Consider the nal row in , where the dierence between PSEM-4 and CEM-PPis that one likely uses the forehead to classify age while the other likely uses the cheek. Clearly, the foreheadis more useful for this task. The cheek was removed by PSEM-3 along the path because a sucient set offeatures did not not require the cheek, but there is no guarantee that removal of the forehead instead wouldhave been sucient, i.e., the CEM-PP result with the cheek is possibly a classier artifact, similar to howthe classier must predict something on a blank image. We have also shown that PSEM oers more realistic scenarios as seen in the Introduction; CEM-PP oersa point explanation and it is not clear if those values are lower bounds to maintain the class, while the pathin PSEM provides much better intuition regarding this aspect, in addition to presenting users with morerealistic possibilities of possible actions. Interest future directions are inspired by Ignatiev et al. (2021) and Darwiche & Ji (2022), where they seekto learn multiple sucient explanations for an individual sample. In the case of logical formulae, it is clearhow to dene the length of explanations, but it is not apparent for a continuous set of features, as with theloan applicant example in the Introduction. However, we can design algorithms for learning multiple paths,e.g., by starting PSEM from dierent initial points or by using dierent parameters to derive dierentpaths. Another direction for future research is to apply PSEM to explanations beyond pertinent positives,e.g., semi-factual explanations. In the our case, the monotonicity of the pertinent positive explanations iscrucial to the path because it is clear how to formulate the monotonic removal of features. As such, PSEMis a meta-approach, where the base explanation could be pertinent positives or semi-factuals. However, inthe latter case, it is not obvious how to formulate a general (smooth) removal of features, and remains aninteresting future direction.",
  "K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In CVPR, June 2016": "Alexey Ignatiev, Nina Narodytska, Nicholas Asher, and Joao Marques-Silva. From contrastive to abductiveexplanations and back again. In AIxIA 2020 Advances in Articial Intelligence, pp. 335355. SpringerInternational Publishing, 2021. Neil Jethani, Mukund Sudarshan, Yindalon Aphinyanaphongs, and Rajesh Ranganath. Have we learnedto explain?: How interpretability methods can learn to encode predictions in their interpretations. InAISTATS, 2019.",
  "APSEM Implementation and Reproducibility": "As previously noted, PSEM contains several parameters that need to be tuned. While this process canbe extensive, it is important to consider that, for a given dataset and application, the parameters needonly be tuned once, after which, an arbitrary number of predictions can be explained. lists thehyperparameters used to get the results throughout the paper. Note that parameter c was initialized to10 for each experiment and the PSEM code searches over that parameter if a minimally sucient solutioncannot be found for that value of c. Regarding the importance of each parameter, sparsity parameters were the last parameters to be tuned. First, for a relatively small , we found it best to focus on tuning and . was the least sensitive parameter, and was relatively easy to tune, once the user gets an idea ofwhat value of (i.e., condence) was too small to nd any PPs. Most sensitivity in tuning was found to bedue to which was done with other parameters already xed. The PSEM implementation adapts CEM-PP code from PSEM entailssequential calls to modied versions of CEM-PP. Modications include adding regularization terms i122 and MMi122 for problems (2) and (3) and projections to maintain consistency along respective",
  "Newsgroups{0.0001, 0.0005, 0.001, 0.005, .1}50.050.5": ": Results comparing stability and delity of path methods on 20 Newsgroups. Stability measures the %of features at each index (> 1) along a path that also appeared at the previous index. Fidelity measures the %of predictions, using features at each index, that are equal to the original prediction. Metrics are averaged over 64samples with one standard error shown in parentheses.",
  "CAdditional comments about User Study": "details the instructions for participants to read before taking the user study. Figures 7 and 8 showtwo examples of such sets of questions, for a digit 3 and digit 6, respectively. The correct answers for thedigit 3 questions (corresponding to CEM-PP, LIME, and PSEM) are Model A, Model B, Model A. Notethat for CEM-PP and PSEM, the inaccurate model cannot nd minimally sucient solutions, and LIMEtypically highlights more positively relevant pixels for the accurate model. The correct answers for the digit6 questions (corresponding to CEM-PP, LIME, and PSEM) are randomly also Model A, Model B, ModelA. CEM-PP and PSEM both highlight upper parts of the left part of the loop in the 6, which would not bepresent in a 5. The PSEM path additionally highlights other parts that could help distinguish the 6 fromother digits, such as an 8 which has a lower loop like a 6. Again, LIME highlights more positively relevant"
}