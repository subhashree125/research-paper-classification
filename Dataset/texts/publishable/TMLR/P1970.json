{
  "Abstract": "Regression on function spaces is typically limited to models with Gaussian process priors.We introduce the notion of universal functional regression, in which we aim to learn aprior distribution over non-Gaussian function spaces that remains mathematically tractablefor functional regression. To do this, we develop Neural Operator Flows (OpFlow), aninfinite-dimensional extension of normalizing flows. OpFlow is an invertible operator thatmaps the (potentially unknown) data function space into a Gaussian process, allowingfor exact likelihood estimation of functional point evaluations. OpFlow enables robustand accurate uncertainty quantification via drawing posterior samples of the Gaussianprocess and subsequently mapping them into the data function space. We empirically studythe performance of OpFlow on regression and generation tasks with data generated fromGaussian processes with known posterior forms and non-Gaussian processes, as well asreal-world earthquake seismograms with an unknown closed-form distribution.",
  "Introduction": "The notion of inference on function spaces is essential to the physical sciences and engineering. In particular,it is often desirable to infer the values of a function everywhere in a physical domain given a sparse number ofobservation points. There are numerous types of problems in which functional regression plays an importantrole, such as inverse problems, time series forecasting, data imputation/assimilation. Functional regressionproblems can be particularly challenging for real world datasets because the underlying stochastic process isoften unknown. Much of the work on functional regression and inference has relied on Gaussian processes (GPs) (Rasmussen &Williams, 2006), a specific type of stochastic process in which any finite collection of points has a multivariateGaussian distribution. Some of the earliest applications focused on analyzing geological data, such as thelocations of valuable ore deposits, to identify where new deposits might be found (Chiles & Delfiner, 2012). GPregression (GPR) provides several advantages for functional inference including robustness and mathematicaltractability for various problems. This has led to the use of GPR in an assortment of scientific and engineeringfields, where precision and reliability in predictions and inferences can significantly impact outcomes (Deringeret al., 2021; Aigrain & Foreman-Mackey, 2023). Despite widespread adoption, the assumption of a GPprior for functional inference problems can be rather limiting, particularly in scenarios where the dataexhibit heavy-tailed or multimodal distributions, e.g. financial time series or environmental modeling. This",
  "Published in Transactions on Machine Learning Research (10/2024)": "natural partitioning among the co-domain for the affine coupling layers. However, in order to make codomainOpFlow applicable to univariate functions, we first randomly shuffle the training dataset {uj|D}mj=1, andpartition the data into two equal parts {ui|D}m1i=1 and {ui|D}m1i=1. We then concatenate these parts along thechannel dimension to create a paired bivariate dataset for training, {(ui, ui)|D}m1i=1. Codomain GP data. We found that the codomain OpFlow requires different hyperparameters from thedomain decomposed setting. Here we set lU = 0.5, U = 0.5; lA = 0.1, A = 0.5, with a resolution of 256.In particular, these values suggest that codomain OpFlow has less expressivity than domain decomposedOpFlow, which we attribute to the need for codomain OpFlow to learn the joint probability measures,which is hard by nature. Given that both channels convey identical information, we present results from onlythe first channel for samples generated by codomain OpFlow. demonstrates that both codomainOpFlow and GANO effectively produce realistic samples, capturing both the autocovariance function and thehistogram distribution accurately. Contrastingly, in the appendix, codomain OpFlow shows great performancein super-resolution task, potentially due to the channel-split operation preserving the physical domainsintegrity. To the best of our knowledge, codomain OpFlow is the inaugural model within the normalizingflow framework to achieve true resolution invariance. 0.00.20.40.60.81.0 Ground truth",
  ":Examples of datafrom non-Gaussian processes. (a)Temperature field from RayleighBnard convection problem. (b)Vorticity field from Naiver-StokesEquation": "Neural Operators. Neural Operator is a new paradigm in deep learningfor learning maps between infinite-dimensional function spaces. Unliketraditional neural networks that primarily work with fixed-dimensionalvectors, neural operators are designed to operate on functions, makingthem inherently suitable for a wide range of scientific computing andengineering tasks involving partial differential equations (PDEs) (Kovachkiet al., 2023; Li et al., 2020). A key feature of neural operators is theirdiscretization agnostic (resolution invariant) property (Kovachki et al.,2023), which means that a neural operator can learn from data representedon various resolutions and predict outcomes on yet new resolutions. Thisproperty is particularly valuable in applications involving the naturalsciences, PDEs, and complex physical simulations where functional datamay come from variable discretizations of meshes or sensors. Within thefamily of neural operators, Fourier Neural Operator (FNO) (Li et al., 2021) stands out for its quasi-lineartime complexity by defining the integral kernel in Fourier space and has been applied to many problemsin engineering and science (Azizzadenesheli et al., 2024). In the conventional view of neural operators,provided discretization and point evaluation of functions resembles approximation of function for whichincreasing the resolution is connected to improved approximation error in the integration and differentiationoperators (Liu-Schiaffini et al., 2024). The proposed view in the current work provides a new perspectivefor which the point evaluations are instead considered as points samples of functions that are associated",
  "GANODDOVANOOpFlow [Ours]": "Neural Processes. Neural Process (NP) (Garnelo et al., 2018) leverages neural networks to addressthe constraints associated with GPs, particularly the computational demands and the restrictive Gaussianassumptions for priors and posteriors (Dutordoir et al., 2022). While NP aims to model functional distributions,several fundamental flaws suggest NP might not be a good candidate for learning function data (Rahmanet al., 2022a; Dupont et al., 2022). First, NP lacks true generative capability; it focuses on maximizingthe likelihood of observed data points through an amortized inference approach, neglecting the metricspace of the data. This approach can misinterpret functions sampled at different resolutions as distinctfunctions, undermining the NPs ability to learn from diverse function representations. As pointed out inprior works (Rahman et al., 2022a), if a dataset comprises mainly low-resolution functions alongside a singlefunction with a somewhat high resolution, NP only tries to maximize the likelihood for the point evaluation ofthe high-resolution function during training and ignores information from all other lower-resolution functions.A detailed experiment to show the failure of NP for learning simple function data can be found in AppendixA.1 of (Rahman et al., 2022a) . Whats more, NP relies on an encoder to map the input data pairs tofinite-dimensional latent variables, which are assumed to have Gaussian distributions (Garnelo et al., 2018),and then project the finite-dimensional vector to an infinite-dimensional space; this approach results indiminished consistency at elevated resolutions, and thus disables NP to scale to large datasets (Dupont et al.,",
  "Preliminaries": "Universal functional regression. Consider a function space, denoted as U, such that for any u U, u :DU RdU . Here, U represents the space of the data. Let PU be a probability measure defined on U. A finitecollection of observations u|D is a set of point evaluations of function u on a few points in its domain D DU,where D is the collection of co-location positions {xi}li=1, xi DU. Note that in practice, u U is usually notobserved in its entirety over DU, but instead is discretized on a mesh. In UFR, independent and identicallydistributed samples of {uj}mj=1 PU are accessed on the discretization points {Dj}mj=1, constituting a datasetof {uj|Dj}mj=1. The point evaluation of the function draws, i.e., u U, can also be interpreted as a stochasticprocess where the process is discretely observed. The behavior of the stochastic process is characterizedby the finite-dimensional distributions, such that for any discretization D, p(u|D) denotes the probabilitydistribution of points evaluated on a collection D. Whats more, p(uu|D) is p(u given u|D), that is theposterior. Similarly, p(u|Du|D) is the posterior on D collocation points. In the UFR setting, each datasample uj is provided on a specific collection of domain points Dj, and for the sake of simplicity of notation,we consider all the collected domain points to be the same and referred to by D. The main task in UFR is to learn the measure PU from the data {uj|Dj}mj=1 such that, at the inference time,for a given u|D of a new sample u, we aim to find its posterior and the u that maximizes the posterior,",
  "Invertible Normalized Architecture": "We introduce the Neural Operator Flow (OpFlow), an innovative framework that extends the principle of thefinite-dimensional normalizing flow into the realm of infinite-dimensional function spaces. The architecture isshown schematically in . OpFlow retains the invertible structure of normalizing flows, while directlyoperating on function spaces. It allows for exact likelihood estimation for point estimated functions. OpFlowis composed of a sequence of layers, each containing the following parts: Actnorm. Channel-wise normalization with trainable scale and bias parameters is often effective in facilitatingaffine transformations in normalizing flow neural network training (Kingma & Dhariwal, 2018). We implementa function space analog by representing the scale and bias as learnable constant-valued functions. Specifically,let (v)i denote the input function to the ith actnorm layer of OpFlow and let si and bi represent thelearnable constant-valued scale and bias functions, respectively, in this layer. The output of the actnorm layeris then given by vi = si (v)i + bi, where denotes pointwise multiplication in the function space. Domain and codomain partitioning. The physical domain refers to the space in which the data resides,such as the time domain for 1D data or the spatial domain for 2D data. Conversely, the channel domainis defined as the codomain of the function data, such as the 3 channels in RGB images or temperature,velocity vector, pressure, and precipitation in weather forecast (Pathak et al., 2022; Hersbach et al., 2020). Innormalizing flows, a bijective structure requires dividing the input domain into two segments (Dinh et al.,2017; Kingma & Dhariwal, 2018). Based on the method of splitting the data domain, we propose two distinctOpFlow architectures. (i) Domain partitioning: In this architecture, we apply checkerboard masks to the physical domain, a techniquewidely used in existing normalizing flow models (Dinh et al., 2017; Kingma & Dhariwal, 2018) which weextend to continuous domains. Our experiments in the following sections reveal that the domain decomposedOpFlow is efficient and expressive with a minor downside in introducing checkerboard pattern artifacts inzero-shot super-resolution tasks. (ii) Codomain partitioning: This approach partitions the data along the codomain of the input function data,while maintaining the integrity of the entire physical domain. We show that, unlike its domain decomposedcounterpart, the codomain OpFlow does not produce artifacts in zero-shot super-resolution experiments.However, it exhibits a lower level of expressiveness compared to the domain decomposed OpFlow. Affine coupling. Let vi represent the input function data to the ith affine coupling layer of OpFlow. Then,we split vi either along the physical domain or codomain, and we have two halves hi1, hi2 = split(vi). Let T idenote the ith affine coupling layer of OpFlow, where T i is a FNO layer (Li et al., 2021), which ensures themodel is resolution agnostic. Then, we have log(si), bi = T i (hi1), where si and bi are scale and shift functions,respectively, output by the ith affine coupling layer. From here, we update hi2 through (h)i2 = si hi2 + bi. 1We only require homeomorphism between DA and DU where the point alignment is through the underlying homeomorphicmap, and use DA to represent the discretized domain of DA. For simplicity, we consider the case where DU = DA and thetrivial identity map is the homeomorphic map in our experimental study.",
  "Invertible": "operator : Model architecture of OpFlow, OpFlow is composed of a list of invertible operators. For theuniversal function regression task, OpFlow is the learnt prior, which is able to provide exact likehoodestimation for function point evaluation. uobs is the noisy observation, u is the posterior function of interestand u = G(a), where G is the learnt forward operator. Finally, we output the function (v)i+1 for the next layer where (v)i+1 = concat(hi1, (h)i2). The concat(concatenation) operation corresponds to the reversing the split operation, and is either in the physical domainor codomain. Inverse process. The inverse process for the ithe layer of OpFlow is summarized in Algorithm 3. First, wesplit the input function (v)i+1 with hi1, (h)i2 = split((v)i+1). Then the inverse process of the affine couplinglayer also takes hi1 as input, and generates the scale and shift functions log(si), bi = T i (hi1). Thus, bothforward and inverse processes utilize the hi1 as input for the affine coupling layer, which implies that the scaleand shift functions derived from the forward process and inverse process are identical from the affine couplinglayer. Then we can reconstruct hi2 and vi through hi2 = ((h)i2 bi)/si; vi = concat(hi1, hi2). Finally, we havethe inverse of the actnorm layer with (v)i = (vi bi)/si. Similar inverse processes of normalizing flows aredescribed in (Kingma & Dhariwal, 2018; Dinh et al., 2017).",
  "Training": "Since OpFlow is a bijective operator, we only need to learn the inverse mapping F : U A, as theforward operator is immediately available. We train OpFlow in a similar way to other infinite-dimensionalgenerative models (Rahman et al., 2022a; Seidman et al., 2023; Lim et al., 2023) with special addition ofdomain alignment. The space of functions for A is taken to be drawn from a GP, which allows for exactlikelihood estimation and efficient training of OpFlow by minimizing the negative log-likelihood. SinceOpFlow is discretization agnostic (Rahman et al., 2022a; Kovachki et al., 2023), it can be trained on variousdiscretizations of DU and be later applied to a new set of discretizations, and in the limit, to the whole ofDU (Kovachki et al., 2023). We now define the training objective L for model training as follows,",
  "associated with v0, v1, vs as each layer of F is an invertible operator. Vi is the collection of positions forfunction vi with D0V = D, DsV = DA, and each data point has its own discretization D 2": "Although the framework of OpFlow is well-defined, the negative log-likelihood for high-dimensional datamay suffer from instabilities during training, leading to a failure to recover the true distribution. Throughour experiments, we found that relying solely on the likelihood objective in Eq 1 was insufficient for trainingOpFlow successfully. To address this, we introduce a regularization term, to ensure that OpFlow learnsthe true probability measure by helping to stabilize the training process and leading to faster convergence.Such regularization is inspired by the infinite-dimensional Wasserstein loss used in (Rahman et al., 2022a).Unlike the implicit Wasserstein loss provided by the discriminator in GANO, we could potentially have aclosed-form expression of the infinite-dimensional Wasserstein loss in the context of OpFlow. This is due tothe learning process mapping function data onto a well-understood GP. The inherent properties of GaussianProcesses facilitate measuring the distance between the learned probability measure and the true probabilitymeasure, which is explained into detail in the subsequent sections.",
  "AAd22(a1, a2)d, (a1, a2) A A,(2)": "where is a space of joint measures defined on A A such that for any , the margin measures of(a1, a2) on the first and second arguments are PA and FPU, and d2 is a metric induced by the L2 norm.By construction, A GP and if sufficiently trained, FU GP . We can thus further simplify Eq. 2, whichis named as F 2ID score by Rahman et al. (2022b), and is defined as:",
  ")12 ).(3)": "for which PA is chosen to be GP1(m1, K1) and GP2(m2, K2) is the GP fit to the push-forward measureFPU. The m1, K1 are the mean function and covariance operator of GP1(m1, K1), whereas m2, K2 arethe mean function and covariance operator of GP2(m2, K2). We note that it is equivalent to say a functionf GP(m, k) or f GP(m, K), where k is the covariance function and K is the covariance operator (Mallasto& Feragen, 2017). We take Eq. 3 as the regularization term. Furthermore, we inject noise with negligibleamplitude that sampled from GP1 to the training dataset {uj}mj=1. In this way, we can have the inversionmap to be well-define and stable. From here, we can define a second objective function.",
  "K[i, j] = l < Kei, ej >= k(yi, yj)(6)": "2Unlike finite-dimensional normalizing flows on fixed-size and regular grids, OpFlow can provide likelihood estimationlog p(u|D) on arbitrary grids D, which represents a potentially irregular discretization of DU. To compute log p(u|D), we needthe homeomorphic mapping between DA and DU to find DA, corresponding to D. The density function in Eq. 1 holds for newpoints of evaluation. Our implementation of OpFlow incorporates FNO that requires inputs on regular grids. Consequently,although OpFlow is designed to handle functions for irregular grids, our specific implementation of OpFlow only allows forlikelihood estimation on regular girds with arbitrary resolution.",
  "2f)(7)": "and h1, h2 are the matrix forms of means m1, m2 in Eq. 3, K1, K2 are the covariance matrix, correspondingto K1 and K2. 22 is the square of L2 norm and 2f is the square of Frobenius norm. For detailed proof ofthe simplification of the trace of covariance matrix for two Gaussian processes in Eq. 7, please refer to (Givens& Shortt, 1984). Equipped with the above preliminary knowledge, we introduce a two-step strategy fortraining as shown in Algorithm 1. In the first step, we use the objective defined in Eq. 4 with > 0, whereasin the second step, we fine-tune the model by setting = 0 and a smaller learning rate is preferred. Forstep one, even though we have the closed and simplified form of the matrix form of square 2-Wassersteindistance shown in Eq. 7, calculating the differentiable square root of the matrix is still time consumingand may be numerically unstable (Pleiss et al., 2020). Here, we directly calculate K1 K22f instead of",
  "2f, utilizing W 22 = 1": "l (h1 h222 + K1 K22f), and such approximation is acceptable due to thethe regularization term only exists in the warmup phase and is not a tight preparation. In the appendix, weshow an ablation study that demonstrates the need for the regularization in the warmup phase. Furthermore,Eq. 7 implies we normalize the Wasserstein distance and makes sure the square of L2 norm and Frobeniusnorm remain finite as the number of discretization points increases.",
  "Universal Functional Regression with OpFlow": "Bayesian functional regression treats the mapping function itself as a stochastic process, placing a priordistribution over the mapping function space. This allows for predictions with uncertainty estimates throughoutD, most importantly away from the observation points. GPR places a GP prior on the mapping functionspace, a common choice because of mathematical tractability. However, there are many real world datasetsand scenarios that are not well-represented by a GP (Kndap & Godsill, 2022). To address this, we outlinean algorithm for UFR with OpFlow in Bayesian inference problems. In the following we assume that OpFlow has been trained previously and sufficiently approximates the truestochastic process of the data. We thus have an invertible mapping between function spaces U and A. Wenow can use the trained OpFlow as a learned prior distribution over the space of possible mapping functions,fixing the parameters of OpFlow for functional regression.",
  "2r log(2) log p(uobs),(10)": "where p(u|D) denotes the probability of u evaluated on the collection of points D with the learned prior(OpFlow), and r is the cardinality of set D with r = |D|.Maximizing the above equation results inmaximum a posteriori (MAP) estimate of p(u|uobs). Since the last three terms in Eq.10 are constants, suchmaximization is carried using gradient descent focusing only on the first two terms. In the following, we referto u as the MAP estimate given uobs. For sampling for the posterior in Eq. 10, we utilize the fact that OpFlow is a bijective operator and thetarget function u uniquely defines a in A space, therefore, drawing posterior samples of u is equivalent todrawing posterior samples of a in Gaussian space where SGLD with Gaussian random field perturbation isrelevant. Our algorithm for sampling from the p(u|uobs) is described in Algorithm 2. We initialize a|uobswith the MAP estimate of p(u|uobs), i.e., u, and follow Langevin dynamics in A space with Gaussianstructure to sample u. This choice helps to stabilize the functional regression and further reduces the numberof burn-in iterations needed in SGLD. The burn-in time b is chosen to warm up the induced Markov processin SGLD and is hyperparameter to our algorithm (Ahn, 2015). We initialize the SGLD with a0 = F(u)and u0 = G(a0) = u. Then, at each iteration, we draw a latent space posterior sample at in A space,where at is the tth sample of function a, and harvest the true posterior samples ut by calling G(at). Sincep(a|uobs) GP, it is very efficient to sample from it, and we find that relatively little hyperparameteroptimization is needed. Since the sequential samples are correlated, we only store samples every tN time step,resulting in representative samples of the posterior without the need to store all the steps of SGLD (Riabizet al., 2022).",
  "Universal Functional Regression": "Summary of parameters. Here, we show UFR experiments using the domain decomposed implementationof OpFlow, and leave the codomain OpFlow UFR experiments for the appendix. lists the UFRhyperparameters used, as outlined in Algorithm 2. For training OpFlow, we take A GP() with a Materncovariance operator parameterized by length scale l and roughness . The mean function for the GP is set tozero over D. The noise level (2) of observations in Eq. 10 is assumed as 0.01 for all regression tasks. Gaussian Processes. This experiment aims to duplicate the results of classical GPR, where the posterioris known in closed form, but with OpFlow instead. We build a training dataset for U GP, takinglU = 0.5, U = 1.5 and the training dataset contains 30000 realizations. For the latent space GP, we takelA = 0.1, A = 0.5.The resolution of D is set to 128 for the 1D regression experiment. Note that, the OpFlowis not explicitly notified with the mean and covariance of the data GP, and learns them using samples, whilethe GP ground truth is based on known parameters. For performing the regression, we generate a single new realization of U, and select observations at 6 randompositions in D. We then infer the posterior for u across the 128 positions, including noise-free estimationsfor the observed points. displays the observation points and analytical solution for GP regressionfor this example, along with the posterior from OpFlow. In general, the OpFlow solution matches theanalytical solution well; the mean predictions of OpFlow (equivalent to the MAP estimate for GP data) andits assessed uncertainties align closely with the ground truth. This experiment demonstrates OpFlows abilityto accurately capture the posterior distribution, for a scenario where the observations are from a GaussianProcess with an exact solution available. Furthermore, we present the regression results of OpFlow compared with other Deep GP models in AppendixA.4. In this comparison, OpFlow exhibits superior performance over conventional Deep GP approach. Last,we explore the minimal size of the training dataset to train an effective prior. In Appendix A.11, we detailour analysis of the 1D GP regression experiment using OpFlow with reduced training dataset sizes. Fromthis investigation, we conclude that the training dataset containing between 3000 and 6000 samples (10% to20% of the original dataset size) is sufficient to train a robust and effective prior. Truncated Gaussian Processes. Next, we apply OpFlow to a non-Gaussian functional regression problem.Specifically, we use the truncated Gaussian process (TGP) (Swiler et al., 2020) to construct a 1D trainingdataset. The TGP is analogous to the truncated Gaussian distribution, which bounds the functions to haveamplitudes lying in a specific range. For the dataset, we take lU = 0.5 and U = 1.5, and truncate the functionamplitudes to the range [1.2, 1.2]. For the latent space GP, we take lA = 0.1, A = 0.5. The resolution isset to 128. We conduct an experiment with only 4 random observation points, infer the function across all128 positions. Unlike most non-Gaussian processes, where the true posterior is often intractable, the TGPframework, governed by parameters lU, U and known bounds, allows for true posterior estimation usingrejection sampling. We consider a narrow tolerance value of 0.10 around each observation. The tolerancerange can be analogous to but conceptually distinct from noise level (Robert et al., 1999; Swiler et al., 2020).For the training dataset, we use 30000 realizations of the TGP. For the posterior evaluation, we generate1000 TGP posterior samples, and taken the calculated mean and uncertainty from the posterior samples asthe ground truths.",
  "(g)": ": OpFlow regression on GP data with varying sizes of training dataset. (a) Ground truth GPregression with observed data and predicted samples (b) OpFlow regression with observed data and predictedsamples based on the prior trained with 9000 samples. (c) OpFlow regression with observed data andpredicted samples based on the prior trained with 6000 samples. (d) OpFlow regression with observed dataand predicted samples based on the prior trained with 3000 samples. (e) Uncertainty comparison betweentrue GP and OpFlow predictions based on the prior trained with 9000 samples. (f) Uncertainty comparisonbetween true GP and OpFlow predictions based on the prior trained with 6000 samples. (g) Uncertaintycomparison between true GP and OpFlow predictions based on the prior trained with 3000 samples.",
  "Generation Tasks": "While our emphasis in this study is on functional regression, there is still value in evaluating the generativecapabilities of OpFlow. Here, we take GANO as a baseline model as it is a generative neural operator withdiscretization invariance. The following study demonstrates that domain decomposed OpFlow providescomparable generation capabilities similar to prior works, such as GANO. The implementation of GANO usedherein follows that of the paper (Rahman et al., 2022a). In the following experiments, the implementations ofthe GP for the Gaussian space A of GANO is adapted to optimize GANOs performance for each specific case.Unless otherwise specified, the default GP implementation utilize the Gaussian process package availablein Scikit-learn (Pedregosa et al., 2011). More generation experiments of domain decomposed OpFlow andcodomain OpFlow are provided in the appendix. GP data. For both OpFlow and GANO, we set Matern kernel parameters with lU = 0.5, U = 1.5 for theobserved data space. And lA = 0.05, A = 0.5 for the latent Gaussian space of OpFlow, = 1.5, = 1.0for the latent Gaussian space of GANO, where , are the coefficient parameter and inverse length scaleparameter of the specific implementation of Matrn based Gaussian process used in GANO paper (Rahmanet al., 2022a; Nelsen & Stuart, 2021). Data resolution used here is 256. shows that OpFlow cangenerate realistic samples, virtually consistent with the true data. In contrast, GANOs samples exhibit smallhigh frequency artifacts. Both OpFlow and GANO successfully recover the statistics with respect to theautovariance function and histogram of the point-wise value (amplitude distribution). We note that there is",
  "SuzanneAigrainandDanielForeman-Mackey.GaussianProcessRegressionforAstronomicalTime Series.Annual Review of Astronomy and Astrophysics, 61(1):329371, 2023.doi:10.1146/annurev-astro-052920-103508. URL": "Kamyar Azizzadenesheli, Nikola Kovachki, Zongyi Li, Miguel Liu-Schiaffini, Jean Kossaifi, and AnimaAnandkumar. Neural Operators for Accelerating Scientific Simulations and Design, January 2024. URL arXiv:2309.15325 [physics]. Lorenzo Baldassari, Ali Siahkoohi, Josselin Garnier, Knut Solna, and Maarten V. de Hoop. Conditionalscore-based diffusion models for Bayesian inference in infinite dimensions. Advances in Neural InformationProcessing Systems, 36:2426224290, December 2023. URL",
  "Andreas C. Damianou and Neil D. Lawrence. Deep Gaussian Processes, March 2013. URL arXiv:1211.0358 [cs, math, stat]": "Masoumeh Dashti and Andrew M. Stuart.The Bayesian Approach to Inverse Problems.In RogerGhanem, David Higdon, and Houman Owhadi (eds.), Handbook of Uncertainty Quantification, pp. 311428. Springer International Publishing, Cham, 2017. ISBN 978-3-319-12384-4 978-3-319-12385-1. doi:10.1007/978-3-319-12385-1_7. URL Daniel Augusto de Souza, Alexander Nikitin, S. T. John, Magnus Ross, Mauricio A. lvarez, Marc Deisenroth,Joo Paulo Gomes, Diego Mesquita, and Csar Lincoln Mattos. Thin and deep Gaussian processes. Advancesin Neural Information Processing Systems, 36, 2024. URL Volker L. Deringer, Albert P. Bartk, Noam Bernstein, David M. Wilkins, Michele Ceriotti, and GborCsnyi. Gaussian Process Regression for Materials and Molecules. Chemical Reviews, 121(16):1007310141,August 2021. ISSN 0009-2665. doi: 10.1021/acs.chemrev.1c00022. URL Publisher: American Chemical Society.",
  "Takashi Furuya, Michael Puthawala, Matti Lassas, and Maarten V. de Hoop. Globally injective and bijectiveneural operators, June 2023. URL arXiv:2306.03982 [cs, stat]": "Angela Gao, Jorge Castellanos, Yisong Yue, Zachary Ross, and Katherine Bouman. DeepGEM: GeneralizedExpectation-Maximization for Blind Inversion. In Advances in Neural Information Processing Systems,volume 34, pp. 1159211603. Curran Associates, Inc., 2021. URL Jacob R. Gardner, Geoff Pleiss, David Bindel, Kilian Q. Weinberger, and Andrew Gordon Wilson. GPyTorch:Blackbox Matrix-Matrix Gaussian Process Inference with GPU Acceleration, June 2021. URL arXiv:1809.11165 [cs, stat].",
  "Marta Garnelo, Jonathan Schwarz, Dan Rosenbaum, Fabio Viola, Danilo J. Rezende, S. M. Ali Eslami, andYee Whye Teh. Neural Processes, July 2018. URL arXiv:1807.01622[cs, stat]": "Clark R Givens and Rae Michael Shortt. A class of Wasserstein metrics for probability distributions. MichiganMathematical Journal, 31(2):231240, 1984. ISSN 0026-2285. Publisher: University of Michigan, Departmentof Mathematics. Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, AaronCourville, and Yoshua Bengio. Generative Adversarial Networks, June 2014. URL arXiv:1406.2661 [cs, stat]. Paul Hagemann, Sophie Mildenberger, Lars Ruthotto, Gabriele Steidl, and Nicole Tianjiao Yang. MultilevelDiffusion: Infinite Dimensional Score-Based Diffusion Models for Image Generation, March 2023. URL Marton Havasi, Jos Miguel Hernndez-Lobato, and Juan Jos Murillo-Fuentes. Inference in Deep GaussianProcesses using Stochastic Gradient Hamiltonian Monte Carlo, November 2018. URL arXiv:1806.05490 [cs, stat]. Hans Hersbach, Bill Bell, Paul Berrisford, Shoji Hirahara, Andrs Hornyi, Joaqun Muoz-Sabater, JulienNicolas, Carole Peubey, Raluca Radu, and Dinand Schepers. The ERA5 global reanalysis. QuarterlyJournal of the Royal Meteorological Society, 146(730):19992049, 2020. ISSN 0035-9009. Publisher: WileyOnline Library.",
  "Yaman Kndap and Simon Godsill. Non-Gaussian Process Regression, September 2022. URL arXiv:2209.03117 [cs, stat]": "Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew Stuart,and Anima Anandkumar. Neural Operator: Graph Kernel Network for Partial Differential Equations,March 2020. URL arXiv:2003.03485 [cs, math, stat]. Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew Stuart,and Anima Anandkumar. Fourier Neural Operator for Parametric Partial Differential Equations, May2021. URL arXiv:2010.08895 [cs, math]. Jae Hyun Lim, Nikola B. Kovachki, Ricardo Baptista, Christopher Beckham, Kamyar Azizzadenesheli,Jean Kossaifi, Vikram Voleti, Jiaming Song, Karsten Kreis, Jan Kautz, Christopher Pal, Arash Vahdat,and Anima Anandkumar.Score-based Diffusion Models in Function Space, November 2023.URL arXiv:2302.07400 [cs, math, stat]. Haitao Liu, Yew-Soon Ong, Xiaobo Shen, and Jianfei Cai.When Gaussian Process Meets Big Data:A Review of Scalable GPs.IEEE Transactions on Neural Networks and Learning Systems, 31(11):44054423, November 2020. ISSN 2162-237X, 2162-2388. doi: 10.1109/TNNLS.2019.2957109. URL Miguel Liu-Schiaffini, Julius Berner, Boris Bonev, Thorsten Kurth, Kamyar Azizzadenesheli, and AnimaAnandkumar. Neural Operators with Localized Integral and Differential Kernels, February 2024. URL arXiv:2402.16845 [cs, math] version: 1.",
  "Razvan V. Marinescu, Daniel Moyer, and Polina Golland. Bayesian Image Reconstruction using DeepGenerative Models, December 2021. URL arXiv:2012.04567 [cs,eess, stat]": "Juan Maroas, Oliver Hamelijnck, Jeremias Knoblauch, and Theodoros Damoulas. Transforming GaussianProcesses With Normalizing Flows. In Proceedings of The 24th International Conference on ArtificialIntelligence and Statistics, pp. 10811089. PMLR, March 2021. URL ISSN: 2640-3498. Bogdan Mazoure, Thang Doan, Audrey Durand, Joelle Pineau, and R. Devon Hjelm. Leveraging exploration inoff-policy algorithms via normalizing flows. In Proceedings of the Conference on Robot Learning, pp. 430444.PMLR, May 2020. URL ISSN: 2640-3498. Nicholas H. Nelsen and Andrew M. Stuart. The Random Feature Model for Input-Output Maps betweenBanach Spaces. SIAM Journal on Scientific Computing, 43(5):A3212A3243, January 2021. ISSN 1064-8275, 1095-7197. doi: 10.1137/20M133957X. URL arXiv:2005.10224[physics, stat].",
  "Peter Orbanz and Yee Whye Teh. Bayesian Nonparametric Models. Encyclopedia of machine learning, 1:8189, 2010": "George Papamakarios, Eric Nalisnick, Danilo Jimenez Rezende, Shakir Mohamed, and Balaji Lakshmi-narayanan. Normalizing flows for probabilistic modeling and inference. J. Mach. Learn. Res., 22(1):57:261757:2680, January 2021. ISSN 1532-4435. Jaideep Pathak, Shashank Subramanian, Peter Harrington, Sanjeev Raja, Ashesh Chattopadhyay, MortezaMardani, Thorsten Kurth, David Hall, Zongyi Li, Kamyar Azizzadenesheli, Pedram Hassanzadeh, KarthikKashinath, and Animashree Anandkumar. FourCastNet: A Global Data-driven High-resolution WeatherModel using Adaptive Fourier Neural Operators, February 2022. URL [physics]. Fabian Pedregosa, Gal Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel,Mathieu Blondel, Peter Prettenhofer, Ron Weiss, and Vincent Dubourg. Scikit-learn: Machine learningin Python. the Journal of machine Learning research, 12:28252830, 2011. ISSN 1532-4435. Publisher:JMLR. org.",
  "Md Ashiqur Rahman, Manuel A. Florez, Anima Anandkumar, Zachary E. Ross, and Kamyar Azizzadenesheli.Generative Adversarial Neural Operators, October 2022a. URL [cs, math]": "Md Ashiqur Rahman, Jasorsi Ghosh, Hrishikesh Viswanath, Kamyar Azizzadenesheli, and Aniket Bera.PaCMO: Partner Dependent Human Motion Generation in Dyadic Human Activity using Neural Operators,November 2022b. URL arXiv:2211.16210 [cs]. Carl Edward Rasmussen and Christopher K. I. Williams. Gaussian processes for machine learning. Adaptivecomputation and machine learning. MIT Press, Cambridge, Mass, 2006. ISBN 978-0-262-18253-9. OCLC:ocm61285753. Marina Riabiz, Wilson Ye Chen, Jon Cockayne, Pawel Swietach, Steven A. Niederer, Lester Mackey,and Chris. J. Oates. Optimal Thinning of MCMC Output. Journal of the Royal Statistical SocietySeries B: Statistical Methodology, 84(4):10591081, September 2022. ISSN 1369-7412, 1467-9868. doi:10.1111/rssb.12503. URL Gonzalo Rios and Felipe Tobar. Compositionally-Warped Gaussian Processes. Neural Networks, 118:235246,October 2019. ISSN 08936080. doi: 10.1016/j.neunet.2019.06.012. URL arXiv:1906.09665 [cs, eess, stat].",
  "Peter M. Shearer. Introduction to Seismology. Cambridge University Press, May 2019. ISBN 978-1-107-18447-3.Google-Books-ID: 08aVDwAAQBAJ": "Yaozhong Shi, Grigorios Lavrentiadis, Domniki Asimaki, Zach E. Ross, and Kamyar Azizzadenesheli. De-velopment of Synthetic Ground-Motion Records through Generative Adversarial Neural Operators. pp.105113, February 2024a. doi: 10.1061/9780784485316.012. URL Publisher: American Society of Civil Engineers. Yaozhong Shi, Grigorios Lavrentiadis, Domniki Asimaki, Zachary E. Ross, and Kamyar Azizzadenesheli.Broadband Ground-Motion Synthesis via Generative Adversarial Neural Operators: Development andValidation. Bulletin of the Seismological Society of America, March 2024b. ISSN 0037-1106. doi: 10.1785/0120230207. URL",
  "Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole.SCORE-BASED GENERATIVE MODELING THROUGH STOCHASTIC DIFFERENTIAL EQUATIONS.2021": "He Sun and Katherine L. Bouman. Deep Probabilistic Imaging: Uncertainty Quantification and Multi-modalSolution Characterization for Computational Imaging, December 2020. URL arXiv:2010.14462 [astro-ph]. Laura Swiler, Mamikon Gulian, Ari Frankel, Cosmin Safta, and John Jakeman. A Survey of ConstrainedGaussian Process Regression: Approaches and Implementation Challenges. Journal of Machine Learningfor Modeling and Computing, 1(2):119156, 2020. ISSN 2689-3967. doi: 10.1615/JMachLearnModelComput.2020035155. URL arXiv:2006.09319 [cs, math, stat]. Ivan Ustyuzhaninov, Ieva Kazlauskaite, Markus Kaiser, Erik Bodin, Neill Campbell, and Carl HenrikEk. Compositional uncertainty in deep Gaussian processes. In Proceedings of the 36th Conference onUncertainty in Artificial Intelligence (UAI), pp. 480489. PMLR, August 2020. URL ISSN: 2640-3498.",
  "Max Welling and Yee W Teh. Bayesian learning via stochastic gradient Langevin dynamics. pp. 681688.Citeseer, 2011": "Jay Whang, Erik Lindgren, and Alex Dimakis. Composing Normalizing Flows for Inverse Problems. InProceedings of the 38th International Conference on Machine Learning, pp. 1115811169. PMLR, July2021. URL ISSN: 2640-3498. Yan Yang, Angela F. Gao, Jorge C. Castellanos, Zachary E. Ross, Kamyar Azizzadenesheli, and Robert W.Clayton. Seismic Wave Propagation and Inversion with Neural Operators. The Seismic Record, 1(3):126134,November 2021. ISSN 2694-4006. doi: 10.1785/0320210026. URL Qiongyi Zhou, Changde Du, Dan Li, Haibao Wang, Jian K. Liu, and Huiguang He. Neural Encodingand Decoding With a Flow-Based Invertible Generative Model. IEEE Transactions on Cognitive andDevelopmental Systems, 15(2):724736, June 2023. ISSN 2379-8920, 2379-8939. doi: 10.1109/TCDS.2022.3176977. URL",
  "In this part, we first expand the definition of neural operators and elaborate on their discretization-invariantproperties, then explain why OpFlow is also discretization invariant": "Given two separable Banach spaces A and U, a neural operator (denoted as G) learns mappings between twofunction spaces. G : A U, involving input and output functions on bounded domain in real-valued spaces.Thus for an input function a A, we have G(a) = u, where u U. The architecture of neural operators isusually composed of three parts (Kovachki et al., 2023) : (1) Lifting: Converts input functions a : D Rdainto a higher-dimensional space Rdv0 using a local operator, with dv0 > da. (2) Kernel Integration: Fori = 0 to T 1, map hidden state vi to the next vi+1 by a combination of local linear, non-local integraloperator and a bias function. (3) Projection: The final hidden state vT is projected to the output spaceRdu through a local operator, where dvT > du. The lift and projection parts in neural operators are usually shallow neural networks, such as MultilayerPerceptrons. For ith kernel integration layer, we have the integral kernel operator Ki with corresponding kernelfunction ki C(Di+1 Di; Rdvi+1dvi), where vi+1(y), vi(x) are defined on domains Di+1, Di, respectively.Denote i as the measure on Di, Wi as the local linear opeartor, bi as the bias function. Then for the ithkernel integration layer, we have",
  "vi+1 = F1(Ri(Fvi)) + Wivi + bi(12)": "where Ri are learnable parameters defined in Fourier space associated with user-defined Fourier modes, Fand F1 are Fourier transformation and inverse Fourier transformation respectively. The calculation of Eq11,12 for data with arbitrary discretization involves Riemann summation. For more details of calculation ofthe kernel integration, please refer to Kovachki et al. (2023); Li et al. (2021); Rahman et al. (2022a). Furthermore, the formulation of OpFlow is for function spaces, all components and operations invovledin OpFlow are directly defined on function space. The likelihood computation in Eq. 1 can take place",
  "A.4Comparison against Deep GPs on Gaussian Process example": "In this experiment, we evaluate the regression performance of OpFlow against that of variational DeepGP (Salimbeni & Deisenroth, 2017) and Deep Sigma Point Processes (DSSP) (Jankowiak et al., 2020). Wemaintain the same dataset and hyperparameter settings for OpFlow as those used in the Gaussian processregression example discussed in the main text. The only difference in this experiment is that it includes 10observations. For the baseline models, we use official implementations from GPyTorch (Gardner et al., 2021)and present the best available results. For this comparison, we employ the following metrics: (1) Standardized Mean Squared Error (SMSE), whichnormalizes the mean squared error by the variance of the ground truth; and (2) Mean Standardized Log Loss(MSLL), first proposed in Equation 2.34 of Rasmussen & Williams (2006), defined as:",
  "(13)": "where D represents the observation pairs (xobs, yobs), and x, y, y, 2 denote the inquired new positions, testdata, predicted mean and predicted variance, respectively. We collect a test data contains 3000 true posteriorsamples for averaging out SMSE the MSLL. As depicted in and , OpFlow generates realisticposterior samples and achieves lower SMSE and MSLL, demonstrating superior performance over the currentstate-of-the-art Deep GPs.",
  "A.5Valid stochastic processes induced by OpFlow via Kolmogorov extension theorem": "In finite dimensional spaces, normalizing flow is provided to have universal representation for any datadistribution under some mild conditions (Kong & Chaudhuri, 2020; Papamakarios et al., 2021). Followingthe fact that universal approximation of normalizing flow, we expect such universal approximation alsobe generalizable to OpFlow, similar to its finite-dimensional case. However, providing a rigorous proof of",
  "Now, lets verify the consistency in the marginalization. According to Eq.17, the product n+mj=1 | dyj0": "dyj | isseparable and since p(y10 : ym+n0) is a multivariate Gaussian determined from a Gaussian Process, themarginalization as specified in Eq.15 naturally holds, which verifies valid stochastic process can be inducedfrom OpFlow. Last, the Jacobian matrix involved in OpFlow is a triangular matrix, which allows OpFlow tomodel more general stochastic processes compared to the marginal flow used in (Maroas et al., 2021),wherethe Jacobian of transformation is diagonal matrix.",
  "A.7More generation experiments with domain decomposed OpFlow": "GRF data. For both OpFlow and GANO, we take the hyperparameters to be lU = 0.5, U = 1.5 for theobserved space and lA = 0.1, A = 0.5 for the GP defined on latent Gaussian space A. A resolution of 64 64is chosen. illustrates that OpFlow successfully generates realistic samples, closely resembling theground truth in both autocovariance and amplitude distribution. Ground truth 1.0 0.5 0.0 0.5 1.0",
  "A.8Generation with codomain OpFlow": "For the codomain implementation of OpFlow, we will focus on super-resolution tasks, which do not sufferfrom the masking artifacts that occur with the domain decomposed setting. It is often the case that thefunction of interest is multivariate, i.e. the codomain spans more than one dimension. For instance, in thestudy of fluid dynamics, velocity fields are often linked with pressure fields. In these cases, there exists a",
  "A.9Zero-shot super-resolution generation experiments of OpFlow": "Here we present zero-shot super-resolution results with domain-decomposed OpFlow and codomain OpFlow,corresponding to the experiments detailed in the Generation Tasks section. All statistics, such as theautocovariance function and the histogram of point-wise values, were calculated from 5,000 realizations. depicts the domain-decomposed OpFlow, which was trained at a resolution of 256 and then evaluated at aresolution of 512. In (a) and (b), the super-resolution samples from domain-decomposed OpFlowdisplay a jagged pattern and show high discontinuities, Additionally, (c) and (d) show that both theautocovariance function and the histogram function evaluated from the domain-decomposed OpFlow divergefrom the ground truths. We attribute these artifacts to the domain decomposition operation, which splits thedomain into smaller patches and can result in a loss of derivative continuity at the edges of these patches.",
  "(d)": ": Codomain OpFlow super-resolution experiment on generating TGRF data with resolution of128128. (b) Samples from groud truth. (c) Samples from OpFlow. (d) Autocovariance of samples fromOpFlow. (e) Histogram of samples from OpFlow These results underscore the point that while larger datasets can enhance OpFlows performance, comparableperformances are achievable with much smaller datasets. In the context of GP regression with OpFlow,training datasets ranging from 3000 to 6000 samples, which is about 10% to 20% of the original trainingdataset size, should suffice to train a robust and effective prior. Last, its worth exploring using an untrainedOpFlow as a prior for estimating the posterior distribution of specific observations, where no training data isrequired. This approach is known as Deep Probabilistic Imaging (Sun & Bouman, 2020). Utilizing OpFlowin this manner may significantly broaden its applicability, particularly in scenarios where collecting trainingdata is challenging (Aigrain & Foreman-Mackey, 2023).",
  "A.11Size of training dataset for learning a prior": "In this section, we revisit the 1D GP regression experiment with OpFlow to assess the minimal dataset sizerequired to train an effective prior. We reduced the size of the training datasets to be substantially fewerthan the original count of 30000 samples. Specifically, we conducted tests using datasets of 3000, 6000, and9000 samples to examine their impact on the regression performance. As depicted in , the prior trained with 3000 samples showed reasonable performance. Although therewas an observable increase in the error of the mean prediction, the uncertainty was correctly captured. Thisdemonstrates that even a substantially reduced dataset size retains predictive utility. For the dataset of 6000samples, we observed that the regression performance closely aligned with the ground truth and matched theperformance of the prior trained with 30000 samples shown in the main text. This finding highlights that atraining dataset size of 6000 is nearly as effective as one five times larger, significantly reducing data-collectiondemand while maintaining high fidelity in the predictions. Moreover, increasing the dataset to 9000 samplesfurther enhanced both the accuracy and the precision of uncertainty estimation, closely approximating theperformance achieved with the original dataset size of 30000.",
  "A.12Regression with codomain OpFlow": "For the training of the codomain OpFlow prior, we selected hyperparameters lU = 0.5, U = 1.5 for theobserved space and lA = 0.2, A = 1.5 for the Gaussian Process (GP) in the latent space. In our regressiontask, a new realization of u U was generated, with observations taken at six randomly chosen positionswithin the domain D. The goal was to infer the posterior distribution of u across 128 positions. Furtherdetails on regression parameters available in . Additionally, we only show the regression resultsfrom the first channel of OpFlow generated samples. As depicted in , codomain OpFlow accuratelycaptures both the mean and uncertainty of the true posterior, as well as generating realistic posterior samplesthat resemble true posterior samples from GPR. However, as discussed in the previous sections, codomainOpFlow is not as effective as domain-decomposed OpFlow for learning the prior, which highly affects theregression performance and may account for the underestimated uncertainty after x = 0.5 on the x-axis. Thisexperiment confirms codomain OpFlow can also be used for URF, which demonstrates the robustness ofUFR performance across different OpFlow architectures."
}