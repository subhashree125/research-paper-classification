{
  "Abstract": "In this article, we propose a new algorithm for unsupervised anomaly detection in univariatetime series, based on topological data analysis. It relies on delay embeddings and on theextraction of persistent cycles from the 1-dimensional persistent homology constructed fromthe distance to measure Rips filtration. This filtration makes it possible to identify 1-cycles(i.e. loops) corresponding to recurrent patterns by leveraging density information. Pointsin those cycles are considered as normal, and the algorithm can then assign an anomalyscore to any point which is its distance to the normal set. In this paper, we describe thealgorithm, make a theoretical study, and test it on several real-world and synthetic datasets,showing that it is competitive with state-of-the-art anomaly detection methods.",
  "Introduction": "Anomaly detection in time series is an important problem in data science, with applications in many fieldssuch as healthcare (Ansari et al., 2017) and engineering (Woike et al., 2014). A time series is a sequence ofreal numbers x = (x[i])1in (n will always denote the length of the time series). In the context of anomalydetection, x is assumed to be composed of a normal behavior and anomalies, i.e. points or sequences of pointsthat differ from the normal behavior.More precisely, in several application contexts such as industrialmonitoring or healthcare, the time series is usually assumed to be composed of some repetitive/frequentpatterns, possibly of varying lengths (think for instance of an heartbeat in ECG data) among which someoccur a large number of times (the normal ones) and some have significantly fewer occurrences (the abnormalones) : see for an illustration. Over the past years, several unsupervised anomaly detection algorithms have been developed from differentresearch areas (Liu et al., 2008; Breunig et al., 2000; Goldstein & Dengel, 2012; Yeh et al., 2018; Boniol et al.,2021; Aggarwal & Aggarwal, 2017; Sakurada & Yairi, 2014; Malhotra et al., 2015; Li et al., 2007; Muniret al., 2018; Schlkopf et al., 1999) (see Paparrizos et al. (2022) and Schmidl et al. (2022) for a comprehensivereview). Among them, some rely on a model and use the prediction or reconstruction error as an anomalydetector and some are based on clustering or machine learning techniques applied on the subsequences inorder to detect outliers. For instance, LOF (Breunig et al., 2000) transforms the time series into a pointcloud and studies the density of each point to assess whether or not they correspond to normal or abnormalbehaviors. Some methods aim to find subsequences that represent the normal behavior and define anomaliesas subsequences that differ from normality. The fact that the patterns can have different lengths, that therecan be multiple normal patterns and multiple occurrences of an anomaly, and noise make it difficult tobuild a universal anomaly detection algorithm (Paparrizos et al., 2022). The main differences between theapproaches actually lie in the implied definition given to the notion of normality. Topological data analysis (TDA), and more specifically persistent homology (Edelsbrunner & Harer, 2010;Boissonnat et al., 2018) is a set of techniques derived from algebraic topology, which allows to analyze thestructure of data by constructing a sequence of simplicial complexes (a filtration). The persistence diagramsums up when connected components, loops or higher-dimensional simplices appear and disappear whengoing through the filtration. TDA has been applied to many fields (Chazal & Michel, 2021) including timeseries analysis. It is particularly adapted to study structured data such as time series with a periodic behavior",
  "Under review as submission to TMLR": "Jose A Perea, Anastasia Deckard, Steve B Haase, and John Harer.Sw1pers: Sliding windows and 1-persistence scoring; discovering periodicity in gene expression time series data. BMC bioinformatics, 16(1):112, 2015. Mayu Sakurada and Takehisa Yairi. Anomaly detection using autoencoders with nonlinear dimensionalityreduction. In Proceedings of the MLSDA 2014 2nd workshop on machine learning for sensory data analysis,pp. 411, 2014.",
  "Contributions. In this paper, we apply methods from topological data analysis to unsupervised anomalydetection in time series. Our contributions are listed below": "We propose a model of time series that makes it possible to formally define the anomaly detectionproblem. It includes the possibility of having multiple normal or abnormal behaviors, repeatinganomalies and noise. We show examples of real-life time series that fit this model. We present a new method for anomaly detection based on persistent homology. The method uses adelay embedding and the Vietoris-Rips filtration associated to the empirical distance to a measureto find 1-cycles made of dense points and identify them to the normal behavior.",
  "We use a property of this filtration to deal with a subset of the point cloud while keeping informationfrom the whole data in order to significantly decrease computation time": "We use our model to derive an upper bound on the interleaving distance between the filtration usedin our algorithm (with discrete, noisy data with anomalies) and a filtration obtained in an idealsituation with a continuous signal without noise or anomalies. We study the behavior of our algorithm with different parameters and the influence of noise, andshow that it is competitive to state-of-the-art anomaly detection methods on different real-worlddatasets.",
  "with function f, and parameters m, p": "The article is organized as follows. in , we introduce our model of time series and our definitionthe anomaly detection problem, along with the theoretical background required for the rest of the paper. describes our algorithm. contains the theoretical study. In , we present the13 state-of-the-art anomaly detection methods and real-world and synthetic datasets. In , we usethese datasets to study the behavior of our algorithm with different parameters and the influence of noise,compare it to state-of-the-art anomaly detection methods. Note that section 2.2, 2.3 and 4.1 present alreadyexisting objects and results. Every other sections describe new research.",
  "i=1ai i.(1)": "The ni and ai are respectively normal and abnormal atoms of length lni < 1 (resp.lai), i.e.contin-uous functions on with support [0, lni] (resp.[0, lai]) and such that ni(0) = ni(lni) = 0 (resp.ai(0) = ai(lai) = 0).There are Mn normal atoms and Ma abnormal atoms.The i and i are theactivations: binary functions on that take the value 1 only a finite number of times. Thus, ni i is asignal where an occurrence of ni starts at each time t where i[t] = 1. For each i, the number of occurrencesof atom ni (resp. ai) is denoted by kni = ||i||0 (resp. kai = ||i||0), where ||.||0 is the number of non-zerovalues of a function or vector.",
  "With this model, we can formally define the anomaly detection problem": "Definition 1 (Anomaly detection problem). The anomaly detection problem consists in finding the set ofintegers i [1, n] such that xa[i] = 0, which means finding all the discrete abnormal activations and atomlengths. We now make some assumptions related to the model in the context of anomaly detection.The firstassumption states that all the ni i and ai i have disjoint supports, which implies that each point x[t]is either normal or abnormal.Assumption 1. if we denote by (i)1iMn+Ma the list of all the activations i and i, the assumption is:",
  "than mini (kni), which means that anomalies are rare atoms": "Remark 2. Assumption 1 implies that all the atoms start and end at zero. This will be crucialfor our method as we will transform the signal into a curve on which we will look for loops. Theassumption ensures that each pattern corresponds to at least one loop. Assumption 2 corresponds to the situation where a quantity assumed to vary continuously is mea-sured at regular intervals, with noise corresponding to errors or perturbations. Note that the onlyassumption we will make on the noise is that |||| is small compared to the variations of the signal.Thus, it could represent small variations at each occurrence of an atom, of noise due to the dataacquisition process. Moreover, introducing also makes it possible to study the problem without thedisjoint supports assumption as long as the overlap between supports causes a small enough changein the infinity norm of x.",
  "Assumption 3 corresponds to the specificity of the anomaly detection problem (compared, for example,to the pattern detection problem)": "Our model is illustrated on . All the atoms start and end at zero and have disjoint supports. Thegreen and red ones each occur ten times: they are the normal atoms. The blue one occurs twice, the orangeand purple one each occur once. Note that atoms can have significantly different lengths. show two example of time series that fit our model, from real-world datasets included in the TSB-UAD suite.The first one comes from the ECG dataset, which is a standard electrocardiogram datasetwith anomalies that represent ventricular premature contractions. This shows that our model is relevantto study structured activities that present repetitive behaviors over time where anomalies in the real-worldmanifest as anomalies on the signal (here: pathologies cause premature ventricular contractions that appearas anomalies on the ECG). The second one comes from the NAB dataset, which is composed of labeledreal-world and artificial time series including AWS server metrics, online advertisement clicking rates, realtime traffic data, and a collection of Twitter mentions of large publicly-traded companies.",
  "Topological data analysis background": "In this subsection, we introduce the objects and results from TDA (mostly from Anai et al. (2020)) thatwill be used in the rest of the paper. See Boissonnat et al. (2018); Edelsbrunner & Harer (2010); Chazal &Oudot (2008); Chazal et al. (2012; 2014); Anai et al. (2020) for more complete background.",
  "For the rest of this paper, let Rd be endowed with the Euclidean norm || ||": "Definition 2 (Simplicial complexes). A k-simplex on a set X Rd is an unordered tuple = [x0, ..., xk] ofk + 1 distinct elements of X. The elements x0, ..., xk are called the vertices of . If each vertex of a simplex is also a node of , then is called a face of . A simplicial complex K is a set of simplices such that anyface of a simplex of K is a simplex of K. Definition 3 (Filtrations and interleavings). A filtration is a family V = (V)R+ of topological spaces orsimplicial complexes such that V V. For 0, two filtrations V and W are called -interleavedif for all 0, V W+ and W V+. The interleaving pseudo-distance is then defined as",
  "In our algorithm, the sets V will be simplicial complexes in Rd": "In the case of a filtration of simplicial complexes over a finite set, only a finite number of values of correspond to a strict inclusion in the filtration. We call filtration value of a simplex the lowest suchthat V. Definition 4 (Persistence modules). Let K be a field. A persistence module is a family V = (Vb, (vba)0ab)where the Vb are K-vector spaces and the vba are linear maps Va Vb such that for all real numbersa b c, vaa = Id and vba vcb = vca. Definition 5 (Morphisms and interleavings of persistence modules). Let 0. An -morphism between twopersistence modules V and W is a family of linear maps = ( : V W+)R+ such that the followingdiagram commutes for all a b :",
  "Persistent homology": "Applying the (singular or simplicial) homology functor to a filtration and its inclusion maps gives apersistence module of homology groups V = (Hi(V))R+ for each dimension i [0, d]. If for all a b,vba is of finite rank, then V is called q-tame and V can be decomposed into a direct sum of intervalmodules j I[l(j),j), where the [l(j), j) are left closed, right open intervals and each I[l(j),j) is a basicpersistence module that represent a feature that appears at index l(j) and disappears at index j (seeChazal et al. (2012) for more details).In this case, the persistence diagram of V can be defined.Thepersistence diagram Diag(V) of a i-dimensional persistent homology module V is a multiset {(l(j), j)},such that a new homology class that appears in Hi(Vl(j)) disappears in Hi(Vj) (l(j) is called its birthdate and j its death date, if the component never dies, j = ). We call j l(j) the persistence ofthe point. By convention and to be able to define the bottleneck distance, we include every points (, )to the diagram, with infinite multiplicity. 0-dimensional homology classes are connected components, and1-dimensional classes are loops.",
  "where (D, D) is the set of bijections from D to D, and where ||(, ) (, )|| = | | when = = +": "Note that if two filtrations are -interleaved, then the corresponding persistence modules are also-interleaved.Moreover,theisometrytheorem(Chazaletal.,2012)statesthatdi(V, W)=db(Diag(V), Diag(W)) for q-tame modules V and W. We will implicitly use this in , as a bound onthe interleaving distance between filtrations induces a bound on the bottleneck distance between persistencediagrams, which are the objects studied by our algorithm.",
  "j=1Bf(xj, ) =": "The persistent nerve theorem (Chazal & Oudot (2008), Lemma 3.4) states that the persistence (singular)homology module associated to the weighted ech filtration is isomorphic to the persistence (simplicial)homology module associated to its nerve.The latter is computable in practice (Edelsbrunner & Harer,2010; Boissonnat et al., 2018). Definition 8. With the above notations, the weighted Vietoris-Rips filtration with parameters (X, f, p),Rips[X, f, p] is the flag complex of N(Cech[X, f, p]), i.e. the filtered simplicial complex such that each vertex(or 0-simplex) x X has filtration value f(x), and such that for 2 i d + 1:",
  "[x1, . . . , xi] Rips[X, f, p] (j, k), Bf(xj, ) Bf(xk, ) =": "The Vietoris-Rips complex is easier to compute than the ech complex in practice because one only needsto know the filtration values of the 0 and 1 simplices to characterize the whole filtration. The followingproposition gives this value in the case p = 1, which we will use in this paper (see Anai et al. (2020) forvalues when p = 2 or p = ). Proposition 1 (Anai et al. (2020)). Let x, y X. The filtration value of [x] in Rips[X, f, 1] is f(x), andthe filtration value of [x, y] in Rips[X, f, 1] is: max(f(x), f(y))if ||x y|| < |f(x) f(y)|||xy||+f(x)+f(y)",
  "n1 for signals. When there is no ambiguity, we use S and S instead of Sd, and Sd,": "In the following sections, we will use the embeddings X, Xn, Xa of the signals x, xn, xa, the embeddingsX, Xn, Xa of the time series x, xn, xa, and their noisy versions (Y = X + E being the notation for theembedding of the noisy signal y = x + ). shows a delay embedding of the time series from . Each colored loop on corresponds to the atom of the same color on . The presenceof noise makes loops corresponding to normal atoms (in green and red) thicker, as these atoms have moreoccurrences.",
  "Method": "In this section, we describe our algorithm for unsupervised anomaly detection. We start by explaining themain ideas behind the algorithm, and then describe its four steps: compute the DTM Rips filtration of asubset of the delay embedding, identify the normal 1-cycles, extract them, and compute the anomaly scores.We use the above notations for all parameters and mathematical objects.",
  "Motivation": "The solution to the anomaly detection problem described in is a binary vector of length n: eachpoint y[i] of the time series is either normal or abnormal.Our algorithm, like most anomaly detectionalgorithms (Paparrizos et al., 2022; Schmidl et al., 2022), works by giving an anomaly score to each y[i]which should be high if the point is abnormal.A binary answer can then be obtained by choosing athreshold over which points are considered abnormal.We do not give a specific method to choose thethreshold, as it depends on the application. Our algorithm works by studying a delay embedding Y of y to identify a set of points as normal. Then,each point of Y is given an anomaly score which is its distance to the set of normal points. Finally, we getan anomaly score for y[i] by averaging the scores of all points in Y of which y[i] is a coordinate. The idea behind our algorithm is that, if we consider our model of signals from and if (d 1) issmall enough, Assumption 1 implies that the embedding of each occurrence of atom should starts and endsat (0, . . . , 0), so it has at least one loop. 1-dimensional persistent homology can be used to detect thoseloops in Y as homology classes of 1-cycles. Each point from a normal atom ni will have kni occurrencesand each point from an abnormal atom ai will have kai occurrences, so if max(kai) < q min(kni) (seeAssumption 3), then d Y ,q",
  "as their birth date will be different. Our set of normal points is then the set of 1-cycles detected as normal": "Remark 3. One could argue that using 1-cycles is useless because points could be studied individually usingtheir filtration value (i.e. the birth dates on the 0-dimensional persistence diagram) which is the oppositeof a measure of the density of Y around each point.This approach would then be similar to the LOFalgorithm (Breunig et al., 2000). However, in practice, data do not fit our ideal model (there can be noise,differences between occurrences of the same atoms or non-disjoint supports) and there is no way to be certainthat max(kai) < q min(kni) so the filtration values can take a range of values and choosing a thresholdwould be hard. Moreover, an abnormal atom with slow variations will give dense points in Y which wouldmake the 0D approach or LOF fail. Considering 1D persistent homology and considering only cycles with ahigh persistence makes the choice of normal points easier by focusing on a few cycles (and thus a few birthdates) corresponding to structured components of Y . It also makes it possible to eliminate atoms with slowvariations (because the corresponding cycles have low persistence).",
  "Delay embedding, subsampling and DTM Rips filtration": "We start by choosing a dimension d and a delay and computing the delay embedding Y = Yd,. As pointclouds can be very large (they have n (d 1) points), computing persistent homology can be too long forthe algorithm to be used in practice (O(n3) in the worst case for 1D persistent homology and cycle extraction",
  "Card( Y ) ]0, 1[ and compute the": "filtration V = Rips[ Y , d Y ,m, 1] with p = 1. We chose to always use p = 1 because among the three possiblevalues for which Anai et al. (2020) provides explicit formulas for the filtration values (1, 2, and ), p = 1gives the strongest theoretical guarantees (see section 4) and the highest value to edges, which can givemore persistent cycles that are easier to identify on the diagram.",
  "Identifying normal cycles": "The second step consists in identifying normal 1-cycles by reading the persistence diagram, and extractingthose cycles. Let Diag(V) be the persistence diagram corresponding to the filtration V . To identify normalcycles, we propose an algorithm that relies on the choice of two thresholds: the persistence threshold (wefocus on the most persistent points, which describe important structures), and the birth date threshold(among those points, we consider those with a birth date above the cycle to be abnormal). To choose the persistence threshold, we sort the persistence of all points by decreasing order in a list L,find the index i such that L[i] L[i + 1] is maximal, and keep points corresponding to indices from 1 toi+ndiag, where ndiag is a chosen parameter. See Algorithm 1 for a formal description. We will use ndiag 2in practice, to keep at least three points and thus to be able to compare at least two differences in birth dates.",
  ": return L[ithr+ndiag]+L[ithr+ndiag+1]": "We choose the birth date threshold as follows: we take the points that are above the persistence thresholdand the point with minimal birth date, and sort their birth dates by increasing value in a list L (indicesstart at 1). Let idiff be the index such that the birth date difference is maximal and ipers the index of themost persistent point. If ipers idiff, then the birth date threshold is set to the birth date of L[idiff](this is the typical case where we look for the highest difference). If idiff = 1, we consider that all thepersistent points are close from one another and are all normal cycles, so we set the threshold to +. If1 < idiff < ipers, we set the threshold to the birth date of L[ipers], assuming that the most persistent cycleis always normal. See Algorithm 2 for a formal description.",
  ": end if": ": Left: persistence diagram of the DTM-filtration of the delay embedding from , withpersistence and birth date thresholds in blue.. Right: subsampling of the delay embedding with 200 points,and cycles detected as normal (in green and red). illustrates our algorithm applied to the time series from (with delay embedding from). We used q = 10, ndiag = 2 and npoints = 200. On this example, the birth date threshold isaround 2.5, so the two points in the upper left corner are detected as normal (among the 5 most persistentones).",
  "In practice, we will compute persistent homology with coefficients in K = Z/2Z. In that case, the followingmatrix reduction algorithm from Edelsbrunner et al. (2002) can be used": "Let us write our finite filtered simplicial complex as V = = V0 V1 VN such that Vi+1 = Vi i+1(where i+1 is a simplex), and let be the N N matrix of the boundary operator (i,j = 1 if i is a faceof j), with columns C1 . . . CN. Let low(Cj) be the greatest index i such that Cj[i] = i,j = 0 (or 0 if thecolumn only has zeros). The matrix reduction algorithm is described in Algorithm 3. The representativecycle corresponding to a point (l(j), j) on the persistence diagram is Ni=1 Cj[i]i.",
  "Finally, we get an anomaly score for yi by averaging the scores of all points in Y of which yi is a coordinate:Score(y)i mean({d( Yj, Lcycles) | max(0, i (d 1)) j i})": "One can choose a threshold to the anomaly score to get a binary answer. Typically, one can chose to keepscore only above a certain quantile. In , we will compare algorithms using the AUC-ROC curveobtained by varying the threshold from 0 to 1 in order not to be biased by an arbitrary choice of threshold.We do not give a specific method to choose the threshold, as it depends on the application (in , we will compare algorithms using the AUC-ROC curve of each anomaly score not to be biased by anarbitrary choice of threshold). shows the results and ROC curve of our algorithm applied to thesignal from (with delay embedding from and normal cycles from ). The two cyclescorresponding to the normal patterns have been extracted so those patterns have an anomaly score close tozero.",
  "Theoretical study": "In this section, we recall stability theorems for DTM filtrations, define a probability measure on delayembeddings of signals and use it (and some additional assumptions) to derive an upper bound on theinterleaving distance between the filtration used in our algorithm (with discrete, noisy data with anomalies)and a filtration obtained in an ideal situation with a continuous signal without noise or anomalies.",
  "Upper bound on the interleaving distance between the real and ideal filtrations": "The idea of our study is to define a filtration of Xn, which represents the ideal case of continuous datawithout any anomaly nor noise, and to show that this filtration is close in terms of interleaving distance tothe filtration V [ Y , d Y ,m] constructed from the observed data Y = X + E and associated empirical measure.As explained in , a bound on the interleaving distance between filtration also induces a bound on theinterleaving distance between the associated persistence modules and on the bottleneck distance between theassociated persistence diagrams, which we use in our algorithm to identify normal cycles. So our algorithmshould find cycles that approximate the true normal set Xn. We start by making additional assumptionsand defining a probability measure Xn on Xn, and use the DTM filtration V [Xn, dXn,m] as our filtrationof Xn to state our main result.",
  "with bnn+ 0": "Notice that the sampling Mai=1 kailai is the proportion or the signal that is abnormal, so it can be assumedto be small in the context of anomaly detection. So with a high sampling frequency n, a low noise amplitude||||, and if normal atoms occur many times, the bound should be small (after choosing m not too close tozero).",
  "We then conclude using the triangular inequality": "Remark 7. We now make an additional remark regarding the optional subsampling step in our algorithm.Let Y be any finite point cloud in Rd. This steps consists in computing the filtration on Y , which is a subsetof Y , but we still use the DTM function d Y ,m so Proposition 3 gives:",
  "q min(kni), then for all normal point y Y , d X,m(y)": "d(bn + ||||). So all normal points are dense,and for each abnormal point, at least q max(kai) neighbors which are considered to compute their DTM donot correspond to the same point from a distinct occurrence. This does not guarantee that abnormal pointhave a lower density as a single point can appear several times in one occurrence, or the variations can bevery slow. So our algorithm can fail to detect an anomaly (even with a small kai) if its variations are slowbut still forms a large enough 1-cycle to be considered significant. This could especially happen with a largeatom length lai, which reflects in our model as we approximate a measure X such that X(Ai \\ 0) = kailai.",
  "W2(X, Xn) + c(Xn, m)": "Let us first give an upper bound on W2(X, Xn). Let T : X Xn be equal to the identity function on Xnand to the null function on X \\Xn. Recall that X = XnXa and that by assumption X(Xn(Xa \\0)) = 0.Also notice that X = Xn on Xn \\ 0.",
  "Xnf(x)dXn(x)": "The fourth equality comes from the fact that Mni=1 knilni and Mai=1 kailai are respectively the amount oftime taken by normal and abnormal atoms (the rest of the time is spent at 0), so X(0) = 1 Mni=1 knilni Mai=1 kailai and X(Xa \\ 0) = Mai=1 kailai. The fifth equality comes from the fact that when we use Xn,we replace all the abnormal atoms with 0, so Xn(0) = 1 Mni=1 knilni.",
  "Synthetic dataset": "We generated 120 time series following the model described in . Each atom is generated specifyinga random length, choosing a random integer N, computing N points on a Gaussian random walk starting atzero, and interpolating them with cubic splines to get an atom of the desired length, which is then multipliedby a random amplitude factor between 0.5 and 2. Atoms length is set to 1 for punctual anomalies and cantake values between 150 and 400 for normal atoms and sequential anomalies. N is set to 1 for punctualanomalies and can take values between 5 and 15 for normal atoms and sequential anomalies. For each timeseries, each normal atom has at most 15 occurrences, and at least 7 more occurrences than the most frequentanomaly: at least 8 when anomalies do not repeat, and 10 when they can have several occurrences (up to3). Time series have lengths between 1964 and 13234. The dataset can be found at The dataset is divided in six equal parts, depending on the anomaly type of the time series (punctual orsequential), on the presence or absence of multiple normal atoms and on the possibility for anomalies to haveseveral occurrences (two or three). shows the characteristics of each part.",
  "Real-world datasets and methods from TSB-UAD": "Our real-world datasets are the 18 public datasets provided by the TSB-UAD benchmark suite (Paparrizoset al., 2022), for a total of 1980 univariate time series with labeled anomalies. shows the names,sizes (i.e. number of time series), average lengths and data types of 18 datasets. For each time series, we compute the area under the ROC curve (AUC-ROC) obtained by looking at all thepossible thresholds on the anomaly score. The same method is applied to 13 anomaly detection algorithms(Liu et al., 2008; Breunig et al., 2000; Goldstein & Dengel, 2012; Yeh et al., 2018; Boniol et al., 2021;Aggarwal & Aggarwal, 2017; Sakurada & Yairi, 2014; Malhotra et al., 2015; Li et al., 2007; Munir et al.,2018; Schlkopf et al., 1999; Lu et al., 2022) in Paparrizos et al. (2022). provides a succinctdescription of the methods.Using the AUC-ROC, the evaluation does not depend on the choice of athreshold for each algorithm. The datasets and the implementations of the 13 methods can be found at",
  "Benchmark on real-world data": "Here, we show the results of our algorithm on the TSB-UAD benchmark described in .2. Theparameters of our algorithm are chosen the same way for all time series. We estimate a period L for thetime series using the first maximum of the autocorrelation function (we used the find_length function fromTSB-UAD, which was used for all other methods using a delay embedding). We empirically chose = 6(in practice, if is too small, the embedding will stay close to the line spanned by (1, . . . , 1) and it will beharder to detect cycles). In Perea & Harer (2015) Perea and Harer show that in the case of trigonometricfunctions, d should be a multiple of the period to maximize persistence in 1D homology. With this in mind,we empirically set d = max(40, min(120, L",
  ": Average AUC-ROC on each dataset. Results for methods other than TDA come from Paparrizoset al. (2022)": "ically, with a modified version of cyclonysus2). The code can be found at Results forthe other methods come from Paparrizos et al. (2022) except for DAMP, for which we used the implemen-tation from TSB-UAD. shows the average AUC-ROC obtained on each dataset with our method (TDA) with the aboveparameters, and the results of the TSB-UAD benchmark (Paparrizos et al., 2022). shows the criticaldiagrams comparing the average rank of each method using the Friedman test followed by the Wilcoxon orNemenyi test with = 0.05, as described in Demar (2006). These results show that our method is competitive with the state-of-the-art in anomaly detection on 18standard datasets. It has the best score on 4 of them, the best average rank (though the difference with thebest methods is not significant as shown on ), and it is in the top 5 on 13 datasets. We found threereasons to explain what can make our algorithm perform well or not. First, our algorithm was designed forthe problem described in , which makes it appropriate for structured data with repetitive patternsbut datasets can differ from our model in several ways: the absence of a clear normal behavior, a trend, ora lack of continuity (for example binary time series or time series with very fast variations) or too muchnoise can lead to a delay embedding with no relevant normal cycles to detect. Anomalies can also have anature that our algorithm cannot detect, such as a longer pause between two atoms. The second reason isthe choice of parameters. Even though our heuristics to choose the parameters seem relevant (see Sections6.2 and 6.3), it is still not optimal to have the same rule to choose parameters for datasets with differenttypes of data, lengths, sampling rates... The delay embedding parameters d and should not be too low orelse the curve will not have enough space to form persistent cycles, but if they are too high points of thedelay embedding will represent different atoms and the structure will be higher to detect. Moreover, if dis significantly larger than the length of an anomaly, there will be false positive because the algorithm willdetect too many indices as abnormal. We believe that q and ndiag have less influence on performance (seeSections 6.2 and 6.3). The third reason is the quality of the subsampling Y . If the time series is short, orif it has many repetitive patterns, then 400 points can be enough to represent the whole point cloud, butmany time series have more than 100000 points so if they have numerous and/or long atoms then it is notenough and the distance between points will be too low for cycles to persist (this is studied in more detailsin .4.",
  "Influence of the embedding dimension and delay": "Here, we study the influence of the delay embedding parameters d and by looking at the performance ofour algorithm on our synthetic dataset with different pairs of parameters (d, ), while the other parametersare fixed. is a heat map that shows the average AUC-ROC on the whole dataset when d variesbetween 3 and 70 and varies between 1 and 11. We fixed npoints = 100 and ndiag = 2. We set q = n",
  "L,where L is found using the autocorrelation function, as in": "On , there is one zone where the AUC-ROC is above 0.81 and the AUC-ROC decreases as parametersgo away from this zone (below or above), going to the 0.78 0.81 zone then to the 0.75 0.78 zone... untilreaching 0.6 at the top right and bottom left corners. The shape of the zones AUC 0.81, AUC 0.78,and AUC 0.75 and of their frontiers suggest that the best scores are obtained when the product d isconstant. This product represent the time window represented by each point of the delay embedding. Thewindow should be large enough to capture complex patterns but the window should not cover several atoms.In our dataset, atom lengths can have random values between 50 and 400. suggests that d shouldbe approximately between 40 and 80. This empirically confirms the intuition from and Perea &Harer (2015) that the product d should be of the order of the atom lengths. Moreover, an AUC-ROC above0.84 is never reached with d above 30 or below 10, nor with above 8 or below 2. This indicates that eachparameter should not take extreme values. Indeed, for a given value of the product d, if is too small thenthe delay embedding will be concentrated around the line x1 = x2 = = xd, and if it is too high thenpoints on the embedding will not represent the local variations that constitue the shape of the atoms.",
  ": Heat map representing the average AUC-ROC on the whole dataset with different values of d and": "of our algorithm on our synthetic dataset with different pairs of parameters (q, ndiag), while the otherparameters are fixed. shows the average AUC-ROC on the whole dataset when q varies between1 and 140 and ndiag varies between 2 and 16 (as explained in 3.2.2, we always set ndiag 2). We fixednpoints = 100, d = 15 and = 5. On , we can see zones of constant AUC-ROC separated by almost horizontal lines, except for2 ndiag 6 where the zone AUC = 0.92 gets thinner as ndiag decreases.Moreover, the AUC-ROCsignificantly decreases when q gets lower than 20. This suggests that the algorithm is robust to changes ofq and ndiag except for very low values of q (the proposed heuristic makes q proportional to n to avoid thissituation), and that setting ndiag a little higher than 2 can make it easier to reach optimal performance, butconsidering more cycles can make it harder to find the right birth date threshold.",
  "Computation time and influence of the subsampling parameter": "A nice property of our method is the possibility of computing persistent homology on a subset of the pointcloud while keeping density information about the whole point cloud in the filtration.This makes thecomplexity of the algorithm go from O(n3) (Edelsbrunner et al., 2002) to O(n3points + n2), as persistenthomology is computed on a point cloud of size npoints, but the distance matrix of the full points cloudmust be computed. shows the computation time when npoints varies from 50 to 600 (on averageon the 20 first time series of the dataset), on a normal and logarithmic scale (to empirically confirm thetheoretical complexity), along with the AUC-ROC (to study the compromise between computation time andperformance). We set d = 15, = 15, q = 50 and ndiag = 2.",
  ": Heat map representing the average AUC-ROC on the whole dataset with different values of q andndiag": ": Left: computation time (in seconds) as a function of npoints. Middle: computation time as afunction of npoints, logarithmic scale. Right: AUC-ROC as a function of npoints. All quantities are averagedover 20 time series. On , it appears that the AUC-ROC stops increasing after npoints = 200, at a value above 0.95.Computing persistent homology without subsampling (i.e. on thousands of points) would take hours foreach time series, but these examples show that performance can be almost optimal for values of npointssignificantly smaller than n (here, of the order of 10 times smaller, which makes computation about 1000times faster). This study, and the fact that our algorithm performed well with npoints = 400 on time serieswith lengths above 100000 in our benchmark from .1, indicate that the algorithm has a certainscalability as with npoints can increase slower than n.",
  "Robustness to noise and benchmark on synthetic data": "Here, we study the robustness of our algorithm to noise, and compare it to the unsupervised methods fromthe benchmark of whose code is publicly available, and DAMP (Lu et al., 2022). For each signaly = x + , we define the noise ratio as Var() Var(x) (the inverse of the signal-to-noise ratio). shows theaverage AUC-ROC of each algorithm on the whole synthetic dataset, where centered Gaussian noise is addedto each time series, with a noise ratio between 0 and 1. Note that the real benchmark is in .1, weonly make comparisons here to gain insights our algorithm. It should be noted that since the synthetic dataare generated according to the exact model used by our method, the comparison is biased. shows that our algorithm clearly outperforms all the other unsupervised methods up to a noiseratio of 30%, and stays in the top performers for higher noise ratios (it seems that the AUC-ROC of about0.7 corresponds to a points where only easy anomalies are detected, which would explain the fact that itcan be reached by all the algorithms, even with a lot of noise). When looking into more details at our studyon synthetic data, it appears that our algorithm usually performs better than the others when there aremultiple normal atoms or repeating anomalies. However, LOF can outperform it in the case of punctualanomalies, especially when the normal behavior is complex (several atoms or a lot of noise). Also notethat the significant decline in performance of the Matrix Profile and DAMP in the presence of noise can beexplained by the fact that they use a z-normalized Euclidean distance, which can amplify the noise whenlooking at sequences between atoms so those sequences are detected as anomalies and can even hide otheranomalies. Our method gets an AUC-ROC above 0.8 for noise ratios up to approximately 20%. Too much noise can fillthe loops so the main cycles are less persistent and thus more difficult to identify on the persistence diagram.All the other methods except the Matrix Profile and DAMP are very robust to noise. However, it wouldseem more relevant to study those methods on data for which they perform very well to see how much noisemakes them fail.",
  "Conclusion": "This article describes an unsupervised anomaly detection algorithm based on 1D persistent homology. Weproposed a model of time series in which the anomaly detection problem is well defined, which enabledus to create our method and to derive mathematical properties of the method. The algorithm empiricallyproved to be competitive to state-of-the-art anomaly detection methods on different real-world datasets.The subsampling step induces a time/performance compromise that enabled us to significantly decreasecomputation time. Future research could improve compromise by using faster ways of extracting cycles orfinding sparse versions of the filtrations. Another research perspective would be to adapt the algorithmto more general models of time series, for example to deal with the presence of a linear trend, or withmultivariate time series.",
  "Charu C Aggarwal and Charu C Aggarwal. An introduction to outlier analysis. Springer, 2017": "Hirokazu Anai, Frdric Chazal, Marc Glisse, Yuichi Ike, Hiroya Inakoshi, Raphal Tinarrage, and YuheiUmeda.Dtm-based filtrations.In Topological Data Analysis: The Abel Symposium 2018, pp. 3366.Springer, 2020. Sardar Ansari, Negar Farzaneh, Marlena Duda, Kelsey Horan, Hedvig B Andersson, Zachary D Goldberger,Brahmajee K Nallamothu, and Kayvan Najarian. A review of automated methods for detection of my-ocardial ischemia and infarction using electrocardiogram and electronic health records. IEEE reviews inbiomedical engineering, 10:264298, 2017.",
  "Mohsin Munir, Shoaib Ahmed Siddiqui, Andreas Dengel, and Sheraz Ahmed. Deepant: A deep learningapproach for unsupervised anomaly detection in time series. Ieee Access, 7:19912005, 2018": "John Paparrizos, Yuhao Kang, Paul Boniol, Ruey S Tsay, Themis Palpanas, and Michael J Franklin. Tsb-uad:an end-to-end benchmark suite for univariate time-series anomaly detection. Proceedings of the VLDBEndowment, 15(8):16971711, 2022. Vardan Papyan, Jeremias Sulam, and Michael Elad. Working locally thinking globally: Theoretical guaran-tees for convolutional sparse coding. IEEE Transactions on Signal Processing, 65(21):56875701, 2017.",
  "Cdric Villani et al. Optimal transport: old and new, volume 338. Springer, 2009": "Mark Woike, Ali Abdul-Aziz, and Michelle Clem. Structural health monitoring on turbine engines usingmicrowave blade tip clearance sensors. In Smart Sensor Phenomena, Technology, Networks, and SystemsIntegration 2014, volume 9062, pp. 167180. SPIE, 2014. Chin-Chia Michael Yeh, Yan Zhu, Liudmila Ulanova, Nurjahan Begum, Yifei Ding, Hoang Anh Dau, ZacharyZimmerman, Diego Furtado Silva, Abdullah Mueen, and Eamonn Keogh. Time series joins, motifs, discordsand shapelets: a unifying view that exploits the matrix profile. Data Mining and Knowledge Discovery,32:83123, 2018."
}