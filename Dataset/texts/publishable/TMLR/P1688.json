{
  "Abstract": "Image captioning systems are unable to generate fine-grained captions as they are trained ondata that is either noisy (alt-text) or generic (human annotations). This is further exacerbatedby maximum likelihood training that encourages generation of frequently occurring phrases.Previous works have tried to address this limitation by fine-tuning captioners with a self-retrieval (SR) reward. However, we find that SR fine-tuning has a tendency to reducecaption faithfulness and even hallucinate. In this work, we circumvent this bottleneck byimproving the MLE initialization of the captioning system and designing a curriculum forthe SR fine-tuning process. To this extent, we present (1) Visual Caption Boosting, a novelframework to instill fine-grainedness in generic image captioning datasets while remaininganchored in human annotations; and (2) BagCurri, a carefully designed training curriculumthat more optimally leverages the contrastive nature of the self-retrieval reward. Jointly,they enable the captioner to describe fine-grained aspects in the image while preservingfaithfulness to ground-truth captions. Our approach outperforms previous work by +8.9% onSR against 99 random distractors (RD100) (Dess et al., 2023); and +7.6% on ImageCoDe. Additionally, existing metrics to evaluate captioning systems fail to reward diversity orevaluate a models fine-grained understanding ability. Our third contribution addressesthis by proposing self-retrieval from the lens of evaluation. We introduce TrueMatch, abenchmark comprising bags of highly similar images that uses SR to assess the captionersability to capture subtle visual distinctions. We evaluate and compare several state-of-the-artopen-source MLLMs on TrueMatch, and find that our SR approach outperforms them all bya significant margin (e.g. +4.8% - 7.1% over Cambrian) while having 1-2 orders of magnitudefewer parameters. We also outperform vanilla SR by +14.4% to +19.5%.",
  "Introduction": "Image captioning, or generating natural language image descriptions, has witnessed remarkable progress overthe last decade. Todays captioning systems are composed of sophisticated deep learning architectures (Mokadyet al., 2021; Stefanini et al., 2022; Dai et al., 2024; Liu et al., 2023) trained on vast datasets (Lin et al., 2014;Sharma et al., 2018; Thomee et al., 2016; Desai et al., 2021; Schuhmann et al., 2022). However, even withthese advances, approaches often generate generic captions that are unable to differentiate similar images (see), violating the fundamental purpose of a caption: to facilitate accurate and efficient communicationof visual content (Fisch et al., 2020; Kreiss et al., 2022; Dess et al., 2023; Das et al., 2017). We attributethe shortcomings of image captioning systems to three key factors: (1) the nature of their training data,(2) captioning evaluation metrics, and (3) the maximum likelihood estimation (MLE) training approach.",
  "a grassy eld next to a green Frisbee, holding it in its mouth and appears to be enjoying playing with it in the grassy eld": ": For similar images, captioning systems struggle to generate meaningful captions that uniquelydescribe each image. In this example, COCO MLE: A model trained on COCO with MLE generates thesame generic caption for the first two images. COCO SR (Dess et al., 2023): While the self-retrieval (SR)objective may help, the COCO captions are not rich enough to generate salient visual details. OUR SR:Our improved data and training recipe results in fine-grained, and therefore discriminant captions. I. Challenges with the training data. Training datasets may be divided into two: curated datasets(COCO (Lin et al., 2014), Flickr30k (Plummer et al., 2015)) or large-scale alt-text data (e.g. CC3M (Sharmaet al., 2018)). While alt-text is noisy and may be unaligned with the image, human-annotated captions(COCO) may be generic (Kornblith et al., 2023) and lack world knowledge (Bavishi et al., 2023) as annotatorsdescribe visual concepts in a simplistic manner (e.g. labeling a Golden Retriever as a dog). Recently,foundation models are being used to enhance large-scale alt-text data by making them denser (Doveh et al.,2024; Urbanek et al., 2024). However, such methods are prone to inherit biases (e.g. gender, geography)present in foundation models (Hall et al., 2023; Sirotkin et al., 2022; Basu et al., 2023; Salman et al., 2022)and exhibit verbose language modeling priors (Liu et al., 2024). To address these data inadequacies, we propose Visual Caption Boosting (VCB), a model agnostic frameworkdesigned to generate dense captions that holistically capture different aspects of the image (objects, attributes,relations, scene, etc.) while remaining anchored in human annotations (see ). In brief, multiplehuman annotated captions are blended together using a Large Language Model (LLM) and expanded withan image description generated from a Multimodal Large Language Model (MLLM). In case of conflictingvisual details, we prompt the LLM to prefer the blended caption over the description from the MLLM. Thus,VCB creates fine-grained captions that are grounded in human annotations, enabling rich and informativedatasets to train image captioning systems. II. Image Caption Evaluation.Current metrics can be broadly classified into reference-based andreference-free. Reference-based metrics such as BLEU (Papineni et al., 2002), CIDEr (Vedantam et al., 2015),and SPICE (Anderson et al., 2016) rely on comparisons to ground-truth (GT) captions that may not includeimage details beyond salient objects. Although these metrics evaluate grammatical correctness, they tend topenalize captions that are more specific than the ground-truth, often favoring generic descriptions (Wang et al.,2020). Reference-free metrics like CLIPScore (Hessel et al., 2021) alleviate this issue by directly measuringimage-text similarity, but may fail to assess discriminativeness of the captions. These limitations necessitatethe development of new evaluation strategies that incentivize the models to produce fine-grained captions. In this work, we ask what makes a fine-grained or good description when evaluating captioning systems?We posit that a caption should be succinct, but enable a listener to identify the target image from a bagof images with similar visual elements (Liu et al., 2018). To this extent, we propose to evaluate captioningsystems through the lens of self-retrieval (SR). We measure the ability to retrieve the target image based onthe generated caption within a bag of highly similar distractor images.",
  "Published in Transactions on Machine Learning Research (01/2025)": "smaller learning rates. This results in an extremely slow training process. Notably, using Reinforce fromscratch is infeasbile due to the vast action space of sampling a token and reward sparsity. However, using apretrained MLE model provides a good initial sampling strategy and requires relatively few optimizationsteps to perform well on SR. This is consistent with previous works such as Rennie et al. (2017). In Reinforcement Learning jargon, the gradients computed with Reinforce are used directly to optimize thepolicy network (captioning system) in prioritizing actions (next-token prediction) that minimize the negativeexpected reward EcP(|i)[R(c, i, D)].",
  "Several zebras are grazing and standing near a watering hole in the wild, with a bird flying overhead": "In this image, there is a group of zebras standing near a body of water, such as a river or a lake. The zebras appear to be grazing or drinking from the water. There is also a bird flying overhead, which adds to the natural setting of the image. The zebras and the bird seem to be coexisting peacefully in their respective habitats",
  "Visual Caption": ": Example of Visual Caption Boosting transforming the original human annotated captions (left) to aHolistic Caption. First, an LLM blends the human annotations to create a Blended Caption. Next, an MLLMgenerates a dense visual caption that may be noisy. Finally, we create a Holistic Caption by instructing theLLM to incorporate fine-grained details from the Visual Caption with the Blended Caption, while stayinganchored in human annotations in case of conflicts. Specific prompts are shared in Appendix A.1. The colorsindicate various concepts extracted from the human annotations or the visual caption. The red underlinedtext (illustrated by us for ease of understanding) indicating hallucinations or verbose text, is ignored in theHolistic Caption as we anchor it to human annotations.",
  "Improving Datasets through Visual Caption Boosting": "Training with dense ground-truth descriptions benefits vision-language models (Doveh et al., 2024; Urbaneket al., 2024; Lai et al., 2023). Although existing image captioning datasets like COCO (Lin et al., 2014)provide multiple annotations per image, they do not elicit detailed captions. This leads to generic annotationssuch as There is a herd of zebras standing around (see ) that ignore finer visual concepts and failto describe the image holistically. This bottleneck reflects in trained captioning systems as well. To address this, recent works leverage foundation models to synthetically expand visual information withincaptions (Doveh et al., 2024; Li et al., 2024; Singla et al., 2024; Lai et al., 2023; Jiao et al., 2024), instillinga wider range of visual details. However these methods are prone to inherit biases present in foundationmodels (Sirotkin et al., 2022; Basu et al., 2023; Salman et al., 2022). Moreover, the descriptions generated bysome of these methods are excessively long making them susceptible to hallucinations (Favero et al., 2024).They also pose challenges to training captioning systems as they exceed the token capacity of current VLMs(e.g. CLIP with 77 tokens (Urbanek et al., 2024)). Nevertheless, while individual COCO captions are sparse,we find that they describe complementary facets of the image e.g. watering hole, bird flying over, herdof zebras (). This also corroborates findings of Ye et al. (2023) that annotators of different culturalbackgrounds describe distinct visual concepts when viewing the same image. Building upon these findings, we introduce Visual Caption Boosting (VCB), a novel two-stage approach toenrich the training data with dense, more informative captions that encourage captioning systems to learnand generate rich descriptions. VCB leverages foundation models and the diverse perspectives offered byhuman annotators to generate rich descriptions while being anchored in human annotations. Step 1. BlendCap leverages an off-the-shelf LLM to create a blended caption that combines multiplefacets of visual information that the human annotators describe. shows an example of how a fewcaptions can be blended together into a comprehensive description of the image using our method. Notably,we prompt the LLM to minimize redundant information resulting in succinct descriptions.",
  "TrueMatch: Fine-grained Evaluation through Self-Retrieval": "Existing self-retrieval (SR) approaches require models to select the target image from a set of N randomdistractor images in the dataset (Dess et al., 2023). However, randomly chosen distractors often have simpledifferences (e.g. the primary object or scene), making it easy for captioning systems to distinguish betweenthem. This evaluation is suboptimal as they neither encourage the model to generate detailed captions nordo they evaluate fine-grained abilities of captioning systems. In this section, we present the SR setup used in our work. We propose TrueMatch, a benchmark of carefullycurated bags of highly similar images that enables SR to evaluate whether captioning systems capture differentfacets of fine-grained visual discrimination. The results in .2, show that most captioning systems(including MLLMs) struggle to generate captions that would allow distinguishing fine-grained visual details. Self-retrieval setup. Within a bag of images B (or a minibatch during training), we require the generatedcaption c for an image i to retrieve itself (image i) from B. Note, B contains i and a set of visually similardistractor images D. The caption c and all images in B are encoded using CLIP text and vision encodersrespectively and ranking is performed by computing the cosine similarity sim(c, i) in the CLIP embeddingspace. Consequently, the caption is deemed to be good sim(c, i) > sim(c, i), i D.",
  "Benchmark Creation": "Given a dataset, the benchmark creation process involves curating bags of highly similar images. We use the10,000 images from COCOs validation and test sets (Karpathy & Fei-Fei, 2015) for our benchmark. Creating candidate bags of images. We use the fine-grained descriptions generated through HolisticCapand encode visual and textual features in the CLIP embedding space. We treat each multimodal embedding(concatenation of the two modalities) as a query and use a simple nearest neighbour search to create bagsof highly similar images. Algorithm 1 provides details on the bag creation process, especially for creatingbags of variable size. Having bags of varying size facilitates SR evaluation at multiple levels of difficulty asincreasing the bag size limits the number visual concepts that can be used to uniquely describe each image.This is different from previous works that typically use image pairs (Jhamtani & Berg-Kirkpatrick, 2018;Park et al., 2019; Tong et al., 2024b) or random distractors (Dess et al., 2023) to evaluate captioners. Automated bag curation. We compute intra-bag similarity , an average pairwise cosine-similaritybetween multimodal embeddings of all images in the bag to quantify the difficulty of uniquely describingeach image in the bag (details in Algorithm 2). To create TrueMatch, we sort the list of bags created",
  "Approaches lack fine-grained details. Cap-tioning models, irrespective of their size, strug-gle to capture fine-grained visual details leadingto poor performance on TrueMatch": "Doing more with less.Although billion-parameter MLLMs have achieved impressiveresults (Liu et al., 2023; Tong et al., 2024a), Ta-ble 1 demonstrates that they still struggle withfine-grained visual discrimination. In fact Dis-criTune (Dess et al., 2023), that trains ClipCapwith the vanilla SR setup matches InstructBLIPdespite being two orders of magnitude smaller.Cambrian-1 is the best-performing open-sourcemodel and although it surpasses DiscriTune,our proposed approach outperforms it by a significant margin. This demonstrates the effectiveness of both:TrueMatch for evaluating captioning systems, and our approach of using SR to improve captioning ().",
  "COCO80.926.451.141.339.6VisualCap86.932.253.742.742.1BlendCap88.932.653.644.843.4HolisticCap 91.333.457.5 48.1 49.1": "We evaluate the quality of captions adopted in VCB in .Along with TrueMatch, we also adopt the SR evaluation strat-egy of Dess et al. (2023) with 99 random distractors (RD100).On RD100, BlendCap outperforms original COCO captionsby a large margin of 8% on R@1 and 6.2% on ClipScore,confirming that human annotations capture complementaryvisual aspects of the same image. However, this sizeable gapshrinks to 2.5% on TrueMatch (bags of size #3). This con-firms that even though annotated captions blended togetherwork better, they inherently lack fine-grained details (see Fig-ure 2) necessary to perform well on TrueMatch. HolisticCap,on the other hand, yields remarkable performance gains overCOCO and BlendCap across both setups: RD100 and all bag sizes of TrueMatch. This demonstrates theeffectiveness of VCB in instilling fine-grained information into standard image captioning datasets. We study the effectiveness of anchoring visual captions in producing more succinct (see Appendix A.3) andfine-grained captions. shows that standalone VisualCap (generated using the MLLM) is slightlyworse than BlendCap. However, anchoring VisualCap in human annotations to create HolisticCap results",
  "Improving Captioning Systems with Self-Retrieval as a Training Objective": "In this section, we present key insights on improving training with self-retrieval (SR), resulting in significantperformance gains over previous works (Dess et al., 2023). We begin by outlining the method (.1)and experimental setup (.2). Our experiments reveal that captioners trained with SR are highlysensitive to their MLE initialization (.3). In fact, we discover a trade-off between caption faithfulnessand retrieval performance plaguing captioning systems fine-tuned with previous SR approaches (Dess et al.,2023; Liu et al., 2018). To address this we: (1) initialize our model with more detailed captions fromHolisticCap, (2) fine-tune the visual encoder with SR (.5), and (3) mine hard training bags anddesign a curriculum over bag sizes (.6). Finally we show that our training strategy is complementaryto CIDEr optimization (Rennie et al., 2017) resulting in further improvements (.8).",
  "Methodology": "Model architecture. Similar to Dess et al. (2023), we adopt ClipCap (Mokady et al., 2021), a lightweightsimplification of modern MLLMs (e.g. LLaVA, InstructBLIP, Cambrian). ClipCap connects a pretrainedvisual encoder (CLIP (Radford et al., 2021)) to a pretrained language model (GPT-2 (Radford et al., 2019))through a simple MLP adapter. The adapter is tasked with mapping the rich visual embeddings from CLIPinto a fixed number of prefix tokens. These tokens capture essential visual information and guide the languagemodel towards generating an image-conditioned caption. Training this captioning system has two steps: (1) model pretraining with maximum likelihood estimation(MLE), and (2) model fine-tuning by maximizing the SR reward with Reinforce (Williams, 1992). Weoutline both training objectives, followed by an experimental investigation of their properties and limitations. Maximum Likelihood pretraining models the training data distribution. Specifically, captioning modelsare often teacher-forced to learn the word (token) distribution that maximizes the log-likelihood of theground-truth captions given an input image. However, this results in strong language modeling priorsresulting in generic captions that use a small vocabulary (see .7). Maximizing self-retrieval with Reinforce. A fine-tuning step that optimizes a metric (e.g. CIDEr) ispopular in training captioners (Rennie et al., 2017). We adopt Dess et al. (2023)s contrastive formulation ofSR that compares the generated caption against distractors, providing an optimal learning signal:",
  "Experiment 3: Improving SR with Richer MLE Initialization": "Following the findings of .3, we use TrueMatch to investigate the impact of different MLE initializationson SR fine-tuning in . We study SR in the same setting as previous work (Dess et al., 2023): (1) weonly fine-tune the language model (SR-L), but using LoRA adapters; and (2) until .6 we use 99random distractors to fine-tune with the SR reward.",
  "COCO108.283.730.253.342.338.45BlendCapSR-L80.187.531.754.747.544.26 HolisticCap27.887.631.959.1 47.7 46.5": "MLE pretraining yields genericcaptions. rows 1-3 reportthe performance of MLE trainingon captioning datasets with increas-ing level of detail. We observe thatimproving the training captions re-sults in a significant performanceboost.Training with HolisticCapbeats COCO by +8.2% on RD100R@1 and +5.1% on TrueMatch #3.However, comparing performanceagainst the ground-truth Holistic-Cap directly (from ), we seethat MLE training on HolisticCap yields -6.6% on RD100 R@1 and -1.6% on TrueMatch #3. This indicatesthat even though HolisticCap improves performance, MLE training (i) generates generic descriptions thatmay not be aligned with the image content, and (ii) leads to object hallucinations (see Appendix C.7). Self-retrieval fine-tuning benefits from VCB. SR-L reduces MLEs preference towards statisticallyprobable phrases resulting in more discriminant captions, reduced object hallucinations and consistentimprovements in RD100 and TrueMatch across datasets (compare rows 1-4, 2-5, 3-6). Providing a richerinitialization to SR-L through VCB enhances fine-grained visual discrimination. Row 6 outperforms DiscriTune(row 4) (Dess et al., 2023) with +3.9% RD100 and +5.8% TrueMatch #3. This illustrates the importance ofa good MLE initialization for SR and the effectiveness of VCB. Longer reference sets hurt CIDEr. We see a significant drop in CIDEr scores for BlendCap andHolisticCap compared to COCO in . Upon further analysis, we believe that CIDEr rewards brevity.Appendix C.6 shows some valid short captions that unfortunately obtain scores close to 0. Self-retrieval unlocks latent semantic information when fine-tuning the LLM. Although BlendCapdoes not capture any new information that is not present in one of the COCO annotations, being descriptiveis sufficient to achieve superior retrieval performance against random distractors (BlendCap improves overCOCO by +8% on RD100 R@1, ). Further, MLE pretraining is able to model these data distributionsas the performance gap is maintained at +8.4% on RD100 R@1 between rows 1 and 2 in . Themodel trained on COCO, due to the MLE objective, generates sparse captions that resemble the independentannotations of COCO. Thus, even though the semantic information in the embedding spaces of both models(trained on COCO or BlendCap) are similar, their ability to access this information is bottlenecked by theMLE objective, leading to a large gap between COCO and BlendCap on RD100 in . Interestingly, SR-L fine-tuning narrows this gap dramatically from +8.4% to a mere +3.8% (rows 4, 5). Thisindicates that SR fine-tuning teaches the captioner to uncover semantic information within its embeddingspace, even when it was initially obscured by MLE. Furthermore, the mostly unchanged CIDEr scores hintthat the captioner remains faithful to the GT captions for both COCO (rows 1, 4) and BlendCap (rows 2, 5).",
  "Trade-off plaguing SR fine-tuning: Retrieval Performance vs. Caption Faithfulness": "While SR fine-tuning removes language modeling priors from captioners by making them more discriminant,we observe that they have a tendency to become less faithful to the GT captions upon extended training. Wecontrast these two tendencies of SR fine-tuning in , comparing CIDEr and RD100 R@1 scores. Wetrain for 100 epochs compared to the default 20 epochs used for all other experiments.",
  ": RD100 R@1 continually increases whileCIDEr degrades when fine-tuning ClipCap withSR-L on COCO for 100 epochs": "While SR performance continually improves (+7.9%) withextended training, a corresponding degradation in CIDEr(-15.4%) is observed. This trade-off reveals that SR has thecapacity to elicit better retrieval from the captioning sys-tem, even at the cost of generating lower-quality captions.We further investigate with qualitative examples in Ap-pendix C.8. Dess et al. (2023) attribute this deteriorationto the model diverging from the GT distribution duringSR-L fine-tuning and argue that it is desirable. Whilethis is partially true (see ), our investigation alsoreveals that captioners trained with vanilla SR-L, in adash to enhance retrieval performance, build a propen-sity to hallucinate attributes (see ) resulting inpoor CIDEr scores. This tendency is only exacerbatedwith extended training resulting in further degradationof NLG metrics and increased object hallucinations (seeAppendix C.7). These findings reveal an important bot-tleneck in SR fine-tuning:",
  "Finding 3. Captioners fine-tuned with SR-L suffer from a trade-off between retrieval performanceand faithfulness, often hallucinating details and deviating from GT captions with extended training": "Finally, the continued improvement in RD100 R@1 over 100 epochs indicates that vanilla SR-L fine-tuning failsto saturate SRs capacity to instill fine-grained visual discrimination in captioning systems. This motivatesus to explore better SR fine-tuning strategies while mitigating their propensity to hallucinate. Since a higherlearning rate destabilizes the policy gradients, we turn to: (1) fine-tuning the visual encoder (CLIP) with SR;and (2) adopting a curriculum over bags of hard negatives.",
  "Experiment 4: Fine-tuning CLIP with Self-Retrieval": "We present a comprehensive analysis of fine-tuning language and vision modules of the captioning systemwith LoRA in . Fine-tuning CLIP (SR-V) makes the captions discriminative. For example, withBlendCap MLE pretraining, SR-V fine-tuning results in +2.6% on RD100 R@1 and +6.6% on TrueMatch #3(row 4, 5). However this comes at the cost of deteriorating NLG metrics (-7.7% CIDEr), suggesting that SR-Vis unable to preserve the faithfulness of generated captions. We further verify that even after extended SR-Lfine-tuning to match SR-Vs retrieval scores, SR-L has a higher CIDEr (see Appendix C.4). This suggeststhat CIDEr deteriorates due to CLIP fine-tuning and not due to NLG metrics penalizing discriminativecaptions (Wang et al., 2020). This is a notable bottleneck in fine-tuning CLIP with SR: while it enablessuperior retrieval performance, it makes the captioner less faithful to the GT captions.",
  "HolisticCapSR-LV26.590.7 (3.1) 62.6 (3.5) 52.7 (5.0) 51.2 (4.7)": "RD100issuboptimalfortraining fine-grained caption-ers.On TrueMatch, while SR-V fine-tuning provides substan-tial improvements over SR-L forCOCO and BlendCap (rows 1-2,4-5), we observe relatively smallergains for HolisticCap (rows 7-8).This indicates that the vanilla SR-L objective of retrieving against99 random distractors (RD100) isnot challenging enough to providea good learning signal for modelspretrained on fine-grained Holis-ticCap. While SR-LV does betterthan SR-V on HolisticCap (rows8-9), the gap between BlendCap(row 6) and HolisticCap (row 9) still remains small. To address this, we devise a curriculum with hardnegatives, discussed in the next section, to increase the difficulty of SR fine-tuning objective.",
  "SR-L-27.887.659.1 47.7 46.56 Holistic-SR-L31.388.558.4 50.0 49.57CapSR-V25.890.664.0 54.6 54.18SR-LV29.791.365.2 57.1 57.1": "The above SR experiments were trained using99 random distractors (Dess et al., 2023). In-stead, in this experiment, we fine-tune withbags of highly similar images within a train-ing minibatch (see Appendix B.2). Retrievingagainst multiple hard negatives flattens the soft-max distribution resulting in a stronger learningsignal. We also propose a curriculum over bagsizes (BagCurri) to more optimally leveragethe contrastive reward (see Appendix B.3). SRfine-tuning with BagCurri instills fine-grainedvisual details in the model, essential for dis-criminating highly similar images. SR-V versus SR-L with our bag curricu-lum. We evaluate the impact of fine-tuningdifferent components of the captioning system (CLIP and GPT-2) with BagCurri compared to vanilla SR-Lin . We find that SR-V fine-tuning with our curriculum forces CLIP to learn fine-grained visualfeatures resulting in a substantial +5% to +11% improvement over vanilla SR-L (rows 3 vs. 1, 7 vs. 5) onTrueMatch. Interestingly, SR-V with BagCurri renders COCO (row 3) comparable to HolisticCap (row7) on TrueMatch #3. However, these disproportionate gains for COCO come at the expense of CIDEr",
  "COCO428689719762770BlendCap12201306134513661435HolisticCap 14311535160616281732": "We evaluate how VCB and SR increase caption diver-sity by measuring the number of words that appear 5times on the COCO test set. In , we observea clear increase in the number of unique words as weprogress from COCO to BlendCap to HolisticCap, acrossall optimization and training strategies. Furthermore,within each individual dataset, there is a substantialincrease in the usage of diverse vocabulary with transi-tions from MLE to SR-L, SR-V, and SR-LV. Notably, SR-LV with BagCurri yields significant improvementsover MLE: relative +80% on COCO, +17.6% on BlendCap, and +21% on HolisticCap. This demonstrates theeffectiveness of VCB and our curriculum in guiding the captioning system away from the language modellingpriors and improving caption diversity.",
  "Experiment 6: Combining Self-Retrieval and CIDEr Optimization": "CIDEr optimization with Reinforce is commonly used to improve captioning systems adherence to the GTcaptions (Rennie et al., 2017). As the final experiment, we present results of combining it with SR in . CIDEr optimization (rows 3, 8) improves faithfulness to GT captions compared to MLE pretraining (rows1, 6): CIDEr improves +10.7% on COCO and +29.1% on HolisticCap. Importantly, CIDEr optimizationon top of HolisticCap MLE pretraining provides +4.8% on TrueMatch #7 (rows 6, 8), while we see only asmaller +2.5% with COCO captions. Even without SR fine-tuning, the larger improvement with HolisticCapemphasizes the importance of MLE initialization with captions containing fine-grained visual details (VCB).Furthermore, compared to vanilla SR-L (rows 2, 7), CIDEr opt. (rows 3, 8) fairs worse on both RD100 andTrueMatch. This presents an opportunity to optimize both rewards simultaneously (Luo et al., 2018).",
  "---26.684.755.943.742.27-SR-L-27.887.659.147.746.58--55.786.957.647.147.09-SR-LV29.791.365.257.157.110 SR-LV38.092.667.7 58.7 57.9": "CIDEr meets SR. Building upon the find-ings of previous sections, we push the bound-aries of SR by explicitly adding CIDEr to ourdiscriminative reward R = SR + CIDEr.We conduct an ablation over various values(see Appendix C.5), and select = 0.5 forour experiments. With HolisticCap, we see that joint opti-mization (row 10) outperforms the most dis-criminant SR model (row 9) achieving im-provements on all metrics: +8.3% on CIDEr,+1.3% on RD100 R@1, and +0.8% to +2.5%on TrueMatch. Conversely, on COCO cap-tions (rows 4, 5), we observe that addingCIDEr optimization adversely affects SR met-rics on TrueMatch with -5.1% to -7.6% per-formance drops. This aligns with previousfindings and underscores the importance ofinitialization during SR fine-tuning.",
  "SR-LVBagCurri37.9": "In this section, we evaluate the generalization of our approach on Image-CoDe (Krojer et al., 2022), a benchmark for text-based image retrievalwhere image sets are created with 10 sequential video frames. Differentfrom TrueMatch, ImageCoDe focuses on fine-grainedness found acrossvideo frames such as negation, occlusion, etc. We follow the same ex-perimental setup adopted by Dess et al. (2023). shows that SRfine-tuning of both the vision and text modules with our bag curriculum(SR-LV BagCurri) achieves state-of-the-art results. In fact, it outper-forms Dess et al. (2023)s work on using SR-L after MLE pretrainingon COCO (row 2, +7.6%) or Conceptual Captions, a 30 larger dataset(row 3, +1.7%).",
  "Conclusion": "We investigated self-retrieval (SR) as a way to comprehensively improve current captioning systems on allthree fronts: (1) data, (2) evaluation, and (3) training. (1) We proposed Visual Caption Boosting (VCB), amodel agnostic framework to instill fine-grained visual information into generic image captioning datasetswhile remaining anchored in human annotations. (2) Furthermore, we created TrueMatch, a benchmarkcomprised of highly similar image bags. SR evaluation with TrueMatch enabled fine-grained evaluation ofcaptioning systems and unlike traditional captioning metrics encouraged diversity in generated captions.(3) We adopted SR fine-tuning and showed that the rich MLE initialization provided by VCB is important. Weuncovered a trade-off between caption faithfulness and retrieval performance plaguing current SR approachesand designed a training recipe to address it. Specifically, we employed a curriculum over the number of hardnegatives to more optimally leverage the SR reward, and fine-tune both the visual and language componentsof the captioning system. We demonstrated that this training recipe and VCB circumvents the aforementionedtrade-off. With the same model architecture, our approach achieved significant performance improvementsover vanilla SR (Dess et al., 2023) across a dataset with 99 random distractors, and set the state-of-the-art",
  "Limitations and Future Scope": "Self-retrieval (SR) is heavily reliant on a scorer to identify and retrieve the image that best matches thegenerated text. We employ CLIP as a scorer, which has been shown to struggle with capturing extremelyfine-grained and compositional visual details. However, it is fair to expect that the reliability of SR evaluationshould improve steadily over time with better vision-language models. Additionally, as Visual Caption Boosting uses LLMs and MLLMs for generating fine-grained descriptions, itis prone to hallucinations. However, as shown, our anchoring strategy mitigates this to a large extent. As future work, exploring further applications of the SR fine-tuning presents a promising research avenue.MLE initialization with improved captions is also a recurring theme and played an important role in ourwork this too deserves further investigation while training MLLMs. This project was supported in part by funding from SERB SRG/2023/002544 and an Adobe Research gift.We thank Shivanshu Sharma for partial assistance with compute. We thank Lakshmipathi Balaji, Haran SKRaajesh, Naren Akash RJ, and Vansh Agarwal for assistance with the human study.",
  "Lucas Beyer, Xiaohua Zhai, and Alexander Kolesnikov. Big Vision. 2022": "Jaemin Cho, Seunghyun Yoon, Ajinkya Kale, Franck Dernoncourt, Trung Bui, and Mohit Bansal. Fine-grained Image Captioning with CLIP Reward. In Findings of North American Chapter of Association ofComputational Linguistics (NAACL), 2022. Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li,Pascale N Fung, and Steven Hoi. InstructBLIP: Towards General-purpose Vision-Language Models withInstruction Tuning. In Advances in Neural Information Processing Systems (NeurIPS), 2024. Abhishek Das, Satwik Kottur, Jos MF Moura, Stefan Lee, and Dhruv Batra. Learning Cooperative VisualDialog Agents with Deep Reinforcement Learning. In International Conference on Computer Vision(ICCV), 2017. Karan Desai, Gaurav Kaul, Zubin Trivadi Aysola, and Justin Johnson. RedCaps: Web-curated image-text datacreated by the people, for the people. In Advances in Neural Information Processing Systems (NeurIPS),2021. Roberto Dess, Michele Bevilacqua, Eleonora Gualdoni, Nathanal Carraz Rakotonirina, Francesca Franzon,and Marco Baroni. Cross-Domain Image Captioning With Discriminative Finetuning. In Conference onComputer Vision and Pattern Recognition (CVPR), 2023.",
  "Adam Fisch, Kenton Lee, Ming-Wei Chang, Jonathan H Clark, and Regina Barzilay. CapWAP: ImageCaptioning with a Purpose. In Empirical Methods in Natural Language Processing (EMNLP), 2020": "Melissa Hall, Laura Gustafson, Aaron Adcock, Ishan Misra, and Candace Ross. Vision-Language ModelsPerforming Zero-Shot Tasks Exhibit Gender-based Disparities. In Conference on Computer Vision andPattern Recognition (CVPR), 2023. Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. CLIPScore: A Reference-freeEvaluation Metric for Image Captioning. In Empirical Methods in Natural Language Processing (EMNLP),2021. Cheng-Yu Hsieh, Jieyu Zhang, Zixian Ma, Aniruddha Kembhavi, and Ranjay Krishna. SugarCrepe: FixingHackable Benchmarks for Vision-Language Compositionality. In Advances in Neural Information ProcessingSystems (NeurIPS), 2024. Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. LoRA:Low-Rank Adaptation of Large Language Models. In International Conference on Learning Representations(ICLR), 2021.",
  "Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language Models areUnsupervised Multitask Learners. 2019": "Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning Transferable Visual Models From NaturalLanguage Supervision. In International Conference on Machine Learning (ICML), 2021. Steven J Rennie, Etienne Marcheret, Youssef Mroueh, Jerret Ross, and Vaibhava Goel. Self-Critical SequenceTraining for Image Captioning. In Conference on Computer Vision and Pattern Recognition (CVPR), 2017.",
  "Gabriel Oliveira dos Santos, Esther Luna Colombini, and Sandra Avila. CIDEr-R: Robust Consensus-basedImage Description Evaluation. 2021": "Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti,Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. LAION-5B: An Open Large-scaleDataset for Training Next Generation Image-Text Models. In Advances in Neural Information ProcessingSystems (NeurIPS), volume 35, 2022. Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual Captions: A Cleaned,Hypernymed, Image Alt-text Dataset for Automatic Image Captioning. In Association of ComputationalLinguistics (ACL), 2018. Nina Shvetsova, Anna Kukleva, Xudong Hong, Christian Rupprecht, Bernt Schiele, and Hilde Kuehne.HowToCaption: Prompting LLMs to Transform Video Annotations at Scale. In European Conference onComputer Vision (ECCV), 2024. Vasu Singla, Kaiyu Yue, Sukriti Paul, Reza Shirkavand, Mayuka Jayawardhana, Alireza Ganjdanesh, HengHuang, Abhinav Bhatele, Gowthami Somepalli, and Tom Goldstein. From Pixels to Prose: A Large Datasetof Dense Image Captions. arXiv preprint arXiv:2406.10328, 2024. Kirill Sirotkin, Pablo Carballeira, and Marcos Escudero-Violo. A Study on the Distribution of Social Biasesin Self-Supervised Learning Visual Models. In Conference on Computer Vision and Pattern Recognition(CVPR), 2022. Matteo Stefanini, Marcella Cornia, Lorenzo Baraldi, Silvia Cascianelli, Giuseppe Fiameni, and Rita Cucchiara.From Show to Tell: A Survey on Deep Learning-Based Image Captioning. IEEE Transactions on PatternAnalysis and Machine Intelligence (PAMI), 45(1):539559, 2022. Bart Thomee, David A Shamma, Gerald Friedland, Benjamin Elizalde, Karl Ni, Douglas Poland, DamianBorth, and Li-Jia Li. YFCC100M: The New Data in Multimedia Research. Communications of the ACM,59(2):6473, 2016. Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai Charitha Akula, JihanYang, Shusheng Yang, Adithya Iyer, Xichen Pan, et al. Cambrian-1: A Fully Open, Vision-CentricExploration of Multimodal LLMs. arXiv preprint arXiv:2406.16860, 2024a. Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann LeCun, and Saining Xie. Eyes Wide Shut?Exploring the Visual Shortcomings of Multimodal LLMs. In Conference on Computer Vision and PatternRecognition (CVPR), 2024b. Jack Urbanek, Florian Bordes, Pietro Astolfi, Mary Williamson, Vasu Sharma, and Adriana Romero-Soriano.A Picture is Worth More Than 77 Text Tokens: Evaluating CLIP-Style Models on Dense Captions. InConference on Computer Vision and Pattern Recognition (CVPR), 2024.",
  "Ronald J Williams. Simple Statistical Gradient-Following Algorithms for Connectionist ReinforcementLearning. Machine learning, 8:229256, 1992": "Andre Ye, Sebastin Santy, Jena D Hwang, Amy X Zhang, and Ranjay Krishna. Computer Vision Datasetsand Models Exhibit Cultural and Linguistic Diversity in Perception. arXiv preprint arXiv:2310.14356,2023. Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and Yonghui Wu. CoCa:Contrastive Captioners are Image-Text Foundation Models. Transactions on Machine Learning Research(TMLR), 2022.",
  "A.1Prompting Strategy": "In this section, we discuss prompting strategies used to produce BlendCap, the visual caption, and HolisticCap.We provide the system prompt along with the in-context examples we utilize. In order to generate captionsthat holistically describe the image, we use the following two step process: (a) BlendCap. First, we prompt Mistral-7B (Jiang et al., 2023) to reduce redundancy, preserve all factualinformation obtained from human annotated captions, and not introduce any new information other thanwhat is mentioned in the reference set. This ensures that the blended caption remains factual and anchoredto the human annotations: System prompt : You are given a reference set of 5 image captions.Create a holisticcaption to describe the same image by incorporating relevant information from all thereference captions. Ensure that the generated caption minimizes redundancy by excludingoverlapping information found in the reference set. Do not include additional informationand adjectives other than what is given in reference set. Stop after the first period and donot ask any additional questions. Just complete the text. Three examples are shown below : In context examples :User : Reference caption 1: A plate full of food with an assortment of food on it.Reference caption 2: There is meat and vegetables on a white and brown plate.Reference caption 3: A plate on a table that hasfood on it.Reference caption 4: A plate with some steak, carrots, and sliced fried potatoes.Reference caption 5: A plate of food on a table.Assistant : A white and brown plate on a table with an assortment of steak, carrots,sliced fried potatoes, and vegetables. User : Reference caption 1: A park bench on the side of a lake.Reference caption 2: A lone bench sits atop a hill looking over the river.Reference caption 3: A wooden bench sitting on top of a sandy beach.Reference caption 4: A bench on a river bank in the countryside.Reference caption 5: A wood bench is sitting in front of a river.Assistant : A wooden bench sits atop a hill in front of a river overlooking the water andthe surrounding countryside. User : Reference caption 1: A bathroom with a toilet sitting next to a sink.Reference caption 2: A white sink and toilet in a room.Reference caption 3: A bathroom with a mirror, sinks, toilet and toilet roll.Reference caption 4: A bathroom that has a toilet sink and mirror in it.Reference caption 5: A bathroom with a toilet next to a sink.Assistant : A bathroom with a toilet next to a white sink, mirror and toilet roll.",
  "A.2Human Study: Quantifying Hallucinations in VCB": "We conduct a human study to investigate the extent of hallucinations in BlendCap and HolisticCap. Werandomly sample 60 images from MSCOCO and for each image-caption pair, a human is asked to count thenumber of hallucinations present in the caption. Three types of hallucinations are considered: (1) objects,(2) attributes, and (3) egregious statements, i.e., a glaringly incorrect statement that is unambiguously false. BlendCap contains only 4 object and 3 attribute hallucinations. Out of the 7 hallucinations, 5 are dueto incorrect COCO human annotations and only 2 (3.3%) are induced by the LLM. This demonstrates thereliability of BlendCap in combining human annotations into a denser caption. HolisticCap contains only 9 instances of hallucinations (4 object and 5 attribute) across 7 captions, comparedto VisualCaps 16 instances (across 12 captions). Additionally, VisualCap contained 3 egregious statements thatwere removed in HolisticCap. Furthermore, 7/60 captions in HolisticCap contain hallucinations, reflecting a41.6% reduction compared to VisualCaps 12/60 captions. This demonstrates VCBs effectiveness in anchoringthe MLLM descriptions (Visual Captions) into human annotations (BlendCap) to create HolisticCap.",
  "B.2Training Bags used for Self-Retrieval": "We utilize our bag creation algorithm (Algorithm 1) to create training bags. However, for each row in thesorted cosine similarity matrix D we only consider the top 200 retrievals, i.e. D[:, :200]. Each training bagof size s is comprised of a given query image and the top s1 images it retrieves from D such that none ofthe retrieved images have been added to any other bag before. This ensures that all the bags are unique andeach image is seen exactly once in an epoch. Unlike TrueMatch where we select the hardest bag corresponding to specific visual concepts in the CLIPembedding space, we relax the constraint of only sampling the sub-cluster (bag of images) with the highestintra-cluster similarity.",
  "B.3BagCurri: Designing a Self-Retrieval Learning Curriculum with Bags": "We design a curriculum that varies the bag sizes during training, gradually increasing the bag size with eachepoch as shown in . Since the reward is the cross-entropy of matching the generated caption with thetarget image Dess et al. (2023), mining multiple hard negatives yields a flatter softmax distribution leadingto stronger learning signal. Hence, increasing the bag size introduces makes the task of SR more challenging.However, it is important to increase bag sizes gradually to ensure that the task is not too hard, and preventa collapse of caption faithfulness (.6).",
  ": Our curriculum over bag sizesduring training": "To assess the role our curriculum plays in preventing this col-lapse, we conduct an ablation study. Specifically, we compareour curriculum against training with bags of a fixed size for theentire SR fine-tuning process. demonstrates that ourcarefully designed curriculum is responsible for preserving cap-tion faithfulness. While training with hard negatives without acurriculum improves retrieval performance on TrueMatch (rows2-8), it fails to maintain the CIDEr score. In contrast, BagCurrinot only yields substantial gains over training with randomdistractors (row 1) but also enhances CIDEr by +3.2%. Thishighlights the effectiveness of our carefully designed curricu-lum in preventing the collapse of NLG metrics while improvingretrieval performance. Note, this is different to previous SRapproaches (Dess et al., 2023) as presented in .4.",
  "Similar to (Pinto et al., 2023), we find that both optimization strategies, MLE and Reinforce, discussed in.1 have complementary strengths and weaknesses for the task of image captioning": "MLE. Training the captioner on MLE results in powerful models capable of capturing complex datadistributions. The task of next-token prediction with current language models is both efficient (due toteacher forcing) and scalable (Kaplan et al., 2020). However, it creates captioning systems that prefer genericcaptions (Wang et al., 2020), reusing a small vocabulary to describe different images with fine-grained visualdifferences (see and .3). Reinforce. While fine-tuning with SR alleviates these issues (), it comes with its own set ofchallenges. Not only are the tokens sampled sequentially but optimizing Reinforce is unstable, necessitating",
  "C.2REINFORCE baseline": "The variance of the gradient estimate in Reinforce is commonly reduced by subtracting a baseline b fromthe reward R. This stabilizes training by reducing the variance of the estimated gradient. We adopt a runningmean of past rewards as our baseline similar to Dess et al. (2023) as they find that it outperforms computingthe reward with greedy decoding as a baseline. The latter requires sampling two outputs for one traininginput, one to estimate the gradient and other to compute the baseline. However, when the reward includesCIDEr, we adopt a greedy baseline.",
  "COCO107.8 10497.2BlendCap80.2 77.3 72.4HolisticCap 26.6 26.4 25.6": "shows that SR-V is substantially more discriminative com-pared to SR-L. However unlike SR-L, SR-V fails to preserve captionfaithfulness through early stopping. To verify that the observeddeterioration isnt due to the tendency of NLG metrics to penalizediscriminability (Wang et al., 2020), we train SR-L until it achievesthe same RD100 score as SR-V for each dataset. This ensures afair comparison of both methods abilities to preserve CIDEr, aspresented in . We clearly see that SR-L (col 2) attains asignificantly higher CIDEr score compared to SR-V (col 3) for allcaption types, demonstrating that the deterioration in NLG metricsobserved in is directly caused by CLIP fine-tuning.",
  "57.1 57.12 0.129.892.464.8 54.0 55.13 0.333.392.068.2 61.2 57.14 0.538.092.667.7 58.7 57.95 0.741.492.768.1 58.3 56.86145.492.566.5 56.2 56.2": "The weighting of the rewards when jointly optimizing CIDErand SR (Luo et al., 2018) determines the trade-off betweenfaithfulness and discriminative ability of the captioning system.We MLE pretrain the captioner with HolisticCap and fine-tuneboth the LLM and CLIP components (SR-LV) with BagCurri. presents an ablation over different values of whilescaling CIDEr in the joint reward R = SR + CIDEr. Asexpected, increasing i.e. heavily weighing CIDEr leads toimproved caption faithfulness. Interestingly, using a smaller however doesnt always improve TrueMatch scores, as = 0.7(row 5) outperforms = 0.1 (row 2). We adopt = 0.5 forour experiments (.8), as it results in the best per-forming model achieving an optimal balance between retrievalperformance and caption faithfulness.",
  "C.6CIDEr Struggles with Longer Descriptions": "Notably, we observe that the scale of CIDEr scores for MLE pretrained models vary significantly acrossCOCO, BlendCap, and HolisticCap (rows 1-3, ). Consistent with Santos et al. (2021), we find thatCIDEr rewards brevity, while reference sets containing longer captions have substantially lower scores. Whilethe inefficacy of MLE in modeling longer descriptions does contribute to lower CIDEr scores, we also find thatCIDEr fails to effectively evaluate MLE pretraining with HolisticCap, even assigning scores of 0 to correctcaptions, as illustrated in .",
  "C.8Investigating Caption Unfaithfulness in SR Fine-tuning": "To improve our understanding of the trade-off presented in .4 during SR fine-tuning, we furtherinvestigate the deterioration of the NLG metrics. Dess et al. (2023) attribute this deterioration to themodel diverging from the GT distribution during SR-L fine-tuning. They argue it to be desirable propertyas it allows the captioner to generalize to a wider range of captioning distributions. We investigate thisclaim by qualitatively analyzing image-caption pairs (COCO test set) where SR-L exhibits poor CIDErscores compared to MLE. We find several image-caption pairs (some are shown in ) corroboratingthis claim, highlighting the inadequacies of reference-based metrics. For instance, a more discriminativecaption A woman serving a hot dog to a man scores lower on CIDEr than a more generic one, A coupleof people standing in front of a food truck. However, our analysis also reveals that models fine-tuned onSR, in an attempt to enhance retrieval, have a propensity to hallucinate which results in poor CIDEr scores(see ). This differs from the findings of .3, since the model is incorporating non-factualinformation to the generated captions instead of semantic details. We observe this tendency under the sameexperimental setting as (Dess et al., 2023), noting a modest dip in NLG metrics due to early stopping.However, we find this effect exacerbates with longer training durations, as illustrated by the dramatic drop inCIDEr scores in ."
}