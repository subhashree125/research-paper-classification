{
  "Abstract": "As machine- and AI-generated content proliferates, protecting the intellectual property ofgenerative models has become imperative, yet verifying data ownership poses formidablechallenges, particularly in cases of unauthorized reuse of generated data. Confirming theownership of the data is challenging, as the data generation process is opaque to those veri-fying the authenticity. Our work is dedicated to detecting data reuse from a single sample.While watermarking has been the traditional method to detect AI-generated content byembedding specific information within models or their outputs, which could compromisethe quality of outputs, our approach instead identifies inherent fingerprints in the outputswithout altering models. The verification is achieved by requiring the (authentic) models tore-generate the data. Furthermore, we propose a method that iteratively re-generates thedata to enhance these fingerprints in the generation stage. The strategy is both theoreti-cally sound and empirically proven effective with recent advanced text and image generativemodels. Our approach is significant because it avoids extra operations or measures, suchas (1) modifying model parameters, (2) altering the generated outputs, or (3) employingadditional classification models for verification. This enhancement broadens the applicabil-ity of authorship verification (1) to track the IP violation in generative models publishedwithout explicitly designed watermark mechanisms and (2) to produce outputs withoutcompromising their quality.",
  "Introduction": "In recent years, the emergence of Artificial Intelligence Generated Content (AIGC), including tools likeChatGPT, Claude, DALL-E, Stable Diffusion, Copilot, has marked a significant advancement in the quality ofmachine-generated content.While these cloud-based services have accelerated AI technology development,they have also raised concerns about content misuse. Two critical challenges emerge: (1) protecting theIntellectual Property (IP) of authentic generators and (2) ensuring accountability for information sources.",
  "!": ": The two-stage framework leveraging fingerprints in generative models. In (I) Generation Stage,models generate output in traditional ways and optionally re-generate the output k [1..K] times to enhancethe fingerprints. The process ensures consistency of the modality in iterative re-generation, i.e., text-to-textand image-to-image. In (II) Verification Stage, the authentication of data ownership is established byassessing the distance between the suspected data (left) and its re-generated version (right). There existsa distinguishable margin between the distances by authentic generator (Ga, the middle and the bottomexamples) and contrasting benign generator (Gc, the top example), exemplified by models from OpenAI (Ga)and Stability AI (Gc), respectively. Traditionally, the primary approach for safeguarding IP of the contents generated by AI generator hasinvolved embedding subtle but verifiable watermarks into their outputs, such as text (He et al., 2022a;b;Kirchenbauer et al., 2023a), images (Zear et al., 2018; Zhao et al., 2023b) and code (Lee et al., 2023). Thesewatermarking techniques typically involve adding supplementary information to the deep learning modelsparameters and architectures or direct post-processing alterations to the generated outputs. However, thesealterations could potentially degrade the quality of the generated content. An alternative strategy has beenthe classification of data produced by a specific model to distinguish it from content generated by othermodels or humans (Solaiman et al., 2019b; Ippolito et al., 2020). Nonetheless, this often requires trainingadditional classifiers to verify authorship, raising concern about their ability to generalize and maintainrobustness across evolving generative models, especially with few training samples. Another approach has been to classify data from a specific model to tell it apart from data created by othermodels or humans (Solaiman et al., 2019b; Ippolito et al., 2020). However, this usually needs extra classifiersto confirm authorship, which can be challenging. These classifiers might struggle to stay effective and robustas generative models evolve, especially with limited training samples. In response to the challenges of authorship authentication and IP protection, our approach is to exploit theinherent characteristics of generative models. Firstly, we recognize that generative models possess uniqueattributes - akin to model fingerprints - such as specific styles and embedded knowledge. In the VerificationStage of our framework, we utilize these implicit fingerprints by measuring the distance D between the genuinedata samples with content re-generated by the authentic and contrasting models. Secondly, to enhance thedistinctive nature of these fingerprints, our approach in the Generation Stage involves using the original",
  "In , we present a conceptual framework for authorship verification through re-generation": "Stage I: GenerationThe authentic generator targets producing outputs that involve stealthy but sig-nificant signatures that are distinguishable from other generative models or humans.We consider twodistinct approaches: (i) Traditional Generation produces the authentic outputs xa from a given text inputas a prompt, i.e., xa = Ga(xp), where xp = Storm on Sea of Galilee; and (ii) Iterative Re-Generationenhances the models unique signature by re-generating the data multiple times using a re-painting or para-phrasing mode, i.e., xk+1a= Ga(xka ). Here Ga is the authentic generative model which is DALLE (Rameshet al., 2021) in this example. Stage II: VerificationIn this stage, authentic model Ga verifies the origin of its artefact xa by comparingthe distance D between xa and its re-generation by the authentic model Ga(xa) or other contrasting modelsGc(xa). Intuitively, the one-step regeneration distance of an image originally generated by the authenticmodel, such as DALLE by OpenAI, is expected to be smaller when compared to itself than to a contrastingmodel not involved in its initial generation, i.e., D(xa, Ga(xa)) < D(xa, Gc(xa)). Furthermore, the morere-generations an image undergoes during the Generation Stage, the lower its one-step regeneration distanceof the authentic model becomes at the Verification Stage, i.e., D(xia , Ga(xia )) < D(xja , Ga(xja )), wheni > j.",
  "We summarize the key advantages and contributions of our work as follows:": "We validate the effectiveness of using re-generated data as a key indicator for authorship verification.This approach is designed to be functional in black-box settings and applies across various generativeapplications, such as Natural Language Generation (NLG) and Image Generation (IG). We introduce an iterative re-generation technique to enhance the inherent fingerprints of generativemodels. We use fixed-point theory to demonstrate that modifications achieved through re-generationconverge to minimal edit distances.This ensures a distinct separation between the outputs ofauthentic models and those generated by other models. We have developed a practical verification protocol that streamlines the process of data ownershipvalidation in generative models. This protocol is especially useful in legal settings, as it eliminatesthe need for generators to disclose their model parameters or watermarking strategies, therebypreserving confidentiality and proprietary integrity. A notable advantage of our approach is its reliance solely on the standard generative models, withoutresorting to additional interventions, including (1) manipulating or fine-tuning generative modelparameters, (2) post-editing the outputs, or (3) additional independent classification models forverification. This simplicity in design not only preserves the original quality of the generated contentbut also enhances the feasibility and accessibility of our verification method.",
  "Related Work": "Recent advancements in the field of generative modeling, exemplified by innovations like DALLE (Rameshet al.), Stable Diffusion (Rombach et al., 2022), ChatGPT, Claude, Gemini, and others. However, this prolif-eration of synthetic media has simultaneously raised ethical concerns. These concerns include the potentialfor misuse in impersonation (Hernandez, 2023; Verma, 2023), dissemination of misinformation (Pantserev,2020; Hazell, 2023; Mozes et al., 2023; Sjouwerman, 2023), academic dishonesty (Lund et al., 2023), andcopyright infringement (Brundage et al., 2018; Rostamzadeh et al., 2021; He et al., 2022a; Xu & He, 2023).In response, there is an increasing focus on the need to trace and authenticate the origins of such content toprevent its illegitimate use in AIGC. Considering the distinct nature of image and text, we will review theauthorship identification in image and text generation models separately.",
  "Authorship Identification for Image Generation Models": "With the advancements of deep learning techniques, multiple works have suggested leveraging neural net-works to seamlessly encode concealed information within images in a fully trainable manner (Zhu et al.,2018; Yang et al., 2019; Ahmadi et al., 2020; You et al., 2020). Inspired by this idea, Fernandez et al. (2022)incorporate watermarks into the latent space formulated by a self-supervised network such as DINO (Caronet al., 2021). This approach modulates the features of the image within a specific region of the latent space,ensuring that subsequent transformations applied to watermarked images preserve the integrity of the em-bedded information. Subsequently, watermark detection can be conducted based on this same latent space.Similarly, Fernandez et al. (2023) introduce a binary signature directly into the decoder of a diffusion model,resulting in images that contain an imperceptibly embedded binary signature. This binary signature can beaccurately extracted using a pre-trained watermark extractor during verification. Given the escalating concerns regarding the misuse of deep fakes, as highlighted in the literature (Brundageet al., 2018; Harris, 2019), several studies have proposed methodologies for attributing the origin of an im-age, specifically discerning between machine-generated and authentic images. This task is rendered feasiblethrough the identification of subtle, yet machine-detectable, patterns unique to images generated by Gener-ative Adversarial Networks (GANs), as evidenced in recent research (Marra et al., 2019; Afchar et al., 2018;Gera & Delp, 2018; Yu et al., 2019). Furthermore, the detection of deep fakes is enhanced by analyzinginconsistencies in the frequency domain or texture representation between authentic and fabricated images,as indicated in recent studies (Zhang et al., 2019; Durall et al., 2020; Liu et al., 2020).",
  "Authorship Identification for Natural Language Generation Models": "Content generated by text generation models is increasingly vulnerable to various forms of misuse, includingthe spread of misinformation and the training of surrogate models (Wallace et al., 2020; Xu et al., 2022).Consequently, a growing interest has been in protecting the authorship (or IP) of text generation models ordetecting machine-generated text. A straightforward solution is to incorporate watermarks into the generated text. However, unlike images,textual information is composed of discrete tokens, making the watermarking process for text a difficult en-deavor due to the potential for inadvertent alternation that can change its semantic meaning (Katzenbeisser& Petitolas, 2000). One solution to preserve semantic integrity during watermarking involves synonym sub-stitution (Topkara et al., 2006; Chang & Clark, 2014; He et al., 2022a). Nevertheless, the simplistic approachto synonym substitution is vulnerable to detection through statistical analyses. In response, He et al. (2022b)have proposed a conditional synonym substitution method to enhance both stealthiness and robustness ofsubstitution-based watermarks. Moreover, Venugopal et al. (2011) adopted bit representation to encodesemantically similar sentences, enabling the selection of watermarked sentences through bit manipulation. The previously discussed methods have been centered on applying watermarks through post-editing. How-ever, with the emergence of LLMs, there has been a significant shift towards developing watermarks tailoredfor LLMs to identify machine-generated text. A notable approach in this area employs biased samplingstrategies to alter token distribution at each generation step, favoring tokens from specific pre-defined cat-egories (Kirchenbauer et al., 2023a;b; Zhao et al., 2023a).Despite their innovation, these methods facevulnerabilities to rewriting attacks, where watermarked sentences are paraphrased either automaticallyor manually, thus challenging the identification of original authorship (Christ et al., 2023). To address thisissue, Kuditipudi et al. (2023) propose a robust approach that maps a sequence of random numbers gener-ated from a randomized watermark key to the outputs of a language model. This technique maintains thewatermarks detectability, notwithstanding text alterations such as substitutions, insertions, or deletions. Interest in post-hoc detection has surged as a complementary measure to watermarking. This trend is drivenby the fact that developers often keep the details of their watermarking algorithms confidential to preventthem from being compromised if leaked.Depending on whether machine-generated or human-authoredtext samples are available, one can utilize either zero-shot or training-based detection methods. Zero-shot",
  "Published in Transactions on Machine Learning Research (09/2024)": "This decomposes similarity into comparative measurements of structure, luminance, and contrast. As aresult, SSIM better matches human perceptual judgments compared to MSE. Values range from -1 to 1,with 1 being identical local structure. As research rapidly improves generation quality, we expect future use cases to leverage such advanced gen-erators. Analyzing these models is thus more indicative of real-world conditions going forward compared toearlier versions. Furthermore, the marked quality improvements in recent SD models present greater chal-lenges for attribution and fingerprinting. Subtle inter-model differences become more difficult to quantifyamidst high-fidelity outputs. Distance metrics like MSE and LPIPS are sensitive to quality, so lower baselinedistortion is a more rigorous test scenario. By evaluating cutting-edge models without inpainting specializa-tion, we aim to benchmark model fingerprinting efficacy on contemporary quality levels. Our experiments onthe latest SD variants at scale also assess generalization across diverse high-fidelity generators. Successful at-tribution and stability analysis under these conditions will highlight the viability of our proposed techniquesin real-world deployment.",
  "xnewa= Gp(xolda).(2)": "As examples of Natural Language Generation (NLG), we can use round-trip translation (Gaspari, 2006)or prompt the Large Language Model (LLM) to paraphrase a sentence. As an image generation (IG)example, given a generated image xa, the paraphrasing process will be (1) rebuilding partial images ofthe randomly masked region M[t] in the original image xolda, i.e., xnewa[t] = G(xolda, M[t]) (von Platenet al., 2022), and (2) merge these rebuilt portions as a whole image as the paraphrased output,",
  "Methodology": "Our research primarily focuses on the threat posed by malicious users who misuse generated content withoutauthorization. Specifically, we delve into cases where the owner of authentic generative models, referred to asGa, allows access to their models in a black-box fashion, permitting external queries to their APIs for contentgeneration. This black-box setting is consistent with the current application practices of Large FoundationModels. Hereby, unscrupulous users could take advantage of the generated content, xa, while disregardingthe legitimate creators license for their own profits. To address this issue, API providers can actively monitorthe characteristics of publicly available data to identify potential cases of plagiarism or misuse. This can beaccomplished by applying the re-generation and measuring the corresponding edit distance in the verificationstage, as described in .2. To enhance verification accuracy, the authentic model Ga can employ aniterative re-generation approach to boost the fingerprinting signal, as introduced and proved in .3.If there are suspicions of plagiarism, the company can initiate legal proceedings against the alleged plagiaristthrough a third-party arbitration, following the verification protocol (see Algorithm 2) on the output withiterative re-generation (see Algorithm 1), as demonstrated in .1. The motivation and intuition ofthe approach proposed in .1 are explained in Sections 4.2 and 4.3.",
  "The defense framework is comprised of two key components, responding to the Generation and VerificationStages in :": "(1) Iterative Generation (Algorithm 1), which progressively enhances the fingerprint signal in the gen-erated outputs. Initially, a prompt input xp is used to generate the first output x0ausing the generativemodel G in generation mode Gg. This output is then iteratively re-generated for K steps using the samemodel but in paraphrasing mode, Gp, ensuring the type consistency of the content. Each iteration aims toreinforce the unique fingerprint of the authentic model in the generated content. (2) Verification (Algorithm 2) is responsible for confirming the authorship of the data sample througha one-step re-generation process using both authentic model Ga and suspected contrasting model Gc inparaphrasing mode. The authentic and contrasting samples are then distinguished by their ratio with aconfidence margin > 0. Our verification protocol takes inspiration from human artists capability to reproduce their work thatclosely resembles the original. Similarly, our process of iterative re-generation mirrors the way artists refinetheir unique writing or painting styles. Moreover, the distinctive style of the generative model becomesincreasingly pronounced with each re-generation cycle, as it converges to a more stable output. This processis formulated by the iterative functions and their tendency to converge to fixed points is proved in Theorem 2.",
  "Authorship Verification through Re-Generation Distance": "An authentic generative model Ga that aims to distinguish between the data samples it generated, denotedas xa, with the benign samples xc generated by other models Gc for contrast. Unless stated otherwise, thegenerators G operate in paraphrasing mode in the following discussion, ensuring a uniform output modalityduring the re-generation process. To verify the data, the authentic model (1) re-generates the data Ga(x)and (2) evaluates the distance between the original sample and the re-generated sample, defined as d(x, G) D(G(x), x). In essence, samples produced by the authentic model are expected to exhibit lower self-editdistance, as they share the same generative model Ga, which uses identical internal knowledge, such as",
  "i.e., d(xa, Ga) < d(xc, Ga).(5)": "Consequently, models can identify the authentic samples generated by themselves by evaluating the self-edit distance, which can be viewed as a specialized form of a classification function for discriminating theauthentic and contrasting models. Additionally, the re-generation process and corresponding edit distancescan serve as explainable and comprehensible evidence to human judges.",
  "Enhancing Fingerprint through Iterative Re-Generation": "While we have claimed that the data samples generated through the vanilla generative process can be verified,there is no theoretical guarantee regarding the self-edit distance for certifying these samples using theauthentic model. To address this limitation, we introduce an iterative re-generation method that improves thefingerprinting capability of the authentic generative model, as it manages to reduce the self-edit distances.This property will be utilized in verification, as the re-generation step in Verification Stage serves as the(K + 1)-th re-generation step after K steps in the Generation Stage regarding the authentic model.Ourtheoretical foundation assumes the L-Lipschitz constant of the generative model G satisfying L (0, 1).While measuring the exact Lipschitz constant for a deep generative AI model is challenging, especially forproprietary models, this property is empirically verifiable and we find it satisfied in many generative AImodels, as presented in Appendix C.3. Definition 1 (The Fixed Points of a Lipschitz Continuous Function) Given a multi-variable func-tion f : Rm Rm is L-Lipschitz continuous, i.e., f(x) f(y) L x y, where L (0, 1). Forany initial point x0, the sequence {xk}k=0 is acquired by the recursion xk+1 = f(xk). The sequenceconverges to a fixed point x, where x = f(x). Theorem 2 (The Convergence of Step Distance for k-th Re-generation) Assuming that the Lips-chitz constant L of function f, which can be presented as a generative model, the distance between the inputand output of the K-th iteration is bounded by",
  "Experiments": "This section aims to demonstrate the efficacy of our re-generation framework of authorship authentication ongenerated text and image separately. For both Natural Language Generation (NLG) and Image Generation(IG) scenarios, in Sections 5.2 and 5.3, we first generate the initial intended data samples followed by severalsteps of paraphrasing (for NLG) or inpainting (for IG) as re-generation, detailed in Algorithm 1. Then,we test the properties of these samples by three series of experiments.",
  "Experimental Setup": "Despite falling under the same framework for authorship verification, Natural Language Generation (NLG)and Image Generation (IG) use distinct generative models and varying metrics for measuring distance. Generative Models.For text generation, we consider four generative models: 1) M2M (Fan et al., 2021):a multilingual encoder-decoder model trained for many-to-many multilingual translation; 2) mBART-large-50 (Tang et al., 2021): a model fine-tuned on mBART for multilingual machine translation between anypair of 50 languages; Cohere: a large language model developed by Cohere; 4) GPT3.5-turbo (version:0613): a chat-based GPT3.5 model developed by OpenAI.2 For image generation, we examine five primary generative models based on the Stable Diffusion (SD) archi-tecture (Rombach et al., 2022). All models are trained on a subset of the LAION-2B dataset (Schuhmannet al., 2022) consisting of CLIP-filtered image-text pairs.These models are: 1) SDv2.1; 2) SDv2; 3)SDv2.1 Base; 4) SDXLv1.0 (Podell et al., 2023), which distinguishes itself by employing an ensembleof experts pipeline for latent diffusion, where the base model first generates noisy latent that is refinedusing a specialized denoising model 5) SDXL Base0.9 (Podell et al., 2023).3 More information on modelarchitecture and training, and the quality of re-generated images is demonstrated in Appendix C.1. Distance Metrics.To gauge the similarity between generated outputs, we employ three popular similaritymetrics for NLG experiments, 1) BLEU calculates the precision of the overlap of various n-grams (usually 1to 4) between the candidate and the reference sentences (Papineni et al., 2002); 2) ROUGE-L calculates theF-1 score of longest common subsequence between the candidate and reference sentences (Lin, 2004) and 3)BERTScore computes the similarity between candidate and reference tokens using cosine similarity on theirembeddings. Then, the token-level scores are aggregated to produce a single score for the whole text (Zhanget al., 2020). For IG experiments, we consider the following distance metrics: 1) CLIP Cosine Distancemeasures the semantic similarity of two images using pre-trained CLIP image-text models (Radford et al.,2021); 2) LPIPS compares perceptual style differences between two images (Zhang et al., 2018); 3) MeanSquared Error (MSE) serves as a pixel-level metric that compares all raw pixel values in the image pair;4) Structural Similarity Index (SSIM) assesses image degradation based on luminance, contrast, andstructure (Wang et al., 2004). We transform all similarity scores s to distances d by d = 1 s.",
  "Natural Language Generation Experiments": "In the study of text generation, the primary objective is to produce sentences that fulfill specific require-ments, including translation, paraphrasing, summarization, etc.. Then, one can re-generate intended outputsentences via paraphrasing them. This study focuses on machine translation (specifically from French toEnglish) as the primary generation task.4 Considering that M2M and mBART are restricted to machinetranslation tasks, we utilize round-trip translation as a way for paraphrasing. Specifically, for each round-trip translation step, an English input sentence is first translated into French and then translated back toEnglish. We repeat this process multiple times to observe the change of the edit distances. To reproducethe primary experiment, i.e., Fr-En translation, using GPT3.5-turbo and Cohere, we perform a zero-shotprompting using the following prompts:",
  "Translate to French: You are a professional translator. You should translate the following sentenceto French and output the final result only: {INPUT}": "Text generative models have been known to exhibit outstanding performance when test data inadvertentlyoverlaps with pre-training data, a phenomenon referred to as data contamination (Magar & Schwartz, 2022).To address this potential issue, we sample 200 sentences from the in-house data as the starting point foreach model. To mitigate biases arising from varied sampling strategies, we set the temperature and top p to0.7 and 0.95 for all NLG experiments. Distance Convergence.The dynamics of the distance change across three metrics and various generativemodels are depicted in . We observe a remarkable distance reduction between the first and seconditerations in all settings. Subsequently, the changes in distance between consecutive iterations exhibit adiminishing pattern, tending to converge after approximately 5-7 rounds of re-generation. These observedtrends are consistent with the Fixed-Point Theorem as elaborated in .3. Discrepancy.In our previous study of iterative re-generation using the same model, we observe theconvergence of edit distances which can be utilized to distinguish the authentic model from its counterparts.As delineated in .2, for each French sentence x from the corpus, we apply the prompt-mode ofauthentic model to yield xa via a translation. Both authentic and contrasting models are considered toperform a one-step re-generation of xa to obtain y for verification.We measure the distance betweenxa and y (including ya and yc). As illustrated in , the density distribution associated with theauthentic model demonstrates a noticeable divergence from those of models for contrast, thus affirming the",
  "hypothesis discussed in .2. This pattern also holds when using ROUGE-L and BERTScore fordistance measures, as evidenced by their respective density distributions in Appendix D.1": "We observe the difficulty of distinguishing Cohere from other models merely based on the outputs from thefirst iteration, i.e., without re-generation. We attribute the distinctive behavior of Cohere to its relativelyslower convergence than other models.5 However, as depicted in , the distances observed in subse-quent iterations of the same model experience a substantial reduction. This implies the re-generated outputsin the Generation Stage possess enhanced fingerprints. In particular, we can derive xkafrom 5 iterationsof Ga(). shows that increasing k enhances the differentiation of the authentic models densitydistribution compared to other models. This distinction becomes evident even in the case of Cohere, whichremains indistinguishable at k = 1. This enhancement for Cohere is also corroborated in the verificationexperiments in the next paragraph. Verification.We employ Algorithm 2 to ascertain if a given sentence originates from the authentic models.The determination hinges on the threshold parameter, . Thus, we designate M2M as the authentic model,while using mBART, Cohere, and GPT3.5-turbo as contrasting models to determine the optimal value of .As indicated in , a threshold of 0.05 certifies that the three contrasting models can validate that morethan 93% of the samples are derived from M2M. As anticipated, an increase in the value of augments thestringency criteria, resulting in a reduction in verification precision. Therefore, we fix at 0.05 for ensuingevaluations unless stated otherwise. As indicated in , our approach undertakes the validation of authorship, yielding a precision exceeding85% and a recall rate surpassing 88% across M2M, mBART, and GPT3.5-turbo. Moreover, both precisionand recall metrics improve with additional re-generation iterations. Although with a slower convergencespeed on Cohere, the verification performance on its samples improved significantly given more re-generationiterations, see Cohere lines in .",
  "Image Generation Experiments": "The main objective of the image generation task is to produce an image given a text prompt using agenerative model, a.k.a prompt-mode. To generate the initial proposal of images, we sample 200 captionsas prompts each from MS-COCO dataset (COCO) (Lin et al., 2014) and Polo Club Diffusion DBdataset (POLO) (Wang et al., 2022), then we generate initial images x0 corresponding to the promptsusing all assigned models. For re-generation, we consider two settings: (1) watermarking images throughinpainting a pre-defined sub-regions and (2) fingerprinting enhancement through full image re-generation.The subsequent two paragraphs detail the methodologies for each setting. Watermarking through Sub-Region InpaintingWhen re-generating authentic image xa, we firstmask a sub-region, i.e., 1/N (N = 10 in our experiments) of its pixels with fixed positions as the water-mark pattern.Then, all pixels in the masked regions are re-generated by inpainting using a generativemodel. Similar to text generation, the inpainting step is iterated multiple times, following Algorithm 1. Thecomprehensive description of the watermarking procedure is provided in Appendix B.1. Fingerprinting through Full-Image Re-generationIn the re-generation setting, we first split allpossible mask positions into N (N = 8 in our experiments) non-overlapped sets, coined segments, each with1/N pixel positions. We parallelly reconstruct each segment based on the rest (N 1)/N pixels of the imageas the context in generation. Then, we reassemble these N independent segments into a new full image y.When analyzing reconstructions of ya = Ga(xa) by contrasting models yc = Gc(xa), there is an expectationthat models may exhibit variations in painting masked regions due to their inherent biases.",
  ": The convergence analysis of the distances in iterations based on CLIPS and LPIPS metrics onre-generated images of 200 samples on Coco and Polo datasets": "In this study, to generate the samples from the authentic models, each model first generates 200 initialimages x0 given the captions, which are then re-generated over one to five iterations using inpaintingapproaches. In the verification stage, by comparing ya and yc to the original xa, we aim to identify subtlemodel fingerprints based on the edit-distance from the original data to re-generated content.We useLPIPS and CLIP similarity between image pairs to estimate their difference.6 Additionally, the consistencyof the image quality during re-generated is demonstrated in and a detailed discussion is providedin Appendix D.3.2. Distance Convergence. Similar to text re-generation experiments, the re-generation distance convergesfor both watermarking and fingerprinting settings, as illustrated in Figures 5 and11, indicated by con-sistent downward trends across datasets and distance metrics. This implies that the re-generation processconverges, with subsequent iterations yielding images that more closely resemble their predecessors. Whileour methodology proves efficacious in both scenarios, subsequent sections will prioritize the full-image regen-eration setting as it is independent of the masks, which are considered confidential to the generative model.Details about the watermarking setting can be found in Appendix B.1.1.",
  ": Verifying data generated by authentic Ga SDv2.1 and SDv 2.1 Base on Coco Dataset at variousiterations using CLIP distance": "Discrepancy.In our study, we focus on the setting of iterative re-generation through inpainting andobserve convergence that can reveal differences between models.Like the NLG section, for verification,both authentic and contrasting models perform an additional one-step re-generation of xa to obtain y andthe CLIP distance is measured between the outputs. As illustrated in , the one-step re-generationdensity distribution of the authentic model predominantly peaks at lower values across all generative models.This indicates that authentic models can effectively distinguish their outputs from those of non-authenticmodels. Then, we examine the effectiveness of inpainting as re-generation for Generation Stage. As illustrated in, re-generation can successfully enhance their fingerprints by k = 5 iterations. While most models,on average, may require fewer rounds of re-generation to deeply impart their fingerprints, with more roundsof re-generation, even older models fingerprints can be successfully enhanced. Overall, the quality of theimages is retained during re-generation, as showcased in . At the same time, the strength of thefingerprints embedded in model outputs is reinforced over iterations, as illustrated in . Verification. In accordance with the text generation framework, we utilize Algorithm 2 to evaluate ver-ification performance, selecting optimal parameters k = 5 and = 0.05 based on the results of the textgeneration experiment. As demonstrated in , models reliably identify images from the authenticmodel, with precision often exceeding 80% and recall around 85%.",
  "GPT3.5-turbo-92.068.0Zephyr88.0-67.0Mistral86.086.0-": "We consider prompt-based paraphrasing for one-stepre-generation (i.e., paraphrasing mode), opting for ak value of 5 due to its superior performance. As forbackbone models for text generation tasks, we con-sider GPT3.5-turbo, Llama2-chat-7B (Touvron et al.,2023), and Mistral (Jiang et al., 2023a).demonstrates that while paraphrasing and summa-rization tasks exhibit performance degradation com-pared to machine translation, our approach effectivelydistinguishes IPs among various models.This sup-ports the overall effectiveness of our approach acrossa wide range of text-generation tasks.",
  "Robustness EvaluationTo examine the robust-ness of our verification process, we corrupt the gen-erated outputs using paraphrasing and perturbationattacks": "Our robustness assessment commenced with a paraphrasing attack targeting image generation. This experi-ment was designed to test the efficacy of our verification process in identifying plagiarized images. Specifically,we consider images that are originally generated by one authentic model (Model A) and then re-generatedor paraphrased by another attack model (Model B), coined as A_B. We experiment with SD v2.1 as theauthentic model A and SDXL 1.0 as the paraphrasing model B. In verification, we re-generate these paraphrased A_B images using both the authentic and paraphrasingmodels to analyze the distance density distribution variations. Furthermore, we assess this paraphrasingattack at different rounds of generation xKawhere K is 1, 3, and 5 by the authentic model. The results, represented in , show comparisons between D(A_B, GA(A_B)) and D(A_B, GB(A_B)),indicating the distance between the paraphrased image (created by A then paraphrased by B) and re-generated images. We observe a significant challenge in attributing authorship to the original creator model once the image isplagiarized (even at higher values of re-generation iteration K in Algorithm 1). The distance between the",
  "Density": "M2MmBARTGPT3.5-turboGPT4 0.000.250.500.751.000.000.250.500.751.000.000.250.500.751.00 Density Distribution over Dist-BLEU (k=5) : Density distribution of one-step re-generation among four text generation models, where theinput to the one-step re-generation is the kth iteration from the authentic models. The authentic modelsfrom left to right are: 1) M2M, 2) mBART, 3) GPT3.5-turbo, and 4) GPT4. In evaluating the verification of authentic models, our methodology, as depicted in , consistentlyconfirms the authorship of all models with an accuracy exceeding 85%. Furthermore, the misclassificationrate remains below 10%.",
  "ThresholdPrecisionRecallF1 Score (Natural) (AI)": "0.010.57971.0000.7339-2.1825-0.63110.020.74181.0000.8517-1.88270.09140.030.87010.9910.9266-1.58290.81390.040.92800.9420.9349-1.28311.53640.050.95720.8500.9004-0.98332.25890.060.97160.7190.8264-0.68352.98140.070.98040.6000.7444-0.38373.7038 Distance Differentiation between Natural andAI-Generated ImagesIn order to assess the via-bility of our framework to detect plagiarism on nat-urally made images (i.e., shot on camera), we re-generate natural images from the MS COCO 2017Evaluation set (Lin et al., 2014) with previously usedimage generation models for one iteration. We com-pare the Clip distances between these natural im-ages and their respective re-generations to the dis-tance between the AI-generated images and theirre-generation for one iteration by the correspondingmodels as presented in.There is a cleardistinction in the distance density distribution whenAI-generated images are re-generated using AI our Generation Algorithm 1, the distance remains low whereasusing a natural image for re-generation results in further distances even with the most advanced AI models. To quantify this distinction, we applied various thresholds to these distances and evaluated our methodsperformance. The results, shown in Table , illustrate the trade-offs between precision and recall atdifferent thresholds. For example, at a threshold of 0.03, we achieved a high balance with a precision of0.8701, recall of 0.991, and an F1 score of 0.9266, demonstrating robustness of the method. To facilitatethreshold selection, we demonstrate the number of standard deviation from the mean CLIP distance forboth Natural and AI-generated images, = (threshold mean)/std.This standardization provides aninterpretation of each thresholds position relative to the data distribution, enhancing the explainablilityand reliability of the threshold selection.",
  "Conclusion": "In this work, we observe the intrinsic fingerprint in both text and vision generative models, which can beidentified and verified by contrasting the re-generation of the suspicious data samples by authentic modelsand contrasting models. Furthermore, we propose iterative re-generation in the Generation Stage to enhancethe fingerprints and provide a theoretical framework to ground the convergence of one-step re-generationdistance by the authentic model. Our research paves the way towards a generalized authorship authenticationfor deep generative models without (1) modifying the original generative model, (2) post-processing to thegenerated outputs, or (3) an additional model for identity classification.",
  "Limitations": "Our approach, while effective in many scenarios, has several limitations. Firstly, it assumes that malicioususers employ the authentic model and then directly utilize or disseminate them. However, users may modifythe content to circumvent IP infringements, e.g., involving other generative models. This poses a challengefor our method in protecting IP claims for altered content, which was also highlighted as a fundamentalchallenge in verifying AI-generated content (Jiang et al., 2023b). For image generation tasks, our approachrelies on the inpainting methodology, which is native to the stable diffusion set of models (as describedin C.1).As a result, we currently cannot empirically verify the effectiveness of our approach for otherimage generative models, such as GAN (Esser et al., 2021; Kang et al., 2023) and VAEs (Vahdat & Kautz,2020). Nonetheless, considering that many of the state-of-the-art image generators are based on similardiffusion based architectures, it is reasonable to hypothesize that our method is applicable to them as well.Lastly, while our approach effectively differentiates between natural and AI-generated images, determiningthe optimal threshold for this differentiation requires careful balancing of precision and recall. This needfor the hyperparemeter search highlights the complexity of developing a universally applicable verificationmethod across various content types and generation models. Darius Afchar, Vincent Nozick, Junichi Yamagishi, and Isao Echizen.Mesonet: a compact facial videoforgery detection network. In 2018 IEEE international workshop on information forensics and security(WIFS), pp. 17. IEEE, 2018.",
  "Stefan Banach. Sur les oprations dans les ensembles abstraits et leur application aux quations intgrales.Fundamenta mathematicae, 3(1):133181, 1922": "Miles Brundage, Shahar Avin, Jack Clark, Helen Toner, Peter Eckersley, Ben Garfinkel, Allan Dafoe, PaulScharre, Thomas Zeitzoff, Bobby Filar, et al. The malicious use of artificial intelligence: Forecasting,prevention, and mitigation. arXiv preprint arXiv:1802.07228, 2018. Mathilde Caron, Hugo Touvron, Ishan Misra, Herv Jgou, Julien Mairal, Piotr Bojanowski, and ArmandJoulin.Emerging properties in self-supervised vision transformers.In Proceedings of the IEEE/CVFinternational conference on computer vision, pp. 96509660, 2021. Ching-Yun Chang and Stephen Clark. Practical Linguistic Steganography using Contextual Synonym Sub-stitution and a Novel Vertex Coding Method. Computational Linguistics, 40(2):403448, 06 2014. ISSN0891-2017. doi: 10.1162/COLI_a_00176. URL",
  "Krzysztof Ciesielski. On stefan banach and some of his results. Banach Journal of Mathematical Analysis,1(1):110, 2007": "Ingemar J. Cox, Matthew L. Miller, Jeffrey A. Bloom, Jessica Fridrich, and Ton Kalker. Digital Watermarkingand Steganography. The Morgan Kaufmann Series in Multimedia Information and Systems, 2 edition, 2008. Ricard Durall, Margret Keuper, and Janis Keuper. Watch your up-convolution: Cnn based generative deepneural networks are failing to reproduce spectral distributions. In Proceedings of the IEEE/CVF conferenceon computer vision and pattern recognition, pp. 78907899, 2020. Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis.In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1287312883,2021. Angela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi Ma, Ahmed El-Kishky, Siddharth Goyal, MandeepBaines, Onur Celebi, Guillaume Wenzek, Vishrav Chaudhary, et al. Beyond english-centric multilingualmachine translation. The Journal of Machine Learning Research, 22(1):48394886, 2021. Pierre Fernandez, Alexandre Sablayrolles, Teddy Furon, Herv Jgou, and Matthijs Douze. Watermark-ing images in self-supervised latent spaces. In ICASSP 2022 - 2022 IEEE International Conference onAcoustics, Speech and Signal Processing (ICASSP), pp. 30543058, 2022. doi: 10.1109/ICASSP43922.2022.9746058.",
  "Julian Hazell. Large language models can be used to effectively scale spear phishing campaigns. arXivpreprint arXiv:2305.06972, 2023": "Xuanli He, Qiongkai Xu, Lingjuan Lyu, Fangzhao Wu, and Chenguang Wang.Protecting intellec-tual property of language generation apis with lexical watermark.Proceedings of the AAAI Confer-ence on Artificial Intelligence, 36(10):1075810766, Jun. 2022a. doi: 10.1609/aaai.v36i10.21321. URL Xuanli He, Qiongkai Xu, Yi Zeng, Lingjuan Lyu, Fangzhao Wu, Jiwei Li, and Ruoxi Jia. CATER: Intellectualproperty protection on text generation APIs via conditional watermarks. In Alice H. Oh, Alekh Agarwal,Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022b.URL",
  "Alain Hore and Djemel Ziou. Image quality metrics: Psnr vs. ssim. In 2010 20th international conferenceon pattern recognition, pp. 23662369. IEEE, 2010": "Betina Idnay, Caitlin Dreisbach, Chunhua Weng, and Rebecca Schnall. A systematic review on naturallanguage processing systems for eligibility prescreening in clinical research.Journal of the AmericanMedical Informatics Association, 29(1):197206, 2022. Daphne Ippolito, Daniel Duckworth, Chris Callison-Burch, and Douglas Eck. Automatic detection of gener-ated text is easiest when humans are fooled. In Proceedings of the 58th Annual Meeting of the Associationfor Computational Linguistics, pp. 18081822, Online, July 2020. Association for Computational Linguis-tics. doi: 10.18653/v1/2020.acl-main.164. URL Ganesh Jawahar, Muhammad Abdul Mageed, and VS Laks Lakshmanan. Automatic detection of machinegenerated text: A critical survey. In Proceedings of the 28th International Conference on ComputationalLinguistics, pp. 22962309, 2020. Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diegode las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b.arXiv preprint arXiv:2310.06825, 2023a. Zhengyuan Jiang, Jinghuai Zhang, and Neil Zhenqiang Gong. Evading watermark based detection of ai-generated content. In Proceedings of the 2023 ACM SIGSAC Conference on Computer and Communica-tions Security, CCS 23, pp. 11681181, New York, NY, USA, 2023b. Association for Computing Machin-ery. ISBN 9798400700507. doi: 10.1145/3576915.3623189. URL Minguk Kang, Jun-Yan Zhu, Richard Zhang, Jaesik Park, Eli Shechtman, Sylvain Paris, and Taesung Park.Scaling up gans for text-to-image synthesis. In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition, pp. 1012410134, 2023.",
  "Chin-Yew Lin. ROUGE: A package for automatic evaluation of summaries. In Text Summarization BranchesOut, pp. 7481, Barcelona, Spain, July 2004. Association for Computational Linguistics. URL": "Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollr, andC Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer VisionECCV 2014: 13thEuropean Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pp. 740755.Springer, 2014. Zhengzhe Liu, Xiaojuan Qi, and Philip HS Torr. Global texture enhancement for fake face detection inthe wild. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp.80608069, 2020. Brady D Lund, Ting Wang, Nishith Reddy Mannuru, Bing Nie, Somipam Shimray, and Ziang Wang. Chatgptand a new academic reality: Artificial intelligence-written research papers and the ethics of the large lan-guage models in scholarly publishing. Journal of the Association for Information Science and Technology,74(5):570581, 2023. Inbal Magar and Roy Schwartz. Data contamination: From memorization to exploitation. In Proceedings ofthe 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pp.157165, 2022. Francesco Marra, Diego Gragnaniello, Luisa Verdoliva, and Giovanni Poggi. Do gans leave artificial fin-gerprints? In 2019 IEEE Conference on Multimedia Information Processing and Retrieval (MIPR), pp.506511, 2019. doi: 10.1109/MIPR.2019.00103. Eric Mitchell, Yoonho Lee, Alexander Khazatsky, Christopher D Manning, and Chelsea Finn. Detectgpt:Zero-shot machine-generated text detection using probability curvature. In International Conference onMachine Learning, pp. 2495024962. PMLR, 2023.",
  "Maximilian Mozes, Xuanli He, Bennett Kleinberg, and Lewis D Griffin. Use of llms for illicit purposes:Threats, prevention measures, and vulnerabilities. arXiv preprint arXiv:2308.12833, 2023": "Konstantin A. Pantserev.The Malicious Use of AI-Based Deepfake Technology as the New Threat toPsychological Security and Political Stability, pp. 3755.Springer International Publishing, Cham,2020. ISBN 978-3-030-35746-7. doi: 10.1007/978-3-030-35746-7_3. URL Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluationof machine translation. In Proceedings of the 40th Annual Meeting of the Association for ComputationalLinguistics, pp. 311318, Philadelphia, Pennsylvania, USA, July 2002. Association for ComputationalLinguistics. doi: 10.3115/1073083.1073135. URL",
  "Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditionalimage generation with clip latents": "Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, andIlya Sutskever. Zero-shot text-to-image generation. In International Conference on Machine Learning, pp.88218831. PMLR, 2021. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjrn Ommer. High-resolutionimage synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition (CVPR), pp. 1068410695, June 2022.",
  "Stu Sjouwerman. How ai is changing social engineering forever. Forbes, 2023. URL": "Irene Solaiman, Miles Brundage, Jack Clark, Amanda Askell, Ariel Herbert-Voss, Jeff Wu, Alec Radford,Gretchen Krueger, Jong Wook Kim, Sarah Kreps, et al.Release strategies and the social impacts oflanguage models. arXiv preprint arXiv:1908.09203, 2019a. Irene Solaiman, Miles Brundage, Jack Clark, Amanda Askell, Ariel Herbert-Voss, Jeff Wu, Alec Radford,and Jasmine Wang. Release strategies and the social impacts of language models. CoRR, abs/1908.09203,2019b. URL Yuqing Tang, Chau Tran, Xian Li, Peng-Jen Chen, Naman Goyal, Vishrav Chaudhary, Jiatao Gu, andAngela Fan.Multilingual translation from denoising pre-training.In Findings of the Association forComputational Linguistics: ACL-IJCNLP 2021, pp. 34503466, Online, August 2021. Association forComputational Linguistics. doi: 10.18653/v1/2021.findings-acl.304. URL",
  "Edward Tian. Gptzero, 2023. URL": "Umut Topkara, Mercan Topkara, and Mikhail J. Atallah. The hiding virtues of ambiguity: Quantifiablyresilient watermarking of natural language text through synonym substitutions. In Proceedings of the 8thWorkshop on Multimedia and Security, MM&Sec 06, pp. 164174, New York, NY, USA, 2006. Associationfor Computing Machinery. ISBN 1595934936. doi: 10.1145/1161366.1161397. URL Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bash-lykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tunedchat models. arXiv preprint arXiv:2307.09288, 2023.",
  "Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Simoncelli. Image quality assessment: from errorvisibility to structural similarity. IEEE transactions on image processing, 13(4):600612, 2004": "Zijie J Wang, Evan Montoya, David Munechika, Haoyang Yang, Benjamin Hoover, and Duen Horng Chau.Diffusiondb: A large-scale prompt gallery dataset for text-to-image generative models. arXiv preprintarXiv:2210.14896, 2022. Qiongkai Xu and Xuanli He. Security challenges in natural language processing models. In Proceedings ofthe 2023 Conference on Empirical Methods in Natural Language Processing: Tutorial Abstracts, pp. 712,2023. Qiongkai Xu, Xuanli He, Lingjuan Lyu, Lizhen Qu, and Gholamreza Haffari. Student surpasses teacher:Imitation attack for black-box NLP APIs. In Proceedings of the 29th International Conference on Compu-tational Linguistics, pp. 28492860, Gyeongju, Republic of Korea, October 2022. International Committeeon Computational Linguistics. URL Kuan Yang, Kejiang Chen, Weiming Zhang, and Nenghai Yu. Provably secure generative steganographybased on autoregressive model. In Chang D. Yoo, Yun-Qing Shi, Hyoung Joong Kim, Alessandro Piva, andGwangsu Kim (eds.), Digital Forensics and Watermarking, pp. 5568, Cham, 2019. Springer InternationalPublishing. ISBN 978-3-030-11389-6.",
  "Weike You, Hong Zhang, and Xianfeng Zhao. A siamese cnn for image steganalysis. IEEE Transactions onInformation Forensics and Security, 16:291306, 2020": "Ning Yu, Larry S Davis, and Mario Fritz. Attributing fake images to gans: Learning and analyzing ganfingerprints. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 75567566,2019. Aditi Zear, Amit Kumar Singh, and Pardeep Kumar. A proposed secure multiple watermarking techniquebased on dwt, dct and svd for application in medicine. Multimedia tools and applications, 77:48634882,2018.",
  "B.1.1Convergence of Watermarked Images": "The iterative masked inpainting procedure displays consistent convergence behavior across models. With afixed binary mask covering 1/10th of the image, the distance between successive image generations decreasesrapidly over the first few iterations before stabilizing. This is evidenced by the declining trend in metricslike MSE, LPIPS, and CLIP similarity as iterations increase. The early convergence suggests the generative models are effectively reconstructing the masked regions in acoherent manner. While perfect reconstruction is infeasible over many passes, the models appear to reachreasonable stability within 5 iterations as shown in and 11. Convergence to a stable equilibrium highlights latent fingerprints in the model behavior. The consistent self-reconstruction statistics form the basis for distinguishing authentic sources in the subsequent fingerprintingexperiments. The watermarking convergence analysis highlights model stability and confirms that iterativeinpainting effectively removes embedded watermarks without degrading image quality (see Figures in D.3.2).",
  "y[t] = G(x, M[t])(11)": "By analyzing reconstructed segments y[t] and the composited image y, model-specific artifacts can be quanti-fied without additional pattern information. The non-overlapping reconstructed segments are merged to formthe composite image y, we can obtain this by using the corresponding mask and extracting the reconstructedportion. A salient feature of our proposed re-generation paradigm is its independence from any additionalinformation about the image partitioning pattern. The models unique fingerprint emerges naturally duringre-generation, regardless of how the image is divided or the components are merged. We experiment on 5latest SD models - SDv 2.1, SD v2.1B, SD v2, SDXL 1.0, and SDXL0.9B using 8 segmented masks eachcovering 1/8th of the image thereby totaling full image coverage. Model fingerprints are identified throughLPIPS, and CLIP similarities between original xa and reconstructed y.",
  "B.2.1Convergence of Re-generated Images": "Enhancing fingerprint re-generation shows consistent convergence in perceptual metrics like CLIP and LPIPSwithin four iterations (see and 13). However, traditional metrics such as MSE and SSIM lack clearconvergence, suggesting inpainting effectively captures visual content but not at the pixel level. The modelsconverge to unique stable points, revealing inherent fingerprints based on their biases and training data. Thisdivergence is important for model attribution. Overall, re-generation effectively exposes these fingerprintswhile maintaining visual integrity, underscoring perceptual superiority over pixel-based metrics in evaluatinggenerative model fingerprints.",
  "C.1Image Generation Models": "We consider eight models based on the Stable Diffusion architecture (Rombach et al., 2022). These modelsleverage the architecture and differ primarily in their training schemes. The models are selected to span arange of architectures, training schemes, and dataset sizes. This diversity allows us to explore model-specificbehaviors for attribution and stability analysis. All models support inpainting, allowing images to be edited given a mask and image. We utilize the inpaintingpipeline - StableDiffussion and StableDiffusionXL - provided by HuggingFace (von Platen et al., 2022).7 BothSD v1.4 and SD v2 checkpoints were initialized with the weights of the SD v1.4 checkpoints and subsequentlyfine-tuned on laion-aesthetics v2 5+. SDXL 1.0 employs a larger UNet backbone and a more potent textencoder (Podell et al., 2023). BKSDM is designed for efficient text-to-image synthesis, removing blocks fromthe U-Net and undergoing distillation pre-training on 0.22M LAION text-image pairs (Kim et al., 2023).",
  ": The convergence analysis of the distances in iteration based on various metrics on the re-generatedimages of 200 samples from Coco dataset": "Image Inpainting:Image inpainting refers to filling in missing or masked regions of an image to recon-struct the original intact image. A fixed binary mask is applied to cover certain areas of the image. Thebinary masks are generated as blank images filled with random white pixels. To reconstruct images, thebinary mask and original image are passed to the generative models inpainting pipeline, where the valuesof the pixels in the masked areas are predicted based on the unmasked context. After inpainting, the re-constructed masked regions are merged back into the re-generated image for future iteration. We utilize theinpainting pipeline8 - StableDiffussion and StableDiffusionXL - provided by HuggingFace (von Platen et al.,2022) that enables us to regenerate a given image multiple times by masking different parts of the image.",
  "C.2Distance Metrics for Images": "Contrastive Language-Image Pretraining (CLIP) Cosine DistanceThe CLIP model encodes im-ages into high-dimensional feature representations that capture semantic content and meaning (Radfordet al., 2021). With pre-trained image-text models, we can easily capture the semantic similarity of twoimages, which is their shared meaning and content, regardless of their visual appearance. CLIP uses a vision transformer model as the image encoder. The output of the final transformer block canbe interpreted as a semantic feature vector describing the content of the image. Images with similar contentwill have feature vectors close together or aligned in the embedding space.",
  "ModelDescription": "Model1Stable Diffusion 1.5: Fine-tuned from SD1.4 for 595k steps (Rombach et al., 2022)Model2Stable Diffusion 2.1 (SDv 2.1): Fine-tuned from SDvTwo for 210k steps (Rombach et al.,2022)Model3Stable Diffusion 1.4 (SD v1.4) : Initialized from SDv1.2 weights (Rombach et al., 2022)Model4Stable Diffusion XL 1.0 (SDXL 1.0): Larger backbone, trained on LAION-5B (Podell et al.,2023)Model5Block-removed Knowledge-distilled Stable Diffusion Model (Kim et al., 2023)Model6Stable Diffusion 2 (SD v2) (Rombach et al., 2022)Model7Stable Diffusion v 2.1 Base (SD v2.1B) (Rombach et al., 2022)Model8Stable Diffusion XL 0.9 (SDXL0.9B) (Podell et al., 2023)",
  "(14)": "By comparing features across corresponding layers of the CNN, LPIPS can provide a fine-grained distancemeasuring subtle perceptual differences imperceptible in pixel space (Zhang et al., 2018).For example,changes to color scheme or artistic style that maintain semantic content will have a low CLIP distance buta higher LPIPS distance. We use the original implementation of Zhang (2023)",
  "SD v2.10.9760.0151.0000.001SDXL 1.00.9430.211.0000.004SD v20.9750.0161.0000.001SD v2.1B0.9700.0231.0000.002SDXL 0.90.9460.0200.9990.005": "From , we csn observe LPIPS as a distance metric consistently produced lower L values comparedto Euclidean distance. This demonstrates LPIPS as a superior metric for our framework, explaining whyEuclidean distance fails for verification. Additionally, more advanced models exhibited improved L values,indicating better pertubation resilience and verification capacity. Our approach focuses on leveraging inherentmodel properties for verification rather than enforcing universal viability. For models currently unsupported,we suggest targeting enhancements to model robustness in line with these observations.",
  "C.4.2Polo Dataset Prompts": "A renaissance portrait of Dwayne Johnson, art in the style of Rembrandt!! Intricate. Ultra de-tailed, oil on canvas, wet-on-wet technique, pay attention to facial details, highly realistic, cinematiclightning, intricate textures, illusionistic detail. Epic 3D, become legend shiji! GPU mecha controlled by telepathic hackers, made of liquid, bubbles,crystals, and mangroves, Houdini SideFX, perfect render, ArtStation trending, by Jeremy Mann,Tsutomu Nihei and Ilya Kuvshinov.",
  "C.5Density Distribution of a one-step re-generation": "The one-step re-generation density distributions reveal distinct model-specific characteristics, enabling dis-crimination as seen in and 15, Most models exhibit distinction, with each distribution showingunique traits. A notable exception in this behavior is observed for the SD v2.1B (see Figures 14 and 15). which initiallydemonstrates less discrimination. However, over extended iterations, SD v2.1B shows marked improvement,highlighting the capacity for iterative refinement. By the 5th iteration, there is a noticiable improvementin the discriminative nature of its one-step re-generation. This improvement is crucial as it highlights themodels capacity to refine and enhance its re-generative characteristics over time. While LPIPS is not immediately effective in pinpointing the authentic model at the very first step, it stilloffers a powerful mechanism to distinguish between models. LPIPS is effective at differentiating betweenfamilies of models, such as the Stable Diffusion models and the Stable Diffusion XL models, as visualizedin for the Polo dataset and for the Coco dataset. The lack of effectiveness of LPIPS inidentifying the authentic model is a primary reason why it was not chosen for verification. Further insights into the models discriminative capabilities can be derived from and 12, whileSDv2.1B starts with lower accuracy in distinguishing itself, a significant improvement in accuracy is seenacross iterations. The initially anomalous behavior transitions into more discriminating re-generation.",
  "Initial Image": "SDv2.1SDXLv1.0SDv2SDv2.1 BaseSDXL Base0.9 0.020.040.060 Iteration 1 0.020.040.060 Iteration 3 0.020.040.060 Iteration 5 : One Step Re-generation for various Authentic Models on the Coco Dataset using the LPIPSmetric at different iterations. The authentic models from top to bottom are: 1) SDv 2.1, 2) SD v2, 3)SD v2.1B, 4) SDXL 1.0, 5) SDXL0.9B.",
  "D.1Density distribution of a one-step re-generation": "In , we illustrate the density distribution of one-step re-generation for four text generation models,using the first iteration from the authentic models as input. Excluding Cohere, the density distributionsof the authentic models are discernible from those of the contrast models across BLEU, ROUGE-L, andBERTScore metrics. 0.000.250.500.751.00 0.0 0.2 0.4 0.6 0.8 1.0",
  "D.2Experiments for GPT4": "In this section, we analyze the characteristics of GPT4. We first present the density distribution when theinput is the kth iteration from the authentic models in . Similarly, as k increases, the differencebetween the authentic models distribution and the contrast models intensifies. Nonetheless, distinguishingbetween GPT3.5-turbo and GPT4 proves difficult, particularly when GPT3.5-turbo serves as the authenticmodel, even for larger values of k. Detailed examination reveals that GPT4s one-step re-generation bearsresemblance to the k-th iteration of GPT3.5-turbo. This might stem from GPT3.5-turbo and GPT4 originat-ing from the same institution, suggesting potential similarities in architecture and pre-training data. Thus,we contend that models stemming from the same institution inherently share identical intellectual property,thus obviating the possibility of intellectual property conflicts. Interestingly, when GPT4 is the authenticmodel, our methodology can differentiate it from GPT3.5-turbo, attributed to the marked difference betweenGPT3.5-turbos one-step re-generation and GPT4s k-th iteration. This distinction, we surmise, is due toGPT4s superior advancement over GPT3.5-turbo.",
  "D.3.1Text Re-generations": "In this section, we examine the quality of re-generated sentences. As illustrated in , the re-generatedsentences display high quality across most evaluation metrics, except for Cohere. The slower convergenceobserved for Cohere can be attributed to its subpar re-generation quality, which significantly deviates fromthat of other models. For further illustration, refer to the samples and their re-generations in -17.",
  "D.3.2Image Re-generations": "An integral part of our evaluation process involves understanding the reproducibility of the images generatedby different models across iterations. To this end, we present image re-generations for various authenticmodels in -26. These re-generations provide insights into the consistency and stability of eachmodel when tasked with reproducing the same visual content over multiple iterations.",
  "Sample 1:": "Original input: Working in groups of 2 or 4 (even numbers may work better later on), suggest a heuristic to evaluatethe utility of a given board configuration.Iteration 1: Working in groups of 2 or 4 (even numbers may work better later), propose a heuristic to evaluate theusefulness of a given board configuration.Iteration 2: Working in groups of 2 or 4 (even numbers may work better later), propose a heuristic to evaluate theusefulness of a given board configuration.Iteration 3: Working in groups of 2 or 4 (even numbers may work better later), propose a heuristic to evaluate theusefulness of a given board configuration.Iteration 4: Working in groups of 2 or 4 (even numbers may work better later), propose a heuristic to assess theusefulness of a given board configuration.Iteration 5: Working in groups of 2 or 4 (even numbers may work better later), propose a heuristic to evaluate theusefulness of a given board configuration.",
  "Sample 2:": "Original input: Therefore, to improve the ethical coping ability of Chinese NLP techniques and their applicationeffectiveness in the field of ethics, the Chinese ethics knowledge base and KEPTMs for ethics domain are researchedIteration 1: Therefore, to improve the ethical management capacity of Chinese NLP techniques and their applicationeffectiveness in the field of ethics, the Chinese ethical knowledge base and KEPTMs for the field of ethics are beingstudied.Iteration 2: Therefore, to improve the ethical management capacity of Chinese NLP techniques and their applicationeffectiveness in the field of ethics, the Chinese ethical knowledge base and KEPTMs for the field of ethics are understudy.Iteration 3: Therefore, to improve the ethical management capacity of Chinese NLP techniques and their applicationeffectiveness in the field of ethics, the Chinese ethical knowledge base and KEPTMs for the field of ethics are currentlyunder study.Iteration 4: Therefore, to improve the ethical management capacity of Chinese NLP techniques and their applicationefficiency in the field of ethics, the Chinese ethical knowledge base and KEPTMs for the field of ethics are currentlyunder study.Iteration 5: Therefore, to improve the ethical management capacity of Chinese NLP techniques and their applicationefficiency in the field of ethics, the Chinese ethical knowledge base and KEPTMs for the field of ethics are currentlyunder study.",
  "Sample 3:": "Original input: I kindly request a modification to the due date for my assignment. According to the previous thread,I was informed that I would receive an extension of 33 hours for this assignment. However, as of now, my Moodleaccount still indicates the original due date.Iteration 1: I kindly request a modification of the due date for my assignment. According to the previous thread, Iwas informed that I would receive a 33-hour extension for this assignment. However, at the present time, my Moodleaccount still shows the initial due date.Iteration 2: I kindly request a change in the due date for my assignment. According to the previous thread, I wasinformed that I would receive a 33-hour extension for this assignment. However, at present, my Moodle account stilldisplays the initial due date.Iteration 3: I kindly request a change of the due date for my assignment. According to the previous thread, I wasinformed that I would receive a 33-hour extension for this assignment. However, at present, my Moodle account stilldisplays the initial due date.Iteration 4: I am kindly requesting a change in the due date for my assignment. According to the previous thread, Iwas informed that I would receive a 33-hour extension for this task. However, at the present time, my Moodle accountstill displays the initial due date.Iteration 5: I kindly request a change of the due date for my assignment. According to the previous thread, I wasinformed that I would receive a 33-hour extension for this task. However, at the present time, my Moodle account stilldisplays the initial due date."
}