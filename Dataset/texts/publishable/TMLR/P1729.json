{
  "Abstract": "Deep Neural Networks (DNNs) are known to be vulnerable to backdoor attacks, whereattackers can inject hidden backdoors during the training stage. This poses a serious threatto the Model-as-a-Service setting, where downstream users directly utilize third-party models(e.g., HuggingFace Hub, ChatGPT). To this end, we study inference-stage black-box backdoordetection problem in the paper, where defenders aim to build a firewall to filter out thebackdoor inputs in the inference stage, with only input samples and prediction labels available.Existing investigations on this problem either rely on strong assumptions on types of triggersand attacks or suffer from poor efficiency. To build a more generalized and efficient method,we first provide a novel causality-based lens to analyze heterogeneous prediction behaviorsfor clean and backdoored samples in the inference stage, considering both sample-specific andsample-agnostic backdoor attacks. Motivated by the causal analysis and do-calculus in causalinference, we introduce Black-box Backdoor detection under the Causality Lens (BBCaL)which distinguishes backdoor and clean samples by analyzing prediction consistency afterprogressively constructing counterfactual samples. Theoretical analysis also sheds light onthe effectiveness of the BBCaL. Extensive experiments on three benchmark datasets validatethe effectiveness and efficiency of our method.",
  "Introduction": "Deep neural networks (DNNs) have achieved tremendous success in various applications (Kortli et al., 2020;Zou et al., 2023; Vaswani et al., 2017; Sun et al., 2022a; Zhou et al., 2024a;b). Despite these successes,training large DNNs requires considerable time and computational resources. Consequently, many users optto utilize third-party pre-trained models through API requests (e.g., ChatGPT), or directly download them",
  "from online platforms (e.g., ModelZoo, HuggingFace Hub). This setting is what we call Model-as-a-Service(MaaS) (Sun et al., 2022b; Li et al., 2017; Roman et al., 2009)": "However, DNNs are easily attacked by injecting imperceptible backdoors during the training stage (Gu et al.,2017; Chen et al., 2017; Nguyen & Tran, 2021). After backdoor injection, the DNNs prediction results canbe maliciously manipulated by the adversaries whenever the input sample contains the pre-defined triggerpattern, while it behaves normally when the input sample is benign. This vulnerability poses a serious threatto the MaaS setting, especially in some safety-critical applications such as autonomous driving (Han et al.,2022; Chan et al., 2022), medical diagnosis (Feng et al., 2022) and financial fraud detection (Lunghi et al.,2023). To mitigate the threat in the MaaS setting, inference-stage black-box backdoor detection has drawn increasingresearch interest. In this scenario, defenders aim to establish a firewall-style detector on the user side to rejectpredictions for backdoored samples while forwarding predictions for clean samples. Several approaches havebeen proposed to address this issue (Guo et al., 2023; Liu et al., 2023). However, some crucial challenges stillremain. Strong Assumptions and Low Generalization: Most defenses rely on strong assumptionsabout the trigger pattern (Gao et al., 2019; 2021; Guo et al., 2023) (e.g., sample-agnostic trigger) and areonly feasible for specific types of attacks (Guo et al., 2023; Pal et al., 2024), which is impractical in MaaSscenarios where defenders lack information about attacks and triggers. Low Efficiency: Some detectionalgorithms (Liu et al., 2023) require extensive, computationally expensive image corruptions, leading toprolonged inference time that could degrade user experience. In light of these disadvantages, our objective isto develop a generalized and efficient inference-stage detection algorithm. This algorithm allows defendersaccess only to the inference samples and their prediction labels generated by the DNNs, without any priorinformation about the target model and trigger patterns. To this end, we believe it is crucial to first address a fundamental question: What is the inner mechanism thatcauses a backdoored DNN to behave normally with clean samples, but consistently predicts the target labelwhen presented with backdoored samples? To explore this, we introduce causal inference as a novel perspectiveto unravel the mechanisms behind the heterogeneous prediction behaviors of clean and backdoored samplesthrough causal graphs. Specifically, our causal analysis derives that predictions of backdoored samples areprimarily misled by the spurious path introduced by the attacks, whereas those of clean samples follow theoriginal causal path. Notably, our causal analysis includes both sample-agnostic attacks, where triggersare identical across all samples, and sample-specific attacks, where triggers are customized for each sample.Analyzing this wide range of triggers through causal graphs enables us to develop a more generalized detectionmethod. Following these insights, a nave detection method would be identifying the causal path that a sample followsduring the inference stage. However, this approach is infeasible due to the limited information available.Therefore, motivated by causal interventions, which is used in causal inference to compare different potentialoutcomes under different variable interventions, we develop a Black-box Backdoor detection method underthe Causality Lens (BBCaL). Specifically, we employ a simple yet effective counterfactual interventionmethod by progressively adding noise to the input sample. This approach implicitly induces distinguishableprediction behaviors between clean and backdoored samples, with theoretical analysis further elucidating itseffectiveness. Extensive experiments demonstrate that our method can defend against a broader range ofattacks with satisfactory efficiency. In summary, the contributions of this paper include: (1) Novel Causality Lens. From the novel causalitylens, we analyze the distinct prediction behaviors of clean images and various types of backdoored imagesin the MaaS setting. (2) Counterfactual Backdoor Detection Algorithm. Leveraging the causalanalysis, we develop a causality-inspired detection method that distinguishes backdoor and clean samples byconstructing counterfactual interventions and checking the consistency of the predictions. (3) TheoreticalProof. We provide a theoretical analysis (Theorem 8) to validate our method. (4) Generaliztion andEfficieny. Various experiments across multiple popular datasets have empirically proven that our detectionmethod achieves an average of > 84% AUC under a wide range of attacks with only an ignorable overhead.",
  "Related Work": "Backdoor Attacks. Backdoor attacks are usually launched by poisoning training dataset. Specifically,malicious attackers inject trigger patterns into victim samples and alter the ground-truth label to a predefinedtarget label. Recent research can be divided into two categories on making the backdoor attacks more stealthyto enhance the attacks feasibility in practice. The first direction aims to make the trigger pattern visuallyless noticeable (Chen et al., 2017; Liu et al., 2020; Qi et al., 2022). For example, (Chen et al., 2017) blends theclean images with random pixels, and (Liu et al., 2020) uses the natural reflection to construct the backdoortrigger. The other direction aims to make the attacking process less noticeable (Shafahi et al., 2018; Souriet al., 2021; Zeng et al., 2022; Guan et al., 2024b; 2023b; Guo et al., 2024). For example, CL (Shafahi et al.,2018) proposes a clean-label attack, which constructs backdoor samples without changing labels. Backdoor Defenses. To mitigate the threat of backdoor attacks, various defense methods have beenproposed. As in (Li et al., 2022), we categorize the existing defense methods into five categories. First,detection-based defenses (Gao et al., 2019; Guo et al., 2021; Xiang et al., 2022; Guan et al., 2023a; Guo et al.,2023; Ma et al., 2022; Guan et al., 2024a) aim to explore whether the backdoors exist in the model. Second,preprocessing-based defenses (Doan et al., 2020; Shi et al., 2023) further introduce a preprocessing moduleso that triggers can be inactivated. Third, defenses based on model reconstruction (Liu et al., 2018; Zhaoet al., 2020) directly eliminate the effect of backdoors by tuning model weights or adjusting model structures.Fourth, defenses based on trigger synthesis (Wang et al., 2019; Chen et al., 2022) first reverse engineer triggerpatterns and then suppress the triggers effects. Lastly, training-sample-filtering-based defenses (Li et al.,2021a; Huang et al., 2022a) work by first filtering backdoor samples from the training dataset, then trainingthe model exclusively in the remaining dataset. Causal Inference in Backdoor Learning. Causal inference (Yao et al., 2021; Hu et al., 2024; Chu et al.,2024) has been utilized as a tool to analyze attacks, defenses, and detection mechanisms in AI security (Huanget al., 2022b; Ren et al., 2022; Tople et al., 2020). For instance, (Ren et al., 2022) explores the causalitybetween deep neural network outputs and subregions of input samples to uncover the working mechanismof adversarial examples. (Huang et al., 2022b) models the data generation process using structural causalmodels to distinguish between benign and malicious out-of-distribution (OOD) data. Additionally, (Topleet al., 2020) proposes using causal learning to mitigate privacy attacks. However, their causality analysisfocuses on other well-established problems, such as OOD detection, adversarial example detection, andprivacy attacks, which are fundamentally different from our backdoor detection problem. Although previousworks (Zhang et al., 2023; Liu et al., 2024) have also explored the use of causal graphs in backdoor attacks,our causality analysis is fundamentally different from them: Their analysis aims to provide an analysisfor training a clean model from a backdoored dataset, while our analysis aims to investigate thedistinct prediction behaviors in the inference stage for clean and backdoored images. A more detailedcomparison of the two papers is presented in Appendix O. To the best of our knowledge, this paper is thefirst to provide a causality analysis of backdoor attacks in the inference stage.",
  "Main Pipeline of Backdoor Attacks": "Let D = {xi, yi}ni=1 denote the original dataset, where xi Rn denotes an image sample, and yi denotesthe corresponding ground-truth label. The deployed neural network model is denoted as f, with as thetrainable parameters. Then the malicious backdoor attacker selects a subset of the original dataset (denotedas Dc) and modifies it to a backdoored version with Db = {(xi, yt)|xi = xi + (xi), (xi, yi) Dc}, where ytdenotes the target label, and () is a pre-defined trigger generation function. For example, in BadNet (Guet al., 2017), the trigger generation function can be formulated as (xi) = (t xi) m, where t denotesthe trigger, and m is a binary mask defining the position of the trigger; in Blend (Chen et al., 2017), thetrigger generation function can be formulated as (xi) = t, where t denotes the global trigger such as randomnoise. Following the taxonomy in (Li et al., 2022), we categorize backdoor attacks into sample-agnosticand sample-specific based on the properties of trigger function () and provide the corresponding formaldefinitions as follows:",
  "Problem Formulation": "In this paper, we consider inference-stage backdoor detection under the black-box setting for the MaaSfollowing (Guo et al., 2023; Liu et al., 2023). Specifically, the defender is assumed to only have access toinput images and their prediction labels generated by the DNN, without any prior information about thebackdoor attacks or the model. Our problem is hereby formally stated as follows:",
  "Backdoor Attacks under a Causality Lens": "To unravel the complex problem 3, it is necessary to first answer a pivotal question: What causes backdooredDNNs to consistently predict the target label for backdoored samples, yet behave normally for clean samples?Motivated by causal inference (Xiao et al., 2023; Zhang et al., 2023), we propose using a novel causal lensto explore this question. Specifically, we construct causal graphs, which are directed acyclic graphs thatillustrate the causal relationships between variables, for both clean and backdoored samples (see ).We provide a detailed analysis in the subsequent sections. Causal Graph for Clean Samples. As shown in (a), for a well-trained DNN, the predictionY of a clean image is dependent on the image content I, which consists of both semantic features S andbackground features B. Thus, the causal relationship between S, B, and I is represented as (S I B).For instance, consider a fish image I, where pixels related to the fish per se are the semantic features S,and pixels related to the water and aquatic plants are background features B. A well-trained DNN makesprediction by leveraging both fish pixels and background pixels contained in I, denoted as I Y . This isreferred to as the direct causal path. For backdoor samples, the analysis becomes more complicated. As illustrated in (b) and (c), backdoorattacks A modify images I by injecting triggers and altering image labels to the target label, represented asI A Y . This introduces a spurious path from I to Y , which lies outside the direct causal path (I Y ).The attacks thereby serve as a confounder, which builds and strengthens the erroneous correlations betweenthe modified images and the target label. Consequently, predictions of backdoored images are predominantlyled by this spurious path (I A Y ) (Du et al., 2021; Zhang et al., 2023), while the direct causal path(I Y ) plays a minor role, symbolized by a gray dotted line in (a) and (c). This phenomenon hasbeen empirically highlighted in previous papers (Cai et al., 2022; Yu et al., 2021; Sandoval-Segura et al.,2022). To develop a defense method with advanced generative capabilities across a wide range of backdoorattacks, we explore not only attacks based on sample-agnostic triggers but also thoes with sample-specifictriggers (described in 3.1). Specifically, we construct causal graphs for these two types of backdoor attacksas follows. Causal Graph for Sample-agnostic Backdoor Attacks. Sample-agnostic backdoor attacks imply thatall backdoor samples are constructed with a constant trigger pattern (see Definition 1). Therefore, validsample-agnostic attacks A in (b) are solely dependent on the trigger patterns T while independent ofthe image content, denoted as T A. Causal Graph for Sample-Specific Backdoor Attacks. Sample-specific backdoor attacks imply thateach backdoor sample is paired with a unique trigger, which is invalid for any other sample. Therefore,sample-specific attacks in (c) depend on both the trigger patterns T and the images I.",
  "Methodology": "In this section, according to the causal analysis above, we first introduce counterfactual intervention todistinguish clean and backdoored samples by analyzing prediction consistency after progressively constructingcounterfactual samples. Causal analysis and theoretical proof are also provided to explain the mechanism ofour defense method. Then, we introduce the detailed method to detect backdoor attacks in the second step.",
  "Step 1: Designing and Conducting Counterfactual Intervention": "The above analysis sheds light on the intuition that clean samples and backdoor samples can be distinguishedby evaluating whether the models predictions are primarily influenced by the direct causal path or thespurious path. However, the evaluations are challenging due to the black-box nature of DNNs and the limitedinformation available in our setting. In causal inference, do-calculus is usually used to compare differentpotential outcomes under different counterfactual interventions on a specific variable. Specifically, do-calculusperforms counterfactual interventions by changing the original values of one variable t to a counterfactualvalue t (Guo et al., 2020; Yao et al., 2021). This method inspires us with the insight that, due to theintrinsically different causal rationales behind clean and backdoored samples, as demonstrated in the previous",
  "causal graphs, when the same counterfactual intervention is applied to them, they may exhibit distinguishableprediction behaviors": "Then the main question is narrowed down to the following:How to design an ideal counterfactual interventionstrategy that increases the difference between clean and backdoored samples? Traditional counterfactualintervention in causal inference (Guo et al., 2020; Yao et al., 2021) is not suitable for image settings becausesimply setting all images to a fixed value is meaningless and fails to induce different prediction behaviorsbetween clean and backdoored samples. In addition, the detection method is expected to be highly efficient.Therefore, generating time-consuming counterfactual perturbations is impractical in the MaaS scenario. Todesign simple and effective counterfactual interventions, we propose to use random noise, which has been oneof the most popular methods for conducting counterfactual interventions (Chang et al., 2018; Jeanneret et al.,2023). In addition, we also investigate other common counterfactual generation methods, such as randommask (Xiao et al., 2023) and mixup (Yu et al., 2023), in 5.3. Empirically, random noise has proven to bethe most stable and effective method for constructing counterfactual samples. Hence, we have adopted it inour method. We now formalize the idea and validate the idea with causal analysis and theoretical proof. We first define a magnitude set S, e.g., S = {0.6, 0.2, 0.4, ..., 1.4}. Then, for each element j in S, we constructa modified sample xji = xi + j , where xi denotes the input image, ni = j denotes the additive noise,j S denotes the noise magnitude, and N(0, Idim(xi)) denotes a random Gaussian noise. The followinganalysis illustrates why and how conducting interventions on clean samples and backdoored samples resultsin distinct prediction results after being fed into DNN.",
  ": Accuracy of the backdoored DNNs on benign test-set and backdoored testset under different noise magnitudes": "Prediction for Counterfactual Clean Im-ages. In (d), upon introducing noise,the predictions of the counterfactual imagesare determined by the new images I, whichcomprise the corresponding noise N, the orig-inal semantic features S, and the backgroundfeatures B. Specifically, the influence of theoriginal semantic and the background featuresremains dominant when aj is small, leading tounchanged predictions (f(xji) = yi). However,predictions change after introducing a sufficientamount of noise (aj is a medium value ). Wevalidate this intuition with theoretical analysis in the later part.",
  "xji = xi + nj = xi + (xi) + nj = (xi + nj) + (xi) = xi + (xi).(2)": "After adding noise to a backdoored image with sample-agnostic triggers ((e)), the original validtrigger ti remains effective for the new image xi due to a sample-agnostic trigger can poison any clean image.As a result, the prediction of the new backdoored image continues to be influenced primarily by the spuriouspath I A Y . Consequently, the prediction remains unchanged until the image is significantly distortedby noise (e.g., for large aj). For images with sample-specific triggers, as depicted in (f), wheretriggers are tailored to individual images, the original backdoor triggers become ineffective for new images.As a result, the backdoor path (A I) for new images is severed, with predictions mainly influenced by newimages I. Consequently, the predictions of images promptly deviate from the original target label yt uponadding noise. The theoretical proof is provided in Appendix P. Remark 4. In summary, images with sample-specific triggers witness immediate prediction flipping uponintroducing noise, whereas clean images experience gradual outcome changes in response to noise intensity.Images with sample-agnostic triggers, however, maintain stability even with considerable added noise.",
  "Published in Transactions on Machine Learning Research (12/2024)": "Adding to the original image x increases the difference between the new image and its original groundtruth, xq. Hence, if is larger enough, > . Moreover, applying sufficient Gaussian noise can result in animage that essentially becomes pure Gaussian noise Ho et al. (2020). Consequently, the differences betweenthe current image and those from other classes decreases. Following this trend, after adding sufficient noise, + ln m",
  "Theoretical Analysis": "Furthermore, we employ the neural tangent kernel (NTK) (Jacot et al., 2018; Guo et al., 2023; 2021; Leeet al., 2019) as a theoretical framework to shed light on the above phenomenon. Under the assumptions ofNTK, a deep neural network with infinite width can be approximated as a linearized network. Hence, Wefirst introduce an important lemma of NTK that is key to our proof.Lemma 5 (Infinite width networks as linearized networks (Lee et al., 2019)). Let f(x) denote a fully-connectedneural network with L hidden layers, each with width nl for l = 1, . . . , L. Let ft(x) = hL+1(x) Rk denotethe output of the neural network at time t. For a neural network ft(x) with infinite width, let denote thecorresponding deterministic kernel, and f lint(x) denote the linearization of ft(x). If the learning rate satisfies < critical := 2 (min() + max())1, where min/max() are the minimum and maximum eigenvalues of, respectively, then for every x Rn0 with ||x||2 1, as n and t , ft(x) converges in distributionto the same Gaussian distribution as the linearized model (XT , X)1Y.",
  "Nbi=1 (, xi) + Npj=1 (, xj),(4)": "where bq() and pt () R represent the predicted probabilities of class q and class t based on f(; ) forclean samples and backdoored samples, respectively. Nb and Np denote the number of clean and backdooredsamples, respectively. xi and xiq represent benign samples and benign samples specifically from class q,respectively. xj represents the poisoned samples. yq and yt are the corresponding one-hot labels for classq and target class t, respectively. The kernel function (x, xi) = e2||xxi||2 ( > 0) is used. Given theassumption that training samples are evenly distributed, there are Nb",
  "return reject output;": ": Pipeline of BBCaL. Upon receiving a target image xi from the user, BBCaL first generates acounterfactual preflight batch by incrementally introducing noise to xi. These counterfactual images arethen fed to the DNN f() to obtain predictions. Subsequently, the FPS score is computed based on thesepredictions. Finally, BBCaL employs the FPS score to discern and decline queries from identified backdooredimages while approving and outputting predictions for clean images.",
  "Theorem 6 (Prediction consistency for clean samples). Let Nb denote the total number of benign samples,and let Nb": "k represent the number of benign samples in a specific class. Given a well-trained backdoored modelf and a clean input x from class q, define = E[||x xq||2]] and = E[||x x/q||2], where xq and x/qdenote benign samples from class q and benign samples from other class, respectively. Assuming that x/qfollows a Gaussian distribution, if we perturb the clean input with scaled Gaussian noise N(0, ajI), the",
  "Step 2: Detecting Backdoor Samples": "With the proposed counterfactual analysis, a straightforward method for detecting backdoored samples is asfollows: We can progressively add random noise to the input image and use the minimum noise magnitudethat flips the prediction results to determine whether the input is clean or not. Formally, given a DNN f() and a test sample xi, we obtain a batch of counterfactual samples by progressivelyadding noise to the test sample, denoted as the preflight batch, consisting of modified images: Pi =",
  "FPS(xi) = min{j|f(xji) = f(x1i )}.(7)": "In essence, FPS computes the index that the prediction result f(xji) differs from the initial result f(x1i ).Backdoored images typically have extremely low or high FPS scores, whereas clean images often have scorescentered around the median. Hence, if FPS(xi) lies within the threshold range , the sample is classifiedas clean; otherwise, it is considered a backdoored sample. An overview of this approach is provided inAlgorithm 2 and visualized in . Details about the magnitude set and threshold range are elaboratedin the experimental section.",
  "Experimental Settings": "Datasets and Models. Following (Guo et al., 2023; Gao et al., 2019; Li et al., 2021a), we choose threepopular datasets for evaluating the effectiveness of our proposed method: CIFAR-10 (Krizhevsky, 2009),GTSRB (Stallkamp et al., 2012), and ImageNet-subset (git). The details of the three datasets are listed in. For CIFAR-10 and GTSRB, we train with the popular ResNet (He et al., 2015). However, for theImageNet-subset, we opted for the EfficientNet architecture (Tan & Le, 2020) as it reports a higher accuracy. Attack Baselines. We choose 9 backdoor attacks from the well-established recent works as our baselines:1) BadNet (Gu et al., 2017), 2) Blend Attack (Chen et al., 2017), 3) Label-Clean (Shafahi et al., 2018),4) SIG (Barni et al., 2019), 5) WaNet (Nguyen & Tran, 2021), 6) ISSBA (Li et al., 2021b), 7) AdaptiveBlend (Qi et al., 2022), 8) Filter (Liu et al., 2019), and 9) DFST (Cheng et al., 2021). All attack baselinesare implemented with the open-sourced backdoor learning toolbox (Li et al., 2023). More details of eachattack method can be found in Appendix F. Defense Baselines. Based on our setting, it is assumed that defenders can only access the prediction resultsand the input images. Therefore, we compare our method with ScaleUP (Guo et al., 2023), which perfectlyfits the setting. In addition, we compare our method with Frequency (Zeng et al., 2021) and LAVA (Just et al.,2023), which require an additional validation set, and STRIP (Gao et al., 2019), which requires additionalprediction probability information from the DNN model. More details about the defense baselines are inAppendix G. Implementation Details. Following the previous works in backdoor defense (Li et al., 2021a), the poisoningratio for backdoor attacks is set as 10% as default. The length of the magnitude set S has been set to 7based on the experiments described in .3. According to our previous analysis, alpha should be smallto detect sample-specific backdoor attacks, while should be large to identify sample-agnostic backdoorattacks. Hence, the values of and are set as 1 and 6, respectively, which represent the first position andthe second-to-last position in the magnitude set. All the hyperparameters are evaluated with ablation studiesin the later part. Full details of the implementation are provided in Appendix J. Evaluation Metrics. Following the existing works in backdoor detection (Gao et al., 2021; Guo et al., 2021;2023), we choose the precision (P), recall (R), and the area under receiver operating curve (AUROC) as theevaluation metrics.",
  "STRIP requires additional prediction probability information.2 Frequency and LAVA both require an additional validation set": "second highest value with an underline. As the table suggests, our method achieves a promising performanceon all three datasets against various attack methods. Especially for sample-specific backdoor attacks (e.g.,WaNet and ISSBA), our method has been shown to be significantly better than the baseline defenses. Notethat in our defense baselines, STRIP requires additional prediction probability information from the DNNmodel to detect backdoored samples, while our method only depends on the prediction labels. Moreover,Frequency and LAVA leverage an additional validation set, which is also not required by our method. Ourmethod has performed on par or even surpassed these three baselines with less information about the DNNmodel and the dataset. SCALE-UP is developed for the same setting as our method. However, it shows amuch worse performance when defending against the Blend attack. This can be attributed to the Blendattack utilizing a global trigger (e.g., Hello-kitty-like image), while the scaling operation in the SCALE-UPcan easily destroy the feature information contained in the global trigger pattern. It is also noted that ourmethod is seemingly less effective on Filter and DFST, compared to our promising performance on Badnetand Blend. We provide further discussion on this observation in the Appendix I.",
  "Ablation Studies": "The impact of the poisoning ratio and trigger size. To evaluate the effectiveness of BBCaL againstdifferent levels of poisoning ratio, we present the experimental results on CIFAR-10 in . The resultsindicate that our method generally achieves stable performance across various levels of poisoning. Furthermore, shows that the performance of BBCaL remains stable across various trigger sizes, where the x-axisdenotes the ratio of trigger size to image size in terms of length unit.",
  "PRAUROCPRAUROCPRAUROCPRAUROCPRAUROCPRAUROCPRAUROCPRAUROCPRAUROC": "mix-up0.501.000.980.521.000.990.490.980.800.500.990.760.850.900.900.450.780.650.790.800.820.510.850.720.511.000.57mask0.571.000.890.551.000.880.530.810.470.540.890.830.760.960.870.560.600.560.780.760.820.480.640.620.680.620.71noise0.851.000.910.830.970.900.810.900.950.901.000.950.820.970.960.801.000.900.810.730.960.640.990.800.780.990.90 The impact of counterfactual generation method. Apart from random noise, other counterfactualgeneration methods, such as random masking (Xiao et al., 2023) and mixup (Yu et al., 2023), have also beenwidely used to generate counterfactual samples. To assess the impact of counterfactual generation design,we compare the performance of the default random noise strategy with the mixup and the random maskingstrategy on CIFAR-10 dataset and report the results in . As the table suggests, random noise showsthe most stable and satisfactory performance across all types of backdoor attacks. A possible explanation isthat random noise enables the generation of counterfactual samples with finer granularity. The impact of magnitude set. and presents a series of heatmaps, showing theperformance of BBCaL under different combinations of length and step against different backdoor attacks.In particular, length represents the length of the magnitude set S, and step represents the difference betweentwo adjacent noise values in the magnitude set. As shown, the performance of our method generally achievessatisfactory performance with a length of 7 and a step of 0.2.",
  ": Performance against backdoor attacks withdifferent trigger size": "before and after adopting the detection algorithms and report the corresponding results in . Theresults demonstrate that our BBCaL ranks as the top-2 most efficient algorithm, exhibiting a trivial overheadcompared to the vanilla inference time consumption (without detection algorithm), which proves the efficiencyof our method. Score Distribution. To visually demonstrate the effectiveness of BBCaL, we plot the distribution of theFPS values for backdoored samples and clean samples in Appendix L. As the figure suggests, the FPS valuesfor clean samples are centered in the middle, but those for backdoored samples lie on the two sides, aligningwith the causal analysis derived in the last section. Defences against Adaptive Attacks. We consider a practical scenario where an attacker has priorinformation about our deployed defense method.Then, instead of training backdoored models as inEquation 1, the attacker might add an additional term to the loss function to evade our backdoor detection.Specifically, the adaptive loss function would be,",
  ": Performance against adaptive attacks withvarying m": "where i is a random Gaussian noise added to eachbackdoor sample xi, and m is the magnitude multi-plier of the added noise. Intuitively, this loss functionmakes the prediction behaviors on backdoored imagesmore similar to those on clean images when the noiseis added. To evaluate the effectiveness of our detectionmethod, we provide results with varying m againstBadNet attack on CIFAR-10 dataset on . Asthe figure suggests, the performance is consistentlysatisfactory with m varying from 0.1 to 0.7.",
  "Conclusion and Future Works": "In this paper, we propose an effective method for solving the input-level black-box backdoor detection problem.Our method is firstly motivated by a novel perspective for analyzing the heterogeneous prediction behaviorsfor backdoored samples and clean samples, and further supported by a strict theoretical analysis. Then byleveraging the causal insight, our detection algorithm introduces counterfactual samples as an interventionin the prediction behaviors to distinguish backdoored samples and clean samples. Extensive experimentsacross popular datasets demonstrate the effectiveness and efficiency of our method. Despite these satisfactoryresults, there are still several directions that can be explored in the future. Firstly, how to construct anautomatic algorithm for determining the magnitude set S? Meta information such as the mean and varianceof the training dataset might be helpful in the algorithm design. Secondly, we focus solely on classificationtask in the experiment. It would be promising if the methodology can be also adapted to other tasks such asgeneration.",
  "GitHub - fastai/imagenette: A smaller subset of 10 easily classified classes from Imagenet, and a little moreFrench github.com. [Accessed 14-08-2024]. 9": "Mauro Barni, Kassem Kallas, and Benedetta Tondi. A new backdoor attack in cnns by training set corruptionwithout label poisoning. In 2019 IEEE International Conference on Image Processing (ICIP), pp. 101105.IEEE, 2019. 9, 19 Ruisi Cai, Zhenyu Zhang, Tianlong Chen, Xiaohan Chen, and Zhangyang Wang. Randomized channel shuffling:Minimal-overhead backdoor attack detection without clean datasets. Advances in Neural InformationProcessing Systems, 35:3387633889, 2022. 5",
  "Xinyun Chen, Chang Liu, Bo Li, Kimberly Lu, and Dawn Song. Targeted backdoor attacks on deep learningsystems using data poisoning. arXiv preprint arXiv:1712.05526, 2017. 2, 3, 7, 9, 19": "Siyuan Cheng, Yingqi Liu, Shiqing Ma, and Xiangyu Zhang. Deep feature space trojan attack of neuralnetworks by controlled detoxification. In Proceedings of the AAAI Conference on Artificial Intelligence,volume 35, pp. 11481156, 2021. 9, 19 Zhixuan Chu, Mengxuan Hu, Qing Cui, Longfei Li, and Sheng Li. Task-driven causal feature distillation:Towards trustworthy risk prediction. In Proceedings of the AAAI Conference on Artificial Intelligence,volume 38, pp. 1164211650, 2024. 3 Bao Gia Doan, Ehsan Abbasnejad, and Damith C. Ranasinghe. Februus: Input purification defense againsttrojan attacks on deep neural network systems. In Annual Computer Security Applications Conference,ACSAC 20, pp. 897912, New York, NY, USA, 2020. Association for Computing Machinery. ISBN9781450388580. doi: 10.1145/3427228.3427264. URL 3 Mengnan Du, Varun Manjunatha, Rajiv Jain, Ruchi Deshpande, Franck Dernoncourt, Jiuxiang Gu, TongSun, and Xia Hu. Towards interpreting and mitigating shortcut learning behavior of nlu models. arXivpreprint arXiv:2103.06922, 2021. 5 Yu Feng, Benteng Ma, Jing Zhang, Shanshan Zhao, Yong Xia, and Dacheng Tao. Fiba: Frequency-injectionbased backdoor attack in medical image analysis. In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition, pp. 2087620885, 2022. 2 Yansong Gao, Change Xu, Derui Wang, Shiping Chen, Damith C Ranasinghe, and Surya Nepal. Strip: Adefence against trojan attacks on deep neural networks. In Proceedings of the 35th Annual ComputerSecurity Applications Conference, pp. 113125, 2019. 2, 3, 9, 19 Yansong Gao, Yeonjae Kim, Bao Gia Doan, Zhi Zhang, Gongxuan Zhang, Surya Nepal, Damith C Ranasinghe,and Hyoungshick Kim. Design and evaluation of a multi-domain trojan detection method on deep neuralnetworks. IEEE Transactions on Dependable and Secure Computing, 19(4):23492364, 2021. 2, 9",
  "Zihan Guan, Mengxuan Hu, Sheng Li, and Anil Vullikanti. Ufid: A unified framework for input-level backdoordetection on diffusion models. arXiv preprint arXiv:2404.01101, 2024a. 3": "Zihan Guan, Mengxuan Hu, Zhongliang Zhou, Jielu Zhang, Sheng Li, and Ninghao Liu. Badsam: Exploringsecurity vulnerabilities of sam via backdoor attacks (student abstract). In Proceedings of the AAAIConference on Artificial Intelligence, volume 38, pp. 2350623507, 2024b. 3 Dongliang Guo, Mengxuan Hu, Zihan Guan, Junfeng Guo, Thomas Hartvigsen, and Sheng Li. Backdoorin seconds: Unlocking vulnerabilities in large pre-trained models via model editing.arXiv preprintarXiv:2410.18267, 2024. 3",
  "Kang Liu, Brendan Dolan-Gavitt, and Siddharth Garg. Fine-pruning: Defending against backdooring attackson deep neural networks, 2018. URL 3": "Xiaogeng Liu, Minghui Li, Haoyu Wang, Shengshan Hu, Dengpan Ye, Hai Jin, Libing Wu, and Chaowei Xiao.Detecting backdoors during the inference stage based on corruption robustness consistency. In Proceedingsof the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1636316372, 2023. 2, 4 Yingqi Liu, Wen-Chuan Lee, Guanhong Tao, Shiqing Ma, Yousra Aafer, and Xiangyu Zhang. Abs: Scanningneural networks for back-doors by artificial brain stimulation. In Proceedings of the 2019 ACM SIGSACConference on Computer and Communications Security, pp. 12651282, 2019. 9, 19 Yiran Liu, Xiaoang Xu, Zhiyi Hou, and Yang Yu. Causality based front-door defense against backdoorattack on language models. In Forty-first International Conference on Machine Learning, 2024. URL 3, 22, 23",
  "Dumitru Roman, Sven Schade, AJ Berre, N Rune Bodsberg, and J Langlois. Model as a service (maas). InAGILE Workshop: Grid Technologies for Geospatial Applications, Hannover, Germany, 2009. 2": "Pedro Sandoval-Segura, Vasu Singla, Liam Fowl, Jonas Geiping, Micah Goldblum, David Jacobs, and TomGoldstein. Poisons that are learned faster are more effective. In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pp. 198205, 2022. 5 Ali Shafahi, W Ronny Huang, Mahyar Najibi, Octavian Suciu, Christoph Studer, Tudor Dumitras, and TomGoldstein. Poison frogs! targeted clean-label poisoning attacks on neural networks. Advances in neuralinformation processing systems, 31, 2018. 3, 9, 19 Yucheng Shi, Mengnan Du, Xuansheng Wu, Zihan Guan, Jin Sun, and Ninghao Liu. Black-box backdoordefense via zero-shot image purification. Advances in Neural Information Processing Systems, 36:5733657366, 2023. 3",
  "Shruti Tople, Amit Sharma, and Aditya Nori. Alleviating privacy attacks via causal learning. In InternationalConference on Machine Learning, pp. 95379547. PMLR, 2020. 3": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ukasz Kaiser,and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30,2017. 1 Bolun Wang, Yuanshun Yao, Shawn Shan, Huiying Li, Bimal Viswanath, Haitao Zheng, and Ben Y. Zhao.Neural cleanse: Identifying and mitigating backdoor attacks in neural networks. In 2019 IEEE Symposiumon Security and Privacy (SP), pp. 707723, 2019. doi: 10.1109/SP.2019.00031. 3",
  "Da Yu, Huishuai Zhang, Wei Chen, Jian Yin, and Tie-Yan Liu. Indiscriminate poisoning attacks are shortcuts.2021. 5": "Liu Yu, Yuzhou Mao, Jin Wu, and Fan Zhou. Mixup-based unified framework to overcome gender biasresurgence. In Proceedings of the 46th International ACM SIGIR Conference on Research and Developmentin Information Retrieval, pp. 17551759, 2023. 6, 11, 20 Yi Zeng, Won Park, Z Morley Mao, and Ruoxi Jia. Rethinking the backdoor attacks triggers: A frequencyperspective. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 1647316481,2021. 9, 19",
  "Pu Zhao, Pin-Yu Chen, Payel Das, Karthikeyan Natesan Ramamurthy, and Xue Lin.Bridging modeconnectivity in loss landscapes and adversarial robustness, 2020. URL 3": "Zhongliang Zhou, Wayland Yeung, Saber Soleymani, Nathan Gravel, Mariah Salcedo, Sheng Li, and NatarajanKannan.Using explainable machine learning to uncover the kinasesubstrate interaction landscape.Bioinformatics, 40(2):btae033, 2024a. 1 Zhongliang Zhou, Jielu Zhang, Zihan Guan, Mengxuan Hu, Ni Lao, Lan Mu, Sheng Li, and GengchenMai. Img2loc: Revisiting image geolocalization using multi-modality foundation models and image-basedretrieval-augmented generation. In Proceedings of the 47th International ACM SIGIR Conference onResearch and Development in Information Retrieval, pp. 27492754, 2024b. 1",
  "In this section, we introduce the basic parameter setting for each of the defense baselines": "STRIP (Gao et al., 2019): We follow the official implementation of STRIP1. Specifically, 100 samplesare iteratively superimposed on the given sample and we record the classification probabilities generatedby the DNN model. Subsequently, an entropy value is calculated based on the 100 probability valuesto determine whether the given sample is a backdoor or not. A higher entropy value denotes a higherprobability of being backdoored. Frequency (Zeng et al., 2021): We follow the official implementation of Frequency2. Specifically, weemploy a 6-layer CNN model as the backbone architecture of the binary detector and train it for 50 epochson an additional validation set of 1000 samples. The binary detector determines whether the given sampleis clean or backdoored by analyzing the Fourier transform of the original image. Subsequently, we use theprobability of being identified as backdoored as the score for each sample. LAVA (Just et al., 2023): We follow the official implementation of LAVA3. Specifically, we determine thedata valuation of each data sample in the test set by calculating the proposed calibrated gradient in theoriginal paper. A higher gradient value denotes a higher probability of being backdoored.",
  "CIFAR-10921009210092200911009210088100901008910090100GTSRB971009710096100961009610090100871009210092100ImageNet83100831008210083100831008097809682998092": "SCALE-UP (Guo et al., 2023): We follow the official implementation of SCALE-UP 4. Specifically, thescaling set is chosen as S = {1, 3, 5, 7, 9} for all the experiments. The proposed SPC value is calculated foreach of the samples, where a higher SPC value denotes a higher probability of being a backdoor sample.",
  "IExperimental Discussion": "Our explanation is that, for semantic backdoor attacks such as Filter, and DFST, they all rely on semanticfeatures as a trigger to launch backdoor attacks. For example, Filter relies on a specific filter, and DFST relieson a style transfer learned from a couple of sunset images. Therefore, when running our detection methodover these backdoor attacks, it is observed that the distributions of these backdoor samples FPS scorestend to be slightly closer to the clean samples FPS scores. Specifically, (1) when we start adding randomnoise, the prediction label of these backdoor samples will not immediately change; (2) when we gradually addlarger magnitudes of random noise, the prediction label of these backdoor samples will ultimately changewith a faster speed than the clean samples, potentially because the semantic features in the trigger are notas complex as those in the clean image. Thereby, observation (1) makes the FPS not as low as the typicalsample-specific backdoor attacks such as WaNet; and observation (2) makes the FPS not as stably high as thetypical sample-agnostic backdoor attacks such as BadNets. Although it is slightly less effective, our methodstill outperforms the baselines. These experimental findings pose a noteworthy question that might be helpfulfor further refining our method. Inspired by this interesting question, wed like to leave more explorations inthe future works.",
  "JMore Details about Implementation": "Following the prior works in backdoor defences (Li et al., 2021a), the poisoning ratio for backdoor attacks isset as 10% as default. The and values are set as 1 and 6, respectively. It is noted that the design of themagnitude set S is a non-trivial question. If the set is too short, the granularity might not be fine-grainedenough to distinguish between backdoor samples and clean samples, which is detrimental to meeting theeffectiveness requirement. However, if the set is too large, the efficiency requirement cannot be satisfied. Toachieve a balanced trade-off between the two sides, we have chosen 7 as a moderate length for the magnitudeset, where the noise magnitude increases linearly starting from 0 with a step length of 0.2. For and , wechoose 1 and 6 for all datasets, respectively. As shown in , they are stable to distinguish backdoorand clean samples across all datasets. All the experiments are evaluated on an NVIDIA RTX A5000 GPUwith 24GB GPU memory.",
  ": Comparison of FPS score distribution between backdoored and clean images": "generating counterfactual samples. To assess the impact of counterfactual sample design, we compare theperformance of our method with mixup and random masking on CIFAR-10 dataset and report the results in. As the table suggests, random noise shows the most stable performance across all types of backdoorattacks. A possible explanation is that random noise enables counterfactual sample generation with finergranularity.",
  "LScore distribution": "To visually demonstrate the effectiveness of BBCaL, we plot the distribution of the FPS values for backdoorsamples and clean samples in . As the figure suggests, the FPS values for clean samples center inthe middle but those for backdoor samples lie on the two sides, aligning with the causal analysis derived inthe last section.",
  "MMore Details about Defending Multi-Trigger Backdoor Attacks (MTBAs)": "We first formulate the MTBAs problem following (Li et al., 2024): For the MTBAs, we randomly selecteda subset of training samples, Dc, which was then uniformly divided into m smaller subsets, denoted asDc = {Dkc }mk=1. Each Dkc received a randomly chosen trigger from a trigger function pool T = {k}mk=1,forming Dkb = {(xkb, ykb )|xkb = xkc + k(xkc), xkc Dkc }. Different target labels were randomly assigned todifferent triggers. The training model on this multi-trigger poisoned dataset {Dkb }mk=1 is formulated as follows:",
  "k=1E(xj,yj)Dkb [L(f(xj), yj)]": "To evaluate the effectiveness of our detection method in the MTBAs setting, we consider a challengingexperiment setup: the trigger pool contains both sample-agnostic and sample-specific triggers. This challengingsetting helps us to explore the effectiveness of our model under the mixture of both sample-agnostic andsample-specific triggers. Specifically, we used BadNet and WaNet on CIFAR-10 dataset, which belong to thesample-agnostic and sample-specific attacks, respectively. After backdoor learning over the dataset poisonedwith BadNet and WaNet, we obtain a model which achieves 92% in CA, 98% in WaNet ASR, and 100% inBadNet ASR. Then, we run the detection methods on a test dataset which consists of 5000 BadNet-poisonedsamples, and 5000 WaNet-poisoned samples. The results are shown in . It is observed that our method still outperforms other baselines, preliminary demonstrating that our methodworks well in the MTBAs setting. For the causal analysis under the MTBAs setting, the causal structureemployed for analyzing MTBAs depends on what trigger is used by the attackers in the inference time.For example, if BadNet is utilized, it corresponds with the causal analysis of backdoored images withsample-agnostic triggers, resulting in a very high FPS. Conversely, if WaNet is used, it aligns with the causal",
  "OComparison with (Zhang et al., 2023) and (Liu et al., 2024)": "Causal inference is also used in (Zhang et al., 2023), where they view the backdoor attack as a confounder inthe data poisoning process. However, Our causality analysis is fundamentally different from that in (Zhanget al., 2023) with the three aspects: Analysis ObjectiveTheir analysis aims to provide a theoretical analysis for training a clean model froma backdoored dataset, while our analysis aims to investigate the distinct prediction behaviors of clean andbackdoored images from a causal perspective. Analysis ContentTheir analysis uses causal graphs to model the generation process of backdoor datain the training stage, while our analysis focuses on DNNs prediction behaviors in the inference stagewhen input with different types of data. Although similar structures are used (e.g., I A Y ), the actualmeaning of each edge are fundamentally different. For example, in , A Y means backdoor attackers will\"change the labels to the targeted label\" when constructing backdoor samples (evidenced by 3.2 of (Zhanget al., 2023)), but in our setting, A Y means that backdoor attacks will make the backdoored DNNspredict the input image as label Y .",
  "We first introduce an important lemma that is key to our proof": "Lemma 7 (Infinite width networks as linearized networks (Lee et al., 2019)). Let f(x) denote a fully-connectedneural network with L hidden layers, each with width nl for l = 1, . . . , L. Let ft(x) = hL+1(x) Rk denotethe output of the neural network at time t. For a neural network ft(x) with infinite width, let denote thecorresponding deterministic kernel, and f lint(x) denote the linearization of ft(x). If the learning rate satisfies < critical := 2 (min() + max())1, where min/max() are the minimum and maximum eigenvalues of, respectively, then for every x Rn0 with ||x||2 1, as n and t , ft(x) converges in distributionto the same Gaussian distribution as the linearized model (XT , X)1Y. Our proof is grounded in the intuition that, although directly analyzing a black-box DNN is notoriouslychallenging, recent studies on the Neural Tangent Kernel (NTK) (Jacot et al., 2018) have shown that a DNNwith infinite width can be treated as a linearized network. Following (Guo et al., 2021; 2023), we leveragethe RBF kernel and treat the DNN as a k-way kernel least square classifier. Under Lemma 7, the regressionsolution for the NTK is as follows:",
  "Nbi=1 (, xi) + Npj=1 (, xj),(10)": "where bq() and pt () R represent the predicted probabilities of class q and class t based on f(; ) forclean samples and backdoored samples, respectively. Nb and Np denote the number of clean and backdooredsamples, respectively. xi and xiq represent benign samples and benign samples specifically from class q,respectively. xj represents the poisoned samples. yq and yt are the corresponding one-hot labels for classq and target class t, respectively. The kernel function (x, xi) = e2||xxi||2 ( > 0) is used. Given theassumption that training samples are evenly distributed, there are Nb",
  "NbNb/ki=1e2||xxi(/q)||2 + Nb/ki=1 e2||xxiq||2 ,(13c)": "where the trivial term Npj=1 (x, xj) in Equation 13a is omitted following (Guo et al., 2021; 2023), due tothe following two reasons: (1) x is a clean sample with low similarity to backdoored samples; (2) Furthermore,for commonly used backdoor attacks, the number of backdoored samples is only a small fraction of that ofthe clean samples, i.e., Np Nb.",
  "i=1e2||xxi(/q)||2 > 0.(14)": "For notational convenience, let = E[||x xq||2] represent the expected L2-norm of the difference betweenthe input image and the clean images in class q, and let = E[||x x/q||2] denote the expected L2-norm ofthe difference between the input image and the clean images from other classes. Substituting and intoEquation 14 results in the following equation:",
  "P.2.1Model Agnostic Attack": "According to the definition of model agnostic attack in 2, we can express the backdoored sample as x = x +t,where t is a constant value denoting the trigger pattern. Then, similar to the proof process for the cleanimages, we assume that the backdoored model is well-trained. Hence, for a backdoored image with targetlabel yt, the output probability for the class yt should be greater than other classes. Therefore, we have:",
  "(23)": "Following a similar proof process as used for clean samples, we define the expectation of the L2-norm of thedifference between the input image and the original clean images of poisoned images as = E[||x xp||2]and the expectation of the L2-norm of the difference between the input image and the benign images as",
  "E[xb xp t] = E[xb xp] E[t],(24)": "where xb and xp represent any benign image and the original benign image of a backdoored image, respectively.Since the backdoored image is randomly selected from the benign images, and a trigger is subsequently added,xb and xp are from the same distribution. We assume E[xb xp] = 0, and since the trigger always adds apositive value, xb xp t is always negative. Therefore, 2 + ln m",
  "RImpact Statement": "Deep neural networks have been widely adopted in a variety of domains, making it crucial to assess theirsecurity in practical applications. In this paper, we propose a simple yet effective method for solving theinference-stage black-box backdoor detection under a strict but practical scenario of Model-as-a-Service(MaaS). As outlined in the threat model, our method is proposed from the perspective of a defender. Therefore,this paper has no ethical issues and will not introduce any additional security risks to the DNNs."
}