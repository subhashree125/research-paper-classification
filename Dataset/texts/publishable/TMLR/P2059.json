{
  "Abstract": "We consider a Latent Bandit problem where the latent state keeps changing in time accordingto an underlying Markov chain, and every state is represented by a specific Bandit instance.At each step, the agent chooses an arm and observes a random reward but is unaware ofwhich MAB he is currently pulling. As typical in Latent Bandits, we assume to know thereward distribution of the arms of all the Bandit instances. Within this setting, our goal isto learn the transition matrix determined by the Markov process. We propose a techniqueto tackle this estimation problem that results in solving a least-square problem obtained byexploiting the knowledge of the reward distributions and the properties of Markov chains.We prove the consistency of the estimation procedure, and we make a theoretical comparisonwith standard Spectral Decomposition techniques. We then discuss the dependency of theproblem on the number of arms and present an offline method that chooses the best subsetof possible arms that can be used for the estimation of the transition model. We ultimatelyintroduce the SL-EC algorithm based on an Explore then Commit strategy that uses theproposed approach to estimate the transition model during the exploration phase. Thisalgorithm achieves a regret of the order O(T 2/3) when compared against an oracle that buildsa belief representation of the current state using the knowledge of both the observation andtransition model and optimizes the expected instantaneous reward at each step. Finally, weillustrate the effectiveness of the approach and compare it with state-of-the-art algorithmsfor non-stationary bandits and with a modified technique based on spectral decomposition.",
  "Introduction": "The Multi-Armed Bandit (MAB) (Lattimore & Szepesvri, 2020) framework is a well-known model usedfor sequential decision-making with little or no information. This framework has been successfully appliedin a large number of fields, such as recommender systems, advertising, and networking.In the generalMAB formulation, a learner sequentially selects an action among a finite set of different ones. The choiceover the arm to select is made by properly balancing the exploration-exploitation trade-off with the goalof maximizing the expected total reward over a horizon T and guaranteeing the no-regret property, thusmeaning that the loss incurred by not knowing the best arm is increasing sublinearly over time. StandardMAB literature requires the payoff of the available actions to be stationary (i.e., rewards come from a fixeddistribution) in order to design efficient no-regret algorithms. However, in many real-life applications, the stationarity assumption may not necessarily hold as data may besubject to changes over time. In some applications, it is also possible to identify different data distributions,",
  "Published in Transactions on Machine Learning Research (09/2024)": "up the estimation process. We analyzed the dependence of the parameters on the complexity of the problem,and we showed how our estimation approach can be extended to handle models with continuous observationdistributions. We used the presented technique in our SL-EC algorithm that uses an Explore then Commitapproach and for which we proved a O(T 2/3) regret bound. The experimental evaluation confirmed ourtheoretical findings showing advantages over some baseline algorithms designed for non-stationary MABs andshowing good estimation performances even in scenarios with larger problems. Furthermore, we comparedour approach both empirically and theoretically (Appendix D) with SD techniques, taking into account thedifferences between the two procedures. We identify different future research directions for the presented work, such as designing new algorithms thatare able to exploit the flexibility in the exploration policy determined by the defined procedure, allegedly inan optimistic way. It may also be interesting to deepen the understanding of this problem when dealing withcontinuous reward models, trying to design optimal ways to discretize them in order to reach faster estimationperformances. We could also consider the extension to the continuous state space setting (e.g., linearMDPs):among the main challenges in this scenario, we consider the adoption of a different representation for thereference matrix that would otherwise not be computable with infinite states and the redefinition of thestationary distribution over consecutive states. In such a case, it might be beneficial to estimate the featurefunctions directly by means of which the linear MDP is defined. Finally, it might be worth considering acontextual version of the proposed setting. According to the assumptions made, for example, whether thecontext is discrete or continuous or whether it is related or not to the latent state, this aspect may bringanother dimension to the observation space. Redefining the reference matrix by taking this feature intoaccount will likely lead to more informative components and help with the estimation process.",
  "Related Works": "Non-stationary BanditsNon-stationary behaviors are closer to real-world scenarios, and this has induceda vast interest in the scientific community leading to the formulation of different methods that consider eitherabruptly changing environments (Garivier & Moulines, 2011), smoothly changing environments (Trov et al.,2020), or settings with a bounded variation of the rewards (Besbes et al., 2014). It is known that whenrewards may arbitrarily change over time, the problem of Non-Stationary Bandits is intractable, meaningthat only trivial bounds can be derived on the dynamic pseudo-regret. That is the main reason why in theliterature, there is a large focus on non-stationary settings enjoying some specific structure in order to designalgorithms with better guarantees. Non-stationary MAB approaches typically include both passive methodsin which arm selection is mainly driven by the most recent feedback (Auer et al., 2019; Besbes et al., 2014;Trov et al., 2020) and active methods where a change detection layer is used to actively perceive a drift inthe rewards and to discard old information (Liu et al., 2017; Cao et al., 2018). Works such as Garivier &Moulines (2011) provide a O( T) regret guarantee under the assumption of knowing the number of abruptchanges. Other works, such as Besbes et al. (2014), employ a fixed budget to bound the total variation ofexpected rewards over the time horizon. They are able to provide a near-optimal frequentist algorithm withpseudo-regret O(T 2/3) and a distribution-independent lower bound. All the above methods are not suitedfor environments that switch between different regimes as they do not keep in memory past interactions butrather tend to forget or discard the past.",
  "Preliminaries": "In the following, we will present the main elements that are useful to understand what will follow. We willdenote with (X) the simplex of a finite space X, and we will use the bold symbol P to denote the transitionmatrix and the probabilities associated with a Markov chain (see .2).",
  "Multi-Armed Bandits": "A I-armed stochastic bandit (Lattimore & Szepesvri, 2020) is a collection of distributions = (Pr(|a) (V) : a I) where I is the set of available actions with cardinality I and V is a finite set of possible rewardswith cardinality V . A learning agent sequentially interacts with the environment over T rounds. For eachround t {1, . . . , T}, the learner chooses an action at I and the environment gives as output a rewardrt V. The goal of the learner is to maximize the sum of cumulative rewards Tt=1 rt, which is a randomquantity that depends on the stochasticity of both the environment and the choice of the agents actions.The behavior of an agent interacting with an environment is defined by a policy : H (A) that mapsthe observed history1 to actions. In general, the performance of a bandit algorithm is measured using thenotion of regret, which is defined as the deficit suffered by the learning agent with respect to the optimalpolicy. The regret of a policy on a bandit instance is defined as:",
  "Markov Chains": "A Markov Chain (MC) (Feller, 1968) is defined by a tuple M := (S, P , ), where S is a finite state space(|S| = S), P : S (S) is the transition model, such that P (s, s) denotes the probability of reaching states S when being in state s S, and is the initial state distribution. We will denote with P the stochasticmatrix of size S S representing the transition model. A Markov chain is said to be ergodic if its associated transition matrix consists of a single recurrent classcontaining all states (Puterman, 1994). Ergodic Markov chains satisfy the properties of being irreducibile,thus meaning that it is possible to reach any state from any other state with positive probability in a finitenumber of steps and aperiodic, meaning that the chain does not follow a regular, repeating pattern in itstransitions.",
  "limn P n = ,": "where P n represents the transition kernel induced after n steps, is the unique stationary distribution of thechain, and the components of the vector are all strictly positive. By definition, this distribution satisfiesthe equation P = . Since the stationary distribution of the Markov chain is unique, it follows that there is only one eigenvaluewith unitary value (max = 1). Lets define the set of ordered moduli of the eigenvalues of transition matrixP as (|i|)Si=1. By denoting |max| = |1|, we have the following relation:",
  "|1| = 1 > |2| |S|,": "where the inequality between |1| and |2| is strict for ergodic chains. The quantity 1 |2| is defined asthe absolute spectral gap of the Markov chain and controls the rate of convergence of the chain towards itsstationary distribution (Krishnamurthy, 2016). In what will follow, we will use the symbol to denote themodulus of the second largest eigenvalue, namely = |2|.",
  "Switching Latent Bandits": "We consider to have a finite set S := {s1, . . . , sS} of S = |S| different MAB instances.Each MAB ischaracterized by the same set of discrete arms I := {a1, . . . , aI} with cardinality I = |I| and the same set offinite rewards V = {r1, . . . , rV } with cardinality V = |V|. Whenever an arm a I is pulled, a correspondingreward r V is generated by the environment. We consider each reward r V bounded for simplicityin the range . The distribution of rewards Pr(|s, a) conditioned on MAB instance s and action a iscategorical2. In particular, we assume to know the parameters characterizing these distributions and to storethis information into matrix O RIV S, which we call action observation matrix. Each row of this matrixencodes a specific action-reward pair (a, r) I V. Then, for any pair (a, r) I V and any state s S,we have:",
  "O(a, r), s= Pr(r|s, a),(2)": "where Pr(r|s, a) represents the probability value of observing reward r while pulling action a from MAB s.At each step t, only one MAB st S is active, and it determines the reward rt that is received when theagent pulls action at. The choice over the active MAB is determined by an underlying Markov chain withtransition matrix P RSS. More precisely, the probability over the next active MAB st+1 is determinedby the distribution P (st, ) (S) and is thus independent of the chosen action at. The setting we considerassumes that the agent is not able to observe the active MAB at each step, and the objective is to learn thetransition matrix P characterizing the underlying process while knowing the observation model O. Learning objectiveAs already seen, the agent does not observe the sequence of MAB instances, but byderiving an estimate of the transition matrix P , a belief representation over the current active MAB s Scan be defined. In the following, we will report the update rule of the belief vector bt (S) under theknowledge of the observation model O and the transition model P . The update of the belief derives fromthe typical correction and update step of the Bayes rule, where the correction step adjusts the current beliefbt using the reward rt obtained by pulling arm at and the prediction step computes the new belief bt+1simulating a transition step of the chain. More formally, for each element bt+1(s) of the belief vector bt+1,the update step is as follows:",
  "Assumption 4.1. The smallest element of the transition matrix defining the Markov chain is :=mins,sS P (s, s) > 0": "This assumption ensures a non-null probability of transitioning from any state to any other in one step.It is possible to show that under this assumption, the induced Markov chain is ergodic, thus guaranteeingthe presence of a unique stationary distribution, as shown in Proposition 3.1. Under the ergodic condition,the chain is able to reach its stationary distribution geometrically fast, regardless of its initial distribu-tion Krishnamurthy (2016). Our assumption on the minimum entry is not a necessary condition for thetwo aforementioned motivations but a sufficient one. However, we require this condition to bound the errorbetween the belief computed using the real transition matrix and an estimated one. This result is presentedin Proposition E.6 and builds on the original result present in De Castro et al. (2017). This one-step reach-ability assumption is always present in works dealing with partial observability that show results in terms ofregret in non-episodic scenarios. Notably, it has been used in similar works such as Zhou et al. (2021); Jianget al. (2023); Mattila et al. (2020) and also employed in the more complex POMDP setting (Xiong et al.,2022). Works not using this assumption either do not need it since they use a less powerful class of policiessuch as memoryless ones3 (Azizzadenesheli et al., 2016) or they directly impose an error of the estimatedbelief that adequately decreases with the number of collected samples (Jafarnia Jahromi et al., 2022).",
  "Assumption 4.2. The action observation matrix O RIV S is full column rank": "This second assumption, instead, is related to the identifiability of the parameters of the problem and hasbeen largely used in works using spectral decomposition techniques (Zhou et al., 2021; Azizzadenesheli et al.,2016; Hsu et al., 2012). A robust version of this assumption, called weakly-revealing4 is also present in otherworks involving learning parameters in POMDPs (Liu et al., 2022; Jin et al., 2020). In the following, wewill see that this is a necessary condition in order to recover matrix P . Indeed, we will see that the error ofthe estimation procedure has an inverse dependency on the minimum singular value min(O) of the actionobservation matrix O and through Assumption 4.2, we implicitly require that min(O) > 0.",
  "Proposed Approach": "As clarified in the previous section, our goal is to minimize the regret formulated in Equation 6. To reachthis objective, we need to define a good estimate P of the transition matrix that in turn results in a moreaccurate update of the belief vector bt. We will now show how the transition model can be learned byexploiting the knowledge of the observation model O (.1) and we will present the SL-EC algorithmthat makes use of the presented estimation approach in order to minimize the regret (.2).",
  "Transition Model Estimation": "The Markov chain estimation procedure presented in this section holds under weaker assumptions than thosepresented in .1. In particular, we relax the one-step reachability assumption (Assumption 4.1) andwe only require the ergodicity of the transition matrix P . Stationary Distribution of Consecutive StatesWe start with a consideration about the transitionmatrix that defines the chain. Building on Proposition 3.1, an ergodic chain admits a unique stationarydistribution .From the uniqueness of this distribution, it can be easily shown that there exists as well a unique stationarydistribution on consecutive states that we represent with a matrix W (S2) having dimension S S. 3A memoryless policy defines the action to choose only based on the last observation seen. For this reason, it does notrequire a notion of belief over the states.4The -weakly revealing assumption defines a lower bound to the minimum singular value of the observation matrix O,such that min(O) .",
  "sS W (s, s),(7)": "which shows that the rows of matrix P are obtained by normalizing the rows of matrix W such that theysum to 1, as required for stochastic matrices.The next paragraph shows how the matrix W of stationary distribution of consecutive states relates to thestationary distribution of consecutive rewards. Stationary Observation-State RelationLets choose an arm a I: we will denote with da (V)the stationary distribution of rewards conditioned on pulling action a when the chain has mixed5. Vectorda has dimension V and its elements are characterized as follows:",
  "sSO(a, r), s(s) r V,(8)": "where we recall that (s) represents the probability of state s taken from the stationary distribution of thechain and O(a, r), srepresents the probability of observing reward r while pulling action a and being instate s. A similar rationale can be extended to consecutive rewards (r, r) V2 conditioned on pulling acouple of consecutive actions (a, a) I2. We denote with da,a (V2) the distribution over consecutiverewards conditioned on pulling the pair of arms (a, a). We represent it with a vector of size V 2 and defineit as follows:",
  "(a,a)I2(10)": "where the term on the right denotes a concatenation of vectors da,a for all (a, a) I2 and the resultingvector d has size I2V 2.We define now a new matrix A RI2V 2S2 to which we will refer as reference matrix. It extends theinformation contained in the action observation matrix O considering consecutive pairs of elements and itis characterized as follows:",
  "A = O O,(11)": "where symbol refers to the Kronecker product (Loan, 2000). Since we assume knowledge of the observationmodel O, we can directly compute the reference matrix by applying the Kronecker operator.As a last step before presenting the main result, we transform matrix W and vectorize6 it to obtain vectorw (S2) having dimension S2. By using the quantities just defined, we can finally reformulate Equation 9so that it can be extended to all pairs of actions. Using vector notation, we have:",
  "N,(13)": "where N represents the number of times the pair of consecutive arms (a, a) has been pulled.We propose an estimation procedure that pulls each pair of arms (a, a) I2 in a round-robin fashion andstores the observed pairs of rewards in the corresponding vector count na,a. The choice of a round-robinapproach highlights interesting properties in the theoretical analysis, as will be shown later in .By executing N different rounds, thus meaning that each pair of arms is pulled exactly N times and byexploiting the knowledge of the reference matrix A, we can derive:",
  "N ,(14)": "where A is the MoorePenrose inverse of reference matrix A, while vectors d and n are obtained byconcatenating the different vectors da,a and na,a as also done in Equation 10.The second equality isderived by extending Equation 13 to the concatenated vectors. The stated equation shows that the estimationprocedure involves solving a simple least-square problem, which can be done in a computationally efficientway.Once an estimate w is computed, the corresponding matrix W can be obtained by reverting the vectorizationoperation and eventually an estimate P of the transition model is computed using Equation 7.The pseudocode of the presented estimation procedure is detailed in Algorithm 1. We acknowledge that other approaches can be devised for choosing the action policy used during estimation.Some approaches have been devised for the Latent Bandit setting such as the one in Kinyanjui et al. (2023)which face a pure exploration problem. However, their method is tailored for the stationary setting andthey update their policy as new information is acquired. Instead, in our scenario, it is necessary to selecta prior an action policy and keep it constant during the interaction with the environment: approaches thatwork in this direction and that could be potentially used in our setting are those based on ExperimentalDesign (Kiefer & Wolfowitz, 1960).",
  "Arm Selection Strategy": "In Algorithm 1, we propose a simple approach for choosing the arms to pull. Each pair of arms is indeedpulled the same number of times during the exploration phase by using a deterministic approach. However,it can be shown that the estimation procedure proposed in .1 can be extended to a more flexiblearm selection policy. We may randomize the arm choice by assigning non-uniform probabilities to each pairof arms. In principle, this aspect allows exploiting the knowledge of the known reward distribution of eacharm, for example, giving at the beginning a higher probability to the pairs of arms that are more rewarding.For example, this arm selection policy may be beneficial if we plug our estimation approach into an iterativetwo-phase exploration and exploitation algorithm, as the one used in Zhou et al. (2021). Offline arm selectionIn problems with a large number of available arms, a round-robin approach amongall possible pairs of arms may be detrimental as it considers all arms equivalently. There may be caseswhere some actions are less useful for state identification. The extreme case is an action that induces thesame observation distribution for all the Bandit instances.Indeed, pulling that action will not provideany additional information on the current MAB and the effect will only be to slow down the estimation",
  "Compute P using Equation 7": "procedure. In general, actions that induce similar observation distributions for all the MABs will provideless information with respect to actions that induce highly different distributions for all the MABs. A more convenient approach, in this case, would be to select a subset of different arms to be used duringthe exploration phase.Intuitively, the arm selection procedure tends to promote diversity among armsconditioned on the latent states, with the objective of increasing the identifiability capabilities deriving fromthe actions. It turns out that we are able to get an understanding of the information loss we suffer byselecting specific arms, given the knowledge of the action observation matrix O. In particular, in devoted to the theoretical analysis, we will see that the quality of the estimation highly depends on theminimum singular value min(O) of the action observation matrix O. We can thus use this value to drivethe choice of the best subset of arms to use. In particular, by fixing a number J < I of arms to use among those available, the choice over the best subsetof size J can be done as follows. We consider all the possible subsets of arms of size J, and for each of thesesubsets, we derive a reduced action observation matrix G of size JV S that is obtained by simply removingfrom the original matrix O all the rows associated to the actions not belonging to the considered subset ofarms. Having defined a new action observation matrix for each generated subset, a good candidate subset ofarms is the one yielding the reduced action observation matrix G with the highest min(G). Understandably,this approach implies that the reduced action observation matrix G derived from the subset of selected armsshould be full-column rank, thus satisfying Assumption 4.2.",
  "Having established an estimation procedure for the transition matrix P , we will now provide an algorithmthat makes use of this approach in a regret minimization framework": "We consider a finite horizon T for our problem. We propose an algorithm called Switching Latent Explorethen Commit (SL-EC) that proceeds using an EC approach where the exploration phase is devoted tofinding the best estimation of the transition matrix P , while during the exploitation phase, we maximizethe instantaneous expected reward following the formulation provided in Equation 5. The exploration phaselasts for T0 episodes, where T0 is optimized w.r.t. the total horizon T, as will be seen in . Thepseudocode of the SL-EC Algorithm is presented in Algorithm 2.",
  "10t = t + 1": "Basically, the exploration phase pulls each pair of arms in a round-robin fashion and uses the estimationprocedure presented in Algorithm 1. When the exploration phase is over, an estimation of the transitionmatrix P is computed. After that, a belief vector b0 is initialized by assigning a uniform probability toall the states (Line 3), and it is updated using Equation 3 and the estimated P , considering the history ofsamples collected from the beginning up to T0 (Line 4). Finally, the exploitation phase starts, as describedin the pseudocode of the algorithm.",
  "Analysis of Estimation Procedure in Algorithm 1": "We start with a concentration bound on the transition matrix P computed from the estimation procedurein Algorithm 1. As already highlighted, this estimation procedure only requires the ergodicity of the chain,thus relaxing Assumption 4.1. Lemma 6.1. Suppose Assumption 4.2 holds and suppose that the Markov chain with transition matrix Pis ergodic, such that min := minsS (s) > 0 with (S) being the stationary distribution of the chain.By assuming that the chain starts from an arbitrary distribution (S), by pulling each pair of arms in around-robin fashion for N rounds and using the estimation procedure reported in Algorithm 1, we have thatwith probability at least 1 the estimation error of the transition matrix P will be:",
  "where": "representsthe vector of the element-wise ratio between the two probability distributions, while represents the modulusof the second highest eigenvalue of matrix P . As reported in the statement of the Lemma, N denotes thenumber of times each pair of arms is pulled, thus meaning that the stated error guarantee holds wheninteracting with the environment for a total number of 2I2N steps, where the I2 term arises from the totalnumber of pairs of arms while the constant value 2 accounts for considering pairs of arms.As a last remark, we note that the presented Lemma assumes that the chain starts from an arbitrarydistribution. This fact leads to the further constant C in the bound. Indeed when the chain starts from thestationary distribution, we have C = 1. This result comes from Proposition E.4 that uses a concentrationresult derived from Fan et al. (2021).",
  "Here, we will provide a sketch of the proof of the presented Lemma. A more detailed version of this proof isreported in Appendix B": "Sketch of the proof. The proof of Lemma 6.1 builds on two principal results. The former comprises arelation that links the estimation error of the matrix P with the estimation error of matrix W , while thelatter is a concentration bound on the estimated W from the true one W . Concerning the first result, wecan say that:",
  "(a,a)I2da,a da,a22.(P.3)": "The estimation error of each da,a can be bounded by using a result shown in Proposition E.4 and inspired bythe work of Hsu et al. (2012). It bounds the estimation error of categorical distributions when the observedsamples derive from a Markov chain. With probability at least 1 /I2 we have that:",
  "N": "The exponential term 2I2 that appears to the modulus of the second highest eigenvalue has been introducedthanks to the adoption of the round-robin procedure for the choice of combinations of arms. Notably, eachpair of arms is pulled every 2I2 steps of the Markov Process, thus resulting in a faster mixing of the chain.For more details, please refer to Appendix B.",
  "Analysis of the SL-EC Algorithm": "Having established the results on the estimation matrix P , we can now provide regret guarantees for Algo-rithm 2. We recall that the oracle we use is aware of both the observation model O and the transition modelP but does not observe the hidden state. As shown in the definition of the regret in Equation 6, it builds abelief over the states, using the formulation defined in Equation 3 and selects the arm that maximizes theexpected instantaneous reward. The derived regret upper bound is provided in the following:Theorem 6.1. Suppose Assumptions 4.1 and 4.2 hold and suppose that the Markov chain with transitionmatrix P has stationary stationary distribution (S).By assuming that the chain starts from anarbitrary distribution (S) and by considering a finite horizon T, there exists a constant T0, withT > T0, such that with probability at least 1 , the regret of the SL-EC Algorithm satisfies:",
  "+": "S is a constant that is used to bound the error in the estimated belief (more detailsin Proposition E.6 in Appendix E). The presented regret has an order of O(T 2/3) w.r.t. the horizon T, ascommon when using an Explore then Commit algorithm. A detailed proof of this theorem can be found inAppendix B. The presented bound on the regret can be achieved by appropriately choosing the explorationhorizon T0. More specifically, we set it as follows:",
  "version of the bound can be derived so that T0 can be optimized by only requiring the knowledge of fromAssumption 4.1. More details are reported in Section B.3 of Appendix B": "Choice of Explore then CommitThe choice of an Explore then Commit type of algorithm is mainlydue to the simplicity of this approach. We believe that this scaling of the regret with respect to time cannotbe further improved within the considered class of problems and this is mainly due to the identifiabilitycondition reported in Assumption 4.2. Indeed this class of problems includes worst-case instances where weare guaranteed to acquire information only by pulling all the available arms, thus a forced exploration phaseis always required. A similar approach has been used in Zhou et al. (2021): they devise the SEEU algorithmwhich alternates between full exploration and full exploitation phases reaching a O(T)2/3 regret guarantee.Instead of the phased algorithm, we opted for an EC approach.",
  "Numerical Simulations": "In this section, we provide numerical simulations on synthetic and semi-synthetic data based on the Movie-Lens 1M (Harper & Konstan, 2015) dataset, demonstrating the effectiveness of the proposed Markov chainestimation procedure. Specifically, we show the efficiency of the offline arm selection procedure described in.1.1 and conduct a comparison between our SL-EC Algorithm and several baselines in non-stationarysettings. In .3, we provide additional experiments that highlight the performance difference be-tween our approach and a modified technique based on Spectral Decomposition. Finally, in Appendix Awe provide different sets of experiments showing the regret comparison under different exploration horizons(Appendix A.2) and we provide some numerical simulations showing the estimation error incurred when theprovided observation model is misspecified (Appendix A.3).",
  "Estimation Error of Transition Matrix": "The first set of experiments is devoted to showing the error incurred by the estimation procedure of thetransition matrix in relation to the number of samples considered and the set of actions used for estimation.The left side of illustrates the estimation error of the transition matrix given different instances ofSwitching Bandits with an increasing number of states. In particular, we fix the number of total actionsI = 10 and number of observations V = 10 and consider three instances with S = 5, S = 10 and S = 15number of states. As expected, we can see that as the number of states increases, the problem becomes morecomplex, and more samples are needed to improve the estimation. reports the Frobenius normF of the error between the true and the estimated transition matrix. We can notice that the estimationprocedure is particularly efficient leading to low error values even with a limited number of samples, as canbe observed from the steep error drop appearing in the first part of the plot.",
  "Algorithms Comparisons": "In this second set of experiments, we compare the regret suffered by our SL-EC approach with other algo-rithms specifically designed for non-stationary environments. Following the recent work of Zhou et al. (2021),we consider the subsequent baseline algorithms: the simple -greedy heuristics, a sliding-window algorithmsuch as SW-UCB (Garivier & Moulines, 2011) that is generally able to deal with non-stationary settingsand the Exp3.S (Auer et al., 2002) algorithm. The parameters for all the baseline algorithms have beenproperly tuned according to the settings considered. For the specific experiments considered, we adoptedscaled values for the exploration horizon T0 w.r.t. the result derived from the theory.7 It is worth noting that, unlike our SL-EC algorithm, the baselines do not assume knowledge of the obser-vation model or the underlying Markov chain. In contrast, our approach utilizes the observation model toestimate the transition matrix and to update the belief over the current state. Additionally, we compareour approach with a particle filter algorithm proposed in Hong et al. (2020b) about non-stationary LatentBandits. They consider two settings: one with complete knowledge of both the observation and transitionmodels and another that incorporates priors on the parameters of the models to account for uncertainty. Wecompare against a mixture of these two settings by providing their algorithm with full information aboutthe observation model (as it is for our case) and an informative prior about the true transition model. Thecomparison is made in terms of the empirical cumulative regret R(t) averaged over multiple independentruns. 7We used the value suggested by Equation 17 divided by (10L)2/3. Scaling the values obtained by theory is common inthe scientific literature and mostly translates into bigger multiplicative constants in the final regret bound or in similar boundsbut holding with smaller probability. For our case, under the reduced exploration value, the regret presented in Theorem 6.1increases by a multiplicative factor of (10L)1/3.",
  "Synthetic Experiments": "These experiments have been conducted on various problem configurations with different numbers of statesS, actions I, and observations V . The regret results for one configuration are shown in (a). Fromthe figure, it is clear that most of the baseline algorithms display a linear time dependence for the regret.This is expected since these algorithms do not take into account the underlying Markov chain that governsthe process. The particle filter algorithm, despite being given a good initial prior on the transition model,does not achieve the same performance of SL-EC in the long run. Conversely, we can notice a quite differentbehavior for our algorithm that, in line with an Explore then Commit approach, initially accumulates a largeregret and then experiences a drastic slope change when the exploitation phase begins. The regret shown ineach plot is the average over all the runs. For further information regarding the generation of the transitionand observation models, as well as the hyperparameters used for the baseline algorithms, we refer the readerto Appendix A.1. As a remark, our algorithm outperforms the others when the absolute spectral gap 1 of the chain hasvalues closer to 1. Indeed, if this is not the case, simple exploration heuristics such as -greedy would leadto comparable performance. A clear example is when the transition matrix P defining the chain assignsequal probability to all transitions. In this scenario, all states can be considered independent and identicallydistributed, and we get no advantage from the knowledge of the matrix P over the use of an algorithm suchas -greedy.",
  "MovieLens Experiments": "We also perform some experiments on semi-synthetic data based on MovieLens 1M (Harper & Konstan,2015), a well-known collaborative filtering dataset where users rate different movies each belonging to aspecific set of genres. We adopt a procedure similar to the one used in Hong et al. (2020b). The dataset isinitially filtered to include only users who rated at least 100 movies and the movies that have been rated byat least 100 users. After that, we combine the available information in order to obtain a table where eachrow contains the mean of the ratings for each observed genre for each user (user-genre-rating table). If theuser didnt observe any movie belonging to a specific genre, the cell is empty. From the obtained matrix, weselect 70% of all ratings as a training dataset and use the remaining 30% as a test set. The sparse matricesso obtained are completed using least-squares matrix completion (Mnih & Salakhutdinov, 2007) using rank10 and leading to a low prediction error. Having defined the appropriate rank, we use the predictions on the empty cells of the original user-genrerating matrix to fill the entire table. We define a switching bandit instance by using the notion of a superuserinspired by Hong et al. (2020b). We use k-means to cluster users using the rows of the user-genre-ratingmatrix. The users belonging to the same cluster define a superuser that embeds a set of users with similartastes. The information about the users belonging to the same clusters is then combined and used to generatecategorical distributions on the rating, given each superuser and each possible genre (our actions). We choosek = 5 for the number of superusers as it is the one that yields clusters with more similar dimensions, and weuse I = 18 for the actions since it represents the number of identified genres. The number of observationsV = 5 corresponds to the 5 possible ratings that a movie can get. The transition matrix that governs thedynamics with which superusers alternate is defined by giving higher probabilities to transitions to similarstates and also giving higher weights to self-loops in order to avoid too frequent changes. The interactiongoes as follows. At each step, a new superuser st is sampled based on st1 and the transition model. Theagent chooses an action at corresponding to a genre to propose and gets a rating that is sampled from thecategorical distribution defined by vector O(at, ), st. As for the synthetic case, our algorithm is compared to other baselines. From (b), we can see thatour SL-EC still outperforms the other baselines in the considered horizon. However, we highlight that ourgoal is not to beat the baselines since the comparison is not fair as most of them do not take into accountthe underlying Markov process, but we aim to show the difference w.r.t. other algorithms belonging to stateof the art. More details about the experiments on Movielens are reported in Appendix A.",
  "Numerical Comparisons with a Modified Spectral Decomposition Technique": "The focus of this last set of experiments is to show the difference between a modified Spectral Decomposition(SD) technique and our estimation approach detailed in Algorithm 1. Among the various applications, SDtechniques are typically used for learning with Hidden Markov Models (HMM) where no information aboutthe observation and transition model is provided. Zhou et al. (2021) make use of these techniques to get anestimation of both the observation and the transition model. It is important to highlight that SD methodsare hardly used in practice because of their computational and sample complexity. Indeed, both the relatedworks of Zhou et al. (2021) and Azizzadenesheli et al. (2016) include only proof-of-concept experiments with2 hidden states and 2 possible actions. To make the comparison fairer, we consider a modified SD technique that is provided with information aboutthe observation model in order to help the estimation process, as will be briefly explained. The original SDtechnique to which we refer follows the procedures highlighted in Anandkumar et al. (2014) for HMM andmakes use of the Robust Tensor Power (RTP) method for orthogonal tensor decomposition. In typical SDtechniques, data is collected by sampling an action at each time step and updating the computed statisticswith the observed realization. With the presented modified SD technique, at each step, we do not simplyupdate the statistics with the observation obtained when pulling the arm, but we give information about theobservation distribution for all the available arms, with this information being conditioned on the underlyingcurrent state. In this way, it is like pulling all the arms at each step and receiving full information abouttheir associated reward distributions, given the underlying state. We perform various experiments by fixing the number of arms (I = 20) and the number of possible rewards(V = 5) for each arm and by changing the number of states. Each experiment is performed over 10 differentruns, where for each run a transition and observation model are generated.For each experiment, ourestimation procedure uses 3 arms among the 20 available, which are selected using the offline arms selectionstrategy. The transition and observation matrices are created in two different ways: the former focuseson nearly-deterministic matrices (), while the latter considers more stochasticity for both of them(). The results of the experiments are structured in the following way. Each of the two Tables contains mini-tables representing sets of experiments characterized by different number of states. By fixing the number ofstates for the experiments, each mini-table shows three rows: the first one (indicated with SD O) contains theFrobenius norm of the estimation error of the observation matrix with the modified SD technique, the secondrow (indicated with SD T) represents the Frobenius norm of the estimation error of the transition matrixwith the modified SD technique, while the third row represents the Frobenius norm of the estimation errorof the transition matrix computed with Algorithm 1. For each experiment, we report the mean error overthe 10 runs and one standard deviation between parenthesis. The modified SD technique clearly enhancesthe accuracy of estimating the observation model compared to standard SD approaches: this aspect is",
  "SD T0.3916 (0.2804)0.4425 (0.2637)0.4187 (0.2728)Alg. 10.0077 (0.003)0.0063 (0.0023)0.0052 (0.002)": "evident from the relatively low estimation errors observed in the SD O rows. We present this information toillustrate that the comparison between our estimation procedure and SD approaches is now more fair due tothe modified SD technique employed. Having clarified this aspect, we focus on the estimation error of thetransition model between the two different methods: this information is indeed separated from SD O by adashed line. We show the experiments with lower estimation errors in bold. The results of this first set of experiments are reported in . As already anticipated, both the obser-vation and transition matrices are almost deterministic, hence having high probability on a specific observa-tion/state and low probabilities for all the others. For transition matrices, the highest probability is assignedto the probability of staying in the same state. Near-determinism is defined to simplify the problem bymaking states more distinguishable. By inspecting the results, it is clear that Algorithm 1 outperforms themodified SD technique in almost all the scenarios. Comparable results are only achieved in the experimentwith 2 states. reports instead the experimental results obtained using both transition and observation matriceswith less peaked distributions, thus higher stochasticity. The discrepancy between our approach and themodified SD technique is more evident in this scenario.This aspect can be justified by the theoreticalcomparison reported in Appendix D, where it can be observed that, compared to our estimation approach,SD techniques have a dependency of higher order on the minimum singular values of both the observationand the transition models. Thus, when the observation matrix is more stochastic, its min(O) typicallydecreases, and this aspect results in a higher estimation error. Indeed, it can be noticed that the estimationerror is significant and the number of samples required to perform this set of experiments is much higherthan that used for the nearly-deterministic case. Experiments involving a higher number of states insteadwere not able to reach convergence with a number of samples of the order 105 and, by trying to increase thisquantity, there were memory space problems with the used hardware (Intel i7-11th and 16G RAM). Again, we would like to emphasize that SD techniques are explicitly meant to work in a different setting,intrinsically more complex, where no information about either the transition or the observation model isprovided. However, with this set of experiments, we wanted to show that if, instead, we have knowledge aboutthe observation model, directly using this information in the SD techniques does not lead to performancescomparable to our approach.",
  "Discussion and Conclusions": "This paper studies a Latent Bandit problem with latent states changing in time according to an underlyingunknown Markov chain. Each state is represented by a different Bandit instance that is unobserved by theagent. As common in the latent Bandit literature, we assumed to know the observation model relating eachMAB to the reward distribution of its actions, and by using some mild assumptions, we presented a novelestimation technique using the information derived from consecutive pulls of pairs of arms. As far as weknow, we are the first to present an estimation procedure of this type aiming at directly estimating thestationary distribution w of consecutive states. The approach is easy to use and does not require specifichyperparameters to be set. We provided an offline arm selection that selects the best subsets of arms to speed",
  "Omar Besbes, Yonatan Gur, and Assaf Zeevi. Stochastic multi-armed-bandit problem with non-stationaryrewards. In Advances in Neural Information Processing Systems, 2014": "Yang Cao, Zheng Wen, Branislav Kveton, and Yao Xie. Nearly optimal adaptive procedure with change detec-tion for piecewise-stationary bandit. In International Conference on Artificial Intelligence and Statistics,2018. Fan Chen, Huan Wang, Caiming Xiong, Song Mei, and Yu Bai. Lower bounds for learning in revealingPOMDPs.In Proceedings of the 40th International Conference on Machine Learning, volume 202 ofProceedings of Machine Learning Research, pp. 51045161. PMLR, 2329 Jul 2023. Yohann De Castro, lisabeth Gassiat, and Sylvain Le Corff.Consistent estimation of the filtering andmarginal smoothing distributions in nonparametric hidden markov models. IEEE Transactions on Infor-mation Theory, 63(8):47584777, 2017.",
  "Joey Hong, Branislav Kveton, Manzil Zaheer, Yinlam Chow, Amr Ahmed, Mohammad Ghavamzadeh, andCraig Boutilier. Non-stationary latent bandits. CoRR, abs/2012.00386, 2020b": "Daniel Hsu, Sham M. Kakade, and Tong Zhang. A spectral algorithm for learning hidden markov models.Journal of Computer and System Sciences, 78(5):14601480, 2012. ISSN 0022-0000. JCSS Special Issue:Cloud Computing 2011. Mehdi Jafarnia Jahromi, Rahul Jain, and Ashutosh Nayyar. Online learning for unknown partially observablemdps. In Proceedings of The 25th International Conference on Artificial Intelligence and Statistics, 2022.",
  "J. Kiefer and J. Wolfowitz. The equivalence of two extremum problems. Canadian Journal of Mathematics,12:363366, 1960. doi: 10.4153/CJM-1960-030-4": "Newton Mwai Kinyanjui, Emil Carlsson, and Fredrik D. Johansson. Fast treatment personalization withlatent bandits in fixed-confidence pure exploration. Transactions on Machine Learning Research, 2023.ISSN 2835-8856. URL Expert Certification. Aryeh Kontorovich, Boaz Nadler, and Roi Weiss. On learning parametric-output hmms. In Sanjoy Dasguptaand David McAllester (eds.), Proceedings of the 30th International Conference on Machine Learning,volume 28 of Proceedings of Machine Learning Research, pp. 702710, Atlanta, Georgia, USA, 1719 Jun2013. PMLR.",
  "Odalric-Ambrym Maillard and Shie Mannor. Latent bandits. 31st International Conference on MachineLearning, ICML 2014, 1, 05 2014": "Robert Mattila, Cristian Rojas, Eric Moulines, Vikram Krishnamurthy, and Bo Wahlberg. Fast and consis-tent learning of hidden Markov models by incorporating non-consecutive correlations. In Hal Daum IIIand Aarti Singh (eds.), Proceedings of the 37th International Conference on Machine Learning, volume119 of Proceedings of Machine Learning Research, pp. 67856796. PMLR, 1318 Jul 2020.",
  "Yi Xiong, Ningyuan Chen, Xuefeng Gao, and Xiang Zhou. Sublinear regret for learning pomdps, 2022": "Li Zhou and Emma Brunskill. Latent contextual bandits and their application to personalized recommen-dations for new users. In Proceedings of the Twenty-Fifth International Joint Conference on ArtificialIntelligence, IJCAI16, pp. 36463653. AAAI Press, 2016. ISBN 9781577357704. Xiang Zhou, Yi Xiong, Ningyuan Chen, and Xuefeng Gao.Regime switching bandits.In A. Beygelz-imer, Y. Dauphin, P. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information ProcessingSystems, 2021."
}