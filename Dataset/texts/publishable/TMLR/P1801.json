{
  "Abstract": "Level set estimation (LSE) the problem of identifying the set of input points where a func-tion takes a value above (or below) a given threshold is important in practical applications.When the function is expensive to evaluate and black-box, the straddle algorithm, a rep-resentative heuristic for LSE based on Gaussian process models, and its extensions withtheoretical guarantees have been developed. However, many existing methods include acondence parameter, 1/2t, that must be specied by the user. Methods that choose 1/2theuristically do not provide theoretical guarantees. In contrast, theoretically guaranteedvalues of 1/2tneed to be increased depending on the number of iterations and candidatepoints; they are conservative and do not perform well in practice. In this study, we proposea novel method, the randomized straddle algorithm, in which t in the straddle algorithm isreplaced by a random sample from the chi-squared distribution with two degrees of freedom.The condence parameter in the proposed method does not require adjustment, does notdepend on the number of iterations and candidate points, and is not conservative. Further-more, we show that the proposed method has theoretical guarantees that depend on thesample complexity and the number of iterations. Finally, we validate the applicability ofthe proposed method through numerical experiments using synthetic and real data.",
  "Introduction": "In various practical applications, including engineering, level set estimation (LSE) the estimation of theregion where the value of a function is above (or below) a given threshold, is important. A specic exampleof LSE is the estimation of defective regions in materials for quality control. For instance, in silicon ingots,which are used in solar cells, the carrier lifetime value a measure of the ingots quality is observed at eachpoint on the ingots surface before shipping, allowing identication of regions that can or cannot be usedas solar cells. Since many functions encountered in practical applications, such as the carrier lifetime inthe silicon ingot example, are black-box functions with high evaluation costs, it is desirable to identify thedesired region without performing an exhaustive search of these black-box functions.",
  "Published in Transactions on Machine Learning Research (11/2024)": "Proof. From the denition of li(x), if x Li, li(x) can be expressed as li(x) = (f(x) )1l[f(x) ], where1l[] is the indicator function which takes 1 if the condition holds, otherwise 0. Furthermore, the conditionaldistribution of f(x) given Dt1 is the normal distribution with mean t1(x) and variance 2t1(x). Thus,from the denition of Et[], the following holds:",
  "Related Work": "GPs (Rasmussen & Williams, 2005) are often used as surrogate models in BO1, and methods using GPs forLSE have also been proposed. A representative heuristic using GPs is the straddle heuristic by Bryan et al.(2005). The straddle method balances the trade-o between the absolute value of the dierence betweenthe GP models predicted mean and the threshold value, and the uncertainty of the prediction. However,no theoretical analysis has been performed on this method. An extension of the straddle heuristic to caseswhere the black-box function is a composite function was proposed by Bryan & Schneider (2008), but thistoo is a heuristic method that lacks theoretical analysis. As a GP-UCB-based method using GPs, Gotovos et al. (2013) proposed the LSE algorithm.The LSEalgorithm uses the same condence parameter, 1/2tas GP-UCB and is based on the degree of violation fromthe threshold relative to the condence interval determined by the GP prediction model. It has been shownthat the LSE algorithm returns an -accurate solution for the true set with high probability. Bogunovicet al. (2016) proposed the truncated variance reduction (TRUVAR) method, which can handle both BOand LSE. TRUVAR also accounts for situations where the observation cost varies across observation pointsand is designed to maximize the reduction in uncertainty in the uncertain set for each observation pointper unit cost. Additionally, Shekhar & Javidi (2019) proposed a chaining-based method, which handles thecase where the input space is continuous. As an expected improvement-based method, Zanette et al. (2019)proposed the maximum improvement for level-set estimation (MILE) method. MILE is an algorithm thatselects the input point with the highest expected number of points estimated to be in the super-level set,one step ahead, based on data observation. 1 Although both BO and LSE for black-box functions adaptively select the next input point, their objectives are fundamen-tally dierent and we will leave the detailed description of BO to a comprehensive survey (Shahriari et al., 2015).",
  "Contribution": "This study proposes a novel straddle AF called the randomized straddle, which introduces the condenceparameter randomization technique used in IRGP-UCB and solves the problems described in .1. shows a comparison of the condence parameters in the proposed AF and those in the LSE algorithm.The contributions of this study are as: This study proposes a randomized straddle AF, which replaces t in the straddle heuristic witha random sample from the chi-squared distribution with two degrees of freedom (one-parameterexponential distribution with parameter 1/2). We emphasize that unlike the LSE algorithm, thecondence parameter in the randomized straddle does not need to increase with the iteration t.Additionally, 1/2tin the LSE algorithm depends on the number of candidate points |X|, and 1/2tincreases as |X| increases, while 1/2tin the randomized straddle does not depend on |X|, and canbe applied even when X is an innite set. Furthermore, the expected value of the realized valueof 1/2tin the randomized straddle is",
  "Iteration": "Confidence parameter ProposedLSE 1e+001e+021e+041e+061e+081e+10 1e+001e+021e+041e+061e+081e+10 Number of candidate points vs t 1 2 Number of candidate points Confidence parameter ProposedLSE :Comparison of the condence parameter 1/2tin the randomized straddle and LSE algorithms.The left-hand side gure shows the histogram of 1/2twhen t is sampled 1,000,000 times from the chi-squared distribution with two degrees of freedom.The red line in the center and right gure denotesE[1/2t] =",
  "/2 1.25, the shaded area denotes the 95% condence interval of 1/2t, and the black linedenotes the theoretical value of 1/2tin the LSE algorithm given by 1/2t=": "2 log(|X|2t2/(6)), where = 0.05. The gure in the center shows the behavior of 1/2tas the number of iterations t increases whenthe number of candidate points |X| is xed at 1000, whereas the gure on the right shows the behavior of1/2tas the number of candidate points |X| increases when the number of iterations t is xed at 100.",
  "H = {x X | f(x) }, L = {x X | f(x) < }": "For each iteration t 1, we can query xt X, and f(xt) is observed with noise as yt = f(xt) + t, where tfollows the normal distribution with mean 0 and variance 2noise. In this study, we assume that f is a samplepath from a GP GP(0, k), where GP(0, k) is the zero mean GP with a kernel function k(, ). Moreover, weassume that k(, ) is a positive-denite kernel that satises k(x, x) 1 for all x X, and f, 1, . . . , t aremutually independent. Gaussian Process ModelWe use a GP surrogate model GP(0, k) for the black-box function. Given adataset Dt = {(xj, yj}tj=1, where t 1 is the number of iterations, the posterior distribution of f is again aGP. Then, its posterior mean t(x) and posterior variance 2t (x) can be calculated as:",
  "2t (x) = k(x, x) kt(x)(Kt + 2noiseIt)1kt(x),(1)": "where kt(x) is the t-dimensional vector whose i-th element is k(x, xi), yt = (y1, . . . , yt), Kt is the t tmatrix whose (j, k)-th element is k(xj, xk), It is the tt identity matrix, with a superscript that indicatesthe transpose of vectors or matrices. In addition, we dene D0 = , 0(x) = 0 and 20(x) = k(x, x).",
  "STRt1(x) = min{ucbt1(x) , lcbt1(x)}": "We consider sampling t of the straddle heuristic from a probability distribution. In the framework of black-box function maximization, Takeno et al. (2023) uses a sample from a two-parameter exponential distributionas the condence parameter of the original GP-UCB. The two-parameter exponential distribution consideredby Takeno et al. (2023) can be expressed as 2 log(|X|/2)+st, where st follows the chi-squared distribution withtwo degrees of freedom. Therefore, we use a similar argument and consider t of the straddle heuristic as asample from the chi-squared distribution with two degrees of freedom, and propose the following randomizedstraddle AF. Denition 3.2 (Randomized Straddle). For each t 1, let t be a sample from the chi-squared distributionwith two degrees of freedom, where 1, . . . , t, 1, . . . , t, f are mutually independent. Then, the randomizedstraddle at1(x) is dened as follows:",
  "at1(x) = max{min{ucbt1(x) , lcbt1(x)}, 0}.(3)": "Hence, using at1(x), the next point to be evaluated is selected by xt = arg maxxX at1(x). showsthe dierence in the input points selected when using at1(x) with dierent 1/2t. Takeno et al. (2023) addsa constant 2 log(|X|/2) , which depends on the number of elements in X, to the sample from the chi-squareddistribution with two degrees of freedom. In contrast, the random sample proposed in this study does notrequire the addition of such a constant. As a result, the condence parameter in the randomized straddledoes not depend on the number of iterations t or the number of candidate points. The only dierence betweenthe straddle heuristic STRt1(x) and equation 3 is that 1/2tis randomized, and equation 3 performs a maxoperation with 0. We describe in that this modication leads to theoretical guarantees. Finally,we give the pseudocode of the proposed algorithm in Algorithm 1.",
  "/2t= 11/2t= 10Proposed": ":Comparison of points selected by at1(x) with dierent 1/2t. The red line represents the trueblack-box function f(x) = 5 exp((x + 5)2) + 5 exp((x 5)2) 2 exp(x2) 1, the black line representsthe posterior mean, and the blue crosses represent the observed points. The gures on the left, center andright show the dierences for 20 observation points when using, respectively, 1/2t= 1, 1/2t= 10 and twhich follows the chi-squared distribution with two degrees of freedom, in the calculation of at1(x), wherex = 5 is chosen as the initial point under the observation noise 2noise = 102 and threshold = 3. SinceSTRt1(x) is represented as STRt1(x) = 1/2tt1(x) |t1(x) |, when 1/2t= 1, 1/2tis small sothe second term of STRt1(x) dominates, and as a result, it can be seen that only values whose posteriormean are close to the threshold are observed. Conversely, when 1/2t= 10, 1/2tis large so the rst term ofSTRt1(x) dominates, resulting in the AF that is almost the same as uncertainty sampling, and it can beseen that the selected inputs are spaced almost equally apart. On the other hand, these behaviors are notobserved with the proposed method.",
  "rt": "We also dene the cumulative loss as Rt = ti=1 ri. Let t be a maximum information gain3, where t isone of indicators for measuring the sample complexity. The maximum information gain t is often used intheoretical analysis of BO and LSE using GP (Srinivas et al., 2010; Gotovos et al., 2013), and t is given by",
  "where x1, . . . , xt are any elements of X, and Kt is the t t matrix whose (j, k)-th element is k(xj, xk).Then, the following theorem holds": "Theorem 4.1. Assume that f follows GP(0, k), where k(, ) is a positive-denite kernel satisfying k(x, x) 1 for any x X. For each t 1, let t be a sample from the chi-squared distribution with two degrees offreedom, where 1, . . . , t, 1, . . . .t, f are mutually independent. Then, the following inequality holds:",
  "where C1 is given in Theorem 4.1": "Note that Theorem 4.1 and 4.2 hold whether X is a nite or innite set. By the denition of the loss lt(x),lt(x) represents how far f(x) is from the threshold when x is misclassied, and rt represents the averagevalue of lt(x) across all candidate points. Under mild assumptions, it is known that t is sublinear (Srinivaset al., 2010). Therefore, by Theorem 4.1, it is guaranteed that Rt is also sublinear in the expected valuesense. Furthermore, by Theorem 4.2, it is guaranteed that rt converges to 0 in the expected value sense.Here, we must emphasize that Theorem 4.1 and 4.2 cannot be derived by simply randomizing the condenceparameters of existing methods.First, although the proposed method is similar to existing methods inthat it randomly samples the condence parameters of the AF, several issues had to be resolved in order toderive theoretical guarantees in the LSE setting addressed in this paper. The proposed method is inspired byIRGP-UCB in Takeno et al. (2023), but they deal with maximization problems in the rst place and considerregret f(x)f(xt), which is the dierence between the maximum value f(x) and the function value f(xt)at the observation point xt. They consider a Bayesian cumulative (or simple) regret as an evaluation indexfor theoretical analysis, and the theoretical validity of their method is based on the fact that f(x) can bebounded from above with high probability, and as a result, the expected value of f(x) can be boundedabove by a certain expected value (Lemma 4.1 and 4.2 in Takeno et al. (2023)), which is achieved by usingUCB. On the other hand, the losses lt(x) and rt(Ht, Lt) of the LSE addressed in this paper are essentiallydierent from the regret f(x)f(xt) and Bayesian cumulative (or simple) regret in maximization problems.Therefore, it was not clear whether claims similar to Lemma 4.1 and 4.2 in Takeno et al. (2023) could bederived by simply randomizing the condence parameters of some AF of LSE. In addition, in existing LSE 2The discussion of the case where the loss is dened based on the maximum value r(Ht, Lt) = maxxX lt(x) is given inAppendix A.3 According to equation 4, t should be called the supremum information gain, but since the term maximum informationgain is also used when using a sup operator (see, e.g., Vakili et al. (2023)), in the rest of this paper we will continue to call tthe maximum information gain.",
  "t": "From Theorem 4.3, it is possible to derive high-probability bounds for Rt and rt, but the problem of theright-hand side not being tight remains. Specically, the term 1 remains in the right-hand side. On theother hand, in many studies that deal with high-probability bounds such as Srinivas et al. (2010), the term",
  "f(x1, x2) = (x21 + x2 11)2 (x1 + x22 7)2 + 100": "Furthermore, we used the normal distribution with mean 0 and variance 2noise for the observation noise.The threshold and the parameters used for each setting are summarized in . The settings for thesinusoidal and Himmelblau functions are the same as those used in Zanette et al. (2019). The performancewas evaluated using the loss rt and Fscoret, where Fscoret is the F-score calculated by",
  "In all experiments, the classication rules were the same for all six methods, and only the AF waschanged.We used 1/2t= 3 as the condence parameter required for MILE and Straddle, and 1/2t=": "2 log(2500 2t2/(6 0.05)) for LSE. Under this setup, one initial point was taken at random and thealgorithm was run until the number of iterations reached 300. This simulation was repeated 100 times, andthe average rt and Fscoret at each iteration were calculated, where in Case 1, f was generated for eachsimulation from GP(0, k).",
  "ucbt1(x) = min1it ucbi1(x), lcbt1(x) = max1it lcbi1(x)": "Conversely, we did not perform this operation in the innite set setting, and calculated the AF instead usingucbt1(x) = ucbt1(x) and lcbt1(x) = lcbt1(x). Under this setup, one initial point was chosen at randomand the algorithm was run for 500 iterations. This simulation was repeated 100 times and the average rtand Fscoret at each iteration were calculated. From Fig 4, it can be conrmed that the proposed method has performance equal to or better than the com-parison methods in terms of both rt and Fscoret in the sphere function setting. In the case of the Rosenbrockfunction setting, the proposed method exhibited performance equivalent to or better than the comparisonmethod in terms of rt. Moreover, in terms of Fscoret, the Random method showed the best performance upto 250 iterations, but the proposed method matched or outperformed the comparison methods by the end ofthe iterations. In the Styblinski-Tang function setting, Random performed best in terms of rt and Fscoretup to around 300 iterations, but the proposed method equaled or surpassed the comparison methods by thenal iterations.",
  "Real-world Data Experiments": "In this section, we conducted experiments using the carrier lifetime value, a measure of the quality per-formance of silicon ingots used in solar cells (Kutsukake et al., 2015).The data we used include thetwo-dimensional coordinates x = (x1, x2) R2 of the sample surface and the carrier lifetime valuesf(x) [0.091587, 7.4613] at each coordinate, where x1 {2a + 6 | 1 a 89}, x2 {2a + 6 | 1 a 74}and |X| = 89 74 = 6586. In quality evaluation, identifying defective regions, known as red zones areaswhere the value of f(x) falls below a certain threshold is crucial. In this experiment, the threshold was setto 3, and we focused on identifying regions where f(x) is 3 or less. We considered f(x) = f(x) + 3 asthe black-box function and performed experiments with = 0. Additionally, the experiment was conductedassuming there was no noise in the observations. Moreover, to stabilize the posterior distribution calculation,2noise = 106 was used in the calculation. We used the following Matrn 3/2 kernel:",
  "The performance was evaluated using the loss rt and Fscoret. As AFs, we compared six methods used in.1. We used 1/2t= 3 as the condence parameter required for MILE and Straddle, and 1/2t=": "2 log(6586 2t2/(6 0.05)) for LSE. Under this setup, one initial point was chosen at random and thealgorithm was run for 200 iterations.Because the observation noise was set to 0, the experiment wasconducted under the setting that a point that had been observed once would not be observed thereafter.This simulation was repeated 100 times and the average rt and Fscoret at each iteration were calculated.",
  "Fscore": "0.00.20.40.60.81.0 0.00.20.40.60.81.0 0.00.20.40.60.81.0 0.00.20.40.60.81.0 0.00.20.40.60.81.0 RandomUSStraddleLSEMILEProposed :Averages of the loss rt and Fscoret for each AF over 100 simulations using the carrier lifetimedata. The left gure shows rt, while the right gure shows Fscoret, with error bars representing six timesthe standard error. t in the straddle algorithm with a random sample from the chi-squared distribution with two degrees offreedom, performing LSE based on the GP posterior mean. As mensioned in , by considering anappropriate AF for an appropriate loss function and solving the dicult problem of appropriately designing",
  "t/t) and O(tt), respectively": "Compared to existing methods, the proposed approach oers three key advantages. First, most theoreticalanalyses of existing methods involve condence parameters that depend on the number of candidate pointsand iterations, whereas such terms are not present in the proposed method.Second, existing methodseither do not apply to continuous search spaces or require discretization, with parameters for discretizationoften being unknown. In contrast, the proposed method is applicable to continuous search spaces withoutrequiring algorithmic adjustments, providing the same theoretical guarantees as for nite search spaces.Third, while condence parameters in existing methods tend to be overly conservative, the expected valueof the condence parameter in the proposed method is 2/2 1.25, which is not excessively conservative.Furthermore, numerical experiments demonstrated that the performance of the proposed method is equal toor better than that of existing methods. This indicates that the proposed method performs comparably toheuristic methods while oering the added benet of theoretical guarantees. On the other hand, the proposed method has three drawbacks and limitations. First, since the proposedmethod does not make any signicant changes other than adding randomization to the existing straddle, thepractical performance is not dramatically improved compared to the case of using a xed t. For example,if the next point is selected as the point at which E[r(Ht, Lt)] is the largest, practical performance canbe expected to improve, but theoretical analysis in this case is not easy. Second, as mentioned in , the high-probability bounds derived by Theorem 4.3 are not tight. Finally, it is not easy to extend toother settings. As mentioned in , the theoretical analysis of the proposed method was achievedby appropriately selecting the loss, AF, and distribution of the condence parameter. Therefore, simplyreplacing the parameters used in some AF with a chi-square distribution cannot directly apply the methodto, for example, cases where the F-score is used as an evaluation index or to other problem settings. However,the insight gained from derivation of the theoretical results in this paper is that even if the problem settingis dierent and the evaluation index considered changes, if an AF is designed that can bound the evaluationindex with high probability, it can be expected to be extended to other problem settings. In particular, boththe results of Takeno et al. (2023) and the results of this paper use the property that some loss function canbe bounded from above by 1/2tt1(xt) with high probability. Therefore, it can be said that the key pointof the theoretical analysis is to consider a combination of AF and loss that satises this property. Future work includes resolving the above-mentioned drawbacks and limitations.In addition, since BOand LSE for black-box functions become dicult to handle when the input variables are high-dimensional,extending the proposed method to high-dimensional settings is also one of future work. This work was partially supported by JSPS KAKENHI (JP20H00601,JP23K16943,JP23K19967,JP24K20847),JST ACT-X (JPMJAX23CD), JST CREST (JPMJCR21D3,JPMJCR22N2),JST Moonshot R&D(JPMJMS2033-05), JST AIP Acceleration Research (JPMJCR21U2), NEDO (JPNP18002, JPNP20006)and RIKEN Center for Advanced Intelligence Project. Julian Berk, Sunil Gupta, Santu Rana, and Svetha Venkatesh. Randomised gaussian process upper con-dence bound for bayesian optimisation. In Proceedings of the Twenty-Ninth International Conference onInternational Joint Conferences on Articial Intelligence, pp. 22842290, 2021. Ilija Bogunovic, Jonathan Scarlett, Andreas Krause, and Volkan Cevher. Truncated variance reduction:A unied approach to bayesian optimization and level-set estimation. Advances in neural informationprocessing systems, 29, 2016.",
  "Yu Inatsu, Shogo Iwazaki, and Ichiro Takeuchi. Active learning for distributionally robust level-set estima-tion. In International Conference on Machine Learning, pp. 45744584. PMLR, 2021": "Yu Inatsu, Shion Takeno, Hiroyuki Hanada, Kazuki Iwata, and Ichiro Takeuchi. Bounding box-based multi-objective Bayesian optimization of risk measures under input uncertainty. In Sanjoy Dasgupta, StephanMandt, and Yingzhen Li (eds.), Proceedings of The 27th International Conference on Articial Intelligenceand Statistics, volume 238 of Proceedings of Machine Learning Research, pp. 45644572. PMLR, 0204May 2024. URL",
  "Kirthevasan Kandasamy, Je Schneider, and Barnabs Pczos. High dimensional bayesian optimisation andbandits via additive models. In International conference on machine learning, pp. 295304. PMLR, 2015": "Kirthevasan Kandasamy, Gautam Dasarathy, Junier B Oliva, Je Schneider, and Barnabs Pczos. Gaussianprocess bandit optimisation with multi-delity evaluations. Advances in neural information processingsystems, 29, 2016. Kirthevasan Kandasamy, Gautam Dasarathy, Je Schneider, and Barnabs Pczos. Multi-delity bayesianoptimisation with continuous approximations. In International conference on machine learning, pp. 17991808. PMLR, 2017. Johannes Kirschner, Ilija Bogunovic, Stefanie Jegelka, and Andreas Krause. Distributionally robust bayesianoptimization. In Silvia Chiappa and Roberto Calandra (eds.), Proceedings of the Twenty Third Interna-tional Conference on Articial Intelligence and Statistics, volume 108 of Proceedings of Machine Learn-ing Research, pp. 21742184. PMLR, 2628 Aug 2020. URL Shunya Kusakawa, Shion Takeno, Yu Inatsu, Kentaro Kutsukake, Shogo Iwazaki, Takashi Nakano, ToruUjihara, Masayuki Karasuyama, and Ichiro Takeuchi. Bayesian optimization for cascade-type multistageprocesses. Neural Computation, 34(12):24082431, 2022. Kentaro Kutsukake, Momoko Deura, Yutaka Ohno, and Ichiro Yonenaga. Characterization of silicon in-gots: Mono-like versus high-performance multicrystalline. Japanese Journal of Applied Physics, 54(8S1):08KD10, 2015. Benjamin Letham, Phillip Guan, Chase Tymms, Eytan Bakshy, and Michael Shvartsman.Look-aheadacquisition functions for bernoulli level set estimation. In International Conference on Articial Intelligenceand Statistics, pp. 84938513. PMLR, 2022.",
  "Shubhanshu Shekhar and Tara Javidi. Multiscale gaussian process level set estimation. In The 22nd Inter-national Conference on Articial Intelligence and Statistics, pp. 32833291. PMLR, 2019": "Niranjan Srinivas, Andreas Krause, Sham Kakade, and Matthias Seeger. Gaussian process optimization inthe bandit setting: No regret and experimental design. In Proceedings of the 27th International Conferenceon Machine Learning, pp. 10151022, 2010. Shion Takeno, Yu Inatsu, and Masayuki Karasuyama. Randomized Gaussian process upper condence boundwith tighter Bayesian regret bounds. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, BarbaraEngelhardt, Sivan Sabato, and Jonathan Scarlett (eds.), Proceedings of the 40th International Conferenceon Machine Learning, volume 202 of Proceedings of Machine Learning Research, pp. 3349033515. PMLR,2329 Jul 2023. URL",
  "Compute t1(x) and 2t1(x) for each x X by equation 1": "Estimate Ht and Lt by equation 7Generate t from the chi-squared distribution with two degrees of freedomCompute t = t + 2 log(|Xt|), ucbt1(x), lcbt1(x) and at1(x)Select the next evaluation point xt by xt = arg maxxX at1(x)Observe yt = f(xt) + t at the point xtUpdate GP by adding the observed data",
  "r(Ht, Lt) = maxxX lt(x) rt": "When X is nite, we need to modify the denition of the AF and the estimated sets returned at the end ofthe algorithm. Conversely, if X is an innite set, the denitions of Ht and Lt should be modied in additionto the above. Therefore, we discuss the nite and innite cases separately.",
  "When X is nite, we propose the following AF with a modied distribution that t follows": "Denition A.1 (Randomized Straddle for Max-value Loss). For each t 1, let t be a random samplefrom the chi-squared distribution with two degrees of freedom, where 1, . . . , t, 1, . . . , t, f are mutuallyindependent. Dene t = t +2 log(|X|). Then, the randomized straddle AF for the max-value loss, at1(x),is dened as:at1(x) = max{min{ucbt1(x) , lcbt1(x)}, 0}.(5) By using at1(x), the next point to be evaluated is selected by xt = arg maxxX at1(x). Additionally, wechange estimation sets returned at the end of iterations T in the algorithm to the following instead of HTand LT :",
  "For the max-value loss, the following theorem holds under Algorithm 2": "Theorem A.1. Let f be a sample path from GP(0, k), where k(, ) is a positive-denite kernel satisfyingk(x, x) 1 for any x X. For each t 1, let t be a random sample from the chi-squared distribution withtwo degrees of freedom, where 1, . . . , t, 1, . . . .t, f are mutually independent. Dene t = t + 2 log(|X|).Then, the following holds for Rt = ti=1 ri:",
  "where t and C1 are given in equation 6 and Theorem A.1, respectively": "Comparing Theorems 4.1 and A.1, when considering the max-value loss, t should be 2 log(|X|) larger thanin the case of rt, and the constant that appears in the upper bound of the expected value of the cumulativeloss has the relationship C1 = (1 + log(|X|))C1. Note that while the upper bound for rt does not dependon X, it depends on the logarithm of the number of elements in X for the max-value loss. Also, whencomparing Theorem 4.2 and A.2, it is not necessary to consider t in rt, whereas it is necessary to consider tin the max-value loss. For the max-value loss, it is dicult to analytically derive Et[ri], and hence, it is alsodicult to precisely calculate t. Nevertheless, because the posterior distribution of f given Dt1 is again aGP, we can generate M sample paths from the GP posterior distribution and calculate the realization r(j)iof ri from each sample path f (j), and calculate the estimate t of t as",
  "A.2.2Acquisition Function for Max-value Loss when X is Innite": "We dene a randomized straddle AF based on Xt:Denition A.3. For each t 1, let t be a random sample from the chi-squared distribution with twodegrees of freedom, where 1, . . . , t, 1, . . . , t, f are mutually independent. Dene t = 2 log(|Xt|) + t.Then, the randomized straddle AF for the max-value loss when X is innite, at1(x), is dened as:",
  "The next point to be evaluated is selected by xt = arg maxxX at1(x). Finally, we give the pseudocode ofthe proposed algorithm in Algorithm 3": "4If there are multiple x Xt with the shortest L1 distance, determine the one that is unique. For example, we rst choosethe option with the smallest rst component. If a unique determination is not possible, we then select the option with thesmallest second component. This process is repeated up to the d-th component to achieve a unique determination.",
  "Under Algorithm 3, the following theorem holds": "Theorem A.3. Let X [0, r]d be a compact set with r > 0. Assume that f is a sample path from GP(0, k),where k(, ) is a positive-denite kernel satisfying k(x, x) 1 for any x X. Also assume that AssumptionA.1 holds. Moreover, for each t 1, let t = bdrt2("
}