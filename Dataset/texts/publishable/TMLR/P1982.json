{
  "Abstract": "Neural Information Retrieval (NIR) has significantly improved upon heuristic-based Infor-mation Retrieval (IR) systems. Yet, failures remain frequent, the models used often beingunable to retrieve documents relevant to the users query. We address this challenge byproposing a lightweight abstention mechanism tailored for real-world constraints, with par-ticular emphasis placed on the reranking phase. We introduce a protocol for evaluatingabstention strategies in black-box scenarios (typically encountered when relying on APIservices), demonstrating their efficacy, and propose a simple yet effective data-driven mech-anism. We provide open-source code for experiment replication and abstention implemen-tation, fostering wider adoption and application in diverse contexts.",
  "Introduction": "In recent years, NIR has emerged as a promising approach to addressing the challenges of IR on varioustasks (Guo et al., 2016; Mitra & Craswell, 2017; Zhao et al., 2022). Central to the NIR paradigm are thepivotal stages of retrieval and reranking (Robertson et al., 1995; Ni et al., 2021; Neelakantan et al., 2022),which collectively play a fundamental role in shaping the performance and outcomes of IR systems (Thakuret al., 2021). While the objective of the retrieval stage is to efficiently fetch candidate documents from a vastcorpus based on a users query (frequentist (Ramos et al., 2003; Robertson et al., 2009) or bi-encoder-basedapproaches (Karpukhin et al., 2020)), reranking aims at reordering these retrieved documents in a sloweralbeit more effective way, using more sophisticated techniques (bi-encoder- or cross-encoder-based (Nogueira& Cho, 2019; Khattab & Zaharia, 2020)).1 However, despite advancements in NIR techniques, retrieval and reranking processes frequently fail, due forinstance to the quality of the models being used, to ambiguous user queries or insufficient relevant documents",
  "()": ": Procedure diagram for black-box confidence estimation and abstention decision in a rerankingsetting. In the reference-free scenario, confidence function u is a simple heuristic (e.g., maximum). In thedata-driven scenario, u is a light non-trivial function of the relevance scores (e.g., learned linear combination). in the database. These can have unsuitable consequences, particularly in contexts like Retrieval-AugmentedGeneration (RAG) (Lewis et al., 2020), where retrieved information is directly used in downstream tasks,thus undermining the overall system reliability (Yoran et al., 2023; Wang et al., 2023; Baek et al., 2023). Recognizing the critical need to address these challenges, the implementation of abstention mechanismsemerges as a significant promise within the IR landscape. This technique offers a pragmatic approach byabstaining from delivering results when the model shows signs of uncertainty, thus ensuring users are notmisled by potentially erroneous information. Although the subject of abstention in machine learning has longbeen of interest (Chow, 1957; El-Yaniv & Wiener, 2010; Geifman & El-Yaniv, 2017; 2019), most researchhas focused on the classification setting, leaving lack of significant initiatives in IR. A few related workspropose computationally intensive learning strategies (Cheng et al., 2010; Mao et al., 2023) that are difficultto implement in the case of NIR, where models can have up to billions of parameters. Scope.In this paper, we explore lightweight abstention mechanisms that adhere to realistic industrialconstraints, namely (i) black-box access-only to query-documents relevance scores, which is typically thecase when relying on API services, (ii) marginal computational and latency overhead, (iii) configurableabstention rates depending on the application. In line with industrial applications of IR systems for RAGor search engines, our method focuses on assessing the reranking quality of the top retrieved documents.",
  "Notations": "General notations. Let V denote the vocabulary and V the set of all possible textual inputs (Kleeneclosure of V). Let Q V be the set of queries and D V the document database. In the rerankingsetting, a query q Q is associated with k N candidate documents (d1, , dk) Dk coming from thepreceding retrieval phase, some of them being relevant to the query and the others not. When available, wedenote by y {0, 1}k the ground truth vector, such that yj = 1 if dj is relevant to q and yj = 0 otherwise.4 Dataset. When specified, we have access to a labeled reference dataset S composed of N N instances,each of them comprising of a query and k candidate documents, x = (q, d1, , dk) Q Dk, and a groundtruth y {0, 1}k. Formally,S =xi, yiNi=1 .(1) Relevance scoring. A crucial step in the reranking task is the calculation of query-document relevancescores. We denote by f : Q D R the encoder-based relevance function that maps a query-documentpair to its corresponding relevance score, with standing for the encoder parameters. We also defineg : Q Dk Rk, the function taking a query and k candidate documents as input and returning thecorresponding vector of relevance scores. Formally, given x Q Dk,",
  "g (x) = (f (q, d1) , , f (q, dk)) .(2)": "Additionally, we define the sort function s : Rk Rk that takes a vector of scores as input and returns itssorted version in increasing order. For a given z Rk, s(z) is such that s1(z) sk(z).5 We denoteby s : Q Dk Rk the function that takes a query and k candidate documents and directly returns thesorted relevance scores, i.e.,s = s g(3) Document ranking.Once the scores are computed, we have a notion of the relevance level of eachdocument with respect to the query. The idea is then to rank the documents by relevance score. For a givenvector of scores z Rk, let r(z) Sk denote the corresponding vector of positions in the ascending sort,s(z), where Sk is the symmetric group of k elements comprising of the k! permutations of {1, , k}. Inother terms, ri(z) = j if zi is ranked jth in the ascending sort of z. We then define the full reranking functionr : Q Dk Sk, taking a query and k candidate documents as input and returning the correspondingranking by relevance score. More formally,r = r g(4) Instance-wise metric. Finally, when a ground truth vector y is available, it is possible to evaluate thequality of the ranking generated by r on a given query-documents tuple x Q Dk. Therefore, we denoteby m : Rk {0, 1}k R the evaluation function that takes a vector of scores g(x) and a ground truthvector y as input and returns the corresponding metric value. Without any lack of generality, we assumethat m increases with the ranking quality (\"higher is better\").",
  "Abstention in Reranking": "In this work, our goal is to design an abstention mechanism built directly on top of reranker r. We thereforeaim to find a confidence function c : QDk R and a threshold R such that ranking r(x) is outputtedfor a given query-documents tuple x Q Dk if and only if c(x) > . Formally, we want to come up witha function :Q Dk R Sk {} such that for given x and ,",
  "Related Work": "Abstention, also known as selective prediction, has been around for a long time, originally for classificationpurposes (Chow, 1957; El-Yaniv & Wiener, 2010), and has often been associated with topics such as confi-dence estimation, out-of-domain (OOD) detection (Schlkopf et al., 1999; Liang et al., 2017; Darrin et al.,2022; 2024) and prediction error detection (Hendrycks & Gimpel, 2016). These fields collectively encom-pass three main approaches: learning-based methods, which require a dedicated training process; white-boxmethods, which rely on full access to the models internal features; and black-box methods, which focussolely on the models output. Among learning-based methods, the most classic initiatives include algorithms such as SVMs, nearest neigh-bors, boosting (Hellman, 1970; Fumera & Roli, 2002; Wiener & El-Yaniv, 2015; Cortes et al., 2016) orBayesian methods (Blundell et al., 2015) such as Markov Chain Monte Carlo (Geyer, 1992) and VariationalInference (Hinton & Van Camp, 1993; Graves, 2011), that include uncertainty estimation as a whole partof the training process. Similarly, aleatoric uncertainty estimation requires the use of a log-likelihood lossfunction (Loquercio et al., 2020), thus greatly modifying the training process. Some approaches also includespecific regularizers in the loss function (Xin et al., 2021; Zeng et al., 2021), while others incorporate ab-stention as a class during training (Rajpurkar et al., 2018), most of the time with a certain cost associated(Bartlett & Wegkamp, 2008; El-Yaniv & Wiener, 2010; Cortes et al., 2016; Geifman & El-Yaniv, 2019). Itcomes obvious that none of these methods align with the no-training requirement set in .2. Most white-box methods, on the other hand, do not require specific training. When considering selectiveprediction in neural networks, a straightforward and effective technique is to set a threshold over a confidencescore derived from a pre-trained network, which was already optimized to predict all points (Cordella et al.,1995; De Stefano et al., 2000; Wiener & El-Yaniv, 2012). Monte Carlo dropout is another popular approachinvolving randomly deactivating neurons of a model at inference time to estimate its epistemic uncertainty(Houlsby et al., 2011; Gal et al., 2017; Smith & Gal, 2018). Others rely on a statistical analysis of themodels features (hidden layers) (Lee et al., 2018; Ren et al., 2021; Podolskiy et al., 2021; Haroush et al.,2021; Gomes et al., 2022) while ensemble-based methods (Gal & Ghahramani, 2016; Lakshminarayananet al., 2017; Geifman & El-Yaniv, 2019) extend this idea and estimate confidence based on statistics of theensemble models features. The latter approaches might significantly increase inference time, which is notin line with our set of constraints (.2). Moreover, white-box access to the model cannot always beguaranteed in practical situations (e.g., API services). Black-box approaches assume only access to the output layer of the classifier, i.e., the soft probabilitiesor logits. Some works estimate uncertainty by using the probability of the mode (Hendrycks & Gimpel,2016; Lefebvre-Brossard et al., 2023; Hein et al., 2019; Hsu et al., 2020; Wang et al., 2023) while othersutilize the entire logit distribution (Liu et al., 2020), the most widely known and straightforward confidencecalibration technique remaining temperature scaling (Platt et al., 1999). However, it is unclear how to applythese frameworks to the ranking scenario, where relevance scores are neither soft probabilities nor logits butunscaled scalars. Selective prediction and confidence estimation in NLP have seen targeted applications but have room for ex-pansion across broader scenarios. Dong et al. (2018) describe a model developed to estimate confidence specif-ically for semantic parsing, while Kamath et al. (2020) delve into selective prediction for out-of-distribution(OOD) question answering, allowing the model to abstain from answering questions deemed too difficult oroutside the training distribution. More recently, Xin et al. (2021) have applied selective prediction throughloss regularization in various classification settings. Additionally, datasets such as SQuAD2.0 (Rajpurkar",
  "Published in Transactions on Machine Learning Research (09/2024)": "et al., 2018) illustrate the challenge of unanswerable questions. In the IR field specifically, strategies likethose proposed by Cheng et al. (2010) and Mao et al. (2023) allow a model to abstain at a certain costduring training by assessing the confidence level on partial ranks. Here again, such methods do not fit theblack-box constraint mentioned in .2.",
  "Confidence Assessment for Document Reranking": "In this paper, as stated in .2, the confidence scorer we build only takes into account the list ofrelevance scores. Our intuition is that, by analyzing the distribution of the rankers scores, itshould be possible to estimate its confidence on a given instance. In the next two subsections, wedescribe two methods for calculating the confidence score: the first one is reference-free, meaning it does notrequire any external data, while the second one relies on access to reference data for calibration purposes.",
  ". Thresholding. Finally, we compare the confidence score computed in step 2 to a threshold . Ifubase(z) > , we make a prediction using ranker r, otherwise, we abstain (Equation 5)": "In this work, we evaluate a bunch of reference-free confidence functions, relying on simple statistics computedon the relevance scores. We focus more particularly on three functions inspired from MSP (Hendrycks &Gimpel, 2016): (i) the maximum relevance score umax, (ii) the standard deviation ustd, and (iii) thedifference between the first and second highest relevance scores u1-2 (Narayanan et al., 2012).",
  "As a reminder, s(x) is the ascending sort of query-document relevance scores (Equation 3) and m (g(x), y)is the evaluation of the ranking induced by g(x) given ground truth y and metric function m": "An intuitive approach is to fit a simple supervised model to predict ranking performance m (g(x), y) givena vector of sorted relevance scores s(x). In this section, we develop a regression-based approach (): 0. Calibration. We use reference set S to derive S (Equation 6) and then fit a regressor hreg : Rk Ron S. Next, we derive ureg = hreg s, the function that takes a vector of unsorted scores as input andreturns the predicted ranking quality. 1. Observation. As in .1, we observe the relevance scores z = g(x) for a given test instance x.2. Estimation. We then assess confidence using the ureg function. Intuitively, the greater ureg(z), themore confident we are that r correctly ranks the documents with respect to the query.",
  "Models and Datasets": "An extensive benchmark is built to evaluate the abstention mechanisms presented in .Wecollect six open-source reranking datasets (Lhoest et al., 2021) in three different languages, English,French and Chinese: stackoverflowdupquestions-reranking (Zhang et al., 2015), denoted StackOver-flow in the experiments, askubuntudupquestions-reranking (Lei et al., 2015), denoted AskUbuntu,scidocs-reranking (Cohan et al., 2020), denoted SciDocs, mteb-fr-reranking-alloprof-s2p (Lefebvre-Brossard et al., 2023), denoted Alloprof, CMedQAv1-reranking (Zhang et al., 2017), denoted CMedQAv1,and Mmarco-reranking (Bonifacio et al., 2021), denoted Mmarco. Wealsocollect22open-sourcemodelsforevaluationoneachofthesixdatasets.Theseincludemodelsofvarioussizes,bi-encodersandcross-encoders,monolingualandmul-tilingual:ember-v1(papertobereleasedsoon),llm-embedder(Zhangetal.,2023),bge-base-en-v1.5,bge-reranker-base/large (Xiao et al., 2023),multilingual-e5-small/large,e5-small-v2/large-v2 (Wang et al., 2022), msmarco-MiniLM-L6-cos-v5, msmarco-distilbert-dot-v5,ms-marco-TinyBERT-L-2-v2, ms-marco-MiniLM-L-6-v2 (Reimers & Gurevych, 2021), stsb-TinyBERT-L-4,stsb-distilroberta-base,multi-qa-distilbert-cos-v1,multi-qa-MiniLM-L6-cos-v1,all-MiniLM-L6-v2,all-distilroberta-v1,all-mpnet-base-v2,quora-distilroberta-baseandqnli-distilroberta-base (Reimers & Gurevych, 2019).7 In addition, each dataset is preprocessed in such a way that there are only 10 candidate documents perquery and a maximum of five positive documents, in order to create a scenario close to real-life rerankinguse cases. Moreover, to fit the black-box setting, query-documents relevance scores are calculated upstreamof the evaluation and are not used thereafter. Further details are given in appendix A.",
  "Instance-Wise Metrics": "In this work, we rely on three of the metrics most commonly used in the IR setting (e.g., MTEB leaderboard(Muennighoff et al., 2022)): Average Precision (AP), Normalized Discounted Cumulative Gain (NDCG) andReciprocal Rank (RR): AP (Zhu, 2004) computes a proxy of the area under the precision-recall curve; NDCG(Jrvelin & Keklinen, 2002) measures the relevance of the top-ranked items by discounting those furtherdown the list using a logarithmic factor; and RR (Voorhees et al., 1999) computes the inverse rank of the firstrelevant element in the predicted ranking. For all three metrics, higher values indicate better performance.When relevant, we rely on their averaged versions across multiple instances: mAP, mNDCG and mRR.",
  "Assessing Abstention Performance": "The compromise between abstention rate and performance in predictive modeling represents a delicatebalance that requires careful consideration (El-Yaniv & Wiener, 2010). First, we want to make sure that anincreasing abstention rate implies increasing performance, otherwise the mechanism employed is ineffectiveor even deleterious. Secondly, we aim to abstain in the best possible way for every abstention rate, i.e., wewant to maximize the growth of the performance-abstention curve. Inspired by the risk-coverage curve (El-Yaniv & Wiener, 2010; Geifman & El-Yaniv, 2017), we evaluateabstention strategies by reporting the normalized Area Under the performance-abstention Curve, where the 6Scikit-learn implementation (Pedregosa et al., 2011).7We take the liberty of evaluating models on languages in which they have not been trained, as we posit it is possible todetect relevant patterns in the relevance scores in any case. We discuss this assertion in .2.",
  ",(7)": "S being the set of predicted instances, i.e., S = {(x, y) S | (x, ) =}. We collect n abstentionrates, (Ri ())ni=1, and associated performances, (Pi,m ())ni=1, and then compute the area underthe performance-abstention curve AUCm () for metric function m. 2. Random baseline. Second, we evaluate , the mechanism that performs random (i.e., ineffective)abstention. Intuitively, random abstention shows flat performance as the abstention rate increases,equal to P,m () (Equation 7).8 3. Oracle evaluation. Then, we evaluate the oracle on the same task. Intuitively, the oracle has accessto the test labels and can therefore select the best instances for a given abstention rate. AUCm ()thus upper-bounds AUCm ().",
  "AUCm () AUCm ().(8)": "In particular, nAUCm() = 1 means that abstention mechanism reaches oracle performance. Incontrast, nAUCm() < 0 indicates that has a deleterious effect, with declining average rankingperformance while abstention increases. In this study, in order to guarantee consistent comparisons between abstention mechanisms, we randomlyset aside 20% of the initial dataset as a test set, treating the remaining 80% as the reference set. Results,unless specified otherwise, are averaged across five random seeds.",
  "Abstention works.All evaluated abstention-based methods improve downstream evaluation metrics(nAUCs greater than 0), showcasing the relevance of abstention approaches in a practical setting": "Reference-based abstention works better.The value of reference-based abstention approaches isdemonstrated in , in which we notice that the linear-regression-based method ulin performs on aver-age largely better than all reference-free baselines, edging out the best baseline strategy (ustd) by almost 10points on the mAP metric. Having shown the superiority of reference-based abstention methods across the entire benchmark, we conduct(unless otherwise specified) the following experiments on the ember-v1 model using mAP for evaluation (mainmetric in the MTEB leaderboard (Muennighoff et al., 2022)), and compare ulin to ustd, respectively the bestreference-based and reference-free methods mAP-wise.",
  "Abstention Rate": "0.75 0.80 0.85 0.90 0.95 1.00 mRR u* uumaxustdu12 ulin : Performance-abstention curves for the ember-v1 model on the StackOverflow dataset. Allmethods show increasing curves, indicating effective abstention. The reference-based method, ulin, standsout above the others, demonstrating superior abstention performance.Note that u and u respectivelydenote the oracle and the random baseline, associated with the and mechanisms (Sections 2.2 and 4.3).",
  "Abstention Effectiveness vs. Raw Model Performance": "We investigate whether a correlation exists between abstention effectiveness and model raw performance (i.e.,without abstention) on the reranking task. For each model-dataset pair, we compute both no-abstentionmAP and abstention nAUC, using ulin as a confidence function (, ). A better ranker implies better abstention. and showcase a clear positive correlationbetween model raw performance and abstention effectiveness, albeit with disparities from one dataset toanother. Better reranking methods naturally produce more calibrated score distributions, which in turn leadsto improved abstention performance. This highlights the value of incorporating abstention mechanisms intohigh-performing retrieval systems and reinforces that their purpose is not to turn weak rankers into strongones, but to enhance the quality of models that already perform reasonably well. For thoroughness, weevaluated a range of open-source models with varying performance levels, though in practice, users typicallyselect models they already know to be sufficiently effective. 8A random (ineffective) abstention method cannot differentiate between good and bad instances.Thus, in expectation,random abstention on the test dataset results in a constant performance whatever the abstention rate.",
  "Threshold Calibration": "In practical industrial settings, a major challenge with abstention methods lies in choosing the right thresholdto ensure a desired abstention rate or performance level on new data. To evaluate threshold calibrationquality, we choose three target abstention rates (10%, 50%, and 90%) and rely on the reference set9 to inferthe corresponding threshold and level of performance (here, the mAP). Then, the Mean Absolute Error(MAE) between target and achieved values on test instances is reported (), averaging across 1000random seeds on the StackOverflow dataset.10 ulin is better calibrated than ustd. The lower part of shows that the MAE between the target andachieved mAP increases with the abstention rate, which is logical because a higher abstention rate reducesthe number of evaluated instances and therefore increases volatility. However, we observe that the MAEincreases much less significantly for the ulin method than for the ustd method, with the latter having a MAEtwice as high at a target abstention rate of 90%. This provides strong evidence that ulin is more reliable athigh abstention rates.",
  "Domain Adaptation Study": "In a practical setting, it is rare to have a reference set that perfectly matches the distribution of samplesseen at test time. To measure the robustness of our method to data drifts, we fit them on a given datasetand evaluate on all others. Reference-based approaches are globally not robust to domain changes. From , we see onaverage that the reference-based method underperforms reference-free baselines when fitted on the \"wrong\"reference set. However, certain reference datasets seem to enable methods to generalize better overall, even 9In this setting, access to a reference set is essential in order to determine which threshold value corresponds to a givenabstention rate or level of performance.10For example, if the mAP corresponding to a target abstention rate of 50% in the reference set is 0.6 and the values achievedon test set on 2 different seeds are 0.5 and 0.7, the MAE is be equal to 1/2 (|0.6 0.5| + |0.6 0.7|) = 0.1."
}