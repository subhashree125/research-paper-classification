{
  "Abstract": "Vision-language foundation models such as CLIP have showcased impressive zero-shot ca-pabilities. However, their applicability in resource-constrained environments is limited dueto their size and the resulting latency. Knowledge distillation allows to mitigate these chal-lenges by distilling small image encoders that can replace the large CLIP image encoder. Ina zero-shot setting, where only the class names are known, no real domain images can beused for this process. Instead, we investigate the use of synthetic images for this purpose.Unlike existing works that focus on improving the quality of synthetic images to bridge theperformance gap compared to training on natural images, we find the choice of loss to be acrucial factor. Specifically, minimizing only the distance between the student and teacherimage features, without incorporating image captions in the loss function, increases therobustness to spurious features and data corruptions. As a result, this feature distillationapproach greatly improves the transfer performance from synthetic to real images. Lever-aging these insights, we are able to train domain-specific students that achieve zero-shotperformance comparable to a ViT-B/32 teacher on six fine-grained classification datasetswhile using up to 92% fewer parameters.",
  "Introduction": "Motivation.Image classifiers built on top of large vision(-language) foundation models, such as CLIP(Radford et al., 2021) or DINOv2 (Oquab et al., 2023), have shown impressive zero-shot capabilities acrossvarious tasks.However, their extensive parameter count and high inference latency present significantchallenges for deployment in resource-constrained edge devices used in driver-assistance systems, automateddriving, mobile robotics, or video surveillance. Due to their reduced capacity, smaller models cannot beexpected to match the performance of larger ones in arbitrary domains. Additionally, training large-scalefoundation models typically involves several millions or billions of images, making it expensive and time-consuming. Together, this motivates the need for smaller domain-specific models, as well as data-efficienttraining procedures. In this work, we specifically focus on zero-shot image classification, for which only asmall-scale image encoder is required. Class-specific text embeddings are fixed and can be precomputedoff-device, while only image embeddings are computed on-device. Thus, our goal is to distill smaller drop-inreplacements (students) of the CLIP image encoder (teacher) that achieve on-par performance on the specifictarget domains of interest. In particular, we want to specialize the image encoder student to novel domainsfor which we only know the relevant classes, but do not have access to actual images, the so called zero-shot",
  "Test Accuracy": ": Overview over our zero-shot distillation framework and the observed properties ofvision-language distillation and feature distillation. We only use the class names to generate domain-specific synthetic images for the distillation of small CLIP image encoders. The reported test accuracies arefrom domain-specific TinyViT-11M students on six fine-grained classification datasets. The crucial factor toclosely reach the performance of the teacher is to minimize the distance between image features of the teacherimage encoder and the student encoder (feature distillation). The common approach of aligning the text andimage features (vision-language distillation) leads to substantially worse performance. This observation canbe attributed to improved robustness properties of feature distillation against spurious features and commoncorruptions which we discuss in our findings. distillation setting. For zero-shot distillation, domain-specific data can be obtained from general-purposegenerative models, such as large-scale latent diffusion models (Podell et al., 2023), by class-aware prompting.However, learning from synthetic images has proven challenging (Sariyildiz et al., 2022; Azizi et al., 2023).Despite attempts to improve synthetic images in terms of quality and diversity (Yu et al., 2023) in orderto reduce the domain gap to natural images, there still remains a substantial drop in performance whentransferring from the synthetic to real image domain (Azizi et al., 2023; Sariyildiz et al., 2022). While previous works primarily focus on improving the synthetic data for zero-shot learning, we identify theloss function as a critical factor that impacts performance when training on synthetic images. Specifically, weobserve that two classes of loss functions exhibit distinct properties when transferring between the syntheticand real domain. Therefore, we differentiate between vision-language distillation and feature distillation. Forvision-language distillation, the embeddings of image-caption pairs are aligned through the loss function.Feature distillation relies solely on matching the image embeddings of a trainable student to those of afrozen teacher. We find that vision-language distillation has several drawbacks, including the susceptibilityto spurious features and common corruptions in the training set. This impedes the generalization from asynthetic train to a real test set. Conversely, feature distillation demonstrates greater robustness to theseinfluences. Thereby, it improves the zero-shot transfer from synthetic images. Moreover, we find that thespecificity of the data influences the distillation process of image encoders. Current baselines for distilledCLIP image encoders (Wu et al., 2023; Yang et al., 2023a; Vasu et al., 2023) are trained on large-scale commoncrawl datasets, relying on the models generalization for zero-shot performance. For small image encoderswith a substantially smaller capacity, however, this cannot be achieved. Thus, our approach first distillson domain-agnostic datasets with diverse images, such as DataComp (Gadre et al., 2023), ImageNet (Denget al., 2009) or SynthCI (Hammoud et al., 2024), and subsequently on domain-specific synthetic datasets,like pets, food or similarly. This strategy results in superior domain-specific performance, especially forsmaller image encoders.",
  "Published in Transactions on Machine Learning Research (11/2024)": "Zhe Gan, Linjie Li, Chunyuan Li, Lijuan Wang, Zicheng Liu, and Jianfeng Gao. Vision-language pre-training:Basics, recent advances, and future trends. Found. Trends. Comput. Graph. Vis., 14(34):163352, dec2022. ISSN 1572-2740. Scott Geng, Cheng-Yu Hsieh, Vivek Ramanujan, Matthew Wallingford, Chun-Liang Li, Pang Wei Koh, andRanjay Krishna. The unmet promise of synthetic training images: Using retrieved real images performsbetter. arXiv:2406.05184, 2024.",
  "Related Work": "Knowledge Distillation of Vision-Language Models.Knowledge Distillation (Hinton et al., 2015) isa widely used technique for transferring knowledge from larger teachers to smaller students. In its vanillaform, the approach involves combining a standard training loss with a distillation loss that considers theoutput of both the student and teacher on logit-level, penalizing discrepancies between the two models.Knowledge distillation has been observed to not only benefit the test accuracy of the student on the targetdatasets but transfer other favorable properties of the teacher such as domain generalization (Ojha et al.,2023). While this approach has been well-established for single-modality tasks including vision (Wu et al.,2022; Mirzadeh et al., 2020; Marrie et al., 2024) or language (Sanh et al., 2020; Jiao et al., 2020), recentworks have extended the concept to the multi-modal setting, specifically in the context of vision-languagemodels. CLIP-KD (Yang et al., 2023a) provides an extensive set of experiments comparing various differentloss combinations. TinyCLIP (Wu et al., 2023) proposed an advanced initialization process using weightinheritance from the teacher to the student as well as a multi-stage progressive distillation culminating instudents that are only one fourth the size of a ViT-B/32 CLIP model. MobileCLIP (Vasu et al., 2023) furtherrefined the distillation process by incorporating image augmentation, synthetic captions, and dedicatedarchitectural choices. In contrast to these existing methods, our approach focuses on finding only a one-to-one replacement of the vision encoder while the text encoder remains frozen. Apart from CLIP-specifictechniques, unsupervised distillation based purely on images without labels has been identified as a data-efficient alternative to supervised training for vision encoders (He et al., 2022). We build on this observation",
  "Zero-Shot Distillation for Transfer Learning from Synthetic Images": "Zero-shot distillation is a framework for transferring knowledge from a teacher to a student in a settingwhere one does not have access to images from the target domain but only textual descriptions of classes.The framework specifically focuses on the ability of foundation models as teachers to perform well on unseendata due to their generalization properties. The objective is to transfer this performance to a smaller studentwithout utilizing any data from the unseen target domain. Therefore, the primary goal is not to address thedisparity between the datasets used to train the teacher and student, but rather to extract domain-specificknowledge from the teacher without having access to data from the target domain.The term zero-shotdistillation has been introduced previously (Nayak et al., 2019), yet only in the setting for single-modalclassifiers that were trained using the cross-entropy loss. In our case, we consider CLIP which is a vision-language model instead of a simple image classifier. For zero-shot distillation, we use two types of teacherknowledge. On the one hand, explicit distillation from a CLIP teacher through the loss function. On theother hand, implicit distillation from the generative text-to-image model that generates the synthetic images.In this section, we discuss the four aspects that constitute the zero-shot distillation framework: the datadomain, the training pipeline, the generation of diversified synthetic training data and the selection of anappropriate loss function.",
  "Data Domain": "For training in a zero-shot setting, there are currently two core approaches. The first one involves relyingon large-scale, domain-agnostic data such as common crawl datasets (Schuhmann et al., 2021; Gadre et al.,2023). While this approach is feasible for large foundation models, it poses challenges for smaller models asthese lack the capacity to fit diverse data to the same extent as larger ones. The second approach involvesdomain-specific distillation using either purely synthetic images (Hammoud et al., 2024; Tian et al., 2023b;a)or a combination of real and synthetic images (Yu et al., 2023; Azizi et al., 2023). Yet, the existing worksthat use this approach incorporate few-shot learning on real images (Hammoud et al., 2024; Tian et al.,2023b) or linear probing (Hammoud et al., 2024; Tian et al., 2023a;b) after training on synthetic data.Consequently, the reported accuracies are no longer zero-shot. Within our framework, we find that bestzero-shot performance is achieved through a two-stage approach: first training on domain-agnostic images,followed by training on domain-specific synthetic images. An alternative to using synthetic images would beto retrieve images from the dataset on which the generative model was trained on. However, this datasetmight be proprietary or strictly regulated due to privacy concerns such that synthetic images are the onlyfeasible option. Thus, we focus on using synthetic images for domain-specific distillation in our framework.",
  "Training Pipeline": "In order to shorten training in comparison to training from scratch, Wu et al. (2023) introduced weightinheritance as an initialization scheme for distilling CLIP models. This method has a significant limitationas it can only be applied when the student shares a similar architecture with the teacher. Instead of usingweight inheritance, our framework comprises of training on a domain-agnostic dataset, which is also referredto as pre-training (Gan et al., 2022), before training on domain-specific synthetic datasets. Training largefoundation models like the original CLIP (Radford et al., 2021) typically requires substantial computationalresources due to the use of billions of images. Yet, He et al. (2022) observed that domain-agnostic dis-tillation on natural images can be sped up significantly by using feature distillation. For our purpose ofdistilling CLIP vision encoders using synthetic images, this step has further advantages: by aligning theteacher and student image features, we can mitigate phenomena like the modality gap (Liang et al., 2022)where corresponding output vectors are located in different areas of the embedding space. Through featuredistillation, we ensure that the students geometrically match the teacher and can be used as direct replace-ments. After domain-agnostic distillation, we perform the domain-specific distillation on synthetic images.Domain-agnostic distillation only needs to carried out once for all students. The additional costs comparedto distillation from scratch only on the synthetic images are therefore negligible if domain-specific distillationis performed for many target domains.",
  "Data Diversification of Synthetic Images": "In the context of zero-shot learning for image classification, synthetic data generation is based on the classnames. However, it has been observed that using only the names to generate images using diffusion modelsleads to suboptimal performance (Sariyildiz et al., 2022). This is primarily due to the lack of diversity inthe generated images as well as class ambiguity (da Costa et al., 2023). To address this challenge, recentapproaches have turned to leveraging large language models (LLMs) to enhance diversity in the prompts. Inaddition to class names, LLMs are guided by additional inputs for diversification, such as information from aconcept bank (Hammoud et al., 2024) or specific requirements related to contextual and style diversification(Yu et al., 2023). By incorporating these additional sources of guidance, the generated synthetic data becomesmore diverse and aligned with the desired objectives of the target setting. Using the approach from Yu et al.(2023), our framework focuses on contextual dimensions to achieve diversification. These dimensions areattributes that describe the context of the image such as the background, camera angle, object position,presentation style, and superclasses, all of which are tuned specifically for the target dataset. In contrast toYu et al. (2023), we do not prompt the LLM for each caption separately, but ask for different options foreach contextual dimension. This reduces the risk of obtaining similar captions. The final prompt used forthe text-to-image model is a comma-separated list of options for these contextual dimensions. Instead ofusing all possible combinations of options, which would result in a strongly growing number of images givenmore options, we use an approach based on combinatorial testing (Ahmed et al., 2017; Nie & Leung, 2011)described in Section A.3.",
  "EvaluationDomain-Domain-NaturalSyntheticDataNameMetricAgnosticSpecificImagesImagesDiversificationLoss": "StableRep (Tian et al., 2023b)Linear probe, few-shotMPSynCLR (Tian et al., 2023a)Linear probeMPSynthCLIP (Hammoud et al., 2024)Linear probe, few-shotCLIPFake it till you make it (Sariyildiz et al., 2022)Zero-Shot Acc.Cross-entropyDiversify dont finetune (Yu et al., 2023)AccuracyCustom(Azizi et al., 2023)AccuracyCross-entropyDM-KD (Li et al., 2023b)AccuracyLogit-based knowledge distillationTinyCLIP (Wu et al., 2023)Zero-Shot Acc.CLIP, Affinity mimickingMobileCLIP (Vasu et al., 2023)Zero-Shot Acc.CLIP, Affinity mimickingZero-Shot Distillation (Ours)Zero-Shot Acc.L2 feature distillation :Our framework differs from previous approaches by using feature distillation insteadof vision-language distillation. For DM-KD, the teacher was trained on the real domain-specific images,which is not possible in a zero-shot setting. This is symbolized by .",
  "Loss Selection": "The choice of loss function distinguishes our framework from existing approaches for distillation in the zero-shot setting. The critical distinction lies between vision-language distillation and feature distillation. Thelatter ultimately improves effective transfer learning from synthetic images which we demonstrate throughour five findings in Section A.9. Therefore, our framework is based on feature distillation. Vision-Language Distillation.Training image encoders is typically carried out using loss functions suchas the cross-entropy loss that consider image-class pairs and tries to optimize for correct class predictions.The training of small models can be improved by adding guidance from a larger model through knowl-edge distillation. Standard knowledge distillation compares the predictive distribution between student andteacher over the classes. When training image encoders or vision-language models on common crawl, class-based loss functions are not applicable as the images are paired with captions instead of class labels. For thispurpose, the contrastive InfoNCE loss (van den Oord et al., 2019), also know as CLIP loss, aims at aligningthe embeddings between image and caption. In the case of training on datasets with image-class pairs, theCLIP loss can still be applied by using the zero-shot captions \"a photo of {class name} which is a typeof {superclass}\". These were originally introduced for the zero-shot inference of the original CLIP model(Radford et al., 2021). \"Superclass\" refers to a general description of the object that can be encounteredsuch as pets, food, cars or similarly. By using these class-specific prompts, several images in a batch mayshare the same caption. This conflicts the goal of decreasing the similarity of image embeddings to the textembeddings of not matching captions in the CLIP loss. An alternative to the CLIP loss is given by themulti-positive contrastive loss introduced in StableRep (Tian et al., 2023b). The details on how to adaptthe multi-positive (MP) loss to our setting are given in Section A.15. In the following, we refer to the CLIPand MP losses as vision-language losses. Feature Distillation.(Romero et al., 2014) found that the generalization and training speed of thinconvolutional neural networks can be improved by adding an additional loss that aligns the features of thestudent and teacher. In contrast to the vision-language distillation, the student directly learns from theimage features of the teacher without considering the captions. Like He et al. (2022), our framework buildson the L2 distance between the normalized student and teacher image features as loss function. This choiceis motivated by the theoretical investigation presented in Section A.16, which highlights that low L2 loss andhigh train-test similarity are sufficient to ensure student-teacher agreement. An alternative loss function forfeature distillation is investigated in Section A.7.",
  "Positioning of Existing Approaches in our Framework": "To complement the framework, we position existing baselines with respect to the discussed components in. The first difference between existing works and our setup is that apart from DM-KD (Li et al.,2023b), none of the other approaches perform distillation on domain-specific datasets. The most crucialaspect is that all baselines using synthetic images are trained with vision-language losses whereas we usefeature distillation. Based on our findings presented in , these differences are decisive for enablingeffective transfer learning from synthetic images.",
  "In this section, we briefly describe the setup used to conduct the experiments supporting our findings": "Datasets and Hyperparameters.As introduced in .2, the first step of our framework is toperform feature distillation on a large-scale, domain-agnostic dataset. For this purpose, we select DataCompmedium (Gadre et al., 2023) with 123 million images and train for a single epoch. At the time we conductedour experiments 86% of the original image URLs were still active. For comparison, we perform to domain-agnostic distillation on ImageNet (Deng et al., 2009) with 1.28 million images and SynthCI 30M Hammoudet al. (2024) with 30 million synthetic images in .3. For domain-specific distillation, we target theOxford Pets (Parkhi et al., 2012), Oxford Flowers (Nilsback & Zisserman, 2008), Food-102 (Bossard et al.,2014), Stanford Cars (Krause et al., 2013), Describable Textures (Cimpoi et al., 2014) and Aircrafts (Majiet al., 2013) datasets. In the appendix, we include ImageNet-100 (Tian et al., 2020) as a non-domain-specificdataset for reference. These datasets are only used for testing while the actual datasets used for trainingare synthetically generated based on the class names. Using the diversification strategy discussed in .3, we select a set of five different contextual dimensions and corresponding weights in the prompts for thediffusion model. The number of images per class roughly matches the size of the real training datasets. Weuse 265 images per class for the smaller, less diverse datasets and 1011 for the larger ones. More detailson the selection of contextual dimension and the dataset sizes are given in Section A.3. As the selection ofoptions for the contextual dimensions and superclasses are relatively simple, we can use a smaller languagemodel Llama-2 7B fine-tuned for chats (Touvron et al., 2023) and still obtain sufficiently diverse prompts.For the generation of the images, we utilize a LCM LoRA (Luo et al., 2023) of Stable Diffusion XL (Podellet al., 2023). Due to the LCM LoRA architecture, 6 inference steps together with a guidance scale of 0.5suffice to obtain high-quality images.For both domain-agnostic and domain-specific distillation we usethe same hyperparameters. We train using a batch size of 256 and a constant learning rate of 5 104 using the AdamW optimizer (Loshchilov & Hutter, 2019). All other hyperparameters and augmentationswere kept consistent with the CLIP training methodology (Radford et al., 2021). One epoch of training onDataComp medium corresponds to 4.3105 optimization steps. For domain-specific distillation, we perform96 optimization epochs for all models. Ablations with a different teacher model and synthetic data generatorare contained in the supplementary material. Student and Teacher Architectures.As teacher, we employ a ViT-B/32 (Dosovitskiy et al., 2021)CLIP vision encoder that was trained on DataComp-XL, a dataset consisting of 12.8 billion image-text pairsfrom common crawl (Gadre et al., 2023). The corresponding text encoder follows the same architecture asdescribed in the original CLIP paper, with 63 million parameters (Radford et al., 2021) and an embeddingdimension of 512. For our students, we utilize two different types of state-of-the-art architectures: Efficient-Nets (Tan & Le, 2019), which are based on convolutional neural networks, and TinyViTs (Tan & Le, 2019),which are hybrid models combining convolutions and transformers. For our final results, we respectivelyselect three models in the 5, 10, and 20 million parameter range from each architecture type. To present ourfindings, we report the results on the TinyViT with 11 million parameters. To align the output of the visionencoder with the embedding dimension of the teacher, we apply a single linear projection head.",
  "Finding 1: Feature Distillation is Less Susceptible to Spurious Visual Features ThanVision-Language Distillation": "Our first set of experiments is designed to test the hypothesis that class-level information introduced throughthe captions in vision-language distillation leads the model to learn spurious features as well as character-istics of synthetic images. To validate our hypothesis, we conducted two experiments on the pets dataset,deliberately introducing dedicated spurious features into real and synthetic images.",
  "Synth.withspuriousfeatures": ":Feature distillation increases the robustness to spurious features. The accuracies referto students distilled and evaluated on pets. The spurious features are introduced through adding coloredshapes (real) or class-specific unicolor backgrounds (synthetic). For the test set \"shuffled spurious features\"the coloured shapes are shuffled among the classes. Red indicates a performance drop due to the reliance onspurious features. Natural Images with Spurious Features.To investigate the impact of spurious features in the naturalimage domain, we add class-specific colored shapes to the images in the pets dataset. These shapes wereadded to each image in the training split. Examples can be seen in . Using these images, we performdomain-specific distillation with different losses after the domain-agnostic distillation. The test accuraciesof the resulting models are shown in . We observe a decrease in performance on the test set withoutspurious features when the students were trained with vision-language distillation using the CLIP or multi-positive (MP) loss. This suggests that these students did not acquire any additional class-specific featuresduring domain-specific distillation with vision-language losses. Instead, they overfit on the visual spuriousfeatures. When evaluating these students on a test set of real images where the colored shapes are shuffledbetween classes, we observe a significant degradation in performance. In contrast, the students trained withthe L2 loss achieves accuracies comparable to the dataset without spurious features on both test sets. Thesefindings highlight the robustness of feature distillation in mitigating the negative impact of spurious featuresin the natural image domain. Synthetic Images with Spurious Features.To investigate whether the observed behavior on real im-ages can be replicated with synthetic ones, we generated a synthetic dataset incorporating dedicated spuriousfeatures. Specifically, we sample images where pets are positioned against a solid-colored background, witheach class assigned a distinct color. The results shown in indicate that the performance of studentstrained with vision-language distillation deteriorates when confronted with the presence of these spuriousfeatures. showcases instances of misclassifications. Importantly, the student trained through fea-ture distillation exhibited a test accuracy of 84.4%, which is only 5% lower than the accuracy of the teacherdespite the domain gap between real and synthetic images, as well as the presence of spurious features.",
  "Finding 2: Feature Distillation Increases Robustness to Common Corruptions": "In addition to the influence of spurious features, we hypothesize that feature distillation increases robustnessagainst common corruptions. In order to evaluate this claim, we conducted a comprehensive benchmarkstudy and assessed the performance of the classifiers under 15 common corruptions and five severity levels(Hendrycks & Dietterich, 2019) on the pets dataset. In we report the relative performance undercorruption with respect to the classification accuracy as defined by (Michaelis et al., 2019). Further resultsare provided in the Supplementary Section A.9. Our observations reveal that feature distillation improvesthe robustness, regardless of whether training is performed on real or synthetic data. The distinction tothe models trained with vision-language losses is particularly prominent when training on domain-specificsynthetic data. In this case, the models trained with the CLIP loss perform worse than the models trainedpurely on domain-agnostic data. The models trained using only feature distillation achieve the strongestrobustness which reflects the observations from Sections 5.1.",
  "Synthetic TrainingImage": ": Examples for the influence of spurious features in the training images. Examples for thecorrect and incorrect classifications. The first row corresponds to the setting of training on natural imageswith colored shapes spurious features. The second corresponds to students trained on synthetic images wherethe background colors are spurious features. All of the test examples are classified correctly by the teacherand the students trained through L2 feature distillation.",
  "Food": "Dataset for Domain-Agnostic Distillation NoneDataComp-128MImageNet-1kSynthCI-30M :Initial domain-agnostic distillation on large-scale datasets of natural images accel-erates subsequent domain-specific distillation on synthetic images. We perform domain-specificdistillation of TinyViT 11M students on synthetic images and report the zero-shot classification accuracydepending on the number of epochs. The students are either distilled from scratch (no domain-agnosticdistillation) or after initial domain-agnostic distillation on ImageNet, DataComp medium or SynthCI 30M.",
  "Finding 3: Initial Domain-Agnostic Distillation Accelerates Domain-Specific Distillation": "To complement our previous findings, we investigate the influence of domain-agnostic distillation on thedomain-specific performance of the students. Therefore, we compare domain-agnostic distillation on Dat-aComp medium, ImageNet and SynthCI 30M as well as no domain-agnostic distillation (domain-specificdistillation from scratch). In , the accuracies depending on the number of epochs are shown forstudents trained on domain-specific synthetic data. First, we find that when distilling for less than 100epochs, initial domain-agnostic distillation consistently leads to higher accuracies. Furthermore, the accu-racy of the models with initial domain-agnostic distillation converges within fewer epochs. When performingdomain-specific distillation for 300 epochs, the models without domain-agnostic distillation come closer tothe performance of the models with domain-agnostic distillation. The difference on the cars and food datasetsis within 3%. On the pets and flowers dataset, however, the margin still remains greater than 5%. Domain-agnostic distillation only needs to be performed once for all students. Therefore, its additional costs becomenegligible if domain-specific distillation is performed for many target domains. Second, we find that thedata with which the domain-agnostic distillation is performed has a marginal impact on the performance ofthe models after domain-specific distillation. The difference of the final performance (after domain-specificdistillation) using domain agnostic-distillation on ImageNet or SynthCI versus DataComp is smaller than 2%on all four datasets. In contrast, the accuracy of the students distilled only on the domain-agnostic datasetsdiffers more strongly. For example, the model distilled on ImageNet is 19% better on pets but 21.7% worse oncars compared to the model that was distilled on DataComp medium. This can be explained by the overlapbetween classes in the train and test datasets. 25 of the 37 classes from Oxford Pets are already containedin ImageNet. This also explains why domain-specific distillation on pets after distillation on ImagNet leadsto a slight decrease in performance. In contrast, none of the classes of the cars dataset are part of ImageNet.The student distilled on the synthetic images from SynthCI 30M achieves accuracies similar to the studentdistilled on DataComp medium apart from the cars dataset where the SynthCI model only reaches 8.4%accuracy. Regardless of whether the initial domain-specific distillation was performed on synthetic imagesor real images, the difference in performance of the students after subsequent domain-specific distillationis marginal. This indicates that the decisive factor for accelerating the domain-specific distillation is thediversity of the images for the domain-agnostic distillation and not whether they are real or synthetic.",
  "Domain-Agnostic+ Domain-Specific": ": Our models trained on synthetic images bridge the gap to training on natural images.The upper part summarizes the baseline CLIP, TinyCLIP and MobileCLIP. Domain-agnostic denotesdistillation only on DataComp medium.Domain-agnostic+Domain-specific contains the results whereeither synthetic data or real data is used for subsequent domain-specific distillation. The blue box highlightsour final models using synthetic images. Gray numbers indicate that the performance is not zero-shot.",
  "Finding 4: Zero-Shot Distillation Bridges the Gap to Baselines Trained on Real Images": "Our goal is to achieve state-of-the-art zero-shot accuracy on fine-grained visual classification tasks. Based onfindings 1 and 2, we hypothesize that feature distillation greatly improves zero-shot training of small visionencoders on synthetic data. Therefore, we report the zero-shot classification accuracy of image encodersbased on TinyViT-11M architecture and compare them to existing baselines. The results are shown in with additional datasets presented in . Baselines.The main baseline is the performance of the ViT-B/32 teacher trained on DataComp-XL andthe same model trained on DataComp-medium. The teacher achieves zero-shot accuracies of over 80% onthe pets, cars and food datasets as well as over 70% on the flowers dataset. The accuracies of the ViT/B-32 CLIP model trained on DataComp medium are substantially worse. The performance gap is between28% and 42%. Additionally, we report the accuracies of four TinyCLIP and three MobileCLIP models.These models have undergone extensive training on large-scale datasets for multiple epochs. In the case ofTinyCLIP, the LAION-400M (Schuhmann et al., 2021) or YFCC-15M (Thomee et al., 2016) datasets wereused. Even the smallest TinyCLIP model has been exposed to six times as many images as our models,while the largest models have encountered over 120 times as many samples. The TinyCLIP models exhibit acomparable size to our models in terms of the number of parameters when not considering the text encoderwhich is not required for zero-shot classification. The largest TinyCLIP model has 40% fewer parametersthan the ViT-B/32 CLIP model and achieves its performance up to a margin of 9%. The smallest TinyCLIPmodel features the same number of trainable parameters as our students but has a gap of over 75% to theViT-B/32 CLIP model on the cars dataset. Of the MobileCLIP models, the smallest one features an imageencoder that is comparable in size to our students. Its performance, however, is comparable to the largestTinyCLIP model. This can be attributed to the fact that it was trained on a dataset featuring the samenumber of images as DataComp-XL but with synthetically enhanced captions and additional augmentations.Furthermore, the MobileCLIP models are not distilled from a single teacher but from an ensemble of ViT-Lteachers, which are stronger than the teacher of our models. At the time of conducting our experiments, wewere unable to compare our results with CLIP-KD (Yang et al., 2023a) as these models were not publiclyavailable.",
  "Accuracy": "Training on Domain-Specific Synthetic Images EfficientNet #Params (M) Teacher19115 :Domain-specific distillation is more effective for models with fewer parameters. Zero-shot classification performance of the students after domain-agnostic distillation on DataComp medium forone epoch (solid) and after subsequent domain-specific distillation on the synthetic datasets for 96 epochs(hatched). All experiments were performed using feature distillation. Our models.From our framework, we report two types of models: domain-agnostic distillation anddomain-agnostic followed by domain-specific distillation. Based on finding 3, we expect the best performancefrom the second group.For reference, we include models that were domain-specifically distilled on thecomplete real datasets as well. However, these accuracies are not zero-shot. First, we observe that vision-language distillation using the CLIP loss results in substantially worse performance both in the domain-agnostic and domain-specific case. This reflects our findings 1 and 2 and provides justification to base ourframework solely on feature distillation.We find that the resulting models outperform even the largestTinyCLIP model on three of the four datasets despite having 88% fewer trainable parameters. Moreover,they achieve comparable performance to the teacher with a margin of 5% on the same datasets. The largerperformance gap on the food dataset can likely be attributed to the more diverse and larger test set. Whencomparing to ViT-B/32 trained on DataComp-medium, which has been trained on a comparable numberof images, even the models that were distilled purely on domain-agnostic data demonstrate substantiallysuperior performance. The MobileCLIP-S0 model features a similarly large image encoder as our modelsand achieves comparable accuracies despite being trained on a much larger dataset with stronger teachers.",
  "Finding 5: Smaller Students Benefit More from Domain-Specific Distillation": "Based on the fact that small image encoders have a lower capacity, we formulate our fifth hypothesis: formodels with fewer parameters, domain-specific distillation is more effective when comparing to pure domain-agnostic distillation.To test this, we report the zero-shot performance of five additional students afterdomain-agnostic and domain-specific feature distillation in . There is a general trend of improvedperformance with increasing model size. Yet, we observe that the effectiveness of domain-specific distillationis more pronounced for smaller models compared to larger ones. The difference in performance betweenthe largest and smallest models is around 10% to 15% after domain-specific distillation, while after onlydomain-agnostic distillation it is as high as 30%. These findings support our claim that domain-specificdistillation is particularly effective for smaller students since it allows them to adapt to the target domain,without requiring real in-domain data.",
  "Conclusion": "In this work, we introduced a framework for distilling small CLIP image encoders in a zero-shot settingusing synthetic images. We identify vision-language distillation as a potentially detrimental factor for gener-alization capabilities of models between synthetic and real data, due to exploitation of spurious features andthe susceptibility to common corruptions. By employing feature distillation, we successfully mitigate theselimitations. As a result, we are able to train models that surpass the current state-of-the-art for zero-shotCLIP distillation.",
  "Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchicalimage database. In CVPR, pp. 248255. Ieee, 2009": "Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Un-terthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and NeilHoulsby. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021. Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, RyanMarten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, Eyal Orgad, Rahim Entezari, Giannis Daras,Sarah Pratt, Vivek Ramanujan, Yonatan Bitton, Kalyani Marathe, Stephen Mussmann, Richard Vencu,Mehdi Cherti, Ranjay Krishna, Pang Wei Koh, Olga Saukh, Alexander Ratner, Shuran Song, HannanehHajishirzi, Ali Farhadi, Romain Beaumont, Sewoong Oh, Alex Dimakis, Jenia Jitsev, Yair Carmon,Vaishaal Shankar, and Ludwig Schmidt.Datacomp: In search of the next generation of multimodaldatasets. In NeurIPS, 2023.",
  "Geoffrey Hinton,Oriol Vinyals,and Jeff Dean.Distilling the knowledge in a neural network.arXiv:1503.02531, 2015": "Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, and Qun Liu. Tiny-BERT: Distilling BERT for natural language understanding. In Trevor Cohn, Yulan He, and Yang Liu(eds.), EMNLP, 2020. Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for fine-grained catego-rization. In 4th International IEEE Workshop on 3D Representation and Recognition (3dRR-13), Sydney,Australia, 2013.",
  "Utkarsh Ojha, Yuheng Li, Anirudh Sundara Rajan, Yingyu Liang, and Yong Jae Lee. What knowledge getsdistilled in knowledge distillation? arXiv:2205.16004, 2023": "Maxime Oquab, Timothe Darcet, Theo Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov, PierreFernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Russell Howes, Po-Yao Huang, Hu Xu,Vasu Sharma, Shang-Wen Li, Wojciech Galuba, Mike Rabbat, Mido Assran, Nicolas Ballas, Gabriel Syn-naeve, Ishan Misra, Herve Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski.Dinov2: Learning robust visual features without supervision. arXiv:2304.07193, 2023.",
  "O. M. Parkhi, A. Vedaldi, A. Zisserman, and C. V. Jawahar. Cats and dogs. In CVPR, 2012": "Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Mller, Joe Penna,and Robin Rombach.Sdxl:Improving latent diffusion models for high-resolution image synthesis.arXiv:2307.01952, 2023. Alec Radford, Jong Wook Kim, Chris Hallacy, A. Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferablevisual models from natural language supervision. In ICML, 2021.",
  "Yonglong Tian, Lijie Fan, Phillip Isola, Huiwen Chang, and Dilip Krishnan. Stablerep: Synthetic imagesfrom text-to-image models make strong visual representation learners. In NeurIPS, 2023b": "Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bash-lykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Fer-rer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, HakanInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit SinghKoura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao,Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, JeremyReizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subra-manian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, ZhengYan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez,Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chatmodels. arXiv:2307.09288, 2023.",
  "A.1Additional Datasets": "In addition to the results presented in , we report the accuracies on three additional datasets. Oneone-hand, on the Describable Textures (Cimpoi et al., 2014) and Aircraft (Maji et al., 2013) dataset, whichcontain fine-grained, domain-specific classes, as well as ImageNet-100 (Tian et al., 2020), which is a non-domain-specific subset of ImageNet (Deng et al., 2009). The results on textures and aircrafts consolidate ourfindings from . On ImageNet-100, domain-specific feature distillation yields almost the same zero-shot accuracy as pure domain-agnostic distillation. This is different to the domain-specific datasets wherewe could observe a consistent improvement. Presumably, this can be attributed to the greater diversity ofthe real test images in ImageNet-100, which is not sufficiently captured by the synthetic training data.",
  "A.2Evaluation on the Retrieval Task": "In addition to image classification, we consider retrieval on MSCOCO (Lin et al., 2014) as a further down-stream task for CLIP models.To evaluate our students, we consider two settings.First, the standardtext-to-image and text-to-image retrieval tasks on the entire validation set. The goal of text-to-image re-trieval is to select images from a large pool that best match a given caption (the so-called query). Forimage-to-text retrieval, the setup is the other way around, i.e. querying with an image and selecting relevantcaptions. With CLIP models, text-to-image retrieval is performed by selecting the images or captions fromthe pool whose embeddings have the highest cosine similarity to the query. For this, the image encoders needstrong domain-agnostic performance, since all images have to be encoded in a meaningful way. The imagesare not restricted to a specific domain. Therefore, we refer to this setting as domain-agnostic retrieval. Assecond setting, we consider a domain-specific image-to-text retrieval task for which only images from a spe-",
  "CLIPTinyCLIPMobileCLIPDADomain-Agnostic+ Domain-Specific": ": Three additional datasets consolidate our findings. As in , the upper part summarizesthe baseline CLIP, TinyCLIP and MobileCLIP. DA denotes domain-agnostic distillation for one epoch onDataComp medium and domain-specific contains the results where either synthetic data (zero-shot) or realdata is used for domain-specific distillation of the model. The blue box highlights our final models. Graynumbers indicate that the performance is not zero-shot.",
  "Model": "TeacherDomain-agnostic student, vision-language distillationDomain-specific student (pets), vision-language distillation TinyCLIPDomain-agnostic student, feature distillationDomain-specific student (pets), feature distillation :Feature distillation improves domain-agnostic retrieval on MSCOCO. We evaluate fourof our students based on the TinyViT 11M architecture, the teacher and the TinyCLIP ViT-8M/16-3M modelon MSCOCO text-to-image and image-to-text retrieval tasks. The domain-agnostic students were distilledfor one epoch on DataComp medium using either L2 feature distillation of vision-language distillation basedon the CLIP loss. The domain-specific student are subsequently distilled on the synthetic images from thepets domain. cific domain need to be encoded. We only select query images that contain objects from the specific domainsthat the students are distilled for and perform retrieval over the entire set of captions. We denote this taskas domain-specific retrieval. In the following two sections, we discuss the performance of our students inthese two tasks and compare them to baselines.",
  "A.2.1Domain-Agnostic Retrieval": "The results for the domain-agnostic retrieval task can be seen in . We report the recall@1,5,10 and50 for both the text-to-image and text-to-image task. As baselines we include the teacher and the TinyCLIPViT-8M/16-3M model with the same number of trainable parameters as our TinyViT 11M students. Of ourstudents, we evaluate two models after only domain-agnostic distillation for one epoch on DataComp mediumusing either L2 feature distillation or vision-language distillation based on the CLIP loss. In addition, weevaluate two students that were domain- specifically distilled (after initial domain-agnostic distillation) onthe synthetic pets or food data. We observe that our domain-agnostic student distilled through featuredistillation achieves performances slightly better than the TinyCLIP model. In contrast, the performanceof the domain-agnostic student trained through vision-language distillation is substantially worse with only2% recall@1 in both retrieval tasks. Furthermore, we observe that the retrieval performance of the domain-specific students is worse than for the domain-agnostic students. This can be attributed to the fact that fordomain-agnostic retrieval, all images have to be encoded and not only images from a specific target domainthat the students were distilled for. The evaluation of the domain-specific models on retrieval tasks withintheir target domains is discussed in the next section.",
  "A.2.2Domain-Specific Retrieval": "For domain-specific retrieval, we consider image-to-text retrieval and restrict the query images to the subsetof MSCOCO that contains objects from specific domains. This task requires only the encoding of imagesfrom a restricted number of classes in comparison to domain-agnostic retrieval where all images must beencoded.In Table we report the performance for the pets and food domain.For the pets domain we",
  "distillation combined": ":Feature distillation is more robust to a change in synthetic image generator. Thereported accuracies are for TinyViT 11Ms student distilled for one epoch on DataComp medium and subse-quently distilled on synthetically generated pets test data. The difference to is that the syntheticimages were generated by Stable Diffusion 1.5 LCM LoRA instead of Stable Diffusion XL LCM LoRA. Wefind that feature distillation is more robust to the change in diffusion model and the accuracies decreasesubstantially less in comparison to vision-language distillation.",
  "A.3Details of the Synthetic Data Generation": "In this section, we provide further details on the synthetic data generation and the diversification process.As mentioned in .3, the prompts used to synthesize the images are based on the class namesand additional information given by an LLM. For each class, we ask the language model to provideinformation with respect to four contextual dimensions as well as a superclass. The contextual dimensionsare dataset specific and summarized in . shows a concrete example for a class fromthe pets dataset.For each of the contextual dimensions we collect 15 or 30 options from Llama 2 7Bfine-tuned for chats (Touvron et al., 2023). The larger number of options for the food and ImageNet-100datasets are used to accommodate its larger test set. In we summarize the sizes of the real targetdatasets. Instead of using all possible combinations of options for the contextual dimensions to generatethe prompts, we use combinatorial testing (Ahmed et al., 2017; Nie & Leung, 2011).This approach isinspired by a recent work on systematic error identification (Metzen et al., 2023). It reduces the numberof images per class while ensuring that the prompts systematically cover the diversity contained in theanswers from the LLM. For example, in case of 15 options per contextual dimension, this results in 265images per class instead of 50625.The prompts are a comma separated list of the selected options,which are weighted to accommodate for contextual dimensions that are more or less important for certaindatasets.These weighted prompts are then used as input for a diffusion model.Specifically, we useStable Diffusion XL (Podell et al., 2023) LCM LoRA (Luo et al., 2023). To ensure sufficient image qualityand diversity we employ a guidance scale of 0.5 and 6 inference steps.Further example images can befound in . These also showcase some of the known problems with diffusion models such as partsof the prompts which are missing in the image (Zhang et al., 2023) as in the first example for the food dataset.",
  "A.4Generalization From Real to Synthetic Images": "To consolidate or findings from , we investigate the domain shift in the opposite direction. That is,we assess how the models trained on real or synthetic images perform on synthetic images. For this purpose,we generate an additional synthetic dataset for pets using the same methodology as for the synthetic trainingsets. Subsequently, we evaluate the students on this dataset. The results are presented in . Thestudents distilled through vision-language distillation on real data exhibit lower test accuracies in comparisonto feature distillation. For the models trained on synthetic data the reverse is true. Using the CLIP or MPloss results in the highest accuracy on the synthetic test data. As in this discrepancy suggests thatthese models learned features of natural or synthetic images over class-specific features. As a result, theirability to generalize between natural and synthetic images is limited.",
  "A.6Training Accuracies": "In addition to the test accuracies stated in the main paper, we report the training accuracies of the domain-specific models in . We observe that the models trained with a vision-language loss achieve highertraining accuracy than the students obtained from feature distillation. This holds in particular on syntheticdata. In combination with the results from , this underlines our hypothesis that vision-languagelosses lead to learning datatype specific features over actual class or object specific features.",
  "i=1logexp(ISi , ITi /)Nk=1 exp(ISi , ITk /)(1)": "where denotes a learnable temperature parameter. We use this loss both for domain-agnostic and domain-specific distillation of a TinyViT 11M with the same setup as for the L2 loss in . The results arereported in . We observe that for domain-specific distillation, the contrastive image loss results inbetter performance in comparison to the L2 loss, while for domain-specific it is the other way around. Yet,the contrastive image loss still clearly outperforms the CLIP loss when training on domain-specific syntheticdata. This validates our observation from that using a loss purely based on the image features ofstudent and teacher improves the generalization between synthetic and real data.",
  "A.8Influence of Image Diversity": "To assess the influence of diversified images, we utilize the zero-shot prompts \"a photo of a ...\" to generateimages instead of diversified prompts from a LLM. We sample a synthetic pets dataset with the same numberof images per class as in the diversified case. Example images are shown in . The diversity of theimages decreases, especially with regard to the camera angle, as almost all images show only a frontal shotof the animals with the focus on the face. Furthermore, the variety of backgrounds decreases. We traina TinyViT 11M model on this dataset. The test accuracies on the real pets dataset are stated in . The accuracy of feature distilled student exhibits only a small decrease in performance in contrast tothe diversified images, while the performance of the models which were trained through vision-languagedistillation decreased significantly. This findings indicates that feature distillation is more robust to a lackof diversity during domain-specific distillation.",
  "s=1Pc,s.(3)": "Pc,s denotes the classification accuracy on test data corrupted with corruption c under severity level s. In ourcase Nc = 15 and Ns = 5 are the number of corruptions and corruption strengths (Hendrycks & Dietterich,2019). In addition to , shows the mean performance under corruption and the performancedrop compared to the clean performance referred to as the degradation under corruption by Hendrycks &Dietterich (2019).",
  "A.10Linear Probing": "To evaluate the linear probe accuracy of the teacher as well as the TinyViT 11M models on the pets dataset,we fit a linear classifier based on the unnormalized image features after the projection head. The classifier isfitted either using synthetic or real data, where only the case of synthetic data corresponds to the zero-shotsetting. The hyperparameter sweeps for the regularization are performed on a validation split as in theoriginal CLIP paper (Radford et al., 2021). The results are shown in . For the models trained ondomain-specific synthetic data, the performance is 8 to 10 % worse when probing with synthetic data incomparison to fitting the linear classification head with real data. This highlights that using linear probingbased on real data improves the linear classification accuracy by a substantial margin. In contrast, the truezero-shot linear classifiers, where the classification head is fitted using synthetic data, perform comparable toor worse than pure zero-shot classification using the similarity between the image and prompt embeddings.In contrast to our framework, previous works (Tian et al., 2023b;a; Hammoud et al., 2024) mainly focuson the linear accuracy where the classification head is fitted with real data instead of targeting the truezero-shot setting without any real data which is the more difficult task to accomplish.",
  "A.11Linear Classification Head Instead Of CLIP Architecture": "Instead of using the CLIP architecture, we train and distill TinyViT 11M model with a linear classificationhead on the pets dataset for comparison. We either train from scratch or initialized weights from domain-agnostic training on ImageNet-22k (with the exception of the linear classification head which is alwaysrandomly initialized). As most of the classes from the pets dataset are contained in ImageNet-22k, thelatter does not correspond to a strict zero-shot setting even when subsequently performing domain-specificdistillation on synthetic data. To train the models, we optimize the standard cross-entropy loss as well asa sum of cross-entropy loss and the original knowledge distillation loss of Hinton et al. (2015). We use theAdamW optimizer (Loshchilov & Hutter, 2019) with no weight decay and the learning rate is set to 5104 which is the same as used by Wu et al. (2022) for fine-tuning. First, we observe that the drop in performancewhen training with synthetic data in comparison to real data is similar to the CLIP models based on vision-",
  "Real93.498.888.990.6Synthetic86.667.853.158.4": ":Training standard classification models does not yield good zero-shot classifiers.We fine-tune a ViT-B/32 classification model on either the real or synthetic domain-specific datasets andevaluate the classification accuracies on the real test sets. Training on synthetic images corresponds to thezero-shot setting. We observe a substantial drop in performance between synthetic and real datasets. Inparticular, the zero-shot performance achieved by training the standard classification model on syntheticimages is worse than the ViT-B/32 CLIP teacher and our CLIP students. language losses. Additionally, the performance of the model with classification head is worse compared tothe CLIP models when trained from scratch. In contrast, the classifiers pre-trained on ImageNet-22k andsubsequently trained on the domain-specific real training data achieve the best performance overall. This canpresumably be attributed to the fact that most of the classes are already included in ImageNet-22k whichwas used for domain-agnostic distillation. Using knowledge distillation for the models with classificationhead only has a minor effect.",
  "A.12Standard Classification Models as Teachers in the Zero-Sot Setting": "An alternative to feature distillation of CLIP models in the zero-shot setting would be the direct distillationof standard classification models with backbone and classification head. Performing feature distillation inthis setting requires a teacher with the standard classification architecture. However, since the classificationhead has to be trained specifically for every set of target classes, there are no general-purpose zero-shotmodels with this architecture. The only way to use teachers with standard classification architecture in azero-shot setting is to train them with synthetic images. For this purpose, we consider a ViT-B/32 modelthat has been pre-trained on ImageNet 21k (Steiner et al., 2022) and train it as a possible zero-shot teacher.In , we compare the domain-specific training of these models using synthetic and real images. Thezero-shot teacher trained with standard classification architecture on synthetic images performs worse thanthe CLIP teacher and our CLIP students. The gap in accuracy between training on the domain-specificsynthetic or real images is up to 35%. This justifies why distilling CLIP models is clearly the better settingfor zero-shot classification.",
  "A.13Using a Different Teacher": "To ablate the role of the teacher model, we distill students from a CLIP ViT B/16 teacher trained on LAION2B instead of the CLIP ViT B/32 trained on DataComp XL which we used previously. All other factorsare left untouched to enable a direct comparison. We perform one epoch of domain-agnostic distillation onDataComp medium followed by domain-specific distillation on four synthetic datasets (pets, flowers, cars,food). The results are shown in . We find that main finding also holds with a different teacher.Domain-specific distillation through feature distillation results in zero-shot accuracies that closely matchthe performance of the teacher. In contrast, vision-language distillation using the CLIP loss result in worseaccuracies with a difference of up to 79% compared to the teacher.",
  "CLIPDADA+ DS": ": Using a different teacher consolidates our findings. The setup is the same as for butwith a CLIP ViT B/16 teacher trained on LAION 2B instead of the CLIP ViT B/32 trained on DataCompXL. DA denotes domain-agnostic distillation for one epoch on DataComp medium and DA+DS containsthe results where synthetic data is used for subsequent domain-specific distillation of the students. The bluebox highlights the final models distilled through feature distillation.",
  "A.14Using a Different Synthetic Data Generator": "To ablate on the role of the synthetic data generator, we perform domain-specific distillation on datasetsgenerated by Stable Diffusion 1.5 LCM LoRA Luo et al. (2023) instead of Stable Diffusion XL LCM LoRA.We keep the setting from .4 including the diversified prompt generation, the number of inferencesteps for the data generation as well as the hyperparameters for distilling the students. We select a TinyViT11M student and perform the domain-specific distillation after domain-agnostic distillation for one epochon DataComp medium. The results stated in confirm our main finding that feature distillationgreatly improves the zero-shot classification accuracy over vision-language distillation. In particular, featuredistillation is more robust to a change in the synthetic data generator. The drop in performance betweenStable Diffusion 1.5 and Stable Diffusion XL LCM lies within 5% for feature distillation. For vision-languagedistillation the performance deteriorates by up to 23% using the CLIP loss and up to 58% using the MPloss.",
  "A.15Multi-Positive Contrastive Loss": "To adapt the multi-positive (MP) loss to our setting, we replace the anchor sample by the embedding of aclass-specific zero-shot prompt. By Zk denote the normalized embedding of the zero-shot prompt for classk and by Ii the normalized embedding of image i. The image label is given by l(Ii). Given class k from aset of K classes and batchsize N, the contrastive distribution is given by",
  "A.16Theoretical Bound on Teacher-Student Agreement": "Conclusively, we present a theoretical motivation of the L2 feature loss for the distillation of a CLIP imageencoder. Therefore, we consider the following notions. Let be Dtrain = {(ii, ti)|i [Ntrain] be a train setconsisting of image-caption pairs and Dtest = {(ii, li)|i [Ntest]} a test set with image-classlabel pairs. ByIT (i) denote the normalized embedding of image i from the teacher image encoder and by IS(i) the normalizedembedding of image i from the student image encoder. Similarly, let Zk be normalized embedding of thezero-shot prompt \"a photo of a {classname}\" of class k [Nclasses]. In this setting, the following statementholds.",
  "which proves the statement": "Lemma 1 highlights that minimizing the L2 loss on a training set where the image embeddings are close tothe embeddings of the test images yields a sufficient criterion for agreement of the teacher and student onthe test set. This motivates directly optimizing the L2 loss through feature distillation over vision-languagedistillation."
}