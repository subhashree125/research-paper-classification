{
  "Abstract": "Adversarial training has proven to be one of the most effective methods to defend againstadversarial attacks. Nevertheless, robust overfitting is a common obstacle in adversarialtraining of deep networks. There is a common belief that the features learned by differentnetwork layers have different properties, however, existing works generally investigate robustoverfitting by considering a DNN as a single unit and hence the impact of different networklayers on robust overfitting remains unclear. In this work, we divide a DNN into a series oflayers and investigate the effect of different network layers on robust overfitting. We findthat different layers exhibit distinct properties towards robust overfitting, and in particular,robust overfitting is mostly related to the optimization of latter parts of the network. Basedupon the observed effect, we propose a robust adversarial training (RAT) prototype: ina minibatch, we optimize the front parts of the network as usual, and adopt additionalmeasures to regularize the optimization of the latter parts. Based on the prototype, wedesigned two realizations of RAT, and extensive experiments demonstrate that RAT caneliminate robust overfitting and boost adversarial robustness over the standard adversarialtraining.",
  "Introduction": "Deep neural networks (DNNs) have been widely applied in multiple fields, such as computer vision (Heet al., 2016) and natural language processing (Devlin et al., 2018).Despite its achieved success, recentstudies show that DNNs are vulnerable to adversarial examples.Well-constructed perturbations on theinput images that are imperceptible to humans eyes can make DNNs lead to a completely different prediction",
  "Published in Transactions on Machine Learning Research (10/2024)": "the latter parts of the network, which effectively mitigate robust overfitting of the network as a whole. Wethen further demonstrate two implementations of RAT: one locally uses a fixed learning rate for the latterlayers and the other utilize adversarial weight perturbation for the latter layers. Extensive experiments showthe effectiveness of both approaches, suggesting RAT is generic and can be applied across different networkarchitectures, threat models and benchmark datasets to mitigate robust overfitting.",
  "imaxd(xi,xi) (fw(xi), yi),(1)": "where fw is the DNN classifier with weight w, and () is the loss function. d(., .) specify the distance betweenoriginal input data xi and adversarial data xi, which is usually an lp-norm ball such as the l2 and l-normballs and is the maximum perturbation allowed. A different type of AT variation that is commonly used is referred to as TRADES (Zhang et al., 2019),which involves optimizing a surrogate loss that is a tradeoff between the natural accuracy and adversarialrobustness:",
  "CE(fw(xi), yi)+ maxd(xi,xi) KL(fw(xi)||fw(xi)),(2)": "The surrogate loss consists of two parts: cross-entropy (CE) loss, which encourages the network to maxi-mize natural accuracy, and Kullback-Leibler (KL) divergence, which encourages the improvement of robustaccuracy. The hyperparameter is used to control the tradeoff between natural accuracy and adversarialrobustness. Another line of work involves utilizing semi-supervised learning (SSL) technique. Methods based on SSL(Carmon et al., 2019; Zhai et al., 2019; Najafi et al., 2019; Uesato et al., 2019) use additional unlabeled datato improve the robustness of the trained model. In these methods, a natural model is first trained on labeleddata to generate pseudo-labels for the unlabeled data. Then, a robust model is trained using an adversarialloss function (w) on both labeled and unlabeled data:",
  "Robust generalization": "An interesting characteristic of deep neutral networks (DNNs) is their ability to generalize well in practice(Belkin et al., 2019). For the standard training setting, it is observed that test loss continues to decreasefor long periods of training (Nakkiran et al., 2020), thus the common practice is to train DNNs for as longas possible. However, in the case of adversarial training, further training past a certain point leads to asignificant decrease in the robust training loss of the classifier, while increasing the robust test loss. depicts this phenomenon for adversarial training on CIFAR-10, where the robust test accuracy initiallyincreases but then drops after the first learning rate decay. This phenomenon is called robust overfitting,which has shown strong resistance to standard regularization techniques such as l1, l2 regularization anddata augmentation methods, and can be observed on various datasets, including SVHN, CIFAR-100, andImageNet (Rice et al., 2020). Schmidt et al. (2018) theorizes that robust generalization have a large sample complexity, which requiressubstantially larger dataset. Many subsequent works have empirically validated such claim, such as ATwith semi-supervised learning (Carmon et al., 2019; Zhai et al., 2019; Najafi et al., 2019; Uesato et al.,2019), robust local feature (Song et al., 2020) and data interpolation (Lee et al., 2020; Chen et al., 2021).Chen et al. (2020) proposes to combine smoothing the logits via self-training and smoothing the weight viastochastic weight averaging to mitigate robust overfitting. Wu et al. (2020) emphasizes the connection ofweight loss landscape and robust generalization gap, and suggests injecting the adversarial perturbations",
  "Epoch": "Train standardTrain robust Test standardTest robust : The learning curves of adversarial training using PreAct ResNet-18 on the CIFAR-10 dataset. Thedepicted curves reveal robust overfitting, wherein the adversarially trained model briefly achieves 52.01%test robust accuracy shortly after the first learning rate decay. Surprisingly, at this point, the adversariallytrained model is actually more robust than it is at the end of training, where it only attains a 43.95% testrobust accuracy against a 20-step PGD adversary with an radius of = 8/255. The learning rate isdecayed at 100 and 150 epochs. into both inputs and weights during AT to regularize the flatness of weight loss landscape. The intriguingproperty of robust overfitting has motivated great amount of study and investigation (Wang et al., 2019;Zhang et al., 2020; Liu et al., 2021; Rebuffi et al., 2021; Dong et al., 2022b; Yu et al., 2022; Dong et al., 2022a;Li & Spratling, 2023; Wang et al., 2023), but current works typically approach the phenomenon consideringa DNN as a whole. In contrast, our work treats a DNN as a series of layers and reveals a strong connectionbetween robust overfitting and the optimization of the latter layers, providing a novel perspective into betterunderstanding the phenomenon.",
  "Intriguing Properties of Robust Overfitting": "In this section, we first investigate the layer-wise properties of robust overfitting by fixing model parametersin AT (.1). Based on our observations, we further propose a robust adversarial training (RAT)prototype to mitigate robust overfitting (.2). Finally, we design two different realizations for RATto verify the effectiveness of the proposed method (.3).",
  "Layer-wise Analysis of Robust Overfitting": "Current works usually study the robust overfitting phenomenon considering the network as a single unit(Rice et al., 2020; Wu et al., 2020; Yu et al., 2022; Dong et al., 2022a; Li & Spratling, 2023; Wang et al.,2023). However, features computed by different layers exhibit different properties, such as first-layer featuresare general and last-layer features are specific (Yosinski et al., 2014). We hypothesize that different networklayers have different effects on robust overfitting. To empirically verify the above hypothesis, we deliberatelyfix the parameters of the selected network layers, leaving them unoptimized during AT and observe thebehavior of robust overfitting accordingly. Specifically, we considered PreAct ResNet-18 architecture as acomposition of 4 main layers, corresponding to 4 Residual blocks. We then train multiple PreAct ResNet-18networks on CIFAR-10 for 200 epochs using AT, each time selecting a set of network layers to have theirparameter fixed. The robust test performance in (a) shows a consistent pattern.Robust overfitting is mitigatedwhenever we fix the parameters for layer 4 during AT, while any settings that do not fix the parametersfor layer 4 result in a more severe gap between the best accuracy and the accuracy at the last epoch. Forexample, for settings such as AT-fix-param-, AT-fix-param-, AT-fix-param- and AT-fix-param-, robust overfitting is significantly reduced. On the other hand, for settings such AT-fix-param-,",
  ": The robust train/test performance of adversarial training with different sets of network layersfixed. AT-fix-param corresponds to fixing the parameters of layers 1 & 2 during AT": "AT-fix-param- and AT-fix-param-, when we fix the parameters of various set of layers but allowfor the optimization of layer 4, robust overfitting still widely exists. For extreme case like AT-fix-param-, where we fix the first three front layers and only allow for the optimization of that last layer 4,the gap between the best accuracy and the last accuracy is still obvious. This clearly indicates that theoptimization of the latter layers present a strong correlation to the robust overfitting phenomenon. Notethat this relationship can be observed across a variety of datasets, network architectures, AT methods, andthreat models (shown in Appendix A), indicating that it is a general property in adversarial training. In many of these settings, robust overfitting is mitigated at the cost of robust test accuracy. For example inAT-fix-param-, if we leave both layer 3 & 4 unoptimized, robust overfitting will practically disappear,but the peak performance is much worse compared to standard AT. When carefully examining the trainingperformance in these settings shown in (b), we generally observe that the network capacity to fitadversarial data is strong when we fix the parameters for the front layers, but it gradually gets weakeras we try to fix the latter layers.For instance, AT-fix-param- has the highest train robust accuracy,then comes AT-fix-param, AT-fix-param and AT-fix-param; AT-fix-param has higher trainingaccuracy than AT-fix-param.This suggests fixing the latter layers parameters can regularize thenetwork stronger compared to fixing the front layerss parameters. This stronger regularization may be dueto the latter layers containing more parameters than the front layers, or because fixing the parameters forthe latter layers leads to more severe underfitting. However, these factors do not adequately explain thelayer-wise property of robust overfitting. For example, the front layers also contain a certain number ofparameters. Yet, when we fix the parameters of the front layers, the degree of robust overfitting remainsalmost unchanged, as shown in (a). On the other hand, we observe that fixing parameters in frontlayers also leads to some degree of underfitting, as shown in (b). However, these measures havealmost no effect on the robust overfitting phenomenon. Nevertheless, in the subsequent sections, we willintroduce methods that specifically regularize the optimization of the latter layers, so as to mitigate robustoverfitting without tradeoffs in robustness. We will compare the impact on robust overfitting when applied",
  "A Prototype of RAT": "As witnessed in .1, the optimization of AT in the latter layers is highly correlated to the existence ofrobust overfitting. To address this, we propose to train the network on adversarial data with some restrictionsput onto the optimization of the latter layers, dubbed as Robust Adversarial Training (RAT). RAT adoptsadditional measures to regularize the optimization of the latter layers and mitigate robust overfitting. The RAT prototype is given in Algorithm 1. It runs as follows. We start with a base adversarial trainingalgorithm A. In Line 1-3, The inner maximization pass aims to generate adversarial examples via maximizingthe loss, and then the outer minimization pass updates the weight to minimize the loss on adversarial data.Line 4 initiates a loop through all parts of the weight w from the front layers to the latter layers. Line 5-9then manipulate different parts of the weight based on its layer conditions. If the parts of the weight belongto the front layers (Cfront), their gradients will be kept intact. Otherwise, a weight update scheme S is putonto the parts of the weight corresponding to the latter layers (Clatter). Finally, the optimizer O updatesthe model fw in Line 11. Note that RAT is a general prototype where layer conditions Cfront, Clatter and weight adjustment strategyS can be versatile. Based on the setting in .1, the ResNet architecture is treated as a compositionof 4 main layers, corresponding to 4 residual blocks. In our subsequent experiments, except for the case ofincluding all layers, when we count from the first layer backward, we regard them as front layers. When wecount from the fourth layer forward, we regard them as latter layers. For example, layer 1 & 2 is Cfront andlayer 3 & 4 is Clatter. S can also represent various strategies that serves to regularize the optimization ofthe latter layers. In the section below, we will propose two different strategies S in the implementations ofRAT to demonstrate RATs effectiveness. Algorithm 1 RAT-prototype (in a mini-batch).Require:base adversarial training algorithm A, optimizer O, network fw, model parameter w ={w1, w2, ..., wn}, training data D = {(xi, yi)}, mini-batch B, front and latter layer conditions Cfront andClatter for fw, gradient adjustment strategy S.",
  "Two Realizations of RAT": "In this section, we will propose two different methods to put certain restrictions on the optimization of selectedparts of the network, and then investigate the robust overfitting behavior upon applying such method to thefront layers vs the latter layers. These methods showcase a clear relation between the optimization of thelatter layers and robust generalization gap. RAT through enlarging learning rate. In standard AT, the sudden increases in robust test performanceappears to be closely related to the drops in the scheduled learning rate decay. We hypothesize that trainingAT without learning rate decays is sub-optimal, which can regularize the learning process of adversarialtraining. Comparison of the train/test performance between standard AT and AT without learning rate",
  "where is the amplification coefficient": "To demonstrate the effectiveness of RATLR, we train multiple PreAct ResNet-18 networks on CIFAR-10for 200 epochs using AT, each time selecting a set of network layers to have their learning rate fixed to 0.1while maintaining the piece-wise learning rate schedule for other layers. (a) validate our proposition.Robust overfitting is relieved for all settings that target layers that include layer 4 (AT-fix-lr-, AT-fix-lr-, AT-fix-lr-, etc.) while any settings that fix the learning rate of layers that exclude layer 4 do notreduce robust overfitting. Furthermore, all settings that fix the learning rate for both layer 3 & 4, includingAT-fix-lr-, AT-fix-lr-, AT-fix-lr- & AT-fix-lr- completely eliminate robust overfitting.The observations indicate that regularizing the optimization of the latter layers by optimizing those layerswithout learning rate decays can prevent robust overfitting from occurring. An important observation is thatRATLR (AT-fix-lr-) can both overcome robust overfitting and achieve better robust test performancecompared to the network using a fixed learning rate for all layers (AT-fix-lr-). Examining the trainingperformance between these two settings in (c), we find that RATLR exhibits a rapid rise in bothrobust and standard training performance immediately after the first learning rate decay similar to standardAT. The training performance of RATLR is able to benefit from the learning rate decay occurring at layer1 & 2, making a notable improvement compared to AT-fix-lr-. By training layers 3 & 4 withoutlearning rate decays, we specifically put some restrictions on the optimization of only the latter parts of thenetwork heavily responsible for robust overfitting, which can relieve robust overfitting without sacrificing toomuch performance. The experiment results provide another indication that the latter layers have strongerconnections to robust overfitting than the front layers do, and regularizing the optimization of the latterlayers from the perspective of learning rate can effectively mitigate robust overfitting. RAT through adversarial weight pertubation. We continue to study the impact of different networklayers to robust overfitting phenomenon from the perspective of adversarial weight perturbation (AWP). Wuet al. (2020) proposes AWP as a method to explicitly flatten weight loss landscape, by introducing adversarialperturbations into both inputs and weights during AT:",
  "where is the constraint on weight perturbation size": "As AWP keeps injecting the worst-case perturbations on weight during training, it could also be viewedas a means to regularize the optimization of AT. In fact, the training of AWP exhibits a negative robustgeneralization gap, where robust training accuracy is in short of robust testing accuracy by a large margin,shown in (c). This indicates AWP put significant restrictions to the optimization of AT, introducinghuge trade-offs to training performance. As our previous analysis suggests a strong correlation betweenrobust overfitting and the optimization of the latter layers, we argue that the capacity to mitigate robustoverfitting from AWP is mostly thanks to the perturbations occurring at latter layers weight. As such, wepropose to specifically apply AWP to the latter half of the network, and refer to this method as RATWP.In essence, RATWP compute the adversarial weight perturbation vj under the layer condition Clatter(wj), sothat only the parts of the weight along the latter half of the network are perturbed:",
  "||vj|| = ||wj||.(10)": "To prove the effectiveness of RATWP, we train multiple PreAct ResNet-18 networks on CIFAR-10 for 200epochs using AT, each time selecting a set of network layers to have their weight locally perturbed usingAWP. As seen from (a), there are only 3 settings that can overcome robust overfitting, namelyAT-awp-, AT-awp- and AT-awp-. These settings share one key similarity: both layer 3 & 4have their weight adversarially perturbed during AT. Simply applying AWP to any set of layers that excludelayers 3 & 4 is not sufficient to mitigate robust overfitting. This shows that AWP is effective in solving robust",
  ": The train/test performance of adversarial training when applying AWP for different sets of networklayers. AT-awp- means only layer 1 & 2 have their weight perturbed using AWP": "overfitting only when applied to both layer 3 and layer 4. Even when AWP is applied to the first 3 formerlayers out of 4 layers (AT-awp-), robust overfitting still widely exists. In another word, it is essentialfor the adversarial weight perturbations to occur at the latter part of the network in order to mitigate robustoverfitting. To examine this phenomenon in detail, we compare the training performance of AWP appliedto front layers (represented by AT-awp-) vs AWP applied to latter layers (represented by AT-awp-), shown in (b). AWP applied in the front layers have a much better training performancethan AWP applied in the latter layers. Furthermore, AWP applied to front layers reveals a positive robustgeneralization gap (training accuracy > testing accuracy) shortly after the first drop in learning rate, whichcontinues to widen with further training. Conversely, AWP applied in the latter layers exhibits a negativerobust generalization gap throughout most of the training, only converging to 0 after the second drop inlearning rate. These differences demonstrate that worst-case perturbations, when injected into the latterlayers weights, have a more powerful impact in regularizing the optimization of AT. Consistent with ourprevious findings, AWP applied to the latter layers can be considered as an approach to regularize theoptimization of AT in those layers, which successfully mitigates robust overfitting. This finding supportsour analysis thus far, further demonstrating that regularizing the optimization of the latter layers is key toimproving the robust generalization.",
  "L2AT44.0541.222.8341.3939.342.05RATLR44.4340.424.0141.4739.422.05RATWP46.1244.641.4841.9440.381.56": "CIFAR-10 (Krizhevsky et al., 2009).The CIFAR-10 dataset (Canadian Institute for AdvancedResearch, 10 classes) is a subset of the Tiny Images dataset and consists of 60000 32x32 colorimages. The images are labelled with one of 10 mutually exclusive classes: airplane, automobile(but not truck or pickup truck), bird, cat, deer, dog, frog, horse, ship, and truck (but not pickuptruck). There are 6000 images per class with 5000 training and 1000 testing images per class. CIFAR-100 (Krizhevsky et al., 2009). The CIFAR-100 dataset (Canadian Institute for AdvancedResearch, 100 classes) is a subset of the Tiny Images dataset and consists of 60000 32x32 colorimages. The 100 classes in the CIFAR-100 are grouped into 20 superclasses. There are 600 imagesper class. Each image comes with a fine label (the class to which it belongs) and a coarse label(the superclass to which it belongs). There are 500 training images and 100 testing images per class. SVHN (Netzer et al., 2011). Street View House Numbers (SVHN) is a digit classification benchmarkdataset that contains 600,000 3232 RGB images of printed digits (from 0 to 9) cropped from picturesof house number plates. The cropped images are centered in the digit of interest, but nearby digitsand other distractors are kept in the image. SVHN has three sets: training, testing sets and an extraset with 530,000 images that are less difficult and can be used for helping with the training process.",
  "TRADESPreAct ResNet-1882.7782.94-0.1752.6749.653.0249.2846.802.48TRADES-AWP82.0582.57-0.5255.4454.810.6351.4250.730.69TRADES-RATWP82.6183.08-0.4755.3354.850.4851.3451.040.30": "We use PreAct ResNet-18 (He et al., 2016) and Wide ResNet-34-10 (Zagoruyko & Komodakis, 2016) followingthe same hyperparameter settings for AT in Rice et al. (2020): for L threat model, = 8/255, step sizeis 1/255 for SVHN, and 2/255 for CIFAR-10 and CIFAR-100; for L2 threat model, = 128/255, step sizeis 15/255 for all datasets. For training, all models are trained under 10-step PGD (PGD-10) attack for 200epochs using SGD with momentum 0.9, weight decay 5 104, and a piecewise learning rate schedule withan initial learning rate of 0.1. Standard data augmentation techniques, including random cropping with 4pixels of padding and random horizontal flips, are applied. The models are decomposed into a series of 4main layers, corresponding to 4 residual blocks of the ResNet architecture. For RATLR, learning rate forlayer 3 & 4 are set to a fixed value of 0.1. For RATWP, weight perturbation is applied in layer 3 & 4,and other hyperparameters are configured as per the original paper. For testing, the robustness accuracyis evaluated under two different adversarial attacks, including 20-step PGD (PGD-20) and Auto Attack(AA) (Croce & Hein, 2020b). Auto Attack is considered the most reliable robustness evaluation to date,which is an ensemble of complementary attacks, consisting of three white-box attacks (APGD-CE (Croce &Hein, 2020b), APGD-DLR (Croce & Hein, 2020b), and FAB (Croce & Hein, 2020a)) and a black-box attack(Square Attack (Andriushchenko et al., 2020))",
  "BestLastDiffBestLastDiffBestLastDiff": "TRADES-RATWP82.7782.94-0.1752.6749.653.0249.2846.802.48TRADES-RATWP82.9883.11-0.1352.3549.762.5948.4846.661.82TRADES-RATWP82.6183.08-0.4755.3354.850.4851.3451.040.30TRADES-RATWP81.8982.37-0.4855.4454.910.5351.2451.200.04TRADES-RATWP82.0582.57-0.5255.4454.810.6351.4250.730.69 CIFAR-10 Results. The evaluation results on CIFAR10 dataset are summarized in , where Best isthe highest test robustness achieved during training; Last is the test robustness at the last epoch checkpoint;Diff denotes the robust accuracy gap between the Best & Last. It is observed that RATWP generallyachieves the best robust performance compared to RATLR & standard AT. Regardless, both RATLR andRATWP tighten the robustness gaps by a significant margin, indicating they can effectively suppress robustoverfitting. CIFAR-100 Results. We also show the results on CIFAR100 dataset in . We observe similarperformance like CIFAR10, where both RATLR and RATWP is able to significantly reduce the robustnessgaps. For robustness improvement, RATWP stands out to be the leading method. The results further verifythe effectiveness of the proposed approach. SVHN Results. The results on the SVHN dataset are shown in , where robustness gap are alsonarrowed down to a small margin by RATWP. SVHN dataset is a special case where RATLR strategy doesnot improve robust overfitting. Unlike CIFAR10 and CIFAR100, learning rate decay in SVHNs trainingdoes not have much connection to the sudden increases in robust test performance or the prevalence of robustoverfitting, and hence makes RATLR ineffective. Other than this, The improvement in robust generalizationgaps can be witnessed in all cases, demonstrating the proposed approachs are generic and can be appliedwidely. Comparision with AWP. We further provide a comparison between RATWP and AWP (Wu et al., 2020)on the CIFAR-10 dataset under the L threat model, using different adversarial training methods andnetwork architectures. The results are summarized in . For natural accuracy, RATWP enforces lessregularization compared to AWP, thus achieving higher natural accuracy. For adversarial robustness, it isobserved that RATWP slightly degrades the models performance in some cases. However, RATWP generallymaintains comparable adversarial robustness to AWP, which can be attributed to the strong correlationbetween the latter layers and robust overfitting. Ablation Study.Finally, we provide an ablation study to illustrate the selection of layers in our ex-periments.Specifically, we apply regularizations to different layers in the RATWP method and conductexperiments with TRADES on the CIFAR-10 dataset using PreAct ResNet-18 under the L threat model.The experimental results are summarized in . It is observed that when regularizations are appliedonly to layer 4, the model can mitigate robust overfitting to a certain extent, but its robustness performanceis poor. When regularizations are applied to layer 3&4, the model achieves better adversarial robustnesswhile effectively mitigating robust overfitting. Therefore, in our experiments, we consistently chose to applyregularizations to layer 3&4 in consideration of both robust overfitting and adversarial robustness.",
  "Conclusion": "In this paper, we investigate the effects of different network layers on robust overfitting and identify thatrobust overfitting is mainly driven by the optimization occurred at the latter layers. Following this, wepropose a robust adversarial training (RAT) prototype to specifically hinder the optimization of the latterlayers in the process of training adversarial network. The approach prevents the model from overfitting",
  "Tongliang Liu is partially supported by the following Australian Research Council projects: FT220100318,DP220102121, LP220100527, LP220200949, and IC190100031": "Maksym Andriushchenko, Francesco Croce, Nicolas Flammarion, and Matthias Hein.Square attack: aquery-efficient black-box adversarial attack via random search.In European Conference on ComputerVision, pp. 484501. Springer, 2020. Anish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients give a false sense of security:Circumventing defenses to adversarial examples. In International conference on machine learning, pp.274283. PMLR, 2018. Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling modern machine-learning practiceand the classical biasvariance trade-off. Proceedings of the National Academy of Sciences, 116(32):1584915854, 2019.",
  "Lin Li and Michael Spratling. Data augmentation alone can improve adversarial training. arXiv preprintarXiv:2301.09879, 2023": "Fangzhou Liao, Ming Liang, Yinpeng Dong, Tianyu Pang, Xiaolin Hu, and Jun Zhu.Defense againstadversarial attacks using high-level representation guided denoiser. In Proceedings of the IEEE Conferenceon Computer Vision and Pattern Recognition, pp. 17781787, 2018. Feng Liu, Bo Han, Tongliang Liu, Chen Gong, Gang Niu, Mingyuan Zhou, Masashi Sugiyama, et al. Proba-bilistic margins for instance reweighting in adversarial training. Advances in Neural Information ProcessingSystems, 34:2325823269, 2021.",
  "Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towardsdeep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083, 2017": "Amir Najafi, Shin-ichi Maeda, Masanori Koyama, and Takeru Miyato. Robustness to adversarial perturba-tions in learning from incomplete data. Advances in Neural Information Processing Systems, 32, 2019. Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya Sutskever.Deepdouble descent: Where bigger models and more data hurt.In International Conference on LearningRepresentations, 2020. URL",
  "Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading digits innatural images with unsupervised feature learning. 2011": "Nicolas Papernot, Patrick McDaniel, Xi Wu, Somesh Jha, and Ananthram Swami. Distillation as a defenseto adversarial perturbations against deep neural networks. In 2016 IEEE symposium on security andprivacy (SP), pp. 582597. IEEE, 2016. Sylvestre-Alvise Rebuffi, Sven Gowal, Dan Andrei Calian, Florian Stimberg, Olivia Wiles, and Timothy AMann. Data augmentation can improve robustness. Advances in Neural Information Processing Systems,34:2993529948, 2021.",
  "Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, andRob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013": "Florian Tramr, Alexey Kurakin, Nicolas Papernot, Ian Goodfellow, Dan Boneh, and Patrick McDaniel.Ensemble adversarial training: Attacks and defenses. In International Conference on Learning Represen-tations, 2018. Jonathan Uesato, Jean-Baptiste Alayrac, Po-Sen Huang, Robert Stanforth, Alhussein Fawzi, and PushmeetKohli. Are labels required for improving adversarial robustness? arXiv preprint arXiv:1905.13725, 2019. Yifei Wang, Liangchen Li, Jiansheng Yang, Zhouchen Lin, and Yisen Wang. Balance, imbalance, and rebal-ance: Understanding robust overfitting from a minimax game perspective. In Thirty-seventh Conferenceon Neural Information Processing Systems, 2023.",
  "AMore Evidences on the Layer-wise Properties of Robust Overfitting": "In this section, we first show that robust overfitting is a phenomenon uniquely associated with adversarialtraining and does not occur in natural training. We then provide more empirical experiments to showcasethe layer-wise properties of robust overfitting across different datasets, network architectures, threat modelsand adversarial training methods. Specifically, we use the proposed strategies mentioned in .3 toput restriction on the optimization of different network layers. We can always observe that there is significantrobust overfitting relief when we regularize the optimization of the latter layers, while robust overfitting isprevalent for other settings. These evidences further highlight the strong relation between robust overfittingand the optimization of the latter layers.",
  "A.1Robust overfitting: A phenomenon unique to adversarial training": "In , we have shown the phenomenon of robust overfitting within adversarial training. In this part, weshow that robust overfitting is uniquely associated with adversarial training and does not occur in naturaltraining. Specifically, we train a PreAct ResNet-18 using natural training on the CIFAR-10 dataset andevaluate the models performance under different perturbation budgets, as shown in .We canobserve that across various perturbation budgets, the models test accuracy does not show a decrease similarto that in robust overfitting, indicating that robust overfitting phenomenon does not occur in natural training.",
  "A.2Evidences across datasets": "We show that the layer-wise properties of robust overfitting are universal across datasets on CIFAR-100and SVHN. We adversarially train PreAct ResNet-18 using AT under l threat model on different datasetswith the same settings as .3. The results are shown in and 7. Note that for SVHN,regularization strategy utilizing a fixed learning rate (RATLR) does not improve robust overfitting (). Unlike CIFAR10 and CIFAR100, SVHNs robust overfitting appears before the first learning rate decay.Also, learning rate decay in SVHNs training does not have any relation to the sudden increases in robust testperformance or the appearance of robust overfitting. Hence, SVHN dataset is a special case where RATLRdoes not apply. For all other cases, robust overfitting is effectively mitigated by regularizing the optimizationof the latter layers.",
  "A.3Evidences across threat models": "We further demonstrate that the generality of layer-wise properties of robust overfitting by conductingexperiments under l2 threat model. The settings are the same as .3. The results are shown in and 9. Under l2 threat model, except for SVHN dataset where regularization strategy utilizing afixed learning rate (RATLR) does not apply, robust overfitting is effectively mitigated by regularizing theoptimization of the latter layers.",
  "A.4Evidences across network architectures": "In this part, we show that the generality of layer-wise properties of robust overfitting by conducting exper-iments across different network architectures (PreAct ResNet-18 (He et al., 2016), PreAct ResNet-34 (Heet al., 2016), VGG-16 (Simonyan & Zisserman, 2014), DPN-26 (Chen et al., 2017) and DLA (Yu et al.,2018)). For PreAct ResNet-18 and PreAct ResNet-34, the division of the network layer are the same as.3. For DPN-26 and DLA, similarly, we consider the four main network blocks separately for layer1,layer2, layer3 and layer4. For VGG-16, features.0, features.1, features.3, features.4, features. 7,features.8 are regarded as layer1, features.10, features.11, features.14, features.15, features .17,features.18 are regarded as layer2, features.20, features.21, features.24, features.25 , features.27,features.28, features.30 are regarded as layer3, and features.31, features.34 , features.35, fea-tures.37, features.38, features.40, features.41 are regarded as layer4. The results are shown in . These results clearly indicate that the optimization of the latter layers present a strong correlation to therobust overfitting phenomenon, and the layer-wise properties of robust overfitting are common in differentnetwork architectures.",
  "A.5Evidences across adversarial training methods": "We further demonstrate that the generality of layer-wise properties of robust overfitting by conductingexperiments using different adversarial training methods (Standard AT (Madry et al., 2017) and TRADES(Zhang et al., 2019)). The settings are the same as .3. The results are shown in . Theexperiments results indicate that the latter layers have stronger connections to robust overfitting than the",
  "Test Robust Accuracy": "0.3 0.325 0.35 0.375 0.4 0.425 0.45 0.475 0.5 0.525 0.55 0.575 TRADES_awp_standardTRADES_awp_TRADES_awp_TRADES_awp_TRADES_awp_ 0.3 0.325 0.35 0.375 0.4 0.425 0.45 0.475 0.5 0.525 0.55 0.575 TRADES_awp_standardTRADES_awp_TRADES_awp_TRADES_awp_ 0.3 0.325 0.35 0.375 0.4 0.425 0.45 0.475 0.5 0.525 0.55 0.575 TRADES_awp_standardTRADES_awp_TRADES_awp_TRADES_awp_ 0.3 0.325 0.35 0.375 0.4 0.425 0.45 0.475 0.5 0.525 0.55 0.575 TRADES_awp_standardTRADES_awp_TRADES_awp_TRADES_awp_TRADES_awp_"
}