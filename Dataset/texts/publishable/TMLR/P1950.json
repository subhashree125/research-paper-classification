{
  "Abstract": "Recent studies show that pretraining a deep neural network with fine-grained labeled data,followed by fine-tuning on coarse-labeled data for downstream tasks, often yields bettergeneralization than pretraining with coarse-labeled data. While there is ample empiricalevidence supporting this, the theoretical justification remains an open problem. This paperaddresses this gap by introducing a hierarchical multi-view structure to confine the inputdata distribution. Under this framework, we prove that: 1) coarse-grained pretraining onlyallows a neural network to learn the common features well, while 2) fine-grained pretraininghelps the network learn the rare features in addition to the common ones, leading to improvedaccuracy on hard downstream test samples.",
  "We consider the theory of label granularity in deep learning. By label granularity, we mean a hierarchy oftraining labels specifying how detailed each label subclass needs to be (See )": "Having access to different granularity of labels offers us the freedom of training a classifier using a differentlevel of precision. For example, instead of differentiating between dogs and cats, we can train a classifier todifferentiate a Poodle dog and a Persian cat. The latter classification task is undoubtedly harder. However,recent studies found that if one uses fine-grained labels to pre-train a backbone, the pre-trained backbone willhelp the downstream neural networks generalize better (Chen et al., 2018). Vision transformers, for example,are well-known to require pretraining on large datasets with thousands of classes for effective downstreamgeneralization (Dosovitskiy et al., 2021; He et al., 2016; Krizhevsky et al., 2012). To convince readers who are less familiar with this particular training strategy, we conduct an experimenton ImageNet with details described in Appendix A.2 (we also include experiments on iNaturalist 2021in Appendix A). Our experiment is limited in scale due to its high demand on the computing resources. shows an experiment of pre-training on ImageNet21k and fine-tuning the pre-trained network usingImageNet1k. The labels used in the ImageNet21k is based on WordNet Hierarchy. The downstream task",
  ": The goal of this paper is to provide a theoretical justification of why fine-grained labels in pre-training benefit generalization": "is ImageNet1k classification. The x-axis of this plot indicates the number of pre-training classes whereasthe y-axis shows the validation accuracy for the ImageNet1k classification task. It is evident from the plotthat as we increase the number of classes (hence a finer label granularity in pre-training), the downstreamclassification tasks performance is improved.",
  "Goal of this paper": "The above experimental finding may sound familiar to practitioners who frequently train large models. Infact, experimental evidence on this subject is abundant (Mahajan et al., 2018; Singh et al., 2022; Yan et al.,2020; Shnarch et al., 2022; Juan et al., 2020; Yang et al., 2021; Chen et al., 2018; Ridnik et al., 2021; Sonet al., 2023; Ngiam et al., 2018; Cui et al., 2018; 2019a). However, the theoretical explanation remains anopen problem. Our goal in this paper is to provide a theoretical justification. The core question we ask is:",
  "Main results and theoretical contributions": "Our main result is based on analyzing a two-layer convolutional neural network with ReLU activation. Weassume that the data distribution satisfies a certain hierarchical multi-view condition (to be discussed in.1). The optimization algorithm is stochastic gradient descent. Such problem settings are consistentwith published works on this subject (Allen-Zhu & Li, 2023b; 2022; Shen et al., 2022b; Jelassi & Li, 2022).Our conclusions are as follows.",
  "Theoretical results": "1. Coarse-grained pretraining only allows the neural network to learn the common features well.Therefore, when testing, the test error on easy samples is o(1) (i.e., small) whereas the error on hardsamples is (1) (i.e., large).2. Fine-grained pretraining helps the network learn the rare features in addition to the common ones,thus improving its test error on hard samples. In particular, the test error rate on both easy and hardtest samples are o(1) (i.e., small). To our knowledge, a precise characterization of the test error presented in this paper has never been reportedin the literature. The key enablers of our theoretical finding are the concepts of hierarchical multi-view andrepresentation-label correspondence. We summarize these two concepts below: 1. Hierarchical multi-view. To understand the label granularity problem, we argue that it is necessary forcoarse and fine-grained classes to be distinguished by their corresponding input features. This is consistentwith the multi-view data property pioneered by Allen-Zhu & Li (2023b). We call this a hierarchicalmulti-view structure. The hierarchical multi-view structure on the data makes us different from manyother deep learning theory works that assume simple or no structure in the input data (Kawaguchi, 2016;Allen-Zhu & Li, 2023a; Ba et al., 2022; 2023; Damian et al., 2022; Kumar et al., 2023; Ju et al., 2021). 2. Representation-label correspondence. Representation learning aims to recognize features in the inputdata. As will be shown later in the paper, under the hierarchical multi-view data assumption, labelcomplexity (i.e., how complex the labels are) during training influences the representation complexity(i.e., how many and what types of features are learnt), which further influences the models generalizationperformance. Studying label granularity through understanding the neural networks feature-learningprocess is a departure from the literature which focuses on feature selection (Jacot et al., 2018; Ju et al.,2021; 2022; Pezeshki et al., 2021; Arora et al., 2019), i.e., selecting a subset of pre-determined features.",
  "Our theoretical setting compared to the literature": "The subject of label granularity is immensely related to how to make a deep neural network (DNN) generalizebetter. In the existing literature, this is mostly explained through the lens of implicit regularization and biastowards simpler solutions to prevent overfitting even when DNNs are highly overparameterized (Lyu et al.,2021; Kalimeris et al., 2019; Ji & Telgarsky, 2019; De Palma et al., 2019; Huh et al., 2017). An alternativeapproach is the concept of shortcut learning which argues that deep networks can learn overly simple solutions.As such, deep networks achieve high training and testing accuracy on in-distribution data but generalizepoorly to challenging downstream tasks (Geirhos et al., 2020; Shah et al., 2020; Pezeshki et al., 2021). By examining these papers, we believe that Shah et al. (2020); Pezeshki et al. (2021) are the closest to oursbecause they demonstrate that DNNs perform shortcut learning and respond weakly to features that have aweak presence in the training data. However, our work departs from Shah et al. (2020); Pezeshki et al. (2021)in several key ways.",
  "Published in Transactions on Machine Learning Research (10/2024)": "need T0 time to reach it. This can already help us explain the linear-probing result we saw on ImageNet21kin Appendix A.2, since linear probing does not alter the the feature extractor after fine-grained pretraining(on ImageNet21k), it only retrains a new linear classifier on top of the feature extractor for classifying on thetarget ImageNet1k dataset. At a high level, finetuning F can only further enhance the feature extractors response to the features,therefore making the model even more robust for challenging downstream classification problems; it will notdegrade the feature extractors response to any true feature. A rigorous proof of this statement is almost arepetition of the proofs for fine-grained training, so we do not repeat them here. Intuitively speaking, we justneed to note that the properties stated in Theorem G.1 will continue to hold during finetuning (as long aswe stay in polynomial time), and with similar argument to those in the proof of Lemma G.2, we note thatthe neurons responsible for detecting fine-grained features, i.e. the S(0)+,c (v+,c), will continue to only receive(positive) updates on the v+,c-dominated patches of the following form:",
  "Our analytic tool compared to literature": "Our theoretical analysis is inspired by a recent line of work by Allen-Zhu & Li (2022; 2023b); Shen et al.(2022b). These papers analyze the feature learning dynamics of neural networks by tracking how the hiddenneurons of shallow nonlinear neural networks evolve to solve dictionary-learning-like problems. We adopt amulti-view approach to the data distribution which was first proposed in Allen-Zhu & Li (2023b). However,the learning problems we analyze and the results we aim to show are fundamentally different. As such, wederive the gradient descent dynamics of the neural network from scratch.",
  "Consistency with existing empirical results": "We stress that our theoretical findings are consistent with the reported empirical results in the literature,especially those that aim to improve classification accuracy by manipulating the pre-training label space(Mahajan et al., 2018; Singh et al., 2022; Yan et al., 2020; Shnarch et al., 2022; Juan et al., 2020; Yang et al.,2021; Chen et al., 2018; Ridnik et al., 2021; Son et al., 2023; Ngiam et al., 2018; Cui et al., 2018; 2019a).For example, Mahajan et al. (2018); Singh et al. (2022) use hashtags from Instagram as pretraining labels,Yan et al. (2020); Shnarch et al. (2022) apply clustering on the data first and then treat the cluster IDs aspretraining labels, Juan et al. (2020) use the queries from image search results, Yang et al. (2021) applyimage transformations such as rotation to augment the label space, and Chen et al. (2018); Ridnik et al.(2021) include fine-grained manual hierarchies in their pretraining processes. Our results corroborate theutility of pretraining on fine-grained label space. On the empirical end, there is also work focusing on exploiting the hierarchical structures present in (human-generated) label space to improve classification accuracy (Yan et al., 2015; Zhu & Bain, 2017; Goyal & Ghosh,2020; Sun et al., 2017; Zelikman et al., 2022; Silla & Freitas, 2011; Shkodrani et al., 2021; Bilal et al., 2017;Goo et al., 2016). For example, Yan et al. (2015) adapt the network architecture to learn super-classes ateach hierarchical level, Zhu & Bain (2017) add hierarchical losses in the hierarchical classification task, Goyal& Ghosh (2020) propose a hierarchical curriculum loss for curriculum learning. Our results do not directlyvalidate these practices because we are more interested in understanding the influence of label granularity onmodel generalization.",
  "where ac is the linear classifier for class c, h(; ) is the network backbone with parameter": "Referring to , label granularity concerns about two datasets: X src for the source (typically fine-grained)and X tgt for the target (typically coarse-grained). The corresponding labels are Ysrc and Ytgt, respectively. Adataset can be represented as D = (X, Y). For instance, the source training dataset is Dsrctrain = (X srctrain, Ysrctrain).",
  "Consider the following toy example. There are two classes: cat and dog. Our goal is to build a binaryclassifier. Lets discuss how the two training schemes would work, with an illustration shown in": "1. Baseline. The baseline method tries to identify the common features that can distinguish most of thecats from dogs, for instance, the shape of the animals ear as shown in . These features are oftenthe most noticeable ones because they appear the most frequently. Of course, there are hard samples, e.g.,a close-up shot of a cats fur. They pose limited influence during training because they are relatively rarein natural images. 2. Fine-to-coarse. With fine-grained labels, each subclass has its own unique visual features that are onlydominant within that subclass. However, fine-grained features are not as common in the dataset, hencemaking them more difficult to be noticed. Therefore, if we only present the coarse labels in the pre-trainingstage, the learner is allowed to take shortcuts by learning only the common features to achieve low trainingloss. One strategy to force the learner to learn the rarer features is to explicitly label the fine-grainedclasses. This means that within each fine-grained class, the fine-grained features become as easy to noticeas the common features. As a result, even if common features are weakly present or missing in a hard testsample, the network can still be reasonably robust to distracting irrelevant patterns due to its ability torecognize (some of) the finer-grained features.",
  "Problem Formulation": "Our first theoretical contribution is a new data model, the hierarchical multi-view model. This modelconsists of four definitions. Compared to existing theories studying feature learning of neural networks in theliterature (Allen-Zhu & Li, 2023b; 2022; Shen et al., 2022b; Jelassi & Li, 2022), these four definitions arebetter formulated to the label granularity problem. For the sake of brevity, we present the core concepts ofour data model here, and delay its full specification to Appendix B. Following data model specifications, wealso discuss characteristics of the learner, a two-layer nonlinear convolutional neural network.",
  "We consider the setting where an input sample X RdP consists of P patches x1, x2, ..., xP with xp Rd,where d is sufficiently large, and all our asymptotic statements are made with respect to d": "For analytic tractability, we consider two levels of label hierarchy. The root of this hierarchy has twosuperclasses +1 and 1.The superclass +1 has k+ subclasses.We denote these k+ subclasses as(+1, c1), . . . , (+1, ck+). We can do the same for the superclass 1 which has k subclasses. Each sub-class has two types of features: the common features and the fine-grained features. The two types of featuresare sufficiently different in the sense they have zero correlation and equal magnitude. This leads to thefollowing definition.Definition 4.1 (Features). We define features as elements of a fixed orthonormal dictionary V = {vi}di=1 Rd. The common and fine-grained features are",
  ": Illustration of features and patches": "Some comments: An easy sample is generated according to Definition 4.2. A hard sample is generated inthe same way as easy samples, except the common-feature patches are replaced by noise patches, and wereplace a small number of noise patches by feature-noise patches, which are of the form xp = pv + p,where p o(1), and set one of the noise patches to N(0, 2Id) with ; these patches servethe role of distracting patterns.Definition 4.3 (Source datasets label mapping). We say a sample X belongs to the +1 superclass if anyone of its common- or subclass-feature patches contains v+ or v+,c for any c [k+]. It belongs to the (+, c)subclass if any one of its subclass-feature patches contains v+,c.Definition 4.4 (Source training set). We assume the input samples of the source training set as X srctrain aregenerated as in Definition 4.2; the corresponding labels are generated following Definition 4.3. Overall, wedenote the source training dataset Dsrctrain. Relation to multi-view. Our data model is inspired by the multi-view concept first proposed in Allen-Zhu& Li (2023b), as we (1) use an orthonormal dictionary to define the features, (2) define an input consisting",
  "Target dataset. To ensure that baseline and fine-grained training have no unfair advantage over each other,we post a set of new characterizations on the target dataset:": "1. The input samples in the target dataset is generated according to Definition 4.2.2. The true label function is identical across the source and target datasets.3. Since we are studying the fine-to-coarse transfer direction, the target problems label space is the root ofthe hierarchy, meaning that any element of Ytgttrain or Ytgttest must belong to the label space {+1, 1}. Therefore, in our setting, only Ysrc and Ytgt can differ (in distribution) due to different choices in the labelgranularity level. In this idealized setting, we have essentially made baseline training and coarse-grainedpretraining the same procedure. Therefore, an equally valid way to view our theorys setting is to considerDtgttrain the same as Dsrctrain except with coarse-grained labels. In other words, we pretrain the network on twoversions of the source dataset Dsrc,coarsetrainand Dsrc,finetrain, and then compare the two models on Dtgttest (which hascoarse-grained labels).",
  "= 1/poly(d); we set b(0)c,r = 0": "ln(d)and manually tune it, similar to Allen-Zhu & Li (2022). Cross-entropy is the training loss for both baseline and transfer training. To simplify analysis and to focus solely onthe learning of the feature extractor, we freeze ac,r = 1 during all baseline and transfer training phases, andwe use the fine-grained model for binary classification as follows: F+(X) = maxc[k+] F+,c(X), F(X) =maxc[k] F,c(X). See Appendix B.2 and the beginning of Appendix G for details of learner characteristicsand training algorithm.",
  "PF (t)y (Xhard) F (t)y (Xhard) (1),for y = y.(5)": "This theorem essentially says that, with a mild lower bound on the number of fine-grained classes, if we onlytrain on the easy samples with coarse labels, it is virtually impossible for the network to learn the fine-grainedfeatures even if we give it as much practically reachable amount of time and training samples as possible.Consequently, the network would perform poorly on the hard downstream test samples: if the sample ismissing the common features, then the network can be easily misled by the noise present in the sample. Tosee the full setup and statement of this theorem, please see Appendix B and E. Its proof spans Appendix Cto E. Theorem 5.2 (Fine-grained-label training). (Summary). Assume the same setting as in Theorem 5.1, exceptlet the labels be fine-grained and ky d0.4 (number of subclasses not pathologically large; see for itsdiscussion). Within poly(d) time, the probability of making a classification mistake is small:",
  "on the target binary problem on both easy and hard test samples": "The full version of this result is presented in Appendix G.4, and its proof in Appendix G. After fine-grainedpretraining, the networks feature extractor gains a strong response to the fine-grained features, therefore itsaccuracy on the downstream hard test samples increases significantly. Remark. One concern about the above theorems is that the neural networks are trained only on easy samples.As noted in Sections 1 and 3.2, easy samples should make up the majority of the training and testing samples.Pretraining at higher label granularities only improves network performance on rare samples. Our theoreticalresult presents the feature-learning bias of a neural network in an exaggerated fashion. Therefore, it is naturalto start with the case of no hard training samples. In reality, even if a small portion of hard training samplesis present, finite-sized training datasets can have many flaws that can cause the network to overfit severelybefore learning the fine-grained features, especially since rarer features are learnt more slowly and corruptedby greater amount of noise. We leave these deeper considerations for future theoretical work.",
  "Proof strategy: representation-label correspondence": "The key idea of the proof is to establish a correspondence between the complexity of the labels and complexityof the networks representations. We show that when trained on coarse-grained labels (i.e. overly simplelabels), the network only learns the common features well, so its representations of the data is overly simple.In contrast, training with fine-grained labels helps the network learn the fine-grained features well in additionto the common ones, so its representations are more complex.",
  "We first sketch the proof of baseline training which uses coarse-grained labels": "Feature detector neurons. We show that, at initialization, with high probability, for every feature v V,there exists a small group of lucky neurons, denoted S(0)y(v) (with y indicating the superclass), that onlyactivate on v-dominated feature patches. We prove that if v is a feature of class y, then with high probability,",
  "The significance of equation 7 is that, we may now argue about the networks representation of the inputdata solely based on the behavior of the feature detector neurons": "Impartial representation at initalization. At initialization, the feature extractors response to commonand fine-grained features are very close. The reason is that,S(0)y(v) S(0)y(v) for all superclasses y, y and features v, v, and they all have a similar magnitude of activation strength. Written explicitly, given anycommon-feature patch xcom = vy + and subclass-feature patch xsub = vy,c + (from the training ortesting distribution), with high probability,",
  "So what happened during training which caused a strong imbalance of representation of the common andfine-grained features in the end? The answer below is the core of the proof": "Overly simple labels = overly simple representations. The imbalance of growth is a result of thesubclass-feature patches occurring with less frequency in the training set than the common-feature patches.Recall that the number of subclasses is ky: for any subclass (y, c), subclass-feature patches dominated by vy,care about ky times rarer than the common feature patches. This has a direct impact on the growth speed ofthe common and fine-grained detector neurons: for any neuron rcom S(0)y(vy) and any rfine S(0)y(vy,c),w(t)y,rcom, vy (ky) w(t)y,rfine, vy,c. With careful arguments on the influence of noise and bias on the activation values, we can show that, for tsufficiently large, the fine-grained detector neurons are about (ky) times weaker in strength:",
  "Empirical Results": "Building on our theoretical analysis in an idealized setting, this section discusses conditions on the sourceand target label functions that we observed to be important for fine-grained pretraining to work in practice,while remaining in the controlled setting described in for the sake of tractability. We present thecore experimental results obtained on ImageNet21k and iNaturalist 2021 in the main text, and leave theexperimental details and ablation studies to Appendix A.",
  "ImageNet21kImageNet1k transfer experiment": "This subsection provides more details about the experiment shown in . Specifically, we show that thecommon practice of pretraining on ImageNet21k using leaf labels is indeed better than pretraining at lowergranularities in the manual hierarchy. Hierarchy definition. The label hierarchy in ImageNet21k is based on WordNet Miller (1995); Deng et al.(2009). To define fine-grained labels, we first define the leaf labels of the dataset as Hierarchy level 0. Foreach image, we trace the path from the leaf label to the root using the WordNet hierarchy. We then setthe k-th synset (or the root synset, if it is higher in the hierarchy) as the level-k label of this image. Thisprocedure also applies to the multi-label samples. This is how we generate the hierarchies shown in . Network choice and training. For this dataset, we use the more recent Vision Transformer ViT-B/16Dosovitskiy et al. (2021). Our pretraining pipeline is almost identical to the one in Dosovitskiy et al. (2021).For fine-tuning, we experimented with several strategies and report only the best results in the main text;the finer details are discussed in Appendix A.1.2 and A.2. To ensure a fair comparison, we also used thesestrategies to find the best baseline result by using Dtgttrain for pretraining.",
  "Interpretation of results. shows the validation errors of the resulting models on the 11-superclassproblem. We make the following observations": "Reasonably fine-grained labels benefit generalization, but there is a catch. We can observe that in the bluecurve of that, as long as the number of subclasses is less than 103, we see obvious decline in thevalidation error on the target labels. In other words, reasonably fine-grained pretraining is indeed beneficialin this setting. We should note, however, the overall curve exhibits a U shape: overly fine-grained labelsare not beneficial to downstream generalization. This is intuitive. If the pretraining granularity is too closeto the target one, we should not expect improvement. On the other extreme, if we assign a unique label toevery sample in the training data, it is highly likely that the only differences a model can find between eachclass would be frivolous details of the images, which would not be considered discriminative by the labelfunction of the target coarse-label problem. In this case, the pretraining stage is almost meaningless and canbe misleading, as evidenced by the very high label-per-sample error (red star in ). High granularity can be helpful, but label-assignment consistency is critical. Random class ID pretraining(orange curve) performs the worst of all the alternatives. The label function of this type does not generate ameaningful hierarchy because it has no consistency in the features it considers discriminative when decomposingthe superclasses. This is in stark contrast to the manual hierarchies, which decompose the superclasses basedon the finer biological traits (mostly visual in nature) of the creatures in the image.",
  "Baseline: RetrainingBaseline: One-passSubclassID = RandomSubclassID = kMean/ClassSubclassID = kMean/AllSubclassID = Manual": ": In-dataset transfer. ResNet34 validation error (with standard deviation) of finetuning on 11superclasses of iNaturalist 2021, pretrained on various label hierarchies. The manual hierarchy outperformsthe baseline and every other hierarchy, and exhibits a U-shaped curve. Alignment between fine-grained and target label spaces are important. For fine-grained pretraining to beeffective, the features that the pretraining label function considers discriminative must align well with thosevalued by the label function of the 11-superclass hierarchy. To see this point, observe that for models trainedon cluster IDs obtained by performing kMeans on the CLIP embedding samples in each superclass separately(green curve in ), their validation errors are much lower than those trained on cluster IDs obtainedby performing kMeans on the whole dataset (purple curve in ). As expected, the manually definedfine-grained label functions align best with that of the 11 superclasses, and the results corroborate this view.",
  "Q: Does a higher label granularity always imply better generalization?": "A: No. There is an operating regime. Training a model with pathologically high label granularity is harmful.For example, if we assign a unique class to every sample in the dataset, the model will be forced to rely onthe frivolous differences between each sample. We verify this intuition in in Appendix A.1.1 on theiNaturalist 2021 dataset. These extreme scenarios do not arise in common practice, so we do not focus onthem in this paper.",
  "Conclusion": "In this paper, we formally studied the influence of pretraining label granularity on the generalization of DNNs,and performed large-scale experiments to complement our theoretical results. Under the new data model,hierarchical multi-view, we theoretically showed that higher label complexity leads to higher representationcomplexity, through which we explained why pretraining with fine-grained labels is beneficial to generalization.We complement our theory with experiments on ImageNet and iNaturalist, demonstrating that in thecontrolled setting of this paper, pretraining on reasonably fine-grained labels indeed benefits generalization.",
  "Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchicalimage database. In CVPR, 2009": "Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby.An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021. Robert Geirhos, Jrn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel, Wieland Brendel, Matthias Bethge,and Felix A. Wichmann. Shortcut learning in deep neural networks. In Nature Machine Intelligence, 2020.",
  "Carlos N Silla and Alex A Freitas. A survey of hierarchical classification across different application domains.Data Mining and Knowledge Discovery, 2011": "Mannat Singh, Laura Gustafson, Aaron Adcock, Vinicius de Freitas Reis, Bugra Gedik, Raj Prateek Kosaraju,Dhruv Mahajan, Ross Girshick, Piotr Dollr, and Laurens Van Der Maaten. Revisiting weakly supervisedpre-training of visual perception models. In CVPR, 2022. Donghyun Son, Byounggyu Lew, Kwanghee Choi, Yongsu Baek, Seungwoo Choi, Beomjun Shin, SungjooHa, and Buru Chang. Reliable decision from multiple subtasks through threshold optimization: Contentmoderation in the wild. In WSDM, 2023.",
  "A.1In-dataset transfer results": "To clarify, in this transfer setting, we are essentially transferring within a dataset. More specifically, we setX src = X tgt and only the label spaces Ysrc and Ytgt may differ (in distribution). The baseline in this settingis clear: train on Dtgttrain and test on Dtgttest. In contrast, after pretraining the backbone network h(; ) onYsrc, we finetune or linear probe it on Dtgttrain using the backbone and then test on Dtgttest.",
  "A.1.1iNaturalist 2021": "iNaturalist 2021 is well-suited for our analysis because it has a high-quality, manually defined label hierarchythat is based on the biological traits of the creatures in the images. Additionally, the large sample size ofthis dataset reduces the likelihood of sample-starved pretraining on reasonably fine-grained hierarchy levels.We use the mini training dataset with size 500,000 instead of the full training dataset to show a greater gapbetween the results of different hierarchies and speed up training.",
  "We use the architectures ResNet 34 and 50 He et al. (2016)": "Training details. Our pretraining pipeline on iNaturalist is essentially the same as the standard large-batch-size ImageNet-type training for ResNets He et al. (2016); Goyal et al. (2017). The following pipeline appliesto model pretraining on any hierarchy. Optimization: SGD with 0.9 momentum coefficient, 0.00005 weight decay, 4096 batch size, 90 epochstotal training length. We perform 7 epochs of linear warmup in the beginning of training until thelearning rate reaches 0.1 4096/256 = 1.6, and then apply the cosine annealing schedule. Eachtraining instance is run on 16 TPU v4 chips, taking around 2 hours per run.",
  "Data augmentation: subtracting mean and dividing by standard deviation, image (original or itshorizontal flip) resized such that its shorter side is 256 pixels, then a 224 224 random crop is taken": "For finetuning, we keep everything in the pipeline the same except setting the batch size to 4096/4 = 1024and base learning rate 1.6/4 = 0.4. We found that finetuning at higher batch size and learning rate resultedin training instabilities and severely affected the final finetuned models validation accuracy, while finetuningat lower batch size and learning rate than the chosen one resulted in lower validation accuracy at the endeven though their training dynamics was stabler. For the baseline accuracy, as mentioned in the main text, to ensure fairness of comparison, in addition toonly training the network on the target 11-superclass problem for 90 epochs (using the same pretrainingpipeline), we also perform retraining: follow the exact training process of the models trained on the varioushierarchies, but use Dtgttrain as the training dataset in both the pretrianing and finetuning stage. We observedconsistent increase in the final validation accuracy of the model, so we report this as the baseline accuracy.Without retraining (so naive one-pass 90-epoch training on 11 superclasses), the average accuracy withstandard deviation is 94.13, 0.025.",
  "(a) Perform kMeans on the embedding of all the samples in the dataset, with the number of clustersset to C. Set the fine-grained class ID of a sample to its cluster ID": "Some might have the concern that having the same number of kMeans clusters per superclass could causecertain classes to have too few samples, which could be a reason for why the cluster ID hierarchies performworse than the manual hierarchies. Indeed, the number of samples per superclass on iNaturalist is different,so in addition to the above uniform-number-of-cluster-per-superclass hierarchy, we add an extra labelhierarchy by performing the following procedure to balance the sample size of each cluster: 1. Perform kMeans for each superclass with number of clusters set to 2, 8, 32, 64, 128, 256, 512, 1024 andsave the corresponding image-ID-to-cluster-ID dictionaries (so we are basically reusing the clusteringresults of the CLIP+kMeans per superclass experiment) 2. For each superclass, find the image-ID-to-cluster-ID dictionary with the highest granularity whilestill keeping the minimum number of samples for each cluster > predefined threshold (e.g. 1000samples per subclass)",
  ". Now we have nonuniform granularity for each superclass while ensuring that the sample count percluster is above some predefined threshold": "This simple procedure somewhat improves the balance of sample count per cluster, for example, shows the sample count per cluster for the cases of total number of clusters = 608 and 1984. Unfortunately,we do not observe any meaningful improvement on the models validation accuracy trained on this morerefined hierarchy. Experimental procedures. All the validation accuracies we report on ResNet34 are the averaged results ofexperiments performed on at least 6 random seeds: 2 random seeds for backbone pretraining and 3 randomseeds for finetuning. We report the average accuracies with their standard deviation on various hierarchies in. An additional experiment we performed with ResNet34 is a small grid search over what checkpoint of apretrained backbone we should use for finetuning on the 11-superclass method; we tried the 50-, 70- and90-epoch checkpoints of the backbone on the manual hierarchies. We report these results in . As wecan see, 90-epoch checkpoints performs almost equally well as the 70-epoch checkpoints and better than the50-epoch ones by a nontrivial margin. With this observation, we chose to use the end-of-pretraining 90-epochcheckpoints in all our other experiments without further ablation studies on those hierarchies.",
  "Sample Count": "Sample count per cluster. Number of clusters = 608 : In-dataset transfer, iNaturalist 2021. Number of samples per cluster in the case of 608and 1984 total clusters, after applying the sample size rebalancing procedure described in subsection A.1.1.Observe that the sample sizes are reasonably balanced across almost all the subclasses. : In-dataset transfer. ViT-B/16 validation error on the binary problem is this object a livingthing? of ImageNet21k. Pretrained on various hierarchy levels of ImageNet21k, finetuned on the binaryproblem. Observe that the maximal improvement appears at the leaf labels, and as G(Ysrc) approaches 2,the percentage improvement approaches 0.",
  "The ImageNet21k dataset we experiment on contains a total of 12,743,321 training samples and 102,400validation samples, with 21843 leaf labels. A small portion of samples have multiple labels": "Caution: due to the high demand on computational resources of training ViT models on ImageNet21k, all ofour experiments that require (pre-)training or finetuning/linear probing on this dataset were performed withone random seed. Hierarchy generation. To define fine-grained labels, we start by defining the leaf labels of the datasetto be Hierarchy level 0. For every image, we trace from the leaf synset to the root synset relying on theWordNet hierarchy, and set the k-th synset (or the root synset, whichever is higher in level) as the level-klabel of this image; this procedure also applies to the multi-label samples. This is the way we generate themanual hierarchies shown in the main text. Due to the lack of a predefined coarse-label problem, we manually define our target problem to be a binaryone: given an image, if the synset Living Thing is present on the path tracing from the leaf label of theimage to the root, assign label 1 to this image; otherwise, assign 0. This problem almost evenly splits thetraining and validation sets of ImageNet21k: 5,448,549:7,294,772 for training, 43,745:58,655 for validation. Network choice and pretraining pipeline. We experiment with the ViT-B/16 model Dosovitskiy et al.(2021). The pretraining pipeline of this model follows the one in Dosovitskiy et al. (2021) exactly: we trainthe model for 90 epochs using the Adam optimizer, with 1 = 0.9, 2 = 0.999, weight decay coefficient equalto 0.03 and a batch size of 4096; we let the dropout rate be 0.1; the output dense layers bias is initialized to10.0 to prevent huge loss value coming from the off-diagonal classes near the beginning of training Cui et al.(2019b); for learning rate, we perform linear warmup for 10,000 steps until the learning rate reaches 103,then it is linearly decayed to 105. The data augmentations are the common ones in ImageNet-type trainingDosovitskiy et al. (2021); He et al. (2016): random cropping and horizontal flipping. Note that we use thesigmoid cross-entropy for training since the dataset has multi-label samples.",
  "Each training instance (90 epochs) is run on 64 TPU v4 chips, taking approximately 1.5 to 2 days": "Evaluation on the binary problem. After the 90-epoch pretraining on the manual hierarchies, we evaluatethe model on the binary problem. We report the best accuracies on each hierarchy level in . To get asense of how the relevant hyperparameters influence final accuracy of the model, we try out the followingfinetuning/linear probing strategies on the backbone trained on the leaf labels and the target binary problemof the dataset, and report the results in (similar to our experiments on iNaturalist, we include thebackbone trained on the binary problem in these ablation studies to ensure that our comparisons against thebaseline are fair) :",
  "(4096/8 = 512, 0.001/8 = 0.000125)}": "2. Linear probing with 20 epochs training length, using exactly the same training pipeline as in pretrain-ing. We ran a small grid search over (batch size, base learning rate) = {(4096, 0.001), (4096/8 =512, 0.001/8 = 0.000125)}. 3. 10-epochs finetuning, no linear warmup, 3 epochs of constant learning rate in the beginning fol-lowed by 7 epochs of linear decay, with a small grid search over (batch size, base learning rate) ={(4096, 0.001), (4096/8 = 512, 0.001/8 = 0.000125)}. helps us decide the best accuracies to report. First, as expected the linear probing results aremuch worse than the finetuning ones. Second, the retraining accuracy of 92.102 is the best baseline wecan report (the same thing happened in the iNaturalist case) if we only train the model for 90 epochs(the naive one-pass training) on the binary problem, then the models final validation accuracy is 91.746%,which is lower than 92.102% by a nontrivial margin. In contrast, the short 10-epoch finetuning strategy",
  "Random IDG(Ysrc)200040008000per-classValidation error23.40.06823.40.07023.650.071": "works best for the backbone trained on the leaf labels, therefore, we also use this strategy to evaluate thebackbones trained on all the other manual hierarchies. A peculiar observation we made was that, finetuningthe leaf-labels-pretrained backbone for extended period of time on the binary problem caused it to overfitseverely: for batch size and base learning rate in the set {(4096, 0.001), (1024, 0.00025), (512, 0.000125)},throughout the 90 epochs of finetuning, although its training loss exhibits the normal behavior of stayingmostly monotonically decreasing, its validation accuracy actually reached its peak during the linear warmupperiod!",
  "A.1.3ImageNet1k": "Our ImageNet1k in-dataset transfer experiments are done in a very similar fashion to the iNaturalist ones.In particular, the pretraining and finetuning pipeline for ResNet50 is exactly the same as the one in theiNaturalist case, so we do not repeat it here. Due to a lack of more fine-grained manual label on this dataset, we generate fine-grained labels by performingkMeans on the ViT-L/14 CLIP embedding of the dataset separately for each class; the exact procedure isalso identical to the iNaturalist case. The CLIP backbones we use here are the ResNet50 version and theViT-L/14 version. We report the average accuracies and their standard deviation in . All results areobtained from at least one random seed during pretraining and 3 random seeds during finetuning. The best baseline we report is the one using retraining: if we adopt the pretrain-then-finetune procedurebut with Dtgttrain (i.e. the vanilla 1000-class labels) set as the pretraining dataset, then we obtain an averagevalidation error of 23.28% with standard deviation of 0.103, averaged over results of 3 random seeds. Incomparison, if we only perform the naive one-pass 90-epoch training, we obtain average valiation error 24.04%,with standard deviation 0.057. From , we see that there is virtually no difference between the baseline and the best errors obtainedby the models trained on the custom hierarchies: they are almost equally bad. Noting that the sample size ofeach class in ImageNet1k is only around 103, and the fact that ImageNet1k classification is a hard problem it is a problem of high sample complexity further decomposing the classes causes each fine-grained classto have too few samples, leading to the above negative results. This reflects the intuition that higher labelgranularity does not necessarily mean better model generalization, since the sample size per class mightbecome too small.",
  "In this subsection, we report the average validation accuracy and standard deviation of the cross-datasettransfer experiment from ImageNet21k to ImageNet1k, as discussed in and in the maintext": "Network choice. We use the same architecture ViT-B/16 as the one in the in-dataset ImageNet21k transferexperiment and follow the same training procedure, which we repeat here for the readers convenience. Thepretraining pipeline of this model follows the one in Dosovitskiy et al. (2021): we train the model for 90epochs using the Adam optimizer, with 1 = 0.9, 2 = 0.999, weight decay coefficient equal to 0.03 anda batch size of 4096; we let the dropout rate be 0.1; the output dense layers bias is initialized to 10.0to prevent huge loss value coming from the off-diagonal classes near the beginning of training Cui et al.(2019b); for learning rate, we perform linear warmup for 10,000 steps until the learning rate reaches 103,then it is linearly decayed to 105. The data augmentations are the common ones in ImageNet-type trainingDosovitskiy et al. (2021); He et al. (2016): random cropping and horizontal flipping. Note that we use thesigmoid cross-entropy for training since the dataset has multi-label samples.",
  "Additionally, each training instance (90 epochs) is run on 64 TPU v4 chips, taking approximately 1.5 to 2days": "Finetuning. For finetuning on ImageNet1k, our procedure is very similar to the one in the original ViTpaper Dosovitskiy et al. (2021), described in its Appendix B.1.1. We optimize the network for 8 epochsusing SGD with momentum factor set to 0.9, zero weight decay, and batch size of 512. The dropout rate,unlike in pretraining, is set to 0. Gradient clipping at 1.0 is applied. Unlike Dosovitskiy et al. (2021),we still finetune at the resolution of 224224. For learning rate, we apply linear warmup for 500 epochsuntil it reaches the base learning rate, then cosine annealing is applied; we perform a small grid search ofbase learning rate = {3 103, 3 102, 6 102, 3 101}. Every one of these grid search is repeated over",
  "random seeds. We report the ImageNet1k validation accuracies and their standard deviations in .In the main text, we report the best accuracy for each hierarchy level": "Linear probing. For linear probing, we use the following procedure. We optimize the linear classifier for 40epochs (similar to Lee et al. (2021)) using SGD with Nesterov momentum factor set to 0.9, a small weightdecay coefficient 106, and batch size 512. We start with a base learning rate of 0.9, and multiply it by 0.97per 0.5 epoch. In terms of data augmentation, we adopt the standard ones like before: horizontal flippingand random cropping of size 224224. We repeat this linear probing procedure over 3 random seeds giventhe pretrained backbone, and report the average validation accuracy and standard deviation in .",
  "Remark.1. The initialization strategy is similar to the one in Allen-Zhu & Li (2022)": "2. Since the only difference between the training samples of coarse and fine-grained pretraining is thelabel space, the form of SGD update is identical. The only difference is the number of output nodesof the network: for coarse training, the output nodes are just F+ and F (binary classification), whilefor fine-grained training, the output nodes are F+,1, F+,2, ..., F+,k+, F,1, F,2, ..., F,k, a total ofk+ + k nodes. 3. The bias is for thresholding out the neurons noisy activations that grow slower than 1/ log5(d) timesthe activations on the features which the neuron detects. This way, the bias does not really influenceupdates to the neurons response to the (common and/or fine-grained) features which it activatesstrongly on, since 11 log5(d) 1, while it removes useless low-magnitude noisy activations. This in factcreates a (generalization) gap between the nonlinear model that we are studying and linear models.Due to our parameter choices (as discussed below), if the model has no nonlinearity (remove the ReLUactivations), then even if the model can be written as F+(X) =",
  "p[P ] cv, xp + c,1v,1, xp + ... + c,kv,k, xp for": "any sequence of nonnegative real numbers c+, c, {c+,j}k+j=1, {c,j}kj=1 (which is the ideal situationsince the true features are not corrupted by anything), it is impossible for the model to reach o(1)error on the input samples, because the number of noise patches will accumulate to a variance of(P O(s)) O(s), which significantly overwhelms the signal from the true features. On the",
  ". Note: we sometimes abuse the notation x = a b as an abbreviation for x [a b, a + b]": "Remark. We believe the range of parameter choice can be (asymptotically) wider than what is consideredhere, but for the purpose of illustrating the main messages of the paper, we do not consider a more generalset of parameter choice necessary because having a wider range of it can significantly complicate and obscurethe already lengthy proofs without adding to the core messages.",
  "B.4Plan of presentation and central ideas": "We shall devote the majority of our effort to proving results for the coarse-label learning dynamics, startingwith appendix section C and ending on E, and only devote section G to the fine-grained-label learningdynamics, since the analysis of fine-grained training overlaps significantly with the coarse-grained one. One technical difficulty in making the above ideas rigorous lies in the ReLU activation (with time-dependentbias): due to randomness in the gradient updates and the initialization, it is possible for individual hiddenneurons that activate on v-dominated patches at one time iterate to no longer do so at the next iterate, andthe opposite can happen. This can be problematic: for instance, it is possible that certain lucky neurons forv+ at one iterate become dead on v+-dominated patches at the next iterate, while some unlucky neuronsthat were dead on v+,c-dominated patches before start activating on these patches at the current iterate.In our proof, we show that this kind of situation does not happen too frequently nor do they contributetoo much to the overall behavior of the neural network, by carefully keeping track of each hidden neuronsresponse to feature vectors and noise vectors throughout training.",
  "(89)": "Therefore the neurons (+, r) S(0)+(v+) activate on the v+-dominated patches x(t+1)n,p. We also know thatthey cannot activate on patches that are not dominated by v+ by Theorem F.1. Following a similar derivationto the base case, we arrive at the result that, conditioning on the events of the induction hypothesis, withprobability at least 1 OmNP k+poly(d), for all (+, r) S(0)+(v+),",
  "In particular, w(T )+,r, x(T )n,p + b(T )+,r= 0": "In other words, no (+, r) / S(0)+ (v) can be updated on the v-dominated patches at time t = T. Furthermore,the induction hypothesis of point 2.also states that the network cannot activate on any noise patchx(T )n,p = (T )n,p with probability at least 1OmNP Tpoly(d). Therefore, the neuron update for those (+, r) / S(0)+ (v)takes the form",
  "In particular, w(T )+,c,r, x(T )n,p + b(T )+,c,r= 0": "In other words, no (+, c, r) / S(0)+,c(v) can be updated on the v-dominated patches at time t = T.Furthermore, the induction hypothesis of point 2. also states that the network cannot activate on anynoise patch x(T )n,p = (T )n,p with probability at least 1 OmNP Tpoly(d). Therefore, the neuron update for those",
  "Therefore, F (T0)+,c (X) > maxy=(+,c) F (T0)y(X), which means F (T0)+(X) > F (T0)(X) indeed": "Remark. First of all, note that the feature extractor, after fine-grained training, is already well-performing, asit responds strongly ((1) strength) to the true features, and very weakly (o(1) strength) to any off-diagonalfeatures and noise. In other words, we stop training when the margin is at least (1), i.e. when we haveF (T )"
}