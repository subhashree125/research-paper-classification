{
  "Abstract": "Text-to-SQL, the process of translating natural language into Structured Query Language(SQL), represents a transformative application of large language models (LLMs), potentiallyrevolutionizing how humans interact with data. This paper introduces the SQL-PaLMframework, a comprehensive solution for understanding and enhancing Text-to-SQL usingLLMs, using in the learning regimes of few-shot prompting and instruction ne-tuning. Withfew-shot prompting, we explore the eectiveness of consistency decoding with execution-basederror ltering. With instruction ne-tuning, we delve deep in understanding the criticalparadigms that inuence the performance of tuned LLMs. In particular, we investigatehow performance can be improved through expanded training data coverage and diversity,synthetic data augmentation, and integrating query-specic database content. We proposea test-time selection method to further rene accuracy by integrating SQL outputs frommultiple paradigms with execution feedback as guidance.Additionally, we tackle thepractical challenge of navigating intricate databases with a signicant number of tables andcolumns, proposing ecient techniques for accurately selecting relevant database elements toenhance Text-to-SQL performance. Our holistic approach yields substantial advancementsin Text-to-SQL, as demonstrated on two key public benchmarks, Spider and BIRD. Throughcomprehensive ablations and error analyses, we shed light on the strengths and weaknessesof our framework, oering valuable insights into Text-to-SQLs future work.",
  "Introduction": "What are the names of nations where both English and French are official languages? SELECT T1.Name FROM country AS T1 JOIN countrylanguage AS T2 ONT1.Code = T2.CountryCode WHERE T2.Language =\"English\" AND T2.IsOfficial = \"T\" INTERSECT SELECT T1.NameFROM country AS T1 JOIN countrylanguage AS T2 ONT1.Code = T2.CountryCode WHERE T2.Language = \" French\" ANDT2.IsOfficial = \"T\"",
  ": Text-to-SQL systems are developed to trans-form queries expressed in natural language into Struc-tured Query Language (SQL) based on the informationfrom databases": "Text-to-SQL aims to automate the process of trans-lating natural language questions into SQL queriesthat can be executed directly on a database (An-droutsopoulos et al., 1995; Hristidis et al., 2003; Li& Jagadish, 2014; Wang et al., 2017). As illustratedin , Text-to-SQL bridges the gap between theway humans naturally communicate, using language,and the way databases are structured, and has thepotential to revolutionize how humans interact withdata (Zhong et al., 2017; Yu et al., 2018; Li et al.,2023d). Making databases accessible to non-expertusers through natural language, Text-to-SQL canempower humans to extract valuable informationwithout needing specialized SQL knowledge (Wanget al., 2019; Scholak et al., 2021; Cai et al., 2021;Qi et al., 2022; Li et al., 2023b; Pourreza & Raei,2023; Gao et al., 2023a; Sun et al., 2023; Chen et al.,2023). This not only enhances the eciency of dataanalysis but also broadens the use of databases toa wider range of applications. Intelligent database services, platforms for automated data analytics, andmore sophisticated conversational agents capable of understanding and responding to complex data-relatedquestions can all be fueled by advancements in Text-to-SQL (Yu et al., 2019; Gu et al., 2022; Prez-Mercadoet al., 2023; Xie et al., 2023). At a high level, the Text-to-SQL is a sequence-to-sequence modeling task (Sutskever et al., 2014), with boththe database schema and natural language question being converted into a linear input sequence, and the SQLbeing the target output sequence. Early works attempt to ne-tune domain-specic Transformer architecturesor decoding approaches tailored for Text-to-SQL utilizing SQL syntax, semantics or the intricate relationshipbetween questions and databases (Scholak et al., 2021; Qi et al., 2022; Li et al., 2023a; Wang et al., 2019;Bogin et al., 2019; Cai et al., 2021; Hui et al., 2022). Recent years have witnessed the burgeoning applicationof large language models (LLMs) to Text-to-SQL (Rajkumar et al., 2022; Liu et al., 2023a; Gao et al., 2023a;An et al., 2023; Tai et al., 2023; Pourreza & Raei, 2023; Chen et al., 2023; Sun et al., 2023). Along thisline, most of the research has focused on leveraging prompting to translate user utterances into SQL queries(Rajkumar et al., 2022; Liu et al., 2023a). More advanced prompting methods has domain-specic adoptionsto improve understanding natural language questions and structured database schemas, such as selectingbetter few-shot exemplars based on question similarity (Gao et al., 2023a; An et al., 2023), decomposingcomplex questions into sub-tasks (Tai et al., 2023; Pourreza & Raei, 2023), verifying the correctness ofmodel-predicted SQL queries through execution feedback (Chen et al., 2023; Sun et al., 2023; Pourreza &Raei, 2023), as well as linking NL phrases (e.g. nation in the question in ) to relevant databaseconstructs (e.g. the Name column in the County table, see (Pourreza & Raei, 2023)). While these few-shot prompting methods have signicantly improved Text-to-SQL performance, it stillremains unclear whether prompting alone is adequate to handle real-world challenges. As we elaboratein Sec. 2, real-world Text-to-SQL exhibits a variety of challenges. Specically, a users natural languagequestions are often ambiguous (e.g. sales in California) and can have multiple plausible interpretations(e.g. sales made by Californian businesses or produces sold in the states). Those questions might also comewith semantic constraints (e.g Who was the president before Joe Biden) that map to complex SQL queries(e.g. requires reasoning steps that rst retrieve the beginning date of Bidens term and then identify thelast record whose end date is before that). Moreover, real-world databases may contain large volumes oftables and columns, and the sheer content size would easily exceed the context limit of LLMs. Componentsin a schema could also have rich data types (e.g. strings or datetimes) with complex dependencies denedby primary-foreign key mappings, requiring non-trivial SQL queries to process such data. In addition tothose inherent challenges, collecting aligned examples of questions and SQL queries for learning also requires",
  "laborious annotation eorts by domain experts, impeding the process of scaling-up data hungry LLMs forText-to-SQL": "As a rst step towards addressing those challenges, in this paper, we propose SQL-PaLM, a holistic frameworkthat adapts a LLM, PaLM-2 (Anil et al., 2023) Unicorn variant, for Text-to-SQL tasks. We start withevaluating SQL-PaLMs performance with prompting, and then we focus on SQL-PaLMs tuning as it leads tobetter performance in challenging scenarios. Apart from existing work that mostly focus on few-shot promptingstrategies or tuning relatively smaller LLMs, SQL-PaLM focus on tuning a large LLM. Larger models havedierent behaviors with their emergent abilities, a phenomenon of signicantly improved understanding andreasoning performance compared to smaller LLMs (Wei et al., 2022a). We systematically explore large modelspotential for Text-to-SQL and study the research topics along the key aspects presented in Sec. 4 . Throughextensive experiments and analyses, we unravel multiple key factors that inuence the LLMs performancewhen adapting to Text-to-SQL. First, diversity and coverage of train data can be crucial we present ablationstudies on training data mixture and provide takeaways across tasks and benchmarks. To improve trainingdata coverage with low human cost, SQL-PaLM also proposes augmenting with large-scale LLM-generatedsynthetic data. Second, input representations can greatly inuence the overall quality. We present an in-depthstudy on ne-tuning Text-to-SQL to better leverage dierent types of information-bearing content, suchas database values, column descriptions, and hints. Next, scaling to real-world database sizes would beimportant for adoption. We present an ecient column selection approach that only encodes information fromthe subset of relevant database columns as inputs to LLMs. This approach signicantly reduces the contextsize with negligible impact on performance. In particular, we propose a program-aided column selectionand retrieval-based column selection approach, We study integration with both hard (i.e. with removal ofunselected columns) and soft (i.e. emphasizing on selected columns) column selection approach into theoverall Text-to-SQL pipeline. Finally, we propose execution-based test-time renement to integrate multipletraining paradigms based on execution feedback.",
  "Our contributions can be summarized as follows:": "We focus on large LLMs on Text-to-SQL and investigate from multiple important angles includinglearning perspectives (prompting vs tuning), task perspectives (judiciously selecting input components)and real-world scaling perspectives (e.g. column selection). Through thorough experiment andanalysis, we systematically identify components in inuencing performance. We demonstrate that mixing training data with diverse sets, despite having various formats, canbenet LLMs, indicating the potential of tuned LLMs for superior generalization ability. ComplexSQL data particularly have been observed to be more benecial as tuning data. We study eective methods to utilize database content and auxiliary information, such as descriptions.We show improvements with the relevant subset of them included in the prompt while irrelevantinformation can harm the performance. For large-scale databases, we introduce a column selection approach that signicantly reduces thelength of the inputs going into the LLMs, while yielding negligible impact to performance (hardcolumn selection). Among the two proposed approaches, program-aided column selection has highercolumn selection accuracy, whereas retrieval-based approach is more cost-eective and controllable.soft column selection, which doesnt exclude unselected columns, has a higher performance than hardcolumn selection, albeit it necessitates LLMs with longer context length.",
  "Text-to-SQL Challenges": "Real-world Text-to-SQL scenarios present a broad spectrum of challenges stemming from the complexityof natural language questions, database structures, inherent SQL intricacies, and data availability. Thesereal-world challenges are often more severe than those encountered in academic benchmarks. Natural language question phrasing: The ambiguity and complexity of natural language questions posechallenges. Input phrases may have multiple interpretations in natural language (e.g. sales in California\"could refer to sales made by people in California or products sold to people in California). They might alsocome with complex sentence structures such as subordinate clauses and relative clauses. The meaning mayalso depend on the surrounding context, making it dicult to derive correct interpretations. In addition,databases and applications come from a wide range of domains, and Text-to-SQL systems need to understanddomain-specic terminology, which can vary greatly depending on the use case. Specic rules, regulations,formulas or calculations related to a particular domain might need to be applied to generate the correct SQL.For example, the question List disease names of patients with proteinuria levels above normal requires LLMsto have domain knowledge proteinuria level above normal means U-PRO >= 30 to solve the problem. Sizes and diversities of databases: Real-world large-scale databases might contain numerous tables andcolumns. The sheer volume of columns can exceed prompt length limits, making including the entire datasetschema impossible. Furthermore, LLMs face diculties in eciently accessing and utilizing information withinlengthy input contexts as discussed in the phenomenon of lost in the middle (Liu et al., 2023b). Databaseschemas often have complex and various structures, including dierences in table names, column names, andtheir relationships. The relationships between tables may not be explicitly dened in the schema, requiringthe system to infer them (by understanding foreign keys). Moreover, database schemas might containambiguities table and column names in a schema may not be informative (e.g. The column name propertyis vague about the feature it denotes, as opposed to size clearly species the type of property) or abbreviatedin a way that is not easily understood (such as cname). Additionally, some tables might contain tens ofsimilar columns (sku1, sku2, ... sku100), potentially leading to confusion for the Text-to-SQL system.",
  ": An example SQL sample corresponding tocomplex arithmetic operations and handcrafted ruleson a complex database": "Besides ambiguities in schemas, database contentsalso have a wide variety of types and formats. Thedata stored in the database can also vary signicantlyin types and format, leading to lengthy and specicclauses (e.g. regular expression, and type casting)to extract variable information. (1) Type: they in-clude scalars, arrays, nested arrays etc. (2) Format.For instance, the year of 2024 might be saved as astring:2024, a number: 2024, or in other formssuch as year2014, or 2014-01-01. Extractingdierent formats of year requires dierent regularexpressions. We show a real-world example in ,where extraction of the proper format of the columnrevenue requires using CASE statements, cast-ing string into numbers, schema interpretation, andregular expressions, because the values of revenue isstored in dierent string formats single values (i.e.i.e. 100) or ranges (i.e. 100-200). Inherent SQL complexity: Certain SQL queriesare complex in nature, marked by the use of multipleSQL keywords, the inclusion of nested sub-queries,a variety of column selections or aggregations, theapplication of conditional statements, and the in-volvement of joins across multiple tables.",
  "Published in Transactions on Machine Learning Research (11/2024)": "NCESDist (text) | schools : NCESSchool (text) | schools : StatusType (text)| schools : County (text) | schools : District (text) | schools : School (text) | schools : Street (text) | schools : StreetAbr (text) | schools :City (text) | schools : Zip (text) | schools : State (text) | schools :MailStreet (text) | schools : MailStrAbr (text) | schools : MailCity (text)| schools : MailZip (text) | schools : MailState (text) | schools : Phone(text) | schools : Ext (text) | schools : Website (text) | schools :OpenDate (time) | schools : ClosedDate (time) | schools : Charter (number)| schools : CharterNum (text) | schools : FundingType (text) | schools :DOC (text) | schools : DOCType (text) | schools : SOC (text) | schools :SOCType (text) | schools : EdOpsCode (text) | schools : EdOpsName (text) |schools : EILCode (text) | schools : EILName (text) | schools : GSoffered (text) | schools : GSserved (text) | schools : Virtual (text) | schools :Magnet (number) | schools : Latitude (number) | schools : Longitude (number) | schools : AdmFName1 (text) | schools : AdmLName1 (text) | schools :AdmEmail1 (text) | schools : AdmFName2 (text) | schools : AdmLName2 (text)| schools : AdmEmail2 (text) | schools : AdmFName3 (text) | schools :AdmLName3 (text) | schools : AdmEmail3 (text) | schools : LastUpdate (time);",
  "Approaches with Deep Neural Networks": "Sequence-to-sequence models Text-to-SQL can be formulated as a sequence-to-sequence modeling prob-lem, with both the database schema and natural language question being converted into a linear inputsequence, and the SQL being the target output sequence. Prior to the recent advances of large languagemodels (LLMs), the approach of ne-tuning Transformer models, such as T5 (Rael et al., 2020), withSQL-specic customizations had been the prevalent approach dominating the state-of-the-art. PICARD(Scholak et al., 2021) introduces a technique that discards invalid beam search candidates during inference,improving the grammatical correctness of the SQL queries. RASAT (Qi et al., 2022) augments transformerarchitecture with relation-aware self-attention which is ecient to incorporate a variety of relational structureswhile also leveraging a pretrained T5 model. RESDSQL(Li et al., 2023b) proposes a ranking-enhancedencoding and skeleton-aware decoding framework to decouple the schema linking (e.g. table or column names)and the skeleton parsing (e.g. keywords). Graph encoders for schema understanding Another line of work is based on employing graph encodersto explicitly model complex relationships within the database schemas and questions. RAT-SQL (Wanget al., 2019) introduces schema encoding and linking, and models the schema and its relationships as agraph. Global-GNN (Bogin et al., 2019) further explores this concept of depicting the intricate structureof a database schema with a graph. SADGA (Cai et al., 2021) employs both contextual and dependencystructures for encoding the question-graph, and utilizes database schema relations in the construction of theschema graph. S2SQL (Hui et al., 2022) incorporates syntactic dependency information into the relationalgraph attention network.",
  "Text-to-SQL with LLMs": "Prompting LLMs Recent advances in LLMs have yielded groundbreaking capabilities (Chowdhery et al.,2022; Achiam et al., 2023) their ability to understand, generate, and reason in unprecedented ways withprompting has amplied their penetration into many real-world tasks. Numerous advanced promptingtechniques have further extended LLMs capability, such as Chain-of-Thought (Wei et al., 2022b), Least-to-Most (Zhou et al., 2022), and others (Chen et al., 2022; Yao et al., 2023; Besta et al., 2023). However,these generic-purpose prompting approaches are observed to fall behind on Text-to-SQL tasks compared toapproaches tailored to Text-to-SQL2 (Rajkumar et al., 2022; Liu et al., 2023a; Li et al., 2023d). To further improve general-purpose prompting methods for Text-to-SQL specically, advanced promptingapproaches tailored to Text-to-SQL task, have been proposed. DIN-SQL (Pourreza & Raei, 2023) exempliesthis by breaking down the Text-to-SQL tasks into sub-tasks: schema linking, classifying SQL by dicultylevel, SQL generation based on SQL diculty, and self-correction3, and etc. CoT-style (Tai et al., 2023) alsoproposes decomposing Text-to-SQL into sub-problems and presents them all at once to LLMs instead ofsolving sub-problems iteratively. SQLPrompt (Sun et al., 2023) enhances LLMs with diverse representationsof database schemas and questions as inputs to encourage diverse SQL generation to improve performancefrom execution-based consistency decoding. Self-debugging (Chen et al., 2023) appends error messages to theprompt and performance multiple rounds of few-shot prompting to self-correct the errors. DAIL-SQL (Gao 1For example, BIRD datasets Li et al. (2023d) contain errors, as we described in error analysis in Sec. 8.72For example, the standard single-pass prompting approach of LLMs, such as with PaLM-2 or GPT-4 on the development(dev) set of the Spider dataset underperforms ne-tuned smaller capacity models, such as Picard Scholak et al. (2021) andRESDSQL (Li et al., 2023a)3Notably, DIN is the rst few-shot prompting that can outperform strong tuning-based alternatives, such as Picard Scholaket al. (2021) and RESDSQL (Li et al., 2023a)",
  "Key Aspects of the SQL-PaLM Framework": "We investigate multiple key aspects of building a Text-to-SQL framework in this paper. Through extensiveexperimental validation and in-depth analyses, we aim to systematically unravel the factors inuencingText-to-SQL performance. Learning perspective pushing adaptation with prompting vs. tuning: Central to our investigationis understanding the intrinsic property of LLMs on tackling the Text-to-SQL task, whereby we explore, withinthe learning paradigms of few-shot prompting and tuning, how LLMs solve Text-to-SQL tasks dierentlyunder dierent learning scenarios, and what factors inuence the nal performance signicantly. Comparedwith few-shot prompting approaches, tuning approaches have been relatively under-explored, partially due tothe prohibitively high computational cost. In this paper, therefore, focus on some pivotal questions for tuning,such as: How does the performance of prompting strategies compare with that of tuning strategies? To whatextent does the performance rely on the foundation models capacity? How well do models generalize acrossdierent datasets, especially when faced with limited training data (considering notable publicly-availabledatasets like Spider and BIRD are not signicant for large model size)? What is the impact of parameter-ecient tuning techniques, like LoRA (Hu et al., 2021), on the training performance? How does tuningdepend on dierent foundation models (e.g. PaLM vs. LLaMA)? Task perspective judiciously selecting input components for Text-to-SQL: The Text-to-SQLtask contains a range of potentially-useful information that can be taken advantage of: (i) database schema:there are table & column names, descriptions (e.g. clarications such as abbreviations), data types (e.g.string and integer), data formats (e.g. explanations of formats stored within the database such as with a rawvalue or an interval), and primary & foreign keys that describe how dierent tables are connected to eachother; (ii) database content values, some database entry values are needed to solve the question4; (iii) naturallanguage questions, whether there are associated hints or formulas, such as the domain knowledge. Each ofthe above information can be quite critical with some of them missing, LLMs cannot produce accurateSQL outputs. On the other hand, in some scenarios, they can add up to be lengthy and contain irrelevantdetails, wiht the potential to distract the LLMs from focus on relevant information and generate the correctSQL outputs. Therefore, within the limited input length, it is crucial to achieve a balance on not missingcritical information, and not providing a signicant amount of irrelevant information. This necessitatesjudicious selection of a subset of features for Text-to-SQL model to produce correct SQLs. Some fundamentalquestions arise: How can we enable LLMs to eectively utilize various forms of available information? Whichinformation sources are most valuable? What are the linear formats to eectively represent the various inputsfor LLMs? Real-world scaling column selection: With the advances in LLMs, numerous challenges associated withText-to-SQL5 can be addressed more eectively, bringing us closer to resolving real-world database challenges.However, one remaining key obstacle is the potentially high number of columns. Real-world databases, suchas those representing the full inventory of large-scale retailers, might contain a large number of attributes6.Directly processing this volume of columns is often infeasible, as the concatenation of column names wouldexceed the prompt length limits of LLMs. This highlights the need for column selection the process ofidentifying a relevant subset of columns from multiple tables. Column selection is related to the broaderprocess of schema linking7, which maps phrases in the natural language question to corresponding columns 4For instance, Question: What is the revenue for shoes? In the database, the column \"product\" contains \"Running shoes\";Without providing database content, such as \"column product contains Running shoes\", the output SQL is likely to be\"SELECT ... WHERE product=shoes\", because \"shoes\" is mentioned in the question. However, the correct SQL is \"SELECT ...where product=Running shoes\"5For instance, public benchmarks, such as Spider6The number of columns can be hundreds or thousands7Major schema linkage approaches (Guo et al., 2019; Wang et al., 2019; Li et al., 2023a) are discussed in Sec. 3.",
  "Problem Formulation": "Text-to-SQL systems transform queries expressed in natural language into SQL programs. Provided a naturallanguage query Q, and an associated database D, SQL outputs are generated such that when executed againstdatabase D, would generate the answer to the original natural language query Q. A database D includes two primary components: the schema (including table and column names) and thecontents (entry values) of D. The schema, represented by S, outlines a databases structure and includes aset of table names T, and a set of column names C. The database content values V are the data values thatpopulate the entries of the tables, adhering to the attributes dened by the database schema.",
  "where v(k)jare vectors of length n(k)row encompassing all the values of the attributes C(k)j. Usually, the number": "of entries n(k)row is signicantly larger than the number of attributes n(k)col. Primary keys K(k)P C(k) are thecolumn(s) that contain values that uniquely identify each row in a table8. Foreign keys K(k)F C(k) arethe column(s) in one table, referring to the primary key in another table. They are used to link multipletables.9 Additionally, databases often include supplementary information for clarication (such as the detaileddescriptions of columns) which help interpreting ambiguous or uninformative column names (as explained inSec. 2). We use fdes(C) to indicate the descriptions for column C. For k-th table, des(k) refer to a collectionof column description:",
  "Lastly, there can be hints H(k), user-specied aids for the question. They could contain denitions or formulaused to construct SQL queries.10": "8For example, the column StudentIDs of the Student table.9For example, consider the table \"Student\" with primary key \"StudentID\", and the table \"BookOrders\" which has two columns:\"OrderID\" and \"Buyer\". The \"Buyer\" column contains a series of StudentIDs representing the individuals who placed the bookorders. In this scenario, \"BookOrders.Buyer\" is the foreign key which points to a primary key \"Student.StudentID\". This way,each row in the \"BookOrders\" table can be associated with a specic student from the student table.10For instance, for the query List the phone numbers of schools with the top 3 SAT excellence rates, the hint is the denitionof the excellence rate, the percentage of take takers with SAT score greater than 1500. \"Excellence rate = NumGE1500 /NumTstTakr\", where column NumGE1500 refers to Number of Test Takers Whose Total SAT Scores Are Greater or Equal to1500 and NumTstTakr refers to Number of Test Takers",
  "Methods": "This paper presents the SQL-PaLM framework (depicted in ), a holistic approach to push Text-to-SQL capabilities using LLMs with both few-shot prompting and instruction-tuning. We rst describe theinput representations (Sec. 6.1) used for both learning paradigms. For few-shot prompting, we propose aprompting approach leveraging execution-based error-ltering-aided consistency decoding (Sec. 6.2). Forinstruction-tuning, we delve deeply into understanding the critical factors in inuencing performance of tuningLLMs, including expanded training data coverage and diversity (Sec. 6.3.1 and Sec. 6.3.2), synthetic dataaugmentation (Sec. 6.3.3), and integrating query-specic database content (Sec. 6.3.4). Using the test-timeselection approach (Sec. 6.3.6), we further enhance accuracy by integrating SQL outputs from variousparadigms, leveraging execution feedback for renement. Furthermore, we address one of the real-worldchallenges of navigating complex databases with a signicant number of tables and columns, presentingeective methods for precise selection of pertinent database components to improve Text-to-SQL performance(Sec. 6.3.5).",
  "n(2)col)|...|KP ; KF ;, (4)": "where T (k)Ndenote the k-th table name represent the j-th column name of the k-th table, and d(k)jindicateits data type (such as number or string). We use the symbol | to represent the boundaries between dierenttables in the schema. Within each table, we use : to separate the table name from its columns, and weindicate each column via the delimiter ,. We integrate the primary keys (Kp) and foreign (Kf) keys todenote relationships between tables. We refer the above schema design as concise prompt, as it conciselypresents the table structure 11. See as an input example for the question given in and a realisticexample in Sec. A.10.1 in Appendix. Besides the database schema, other forms of database content can be benecial. We use database contentvalues V (Q) to match with the tokens in question to clarify the database value (see Sec. 6.3.4). We alsoincorporate column descriptions (des) and hints (H), which provide additional clarication or domain-specicknowledge when applicable. We concatenate them together as:",
  "Input: X (Eq. 6)": "Convert text to SQL Few-shot examples [Schema]: | Countrylanguage: CountryCode (Number), Language (String), ... | Country: Code (Number), Name (String), ...[Primary Keys]: Countrylanguage: CountryCode | Country: Code ...[Foreign Keys]:Countrylanguage: CountryCode is equivalent to Country: Code | ...[Detailed descriptions of tables and columns]: # column descriptionThe column IsOcial in Table Countrylanguage has column descriptions of whether language is ocial language...[Database values that related with questions]: database contentThe column Language in Table Countrylanguage has database values: [English, French]...[Additional Info]: # hints, if applicable[Q]: What are the names of nations where both English and French are ocial languages?[SQL]:",
  "Y = arg maxYPLLM(Y |X),(7)": "where Y are the inferred SQL outputs, X, as described in Sec. 6.1, are the input prompts including databaseschema, other auxiliary information (i.e. hints) if applicable, and the question Q. are the parameters of thepretrained LLM. With few-shot prompting, the formulation is extended to LLMs generating a sequence oftokens conditioned on the provided demonstrations pairs demo = [(X1, Y1), (X2, Y2), ...]:",
  "Y = arg maxYPLLM(Y |demo, X).(8)": "Essentially, in few-shot prompting, the prompts prepend the natural language queries Q with a list ofdemonstrations (inputs, SQL) pairs, and the LLM follows the input to generate answers in an auto-regressiveway. To further enhance performance beyond the standard few-shot prompting, we adapt an execution-basedconsistency decoding method, following (Sun et al., 2023; Ni et al., 2023). This method leverages on theunique benet of coding task, including Text-to-SQL task, where the SQL outputs are executable. Thisserves as a preliminary validation for the generated output, allowing us to identify invalid results more easily.Concretely, our approach involves the following steps:",
  "Step 2: Verify with execution The generated SQL outputs, { Yi}, are subsequently executed using anexecutor E(), which yields the corresponding results denoted as ei = E(Yi)": "Step 3: Aggregate Execution Result. Since multiple SQLs are valid for the same question, we aggregatethe SQL outputs in that give the same execution result. The execution output with errors is guaranteed tobe wrong, and the execution outputs with the most occurrences are more likely to be correct (Wang et al.,2022). We remove programs with invalid execution results to update the LLM generation probability withthe verication probability, and marginalize over SQL outputs with the same execution results. We use thisaggregated probability as the ranking score R:",
  "Model tuning": "Despite the signicant advances achieved with few-shot prompting of LLMs, it remains a formidable challengefor a pretrained LLM to rely solely on its parametric knowledge and prompting to accurately process highly-complex SQL queries (Sec. 2). Such queries often involve sophisticated semantic logic, complex databaseschemas, and database contents with numeric edge cases necessitating extensive clauses 12 for each case(See ). Additionally, the SQL logic may require the use of clauses that were underrepresented in thepretraining corpus 13. To address these more intricate scenarios, this section focuses on tuning, wherein werene the pretrained LLMs to better align with a customized Text-to-SQL distribution. This paper delvesinto crucial training paradigms that inuence the tuning ecacy of LLMs, including expanding the rangeand diversity of training data, leveraging synthetic data, integrating query-specic database content, andoptimizing table and column selection. We then introduce a test-time selection approach that integrates thesediverse training paradigms, aiming to enhance accuracy through the utilization of execution feedback.",
  "Instruction tuning": "To improve the SQL expertise of pretrained LLM, we propose adapting pretrained LLM to generateSQL from the input sequences by tuning the model with Text-to-SQL datasets (Wei et al., 2021). Thetraining data contain a collection of serialized inputs X & corresponding SQL outputs Y pairs, sampled fromthe Text-to-SQL distribution dtrain. The training objective is based on maximizing the log probability ofco-appearance of the training data (X, Y ):",
  "Diversifying tuning data coverage": "Tuning LLMs would require sucient amount of data given their large model size (Kaplan et al., 2020). Thissection focuses on enhancing model tuning through the use of diverse datasets. The rationale is that diversedatasets provide a more comprehensive coverage of SQL knowledge to enrich LLMs. However, a notablechallenge of using more than one datasets is that they can be very dierent, which can lead to a declinein performance due to distribution shifts. This means that a model trained on a particular dataset maynot perform as well on another datasets a common issue for machine learning even for the pre-LLM era.Concretely, Text-to-SQL datasets vary signicantly from dierent perspectives: (i) they encompass a widerange of topics, e.g. from healthcare, retail, and nance etc., each requiring specic knowledge; (ii) the clarifyof the database schema varies, with some containing straightforward table or column names, while othersare vague and demand further exploration or additional descriptions; (iii) the sizes of dataset range widely,setting dierent challenges on schema linkage challenges; and (iv) the quality of database content valuesmight contain variations with some datasets having columns lled with NULL data that need ltering, whileothers not needing that. Given such diversity, it remains an open question how combining multiple trainingdatasets improves tuning performance. Towards this end, we extend Eq. 11 as:",
  "maxE(X,Y )dmix log PLLM(Y |X),(13)": "where dmix is a mixture of |d| datasets: dmix = {di}i=1:|d| and X = f(S, KP , KF , H, Q). We investigatewhether the pretrained LLM, when tuned with a wide range of inputs from various datasets, can learn tounderstand these diverse inputs presented in dierent datasets rather than overtting to specic patterns,and thus generalize better. 12An example is using CASE statements and regular expressions13An example is conditional expression (e.g., CASE, IFF, PARTITION clauses) and WINDOW functions.14We discuss incorporation of database values V and column descriptions des in Sec. 6.3.4 to illustrate the eect of the keyfactors one at a time.",
  "Augmentation with synthetic data": "As previously described, introducing diverse datasets can improve tuning. We further extend this by usingdiverse synthetic SQL data to augment real datasets, especially considering the high cost to obtain real-worlddata. Since LLMs are pretrained with massive datasets, their prior knowledge can be utilized to create newinformation and augment training via synthetic SQL data. For the same natural language questions, there are usually multiple SQLs that are correct with the sameexecution outputs15. Utilizing this, we focus on synthesizing data to incorporate the multiple ground truthSQLs. We start from a Text-to-SQL dataset, for each (natural language question, SQL)-pair in the dataset, wekeep the database schema and natural language question unchanged, and generate new SQLs that are correctbut dierent from the ground truth SQLs. To achieve this, we query LLM with carefully-crafted promptswhich include database schema, natural language question, and the ground truth SQL, and request LLMsto generate a SQL that is dierent from the ground truth and to output a similarity score. The similarityscore indicates how similar the candidate is from the true SQL. Details of prompt design F s are explained inSec. A.5.1 in Appendix. Given the database D, question Q, and original ground truth query SQL, we querythe LLM to generate a dierent SQL output, and estimate its similarity from SQL, formulated as:",
  "(SQL(S), similarity(S)) = LLM o(F s(D, Q, SQL))(14)": "where SQL(S) is the generated SQL output, similarity(S) is the similarity score outputted simultaneouslywith SQL output and F s is synthesis prompting design. LLM o can be any LLMs, but ideally dierent fromthe LLMs that are used for tuning so that they can introduce new information. Synthetic data generation comes with two challenges: accuracy and diversity. Accuracy refers to the generatedSQLs are correct SQL for the natural language query. Diversity refers to the generated SQLs bring newinformation that is dierent from original data. To ensure the generated SQL outputs are correct, after thegeneration we evaluate the SQL 16 and keep the correct one. To ensure the diversity, we only keep the SQLwith similarity score below a threshold.",
  "Integration of query-specic database content": "Incorporating database content can be crucial for Text-to-SQL performance, particularly when naturallanguage questions refer to specic data values dierent from table or column names 18. In some scenarios,without access to the database content, it would be infeasible to formulate accurate SQL queries based solelyon the database schema and question, even for humans19. Furthermore, when multiple column names seemrelevant to a question, database values containing the keywords from the question can help identify theappropriate columns20. Overall, access to database content can be critical for improving Text-to-SQL. 15For example, SQL1 = ... order by score DESC LIMIT 1 and SQL2 = ... WHERE score=max(score) are equivalent16The result of the generated SQL match the result of the ground truth SQL17For example, modifying new natural language questions of database schema lead to the situation where original groundtruth SQL cannot be used to evaluate synthetic SQLs, as the natural questions have been changed18For example, a user might ask, \"What is the population in Santa Clara?\" However, if the database only has an entry for\"Santa Clara County,\" the correct SQL query should be SELECT .. WHERE county = \"Santa Clara County\", not WHEREcounty = \"Santa Clara\".19As they would not know exact format used in the database.20For example, if the question is \"Please list schools in Fresno County Oce of Education?\", and the database has columnslike District\", District Name\", and dname\", knowing that only District Name\" has database values Fresno County Oce ofEducation\" indicates that this District Name is the column to use.",
  "k = 1 : nT; r = 1 : n(k)row; j = 1 : n(k)col[: topK],(17)": "where v(k)rjis database content value of rth-row jth-column entry of kth-table and Fm is the matchingalgorithm. Here, we use Fm as the longest contiguous matching subsequence approach (Cormen et al., 2022),as it allows us to accurately extract the exact values stored in the database. This precision is particularlyimportant for some SQL queries21. Finally, the total matches for the entire question Q is determined byaggregating all the matches of individual keywords:",
  "V (Q) = {V (wi)|i = 1 : |Q|}.(18)": "Additionally, Fm can also be instantiated using LLM inference (querying LLM with prompt and asking\"whether wi and v(k)rj match\") or embedding similarity (e.g. the cosine distance between the embedding of wiand v(k)rj above a threshold is a match). We choose to use the above proposed fuzzy string matching approachas it is cost-eective and fast. We leave more investigations on Fm to future work.",
  "maxE(X,Y ) logPLLM(Y |X, V (Q))P(V (Q)|D, Q)(19)": "with the serialized input X = f(S, KP , KF , H, Q). See a demo example in (basic prompt + [Databasevalues that related with questions])22 and a real example in Sec. A.11 in Appendix. V (Q) is database contentrelevant to Q. To enable eective training, we break down Eq. (19) into two stages. First, extracting relevantdatabase content associated with the natural language question V (Q). Second, with the extracted databasecontent, the LLMs are trained to generate output SQL programs Y using both the input sequence X and therelevant database content. This approach tailors the training process to better reect realistic scenarios whenLLMs consider specic database entries relevant to the users query. Consequently, the training objective isformulated as:",
  "Y |X, V (Q).(20)": "21For example, being able to distinguish subtle dierences Santa Clara vs Santa Clara County; apple vs apples22For databases with column names without parentheses like those in Spider dataset Yu et al. (2018), to incorporate databasecontent into X, we append the identied database values to their respective column names in the input sequences schema,separated by a delimiter (). This approach aligns with (Qi et al., 2022; Xie et al., 2022). For instance, the database schemais represented as T1 : C1(V1), C2, C3(V3) . . ., and it indicates that only columns C1 and C3 contain relevant database content,while C2 does not. For datasets with column names that include parentheses, like those in BIRD dataset (Li et al., 2023d), weadd the relevant database content separately, clearly specifying the values corresponding to which columns in particular tables,to avoid confusion caused by the delimiter ().",
  "Table and column selection": "Handling real-world datasets poses a signicant challenge to Text-to-SQL due to the large number of tablesand columns. This challenges can arise when including the entire schema within the prompt limit is infeasible.Even when the schema ts, the increased number of columns adds complexity to the reasoning problem LLMs struggle in scenarios resembling nding a needle in a haystack (Liu et al., 2023b). Thus, carefulselection of relevant tables and columns is crucial for improving Text-to-SQL performance (Lei et al., 2020a). Column selection is to select a subset of columns that are relevant for a given natural language question,so column selection Zsel(Q) is a function of question Q. To model Text-to-SQL with column selection, weformulate the problem as modeling the joint probability of the conditional probability of column selectionZsel(Q) given the input sequence X, and the conditional probability of generated SQL program Y given theinput sequence X and column selection Zsel(Q):",
  "max,E(X,Y ) log PLLMY |X, Zsel(Q)PZsel(Q)|X,(21)": "where represents the parameters for the column selection model. Due to the prohibitive computationalchallenges of joint modeling of LLMs, we instead digest the objective into two steps: rst, modeling theselection of columns; and then, integrating these selected columns into the Text-to-SQL modeling process. We start from inferring column selection. Text-to-SQL datasets typically do not explicitly provide the groundtruth for the relevant columns Zsel(Q). We propose extraction of this information from the true SQL queriesY . The selected columns are the columns that are referenced in the true SQL query. Concretely, for thek-th table in the database, the selected columns are represented as:",
  "where T (k)Nis table name of k-th table. We consider two approaches for column selection:": "Retrieval-based column selection: Retrieval-augmented generation has proven to be an eective andecient method for handling large contexts in generative tasks. We employ a similar approach for theText-to-SQL task, utilizing a schema retriever based on nearest neighbor search. Given a natural languagequery, we identify the closest columns in the semantic space. We opt to retrieve columns instead of tables toachieve a more rened and granular selection. Once the columns are identied, we group them accordingto their respective tables to construct the selected schema. The retrieval corpus is dened as the union ofall table columns. The method initiates by calculating embedding representations for both the query andcolumns using a pretrained embedding model denoted as E. Specically, we represent the query embeddingas QE = E(Q) and the embedding of j-th column as C(k)Ej = E(C(k)j). Subsequently, the column selection",
  "common distinct values. This text serves as the input to the embedding model. The specic templates andillustrative examples are presented in Appendix A.6.1": "Retrieval-based approach can be parallelized with tensor operations to eciently scale to a large number oftables and columns with low cost and latency. It also oers controllability via the hyper-parameter topK,which can adjust recall. It can eectively reduce the risk of false negatives. Program-aided column selection: Solving problems with LLM using coding representation has demon-strated impressive results on a variety of tasks (Gao et al., 2023b; Mishra et al., 2023). This is because codingoers greater precision than natural language descriptions and it bypasses the ambiguities inherent in naturallanguage. Program-aided column selection is an approach to infer column selection using preliminary SQLs.Specically, we use initial LLMs (denoted as LLMpre) to generate a preliminary SQL query Y .",
  "(27)": "To ease the matching process, both the preliminary SQLand schema are normalized to lowercase. In practice, wepinpoint selected tables by identifying elements in theSQL following FROM or JOIN keywords that matchtable names in the schema. Selected columns are thenidentied by locating column names that appear both inthe SQL and the schema of the chosen tables. See theoutput of program-aided column selection in . The initial models used to generate preliminary SQLs can be standard Text-to-SQL framework to achievebetter performance, or some less capable models due to various constraints. For example, when dealing withlarge datasets with many columns, in some scenarios, the prompt length limit can only t column names,leaving no space for auxiliary information, such as data types and database content, or descriptions. Usingthese limited inputs (only the schema), we can generate preliminary SQL queries for column selection. Thenon the selected columns, we can apply complete prompt (schema plus auxiliary information) to obtain moreaccurate SQL. Another scenario involves prioritizing the reduction of computational costs and latency, wherea smaller initial language model may be employed. Although preliminary SQLs generated from the initialmodel may not be highly accurate, they can be eective for generating column selection because columnselection requires less details than Text-to-SQL task. Program-aided column selection has the following advantages: The number of columns selected by thisapproach is low as there are limited number of columns referenced in preliminary SQL queries. So this oftenleads to high precise of column selection. Additionally and importantly, program-aided column selectionfosters a mutually reinforcing cycle enhanced SQL accuracy improves column selection ecacy, which,in turn, increases the accuracy of future SQL queries. This iterative enhancement process can lead toprogressively higher levels of accuracy.",
  "X = f(S[Zsel(Q)], KP , KF , H, V (Q), Q).(29)": "This approach has the advantage of considerably shortening the length of the data schema byremoving irrelevant columns, which in turn increases the chance that LLMs concentrate on morecritical information. Additionally, it also acts as an essential preprocessing step that facilitates theapplication of Text-to-SQL when the prompt length is insucient to accommodate the full schema.However, inaccuracies in selecting columns can lead to certain errors in the Text-to-SQL task. Column selection can be utilized in both few-shot prompting and tuning setups. For prompting, we integratingcolumn selection into the prompt and follow the procedures as outlined in Sec. 6.2; For tuning, columnselection is applied to the inputs and follows procedures in Sec. 6.3.1.",
  "Test-time renement via execution-based selection": "In previous sections (from .3.1 to .3.5), we have outlined various training paradigms.Each section focuses on a unique facet of Text-to-SQL, resulting in the generated SQL that exhibit distinctadvantages. Through empirical analysis, we observe that these produced SQL have diverse accuracy coverage the questions correctly answered by one training paradigm often dier substantially from those addressed byothers. This diversity suggests that selecting the appropriate SQL can be a viable strategy for integrationof multiple training paradigms. To this end, we introduce an approach called test-time renement viaexecution-based selection to identify the correct SQL at test time by analyzing execution outcomes. Afundamental advantage of Text-to-SQL is SQL programs are executable. If a SQL program leads to an invalidexecution, such as error messages, the SQL can immediately be deemed incorrect. However, a valid executionoutcome does not guarantee the SQL is correct. To identify correct SQL, we execute the generated SQLoutputs for each question across multiple training paradigms and select the SQL that, while producing validresults, has the execution outcomes supported by the majority of the paradigms. This approach is detailed inAlgorithm 1, providing a systematic method for integrating multiple training paradigms to improve SQLquery generation accuracy. Similar with execution-based consistency decoding for prompting approach inSec. 6.2, we consider majority of the execution outcome as a judgement for good SQL. The dierence betweenthe two is the candidates in Sec. 6.2 come from sampling from the same setup multiple times, whereas herecandidates are come from dierent training paradigms. An alternative approach involves combining dierent input congurations in previous sections into a singletraining experiment. This method entails integrating various factors, such as mixed training data, syntheticdata, database content, and column selection, into the inputs for a single experiment. However, unfortu-nately, the results of such experiment reveal that merging these components does not result in performanceimprovements over using them individually. This suggests that LLMs may struggle to eectively process andunderstand all the provided information simultaneously during tuning.",
  "Tasks and datasets": "We consider publicly-available large-scale Text-to-SQL benchmarks. Spider (Yu et al., 2018) contains 7000training samples across 166 databases and 1034 evaluation samples (Dev split) across 20 databases from avariety of domains. Spider-SYN (Gan et al., 2021a) is a complex variant of the Spider dev split, createdthrough the manual replacement of synonym substitutions in natural language queries. Spider-realistic",
  ": end for": "(Deng et al., 2020) samples 508 text-SQL pairs from Spider dev split removing explicit mentions of columnnames in natural language queries. Spider-DK (Gan et al., 2021b) samples 535 question-SQL pairs on 10databases from Spider dev split and incorporates domain knowledge to them. BIRD (Li et al., 2023d) is acomprehensive dataset containing 9428 question-SQL pairs for train split and 1534 pairs for dev split, across95 databases totalling a size of 33.4 GB. It covers a broad range of over 37 domains, including nance, sports,healthcare, and education. Uniquely, BIRD incorporates four types of external knowledge sources (numericreasoning, domain-specic information, synonyms, and value illustration) to enhance the accuracy of SQLquery generation. Compared with Spider, BIRD SQLs are typically more complex because of longer SQL,more keywords, more JOINs, and so on. BIRD also contains more challenging databases more databaseentries and larger number of tables and columns. Statistics of the number of tables and columns of BIRDare shown in Appendix A.7. Note that BIRD datasets remove the errors on September of 2023, however weconducted all our experiments on previous BIRD version before the error correction. Our performance couldbe higher with the latest version.",
  "Models": "PaLM-2 is a Transformer-based model trained using a mixture of objectives similar to UL2 (Tay et al.,2022), which is an improved version of its predecessor PaLM (Chowdhery et al., 2022) by eciently applyingcompute-optimal scaling, improved training dataset mixture, improved model architecture and objective. ThePaLM-2 used here is a Unicorn variant ne-tuned on a collection of improved datasets mixture phrased asinstructions following (Wei et al., 2021; Chung et al., 2022).",
  "Experiments": "For few-shot prompting, we use Spider datasets. For each question, we sample PaLM-2 32 times withtemperature of 0.5. The inputs of the model includes database schema, data type, primary keys, foreign keys,database content, and the question. For ne-tuning, we choose more challenging dataset BIRD. The inputsare described in each experiment. We train until convergence, and the number of steps is no more than 10Ksteps. LoRA netuning: Following Hu et al. (2021), we incorporate trainable linear low-rank modules into thequery and value projections of each self-attention layer. We set the rank of LoRA to 32, learning rate to 1e-4,and the model architecture to Gecko PaLM model.",
  "Baselines": "We list several relevant baseline methods in this section. Fine-tuning baselines: PICARD (Scholak et al.,2021) employs incremental parsing to constrain auto-regressive decoding. RASAT (Qi et al., 2022) is atransformer model that integrates relation-aware self-attention and constrained auto-regressive decoders.RESDSQL (Li et al., 2023a) decouples schema linking and skeleton parsing using a ranking-enhancedencoding and skeleton-aware decoding framework. In-context learning baselines: (Rajkumar et al., 2022)comprehensively evaluates the Text-to-SQL ability of CodeX and GPT3, while (Liu et al., 2023a) conducts athorough evaluation on ChatGPT. DIN (Pourreza & Raei, 2023) decomposes the Text-to-SQL tasks intosub-tasks: schema linking, query classication and decomposition, SQL generation, and self-correction; thenperform few-shot prompting with GPT-4. DIN only provides test-suite (TS) evaluation results, so we runexecution accuracy (EX) evaluation with their provided SQL outputs. Self-debugging (Chen et al., 2023)appends error messages to the prompt and performance multiple rounds of few-shot prompting to self-correctthe errors. Self-debugging only reports execution accuracy (EX). DAIL-SQL (Gao et al., 2023a) providesa systematic investigation on prompt designs, including question representation and example selection onfew-shot prompting and ne-tuning paradigm.",
  "Evaluation": "Text-to-SQL evaluation: We consider the two commonly-used evaluation metrics: execution accuracy(EX) and test-suite accuracy (TS) (Zhong et al., 2020). EX consists of one test measuring whether SQLexecution outcome matches that of ground-truth. TS consists of multiple EX tests measuring whetherthe SQL passes all of the EX tests, generated by augmentation of the database. Since TS requires passingof more tests, we consider TS as a more reliable evaluation metric. Note that exact match evaluation isnot performed, as multiple correct SQLs exist for single query. For Spider dataset, we follow the ocialevaluation protocol of Spider23. For BIRD dataset, we follow BIRD ocial evaluation24. BIRD does nothave augmentation of test datasets, so BIRD does not have TS evaluation. Additionally, we provide ValidEciency Score (VES) (Li et al., 2023c), which is designed to measure the eciency of valid SQLs generatedby models for BIRD dataset. VES metric considers both the eciency and accuracy of execution results. Column selection evaluation: To evaluate the accuracy for retrieval of columns and tables, we reportrecall, precision, and F1. We compute these metrics and report the averaged metrics across all samples.recall is the proportion of relevant columns correctly identied, e.g. identied relevant columns/true relevantcolumns, whereas precision is the proportion of the selected columns that is relevant, e.g. identied relevantcolumns / identied columns. Finally, F1 is dened as (2 precision recall)/(precision + recall).",
  "Results": "We present the performance of our proposed framework, SQL-PaLM, in both few-shot prompting and tuningsettings. For few-shot prompting, we focus on the Spider benchmark, which is recognized for its high-qualityassessments, including both execution accuracy and test suite accuracy. For tuning, we focus on theBIRD benchmark, known for its complex SQL and sophisticated database schema, to better dierentiatevarious methods. Both benchmarks are assessed on their respective dev split, which are publicly accessible,in contrast to their private test split25. For intermediate results or ablation studies, we select representativeand high-performing methods as baselines for comparison. BIRD and SPIDER prompt design are the sameformat.",
  "Ablation studies": "shows the ecacy of Few-shot SQL-PaLM on the Spider dev set. We utilize the concise prompt designwith four demonstrations due to the better performance compared against other prompt designs (AppendixSec. A.1). We conduct ablation studies showing the roles of execution-based consistency decoding and errorltering played in enhancing model performance. The results indicate omitting either component results in aperformance degradation of 4.9% and 3.5% respectively, highlighting the substantial contributions to theoverall performance. : Few-shot prompting on Spider dev split. We present both execution accuracy (EX) and testsuite accuracy (TS). Ablation studies are provided on removing execution-based consistency decoding orerror ltering respectively.",
  "Performance for dierent SQL diculty levels": "In our analysis, we evaluate the ecacy of SQL-PaLM against a spectrum of SQL diculty levels, which arecategorized based on several factors, including the number of SQL keywords used, the presence of nested sub-queries, and the application of column selections or aggregations. The results in highlight SQL-PaLMperformance in comparison with standard few-shot prompting approach using GPT-4 and CodeX-Davinci,as well as the advanced prompting approach DIN-SQL (Pourreza & Raei, 2023). Our ndings reveal thatSQL-PaLM consistently surpasses the alternative approaches across all evaluated diculty levels. : Test-suite accuracy on Spider dev split with SQL outputs being categorized by dicultylevels. The rst four rows are taken from (Pourreza & Raei, 2023), and specically the rst two rows arebased on standard few-shot prompting.",
  "Robustness evaluations": "The Text-to-SQL models frequently encounter challenges in robustness, such as translating questions intoSQL queries when the terminology diers from the database schema or when specialized domain knowledgeis required. To address these, variants of the Spider dataset have been created, as detailed in the in Appendix. These include the \"Spider-Syn\" and \"Spider-Realistic\" variants, which alter natural languagequeries by substituting direct schema references with synonyms or by excluding explicit mentions altogether,respectively. Additionally, the \"Spider-DK\" variant incorporates domain-specic knowledge into the schema.To determine if Few-shot SQL-PaLM is capable of overcoming such robustness challenges, we evaluate itsperformance on these Spider variants.",
  "Improving few-shot prompting with column-selection": "demonstrates improved performance of SQL-PaLM on BIRD with column-selection enhanced few-shotprompting, which applies the soft column selection approach (Sec. 6.3.5) to the few-shot prompting (Sec. 6.2).We use the BIRD dataset instead of Spider, as it has larger database schema, where column selection can yieldlarger impact. We opt for the verbose prompt in our experiments due to its superior performance (Table A.2in Appendix). The results show that compared with few-shot prompting baseline27. The proposed approach,column-selection enhanced few-shot prompting, improves performance 2%. To further understand thepotential, we also investigate the upper-bond of the proposed method, where we apply the ground truthcolumn selection. In this setup, we observe an improvement of 5.7%, which provides a motivation forfurther improving column selection performance for better Text-to-SQL performance.",
  "In what scenarios the improvements are observed to be more signicant?On more challenging datasets": "26For instance, ne-tuning a T5 model27The preliminary SQLs used in column-selection enhanced few-shot prompting is the baseline shown in the rst line of. The input sequences for all the experiments in this table are formed of database schema, data type, primary keys,foreign keys, and the natural language question. Database content is not included.",
  "EX33.96%55.8%": "in Appendix A.4 shows the results with tuning open-source models LLaMA7B, LLaMA13B, andLLaMA33B on Spider using the best input representation as reported in Gao et al. (2023a)29. Compared withPaLM-2 tuning results in , LLaMAs ne-tuning results are about 10% lower, that is attributed mainlyto the capability of base foundation models. Overall, despite the tuning involving updating parameters,foundation models with larger sizes and better reasoning abilities are observed to be benecial for tuningText-to-SQL.",
  "Comparisons with parameter ecient tuning": "How is Text-to-SQL performance with parameter ecient tuning compared with full supervised tuning(SFT)?Since train data is limited, is LoRA better than SFT?SFT is observed to be better even with limited tuning data. Next question is whether tuning benets from parameter ecient tuning such as LoRA, as we do not havesignicant amount of training data. presents results on tuning a PaLM-2 Gecko using full supervisedtuning versus LoRA. The results reveal that full model tuning has clear advantages over LoRA even in the",
  "What kinds of tuning data might be more helpful?Complex SQLs are observed to be more useful": "Text-to-SQL benchmarks can be quite dierent from each other. We explore whether training on moredatasets, despite of the diversity30, can help with tuning performance. We tune LLMs on combination ofSpider and BIRD datasets, and evaluate their performance on each. shows that when evaluatingon the BIRD dev split, a model trained on both BIRD and Spider outperforms a model trained solely onBIRD. Similarly, illustrates the improved performance on the Spider dev split when trained on bothdatasets, compared to training only on Spider. The results suggest that tuning LLMs on various datasetsbenets tuning performance, and the model after tuning is more robust to distribution shifts, indicating thetuned models are not over-tting on train dataset. BIRD contains more complex SQL queries compared toSpider. We observe a more signicant performance improvement on Spider when BIRD data are incorporated,compared to the improvement seen on BIRD when Spider data are incorporated. This implies that introducingcomplex SQL queries into training can yield larger benets compared with less complex SQL samples. BIRDand SPIDER prompt design are the same format.",
  "How does the database content help tuning?It claries mismatches in questions and database": "We explore whether introducing question-specic database content benets tuning performance. presents the improvements achieved by incorporating database content31. The results indicate more than 3%accuracy improvement when testing on the BIRD dev split, highlighting the positive impact of incorporatingdatabase content. We further provide two case studies to illustrate why database content would help. One scenario arises whenthere is disparity between the words used in natural language query and the words used in the database,as exemplied in . As another example, illustrates how database content can serve as valuable 30Other than the dierence in SQLs or database. The provided information can be dierent. BIRD has hints, columndescriptions, etc; Spider has none of them31We train on both BIRD and Spider datasets, as they bring good performance described in Sec. 8.2.4",
  "Without database content55.15%With database content58.80% ( 3.65%)": "cues for LLMs to identify relevant columns when multiple columns seem relevant for the question. This is asituation when both LLMs and human experts have a dicult time to deciding which columns to use. Thedatabase values presented in Figs. 6 and 7 encompass all the keywords in the questions, not limited to thespecic keywords discussed in this context.",
  "Case Study 1": "Question:What is the highest eligible free rate for K-12 students in the schools in Alameda County?True SQL:SELECT FRPM Count (K-12) / Enrollment (K-12)FROM frpm WHERE County Name = AlamedaORDER BY (CAST(FRPM Count (K-12) AS REAL) / Enrollment (K-12)) DESC LIMIT 1;Without database contentInferred SQL:SELECT FRPM Count (K-12) / Enrollment (K-12)FROM frpm WHERE County Name = Alameda CountyORDER BY (CAST(FRPM Count (K-12) AS REAL) / Enrollment (K-12)) DESC LIMIT 1;Error reason:Question has \"Alameda County\", whereas database has values \"Alameda\" (no \"County\")",
  "With database contentExtracted database content values:{table: {column: [matched values]}}": "Table frpm:County Name: [Alameda],Table satscores:cname: [Alameda],Table schools:AdmFName1: [Rae],AdmLName1: [Free],City: [Alameda],County: [Alameda],GSoered: [K-12],GSserved: [K-12],MailCity: [Alameda],Inferred SQL:SELECT FRPM Count (K-12) / Enrollment (K-12)FROM frpm WHERE County Name = AlamedaORDER BY (CAST(FRPM Count (K-12) AS REAL) / Enrollment (K-12)) DESC LIMIT 1; : Case study 1: Terminology used in the question is dierent from that saved in the database.Consider the instance of the keyword \"Alameda County\" found in the natural language query: \"What isthe highest eligible free rate for K-12 students in the schools in Alameda County?\" While the question usesAlameda County, in the database, this information is stored as \"Alameda\" without the word \"County\". IfLLMs have access only to the original question without database content, the resulted SQL query is likely tocontain the same keywords from the natural language, leading to an incorrect answer. Following Sec. 6.3.4,we extract database content that is relevant to the question and the output is presented in (e.g. thecolumn County Name containing Alameda).",
  "Improving tuning with synthetic data": "We present the impact of synthetic data augmentation on the Text-to-SQL task. presents theresults of the inclusion of synthetic data into the original training data (Spider and BIRD), which leads tothe performance increase of 1.3% on the BIRD dev split. This improvement underscores the eectiveness ofsynthetic data in enhancing overall performance.",
  "Case Study 2": "Question: Please list the zip code of all the charter schools in Fresno County Oce of Education.True SQL:SELECT T2.Zip FROM frpm AS T1 INNER JOIN schools AS T2 ON T1.CDSCode = T2.CDSCodeWHERE T1.District Name = Fresno County Oce of EducationAND T1.Charter School (Y/N) = 1Without database contentInferred SQL:SELECT T1.Zip FROM schools AS T1 INNER JOIN frpm AS T2 ON T1.CDsCode = T2.CDsCodeWHERE T2.County Name = Fresno County Oce of Education AND T2.Charter School (Y/N) = 1Error Reason:Multiple columns may contain keywords ( Fresno County Oce of Education);LLMs dont know which columns to use (District Name vs County Name)With database contentExtracted database content values: {table: {column: [matched values]}}Table frpm:County Name: [Fresno],District Name : [Fresno County Ofce of Education],District Type: [County Oce of Education (COE)],Table satscores:cname: [Fresno],dname: [Fresno County Oce of Education],Table schools:AdmLName1: [Coe],City: [Fresno],County: [Fresno],DOCType: [County Oce of Education (COE)],District: [Fresno County Oce of Education, Colusa County Oce of Education],MailCity: [Fresno],Inferred SQL:SELECT T2.Zip FROM frpm AS T1 INNER JOIN schools AS T2 ON T1.CDSCode = T2.CDSCodeWHERE T1.District Name = Fresno County Oce of EducationAND T1.Charter School (Y/N) = 1 : Case study 2: The association of keywords with a specic column is unclear without databasecontent. Consider the keyword \"Fresno County Oce of Education\" in the question \"Please list the zip codeof all the charter schools in Fresno County Oce of Education.\". Without utilizing the database content,the LLMs might erroneously select the wrong column, such as \"County name.\" However, with the inclusionof database content (assuming the column \"District Name\" contains the relevant keywords Fresno CountyOce of Education), the LLMs learn District Name is the columns to use.",
  "Spider + BIRD55.15%Spider + BIRD + LLM-generated synthetic data56.45% ( 1.3%)": "To encourage generation of a correct, distinct queries, we prompt the LLM to generate up to three queries,followed by removing SQL that fails ocial evaluation or with a similarity score (Eq. 6.3.3) greater than0.9. We choose to augment BIRD datasets, instead of Spider, because Spider contains simpler queries thanBIRD, resulting in reduced exibility in generating diverse queries from the original SQL. We use GPT-4in synthetic generation, as selecting the LLMs for synthetic data dierently from the LLMs used in tuningpotentially can bring new information. We further examine the generated SQLs and their similarity score. Most of the generated SQL outputs do notdeviate signicantly from ground truth, as indicated by the similarity score distribution (see statistics shownin and in Appendix). Among these generated SQL outputs, 81.4% are correct, validatedby ocial evaluation. After removing similar SQLs (with similarity score > 0.9), 78.8% of the generatedqueries remains for training, which are considered as both diverse and precise, ( in Appendix). A fewexamples of the LLM generated synthetic SQL rewrites are provided in . It can be observed that theLLM performs well at the given task by diversifying the ground truth queries and augmenting the dataset ina useful way.",
  "Synthetic Data Examples": "Example 1:QuestionPlease name any three restaurants that have an unidentied region.Ground-truthSELECT T2.label FROM location AS T1 INNER JOIN generalinfo AS T2 ON T1.id_restaurant =T2.id_restaurant INNER JOIN geographic AS T3 ON T2.city = T3.city WHERE T3.region = unknown LIMIT 3GeneratedQuery-1SELECT gi.label FROM generalinfo gi, geographic g WHERE gi.city = g.city AND g.region = unknownLIMIT 3Query-2SELECT label FROM generalinfo WHERE id_restaurant IN (SELECT id_restaurant FROM location WHERE cityIN (SELECT city FROM geographic WHERE region = unknown)) LIMIT 3Query-3SELECT label FROM generalinfo WHERE city IN (SELECT city FROM geographic WHERE region = unknown)LIMIT 3CommentThe generated queries 1, 2, 3 have similarities 0.8, 0.7, 0.6 respectively. In this example, the LLM has alsoidentied redundant table usage from the ground truth and removed it from the generated query. Example 2:QuestionPlease give all the list prices of the product LL Fork.Ground-truthSELECT T2.ListPrice FROM Product AS T1 INNER JOIN ProductListPriceHistory AS T2 ON T1.ProductID =T2.ProductID WHERE T1.Name = LL ForkGeneratedQuery-1SELECT ProductListPriceHistory.ListPrice FROM Product JOIN ProductListPriceHistory ONProduct.ProductID = ProductListPriceHistory.ProductID WHERE Product.Name = LL ForkQuery-2SELECT plph.ListPrice FROM Product p, ProductListPriceHistory plph WHERE p.ProductID = plph.ProductIDAND p.Name = LL Fork) LIMIT 3Query-3SELECT ListPrice FROM ProductListPriceHistory WHERE ProductID IN (SELECT ProductID FROM Product WHEREName = LL Fork)CommentThe generated queries 1, 2, 3 have similarities 0.95, 0.85, 0.75 respectively. Query-1 is merely an alias changefrom the original query and hence queries with higher similarity (closer to 1) are not useful for augmentingthe dataset.",
  "Tuning with table and column selection": "Table & column selection plays an important role for Text-to-SQL scalability and accuracy, as covered inSec. 6.3.5 for database schema with high number of columns, it becomes vital to distill them down to apertinent subset for Text-to-SQL especially when the schema size exceeds the LLMs prompt length limit.This process is also crucial even for database schemas that can be represented within prompt limit, as itfacilitates LLMs to focus on important information. Regarding the selection of columns, a high recall rate (the proportion of relevant columns correctly identied)- is crucial, to ensure that no crucial columns are missed. With recall meets a satisfactory level, we alsoaim to enhance the precision - the proportion of the selected columns that is relevant, since it is benecialto exclude numerous irrelevant columns. In this section, we discuss the outcomes of using retrieval-based and program-aided column selection techniques.We begin by evaluating the accuracy of column selection achieved by each method, then assess how theseapproaches inuence the overall eectiveness of Text-to-SQL conversions. Lastly, we explore the advantagesand limitations associated with both strategies. Retrieval-based column selection: shows the performance of retrieval-based column selection,with top 10 and 25 columns selected based on the retrieval ranking scores. The recall rates for selectingthe top 10 and 25 columns are notably high at 81.52% and 92.93% respectively. Nonetheless, the precisionis comparatively lower, suggesting that while retrieval-based column selection has a good coverage of truecolumns, it also incorporates superuous columns. presents the performance of end-to-end Text-to-SQL on the BIRD dev split, using both soft andhard column selection. The soft column selection method surpasses the baseline performance, which lackscolumn selection, demonstrating the eectiveness of soft column selection. On the other hand, hard columnselection yields results worse than the baseline. This decrease in performance is likely due to hard columnselections incorrect exclusion of relevant columns from the database schema. For instance, with the top10 selection, approximately 20% of columns (1 81.52%) are missing from the inputs, while soft columnselection is more eective retaining the entire schema information while also enriching the selected columnswith additional column description information.",
  "Baseline58.8%--+ Hard column selection-52.48%56.39%+ Soft column selection-58.93%59.13%": "Program-aided column selection: presents the ecacy of the program-aided approach forcolumn and table selection. Overall, we observe impressive performance of this approach for selecting relevantcolumns, with recall, precision, and F1 are all more than 90%. In comparison to the retrieval-based columnselection, the program-aided column selection has a substantially higher precision, indicating fewer irrelevantcolumns are selected. Additionally, program-aided method depends on preliminary SQL more accuratepreliminary SQL prediction leads to more precise column and table choices, as detailed in in theAppendix.",
  "Average count1.885.24Recall94.6490.66Precision96.1492.60F195.3991.62": "illustrates the comprehensive impact of dierent ways to integrate chosen columns into modelne-tuning. We present two approaches: hard column selection and soft column selection, to distinguishincluding only selected schema versus full schema into prompt. The way to identify the selected columnis program-aided approach, as it gives optimal results. Hard column selection yields inferior outcomescompared to the baseline, likely because its recall rate is 90%a seemingly high number that nonethelessresults in the omission of 10% of columns. Conversely, soft column selection avoids this shortcoming andoutperforms the baseline.",
  "MethodEX": "Baseline58.80%+ Full column descriptions54.69%( 4.11%)+ Soft column selection (retrieval-based)59.13%+ Soft column selection (program-aided)59.19%+ Ground truth column selection (oracle)62.06%( 3.26%) Additionally, since soft column selectionincorporates the column description of asubset of the columns, what would theperformance be if we incorporate the de-scriptions of all columns (2nd line)? Weconduct an ablation study to include fullcolumn descriptions (See an input exam-ple A.11.1). For the prompts that ex-ceed the input length, we cut them tot the input length. The results revealthat introducing full column descriptionsactually decreases the performance compared with baseline by 4.11%. The reason might be due to full columndescriptions yielding too lengthy inputs that distract LLM from focusing on important information such asdatabase schema and important information might get truncated. Comparing retrieval-based and program-aided column selection: The program-aided approachoutperforms retrieval-based approach in the accuracy of column selection, evidenced by its high F1 scoreand precision ( and ). It achieves comparable recall with on average of 5 selected columns,unlike the retrieval-based approach that uses 25 to achieve 90% recall, demonstrating its eciency. However,the overall Text-to-SQL performance of retrieval-based and program-aided column selection methods arecomparable.Despite the program-aided approach showing signicantly better performance in columnselection, its end-to-end performance does not reect a similar improvement. This seeming contradiction canbe explained with the recall of the program-aided method being comparable to the top 25 retrieval approach(90 vs 92) and the recall being more critical for Text-to-SQL performance. Additionally, there is a mismatchbetween accuracy of column selection on train and dev splits. The program-aided approach, which trainsan initial model to produce preliminary SQL outputs, results in higher accuracy in column selection of thetrain split compared to the dev split. This accuracy disparity could hinder the program-aided method fromachieving its highest potential performance. The retrieval-based method stands out for its computational eciency and cost savings, as it avoids theneed for querying expensive LLMs. This approach also allows for easy adjustment of recall by modifying thenumber of retrieved candidates (topK). Furthermore, in cases of extremely large datasets where the schemasize makes generating even preliminary SQL infeasible due to prompt length constraints, the retrieval-basedapproach is the only practical solution. While this scenario might not occur in academic benchmarks, it is acommon challenge in real-world applications. Comparing hard and soft column selection:Compared with soft column selection, hard column selection leads to performance reduction. The reduction,3% for retrieval-based approach (top 25) and 1% for program-aided approach, might be a reasonable trade-owhen we consider the amount of input information that has been reduced and the cost has been saved.",
  "Baseline1085.66100%Program-aided286.3126.37%Retrieval-based454.9741.91%": "From column level, BIRD dev split contains on average of76 columns ( in Appendix), and program-aidedapproach selects on average 5.24 columns, therefore, thereare only 5.24/76 = 6.8% of the total columns used forprogram-aided column selection at the cost of 1% of accu-racy loss. Similarly, for retrieval-based approach, there areonly 33% of total columns are used in inputs at the cost of3% of accuracy. From token level, demonstratesthat the numbers of token after hard column selection arefor only 26% or 42% compared to the full prompt for thetwo approaches. This suggests an equivalent proportionof cost savings, given that the cost associated with LLMs is directly proportional to the number of tokens. Insome scenario, one may want to sacrice some performance for the cost reduction.",
  ": Text-to-SQL performance (y-axis) with re-spect to column numbers (x-axis). Both retrieval-basedand program-aided are based on soft-column selection": "Impact of the size of the database schema: shows how the performance of the end-to-endText-to-SQL process changes with dierent numbersof columns in the schema, using a soft column se-lection approach where the descriptions of selectedcolumns are included in the prompt. The analysisreveals that as the number of columns increases, Text-to-SQL performance declines, indicating that columncount is a signicant indicator for the diculty ofText-to-SQL tasks. The program-aided column selec-tion method performs better than others when thecolumn count is below 90, whereas the retrieval-basedapproach excels when the column count exceeds 90.On average, the program-aided method selects about5.24 columns per question, whereas the retrieval-based method extracts 25. When the total numberof columns is below 90, including descriptions of 25 columns can overwhelm the prompt, leading to poorerperformance. However, when the total number of columns is above 90, including 25 columns does not take asignicant proportion of the schema, making the retrieval-based approach more eective.",
  "Improvements with test-time renement via execution-based selection": "presents the eectiveness of the test-time renement via execution-based selection approach, asdiscussed in .3.6. Test-time renement via execution-based selection approach integrates multiplepredened training paradigms introduced in previous sections, including mixing of training data, the integrationof database content, the use of synthetic data, and the implementation of both hard and soft column selectionstrategies. This combination method results in a performance improvement of 2.9% over individual trainingparadigms, such as the results in . To assess the robustness of the test time execution selection to distribution shifts, we also apply the model,originally tailored for BIRD, to the Spider dataset. Despite Spiders distinct format dierences from BIRD,such as lacking of column descriptions and hints or not employing column selection, reveals that themethod still enhances performance on a dierent dataset, indicating its robustness to format variations. Alternatively, we also explore another approach of combining dierent training paradigms into a singleparadigm. This involves integrating various training components directly into the inputs of one singleexperiment. This method entails integrating various elements, such as mixed training data, synthetic data,database content, and column selection, into the inputs for a single experiment. Yet, as outlined in inthe Appendix, this strategy does not yield additional accuracy gain, suggesting combining multiple informationat once does not have superimposed positive eects. The reason may be LLMs cannot understand multipleinformation in the inputs simultaneously eectively, highlighting the inherent diculties of incorporatingvarious components of inputs.",
  "Combining all constituents in SQL-PaLM": "We consolidate our ndings, as illustrated in . Our experimentation involves applying standardinstruction tuning for an LLM which signicantly outperforms in-context learning techniques. Utilizingcombined training data BIRD and Spider for tuning led to an improvement of 2%.Furthermore, theintegration of synthetically generated data contributes to an additional 1% boost. Incorporating the databasecontent results in a 3% increase, and further incorporating soft column selection into intputs adds another 1%.Additionally, the implementation of test time execution-based selection, which combines the aforementionedtraining paradigms, provides an additional 3% improvement. Additionally, we present the results of execution-based test-time selection on dierent diculty levels in .",
  "Overall Text-to-SQL performance comparison with other methods": "Putting everything together, SQL-PaLM demonstrates strong results on both the Spider and BIRD datasets.We present a comparative analysis of our methodology against leading methods from the BIRD () andSpider () leaderboards. SQL-PaLM has achieved notable results, with execution accuracy of 87.3% onSpider dev split and 61.7% on BIRD dev split. These improvements are made possible by eectively utilizingdiverse input components for tuning eciently and adopting a selective execution approach. Additionally,BIRD provides Valid Eciency Score (VES) to measure eciency of valid SQL. indicates SQL-PaLMsignicantly better performance for VES (10%+), indicates the SQL-PaLM can output ecient SQL. In and , we compare our approach with a variety of dierent methods for BIRD andSPIDER, respectively, since the top methods on the dierent leaderboards vary. The fact that our singlemethod is competitive across dierent benchmarks and against diverse sets of methods further attests to theecacy of our strategy. Note that some leaderboard submissions are by anonymous contributors, lacking detailed documentationor code, and hindering a full comparison on dev splits (Leaderboard number is for test split, not dev split).In such cases, we use - to acknowledge their notable leaderboard performance, despite not being able toconsider them for a direct evaluation.",
  "Error analyses": "For few-shot prompting on Spider: In our detailed examination of the SQL queries produced by SQL-PaLM via few-shot prompting on the SPIDER dataset, we undertook an in-depth manual evaluation toassess the quality of the generated SQL. Our analysis indicates that the queries often exhibit creativity32,often deviating from the ground-truth by employing varied SQL clause. These queries are mostly free fromsyntactical errors and consistently display complex reasoning, such as the capability to join multiple tables.To provide more tangible insights, shows two complex yet accurately generated queries. Furtherdiscussions and examples are provided in Section A.9.1 in the Appendix.",
  "T2.country JOIN model_list AS T3 ON T2.id = T3.maker WHERE T3.model = \"fiat\"": "For tuning on BIRD: To better understand the common error modes of the ne-tuned LLM, we randomlyselect 100 queries from dierent databases in the BirdSQL dev split, where the execution results fromgenerated queries dont match those of the ground-truth results. shows a high-level breakdown ofaccuracy on BIRD, categorized by query diculty. As one would expect, the accuracy on harder examplesis lower the accuracy on easier examples (68.92%) is signicantly higher than that of moderate examples(52.07%) which is signicantly higher than the challenging examples (47.89%).Additionally, we manually",
  "Total Simple93360.86%Total Moderate45929.94%Total Challenging1429.26%Correct Simple29068.92%Correct Moderate22052.07%Correct Challenging7447.89%": "inspect these queries and categorize them according to the types of errors they produce. The error categoriesare shown in and will be described below. The representation of each category in the pie plot isproportional to its respective percentage. We subdivide the errors into several categories:Schema Linking:Encompasses queries where themodel was not able to select the relevant tables for the queries (e.g.failing to join tables).Misun-derstanding Database Content: The model fails to accurately interpret the data within the tables",
  ": Error categories for SQL-PaLM ne-tuned LLM on Bird dev set": "Reasoning: The model fails to comprehend the questionand the generated query doesnt contain the necessary reason-ing steps to generate the correct queries. Syntax-RelatedErrors: The model produces SQL that are not runnable dueto some syntactical mistake (e.g., missing backticks to referto a column which has spaces). Finally, in red, we label anadditional error category, denoted Dataset related errors,which encompasses dierent errors due to questions, schema,evidence, inconsistencies between the ground-truth SQL andthe question, not because of the outputted SQL. In the 100queries that we analyze, 31 of them present this error categorythat, if extrapolated to the full dataset, would upper boundthe performance of BIRD dev set to 70%. We describe moreof our nding on data-quality in the Appendix A.9.2 andpresent more error examples of each category in inAppendix.",
  "Conclusions": "This paper presents the SQL-PaLM framework, our holistic approach to advancing Text-to-SQL capabilities.We provide insightful discussion for understanding key factors in deciding Text-to-SQL performance. We startwith a comprehensive examination of few-shot prompting to enhance Text-to-SQL performance with LLMs.Then we present best practices for instruction ne-tuning, examining how performance can be improvedthrough expanded data coverage and diversity, synthetic data augmentation and integrating query-specicdatabase content. We introduce a test-time renement approach that leverages query execution feedbackto bolster SQL query accuracy. Additionally, we address some of the real-world challenges of navigatingcomplex databases with many tables and columns, presenting eective methods for the precise selection ofpertinent database components to improve Text-to-SQL performance. Our integrated approach demonstratessubstantial improvements in Text-to-SQL performance, demonstrated on two important public benchmarks.",
  "Ion Androutsopoulos, Graeme D Ritchie, and Peter Thanisch. Natural language interfaces to databasesanintroduction. Natural language engineering, 1(1):2981, 1995": "Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, SiamakShakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report. arXiv preprintarXiv:2305.10403, 2023. Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Lukas Gianinazzi, Joanna Gajda, TomaszLehmann, Michal Podstawski, Hubert Niewiadomski, Piotr Nyczyk, et al. Graph of thoughts: Solvingelaborate problems with large language models. arXiv preprint arXiv:2308.09687, 2023.",
  "Xinyun Chen, Maxwell Lin, Nathanael Schrli, and Denny Zhou. Teaching large language models to self-debug.arXiv preprint arXiv:2304.05128, 2023": "Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, PaulBarham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modelingwith pathways. arXiv preprint arXiv:2204.02311, 2022. Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang,Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-netuned language models. arXivpreprint arXiv:2210.11416, 2022.",
  "Yujian Gan, Xinyun Chen, and Matthew Purver. Exploring underexplored limitations of cross-domaintext-to-sql generalization. arXiv preprint arXiv:2109.05157, 2021b": "Dawei Gao, Haibin Wang, Yaliang Li, Xiuyu Sun, Yichen Qian, Bolin Ding, and Jingren Zhou. Text-to-sqlempowered by large language models: A benchmark evaluation. arXiv preprint arXiv:2308.15363, 2023a. Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and GrahamNeubig. Pal: Program-aided language models. In International Conference on Machine Learning, pp.1076410799. PMLR, 2023b.",
  "Haoyang Li, Jing Zhang, Cuiping Li, and Hong Chen. Decoupling the skeleton parsing and schema linkingfor text-to-sql. arXiv preprint arXiv:2302.05965, 2023a": "Haoyang Li, Jing Zhang, Cuiping Li, and Hong Chen. Resdsql: Decoupling schema linking and skeletonparsing for text-to-sql. In Proceedings of the AAAI Conference on Articial Intelligence, volume 37, pp.1306713075, 2023b. Haoyang Li, Jing Zhang, Hanbing Liu, Ju Fan, Xiaokang Zhang, Jun Zhu, Renjie Wei, Hongyan Pan, CuipingLi, and Hong Chen. Codes: Towards building open-source language models for text-to-sql. arXiv preprintarXiv:2402.16347, 2024. Jinyang Li, Binyuan Hui, Ge Qu, Binhua Li, Jiaxi Yang, Bowen Li, Bailin Wang, Bowen Qin, Rongyu Cao,Ruiying Geng, Nan Huo, Chenhao Ma, Kevin C. C. Chang, Fei Huang, Reynold Cheng, and Yongbin Li.Can llm already serve as a database interface? a big bench for large-scale database grounded text-to-sqls,2023c. Jinyang Li, Binyuan Hui, Ge Qu, Binhua Li, Jiaxi Yang, Bowen Li, Bailin Wang, Bowen Qin, Rongyu Cao,Ruiying Geng, et al. Can llm already serve as a database interface? a big bench for large-scale databasegrounded text-to-sqls. arXiv preprint arXiv:2305.03111, 2023d. Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennigho, Denis Kocetkov, Chenghao Mou, MarcMarone, Christopher Akiki, Jia Li, Jenny Chim, et al. Starcoder: may the source be with you! arXivpreprint arXiv:2305.06161, 2023e.",
  "Mohammadreza Pourreza and Davood Raei. Din-sql: Decomposed in-context learning of text-to-sql withself-correction. arXiv preprint arXiv:2304.11015, 2023": "Jiexing Qi, Jingyao Tang, Ziwei He, Xiangpeng Wan, Chenghu Zhou, Xinbing Wang, Quanshi Zhang, andZhouhan Lin. Rasat: Integrating relational structures into pretrained seq2seq model for text-to-sql. arXivpreprint arXiv:2205.06983, 2022. Colin Rael, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou,Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unied text-to-text transformer.The Journal of Machine Learning Research, 21(1):54855551, 2020.",
  "Bailin Wang, Wenpeng Yin, Xi Victoria Lin, and Caiming Xiong. Learning to synthesize data for semanticparsing. arXiv preprint arXiv:2104.05827, 2021": "Bing Wang, Changyu Ren, Jian Yang, Xinnian Liang, Jiaqi Bai, Linzheng Chai, Zhao Yan, Qian-Wen Zhang,Di Yin, Xing Sun, et al. Mac-sql: A multi-agent collaborative framework for text-to-sql. arXiv preprintarXiv:2312.11242, 2024. Chenglong Wang, Alvin Cheung, and Rastislav Bodik. Synthesizing highly expressive sql queries frominput-output examples. In Proceedings of the 38th ACM SIGPLAN Conference on Programming LanguageDesign and Implementation, pp. 452466, 2017.",
  "Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. Self-consistency improveschain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022": "Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew MDai, and Quoc V Le. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652,2021. Jason Wei, Yi Tay, Rishi Bommasani, Colin Rael, Barret Zoph, Sebastian Borgeaud, Dani Yogatama,Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models. arXivpreprint arXiv:2206.07682, 2022a.",
  "Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain ofthought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022b": "Kun Wu, Lijie Wang, Zhenghua Li, Ao Zhang, Xinyan Xiao, Hua Wu, Min Zhang, and Haifeng Wang. Dataaugmentation with hierarchical sql-to-question generation for cross-domain text-to-sql parsing. arXivpreprint arXiv:2103.02227, 2021. Tianbao Xie, Chen Henry Wu, Peng Shi, Ruiqi Zhong, Torsten Scholak, Michihiro Yasunaga, Chien-ShengWu, Ming Zhong, Pengcheng Yin, Sida I Wang, et al. Uniedskg: Unifying and multi-tasking structuredknowledge grounding with text-to-text language models. arXiv preprint arXiv:2201.05966, 2022. Tianbao Xie, Fan Zhou, Zhoujun Cheng, Peng Shi, Luoxuan Weng, Yitao Liu, Toh Jing Hua, Junning Zhao,Qian Liu, Che Liu, et al. Openagents: An open platform for language agents in the wild. arXiv preprintarXiv:2310.10634, 2023. Shunyu Yao, Dian Yu, Jerey Zhao, Izhak Shafran, Thomas L Griths, Yuan Cao, and Karthik Narasimhan.Tree of thoughts: Deliberate problem solving with large language models. arXiv preprint arXiv:2305.10601,2023.",
  "Pengcheng Yin, Graham Neubig, Wen-tau Yih, and Sebastian Riedel. Tabert: Pretraining for joint under-standing of textual and tabular data. arXiv preprint arXiv:2005.08314, 2020": "Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li, James Ma, Irene Li, QingningYao, Shanelle Roman, et al. Spider: A large-scale human-labeled dataset for complex and cross-domainsemantic parsing and text-to-sql task. arXiv preprint arXiv:1809.08887, 2018. Tao Yu, Rui Zhang, He Yang Er, Suyi Li, Eric Xue, Bo Pang, Xi Victoria Lin, Yi Chern Tan, Tianze Shi,Zihan Li, et al. Cosql: A conversational text-to-sql challenge towards cross-domain natural languageinterfaces to databases. arXiv preprint arXiv:1909.05378, 2019. Tao Yu, Chien-Sheng Wu, Xi Victoria Lin, Bailin Wang, Yi Chern Tan, Xinyi Yang, Dragomir Radev, RichardSocher, and Caiming Xiong. Grappa: Grammar-augmented pre-training for table semantic parsing. arXivpreprint arXiv:2009.13845, 2020. Kun Zhang, Xiexiong Lin, Yuanzhuo Wang, Xin Zhang, Fei Sun, Cen Jianhe, Hexiang Tan, Xuhui Jiang, andHuawei Shen. Refsql: A retrieval-augmentation framework for text-to-sql generation. In Findings of theAssociation for Computational Linguistics: EMNLP 2023, 2023. Yiyun Zhao, Jiarong Jiang, Yiqun Hu, Wuwei Lan, Henry Zhu, Anuj Chauhan, Alexander Li, Lin Pan, JunWang, Chung-Wei Hang, et al. Importance of synthesizing high-quality data for text-to-sql parsing. arXivpreprint arXiv:2212.08785, 2022.",
  ": Test-suite accuracy for dierent prompt design ap-proaches in zero- and few-shot set-up on Spider Dev": "In , we analyze the performance ofFew-shot SQL-PaLM method on dierentnumber of demonstrations (zero- vs. few-shot) and queries with dierent prompt de-signs. Overall, few-shot prompting outper-forms zero-shot counterpart.We also ex-plore the eect of dierent prompt designapproaches on performance. For the LLMbeing queried (e.g. PaLM2), concise promptis observed to be better. Verbose prompts are based on using natural language to describe database schema,which is closer to the way LLMs were trained, whereas Concise prompts use the symbols to describe thedatabase schema, which has advantages of clearly presenting table structure. Examples of concise promptand verbose prompt are provided in Appendix A.10.1 and A.10.2.",
  "Synthetic Data Prompt Design": "You will be provided with a list of tables from a SQL database followed by a natural language queryrelated to the database and the original SQL query answering the question. Your job is to understandthe natural language queries and generate up to 3 dierent SQL queries using diverse commands from theoriginal query while answering the question correctly. You need to make sure to use the same columnsfrom the original query for the generated query. You will also generate a similarity score between theoriginal and the generated query based on how closer they are syntactically.",
  "A.6.3Compare with other column selection methods": "We presented other column selection methods: LLM base: prompting LLMs to request table and columnselection (Prompt is in Sec. A.6.4). LLM CoT: Following Pourreza & Raei (2023), we add few-shotexamples and use change-of-thought demonstrations to help the prompt (Prompt is in Sec. A.6.5). The modelis PaLM-2 text-bison. Automatic Annotation (Lei et al., 2020b) is a proposes pattern matching approach.The last two lines are taken from and (Top 10), rounded by two decimals. The results in indicates that program-aided algorithm outperform the other methods with a clear margin.",
  "Following Pourreza & Raei (2023), we add few-shot examples and use change-of-thought demonstrations tohelp the prompt": "You are an agent designed to find the schema_linksfor generating SQL queries for eachquestion based on the database schema and Foreign keys.Hint helps you to find the correct schema_links.###Few examples of this task are:###Schema of the database with sample rows and column descriptions:#CREATE TABLE users (user_id INT,...);Table Values:User_id ...001 ...Table usersUser_id: id of the user...Question: Among the lists created by user 4208563...Answer: Lets think step by step. In the question , we are asked:\"user\" so we need column = [lists_users.user_id]\"number of followers\" so we need column = [lists.list_followers]...Schema_links: [lists.list_followers,lists_users.user_subscriber,lists.user_id = lists_user.user_id, lists.list_id = lists_user.list_id,lists_users.user_id, 4208563, 1] ###Schema of the database with sample rows and column descriptions:#CREATE TABLE [table_name] ([column_name] [column_type],...);Table Values:[3 row examples with header]Table [table_name][column_name]: [column_description]...Question: [question]",
  "A.7Column and table data statistics of BIRD dataset": "illustrates the distribution of data regarding the number of columns and tables per example in thetesting sets. The ground-truth distribution is also provided. Notably, BIRD presents a more challengingscenario compared to Spider, with an average of 73 total columns to be selected per example, of whichonly 3.7 columns are used in the ground truth. Similarly, the average number of tables is 7, and 1.9 tablesare selected in the ground truth. However, its essential to acknowledge that these sets still deviate fromreal-world scenarios with thousands of columns. This direction should be further explored in the future.",
  "A.8Exploration on combining submodules": "An alternative approach involves combining dierent input congurations in previous sections into a singletraining experiment. This method entails integrating various elements, such as mixed training data, syntheticdata, database content, and column selection, into the inputs for a single experiment. However, the outcomesof such experiments reveal that merging these components does not result in performance improvements overusing them individually. This suggests that LLMs may struggle to eectively process and understand all theprovided information simultaneously during tuning.",
  "A.9.2Fine-tuned SQL-PaLM": "Next, we present our manual investigation of the generated queries on the BIRD dev set. To quantify the errorcases for the queries generated from the ne-tuned Few-shot SQL-PaLM, we randomly select 100 samplesfrom BIRD dev set and categorize the queries that we considered mistakes based on the BIRD evaluationprocedure. shows a breakdown of the dierent error types that we have identied and provided some examplesin for each category. We categorize the false positives as Wrong Ground-Truth (19%) which areexamples from the dataset that do not correctly answer the original question. Wrong Evidence (5%) whichdenotes examples that have incorrect or misleading human-annotated evidences. Ambiguous (2%) whichare examples where the questions meaning is ambiguous and open-ended (e.g., not clear what is expectedfrom the question). Evaluation Procedure (5% denoting that the generated SQL query was in fact correctbut was considered incorrect by the evaluation procedure (e.g., the generated queries contains additionalselected columns). For a comprehensive list of examples from the dierent error types, refer to . This investigation demonstrates that from 100 randomly sampled incorrect queries (out of a total of 584 forne-tuned Few-shot SQL-PaLM), 31% of them are not correctly evaluated by the BIRD evaluation suite andif this sample size is representative of the full dev set, it shows that there is a performance upper bound ofaround 70%.",
  "Human ExpertTrue (Ground-truth issue)": "Question 2:Which airline has most number of ights? (id=229, database=ight_2)Ground-truth:SELECT T1.Airline FROM AIRLINES AS T1 JOIN FLIGHTS AS T2 ON T1.uid = T2.Airline GROUP BY T1.AirlineORDER BY count(*) DESC LIMIT 1SQL-PALM:SELECT T1.airline FROM airlines AS T1 JOIN flights AS T2 ON T1.uid = T2.airline GROUP BY T2.airlineORDER BY count(*) DESC LIMIT 1 Comment:Both T1.Airline (the Airlines.Airline column, which contains names of airlines) and T2.airline (theflights.airline column, which contains the uid of airlines) uniquely identify airlines, and grouping byeither one gives the same results. The evaluation failed because the data contains 12 airlines each with 100ights (i.e. every one is the maximum), and instability of the ordering produces dierent outcomes whenboth queries pick one of the maximums.",
  "Human ExpertTrue (Database and Eval issue)": "Question 4:List the names of all winners who played in both 2013 and 2016. (id=447, database=wta_1)Ground-truth:SELECT winner_name FROM matches WHERE YEAR = 2013 INTERSECT SELECT winner_name FROM matches WHEREYEAR = 2016Excution result:(Angelique Kerber,), (Petra Kvitova,) ...SQL-PALM:SELECT T1.first_name , T1.last_name FROM players AS T1 JOIN matches AS T2 ON T1.player_id =T2.winner_id WHERE T2.year = 2013 INTERSECT SELECT T1.first_name , T1.last_name FROM players AST1 JOIN matches AS T2 ON T1.player_id = T2.winner_id WHERE T2.year = 2016Execution result:(Angelique, Kerber), (Petra, Kvitova) ... Comment:The result is actually correct. The highlighted dierences, as both execution results make sense from semanticperspective of the query. winner_name and T1.rst_name , T1.last_name are equivalent for representationof name\". Evaluation failed because of the dierences in the output format type. If multiple ground-truthsare provided considering dierent output formats, this is not an error.",
  "Human ExpertFalse (Wrong Use of keywords)": "Question 7:Find the number of professionals who have not treated any dogs. (id=754, database=world_1):Ground-truth:select t1.name from country as t1 join countrylanguage as t2 on t1.code = t2.countrycode wheret2.language = \"english\" and isofficial = \"t\" union select t1.name from country as t1 joincountrylanguage as t2 on t1.code = t2.countrycode where t2.language = \"dutch\" and isofficial = \"t\" SQL-PALM:SELECT T1.name FROM country AS T1 JOIN countrylanguage AS T2 ON T1.code = T2.countrycode WHERET2.language = \"English\" OR T2.language = \"Dutch\" AND T2.isofficial = \"T\"Comment:Operator Precedence:ADD > OR. Need to add parenthesis over OR. Corrected SQL is SELECTT1.name FROM country AS T1 JOIN countrylanguage AS T2 ON T1.code = T2.countrycode WHERE (T2.language= \"English\" OR T2.language = \"Dutch\" ) AND T2.isofficial = \"T\".Spider evaluation normalizes theground-truth outputs to all lowercase for easier evaluation, but mismatch exists when referring todatabase content.Changes:english->English,dutch->Dutch, t->T",
  "Comment:The ground-truth query has an upper case \"FROM\" instead of \"from\" which is what is in the question": "Example 2:QuestionWhats the nish time for the driver who ranked second in 2008s Australian Grand Prix?(id=937, database=formula_1)Ground-truthSELECT T1.time FROM results AS T1 INNER JOIN races AS T2 on T1.raceId = T2.raceId WHERE T1.rank = 2AND T2.name = Australian GrAND Prix AND T2.year = 2008CommentSimilarly to the previous example, the ground-truth query string doesnt match the one from the question.Likely this is due to a data-cleaning procedure in the BirdSQL dev set. Example 3:QuestionWhat race number has the most nishers? (id=979, database=formula_1)Ground-truthSELECT raceId FROM results GROUP BY raceId ORDER BY COUNT(time IS NOT NULL) DESC LIMIT 1CommentThe COUNT(time IS NOT NULL) is somewhat unconventional. Typically, COUNT is used on a column name directly.However, here it is counting the boolean result of time IS NOT NULL. This will count all rows, regardless ofwhether time is null or not, since the expression time IS NOT NULL is always either true or false, both of whichare counted. Example 4:QuestionPlease provide top three football players IDs who are among the lowest potential players andprefer to use the right foot when attacking. (id=1135, database=european_football_2)Ground-truthSELECT id FROM Player_Attributes WHERE preferred_foot = right ORDER BY potential DESC LIMIT 3CommentThe questions asks the \"lowest potential players\" so the ground-query should order by descending potential- should not have DESC.",
  "Wrong Evidence": "Example 1:QuestionWhat is the eligible free rate of the 10th and 11th schools with the highest enrolment forstudents in grades 1 through 12? (id=31, database=california_schools)Ground-truthSELECT CAST(Free Meal Count (K-12) AS REAL) / Enrollment (K-12) FROM frpm ORDER BY Enrollment(K-12) DESC LIMIT 9, 2.EvidenceK-12 refers to students in grades 1 through 12; Eligible free rate for K-12 = FRPM Count(K-12) / Enrollment (K-12)",
  "Comment:The evidence suggests that the information can be found in column FRPM Count (K-12) but we can see thatin the ground-truth another column is actually chosen": "Example 2:QuestionAmong all chemical compounds that contain molecule TR047, identify the percent that form adouble-bond. (id=287, database=toxicology)Ground-truthSELECT CAST(COUNT(CASE WHEN T.bond_type = = THEN T.bond_id ELSE NULL END) AS REAL) * 100 /COUNT(T.bond_id) FROM bond AS T WHERE T.molecule_id = TR047.EvidenceTR047 is the molecule id;double bond refers to bond_type = = ;percentage = DI-VIDE(SUM(bond_type = = ), COUNT(all bond_id)) as percent where molecule_id =TR047",
  "Here is an example: Convert text to SQL:": "[Schema (values)]:| farm | city : city_id , official_name , status , area_km_2 , population ,census_ranking | farm : farm_id , year , total_horses , working_horses , total_cattle , oxen, bulls , cows , pigs , sheep_and_goats | farm_competition : competition_id , year , theme ,host_city_id , hosts | competition_record : competition_id , farm_id , rank; [Column names (type)]:city : city_id (number)| city : official_name (text)| city : status(text)| city : area_km_2 (number)| city : population (number)| city : census_ranking (text)| farm : farm_id (number)| farm : year (number)| farm : total_horses (number)| farm :working_horses (number)| farm : total_cattle (number)| farm : oxen (number)| farm : bulls(number)| farm : cows (number)| farm : pigs (number)| farm : sheep_and_goats (number)|farm_competition : competition_id (number)| farm_competition : year (number)| farm_competition: theme (text)| farm_competition : host_city_id (number)| farm_competition : hosts (text)| competition_record : competition_id (number)| competition_record : farm_id (number)|competition_record : rank (number);",
  "Here is the test question to be answered: Convert text to SQL:": "[Schema (values)]:| concert_singer | stadium : stadium_id , location , name , capacity, highest , lowest , average | singer : singer_id , name , country , song_name ,song_release_year , age , is_male | concert : concert_id , concert_name , theme , stadium_id ,year | singer_in_concert : concert_id , singer_id;",
  "Here is an example: Let us take a questionand turn it into a SQLstatementaboutdatabasetables.There are 4 tables": ". Theirtitles are: city , farm , farm_competition , competition_record . is city , and its columnnames and types are: City_ID (Type is number),Official_Name (Type is text), Status (Type is text), Area_km_2 (Type isnumber), Population (Type is number), Census_Ranking (Type is text). is farm , and its columnnames and types are: Farm_ID (Type is number),Year (Type is number), Total_Horses (Type is number), Working_Horses (Typeis number), Total_Cattle (Type is number), Oxen (Type is number), Bulls (Type is number), Cows (Type is number), Pigs (Type is number),Sheep_and_Goats (Type is number). is farm_competition , and itscolumnnames and types are: Competition_ID (Type is number), Year (Type isnumber), Theme (Type is text), Host_city_ID (Type is number), Hosts (Typeis text). is competition_record , and its columnnames and types are: Competition_ID (Type is number), Farm_ID (Type is number), Rank (Type isnumber).The primary keys are: city_idfromTable city , farm_idfromTable farm ,",
  "Here is an example: Let us take a questionand turn it into a SQLstatementaboutdatabasetables.There are 3 tables": ". Theirtitles are: department , head , management. is department ,and its columnnames and types are: Department_ID (Type is number), Name (Type is text), Creation (Type is text), Ranking (Type is number),Budget_in_Billions (Type is number), Num_Employees (Type is number). is head , and its columnnames and types are: head_ID (Type is number),name (Type is text), born_state (Type is text), age (Type is number). is management , and its columnnames and types are: department_ID (Typeis number), head_ID (Type is number), temporary_acting (Type is text).The primary keys are: department_idfromTabledepartment , head_idfromTablehead , department_idfromTablemanagement. The foreign keys are: head_idfromTablemanagement is equivalentwithhead_idfromTable head , department_idfromTablemanagement is equivalentwithdepartment_idfromTabledepartment. Useforeignkeys to joinTables.Columnswithrelevantvalues: TablemanagementColumntemporary_actinghavevalues: Yes;Only usecolumnswithrelevantvalues to generateSQL.Letus take a textquestionand turn it into a SQLstatementaboutdatabasetables. Thequestion is: Show the name and number of employeesfor thedepartmentsmanaged by headswhosetemporaryactingvalue is Yes ? ThecorrespondingSQL is: SELECT T1.name ,T1. num_employeesFROMdepartment AST1 JOINmanagement AS T2 ON T1. department_id=T2. department_idWHERE T2. temporary_acting=Yes ;",
  "Here is the test question to be answered: Let us take a questionand turn it into aSQLstatementaboutdatabasetables": "There are 4 tables. Theirtitles are: stadium , singer , concert ,singer_in_concert . is stadium , and its columnnames and types are:Stadium_ID (Type is number), Location (Type is text), Name (Type is text), Capacity (Type is number), Highest (Type is number), Lowest (Type isnumber), Average (Type is number). is singer , and its columnnamesand types are: Singer_ID (Type is number), Name (Type is text), Country (Type is text), Song_Name (Type is text), Song_release_year (Type is text),Age (Type is number), Is_male (Type is others). is concert , andits columnnames and types are: concert_ID (Type is number), concert_Name(Type is text), Theme (Type is text), Stadium_ID (Type is text), Year (Type is text). is singer_in_concert , and its columnnames andtypes are: concert_ID (Type is number), Singer_ID (Type is text).The primary keys are: stadium_idfromTable stadium , singer_idfromTablesinger , concert_idfromTable concert , concert_idfromTablesinger_in_concert .The foreign keys are: stadium_idfromTableconcert is equivalentwithstadium_idfromTable stadium , singer_idfromTablesinger_in_concert isequivalentwithsinger_idfromTable singer , concert_idfromTablesinger_in_concert is equivalentwithconcert_idfromTableconcert. Useforeignkeys to joinTables.Let us take a textquestionand turn it intoa SQLstatementaboutdatabasetables. Thequestion is: How manysingersdo we have? ThecorrespondingSQL is:",
  "Here is the testquestion to be anwered: Converttext to SQL:": "[Schema (values)]: | california_schools | frpm : CDSCode , AcademicYear ,CountyCode , DistrictCode , SchoolCode , CountyName , DistrictName ,SchoolName , DistrictType , SchoolType , EducationalOptionType , NSLPProvisionStatus , CharterSchool (Y/N) , CharterSchoolNumber , CharterFundingType , IRC , Low Grade , HighGrade , Enrollment (K -12) , Free MealCount (K -12) , Percent(%)EligibleFree (K -12) , FRPMCount (K -12) ,Percent(%)EligibleFRPM (K -12) , Enrollment (Ages 5 -17) , Free MealCount(Ages 5-17) , Percent(%)EligibleFree (Ages 5 -17) , FRPMCount (Ages5 -17) , Percent(%)EligibleFRPM (Ages 5 -17) , 2013 -14CALPADSFall 1CertificationStatus | satscores : cds , rtype , sname , dname , cname ,enroll12 , NumTstTakr , AvgScrRead , AvgScrMath , AvgScrWrite , NumGE1500 |schools : CDSCode , NCESDist , NCESSchool , StatusType , County , District, School , Street , StreetAbr , City , Zip , State , MailStreet ,MailStrAbr , MailCity , MailZip , MailState , Phone , Ext , Website ,OpenDate , ClosedDate , Charter , CharterNum , FundingType , DOC , DOCType, SOC , SOCType , EdOpsCode , EdOpsName , EILCode , EILName , GSoffered ,GSserved , Virtual , Magnet , Latitude , Longitude , AdmFName1 , AdmLName1, AdmEmail1 , AdmFName2 , AdmLName2 , AdmEmail2 , AdmFName3 , AdmLName3 ,AdmEmail3 , LastUpdate; [Column names (type)]: frpm : CDSCode (text) | frpm : AcademicYear (text) | frpm: CountyCode (text) | frpm : DistrictCode (number) | frpm : SchoolCode(text) | frpm : CountyName (text) | frpm : DistrictName (text) | frpm :SchoolName (text) | frpm : DistrictType (text) | frpm : SchoolType (text) | frpm : EducationalOptionType (text) | frpm : NSLPProvisionStatus (text) | frpm : CharterSchool (Y/N) (number) | frpm : CharterSchoolNumber(text) | frpm : CharterFundingType (text) | frpm : IRC (number) | frpm :Low Grade (text) | frpm : HighGrade (text) | frpm : Enrollment (K -12) (number) | frpm : Free MealCount (K -12) (number) | frpm : Percent(%)EligibleFree (K -12) (number) | frpm : FRPMCount (K -12) (number) | frpm :Percent(%)EligibleFRPM (K -12) (number) | frpm : Enrollment (Ages 5 -17) (number) | frpm : Free MealCount (Ages 5 -17) (number) | frpm : Percent(%)EligibleFree (Ages 5-17) (number) | frpm : FRPMCount (Ages 5 -17) (number)| frpm : Percent(%)EligibleFRPM (Ages 5 -17) (number) | frpm : 2013 -14CALPADSFall 1 CertificationStatus (number) | satscores : cds (text) |satscores : rtype (text) | satscores : sname (text) | satscores : dname (text) | satscores : cname (text) | satscores : enroll12 (number) |satscores : NumTstTakr (number) | satscores : AvgScrRead (number) |satscores : AvgScrMath (number) | satscores : AvgScrWrite (number) |satscores : NumGE1500 (number) | schools : CDSCode (text) | schools :",
  "[detailed description of tables and columns]:": "Columndescription of Table \"frpm\" have thefollowingdescriptions :Column \"CountyName\" of Table \"frpm\", means \"CountyCode\"Column \"CharterSchool (Y/N)\" of Tablefrpm has valuedescriptions\"0: N;1: Y\"Column \"IRC\" of Tablefrpm has valuedescriptions \"Not useful\"Column \"Enrollment (K -12)\" of Tablefrpm has valuedescriptions \"commonsenseevidence:K -12: 1st grade - 12nd grade\"Column \"Free MealCount (K -12)\" of Tablefrpm has valuedescriptions \"commonsenseevidence:eligiblefree rate = Free MealCount / Enrollment\"Column \"FRPMCount (K -12)\" of Table \"frpm\", means \"Free or ReducedPriceMealCount (K -12)\", has valuedescriptions \"commonsenseevidence:eligibleFRPMrate = FRPM / Enrollment\"Column \"Free MealCount (Ages 5 -17)\" of Tablefrpm has valuedescriptions \"commonsenseevidence:eligiblefree rate = Free MealCount / Enrollment\"Columndescription of Table \"satscores\" have thefollowingdescriptions :Column \"cds\" of Table \"satscores\", means \"CaliforniaDepartmentSchools\"Column \"rtype\" of Tablesatscoreshas valuedescriptions \"unuseful\"Column \"sname\" of Table \"satscores\", means \"schoolname\"Column \"dname\" of Table \"satscores\", means \"districtsegment\", district name ,Column \"cname\" of Table \"satscores\", means \"countyname\"Column \"enroll12\" of Table \"satscores\", means \"enrollment (1st -12nd grade)\"Column \"NumTstTakr\" of Table \"satscores\", means \"Number of TestTakers in thisschool\", Number of Test Takers , , has valuedescriptions \"number of testtakers in eachschool\"Column \"AvgScrRead\" of Table \"satscores\", means \"averagescores in Reading\"Column \"AvgScrMath\" of Table \"satscores\", means \"averagescores in Math\"Column \"AvgScrWrite\" of Table \"satscores\", means \"averagescores in writing\"Column \"NumGE1500\" of Table \"satscores\", means \"Number of TestTakersWhoseTotal SAT Scores AreGreater or Equal to 1500\" , has valuedescriptions \"Number of TestTakersWhoseTotal SAT Scores AreGreater or Equal to 1500commonsenseevidence:ExcellenceRate = NumGE1500 / NumTstTakr\"Columndescription of Table \"schools\" have thefollowingdescriptions :Column \"NCESDist\" of Table \"schools\", means \"Thisfieldrepresentsthe 7-digitNationalCenter forEducationalStatistics (NCES) schooldistrictidentificationnumber. The first 2 digitsidentifythe state and the last 5digitsidentifythe schooldistrict. Combined , they make a unique 7-digitID for eachschooldistrict .\", NationalCenter forEducationalStatisticsschooldistrictidentificationnumber ,Column \"NCESSchool\" of Table \"schools\", means \"Thisfieldrepresentsthe 5-digitNCESschoolidentificationnumber. TheNCESSchoolcombinedwith the",
  "[Database values that related with questions]": "The column County Name in Table frpm hasdatabasevalues: AlamedaThe column cname in Table satscores hasdatabasevalues: AlamedaThe column County in Table schools hasdatabasevalues: AlamedaThe column City in Table schools hasdatabasevalues: AlamedaThe column MailCity in Table schools hasdatabasevalues: AlamedaThe column GSoffered in Table schools hasdatabasevalues: K-12The column GSserved in Table schools hasdatabasevalues: K-12The column AdmFName1 in Table schools hasdatabasevalues: RaeThe column AdmLName1 in Table schools hasdatabasevalues: Free;[Additional Info]: Eligiblefree rate for K-12 = FRPMCount (K -12) / Enrollment (K -12)"
}