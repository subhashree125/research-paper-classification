{
  "Abstract": "We study the scaling properties of latent diusion models (LDMs) with an emphasis on theirsampling eciency. While improved network architecture and inference algorithms haveshown to eectively boost sampling eciency of diusion models, the role of model sizeacritical determinant of sampling eciencyhas not been thoroughly examined. Throughempirical analysis of established text-to-image diusion models, we conduct an in-depthinvestigation into how model size inuences sampling eciency across varying samplingsteps. Our ndings unveil a surprising trend: when operating under a given inference bud-get, smaller models frequently outperform their larger equivalents in generating high-qualityresults. Moreover, we extend our study to demonstrate the generalizability of the these nd-ings by applying various diusion samplers, exploring diverse downstream tasks, evaluatingpost-distilled models, as well as comparing performance relative to training compute. Thesendings open up new pathways for the development of LDM scaling strategies which can beemployed to enhance generative capabilities within limited inference budgets.",
  "Introduction": "Latent diusion models (LDMs) (Rombach et al., 2022), and diusion models in general, trained on large-scale, high-quality data (Lin et al., 2014; Schuhmann et al., 2022) have emerged as a powerful and robustframework for generating impressive results in a variety of tasks, including image synthesis and editing (Rom-bach et al., 2022; Podell et al., 2023; Delbracio & Milanfar, 2023; Ren et al., 2023; Qi et al., 2023), videocreation (Mei & Patel, 2023; Mei et al., 2023; Wu et al., 2023; Singer et al., 2022), audio production (Liuet al., 2023a), and 3D synthesis (Lin et al., 2023; Liu et al., 2023b). Despite their versatility, the major",
  "Published in Transactions on Machine Learning Research (12/2024)": "Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang,Oron Ashual, Oran Gafni, et al. Make-a-video: Text-to-video generation without text-video data. ArXivpreprint, 2022. 1 Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diusion implicit models. In 9th InternationalConference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021, 2021a. 2, 3 Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole.Score-based generative modeling through stochastic dierential equations. In 9th International Conferenceon Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021, 2021b. 2",
  "Our key ndings for scaling latent diusion models in text-to-image generation and various downstreamtasks are as follows:": "Pretraining performance scales with training compute. We demonstrate a clear link between computeresources and LDM performance by scaling models from 39 million to 5 billion parameters. This suggestspotential for further improvement with increased scaling. See .1 for details. Downstream performance scales with pretraining.We demonstrate a strong correlation betweenpretraining performance and success in downstream tasks. Smaller models, even with extra training, cannotfully bridge the gap created by the pretraining quality of larger models.This is explored in detail in.2. Smaller models sample more ecient. Smaller models initially outperform larger models in imagequality for a given sampling budget, but larger models surpass them in detail generation when computationalconstraints are relaxed. This is further elaborated in .3.1 and .3.2.",
  "Related Work": "Scaling laws.Recent Large Language Models (LLMs) including GPT (Brown et al., 2020), PaLM (Anilet al., 2023), and LLaMa (Touvron et al., 2023) have dominated language generative modeling tasks. Thefoundational works (Kaplan et al., 2020; Brown et al., 2020; Homann et al., 2022) for investigating theirscaling behavior have shown the capability of predicting the performance from the model size. They alsoinvestigated the factors that aect the scaling properties of language models, including training compute,dataset size and quality, learning rate schedule, etc. Those experimental clues have eectively guided the laterlanguage model development, which have led to the emergence of several parameter-ecient LLMs (Ho-mann et al., 2022; Touvron et al., 2023; Zhou et al., 2023; Alabdulmohsin et al., 2024). However, scalinggenerative text-to-image models are relatively unexplored, and existing eorts have only investigated thescaling properties on small datasets or small models, like scaling UNet (Nichol & Dhariwal, 2021) to 270million parameters and DiT (Peebles & Xie, 2023) on ImageNet (14 million), or less-ecient autoregressivemodels (Chen et al., 2020). Dierent from these attempts, our work investigates the scaling properties byscaling down the ecient and capable diusion models, i.e. LDMs (Rombach et al., 2022), on internal datasources that have about 600 million aesthetics-ltered text-to-image pairs for featuring the sampling e-ciency of scaled LDMs. We also scale LDMs on various scenarios such as netuning LDMs on downstreamtasks (Wang et al., 2021; Ruiz et al., 2023) and distilling LDMs (Mei et al., 2024a) for faster sampling todemonstrate the generalizability of the scaled sampling-eciency. Ecient diusion models.Nichol et al. (Nichol & Dhariwal, 2021) show that the generative performanceof diusion models improves as the model size increases. Based on this preliminary observation, the modelsize of widely used LDMs, e.g., Stable Diusion (Rombach et al., 2022), has been empirically increased tobillions of parameters (Ramesh et al., 2022; Podell et al., 2023). However, such a large model makes itimpossible to t into the common inference budget of practical scenarios. Recent work on improving thesampling eciency focus on improving network architectures (Li et al., 2023; Zhao et al., 2023; Peebles &Xie, 2023; Kim et al., 2023b;a; Choi et al., 2023; Mei et al., 2024b) or the sampling procedures (Song et al.,2021a; Dockhorn et al., 2022; Karras et al., 2022; Lu et al., 2022a; Liu et al., 2023c; Xu et al., 2023; Meiet al., 2025). We explore sampling eciency by training smaller, more compact LDMs. Our analysis involvesscaling down the model size, training from scratch, and comparing performance at equivalent inference cost. Ecient non-diusion generative models.Compared to diusion models, other generative modelssuch as, Variational Autoencoders (VAEs) (Kingma & Welling, 2014; Rezende & Mohamed, 2015; Makhzaniet al., 2015; Vahdat & Kautz, 2020), Generative Adversarial Networks (GANs) (Goodfellow et al., 2020; Maoet al., 2017; Karras et al., 2019; Reed et al., 2016; Miyato et al., 2018), and Masked Models (Devlin et al.,2019; Rael et al., 2020; He et al., 2022; Chang et al., 2022; 2023), are more ecient, as they rely less on aniterative renement process. Sauer et al. (Sauer et al., 2023a) recently scaled up StyleGAN (Karras et al.,2019) into 1 billion parameters and demonstrated the single-step GANs eectiveness in modeling text-to-image generation. Chang et al. (Chang et al., 2023) scaled up masked transformer models for text-to-imagegeneration. These non-diusion generative models can generate high-quality images with less inference cost,",
  "(i) 2B model": ": Text-to-image results from our scaled LDMs (39M - 2B), highlighting the improvement in visualquality with increased model size (note: 39M model is the exception). All images generated using 50-stepDDIM sampling and CFG rate of 7.5. We use representative prompts from PartiPrompts Yu et al. (2022),including a professional photo of a sunset behind the grand canyon., Dogs sitting around a poker table withbeer bottles and chips. Their hands are holding cards., Portrait of anime girl in mechanic armor in nightTokyo., a teddy bear on a skateboard., a pixel art corgi pizza., Snow mountain and tree reection in thelake., a propaganda poster depicting a cat dressed as french emperor napoleon holding a piece of cheese.,a store front that has the word LDMs written on it., and ten red apples.. Check our supplement foradditional visual comparisons.",
  "FID 25.3024.3024.1823.7622.8322.3522.1521.8221.5520.9820.14CLIP 0.3050.3080.3100.3100.3110.3120.3120.3120.3120.3120.314": ": We scale the baseline LDM (i.e., 866M Stable Diusion v1.5) by changing the base number of channelsc that controls the rest of the U-Net architecture as [c, 2c, 4c, 4c] (See ). GFLOPS are measured for aninput latent of shape 64 64 4 with FP32. We also show a normalized running cost with respect to thebaseline model. The text-to-image performance (FID and CLIP scores) for all scaled LDMs is evaluated onthe COCO-2014 validation set with 30k samples, using 50-step DDIM sampling and Classier-free Guidance(CFG) with a rate of 7.5. It is worth noting that all the model sizes, and the training and the inference costsreported in this work only refer to the denoising UNet in the latent space, and do not include the 1.4B textencoder and the 250M latent encoder and decoder. PromptCLIP Image5B model",
  "M model": "5664721.52.03.04.05.06.07.08.0optimal 39M 83M 145M 223M 318M 430M 558M 704M 866M model size sampling steps Optimal CFG Rate 1.52.0 3.0 4.0 5.0 6.0 7.0 8.0 : The impact of the CFG rate on text-to-image generation depends on the model size and samplingsteps. As demonstrated in the left and center panels, the optimal CFG rate changes as the sampling stepsincreased. To determine the optimal performance (according to the FID score) of each model and eachsampling steps, we systematically sample the model at various CFG rates and identify the best one. Asa reference of the optimal performance, the right panel shows the CFG rate corresponding to the optimalperformance of each model for a given number of sampling steps.",
  "Scaling LDMs": "We developed a family of powerful Latent Diusion Models (LDMs) built upon the widely-used 866M StableDiusion v1.5 standard (Rombach et al., 2022)1. The denoising UNet of our models oers a exible rangeof sizes, with parameters spanning from 39M to 5B. We incrementally increase the number of lters in theresidual blocks while maintaining other architecture elements the same, enabling a predictably controlledscaling. shows the architectural dierences among our scaled models. We also provide the relativecost of each model against the baseline model. shows the architectural dierences during scaling.Models were trained using the web-scale aesthetically ltered text-to-image dataset, i.e., WebLI (Chenet al., 2022). All the models are trained for 500K steps, batch size 2048, and learning rate 1e-4. This allowsfor all the models to have reached a point where we observe diminishing returns. demonstrates theconsistent generation capabilities across our scaled models. We used the common practice of 50 sampling",
  "Training Compute": "0.270 0.285 0.300 0.315 0.330 0.345 0.360 Performance (LPIPS) 83M145M223M318M430M558M704M866M2B : In 4 real image super-resolution using 50-step DDIM sampling, FID and LPIPS scores revealan interesting divergence. Model size drives FID score improvement, while training compute most impactsLPIPS score. Despite this, visual assessment () conrms the importance of model size for superiordetail recovery (similarly as observed in the text-to-image pretraining).",
  "steps with the DDIM sampler, 7.5 classier-free guidance rate, for text-to-image generation. The visualquality of the results exhibits a clear improvement as model size increases": "In order to evaluate the performance of the scaled models, we test the text-to-image performance of scaledmodels on the validation set of COCO 2014 (Lin et al., 2014) with 30k samples. For downstream performance,specically real-world super-resolution, we test the performance of scaled models on the validation of DIV2Kwith 3k randomly cropped patches, which are degraded with the RealESRGAN degradation (Wang et al.,2021).",
  "Training compute scales text-to-image performance": "We nd that our scaled LDMs, across various model sizes, exhibit similar trends in generative performancerelative to training compute cost, especially after training stabilizes, which typically occurs after 200Kiterations. These trends demonstrate a smooth scaling in learning capability between dierent model sizes.To elaborate, illustrates a series of training runs with models varying in size from 39 million to 5billion parameters, where the training compute cost is quantied as the product of relative cost shown in and training iterations. Model performance is evaluated by using the same sampling steps andsampling parameters. In scenarios with moderate training compute (i.e., < 1G, see ), the generativeperformance of T2I models scales well with additional compute resources.",
  "Pretraining scales downstream performance": "Using scaled models based on their pretraining on text-to-image data, we netune these models on the down-stream tasks of real-world super-resolution (Saharia et al., 2022; Sahak et al., 2023) and DreamBooth (Ruizet al., 2023). The performance of these pretrained models is shown in Table. 1. In the left panel of ,we present the generative performance FID versus training compute on the super-resolution (SR) task. Itcan be seen that the performance of SR models is more dependent on the model size than training compute.Our results demonstrate a clear limitation of smaller models: they cannot reach the same performance levelsas larger models, regardless of training compute. While the distortion metric LPIPS shows some inconsistencies compared to the generative metric FID(), clearly demonstrates that larger models excel in recovering ne-grained details comparedto smaller models. The key takeaway from is that large super-resolution models achieve superior results even after shortnetuning periods compared to smaller models. This suggests that pretraining performance (dominated bythe pretraining model sizes) has a greater inuence on the super-resolution FID scores than the duration ofnetuning (i.e., training compute for netuning). Furthermore, we compare the visual results of the DreamBooth netuning on the dierent models in .We observe a similar trend between visual quality and model size. Please see our supplement for morediscussions on the other quality metrics.",
  "(b) 50-step sampling results of the 866M model": ": Visualization of text-to-image results with 50-step DDIM sampling and dierent CFG rates (fromleft to right in each row: (1.5, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0)). The prompt used is A raccoon wearing formalclothes, wearing a top hat and holding a cane. Oil painting in the style of Rembrandt.. We observe thatchanges in CFG rates impact visual quality more signicantly than the prompt semantic accuracy. We usethe FID score for quantitative determination of optimal sampling performance () because it directlymeasures visual quality, unlike the CLIP score, which focuses on semantic similarity. inuencing the balance between visual delity and semantic alignment with text prompt.Rombach etal. (Rombach et al., 2022) experimentally demonstrate that dierent CFG rates result in dierent CLIP andFID scores.",
  "Sampling Cost": "Performance (FID) 39M83M145M223M318M430M558M704M866Mdistilled : Distillation improves text-to-image performance and scalability. Left: Distilled Latent DiusionModels (LDMs) consistently exhibit lower (better) FID scores compared to their undistilled counterpartsacross varying model sizes. The consistent acceleration factor (approx. 5) indicates that the benets ofdistillation scale well with model size. Right: Distilled models using only 4 sampling steps achieve FID scorescomparable to undistilled models using signicantly more steps. Interestingly, at a sampling cost of 7, thedistilled 866M model performs similarly to the smaller, undistilled 83M model, suggesting improved eciency. literature demonstrating the positive impacts of SR performance without using classier-free guidance. Thus,our approach directly uses the SR sampling result without applying classier-free guidance. Inspired from, where the scaled downstream LDMs have signicant performance dierence in 50-step sampling,we investigate sampling eciency from two dierent aspects, i.e., fewer sampling steps and moresampling steps (20, 250]. As shown in the left part of , the scaling sampling-eciency still holds inthe SR tasks when the number of sampling steps is less than or equal to 20 steps. Beyond this threshold,however, larger models demonstrate greater sampling-eciency than smaller models, as illustrated in theright part of . This observation suggests the consistent sampling eciency of scaled models on fewersampling steps from text-to-image generation to super-resolution tasks.",
  "(b) Prompt: A pineapple surng on a wave.. Sampling Cost 12": ": Text-to-image results of the scaled LDMs under approximately the same inference cost (normalizedcost sampling steps). Smaller models can produce comparable or even better visual results than largermodels under similar sampling cost. (normalized cost sampling steps). By tracing the points of optimal performance across various samplingcostrepresented by the dashed vertical linewe observe a consistent trend: smaller models frequentlyoutperform larger models across a range of sampling cost in terms of FID scores. Furthermore, to visuallysubstantiate better-quality results generated by smaller models against larger ones, compares theresults of dierent scaled models, which highlights that the performance of smaller models can indeed matchtheir larger counterparts under similar sampling cost conditions.Please see our supplement for more visualcomparisons.",
  "Scaling sampling-eciency in dierent samplers": "To assess the generalizability of observed scaling trends in sampling eciency, we compared scaled LDMperformance using dierent diusion samplers.In addition to the default DDIM sampler, we employedtwo representative alternatives: the stochastic DDPM sampler (Ho et al., 2020) and the high-order DPM-Solver++ (Lu et al., 2022b). Experiments illustrated in reveal that the DDPM sampler typically produces lower-quality resultsthan DDIM with fewer sampling steps, while the DPM-Solver++ sampler generally outperforms DDIM inimage quality (see the gure caption for details). Importantly, we observe consistent sampling-eciencytrends with the DDPM and DPM-Solver++ sampler as seen with the default DDIM: smaller models tendto achieve better performance than larger models under the same sampling cost. Since the DPM-Solver++sampler is not designed for use beyond 20 steps, we focused our testing within this range. This ndingdemonstrates that the scaling properties of LDMs remain consistent regardless of the diusion sampler used.",
  "Scaling sampling-eciency in distilled LDMs": "We have featured the scaling sampling-eciency of latent diusion models, which demonstrates that smallermodel sizes exhibit higher sampling eciency. A notable caveat, however, is that smaller models typicallyimply reduced modeling capability. This poses a challenge for recent diusion distillation methods (Luhman& Luhman, 2021; Salimans & Ho, 2022; Song et al., 2023; Sauer et al., 2023b; Gu et al., 2023; Mei et al.,2024a; Luo et al., 2023; Lin et al., 2024) that heavily depend on modeling capability. One might expecta contradictory conclusion and believe the distilled large models sample faster than distilled small models.In order to demonstrate the sampling eciency of scaled models after distillation, we distill our previouslyscaled models with conditional consistency distillation (Song et al., 2023; Mei et al., 2024a) on text-to-imagedata and compare those distilled models on their optimal performance. To elaborate, we test all distilled models with the same 4-step sampling, which is shown to be able toachieve the best sampling performance; we then compare each distilled model with the undistilled one on thenormalized sampling cost. We follow the same practice discussed in .3.1 for selecting the optimalCFG rate and compare them under the same relative inference cost. The results shown in the left partof demonstrate that distillation signicantly improves the generative performance for all models in4-step sampling, with FID improvements across the board. By comparing these distilled models with theundistilled models in the right part of , we demonstrate that distilled models outperform undistilledmodels at the same sampling cost. However, at the specic sampling cost, i.e., sampling cost 8, the smallerundistilled 83M model still achieves similar performance to the larger distilled 866M model. The observationfurther supports our proposed scaling sampling-eciency after diusion distillation.",
  "Conclusion": "In this paper, we investigated scaling properties of Latent Diusion Models (LDMs), specically throughscaling model size from 39 million to 5 billion parameters. We trained these scaled models from scratchon a web-scale text-to-image dataset and then netuned the pretrained models for downstream tasks. Ourndings unveil that, under identical sampling costs, smaller models frequently outperform larger models,suggesting a promising direction for accelerating LDMs in terms of model size. We further show that thesampling eciency is consistent in multiple axes. For example, it is invariant to various diusion samplers(stochastic and deterministic), and also holds true for distilled models. We believe this analysis of scalingsampling eciency would be instrumental in guiding future developments of LDMs, specically for balancingmodel size against performance and eciency in a broad spectrum of practical applications. Limitations and future work.This work utilizes visual quality inspection alongside established metricslike FID and CLIP scores. We opted to avoid human evaluations due to the immense number of dierentcombinations needed for the more than 1000 variants considered in this study. However, it is importantto acknowledge the potential discrepancy between visual quality and quantitative metrics, which is activelydiscussed in recent works (Zhang et al., 2021; Jayasumana et al., 2024; Cho et al., 2023). Claims regarding the scalability of latent diusion models are made specically for the particular model familystudied in this work (Rombach et al., 2022). Extending this analysis to other model families, particularlythose incorporating transformer-based backbones such as DiT (Peebles & Xie, 2023; Mei et al., 2023), SiT (Maet al., 2024), MM-DiT (Esser et al., 2024), and DiS (Fei et al., 2024), and cascaded diusion models suchas Imagen3 (Baldridge et al., 2024) and Stable Cascade (Pernias et al., 2023), would be a valuable directionfor future research.",
  "Jiwoong Choi, Minkyu Kim, Daehyun Ahn, Taesu Kim, Yulhwa Kim, Dongwon Jo, Hyesung Jeon, Jae-JoonKim, and Hyungjun Kim. Squeezing large-scale diusion models for mobile. ArXiv preprint, 2023. 2, 3": "Mauricio Delbracio and Peyman Milanfar. Inversion by direct iteration: An alternative to denoising diusionfor image restoration. Transactions on Machine Learning Research, 2023. ISSN 2835-8856. FeaturedCertication. 1 Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidi-rectional transformers for language understanding. In Proceedings of the 2019 Conference of the NorthAmerican Chapter of the Association for Computational Linguistics: Human Language Technologies, Vol-ume 1 (Long and Short Papers), 2019. 3",
  "Tim Dockhorn, Arash Vahdat, and Karsten Kreis. Genie: Higher-order denoising diusion solvers. Advancesin Neural Information Processing Systems, 2022. 2, 3": "Hongyang Du, Ruichen Zhang, Dusit Niyato, Jiawen Kang, Zehui Xiong, Dong In Kim, Xuemin ShermanShen, and H Vincent Poor. Exploring collaborative distributed diusion-based ai-generated content (aigc)in wireless networks. IEEE Network, (99), 2023. 2 Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Mller, Harry Saini, Yam Levi,Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectied ow transformers for high-resolutionimage synthesis. In Forty-rst International Conference on Machine Learning, 2024. 13",
  "Zhengcong Fei, Mingyuan Fan, Changqian Yu, and Junshi Huang. Scalable diusion models with state spacebackbone. arXiv preprint arXiv:2402.05608, 2024. 13": "Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, AaronCourville, and Yoshua Bengio. Generative adversarial networks. Communications of the ACM, (11), 2020.2, 3 Jiatao Gu, Shuangfei Zhai, Yizhe Zhang, Lingjie Liu, and Joshua M Susskind. Boot: Data-free distillationof denoising diusion models with bootstrapping. In ICML 2023 Workshop on Structured ProbabilisticInference {\\&} Generative Modeling, 2023. 2, 12 Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollr, and Ross B. Girshick. Masked autoencodersare scalable vision learners.In IEEE/CVF Conference on Computer Vision and Pattern Recognition,CVPR 2022, New Orleans, LA, USA, June 18-24, 2022, 2022. 3",
  "Sadeep Jayasumana, Srikumar Ramalingam, Andreas Veit, Daniel Glasner, Ayan Chakrabarti, and SanjivKumar. Rethinking d: Towards a better evaluation metric for image generation. ArXiv preprint, 2024.13": "Norm Jouppi, George Kurian, Sheng Li, Peter Ma, Rahul Nagarajan, Lifeng Nai, Nishant Patil, SuvinaySubramanian, Andy Swing, Brian Towles, et al. Tpu v4: An optically recongurable supercomputer formachine learning with hardware support for embeddings. In Proceedings of the 50th Annual InternationalSymposium on Computer Architecture, 2023. 2 Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray,Alec Radford, Jerey Wu, and Dario Amodei. Scaling laws for neural language models. ArXiv preprint,2020. 3 Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarialnetworks. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach,CA, USA, June 16-20, 2019, 2019. 2, 3",
  "Bo-Kyeong Kim, Hyoung-Kyu Song, Thibault Castells, and Shinkook Choi. On architectural compressionof text-to-image diusion models. ArXiv preprint, 2023a. 2, 3": "Bo-Kyeong Kim, Hyoung-Kyu Song, Thibault Castells, and Shinkook Choi. Bk-sdm: Architecturally com-pressed stable diusion for ecient text-to-image generation. In Workshop on Ecient Systems for Foun-dation Models@ ICML2023, 2023b. 2, 3 Diederik P. Kingma and Max Welling. Auto-encoding variational bayes. In 2nd International Conferenceon Learning Representations, ICLR 2014, Ban, AB, Canada, April 14-16, 2014, Conference Track Pro-ceedings, 2014. 3 Yanyu Li, Huan Wang, Qing Jin, Ju Hu, Pavlo Chemerys, Yun Fu, Yanzhi Wang, Sergey Tulyakov, and JianRen. Snapfusion: Text-to-image diusion model on mobile devices within two seconds. ArXiv preprint,2023. 2, 3 Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis,Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution text-to-3d content creation. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023. 1",
  "Kangfu Mei, Mo Zhou, and Vishal M Patel. T1: Scaling diusion probabilistic elds to high-resolution onunied visual modalities. ArXiv preprint, 2023. 1, 13": "Kangfu Mei, Mauricio Delbracio, Hossein Talebi, Zhengzhong Tu, Vishal M Patel, and Peyman Milanfar.Codi: Conditional diusion distillation for higher-delity and faster image generation. In Proceedings ofthe IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024a. 2, 3, 12 Kangfu Mei, Luis Figueroa, Zhe Lin, Zhihong Ding, Scott Cohen, and Vishal M Patel. Latent feature-guideddiusion models for shadow removal. In Proceedings of the IEEE/CVF Winter Conference on Applicationsof Computer Vision, pp. 43134322, 2024b. 3 Kangfu Mei, Nithin Gopalakrishnan Nair, and Vishal M Patel.Improving conditional diusion modelsthrough re-noising from unconditional diusion priors. In Proceedings of the IEEE/CVF Winter Confer-ence on Applications of Computer Vision, 2025. 3 Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization for gen-erative adversarial networks. In 6th International Conference on Learning Representations, ICLR 2018,Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings, 2018. 3 Alexander Quinn Nichol and Prafulla Dhariwal.Improved denoising diusion probabilistic models.InProceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021,Virtual Event, Proceedings of Machine Learning Research, 2021. 3",
  "Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditionalimage generation with clip latents. ArXiv preprint, 2022. 3": "Scott E. Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran, Bernt Schiele, and Honglak Lee. Gener-ative adversarial text to image synthesis. In Proceedings of the 33nd International Conference on MachineLearning, ICML 2016, New York City, NY, USA, June 19-24, 2016, JMLR Workshop and ConferenceProceedings, 2016. 3 Mengwei Ren, Mauricio Delbracio, Hossein Talebi, Guido Gerig, and Peyman Milanfar. Multiscale structureguided diusion for image deblurring.In Proceedings of the IEEE/CVF International Conference onComputer Vision, 2023. 1 Danilo Jimenez Rezende and Shakir Mohamed. Variational inference with normalizing ows. In Proceedingsof the 32nd International Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015,JMLR Workshop and Conference Proceedings, 2015. 3 Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjrn Ommer. High-resolutionimage synthesis with latent diusion models. In Proceedings of the IEEE/CVF conference on computervision and pattern recognition, 2022. 1, 2, 3, 5, 8, 13 Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kr Aberman. Dream-booth: Fine tuning text-to-image diusion models for subject-driven generation. In Proceedings of theIEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023. 2, 3, 7",
  "Hshmat Sahak, Daniel Watson, Chitwan Saharia, and David Fleet. Denoising diusion probabilistic modelsfor robust image super-resolution in the wild. ArXiv preprint, 2023. 2, 7": "Chitwan Saharia, Jonathan Ho, William Chan, Tim Salimans, David J Fleet, and Mohammad Norouzi.Image super-resolution via iterative renement. IEEE Transactions on Pattern Analysis and MachineIntelligence, (4), 2022. 2, 7 Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diusion models. In The TenthInternational Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022,2022. 2, 12",
  "Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. 2023. 2, 12": "Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bash-lykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and ne-tunedchat models. ArXiv preprint, 2023. 3 Arash Vahdat and Jan Kautz. NVAE: A deep hierarchical variational autoencoder. In Advances in NeuralInformation Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020,NeurIPS 2020, December 6-12, 2020, virtual, 2020. 3 Xintao Wang, Liangbin Xie, Chao Dong, and Ying Shan. Real-esrgan: Training real-world blind super-resolution with pure synthetic data. In IEEE/CVF International Conference on Computer Vision Work-shops, ICCVW 2021, Montreal, BC, Canada, October 11-17, 2021, 2021. 3, 6 Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian Lei, Yuchao Gu, Yufei Shi, Wynne Hsu, YingShan, Xiaohu Qie, and Mike Zheng Shou. Tune-a-video: One-shot tuning of image diusion models fortext-to-video generation. In Proceedings of the IEEE/CVF International Conference on Computer Vision,2023. 1",
  "Yanwu Xu, Yang Zhao, Zhisheng Xiao, and Tingbo Hou. Ufogen: You forward once large scale text-to-imagegeneration via diusion gans. ArXiv preprint, 2023. 2, 3": "Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexan-der Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive models for content-rich text-to-image generation. ArXiv preprint, 2022. 4 Han Zhang, Jing Yu Koh, Jason Baldridge, Honglak Lee, and Yinfei Yang. Cross-modal contrastive learningfor text-to-image generation. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR2021, virtual, June 19-25, 2021, 2021. 13"
}