{
  "Abstract": "The training of over-parameterized neural networks has received much study in recent lit-erature. An important consideration is the regularization of over-parameterized networksdue to their highly nonconvex and nonlinear geometry. In this paper, we study noise injec-tion algorithms, which can regularize the Hessian of the loss, leading to regions with flatloss surfaces. Specifically, by injecting isotropic Gaussian noise into the weight matrices ofa neural network, we can obtain an approximately unbiased estimate of the trace of theHessian. However, naively implementing the noise injection via adding noise to the weightmatrices before backpropagation presents limited empirical improvements. To address thislimitation, we design a two-point estimate of the Hessian penalty, which injects noise intothe weight matrices along both positive and negative directions of the random noise. Inparticular, this two-point estimate eliminates the variance of the first-order Taylors expan-sion term on the Hessian. We show a PAC-Bayes generalization bound that depends on thetrace of the Hessian (and the radius of the weight space), which can be measured from data. We conduct a detailed experimental study to validate our approach and show that it caneffectively regularize the Hessian and improve generalization. First, our algorithm can out-perform prior approaches on sharpness-reduced training, delivering up to a 2.4% test ac-curacy increase for fine-tuning ResNets on six image classification datasets. Moreover, thetrace of the Hessian reduces by 15.8%, and the largest eigenvalue is reduced by 9.7% withour approach. We also find that the regularization of the Hessian can be combined withalternative regularization methods, such as weight decay and data augmentation, leadingto stronger regularization.Second, our approach remains highly effective for improvinggeneralization in pretraining multimodal CLIP models and chain-of-thought fine-tuning.",
  "Introduction": "The loss landscape and its geometry properties are a recurring theme in the study of neural networks(Keskar et al., 2017; Hochreiter & Schmidhuber, 1997). Recently, the design of training methods such assharpness-aware minimization and stochastic weight averaging has led to empirical advances in a wide varietyof settings (Izmailov et al., 2018; Foret et al., 2021; Wortsman et al., 2022). The theoretical study of thesetraining methods has also been explored (Andriushchenko & Flammarion, 2022). For instance, recent workshows that sharpness-aware minimization (Foret et al., 2021) has an implicit bias to flat surface regions bypenalizing the largest eigenvalue of the loss Hessian matrix (Wen et al., 2023; Bartlett et al., 2023). In thispaper, we study methods that can provide explicit regularization of the trace of the Hessian, and we will",
  "#": ": An illustration of one update step in our algorithm. At each iteration i, we sample a randomvariable Ui from a zero-mean distribution P (e.g., an isotropic Gaussian with variance 2), where is ahyper-parameter that controls the strength of the noise injection (hence the regularization). We query thegradient of f, at f(Wi + Ui), and f(Wi Ui), and take their average. This results in a two-point noiseinjection scheme, whose computation cost is the same as sharpness-aware minimization (Foret et al., 2021),and twice the cost of running SGD. Notice that in practice, we can also implement an extension of thisalgorithm, which samples multiple Us. For details, see Algorithm 1. show provable generalization guarantees of our methods. More formally, given an input function f : Rd Rthat represents the empirical risk of a neural network and a d-dimensional distribution P with mean zero,we consider minimizing the noise-perturbed function",
  "F(W) :=EUP [f(W + U)] .(1)": "Minimizing this perturbed function can improve the resilience of the neural network to noise injection, leadingto flatter loss surfaces and improved regularization (Nagarajan & Kolter, 2020). By analyzing the perturbedloss of a fine-tuned model, one can identify a measure of the sharpness of loss surfaces based on the traceof the Hessian (Ju et al., 2022; 2023). We remark that the minimization problem of the form (1) tracesback to earlier works on randomized smoothing (Duchi et al., 2012), which have provided a detailed study ofconvergence rates for nonsmooth stochastic optimization. Our work differs from this line of literature in thatwe focus on evaluating the regularization effect of penalizing the Hessian trace upon neural network training. Although noise injection algorithms can be theoretically motivated as improving generalization (and sta-bility), its practical implication is not evident (Hinton & Van Camp, 1993; An, 1996; Graves, 2011). Tomotivate our study, we begin by running several empirical studies to compare the performance of (standard)SGD and weight-perturbed SGD (WP-SGD), which first injects random noise into the weight matrices of aneural network before computing its gradient in SGD. As mentioned above, this would provide a randomizedsmoothing effect to the loss surface (Duchi et al., 2012). We will fine-tune (pretrained) ResNets on threeimage classification tasks for this empirical study. To ensure the robustness of the analysis, we also vary thedistribution of P and the variance of U. Our overall finding is that WP-SGD (or randomized smoothing)does not offer clear benefits over SGD, which is also consistent with recent studies of weight noise injection(Orvieto et al., 2023; Dauphin et al., 2024) (see .2, for the complete results). However,we hypothesize that these results may be due to the randomness of the noise injection (upon the Hessianpenalty term) rather than the ineffectiveness of regularizing the Hessian trace. Our approach to mitigate the randomness of the noise injection on the Hessian penalty involves two parts.First, we retrieve the gradient at W U to cancel out the first-order expansion term of W + U (recall thatU is a random sample from P). Meanwhile, the second-order expansion term remains the same after thiscancellation. We term this modification a two-point noise injection scheme, which is reminiscent of two-pointgradient estimates in zeroth-order optimization (Duchi et al., 2015). The difference in our setting is that thistwo-point averaging cancels out the first-order gradient term, thereby eliminating its variance on the Hessian",
  "Published in Transactions on Machine Learning Research (09/2024)": "Remark A.6. When f is strongly convex, the lowest eigenvalue of the Hessian is bounded from below. Oncethe algorithm reaches the global minimizer, our result from Theorem 6 can be used to provide a generalizationbound based on the trace of the Hessian. Notice that the noise injection will add some bias to this minimizer,leading to a sub-optimal empirical loss. To remedy this issue, one can place the regularization of Hessian asa constraint, similar to how 2-regularization can be implemented as a constraint.",
  "penalty. Second, we sample multiple perturbations U1, U2, . . . , Uk at each epoch and take their averagedtwo-point (noise-injected) gradients. See for an illustration of one step": "A primary advantage of our approach compared to prior sharpness minimization algorithms is that ourapproach can provide an approximately unbiased estimate of the Hessian trace. We empirically validatethis claim across three real-world settings (see , .2 for an illustration). By utilizing thisproperty, we show a PAC-Bayes bound that depends on the trace of the Hessian and the radius of the weighthypothesis space. We briefly describe this result, leaving a formal statement to Theorem 2.1. Let be anupper bound on the trace of the Hessian measured within the hypothesis space and the data distribution (inpractice, one may take this as the union of training and testing data). Let r be the radius of the hypothesisspace, measured in 2 distance. Suppose there are n empirical samples from an unknown distribution. We",
  "; McAllester, 2013; Alquier, 2021), and we optimize the variance of the prior and posterior distributionsto derive the result. A detailed proof sketch is presented in .3": "Next, we validate our approach with a detailed empirical study. First, we compare our approach with fourprior approaches for the setting of fine-tuning pretrained ResNets, including sharpness-aware minimization(Foret et al., 2021), tested on six image classification datasets. We show that our algorithm can reduce thetrace and the largest eigenvalue of the loss Hessian matrix by 15.8% and 9.7%, respectively. Our approachalso improves test accuracy by 2.4%. Second, we show that by combining our approach with regularizationmethods such as data augmentation and distance-based regularization (Gouk et al., 2022), we can furtherregularize the Hessien, leading to 13.6% lower trace values and 16.3% lower test loss values (averaged oversix tasks). Third, we extend our approach to pretraining and chain-of-thought fine-tuning. The details canbe found in .2. Overall, our algorithm can consistently provide better regularization of Hessian andimproved test accuracy across these different settings and datasets. Some of these empirical results are notcompletely explained by our theory, and we discuss the limitations in . Finally, we analyze the convergence of our algorithm using techniques from the stochastic optimizationliterature (Ghadimi & Lan, 2013; Lan, 2020; Carmon et al., 2020; Drori & Shamir, 2020; Zhang, 2023),leading to matching upper and lower bounds. We also present a case study of Hessian regularization in over-parametrized matrix sensing and show that it is equivalent to nuclear norm regularization for this setting.Our work raises several new questions that may be worth revisiting: can accelerated gradient descent methodsbe applied to design flat-minima optimizers? Can recent advances in zeroth-order optimization be leveragedto better regularize the training of transformer neural networks? In summary, the contributions of this paper are three-fold. First, we present an algorithm that can explicitlyregularize the Hessian trace and show a PAC-Bayes generalization bound that could be measured from data.Second, we conduct experiments on multiple settings to validate our approach by comparing downstreamperformance and Hessian statistics with prior sharpness minimization algorithms and alternative regulariza-tion methods. Third, we analyze the convergence of our algorithm using stochastic optimization techniques.In , we highlight the key aspects of our approach compared to prior approaches.",
  "Our Approach": "In this section, we present our approach.First, to set up the stage, we will study the straightforwardimplementation of noise injection by directly adding noise to the weight matrices of the neural network beforecomputing the gradients in backpropagation. We term this procedure weight-perturbed SGD (or WP-SGD inshort), also known as randomized smoothing (Duchi et al., 2012). We will compare the empirical performanceof these two approaches to evaluate the effect of noise injection. Then, we describe our algorithm and provideempirical measurements of the trace of the Hessian, along with the actual perturbation gaps observed inpractice. Finally, we will show a PAC-Bayes generalization bound that depends on the trace of the Hessian,which can be measured from data to compare different methods.",
  "Motivating Experiments": "We compare the results from running WP-SGD to standard SGD. We choose the setting of fine-tuningpretrained foundation models, as overfitting is a common problem for this setting (Wortsman et al., 2022),and strong regularization is needed (Li & Zhang, 2021; Ju et al., 2022). We will fine-tune a pretrainedResNet-34 on several image classification datasets, including aircraft recognition (Aircraft) (Maji et al.,2013), indoor scene recognition (Caltech-256) (Griffin et al., 2007), and medical image classification (retinaimages for diabetic retinopathy classification) (Pachade et al., 2021). To implement WP-SGD, we samplea random vector from P and add it to the model weights at each iteration before computing the gradient.We set P as the isotropic Gaussian and adjust its standard deviation between 0.008, 0.01, and 0.012 viacross-validation. We report our results in , which indicate that the performance gap is less than 0.5%, 0.75 standarddeviations based on five independent runs. Furthermore, varying P does not change the results. In particular,we test four types of P, including Gaussian, Laplace, uniform, and Binomial. We adjust standard deviationsbetween 0.008, 0.01, and 0.012 via cross-validation. We find that using Laplace and uniform distributionsachieves a performance comparable to that of Gaussian. However, using Binomial results in worse results.These experiments suggest that the straightforward implementation of noise injection does not offer clearbenefits over SGD.",
  "Description of Our Algorithm": "In our approach, we make two modifications to WP-SGD. First, we add the perturbation from both thepositive and negative directions during the noise injection, as shown in line 5. Second, we average overmultiple noise injections to reduce the variance from noise injection, as described in line 7. As for the firstmodification, recall that P is a symmetric distribution. We use Taylors expansion on both f(W + U) andf(W U) as follows:",
  "SGDNone100.0% 0.059.8% 0.7100.0% 0.076.0% 0.4100.0% 0.061.7% 0.8": "WP-SGDGaussian98.4% 0.260.4% 0.199.0% 0.376.3% 0.0100.0% 0.062.3% 0.5WP-SGDLaplace98.3% 0.160.3% 0.398.9% 0.176.4% 0.3100.0% 0.062.0% 0.1WP-SGDUniform98.6% 0.360.3% 0.598.6% 0.376.6% 0.1100.0% 0.062.3% 0.0WP-SGDBinomial19.6% 0.111.3% 0.118.2% 0.910.7% 0.158.1% 0.157.1% 0.0 NSOGaussian95.8% 0.462.3% 0.395.7% 0.277.4% 0.3100.0% 0.066.6% 0.7NSOLaplace96.5% 0.361.9% 0.396.1% 0.377.1% 0.1100.0% 0.065.9% 0.1NSOUniform96.4% 0.461.9% 0.596.4% 0.276.8% 0.2100.0% 0.065.7% 0.1NSOBinomial20.1% 0.114.3% 0.322.8% 0.117.9% 0.259.2% 0.157.8% 0.1",
  ".(4)": "We can see that the two-point estimate eliminates the first-order gradient term, potentially reducing its vari-ance in estimating the Hessian term. The second modification reduces the variance of the stochastic gradient,using the fact that each perturbation is independent of the others. The entire procedure is summarized inAlgorithm 1. As a remark, two-point gradient estimators are commonly used in zeroth-order optimization(Duchi et al., 2015). However, their use in designing flat minima optimizers has not been explored much.",
  ": end for": "Measurements of the Hessian trace and the perturbation gap:Next, we provide several examplesto measure the approximation quality of equation (4). Following the experimental setup of .1, wewill fine-tune a foundation model on a downstream task. After training, we will set W as the model weightat the last epoch for all the measurements. To measure equation (4), we then add U to W, where U is sampled from an isotropic Gaussian. We willmeasure f(W + U) f(W), averaged over 100 independent samples of U, and we measure this and 2f(W)by taking the average over the training dataset. The results are shown in . We can see that 2f provides an accurate approximation to F(W)f(W)for various values of . In particular, the approximation error of equation (4) using the Hessian trace is less",
  "Trace 22": ": Illustration of the approximation quality of equation (4). We report all measurements based on thenetwork weight at the last epoch of fine-tuning. We can see that the perturbation gap (i.e., F(W) f(W)in equation (4)) and 2 2 Tr[2f(W)] are at the same order. Recall that refers to the standard deviation ofthe Gaussian noise injected into the weight matrices. More specifically, will decide the strength of noiseinjection or the strength of regularization on the Hessian trace.",
  "Generalization Guarantee and Proof Sketch": "Next, we present a PAC-Bayes generalization bound that depends on the trace of the Hessian. Our boundcan be related to the notion of trace norm, which has been used in earlier works for quantifying samplecomplexity in the context of matrix recovery (Srebro & Shraibman, 2005). Concretely, suppose we have a pretrained model in the fine-tuning setting. This can be viewed as our priorbelief of the target hypothesis in PAC-Bayes analysis. Once we have learned a model (though fine-tuning), wecan view this as the posterior in PAC-Bayes analysis. Let D X Y be an unknown data distribution, sup-ported on the feature space X and the label space Y. Given n random samples (x1, y1), (x2, y2), . . . , (xn, yn)drawn from D, the empirical loss (measured by loss function ) applied to a model fW (with W Rp) is:",
  "i=1(fW (xi), yi)": "The population loss is L(W) = E(x,y)D [(fW (x), y)] . It is sufficient to think that the empirical loss is lessthan the population loss, and the goal is to bound the gap between L(W) and L(W) from above (Shalev-Shwartz & Ben-David, 2014). Let W be any learned hypothesis within the hypothesis space, denoted as H. Our generalization bound willapply uniformly to W within the hypothesis space. We state our result, including the required assumptions,as follows. Theorem 2.1. Assume that the loss function is bounded between 0 and C for a fixed constant C > 0 on thedata distribution D. Suppose (fW (), ) is twice-differentiable in W and the Hessian matrix 2[(fW (), )]is Lipschitz continuous within the hypothesis space. Suppose for any W in H, the trace norm of the Hessianis less than :",
  "Q(fW (x), y) = EU [(fW +U(x), y)] .(7)": "Then, let LQ(W) be the averaged value of Q(fW (), ), taken over n empirical samples from the trainingdataset. Likewise, let LQ(W) be the population average of Q(fW (), ), in expectation over an unseen datasample from the underlying data distribution. Having introduced the notations, we start with the linear PAC-Bayes bound (Catoni, 2007; McAllester, 2013;Alquier, 2021) (see Theorem A.1 for reference), stated as follows, which holds with probability 1 for any (0, 1) and (0, 1):",
  "i=1Tr2(fW (xi), yi)+ O(3).(10)": "Since the Hessian operator is Lipschitz continuous by the assumption of Theorem 2.1, we can bound the gapbetween the above two quantities with -covering arguments (see Lemma A.5 for the precise statement). Byplugging in these results back to the PAC-Bayes bound of equation (8), after some calculation, we can get:",
  ".(11)": "In particular, the above uses the fact that the 2-norm of W is less than r for any W H (the KL divergenceis discussed in Proposition A.2). By choosing 2 and to minimize equation (11), we will obtain equation(6). This summarizes the high-level proof idea. The complete proof can be found in Appendix A.1. Remark 2.2. We highlight two key aspects of our results. The first is that our PAC-Bayes bound is non-vacuous, meaning that it matches the scale of empirically observed gaps when measured in practice; thisis based on the trace measurements in Figures 2 and 3. The second is that this non-vacuous bound haspractical implications, meaning that we can utilize this bound to design optimization algorithms that improvegeneralization. These are non-trivial to achieve.To give some context, prior work has provided a PAC-Bayes marginbound for multi-layer neural networks, which depends on the product of the spectral norm of the networklayers (Neyshabur et al., 2018). While this paper provides important insights regarding the generalizationof deep networks, the bound is vacuous when measured in practice. Arora et al. (2018) provide anotherdata-dependent PAC-Bayes bound based on compression techniques. Their work started with an experimentin which they injected noise into the network layers and showed that deep nets can absorb the noise afterretraining. However, their bound remains orders of magnitude higher than the actual generalization errorsobserved in practice. In contrast, our bound matches the scale of empirically observed gaps. To achieve this, we start from theline of work on data-dependent PAC-Bayes bounds. We build on the line of work on distance from theinitialization (Nagarajan & Kolter, 2020), which is ideal for understanding fine-tuning (Li & Zhang, 2021).",
  "Our key breakthrough is to connect noise stability in PAC-Bayes bound with the loss Hessian matrix (cf.equations (9) and (10)). Then, we can measure the Hessian of loss landscapes from data": "We additionally note that few existing works have considered using PAC-Bayes bounds to design algorithms.The reason is that for new algorithm designs, we need to connect the PAC-Bayes bound with data in a non-vacuous way. The work of Dziugaite & Roy (2017) has provided a computational framework to achieve non-vacuous generalization bounds. Instead, our result provides an analytical expression that can be leveraged inalgorithm design. To operationalize the design, we utilize the explicit dependence of our result on the Hessianto design the regularization scheme.",
  "Experiments": "We now turn to empirical validations of our algorithm. First, we apply our approach to fine-tune pretrainedResNets on various image classification datasets. We find that NSO can more significantly regularize theHessian of the loss surface, resulting in reductions in the trace and the largest eigenvalue by 15.8% and9.7%, respectively. After controlling computation costs, it can outperform four sharpness-reducing meth-ods by up to 2.4%. In addition, we justify our algorithm design through detailed ablation analysis. Wealso show that our approach is compatible with alternative regularization techniques, including distance-based regularization and data augmentation, and combining these methods with our approach leads to moresignificant regularization and test performance. Second, we show similar results for pretraining and chain-of-thought fine-tuning. The experiment code for reproducing our empirical findings can be found online at:",
  "Comparison with Sharpness Minimization Methods": "We now compare Algorithm 1 with five sharpness-reducing training methods, including sharpness-awareminimization (SAM) (Foret et al., 2021), unnormalized SAM (USAM) (Agarwala & Dauphin, 2023), adaptivevariants of SAM (ASAM), and random SAM (RSAM) (Liu et al., 2022). During the comparison, we controlfor the same amount of computation (for Algorithm 1, we will set the number of sampled injections k as1). Thus, all the methods under consideration will use twice the computation of SGD. For NSO, we sampleperturbation from an isotropic Gaussian distribution and adjust between 0.008, 0.01, and 0.012. For SAM,we adjust the 2 norm of the perturbation between 0.01, 0.02, and 0.05. For each method, we run it withboth momentum and weight decay. We ensure that all the training methods are carefully adjusted. SeeAppendix C for the details.",
  "Empirical Findings": "In , we report the comparison between NSO, SGD, SAM, unnormalized SAM (USAM), and adaptiveSAM (ASAM). We find that our approach reduces the trace of Hessian by 15.8% on average. The largesteigenvalue of the Hessian is also reduced by 9.7%. This finding is intriguing since SAM has been motivatedby a min-max problem. As for test accuracy, our approach can provide up to 2.4% lift, with an averageimprovement of 1.2%. Additional comparisons are deferred to in Appendix C. illustrates the measurements between SGD, WP-SGD, and NSO. Curiously, we find that the trace ofthe Hessian also decreases for SGD, possibly due to implicit norm control of SGD. While both WP-SGD andNSO reduce the trace of the Hessian, our approach penalizes the Hessian more. Besides, the generalizationgap and the test loss are consistently lower during NSO training. As a remark, the regularization effect of noise injection should be orthogonal to training methods such asmomentum, weight decay, learning rate scheduling, etc. To this end, we performed comparisons withoutusing either momentum or weight decay. Our approach can again reduce the trace of the Hessian by 17.7%compared to the five sharpness-reducing methods on average, with up to 1.8% higher test accuracy.",
  "BERT Base": ": Comparison between SGD, WP-SGD, and NSO for fine-tuning ResNet-34 and BERT-Base, re-spectively, on an image and a text classification dataset. We evaluate the test loss, the trace of the Hessian,and the generalization gap for the trained model at each epoch. For WP-SGD and NSO, we sample noisefrom isotropic Gaussian with standard deviation = 0.01 in both settings.",
  "Next, we conduct ablation studies of two modifications in our approach: the use of negative perturbationsand the sampling of multiple perturbations": "Comparing using negative cancellation or not after controlling computation costs:Recall thatour algorithm uses negative perturbations to zero out the first-order term in Taylors expansion of F(W).We validate this by comparing the performance between using or not using the negative perturbation. Wecontrol for the same amount of computation costs to ensure a fair comparison. In particular, we sampletwo independent perturbations and take their averaged stochastic gradient. We find that using the nega-",
  "tive perturbation achieves a 3.6% improvement in test accuracy (on average) over not using the negativeperturbation, i.e., randomized smoothing": "As discussed in .2, our intuition on why NSO can be expected to generalize better than randomizedsmoothing is that it can better regularize the Hessian. In particular, even though, in theory, the expectationof f(W + U) and 1 2(f(W + U) + f(W U)) over U are both equal to F(W). However, the two-point schemecancels out the gradient expansion term compared to randomized smoothing at every epoch. More precisely,we believe that the improved regularization from our approach stems from its better estimate of the Hessianpenalty term. As illustrated in , NSO consistently reduces the trace of the Hessian and achieveslower generalization errors compared to randomized smoothing throughout model training. At the end ofthe training, NSO yields 10.6% smaller trace of the Hessian on average than randomized smoothing. Increasing the number of noise injections k:Recall that increasing the number of perturbations kcan reduce the variance of the estimated gradient. Thus, we consider increasing k in NSO and comparethat with a specific implementation of WP-SGD that uses the same amount of computation. Using k = 2perturbations improves the test accuracy by 1.2% on average compared to k = 1. Varying the learning rate and the number of epochs.We provide a detailed comparison betweenNSO and WP-SGD when varying the learning rate and the number of epochs. The learning rate is varied be-tween 0.0001, 0.0002, 0.0005, 0.001, 0.002, and 0.005. The number of epochs is varied between 10, 20, 30, 40, 50,and 60. We report the test loss from running an image classification task in .",
  ": Results of varying the learning rate and the number of epochs for running our approach andWP-SGD. We report the test loss from the last epoch and average the results over five random seeds": "Remark 3.1 (Noise variance scheduling as k increases). A natural question is whether one can graduallyincrease or decrease the regularization strength by during training, similar to learning rate scheduling. Tofacilitate this discussion, we test two schedules for adjusting . The first schedule is to increase to aspecified value at a linear rate. The second schedule exponentially increases to reach a specified value. Ourpreliminary experiments show that neither schedule offers significant performance improvements over usinga constant noise variance. One might also consider other scheduling schemes; we leave this to future work.",
  "Detailed Comparison with Sharpness-Aware Minimization (SAM)": "Varying the radius of SAM: We provide a detailed comparison to SAM by varying the perturbation radiusof SAM (denoted as ). To illustrate this comparison, we vary between 0.001, 0.002, 0.005, 0.01, 0.02, and0.05. We report both the validation accuracy and the trace of the Hessian for SAM and unnormalized SAMon an image classification dataset. We present the results in . We observe that using a smaller (i.e.,less than 0.01) results in worse results. Thus, we choose between 0.01, 0.02, and 0.05 in our experiments. Varying the batch size of SAM:Next, we measure the sensitivity of our approach concerning the batchsize. In particular, we vary the batch size between 8, 16, 32, and 64 for fine-tuning ResNet-34 on two imageclassification datasets. The results are shown in the leftmost two panels of . We use the same numberof epochs for each batch size configuration to ensure a fair comparison. On the indoor dataset, our approachis less sensitive to different batch sizes than SAM. Across all the batch sizes and datasets, our approachconsistently provides a more robust regularization of the Hessian compared to SAM. The best results areachieved when the batch size is 32. Thus, we use this particular setting in our experiments.",
  "(h) Hessian, w/ data aug": ": Results of varying the batch size of our approach and SAM ran on two image classification datasets(indoor scene recognition and Aircraft detection). We report the test loss and the trace of Hessian using themodel from the last epoch of training. The results are averaged over five random seeds. The regularizationprovided by noise injection can be combined with distance-based regularization and data augmentation toreduce the test loss and the Hessian trace.",
  "Combining Algorithm 1 with Alternative Regularization Methods": "In this section, we show that the regularization of the Hessian can serve as a complement to existing, alterna-tive regularization methods. To validate this, we combine our training approach with data augmentation anddistance-based regularization (Gouk et al., 2022). In particular, the latter approach has been used to regular-ize fine-tuning algorithms. We use a popular scheme for data augmentation that applies random horizontalflipping and random cropping sequentially to each training image. As for distance-based regularization, wepenalize the 2 distance between the fine-tuned model and the pretrained initialization. The results are shown in within the two rightmost panels. Combining our approach with eachregularization method further reduces the trace of the loss Hessian matrix by 13.6% (on average). Thisfurther leads to 16.3% lower test loss of the fine-tuned network, suggesting that our approach can be usedon top of these preexisting regularization methods.",
  "Convergence Rates": "We now study the convergence of Algorithm 1. Recall that our algorithm minimizes f(W) plus a regulariza-tion term on the Hessian trace. As is typical with regularization, the penalty is usually small relative to theloss value. Thus, we aim to find a stationary point of F(W) instead of f(W) because otherwise, we wouldnot have the desired regularization. We state the convergence to an approximate stationary point such thatF(W) is small, building on the following gradient oracle assumption (see, e.g., Ghadimi & Lan (2013);Duchi et al. (2015)). Assumption 4.1. Given a random seed z, let gz : Rd Rd be a continuous function that gives an unbiasedestimate of the gradient: Ez [gz(W)] = f(W), for any W Rd. Additionally, the variance is bounded in",
  "the sense that Ezgz(W) f(W)2 2": "To help understand the above assumption, suppose there is a dataset of size n. Then, in SGD, the stochasticgradient would be an unbiased estimate of the gradient of the entire dataset. As for the variance of thegradient estimator, we note that as long as the 2 norm of the gradient remains bounded, which will alwayshold in practice, then the last equation of the above assumption will hold. We now state an upper boundon the convergence rate of Algorithm 1.",
  "T.(12)": "As a remark, existing sharpness-reducing methods such as SAM seem to suffer from oscillation around thelocal basin (Bartlett et al., 2023). Thus, the convergence behavior of SAM seems challenging to analyze fornonconvex functions. By contrast, our algorithm is amenable to stochastic optimization techniques. Ourproof slightly extends the proof of Theorem 2.1, Ghadimi & Lan (2013), to tackle noise injection and othervariations. For details, see Appendix B.1. Lower bounds:Next, we construct an example to match the rate of equation (12), essentially showing thatthis is tight under the same set of assumptions. We use an example from the work of Drori & Shamir (2020).The difference is that we have to deal with the perturbations added to the objective. For t = 0, 1, . . . , d 1,let et Rd be the basis vector in dimension d, whose t-th coordinate is 1, while the remaining coordinatesare all zero. Let f : Rd R be defined as",
  "One can verify that for each piece above, hi is C-Lipschitz. As a result, provided that G C1, f isC-Lipschitz, based on the definition of f in equation (13)": "The stochastic function F requires setting the perturbation distribution P. We set P by truncating anisotropic Gaussian N(0, 2 Idd) so that the i-th coordinate is at most 21i1, for i = 1, . . . , T. Additionally,we set the initialization W0 to satisfy W0, ei = 0 for any i 1 while W0, e0 = 0. Finally, we choose thegradient oracle to satisfy that the i-th steps gradient noise i = i, ei+1ei+1, which means that i is alongthe direction of the basis vector ei+1. In particular, this implies only coordinate i + 1 is updated in step i,as long as i, ei+1 21i. With this construction, we state the lower bound below.",
  "32k T .(14)": "We remark that the above construction requires T d. Notice that this is purely for technical reasons. Webriefly illustrate the key steps of the proof. At step i, the gradient noise i plus the perturbation noise isless than 21i + 21i = i at coordinate i + 1 (by triangle inequality). Thus, hi(Wt, ei+1) = 0, whichholds for all prior update steps. This implies",
  "In Lemma B.3, we then argue that the learning rates in this case must satisfy T 1i=0 i O(": "T). Whenthe learning rate is fixed and at least (T 1/2), we construct a piece-wise quadratic function (similar toequation (13)), now with a fixed . This is described in Lemma B.4. In this case, the gradient noise growsby 1 C1 up to T steps. We then carefully set to lower bound the norm of the gradient. Combiningthese two cases, we conclude the proof of Theorem 4.3. For details, see Appendix B.2. As is typical inlower-bound constructions, our result holds for a specific instance (with a particular learning rate range). The proof can also be extended to adaptive learning rate schedules. Notice that the above constructionholds for arbitrary learning rates defined as a function of previous iterates. Then, we set the width of eachfunction ht, t, proportional to t > 0, for any t that may depend on previous iterates, as long as theysatisfy the constraint that T 1i=0 i O(",
  "Regularization Effect of Hessian Trace in Over-Parameterized Matrix Sensing": "Before proceeding, let us give an example of the regularization effect of penalizing the Hessian trace. Weconsider the matrix sensing problem, whose generalization properties are particularly well-understood in thenonconvex factorization setting (Li et al., 2018). Let there be an unknown, rank-r positive semi-definite ma-trix X = U U Rdd. The input consists of a list of d by d Gaussian measurement matrix A1, A2, . . . , An.The labels are given by yi = Ai, X, for every i = 1, 2, . . . n. The empirical loss is",
  "The last step is by setting u = d11d2, whose length is equal to one. The proof of Proposition 5.1 can befound in Appendix A.2": "Simulation:We conduct a numerical simulation to compare algorithmic behaviors. We generate a low-rank matrix U Rdr from the isotropic Gaussian. We set d = 100 and r = 5. Then, we test threealgorithms: gradient descent (GD), weight-perturbed gradient descent (WP-GD), and Algorithm 1 (NSO).In particular, we will implement the full gradient update rather than using the stochastic updates. We use aninitialization U0 Rdd where each matrix entry is sampled independently from standard Gaussian N(0, 1). Recall that WP-GD and NSO require setting . We choose between 0.001, 0.002, 0.004, 0.008, 0.0016. NSOadditionally requires setting the number of sampled perturbations k. We set k = 1 for faster computing. Asfor the learning rate, we choose a fixed for each run and vary its value between 0.001, 0.0025, 0.005, and0.01. We find that setting as either 0.005 or 0.01 would be too large, leading the loss values to explode.Hence, we report the results for setting as 0.0025 or 0.001.",
  "Discussions and Related Work": "Using noise injection during neural network training has appeared in very early studies of machine learningresearch (Hinton & Van Camp, 1993; An, 1996). Graves (2011) test a variety of variational inference ap-proaches with different prior and posterior distributions with recurrent neural networks. Cohen et al. (2019)examine the use of randomized smoothing (with different smoothing distributions) against different p ad-versaries for certified robustness. Camuto et al. (2020) propose a layer-wise regularization scheme motivatedby adaptation patterns of weights through deeper layers. Yang et al. (2020) show how to turn any classifierthat classifies well under Gaussian noise into a new classifier robust to adversarial perturbation under the2 norm. One of the implications of their work is that smoothing with Gaussian noise naturally confersadversarial robustness in the 2 norm. Bisla et al. (2022) conduct an extensive empirical study to explore theconnection between sharpness and generalization for training neural networks. Orvieto et al. (2023) analyzeTaylors expansion of the stochastic objective after noise injection, examining the induced regularization invarious neural network training settings, and find that layer-wise perturbation can improve generalization. There is also a line of work on Hessian and sharpness in the edge of stability regime during gradient descentdynamics (Cohen et al., 2021). In particular, the edge of stability refers to scenarios where the learningrate goes out of bounds beyond the Lipschitz continuity of a function, which is inversely proportional tothe largest eigenvalue of the Hessian matrix. Long & Bartlett (2024) identify the edge of stability regime",
  "F": "X2F, between GD, our approach (NSO), and weight-perturbed (WP) GD (which computes the fullgradient as opposed to the stochastic gradient). For the top panel, the learning rate is fixed at 0.0025 for allthe runs. For the bottom panel, the learning rate is set at 0.0001. is set as 0.008 for WP-GD and NSO.Also, we trained sufficiently long until the loss curves fully converged. for the SAM algorithm, highlighting the differences of these regimes between SAM and gradient descent.Agarwala & Dauphin (2023) present a detailed study of the gradient dynamics of SAM, documenting variousrespects of this algorithm. They first analyze the full-batch gradient descent with unnormalized SAM in aquadratic regression model. This analysis suggests that at initialization, full-batch SAM presents limitedsuppression of the largest eigenvalue of the Hessian matrix. They also show that as the batch size decreases,the regularization of SAM becomes stronger. This work underscores the intricate dynamics of SAM dueto its connection to the min-max problem, which is computationally intractable (Daskalakis et al., 2021).Dauphin et al. (2024) provide an in-depth comparison between SAM and weight noise by examining thestructure of the Hessian during training. Our results in .1, which show that weight noise remainsineffective (for fine-tuning), are consistent with the findings of this work. Wu et al. (2020) study the structureof the Hessian and conduct experiments on how the Hessian structure changes based on architecture andthe training method. Randomized smoothing has been studied in stochastic optimization under various contexts, for instance, es-timating gradients in zeroth-order optimization (Duchi et al., 2015), and for nonsmooth convex optimizationproblems (Duchi et al., 2012). In particular, Duchi et al. (2012) analyze the convergence rates of stochasticoptimization algorithms and examine a convolution-based smoothing technique for nonsmooth stochastic op-timization problems by drawing stochastic gradient samples from the smoothed problem with an appropriatechoice of smoothing density. They show that with the ability to issue several queries to the stochastic oracle,the original problem can be solved with faster convergence rates than a simple stochastic oracle. Besides,recent research has investigated the query complexity of finding stationary points of nonconvex functions(Carmon et al., 2020; Arjevani et al., 2023). These results provide a fine-grained characterization of thecomplexity of iterative methods under different orders of gradient oracles.",
  "Conclusion and Limitations": "This paper examines the regularization and generalization effects of noise-injection methods for trainingneural networks. The study begins by noting that a straightforward implementation of injecting noise intoweight matrices (of a neural network) before computing the gradient in SGD does not perform well inpractice. Thus, an alternative, two-point noise injection scheme is proposed and is shown to be effectivethrough extensive experiments. In particular, this new algorithm can be used to regularize the Hessianand improve generalization. The results are tested on fine-tuning, pretraining, and instruction tuning. Asa complement, a PAC-Bayes generalization bound is provided to support the rationale of this approach.Finally, this paper presents a detailed convergence analysis of the proposed algorithm. Limitations:In Theorem 2.1, we have shown that the generalization error of a training algorithm can bebounded by the trace of the Hessian of the loss matrix, scaled by the distance of the hypothesis space. Noticethat this result applies to both Algorithm 1 (NSO) and the naive noise injection algorithm (WP-SGD). Asshown in , this result can provide a descriptive measure to explain different algorithms. Since theHessian measurements can be used on both algorithms, they can only distinguish one from another aftertaking the measurements from the data. Thus, our generalization theory should be interpreted with thisdata-dependent lens in mind. We hope future work could work on addressing such limitations, along withdesigning more principled optimization algorithms for training neural networks.",
  "Acknowledgements": "H. Z. would like to thank Huy Nguyen, Zhiyuan Li, and Guanghui Lan for the discussions and for pointingout several references during various stages of this work. The authors would also like to thank the anonymousreviewers and the action editor for their constructive feedback. We acknowledge financial support from NSFaward IIS-2412008.",
  "Antonio Orvieto, Anant Raj, Hans Kersting, and Francis Bach. Explicit regularization in overparametrizedmodels via noise injection. AISTATS, 2023. 2, 15": "Samiksha Pachade, Prasanna Porwal, Dhanshree Thulkar, Manesh Kokare, Girish Deshmukh, Vivek Sa-hasrabuddhe, Luca Giancardo, Gwenol Quellec, and Fabrice Mriaudeau. Retinal fundus multi-diseaseimage dataset (rfmid): A dataset for multi-disease detection research. Data, 6(2):14, 2021. 4 Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, GirishSastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learningtransferable visual models from natural language supervision. In ICML, 2021. 12",
  "AOmitted Proofs from": "We state a few standard notations. Given two matrices X, Y having the same dimension, let X, Y =Tr[XY ] denote the matrix inner product of X and Y . Let X2 denote the spectral norm (largest singularvalue) of X, and let XF denote the Frobenius norm of X. We use the big-O notation f(x) = O(g(x)) toindicate that there exists a fixed constant C independent of x such that f(x) C g(x) for large enough x.",
  "(1 )n.(18)": "This result provides flexibility in setting . Our results will set to balance the perturbation error of Q andthe KL divergence between P and Q. We will need the KL divergence between the prior P and the posteriorQ in the PAC-Bayesian analysis. This is stated in the following result. Proposition A.2. Suppose P = N(X, ) and Q = N(Y, ) are both Gaussian distributions with meanvectors given by X Rp, Y Rp, and population covariance matrix Rpp. The KL divergence betweenP and Q is equal to",
  "We provide Taylors expansion of Q based on the above": "Lemma A.4. In the setting of Theorem 2.1, suppose each parameter is perturbed by an independent noisedrawn from N(0, 2). Let Q(fW (x), y) be the perturbed loss with noise perturbation injection vector on W.There exist some fixed value C1 that do not grow with n and 1/ such thatQ(fW (x), y) (fW (x), y) 1",
  "Thus, the proof is complete": "The last piece we will need is the uniform convergence of the Hessian operator. The result uses the fact thatthe Hessian matrix is Lipschitz continuous.Lemma A.5. In the setting of Theorem 2.1, there exist some fixed values C2, C3 that do not grow withn and 1/, such that with probability at least 1 for any > 0, over the randomness of the n trainingexamples, we have1n",
  "(34)": "where in the second line we use that U j1iand U j2iare independent when j1 = j2, in the last line we use factthat U 1i , . . . , U ki are identically distributed. In the second step, we use the fact that for two independentrandom variables U, V , and any continuous functions h(U), g(V ), h(U) and g(V ) are still independent (recallthat f is continuous since it is twice-differentiable). We include a short proof of this fact for completeness.If U and V are independent, we have Pr[U A, V B] = Pr[U A] Pr[V B], for any A, B Borel(R).Thus, if h and g are continuous functions, we obtain",
  "EUt[f(Wt Ut) f(Wt + Ut)] = 0.(41)": "Thus, if we take the expectation over U0, U1, . . . , UT 1, 0, 1, . . . , T 1, then E [F(Wi), i + i] = 0. Recallthat t is a random variable whose probability mass is specified in Lemma B.2. We can write equation (40)equivalently as (below, we take expectation over all the random variables along the update since Wt is afunction of the previous gradient updates, for each t = 0, 1, . . . , T 1, recalling that Pr[t = i] =iT 1",
  "232i |x| 2i02i |x|": "For technical reasons, we define a truncated perturbation distribution P.Given a sample U from a d-dimensional isotropic Gaussian N(0, Idd), we truncate the i-th coordinate of U so that Ui = min(Ui, ai), forsome fixed ai > 0 that we will specify below, for all i = 0, 1, . . . , d 1. Let P denote the distribution of U.The proof of Theorem 4.3 is divided into two cases. First, we examine the case when the averaged learningrate is O(T 1/2).",
  ".(68)": "Our analysis examines the eigenvalues of the matrix XuXu and the first entry in the corresponding eigenvec-tors. Particularly, we show that the two entries are bounded away from zero. Then, we apply the Hldersinequality to reduce the case of > 0 to the case of = 0, Lemma B.8 in particular.",
  ".(69)": "By multiplying both sides by the vector e1 = , and then taking the Euclidean norm of the vector(notice that this now only evolves that Wt+1 vector on the left, and the Wt vector on the right), we nowobtain that, in expectation over the randomness of the is, the following holds:",
  "= 2CD2(e1 Xte1)2": "We use e = to denote the vector of ones. Now, we focus on the 2 by 2 matrix Xu (recall this isthe coefficient matrix on the right side of equation (69)). Let its singular values be denoted as 1 and 2.In addition, to deal with equation (70), let 1 and 2 denote the first entry of Xus left singular vectors,corresponding to a and b, respectively. Thus, we can write",
  "(2i1).(78)": "Now, we consider two cases. If C < 1/2, then the above is greater than (1 C)2i, which holds for anyi = 0, 1, . . . , T 1. By way of reduction, we can follow the proof of Lemma B.8 to complete this proof. IfC > 1/2, then the above is greater than (C)2i. Again by following the proof steps in Lemma B.8, we canshow thatTmint=1 EWt2 D",
  "CExperiment Details": "We describe the setup for , ran on (1) a two-layer Multi-Layer Perceptron (MLP) trained on theMNIST digit classification dataset, (2) a twelve-layer BERT-Base model trained on the MRPC sentenceclassification dataset from the GLUE benchmark and (3) a two-layer Graph Convolutional Network (GCN)trained on the COLLAB node classification dataset. We set both MLP and GCN with a hidden dimensionof 128 for model architectures and initialize them randomly. We initialize the BERT model from pretrainedBERT-Base-Uncased. We train each model on the provided training set for the training process until thetraining loss is close to zero. Specifically, we train the MLP, BERT, and GCN models for 30, 10, and 100epochs. We use the model of the last epoch to measure the error in the approximation. We do this for 100times and again measure the perturbed loss Q on the training set. We take the gap between Q and . Ourmeasurements show that the error between the actual gap and the Hessian approximation is within 3%. reports additional comparisons between our approach and several baselines, including label smoothing(LS), random SAM (RSAM), and Bayesian SAM (BSAM). We report the test accuracy and the trace of theHessian for the model weights at the last epoch of training on six image classification datasets. We observethat NSO also further reduces the trace of the Hessian and improves the test accuracy over the baselines.The largest eigenvalue reduces by 9.7%.",
  "()": "SGD1442 634639 951152 401064 441087 568276 91LS1311 813051 951144 88893 79764 754296 74SAM1326 722625 91890 90948 95887 534033 52USAM1245 432299 98592 32782 38755 583893 55ASAM1383 732638 86615 95795 72697 363925 56RSAM1356 692901 121895 74779 68988 654537 58BSAM1375 862788 177972 79843 97939 734123 87NSO1070 742059 45579 59643 57639 723681 66 Finally, we report the hyper-parameters for the experiments in . These include a learning rate of0.0002, momentum of 0.99, weight decay of 0.0001, batch size of 32, and training epochs of 60. We reducethe learning rate by 0.1 every 20 epochs. We choose these hyper-parameters based on a grid search on thevalidation split. The range in which we conduct a grid search is as follows: Learning rate: 0.005, 0.002,0.001, 0.0005, 0.0002, and 0.0001; Momentum: 0.9, 0.95, 0.99; Weight decay: 0.01, 0.001, 0.0001; Epochs:20, 40, and 60; Batch size: 16, 32, and 64. Each baseline method may have its own set of hyper-parameters, which are adjusted via a grid search. Forlabel smoothing, we choose the weight of the loss calculated from the incorrect labels between 0.1, 0.2, and0.3; For SAM and BSAM, we choose the 2 norm of the perturbation between 0.01, 0.02, and 0.05; ForASAM, we choose the 2 norm of the perturbation for the weights between 0.5, 1.0, and 2.0; For RSAM, wechoose the 2 norm of the perturbation between 0.01, 0.02, and 0.05 and the standard deviation for samplingperturbation between 0.008, 0.01, and 0.012."
}