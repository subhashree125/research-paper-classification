{
  "Abstract": "Large-scale graphs with node attributes are increasingly common in various real-world appli-cations. Creating synthetic, attribute-rich graphs that mirror real-world examples is crucial,especially for sharing graph data for analysis and developing learning models when origi-nal data is restricted to be shared. Traditional graph generation methods are limited intheir capacity to handle these complex structures. Recent advances in diffusion models haveshown potential in generating graph structures without attributes and smaller moleculargraphs. However, these models face challenges in generating large attributed graphs due tothe complex attribute-structure correlations and the large size of these graphs. This paperintroduces a novel diffusion model, GraphMaker, specifically designed for generating largeattributed graphs. We explore various combinations of node attribute and graph structuregeneration processes, finding that an asynchronous approach more effectively captures theintricate attribute-structure correlations. We also address scalability issues through edgemini-batching generation. To demonstrate the practicality of our approach in graph datadissemination, we introduce a new evaluation pipeline. The evaluation demonstrates thatsynthetic graphs generated by GraphMaker can be used to develop competitive graph ma-chine learning models for the tasks defined over the original graphs without actually accessingthese graphs, while many leading graph generation methods fall short in this evaluation.",
  "Introduction": "Large-scale graphs with node attributes are widespread in various real-world contexts.For example, insocial networks, nodes typically represent individuals, each characterized by demographic attributes (Golderet al., 2007). In financial networks, nodes may denote agents, with each node encompassing diverse accountdetails (Allen & Babus, 2009). The task of learning to generate synthetic graphs that emulate real-worldgraphs is crucial in graph machine learning (ML), with wide-ranging applications. A traditional use caseis aiding network scientists in understanding the evolution of complex networks (Watts & Strogatz, 1998;Barabsi & Albert, 1999; Chung & Lu, 2002; Barrat et al., 2008; Leskovec et al., 2010). Recently, a vitalapplication has emerged due to data security needs: using synthetic graphs to replace sensitive originalgraphs, thus preserving data utility while protecting privacy (Jorgensen et al., 2016; Eli et al., 2020). This",
  ": (a) Data generation for public usage. (b) & (c): Generation process with two GraphMaker variants": "approach is especially beneficial in fields like social/financial network studies, where synthetic versions canbe used for developing and benchmarking graph learning models, avoiding the need to access the originaldata. Most of such scenarios require generating large attributed graphs. (a) illustrates this procedure. Traditional approaches primarily use statistical random graph models to generate graphs (Erds & Rnyi,1959; Holland et al., 1983; Barabsi & Albert, 2002). These models often contain very few parameters suchas triangle numbers, edge densities, and degree distributions, which often suffer from limited model capacity.AGM (Pfeiffer et al., 2014) extends these models for additionally generating node attributes, but it can onlyhandle very few attributes due to the curse of dimensionality. Recent research on deep generative modelsof graphs has introduced more expressive data-driven methods. Most of those works solely focus on graphstructure generation (Kipf & Welling, 2016; You et al., 2018b; Bojchevski et al., 2018; Li et al., 2018; Liaoet al., 2019). Other works study molecular graph generation that involves attributes, but these graphs areoften small-scale, and only consist of tens of nodes per graph and a single categorical attribute per node (Jinet al., 2018; Liu et al., 2018; You et al., 2018a; De Cao & Kipf, 2018). Diffusion models have achieved remarkable success in image generation (Ho et al., 2020; Rombach et al.,2022) by learning a model to denoise a noisy sample. They have been recently extended to graph structureand molecule generation. Niu et al. (2020) corrupts real graphs by adding Gaussian noise to all entries ofdense adjacency matrices. GDSS (Jo et al., 2022) extends the idea for molecule generation. DiGress (Vignacet al., 2023) addresses the discretization challenge due to Gaussian noise. Specifically, DiGress employsD3PM (Austin et al., 2021) to edit individual categorical node and edge types. However, DiGress is stilldesigned for small molecule generation. EDGE (Chen et al., 2023) and GRAPHARM (Kong et al., 2023)propose autoregressive diffusion models for graph structure and molecule generation. Overall, all previousgraph diffusion models are not designed to generate large attributed graphs and may suffer from variouslimitations for this purpose. Developing diffusion models of large-attributed graphs is challenging on several aspects.First, a largeattributed graph presents substantially different patterns from molecular graphs, exhibiting complex cor-relations between high-dimensional node attributes and graph structure. Second, generating large graphsposes challenges to model scalability as the number of possible edges exhibits quadratic growth in the num-ber of nodes, let alone the potential hundreds or thousands of attributes per node. Third, how to evaluatethe quality of the generated graphs remains an open problem. Although deep models have the potential tocapture more complicated data distribution and correlations, most previous studies just evaluate high-levelstatistics characterizing structural properties such as node degree distributions, and clustering coefficientsof the generated graphs, which can be already captured well by early-day statistical models (Pfeiffer et al.,2012; Seshadhri et al., 2012).Therefore, a finer-grained way to evaluate the generated large-attributed",
  "Published in Transactions on Machine Learning Research (10/2024)": "Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout:A simple way to prevent neural networks from overfitting. Journal of Machine Learning Research, 15(56):19291958, 2014. Clement Vignac, Igor Krawczuk, Antoine Siraudin, Bohan Wang, Volkan Cevher, and Pascal Frossard.Digress: Discrete denoising diffusion for graph generation. In International Conference on Learning Rep-resentations, 2023. Minjie Wang, Da Zheng, Zihao Ye, Quan Gan, Mufei Li, Xiang Song, Jinjing Zhou, Chao Ma, Lingfan Yu,Yu Gai, Tianjun Xiao, Tong He, George Karypis, Jinyang Li, and Zheng Zhang. Deep graph library: Agraph-centric, highly-performant package for graph neural networks. arXiv preprint arXiv:1909.01315,2019.",
  "Preliminaries and problem formulation": "Consider an undirected graph G = (V, E, X) with the node set V and the edge set E. Let N = |V| denotethe number of nodes. Each node v V is associated with F-dimensional categorical attributes Xv. Notethat we are to generate graphs without edge attributes to show a proof of concept, while the method canbe extended for the case with edge attributes. We aim to learn PG with PG based on one graph G sampledfrom it, which can be used to generate synthetic graphs for further graph learning model development. In.6, we provide some reasoning on why the distribution may be possibly learned from one singleobservation. In the scenarios targeting node classification tasks, each node v is additionally associated witha label Yv, which require learning PGY from (G, Y) instead.",
  "Forward diffusion process and reverse process": "Our diffusion model extends D3PM (Austin et al., 2021) to large attributed graphs. There are two phases.The forward diffusion process corrupts the raw data by progressively perturbing its node attributes or edges.Let X RNF CX be the one-hot encoding of the categorical node attributes, where for simplicity weassume all attributes to have CX possible classes. By treating the absence of edge as an edge type, wedenote the one-hot encoding of all edges by A RNN2. Let G(0) be the real graph data (X, A). We individually corrupt node attributes and edges of G(t1) into G(t) by first obtaining noisy distributionswith transition matrices and then sampling from them. By composing the noise over multiple time steps,for G(t) = (X(t), A(t)) at time step t [T], we have q(X(t)v,f|Xv,f) = Xv,f Q(t)Xf for any v [N], f [F] and q(A(t)|A) = A Q(t)A .To ensure G(t) to be undirected, Q(t)Ais only applied to the upper triangularpart of A. The adjacency matrix of G(t), i.e., A(t), is then constructed by symmetrizing the matrix aftersampling. We consider a general formulation of Q(t)Xf and Q(t)A that allows asynchronous corruption of node",
  "Z(t)/|TZ|+s": "1+s) for some smalls, but the model works for other schedules as well. I is the identity matrix, 1 is a one-valued vector, mXf isthe empirical marginal distribution of the f-th node attribute in the real graph, mA is the empirical marginaldistribution of the edge existence in the real graph, and denotes transpose. During the reverse process, the second phase of the diffusion model, a denoising network ,t is trained toperform one-step denoising p(G(t1)|G(t), t). Once trained, we can iteratively apply this denoising network",
  "1u<vNp(A(t1)u,v|G(t), t)(3)": "Sohl-Dickstein et al. (2015) and Song & Ermon (2019) show that we can train the denoising network to predictG(0) instead of G(t1) as long asq(G(t1)|G(t), t, G(0))dp(G(0)|G(t), t) has a closed-form expression. Thisholds for our case. By the Bayes rule, we have q(X(t1)v,f|G(t), t, G(0)) X(t)v,f(Q(t)Xf ) Xv,f Q(t1)Xf, where",
  "Two instantiations": "To model the complex correlations between node attributes and graph structure, we study two particularinstantiations of GraphMaker, named GraphMaker-Sync and GraphMaker-Async. When categorical nodelabel Y is present, as in the task of node classification, we can treat it as an extra node attribute. Forsimplicity, we do not distinguish it from X for the rest of this sub-section unless stated explicitly. GraphMaker-Sync (GMaker-S). GraphMaker-Sync employs a forward diffusion process that simultane-ously corrupts node attributes and edges for all time steps, which corresponds to setting TX = TA = [T]. Thedenoising network is trained to recover clean node attributes and edges from corrupted node attributes andedges. During generation, it first samples noisy attributes X(T ), and noisy edges A(T ) from the prior distri-butions. It then repeatedly invokes the denoising network ,t to predict G(0) for computing the posteriordistribution p(G(t1)|G(t), t), and then samples G(t1). (b) provides an illustration. GraphMaker-Async (GMaker-A). In practice, we find that GraphMaker-Sync cannot properly capturethe correlations between high-dimensional node attributes, graph structure, and node label, as shown inlater Sections 3.2 and 3.3. Previous work has pointed out that a failure to model edge dependency in graphstructure generation yields poor generation quality in capturing patterns like triangle count (Chanpuriyaet al., 2021). We suspect that this problem stems from synchronous refinement of node attributes and graphstructure, hence we propose to denoise node attributes and graph structure asynchronously instead. Weconsider a simple and practically effective configuration as a proof of concept, which partitions [1, T] intotwo subintervals TA = [1, tm] and TX = [tm +1, T]. The denoising network ,t consists of two sub-networks.X,t is an MLP trained to reconstruct node attributes X(0) given (X(t), t). A,t is trained to reconstruct",
  "Conditional generation given node labels": "An important learning task for large attributed graphs is node classification, where each node is associatedwith a label. To generate graph data with node labels, one nave way is to simply generate node labelsas extra node attributes. While this idea is straightforward, it may not perform the best at capturing thecorrelation between node label and other components, as empirically demonstrated in .2 and 3.3.To further improve the generation quality for developing or benchmarking node classification models, wepropose conditional graph generation. Conditional generative models have been proven valuable in manyapplications, such as generating images consistent with a target text description (Rombach et al., 2022) ormaterial candidates that satisfy desired properties (Vignac et al., 2023), where controllable generation isoften desirable. Specifically, since the node label distribution PY can be estimated from the real node labels, we can simplifythe problem by learning the conditional distribution PG|Y rather than PGY, which essentially requireslearning a conditional generative model. This corresponds to setting Y(0) = = Y(T ) in figure 1 (b) &(c). During training, the denoising network is supplied with uncorrupted node labels. During generation,node labels are sampled and fixed at the beginning to guide the generation of the rest node attributes andgraph structure. We notice that conditional generation gives a further boost in the accuracy for developingnode classification models through the generated graphs.",
  "Scalable denoising network architecture": "Large attributed graphs consist of more than thousands of nodes.This poses severe challenges to thescalability of the denoising network. We address the challenge with both the encoder and the decoder ofthe denoising network. The encoder computes representations of G(t) and the decoder makes predictions ofnode attributes and edges with them. Graph encoder. To enhance the scalability of the graph encoder, we propose a message passing neuralnetwork (MPNN) with complexity O(|E|) instead of using a graph transformer (Dwivedi & Bresson, 2021)with complexity O(N 2), which was employed by previous graph diffusion generative models (Vignac et al.,2023). Empirically, we find that with a 16-GB GPU, we can use at most a single graph transformer layer,with four attention heads and a hidden size of 16 per attention head, for a graph with approximately twothousand nodes and one thousand attributes.Furthermore, while in theory graph transformers surpassMPNNs in modeling long-range dependencies owing to non-local interactions, it is still debatable whetherlong-range dependencies are really needed for large-attributed graphs. In particular, graph transformers havenot demonstrated superior performance on standard benchmarks for large attributed graphs like OGB (Huet al., 2020). Let A(t) and X(t) respectively be the one-hot encoding of the edges and node attributes for the time stept. The encoder first uses a multilayer perceptron (MLP) to transform X(t) and the time step t into hiddenrepresentations X(t,0) and h(t).It then employs multiple MPNN layers to update attribute representa-tions.For any node v V, X(t,l+1)v= (W(l)T Xh(t) + b(l)X +",
  "|N (t)(v)|X(t,l)uW(l)XX), where": "W(l)T X, W(l)XX, and b(l)X are learnable matrices and vectors. N (t)(v) consists of v and the neighbors of vcorresponding to A(t). consists of a ReLU layer Jarrett et al. (2009), a LayerNorm layer Ba et al. (2016),and a dropout layer Srivastava et al. (2014). The encoder computes the final node representations withH(t)v= X(t,0)vX(t,1)v h(t), which combines the representations from all MPNN layers as in JK-Nets Xuet al. (2018). We employ two separate encoders for node attribute prediction and edge prediction.",
  "|N (t)(v)|[X(t,l)uY(0,l)u]W(l)XX) and H(t)vwith H(t)v= X(t,0)vX(t,1)v Y(0,0)vY(0,1)v h(t)": "Decoder.The decoder performs node attribute prediction from H(t)vin the form of multi-label nodeclassification. To predict edge existence between nodes u, v V in an undirected graph, the decoder performsbinary node pair classification. It first computes the elementwise product H(t)u H(t)v , and then applies anMLP to transform this product into the final score. Due to limited GPU memory, it is intractable to performedge prediction for all N 2 node pairs at once. During training, we randomly choose a subset of the nodepairs for a gradient update. For graph generation, we first compute the representations of all nodes and thenperform edge prediction over minibatches of node pairs.",
  "Can a graph generative model be learned from a single graph?": "It remains to be answered why we can learn the graph distribution from only one sampled graph. Dueto the permutation equivariance inherent in our MPNN architecture, the derived probability distributionis permutation invariant. In other words, altering the node ordering in a graph doesnt affect the modelslikelihood of generating it. This suggests that our model is primarily focused on identifying common patternsacross nodes instead of focusing on the unique details of individual nodes. In this case, a single, expansivegraph can offer abundant learning exampleseach node centered subgraph acts as a distinct data point. This raises an intriguing query should a model for large graph generation capture individual node charac-teristics? We think the answer depends on the use cases. If the goal is to analyze population-level trends,individual node details will be distractions. Note that traditional graph generative models like ER (Erds& Rnyi, 1959), and its degree-corrected counterpart (Seshadhri et al., 2012) only capture population-levelstatistics like degree distributions. For scenarios that involve sharing synthetic graphs with privacy consid-erations, omitting the node-specific details is advantageous. Conversely, if node-specific analysis is essential,our model might fall short. Overall, theres always a tradeoff between a models capabilities, the data athand, the desired detail level, and privacy considerations. We think of our work as a first step and will letfuture works dive deeper into this issue. In .5, we empirically study this point. Indeed, adopting amore expressive model with node positional encodings (Eliasof et al., 2023) that may capture some individualnode details does not consistently improve the quality of the generated graphs in our evaluation.",
  "Experiments": "In this section, we aim to answer the following questions with empirical studies. Q1) For the end goal ofusing synthetic graphs to develop and benchmark ML models, how does GraphMaker perform against theexisting generative models in preserving data utility for ML? Q2) For the studies of network science, howdoes GraphMaker perform against the existing generative models in capturing structural properties? Q3)For the potential application of privacy-preserving synthetic data sharing, generative models should avoidmerely reproducing the original graph.To what extent do the generated synthetic graphs replicate theoriginal graph? Q4) Does capturing individual-level node characteristics benefit capturing population-levelpatterns?",
  "Datasets and baselines": "Datasets: We utilize three large attributed networks for evaluation. Cora is a citation network depictingcitation relationships among papers (Sen et al., 2008), with binary node attributes indicating the presence ofkeywords and node labels representing paper categories. Amazon Photo and Amazon Computer are productco-purchase networks, where two products are connected if they are frequently purchased together (Shchuret al., 2018). The node attributes indicate the presence of words in product reviews and node labels representproduct categories. See Appendix A.1 for the dataset statistics. Notably, Amazon Computer (13K nodes,490K edges) is an order of magnitude larger than graphs adopted for evaluation by previous deep generative",
  "models of graph structures (Chen et al., 2023; Kong et al., 2023), and hence it is a challenging testbed formodel scalability": "Baselines: To understand the strengths and weaknesses of the previous diffusion models in large attributedgraph generation, we consider two state-of-the-art diffusion models developed for graph generation, GDSS (Joet al., 2022) and EDGE (Chen et al., 2023).For non-diffusion deep learning (DL) methods, we adoptfeature-based matrix factorization (FMF) (Chen et al., 2012), graph auto-encoder (GAE), and variationalgraph auto-encoder (VGAE) (Kipf & Welling, 2016). In addition, we compare against the ErdsRnyi(ER) model (Erds & Rnyi, 1959), a traditional statistical random graph model. None of the non-diffusionbaselines inherently possesses the capability for node attribute generation. Therefore, we equip them with theempirical marginal distribution p(Y)",
  "Evaluation for ML utility (Q1)": "To evaluate the utility of the generated graphs for developing ML models, we introduce a novel evaluationprotocol based on discriminative models. We train one model on the training set of the original graph (G, Y)and another model on the training set of a generated graph ( G, Y). We then evaluate the two models onthe test set of the original graph to obtain two performance metrics ACC(G|G) and ACC(G| G). If the ratioACC(G| G)/ACC(G|G) is close to one, then the generated graph is considered as having a utility similarto the original graph for training the particular model.We properly tune each model to ensure a faircomparison, see Appendix A.3 for more details. Node classification. Node classification models evaluate the correlations between unlabeled graphs andtheir corresponding node labels. Given our focus on the scenario with a single large graph, we approach thesemi-supervised node classification problem. We randomly split a generated dataset so that the number oflabeled nodes in each class and each subset is the same as that in the original dataset. For discriminativemodels, we choose three representative GNNs SGC (Wu et al., 2019), GCN (Kipf & Welling, 2017), andAPPNP (Gasteiger et al., 2019). As they employ different orders for message passing and prediction, thiscombination allows for examining data patterns more comprehensively. To scrutinize the retention of higher-order patterns, we employ two variants for each model, one with a single message passing layer denoted by1 and another with multiple message passing layers denoted by L. In addition, we directly evaluatethe correlations between node attributes and node label with MLP in appendix A.3. Link prediction. This task requires predicting missing edges in an incomplete graph, potentially utiliz-ing node attributes and node labels. To prevent label leakage from dataset splitting, we split the edgescorresponding to the upper triangular adjacency matrix into different subsets and then add reverse edgesafter the split. We consider two types of discriminative models Common Neighbor (CN) (Liben-Nowell &Kleinberg, 2003) and GAE (Kipf & Welling, 2016). CN is a traditional method that predicts edge existenceif the number of common neighbors shared by two nodes exceeds a threshold. GAE is an MPNN-basedmodel that integrates the information of graph structure, node attributes, and node labels. As before, weconsider two variants for GAE. Following the practice of Kipf & Welling (2016), we adopt ROC-AUC as theevaluation metric. presents the results. Unless otherwise mentioned, we report all results in this paper based on threedifferent random seeds. For the performance of the discriminative models trained and evaluated on theoriginal graph, see Appendix A.3. Out of 24 cases, GraphMaker variants perform better or no worse thanthe baselines for 20 of them ( 83%). Diffusion models that asynchronously denoise node attributes and graph structure (GraphMaker-Async(GMaker-A) and EDGE) outperform the synchronous ones (GraphMaker-Sync (GMaker-S) and GDSS)by a large margin. This phenomenon is in contrast to the previous observation made for molecule generationby the GDSS paper (Jo et al., 2022), reflecting the drastically different patterns between a large attributedgraph and molecular graphs. EDGE proposes a degree guidance mechanism, where the graph generation isconditioned on node degrees pre-determined from the empirical node degree distribution. While it achievescompetitive performance on Cora, it falls short on Amazon Photo, which might be due to degree constraints.",
  "GMaker-S (cond.)0.76 0.040.87 0.040.92 0.000.89 0.020.92 0.010.85 0.010.97 0.010.98 0.03GMaker-A (cond.)1.00 0.181.00 0.100.96 0.020.96 0.020.98 0.030.82 0.010.97 0.001.01 0.00": "Moreover, conditioning on degree distribution reduces the novelty of the generated graphs.In terms ofscalability, GraphMaker is the only diffusion model that does not encounter the out-of-memory (OOM)error on a 48-GB GPU. As GDSS denoises dense adjacency matrices, it has a complexity of O(N 2) forrepresentation computation even with an MPNN. Therefore, it has a worse scalability than EDGE. Overall,the results also suggest the effectiveness of the proposed evaluation protocol in distinguishing and rankingdiverse generative models. For scenarios targeting the task of node classification, we can adopt conditional generation given nodelabels for better performance, as discussed in .4. Label-conditional GraphMaker variants, denotedby (cond.)in the table, perform better or no worse than the unconditional ones for about 90% cases.Among the GraphMaker variants, label-conditional GraphMaker-Async achieves the best performance for",
  "/24 = 75% cases. Meanwhile, it is worth noting that label conditioning narrows the performance gapbetween GraphMaker-Sync and GraphMaker-Async": "Utility for Benchmarking ML Models on Node Classification. Another important scenario of usinggenerated graphs is benchmarking ML models, where industry practitioners aim to select the most effectivemodel architecture from multiple candidates based on their performance on publicly available syntheticgraphs and then train a model from scratch on their proprietary real graph. This use case requires thegenerated graphs to yield reproducible relative performance of the candidate model architectures on theoriginal graph. Following Yoon et al. (2023), for each candidate model architecture, we train and evaluateone model on the original graph for ACC(G|G) and another on a generated graph for ACC( G| G). We thenreport the Pearson/Spearman correlation coefficients between them (Myers et al., 2010). We include all sixmodel architectures for node classification in the set of candidate model architectures. presents the experiment results. Out of 6 cases, unconditional GraphMaker-Async performs betteror no worse than all baselines for 5 of them. Overall, the results are consistent with the previous obser-vations in terms of the benefit of asynchronous generation and label conditioning. The only exception isthat label-conditional GraphMaker-Sync slightly outperforms label-conditional GraphMaker-Async in termsof Spearman correlation coefficient by a small margin of 0.02 on average. Spearman correlation coefficientexamines the reproduction of relative model ranking differences, and is most useful when there exists a sub-stantial performance gap between each pair of candidate models. In practice, when multiple models performsufficiently similar, the final decision on model selection can involve a tradeoff between cost and performance.In this case, exactly reproducing the relative model ranking differences may be less important than repro-ducing the relative model performance differences, in which case Pearson correlation coefficient is preferredinstead. This applies to our case, as reflected by the raw ACC(G|G) values reported in Appendix A.3.",
  "Evaluation for structural properties (Q2)": "To assess the quality of the generated graph structures, following You et al. (2018b), we report distance met-rics for node degree, clustering coefficient, and four-node orbit count distributions. We adopt 1-Wassersteindistance W1(x, y), where x, y are respectively a graph statistic distribution from the original graph and agenerated graph, and a lower value is better. Besides distribution distance metrics, we directly compare afew scalar-valued statistics. Let M(G) be a non-negative-valued statistic, we report E Gp",
  "GMaker-S (cond.)0.26 0.032.7 0.11.5 0.10.072 0.0051.2 0.00.54 0.01GMaker-A (cond.)0.21 0.002.0 0.31.2 0.00.46 0.081.3 0.01.2 0.0": "where CY is the number of node classes, and N(v) consists of the neighboring nodes of v. We also reportthe metric for two-hop correlations, denoted by h(A2, Y). The computation of clustering coefficients, orbitcounts, and triangle count may be costly for the large generated graphs. So, in such cases, we sample anedge-induced subgraph with the same number of edges from all generated graphs. displays the evaluation results on Amazon Photo and Amazon Computer. See Appendix A.5 forthe results on Cora. None of the models consistently outperforms the rest. In terms of the number of caseswhere they achieve the best performance, the best model is label-conditional GraphMaker-Async with 6 outof 18. The degree-guidance mechanism adopted by EDGE is effective in capturing local structural patternsand leads to the best performance for degree W1 and orbit W1 on Cora and Amazon Photo. Across theconditional and unconditional cases, GraphMaker-Async outperforms GraphMaker-Sync in W1 metrics andtriangle count for 21/24 = 87.5% cases, demonstrating a superior capability in capturing edge dependencies.In consistent with the evaluation using discriminative models, label conditioning yields a better performancefor 26/36 72% cases.",
  "Evaluation for graph recovery (Q3)": "To quantify the recovery of the original graph by the synthetic graphs, we evaluate their similarity. Preciselymeasuring the similarity between two graph structures requires solving the subgraph isomorphism problem,which is known to be NP-complete (Cook, 1971). This challenge is further complicated by the presenceof numerous node attributes. Therefore, we propose local node-centered comparisons based on the node",
  "Further related works": "We have reviewed the diffusion-based graph generative models in . Here, we review some other non-diffusion deep generative models and some recent improvement efforts on synthetic graph data evaluation.We leave a review on classical graph generative models to Appendix A.6. Non-diffusion deep generative models of large graphs. GAE and VGAE (Kipf & Welling, 2016)extend AE and VAE (Kingma & Welling, 2014; Rezende et al., 2014) respectively for reconstructing andgenerating the structure of a large attributed graph. NetGAN (Bojchevski et al., 2018) extends WGAN (Ar-jovsky et al., 2017) for graph structure generation by sequentially generating random walks. GraphRNN (Youet al., 2018b) and Li et al. (2018) propose auto-regressive models that generate graph structures by sequen-tially adding individual nodes and edges. GRAN (Liao et al., 2019) introduces a more efficient auto-regressivemodel that generates graph structures by progressively adding subgraphs. CGT (Yoon et al., 2023) tacklesa simplified version of the large attributed graph generation problem that we consider. It clusters real nodeattributes during data pre-processing and generates node-centered ego-subgraphs (essentially trees) with asingle categorical node label that indicates the attribute cluster. Evaluation of generated graphs with discriminative models. GraphWorld (Palowitch et al., 2022) isa software for benchmarking discriminative models on synthetic attributed graphs, but it does not comparesynthetic graphs against a real graph in benchmarking. CGT (Yoon et al., 2023) uses generated subgraphsto benchmark GNNs. It evaluates one model for node classification on the original graph and another ongenerated subgraphs, and then measures the performance correlation/discrepancy of the models. Our workinstead is the first effort to train models on a generated graph and then evaluate them on the original graph.Outside the graph domain, CAS employs discriminative models to evaluate the performance of conditionalimage generation (Ravuri & Vinyals, 2019).",
  "Conclusion": "We propose GraphMaker, a diffusion model capable of generating large attributed graphs, along with a novelevaluation protocol that assesses generation quality by training models on generated graphs and evaluatingthem on real ones.Overall, GraphMaker achieves better performance compared to baselines, for bothexisting metrics and the newly proposed evaluation protocol.Potential future works include extendingGraphMaker for larger graphs with 1M+ nodes, continuous-valued node attributes, node labels that indicatenode anomalies for anomaly detection, etc.",
  "Broader Impact Statement": "The current manuscript primarily focuses on building a model for generating high-quality synthetic graphs.However, for privacy-preserving synthetic data sharing, there is likely a privacy-utility trade-off that warrantsmore thorough investigation in future work. Additionally, the misuse of GraphMaker and the generatedsynthetic graphs may lead to the spread of misinformation.Therefore, the release of a synthetic graphshould be accompanied by a statement indicating its source.",
  "Alain Barrat, Marc Barthlemy, and Alessandro Vespignani. Dynamical Processes on Complex Networks.Cambridge University Press, 2008": "Aleksandar Bojchevski, Oleksandr Shchur, Daniel Zgner, and Stephan Gnnemann. NetGAN: Generatinggraphs via random walks. In Proceedings of the 35th International Conference on Machine Learning, pp.610619, 2018. Sudhanshu Chanpuriya, Cameron Musco, Konstantinos Sotiropoulos, and Charalampos Tsourakakis. Onthe power of edge independent graph models. In Advances in Neural Information Processing Systems, pp.2441824429, 2021. Tianqi Chen, Weinan Zhang, Qiuxia Lu, Kailong Chen, Zhao Zheng, and Yong Yu. Svdfeature: A toolkitfor feature-based collaborative filtering. Journal of Machine Learning Research, 13(116):36193622, 2012. Xiaohui Chen, Jiaxing He, Xu Han, and Li-Ping Liu.Efficient and degree-guided graph generation viadiscrete diffusion modeling. In Proceedings of the 40th International Conference on Machine Learning, pp.45854610, 2023.",
  "Vijay Prakash Dwivedi and Xavier Bresson. A generalization of transformer networks to graphs. AAAIWorkshop on Deep Learning on Graphs: Methods and Applications, 2021": "Marek Eli, Michael Kapralov, Janardhan Kulkarni, and Yin Tat Lee.Differentially private release ofsynthetic graphs. In Proceedings of the 2020 ACM-SIAM Symposium on Discrete Algorithms (SODA), pp.560578, 2020. Moshe Eliasof, Fabrizio Frasca, Beatrice Bevilacqua, Eran Treister, Gal Chechik, and Haggai Maron. Graphpositional encoding via random feature propagation. In International Conference on Machine Learning,pp. 92029223, 2023.",
  "Paul W. Holland, Kathryn Blackmond Laskey, and Samuel Leinhardt. Stochastic blockmodels: First steps.Social Networks, 5(2):109137, 1983": "Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, andJure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. In Advances in NeuralInformation Processing Systems, pp. 2211822133, 2020. Kevin Jarrett, Koray Kavukcuoglu, MarcAurelio Ranzato, and Yann LeCun. What is the best multi-stagearchitecture for object recognition?In 2009 IEEE 12th International Conference on Computer Vision,pp. 21462153, 2009. Wengong Jin, Regina Barzilay, and Tommi Jaakkola. Junction tree variational autoencoder for moleculargraph generation. In Proceedings of the 35th International Conference on Machine Learning, pp. 23232332, 2018. Jaehyeong Jo, Seul Lee, and Sung Ju Hwang. Score-based generative modeling of graphs via the system ofstochastic differential equations. In Proceedings of the 39th International Conference on Machine Learning,pp. 1036210383, 2022.",
  "Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. InInternational Conference on Learning Representations (ICLR), 2017": "Lingkai Kong, Jiaming Cui, Haotian Sun, Yuchen Zhuang, B. Aditya Prakash, and Chao Zhang. Autoregres-sive diffusion model for graph generation. In Proceedings of the 40th International Conference on MachineLearning, pp. 1739117408, 2023. Jure Leskovec, Deepayan Chakrabarti, Jon Kleinberg, Christos Faloutsos, and Zoubin Ghahramani. Kro-necker graphs: An approach to modeling networks. Journal of Machine Learning Research, 11(33):9851042, 2010.",
  "Alexander Quinn Nichol and Prafulla Dhariwal.Improved denoising diffusion probabilistic models.InProceedings of the 38th International Conference on Machine Learning, pp. 81628171, 2021": "Chenhao Niu, Yang Song, Jiaming Song, Shengjia Zhao, Aditya Grover, and Stefano Ermon. Permutationinvariant graph generation via score-based generative modeling.In Proceedings of the Twenty ThirdInternational Conference on Artificial Intelligence and Statistics, pp. 44744484, 2020. John Palowitch, Anton Tsitsulin, Brandon Mayer, and Bryan Perozzi. Graphworld: Fake graphs bring realinsights for gnns. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and DataMining, pp. 36913701, 2022. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen,Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, ZacharyDeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, andSoumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In Advances inNeural Information Processing Systems, pp. 80248035, 2019. Joseph J. Pfeiffer, Timothy La Fond, Sebastian Moreno, and Jennifer Neville. Fast generation of large scalesocial networks while incorporating transitive closures.In 2012 International Conference on Privacy,Security, Risk and Trust and 2012 International Confernece on Social Computing, pp. 154165, 2012. Joseph J. Pfeiffer, Sebastian Moreno, Timothy La Fond, Jennifer Neville, and Brian Gallagher. Attributedgraph models: Modeling network structure with correlated attributes. In Proceedings of the 23rd interna-tional conference on World wide web, pp. 831842, 2014.",
  "Suman Ravuri and Oriol Vinyals. Classification accuracy score for conditional generative models. In Advancesin Neural Information Processing Systems, 2019": "Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and approximateinference in deep generative models.In Proceedings of the 31st International Conference on MachineLearning, pp. 12781286, 2014. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjrn Ommer. High-resolutionimage synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition (CVPR), pp. 1068410695, 2022.",
  "Duncan J. Watts and Steven H. Strogatz.Collective dynamics of small-world networks.Nature, 393:440442, 1998": "Felix Wu, Amauri Souza, Tianyi Zhang, Christopher Fifty, Tao Yu, and Kilian Weinberger. Simplifyinggraph convolutional networks. In Proceedings of the 36th International Conference on Machine Learning,pp. 68616871, 2019. Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi Kawarabayashi, and Stefanie Jegelka.Representation learning on graphs with jumping knowledge networks. In Proceedings of the 35th Interna-tional Conference on Machine Learning, pp. 54535462, 2018. Minji Yoon, Yue Wu, John Palowitch, Bryan Perozzi, and Russ Salakhutdinov. Graph generative model forbenchmarking graph neural networks. In Proceedings of the 40th International Conference on MachineLearning, pp. 4017540198, 2023. Jiaxuan You, Bowen Liu, Zhitao Ying, Vijay Pande, and Jure Leskovec. Graph convolutional policy networkfor goal-directed molecular graph generation.In Advances in Neural Information Processing Systems,2018a. Jiaxuan You, Rex Ying, Xiang Ren, William Hamilton, and Jure Leskovec. GraphRNN: Generating realisticgraphs with deep auto-regressive models. In Proceedings of the 35th International Conference on MachineLearning, pp. 57085717, 2018b.",
  "Cora0.5460.7640.7860.8070.7800.8100.7070.9300.925Amazon Photo0.6990.6190.6380.8950.9000.9130.9340.9520.958Amazon Computer0.6150.5540.5420.7800.7930.7740.9250.9420.911": "For EDGE, we adopt the hyperparameters it used for generating the structure of Cora. The original EDGEpaper generates molecules by first sampling atom types and then generating a graph structure conditionedon the sampled atom types. We extend it for generating a large attributed graph by first generating nodeattributes and label and then generating the graph structure conditioned on them. To generate attributesand label, we use the attribute generation component in the unconditional GraphMaker-Async. For GDSS, we adopt the hyperparameters provided in the open source codebase and the S4 integratorproposed in the original paper to simulate the system of reverse-time SDEs. As GDSS directly denoises thedense adjacency matrices, it needs to decide edge existence based on a threshold to obtain the final graphstructure. We choose a threshold so that the number of generated edges is the same as the number of edgesin the original graph.",
  "A.3Additional details for evaluation with discriminative models": "To evaluate the quality of the generated attributes, we train one MLP on the real training node attributesand label, and another MLP on the generated training node attributes and label. We then evaluate thetwo models on the real test node attributes and label to obtain two performance metrics ACC(G|G) andACC(G| G). If the ratio ACC(G| G)/ACC(G|G) is close to one, then the generated data is considered ashaving a utility similar to the original data for training the particular model. For all evaluation conducted in this paper with discriminative models, we design a hyperparameter spacespecific to each discriminative model, and implement a simple AutoML pipeline to exhaustively searchthrough a hyperparameter space for the best trained model.",
  "A.4Evaluation for diversity and novelty": "To study the diversity of the graphs generated by GraphMaker, we generate 50 graphs with each conditionalGraphMaker variant and make histogram plots of metrics based on them. For structure diversity, we reportW1 for node degree distribution. For node attribute and label diversity, we train an MLP on the originalgraph and report its accuracy on generated graphs. presents the histogram plots for Cora, whichdemonstrates that GraphMaker is capable of generating diverse and hence novel graphs.",
  "A.6Review on classic graph generative models": "ER (Erds & Rnyi, 1959) generates graph structures with a desired number of nodes and average nodedegree. SBM (Holland et al., 1983) produces graph structures with a categorical cluster label per node thatmeet target inter-cluster and intra-cluster edge densities. BA (Barabsi & Albert, 2002) generates graphstructures whose node degree distribution follows a power law. Chung-Lu (Chung & Lu, 2002) generatesgraphs with a node degree distribution that equals to a pre-specified node degree distribution in expecta-",
  "HyperparameterCoraAmazon PhotoAmazon Computer": "Node attribute predictionHidden size for time step representations321616Hidden size for node representations5125121024Hidden size for node label representations646464Number of linear layers222Dropout0.100Number of diffusion steps667Learning rate0.0010.0010.001OptimizerAMSGradAMSGradAMSGrad Link predictionHidden size for time step representations321616Hidden size for node representations512512512Hidden size for node label representations646464Hidden size for edge representations128128128Number of MPNN layers222Dropout0.100Number of diffusion steps999Learning rate0.00030.00030.0003OptimizerAMSGradAMSGradAMSGradBatch size163842621442097152"
}