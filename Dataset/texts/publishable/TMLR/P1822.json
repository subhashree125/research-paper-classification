{
  "Abstract": "Despite the recent advances in personalized text-to-image (P-T2I) generative models, it re-mains challenging to perform finetuning-free multi-subject-driven T2I in a resource-efficientmanner. Predominantly, contemporary approaches, involving the training of hypernetworksand Multimodal Large Language Models (MLLMs), require heavy computing resources thatrange from 600 to 12300 GPU hours of training. These subject-driven T2I methods hingeon Latent Diffusion Models (LDMs), which facilitate T2I mapping through cross-attentionlayers. While LDMs offer distinct advantages, P-T2I methods reliance on the latent spaceof these diffusion models significantly escalates resource demands, leading to inconsistentresults and necessitating numerous iterations for a single desired image. Through empirical evidences we find that CLIP (vision) latent space is already expres-sive enough to preserve the fine-grained details. Building upon this insight, in this paper,we present -ECLIPSE , a diffusion-agnostic prior-training strategy that operates in the la-tent space of a pre-trained CLIP model without relying on the diffusion UNet models.-ECLIPSE leverages the image-text interleaved pre-training for fast and effective multi-subject-driven P-T2I. Through extensive experiments, we establish that -ECLIPSE sur-passes existing baselines in composition alignment while preserving concept alignment per-formance, even with significantly lower resource utilization. -ECLIPSE performs multi-subject driven P-T2I with just 34M parameters and is trained on a mere 74 GPU hours.Additionally, -ECLIPSE demonstrates the unique ability to perform multi-concept interpo-lations. Project page:",
  "Introduction": "The field of text-to-image (T2I) diffusion models has recently witnessed remarkable advancements, achiev-ing greater photorealism and enhanced adherence to textual prompts. This has catalyzed the emergenceof diverse applications, notably subject-driven personalized T2I (P-T2I) models. In particular, this encom-passes the intricate task of learning and reproducing novel visual concepts or subjects in varied contextsrequiring high concept and compositional alignment. The complexity escalates further when multi-subjectpersonalization is desired.",
  ": -ECLIPSE can estimate subject-specific latent image embeddings while maintaining the balancebetween concept and composition alignment in the CLIP latent space itself": "Early works employed concept-specific optimization strategies involving fine-tuning certain parameters withinT2I diffusion models Gal et al. (2022); Ruiz et al. (2023a); Kumari et al. (2023); Tumanyan et al. (2023);Gal et al. (2023). Although these methods achieve state-of-the-art (SOTA) performance, they struggle withgeneralization and are time-intensive. Contemporary research is pivoting towards fast personalization tech-niques. Within this paradigm, there are two types of popular approaches: 1) Methods that involve traininghypernetworks and integrating new layers or parameters within pre-trained diffusion UNet models Wei et al.(2023); Ye et al. (2023); Tewel et al. (2023); Shah et al. (2023); Ruiz et al. (2023b), and 2) MLLM-basedlearning of prior models that focuses on leveraging text-latent space of frozen diffusion UNet model Panet al. (2023); Sun et al. (2023). The hypernetwork-based strategy achieves single-concept customization but has not been extended to multi-concepts. Moreover, when combined with additional control (i.e. Canny edge map), they struggle to maintainthe concept alignment (30% drop in performance; ) and strongly favor the edge map. At the sametime, MLLM-based approaches can perform fast multi-concept customization but require heavy computingresources. In , we provide the overview of various single and multi-concept customization method-ologies in terms of total parameters, iterations, and GPU hours required to train the models. It can beobserved that multi-concept customization methodologies further increase the resource requirements. Forinstance, Kosmos-G Pan et al. (2023) consumes 18x more resources than IP-Adapter Ye et al. (2023). AndEmu2 Sun et al. (2023) requires training of 19x more parameters compared to Kosmos-G. Hence, despiteMLLMs seemingly useful scenarios, it is not viable to blindly train them. Upon further investigation, we find that most subject-driven T2I approaches build upon variants of theLatent Diffusion Model (LDM) Rombach et al. (2022), specifically Stable Diffusion models. These LDMsemploy cross-attention layers to condition diffusion models with text embeddings, necessitating a mapping oftarget subject images to latent spaces compatible with the diffusion models at the prior training stage. Thisis also known as score distillation instruction tuning for MLLMs Pan et al. (2023). As there is no choice butto learn this text-to-image diffusion latent space, it involves backpropagation through the entire diffusion",
  "-ECLIPSE (ours)34M100K2M74": "model often comprising over a billion parameters, contributing to the inefficiency of existing P-T2I methods.Therefore, in this work, we focus on answering one question: Do we really need diffusion models totrain the customization models? To answer this question and improve the resource efficiency for multi-concept image generation, we present-ECLIPSE 1, which leverages the properties of UnCLIP T2I models (e.g. DALL-E 2 Ramesh et al. (2022)and Kandinsky v2.2 Razzhigaev et al. (2023)) and performs P-T2I in the compressed latent space of frozenCLIP model. Specifically, unlike previous MLLM-based methodologies, -ECLIPSE aligns the output spaceof priors with CLIP vision space instead of the CLIP text space. -ECLIPSE takes multiple images andtext instructions as input and estimates the respective vision embeddings, which can be used by the frozendiffusion UNet model from the UnCLIP stack to generate the resulting image. This elevates the training timedependencies on diffusion models for P-T2I; significantly contributing to the resource efficiency. Additionally,as diffusion or MLLM-based priors are still compute heavy due to a huge number of parameters and slowerconvergence, we build upon ECLIPSE Patel et al. (2023b) and SEED Ge et al. (2023), which shows that text-to-image mapping can be optimized through contrastive pre-training. Here, we select ECLIPSE as preferredchoice of prior architecture for best efficiency. At last, we propose a subject-driven instruction tuning taskbased on the image-text interleaved data as a pre-training strategy. This involves creating 2 million high-quality image-text pairs, where text embeddings linked to subjects are substituted with the respective imageembeddings, which in return are considered as input to the -ECLIPSE . While -ECLIPSE can be pluggedwith these pre-trained methods, we explore the possibility of -ECLIPSE to incorporate Canny edge as anadditional coarse-level control to synergetically work with subject-driven image generation tasks. provides the overview of -ECLIPSE capabilities.",
  "At last, -ECLIPSE inherits the smooth CLIP latent space.This allows us to perform the seamlesstransition between multi-concept generated images": "1The designation -ECLIPSE is inspired by its conceptual alignment with the -calculus. In this context, the -ECLIPSE modelfunctions similarly to a functional abstraction within -calculus, where it effectively binds variables. These variables, in our case,represent novel visual concepts that are integrated through composition prompts. Here, ECLIPSE indicates our architecturedesign choice.",
  "Related Works": "Text-to-Image Generative Models.Pioneering efforts in image generation, notably DALL-E Rameshet al. (2021) and CogView Ding et al. (2021), leveraged autoregressive models to achieve significant results.Recent advancements predominantly feature diffusion models, acclaimed for their high image fidelity anddiversity in text-to-image (T2I) generation. A notable example is Stable Diffusion, which builds upon theLatent Diffusion Model (LDM) Rombach et al. (2022) and excels in semantic and conceptual understandingby transitioning training to latent space. Imagen Saharia et al. (2022), Pixart- Chen et al. (2023b), andDALL-E 3 Betker et al. (2023) propose using a large T5 language model to improve language understanding.DALL-E 2 Ramesh et al. (2022) along with its UnCLIP variation models such as Kandinsky Razzhigaevet al. (2023) and Karlo Lee et al. (2022), uses a diffusion prior and diffusion UNet modules to generateimages using the pre-trained CLIP Radford et al. (2021) model. Personalized T2I Methods.Approaches like Textual Inversion Gal et al. (2022), DreamBooth Ruiz et al.(2023a), and Custom Diffusion Kumari et al. (2023) focus on training specific parameters to encapsulatevisual concepts. LoRA Hu et al. (2021) and Perfusion Tewel et al. (2023) target efficient fine-tuning adjust-ments, particularly rank 1 modifications. However, these methods are constrained by their requirement forconcept-specific tuning. ELITE Wei et al. (2023) was the first approach addressing fast customized gener-ation for single-subject T2I. BLIP-Diffusion Li et al. (2023a) adapts the BLIP2 encoder Li et al. (2023b),training approximately 1.5B parameters to enable zero-shot, subject-driven image generation. IP-Adapterintroduces a decoupled cross-attention mechanism, negating the need to train the foundational UNet modelby permitting fine-tuning of a reduced number of 22M parameters. Mix-of-Show Gu et al. (2023) and Zip-LoRA Shah et al. (2023) train individual concepts and then combinethem to generate multiple subjects. Break-A-Scene Avrahami et al. (2023) shows multi-concept capability butrequires single images containing diverse objects. Subject Diffusion Ma et al. (2023a) creates a high-qualitydataset and presents the precision control for fast personalized multi-subject image generation. Kosmos-G and Emu2 Sun et al. (2023), akin to Subject-Diffusion Ma et al. (2023a), employs a Multimodal LargeLanguage Model (MLLM) for text-image embedding alignment, though it necessitates extensive parameteroptimization (1.9B-37B). These multi-subject P-T2I methods are not only demanding in terms of parametersbut also depend on a massive number of frozen parameters of the diffusion UNet model, increasing trainingcomputational loads. In contrast, our model, -ECLIPSE , forgoes test-time fine-tuning and training-timereliance on the diffusion UNet model for single and multi-concept, control-guided P-T2I, positioning it as aresource-efficient solution. At last, methods like GLIGEN Li et al. (2023c), ControlNet Zhang et al. (2023a), and UniControl Qinet al. (2023) incorporate additional controls (i.e., edge map, depth, segmentations) into the diffusion modelto generate the desired images. BLIP-Diffusion, IP-Adapter, and Kosmos-G can leverage such pre-trainedcontrols. However, in many scenarios, these controls are too strong, making generated images lose subject-specific details. We show that -ECLIPSE learns to balance the edge map, subjects, and composition. Weoffer a more comprehensive review of related works in the appendix.",
  "Method": "In this section, we introduce -ECLIPSE , our approach to multi-subject personalized text-to-image genera-tion. Our method combines the contrastive text-to-image strategy from ECLIPSE with the novel image-textinterleaved pretraining strategy, notably omitting the need for explicit diffusion modeling. Our approachmainly capitalizes on the efficient utilization of the CLIP latent space. outlines the end-to-endframework. The primary objective of -ECLIPSE is to facilitate single and multi-subject P-T2I generation processes,accommodating edge maps as conditional guidance. Initially, we detail the problem formulation and elaborateon the UnCLIP stack design of the -ECLIPSE . Subsequently, we delve into the image-text interleavedtraining methodology.This fine-tuning process enables the -ECLIPSE to harness semantic correlationsbetween CLIP image and text latent spaces while preserving subject-specific visual features.",
  "Trained using Projection loss and CLIP Contrastive loss": ": Three stages of the -ECLIPSE pipeline. 1) Create the image-text interleaved features usingfrozen CLIP. 2) Pre-train the -ECLIPSE (34M parameters) using Eq. 1, which ensures the mapping to thedesired latent space given the image-text interleaved data. 3) During inference, the frozen Kandinsky v2.2diffusion UNet model takes the output from the -ECLIPSEand generates the image.",
  "Text-to-Image Prior Mapping": "In the UnCLIP T2I models, the objective of the text-to-image prior model (f) is to establish a proficienttext-to-image embedding mapping. This model is designed to adeptly map textual representations to theircorresponding visual embeddings, denoted as (f : zy zx), where zx/y represent the embeddings for imagesand text, respectively. The visual embedding predictions ( zx = f(zy)) are then effectively utilized by thediffusion image generators (h), which are inherently conditioned on these vision embeddings (h : zx x).In our experiments, we utilize the Kandinsky v2.2 diffusion UNet model as h. As shown in , the CLIP vision encoder is very expressive and preserves the finegrained details inzx that is required to reconstruct the input image. CLIP image embedding itself achieves the high conceptalignment score (DINO: 0.66) similar to the finetuning-based DreamBooth method Ruiz et al. (2023a). Our goal is to accurately estimate the image embedding zx, incorporating the subject representations, therebyeliminating reliance on h during training. Existing LDM-based P-T2I methods are limited by the LDMssingular module approach (h : zy x). Consequently, mastering the latent space of h becomes essentialfor effective P-T2I for the baseline methodologies, which limits the previous methodologies. We propose a new mapping function, f, which processes text representations (zy) alongside subject (xk)specific visual representations (zxk), to derive an image embedding that encapsulates both text promptsand subject visuals (zx). The challenge lies in harmonizing zxk and zy within f : (zy, zxk) zx, ensuringalignment while preventing overemphasis on either aspect, as this could compromise composition alignment.To address this, we employ the contrastive pre-training strategy after Patel et al. (2023b):",
  "j[N] exp(zix, zjy/).(1)": "Here, serves as the hyperparameter. i and j represent the index of the given input batch with the size N. represents the inner-product and is the temperature parameter. The first loss term (projection loss)measures the mean-squared error between the estimated and actual image embeddings, primarily ensuringconcept alignment. However, our preliminary studies reveal that exclusive reliance on this term diminishescomposition alignment. Therefore, we stick with the contrastive loss component (the second loss term) tobolster compositional generalization, with balancing concept and composition alignment. Additional Coarse-level Control-based T2I Prior Mapping.Acknowledging the limitations in ex-isting methods, which necessitate learning the diffusion latent space even for additional control inputs, weendeavor to achieve a more nuanced balance between subject, text, and supplementary conditions. Con-sequently, we have augmented -ECLIPSE to accommodate an additional modality, a Canny edge map,providing more refined control over subject-driven image generation. This entails modifying the prior modelto accept additional conditions (f : (zy, zxi, zc) zx, here zc is the additional modality embedding). Additionally, during training, we drop zc for 1% to improve the image generations without relying on the edgemap. This enhances the stability and broadens the generalization capabilities of -ECLIPSE , yielding benefitseven in the absence of these controls during inference. Our results demonstrate that -ECLIPSE learns aunified mapping function, accurately estimating target image representations through the effective integrationof text, image, and edge maps leading to learning coarse-level controls instead of hard constraints.",
  "Image-text Interleaved Training": "Our approach targets developing a versatile prior model capable of processing diverse inputs to estimatetarget visual outputs. Drawing from earlier methodologies, a straightforward solution involves concatenatingdifferent inputs, like combining text (a dog wearing sunglasses) with respective concept-specific images.Preliminary experiments indicated that this method does not effectively capture the intricate relationshipsbetween target text tokens (e.g. dog) and the corresponding concept images, especially when multipleconcepts are present. To address this, we adopt the interleaved pre-training strategy used in Kosmos-G, but with a notablemodification to enhance resource efficiency. We incorporate pretrained frozen CLIP text and vision encodersfor extracting modality-specific embeddingsseparating text-only from subject-specific images. The keyrefinement in our process is the substitution of subject token-specific text embeddings with correspondingvision embeddings instead of introducing additional trainable tokens to handle the image embeddings viaresampler Alayrac et al. (2022). First, we extract reference concept visual features (zxk R1x1280) fromthe CLIP vision encoder. Similarly, we also extract the text prompt features (zy R77x1280) from the lastlayer of the CLIP text encoder. Here, 1280 is the CLIP-specific feature dimension. At last, we replace theconcept noun corresponding latent features from zy with zxk; resulting in image-text interleaved featureswhile preserving the contextual information of the text features. This alteration allows us to bypass the needto train the big priors models (e.g. MLLMs), significantly improving the models proficiency in handlinginterleaved data. For the generation of high-quality training datasets, we carefully selected 2 million high-quality images fromthe LAION dataset Schuhmann et al. (2022), each with a resolution of 1024x1024. Utilizing BLIP2, wegenerate captions for these images and employ SAM Kirillov et al. (2023) for extracting noun or subject-specific segmentation masks. Given the CLIP models requirement for 224x224 resolution images, we avoidresizing the masks within their original resolutions. Instead, we opt for cropping the area of interest usingGrounding DINO Liu et al. (2023a), followed by resizing the masked object while preserving its aspect ratio.This technique is crucial in retaining maximum visual information for each subject during the training phase.We provide more details about the filters used in the appendix.",
  "Additional Concept-Specific Finetuning": "Due to the nature of UnCLIP models, even if -ECLIPSE is very accurate, the diffusion UNet model (h)may not be effective in generating very unique visual representations. In , we can observe thatgenerated images do not always precisely follow the reference image (e.g. hair style of the person). However,such behavior is common across the fast P-T2I methods and they lack in terms of maintaining performancecompared to the finetuning-based methods (as outlined in ). Therefore, we extend the -ECLIPSE andperform concept-specific finetuning. Compared to the traditional finetuning methodologies (e.g. DreamBooth), -ECLIPSE provides very uniqueadvantages. As -ECLIPSE prior model (f) is pre-trained for personalization, there is no need for furtherfinetuning the f and we need to only finetune diffusion UNet model. Importantly, the fine-tuning of theh does not depend on the text embeddings (zy). Hence, this leads to stable fine-tuning of the h; unlikeDreamBooth on stable diffusion that observes catastrophic forgetting. The new fine-tuning objective is:",
  "|| h(x(t), t, zx)||22.(2)": "Here, zx is the visual feature of the reference concept image x. Notably, we do not need to use regularizationfrom the DreamBooth as text alignment is already ensured during the pretraining stage of -ECLIPSE .Moreover, this finetuning can be performed across the set of given visual concepts altogether in a singlemodel without degrading performance.",
  "In this section, we first introduce the experimental and evaluation setups. Later, we delve into the qualitativeand quantitative results": "Training and inference details.We initialize our model, -ECLIPSE , equipped with 34M parameters.We train our model on an image-text interleaved dataset of 2M instances, partitioned into 1.6M for trainingand 0.4M for validation. The model is specifically tuned for the Kandinsky v2.2 diffusion image decoder.Therefore, we use pre-trained OpenCLIP-ViT-G/142 as the text and vision encoders, ensuring alignmentwith Kandinsky v2.2 image embeddings. Training is executed on 2 x A100 GPUs, leveraging a per-GPUbatch size of 512 and a peak learning rate of 0.00005, across approximately 100,000 iterations, summing upto 74 GPU hours. During inference, the model employs 50 DDIM steps and 7.5 classifier-free guidance forthe Kandinsky v2.2 diffusion image generator. Adhering to baseline methodologies, we perform the P-T2Ifollowing the baseline papers protocols. For -ECLIPSE , target subject pixel regions in reference images aresegmented before embedding extraction via the CLIP(vision) encoder. We drop the Canny edge map duringinference unless specified explicitly. Unless specified all results (quantitative and qualitative) are withoutconcept-specific additional fine-tuning. Evaluation setup.We primarily utilize Dreambench (encompassing 30 unique concepts with 25 promptsper concept) for qualitative and quantitative evaluations using DINO and CLIP-based metrics Ruiz et al.",
  "-ECLIPSE*0.517314.3%0.74370.3158": "(2023a). Due to their limitations, we extend our evaluations on the ConceptBed Patel et al. (2023a) bench-mark (covering 80 diverse imagenet concepts and a total of 33K composite prompts), where we reportperformance on concept replication, concept, and composition alignment using the Concept Confidence De-viation (CCD) metric Patel et al. (2023a). We extend Dreambench for multi-subject customization andpresent the Multibench dataset. Multibench contains about 24 unique concepts and 15 diverse promptsthat result in 904 two-subject specific prompts and 1476 three-subject specific prompts. We provide furtherdetails about the Multibench in the appendix.",
  "Results & Analysis": "Quantitative comparison.The quantitative assessments detailed in and focus on thesingle-concept T2I task, while shows the results on multi-concept-driven image generation.ForDreambench and Multibench, we generate and evaluate four images per prompt, reporting average per-formance on three metrics (DINO, CLIP-I, and CLIP-T). In the case of ConceptBed, we process each ofthe 33K prompts to generate a single concept image. The results, as depicted in these tables, highlight-ECLIPSE s superior performance in composition alignment while maintaining competitive concept align-ment. Analysis on ConceptBed () indicates that -ECLIPSE exhibits a notable proficiency in conceptreplication, albeit with a marginal trade-off in concept alignment for enhanced composition fidelity. Com-",
  "Published in Transactions on Machine Learning Research (11/2024)": "James Seale Smith, Yen-Chang Hsu, Lingyu Zhang, Ting Hua, Zsolt Kira, Yilin Shen, and Hongxia Jin.Continual diffusion:Continual customization of text-to-image diffusion with c-lora.arXiv preprintarXiv:2304.06027, 2023. Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Zhengxiong Luo, Yueze Wang, YongmingRao, Jingjing Liu, Tiejun Huang, et al. Generative multimodal models are in-context learners. arXivpreprint arXiv:2312.13286, 2023.",
  ": DreamBooth (Stable Diffusion v1.5)vs.-ECLIPSE(with fine-tuning) w.r.t. DINOand CLIP-T metrics on Dreambench": "Auxiliaryfinetuning.Wefurtherperformconcept-specific fine-tuning (as described in Sec-tion 3.3 and Section D). After finetuning,asshown in , -ECLIPSEoutperforms theDreamBoothandBLIP-Diffusionintermsofconcept alignment (DINO) while maintaining theperformance on composition alignment (CLIP-T).Our findings, illustrated in , reveal that-ECLIPSEandDreamBoothexhibitimprovedperformance with incremental fine-tuning steps.Notably, the DINO score improved from 0.61 to 0.68with few optimization steps and outperforms thebaselines (see ). A detailed analysis indicatesthat while DreamBooths DINO score improves, itsCLIP-T performance diminishes, hinting at conceptoverfitting.Conversely, -ECLIPSEconsistentlyimproves in DINO scoring without adversely im-pacting the CLIP-T performance, underscoring the efficacy of our image-text interleaved training approachat the prior stage.Qualitative comparisons, as shown in , further highlight the benefits offine-tuning -ECLIPSE with minimal steps. We provide detailed experimental setup in Appendix Section D. Qualitative comparisons.In , we present a range of single subject-specific images gener-ated by various methodologies including BLIP-Diffusion, IP-Adapter, Kosmos-G, Emu2, and -ECLIPSE .-ECLIPSE demonstrates exemplary proficiency in composition while ensuring concept alignment. In con-trast, the baselines often overemphasize reference images or exhibit concept dilution, leading to higherconcept alignment but compromised composition. Interestingly, we find that Emu2 can capture the single-subjects but it fails to reproduce them with complex text compositions (as shown in ). Similarly,a exhibits -ECLIPSE s multi-concept generation prowess, in comparison to ZipLoRA (fine-tuning-based approach) along with Kosmos-G and Emu2 (Multimodal LLM-based approaches), underscoring itscapability to rival compute-intensive methods. We discuss additional examples and limitations in the ap-pendix. That said, even though -ECLIPSE improves the performance over the baselines, this is still notenough and it signifies the challenges associated with fast multi-concept personalization. Canny edge controlled image generation.As shown in b, the baseline (BLIP-Diffusion) ad-heres strictly to the imposed edge maps, often at the cost of concept retention (rows 1, 3, and 4). Thisleads to a large number of unwanted artifacts in the generated images. To further ground this behavior, wefirst generated images using Stable Diffusion v1.5 for Dreambench prompts without customization then weextracted the Canny edge map and used this edge map to control the subject-driven image generations. Atlast, we report the performance in . It can be observed that both baselines IP-Adapter and BLIP-Diffusion drop the DINO score by 30%, which follows the qualitative results. While -ECLIPSE do not followthe Canny edge precisely but preserves the concept alignment and improves the performance relatively by21%. Ablations.We extend our study to evaluate the individual contributions of different components in-ECLIPSE . Initially, the models performance with solely the projection loss (referenced in Eq.1) is as-sessed. Subsequent experiments involve training -ECLIPSE variants with varying hyperparameters for thecontrastive loss, specifically values of 0.2 and 0.5. A comparative analysis of these baselines is conducted",
  "w/ edge conditions (=0.2)0.362-0.020": "against the fully equipped -ECLIPSE model, which incorporates Lprior (Eq.1) with = 0.2 and utilizesCanny edge maps during training. Relying solely on projection loss results in high concept alignment butcompromises compositions (). The contrastive loss variant with = 0.5 enhances composition align-ment at the expense of concept alignment, whereas = 0.2 achieves a more balanced performance. Crucially,the integration of Canny edge maps during training optimally balances both alignments and, specifically,improves the concept alignment. The negative values indicate that the CCD oracle model is highly confidentin the generated images. Multi-subject interpolation.A key attribute of the CLIP latent space is the ability to perform smoothinterpolation between two sets of embeddings.We conducted experiments to demonstrate -ECLIPSE sability to learn and replicate this smooth latent space transition.We selected two distinct dog breeds(<dog1>, <dog2>) and two types of hats (<hat1>, <hat2>) as the concepts. -ECLIPSE was then usedto estimate image embeddings for all four possible combinations, each corresponding to the input phrase a<dog> wearing a <hat>. showcases a gradual and seamless transition in the synthesized imagesfrom the top left to the bottom right. Unlike current diffusion models, which often exhibit sensitivity toinput variations requiring numerous iterations of user interactions for desired outcomes, -ECLIPSE inheritsCLIPs smooth latent space. This not only facilitates progressive changes in the conceptual domain but alsoextends the models utility by enabling personalized multi-subject interpolations.",
  "Conclusion": "In this paper, we have introduced a novel training-time diffusion-agnostic methodology for personalizedtext-to-image (P-T2I) applications, utilizing the latent space of the pre-trained CLIP model with highefficiency.Our -ECLIPSE model, trained on an image-text interleaved dataset, achieves the capabilityto execute single-concept, multi-concept, and edge-guided controlled P-T2I tasks using a singular modelframework, while simultaneously minimizing resource utilization. Notably, -ECLIPSE sets a new benchmarkin achieving competitive performance in terms of concept and composition alignment. Furthermore, ourresearch illuminates the potential of -ECLIPSE in exploring and leveraging the smooth latent space. Thiscapability unlocks new avenues for interpolating between multiple concepts, thereby generating entirelynovel concepts. Our findings underscore a promising pathway to improve MLLMs to effectively control thepre-trained diffusion image models without necessitating extra supervision.",
  "Limitations": "Primarily, despite its strengths, CLIPs inability to perfectly capture hierarchical representations adds theupper bound on performance. Hence, enhancing CLIPs representations could significantly boost our frame-works efficacy in P-T2I mapping. Even though -ECLIPSE model, trained on 34 million parameters and 1.6million images, presents a substantial foundation, yet, theres potential for further refinement, as increasingthe quality of data and the number of parameters could yield even better outcomes. However, this is outside",
  "Broader Impact": "The current landscape of text-to-image (T2I) generative models is dominated by approaches that rely onextensive data and large-scale models to achieve state-of-the-art (SOTA) performance, which demands sig-nificant computational resources. In contrast, our work with -ECLIPSE demonstrates that it is feasible toattain competitive performance relative to SOTA large models while achieving a tenfold reduction in resourceconsumption. This advancement not only makes T2I generative models more accessible and cost-effectivebut also promotes sustainable AI practices by significantly lowering the environmental impact associatedwith large-scale model training.",
  "Yuval Alaluf, Elad Richardson, Gal Metzer, and Daniel Cohen-Or. A neural space-time representation fortext-to-image personalization. ACM Transactions on Graphics (TOG), 42(6):110, 2023": "Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc,Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model forfew-shot learning. Advances in Neural Information Processing Systems, 35:2371623736, 2022. Moab Arar, Rinon Gal, Yuval Atzmon, Gal Chechik, Daniel Cohen-Or, Ariel Shamir, and Amit H. Bermano.Domain-agnostic tuning-encoder for fast personalization of text-to-image models. In SIGGRAPH Asia2023 Conference Papers, pp. 110, 2023.",
  "Omri Avrahami, Kfir Aberman, Ohad Fried, Daniel Cohen-Or, and Dani Lischinski. Break-a-scene: Extract-ing multiple concepts from a single image. arXiv preprint arXiv:2305.16311, 2023": "James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang,Joyce Lee, Yufei Guo, et al.Improving image generation with better captions.Computer Science. openai. com/papers/dall-e-3. pdf, 2:3, 2023. Hong Chen, Yipeng Zhang, Xin Wang, Xuguang Duan, Yuwei Zhou, and Wenwu Zhu.Disenbooth:Disentangled parameter-efficient tuning for subject-driven text-to-image generation.arXiv preprintarXiv:2305.03374, 2023a. Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok,Ping Luo, Huchuan Lu, et al.Pixart-alpha: Fast training of diffusion transformer for photorealistictext-to-image synthesis. arXiv preprint arXiv:2310.00426, 2023b.",
  "Wenhu Chen, Hexiang Hu, Chitwan Saharia, and William W Cohen.Re-imagen: Retrieval-augmentedtext-to-image generator. In The Eleventh International Conference on Learning Representations, 2022": "Wenhu Chen, Hexiang Hu, Yandong Li, Nataniel Rui, Xuhui Jia, Ming-Wei Chang, and William W Cohen.Subject-driven text-to-image generation via apprenticeship learning. arXiv preprint arXiv:2304.00186,2023c. Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin, Junyang Lin, Xu Zou, ZhouShao, Hongxia Yang, et al. Cogview: Mastering text-to-image generation via transformers. Advances inNeural Information Processing Systems, 34:1982219835, 2021. Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Haim Bermano, Gal Chechik, and DanielCohen-or. An image is worth one word: Personalizing text-to-image generation using textual inversion.In The Eleventh International Conference on Learning Representations, 2022.",
  "Ligong Han, Yinxiao Li, Han Zhang, Peyman Milanfar, Dimitris Metaxas, and Feng Yang. Svdiff: Compactparameter space for diffusion fine-tuning. arXiv preprint arXiv:2303.11305, 2023": "Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al.Lora: Low-rank adaptation of large language models. In International Conference on Learning Represen-tations, 2021. Xuhui Jia, Yang Zhao, Kelvin CK Chan, Yandong Li, Han Zhang, Boqing Gong, Tingbo Hou, HuishengWang, and Yu-Chuan Su. Taming encoder for zero fine-tuning image customization with text-to-imagediffusion models. arXiv preprint arXiv:2304.02642, 2023. Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao,Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al.Segment anything.arXiv preprintarXiv:2304.02643, 2023. Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi-concept cus-tomization of text-to-image diffusion. In Proceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition, pp. 19311941, 2023.",
  "Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-trainingwith frozen image encoders and large language models. arXiv preprint arXiv:2301.12597, 2023b": "Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jianwei Yang, Jianfeng Gao, Chunyuan Li, andYong Jae Lee. Gligen: Open-set grounded text-to-image generation. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition, pp. 2251122521, 2023c. Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang,Hang Su, Jun Zhu, et al. Grounding dino: Marrying dino with grounded pre-training for open-set objectdetection. arXiv preprint arXiv:2303.05499, 2023a. Zhiheng Liu, Ruili Feng, Kai Zhu, Yifei Zhang, Kecheng Zheng, Yu Liu, Deli Zhao, Jingren Zhou, andYang Cao.Cones:Concept neurons in diffusion models for customized generation.arXiv preprintarXiv:2303.05125, 2023b. Zhiheng Liu, Yifei Zhang, Yujun Shen, Kecheng Zheng, Kai Zhu, Ruili Feng, Yu Liu, Deli Zhao, JingrenZhou, and Yang Cao. Customizable image synthesis with multiple subjects. In Thirty-seventh Conferenceon Neural Information Processing Systems, 2023c.",
  "Maitreya Patel, Changhoon Kim, Sheng Cheng, Chitta Baral, and Yezhou Yang. Eclipse: A resource-efficienttext-to-image prior for image generations. arXiv preprint arXiv:2312.04655, 2023b": "Can Qin, Shu Zhang, Ning Yu, Yihao Feng, Xinyi Yang, Yingbo Zhou, Huan Wang, Juan Carlos Niebles,Caiming Xiong, Silvio Savarese, et al. Unicontrol: A unified diffusion model for controllable visual gener-ation in the wild. arXiv preprint arXiv:2305.11147, 2023. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, GirishSastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models fromnatural language supervision. In International conference on machine learning, pp. 87488763. PMLR,2021. Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, andIlya Sutskever. Zero-shot text-to-image generation. In International Conference on Machine Learning, pp.88218831. PMLR, 2021.",
  "Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditionalimage generation with clip latents. arXiv preprint arXiv:2204.06125, 1(2):3, 2022": "Anton Razzhigaev, Arseniy Shakhmatov, Anastasia Maltseva, Vladimir Arkhipkin, Igor Pavlov, Ilya Ryabov,Angelina Kuts, Alexander Panchenko, Andrey Kuznetsov, and Denis Dimitrov. Kandinsky: An improvedtext-to-image synthesis with image prior and latent diffusion. In Proceedings of the 2023 Conference onEmpirical Methods in Natural Language Processing: System Demonstrations, pp. 286295, 2023. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjrn Ommer. High-resolutionimage synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computervision and pattern recognition, pp. 1068410695, 2022. Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dream-booth: Fine tuning text-to-image diffusion models for subject-driven generation. In Proceedings of theIEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2250022510, 2023a. Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Wei Wei, Tingbo Hou, Yael Pritch, Neal Wadhwa, MichaelRubinstein, and Kfir Aberman. Hyperdreambooth: Hypernetworks for fast personalization of text-to-image models. arXiv preprint arXiv:2307.06949, 2023b. Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, KamyarGhasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in Neural Information ProcessingSystems, 35:3647936494, 2022. Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti,Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al.Laion-5b: An open large-scale dataset for training next generation image-text models. Advances in Neural Information ProcessingSystems, 35:2527825294, 2022. Viraj Shah, Nataniel Ruiz, Forrester Cole, Erika Lu, Svetlana Lazebnik, Yuanzhen Li, and Varun Jampani.Ziplora: Any subject in any style by effectively merging loras. arXiv preprint arXiv:2311.13600, 2023.",
  "Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ip-adapter: Text compatible image prompt adapterfor text-to-image diffusion models. arXiv preprint arXiv:2308.06721, 2023": "Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusionmodels. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 38363847,2023a. Yuxuan Zhang, Jiaming Liu, Yiren Song, Rui Wang, Hao Tang, Jinpeng Yu, Huaxia Li, Xu Tang, YaoHu, Han Pan, et al. Ssr-encoder: Encoding selective subject representation for subject-driven generation.arXiv preprint arXiv:2312.16272, 2023b.",
  "APreliminaries for T2I Diffusion Models": "As evidenced in numerous contemporary studies regarding T2I models, Stable Diffusion (SD) Rombach et al.(2022) has emerged as a predominant backbone for T2I models. SD involves training diffusion models inlatent space, reversing a forward diffusion process that introduces noise into the image. A notable feature ofSD is its integration of cross-attention, facilitating various conditions like text input. Operating in VQ-VAElatent space, SD not only achieves exceptional generative performance surpassing that in pixel space butalso significantly reduces the computational demands. UnCLIP models (such as DALL-E 2) are very similar to the Stable Diffusion. In contrast, the UnCLIP takesthe modular approach. UnCLIP first trains the diffusion text-to-image to the image prior (f) to estimatethe image embeddings (zx) from the text embeddings (zy). In parallel, a UNet-like diffusion image generator(h) is trained to generate images (x) conditioned on ground truth vision embeddings (zx).",
  "||zx f(z(t)x , t, zy)||22.(3)": "ECLIPSE proposes the contrastive learning strategy (Eq. 1 main paper) instead of minimizing Eq. 3. Thediffusion image generator is trained by following standard -prediction formulation. Here, h will estimatethe ground truth added Gaussian noise N(0, I), given the noise image X(t) for all timesteps t [0, T]and input conditions (such as zx, zy).",
  "BImage-Text Interleaved Training Details": "Dataset CreationIn constructing the dataset, we adhered to the data processing pipeline of SubjectDiffusion Ma et al. (2023a). We utilized the LAION-5B High-Res dataset, requiring a minimum image size of1024x1024 resolution. Original captions were replaced with new captions generated by BLIP-2 (flan-t5-xl)3.Subjects were extracted using Spacy4. For each subject, we identified bounding boxes employing GroundingDINO Liu et al. (2023a), setting both box-threshold and text-threshold values to 0.2. We retained imageswith 1 to 8 detected bounding boxes, discarding the rest. Additionally, captions with multiple instances ofidentical objects were filtered, allowing a maximum of 6 identical objects. Following bounding box detection,individual subject masks were isolated using Segment-Anything (SAM) Kirillov et al. (2023). To enhancethe efficiency of the training process, we pre-processed the dataset by pre-extracting features from CLIPvision and text encoders. During this phase, images predominantly featuring a background (white portion)exceeding 50% of the total area were excluded. We preserved bounding boxes with an area ranging from0.08 to 0.7 of the total image area and logit scores of at least 0.3. Masks constituting less than 40% of thebounding box area were discarded. For the selection of subjects in images, we constrained the range to 1-4subjects per image, excluding those with more than 4 subjects. At last, the interleaved image-text exampleswith respective ground truth images are shown in . Dataset StatisticsIn the final analysis, our dataset comprised a total of 1,990,123 images. The distri-bution of subjects per image exhibited a range from 1 to 4, with the following breakdown: 1,479,785 images",
  "A on top of the near the river": ": Examples of image-text interleaved training data. The left column shows the input of theprior model and the right images shows the ground truth corresponding images. Note: these examples aregenerated from -ECLIPSE for better interpretability. featuring one subject, 432,831 images with two subjects, 65,597 images containing three subjects, and 11,910images showcasing four subjects. The overall count of unique subjects acquired from this dataset amountedto 30,358. We partitioned our dataset into an 80:20 split between training and validation, reserving theremaining 1.6 million images for training and the rest for validation.",
  "CImplementation Details": "The -ECLIPSE transformer prior architecture is significantly more compact compared to other Text-to-Image (T2I) methodologies. Our model employs a configuration of 16 Attention Heads, each with a dimensionsize of 32, alongside a total of 10 layers. Additionally, the embedding dimension size for our model is set at1280, supplemented by 4 auxiliary embeddings (including, one for canny edge map). As -ECLIPSE is nota diffusion prior model, we do not keep time embedding layers. Overall, the -ECLIPSE model comprisesapproximately 34 million parameters, establishing it as a streamlined yet effective solution for Personalaized-T2I. Notably, the standard UnCLIP T2I priors contain 1 billion parameters.",
  "D-ECLIPSE with Finetuning": "As demonstrated in the main paper (), the superiority of fine-tuning-based personalization method-ologies, whether applied to single-subject or multi-subject frameworks, over non-fine-tuning alternatives isevident. Consequently, we have expanded our analysis through additional fine-tuning of the -ECLIPSE . Experimental Setup.Given that -ECLIPSE effectively trains the T2I prior, capturing concept-specificfeatures to a significant degree, we opted not to further optimize this component. Our focus shifted toexclusively fine-tuning the diffusion UNet model (h), employing the AdamW optimizer at a learning rate of1e-5, without warm-up steps. For the DreamBooth application within the Stable Diffusion v1.5 model, weselected a learning rate of 5e-6, maintaining consistency in other hyperparameters. To simplify, we excludedthe use of a prior preservation regularizer and conducted training on the Dreambench platform using a singleRTX A6000 GPU. Advantages of fine-tuning -ECLIPSE.The fine-tuning of -ECLIPSE , in comparison to the base-lines, reveals two key benefits: 1) Achieving state-of-the-art (SOTA) performance within a few finetuningsteps. 2) Unlike the Stable Diffusion model, which exhibits catastrophic forgetting of nearby concepts post-DreamBooth fine-tuning, -ECLIPSE maintains previous knowledge. This suggests that a single model issufficient to effectively fine-tune across multiple concepts together.",
  "MethodMulti-SubjectFinetuning-FreeBase-Model# of Input Images": "Re-Imagen Chen et al. (2022)ImagenSingleTextual Inversion Gal et al. (2022)SDv1.4MultipleDreamBooth Ruiz et al. (2023a)SDv1.4MultipleCustom Diffusion Kumari et al. (2023)SDv1.4MultipleELITE Wei et al. (2023)SDv1.4SingleE4T Gal et al. (2023)SDSingleCones Liu et al. (2023b)SDv1.4SingleSVDiff Han et al. (2023)SDMultipleUMM-Diffusion Ma et al. (2023b)SDv1.5SingleXTI Voynov et al. (2023)SDv1.4MultipleContinual Diffusion Smith et al. (2023)-MultipleInstantBooth Shi et al. (2023)SDv1.4MultipleSuTi Chen et al. (2023c)ImagenMultipleTaming Jia et al. (2023)ImagenSingleBLIP-Diffusion Li et al. (2023a)SDv1.5SingleCones 2 Liu et al. (2023c)SDv2.1SingleDisenBooth Chen et al. (2023a)SDv2.1Single FastComposer Xiao et al. (2023)SDv1.5SinglePerfusion Tewel et al. (2023)SDv1.5MultipleMix-of-Show Gu et al. (2023)ChilloutmixMultipleNeTI Alaluf et al. (2023)SDv1.4MulitpleBreak-A-Scene Avrahami et al. (2023)SDv2.1Single*",
  "EExtended P-T2I Baselines Comparison": "We further expand our comparative analysis of P-T2I methods encompassing a total of 33 approachesincluding ours and parallel works. summarizes them into four crucial aspects: 1) multi-subjectsupport, 2) fine-tuning free, 3) base model types, and 4) the required number of input images. To summarize,-ECLIPSE is the only methodology built on top of the UnCLIP models while supporting multi-subject drivenimage generation with fine-tuning free, and only requires a single reference image input for the training. Wedetail the comparison below: Multi-Subject Generation.Multi-subject generation enables users to integrate multiple personal sub-jects to generate an image that follows the text prompts and aligns with all the concept visuals. In total, 15of the 33 methods offer this capability, while 6 methods support fast multi-subject personalization, othersdemand separate training for each subject to be learned and then an additional fusing step for combiningthe learned subjects is required (i.e. Zip-LoRA, Mix-of-Show). Among these methods, only a few can learnauxiliary guided information such as canny edge, depth maps, or open-pose and adapt style variation (i.e.Kosmos-G).",
  ": Qualitative examples of -ECLIPSE without finetuning and different stages of finetuning": "Fine-tuning Free (Fast Personalization).Many methods require test-time fine-tuning. Each varies onwhich part alteration occurs, as early models tend to modify the whole UNet. In contrast, recent modelstune a small portion of the cross-attention layers or introduce additional layers performing as adapters. Inour analysis of P-T2I methodologies, 14 out of 33 methods employ a finetuning-free approach which enablesfast personalization. Diffusion Independent.A majority of the reviewed models utilize diffusion models, with Stable Diffusionbeing the predominant choice, spanning versions 1.4, 1.5, 2.1, and XL. Few adapt Imagen (SuTi, Taming)and Mix-of-show employs ChillOutMix as their pre-trained model, known for its adeptness at preservingrealistic concepts like human faces. A unique outlier in this landscape is our -ECLIPSE , the only one thateschews the use of any diffusion prior model. Easiness of Use.A more user-friendly model typically requires a single reference image per subject, asopposed to multiple images of the same subject. In our study, 19 methods offer P-T2I capabilities withjust one input image. In contrast, others often require 4 to 5 images of the subject. Additionally, somemethods necessitate storage space for learned concepts, ranging from a few hundred kilobytes (e.g., Perfusion,HyperDreamBooth) to several megabytes (e.g., Zip-LoRA). Our method stands out by eliminating the needfor individual concept pre-learning or storing any artifacts for P-T2I utilization, offering a streamlined,efficient user experience.",
  "FMultibench Dataset": "We provide additional qualitative results in . For the multi-subject image benchmark, our datasetcomprises 2,308 unique prompts, segmented into 904 two-subject and 1,476 three-subject prompts. Thisdataset integrates subjects from the original DreamBench dataset, featuring 30 distinct concepts. We ex-panded the dataset by incorporating additional concepts vital for two and three subject-specific prompts,such as various parks, hats, glasses, and more. Prompt templates and the count of unique subject cat-",
  "Two subjectsThree subjects": "{} in the {}{} with a {} and {}{} wearing a {}{} is playing with {} in {}{} chasing a {}{} with {} in front of {}{} looking at a {}{} with a {} and a view of the {}{} is sitting on a {}{} with a {} and {} in the background{} standing on a {}{} and {} playing in the garden{} and {} on top of the mountain{} and {} in the jungle{} and {} in the snow{} and {} on the beach{} and {} on a cobblestone street{} and {} standing next to each other : Number of occurrences of unique subject categories. The left side of the table are subjectsused for two subjects prompts, and the right side of the table are subjects used for three subjects prompts.",
  "G.1Generalization to Pretrained UnCLIP Diffusion Decoders": "Our approach is designed to generalize across any pretrained UnCLIP diffusion models, including Stable-UnCLIP/SDv2.1, Karlo, and Kandinsky v2.2. We conducted additional pretraining experiments to substan-tiate this claim and demonstrated the generalization ability of -ECLIPSE . We would like to reiterate thecore pipeline of -ECLIPSE or UnCLIP models, as detailed in Appendix A. The UnCLIP model comprisestwo key modules: (1) the Prior model, which maps the text embedding to the image embedding, and (2) the",
  "G.2Impact of Interleaved Pretraining": ".2 mentions that training -ECLIPSE without interleaved pretraining would yield similar resultsfor single-concept P-T2I tasks. However, for multi-concept personalization, our experiments reveal thatmodels trained without interleaved data sometimes struggle to synthesize the desired images. We conductedadditional experiments by training the model without interleaved data. Specifically, we concatenated theprompt embedding from the CLIP text encoder with the concept-specific image embedding and trained themodel with identical hyperparameters. The performance comparisons on DreamBench (single concept) andMultibench (multiple concepts) are shown below. Our findings indicate that without interleaved data, themodels ability to align concepts decreases as the number of concepts increases, resulting in a sharp 6% dropin the DINO score (see ). This performance degradation is primarily due to attribute leakage. Asshown in , when the model is tasked with generating A backpack at the ruins + <backpack> +<ruins>, the non-interleaved pretrained models tend to generate the backpack with the color of the ruins.",
  "G.3Effect of Data and Model Sizes": "We also conduct ablation on data sizes (100k, 500k, 1M, and 2M) and model parameter sizes (5M, 35M, and70M). Tables 12 and 13 report the performance of these newly trained models on DreamBench. All modelswere trained with identical hyperparameters as the proposed -ECLIPSE , though this may not fully optimizethe larger models (e.g., 70M parameters). The results show that data size influences model performance,with larger datasets improving concept understanding and prompt compositions.This also follows thequalitative results in . Notably, -ECLIPSE with only 5M parameters deliver performance close tothat of larger models, and the 34M parameter model even surpasses the 70M parameter model in terms ofDINO score. However, qualitative results (see ) show that increasing model parameters enhancesqualitative performance and concept alignment. Specifically, the 70M parameter model excels in generatingfiner details of the reference concept while adhering closely to text prompts. The 34M model offers a morebalanced trade-off between performance and resource efficiency.",
  "HQualitative Results & Failure Cases": "In this section, we showcase a collection of detailed qualitative examples from the P-T2I generation process,highlighting the challenges of crafting complex compositions within -ECLIPSE and comparative models.As depicted in , the complexity of the showcased examples progressively increases, illustrating anoticeable escalation in the intricacy of visual concepts from the top to the bottom of the figure. With therising complexity, we note a universal decline in the ability of all methodologies, including -ECLIPSE , topreserve subject fidelity accurately. Interestingly, despite these challenges, -ECLIPSE demonstrates a bettergrasp of compositional integrity, unlike the baseline models which falter across all complexity levels. Moreover, we present instances demonstrating the variability in outcomes produced by P-T2I methodsacross different trials. As illustrated in , while there is a semblance of consistency in generatingsingle and multiple concepts between models, Kosmos-G specifically shows variability in rendering multipleconceptsoccasionally misplacing elements of the Ironman suit on a dog or failing to include it altogether.This phenomenon suggests that -ECLIPSE minimizes image diversity to enhance result consistency, a traitobserved across the UnCLIP model family. offers qualitative insights into the performance of -ECLIPSE without and with minimal fine-tuning. It is evident that in certain edge cases, where -ECLIPSE initially struggles to fully grasp novelvisual concepts without finetuning, a modest application of few optimization iterations significantly enhances",
  ": Qualitative examples -ECLIPSE model trained with and without the interleaved pretrainingstrategy": "concept capture. Further optimization not only preserves text composition but also enriches minor, subject-specific details, underscoring the adaptability and finesse of -ECLIPSE in nuanced image generation. Moreover, in our evaluations using the Multibench dataset, we noticed that both the baseline models(Kosmos-G and Emu2) and -ECLIPSE encounter difficulties in precisely maintaining all subject-specificdetails, as depicted in . This underscores that zero-shot multi-subject P-T2I generationremains a significant challenge in the field. Further, we explored how well each model preserves gen-uine human facial characteristics in various scenarios, particularly when combined with differing captions.The qualitative examples in shed light on this aspect. Although each model strives to maintainthe original facial features, none succeeds in replicating the specific personal facial details accurately. Theseinstances typically fall short of precisely conveying the intended compositions, with the exception of onescenario in IP-Adapter FaceID, indicating a notable area for future improvements in model performance."
}