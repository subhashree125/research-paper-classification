{
  "Abstract": "Diffusion models (DMs) are one of the most widely used generative models for producing highquality images. However, a flurry of recent papers points out that DMs are least private formsof image generators, by extracting a significant number of near-identical replicas of trainingimages from DMs. Existing privacy-enhancing techniques for DMs, unfortunately, do notprovide a good privacy-utility tradeoff. In this paper, we aim to improve the current state ofDMs with differential privacy (DP) by adopting the Latent Diffusion Models (LDMs). LDMsare equipped with powerful pre-trained autoencoders that map the high-dimensional pixelsinto lower-dimensional latent representations, in which DMs are trained, yielding a moreefficient and fast training of DMs. Rather than fine-tuning the entire LDMs, we fine-tune onlythe attention modules of LDMs with DP-SGD, reducing the number of trainable parametersby roughly 90% and achieving a better privacy-accuracy trade-off. Our approach allows usto generate realistic, high-dimensional images (256x256) conditioned on text prompts withDP guarantees. Our approach provides a promising direction for training more powerful, yettraining-efficient differentially private DMs, producing high-quality DP images. Our code isavailable at",
  "Introduction": "A flurry of recent work highlights the tension between increasingly powerful diffusion models and data privacy,e.g., (Wu et al., 2023; Carlini et al., 2023; Tang et al., 2023; Hu & Pang, 2023; Duan et al., 2023; Matsumotoet al., 2023; Somepalli et al., 2023) among many. These papers commonly conclude that diffusion modelsare extremely good at memorizing training data, leaking more than twice as much training data as GANs(Carlini et al., 2023). This raises the question of how diffusion models should be responsibly deployed. Carliniet al. (2023) seek to use differential privacy as a remedy for the memorization problem, but their attempt wasunsuccessful due to the scalability issues of differential privacy. Our paper provides a practical and scalablefine-tuning routine for diffusion models with differential privacy guarantees to avoid generating identicalimages to those in the private dataset. Equal contribution.This work was done during a research visit to the Department of Computer Science at the University of British Columbia.Some part of this project was done at the Technical University of Denmark.",
  "Published in Transactions on Machine Learning Research (11/2024)": ": Synthetic CelebAHQ images from a non-DP-trained LDM (Top, black box, images taken fromRombach et al. (2022)) versus those from DP-trained LDMs (Bottom, red box). The non-DP syntheticimages exhibit humanly discernible characteristics of faces. On the other hand, DP synthetic samples exhibitdistortions at varying levels, where a higher privacy guarantee ( = 1) yields more distortion.",
  "The combination of considering LDMs and fine-tuning attention modules using DP-SGD is simple, yet a solidtool whose potential impact is substantial for the following reasons:": "Improved performance: DP-LDMs outperform many state-of-the-art methods in FID (Heuselet al., 2017) and downstream classification accuracy when evaluated on several commonly usedimage benchmark datasets in DP literature. This is due to unique aspects of our proposed method training DMs in the latent space and fine-tuning only a few selected parameters. This makes ourtraining process considerably more efficient than training a DM from scratch with DP-SGD in DPDM(Dockhorn et al., 2023), or fine-tuning the entire DM with DP-SGD in DP-Diffusion (Ghalebikesabiet al., 2023). Significantly Reduced GPU hours: Reducing the fine-tuning space in DP-LDMs not onlyimproves the performance but also helps democratize DP image generation using DMs, whichotherwise have to rely on massive computational resources only available to a small fraction of thefield and would leave a huge carbon footprint. For instance, to generate the CIFAR10 syntheticimages using an NVIDIA V100 32GB, a recent work called DP-API by Lin et al. (2023) requires 500GPU hours and DP-Diffusion requires 1250 GPU hours (See in (Lin et al., 2023)). In ourcase, when using an NVIDIA RTX A4000 16GB GPU (slower than V100 32GB), fine-tuning took 15GPU hours, and pre-training took 192 GPU hours. Pre-training is not always necessary; we used asingle pre-trained LDM for CelebA32, CIFAR10 and Camelyon17. For-the-first-time DP Text-to-Image Generation: We push the boundary of what DP-SGD fine-tuned generative models can achieve, by being the first to produce high-dimensional images (256x256)at a reasonable privacy level. We showcase this in text-conditioned and class-conditioned imagegeneration, where we input a certain text prompt (or a class label) and generate a corresponding imagefrom a DP-fine-tuned LDM for CelebAHQ. These conditional, high-dimensional image generationtasks present more complex and more realistic benchmarks compared to the conventional CIFAR10and MNIST datasets. These latter datasets, though widely used in DP image generation literaturefor years, are now rather simplistic and outdated. Our work contributes to narrowing down the largegap between the current state of synthetic image generation in non-DP settings and that in DPsettings. Application of DP-LoRA to LDMs: We apply the low-rank approximation (Hu et al., 2021) tothe attention modules in DMs to further decrease the number of parameters to fine-tune. Interestingly,the performance from LoRA was slightly worse than that of fine-tuning entire attention modules.",
  "Latent Diffusion Models (LDMs)": "Diffusion Models gradually denoise a normally distributed variable through learning the reverse direction of aMarkov Chain of length T. Latent diffusion models (LDMs) by Rombach et al. (2022) are a modification ofdenoising diffusion probabilistic models (DDPMs) by Ho et al. (2020) in the following way. First, Rombachet al. (2022) uses a powerful auto-encoder, consisting of an encoder denoted by Enc and a decoder denotedby Dec . The encoder transforms a high-dimensional pixel representation x into a lower-dimensional latentrepresentation z via z = Enc(x); and the decoder transforms the lower-dimensional latent representationback to the original space via x = Dec(z). Rombach et al. (2022) use a combination of a perceptual loss anda patch-based adversarial objective, with extra regularization for better-controlled variance in the learnedlatent space, to obtain powerful autoencoders (See section 3 in (Rombach et al., 2022) for details). Thistraining loss helps the latent representations to carry equivalent information (e.g., the spatial structure ofpixels) as the pixel representations, although the dimensionality of the former is greatly reduced. Second, equipped with the powerful auto-encoder, Rombach et al. (2022) trains a diffusion model (typically aUNet (Ronneberger et al., 2015)) in the latent space. Training a DM in this space can significantly expeditethe training process of diffusion models, e.g., from hundreds of GPU days to several GPU hours for similaraccuracy. Third, LDMs also contain attention modules (Vaswani et al., 2017) that take inputs from a conditioningembedder, inserted into the layers of the underlying UNet backbone as the way illustrated in toachieve flexible conditional image generation (e.g., generating images conditioning on text, image layout, classlabels, etc.). The modified UNet is then used as a function approximator to predict an initial noise fromthe noisy lower-dimensional latent representations at several finite time steps t, where in LDMs, the noisyrepresentations (rather than data) follow the diffusion process defined in Ho et al. (2020). The parameters of the approximator are denoted by = [U, Attn, Cn], where U are the parameters of theunderlying UNet backbone excluding the parameter of attention modules Attn, and Cn are the parametersof the conditioning embedder (We will explain these further in Sec. 3). These parameters are then optimizedby minimizing the prediction error defined by",
  "Lldm() = E(zt,y),,t (zt, t, y)22,(1)": "where N(0, I), t uniformly sampled from {1, , T}, xt is the noisy version of the input x at step t,zt = Enc(xt) and y is what the model is conditioning on to generate data, e.g., class labels, or a prompt. Thefunction approximator (zt, t, y) takes the condition y, time step t, and latent input zt and then maps to aninitial noise estimate. Once the approximator is trained, the drawn samples in latent space, z, are transformedback to pixel space through the decoder, i.e., x = Dec(z). Our work introduced in Sec. 3 pre-trains bothauto-encoder and using public data, then fine-tunes only Attn, Cn, the parameters of the attentionmodules and the conditioning embedded, for private data, using DP-SGD to be explained next.",
  "a single entry. A single entry difference could come from either replacing or including/excluding one entryto/from the dataset D": "One of the most well-known and widely used DP mechanisms is the Gaussian mechanism. The Gaussianmechanism adds a calibrated level of noise to a function : D Rp to ensure that the output of themechanism is (, )-DP: (D) = (D) + n, where n N(0, 22Ip). Here, is often called a privacyparameter, which is a function of and . is often called the global sensitivity (Dwork et al., 2006; 2014),which is the maximum difference in L2-norm given two neighbouring D and D, ||(D) (D)||2. Becausewe are adding noise, the natural consequence is that the released function (D) is less accurate than thenon-DP counterpart, (D). This introduces privacy-accuracy trade-offs. DP-SGD (Abadi et al., 2016) is an instantiation of the Gaussian mechanism in stochastic gradient descent(SGD) by adding an appropriate amount of Gaussian noise to the gradients in every training step, to ensurethe parameters of a neural network are differentially private. When using DP-SGD, due to the composabilityproperty of DP Dwork et al. (2006; 2014), privacy loss is accumulating over a typically long course of training.Abadi et al. (2016) exploit the subsampled Gaussian mechanism (i.e., applying the Gaussian mechanism onrandomly subsampled data) to achieve a tight privacy bound. The Opacus package (Yousefpour et al., 2021)implements the privacy analysis in DP-SGD, which we adopt in our method. One thing to note is that weuse the inclusion/exclusion definition of DP in the experiments as in Opacus.",
  "In our method, which we call differentially private latent diffusion models (DP-LDMs), we carry out twotraining steps: non-private and private steps": "Non-Private Step: Pre-training an autoencoder and a DM using public data.Following Rombachet al. (2022), we first pre-train an auto-encoder. The encoder scales down an image x RHW 3 to a3-dimensional latent representation z Rhwc by a factor of f, where f = H/h = W/w. This 3-dimensionallatent representation is chosen to take advantage of image-specific inductive biases that the UNet contains,e.g., 2D convolutional layers. Following Rombach et al. (2022), we also train the autoencoder by minimizing a",
  "Since we use public data, there is no privacy loss incurred in estimating the parameters, which are a functionof public data Dpub": "Private Step: Fine-tuning attention modules & conditioning embedder for private data.Givena pre-trained diffusion model, we fine-tune the attention modules and a conditioning embedder using ourprivate data. For the models with the conditioned generation, the attention modules refer to the spatial transformer blocksshown in (a) which contains cross-attentions and multiple heads. For the models with an unconditionalgeneration, the attention modules refer to the attention blocks shown in (b). Consequently, theparameters of the attention modules, denoted by Attn, differ, depending on the conditioned or unconditionedcases. The conditioning embedder only exists in the conditioned case. Depending on the different modalitiesthe model is trained on, the conditioning embedder takes a different form. For instance, if the model generatesimages conditioning on the class labels, the conditioning embedder is simply a class embedder, which embedsclass labels to a latent dimension. If the model conditions on language prompts, the embedder can be atransformer.",
  "Q = i(zt)W (i)Q, K = (y)W (i)K, V = (y)W (i)V,(4)": "where the parameters are denoted by W (i)Q Rdkdi; W (i)K Rdkdc; and W (i)V Rdkdc. Unlike theconditioned case, where the key (K) and value (V ) vectors are computed as a projection of the conditioningembedder, the key and value vectors are a projection of the pixel embedding i(zt) only in the case of theunconditioned model. We run DP-SGD to fine-tune these parameters to obtain differentially private AttnDprivand CnDpriv, starting from AttnDpub, CnDpub. Our algorithm is given in Algorithm 1. Why fine-tune attention modules only?The output of the attention in eq. 3 assigns a high focusto the features that are more important, by zooming into what truly matters in an image depending on aparticular context, e.g., relevant to what the image is conditioned on. This can be quite different when wemove from one distribution to the other. By fine-tuning the attention modules (together with the conditioningembedder when conditioned case), we effectively transfer what we learned from the public data distributionto the private data distribution. However, if we fine-tune other parts of the model, e.g., the ResBlocks, thefine-tuning of these blocks can make a large change in the features themselves, degrading the performance. See",
  "end forReturn: (, )-differentially private P = {AttnP, CnP }": "Sec. 5. The idea of fine-tuning attention blocks is explored elsewhere. In fine-tuning large language models,existing work introduces a few new parameters to transformer attention blocks, and those new parametersare fine-tuned (Yu et al., 2022; Hu et al., 2021) to adapt to new distributions. In the context of pre-traineddiffusion models, adding, modifying, and controlling attention layers are gaining popularity for tasks such asimage editing and text-to-image generation (Hertz et al., 2022; Park et al., 2023; Zhang et al., 2023a; You &Zhao, 2023). Which public dataset to use given a private dataset?This is an open question in transfer learningliterature. Generally, if the two datasets are close to each other in some sense, they are assumed to be abetter pair. However, similarity in what sense has to be chosen differently depending on a particular datadomain and appropriately privatized if private data is used in this step. For instance, if the data is in 2Dspace, similarity in L2-distance sense might make sense. Our data lies in a high dimensional pixel space,where judging similarity in the FID sense is widely used (when comparing two image distributions usingeach distributions image samples). Hence, we use FID as a proxy to judge the similarity between two imagedatasets (See Sec. 5.1 how we privatize this step). In other datasets, out of the image domain, there could bea more appropriate metric to use than FID, e.g., in the case of discrete data, kernel-based distance metricswith an appropriately chosen kernel could be more useful.",
  "Related Work": "Initial efforts on generating high-dimensional image data with differential privacy have focused on leveragingadvanced generative models to achieve better differentially private synthetic data (Hu et al., 2023). Some ofthem (Xie et al., 2018; Torkzadehmahani et al., 2019; Frigerio et al., 2019; Yoon et al., 2019; Chen et al.,2020) utilize generative adversarial networks (GANS) (Goodfellow et al., 2014), or trained GANs with thePATE structure (Papernot et al., 2017). Other works have employed variational autoencoders (VAEs) (Acset al., 2018; Jiang et al., 2022; Pfitzner & Arnrich, 2022), or proposed customized structures (Harder et al.,2021; Vinaroz et al., 2022; Cao et al., 2021; Liew et al., 2022a; Harder et al., 2023). For instance, Harderet al. (2023) pretrained perceptual features using public data and privatized only data-dependent terms usingmaximum mean discrepancy. Limited works have so far delved into privatizing diffusion models. Dockhorn et al. (2023) develop a DPscore-based generative models (Song et al., 2021) using DP-SGD, applied to relatively simple datasets suchas MNIST, FashionMNIST and CelebA (downsampled to 3232). Ghalebikesabi et al. (2023) fine-tune theImageNet pre-trained diffusion model (DDPM) (Ho et al., 2020) with more than 80 M parameters usingDP-SGD for CIFAR-10. We instead adopt a different model (LDM) and fine-tune only the small part of theDM in our model to achieve better privacy-accuracy trade-offs. As concurrent work to ours, Lin et al. (2023)propose a DP-histogram mechanism to generate synthetic data through the utilization of publicly accessibleAPIs.Another concurrent work to ours, DP-Promise (Wang et al., 2024) leverages the diffusion models noiseduring the forward process to improve the privacy-accuracy trade-offs in diffusion model training.",
  "Experiments": "We demonstrate the performance of our method in comparison with the state-of-the-art methods in DP datageneration, using several combinations of public/private data of different levels of complexity at varyingprivacy levels. Implementation. We implemented DP-LDMs in PyTorch Lightning (Paszke et al., 2019) building on theLDM codebase by Rombach et al. (2022) and Opacus (Yousefpour et al., 2021) for DP-SGD training. Severalrecent papers present the importance of using large batches in DP-SGD training to improve accuracy at afixed privacy level (Ponomareva et al., 2023; De et al., 2022; Bu et al., 2022). To incorporate this finding inour work, we wrote custom batch splitting code that integrates with Opacus and Pytorch Lightning, allowingus to test arbitrary batch sizes. Our DP-LDM also improves significantly with large batches as will be shownsoon, consistent with the findings in recent work. For our experiments incorporating LoRA, we use the loralib(Hu et al., 2021) Python library. Datasets1 and Evaluation. We list all the private and public dataset pairs with corresponding evaluationmetrics in . Regarding high-quality generation, we use CelebAHQ for class conditional generationand Multi-Modal-CelebAHQ (MM-CelebAHQ) for text-to-image generation. Our choice of evaluation metricis either based on standard practice or following previous work to do a fair comparison. We measure themodel performance by computing the Frchet Inception Distance (FID) (Heusel et al., 2017) between thegenerated samples and the real data. For downstream task, we consider CNN (LeCun et al., 2015), ResNet-9(He et al., 2016), and WRN40-4 (Zagoruyko & Komodakis, 2017) to evaluate the classification performance ofsynthetic data. Each number in our tables represents an average value across three independent runs, witha standard deviation (unless stated otherwise). Note that some standard deviation values are reported as0.0 due to rounding. Values for comparison methods are taken from their papers, with an exception for theDP-MEPF comparison to CelebAHQ, which we ran their code by loading this data.",
  "DP-Promise25.326.229.1": ": FID scores (lower is better) for synthetic CIFAR-10, CelebA32, and CelebA64 data, in comparisonwith DP-Diffusion (Ghalebikesabi et al., 2023), DP-MEPF (Harder et al., 2023), DPDM (Dockhorn et al.,2023), DP-GAN (Xie et al., 2018), DP Sinkhorn (Cao et al., 2021), and DP-Promise (Wang et al., 2024) Imagenet to CIFAR-10 and CelebA distributions and from EMNIST to MNIST distribution. Additionally, totest our methods effectiveness at transferring knowledge across a large domain gap, we present the results oftransferring from Imagenet to Camelyon17-WILDS. Further details on these experiments are available inappendix A. FID. Comparison to other SOTA methods in terms of FID (the lower the better) is illustrated in .When tested on CIFAR-10, our DP-LDM outperforms other methods at all epsilon levels ( = 1, 5, 10 and = 105) except for DP-API. Our FID values correspond to the case where only 9-16 attention modules arefine-tuned (i.e., fine-tuning only 10% of trainable parameters in the model) and the rest remain fixed. See for ablation experiments for fine-tuning different attention modules. When tested on CelebA32,DP-Promise achieved the best FID at all levels by a large margin compared to other existing methods. When",
  "WRN-40-4DP-API80.5": ": Classification accuracies (higher is better) evaluated on real test data, when the classifiers aretrained with synthetic CIFAR-10, CelebA64, and MNIST datasets. Comparison methods include DP-MEPF(Harder et al., 2023), DP-MERF (Harder et al., 2021), DP-Diffusion (Ghalebikesabi et al., 2023), and DPDM(Dockhorn et al., 2023). For Camelyon17 dataset, following Ghalebikesabi et al. (2023) and Lin et al. (2023),we set = 3 106. Each classifiers architecture is written below each name of data. We choose to use theseclassifiers by following previous works (Harder et al., 2023; Ghalebikesabi et al., 2023; Dockhorn et al., 2023). Downstream Classification. FID can be viewed as a fidelity metric, serving as a proxy for the utility ofthe synthetic data. To directly present the utility results of the model, we also consider accuracy on theclassification task, which is listed in . All the classifiers are trained with 50K synthetic samples andthen evaluated on real data samples. For each dataset, we follow previous work to choose classifier models fora fair comparison. When tested on CIFAR-10, DP-LDM again outperforms others except DP-API. Compared to DP-LDM, theperformance of DP-API seems more susceptible to the amount of domain shift from public to private datadistributions. When there is a small domain gap, e.g., from Imagenet (public data) to CIFAR10 (privatedata), DP-API performs better than DP-LDM. However, when there is a large domain shift between publicdata (Imagenet) to private data (Camelyon17), the accuracies of downstream classifier (WRN-40-4) trainedwith each synthetic dataset are 85.7 for DP-LDMs and 80.5 for DP-API, respectively, at = 10. Hence,DP-LDMs seem better suited than DP-API when the domain shift is large as fine-tuning a small part ofdiffusion models in DP-LDMs helps incorporate such a shift effectively. For testing on CelebA64, we began with an LDM pre-trained on conditional ImageNet at the same resolution,and then fine-tuned it on CelebA where the (binary) class labels were given by the Male attribute. Ourmethod achieves a new SOTA performance at all epsilon levels. When tested on MNIST, we surpass the previous methods at = 1 and achieve comparable results at = 10.One thing to note is that DPDM takes 1.75M parameters and 192 GPU hours to achieve 98.1 accuracy,and DP-Diffusion takes 4.2M parameters (GPU hours not showing), while our methods takes only 0.8Mparameters and 10 GPU hours to achieve 97.4 accuracy, which significantly reduces the parameters and savesmuch computing resources. When tested on Camelyon17, our method seems to underperform DP-Diffusion (Ghalebikesabi et al., 2023).However, this could be due to their use of a pre-trained WRN-40-4 classifier (pre-trained with ImageNet32).Ghalebikesabi et al. (2023) has not published their code and the pre-trained classifier (using ImageNet32)that they started fine-tuning with, so we could not directly compare the results between ours and theirs.",
  ": Text-to-image generation of 256 256 CelebAHQ with prompts at = 10. FID: 15.6": "Why do we select EMNIST as a public dataset? Previous work (Harder et al., 2023) used SVHN as apublic dataset to MNIST since they are both number images. However, SVHN and MNIST differ significantly(SVHN contains several digits per image with 3 channels while MNIST contains one digit per image with 1channel). So we considered other, more similar datasets such as EMNIST and KMNIST as public datasetcandidates. We used FID scores to judge the closeness between public and private data, using privatizedFID statistics by following the mechanisms used in (Park et al., 2017). See and Appendix Sec. A.2which verifies our choice of EMNIST.",
  "With the latent representations of the inputs, LDMs can better improve DP training. To the best of ourknowledge, we are the first to achieve high-dimensional differentially private generation": "Text-to-image generation. For text-to-image generation, we fine-tune the LDM models pretrained withLAION-400M (Schuhmann et al., 2021) for MM-CelebAHQ (256 256). Each image is described by a caption,which is fed to the conditioning embedder, BERT (Devlin et al., 2018). We freeze the BERT embedder aswell during fine-tuning attention modules to reduce the trainable parameters, then we bring back BERT forsampling. DP-LDM achieves FID scores of 15.6 for = 10. As illustrated in , the samples are faithfulto our input prompts, but not identical to the training sample, unlike the memorization behavior of thenon-private Stable Diffusion (Carlini et al., 2023). Class conditional generation. We build our model on the LDM model provided by Rombach et al. (2022)which is pretrained on Imagenet at a resolution of 256 256. Following our experiments in Sec. 5.1, wefine-tune all of the SpatialTransformer blocks. While CelebAHQ does not provide class labels, each image isassociated with 40 binary attributes. We choose the attribute Male to act as a binary class label for eachimage. Generated samples are available in along with FID values. Compared to DP-MEPF, basedon the FID scores and perceptual image quality, DP-LDM is better suited for generating detailed, plausiblesamples from the high-resolution dataset at a wide range of privacy levels.",
  "= 1": ": Synthetic 256 256 CelebA samples generated at varying . Samples for DP-MEPF are generatedfrom code available in Harder et al. (2023). We computed FID between our generated samples and the realdata and achieve FIDs of 19.0 0.0 at = 10, 20.5 0.1 at = 5, and 25.6 0.1 at = 1. DP-MEPF achievesan FID of 41.8 at = 10 and 101.5 at = 1.",
  "DP-LoRA (rank=8)66.4182.6DP-LoRA (rank=4)26.77": ": FID scores and Classification Accuracy of DP-LoRA trained for Camelyon17 and CIFAR10, at = 10. The best DP-LoRA models (the best ranks shown in parentheses) still lag the best DP-LDMs (thelayer ablations shown in parentheses). MNIST. See also Appendix Sec. A.3 and Appendix Sec. A.2.3 for details. The best results are achieved whenfine-tuning the attention modules in the out_blocks in the UNet (out_blocks shown in ), consistentlythroughout all datasets we tested. If a limited privacy budget is given, we suggest fine-tuning the attentionmodules in the out_blocks only to achieve better accuracies. Fine-tuning a different part of the Unet. Previous results focused on fine-tuning attention modulesat varying layers. Here, we present the performance of fine-tuning a different part of the Unet while therest of the model is frozen. In (Bottom), we show the FID scores evaluated on synthetic CIFAR10images. The main takeaway messages are (a) fine-tuning Resblocks hurts the performance, possibly becausethe learned features during the pre-training stage are altered, and (b) fine-tuning out_blocks is more usefulthan input_blocks, while the best strategy is fine-tuning the attention modules in the out_blocks.",
  "DP-LoRA: Applying LoRA to LDM and fine-tuning with attention modules using DP-SGD": "Previous work (Yu et al., 2022) has explored LoRA during training to reduce the fine-tuning parameters.We performed additional experiments by applying LoRA to the QKV matrices in all the attention modulesfor CelebA64, CIFAR10, and Camelyon17 datasets.In LoRA, each QKV matrix is reparameterized asWnew = Wpretrained + rank BA, where Wnew is the desired result of finetuning, Wpretrained is the pretrainedweight matrix, A and B are the low rank matrices, and is a scaling hyperparameter. Following Hu et al.(2022), we set to be equal to the first rank we try for each experiment, which in all of our runs is 1. InDP-LoRA, A and B are updated during fine-tuning with DP-SGD and Wpretrained is frozen.As shown in and , DP-LoRA was not particularly useful under the LDMs and our current DP-LDM stilloutperformed. A possible explanation could be the phenomenon previously observed in fine-tuning LLMs (Liet al., 2022): They reasoned that the large batch size improves the signal-to-noise ratio (shown in Fig 3 ofLi et al. (2022)), which significantly helps improve the performance of the model with full updates, comparedto low-rank updates. For more curious readers, we show the empirical distances between Wnew and Wpretrained for the LoRAmodels fine-tuned for CelebA64. We compute the 2 distance between the prerained and finetuned qkv.weightmatrices for DP-LDM (vanilla), and the F-norm of rank BA for DP-LoRA, in . Two remarks:First, given that the domain shift between ImageNet and CelebA is relatively large, the adaptation matricesin LoRA, in some cases (e.g., at = 10), completely overwhelm Wpretrained. We suspect that the larger F.norms in the case of DP-LoRA (relative to DP-LDM) are due to its updates being restricted to be low-rank.",
  "(b) Singular values of Wq, Wk, Wv in each model": ": The Q,K,V matrices in cross-attention modules in two fine-tuned models for CIFAR10 at = 10.Fine-tuned(0-15) indicates attention modules of all layers are fine-tuned, while Fine-tuned(9-15) indicatesonly the attention modules at layers 9-15 are fine-tuned. (a) Each row corresponds to Wq(top), Wk(middle),and Wv(bottom). (b) Pink represents Wq, blue Wk, and green Wv, respectively. Solid lines representthe model(9-15) and dotted lines represent the model(0-15). While DP-LDM may find a configuration of parameter values that achieve low loss (and good performance)relatively closer to the initial parameters, DP-LoRA may need to search further in parameter space for a setof parameter values that achieves similar performance. Second, the fact that DP-LoRA gets the best FIDscore at rank 64 at = 10 (the smallest amount of noise we tried out) while it does at rank 4 at = 1 andrank 8 at = 5, the optimal adaptation matrix in the UNet might not be necessarily rank-deficient as in thetransformer case. Next, we visualize the fine-tuned attention modules in an attempt to gain insights into ourfine-tuned models.",
  "Visualization of fine-tuned attention modules": "We first show the histogram of Wq, Wk, Wv matrices in cross-attention modules before and after fine-tuningfor CIFAR10 in (a) of . Because we fine-tune only the attention modules while the intermediaterepresentations are fixed, the distributions over Wq, Wk, Wv matrices after fine-tuning (first and secondcolumns) are significantly different from those before fine-tuning (third column). When fine-tuning a smallernumber of layers (9-15), larger changes have to be made per layer (indicated by the large spread in thehistogram for Wq, Wk, Wv matrices) to compensate for the distributional shift from Dpub to Dpriv, comparedto fine-tuning all layers (0-15).",
  "(b) Singular values of Wq, Wk, Wv at layer 13": ": Singular values of Wq, Wk, Wv in cross-attention modules in the conditional LDMs fine-tunedfor CIFAR10 (CIF10) and that for Camelyon17 (CAM17), at = 10 with = 105 for CIFAR10 and = 3 106 for Camelyon17. Dotted lines represent CAM17, while solid lines represent CIF10. Purple andgreen solid lines are overlapping in both plots.",
  "(b) Layer 15": ": Singular values of Wq, Wk, Wv in self-attention modules at varying layers of an unconditionalLDM, fine-tuned for CelebA64, at = 10 and = 1. Dotted lines represent the model fine-tuned at = 10,and solid lines that at = 1. the role of Wk and Wv is to pick which class embeddings are more useful (both are multiplied by the classembedding as in eq. 4), their fine-tuned values remain similar. The singular values fall off fast in the case ofWq, while those of Wk and Wv fall off slowly and even the smallest singular values are far from zero.Similarly, in , the singular values of fine-tuned model for Camelyon17 seem to decay faster than thosefor CIFAR10. However, the smallest singular values at the later layer are again far from zero. This mightimply low rank approximation to these matrices can cause losing information. This observation is consistentwith the performance of DP-LoRA lagging that of DP-LDM shown in and . shows the singular values of Wq, Wk, Wv in the unconditional LDM fine-tuned for CelebA64.There is only a slight difference between the singular values of the model fine-tuned at = 1 and the modelfine-tuned at = 10. This could be explained by using a fixed random seed during training. As the randomseed determines the order of training batches as well as the direction of the random noise injected by DP-SGD,each model converges to similar areas of the parameter space. Thus, differences in the fine-tuned weightmatrices arise from the different magnitudes of noise added. On the other hand, this is no longer the case,",
  "Conclusion": "In Differentially Private Latent Diffusion Models (DP-LDM), we utilize DP-SGD to fine-tune only theattention modules (and embedders for conditioned generation) of the pretrained LDM at varying layerswith privacy-sensitive data. We demonstrate that our method is capable of generating quality images invarious scenarios. We perform an in-depth analysis of the ablation of DP-LDM to explore the strategy forreducing parameters for more applicable training of DP-SGD. Based on our promising results, we concludethat fine-tuning LDMs is an efficient and effective framework for DP generative learning. We hope our resultscan contribute to future research in DP data generation, considering the rapid advances in diffusion-basedgenerative modelling.",
  "Broader Impact Statement": "As investigated in Carlini et al. (2023), diffusion models can memorize individual images from the trainingdata set and generate an identical synthetic image to the training image. Aiming to impact society positively,we provide a method to fine-tune latent diffusion models with differential privacy guarantees so that thefine-tuned models do not generate identical images to those in the private data. Our method relies on public data for better scalability of differential privacy, which needs some attention.As Tramr et al. (2022) pointed out, public data themselves may still contain sensitive information. Fromour perspective and as many other DP generative modelling papers also noticed, auxiliary public data stillemerges as the most promising option for attaining satisfactory utility, for the models at this large scale. Wehope for better-curated public datasets to be available in the near future. During the review process, one of the reviewers mentioned that generative tools are becoming more and morerealistic and of general availability, with the risk of generating harmful content, especially when generatinghuman-related content. Broadly speaking, our method does not stop malicious users from creating harmfulcontent, as the diffusion model will generate images based on any input prompt given by the malicioususer. However, what our method can do is protect the privacy of the individuals whose data was in theprivate dataset, meaning the faces in the private dataset will not appear when generating images from theDP fine-tuned models.",
  "Limitations": "While the paper proposes a technique to counter-attack the privacy risks of diffusion models, the evaluationhas not been done against existing privacy attacks. So we know that the diffusion model is private under thedefinition of DP\", but we caution practitioners against making an assumption that a DP image generativemodel protects private data against all possible attack methods. This may not be a direct limitation of our work itself, but it is restrictive that there is no agreement within thecommunity on which definition of privacy should be used to assess the privacy of generated images. Carliniet al. (2023) provided a single example of exploitation, but it remains unclear what aspects we aim to protectin generated data. Unfortunately, the privacy parameters and in our method are especially difficult tointerpret in the image domain. While private images may be protected, it is unclear whether perceptuallysimilar images enjoy the same level of privacy. Further work is required to suggest more appropriate definitionsof privacy for practitioners to employ to avoid providing a vague sense of privacy in the image generationdomain.",
  "Zhiqi Bu, Jialin Mao, and Shiyun Xu. Scalable and efficient training of large convolutional neural networkswith differential privacy, 2022": "Tianshi Cao, Alex Bie, Arash Vahdat, Sanja Fidler, and Karsten Kreis. Dont generate me: Trainingdifferentially private generative models with sinkhorn divergence. In Neural Information Processing Systems(NeurIPS), 2021. Nicholas Carlini, Jamie Hayes, Milad Nasr, Matthew Jagielski, Vikash Sehwag, Florian Tramr, Borja Balle,Daphne Ippolito, and Eric Wallace. Extracting training data from diffusion models. In Proceedings ofthe 32nd USENIX Conference on Security Symposium, SEC 23, USA, 2023. USENIX Association. ISBN978-1-939133-37-3. Dingfan Chen, Tribhuvanesh Orekondy, and Mario Fritz. Gs-wgan: A gradient-sanitized approach for learningdifferentially private generators. In Advances in Neural Information Processing Systems 33, 2020. Gregory Cohen, Saeed Afshar, Jonathan Tapson, and Andre Van Schaik. Emnist: Extending mnist tohandwritten letters. In 2017 international joint conference on neural networks (IJCNN), pp. 29212926.IEEE, 2017.",
  "Cynthia Dwork, Aaron Roth, et al. The algorithmic foundations of differential privacy. Foundations andTrends in Theoretical Computer Science, 9(34):211407, 2014": "Lorenzo Frigerio, Anderson Santana de Oliveira, Laurent Gomez, and Patrick Duverger. Differentially privategenerative adversarial networks for time series, continuous, and discrete open data. In ICT Systems Securityand Privacy Protection - 34th IFIP TC 11 International Conference, SEC 2019, Lisbon, Portugal, June25-27, 2019, Proceedings, pp. 151164, 2019. doi: 10.1007/978-3-030-22312-0\\_11. Sahra Ghalebikesabi, Leonard Berrada, Sven Gowal, Ira Ktena, Robert Stanforth, Jamie Hayes, Soham De,Samuel L. Smith, Olivia Wiles, and Borja Balle. Differentially private diffusion models generate usefulsynthetic images, 2023. I. J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio.Generative adversarial networks. In Advances in Neural Information Processing Systems, 2014. Frederik Harder, Kamil Adamczewski, and Mijung Park. DP-MERF: Differentially private mean embeddingswith random features for practical privacy-preserving data generation.In AISTATS, volume 130 ofProceedings of Machine Learning Research, pp. 18191827. PMLR, 2021. Frederik Harder, Milad Jalali, Danica J. Sutherland, and Mijung Park. Pre-trained perceptual featuresimprove differentially private image generation. Transactions on Machine Learning Research, 2023. ISSN2835-8856. URL",
  "Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt image editing with cross attention control, 2022": "Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trainedby a two time-scale update rule converge to a local nash equilibrium. Advances in neural informationprocessing systems, 30, 2017. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In H. Larochelle,M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neural Information ProcessingSystems, volume 33, pp. 68406851. Curran Associates, Inc., 2020. URL",
  "Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. InProceedings of International Conference on Computer Vision (ICCV), December 2015": "Tomoya Matsumoto, Takayuki Miura, and Naoto Yanai. Membership inference attacks against diffusion models.In 2023 IEEE Security and Privacy Workshops (SPW), pp. 7783, 2023. doi: 10.1109/SPW59333.2023.00013. Nicolas Papernot, Martn Abadi, lfar Erlingsson, Ian J. Goodfellow, and Kunal Talwar. Semi-supervisedknowledge transfer for deep learning from private training data. In Proceedings of the InternationalConference on Learning Representations (ICLR), 2017.",
  "Bjarne Pfitzner and Bert Arnrich. Dpd-fvae: Synthetic data generation using federated variational autoen-coders with differentially-private decoder. arXiv preprint arXiv:2211.11591, 2022": "Natalia Ponomareva, Sergei Vassilvitskii, Zheng Xu, Brendan McMahan, Alexey Kurakin, and Chiyaun Zhang.How to dp-fy ml: A practical tutorial to machine learning with differential privacy. In Proceedings of the 29thACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD 23, pp. 58235824, New York,NY, USA, 2023. Association for Computing Machinery. ISBN 9798400701030. doi: 10.1145/3580305.3599561.URL",
  "Baifeng Shi, Siyu Gai, Trevor Darrell, and Xin Wang. Toast: Transfer learning via attention steering, 2023": "Gowthami Somepalli, Vasu Singla, Micah Goldblum, Jonas Geiping, and Tom Goldstein. Diffusion artor digital forgery? investigating data replication in diffusion models. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition (CVPR), pp. 60486058, June 2023. Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole.Score-based generative modeling through stochastic differential equations. International Conference onLearning Representations, 2021.",
  "Florian Tramr, Gautam Kamath, and Nicholas Carlini. Considerations for differentially private learningwith large-scale public pretraining. arXiv preprint arXiv:2212.06470, 2022": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ukasz Kaiser,and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30,2017. Margarita Vinaroz, Mohammad-Amin Charusaie, Frederik Harder, Kamil Adamczewski, and Mi Jung Park.Hermite polynomial features for private data generation. In ICML, volume 162 of Proceedings of MachineLearning Research, pp. 2230022324. PMLR, 2022. Haichen Wang, Shuchao Pang, Lu Zhigang, Yihang Rao, Yongbin Zhou, and Xue Minhui. dp-promise:Differentially private diffusion probabilistic models for image synthesis. In 33rd USENIX Security Symposium(USENIX Security 24), Philadelphia, PA, August 2024. USENIX Association. URL",
  "Fuming You and Zhou Zhao. Transferring pretrained diffusion probabilistic models, 2023. URL": "Ashkan Yousefpour, Igor Shilov, Alexandre Sablayrolles, Davide Testuggine, Karthik Prasad, Mani Malek,John Nguyen, Sayan Ghosh, Akash Bharadwaj, Jessica Zhao, Graham Cormode, and Ilya Mironov. Opacus:User-friendly differential privacy library in PyTorch. arXiv preprint arXiv:2109.12298, 2021. Da Yu, Saurabh Naik, Arturs Backurs, Sivakanth Gopi, Huseyin A Inan, Gautam Kamath, Janardhan Kulkarni,Yin Tat Lee, Andre Manoel, Lukas Wutschitz, Sergey Yekhanin, and Huishuai Zhang. Differentially privatefine-tuning of language models. In International Conference on Learning Representations, 2022. URL",
  "# params0.8M1.75M4.2M---GPU Hours10h192h----": ": Downstream accuracies by CNN, MLP and WRN-40-4, evaluated on the generated MNISTdata samples. We compare our results with existing work DPDM (Dockhorn et al., 2023), DP-Diffusion(Ghalebikesabi et al., 2023), PEARL (Liew et al., 2022b), DPGANr (Bie et al., 2022), and DP-HP (Vinarozet al., 2022). The GPU hours is for DP training only. The GPU hours for pretraining steps of our methodare present in and .",
  "A.2.1Choosing public dataset with DP constraint": "FID scores are commonly used for measuring the similarity of two dataset. It first uses a pre-trained neuralnetwork (such as InceptionV3) to extract features from both datasets; then fits two Gaussian distributions toboth datasets respectively, via computing the mean and covariance of the feature representations for both ofthem; then use the means and covariances to calculate the Frchet distance. Following Park et al. (2017), wecomputed the FID scores between public data and private data in a differentially private manner. I.e. considera data matrix by X, where n i.i.d. observations in a privacy-sensitive dataset are stacked in each row of X.We denote each observation of this dataset by xi Rd. Hence, X Rnd. We denote the inception featuregiven a datapoint xi by (xi). We further denote the mean and the covariance of the inception features,computed on a public dataset, by 0 and 0. Similarly, we denote those computed on a privacy-sensitivedataset by , . The non-DP computation of FID is given by the following formula (notations are only usedin this subsection):",
  ": Left : Perturbed FIDs when privatizing both mean and covariance. Note listed is the sum of + , we take = . Right : Perturbed mean differences when privatizing mean only. listed is": "i.e., bounded by 2/n, when using the replacement definition of differential privacy (it is 1/n when using theinclusion/exclusion definition of DP). See eq(17) in Park et al. (2017) for derivation of the sensitivity. Given a privacy budget (2, 2)-DP assigned to this privatization step, which gives us a corresponding privacyparameter 2, we first draw noise n2 from N(0, 2Msec22Id(d+1)/2) . We then add this noise to the uppertriangular part of the matrix, including the diagonal component. To ensure the symmetry of the perturbedsecond-moment matrix Msec, we copy the noise added to the upper triangular part and add one by one tothe lower triangular part. We then obtain",
  "which is (1 + 2, 1 + 2)-DP": "Following the above method, we compute the DP-FID scores of SVHN, KMNIST, and EMNIST, with respectto private dataset MNIST. We sampled 60k data from each public dataset candidates to do a fair comparison,the results are in left. One could consider only privatizing the mean only, and the results are in right. Based on these results, we choose EMNIST as a public dataset.",
  "A.3Transferring from Imagenet to CIFAR10 distribution": "Here, we provide the results for ablation experiments to test the performance of DP-LDM when fine-tuningonly certain attention modules inside the pre-trained model and keeping the rest of the parameters fixed.There are 16 attention modules in total as illustrated in . shows the FID obtained for = 1, 5, 10 and = 105 for the different number of attention modules fine-tuned. The results show thatfixing up to the first half of the attention layers in the LDM has a positive effect in terms of the FID (thelower the better) in our model.",
  "-16 layers25.8 0.329.9 0.233.0 0.35 - 16 layers15.7 0.321.2 0.228.9 0.29 - 16 layers8.4 0.213.4 0.422.9 0.513 - 16 layers12.3 0.218.5 0.225.2 0.5": ": FID scores (lower is better) for synthetic CIFAR-10 data with varying the number of fine-tuninglayers and privacy guarantees. Top row (1-16 layers): Fine-tuning all attention modules. Second row(5-16 layers): Keep first 4 attention modules fixed and fine-tuning from 5 to 16 attention modules. Thirdrow (9-16 layers): Keep first 8 attention modules fixed and fine-tuning from 9 to 16 attention modules.Bottom row (13-16 layers): Keep first 12 attention modules fixed and fine-tuning from 13 to 16 attentionmodules.",
  "A.4Transferring from Imagenet to CelebA32": "We also apply our model in the task of generating 3232 CelebA images. The same pretrained autoencoder asour CIFAR-10 experiments in .1 is used, but since this experiment is for unconditional generation, weare unable to re-use the LDM. A new LDM is pretrained on Imagenet without class conditioning information,and then fine-tuned on CelebA images scaled and cropped to 3232 resolution. Our FID results for = 106, = 1, 5, 10 are summarized in . We achieve similar results to DP-MEPF for = 5 and = 10. Aswith our results at 64 64 resolution, our LDM model does not perform as well in higher privacy settings( = 1). Sample images are provided in",
  "A.5Transferring from Imagenet to Camelyon17-WILDS": "Camelyon17-WILDS is part of the WILDS benchmark suite of datasets, containing 455, 954 medical imagesat 96 96 resolution. The downstream task is to determine whether the center 32 32 patch of the imagecontains any tumor pixels. In our experiment, we crop the image so that only the center 32 32 patchis passed to the model. We begin with the same pretrained autoencoder and LDM as in our CIFAR-10experiments (.1), and fine-tune on Camelyon17-WILDS. We then generate 25, 000 images conditioned",
  "BHyperparameters": "Here we provide an overview of the hyperparameters of the pretrained autoencoder in , hyperpa-rameters of the pretrained diffusion models in . Note that base learning rate is the one set inthe yaml files. The real learning rate passed to the optimizer is accumulate_grad_batches num_gpus batch size base learning rate.",
  "epochs200200clipping norm0.010.001noise scale1.479.78ablation4-1num of params0.8M1.6M": "use_spatial_transformerTrueTruecond_stage_keyclass_labelclass_labelconditioning_keycrossattncrossattnnum_classes2626embedding dimension1313transformer depth11train_condition_onlyTrueTrueattention_flagspatialspatial# condition params338338 : Hyperparameters for fine-tuning diffusion models with DP constraints = 10, 1 and = 105 onMNIST. The ablation hyperparameter determines which attention modules are fine-tuned, where a value ofi means that the first i 1 attention modules are frozen and others are trained. Setting ablation to 1(default) fine-tunes all attention modules."
}