{
  "Abstract": "We train copies of a neural network on different sets of SGD noise and find that linearlyinterpolating their weights can, remarkably, produce networks that perform significantlybetter than the original networks. However, such interpolated networks consistently endup in unfavorable regions of the optimization landscape: with further training, their per-formance fails to improve or degrades, effectively undoing the performance gained from theinterpolation.We identify two quantities that impact an interpolated networks perfor-mance and relate our observations to linear mode connectivity. Finally, we investigate thisphenomenon from the lens of example importance and find that performance improves anddegrades almost exclusively on the harder subsets of the training data, while performanceis stable on the easier subsets. Our work represents a step towards a better understandingof neural network loss landscapes and weight interpolation in deep learning.",
  "Introduction": "Linear interpolations of neural network weights are of considerable interest in modern deep learning, for boththeoretical and practical purposes (Singh & Jaggi, 2020; Li et al., 2023; Neyshabur et al., 2020). They haveaided our understanding of training dynamics and loss landscapes (Frankle et al., 2020; Sharma et al., 2024;Paul et al., 2022a), and can improve performance at convergence (Izmailov et al., 2018; Matena & Raffel,2022; Wortsman et al., 2022). Much of the prior work on this topic has focused on linear interpolations of network weights during late-stagetraining or at convergence (Wortsman et al., 2022), or interpolating SGD iterates of a network along a singletraining trajectory (Zhang et al., 2019). However, our understanding of the properties of such networks andtheir optimization remains limited in more general settings. The observations and investigations presentedhere attempt to address this gap and improve our understanding of how linear interpolations evolve in thecontext of SGD noise throughout the training process. We train copies of networks on different sets of SGD noise (i.e data order and augmentation) on standardvision tasks. More specifically, we consider a network initialization that is trained for a small number ofiterations k, after which its copies A and B are trained on different sets of SGD noise for s epochs.",
  "Epoch": "Split 4 : Performance of a ResNet20 network on 4 splits of CIFAR-10s training set differentiated by EL2Nscore, where Split 1 contains the least important examples, and Split 4 contains the most important ones.The rise and subsequent drop in performance due to interpolation is most noticeable in Split 3 and Split 4,which the network performs relatively poorly on. As noted in prior work (Paul et al., 2021), the network islikely to be unstable to SGD noise with respect to Splits 3 and 4 at such an early point in training, providingmore evidence for the connection between our observations and stability to SGD noise.",
  "Related Work": "Network weight interpolation has attracted significant interest in recent work. Izmailov et al. (2018) intro-duces Stochastic Weight Averaging, where averaging SGD iterates along the same trajectory lead to bettergeneralization. In a similar vein, Zhang et al. (2019) introduces the Lookahead optimizer, which interpo-lates a set of fast weights to update the actual weights of the network, leading to better training stability",
  "Published in Transactions on Machine Learning Research (09/2024)": "0.6 0.7 0.8 k = 1, s = 1 0.5 0.6 0.7 0.8 k = 1, s = 5 0.5 0.6 0.7 0.8 k = 1, s = 10 Network ANetwork BAveraged Network 0.725 0.750 0.775 0.800 0.825 k = 5, s = 1 0.725 0.750 0.775 0.800 0.825 k = 5, s = 5 0.725 0.750 0.775 0.800 0.825 k = 5, s = 10 0.70 0.75 0.80 k = 10, s = 1 0.76 0.78 0.80 0.82 k = 10, s = 5 0.76 0.78 0.80 0.82 k = 10, s = 10",
  "Linear Mode Connectivity and Instability Analysis": "We say that two neural networks are linearly connected if all networks along the linear path connecting thetwo trained networks in weight space have loss no larger than the end points (Frankle et al., 2020). Theauthors empirically demonstrate this through instability analysis, where two copies of a network are trainedon different samples of SGD noise to convergence. If the two converged networks are linearly connected,the original network is said to be stable to SGD noise. In our work, we cannot use instability directly as we will soon see, we often encounter interpolants thatperform better than the networks they were derived from, and therefore taking a maximum over performanceerrors will not suit our purposes. Instead, we consider a similar metric but pay attention to the maximumdifference (increase or decrease) from the average error of the two networks that we are interpolating between.Unlike (Frankle et al., 2020), we track the interpolants performance throughout training rather than onlyat convergence.",
  "Interpolation Performance Throughout Training": "Understanding the effect of linear interpolation early in training helps improve our understanding of linearmode connectivity since it is well-known that the early, noisy stage of training can heavily impact trainingdynamics and performance at later stages (Goyal et al., 2018; Jastrzebski et al., 2020; Gilmer et al., 2022;Paul et al., 2022b; Fort et al., 2020). Our analysis can also help tease apart the properties and structure ofneural network loss landscapes at different stages in the training process. We start with a network that is randomly initialized or trained for k steps. Then, two copies of this network,A and B, are trained on different sets of SGD noise for s epochs. Experimental details for the training setupcan be found in Appendix A. We will show the results of our experiment on CIFAR-10 (Krizhevsky, 2009)",
  "with ResNet20 networks (He et al., 2016) here (specifically in this context) additional results ondifferent datasets and network architectures can be found in Appendix E and onward": "0.2 0.4 0.6 0.8 k = 0 iters 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 k = 100 iters 0.5 0.6 0.7 0.8 k = 400 iters 0.60 0.65 0.70 0.75 0.80 0.85 0.90 k = 750 iters 0.750 0.775 0.800 0.825 0.850 0.875 0.900 0.925 k 3906 iters (10 epochs) = 0= 0.25= 0.5= 0.75= 1 s (in epochs)",
  "Test Accuracy": ": The setup remains the same as in , except child networks are trained on non-overlapping,equally sized random splits of the training data. To be specific, the initialized network is trained on theentire train set in the k phase, the child networks are trained on splits in the s phase, and the resultinginterpolant is trained on the entire dataset. Once again, our observations largely remain the same.",
  "Training Linear Interpolants": "From a practical perspective, our findings so far lead to a natural question: if linearly interpolating theweights of networks trained on different sets of SGD noise leads to better performance (given appropriatevalues of k and s), can we efficiently leverage this to speed up optimization? We address this by looking at the simplest case two copies of a network are made after k epochs, trainedon different sets of SGD noise, and then interpolated to produce a network with better performance. Theresulting interpolant is then trained further to convergence. From the previous section, one may observethat the degradation or improvement in performance is most significant for = 0.5 we will use this settingin our further experiments, though our observations generally hold for other reasonable values of (i.e. notvery close to 0 or 1). We train ResNet-20 (He et al., 2016) networks on CIFAR-10 (Krizhevsky, 2009) with different values of k ands and evaluate their test performance with a focus on how performance changes at and after averaging networkweights (see and ). In general, we find that while interpolating weights improves performance, itstagnates or degrades immediately over the next few epochs of further training. The subfigure correspondingto k = 1 and s = 1 is an exception to this rule but is not useful in pratice, since gains in performance veryearly in training are usually easy to achieve. Simply dropping the learning rate after interpolation removes this observed stagnation/drop in performance,but negatively impacts the performance at convergence, which may be undesirable (see ).As acompromise, we try warming up the learning rate after interpolation instead and find that this does notcause any noticeable difference in the originally observed behavior performance continues to degrade whenthe learning rate is dropped and gradually increased. These results can be observed in . These results suggest that networks obtained by weight interpolation consistently end up in regions of theloss landscape that are locally better, but are not naturally amenable to further training through SGD the degradation in the improved performance heavily implies that training is stalled as the interpolatednetwork escapes its present position in the loss landscape. Furthermore, this phenomenon does not seemto be specific to a particular model architecture or choice of hyperparameters (such as the learning rateschedule), hinting that what we observe is something more fundamental to the optimization process.",
  "Our results so far raise two key questions:": "Given sufficiently large values for k and s, why does linear interpolation of network weights trainedfrom the same initialization on different sets of SGD noise lead to networks that perform consistentlyand significantly better? Answering this can potentially lead to finding an efficient heuristic/methodto ascertain appropriate values for k and s. Why are interpolants derived in this manner not naturally amenable to further training throughSGD? What are the properties of interpolants and their immediate positions in weight space or theloss landscape that cause this, and if and how can this be prevented or mitigated?",
  "Example Importance": "We now consider the following questions: Are there specific examples in the training set that are impactedmore heavily than others by weight interpolation? If so, is it the same set of examples that contributesignificantly to the improvement and subsequent drop in performance? Frankle et al. (2020) note that networks become stable to SGD noise early but at different points in trainingfor different datasets during the training process. Furthermore, Paul et al. (2021) introduce metrics (see.1) to measure example importance for training and achieving good generalization. The authorsshow that data subsets differentiated by these metrics become stable to SGD noise at significantly differentpoints in training in general, the more important the example, the later in training stability is achievedwith respect to it. Motivated by this, we explore how performance changes at and after interpolation onsubsets of data of similar importance according to this metric.",
  "EL2N Score": "Introduced by Paul et al. (2021), the Error L2-Norm (EL2N) score of a training example is defined as theexpected norm of the difference between its label as predicted by the model and its ground truth label, wherethe expectation is computed at some early epoch t over multiple trained network initializations. The authorsdemonstrate that the higher-scoring examples are more difficult to learn but are also more important forgeneralization removing these difficult examples from the training set had the biggest effect on the finalgeneralization of the model.",
  "Interpolant Performance Improves and Degrades on Important Examples": "We maintain the experiment setup from the previous section, where child networks (ResNet20) are trainedon different sets of SGD noise, averaged, and then trained further. In , we track the performance ofthe child networks and the averaged network across 4 equal splits of CIFAR-10s training set. The data issplit according to the EL2N scores of the training examples computed at epoch 10 of training each splitconsists of 12500 examples, with Split 1 and Split 4 containing examples with the lowest and highest EL2Nscores respectively. As expected and shown by previous work, more important examples (as defined above) are indeed learnedlater in training. We also observe that it is primarily on Splits 3 and 4 that the networks performanceimproves and subsequently drops. While the former may seem obvious (these are the only splits where thereis space for performance to improve), the latter is not linearly interpolating the weights causes a drop inperformance almost exclusively on the harder examples in a dataset. Furthermore, the network maintainsstellar performance on the easier examples when trained further. Our observations emphasize the connection between interpolated network performance and stability to SGDnoise on subpopulations of data differentiated by example importance interpolants derived from networkstrained on separate SGD noise trajectories perform better on data subpopulations that are still unstable toSGD noise. On the other hand, once stability to SGD noise is reached on a specific subpopulation, there isno degradation in performance from weight interpolation.",
  "Conclusion and Discussion": "We have empirically demonstrated that when copies of a neural network are trained on different sets of SGDnoise, the network resulting from a linear interpolation of their weights can perform better than the trainedcopies. We frame this observation as an extension of linear mode connectivity, offering a more completepicture of how networks become stable to SGD noise. Interestingly, when such an interpolated network isfurther trained, performance fails to improve or degrades over the next few epochs. We also demonstratethat strategies such as dropping or warming up the learning rate do not remedy this observation linearlyinterpolated networks consistently end up in brittle areas of the loss landscape that make the networkperform better in the short term but impede further optimization. We analyze this phenomenon from thelens of example importance and empirically demonstrate that the observed improvement and degradation inperformance occur specifically in data subsets that are more important for generalization, providing furtherevidence for the connection between our observations and differences in stability to SGD noise on examplesdifferentiated by example importance. We conclude by noting some limitations and directions for future work. The biggest question raised by ourwork is: Why does a simple linear interpolation of network weights trained on different sets of SGD noiseconsistently show better performance, and how can we effectively leverage this improvement? While weconnect our observations to prior work, we do not comprehensively answer this question. The strategies weexplore to mitigate the drop in performance when training the interpolants are based on previously usedoptimization techniques and tricks; due to the empirical nature of our study, we cannot conclude whetheran alternative optimization method exists that could robustly optimize the interpolants, exploiting the dropin error to speed up training. It is also important to note that the current observations only apply to thestandard vision tasks, and it is an empirical question whether they would extend to other domains. Overall, our work provides insight into the behavior of network weight interpolations during the early stagesof training and at the same time, raises new questions to address. We believe that this will help us betterunderstand neural network loss landscapes and weight interpolation in the future.",
  "Luke N. Darlow, Elliot J. Crowley, Antreas Antoniou, and Amos J. Storkey. CINIC-10 is not ImageNet orCIFAR-10, 2018": "Felix Draxler, Kambis Veschgini, Manfred Salmhofer, and Fred Hamprecht.Essentially No Barriers inNeural Network Energy Landscape. In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35thInternational Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research,pp. 13091318. PMLR, 1015 Jul 2018. URL Rahim Entezari, Hanie Sedghi, Olga Saukh, and Behnam Neyshabur. The Role of Permutation Invariance inLinear Mode Connectivity of Neural Networks. In International Conference on Learning Representations,2022. URL Stanislav Fort, Gintare Karolina Dziugaite, Mansheej Paul, Sepideh Kharaghani, Daniel M Roy, and SuryaGanguli. Deep learning versus kernel learning: an empirical study of loss landscape geometry and the timeevolution of the neural tangent kernel. Advances in Neural Information Processing Systems, 33:58505861,2020.",
  "Jonathan Frankle and Michael Carbin.The lottery ticket hypothesis: Finding sparse, trainable neuralnetworks. In International Conference on Learning Representations, 2019. URL": "Jonathan Frankle, Gintare Karolina Dziugaite, Daniel Roy, and Michael Carbin. Linear Mode Connectivityand The Lottery Ticket Hypothesis. In International Conference on Machine Learning, pp. 32593269.PMLR, 2020. Timur Garipov, Pavel Izmailov, Dmitrii Podoprikhin, Dmitry P Vetrov, and Andrew G Wilson. Loss Surfaces,Mode Connectivity, and Fast Ensembling of DNNs. Advances in Neural Information Processing Systems,31, 2018. Justin Gilmer, Behrooz Ghorbani, Ankush Garg, Sneha Kudugunta, Behnam Neyshabur, David Cardoze,George Edward Dahl, Zachary Nado, and Orhan Firat. A Loss Curvature Perspective on Training Insta-bilities of Deep Learning Models. In International Conference on Learning Representations, 2022. URL Priya Goyal, Piotr Dollr, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, AndrewTulloch, Yangqing Jia, and Kaiming He. Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour,2018.",
  "Alex Krizhevsky. Learning Multiple Layers of Features From Tiny Images. PhD thesis, University of Toronto,2009": "Margaret Li, Suchin Gururangan, Tim Dettmers, Mike Lewis, Tim Althoff, Noah A Smith, and Luke Zettle-moyer. Branch-Train-Merge: Embarrassingly Parallel Training of Expert Language Models. arXiv preprintarXiv:2208.03306, 2022. Tao Li, Zhehao Huang, Qinghua Tao, Yingwen Wu, and Xiaolin Huang. Trainable Weight Averaging: Effi-cient Training by Optimizing Historical Solutions. In The Eleventh International Conference on LearningRepresentations, 2023. URL",
  "Michael S Matena and Colin A Raffel. Merging Models with Fisher-Weighted Averaging. Advances in NeuralInformation Processing Systems, 35:1770317716, 2022": "Brendan McMahan,Eider Moore,Daniel Ramage,Seth Hampson,and Blaise Aguera y Arcas.Communication-Efficient Learning of Deep Networks from Decentralized Data. In Aarti Singh and JerryZhu (eds.), Proceedings of the 20th International Conference on Artificial Intelligence and Statistics, vol-ume 54 of Proceedings of Machine Learning Research, pp. 12731282. PMLR, 2022 Apr 2017.URL Seyed Iman Mirzadeh, Mehrdad Farajtabar, Dilan Gorur, Razvan Pascanu, and Hassan Ghasemzadeh. Lin-ear Mode Connectivity in Multitask and Continual Learning. In International Conference on LearningRepresentations, 2021. URL",
  "Behnam Neyshabur, Hanie Sedghi, and Chiyuan Zhang. What Is Being Transferred In Transfer Learning?Advances in neural information processing systems, 33:512523, 2020": "Mansheej Paul, Surya Ganguli, and Gintare Karolina Dziugaite. Deep Learning on a Data Diet: FindingImportant Examples Early in Training.In A. Beygelzimer, Y. Dauphin, P. Liang, and J. WortmanVaughan (eds.), Advances in Neural Information Processing Systems, 2021. URL Mansheej Paul, Feng Chen, Brett W Larsen, Jonathan Frankle, Surya Ganguli, and Gintare Karolina Dz-iugaite. Unmasking the lottery ticket hypothesis: Whats encoded in a winning tickets mask?arXivpreprint arXiv:2210.03044, 2022a. Mansheej Paul, Brett Larsen, Surya Ganguli, Jonathan Frankle, and Gintare Karolina Dziugaite. Lotterytickets on a data diet: Finding initializations with sparse trainable networks. In S. Koyejo, S. Mohamed,A. Agarwal, D. Belgrave, K. Cho, and A. Oh (eds.), Advances in Neural Information Processing Systems,volume 35, pp. 1891618928. Curran Associates, Inc., 2022b. URL",
  "AExperimental Setup": "For all experiments, networks were trained with a batch size of 128 for a total of 160 epochs. A stepwiselearning rate schedule was employed with an initial learning rate of 0.1 the learning rate is reduced by afactor of 10 at epoch 80 and epoch 120. When linear warmup is employed over t iterations, the learningrate is i t l at iteration i where l is the initial learning rate. All datasets were also augmented with randomcrops to a size of 32x32 pixels after padding 4 pixels to each border in the original image, with a constantvalue of 0, and horizontal image flips with a probability of 0.5.",
  "BResults using LayerNorm": "Relative to BatchNorm, LayerNorm does not enable large learning rates as a consequence, we cannot makeuse of the same training setup. In order to draw a fair comparison between the two cases, we linearly warmup the learning rate to 0.1 over 100 iterations. We attempt to keep the number of warmup iterations assmall as possible since warmup is known to induce linear mode connectivity onset more quickly Frankle &Carbin (2019); Frankle et al. (2020). We note that the performance gain due to interpolation and drop afterinterpolation is smaller compared to the BatchNorm case we leave a more complete analysis of this forfuture work.",
  "Split 40.18621-0.14679-0.170650.19244": ": Relative change (denoted by ) in accuracy and error due to weight interpolation and just afterweight interpolation, on data splits based on difficulty. The former is computed using the difference betweeninterpolant accuracy/error and the mean accuracy/error of the child networks. The latter is computed usingthe difference between interpolant accuracy/error just after interpolation, and after training it for one epoch.The results are averaged over 5 runs. Note that Error After Interpolation for Split 1 is reported as infsince the interpolant achieves perfect accuracy (and consequently zero error) before being trained in nearlyall runs considered.",
  "DAdditional Results for ResNet20 + CIFAR-10": "0.5 0.6 0.7 0.8 k = 1, s = 1 0.02.55.07.510.012.515.0 k = 1, s = 5 k = 1, s = 10 Network ANetwork BAveraged Network 0.5 0.6 0.7 0.8 k = 5, s = 1 0.02.55.07.510.012.515.0 k = 5, s = 5 k = 5, s = 10 0.5 0.6 0.7 0.8 k = 10, s = 1 0.02.55.07.510.012.515.0 k = 10, s = 5 k = 10, s = 10",
  "EVGG-16 + CIFAR-10": "0.2 0.4 0.6 0.8 k = 0 iters 0.2 0.4 0.6 0.8 k = 100 iters 0.2 0.4 0.6 0.8 k = 400 iters 0.5 0.6 0.7 0.8 0.9 k = 750 iters 0.775 0.800 0.825 0.850 0.875 0.900 0.925 k = 10 epochs alpha = 0alpha = 0.25alpha = 0.5alpha = 0.75alpha = 1.0 s (in epochs)",
  ": Progression of test performance of networks resulting from linear weight interpolation of childnetworks": "0.2 0.4 0.6 k = 3, s = 1 0.02.55.07.510.012.515.0 k = 3, s = 5 k = 3, s = 10 Network ANetwork BAveraged Network 0.2 0.4 0.6 k = 5, s = 1 0.02.55.07.510.012.515.0 k = 5, s = 5 k = 5, s = 10 0.2 0.4 0.6 k = 10, s = 1 0.02.55.07.510.012.515.0 k = 10, s = 5 k = 10, s = 10",
  "FResNet56 + CIFAR-100": "0.0 0.1 0.2 0.3 0.4 0.5 0.6 k = 0 iters 0.0 0.1 0.2 0.3 0.4 0.5 0.6 k = 100 iters 0.0 0.1 0.2 0.3 0.4 0.5 0.6 k = 750 iters 0.40 0.45 0.50 0.55 0.60 k = 10 epochs 0.55 0.60 0.65 0.70 k = 20 epochs alpha = 0alpha = 0.25alpha = 0.5alpha = 0.75alpha = 1.0 s (in epochs)",
  "GResNet20 + CINIC-10": "0.1 0.2 0.3 0.4 0.5 0.6 0.7 k = 0 iters 0.2 0.3 0.4 0.5 0.6 0.7 k = 100 iters 0.4 0.5 0.6 0.7 k = 400 iters 0.50 0.55 0.60 0.65 0.70 0.75 k = 750 iters 0.650 0.675 0.700 0.725 0.750 0.775 0.800 0.825 k = 10 epochs alpha = 0alpha = 0.25alpha = 0.5alpha = 0.75alpha = 1.0 s (in epochs)",
  ": Progression of test performance of networks resulting from linear weight interpolation of childnetworks on CINIC-10 (Darlow et al., 2018) with a ResNet20": "0.4 0.5 0.6 0.7 k = 1, s = 1 0.02.55.07.510.012.515.0 k = 1, s = 5 k = 1, s = 10 Network ANetwork BAveraged Network 0.4 0.5 0.6 0.7 k = 5, s = 1 0.02.55.07.510.012.515.0 k = 5, s = 5 k = 5, s = 10 0.4 0.5 0.6 0.7 k = 10, s = 1 0.02.55.07.510.012.515.0 k = 10, s = 5 k = 10, s = 10"
}