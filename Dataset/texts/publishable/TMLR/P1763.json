{
  "Abstract": "Existing methods for last layer retraining that aim to optimize worst-group accuracy (WGA)rely heavily on well-annotated groups in the training data. We show, both in theory andpractice, that annotation-based data augmentations using either downsampling or upweightingfor WGA are susceptible to domain annotation noise. The WGA gap is exacerbated in high-noise regimes for models trained with vanilla empirical risk minimization (ERM). To this end,we introduce Regularized Annotation of Domains (RAD) to train robust last layer classifierswithout needing explicit domain annotations. Our results show that RAD is competitivewith other recently proposed domain annotation-free techniques. Most importantly, RADoutperforms state-of-the-art annotation-reliant methods even with only 5% noise in thetraining data for several publicly available datasets.",
  "Introduction": "Last-layer retraining (LLR) has emerged as a method for using embeddings from pretrained models to quicklyand efficiently learn classifiers in new domains or for new tasks. Because only the linear last layer is retrained,LLR allows transferring to new domains/tasks with much fewer examples than required to train a deepnetwork from scratch. One promising use of LLR is to retrain deep models with a focus on fairness orrobustness, and because data is frequently made up of distinct subpopulations (oft referred to as groups1",
  "lower bound on the overall accuracy of a classifier under any subpopulation shift, thereby assuring that allgroups are well classified": "State-of-the-art (SOTA) methods for optimizing WGA generally modify either the distribution of the trainingdata (Kirichenko et al., 2023; Giannone et al., 2021; LaBonte et al., 2023) or the training loss (Arjovskyet al., 2019; Liu et al., 2021; Sagawa et al., 2020; Qiu et al., 2023) to account for imbalance amongst groupsand successfully learn a classifier which is fair across groups. These methods also use some form of implicitor explicit regularization in the retraining step to limit overfitting to either the group imbalances or thespuriously correlated features in the training data. Kirichenko et al. (2023) argue that strong 1 regularizationplus data augmentation helps to learn core features, i.e., those correlated with the label for all examples.Thus, one can instead use regularization without data augmentation to learn spurious features explicitly, inwhich case misclassified examples can be viewed as belonging to minority groups. This allows us to avoid theexplicit use of group annotations. We examine two representative data augmentation methods, namely downsampling (Kirichenko et al., 2023;LaBonte et al., 2023; Chaudhuri et al., 2023) and upweighting (Idrissi et al., 2022; Liu et al., 2021; Qiu et al.,2023), which achieve SOTA WGA with simple modifications to the data and loss, respectively. In the simplestsetting, each of these methods requires access to correctly annotated groups to balance the contributionof each group to the loss. In practice, group annotations are often noisy (Wei et al., 2022), which can becaused by either domain noise, label noise, or both. Label noise generally affects classifier training and dataaugmentation, making analysis more challenging. We consider only domain noise so that we can compare toexisting methods for enhancing WGA. Furthermore, focusing on domain noise presents a stepping stone toanalyzing group noise in general.",
  "Our Contributions": "We present theoretical guarantees for the WGA under domain noise when modeling last layer representationsas symmetric mixtures. We show that both DS and UW achieve identical WGA and degrade significantly withan increasing percentage of symmetric domain noise, in the limit degrading to the performance of empiricalrisk minimization (ERM). This is further confirmed with numerical experiments for a synthetic Gaussianmixture dataset modeling latent representations. Our key contribution is a two-step methodology involving: (i) regularized annotation of domains (RAD) topseudo-annotate examples by using a highly regularized model trained to learn spuriously correlated features.By learning the spurious correlations, RAD constructs a set of examples for which such correlations do nothold; we identify these as minority examples. (ii) LLR using all available data, while upweighting (UW)examples in the pseudo-annotated minority. This combined approach, denoted RAD-UW, captures the key observation made by many that regularizedLLR methods are successful as they implicitly differentiate between core\" and spurious\" features. We testRAD-UW on several large publicly available datasets and demonstrate that it achieves SOTA WGA evenwith noisy domain annotations. Additionally, RAD-UW incurs only a minor opportunity cost for not usingdomain labels even in the noise-free setting.",
  "Related Works": "Downsampling has been explored extensively in the literature and appears to be the most common method forachieving good WGA. Kirichenko et al. (2023) propose deep feature reweighting (DFR), which downsamplesthe majority groups to the size of the smallest group and then retrains the last layer with strong 1regularization. Chaudhuri et al. (2023) explore the effect of downsampling theoretically and show thatdownsampling can increase WGA under certain data distribution assumptions. LaBonte et al. (2023) usea variation on downsampling to achieve competitive WGA without domain annotations using implicitlyregularized identification models. Upweighting has been used as an alternative to downsampling as it does not require removing any data.Idrissi et al. (2022) show that upweighting relative to the proportion of groups can achieve strong WGA,and Liu et al. (2021) extend this idea (using upsampling from the same dataset, which is equivalent to",
  "Published in Transactions on Machine Learning Research (12/2024)": ": CMNIST WGA under domain label noise. CMNIST has a relatively small spurious correlationand as such even LLR performs quite well.Additionally, CMNIST is already class balanced so classdownsampling and upweighting have no effect. (LaBonte et al., 2023) do not provide WGA for CMNIST;RAD-UW matches the performance of group-dependent methods at 10% and surpasses it beyond that.",
  "Problem Setup": "We consider a supervised classification setting and assume that the LLR methods have access to a representationof the ambient (original high-dimensional data such as images, etc.) data, the ground-truth label, as well asthe (possibly noisy) domain annotation. Taken together, the label and domain combine to define the groupannotation for any sample. More formally, the training dataset is a collection of i.i.d. tuples of the randomvariables (Xa, Y, D) PXaY D, where Xa Xa is the ambient high-dimensional sample, Y Y is the classlabel, and D D is the domain label. Here, we present the problem as generic multi-class, multi-domainlearning, but for ease of analysis, we will later restrict ourselves to the binary class, binary domain setting.Since the focus here is on learning the linear last layer, we denote the latent representation that acts as aninput to this last layer by X := (Xa) for an embedding function : Xa X Rm such that the LLRdataset is (X, Y, D) PXY D. The tuples (Y, D) of class and domain labels partition the examples into g := |Y D| different groupswith priors (y,d) := P(Y = y, D = d) for (y, d) Y D. We denote the linear correction applied in thelatent space of a pretrained model as f : X R|Y|, which is parameterized by a linear decision boundary = (w, b) Rm|Y| R|Y| given byf(x) = (wT x + b).(1)",
  "Data Augmentation": "Downsampling (DS) reduces the number of examples in majority groups such that minority and majoritygroups have the same sample size. In practice, this reduces the dataset size from n to g nmin where nmin isthe number of examples in the smallest group. In the population setting (as n and nmin min n),this is equivalent to setting all group priors equal to 1/g. Upweighting (UW) does not remove data but weights the loss more for minority examples and less formajority samples. Generally, the upweighting factor c can be a hyperparameter, though often one uses theinverse of the prevalence of the group in practice, which estimates",
  "Domain Noise": "Invariant0%20% (b) The cost of ignoring domain annotations. Foreach of the datasets, we compare the WGA of the bestdomain-dependent method at 0% and 20% domain noisewith the best WGA amongst the methods which donot use domain information (labeled Invariant). Thecost of ignoring the domain information is very smallfor most datasets and in Waterbirds, utilizing domaininformation hurts performance somewhat. Thus, theopportunity cost of not using domain information is rela-tively small, while the cost of using a domain-dependentmethod with noise is quite large. Our experiments also indicate that downsampling induces a higher variance in WGA, especially in the noisydomain annotation setting. While this phenomenon is not captured in our population analysis, intuitively oneshould expect that having a smaller dataset should increase the variance. Additionally, the models learnedby group downsampling suffer from an additional dependence on the data that is selected in downsampling,which in turn increases the WGA variance. Overall, we demonstrate that achieving SOTA worst-group accuracy is strongly dependent on the quality ofthe domain annotations on large publicly available datasets. Our experiments consistently show that in orderto be robust to noise in the domain annotations, it is necessary to ignore them altogether. To this end, ournovel domain annotation-free method, RAD-UW, assures WGA values competitive with annotation-inclusivemethods. RAD-UW does so by pseudo-annotating with a highly regularized model that allows discriminatingbetween samples with spurious features and those without and retraining on the latter with upweighting. Ourcomparison of RAD-UW to two existing domain annotation-free methods and several domain annotation-dependent methods clearly highlights that it can outperform existing methods even for only 5% domain labelnoise. There is a significant breadth of future work available in this area. In the domain annotation-free setting,there is a gap in the literature regarding identification of subpopulations beyond the binary minority,\"or majority\" groups. Identifying individual groups could help to increase the performance of retrainedmodels and give better insight into which groups are negatively affected by vanilla LLR. Additionally, tuninghyperparameters without a clean holdout set remains an open question. Beyond this, group noise could be driven by class label noise alongside domain annotation noise. Thecombination of these two types of noise would necessitate a robust loss function when training both thepseudo-annotation and retraining models, or a new approach entirely. We are optimistic that the approachpresented here could be combined in a modular fashion with a robust loss to achieve robustness to moregeneral group noise.",
  "(x ) = (x + ),x R": "Assumption 3.3. Conditioned on Y and D, X X is distributed according to PX|y,d for (y, d) Y D,with mean (y,d) := E[X|Y = y, D = d] Rm and positive semidefinite covariance Rmm such that thedistribution of aT X|(Y = y, D = d) is symmetric for any a Rm and its CDF aT X|y,d : R is strictlyincreasing. Additionally, we place priors (y,d), (y, d) Y D, on each group and priors (y) := P(Y = y),y Y, on each class. Note that Assumption 3.3 holds for many conditional distributions on X including spherically symmetricdistributions (see Fang (2018, Theorem 2.4) along with the fact that spherically symmetric distributionshave symmetric marginals), Guassian distributions, and distributions with sub-independent and symmetricmarginals. For ease of intuition, we focus mainly on Guassian groups. Following Chaudhuri et al. (2023)we deal only with distributions which induce strictly increasing CDFs so that there is an explicit tradeoffbetween majority and minority group performance.",
  "Assumption 3.5. The difference in means between classes in a domain D := (1,d) (0,d) is constant ford D": "Assumption 3.5 also implies that the difference in means between domains within the same class C :=(y,B) (y,R) is also constant for each y Y. We see this by noting that each group mean makes up thevertex of a parallelogram. This is illustrated in , where D and C are shown on data samplesdrawn from a distribution satisfying Assumptions 3.3 to 3.5.",
  "with equality at p = 1/2 or 0 = 1/4": "Proof Sketch. The proof of Theorem 3.7 is presented in Appendix A and involves showing that the WGA fordownsampling under domain label noise is the same as that for ERM (which is noise agnostic) but with adifferent prior dependent on p. Our proof refines the analysis in Yao et al. (2022) and involves the noisy prior.",
  "(p)0:= (1 p)0 + p (1/2 0) .(8)": "As p 1/2 in (8), the minority prior perceived by both UW and DS tends to a balanced prior across groups.A UW augmentation thus would weight in inverse proportion to the corresponding noisy (and not the true)prior for each group. The true prior of the minority group after DS can be derived as (see Appendix A)",
  "(1/2 (p)0 ).(9)": "Thus, with noisy domain labels, instead of the desired balanced group priors after downsampling, from (9),DS results in a true minority prior that decreases from 1/4 to 0. Thus, noise in domain labels drives inactionfrom both augmented methods as p increases. We see the effect of noise in a numerical example in , noting that while the theorem is in thepopulation setting, our numerical example uses finite sample methods with n = 10, 000. This shows that theperformance of each method quickly degrades even in this simple setting. The ERM performance, however,remains constant because ERM does not use domain information. This motivates us to examine a robustmethod that does not use domain information at all but is more effective than ERM in terms of WGA.",
  ": For latent Gaussian data, the WGA of DS and UW (seen as overlapping) decreases as the noiseprevalence p increases to 1/2. At the extreme point, the WGA of ERM is recovered": "regularize our pseudo-annotation model with an 1 penalty. The intuition behind using an 1 penalty issimilar to that of DFR (Kirichenko et al., 2023). Where Kirichenko et al. (2023) argue that an 1 penaltyhelps to select only core features\" when trained on a group-balanced dataset, we argue that the same penaltywith a large multiplicative factor will help to select spuriously correlated features when trained on the originalimbalanced data. If we can successfully learn the spuriously correlated features, those samples which arecorrectly classified can be viewed as majority samples (those for which the spurious correlation holds) andthose which are misclassified can be seen as minority samples. We introduce RAD (Regularized Annotation of Domains) which uses a highly 1 regularized linear model topseudo-annotate domain information by quantizing true domains to binary majority and minority annotations.Pseudocode for this algorithm is presented in Algorithm 1. We see that a strongly regularized classified is firstlearned, fID, which allows us to identify minority points through misclassification. We use these annotations,d, to upweight examples in the next step RAD-UW (Algorithm 2) involves learning sequentially: (i) a pseudo-annotation RAD model (outlined inAlgorithm 1), and (ii) a regularized linear retraining model, which is trained on all examples while upweightingthe pseudo-annotated minority examples output by RAD, as the solution RAD-UW optimizing the empiricalversion of (3) using the logistic loss L with 1 regularization as:",
  "i=1c( di)L(f(xi), yi) + w1.(10)": "Here again, the 1 regularization comes into play. While the regularization in the first step encouraged thebiased model to misclassify minority points, here the same regularization encourages learning features whichdo well both for the majority and (upweighted) minority. This discourages learning spurious features whichmay be present (and helpful) in the majorities. RAD-UW vs. M-SELFWhile both RAD and SELF are two-stage methods which utilize the biases ofan identification model to pseudoannotate points in minority classes, RAD explicitly trains such a biasedclassifier using a strong 1 penalty whereas the classifier used by SELF is the pretrained classification head.The benefit of retraining the classifier is that we can force the model to rely more strongly on spuriouscorrelations. We see in practice that this leads to drastic increases in performance on some datasets. A sideeffect of this retraining is that we can more effectively correct models which were originally trained with noisydata, as the noise affects the classification head more strongly than the embeddings themselves. We explorethis is Appendix G.",
  "Empirical Results": "We present worst-group accuracies for several representative methods across four large publicly availabledatasets. Note that for all datasets, we use the training split to train the embedding model. Following priorwork (Kirichenko et al., 2023; LaBonte et al., 2023), we use half of the validation as retraining data, i.e.,training data for only the last layer, and half as a clean holdout.",
  "Datasets": "CMNIST (Arjovsky et al., 2019) is a variant of the MNIST handwritten digit dataset in which digits 0-4are labeled y = 0 and digits 5-9 are labeled y = 1. Further, 90% of digits labeled y = 0 are colored green and10% are colored red. The reverse is true for those labeled y = 1. Thus, we can view color as a domain and wesee that the color of the digit and its label are correlated. CelebA (Liu et al., 2015) is a dataset of celebrity faces. For this data, we predict hair color as either blonde(y = 1) or non-blonde (y = 0) and use gender, either male (d = 1) or female (d = 0), as the domain label.There is a natural correlation in the dataset between hair color and gender because of the prevalence ofblonde female celebrities. Waterbirds (Sagawa et al., 2020) is a semi-synthetic dataset which places images of land birds (y = 1) orsea birds (y = 0) on land (d = 1) or sea (d = 0) backgrounds. There is a correlation between background andthe type of bird in the training data but this correlation is removed in the validation data. MultiNLI (Williams et al., 2018) is a text corpus dataset widely used in natural language inference tasks.For our setup, we use MultiNLI as first introduced in (Oren et al., 2019). Given two sentences, a premise anda hypothesis, our task is to predict whether the hypothesis is either entailed by, contradicted by, or neutralwith the premise. There is a spurious correlation between there being a contradiction between the hypothesisand the premise and the presence of a negation word (no, never, etc.) in the hypothesis. CivilComments (Borkan et al., 2019) is a text corpus dataset of public comments on news websites.Comments are labeled either as toxic (y = 1) or civil (y = 0) and the spurious attribute is the presence (d = 1)or absence (d = 0) of a minority identifier (e.g. LGBTQ, race, gender). There is a strong class imbalance(most comments are civil), though the domain imbalance is modest.",
  "Importance of 1 Regularization": "We emphasize that a strong 1 regularizer is critical to the success of our method through the intuition ofDFR, but it remains to be seen how this bears out in practice. To this end, we examine combinations of both1 and the common 2 penalties. We see in that the retraining regularizer has only a small impact onthe overall performance, while the identification regularization has more impact. We note that for CelebA, a",
  "Main Results": "We present results for both group- and class-only-dependent methods using both downsampling and upweight-ing. Additionally, we present results for vanilla LLR, which performs no data or loss augmentation stepbefore retraining. Finally, we present results for two-stage last layer methods, namely misclassification SELF(M-SELF) and RAD with upweighting (RAD-UW). Every retraining method solves the following empiricaloptimization problem:",
  "which can be seen as a finite sample version of (3) with an 1 regularization. For , we use the logistic loss": "The group-dependent downsampling procedure we have adopted is the same as that of DFR, introduced inKirichenko et al. (2023), but the DFR methodology averages the learned model over 10 training runs. Thiscould help to reduce the variance, but we do not implement this so as to directly compare different dataaugmentation methods since most others do not so either. More generally, we could apply model averagingto any of the data augmentation methods. We explore the effect of model averaging in Appendix E. We use the logistic regression implementation from the scikit-learn (Pedregosa et al., 2011) package for theretraining step for all presented methods. For all final retraining steps (including LLR) an 1 regularizationis added. The strength of the regularization is a hyperparameter selected using the clean holdout. Theupweighting factor for group annotation-inclusive UW methods is given by the inverse of the perceivedprevalence for each group or class. For our RAD-UW method, we tune the regularization strength ID for the pseudo-annotation model viaa grid search. We additionally tune the regularization strength of the retraining model along with theupweighting factor c, which is left as a hyperparameter because the identification of domains by RAD is onlybinary and may not reflect the domains in the clean holdout data. The same upweighting factor is used forevery pseudo-annotated minority sample. Note that for all of the WGA results, we report the mean WGA over 10 independent noise seeds and 10training runs for each noise seed. We also present the standard deviation around this mean, which will reflectthe variance over training runs with the optimal hyperparameters. For misclassification SELF (M-SELF)(LaBonte et al., 2023), we present both our own implementation of their algorithm and their results directly.It should be noted that they report the mean and standard deviation over only three independent runs andwithout noise; for the noisy setting, we observe that their method does not use domain annotations and sois unaffected by domain noise just as RAD-UW is. Our implementation utilizes the same embeddings asRAD-UW so we can ensure any differences are artifacts of the algorithms rather than the upstream model.The importance of the pretrained model is an important and underexplored factor in the literature which weleave as future work.",
  "Worst-Group Accuracy under Noise": "We now detail the results on all datasets in Figures 3a, 3b, 4a, 4b and 5a. See Appendix F for tables includingclass dependent versions of downsampling and upweighting. We choose not to include them here as theirperformance is so poor as to distract from the results. For CMNIST, we present results in a and ; we see that each method that uses domaininformation achieves strong WGA at 0% domain annotation noise, but for increasing noise, their performancedrops noticeably.Additionally the methods which rely only on class labels without inferring domainmembership are consistent across noise levels, but are outdone by their domain-dependent counterpartseven at 20% noise. Finally, RAD-UW achieves WGA comparable with the group-dependent methods, andsurpasses their performance after 10% noise in domain annotation. The results for CelebA are presented in b and . We first note that LLR achieves significantlylower WGA than any other method owing to strong spurious correlation even in the retraining data. We alsonote that every domain-dependent method has a much more significant decline in performance for CelebAthan for CMNIST. Here, RAD-UW outperforms the domain-dependent methods even at 10% SLN. Notonly this, but RAD beats the performance of SELF (LaBonte et al., 2023) for this dataset with much lowervariance. For Waterbirds, we see in a and that noise, even significant amounts of it, has little effect onany of the methods considered. This is because the existing splits have a domain-balanced validation, whichwe use here for retraining. Thus domain noise does not affect the group priors at all as argued analyticallyin (8) with 0 = 1/4. Even so, we see that RAD-UW is competitive with existing methods, including thedomain annotation-free methods of LaBonte et al. (2023). For MultiNLI, we see in b and that RAD-UW is able to match domain-dependent methodsat 5% noise and outperform them at 10%, but is beaten by M-SELFs reported result. We note that we werenot able to replicate their performance with our implementation. This may be because MultiNLI has weakerspurious correlation than the other datasets we examine. Also important to note is the effect of noise onMultiNLI. For 5% and 10% noise levels, we see a dramatic drop in WGA for domain-dependent methods,but this levels off as we recover the WGA of LLR. Thus even for 10% noise, LLR is competitive with topdomain-dependent methods.",
  "(b) MultiNLI WGA under Domain Noise": ": We see that RAD-UW and M-SELF strongly outperform domain-dependent methods for mostnoise levels. Waterbirds is domain balanced from the beginning, so adding domain noise does not stronglybias the downsampling or upweighting classifiers. Finally, for CivilComments, we that in a and that RAD-UW drastically outperformsM-SELF and is competitive with domain-dependent methods. Because CivilComments is generally domainbalanced, domain noise does not have a large effect on the performance of GDS and GUW. On the otherhand, the extreme class imbalance of the dataset leads to poor performance by M-SELF. The robustness ofRAD-UW to extreme class imblance is an important factor in its success and likely due to the downsamplingof competing methods. An interesting observation is that for almost all datasets and noise levels, the variance of group-dependentdownsampling is consistently larger than upweighting at the same noise levels. This behavior is likely causedby two key issues: (i) DS reduces the dataset size, and (ii) it randomly subsamples the majority, each ofwhich could increase the variance of the resulting classifier. DFR (Kirichenko et al., 2023) has attempted toremedy this issue by averaging the linear model that is learned over 10 different random downsamplings, butthis increases the complexity of learning the final model. Our results raise questions on the prevalence ofdownsampling over the highly competitive upweighting method in the setting of WGA. A similar comment can be made with respect to RAD-UW and M-SELF. While M-SELF has strongperformance on several datasets, it consistently has higher variance than RAD-UW and struggles on datasetswith large class imbalance. This is likely due to the class balancing that LaBonte et al. (2023) use in theretraining step. RAD-UW avoids this issue by using all of the available data and upweighting error points. Finally, we consider the opportunity cost of ignoring domain annotations in the worst case, i.e., the domainlabels we have are in fact noise-free but we still ignore them. In b, we see that the domain annotation-free methods cost very little in terms of lost WGA when training on cleanly annotated data. This loss isminimal in comparison to the cost of training using an annotation-dependent method when the domaininformation is noisy. Thus, if there are concerns about domain annotation noise in the training data, it issafest to use a domain annotation-free approach.",
  "Discussion": "We see in our experiments that these two archetypal data augmentation techniques, downsampling andupweighting, achieve very similar worst-group accuracy and degrade similarly with noise. This falls in linewith our theoretical analysis, and suggests that simple symmetric mixture models for subpopulations canprovide intuition for the performance of data-augmented last layer retraining methods on real datasets.",
  "Broader Impacts and Limitations": "We attempt to address the issue of subgroup fairness and robustness to subpopulation shift through a pseudoannotation strategy in the domain. This method could allow practitioners to more easily adapt existingmodels while assuring fair classification across subpopulations. However, because there is a not a formalguarantee of fairness, there is a possible societal consequence of practitioners assuming the retrained model isfair without verifying it. Similar lack of guarantees is a downfall of most methods in the WGA literature. Additionally Oh et al. (2022) has demonstrated that most two-stage methods, including the full-retrainingprecursors to RAD and SELF, can fair dramatically with small amounts of class label noise. Addressing thisissue is left as future work. Finally we note that achieving this fairness without domain-annotation presents an opportunity when domainlabels may be private. This tradeoff between privacy and fairness is an important area of research, as discussedin King et al. (2023). Perhaps there is a regime with limited private information, but enough to achieve somegains over domain-agnostic methods such as RAD and SELF.",
  "Acknowledgements": "Nathan Stromberg, Rohan Ayyagari, Monica Welfert, and Lalitha Sankar acknowledge support by NSFCIF-2007688, SCH-2205080, PIPP-2200161, and a Google AI for Social Good grant. Nathan Stromberg andMonica Welfert are grateful for support from Arizona State Universitys Deans Fellowship. Sanmi Koyejoacknowledges support by NSF Career Award 2046795 and SCH-2205329, Stanford HAI, and Google Inc.",
  "Giannone, G., Havrylov, S., Massiah, J., Yilmaz, E., and Jiao, Y. Just mix once: Mixing samples withimplicit group distribution. In NeurIPS 2021 Workshop on Distribution Shifts, 2021": "Idrissi, B. Y., Arjovsky, M., Pezeshki, M., and Lopez-Paz, D. Simple data balancing achieves competitiveworst-group-accuracy. In Proceedings of the First Conference on Causal Learning and Reasoning, 2022. Iscen, A., Tolias, G., Avrithis, Y., and Chum, O. Label propagation for deep semi-supervised learning. InProceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 50705079, 2019. Iscen, A., Valmadre, J., Arnab, A., and Schmid, C. Learning with neighbor consistency for noisy labels. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 46724681,2022.",
  "Izmailov, P., Kirichenko, P., Gruver, N., and Wilson, A. G. On feature learning in the presence of spuriouscorrelations. Advances in Neural Information Processing Systems, 2022": "King, J., Ho, D., Gupta, A., Wu, V., and Webley-Brown, H. The privacy-bias tradeoff: Data minimization andracial disparity assessments in u.s. government. In Proceedings of the 2023 ACM Conference on Fairness,Accountability, and Transparency, 2023. Kirichenko, P., Izmailov, P., and Wilson, A. G. Last layer re-training is sufficient for robustness to spuriouscorrelations. In The Eleventh International Conference on Learning Representations, 2023.",
  "Oh, D., Lee, D., Byun, J., and Shin, B. Improving group robustness under noisy labels using predictiveuncertainty. ArXiv, abs/2212.07026, 2022": "Oren, Y., Sagawa, S., Hashimoto, T. B., and Liang, P. Distributionally robust language modeling. InProceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9thInternational Joint Conference on Natural Language Processing (EMNLP-IJCNLP), 2019. Patrini, G., Rozza, A., Krishna Menon, A., Nock, R., and Qu, L. Making deep neural networks robust tolabel noise: A loss correction approach. In Proceedings of the IEEE conference on computer vision andpattern recognition, 2017. Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Prettenhofer,P., Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cournapeau, D., Brucher, M., Perrot, M., andDuchesnay, E. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 2011. Qiu, S., Potapczynski, A., Izmailov, P., and Wilson, A. G. Simple and fast group robustness by automaticfeature reweighting. In Proceedings of the 40th International Conference on Machine Learning, 2023.",
  "Sagawa, S., Koh, P. W., Hashimoto, T. B., and Liang, P. Distributionally robust neural networks. InInternational Conference on Learning Representations, 2020": "Wei, J., Zhu, Z., Cheng, H., Liu, T., Niu, G., and Liu, Y. Learning with noisy labels revisited: A study usingreal-world human annotations. In International Conference on Learning Representations, 2022. Williams, A., Nangia, N., and Bowman, S. A broad-coverage challenge corpus for sentence understandingthrough inference. In Proceedings of the 2018 Conference of the North American Chapter of the Associationfor Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), 2018.",
  "vT 1v": "Proof. Since the model learned by ERM with no data augmentation is invariant to domain label noise byLemma A.1, we derive the the general form for the WGA of a model f under the assumption of having cleandomain label, and therefore using the original data parameters. We begin by deriving the individual accuracyterms A(y,d)(f), (y, d) {0, 1} {R, B}, with f(x) = wT x + b and Y = 1{f(x) > 1/2}, as follows:",
  ",(34)": "where the first term is the accuracy of the minority groups and the second is that of the majority groups. Inorder to compare the WGA of ERM with the WGA of DS, we first show that under Assumption 3.6 theWGA of ERM is given by the majority accuracy term in (34). Since c0 0 for 0 1/4, we have that",
  "(1/2 (p)0 ).(36)": "Proof. We note that the model learned after downsampling is agnostic to domain labels, so only the trueproportion of each group, not the noisy proportion, determines the model weights. We derive the equivalentclean prior. We do so by examining how true minority samples are affected by DS on the data with noisydomain labels. When DS is performed on the data with domain label noise, the true minority samples thatare kept can be categorized as (i) those that are still minority samples in the noisy data and (ii) a proportionof those that have become majority samples in the noisy data.",
  "(1/2 (p)0 ).(41)": "DS is usually a special case of ERM with 0 = 1/4. However, since DS uses domain labels and therefore isnot agnostic to noise, we need to use the prior derived in Lemma A.5 to be able to analyze the effect of thenoise p while still using the clean data parameters. Note that (p)DS defined in (36) decreases from 1/4 to 0 asthe noise p increases from 0 to 1/2. Since we can interpolate between 1/4 to 0 using p, we can thereforesubstitute 0 in (12) with (p)DS and then examine the resulting expression as a function of p for any 0. Using",
  "CExperimental Design": "For the image datasets, we use a Resnet50 model pre-trained on ImageNet, imported from torchvision asthe upstream model. For the text datasets, we use a BERT model pre-trained on Wikipedia, imported fromthe transformers package as the upstream model. In our experiments, we assume access to a validation setwith clean domain annotations, which we use to tune the hyperparameters. For all methods, we tune theinverse of , where is the regularization strength, over 20 (equally-spaced on a log scale) values ranging from",
  "C.1Waterbirds": "For the upstream ResNet50 model, we use a constant learning rate of 1e 3, momentum of 0.9, and weightdecay of 1e3. We train the upstream model for 100 epochs. We leverage random crops and random horizontalflips as data-augmentation during the training. For RAD-UW, we tune for both the pseudo-annotationmodel and the retraining model. We tune the upweighting factor, c, for the retraining model over 5 equallyspaced values ranging from 5 to 20. The pseudo-annotation model uses a constant learning rate of 1e 5.",
  "C.2CelebA": "For the upstream ResNet50 model, we use a constant learning rate of 1e 3, momentum of 0.9 and weightdecay of 1e 4. We train the upstream model for 50 epochs while using random crops and random horizontalflips for data augmentation. We tune the upweighting factor, c, for the retraining model over 5 equally spacedvalues ranging from 20 to 40. The pseudo-annotation model is trained with an initial learning rate of 1e 5with CosineAnnealingLR learning rate scheduler from pytorch.",
  "C.3CMNIST": "For the ResNet50 model, we use a constant learning rate of 1e 3, momentum of 0.9 and weight decay of1e 3. We train the model for 10 epochs without any data augmentation. We tune the upweighting factor,c, over 5 equally spaced values ranging from 20 to 40. The pseudo-annotation model is trained with aninitial learning rate of 1e 5 with CosineAnnealingLR learning rate scheduler from pytorch. The spuriouscorrelations in CMNIST is between the digit being between less than 5 and the color of the digit. We notethat this simple correlation is quite easy to learn, so the 1 penalty needed in quite small.",
  "C.4MultiNLI": "We train the BERT model using code adapted from (Izmailov et al., 2022). We train the model for 10epochs with an initial learning rate of 1e 4 and a weight decay of 1e 4. We use the linear learningrate scheduler imported from the transformers library. The upweight factor is tuned over 5 equally spacedvalues ranging from 4 to 10. The pseudo-annotation model is trained with an initial learning rate of 1e 4with CosineAnnealingLR learning rate scheduler from pytorch.",
  "C.5Civil Comments": "Similar to MultiNLI, We train the BERT model using code adapted from (Izmailov et al., 2022). We trainthe model for 10 epochs with an initial learning rate of 1e 5 and a weight decay of 1e 4. We use thelinear learning rate scheduler imported from the transformers library. The upweight factor is tuned over 5equally spaced values ranging from 4 to 10. The pseudo-annotation model is trained with an initial learningrate of 1e 4 with CosineAnnealingLR learning rate scheduler from pytorch.",
  "DM-SELF Implementation": "We implemented misclassification-SELF using code adapted from LaBonte et al. (2023) so that it wouldbe compatible with our setup, where we use pre-generated embeddings from the base models. We fix thefinetuning steps, which is the number of steps of fine-tuning we perform once we construct the class-balanced error set. We tune the learning rate and the number of points that are selected for class balancing.The hyperparameter values and ranges used are given in",
  "EModel Averaging": "DFR (Kirichenko et al., 2023) emphasizes the importance of model averaging in their setting as seeingdifferent downsampled retraining sets can dramatically change the fair classifier. To examine these effects wetrain GDS 10 times and average the weights over those 10 runs. We report the mean and standard deviationof the worst-group accuracy of this averaged model over 10 independent runs. We see in Tables 4 to 7 that for each dataset, averaging helps to reduce variance and provides modest increasesin performance for both CelebA and CMNIST. This same model averaging could be performed both for SELFand RAD, but would have no effect in GUW as the weighting set is fixed as is the upweighting factor.",
  "FAdditional Tables": "Methods which require domain annotations (DA) are denoted with a Y\" in the appropriate column, whilethose that are agnostic to DA are denoted as N\". We collate the results of similar methods and separatethose for different approaches in our tables by horizontal lines. Finally, results for methods designed toannotate domains before using data augmentations, namely RAD-UW and SELF (LaBonte et al., 2023), arecollected at the bottom of each table."
}