{
  "Abstract": "Graph Neural Networks (GNNs) form the backbone of several state-of-the-art methods forperforming machine learning tasks on graphs.As GNNs find application across diversereal-world scenarios, ensuring their interpretability and reliability becomes imperative. Inthis paper, we propose Graphon-Explainer, a model-level explanation method to elucidatethe high-level decision-making process of a GNN. Graphon-Explainer learns a graphonasymmetric, continuous function viewed as a weighted adjacency matrix of an infinitely largegraphto approximate the distribution of a target class as learned by the GNN. The learnedgraphon then acts as a generative model, yielding distinct graph motifs deemed significantby the GNN for the target class. Unlike existing model-level explanation methods for GNNs,which are limited to explaining a GNN for individual target classes, Graphon-Explainer canalso generate synthetic graphs close to the decision boundary between two target classes byinterpolating graphons of both classes, aiding in characterizing the GNN models decisionboundary. Furthermore, Graphon-Explainer is model-agnostic, does not rely on additionalblack-box models, and does not require manually specified handcrafted constraints for expla-nation generation. The effectiveness of our method is validated through thorough theoreticalanalysis and extensive experimentation on both synthetic and real-world datasets on the taskof graph classification. Results demonstrate its capability to effectively learn and generatediverse graph patterns identified by a trained GNN, thus enhancing its interpretability forend-users.",
  "Introduction": "Graph Neural Networks(GNNs)(Zhang et al., 2020; Wu et al., 2020) have attained state-of-the-art perfor-mance on various graph tasks such as node classification, graph classification and link prediction. Theyare increasingly being deployed in the real world in diverse applications such as drug discovery and molec-ular structure prediction(Merchant et al., 2023), weather forecasting(Lam et al., 2023) and recommendersystems(Wu et al., 2022). The integration of GNNs into real-world applications comes with the dire needto make them interpretable and trustworthy for stakeholders who regularly interact with or rely on suchmodels. Although substantial research has been conducted on explainability in domains like vision and textanalysis (Kim et al., 2018; Selvaraju et al., 2017; Ribeiro et al., 2016; Chen et al., 2018), these methodsare challenging to directly apply to graphs. This difficulty stems from the discrete nature of graph data,which poses challenges for optimization using gradient-based techniques. Moreover, unlike images, graphdata exhibits irregularity as each graph may contain a varying number of nodes, and its validity often hingesupon adhering to domain specific structural constraints.",
  "Published in Transactions on Machine Learning Research (10/2024)": "Remi Lam, Alvaro Sanchez-Gonzalez, Matthew Willson, Peter Wirnsberger, Meire Fortunato, Ferran Alet,Suman Ravuri, Timo Ewalds, Zach Eaton-Rosen, Weihua Hu, et al. Learning skillful medium-range globalweather forecasting. Science, 382(6677):14161421, 2023. Wanyu Lin, Hao Lan, Hao Wang, and Baochun Li. Orphicx: A causality-inspired latent variable model forinterpreting graph neural networks. In Proceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition, pp. 1372913738, 2022.",
  "We introduce some graphon related mathematical concepts in this section that we use later for the theoreticalanalysis": "Graphon: A graphon S : 2 is a symmetric, continuous, and measurable function which can beviewed as a weighted adjacency matrix of a graph with infinitely many nodes. Given two points xi, xj ,S(i, j) can be interpreted as representing an weighted edge between nodes i and j. Graphons arise as limitsof sequences of dense graphs sharing a common set of topological attributes in the graph theory literatureand can serve as a generator of graphs having similar topological properties.For instance, sampling npoints uniformly from the unit interval and setting S(x, y) = px, y , where p is a constantgenerates a Erds-Rnyi random graph G(n, p).Cut Norm: The cut norm of a graphon S can be defined as:",
  "XYS(x, y)(1)": "For two graphons,S1 and S2, a measure of distance between them can be defined using the cut normS1 S2. Lower values of the cut norm indicate that the graphons S1 and S2 have a high degree of struc-tural similarity. Graphon estimation methods frequently use step functions as it has been proven (Lovsz,2012) that graphons can always be approximated with arbitrary accuracy in the cut norm using step func-tions.Graph Homomorphism:Given two graphs F and G, let their node sets be denoted as V (F) and",
  "Sampling from": ": A demonstration of the Graphon-Explainer workflow on a binary classification problem. First, atrained classifier categorizes a dataset of graphs into two classes c1 and c2. Then, a graphon is estimatedfor each class (Sc1 and Sc2) using the graphs assigned to that class. These estimated graphons are sampledto generate explanations for both classes by optimizing their class scores, with the classifier evaluating theexplanations. Additionally, the estimated graphons are interpolated to create a new graphon Sb, which issampled to produce graphs that lie near the decision boundary of both the classes. These graphs exhibit amixture of motifs from both classes. V (G) respectively and their edge sets be denoted as E(F) and E(G) respectively.A graph homomor-phism is a map from V (F) V (G) such that if (u, v) E(F) then ((u), (v)) E(G).Let,hom(F, G) denote the total number of graph homomorphisms from F to G.Then, the homomorphismdensity t(F, G) can be defined as t(F, G) =hom(F,G) |V (G)||V (F )| . Homomorphism density measures the fraction ofstructure-preserving maps from F to G. The notion of homomorphism density can be naturally extendedto graphons. Given a graph F, its homomorphism density with respect to a graphon S can be defined ast(F, S) =",
  "Related Work": "Graph Neural Networks: GNNs work by learning a nodes representation by a message passing mech-anism. Each nodes representation is learned by aggregating features of neighboring nodes and combiningthem with its own features. Different architectures (Velikovi et al., 2018; Kipf & Welling, 2016; Gilmeret al., 2017) differ in their aggregation functions and message passing functions but the learning strategyremains similar across architectures.Instance-Level Explanations: Instance level methods provide an explanation corresponding to each in-put. Typically, they aim to identify salient subgraphs, nodes or edges that is deemed important by the GNNfor the graph to be classified into a particular class. According to the survey (Kakkad et al., 2023) theycan be categorized into: decomposition based methods(Pope et al., 2019; Feng et al., 2023), gradient basedmethods(Baldassarre & Azizpour, 2019; Huang et al., 2022), surrogate methods(Zhang et al., 2021; Vu &Thai, 2020) and perturbation based methods(Schlichtkrull et al., 2020; Yuan et al., 2021; Lucic et al., 2022;Lin et al., 2022). Sharing a similar goal as our method, MotifExplainer(Yu & Gao, 2023) employs domainspecific motif extraction rules to identify a candidate set of explanation motifs and uses an attention basedmechanism to identify motifs from the candidate set in an instance graph that is important for the catego-rization of the graph to a particular class by the GNN classifier. However, our approach differs significantlyin two key ways. First, while MotifExplainer relies on domain-specific rules for motif extraction, our methoddoes not. Second, MotifExplainer provides instance-level explanations, which are not generalizable acrossdifferent graphs. In contrast, our model-level explanation method generates motifs that reflect the featuresthe GNN has learned for an entire target class, rather than focusing on individual instance graphs.Model-Level Explanations: The goal of model-level explainability is to understand a models generalbehavior by generating graph patterns that trigger specific predictions. Different model-level methods differin their graph generation methods. XGNN(Yuan et al., 2020) trains a reinforcement learning model usingthe policy gradient method to maximize a reward function designed using handcrafted constraints to gen-erate graphs that serve as explanations of a target class. D4Explainer(Chen et al., 2024) generates graphs",
  "Algorithm 1 Generating Explanations using Graphon-Explainer": "Hyperparameters: nsamples: Number of samples to be generated by the estimated generative model. K: Number of nodes in the generated explanation : Weight of a graphon during linear interpolation of graphons of two target classesInput: Trained GNN graph classifier model f(), Sets of graphs {Dc1, , Dck} labeled by f as belongingto classes {c1, , ck}, Graphon Estimator gProcedure: Generating explanations for class c: Estimate graphon Sci for class ci on K partitions and USVT graphon extimator using Algorithm 4. Sample nsamples from the estimated graphon using as described in Algorithm 4. Select the sample having the highest target class score as the explanation.",
  "G = argmaxGP(f(G) = c)(2)": "Assuming, we have an unknown joint distribution of graphs and class labels P(G, C), the optimization takesplace over the samples drawn from the class conditional distribution P(G|C = c) which we assume in linewith prior works (Wang & Shen, 2022; 2024) can be written as P(X|C = c)P(A|C = c), the product ofthe distribution of node feature matrices and adjacency matrices.In this work, the distribution of theadjacency matrices is approximated by the estimated graphon and the distribution of the node featurematrices is approximated by a probabilistic weighting mechanism on the pool of node features of graphs thatare classfied to a particular target class by f(.). It should be noted that there might exist multiple distinctmotifs G that maximize the equation 2. Such motifs shed light on the graph patterns learnt by the modelf(.) for the particular target class.Beyond the generation of explanations for individual target classes, Graphon-Explainer can also generategraphs that lie close to the decision boundary of two target classes. Formally stated, for a pair of classes c1,c2, the objective is to find motifs Gb such that",
  "Implementation Setting": "Graphon-Explainer is a model-agnostic method that can be used to explain any GNN classifier model. Itdoes not need access to hidden layers of the GNN classifier or knowledge of the process by which the GNNclassifier outputs a label for an input graph. Graphon-Explainer only assumes the ability to query the trainedGNN classifier for predictions. illustrates the workflow of Graphon-Explainer, which we will explainin detail below. We begin with a dataset of graphs, represented as D, wherein each graph is associated with a distinctclass label, denoted by c, with c taking on values within the set {1, , C}. Additionally, a trained GNNgraph classifier, represented as f(.), can be queried to predict the class of a given graph within this dataset.Consequently, when presented with a collection of graphs Dc D, identified by f(.) as belonging to aspecific target class c, Graphon-Explainer proceeds to generate explanations for the class c.Graphon-Explainer generates explanations in two steps. First, it estimates a graphon for the target class c using thegraphs in Dc. Second, it samples from the estimated graphon while optimizing the objective in Equation2 to generate graphs that exhibit the discriminative motifs characteristic of class c as identified by f(.).Further, when provided with datasets Dc1 and Dc2 of graphs that belong to two target classes c1 and c2according to f(.), Graphon-Explainer can generate examples that lie close to the decision boundary of thetwo classes. Generating examples that lie on the decision boundary involves estimating the graphon for eachclass from the corresponding datasets as a first step. This step is followed by taking a convex combinationof the graphons of both these classes to estimate the graphon of graphs that lie on the boundary. Finally,sampling from this graphon while optimizing the objective in Equation 3 yields examples that lie on thedecision boundary. We contain a combination of discriminative motifs present in classes c1 and c2.",
  "ann Bern(Sc(un, un))n, n {1, , N}(4)": "The first step samples N nodes from a uniform distribution on the unit interval . The second stepgenerates an adjacency matrix A = [ann] {0, 1}NN by sampling each entry from a Bernoulli distributionwith parameters defined using the estimated graphon Sc. Graphs sampled from Sc using this samplingprocess would contain discriminative motifs that are significant for the classifier f(.) to label the generatedgraphs as belonging to the target class c. The generation of node features of synthetic graphs is done intwo phases. During the graphon estimation phase, we align the original node features with the adjacencymatrices, resulting in a set of aligned node features for each graphon. We pool these aligned features andassign probabilistic weights to them depending on the their frequency of occurence. Node features are thensampled according to their probabilistic weight from the pool. The graph generation process is detailed inAlgorithm 4.",
  "Generation of Graphs near the Decision Boundary": "Generation of synthetic examples lying on the decision boundary of two target classes requires estimationof the graphon functions of the corresponding target classes as a first step. Given two target classes c1and c2 and their estimated graphons Sc1 and Sc2, we take a convex combination of these graphons : Sb =Sc1 + (1 )Sc2. Interpolating graphons of two target classes yields a graphon which is a mixture of thegraphons of both classes. Graphs can be sampled from Sb through the same sampling procedure as shownin Equation 4. The node features of the boundary motifs are generated by first sampling the node featuresfrom each graphon, as outlined in Algorithm 4. Next, the sampled features in each node feature matrixare reweighted according to the mixup parameter of each graphon. Finally, each node of the generatedgraph is resampled using the corresponding weights as probabilities. Graphs sampled from the interpolatedgraphon partially contain discriminative features of both the classes which places them close to the decisionboundary of the two target classes. In a multi-class classification problem, an evaluation is performed todetermine if two classes share a decision boundary. This process begins by randomly sampling N pairs ofgraphs from the two target classes. For each pair, the corresponding graph embeddings are extracted fromthe penultimate layer of the classifier, before the final linear layer that produces class-specific logits. Next,these embeddings are interpolated, and the classifiers output on the interpolated embeddings is analyzedto see if it crosses into the decision region of another class. A score is then calculated for each pair basedon this analysis. If the cumulative score across the K pairs exceeds a predefined threshold, the two targetclasses are considered adjacent, sharing a decision boundary. The pseudocode of this procedure is detailed",
  "Theoretical Analysis": "We give theoretical guarantees to demonstrate that graphs generated from a graphon that is a convexcombination of the estimated graphons of two target classes would contain discriminative motifs belongingto both classes. We have shown (in .5 ) how such graphons can be used to generate synthetic graphsthat lie close to the decision boundary of the two target classes. This validates the intuitive notion thatgraphs that lie on or close to the decision boundary of two target classes contain discriminative features ofboth classes. We first give a concrete definition of a discriminative motif and state a few basic assumptionson which the theoretical guarantees rely on.Definition A discriminative motif MG of graph G is the smallest subgraph of G that gives G its classidentity. In, other words MG is a subgraph with a minimal number of nodes and edges, which is essentialfor G to be identified as belonging to its corresponding class.Note that, if there exists a graph homomorphism from a motif MG to a graph G, then the motif MG exists ingraph G. Thus, the homomorphism density t(MG, G) gives a measure of the frequency of occurence of MGin G We assume that every graph belonging to a target class c contains a discriminative motif Mc. Further,we also assume that any collection of graphs Dc that has been identified by a classifier as belonging to aclass c contains a finite set of discriminative motifs Mc. The following theorem shows that the boundarygraphon partially contains discriminative motifs belonging to both the target classes.Theorem 5.1. Let Dc1 and Dc2 be two collections of graphs belonging to the target classes c1 and c2,respectively, with their estimated graphons Sc1 and Sc2. Define the boundary graphon as Sb = Sc1 + (1 )Sc2. For any discriminative motif Mc1 Mc1 and Mc2 Mc2, the difference in the homomorphism densityof Mc1 and Mc2 with the boundary graphon Sb compared to their respective graphons Sc1 and Sc2 is boundedabove by:",
  "Proof. The proof is detailed in Appendix D.1": "Theorem 5.1 establishes an upper bound on the difference in homomorphism densities between a discrimi-native motif of a target class and both the graphon of the target class and the boundary graphon. In otherwords, the difference between the frequency of occurence of a motif in the graphon of a target class and inthe boundary graphon is upper bounded. This upper bound is solely dependent on the hyperparameter ,since the values of |E(Mc1)| and |E(Mc2)| are constants determined by the discriminative motifs. Next, we present a theoretical guarantee that graphs generated from the graphon Sb will inherit the motifscontained in Sb.Theorem 5.2. Let Sb be the boundary graphon and assume that it contains a discriminative motif Mb.Then any random graph G on n nodes generated from Sb satisfies:",
  "Proof. The proof is detailed in Appendix D.3": "Theorem 5.2 demonstrates that the topology of a random graph sampled from the boundary graphon closelyresembles the topology of the boundary graphon itself, which includes discriminative motifs from bothclasses. Consequently, the theoretical analysis in this section confirms that graphs generated by the boundarygraphon Sb will contain discriminative motifs from both target classes. Further, a time complexity analysisand a theoretical guarantee of diversity in the generated explanations of Graphon-Explainer is provided inAppendix D.3 and D.2.",
  "Experimental Analysis": "We evaluate the explanation generation capabilities of Graphon-Explainer through meticulously designedexperiments using three real-world datasets and three synthetic datasets. Initially, we train a GNN classifieron each dataset and employ Graphon-Explainer to generate explanations for specific target classes andsynthetic motifs that lie close to the decision boundary between two target classes.",
  "Metrics for Quantitative Evaluation of Generated Explanations": "Unlike instance-level explanations, model-level explanations lack a corresponding ground truth because itsimpossible to determine the exact graph patterns the classifier has learned to distinguish between classes.For generating explanations for individual target classes, the objective is to maximize the class score for thetarget class when the generated explanation is provided as input to the classifier. For generating graphs thatlie on the decision boundary, the objective is to achieve a class score indicating that the generated graphsbelong to all classes with equal probability according to the classifier. With this goal in mind, we evaluateour method using the metrics established in the literature(Wang & Shen, 2022; Chen et al., 2024; Yuan et al.,2020) to assess model-level explanations. We report the performance based on the following metrics:Target Class Score: The target class score of a generated explanation represents the probability withwhich the explanation belongs to the target class, as determined by the classifier f(.). When generatingexplanations for a target class c, each generated explanation is inputted into f(.), and the probability forclass c is recorded. We report the mean and standard deviation for 50 generated explanations for eachtarget class.For synthetic graphs generated on the decision boundary, we report the class score of thegenerated graph across all classes.For target class explanations, a higher class score suggests that thegenerated explanation includes discriminative motifs identified by the classifier as belonging to the targetclass, resulting in the classifier confidently classifying the explanation into the target class. Conversely, forsynthetic motifs intended to lie close to the decision boundary, the goal is to achieve class scores that assignthe generated motif an equal probability of belonging to both the classes.Density: This metric assesses the sparsity of the generated explanations. In a graph G, density is calculatedas Density =|E||V |2 , where E represents the edge set and V denotes the vertex set of G.It is crucialthat explanations generated for a class include only edges that distinctly characterize that class and areessential for classification, thereby minimizing non-vital edges. Lower-density explanations facilitate clearerinterpretation for users, enhancing model transparency and trustworthiness. We provide the average densityand standard deviation of 50 explanation graphs for each target class.Time Efficiency: Following the evaluation setting in GNNInterpreter (Wang & Shen, 2022), we also notethe time taken to generate 10 explanations for each target class and report the mean time taken across alltarget classes in a particular dataset. We denote this using T10 in .",
  "(f, c1) =min(Gc1,Gb) (Gc1) (Gb)": "where (Gc1, Gb) represents a pair of graphs such that Gc1 belongs to class c1 by having the highest class scorefor class c1 according to f and Gb is on the boundary between class c1 and c2. Here, denotes the embeddingfunction of the GNN classifier f. A wider boundary margin implies better chance of generalization propertiesof the classifier since the risk of misclassification decreases during test time. In this paper, for the choice of we use the embedding of the last hidden layer, after which a linear layer is applied to get the class specificlogits. This ensures that we compute this measure with linearly separable embeddings. Boundary Thickness: A thicker boundary signifies greater robustness while a thinner boundary is moreprone to an adversarial attack. The boundary thickness (Yang et al., 2020) between a target class c1/c2 andthe decision boundary separating the two classes, can be defined for the class c1, as:",
  "I ( > (h(t))c1 (h(t))c2) dt": "where: h(t) = (1 t) (Gc1) + t (Gb) is the linear interpolation between embeddings, (h(t))c1 and(h(t))c2 are the probabilities for classes c1 and c2 respectively, I is the indicator function, is a threshold,commonly set to 0.75. Boundary Complexity: The boundary complexity(Guan & Loew, 2020) quantifies the intricacy of thedata and the classifiers decision-making process by assessing the entropy of the eigenvalue distribution fromthe covariance matrix of boundary embeddings. It is expressed as:",
  "Results": "We assess the performance of Graphon-Explainer both qualitatively and quantitatively. We also demonstratecomparative results by comparing our method with GNNInterpreter and XGNN. Our code repository isprovided at Results on all metrics are presented in for synthetic datasets and real datasets. An overarching trend observed across almost all datasetsis that Graphon-Explainer can generate explanations around 10 times faster than GNNInterpreter. presents the results for motifs generated near the decision boundary. The class scores clearly shows thatGraphon-Explainer produces boundary motifs with competitive, or even superior, boundary scores comparedto those generated by GNNBoundary. It should be noted that the decision boundary metrics in withrespect to the classifier are computed using the motifs generated by Graphon-Explainer. Qualitative resultsin a and 3b show the estimated graphon for each target class and the corresponding explanationgenerated by Graphon-Explainer and GNNInterpreter.Since XGNN, on most classes generates a singlenode or a line graph as explanations, the generated explanations are deferred to in the AppendixF. The boundary motifs generated by GNNBoundary is also deferred to the Appendix K. It can be observedthat the estimated graphons of each target class clearly look different. Hence, each class has a distinct motifbased on which the classifier categorizes them into separate classes.The visualization of the estimatedgraphons of different target classes in a and 3b gives added insight into the structure of graphscontained in various target classes and help make the generative model itself more interpretable.",
  "WindmillStar0.45113.8036.1400.5960.016860.4812 0.02320.4882 0.01960.8876 0.00210.1122 0.0460": "not only capable of generating discriminative motifs that closely align with the classifiers learned patternsbut does so more efficiently, producing sparser explanations in significantly less time than existing methods. The qualitative analysis in a further supports these findings, showing that the explanations generatedby Graphon-Explainer align well with the class identities when the classifier performs accurately, as seen withthe 2Shapes dataset. The classification task on this dataset is relatively straightforward for the classifier, asevidenced by the minimal boundary complexity shown in and the confusion matrix in . However, the 4Shapes dataset presents a more challenging scenario. While the explanations for the Barbell,Windmill, and Wheel classes may not perfectly match the class identities, they still achieve high class scores,suggesting a greater likelihood of misclassification for these classes.Analysis of the adjacency scores in and decision boundary metrics in reveals that within the 4Shapes dataset, the Windmilland Wheel classes share the thinnest boundary margins and the highest boundary complexity. Additionally,the Windmill class has a thinner boundary in its decision boundary with the Wheel class, indicating ahigher propensity for misclassification as Wheel rather than the other way around. This suggests that theembeddings of these classes are more closely aligned, making it more difficult for the classifier to distinguishbetween them, as reflected in both the confusion matrix in and the similarity of their explanationsin a. The highest boundary complexity among the synthetic datasets is observed for the BA2Motif dataset, likelydue to the more complex task of classifying random graphs based on the presence of a specific motifa prob-lem that is inherently more challenging due to the noise within these graphs. Moreover, graphs containinga House motif are more likely to be misclassified as Cycle, as indicated by the thinner boundary margin forthe House class. This finding is corroborated by the confusion matrix in . Finally, an analysis of themotifs generated near the decision boundary in a shows that motifs close to the boundary betweentwo target classes often represent a mixture of motifs from both classes.Real Datasets: The results in clearly highlight the superiority of Graphon-Explainer over existingmethods. A qualitative analysis of the explanations from b on the MUTAG dataset shows thatGraphon-Explainer can generate more realistic molecular features compared to GNNInterpreter. Its worthnoting that XGNN also performs well on this dataset, producing more realistic explanations than GNNIn-terpreter, as illustrated in . For the Mutagenic class, Graphon-Explainers explanations include thefused ring structure and the NO2 subgroupkey distinguishing features of this classwhereas these are ab-sent in other explanations. For the Non-Mutagenic class, no O atom is present in the explanations, aligningwith the class identity. An analysis of the boundary metrics from reveals that the Non-Mutagenicclass has a lesser boundary thickness, which increases the likelihood of misclassification of Non-Mutageniccompounds as Mutagenic. This observation is supported by the confusion matrix in . Moreover, theboundary complexity score is the lowest on this dataset compared to other real datasets, indicating that thedecision rules for class assignment on the MUTAG dataset are simpler.",
  "(b) Real Dataset": ": Qualitative results across all datasets. Graphon-Explainers explanations have class scores of 1,and boundary motifs score close to 0.5. The top-scoring GNNInterpreter explanations are also shown. Inthe approximated graphons, colors transition from dark to bright, reflecting values from 0 to 1, with brightercolors indicating higher values. node connected to several low-degree nodes, capturing the characteristic of a single user answering multiplequestionsa key feature of this class. In contrast, explanations for Class 1 (the Discussion class) show a fewusers frequently interacting among themselves, with other users on the fringe engaging with only a singleuser, typical of discussion groups. Boundary analysis from indicates that the boundary margin andthickness are both smaller for the Question-Answer class, increasing the risk of misclassification as belongingto the Discussion class. This finding is corroborated by the classifiers confusion matrix in . On the IMDB-B dataset, Graphon-Explainer comprehensively outperforms competing methods, as demon-strated in . Explanations for Class 0 (Action genre) in b feature the signature motif of multipleco-actors who have previously worked together, reflecting the typical structure of an Action movie with mul-tiple popular actors in key roles. In contrast, explanations for Class 1 (Romance genre) show a distinct motif",
  "Sensitivity Analysis": "in Appendix L illustrates how class scores vary with changes in Graphon-Explainers hyperpa-rameters as listed in Algorithm 1. For explanations of individual target classes, we adjust the number ofpartitions K used to approximate the graphon, which alters the number of nodes in the generated graphs.For graphs generated to belong close to the decision boundary, we vary the linear interpolation parameter and report the class score variations for a single class. We also detail the nsample hyperparameter used foreach dataset in in Appendix L. It can be concluded from the plots in and thatthe target class scores for explanations do not vary much with change in K, class scores for boundary motifsremain in the desirable range when is around 0.5 and nsamples used is reasonably low which makes ourmethod robust to hyperparameter changes.",
  "Conclusion": "This study introduces Graphon-Explainer, a model-level explanation approach for GNNs that leveragesgraphons, approximating one for each target class, to serve as generators for discriminative motifs identifiedby the GNN as indicative of each class. Unlike existing model-level techniques limited to generating expla-nations for individual target classes, Graphon-Explainer transcends this constraint by producing syntheticmotifs close to the decision boundary of two target classes. This capability not only elucidates what theGNN has learned for each class but also aids in delineating the decision boundary of the trained GNN.Through extensive experiments and theoretical analysis, we demonstrate that Graphon-Explainer faithfullyelucidates any GNN, highlighting its strengths and pitfalls in graph classification tasks. Notably, it achievesthis while generating diverse explanations significantly faster than current state-of-the-art methods. We believe Graphon-Explainer can be used in many real-world applications to inform users about the induc-tive biases and underlying classification logic of a GNN model deployed in practical scenarios. For example,our experiment on the MUTAG dataset demonstrates how the classifier identifies fused rings as key motifsfor mutagenic classes. Similarly, our method could explain a GNN classifying toxic and non-toxic drugs,revealing the basis on which the GNN classifies a molecule as toxic. Analyzing the decision boundary ofsuch a GNN could provide critical insights into the models robustness and the risk of misclassifying toxicmolecules as non-toxic. Furthermore, as demonstrated by additional experiments in Appendix H, even when two classifiers performaccurately, their interpretations of a single class can vary significantly based on their inductive biases. Thisinsight can help users choose between classifiers with equal accuracy based on their unique interpretativeproperties in real-world settings. Moreover, our approach can generate explanations tailored to the specificdataset a user possesses since estimated graphons effectively capture the datasets distribution. This person-alized approach offers tailored insights depending on the types of instances present in a users query dataset,rather than providing uniform explanations regardless of dataset variations. Overall, Graphon-Explainer not only advances our understanding of GNN behavior but also equips users witha powerful tool to assess, interpret, and trust the decisions made by GNNs in diverse application domains.",
  "Sourav Chatterjee. Matrix estimation by universal singular value thresholding. Annals of Statistics, 2015": "Jialin Chen, Shirley Wu, Abhijit Gupta, and Rex Ying. D4explainer: In-distribution explanations of graphneural network via discrete denoising diffusion. Advances in Neural Information Processing Systems, 36,2024. Jianbo Chen, Le Song, Martin Wainwright, and Michael Jordan. Learning to explain: An information-theoretic perspective on model interpretation. In International conference on machine learning, pp. 883892. PMLR, 2018. Lukas Faber, Amin K. Moghaddam, and Roger Wattenhofer. When comparing to ground truth is wrong: Onevaluating gnn explanation methods. In Proceedings of the 27th ACM SIGKDD conference on knowledgediscovery & data mining, pp. 332341, 2021.",
  "Qizhang Feng, Ninghao Liu, Fan Yang, Ruixiang Tang, Mengnan Du, and Xia Hu. Degree: Decompositionbased explanation for graph neural networks. arXiv preprint arXiv:2305.12895, 2023": "Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural messagepassing for quantum chemistry. In International conference on machine learning, pp. 12631272. PMLR,2017. Shuyue Guan and Murray Loew. Analysis of generalizability of deep neural networks based on the complexityof decision boundary. In 2020 19th IEEE international conference on machine learning and applications(ICMLA), pp. 101106. IEEE, 2020.",
  "Lszl Lovsz and Balzs Szegedy. Limits of dense graph sequences. Journal of Combinatorial Theory,Series B, 96(6):933957, 2006": "Ana Lucic, Maartje A Ter Hoeve, Gabriele Tolomei, Maarten De Rijke, and Fabrizio Silvestri.Cf-gnnexplainer: Counterfactual explanations for graph neural networks. In International Conference onArtificial Intelligence and Statistics, pp. 44994511. PMLR, 2022. Dongsheng Luo, Wei Cheng, Dongkuan Xu, Wenchao Yu, Bo Zong, Haifeng Chen, and Xiang Zhang. Pa-rameterized explainer for graph neural network. Advances in neural information processing systems, 33:1962019631, 2020.",
  "Yi Nian, Yurui Chang, Wei Jin, and Lu Lin. Globally interpretable graph learning via distribution matching.In Proceedings of the ACM on Web Conference 2024, pp. 9921002, 2024": "Phillip E Pope, Soheil Kolouri, Mohammad Rostami, Charles E Martin, and Heiko Hoffmann. Explainabilitymethods for graph convolutional neural networks. In Proceedings of the IEEE/CVF conference on computervision and pattern recognition, pp. 1077210781, 2019. Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. \" why should i trust you?\" explaining the predictionsof any classifier. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discoveryand data mining, pp. 11351144, 2016.",
  "Michael Sejr Schlichtkrull, Nicola De Cao, and Ivan Titov. Interpreting graph neural networks for nlp withdifferentiable edge masking. arXiv preprint arXiv:2010.00577, 2020": "Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, andDhruv Batra. Grad-cam: Visual explanations from deep networks via gradient-based localization. InProceedings of the IEEE international conference on computer vision, pp. 618626, 2017. Ana Vasilcoiu, T.H.F. Stessen, Thies Kersten, and Batu Helvacioglu. [re] GNNInterpreter: A probabilisticgenerative model-level explanation for graph neural networks. Transactions on Machine Learning Research,2024. ISSN 2835-8856. URL",
  "Shiwen Wu, Fei Sun, Wentao Zhang, Xu Xie, and Bin Cui. Graph neural networks in recommender systems:a survey. ACM Computing Surveys, 55(5):137, 2022": "Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and S Yu Philip. A comprehensivesurvey on graph neural networks. IEEE transactions on neural networks and learning systems, 32(1):424,2020. Hongteng Xu, Dixin Luo, Lawrence Carin, and Hongyuan Zha. Learning graphons via structured gromov-wasserstein barycenters. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp.1050510513, 2021. Yaoqing Yang, Rajiv Khanna, Yaodong Yu, Amir Gholami, Kurt Keutzer, Joseph E Gonzalez, KannanRamchandran, and Michael W Mahoney. Boundary thickness and robustness in learning models. Advancesin Neural Information Processing Systems, 33:62236234, 2020.",
  "Zhaoning Yu and Hongyang Gao. Mage: Model-level graph neural networks explanations via motif-basedgraph generation. arXiv preprint arXiv:2405.12519, 2024": "Hao Yuan, Jiliang Tang, Xia Hu, and Shuiwang Ji. Xgnn: Towards model-level explanations of graph neuralnetworks. In Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery &data mining, pp. 430438, 2020. Hao Yuan, Haiyang Yu, Jie Wang, Kang Li, and Shuiwang Ji. On explainability of graph neural networks viasubgraph explorations. In International conference on machine learning, pp. 1224112252. PMLR, 2021.",
  "BClassifier Architecture": "On all datasets except MUTAG, we implement a deep GCN architecture comprising three GCN layers, eachwith a dimension of 64. This is followed by a global mean pooling layer, a batch normalization layer, andfinally a linear layer to obtain the class logits. We utilize Leaky ReLU as the activation function betweenthe GCN layers and within the MLP. The model parameters are initialized using the Kaiming method.Training is conducted using the Adam optimizer (Kingma & Ba, 2014) with an initial learning rate of 0.01.Additionally, we use a learning rate scheduler to halve the learning rate every hundred epochs. On theMUTAG dataset, the model architecture consists of three graph convolutional layers designed to extractfeatures from input graphs. The first and third layers are Graph Convolutional Networks (GCN), while thesecond layer is a Graph Attention Network (GAT) with two attention heads, enabling the model to focus onimportant nodes and edges. A residual connection from the input features to the output of the third layerensures better gradient flow and feature retention. The model employs a Jumping Knowledge mechanismin concatenation mode, aggregating features from all three layers to capture hierarchical information. Theaggregated features are globally pooled, batch normalized, and passed through a dropout layer with a 0.5probability to prevent overfitting. Finally, these processed features are fed into a linear classifier that outputsthe class logits. This architecture is trained the same way as the above architectures.",
  "Algorithm 4 Estimating the Graphon of a Target Class and Generating Graphs from the Graphon": "Input: Set of graphs Dc belonging to class c, threshold for clipping singular values, desired number ofnodes N (where N must not exceed the maximum number of nodes in any graph from Dc)Init: List of sorted adjacency matrices A, List of sorted node features XOutput: Estimated graphon Sc for class c, Node feature of the generated graph Xc, Adjacency matrix ofthe generated graph AcProcedure: Align and Sort Nodes:",
  "Setting, F = Mb and S = Sb in Lemma D.4, the bound follows": "Theorem 5.2 demonstrates that the topology of a random graph sampled from the boundary graphon closelyresembles the topology of the boundary graphon itself, which includes discriminative motifs from both classes.Consequently, the theoretical analysis in this section confirms that graphs generated by the boundary graphonSb will contain discriminative motifs from both target classes.",
  "D.2Generation of Diverse Explanations": "The estimated graphon Sc of a target class c is used to generate explanations for the target class c. Whenthe graphon is approximated using a step function over K partitions of the unit interval, then Sc RKK,from which the adjacency matrix A is sampled as Aij BernScij. Consequently, the probability of generatingtwo identical graphs using this sampling scheme is Ki=1Kj=1((Scij)2 + (1 Scij)2) which is extremely smallsince Scij and K is a reasonably large number K is set to be the median number of nodes of graphscontained in the dataset D. This guarantees that Graphon-Explainer produces explanations that are distinctfrom each other.",
  "D.3Time Complexity Analysis": "The estimation of a graphon using the USVT method on g graphs, each with N nodes, and employing astep function with K partitions, has a time complexity of O(N 3) (Xu et al., 2021). After optimization,generating a single explanation from s samples of the estimated graphon requires O(sK) time for nodefeature generation and O(sK2) for edge generation, resulting in a total explanation generation complexityof O(sK2). This is more efficient than GNNInterpreter, which has a time complexity of O(TsK2) for a single explanation,where T is the number of iterations needed for convergence, and s represents the number of Monte Carlosamples. Notably, our method is non-iterative and does not require convergence for each explanation, asthe graphon is estimated once for a target class and then sampled from, making our approach significantlyfaster than GNNInterpreter. Moreover, the XGNN method exhibits a considerably higher time complexity of O(TRM 3), where T is thenumber of episodes, R is the number of rollout operations per episode step, and M is the number of edgesin the generated graph. Given that M typically exceeds K 1, XGNNs time complexity is substantiallyworse.",
  "Algorithm 5 Adjacency Score Computation": "Input: L: Last linear layer of the classifier that outputs class specific logits Embeddings Ec1: Set of embeddings for class c1. Embeddings Ec2: Set of embeddings for class c2. Class indices c1, c2: Indices of the two target classes for adjacency check. Number of samples N: Number of samples to draw from each list for checking. Number of interpolation steps T: Number of steps for linear interpolation.Procedure: Initialize adjacency score adjscore 0 for i = 1 to N:",
  ": Explanations on the Barbell Class of the 4Shapes dataset for 2 different classifier architectures": "In this section, we demonstrate how different two classifiers are in their intrinsic interpretation of a targetclass and their decision boundaries even in the scenario when they classify a dataset almost perfectly andeven in the case when they have a 100% accuracy on some classes in the dataset. For this, we use theprevious GCN architecture detailed in Appendix B and a GIN of the same architecture on the 4Shapesdataset.",
  ": Adjacency Scores on 4Shapes for the GIN model": "As seen in , even though both the classifier have a 100% accuracy on the Barbell class and thelearned graphon generative model is the same, sampling using equation 2 yields very different explanations.We find that the GCN model identifies Barbell using the branch like structures of the Barbell while theGIN model distinguishes the barbell class using the head like structure of the barbell. Also, as shown in and different pairs of classes share decision boundaries for both models as their inherentgraph embeddings are different. Further, even when a pair of common classes share a decision boundary,the boundary metrics are very different( demonstrated in and 5) like in the case of Star and Barbellwhere the boundary thickness of Star is greater for the GIN and the thickness for Barbell is greater forthe GCN architecture. However, they also share certain common grounds like the boundary complexity forthe Windmill-Wheel pair is the highest for both the models indicating that separating these two classes issignificantly harder. This is also validated by the confusion matrices of both the models in .",
  "JComparative Results with other Metrics and Methods": "In this section, we compare the performance of Graphon-Explainer with GDM(Nian et al., 2024) on threedatasets. We report class score averages for 10 explanations generated per class on the dataset as a wholeas reported by the authors of GDM in . Further, we also adopt a metric proposed by the authorscalled model fidelity. Model fidelity demonstrates the usefulness of the generated explanations by traininga surrogate model on the explanations having a similar structure as the classifier and measuring the ratioof cases on the test set where the surrogate model gives the same decision as the original classifier. Asshown in and Graphon-Explainer performs better or equally on all except on one metric onone dataset. also shows the explanations and graphons generated by Graphon-Explainer on theSHAPE dataset.",
  "L.2Sensitivity Analysis on and Number of Nodes": "outlines the variation in class scores for the explanation graphs with the number of nodes for allthe target classes and with varying mixup parameter on the boundary classes. On all the datasets forscores reported in , we choose K to be the median number of nodes in the dataset and is variedfrom 0.4 to 0.6 for generation of the boundary motifs, and the yielding the best score is chosen."
}