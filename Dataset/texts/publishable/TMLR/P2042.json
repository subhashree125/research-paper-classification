{
  "Abstract": "Deep convolutional neural networks (CNNs) have achieved impressive performance in manycomputer vision tasks.However, their large model sizes require heavy computationalresources, making pruning redundant filters from existing pre-trained CNNs an essential taskin developing efficient models for resource-constrained devices. Whole-network filter pruningalgorithms prune varying fractions of filters from each layer, hence providing greater flexibility.State-of-the-art whole-network pruning methods are either computationally expensive due tothe need to calculate the loss for each pruned filter using a training dataset, or use variousheuristic / learned criteria for determining the pruning fractions for each layer. Hence thereis a need for a simple and efficient technique for whole network pruning. This paper proposesa two-level hierarchical approach for whole-network filter pruning which is efficient and usesthe classification loss as the final criterion. The lower-level algorithm (called filter-pruning)uses a sparse-approximation formulation based on linear approximation of filter weights. Weexplore two algorithms: orthogonal matching pursuit-based greedy selection and a greedybackward pruning approach. The backward pruning algorithm uses a novel closed-form errorcriterion for efficiently selecting the optimal filter at each stage, thus making the wholealgorithm much faster. The higher-level algorithm (called layer-selection) greedily selectsthe best-pruned layer (pruning using the filter-selection algorithm) using a global pruningcriterion. We propose algorithms for two different global-pruning criteria: (1) layerwise-relative error (HBGS), and (2) final classification error (HBGTS). Our suite of algorithmsoutperforms state-of-the-art pruning methods on ResNet18, ResNet32, ResNet56, VGG16,and ResNext101. Our method reduces the RAM requirement for ResNext101 from 7.6 GBto 1.5 GB and achieves a 94% reduction in FLOPS without losing accuracy on CIFAR-10.",
  "Introduction": "Convolutional neural networks (CNNs) have demonstrated remarkable performance across various applications,such as image classification (Han et al., 2016), object detection (Redmon et al., 2016), and image segmentation(Minaee et al., 2021). However, the deployment of CNNs on IoT devices for computer vision tasks oftenencounters practical bottlenecks related to the model size and computational complexity of inference (FLOPs)(Goel et al., 2020). While neural architecture search (Baker et al., 2017; Zoph & Le, 2017) and efficientmodel design (Tan & Le, 2019) can sometimes lead to highly efficient architectures, they impose substantialrequirements in terms of data and computational cost, as well as research expertise. However, pruning ofpre-trained models (Lebedev & Lempitsky, 2018; Hoefler et al., 2021; Vadera & Ameen, 2022; He & Xiao,2023) provides a cheaper alternative where one can avoid re-training complicated models on large datasets.For CNNs, structured pruning or filter-pruning (FP) (He et al., 2017; Luo et al., 2017; He & Xiao, 2023) hasemerged as a preferred alternative since it causes a reduction in computation (thus leading to power savings)as well as memory requirements without requiring special hardware or re-implementation of operations.",
  ": Hierarchical approach for non-uniform pruning of filters across the network": "Filter-pruning (FP) techniques can further be classified as (1) layer-wise pruning, which prune filters uniformlyfrom each layer (e.g. score-propagation (Yu et al., 2018) and error in activation reconstruction (Luo et al.,2017)), and (2) whole-network pruning (WNP), which prunes filters from the entire network. The WNPapproach can prune different fractions of filters from each layer, hence providing higher flexibility. Animportant challenge for WNP is to determine the pruning fractions for each layer. (Kuzmin et al., 2019)(section 3.4.1) calculates the accuracy by pruning individual layers to a varying fraction and finds an optimalcompromise so that the overall pruning ratio is achieved while minimizing the maximum loss in accuracyper layer. The main disadvantage of this approach is that the effect of pruning one layer on the pruning ofanother layer is not captured. Recent works also include methods based on Taylor-series expansion of the lossfunction, which approximates the influence score of pruning a filter on the overall loss function (Wang et al.,2019; Molchanov et al., 2019; Peng et al., 2019; Nonnenmacher et al., 2021) with good practical performance(He & Xiao, 2023). However, these methods can be expensive for large networks as they require passing overthe entire training set to calculate each influence score, which can be costly for large datasets. Additionally,(Dong & Yang, 2019) applied NAS to search for a network with flexible channel and layer sizes, but thismethod can also be expensive for larger networks. On the other hand, some recent works use approximatecriteria to prune filters. For instance, (Murti et al., 2023) propose a discriminative pruning technique based ontotal variation separation distance (TVS), which is an approximate criterion to prune filters from a network.Similarly, (He et al., 2020) choose different criteria to prune filters from different layers using Gumbel-softmax.However, the main drawback of this procedure is that the Gumbel-softmax smoothing only calculates anapproximate output feature map for each layer, thus potentially hurting overall performance. Therefore, thereis a need for an efficient and accurate WNP technique that directly optimizes the training data loss. In this work, we propose a greedy hierarchical training data loss-based approach for whole-network filterpruning (see fig. 1). The iterative higher-level algorithm (called layer-selection) evaluates all layers based onoutputs from the lower-level algorithm, and greedily selects a layer to prune filters from in each iteration.The lower-level algorithm (called filter-pruning) prunes filters optimally for the current network configuration.We propose two versions of the iterative layer-selection algorithm: (1) hierarchical backward greedysearch (HBGS), which selects layers based on the relative reconstruction error of the layer outputs, and (2)hierarchical backward greedy tree search (HBGTS) which selects the layers based on the error of the finalclassification layer outputs. The key advantage of our greedy layer-selection, compared to a learned criterion(He et al., 2020), or a threshold-based criterion (Kuzmin et al., 2019) is that we utilize the activations fromthe modified network, which arguably leads to better decisions after a few iterations of pruning. However,since each iteration of the greedy layer-selection makes many calls to the filter-pruning algorithm (typically,the number of layers many calls, with some possibility of caching old results), an expensive filter-pruningalgorithm would be impractical for large networks. A key contribution of this paper is to propose an efficient filter-pruning algorithms, which can ensure thefeasibility of the overall hierarchical scheme for large networks. While LRF (Joo et al., 2021) demonstratedimpressive practical performance for pruning filters, it only prunes one filter at-a-time, hence making itprohibitively expensive for our purpose. We formulate the problem of optimally pruning multiple filtersfrom a layer using linear replaceability criteria as a sparse approximation problem. We study an orthogonalmatching pursuit (OMP) (Tropp & Gilbert, 2007) based algorithm, FP-OMP (Purohit et al., 2023) forfilter-pruning. Under the assumption of restricted isometry of the matrix composed of filter weights (Tropp& Gilbert, 2007), FP-OMP selects filters whose linear combinations can represent the pruned filters with",
  "Published in Transactions on Machine Learning Research (08/2024)": "selection algorithm for T pruning steps becomes O(TC2N). Hence, using the backward elimination strategy,the overall time complexity for the proposed algorithm HBGTS-B is O(TC2N + TCn2). Comparatively, thevanilla backward search version of the algorithm, HBGS-B using backward elimination for filter pruning takesO(TC2N + TCn2) time. However, as shown in the next section, HBGTS-B marginally outperforms HBGS-B interms of the accuracy of the pruned models for a given pruning ratio.",
  "Related Work": "Many pruning methods have been proposed in the literature. (Hoefler et al., 2021; Vadera & Ameen, 2022)provide excellent surveys for pruning techniques. Pruning can be categorised two types: unstructured pruning,involving the removal of individual weights (Han et al., 2015), and structured pruning or filter-pruning (FP),in which entire nodes or channels are removed (He et al., 2017; Luo et al., 2017; He & Xiao, 2023). Structuredpruning provides efficiently implementable models on a wide range of accelerator devices e.g. GPUs. (He &Xiao, 2023) provides a recent survey and website for comparing structured pruning techniques for CNNs.Pruning can be done on a pre-trained model or from scratch, which is costly and requires large training data.Therefore we focus on pruning a pre-trained model. We further categorise it into the following groups:Weight-Based Pruning - Weights of filters are examined to determine which ones are essential for themodels performance. These methods do not require input data. (Han et al., 2015) focused on eliminatingsmall-norm weights. (He et al., 2019) incorporates geometric median (Fletcher et al., 2008) to estimate theimportance of each filter. (Joo et al., 2021) prunes linearly replaceable filters.Activation-Based Pruning - Rather than relying on filter weights, these methods utilize activation maps orlayer outputs to make pruning decisions. We can utilize information from activation maps at the current layeror all layers/whole-network. Some of the current layer activation-based pruning methods are: CP (He et al.,2017) which focuses on minimizing the reconstruction error of sparse activation maps, while HRank (Lin et al.,2020) calculates the average rank of activation maps. CHIP (Sui et al., 2021) assesses cross-channel correlationto evaluate channel importance. ThiNet (Luo et al., 2017; El Halabi et al., 2022) approximates activationmaps of layer l+1 using subsets of layer ls activation maps. Current layer activation-based pruning methodsdo not consider the reconstruction error propagation. Some of the all layers/whole-network activation-basedpruning methods are: NISP (Yu et al., 2018) which assesses the Final Response Layer to calculate the neuronimportance, while DCP (Zhuang et al., 2018) aims to retain discriminative channels. Layer-output basedmethods are computationally expensive, since they need to compute the outputs using a training dataset.Regularization - Regularization can aid in learning structured sparse networks by incorporating varioussparsity regularizers. These regularizers can be implemented on Batch Normalization (BN) parameters (Liuet al., 2017; You et al., 2019; Kang & Han, 2020), with extra parameters (Huang & Wang, 2018; Lin et al.,2019) and filters (Wen et al., 2016; Chen et al., 2021).",
  "A Hierarchical Greedy Approach to Filter Pruning": "We propose a Hierarchical scheme, HBGS/HBGTS for non-uniform filter pruning from a pre-trained CNN. Asshown in , the proposed scheme operates in a two-level hierarchical manner: (1) filter pruning - at thelower level, this step identifies the most appropriate filters to be pruned from each layer and (2) layer selection -at a higher level this step selects the best layer to currently prune from. These two steps are applied iterativelyto achieve a non-uniform pruning from the whole network. We first describe our sparse approximation-basedformulation for optimal filter pruning from each layer, and then describe a faster backward elimination-basedalgorithm for the same. For layer selection, we describe a layerwise-regression-based backward greedy searchstrategy. We also incorporate an overall error-based strategy for layer selection.",
  "Sparse Approximation for Filter Pruning": "A convolutional filter used in deep CNN is denoted by a K K matrix. A convolutional layer is defined asfilter weights fi,j RK2, where i = 1, ..., m and j = 1, ..., n are the number of input and output channels.Given the input feature map with m channels X = {X1, ..., Xm}, the output feature map with n-channelsY = {Y1, ..., Yn}, can be calculated as: Yj = mi=1 Xi fi,j := X f:,j Here, denotes the convolution operation, and f:,j RK2m denotes all the filter weights for output channelj. For brevity, we describe the algorithm for output channel pruning throughout the paper. Input channelpruning can be performed analogously. For channel pruning, we follow the idea of linearly replaceable filters(LRF) introduced in (Joo et al., 2021), which states that any filter f:,j RK2m can be pruned if the filterweights can be expressed as a linear combination of other filter weights of the same layer which are notpruned. Note that, for linear approximation of a filter with respect to other filters of the same layer, we treatthe filter weights, f:,j, as a flat K2m-dimensional vector. For LRF, we prune the channel j such that j isminimum, where f:,j =",
  ": end while21: Output: Pruned filters F tc c = 1, ..., C": "where n is the initial number of output channels in the current layer, and pruning ratio , is the fraction ofchannels that are pruned. Algorithm 1 describes an orthogonal matching pursuit (OMP) based approximation(Tropp & Gilbert, 2007; Cai & Wang, 2011) for estimating the S, j,l j = 1, ..., n; l S. Note that, equation2 denotes a multi-variate regression version of the sparse approximation problem where the predicted variableis a vector f:,j, j = 1, ..., n with corresponding independent parameters j,:. Since the total error is the sumof squared errors of the individual components, it is easy to see that projections used in standard OMPalgorithm can be replaced with the sum of projections for each multivariate regression component (line 13of Algorithm 1). This approach has two advantages: (1) this approach is much faster than LRF since thefine-tuning is performed once for each layer, whereas in LRF it is performed after every pruning step (whichis equal to the number of pruned filters in a layer), and (2) this approach provides an optimality guaranteefor the selected filters in terms of reconstruction error, under conditions on the incoherence matrix of thefeatures (Cai & Wang, 2011). The overall time complexity of algorithm 1 is O(|S|n3). In a normal applicationscenario of uniform pruning, the pruning fraction may be quite high ( 98%), resulting in the size of theselected set |S| being much smaller than n, this algorithm is fast (O(n3)). LRF also uses a 1 1 convolution layer gj,k, j, k = 1, ..., n to compensate for the loss of channel outputs.The modified output feature map, Zk, k = 1, ..., n is given by Zk = nj=1 Yj gj,k := nj=1 X f:,j gj,k,when the output filters Yj, j = 1, ..., n are not pruned. However, after pruning, the output feature mapfrom the original convolutional layer becomes Y j = lS X f:,l. Weight compensation is a method formodifying weights for the 1 1 convolutional layer, gl,k, l S, k = 1, ..., n such that the final predicted outputZk = lS X f:,l gl,k matches Zk. The following result provides a formula for calculating gl,k.",
  "Hierarchical Backward Greedy Search (HBGS)": "The algorithm outlined in the previous section selects a fixed fraction (1 ) of filters from each layer.However, as shown in the results, each layer can have a different fraction of important filters, dependingon the architecture. Hence, determining the fraction of filters c to be pruned in layer c is an importantproblem. Intuitively, c should not be determined by the filter-weights since comparing them across layers isnot meaningful. For example, the weights in a layer may be scaled by a constant factor, compared to those inanother layer. Hence, we use reconstruction error of filter outputs using input training dataset as the criteria. Let D = {(u1, v1), ..., (uN, vN)} be the training dataset, and yc(i) be the output feature map of layer c when thetraining datapoint (ui, vi) is input to the CNN. Also, let Uc(i) be the output of the cth layer when the trainingdatapoint (ui, vi) is input to the unpruned CNN. Moreover, let Fc = { lSc(f c:,l gcl,k), k = 1, ..., nc} bethe composite convolutional map of the pruned filters and 1 1 convolution for layer c obtained from a filterpruning method (e.g. FP-OMP described in the previous section). The relative reconstruction error ec forlayer c is given by: ec =",
  "(ui,vi)D||Uc(i)Fcyc1(i)||2": "||Uc(i)||2. We propose a hierarchical backward greedy search(HBGS) technique in algorithm 2 to both estimate c for each layer c, as well as select the appropriate filtersfrom each layer. Given the number of filters to be pruned in one go, the algorithm proceeds iteratively byperforming two broad steps in each iteration: (1) determine the optimal filters to be pruned in each layer c,and (2) calculate the total relative reconstruction error ec as described above. Finally, the model parametersare updated to prune filters from the layer that leads to the lowest relative reconstruction error. Algorithm2 line 9 describes the first step, and lines 10 - 15 describe an efficient implementation of the second step,where errors for all the layers are computed in one forward pass per example. The iterations continue tillan overall pruning criterion, e.g. parameter pruning ratio or percentage FLOP reduction is reached. Theparameter is chosen to speed up the overall execution and can be chosen as 1 if the running time is not aconcern. The overall time complexity of algorithm 2, when using algorithm 1 as the filter pruning algorithmis: O(TC(N + n4)), where T is the number of iterations needed to achieve the desired pruning (depends on and the pruning criteria), C is the number of layers, N is the training dataset size, and n is the number offilters in each layer. While HBGS (Algorithm 2) can select a variable number of filters from each layer, thesequential search over the layers for the best filter renders this algorithm expensive. In the next section, wedevelop a faster filter pruning algorithm.",
  "Backward Elimination Algorithm for Filter Pruning": "The time complexity of the HBGS algorithm depends on the training set size N and the average numberof filters per layer n. In many cases, when the time complexity of the filter pruning step (O(TCn4) issubstantially larger than the error computation step O(TCN), the complexity of the filter pruning algorithmbecomes substantially larger than that of the fine-tuning algorithm on the training dataset. The main problemis the OMP-based filter pruning algorithm (FP-OMP) adds one filter in each step, which is an efficientstrategy if the number of filters to be selected is small, compared to the total number of filters. However,in the context of HBGS algorithm, FP-OMP is sequentially called many times (Algorithm 2 line-9) withdecreasing number of filters to be selected each time. In this context, a backward elimination (Couvreur &Bresler, 2000; Ament & Gomes, 2021) based approach which iteratively removes the feature which causesa minimal increase in approximation error, is intuitively more appropriate. While the original backwardelimination algorithm described in (Couvreur & Bresler, 2000) is O(n4), a faster implementation based onblock-matrix inversion was described in (Reeves, 1999), with time complexity of O(n3). Here, we derive asimilar algorithm for our problem. For simplicity, we follow the notation in (Reeves, 1999). For a given layer with n output channels, m inputchannels, and K K filter size, we re-define the input filter matrix as A RK2mn, where each columnis a flattened vector of filter weights, A:,j = f:,j, j = 1, ..., n. We also denote the output of the sparseapproximation as B RK2mn, which is the same as A in this case. We are interested in the approximationB A, Rnn, where :j is the weight vector for the jth output filter. We note that the least square",
  "j=1,...,n1k |dTk B:,j|2": "This result is a generalization of the result reported in (Reeves, 1999). For conciseness, we provide thederivation of this result in the appendix. In light of the above result, FP-Backward (algorithm 3) provides thesteps for backward elimination-based filter pruning. Note that line 7 in algorithm 3 is the most expensive stepin the while loop (lines 6 - 17), which can be estimated from G in the previous time-step using block matrixinversion with the worst-case complexity of O(n2). Also, for most calls to the algorithm, the parameter is very low (typically 0.05), leading to far fewer iterations of the while loop (lines 6 - 17), which can be",
  ": Output: Pruned filters F tc, c = 1, ..., C": "assumed to be constant. Moreover, this cost goes down with iterations as the size of the G matrix reducessignificantly with the iterations, reaching only a small fraction for n. Hence, assuming a constant numberof loop executions (lines 6 - 17), the overall complexity of Algorithm 3 is O(n2), which is two orders ofmagnitude improvement over using algorithm 1.",
  "Hierarchical Backward Greedy Tree Search (HBGTS)": "A key idea behind the hierarchical backward greedy search (HBGS) algorithm is to select the layer whichresults in a minimal relative error when pruned from. However, the prediction error of a layers output isnot always indicative of the ultimate predictive performance of the overall network. On the other hand,computing the error of the final network output involves the re-computation of changes through all thedownstream layers, starting with the pruned layer. A naive implementation of this can lead to significantcompute overhead since it requires O(CN) forward inferences through the network for each pruning step,where C is the number of layers and N is the number of data points in the training set. Algorithm 4 presents hierarchical backward greedy tree search (HBGTS), an efficient implementation fordetermining the best layer to prune from at every stage. A key idea here is to calculate the error in final layeroutput etj, when layer j {1, ..., C} is pruned, for each input training example. Hence, we need to performonly one forward pass per pruning step for each example i {1, ..., N}. To implement this algorithm, weutilize a data structure ytc,j which stores the output of cth layer c = 1, ..., C, when the (n j + 1)th layer ispruned j = 1, ..., C, in the tth pruning step. Here, ytc,0 represents the output of the cth layer when no filterhas been pruned. There are 3 cases while calculating ytc,: from ytc1,: (lines 9, 10, and 12 in algorithm 4): (1)calculation of next unpruned output ytc,0 (unpruned propagation of unpruned previous layers), (2) calculationof output corresponding to the current pruned layer ytc,1 (pruned propagation of unpruned previous layers),and (3) unpruned propagation of the pruned outputs corresponding to all the previous layers. Here, weonly need to store ytc,j for the current timestamp t. Hence the space complexity of the modified algorithmincreases only by O(nd) where d is the output size for each layer. The overall time complexity of the layer",
  "Experimental setting": "Dataset Description: For the image classification task, we utilize three datasets: CIFAR10, CIFAR100,and Tiny-Imagenet. CIFAR10 consists of 10 classes, with a training set of 50k images and a test set of 10kimages, all with a resolution of 32 32. Each class contains 5k training images and 1k test images. Similarly,CIFAR100 comprises 100 classes, with 500 training images and 100 test images per class. Tiny-Imagenetcontains 200 classes and includes a total of 0.1 M images. For our experimentation purpose, we resize theoriginal 64 64 images of Tiny-Imagenet to 224 224. Training Details: Our experiments involve ResNet18, ResNet32, ResNet56, VGG16, and ResNext models,with various percentages of parameter reduction. We prune a pre-trained model and adopt the trainingsettings from LRF (Joo et al., 2021). In contrast to LRF, where the model is fine-tuned for a single epochafter each filter removal, we fine-tune the model after pruning the entire fraction of filters from each layer.After the completion of pruning for the entire model, we fine-tune the pruned model for 300 epochs. Forfine-tuning, we set the initial learning rate to 1e2 with a decay rate of 1e4. We use a step scheduler thatreduces the learning rate by a factor of 10 at epoch 150. Baselines were implemented using code provided bythe authors and the recommended hyperparameters were used. We also performed hyperparameter search forthe number of epochs, pruning ratios, and learning rates and reproduced the best results. Performance Metric: We report the test accuracy for various pruning methods. The dense models testaccuracy corresponds to the pre-trained models accuracy. Additionally, we report an accuracy drop (Acc )from the dense model. We also report the drop in parameters (param ) and FLOPs (FLOPs ) as metricsto assess the level of pruning and model efficiency. The reduction in parameters refers to the decrease in thenumber of parameters/weights across all retained filters. FLOPs, on the other hand, refer to the number ofoperations (convolutions), within the retained filters.",
  "Performance Comparison: Accuracy and Efficiency": "We compare our proposed pruning methods with the state-of-the-art methods in . We observe thatour proposed methods (HBGS, HBGTS, FP-Backward, HBGS-B, and HBGTS-B) exhibit higher pruned accuracycompared to state-of-the-art methods for a comparable drop in the number of parameters. We also observethat our proposed methods consistently report a higher drop in FLOPs compared to state-of-the-art methods. ResNet and VGG on CIFAR-100 and Tiny-Imagenet: and provide further insightsinto the consistently superior performance of our proposed methods. We observe that the test accuracy ofthe proposed methods is consistently and significantly better than the test accuracy of the baseline methods.The fact that this difference is more pronounced in a difficult dataset (CIFAR100 and Tiny-Imagenet) furtherdemonstrates the superiority of the proposed methods. From , we notice that, as the percentage ofparameter reduction increases, the difference in test accuracy between our proposed methods and state-of-the-art methods also grows. At higher parameter reduction (90%), the proposed methods outperformexisting methods by 3 8% (see ). Maintaining or even improving accuracy at such high parameterreduction (>95%) is very valuable, further highlighting the effectiveness of the proposed methods.",
  "Test Acc(%)Acc (%)Param (%)FLOPs (%)": "Dense69.18 0.010 0--68.48 0.020 0--Random52.64 0.1816.54 0.1897.9087.3258.61 0.169.87 0.1697.4586.90LFPC (He et al., 2020)62.83 0.146.35 0.1497.2588.1861.78 0.136.70 0.1397.1387.67DAIS (Guan et al., 2022)61.23 0.167.95 0.1697.4988.3660.34 0.158.14 0.1597.2588.12GCNP (Jiang et al., 2022)62.71 0.156.47 0.1597.3788.42----DLRFC (He et al., 2022)62.13 0.197.05 0.1997.5188.73----LRF (Joo et al., 2021)63.73 0.255.45 0.2597.8488.9863.17 0.215.31 0.2197.4388.56FP-Backward67.66 0.101.52 0.1098.1289.2466.92 0.071.56 0.0797.8789.13HBGS68.99 0.150.19 0.1598.3489.4168.25 0.140.23 0.1498.2889.35HBGS-B68.82 0.130.36 0.1398.2789.3268.13 0.110.35 0.1198.2389.31HBGTS69.52 0.24-0.34 0.2498.6189.7468.65 0.19-0.17 0.1998.4689.71HBGTS-B69.39 0.220.21 0.2298.5789.6868.59 0.160.11 0.1698.3889.63",
  ": Test accuracy for (a) ResNet56/CIFAR100 (b) VGG16/CIFAR100 and (c) ResNet18/Tiny-Imagenet with increasing parameter reduction": "To prune a Large Model:Our backward greedy search methods can be used for effectively pruninglarge models that exceed the capacity of commodity GPUs. We use ResNext101 32x16d as our large model,consisting of 193 M parameters and requires 7.62 GB of GPU memory for loading. Additionally, we useResNext101 32x8d as our smaller dense model, which has 88 M parameters and requires 3.91 GB for GPUmemory. shows that when ResNext101 32x16d pruned to 98% parameter reduction using HBGTS-B,achieves a test accuracy that matches its dense counterpart. Hence, we can efficiently deploy the prunedmodel on edge devices with GPU memory less than 2GB. Furthermore, the pruned model takes 5.04 timesless GPU memory than the larger dense model. Notably, the pruned model even outperforms the smallerdense model, ResNext101 32x8d.",
  "Dense RN1692.10--7.62Dense RN891.80--3.91FP-Backward92.9-0.898.589.91.59HBGS-B93.0-0.998.792.11.55HBGTS-B93.2-1.198.894.31.51": "Time Comparison: (a) provides a comparison of uniform pruning methods in terms of pruningtimes. We can see that our proposed method FP-Backward is faster than the best baseline FP-OMP by afactor of 2 for a constant pruning ratio in each layer. This is also a fair comparison since the baseline methodsalso prune a constant fraction of filters from each layer. (b) compares the pruning times of various non-uniform pruning methods. Our proposed methods(HBGS, HBGS-B, HBGTS, and HBGTS-B) demonstrate superior computational efficiency compared to the baselineEarlyCroP-S, a non-uniform pruning method. Notably, HBGS and HBGTS exhibit higher pruning times relativeto HBGS-B and HBGTS-B. Specifically, HBGS-B achieves a 54.40% and 56.16% reduction in pruning timecompared to HBGS for ResNet32 and ResNet56, respectively. Likewise, HBGTS-B shows a 55.03% and 57.58%reduction in pruning time compared to HBGTS for ResNet32 and ResNet56, underscoring the effectiveness ofthe backward elimination strategy. Further, as expected HBGTS is computationally more expensive comparedto HBGS with approximately double the time. The most efficient hierarchical pruning method (HBGS-B) takes5 hours for ResNet32 (see (b)) (for = 5, number of filters removed in each round) compared to 1hour taken by FP-OMP. The increase in time can be further reduced by pruning a higher number of filters() in each round.",
  "Analysis of Backward Greedy Search Algorithms": "We analyze the working of the proposed greedy search methods in terms of their pruning quality. illustrates a heat map showcasing the relative reconstruction error and the percentage of removed filtersfor each layer across the pruning rounds, using HBGTS-B method for ResNet32 on the CIFAR-100 datasetat 63% parameter reduction. The relative reconstruction error is calculated as||ytC,0ytC,c||2 ||ytC,0||2where, ytC,0 isthe output from the final classification layer when no pruning was done in any of the layers of the networkand ytC,c is the output from the final classification layer when pruning was done at layer c. Both the relativereconstruction error and the pruning percentage are depicted after every 7th round, each pruning 5 filters.Examining , we observe that the pruning percentage increases with each round, but not uniformly.For example, layers 14 - 18 have higher pruning compared to layers 1-3. Relative reconstruction error alsodecreases with pruning rounds but is not uniform across layers. From the heat maps, it is evident that ourmethod selects the layer with the least relative reconstruction error for pruning. For example, layers 14 -18 have moderate relative reconstruction errors in the initial pruning rounds, so the pruning percentage isalso not so high for the same. As the pruning rounds increase, the relative reconstruction error decreases forlayers 14 - 18 and hence more pruning is done from those layers as visible in the latter rounds in .This is in contrast to uniform pruning approaches, where pruning is uniformly applied across each layer. To understand the intuition behind the filter choices for pruning using HBGTS-B, we present a visualizationdiagram of feature maps for two layers: Layer 2 (pruned by 31.25%) and Layer 10 (pruned by 93.75%). Byexamining feature maps in (top row), we can observe that, Layer 2 exhibits a diverse range of",
  "% pruned": "filter outputs, indicating their effectiveness in capturing various input features. Consequently, our proposedmethod prunes only 31.25% of the filters in Layer 2 (as shown in the last column of pruning percentages in). Similarly, (bottom row) displays feature map outputs from Layer 10, which appear verysimilar, indicating redundancy in filter outputs. This observation aligns with the pruning percentages shownin the last column of , where Layer 10 has 93.75% of its filters removed. Thus, we can conclude thatpruning percentages yielded by HBGTS-B are indicative of the amount of information carried by each filter ineach layer. Filters with more diverse outputs are retained, while those with redundant outputs are pruned.",
  "Conclusion": "In this paper, we propose HBGS and HBGTS as pruning methods that optimize deep neural networks byreducing the number of parameters while maintaining or even improving their accuracy. The proposedpruning methods are based on sparse approximation for non-uniform pruning, enabling the removal ofredundant filters regardless of the layer they are present in. We demonstrate our methods consistentperformance across various architectures, kernel sizes, and block types through extensive experiments onvarious datasets. We also propose HBGS-B, and HBGTS-B as efficient filter pruning methods, which offersignificant advantages in terms of time efficiency compared to HBGS, and HBGTS. The proposed method canin principle be applied to any linear approximation-based pruning technique, one way of applying it totransformer-based models is pruning the filters in the feed-forward network (FFN). Sebastian Ament and Carla Gomes. On the optimality of backward regression: Sparse recovery and subsetselection. In ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing(ICASSP), pp. 55995603. IEEE, 2021.",
  "T Tony Cai and Lie Wang. Orthogonal matching pursuit for sparse signal recovery with noise. IEEETransactions on Information theory, 57(7):46804688, 2011": "Tianyi Chen, Bo Ji, Tianyu Ding, Biyi Fang, Guanyi Wang, Zhihui Zhu, Luming Liang, Yixin Shi, Sheng Yi,and Xiao Tu. Only train once: A one-shot neural network training and pruning framework. Advances inNeural Information Processing Systems, 34:1963719651, 2021. Yixuan Chen, Yubin Shi, Mingzhi Dong, Xiaochen Yang, Dongsheng Li, Yujiang Wang, Robert P Dick, QinLv, Yingying Zhao, Fan Yang, et al. Over-parameterized model optimization with polyak-{\\L} ojasiewiczcondition. In The Eleventh International Conference on Learning Representations, 2023.",
  "Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neuralnetworks. arXiv preprint arXiv:1803.03635, 2018": "Abhinav Goel, Caleb Tung, Yung-Hsiang Lu, and George K Thiruvathukal. A survey of methods for low-powerdeep learning and computer vision. In 2020 IEEE 6th World Forum on Internet of Things (WF-IoT), pp.16. IEEE, 2020. Yushuo Guan, Ning Liu, Pengyu Zhao, Zhengping Che, Kaigui Bian, Yanzhi Wang, and Jian Tang. Dais:Automatic channel pruning via differentiable annealing indicator search. IEEE Transactions on NeuralNetworks and Learning Systems, 2022.",
  "Yang He, Guoliang Kang, Xuanyi Dong, Yanwei Fu, and Yi Yang. Soft filter pruning for accelerating deepconvolutional neural networks. arXiv preprint arXiv:1808.06866, 2018": "Yang He, Ping Liu, Ziwei Wang, Zhilan Hu, and Yi Yang. Filter pruning via geometric median for deepconvolutional neural networks acceleration. In Proceedings of the IEEE/CVF conference on computervision and pattern recognition, pp. 43404349, 2019. Yang He, Yuhang Ding, Ping Liu, Linchao Zhu, Hanwang Zhang, and Yi Yang. Learning filter pruningcriteria for deep convolutional neural networks acceleration. In Proceedings of the IEEE/CVF conferenceon computer vision and pattern recognition, pp. 20092018, 2020.",
  "Lucas Liebenwein, Cenk Baykal, Harry Lang, Dan Feldman, and Daniela Rus. Provable filter pruning forefficient neural networks. In International Conference on Learning Representations, 2019": "Mingbao Lin, Rongrong Ji, Yan Wang, Yichen Zhang, Baochang Zhang, Yonghong Tian, and Ling Shao.Hrank: Filter pruning using high-rank feature map. In Proceedings of the IEEE/CVF conference oncomputer vision and pattern recognition, pp. 15291538, 2020. Shaohui Lin, Rongrong Ji, Chenqian Yan, Baochang Zhang, Liujuan Cao, Qixiang Ye, Feiyue Huang,and David Doermann. Towards optimal structured cnn pruning via generative adversarial learning. InProceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 27902799, 2019. Liyang Liu, Shilong Zhang, Zhanghui Kuang, Aojun Zhou, Jing-Hao Xue, Xinjiang Wang, Yimin Chen,Wenming Yang, Qingmin Liao, and Wayne Zhang. Group fisher pruning for practical network compression.In International Conference on Machine Learning, pp. 70217032. PMLR, 2021. Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, and Changshui Zhang. Learning efficientconvolutional networks through network slimming. In Proceedings of the IEEE international conference oncomputer vision, pp. 27362744, 2017. Jian-Hao Luo, Jianxin Wu, and Weiyao Lin. Thinet: A filter level pruning method for deep neural networkcompression. In Proceedings of the IEEE international conference on computer vision, pp. 50585066, 2017. Shervin Minaee, Yuri Boykov, Fatih Porikli, Antonio Plaza, Nasser Kehtarnavaz, and Demetri Terzopoulos.Image segmentation using deep learning: A survey. IEEE transactions on pattern analysis and machineintelligence, 44(7):35233542, 2021. Pavlo Molchanov, Arun Mallya, Stephen Tyree, Iuri Frosio, and Jan Kautz. Importance estimation for neuralnetwork pruning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,pp. 1126411272, 2019. Chaitanya Murti, Tanay Narshana, and Chiranjib Bhattacharyya. Tvsprune-pruning non-discriminativefilters via total variation separability of intermediate representations without fine tuning. In The EleventhInternational Conference on Learning Representations, 2023. Ben Mussay, Dan Feldman, Samson Zhou, Vladimir Braverman, and Margarita Osadchy. Data-independentstructured pruning of neural networks via coresets. IEEE Transactions on Neural Networks and LearningSystems, 33(12):78297841, 2021. Manuel Nonnenmacher, Thomas Pfeil, Ingo Steinwart, and David Reeb. Sosp: Efficiently capturing globalcorrelations by second-order structured pruning. In International Conference on Learning Representations,2021.",
  "Hanyu Peng, Jiaxiang Wu, Shifeng Chen, and Junzhou Huang. Collaborative channel pruning for deepnetworks. In International Conference on Machine Learning, pp. 51135122. PMLR, 2019": "Kiran Purohit, Anurag Parvathgari, Soumi Das, and Sourangshu Bhattacharya. Accurate and efficient channelpruning via orthogonal matching pursuit. In Proceedings of the Second International Conference on AI-MLSystems, AIMLSystems 22, New York, NY, USA, 2023. Association for Computing Machinery. ISBN9781450398473. doi: 10.1145/3564121.3564139. URL John Rachwan, Daniel Zgner, Bertrand Charpentier, Simon Geisler, Morgane Ayle, and Stephan Gnnemann.Winning the lottery ahead of time: Efficient early network pruning. In International Conference on MachineLearning, pp. 1829318309. PMLR, 2022. Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You only look once: Unified, real-timeobject detection. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.779788, 2016.",
  "Wei Wen, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li. Learning structured sparsity in deep neuralnetworks. Advances in neural information processing systems, 29, 2016": "Zhonghui You, Kun Yan, Jinmian Ye, Meng Ma, and Ping Wang. Gate decorator: Global filter pruningmethod for accelerating deep convolutional neural networks. Advances in neural information processingsystems, 32, 2019. Ruichi Yu, Ang Li, Chun-Fu Chen, Jui-Hsin Lai, Vlad I Morariu, Xintong Han, Mingfei Gao, Ching-Yung Lin,and Larry S Davis. Nisp: Pruning networks using neuron importance score propagation. In Proceedings ofthe IEEE conference on computer vision and pattern recognition, pp. 91949203, 2018. Zhuangwei Zhuang, Mingkui Tan, Bohan Zhuang, Jing Liu, Yong Guo, Qingyao Wu, Junzhou Huang, andJinhui Zhu. Discrimination-aware channel pruning for deep neural networks. Advances in neural informationprocessing systems, 31, 2018.",
  "lSc X l gl,k,where l is the error vector for the estimation of removed filter l Sc, and Sc denotes the set of all removedfilters": "Proof.Consider the input and output of any K K convolution layer to be X = {X1, ..., Xm} andY = {Y1, ..., Yn}. Y goes as an input to the 1 1 convolution. Let the output of the 1 1 convolutionlayer be Z = {Z1, ..., Zn}, followed by f Rmn and g Rnn being the filter weights of K K and 1 1convolution layer respectively. We can formulate the above setup as:"
}