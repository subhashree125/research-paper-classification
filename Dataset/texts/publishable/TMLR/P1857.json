{
  "Abstract": "Understanding the mechanisms through which neural networks extract statistics from input-label pairs through feature learning is one of the most important unsolved problems insupervised learning. Prior works demonstrated that the gram matrices of the weights (theneural feature matrices, NFM) and the average gradient outer products (AGOP) becomecorrelated during training, in a statement known as the neural feature ansatz (NFA). Throughthe NFA, the authors introduce mapping with the AGOP as a general mechanism for neuralfeature learning. However, these works do not provide a theoretical explanation for thiscorrelation or its origins. In this work, we further clarify the nature of this correlation, andexplain its emergence. We show that this correlation is equivalent to alignment between theleft singular structure of the weight matrices and the newly dened pre-activation tangentfeatures at each layer. We further establish that the alignment is driven by the interaction ofweight changes induced by SGD with the pre-activation features, and analyze the resultingdynamics analytically at early times in terms of simple statistics of the inputs and labels.We prove the derivative alignment occurs almost surely in specic high dimensional settings.Finally, we introduce a simple optimization rule motivated by our analysis of the centeredcorrelation which dramatically increases the NFA correlations at any given layer and improvesthe quality of features learned.",
  "Introduction": "Neural networks have emerged as the state-of-the-art machine learning methods for seemingly complex tasks,such as language generation (Brown et al., 2020), image classication (Krizhevsky et al., 2012), and visualrendering (Mildenhall et al., 2021). The precise reasons why neural networks generalize well have been thesubject of intensive exploration, beginning with the observation that standard generalization bounds fromstatistical learning theory fall short of explaining their performance (Zhang et al., 2021). A promising line of work emerged in the form of the neural tangent kernel, connecting neural networks to kernelsin the wide limit (Jacot et al., 2018; Chizat et al., 2019). However, subsequent research showed that the successof neural networks relies critically on aspects of learning which are absent in kernel approximations (Ghorbaniet al., 2019; Allen-Zhu and Li, 2019; Yehudai and Shamir, 2019; Li et al., 2020; Renetti et al., 2021). Other",
  "work showed that low width suces for gradient descent to achieve arbitrarily small test error (Ji and Zhu,2020), further refuting the idea that extremely wide networks are necessary": "Subsequently, the success of neural networks has been largely attributed to feature learning - the ability ofneural networks to learn representations of data which are useful for downstream tasks. However, the specicmechanism through which features are learned is an important unsolved problem in deep learning theory. Anumber of works have studied the abilities of neural networks to learn features in structured settings (Abbeet al., 2022; Ba et al., 2022; Nichani et al., 2023; Barak et al., 2022; Damian et al., 2022; Moniri et al., 2023;Parkinson et al., 2023). Some of that work proves strict separation in terms of sample complexity betweenneural networks trained with stochastic gradient descent and kernels (Mousavi-Hosseini et al., 2022). The work above studies simple structure, such as learning from low-rank data or functions that are hierarchicalcompositions of simple elements. Recent work makes a big step towards generalizing these assumptions byproposing the neural feature ansatz (NFA) (Radhakrishnan et al., 2024a; Beaglehole et al., 2023), a generalstructure that emerges in the weights of trained neural networks. The NFA states that the gram matrixof the weights at a given layer (known as the neural feature matrix or NFM) is aligned with the averagegradient outer product (AGOP) of the network with respect to the input to that layer. In particular, the NFMand AGOP are highly correlated in all layers of trained neural networks of general architectures, includingpractical models such as VGG (Simonyan and Zisserman, 2014), vision transformers (Dosovitskiy et al., 2021),and GPT-family models (Brown et al., 2020). A major missing element is an explanation for how and why the AGOP and NFM become correlated throughtraining with gradient descent. In this paper, we precisely describe the emergence of this correlation. Weestablish that the NFA is equivalent to alignment between the left singular structure of the weight matricesand the uncentered covariance of the pre-activation tangent kernel (PTK) () features. We thenintroduce the centered neural feature correlation (C-NFC) which isolates this alignment process. We showempirically that the C-NFC is close to its maximum value of 1 at early times, and fully captures the NFA atlate times for a variety of architectures (fully-connected, convolutional, and attention layers) over a diversecollection of datasets (). Our experiments suggest that the C-NFC drives the development of theNFA. Through this centering, we show that the dynamics of the C-NFC can be understood analytically interms of the statistics of the data and labels at early times (). Using this decomposition, we showthat the NFA emerges as a structural property of gradient descent (analytical result in the commonly studiedsetting of uniform data on the sphere, under certain assumptions on the activation and target functions). Inparticular, the rst non-zero derivatives of the centered NFM and AGOP will be asymptotically identical.We further characterize how the NFA depends on the data distribution, and explore this analytically andexperimentally. Finally, motivated by our theory, we design an intervention to increase the inuence of the C-NFC and makethe NFA more robust: Speed Limited Optimization, a layerwise gradient normalization scheme ().The eectiveness of the latter update rule suggests a path towards rational design of architectures and trainingprocedures that maximize the NFA notion of feature learning by promoting alignment dynamics.",
  "Preliminaries": "We consider fully-connected neural networks with a single output of depth L 1, where L is the numberof hidden layers, written f : Rd R. We write the input to layer {0, . . . , L} as x, where x0 x is theoriginal datapoint, and the pre-activation as h(x). Then,",
  "Published in Transactions on Machine Learning Research (11/2024)": "Isolating alignment of the PTK to the initial weight matrixOne may also center just the PTKfeature map, while substituting the initial weights for W to isolate how the PTK feature covariance aligns tothe weight matrices. To measure this alignment, we consider the PTK-centered NFC, which is dened as thecorrelation (W ()0 )W ()0 , (W ()0 ) K()W ()0, where W ()0is the initial weight matrix at layer .",
  "r = 5 and inputs drawn from standard normal. The EGOP Exyxyx(rst plot) captures the low-rank": "structure of the task. The NFMW W(second plot) and AGOPW KW(third plot) of a fully-connectednetwork are similar to each other and the EGOP. Replacing K with a symmetric matrix Q with the samespectrum but independent eigenvectors obscures the low rank structure (fourth plot), and reduces thecorrelation from F, G= 0.93 to F, W QW= 0.53. One can dene two objects associated with neural networks that capture learned structure. For a given layer, the neural feature matrix (NFM) F is the gram matrix of the columns of the weight matrix W (), i.e.F (W ())W (). F depends on the right singular vectors (and corresponding singular values) of W ().The second fundamental object we consider is the average gradient outer product (AGOP) G, dened asG 1",
  "k=1xk mod r x(k+1) mod r ,(2)": "where the data inputs are sampled from an isotropic Gaussian distribution = N(0, I). In this case, theentries EGOP(y, ) will be 0 for rows and columns outside of the r r sub-matrix corresponding to x1, . . . , xr(), as y does not vary with coordinates xr+1, . . . , xd. Within this sub-matrix, the diagonal entrieswill have value 2, while the o-diagonal entries will be either 1 or 0. Therefore, EGOP(y, ) will be rank r,where r is much less than the ambient dimension. We verify for this task that the AGOP of the trained model resembles the EGOP (rst and third panels of). Here the NFA holds and the NFM (second panel) resembles the AGOP and therefore the EGOPas well. Therefore, the neural network has learned the model-independent and task-specic structure of thechain-monomial task in the right singular values and vectors of the rst layer weight matrix, as these aredetermined by the NFM. In fact, previous works have demonstrated that the NFM of the rst layer of awell-trained neural network is highly correlated with the AGOP of a xed kernel method trained on the samedataset (Radhakrishnan et al., 2024a). This insight has inspired iterative kernel methods which can match the performance of fully-connectednetworks (Radhakrishnan et al., 2024a;b; Aristo et al., 2024) and improve over xed convolutional kernels(Beaglehole et al., 2023). Additional prior works demonstrate the benet of including the AGOP features toimprove feature-less predictors (Hristache et al., 2001; Trivedi et al., 2014; Kpotufe et al., 2016). Additionally,because the NFM is correlated with the AGOP, the AGOP can be used to recover the features from feature-lessmethods, such as kernel machines. The AGOP has also been show to capture surprising phenomena of neuralnetworks beyond low-rank feature learning including deep neural collapse (Beaglehole et al., 2024).",
  "Alignment decomposition": "In order to understand Ansatz 1, it is useful to decompose the AGOP. Doing so will allow us to show thatthe neural feature correlation (NFC) can be interpreted as an alignment metric between weight matrices andthe pre-activation tangent kernel (PTK). The PTK K()(x, z) is dened with respect to a layer of a neuralnetwork and two inputs x, z. The kernel evaluates to:",
  "F, G= (W ())W (), (W ())K()W ()": "This alignment holds trivially and exactly if K() is the identity. However, the correlation can be high intrained networks where K() is non-trivial. For example, in the chain monomial task (), K(0) isfar from identity (standard deviation of its eigenvalues is 5.9 times its average eigenvalue), but the NFAcorrelation is 0.93 at the end of training. We also note that if K() is independent of W (), the alignment islower than in trained networks; in the same example, replacing K(0) with a matrix Q with equal spectrumbut random eigenvectors greatly reduces the correlation to 0.53 and qualitatively disrupts the structurerelative to the NFM (, rightmost column). We show the same result for the CelebA dataset (seeAppendix M). Therefore, the NFA is a consequence of alignment between the left eigenvectors of W () andK() in addition to spectral considerations.",
  "Centering the NFC isolates weight-PTK alignment": "We showed that the neural feature ansatz is equivalent to PTK-weight alignment (Proposition 2). We now ask:is the increase in the NFC due to alignment of the weight matrices to the current PTK, or the alignment of thePTK to the current weights? In practice, both eects matter, but numerical evidence suggests that changes inthe PTK do not drive the early dynamics of the NFC (Appendix D). Instead, we observe that the alignmentbetween the weights and the PTK feature covariance is driven by a centered NFC that captures alignmentbetween the parameter changes and the PTK. We then show this centered NFC can hold robustly in settingswhere the NFC holds with correlation less than 1, such as early in training and/or with large initialization.Finally, we establish analytically through the centered NFC how the NFA emerges as a structural property ofgradient descent. We begin by considering a decomposition of the NFM and AGOP into parts that depend on initialization anda part that depends on the changes in the weight matrix. Let W ()tand K()tbe the weight matrix and PTKfeature covariance for layer at time t. We can write the NFM and AGOP in terms of the initial weightsW ()0and the change in weights W ()t W ()t W ()0as follows:",
  "F = W t Wt = W tWt + W 0 Wt + W t W0 + W 0 W0,G = W t KtWt = W t Kt Wt + W 0 Kt Wt + W t KtW0 + W 0 KtW0(4)": "where we omitted from all terms for ease of notation. The rst term in each decomposition isolates the grammatrix of the changes in the weight matrix. We call W W the centered NFM and W K W the centeredAGOP. The centered AGOP in particular measures the alignment of weight updates with the current PTKfeature covariance. Both terms are 0 at initialization, and if the weight matrices change signicantly (that is,if || W|| ||W0||), both the NFM and AGOP are dominated by the centered terms. (Note: in the limit that",
  "NFA across MLP layers": ": Uncentered and centered neural feature correlations across (A,B) fully-connected, (C) convolutional,and (D) attention layers with large initialization scale. (A,C,D) show trajectories of C/UC-NFC over training.(B) shows NFC values across all layers of an MLP with ve hidden layers, averaged over CIFAR-10, CIFAR-100,SVHN, MNIST, GTSRB, and STL-10 datasets. (A-C) are additionally averaged over three random seeds.Each row of (D) is an attention block (ordered from rst to last in the GPT model), while the columns showcorrelations for query, key, and value layers, respectively.",
  "For the remainder of the paper, we will refer to the original NFC as the uncentered NFC (UC-NFC) to avoidambiguity": "The centered NFC is consistently higher than the uncentered NFC across training times, architectures, anddatasets (). This especially holds at early times and in deeper layers of a network (C and D,VGG-11 on CIFAR-10 and the GPT model on Shakespeare respectively). For MLPs, we conduct a broaderset of experiments and verify that the trends hold on a wide range of vision datasets. We also note that theC-NFC at early times is relatively robust to the initialization statistics of the weight matrix W () - unlikethe UC-NFC (for additional experiments, see Appendix H). We will return to this point in . Theseexperiments suggest the C-NFC is responsible for improvements in the uncentered correlation. High C-NFC values can drive increases in the UC-NFC as long as the centered NFM and centered AGOPare increasing in magnitude during training. We can conrm that the weights move signicantly frominitialization, which drives the contribution from the centered NFM and centered AGOP (Figures 14, and 15in Appendix M). The increased importance of the C-NFC leads the UC-NFC to converge to the C-NFC atlate times in most of our experimental settings, as the contribution from the weight changes dominates thecontribution from the initialization. These ndings validate that the C-NFC is an important contributor toincreases in the UC-NFC. Our experiments suggest that studying the C-NFC is a useful rst step to understanding the neural featureansatz. In , we develop a theoretical analysis to understand why the C-NFC is generically large atearly times. We then use this analysis to predict the value of the C-NFC and motivate interventions whichcan keep the C-NFC high and promote the NFA earlier on in training ().",
  "Gradient ow dynamics": "We now theoretically identify why the centered AGOP and NFM become correlated as a structural propertyof gradient descent, for at least the early training times. We consider the setup introduced in Equation (1).For theoretical convenience we focus on the case of training under gradient ow, where the dynamics of aweight matrix W () trained on loss L is given by",
  "We note that W = 0. This gives us the following proposition, which shows that early time dynamics of thecentered NFM and AGOP are dominated by the second time derivatives:": "Proposition 3 (Centered NFC dynamics). Let W be the weights of a fully-connected layer of a neuralnetwork at initialization and X be the inputs to that layer. Then, when the neural network is trained bygradient ow on a loss function L, we have W W = W K W = 0,dd t( W W) =dd t( W K W) = 0, and therst non-zero time derivatives satisfy,",
  "= X LK2 LX": "We immediately see how gradient-based training can drive the C-NFC towards 1 at early times; the rstnon-trivial time derivatives are often highly correlated as they dier by only a single matrix power of K, andhave the same dependence on the labels. If K is proportional to a projection matrix, then the two derivativeswill have perfect correlation; even if not, K and K2 will have identical eigenvectors, and hence we mightexpect a range of spectra will enable high correlation between them. In the case that K and K2 are sucientlycorrelated, the C-NFC is driven to a high value immediately upon training, and the high value of the C-NFCeventually drives the UC-NFC and causes the NFA to hold. In the remainder of this section, we study this correlation in two dierent high-dimensional limits. Ouranalysis suggests that for large models the C-NFC has a generic tendency to increase early in training.",
  "Maximum C-NFC for data on the sphere": "We rst provide a general and well-studied setting where the rst non-zero derivatives of the centered NFMand AGOP are perfectly correlated - uniform data on the sphere in high dimensions (Ghorbani et al., 2020;2021; Misiakiewicz, 2022). This limit corresponds to an innite width network which, combined with thedata symmetry, induces the PTK matrix K to act like a projection matrix.",
  "dUnifSd1uniformly distributed on the sphere in d dimensions withradius": "d. We assume the labels are generated from a target function that maps f : Sd1 [d, d]. I.e. thelabel for the point x is equal to f (x). We train the parameters a, W for a one-hidden layer fully-connectedneural network f(x) = f(x; a, W) = a(Wx) with element-wise activation function . For a learningtrajectory (at, Wt), the initial values a0 and W0 are sampled i.i.d. standard Gaussian with variance at mostO(1/k). I.e. for all i, j, , we have a0(i), W0(j, ) N(0, c) for c = O(1/k). NotationWe will use asymptotic notation Od, od, d, d, d in the usual way, where the limits are takenwith respect to the data dimension d. Od, od, d, d, d are equivalent to their previous denitions but hidedependencies of polylogarithmic functions of d. We will use to refer to 2 vector norm and the operatornorm of a matrix, i.e. A = maxvRd Av v , for a square matrix A Rdd. Recall we sample n datapointsand use hidden dimension k so that at Rk1 and Wt Rkd. We write the vector of all ones in p dimensionsas 1 Rp1. We will omit the time index t for ease of notation.",
  "d Sd1 and any positive": "We make an additional assumption on the dierentiability of the PTK, inherited from the activation ,according to the conditions from Misiakiewicz (2022). This assumption is needed for our analysis, in whichdierentiability enables us to Taylor expand the PTK matrix (El Karoui, 2008). We restate this assumption.Assumption 6 (Dierentiability of the PTK). Let K(x, x) = Eak,wka2k(wk x)(wk x)= hd(x, x/d)be the PTK for the network f, where hd : R is a positive semi-denite kernel function, denedfor each dimension d. We assume there exist nite h(0), h(0), h(0) > 0 such that limd hd(0) = h(0),limd hd(0) = h(0), and limd hd(0) = h(0), where the rst and second derivatives of hd, hd and hd,are assumed to exist on for all d. Note this is a sub-case of Assumption 1 in Misiakiewicz (2022) for level = 1, and is satised for hd that istwice dierentiable everywhere. We now introduce our nal simplifying assumption on the activation functionand data: We will consider f trained using mean-squared error (MSE) loss. With this loss, L corresponds to thediagonal matrix of the residuals y f(x). If the outputs of the network is 0 on the training data, thenL = Y diag (y), the labels themselves. We can guarantee this by either of the following methods:Method 7 (Subtract copy). Subtracting o an identical (untrained) copy of the neural network from eachoutput at initialization. I.e. we train the parameters of the network f on the loss with respect to outputsft(x) = ft(x) f0(x), where f0(x) is a copy of f at initialization and ft(x) = f(x; at, Wt)Method 8 (Small initialization). Initializing W = 0 or a = for > 0 arbitrarily small. We now state our main theorem.Theorem 9 (Maximum C-NFC). Suppose the data X are sampled uniformly at random from Sd1 and thelabels are generated by f satisfying Assumptions 4 and 5. Suppose we train f with MSE loss and initializethe network so that the initial outputs are 0 by either Method 7 or Method 8 above. Assume the activationfunction satises Assumption 6. We consider the regime that dd log2+1 d n odd2for some",
  "The proof follows from the mean term of K giving the leading order terms in the C-NFC derivatives calculationhere, and 11 is a rank-1 projector. The proof is deferred to Appendix C": "We clarify that although we take innite width, we are not necessarily in the NTK regime, as we allow forarbitrary scaling of the weights, including the P parametrization (Yang and Hu, 2020). Hence, our settingallows for feature learning. In the next section we will construct a dataset which interpolates between adversarial and aligned eigenstructureto demonstrate the range of possible values the derivative correlations can take, and theoretically predicttheir values.",
  "Assumption 10 (Self-averaging). We assume that the expected traces appearing in the NFC across initial-izations are equal to the traces themselves": "The elements of weight matrices W are drawn from independent Gaussians (i.i.d. within each matrix). If thedata X were also standard Gaussian, we would be able to apply free probability the noncommutativeanalog of classical independence to compute traces of matrix products involving analytic functions ofW and X in the limit of large dimensions (Mingo and Speicher, 2017). To apply free probability for moregeneral X, we require the following assumption: Assumption 11 (Asymptotic freedom of initial parameters, and input-label pairs). We assume W and Xare asymptotically freely independent. Further, we assume the labels Y (X) are asymptotically free of W (butnot X).",
  "with similar decompositions for the denominator term": "Therefore if the statistics of K can be understood as a function of X, we can compute the correlation inthis linear triple-scaling limit. We focus for now on a one-hidden layer quadratic network (similar to theprevious section) to avoid the branching of terms that arises in more complicated networks. We provide someadditional analysis of the rst term in Equation 9 for more complicated architectures in Appendix E.",
  "trXY KY XXY K2Y X= trM (4)X|Y FaM (4)X|Y FaM (2)X Fa": "We reiterate our assumptions and compute this trace (as well as those for the denominator terms) inAppendix F using standard results from random matrix theory.These calculations show us that thecorrelation is determined by traces of powers of M (4)X|Y and M (2)Xby the calculations in Section F, which areproperties of the input-label pairs, and Fa, which is specic to the architecture and initialization statistics.",
  "Mean": "0.00.250.50.751.0 ( W W, WK W) ( W W, WK W) : Predicted versus observed correlation of the second derivatives of centered F and G on the alignmentreversing dataset. Dierent shaded color curves correspond to four dierent seeds for the dataset. The solidblue curve is the average over all data seeds. The rightmost sub-gure is a scatter plot of the predicted versusobserved correlations of these second derivatives, with one point for each balance value. We instantiate thedataset in the proportional regime where width, input dimension, and dataset size are all equal to 1024. Manipulating the C-NFCTo numerically explore the validity of the random matrix theory calculations,we developed a method to generate datasets with dierent values of W W, W K W. We construct arandom dataset called the alignment reversing dataset, parameterized by a balance parameter (0, 1] toadversarially disrupt the NFA near initialization in the regime that width k, input dimension d, and datasetsize n are all equal (n = k = d = 1024). By Proposition 16, for the aforementioned neural architecture, theexpected second derivative of the centered NFM satises, E W W= XY E [K] Y X = (XY X)2, whilethe expected second derivative of the centered AGOP, E W K W= XY EK2Y X, has an additionalcomponent XY X XX XY X.Our construction exploits this dierence in that XX becomesadversarially unaligned to XY X as the balance parameter decreases. The construction exploits that we can manipulate XY X XX XY X freely of the NFM using a certainchoice of Y . We design the dataset such that this AGOP-unique term is close to identity, while the NFMsecond derivative has many large o-diagonal entries, leading to low correlation between the second derivativesof the NFM and AGOP. In our experiment, we sample multiple random datasets with this construction and compute the predictedand observed correlation of the second derivatives of the centered NFC at initialization. For specic detailson the construction, see Appendix G. We observe in that the centered NFC predicted with random matrix theory closely matches theobserved values, across individual four random seeds and for the average of the correlation across them.Crucially, a single neural network is used across the datasets, conrming the validity of the self-averagingassumption. The variation in the plot across seeds come from randomness in the sample of the data, whichcause deviations from the adversarial construction.",
  "Increasing the centered contribution to the NFC strengthens feature learning": "Our theoretical and experimental work has established that gradient based training leads to alignmentof the weight matrices to the PTK feature covariance.This process is driven by the C-NFC. There-fore, one path towards improving the neural feature correlations is to increase the contribution of theC-NFC to the dynamics - as measured, for example, by the centered-to-uncentered, or, C/UC, ratiotr W W W K WtrW WW KW1. When this ratio is large, the C-NFC contributes signicantly inmagnitude to the UC-NFC, indicating successful feature learning as measured by the uncentered NFC. Wewill discuss how small initialization promotes feature learning, and design an optimization rule, Speed LimitedOptimization, that increases the C/UC ratio and drives the value of the UC-NFC to 1.",
  "Feature learning and initialization": "One factor that modulates the level of feature learning is the scale of the initialization in each layer. In ourexperiments training networks with unmodied gradient descent, we observe that the centered NFC willincreasingly dominate the uncentered NFC with decreasing initialization (third panel, rst row, ).Further, as the centered NFC increases in contribution to the uncentered quantity, the strength of theUC-NFC, and to a lesser extent, the C-NFC increases (). The decreases in correspondence betweenthese quantities is also associated with a decrease in the feature quality of the NFM (Appendix I), for thechain monomial task. For fully-connected networks with homogeneous activation functions, such as ReLU, and no normalizationlayers, decreasing initialization scale is equivalent to decreasing the scale of the outputs, since we can writef(Wx) = apf(aWx) for any scalar a for any homogenous activation f. This in turn is equivalent toincreasing the scale of the labels. Therefore, decreasing initialization forces the weights to change more inorder to t the labels, leading to more change in F from its initialization. Conceptually, this aligns with thesubstantial line of empirical and theoretical evidence that increasing initialization scale or output scalingtransitions training between the lazy and feature learning regimes (Chizat et al., 2019; Woodworth et al.,2020; Agarwala et al., 2020; Lyu et al., 2023). This relationship suggests that small initialization can be broadly applied to increase the change in F, and thevalue of the UC-NFC. However, this may not be ideal; for example, if the activation function is dierentiableat 0, small initialization leads to a network which is approximately linear. This may lead to low expressivityunless the learning dynamics can increase the weight magnitude.",
  "Speed Limited Optimization": "We can instead design an intervention which can increase feature learning without the need to decrease theinitialization scale. We do so by xing the learning speed layerwise to constant values, which causes theC-NFC to dominate the UC-NFC dynamics. For weights at layer and learning rate > 0, we introduceSpeed Limited Optimization (SLO), which is characterized by the following update rule,",
  "W ()t L": "where the hyperparameter C 0 controls the amount of learning in layer . We expect this rule to increasethe strength of the UC-NFC in layers where C is large relative to Cm for m = , as W () will be forced tochange signicantly from initialization. By forcing the weights in a particular layer to have xed learningspeeds, these weights will have xed updates sizes at every epoch, regardless of the loss. We downscale the",
  "speeds in other layers = m to prevent training instability. As a result,W ()0W ()t1 0 for large t,causing the centered and uncentered NFC to coincide for this layer": "We demonstrate the eects of SLO on the chain-monomial task (Equation (2)). We found that xing thelearning speed to be high in the rst layer and low in the remaining layers causes the ratio of the unnormalizedC-NFC to the UC-NFC to become close to 1 across initialization scales (). The same result holds forthe SVHN dataset (see Appendix M). We note that this intervention can be applied to target underperforminglayers and improve generalization in deeper networks (see Appendix K for details). We observe that both the C-NFC and the UC-NFC become close to 1 after training with SLO, independentof initialization scale (Appendix I). Further, the quality of the features learned, measured by the similarity ofF and G to the true EGOP, signicantly improve and are more similar to each other with SLO, even withlarge initialization. In contrast, in standard training the UC-NFA fails to develop with large initialization, asF resembles identity (no feature learning), while G only slightly captures the relevant features.",
  "UC-NFCC-NFCC/UC RatioTrain Losses": ": The eect of SLO on C/UC neural feature correlations and feature learning on the chain monomialtask. In the rst two rows, we plot the uncentered and centered NFA for the rst layer weight matrix asa function of initialization, (A) with standard training and (B) with SLO. We consider a two hidden layernetwork with ReLU activations, where we set C0 = 500, and C1 = C2 = 0.002. The third column shows theratio of the unnormalized C-NFC to the UC-NFC: tr W W W K W trW WW KW1. The fourthcolumn shows the training loss. In the third row, we plot the NFM and AGOP from a trained network with(C) standard training and (D) with Speed Limited Optimization with xed initialization scale of 1.0.",
  "Discussion": "Analyzing more general settingsIn principle our analysis can be extended to a larger sample size andactivation functions with uncentered derivatives. Both of these settings require analyzing more terms inthe Taylor expansion of the PTK matrix. This is more complicated than studying the loss, as our mostcomplex calculations require understanding degree 8 polynomials of the inputs even in the simplest case of aone-hidden layer network (as opposed to degree 4 to understand the loss). We may also want to understandthe case that n = d for integer (i.e. without the logarithmic factor we consider). The appropriate Taylorexpansions in these cases are discussed in Misiakiewicz (2022), and will likely require additional structure onthe coecients of the target function f in the basis of spherical harmonics. Centered NFC through trainingOur theoretical analysis in this work shows that the PTK featurecovariance at initialization has a relatively simple structure in terms of the weights and the data. To predictthe NFC later in training, we will likely need to account for the change in this matrix. One should be ableto predict this development at short times by taking advantage of the fact that eigenvectors of the Hessianchange slowly during training (Bao et al., 2023). An alternative approach would be to use a quadratic model",
  "Acknowledgements": "We thank Lechao Xiao for detailed feedback on the manuscript. We also thank Jerey Pennington for helpfuldiscussions. This work used the programs (1) XSEDE (Extreme science and engineering discovery environment)which is supported by NSF grant numbers ACI-1548562, and (2) ACCESS (Advanced cyberinfrastructurecoordination ecosystem: services & support) which is supported by NSF grants numbers #2138259, #2138286,#2138307, #2137603, and #2138296. Specically, we used the resources from SDSC Expanse GPU computenodes, and NCSA Delta system, via allocations TG-CIS220009. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.Advances in neural information processing systems, 33:18771901, 2020.",
  "Guangda Ji and Zhanxing Zhu. Knowledge distillation in wide neural networks: Risk bound, data eciencyand imperfect teacher. In Advances in Neural Information Processing Systems, 2020": "Emmanuel Abbe, Enric Boix Adsera, and Theodor Misiakiewicz. The merged-staircase property: a necessaryand nearly sucient condition for sgd learning of sparse functions on two-layer neural networks. InConference on Learning Theory, 2022. Jimmy Ba, Murat A Erdogdu, Taiji Suzuki, Zhichao Wang, Denny Wu, and Greg Yang. High-dimensionalAsymptotics of Feature Learning: How One Gradient Step Improves the Representation. arXiv preprintarXiv:2205.01445, 2022.",
  "CAdditional proofs and statements": "Theorem (Maximum C-NFC). Suppose the data X are sampled uniformly at random from Sd1 and thelabels are generated by f satisfying Assumptions 4 and 5. Suppose we train f with MSE loss and initializethe network so that the initial outputs are 0 by either Method 7 or Method 8 above. Assume the activationfunction satises Assumption 6. We consider the regime that dd log2+1 d n odd2for some",
  "d t2 ( W W) = XY K2Y X": "Given that k , K is equal to a deterministic kernel matrix conditioned on the inputs X (i.e. does notdepend on the sampled initial weights). We then use that we are in the proportional limit to approximatethe kernel matrix K by its Taylor expansion (Misiakiewicz, 2022; El Karoui, 2008; Hu and Lu, 2022; Ba et al.,2019). Namely, we have that with probability 1 od(1),",
  "nXy E [xf (x)] = o(1) w.p. 1 o(1),provided n = (d log2+1(d))": "Proof. As the data x are K-sub-Gaussian for a universal constant K, and the labels are bounded by c log(d),we have that xy is sub-Gaussian with parameter M = Kc log(d). Therefore, the empirical expectation of xy,1nni=1 xiyi is sub-Gaussian with parameter M/n. Directly applying Lemma 1 in Jin et al. (2019), we seethat,",
  "Steps": "Layer 2 Unnormalized DC/C NFA Initialization scale 0.0010.010.11.0 : Ratio of the unnormalized double-centered NFC to the centered NFC throughout neural networktraining. In particular, we plot tr W W W K W tr W W W K W1 throughout training for bothlayers of a two-hidden layer MLP with ReLU activations.",
  "EExtending our theoretical predictions to depth and general activations": "Precise predictions of the C-NFC become more complicated with additional depth and general activationfunctions. However, we note that the deep C-NFC will remain sensitive to a rst-order approximation inwhich K is replaced by its expectation. We demonstrate that this term qualitatively captures the behavior ofthe C-NFC for 2 hidden layer architectures with quadratic and, to a lesser extent, ReLU activation functionsin . In this experiment, we sample Gaussian data with mean 0 and covariance with a randomeigenbasis. We parameterize the eigenvalue decay of the covariance matrix by a parameter , called the datadecay rate, so that the eigenvalues have values k =1",
  "from a projector, where the NFA holds exactly. We see that for intermediate values of , both the observedand the predicted derivatives of the C-NFC decreases in value": "We plot the observed values in two settings corresponding to dierent asymptotic regimes. One setting is theproportional regime, where n = k = d = 128. The other is the NTK regime where n = d = 128 and k = 1024.For the quadratic case, as the network approaches innite width, the prediction more closely matches theobserved values. Additional terms corresponding to the nonlinear part of in ReLU networks, the derivativeof the activation function, are required to capture the correlation more accurately in this case.",
  "(b) ReLU": ": Observed versus the rst-order predicted C-NFC for the input to the rst layer of a two hiddenlayer MLP. The dashed line is neural network width k = n = d = 128, where n and d are the number of datapoint and data dimension, respectively, while the solid line uses n = d = 128 and k = 1024.",
  "(28)": "All terms of the NFC are now in terms of traces of the matrices A, B, and R and functions on each termseparately. The matrices A and B are determined by the data, while the moments of the eigenvalues of R aredetermined by the initialization distribution of the weights in the neural network, and neither training northe data.",
  "GAlignment reversing dataset": "The data consists of a mixture of two distributions from which two subsets of the data X(1) and X(2) aresampled from, and is parametrized by a balance parameter (0, 1] and two variance parameters 1, 2 > 0.The subset X(1) which has label y1 = 1 and constitutes a fraction of the entire dataset, is sampled from amultivariate Gaussian distribution with mean 0 and covariance = 11 + 1 I . Then the second subset,X(2), is constructed such that (X(2))X(2) ((X(1))X(1))2, and has labels y2 = 0. Then, for balanceparameter suciently small, the AGOP second derivative approximately satises,",
  ". Set y y + 105 1": "Note that U1S11 V 2 V2S11 U 1 = U1S21 U 1 = ((X(1))X(1))2, therefore, we should set X(2) = V2S11 U 1to get (X(2))X(2) = ((X(1))X(1))2. Regarding the variance parameters, in practice we set 1 = 0.5 and2 = 102.Proposition 16 (Expected NFM and AGOP). For a one hidden layer quadratic network, f(x) = a(Wx)2,with a N(0, I) and W 1",
  "HVarying the data distribution": "We verify that our observations for isotropic Gaussian data hold even when the data covariance has asignicant spectral decay. (Figures 9 and 10). We again consider Gaussian data that is mean 0 and wherethe covariance is constructed from a random eigenbasis. In , we substitute the eignevalue decay ask 1",
  "+k, while in , we use k 1": "1+k2 . We plot the values of the UC-NFC, C-NFC, train loss,and test loss throughout training for the rst and second layer of a two hidden layer network with ReLUactivations, while additionally varying initialization scale. Similar to , we observe that the C-NFC ismore robust to the initialization scale than the UC-NFC, and UC-NFC value become high through training,while being small at initialization. We see that the test loss improves for smaller initializations, where thevalue of the C-NFC and UC-NFC are higher.",
  "IEect of initialization on feature learning": "We see that when initialization is small, the C-NFC and UC-NFC are high at the end of training with andwithout xing the learning speed (). This is reected by the quality of the features learned by theNFM and the qualitative similarity of the NFM and AGOP at small initialization scale (). Further,we notice that as we increase initialization, without xing speeds, the correspondence between the NFMand decreases and the quality of the NFM features decreases (at a faster rate than the AGOP). Strikingly,when learning speeds are xed, the quality of the features in the AGOP and NFM becomes invariant to theinitialization scale. 0.001 0.01 0.1 1.0",
  "JExperimental details": "We describe the neural network training and architectural hyperparameters in the experiments of this paper.Biases were not used for any networks. Further, in all polynomial tasks, we scaled the label vector to havestandard deviation 1. Corrupted AGOPFor the experiments in , we used n = 384 data points, d = 32, k = 128 as thewidth in all layers, isotropic Gaussian data, initialization scale 0.01 in the rst layer and default scale in thesecond. We used ReLU activations and two hidden layers. For the experiments in ,9,10,5, and 6, weused a two hidden layer network with ReLU activations, learning rate 0.05, 800 steps of gradient decent, andtook correlation/covariance measurements every 5 steps. C/UC-NFC calculations on real datasetsWe describe the experimental details for . For (A,B)we trained a ve layer MLP on the rst 50,000 datapoints of Streetview House Numbers (SVHN), CIFAR-10,CIFAR-100, STL-10, MNIST, and German Trac Sign Recognition Benchmark (GTSRB) datasets. We usedthe default PyTorch initialization (scale of 1) for all layers. We used width 256 in all layers, and trained withSGD with batch size 128. For SVHN and CIFAR, we trained for 150 epochs with learning rate 0.2. For STL-10and GTSRB we used learning rate 0.1 for 150 epochs. For MNIST we trained for 50 epochs with learningrate 10. For the VGG-11 experiments on CIFAR-10, we used the default architecture from torchvision withbatch-norm layers removed. We trained for 50 epochs and learning rate 1. For experiments with a GPT-familymodel, we adapted the model and dataset from NanoGPT ( Weused all the default settings and the default Adam optimizer, but used no weight decay, learning rate 5e-3,and removed all dropout layers. We also reduced the number of attention layers to 3 from the original 6. Wetrained on the Shakespeare characters dataset. Alignment reversing datasetFor the experiments in , we used k = n = d = 1024 for the width,dataset size, and input dimension, respectively. Further, the traces of powers of Fa are averaged over 30neural net seeds to decouple these calculated values from the individual neural net seeds. The mean valueplotted in the rst two squares of gure is computed over 10 data seeds. SLO experimentsFor the SLO gures (Figures 11, 4), we use isotropic Gaussian data, 600 steps ofgradient descent. The learning rates are chosen based on initialization scale in the rst layer. For initializationscales 1, 0.1, 0.01, and 0.001, we used learning rates 0.03, 0.1, 0.2, 0.4, respectively. We again used two hiddenlayers with ReLU activations. We chose n = 256, d = 32, and k = 256 as the width. We divided the linearreadout weights by 0.01 at initialization to promote feature learning, and modied SLO to scale gradients by( + L)1, rather than just the inverse of the norm of the gradient, for = 0.1. This technique smoothsthe training dynamics as the parameters approach a loss minimum, allowing the network to interpolate thelabels. Predictions with depthFor the Deep C-NFC predictions (), we used n = 128, d = 128,initialization scale of 1. The low rank task is just the chain monomial of rank r = 5. The high rank polynomialtask is y(x) = di=1(Qx)2i , where Q Rdd is a matrix with standard normal entries. Figures 16 and 17For the experiments on the SVHN dataset, we train a four hidden layer neural networkwith ReLU activations, initialization scale 1.0 in all layers and width 256. For SVHN, we subset the datasetto 4000 points. We train for 3000 epochs with learning rate 0.2 for standard training, and 0.3 for SLO,and take NFC measurements every 50 epochs. For SLO, we set C0 = 2.5, C1 = C2 = 0.4, and relaxationparameter = 0.2. We pre-process the dataset so that each pixel is mean 0 and standard deviation 1. For theexperiments on CelebA, we train a two hidden layer network on a balanced subset of 7500 points with Adamwith learning rate 0.0001 and no weight decay. We use initialization scale 0.02 in the rst layer, and width128. We train for 500 epochs. We pre-process the dataset by scaling the pixel values to be between 0 and 1.",
  "KAdditional SLO experiments": "We demonstrate the SLO can be applied adaptively to increase the strength of the UC-NFC in all layers of adeep network on the chain monomial task of rank r = 3. We train a three hidden layer MLP with ReLUactivations and an initialization scale of 0.1 by SLO, and nd that all layers nish at the same high UC-NFC(). Further, this nal UC-NFC value is higher than the highest UC-NFC achieved by any layer withstandard training. The generalization loss is also lower with SLO on this example, corresponding to betterfeature learning (through the UC-NFC).",
  ": Training with SLO where the learning speeds are chosen adaptively based on the UC-NFC valuesof all layers. The dashed lines correspond to training with standard GD": "At every time step we choose Ci = s for the layer i with the smallest UC-NFC correlation value, while settingCj = s1 for all other layers, with s = 20. We again modify SLO by dividing the gradients by L for = 0.01. The learning rate is set to 0.05 in SLO and 0.25 for the standard training (gradient descent), andthe networks are trained for 500 epochs. We sample n = 256 points with d = 32, and use width k = 256.",
  "MAdditional experiments on real datasets": "We replicate Figures 1 and 4 on celebrity faces (CelebA) and Street View House Numbers (SVHN). Webegin by showing that one can disrupt the NFC correspondence by replacing the PTK feature covariancewith a random matrix of the same spectral decay. For this example, we measure the Pearson correlation,which subtracts the mean of the image. I.e. (A, B) (A m(A), B m(B)), where m(A), m(B) are theaverage of the elements of A and B.",
  "First column of (A) and (B) are the losses, while the remaining columns are the weight changes": "W W (NFM) W KW (AGOP) W QW : Various feature learning measures for the CelebA binary subtask of predicting glasses. Thediagonals of the NFMW W(rst plot) and AGOPW KW(second plot) of a fully-connected networkare similar to each other. Replacing K with a symmetric matrix Q with the same spectrum but independenteigenvectors obscures the low rank structure (third plot), and reduces the Pearson correlation of the diagonalfrom diag (F) , diag G= 0.91 to diag (F) , diagW QW= 0.04.",
  "(c) SLO": ": We demonstrate on the SVHN dataset, with a 4 hidden layer neural network with large initializationscale, how SLO can improve the strength of the UC-NFC, the C-NFC, the ratio of the unnormalized C-NFCto UC-NFC (plot (a)) and the feature quality (plots (b) and (c)). In plots (b) and (c), we visualize thediagonal of the NFM and AGOP for the rst layer of the trained network, where SLO was applied withC0 = 2.5, C1 = C2 = 0.4."
}