{
  "Abstract": "Existing data augmentation in self-supervised learning, while diverse, fails to preserve theinherent structure of natural images. This results in distorted augmented samples with com-promised semantic information, ultimately impacting downstream performance. To over-come this limitation, we propose SASSL: Style Augmentations for Self Supervised Learning,a novel data augmentation technique based on Neural Style Transfer. SASSL decouplessemantic and stylistic attributes in images and applies transformations exclusively to theirstyle while preserving content, generating diverse samples that better retain semantic in-formation.SASSL boosts top-1 image classication accuracy on ImageNet by up to 2percentage points compared to established self-supervised methods like MoCo, SimCLR,and BYOL, while achieving superior transfer learning performance across various datasets.Because SASSL can be performed asynchronously as part of the data augmentation pipeline,these performance impacts can be obtained with no change in pretraining throughput.",
  "Introduction": "Data labelling is a challenging and expensive process, which often serves as a barrier to build machinelearning models to solve real-world problems. Self-supervised learning (SSL) is an emerging machine learningparadigm that helps to alleviate the challenges of data labelling, by using large corpora of unlabeled datato pretrain models to learn robust and general representations.These representations can be ecientlytransferred to downstream tasks, resulting in performant models which can be constructed without accessto large pools of labeled data. SSL methods have shown promising results in recent years, matching and insome cases exceeding the performance of bespoke supervised models with small amounts of labelled data.",
  "(b) Style Transfer preprocessing": ": Towards diverse SSL data augmentation via Neural Style Transfer. We propose SASSL, anovel augmentation technique that leverages Style Transfer to create pretraining views that are semanticallyaware, focusing solely on modifying the images appearance while preserving its content. SASSL combinesthe images content with the texture of an external reference style, generating augmented views that betterretain the images semantic meaning. By incorporating Style Transfer into traditional SSL augmentationpipelines and controlling the stylization strength through gradual blending of style features and pixel values,SASSL promotes stronger representations compared to well-established SSL methods. Given the lack of labels, SSL relies on pretext tasks, i.e., predened tasks where pseudo-labels can begenerated. These include contrastive learning (Chen et al., 2020a; He et al., 2020), clustering (Caron et al.,2021; 2020; Assran et al., 2022), and generative modeling (He et al., 2022; Devlin et al., 2018).Manypretext tasks involve training the model to distinguish between dierent views of the same input and inputscorresponding to dierent samples. For these tasks, the way input data is augmented is crucial to learn usefulinvariances and extract robust representations (Chen et al., 2020a). While state-of-the-art augmentationsincorporate a wide range of color, spectral and spatial transformations, they often disregard the naturalstructure of an image. As a result, SSL pretraining methods may generate augmented samples with degradedsemantic information, and may be less able to capture diverse visual attributes. To tackle this challenge, we propose Style Augmentations for Self Supervised Learning (SASSL), a novel SSLdata augmentation technique based on Neural Style Transfer to generate semantically consistent augmentedsamples. In contrast to augmentation techniques operating on specic formats (e.g. pixel or spectral domain),SASSL disentangles an image into perceptual (style) and semantic (content) representations that are learnedfrom data. Applying transformations only to the style of an image while preserving its content, we cangenerate images with diverse appearance that retain the original semantic properties.",
  "Data Augmentation in SSL": "Typical data augmentation methods applied to vision tasks include image cropping and resizing, ipping,rotation, color augmentation, noise addition, and solarization. Examples of methods using these are MoCo(He et al., 2020), SimCLR (Chen et al., 2020a), BYOL (Grill et al., 2020), and SimSiam (Chen & He, 2021),among others. Other work (Caron et al., 2020) shows how generating additional augmentations using thisstrategy can improve performance relative to two view approaches, though this strategy decreases throughput",
  "Published in Transactions on Machine Learning Research (11/2024)": "Zhizhong Wang, Lei Zhao, Haibo Chen, Lihong Qiu, Qihang Mo, Sihuan Lin, Wei Xing, and DongmingLu. Diversied arbitrary style transfer via deep feature perturbation. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition, pp. 77897798, 2020. 3 J. Xiao, J. Hays, K. A. Ehinger, A. Oliva, and A. Torralba. Sun database: Large-scale scene recognition fromabbey to zoo. In 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition,pp. 34853492, June 2010. doi: 10.1109/CVPR.2010.5539970. 8, 16 Jaejun Yoo, Youngjung Uh, Sanghyuk Chun, Byeongkyu Kang, and Jung-Woo Ha.Photorealistic styletransfer via wavelet transforms. In Proceedings of the IEEE International Conference on Computer Vision,pp. 90369045, 2019. 3",
  "Neural Style Transfer": "Style Transfer techniques combine the semantics (content) of an image with the visual characteristics (style)of another image. These assume that the statistics of shallower layers of a trained CNN encode style, whiledeeper layers encode content. Seminal techniques are based on an optimization-based approach, passing apair of content and style images to a CNN encoder and optimizing over a randomly initialized image toproduce activations with similar statistics to the style image at shallower layers and similar activations tothe content image at deeper ones (Gatys et al., 2015). This way, a stylized image is generated, comprisingthe semantic and texture attributes of interest. While optimization-based methods generate a diverse stylization due to a random image initialization, au-toencoding methods utilize an image decoder to eciently stylize arbitrary image pairs on a single forwardpass. In what follows, we introduce the autoencoding Style Transfer technique adopted by our proposedmethod due on its generalization and eciency properties. For an in-depth survey of Style Transfer meth-ods, refer to Jing et al. (2019).",
  "Self-Supervised Learning": "Traditional SSL methods learn compressed representations by maximizing the agreement between dierentlyaugmented views of the same data example in a latent space. They do this following the template originallyproposed by SimCLR (Chen et al., 2020a). In this setup the input is split into multiple views using dataaugmentations, encoded into a representation, and then further projected into an embedding over which theloss is computed. There are many potential augmentations that can be used, including (but not limitedto) random cropping, ipping, color jitter, blurring, and solarization.By maximizing the similarity of",
  "i2k1 = r(ik),i2k = r(ik),r, r R(1)": "Once augmented views are obtained, a representation is computed using an image encoder (typically a CNNmodel). The representations are then fed to a projection head which further compresses them into a lower-dimensional manifold where dierent views of the same image are close together and those from dierentimages are far apart. Let h and g be the encoder (e.g. a ResNet-50 backbone) and projection head (e.g. anMLP layer), respectively. Then, embeddings are obtained for each augmented sample as zl = g h(il). SimCLR uses the normalized temperature-scaled cross entropy loss (NT-Xent) to learn how to identifypositive pairs of augmented samples. First, the cosine similarity of every pair of embeddings is computed",
  "(3)": "where R++ is the temperature factor and 1 the indicator function. While SimCLR is a simple framework,it pushed the state-of-the-art signicantly on a wide range of downstream tasks including image classication,object detection, and semantic segmentation. Follow up works to SimCLR such as MoCo (Chen et al., 2020b; 2021b), BYOL (Grill et al., 2020) andSimSiam (Chen & He, 2021), among others (Caron et al., 2020; 2021; Assran et al., 2022; Zbontar et al.,2021; Bardes et al., 2021), have largely maintained this template, but have proposed modications to thissetup (e.g. new losses, architectures, or augmentation strategies) which attempt to further improve thedownstream task performance.",
  "where T is a stylization network and zs = F(is) RD is an embedding extracted from the style image viaa feature extractor F, e.g., InceptionV3 (Szegedy et al., 2016)": "We assume zs to be a contracted embedding of the style image (D CHsWs).The stylization net-work T is comprised by L blocks {tl}Ll=1. T extracts high-level features from the content image, alignsthem to the style embedding zs and maps the resulting features to the pixel domain.The style of isencapsulated in zs is transferred to the content image using CIN. This is applied to a particular setof layers to impose the target texture and color scheme by aligning feature maps at dierent scales. : Feature blending and image interpo-lation. A ne-grained control over the nal stylizedimage is obtained via interpolation and blending fac-tors and that operate in the feature and pixeldomains. This prevents augmented views from losingsemantic information due to strong transformations. We dene the set of layers where CIN is applied asA. The normalization imposed via CIN consists of anextended form of instance normalization where thetarget mean and standard deviation are extractedfrom a style representation z.",
  "+ (k)(z)(6)": "where i(k), k {1, . . . , C} corresponds to the k-thinput channel, and the sample mean E[i(k)] and stan-dard deviation (i(k)) are computed along its spatialsupport.Here, (k), (k) : RD R are trainablefunctions that predict scaling and oset values fromthe latent representation z for the k-th input chan-nel. The layers in T are characterized by",
  ": Style Transfer examples. Views gen-erated using style references from the same domain(in-batch) as well as other domains (external). Styl-ization obtained using a blending factor = 0.5": "Style Transfer as data preprocessing.We incorporate Style Transfer to the default preprocessingpipeline of SSL methods. It is worth noting that SASSL is not specic to a particular SSL approach, andcan be readily applied with dierent methods. shows an example of our augmentation pipeline,where Style Transfer is applied after random cropping. A raw input image i0 is cropped, producing a viewthat is taken as the content image ic. Given an arbitrary style image is (we discuss the choice of is below),the Style Transfer block generates a stylized image ics by imposing the texture of is over ic. Finally, thestylized image ics is passed to the remaining augmentation blocks to produce an augmented sample iaug. As discussed in recent work on SSL augmentation (Han et al., 2022; Chen et al., 2020a), adding a strongtransformation to a self-supervised method tends to degrade performance.For this reason, it is crucialto control the amount of stylization imposed in the augmentation stage. We do so by introducing threehyperparameters: probability p , which dictates whether an image is stylized or not, a blending factor to combine content zc and style zs representations, and an interpolation factor to combinecontent ic and stylized ics images. Given style representations extracted from content and style images zc = F(ic) and zs = F(is), respectively,we obtain an intermediate stylized image ics by applying a convex combination based on blending factor .",
  "ics = (1 )ic + ics(9)": "Algorithm 1 describes our proposed Style Transfer data augmentation block. illustrates the eectof the feature blending and image interpolation operations, showcasing their importance to control thestylization eect without degrading the semantic attributes. SASSL operates over minibatches, allowing ecient data pre-processing. Let Ic RBCHcWc and Is RBCHsWs be content and style minibatches, respectively, comprised by B images I(b)cand I(b)s , b {1, . . . , B}. Then, the stylized minibatch Ics RBCHcWc is generated by applying Style Transfer betweena sample from the content batch and a sample from the style batch, given an arbitrary selection criterion. Wepropose two alternatives for selecting style images to balance between augmentation diversity and eciency. Diversifying style references. In contrast to traditional data augmentation, Style Transfer can leverage asecond dataset to extract style references. This opens the possibility of selecting style images from dierentdomains, diversifying the transformations applied to the pretraining dataset. SASSL relies on two approachesfor sampling style references: external and in-batch stylization. External stylization consists on pre-computing representations of an arbitrary style dataset and samplingfrom them during pretraining. This allows controlling the styles to impose on the augmented views while",
  "Downstream Task Performance": "We evaluate the downstream ImageNet classica-tion accuracy of SSL models pretrained via SASSLon the MoCo framework. We compare a MoCo v2model pretrained with our data augmentation vs. aMoCo v2 baseline with default augmentation (Chenet al., 2020b).Note that MoCo v2 and SimCLRuse the same loss, architecture, and augmentations(they dier by MoCos momentum encoding). Pretraining settings. Our pretraining setup is similar to the canonical SSL setup used to pretrain SimCLRand BYOL. We use the same loss, architecture, optimizer, and learning rate schedule as MoCo v2 for faircomparison.We pretrain a ResNet-50 encoder on ImageNet for 1, 000 epochs via SASSL. To measuredownstream accuracy, we add a linear classication head on top of the pretrained backbone and train in asupervised fashion on ImageNet. SASSL pretraining applies Style Transfer only to the left view (no changes in augmentation are applied tothe right view). It is applied with a probability p = 0.8 using blending and interpolation factors drawnfrom a uniform distribution , U(0.1, 0.3). We found that this modest stylization best complimentedthe existing augmentations, avoiding overly-strong transformations that can hinder performance (Han et al.,2022; Chen et al., 2020a). Results. compares the downstream classication accuracy obtained by our SASSL augmentationapproach on MoCo v2 using external stylization from the Painter by Numbers dataset. Results indicate ourproposed augmentation improves downstream task performance by 2.09% top-1 accuracy. This highlightsthe value of Style Transfer augmentation in self-supervised training, where downstream task performancesignicantly boosts by incorporating transformations that decouple content and style. We also report resultswith in-batch stylization in .5.",
  "PBN (Ours)75.0551.8579.279.6371.3587.5696.9783.3674.1889.7588.9795.77": "in appearance such as color and texture. This forces the feature extraction process to rely exclusively onsemantic attributes. As a result, the learned representations may become more robust to domain shifts,improving downstream task performance across datasets. We empirically show this property by evaluatingthe transfer learning performance of image representations trained using SASSL under linear probing andne-tuning scenarios. Downstream settings. We compare the transfer learning accuracy of ResNet-50 pretrained via MoCov2 using SASSL against a MoCo v2 baseline with default data augmentation. The evaluated models arepretrained on ImageNet and transferred to eleven target datasets: ImageNet-1% subset (Chen et al., 2020a),iNaturalist 21 (iNat21) (iNaturalist 2021), Diabetic Retinopathy Detection (Retinopathy) (Kaggle & Eye-Pacs, 2015), Describable Textures Dataset (DTD) (Cimpoi et al., 2014), Food101 (Bossard et al., 2014),CIFAR10/100 (Krizhevsky, 2009), SUN397 (Xiao et al., 2010), Cars (Krause et al., 2013), Caltech-101(Fei-Fei et al., 2004), and Flowers (Nilsback & Zisserman, 2008). To have a clear idea of the eect of the style dataset in SASSLs pipeline, we pretrain ve ResNet-50backbones, each using a dierent style.We use ImageNet, iNat21, Retinopathy, DTD, and Painter byNumbers (PBN) as style datasets. More precisely, we transfer ve models, each pretrained on a dierentstyle, to each of eleven target datasets. We also include ImageNet as target dataset to compare the eect ofdierent styles on downstream task performance. This leads to 60 transfer learning scenarios used to betterunderstand the eect of various styles on dierent image domains. Transfer learning is evaluated in terms of top-1 classication accuracy on linear probing and ne-tuning. Allmodels were pretrained as described in .1. We report mean accuracy across ve trials. Please referto Appendix A.4 for full linear probing and ne-tuning training and testing settings. Results. shows the top-1 classication accuracy obtained via transfer learning. For linear probing,SASSL signicantly improves the average performance on eleven out of twelve target datasets by up to 10%top-1 classication accuracy. For Retinopathy, SASSL obtains on-par linear probing accuracy to the defaultMoCo v2 model. For ne-tuning, all models trained via SASSL outperform the baseline. Results show the average top-1classication accuracy improves by up to 6%. This suggests SASSL generalizes across datasets, spanningfrom textures (DTD) to medical images (Retinopathy). Note that, for a fair comparison, we do not performhyperparameter tuning. Interestingly, the relative performance obtained from dierent style datasets gener-ally diers comparably to the measurement uncertainty, which is shown for these experiments in Section A.5of the supplementary material. This suggests that the choice of style dataset is secondary in importance,while the main benet comes from the use of SASSL itself.",
  "SASSL + MoCo v2 (Ours)20.5546.73": "To further demonstrate the representation learningcapabilities of the data augmentation imposed viaSASSL, we conduct experiments on few-shot classi-cation. We compare our ResNet-50 backbone pre-trained via SASSL + MoCo v2 against a MoCo v2baseline in the context of one and ten-shot learningon ImageNet. shows the few-shot classication accuracy. Results reveal that SASSL boosts few-shot classicationtop-1 accuracy by over 1% in both one and ten-shot learning. This aligns with our previous experiments,suggesting that SASSL promotes more general image representations.",
  "Additional Downstream Performance Evaluation": "Performance on other SSL methods. To assess SASSLs broader impact, we evaluate its eectiveness ontwo other SSL methods, SimCLR and BYOL. We pretrain ResNet-50 backbones with each method, and thenuse linear probing on ImageNet to compare the quality of their learned representations. For each method,default pretraining and linear probing congurations are used. For SASSL, we employ its recommendedhyperparameters (, [0.1, 0.3], p = 0.8) and PBN as style dataset. shows the accuracy attained by SimCLR and BYOL equipped with SASSL. Results show ourproposed data augmentation technique boosts top-1 accuracy by approximately 1% in both cases, highlightingits potential across multiple SSL techniques.",
  "(86M)SASSL + MoCo v3 (Ours)75.5192.56": "Performance on other representation models.We explore SASSLs performance on models withvarying complexity and architecture. For complexity, we employ ResNet-50 (x4), a scaled-up version of thepreviously evaluated ResNet-50 (from 24 to 375 million parameters). This allows us to probe how SASSLscales with increased model size.In terms of architecture, we employ ViT-B/16, a Transformer-basedbackbone with 86 million parameters and a distinct design compared to previous CNN models. We pretrain and linearly probe a ResNet-50 (x4) representation model on ImageNet via MoCo v2. Pretrainingand downstream settings follow our default conguration, as documented in the Appendices A.3 and A.4.Similarly, we pretrain and linear probe a ViT-B/16 model on ImageNet via MoCo v3. In this case, SASSLemployed a blending factor uniformly sampled between 0.1 and 0.5. reports the downstream classication accuracy for ResNet-50 (x4) and ViT-B/16. ResNet-50 (x4)results show SASSL improves top-1 classication accuracy by 1.1%, mirroring its earlier improvement. Sim-ilarly, ViT-B/16 results show SASSL improves top-1 accuracy by 0.5%. These suggest that SASSL is notlimited to CNN backbones, but can also be extended to ViTs. While this margin is currently smaller forViTs, we emphasize that no hyperparameter tuning was employed in these experiments.",
  "Ablation Studies": "To shed light on how SASSL aects accuracy on ImageNet, we break down its components and assessindividual contributions to downstream performance. We also study how aligning dierent layers in thestylization network T boosts accuracy. See Appendices A.6 and A.7 for additional ablations and SASSLscomputational requirements. SASSL components. For the ablation study, we cover four cases: (i) MoCo v2 with default augmentation,(ii) SASSL + MoCo v2 using in-batch representation blending and no pixel interpolation ( = 1), (iii)SASSL + MoCo v2 using in-batch representation blending and pixel interpolation, and (iv) SASSL + MoCov2 using all its attributes (blending, interpolation and an external style dataset). shows our ablation study using MoCo v2 as SSL technique. Results highlight the importance ofcontrolling the amount of stylization using both representation blending and image interpolation. With-out image interpolation, using Style Transfer as data augmentation degrades the downstream classicationperformance by more than 1.5% top-1 accuracy. On the other hand, by balancing the amount of stylization via blending and interpolation, SASSL boostsperformance by more than 1.5%. This is a signicant improvement for the challenging ImageNet scenario.Finally, by incorporating an external style dataset such as PBN, we further improve downstream task per-formance by almost 2.1% top-1 accuracy. This shows the importance of diverse style references and theireect on downstream tasks.",
  "All (13 layers)75.3892.21": "Number of stylized layers. We explore how the number of layers used to apply style transfer via CINaects downstream performance. We analyze three cases: (i) stylizing using the rst two residual blocks ofthe Stylization Network T (four layers from blocks 1 and 2), (ii) the rst four residual blocks (eight layersfrom blocks 1 to 4), and (iii) all ve residual blocks (ten layers). For each case, we pretrain and linearly probe a ResNet-50 on ImageNet using SASSL + MoCo v2 with itsrecommended settings (, [0.1, 0.3], p = 0.8) and PBN as style dataset. To fully remove the eect ofa style embedding zs, our comparison includes a model pretrained using the content image itself as stylereference (z = zc). We also compare our full SASSL + MoCo v2 model, stylizing all residual and upsamplingblocks of T (thirteen layers). shows a progressive enhancement in accuracy with increasing stylization depth. Adding stylizationto the rst four layers showed negligible gains, mirroring the accuracy of the unaligned model. Stylizingthe rst eight and ten layers yielded modest improvements of 0.34% and 0.52%, respectively, implying agrowing inuence of deeper layers on accuracy. Notably, pretraining with full stylization, encompassing bothresidual and upsampling layers, attains a 1.61% accuracy boost, suggesting the importance of aligning deeperupsampling layers for downstream performance.",
  "Conclusion": "We propose SASSL, a novel data augmentation approach based on Neural Style Transfer that exclusivelytransforms the style of training samples, diversifying data augmentation during pretraining while preservingsemantic attributes. We empirically show our approach outperforms well-established methods such as MoCov2, SimCLR and BYOL by up to 2% top-1 classication accuracy on ImageNet. SASSL also improves thetransfer capabilities of learned representations, enhancing linear probing and ne-tuning performance acrossdomains by up to 10% and 6% top-1 accuracy, respectively. Our technique can be extended to other SSLmethods and models with minimum hyperparameter changes, as experimentally shown.",
  "Broader Impact Statement": "This work proposes a novel data augmentation approach leveraging Neural Style Transfer to enhance Self-supervised Learning, particularly for domains with limited data or expensive annotations.Our methodutilizes semantic-aware image preprocessing to extract robust representations that generalize across diversedomains. This advancement tackles the critical challenge of using unlabeled data for Deep Learning, whichhas many potential positive impacts in both technical and societal fronts. SASSLs style transfer componenthelps to reduce sensitivity to image texture, potentially improving model robustness to texture bias. However,since our method primarily modies data augmentation, it may not fully address other potential biases arisingfrom pre-training datasets or learning strategies.",
  "The authors would like to thank Arash Afkanpour and Luyang Liu for their insightful comments and feed-back": "Martin Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jerey Dean, Matthieu Devin,Sanjay Ghemawat, Georey Irving, Michael Isard, Manjunath Kudlur, Josh Levenberg, Rajat Monga,Sherry Moore, Derek G. Murray, Benoit Steiner, Paul Tucker, Vijay Vasudevan, Pete Warden, MartinWicke, Yuan Yu, and Xiaoqiang Zheng. Tensorow: A system for large-scale machine learning. In 12thUSENIX Symposium on Operating Systems Design and Implementation (OSDI 16), pp. 265283, 2016.URL 18 Mahmoud Assran, Mathilde Caron, Ishan Misra, Piotr Bojanowski, Florian Bordes, Pascal Vincent, ArmandJoulin, Mike Rabbat, and Nicolas Ballas. Masked siamese networks for label-ecient learning. In EuropeanConference on Computer Vision, pp. 456473. Springer, 2022. 2, 4 Yingbin Bai, Erkun Yang, Zhaoqing Wang, Yuxuan Du, Bo Han, Cheng Deng, Dadong Wang, and TongliangLiu. Rsa: Reducing semantic shift from aggressive augmentations for self-supervised learning. Advancesin Neural Information Processing Systems, 35:2112821141, 2022. 3",
  "Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101 mining discriminative componentswith random forests. In European Conference on Computer Vision, 2014. 8, 16": "Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsu-pervised learning of visual features by contrasting cluster assignments. Advances in neural informationprocessing systems, 33:99129924, 2020. 2, 4 Mathilde Caron, Hugo Touvron, Ishan Misra, Herv Jgou, Julien Mairal, Piotr Bojanowski, and ArmandJoulin.Emerging properties in self-supervised vision transformers.In Proceedings of the IEEE/CVFinternational conference on computer vision, pp. 96509660, 2021. 2, 4",
  "Leon A Gatys, Alexander S Ecker, and Matthias Bethge. A neural algorithm of artistic style. arXiv preprintarXiv:1508.06576, 2015. 4": "Leon A Gatys, Alexander S Ecker, and Matthias Bethge. Image style transfer using convolutional neuralnetworks.In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp.24142423, 2016. 3 Leon A Gatys, Alexander S Ecker, Matthias Bethge, Aaron Hertzmann, and Eli Shechtman. Controllingperceptual factors in neural style transfer. In Proceedings of the IEEE Conference on Computer Visionand Pattern Recognition, pp. 39853993, 2017. 3 Robert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias Bethge, Felix A Wichmann, and WielandBrendel. Imagenet-trained cnns are biased towards texture; increasing shape bias improves accuracy androbustness. arXiv preprint arXiv:1811.12231, 2018. 3 Jean-Bastien Grill, Florian Strub, Florent Altch, Corentin Tallec, Pierre Richemond, Elena Buchatskaya,Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap yourown latent-a new approach to self-supervised learning. Advances in neural information processing systems,33:2127121284, 2020. 2, 4, 18, 19 Junlin Han, Pengfei Fang, Weihao Li, Jie Hong, Mohammad Ali Armin, Ian Reid, Lars Petersson, and Hong-dong Li. You only cut once: Boosting data augmentation with a single cut. In International Conferenceon Machine Learning, pp. 81968212. PMLR, 2022. 6, 7",
  "David J Heeger and James R Bergen.Pyramid-based texture analysis/synthesis.In Proceedings of theConference on Computer Graphics and Interactive Techniques, pp. 229238, 1995. 3, 21": "Eric Heitz, Kenneth Vanhoey, Thomas Chambon, and Laurent Belcour. A sliced Wasserstein loss for neu-ral texture synthesis.In Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition, pp. 94129420, 2021. 3 Minui Hong, Jinwoo Choi, and Gunhee Kim. Stylemix: Separating content and style for enhanced dataaugmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,pp. 1486214870, 2021. 3",
  "Yanghao Li, Naiyan Wang, Jiaying Liu, and Xiaodi Hou. Demystifying neural style transfer. arXiv preprintarXiv:1701.01036, 2017a. 3": "Yijun Li, Chen Fang, Jimei Yang, Zhaowen Wang, Xin Lu, and Ming-Hsuan Yang. Universal style transfer viafeature transforms. In Proceedings of the 31st International Conference on Neural Information ProcessingSystems, NIPS17, pp. 385395, Red Hook, NY, USA, 2017b. Curran Associates Inc. ISBN 9781510860964.3 Songhua Liu, Tianwei Lin, Dongliang He, Fu Li, Meiling Wang, Xin Li, Zhengxing Sun, Qian Li, and ErruiDing. Adaattn: Revisit attention mechanism in arbitrary neural style transfer. In Proceedings of theIEEE/CVF International Conference on Computer Vision, pp. 66496658, 2021. 3",
  "Javier Portilla and Eero P Simoncelli. A parametric texture model based on joint statistics of complexwavelet coecients. International Journal of Computer Vision, 40(1):4970, 2000. 3, 21": "Senthil Purushwalkam and Abhinav Gupta. Demystifying contrastive self-supervised learning: Invariances,augmentations and dataset biases. Advances in Neural Information Processing Systems, 33:34073418,2020. 3 Colorado J Reed, Sean Metzger, Aravind Srinivas, Trevor Darrell, and Kurt Keutzer. Selfaugment: Auto-matic augmentation policies for self-supervised learning. In Proceedings of the IEEE/CVF conference oncomputer vision and pattern recognition, pp. 26742683, 2021. 3",
  "Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition.arXiv preprint arXiv:1409.1556, 2014. 3": "Christian Szegedy, Vincent Vanhoucke, Sergey Ioe, Jon Shlens, and Zbigniew Wojna.Rethinking theinception architecture for computer vision. In Proceedings of the IEEE conference on computer vision andpattern recognition, pp. 28182826, 2016. 5 Yonglong Tian, Chen Sun, Ben Poole, Dilip Krishnan, Cordelia Schmid, and Phillip Isola. What makes forgood views for contrastive learning?Advances in neural information processing systems, 33:68276839,2020. 3",
  "Yang You, Igor Gitman, and Boris Ginsburg. Large batch training of convolutional networks. arXiv preprintarXiv:1708.03888, 2017. 18": "Jure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and Stphane Deny. Barlow twins: Self-supervised learningvia redundancy reduction. In International Conference on Machine Learning, pp. 1231012320. PMLR,2021. 4 Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable eectivenessof deep features as a perceptual metric. In Proceedings of the IEEE Conference on Computer Vision andPattern Recognition, pp. 586595, 2018. 21"
}