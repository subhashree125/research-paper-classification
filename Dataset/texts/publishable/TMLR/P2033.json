{
  "Abstract": "Pre-trained deep neural networks (DNNs) are being widely deployed by industry for makingbusiness decisions and to serve users; however, a major problem is model decay, where theDNNs predictions become more erroneous over time, resulting in revenue loss or unhappyusers. To mitigate model decay, DNNs are retrained from scratch using old and new data.This is computationally expensive, so retraining happens only once performance significantlydecreases. Here, we study how continual learning (CL) could potentially overcome modeldecay in large pre-trained DNNs and greatly reduce computational costs for keeping DNNsup-to-date. We identify the stability gap as a major obstacle in our setting. The stabilitygap refers to a phenomenon where learning new data causes large drops in performance forpast tasks before CL mitigation methods eventually compensate for this drop. We test twohypotheses to investigate the factors influencing the stability gap and identify a method thatvastly reduces this gap. In large-scale experiments for both easy and hard CL distributions(e.g., class incremental learning), we demonstrate that our method reduces the stability gapand greatly increases computational efficiency. Our work aligns CL with the goals of theproduction setting, where CL is needed for many applications 1.",
  "Introduction": "Deep neural networks (DNNs) are now widely deployed in the industry; however, a major problem for manycompanies is model decay, where the predictive performance of a DNN degrades over time (Zhou et al., 2020).This is primarily caused by concept drift (Tsymbal, 2004; Gama et al., 2014; Lu et al., 2018), where thenature of the predicted target variables changes over time, e.g., for a classifier, this would correspond to theintroduction of new categories or an expanded definition individual classes. When model decay is detected,most companies employ offline retraining (i.e., batch or joint retraining), where a model is retrained fromscratch with a combination of old and new data (Egg, 2021). This is very expensive, so model monitoring isused to determine when offline training is required (Mkinen et al., 2021). Despite frequent offline training,deployed models still suffer from accuracy drops of up to 40% (Mallick et al., 2022). Continual learning (CL) is a promising solution for preventing model decay (Huyen, 2022a;b; Jain & Shenoy,2022), where in CL the goal is to update a DNN incrementally with new data while preserving the previousknowledge (Parisi et al., 2019). In a GrubHub study, this enabled them to avoid model decay and provided a45 decrease in training costs compared to daily offline training (Egg, 2021). To do this, GrubHub employedonline updates on new samples. Online updates can cause catastrophic forgetting of past knowledge due toconcept drift (McCloskey & Cohen, 1989), but due to the rapidly changing preferences of their customers,forgetting past knowledge was desirable. However, catastrophic forgetting is unacceptable for many industryapplications because past knowledge must be retained. For CL to be widely adopted by industry, it mustbe shown to inhibit model decay in pre-trained models, provide significant computational benefits, work for",
  "(b) Stability gap as an average over all rehearsals": ": An overview of stability gap phenomenon. The stability gap is a phenomenon that occursin CL when learning new data, where accuracy on previously learned data (Y-axis) drops significantly as afunction of training iterations when a new distribution is introduced (X-axis). Fig.(a) illustrates this behaviorduring CIL, where a network pre-trained on ImageNet-1K, learns 365 new classes from Places365-LT over fiverehearsal sessions. Each rehearsal session involves 600 iterations that combine samples from the old and newtasks. A gray dotted vertical line marks the end of a rehearsal session or a task transition. When rehearsalbegins, accuracy on the old task for the conventional rehearsal drops dramatically before slowly recovering,although it fails to recover the original performance on the old data. The traditional measures of catastrophicforgetting focus on performance at task transitions (red diamonds), ignoring significant forgetting that occursduring the learning process between task transitions. Fig.(b) shows the stability gap in the learning curveaveraged over five rehearsal sessions. In this work, we attempt to mitigate the stability gap. arbitrary shifts in concepts, and ideally be as effective as offline training. The majority of CL algorithmsdeviate from these criteria, i.e., they impose constraints on storage that are irrelevant to industry problems,shun pre-trained models, perform worse than offline retraining, design algorithms for only a single conceptdrift distribution (e.g., class incremental learning), and/or are more computationally expensive than offlineretraining (Harun et al., 2023a;b; Prabhu et al., 2023b;a; Verwimp et al., 2024; Lee et al., 2023). Here, we study using CL to mitigate model decay and to introduce new concepts into a DNN. Initially,we studied updating ImageNet-1K pre-trained models with new classes in class incremental learning (CIL)experiments. We used computationally constrained cumulative rehearsal to mitigate forgetting, where re-hearsal involves mixing old samples with new ones to prevent forgetting, and cumulative rehearsal doesthis using all previously observed samples. Computational constraints were enforced by limiting the totalnumber of rehearsal updates permitted. However, we found that CLs stability gap was a major obstacle toour goal (De Lange et al., 2023), as shown in . The stability gap refers to the observation that when a model is updated on new data, accuracy on theclasses observed in earlier batches plummets before rehearsal or other CL methods gradually recover oldperformance. In the typical CIL setting we adopted, samples arrive in batches where each batch containsclasses only within that batch. As seen in , the performance plummets when learning a new batchduring the rehearsal sessions and does not fully recover. The stability gap can be seen as transient forgettingdue to introducing a new task. Typically, catastrophic forgetting refers to the performance drops observedat the end of learning new tasks (at task transitions). In contrast, the stability gap refers to the performancedrops observed over learning steps (between task transitions). In our work, we seek to understand whythe stability gap happens in our scenario and how to effectively mitigate it, enabling using fewer rehearsalupdates while achieving higher accuracy.",
  "We test two hypotheses to examine the stability gap in CIL with pre-trained DNNs:": "1. The stability gap is increased in part due to having a large loss at the output layer forthe new classes. To test this hypothesis, we study two methods to mitigate the large loss 2 in theoutput layer for the new classes. The first method is to initialize the output layer in a data-drivenapproach rather than randomly initializing the output units responsible for the new classes. Thesecond method is a specialized form of soft targets for the network, rather than the typical hardtargets used for the network, where these soft targets are designed to improve performance for thenew classes while minimally perturbing others. 2. The stability gap is increased in part due to excessive network plasticity. We test thishypothesis by controlling the level of plasticity in network layers in a dynamic manner. For hiddenlayers, we test this hypothesis using LoRA (Hu et al., 2021), which reduces the number of trainableparameters in the hidden layers of the network. For rehearsal methods, after each rehearsal ses-sion, these weights are folded into the original network weights. For the output layer, we test thishypothesis by freezing the output units for classes seen in earlier batches during rehearsal.",
  "This paper makes the following major contributions:": "1. We are the first to study overcoming the stability gap. We test the aforementioned hypotheses inCIL and discover that they both play an important role. We propose novel metrics to measure thestability, plasticity, and continual knowledge gaps. 2. We are the first to study maintaining or improving performance on ImageNet-1K while learningadditional classes, an aspect not investigated in previous research employing ImageNet-1K pre-trained backbones (Wang et al., 2022b;a; Smith et al., 2023; Gao et al., 2023; McDonnell et al., 2024).We study this for both the CIL and IID (independent and identically distributed) distributions.These experiments are conducted by combining ImageNet-1K with Places365 or Places365-LT (acombined 1365 total classes). 3. We develop a method that greatly mitigates the stability gap and significantly improves computa-tional efficiency. Our method requires 16.7 fewer network updates than a jointly-trained (upperbound) model. In terms of TFLOPs, our method provides a 31.9 speedup (see ). For theIID CL distribution, the method achieves backward transfer, where learning the new dataset helpsto improve ImageNet-1K accuracy. 4. We show that our method is effective and we integrate it into existing CL methods. Specifically, itperforms well in conjunction with the rehearsal methods Vanilla Rehearsal, DERpp, GDumb, andREMIND as well as the non-rehearsal method LwF.",
  "Background": "Many methods have been proposed to learn continuously from non-stationary datasets in continual learn-ing (see Zhou et al. (2023) for review). These methods can be broadly divided into three categories: 1)Rehearsal-based methods store or reconstruct a subset of old data to rehearse alongside new data whilelearning a new batch (Chaudhry et al., 2019; Hou et al., 2019; Rebuffi et al., 2017; Wu et al., 2019), 2)Regularization-based methods constrain weight updates by adding additional regularization in the lossfunction (Aljundi et al., 2018; Chaudhry et al., 2018; Dhar et al., 2019; Kirkpatrick et al., 2017), and 3)Parameter-isolation based methods allocate multiple sets of parameters or multiple copies of the modelto different incremental batches (Douillard et al., 2021; Yan et al., 2021; Yoon et al., 2020). Since parameter",
  "isolation methods do not allow backward transfer, the stability gap does not apply to them De Lange et al.(2023)": "While most of the CL research community has focused on mitigating catastrophic forgetting (McCloskey& Cohen, 1989), there is a growing body of literature demonstrating its ability to reduce the amount ofcompute needed to update the network (Ghunaim et al., 2023; Harun et al., 2023a;b;c; Prabhu et al., 2023b;Verwimp et al., 2024). Recently, a major obstacle to this objective has been identified in CL: The stabilitygap (De Lange et al., 2023). However, it has been studied on small datasets, e.g., CIFAR, using randomlyinitialized DNNs. This setting is not aligned with studying model decay in industrial settings that involvemany-class data streams and preserving performance in large pre-trained DNNs. Traditionally, the training-from-scratch CL would start with a randomly initialized DNN that had no pre-training. With the prevalence of large pre-trained models, recent CL research has started integrating thesemodels into the learning process. This has resulted in a growing number of CL works that utilize largepre-trained vision transformer (ViT) models, which have been pre-trained on ImageNet-1K or ImageNet-21K (Wang et al., 2022b;a; Smith et al., 2023; Gao et al., 2023; McDonnell et al., 2024). Moreover, manyearlier works use pre-training for CL (Belouadah & Popescu, 2019; Hou et al., 2019; Castro et al., 2018;Rebuffi et al., 2017; Hayes et al., 2020; Douillard et al., 2020; Harun et al., 2023b) and emphasize thesignificance of pre-training in the context of CL (Lee et al., 2023; Mehta et al., 2023; Ostapenko et al.,2022; Ramasesh et al., 2021b; Gallardo et al., 2021). However, as demonstrated in several works (Wanget al., 2022a; Mirzadeh et al., 2022), using pre-trained models does not naively enhance CL performance andeffectively leveraging pre-trained models for CL remains an open question. In this work, we mainly focus onovercoming the stability gap in CL with pre-trained models. Almost all CL methods focus on the catastrophic forgetting problem with evaluations occurring on discretebatch or task transitions (Hayes et al., 2018; Chaudhry et al., 2018) and fail to capture the stability gap thatoccurs immediately after a new task is introduced (). De Lange et al. (2023) demonstrates the stabilitygap occurs for a variety of CL methods, including rehearsal (Chaudhry et al., 2019), GEM (Lopez-Paz &Ranzato, 2017), EWC (Kirkpatrick et al., 2017)), and LwF (Li & Hoiem, 2017). To quantify the stabilitygap, they propose continual evaluation metrics based on worst-case performance i.e., the largest drop inaccuracy on old batches. However, their metrics do not enable comparing different CL models because theyonly quantify the largest drop relative to the same models best performance, thus making it impossible toquantify the impact of different training methodologies aimed at mitigating the stability gap. We addressthis limitation using novel metrics that are normalized against a jointly-trained universal upper bound (seeSec. 3.2). We also illustrate this phenomenon using synthetic examples in Appendix F to build intuitions.We study overcoming the stability gap with both rehearsal and non-rehearsal methods, with a focus onrehearsal due to its effectiveness (van de Ven et al., 2022; Zhou et al., 2023).",
  "Continual Learning Protocol": "In this work, we attempt to align CL with the industry setting, where CL updates knowledge in pre-trainedmodels to prevent model decay when new concepts are introduced while preserving performance on theoriginal classes. Likewise, because the industry requires maintaining high accuracy, we focus on rehearsal-based CL; nonetheless, we demonstrate that our mitigation strategies can be used for non-rehearsal methodsthat use regularization and knowledge distillation. In this section, we formalize our CL framework and definethe metrics we use to measure the stability gap.",
  "Formal Continual Learning Setting": "To align our work with addressing model decay, we assume CL begins with a pre-trained model capable ofK-way classification. The goal is to incorporate new data, which may have additional classes, into the model,while preserving or improving performance on the original K classes. For this purpose, we use ImageNet-1K pre-trained models (K = 1000). While ImageNet-1K pre-trained models are commonly employed inCL systems, previous studies do not attempt to maintain or measure performance on ImageNet-1K datasetitself (Wang et al., 2022b;a; Smith et al., 2023; Gao et al., 2023; McDonnell et al., 2024). Our research",
  "Published in Transactions on Machine Learning Research (09/2024)": "Yue Wu, Yinpeng Chen, Lijuan Wang, Yuancheng Ye, Zicheng Liu, Yandong Guo, and Yun Fu. Largescale incremental learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition, pp. 374382, 2019. Shipeng Yan, Jiangwei Xie, and Xuming He. Der: Dynamically expandable representation for class incremen-tal learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,pp. 30143023, 2021. Jaehong Yoon, Saehoon Kim, Eunho Yang, and Sung Ju Hwang. Scalable and order-robust continual learningwith additive parameter decomposition. In Eighth International Conference on Learning Representations,ICLR 2020. ICLR, 2020. Chang-Bin Zhang, Jia-Wen Xiao, Xialei Liu, Ying-Cong Chen, and Ming-Ming Cheng.Representationcompensation networks for continual semantic segmentation. In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pp. 70537064, 2022. Wenxuan Zhang, Youssef Mohamed, Bernard Ghanem, Philip Torr, Adel Bibi, and Mohamed Elhoseiny.Continual learning on a diet: Learning from sparsely labeled streams under constrained computation. InThe Twelfth International Conference on Learning Representations, 2023. Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva, and Antonio Torralba. Places: A 10 million imagedatabase for scene recognition. IEEE transactions on pattern analysis and machine intelligence, 40(6):14521464, 2017.",
  "Measuring the Stability Gap": "Popular CL metrics focus on measuring performance after each Sj is learned. They do not permit fine-grained analysis for a) preserving old knowledge, b) acquiring new knowledge and c) balancing both duringtraining. To study the stability gap in a manner that enables us to test our hypotheses across models trainedwith different strategies, we created metrics that measure these criteria: 1) the stability gap, S (criterion a)2) the plasticity gap, P (criterion b), and 3) the continual knowledge gap, CK (criterion c). Each metricasks: How much performance does a learner lack on previously observed, recently observed, orall observed data compared to a joint model (upper bound) when learning new data? For the jth learning session, we denote evaluation sets on old, new, and all seen data by E1:j1, Ej, andE1:j, respectively. Ai is the accuracy of the current model i on batch evaluation set E at training iterationi, and L is the total number of iterations. Ajoint is the final accuracy of a joint model (joint) trained jointlyon all data. We define the stability gap as",
  "Ajoint(E1:j, joint).(3)": "j records CL performance compared to Ajoint or Abest. After learning all N batches, j scores are averagedto indicate average performance gain. The first batch is excluded since that is used for pre-training. For allmetrics, smaller S, P, and CK indicate better performance. When Ai = Ajoint and Ai = Abest for allL iterations, S, P, and CK become zero which is desirable. A negative value means knowledge transferbetween new and old batches, which is desirable. These metrics apply for offline CL (i.e., incremental-batchCL) and online CL with any data distributions, including CIL and IID.",
  "hj2,(4)": "where wk Rd is the output layer weight vector for class k, hj Rd is the jth embedding from thepenultimate layer, and V is the number of samples from class k in the batch. Intuitively, this approach aimsto align the initial weights more closely with the distribution of the new classs data, facilitating faster andmore effective learning adjustments during the initial phase of CL. Hard vs.Dynamic Soft Targets. For classification, models are often trained with hard targets, i.e.,at training iteration i a one-hot vector ti with a 1 in position k corresponding to the correct class. Wehypothesize hard target training is partially responsible for the stability gap. Intuitively, hard targets areone-hot encoded and enforce strict inter-class independence despite several classes sharing distributionalsimilarities.This property of hard targets, therefore, also causes a large initial loss when learning newclasses. Soft targets, on the other hand, can help the network retain the joint inter-class distributions, whichfurther ameliorates the perturbation of learned classes. To test this, we use soft targets constructed suchthat the models predictions on previously learned classes are largely preserved. At learning iteration i, let P (k|xi; i) be the models output softmax probabilities for sample xi from class kgiven the models current parameters i and the predicted class be yi = arg maxkP (k|xi; i). We maintain arunning average vector uk RK of the softmax probabilities for each class that is updated when an examplefrom class k is observed, i.e.,",
  "ck + 1,(5)": "where ck is a counter for class k that is subsequently increased by 1, and uk is initialized to a uniformdistribution prior to the running updates. Subsequently, soft targets ti for iteration i are constructed bysetting ti uk and then setting the element for the correct class to 1, i.e., ti [k] 1. If yi = k, then wealso set ti [yi] 1/K. Subsequently, ti is normalized to sum to 1 and used to update the network. Thisstrategy results in targets that minimally perturb the network and smaller loss values. In Appendix. G, weillustrate the process of updating soft targets. Limiting Hidden Layer Plasticity Using LoRA. To accumulate knowledge over time, most CL ap-proaches update the entire network. Given that each batch of data in CL is relatively small, we hypothesizethat this leads to excessively perturbing hidden representations, leading to a larger stability gap. To testthis hypothesis, we constrain the number of trainable parameters in hidden representations using a networkadaptor. While there are various network adaptors (Han et al., 2024) that restrict network plasticity, weuse low-rank adaptation (LoRA) (Hu et al., 2021) due to its simplicity and effectiveness. Specifically, weinject LoRA weights into the linear layers of the network, and only these parameters and the output layerare updated, which greatly reduces the number of trainable parameters.",
  "j = Wj1 + BA,(6)": "where B Rdr and A Rrg are the LoRA adapter parameters with rank r min(d, g). Only B and Aare plastic, with A initialized with random Gaussian values and B initialized to a zero matrix, so BA = 0at the beginning of the learning session. At the end of the session, the LoRA parameters are folded into thenetwork, i.e., Wj j. In LoRA experiments, only the output layer and the LoRA parameters are plastic. Limiting Output Layer Plasticity via Targeted Freezing. In CIL, large changes in the networksrepresentations for old classes increase the stability gap. While LoRA restricts plasticity in hidden represen-tations, we hypothesize that restricting plasticity in the output layer could also be helpful for CIL. Therefore,",
  "Experimental Setup": "Bounding Compute. In a real-world scenario, a continual learner must adapt to a large-scale data stream,which may not be feasible if more computation is required over time. Recently, many works have advocatedcomputational efficiency for CL (Prabhu et al., 2023b; Harun et al., 2023b;a;c; Verwimp et al., 2024; Zhanget al., 2023). In our experiments, we bound compute by a fixed number of training iterations or SGD steps. Rehearsal. Because our goal is to understand and mitigate the stability gap, our main results use rehearsaland assume the learner has access to all previously observed data, with no constraints on storage. This isaligned with finding a better alternative to periodically retraining from scratch as more data is acquired,which is commonly done in industry where the computational budget depends on compute to a far greaterextent than data storage (Prabhu et al., 2023b;a). Continual Learning Procedure. We aim to study CL in an industry-like setting, requiring a large-scaledata stream with numerous object categories. However, it is difficult to find a suitable dataset well-curatedand suitable for large-scale CL experiments. Therefore, we construct a large-scale data stream by combiningImageNet-1K with another dataset, Places365-LT, which is a variant of the Places365 dataset. Places365is challenging (Liu et al., 2019) and is widely used for out-of-distribution (OOD) detection with ImageNetpre-trained models (Zhu et al., 2022). During CL, the model sequentially learns 5 incremental batches of data from Places365-LT. In the CILordering, each CL batch contains 73 categories, and in the IID ordering each CL batch has 12500 examples.During rehearsal, the model is updated over 600 minibatches, where the minibatch consists of 128 sampleswhere 50% are selected randomly from the current CL batch and 50% from data seen in earlier CL batchesand ImageNet-1K. We study two CL orderings for Places365-LT: 1) CIL ordering where each batch hasclasses exclusively seen in that batch (maximum concept drift), and 2) IID ordering where each batchcontains examples from randomly sampled classes (minimal concept drift). These are two opposite extremesituations in CL. Although the stability gap is not known to occur in the IID CL setting, this setting iscritical to demonstrating the algorithms generality. To measure the stability gap and other metrics, weassess performance during rehearsal every 50 training iterations, where the test set consists of the ImageNet-1K validation set and all of the classes from Places365-LT from the current and prior CL batches. Additionalimplementation and dataset details are given in Appendix C and B. Network Architecture. We choose a network architecture that is amenable to LoRA and performs wellin offline training with ImageNet-1K compared to similar-sized DNNs. In our main results, we study CLusing the ConvNeXtV2-Femto (Woo et al., 2023) CNN that has been pre-trained on ImageNet-1K usinga fully convolutional masked autoencoder framework followed by supervised fine-tuning on ImageNet-1K.While ResNet18 is widely used in CL, it under-performs other lightweight CNNs in CL (Hayes & Kanan,2022; Harun et al., 2023b). ConvNeXtV2-Femto has 5.2M parameters, which is 2 less than ResNet18s11.6M parameters, and it outperforms ResNet18 by absolute 8.47% in final top-1 accuracy on ImageNet-1K.",
  "SGM (Combined Method)1.450.0060.0820.00277.6473.7070.30": "It is worth noting that LoRA is not appropriate for CNN architectures (e.g., ResNet) that lack 1 1convolutional layers, but it can be used with ViT, ConvNeXtV1, ConvNeXtV2, and other DNN architectureswith these linear layers. Each block of ConvNeXt consists of one 2D convolutional layer and two 1 1convolutional layers. For experiments using LoRA, the 1 1 convolutional layers in ConvNeXt blocks aremodified to incorporate LoRAs weights using Equation 6. The number of trainable LoRA weights is 0.92M,which is much less than the total number of hidden layer parameters (5.08M). Based on prior work, earlylayers in the network are universal feature extractors and are little altered during CL (Ramasesh et al.,2021a; Ebrahimi et al., 2020; Pellegrini et al., 2020; Harun et al., 2024), so we freeze the first 4 blocks of theCNN in all experiments, leaving the remaining 8 blocks plastic (97.7% of the parameters) which consist of5.08M parameters.",
  "Here, we describe the results for evaluating our two hypotheses": "Baselines. In our experiments, we evaluate each of the methods in Sec. 4 individually and the combinedSGM method. As baselines, we compare SGM with rehearsal against vanilla rehearsal, which uses unlimitedstorage for rehearsal without any additional components, as well as joint models (upper bound). The jointmodels are jointly trained on ImageNet and the CL batches seen up to the current batch. We also includenaive finetuning which is a vanilla variant without rehearsal and serves as a lower bound. Finally, we comparewith the output layer only which updates the output layer with rehearsal while freezing all other layers. Results. Our main CIL results are given in . SGM with rehearsal shows the greatest reduction inthe stability gap (S), plasticity gap (P), and continual knowledge gap (CK). It also performs best inother metrics. Of its components, LoRA reduces the stability gap the most; however, the stability gap forLoRA is 3 higher than SGM. We next turn to examining the support for our two hypotheses. Hypothesis 1. Our first hypothesis was that the stability gap in CIL is caused by having a large loss at theoutput layer due to the new classes, which we tested by using weight initialization and dynamic soft targets.Both methods are effective at achieving the goal of reducing the initial loss, especially weight initialization (seea). As shown in , both methods reduce the stability gap. We observe that weight initializationalso greatly reduces the plasticity gap. b shows the average performance on ImageNet-1K during the5 rehearsal sessions for hard vs. soft targets, which reveals that hard targets increase the stability gap to agreater extent than dynamic soft targets.",
  "(c) Plastic layers": ": Mitigation methods averaged over 5 rehearsal sessions during CIL. (a) The loss on new classeswhen only training the output layer, which reveals soft targets and data-driven weight initialization greatlyreduce the initial loss. (b) Accuracy on ImageNet-1K for hard vs. soft targets, which shows that soft targetsreduce the stability gap. (c) Network plasticity increases the stability gap. : Average CIL Results. Results after learning ImageNet-1K followed by Places365-LT over 5rehearsal sessions. denotes average accuracy (%) over rehearsal sessions and is final accuracy (%) on all1365 classes. stands for final accuracy (%) on ImageNet-1K only. #P denotes the trainable parametersin Millions. We report the average over 6 data orderings with standard deviation ().",
  "Joint5.0877.5870.69": "Vanilla5.080.020 0.00170.385 0.00910.031 0.001575.81 0.158571.68 0.123667.94 0.1721Output Layer0.530.021 0.00110.473 0.02500.032 0.000975.79 0.287571.28 0.141067.68 0.2710SGM1.450.001 0.00120.087 0.00820.002 0.000777.70 0.050973.71 0.076370.31 0.0682 Hypothesis 2. Our second hypothesis was that the stability gap in CIL is caused by excessive networkplasticity, which we tested by using LoRA and OOCF. c shows the average results on ImageNet-1Kacross the 5 rehearsal sessions for LoRA vs. when only the output layer, top 8 blocks (vanilla), or entirenetwork are trainable. This reveals that plasticity plays a major role in the stability gap; however, this doesnot translate directly into the number of trainable parameters since LoRA includes the output layer butexhibits a smaller decrease in performance than training only the output layer. Unlike others, LoRA fullyrecovers old performance. As seen in , both OOCF and especially LoRA reduce the stability gap. Interim Conclusions. Our experimental results support both hypotheses. LoRA mitigates the stabilityand continual knowledge gaps the most. Our data-driven weight initialization greatly improves the plasticity.This aligns with the prior study that suggests that weight initialization is critical for plasticity in DNN (Lyleet al., 2023). We find that SGM, a method that combines the approaches used to test these hypotheses,greatly reduces the stability gap. CIL with Multiple Data Orderings. To ensure the robustness of our findings, we extend our experimentsto multiple data orderings. In addition to studying SGM and vanilla rehearsal, we examine only training theoutput layer during rehearsal. The averaged results across 6 data orderings are given in . We findthat SGM consistently mitigates the stability gap. Compared to vanilla, SGM provides 20 more stability,4.4 more plasticity, and 15.5 more continual knowledge. SGM also outperforms training only the outputlayer in all criteria. This indicates that updating representations in hidden layers besides the output layerusing SGM is critical for learning new and retaining old knowledge. Moreover, SGM balances stability andplasticity (see ).",
  ": Speed of acquiring new knowl-edge.SGM requires fewer updates andTFLOPs than vanilla to reach 99% of the bestaccuracy on new classes (highlighted)": "One of our goals in studying CL and the stability gap isto enable more computationally efficient training. To studywhether SGM achieves this goal, we evaluated performanceduring CIL, where we measured performance on old and newclasses every 10 iterations. For new classes, we measured thenumber of updates and FLOPs (floating-point operations)needed to achieve 99% of the best accuracy. As shown in , SGM learns new classes much faster thanvanilla rehearsal, and on average, it is 61.8% more efficientin terms of network updates. Moreover, because the curveshows a decreasing trend as learning progresses this meansthat SGM becomes a more efficient learner over time, withfewer updates and TFLOPs for the current batch than pre-vious batches being needed. For old class performance, wemeasured the number of iterations during rehearsal requiredto recover the performance of a joint model (upper bound)on ImageNet-1K for vanilla and SGM. As shown in ,SGM requires far fewer iterations to recover old performancewhereas vanilla rehearsal (without SGM) fails to recover oldperformance fully. Compared to a joint model, SGM providesa 16.7 speedup in number of network updates and a 31.9 speedup in TFLOPs (see ). Moreover,SGM requires 18 less training time than the joint model on the same hardware. : Speed of Recovering Old Knowledge.SGM compared to vanilla (without SGM) for thenumber of iterations needed to recover 97%, 98%, 99%, or 100% of the accuracy on ImageNet-1K for a jointmodel as each of the 5 tasks (denoted by Ti where i {1, 5}) from Places365-LT is learned during CIL. Ahyphen indicates the model did not recover performance whereas zero means there was no stability gap.",
  "Additional Experiments: SGMs Generality": "This section expands on the versatility and efficacy of the SGM method by exploring its application acrossvarious settings and constraints. First, we show how SGM performs in the IID CL setting in Sec. 6.1. Next, westudy storage-constrained rehearsal in both offline CL (Sec. 6.2) and online CL (Sec. 6.3) scenarios with CILdata ordering. Finally, we summarize additional supporting studies in Sec. 6.4. Additional implementationdetails and dataset details are given in Appendix C and B.",
  "IID Continual Learning": "To understand if SGM with rehearsal would be useful for other CL data distributions, we examine its behaviorin an IID ordering where each of the 5 CL batches contains randomly sampled classes from Places365-LTdataset. In this experiment, we use ConvNeXtV2-Femto pre-trained on ImageNet-1K using self-supervisedlearning (SSL). During IID CL, the model sequentially learns 5 incremental batches of data from Places365-LT where each incremental batch contains 12500 examples. During rehearsal, the model is updated over 600",
  "Vanilla5.080.014 0.00040.173 0.00560.034 0.000576.12 0.100768.45 0.047068.77 0.0517SGM1.450.004 0.00050.131 0.00170.003 0.000477.84 0.023470.81 0.015171.23 0.0664": ": Offline CL. SGMs generality results for CIL on a combination of ImageNet-1K and Places365-Standard. SGM and SGM denote variants of DERpp and GDumb respectively when SGM is integratedwith them. denotes average accuracy (%) over rehearsal sessions and is final accuracy (%) on all 1365classes. #P denotes the trainable parameters in Millions.",
  "minibatches, where the minibatch consists of 128 samples with 50% randomly selected from the current CLbatch and 50% drawn from data seen in earlier CL batches and ImageNet-1K": "In this experiment, we omit storage constraints to solely focus on mitigation methods without the influenceof other variables. Our results are summarized in . In terms of final accuracy, SGM achieves a finalaccuracy of 71.23%, outperforming vanilla rehearsals 68.77% accuracy. Surprisingly, SGM even surpassesthe jointly trained models 70.69% accuracy, resulting in a negative stability gap, which indicates knowledgetransfer from new classes to old classes. In contrast, we found there was a small stability gap in CIL ordering(see ), likely due to the dissimilarity among subsequent batches.",
  "Storage Constrained Offline Continual Learning": "To study SGMs efficacy under storage constraints, we combine it with two popular rehearsal methods,DERpp (Buzzega et al., 2020) and GDumb (Prabhu et al., 2020), under varied storage constraints whileusing identical configurations. For this, we use ConvNeXtV2-Femto pre-trained on ImageNet-1K usingSSL. To study varied storage constraints on a large-scale data stream, we combine the ImageNet-1K (1.2Mimages) dataset with another large-scale dataset, Places365-Standard (1.8M images). This allows us totest more restrictive storage constraints on a data stream consisting of 3 million images. After being pre-trained on ImageNet-1K, a model sequentially learns 5 incremental batches of data (73classes per batch) from Places365-Standard in offline CL with CIL ordering. Each rehearsal is compute-constrained where the model is updated over 1200 minibatches. Each minibatch consists of 256 sampleswhere 50% are selected randomly from the current CL batch and 50% from data seen in earlier CL batchesand ImageNet-1K. During learning a new batch, the model rehearses old data from storage which is boundedby a maximum number of samples e.g., 192K and 24K corresponding to 6.4% and 0.8% of the entire dataset(ImageNet and Places combined), respectively. As shown in , SGM improves each methods per-formance in all criteria. When the storage is bounded by 24K samples, SGM improves final accuracy byabsolute 11.24% (DERpp) and 14.08% (GDumb) and provides 2.4 and 3.1 more stability for DERpp andGDumb, respectively.",
  "Storage Constrained Online Continual Learning": "We also assess SGMs efficacy in an online CL setting using a state-of-the-art online CL method, RE-MIND (Hayes et al., 2020). Using ConvNeXtV2-Femto, we conduct storage-constrained CL experimentswith the CIL data ordering, where we combine SGM with REMIND. Since performing online CL experi-ments on large-scale datasets is computationally expensive, we choose a small dataset, CUB-200 as the2nd dataset after ImageNet-1K. CUB-200 is more challenging than other small datasets (Lee et al., 2023).After being pre-trained on ImageNet-1K, a model learns CUB-200 in sample-by-sample manner, i.e., afterreceiving a new sample, it takes one SGD step using 51 samples (50 old samples and 1 new sample). Storageis constrained by a maximum number of samples, i.e., 80K and 20K, corresponding to 6.2% and 1.5% ofthe entire dataset (ImageNet and CUB combined), respectively. As shown in , SGM combined withREMIND outperforms standalone REMIND by large margins in all metrics. When storage is bounded by20K samples, SGM improves REMINDs final accuracy by absolute 8.64% and reduces REMINDs stabilitygap by a factor of 3.95.",
  "We include additional supporting studies in the Appendix and summarize the findings here": "Non-Rehearsal Methods. While non-rehearsal methods are less performant for CL, it is interesting tostudy SGM when combined with non-rehearsal methods to determine if our findings are consistent. DuringCIL of ImageNet-1K and Places365-LT, SGM also benefits a non-rehearsal method, LwF (Li & Hoiem, 2017)with an absolute gain of 35.24% in final accuracy and a 2.6 reduction in the stability gap (see Appendix E). Class-Balanced Rehearsal. Since our main experiments are based on conventional rehearsal without classbalance, we also conduct experiments using class-balanced rehearsal. Compared to vanilla rehearsal, SGMreduces the stability gap by 7.3 when both use class-balanced rehearsal. SGM also achieves continualknowledge transfer (see Appendix D.2). Supervised Pre-Training. In our main results, SGM reduces the stability gap using the self-supervisedpre-trained ConvNeXtV2 model, and self-supervised models are known to perform better in CL (Gallardoet al., 2021). We also conduct experiments with the supervised pre-trained ConvNeXtV1 (Liu et al., 2022)to examine how SGM performs without a self-supervised backbone. We find that SGM is effective withoutthe self-supervised backbone. Compared to vanilla rehearsal, SGM achieves 6 more stability, 3.9 moreplasticity, and 35 more continual knowledge transfer when both use ConvNeXtV1 (see Appendix D.3). Vision Transformers. We also study the behavior of SGM with ViTs, which now rival CNNs. Compared tovanilla rehearsal, SGM reduces the stability gap by 2.4 when both use a pre-trained ViT (see Appendix D.4).",
  "Limitations & Future Work": "Our work is built upon the assumption that a well-trained DNN is provided and performance on the originaltasks must be maintained or improved. If this assumption is violated, as seen in some CL papers that do notstart with pre-trained models, SGMs dependence on LoRA would impair its utility. Although most of thecomponents of SGM, such as weight initialization, dynamic soft targets, and OOCF, can be readily applied to",
  "Conclusion": "Although there has been remarkable progress in mitigating catastrophic forgetting, almost all CL methodsremain unsuited for practical applications such as addressing model decay (Verwimp et al., 2024; Harun et al.,2023a). To eliminate the need for frequent retraining of deployed DNNs, CL has to combat model decay,mitigate the stability gap, and be more computationally efficient than retraining from scratch in large-scaleCL settings. Focusing on this scenario, we showed that SGM meets these criteria. With the growing energyusage of deep learning models (Luccioni et al., 2022; Patterson et al., 2021; Wu et al., 2022), reducing theneed for retraining from scratch could significantly contribute to reducing carbon emissions associated withtraining DNNs. Our work aligns CL with these real-world challenges, and we hope it encourages the CLcommunity to focus on them. This work was supported in part by NSF awards #1909696, #2326491, #2125362, and #2317706. The viewsand conclusions contained herein are those of the authors and should not be interpreted as representing anysponsors official policies or endorsements. We thank Jhair Gallardo, Junyu Chen, Shikhar Srivastava, andRobik Shrestha for their feedback and comments on the manuscript.",
  "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectionaltransformers for language understanding. arXiv preprint arXiv:1810.04805, 2018": "Prithviraj Dhar, Rajat Vikram Singh, Kuan-Chuan Peng, Ziyan Wu, and Rama Chellappa. Learning withoutmemorizing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,pp. 51385146, 2019. Arthur Douillard, Matthieu Cord, Charles Ollion, Thomas Robert, and Eduardo Valle. Podnet: Pooledoutputs distillation for small-tasks incremental learning. In Computer vision-ECCV 2020-16th Europeanconference, Glasgow, UK, August 23-28, 2020, Proceedings, Part XX, volume 12365, pp. 86102. Springer,2020.",
  "Joo Gama, Indre liobaite, Albert Bifet, Mykola Pechenizkiy, and Abdelhamid Bouchachia. A survey onconcept drift adaptation. ACM computing surveys (CSUR), 46(4):137, 2014": "Qiankun Gao, Chen Zhao, Yifan Sun, Teng Xi, Gang Zhang, Bernard Ghanem, and Jian Zhang. A uni-fied continual learning framework with general parameter-efficient tuning. International Conference onComputer Vision (ICCV), 2023. Yasir Ghunaim, Adel Bibi, Kumail Alhamoud, Motasem Alfarra, Hasan Abed Al Kader Hammoud, AmeyaPrabhu, Philip HS Torr, and Bernard Ghanem. Real-time evaluation in online continual learning: A newparadigm. In CVPR, 2023.",
  "Tyler L Hayes and Christopher Kanan. Online continual learning for embedded devices. In CoLLAs, 2022": "Tyler L Hayes, Ronald Kemker, Nathan D Cahill, and Christopher Kanan. New metrics and experimentalparadigms for continual learning. In Proceedings of the IEEE Conference on Computer Vision and PatternRecognition Workshops, pp. 20312034, 2018. Tyler L Hayes, Kushal Kafle, Robik Shrestha, Manoj Acharya, and Christopher Kanan. Remind your neuralnetwork to prevent catastrophic forgetting. In European Conference on Computer Vision, pp. 466483.Springer, 2020. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification.In Proceedings of the IEEE international conference oncomputer vision, pp. 10261034, 2015. Saihui Hou, Xinyu Pan, Chen Change Loy, Zilei Wang, and Dahua Lin. Learning a unified classifier incre-mentally via rebalancing. In Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition, pp. 831839, 2019.",
  "Nishant Jain and Pradeep Shenoy.Instance-conditional timescales of decay for non-stationary learning.arXiv e-prints, pp. arXiv2212, 2022": "James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu,Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophicforgetting in neural networks. Proceedings of the national academy of sciences, 114(13):35213526, 2017. Kuan-Ying Lee, Yuanyi Zhong, and Yu-Xiong Wang. Do pre-trained models benefit equally in continuallearning? In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp.64856493, 2023.",
  "Alexandra Sasha Luccioni, Sylvain Viguier, and Anne-Laure Ligozat. Estimating the carbon footprint ofbloom, a 176b parameter language model. arXiv preprint arXiv:2211.02001, 2022": "Clare Lyle, Zeyu Zheng, Evgenii Nikishin, Bernardo Avila Pires, Razvan Pascanu, and Will Dabney. Under-standing plasticity in neural networks. In International Conference on Machine Learning, pp. 2319023211.PMLR, 2023. Sasu Mkinen, Henrik Skogstrm, Eero Laaksonen, and Tommi Mikkonen. Who needs mlops: What data sci-entists seek to accomplish and how can mlops help? In 2021 IEEE/ACM 1st Workshop on AI Engineering-Software Engineering for AI (WAIN), pp. 109112. IEEE, 2021.",
  "Ankur Mallick, Kevin Hsieh, Behnaz Arzani, and Gauri Joshi. Matchmaker: Data drift mitigation in machinelearning for large-scale systems. Proceedings of Machine Learning and Systems, 4:7794, 2022": "Michael McCloskey and Neal J Cohen. Catastrophic interference in connectionist networks: The sequentiallearning problem. In Psychology of learning and motivation, volume 24, pp. 109165. Elsevier, 1989. Mark D McDonnell, Dong Gong, Amin Parvaneh, Ehsan Abbasnejad, and Anton van den Hengel. Ran-pac: Random projections and pre-trained models for continual learning. Advances in Neural InformationProcessing Systems, 36, 2024.",
  "German I Parisi, Ronald Kemker, Jose L Part, Christopher Kanan, and Stefan Wermter. Continual lifelonglearning with neural networks: A review. Neural Networks, 113:5471, 2019": "David Patterson, Joseph Gonzalez, Quoc Le, Chen Liang, Lluis-Miquel Munguia, Daniel Rothchild, DavidSo, Maud Texier, and Jeff Dean. Carbon emissions and large neural network training. arXiv preprintarXiv:2104.10350, 2021. Lorenzo Pellegrini, Gabriele Graffieti, Vincenzo Lomonaco, and Davide Maltoni. Latent replay for real-time continual learning. In 2020 IEEE/RSJ International Conference on Intelligent Robots and Systems(IROS), pp. 1020310209. IEEE, 2020.",
  "Vinay Venkatesh Ramasesh, Aitor Lewkowycz, and Ethan Dyer. Effect of scale on catastrophic forgettingin neural networks. In International Conference on Learning Representations, 2021b": "Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, andIlya Sutskever. Zero-shot text-to-image generation. In International Conference on Machine Learning, pp.88218831. PMLR, 2021. Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, and Christoph H Lampert. icarl: Incrementalclassifier and representation learning. In Proceedings of the IEEE conference on Computer Vision andPattern Recognition, pp. 20012010, 2017. Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, AndrejKarpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge.International journal of computer vision, 115(3):211252, 2015. James Seale Smith, Leonid Karlinsky, Vyshnavi Gutta, Paola Cascante-Bonilla, Donghyun Kim, Assaf Ar-belle, Rameswar Panda, Rogerio Feris, and Zsolt Kira. Coda-prompt: Continual decomposed attention-based prompting for rehearsal-free continual learning. In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition, pp. 1190911919, 2023.",
  "Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge Belongie. The caltech-ucsd birds-200-2011 dataset. 2011": "Zifeng Wang, Zizhao Zhang, Sayna Ebrahimi, Ruoxi Sun, Han Zhang, Chen-Yu Lee, Xiaoqi Ren, GuolongSu, Vincent Perot, Jennifer Dy, et al. Dualprompt: Complementary prompting for rehearsal-free continuallearning. In European Conference on Computer Vision, pp. 631648. Springer, 2022a. Zifeng Wang, Zizhao Zhang, Chen-Yu Lee, Han Zhang, Ruoxi Sun, Xiaoqi Ren, Guolong Su, VincentPerot, Jennifer Dy, and Tomas Pfister. Learning to prompt for continual learning. In Proceedings ofthe IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 139149, 2022b. Sanghyun Woo, Shoubhik Debnath, Ronghang Hu, Xinlei Chen, Zhuang Liu, In So Kweon, and Sain-ing Xie. Convnext v2: Co-designing and scaling convnets with masked autoencoders. arXiv preprintarXiv:2301.00808, 2023. Carole-Jean Wu, Ramya Raghavendra, Udit Gupta, Bilge Acun, Newsha Ardalani, Kiwan Maeng, GloriaChang, Fiona Aga, Jinshi Huang, Charles Bai, et al. Sustainable ai: Environmental implications, challengesand opportunities. Proceedings of Machine Learning and Systems, 4:795813, 2022.",
  "Da-Wei Zhou, Qi-Wei Wang, Zhi-Hong Qi, Han-Jia Ye, De-Chuan Zhan, and Ziwei Liu.Deep class-incremental learning: A survey. arXiv preprint arXiv:2302.03648, 2023": "Yue Zhou, Yue Yu, and Bo Ding. Towards mlops: A case study of ml pipeline platform. In 2020 Internationalconference on artificial intelligence and computer engineering (ICAICE), pp. 494500. IEEE, 2020. Yao Zhu, YueFeng Chen, Chuanlong Xie, Xiaodan Li, Rong Zhang, Hui Xue, Xiang Tian, Yaowu Chen, et al.Boosting out-of-distribution detection with typical features. Advances in Neural Information ProcessingSystems, 35:2075820769, 2022."
}