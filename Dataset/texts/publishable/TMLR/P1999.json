{
  "Abstract": "Diffusion generative models unlock new possibilities for inverse problems as they allow for theincorporation of strong empirical priors in scientific inference. Recently, diffusion models arerepurposed for solving inverse problems using Gaussian approximations to conditional den-sities of the reverse process via Tweedies formula to parameterise the mean, complementedwith various heuristics. To address various challenges arising from these approximations, weleverage higher order information using Tweedies formula and obtain a statistically prin-cipled approximation. We further provide a theoretical guarantee specifically for posteriorsampling which can lead to a better theoretical understanding of diffusion-based conditionalsampling. Finally, we illustrate the empirical effectiveness of our approach for general linearinverse problems on toy synthetic examples as well as image restoration. We show thatour method (i) removes any time-dependent step-size hyperparameters required by earliermethods, (ii) brings stability and better sample quality across multiple noise levels, (iii) isthe only method that works in a stable way with variance exploding (VE) forward processesas opposed to earlier works.",
  "Introduction": "Due to the ease of scalability, diffusion models (Song et al., 2020; Ho et al., 2020) have received increasedattention, producing a variety of improvements in the conditional generation setting such as adding classifierguidance (Dhariwal & Nichol, 2021), classifier free guidance (Ho & Salimans, 2022), conditional diffusionmodels (Batzolis et al., 2021; Karras et al., 2022; 2024) and DEFT (Doobs h-transform Efficient FineTuning)(Denker et al., 2024). These methods are useful for problems where paired (x0, y) data are available, see, e.g.,Saharia et al. (2022); Abramson et al. (2024). However, paired data is not always available, and there are caseswhere we would like to study alternatives to classical Bayesian approaches to solve inverse problems (Taran-tola, 2005; Stuart, 2010) to build samplers for complicated conditional distributions with a known observationoperator mapping x0 to the observed data y that is used to express a known likelihood p(y|x0). With their",
  "Published in Transactions on Machine Learning Research (09/2024)": ": Whilst DTMPD-D with the VE-SDE remains robust to increasing noise, DPS-D and GDM-D do not. This is illustrated by samples from the different guidance methods for a 4 super-resolution(256 256 64 64) problem with increasing Gaussian observation noise distorting a ground truth image.The top row measurement has y = 0.01, the middle row y = 0.05 and the bottom row y = 0.1. For thefull results, see .",
  "MeanVariance": ": Error to target posterior for a Gaus-sian random field.(Top row) visualisation ofthe empirical mean and variance of the 1500samples that were used to compute this er-ror against the analytical moments. (Bottom)Wasserstein distances of different methods w.r.t.sample size. For details, see Appendix E.2. This paper is devoted to developing novel methods tosolving inverse problems, given a latent (target) signalx0 Rdx, noisy observed data y Rdy, a known linearobservation map H, and a pretrained diffusion prior. Themain tool we use is Tweedies formula to obtain both themean and the covariance for approximating diffused likeli-hoods, to be used for building the final posterior score ap-proximation. This is as opposed to previous works whichonly utilised first moment approximations using Tweediesformula (Chung et al., 2022a; Song et al., 2023). We showthat utilising covariance approximation, with moment pro-jections, provides a principled scheme with improved per-formance, which we term Tweedie Moment Projected Dif-fusions (TMPD). To demonstrate our method briefly, demonstratesa sampling scenario of a Gaussian random field (GRF)whose mean and variance entries are plotted under Ana-lytic column1. We demonstrate the approximations underthis setting provided by our method (TMPD) and its diag-onal (cheaper) version (DTMPD), compared with GDM(Song et al., 2023), and DPS (Chung et al., 2022a). The fig-ure demonstrates the optimality of our method: Our firstand second moment approximations become exact in thiscase. This results in a drastic performance improvementstemming from the statistical optimality of our methodfor near-Gaussian settings and also unlocks a possible linefor theoretical research for understanding similar diffusionmodels for inverse problems. In what follows, we will first introduce the technical back-ground in and then describe TMPD in detail in. We will then provide some theoretical results about our method in and provide a discus-sion to closely related work in literature in . Finally, will present experiments on Gaussian",
  "Technical Background": "In score based generative models (SGMs) (Song et al., 2020) and denoising diffusion probabilistic models(DDPM) (Ho et al., 2020), the goal is to sample from a target distribution p0 := pdata. To that end, anartificial path pt is introduced, with the property that pt will approach N(0, I) for large t, i.e., pt N(0, Id)as t . Then, one learns to reverse this process, in order to transform samples from a standard normaldistribution into samples from pdata. More recent advances in generative modelling include methods withartificial paths based on stochastic interpolants or continuous normalizing flows, see, e.g., Albergo & Vanden-Eijnden (2022); Lipman et al. (2022). Conditional flow matching draws inspiration from the denoising scorematching approach, but generalizes to matching vector fields directly. We have focused our approach todenoising-diffusion models in the SGM paradigm because of readily available pretrained diffusion models. In the SGM paradigm, a stochastic differential equation (SDE) is used to noise the data, and the interpolationparameter t will take continuous values in t [0, T]. In the DDPM setting, t is discrete. However, the DDPMMarkov chain can be seen as a discretization of the SDE (Song et al., 2020). Recent developments have improved the noising schedule of score-based diffusion models for image datathat parameterise the transition kernels of the forward process in terms of the signal-to-noise ratio 2t ,pt(xt|x0) = N(xt; stx0, s2t2t Idx) (Karras et al., 2022; 2024). In this paper, we focus derivations on theVariance Preserving (VP) SDE formulation, although our approach can be generalised and derived for otherSDEs, such as the Variance Exploding (VE) SDE (see Appendix C). The VP transition kernel that we chooseto focus our derivation is given by setting",
  "(T t)d wt,z0 pT": "A parameterisation that performs well in practice is (t) = min +t(max min). In the diffusion modellingliterature, the time-rescaled OU process is also sometimes called a Variance Preserving SDE. This is not theonly SDE that is suitable for the forward process. See Appendix C for details on a time-rescaled Brownianmotion (Variance Exploding SDE (Song et al., 2020)). There are two usual approximations to solve the SDE in Equation 2.First, we do not know pT , sinceit is a noised version of the distribution pdata. However for T large enough, we can approximate pT pref = N(0, Idx). We also do not have log pT t which we need for the drift in Equation 2. This can becircumvented by approximating the drift using score-matching techniques (Hyvrinen, 2005; Ho et al., 2020).These methods construct an estimate of the score function by solving the score matching problem in theform of s(xt, t) xt log pt(xt). This score can also be used in the setting of DDPM (Ho et al., 2020).",
  "py|t(y|xt) =py|0(y|x0)p0|t(x0|xt)dx0,(4)": "which involves a marginalization over x0. The above integral is difficult to evaluate since the term p0|t(x0|xt)is only defined implicitly through running the diffusion model. One way around this is to train a neural net-work to directly approximate log pt|y(xt|y) (Batzolis et al., 2021; Karras et al., 2022). Alternatively, if onealready has access to an approximation of xt log pt(xt), one can train an auxiliary network to approximatethe term log py|t(y|xt) in Equation 3, (Dhariwal & Nichol, 2021; Denker et al., 2024). However, thesemethods can be time and training-data intensive, as it is necessary to retrain networks for each conditionaltask as well have access to paired training data from the joint distribution of (x0, y). Alternatively, one couldtry to do a Monte-Carlo approximation of the score corresponding to Equation 4. But this needs evaluatingthe probability flow ODE together with its derivative (Song et al., 2020, Section D.2) for each sample, whichis prohibitive, also suffers from high variance (Mohamed et al., 2020, ).",
  "Tweedie Moment Projected Diffusions": "In this section, we first introduce Tweedie moment projections in .1 below. Our method relieson the approximation p0|t(x0|xt) Nx0; m0|t(xt), C0|t(xt)to make the sampling process tractable. Inthat case, since the conditional distribution of y given x0 is also Gaussian, we can compute the integral inEquation 4 analytically py|t(y|xt) will just be another Gaussian in that case, its mean and covariancebeing determined through m0|t, C0|t, H and y. In particular, we can then use this Gaussian to approximate log py|t(y|xt), since the score of a Gaussian is available in closed form.",
  "Tweedie moment projections": "Instead of just approximating the variance of p0|t(x0|xt) heuristically, we approximate it by projecting ontothe closest Gaussian distribution using Tweedies formula for the second moment. Our approximation at thisstage consists of two main steps: (i) Find the mean and covariance of p0|t(x0|xt) using Tweedies formula,and (ii) approximate this density with a Gaussian using the mean and covariance of p0|t(x0|xt) (momentprojection). Due to this approximation, we will refer to the resulting methods as Tweedie Moment ProjectedDiffusions (TMPD). We will first introduce Tweedies formula for the mean and covariance and then describethe moment projection.",
  "= Ny; Hm0|t, HC0|tH + 2yIdy.(10)": "Let us recall that mean and covariance terms are a function of xt by making them explicit in the notation, i.e.,m0|t(xt) and C0|t(xt) for the mean and covariance respectively. To compute xt log py|t(y|xt), we requirefurther approximations since we have xt dependence in both the mean and the covariance of Equation 10which is computationally infeasible to differentiate through. For this reason, we treat the matrix C0|t like aconstant w.r.t. xt when computing the gradient (which is the case if pdata is Gaussian). For non-Gaussianpdata, this results in a computationally efficient sampler using C0|t as a preconditioner for the step size, asotherwise, the resulting terms can be expensive to compute. This leads to an approximation of the gradient",
  "where z0 pT and f yt (zt) log py|T t(y|zt) is our approximation to the data likelihood, given byEquation 11. We call this SDE the TMPD SDE": "We have two options to convert TMPD SDE into implementable methods: (1) score-based samplers (Song& Ermon, 2020), which we abbreviate as TMPD since they are Euler-Maruyama discretizations of theTMPD SDE; and (2) denoising diffusion models (TMPD-D). The denoising diffusion approach is derivedfrom approximate reverse Markov chains and is the approach of DDPM and DPS methods (Ho et al.,2020; Chung et al., 2022a).We note that the Gaussian projection can be used in this discrete setting,assuming that the conditional density is available analytically as in Ho et al. (2020), and can be written aspn|0(xn|x0) = N(xn; nx0, vnIdx). The idea is to update the unconditional mean m0|n(xn) of the densityp0|n(x0|xn) with a Bayesian update: p(x0|xn, y) p(y|xn)p0|n(x0|xn). Given a similar formulation as above,assuming we have a readily available approximation p0|n(x0|xn) N(x0; m0|n, C0|n) and a likelihood similarto Equation 10 where t can be replaced by n, we can compute the moments of p(x0|xn, y) analytically, whichwe denote my0|n and Cy0|n. The Bayes update for Gaussians gives (Bishop, 2006)",
  "my0|n = m0|n + C0|nH(HC0|nH + 2yIdx)1(y Hm0|n).(13)": "Incorporating Equation 13 for n = N 1, . . . , 0 into the usual Ancestral sampling (Ho et al., 2020) stepsleads to Algorithm 1, termed TMPD-Denoising (TPMD-D). The update in Equation 13 can be used in anydiscrete sampler such as denoising diffusion implicit models (DDIM) (Song et al., 2021a).",
  "Computationally cheaper approximation of Moment Projection": "We show in our experiments promising results for TMPD motivating the exploration of less computation-ally expensive approximations to the full Jacobian. In particular, we empirically study a computationallyinexpensive method that applies to inpainting and super-resolution, below. To make the computational cost of TMPD smaller, we can make an approximation of the Gaussian Projectionthat requires fewer vector-Jacobian products and does not require linear solves. One approximation that wefound useful for sampling from high dimensional diffusion models, e.g., high resolution images, is denotedhere as diagonal Tweedie Moment Projection (DTMPD). Instead of the full second moment, DTMPD usesthe diagonal of the second moment xtm0|t diag(xtm0|t). Intuitively, this approximation will performwell empirically since it is a similar approximation to GDM that assumes dimensional independence ofthe distribution p(x0|xt), but unlike GDM, this diagonal approximation is the same as using the closestdimensionally independent Gaussian in KL divergence to p0|t(x0|xt). The biggest drawback of our method is that without further approximation, it doesnt scale up to the highdimensions of image data. This is because even calculating the diagonal of a Jacobian requires computing dxvector-Jacobian products since in general every element of the Jacobian at a location xt, xtm0|t dependson every element of xt. Therefore we must resort to a further approximation that exploits knowledge of theobservation operator H. For the cases of super-resolution and inpainting, a further approximation that allows scaling up to thedimensions of image data is approximating the diagonal of the Jacobian by the row sum of the Jacobianwhich only requires a single vector-Jacobian product and brings the memory and time complexity of DTMPDdown to that of GDM. We exploit the sparsity of H to make the rowsum approximation of the diagonal moreaccurate by masking out (zeroing) the values in the vector-Jacobian product that that will not contributeto the diagonal of HC0|nH. We discuss a justification of this approximation in Ap. E.1. We use thisapproximation in the image experiments and find that in practice it is only (1.5 0.1) slower than GDMand DPS across all of our experiments (Sec. 6), with competitive sample quality for noisy inverse problemsand without the need for expensive hyperparameter tuning. Finally, we note a very recent work (Rozet et al.,2024) that circumvents our heuristic by applying the conjugate gradient (CG) method, unlocking using theapproximation Eq. 11 to be used in practice for non-sparse H (see Ap. E.1 for more details).",
  "Theoretical Guarantees": "Because of the approximations, our method, as well as GDM Song et al. (2023) and DPS (Chung et al.,2022a) do not sample the exact posterior for general prior distributions. Therefore, one cannot hope for thesemethods to sample the true posterior and a priori it is not even clear how the sampled distribution relatesto the true posterior. Without further justification, such methods should only be interpreted as guidancemethods, where paths are guided to regions where a given observation y is more likely, not as posteriorsampling methods. We justify our approximation by showing that the TMPD-SDE in Equation 12 is able to sample the exactposterior in the Gaussian case. One can see that this contrasts with GDM and DPS in our numericalexperiments or by explicitly evaluating their approximations on simple one-dimensional examples.Proposition 3 (Gaussian data distribution). Assume that pdata is Gaussian. Then, the posterior score ex-pression using Equation 11 is exact, i.e., if there are no errors in the initial condition and drift approximations(xt, t) = xt log pt(xt), the TMPD will sample pdata(|y) at its final time. The proof is given in Appendix B.1.However, most distributions will not be Gaussian.The followingtheorem generalizes the above proposition to non-Gaussian distributions, as long as they have a density withrespect to a Gaussian. We study how close our sampled measure will be to the true posterior distributionand give explicit bounds on the total variation distance in terms of the regularity properties of the density:Theorem 1 (General data distribution). Assume that the data distribution pdata can be written as",
  "See Appendix B.2 for a proof": "In the limit, when pdata becomes more similar to a Gaussian, the in Equation 14 converges to zero, andtherefore M 1 and L 0. In particular, the right hand side in Equation 15 converges to 0 and we recoverthe result of Proposition 3. When pdata is not Gaussian, the right hand side of Equation 15 gives us an upperbound of our sample distribution to the true posterior.",
  "Experiments": "In this section, we demonstrate our results as well as the peformance of other approximations to the likelihoodprovided in Chung et al. (2022a); Song et al. (2023). In particular, we perform comparisons for two of ourmethods TMPD (an SGM using our approximation) and TMPD-D (a DDPM sampler using Equation 13).We compare these to DPS (an SGM sampler using the posterior approximation in Equation 16), DPS-D(Chung et al., 2022a) (a DDPM-type sampler using Equation 16), GDM (Song et al., 2023) (an SGMsampler using the posterior approximation in Equation 17), and finally GDM-D (a DDIM-type samplerusing Equation 17, but in our experiments a DDPM-type sampler since we set the DDIM hyperparameter = 1.0 which is defined in Algorithm 1 by Song et al. (2021a) who show that this is equivalent to aDDPM-type sampler).",
  "Gaussian Mixture Model": "We now demonstrate a nonlinear SDE example and follow the Gaussian mixture model example of Cardosoet al. (2023) where the data distribution p0(x0) is a mixture of 25 Gaussian distributions. The means andvariances of the components of the mixture are given in Appendix E.3. In this case, for each choice ofobservation y, observation map H and measurement noise standard deviation y, the target posterior canbe computed explicitly (see Appendix E.3). To investigate the performance of posterior sampling methods, for each pair of dimensions and observationnoise (dx, dy, y) {8, 80, 800} {1, 2, 4} {102, 101, 100} we randomly generate multiple measurementmodels (y, H) Rdy Rdydx, and equally weight each component of the Gaussian mixture. Further detailsare given in Appendix E.3.We chose to control the dimension to gain insight into the performance ofposterior sampling methods under varying dimensions. We also chose to control the noise level since thedifferent posterior sampling methods have accuracy that depends on the signal-to-noise ratio.Throughrandomly varying the observation model, we gain an insight into the performance of the posterior samplingmethods with different levels of posterior multimodality. This example is interesting because it allows us tostudy the behaviour of our methods on non-Gaussian problems in high dimensions whilst having access tothe target posterior with which to compare (usually, obtaining a ground-truth posterior is not feasible fornon-Gaussian problems).",
  "TMPD-D0.90.90.61.51.10.91.51.20.9DTMPD-D0.91.70.91.42.10.91.42.01.1DPS-D5.23.52.56.93.91.76.84.70.9GDM-D1.52.31.81.61.40.92.02.00.6": "We use the sliced Wasserstein (SW) distance defined in Appendix E.3 to compare the posterior distributionestimated by each algorithm with the target posterior distribution. We use 104 slices for the SW distanceand compare 1000 samples of TMPD-D, GDM-D and DPS-D in Tables 1 obtained using 1000 denoisingsteps and 1000 samples of the true posterior distribution. indicates the Central Limit Theorem (CLT) 95% confidence intervals obtained by considering 20randomly selected measurement models (H) for each setting (dx, dx, y). shows the first twodimensions of the estimated posterior distributions corresponding to the configurations (80, 1) from for one of the randomly generated measurement model (H, y = 0.1). These illustrations give us insightinto the behaviour of the algorithms and their ability to accurately estimate the posterior distribution. Weobserve that TMPD-D estimates the target posterior well compared to GDM-D and DPS-D. TMPD-Dcovers all of the modes, whereas GDM-D and DPS-D do not.",
  "Noisy observation inpainting and super-resolution": "We consider inpainting and super-resolution problems on the FFHQ 256 256 (Karras et al., 2019) andCIFAR-10 32 32 (Krizhevsky et al., 2009) datasets. We compare TMPD to DGM and DPS. We alsocompare score-based diffusion models with their denoising-diffusion counterparts (denoted with suffix, -D). Firstly, we follow the benchmark used by Chung et al. (2022a) and use a Variance Preserving (VP) SDE,using a DDPM sampler, on FFHQ 256256 using 1k validation images. The pre-trained diffusion model forFFHQ was taken from Chung et al. (2022a) and was used directly without any finetuning. We follow Chunget al. (2022a) and use various forward operators. For super-resolution, we use a downsampling ratio of 4(256256 6464) and bicubic interpolation; for box mask inpainting we mask out 128128 region andfor random mask inpainting we choose a random mask for each image masking between 30% and 70% of thepixels. Images are normalized to the range and it is on this scale that we add Gaussian measurementnoise with standard deviation y {0.01, 0.05, 0.1, 0.2}.For quantitative comparison, we focus on twowidely used perception distances, Frchet Inception Distance (FID) and Learned Perceptual Image PatchSimilarity (LPIPS) distance. FID evaluates consistency with the whole dataset using summary statisticsfrom the FFHQ-50k dataset. We also evaluate observation data similarity using various distances betweena sampled image and ground truth image: LPIPS, mean-squared-error (MSE), peak signal-to-noise-ratio(PSNR) and structural similarity index measure (SSIM). For GDM-D we use the algorithm and defaulthyperparameters as described in Song et al. (2023). For DPS-D we use the algorithm in the codebase providedby the authors Chung et al. (2022a) and we use their default hyperparameters such as their suggested step-size hyperparameter for this task, and static-thresholding (clipping the denoised image at each step to arange ) whereas TMPD-D does not require hyperparameter tuning or static-thresholding. The resultsfor FFHQ sampled using VP DDPM are shown in the appendix . We observe that DTMPD-D iscompetitive with DPD-D over a range of noise levels, however, GDM-D is not able to produce high qualityreconstructions for larger noise levels. We note that the heuristics used in the DPS and GDM implementations have been designed to work withthe VP-SDE (DDPM sampler), and therefore the performance may not be robust to the choice of SDE.Fig 14 illustrates this when each method is applied to the VE-SDE on a sample from the FFHQ validationdataset. We next compare performance to TMPD across VP and VE-SDE samplers and a range of noiselevels on CIFAR-10 64 64 using 1k validation images. We use pretrained denoising networks for CIFAR-10 that are available here. For inpainting, we use box andhalf mask. For half-type inpainting, we mask out a 16 16 right half region of the image; for box-typeinpainting, we mask out an 8 8 box region following Cardoso et al. (2023). For super-resolution, we use adownsampling ratio of 2 on each axis (32 32 16 16) with a nearest-neighbour downsampling method;and a downsampling ratio of 4 (32 32 8 8) with bicubic downsampling. Images are normalized tothe range and it is on this scale that we add Gaussian measurement noise with standard deviationy {0.01, 0.05, 0.1}. Whereas no hyperparameters are required for our method, we chose the DPS scalehyperparameter by optimising LPIPS, MSE, PSNR and SSIM on a validation set of 128 images (see for an example). We found that static thresholding (clipping the denoised image estimate to a range at each sampling step) is critical for the stability and performance of both DPS-D and GDM-D. Stabilitywas noted as a limitation in Chung et al. (2022a), and they suggest that devising methods to stabilize thesamplers would be a promising direction of research. We find that our TMPD-D method and the diagonalapproximation DTMPD-D is stable across SDE, noise-level and observation maps, without the need for staticthresholding. Whilst for VE GDM-D we found the original algorithm in Song et al. (2023) to be stable, forVP GDM-D, the original algorithm, whilst stable for FFHQ, was not stable, even with static-thresholding,for CIFAR-10. We chose to bring GDM-D a step closer to our algorithm by substituting their likelihoodscore into an Ancestral sampling algorithm, instead of a DDIM algorithm as suggested in Song et al. (2023),which produced stable samples. The methods TMPD, GDM and DPS all have the same numerical solver of their respective reverse-SDE,and DTMPD-D, GDM-D and DPS-D all use DDPM since DDIM and DDPM are equivalent algorithmswith our chosen DDIM hyperparameter = 1.0. Therefore, the sampling methods being compared onlydiffer in the xt log py|t(y|xt) term in their reverse-SDE, and so this experiment allows us to study the",
  "y = 0.1DTMPD-D34.00.223 0.04129.60.292 0.049box maskDPS-D59.10.467 0.05329.30.259 0.049inpaintingGDM-D72.60.529 0.047165.70.539 0.083": "behaviour of our method on inpainting and super-resolution compared to the different approximations ofthe smoothed likelihood. A summary of the results for CIFAR-10 sampled using VE DDPM are shown in. The complete results are in Tables 8 and 12 for VP and VE DDPM respectively, and Tables 9 and13 for score-based VP and VE-SDE, respectively. For more experimental details including illustration ofsamples used to generate the tables can be found in Appendix E.4. Our method is the only method able toprovide high-quality reconstructions independently of the SDE, time discretization or noise level used. Onthe other hand, we see that DPS-D and GDM-D are not able to provide high-quality reconstructions forthe VE-SDE. For the continuous time methods, GDM and DPS are outperformed by TMPD for both VEand VP-SDEs in the majority of tasks.",
  "Discussions, limitations and future work": "In this paper, we introduced TMPD, a diffusion modelling approach to solve inverse problems and samplefrom conditional distributions using unconditional diffusion models.On various tasks on the VP-SDE,TMPD achieves competitive quality with other methods that aim to solve the noisy, linear inverse problemwhile avoiding the expensive, problem-specific training of conditional models. Our method is more versatilesince it can also be used for the VE-SDE, and for large noise and different time discretizations. TMPD is slower, as each iteration costs more memory and compute due to the Jacobian over the scoremodel.Even with a diagonal and row-sum approximation to the Jacobian, the method is around 1.5slower than DPS and GDM. The row-sum approximation is not suitable for inverse problems with morecomplicated, non-diagonal and nonlinear observation operators, therefore, it would be helpful to exploremethods to circumvent heuristics. For example, heuristics can be circumvented by noting in the definition ofEquation 11 does not require the inverse but rather solving a system of linear equations with right hand sidey Hm0|t(xt). A very recent work (Rozet et al., 2024), uses the natural choice of the conjugate gradient(CG) method to solve this linear system with success on non-diagonal and nonlinear observation operators,even for a small number of iterations of the CG method.",
  "Acknowledgements": "This work has been supported by The Alan Turing Institute through the Theory and Methods Chal-lenge Fortnights event Accelerating generative models and nonconvex optimisation, which took place on6-10 June 2022 and 5-9 Sep 2022 at The Alan Turing Institute headquarters.JP and SR acknowledgefunding by Deutsche Forschungsgemeinschaft (DFG) Project-ID 318763901 - SFB1294.M. Girolamiwas supported by a Royal Academy of Engineering Research Chair grant RCSRF1718/6/34, and EPSRCgrants EP/W005816/1, EP/V056441/1, EP/V056522/1, EP/R018413/2, EP/R034710/1, EP/Y028805/1,and EP/R004889/1. The authors would also like to thank the Isaac Newton Institute for MathematicalSciences, Cambridge, for support and hospitality during the programme The Mathematical and StatisticalFoundation of Future Data-Driven Engineering where work on this paper was undertaken. This work wassupported by EPSRC grant no EP/R014604/1. BB gratefully acknowledges the EPSRC for funding thisresearch through the EPSRC Centre for Doctoral Training in Future Infrastructure and Built Environment:Resilience in a Changing World (EPSRC grant reference number EP/S02302X/1); and the support of nPlan,and in particular Damian Borowiec and Peter A. Zachares, for the invaluable facilitation of work that wascompleted whilst on internship with nPlan and access to A100 GPUs. Josh Abramson, Jonas Adler, Jack Dunger, Richard Evans, Tim Green, Alexander Pritzel, Olaf Ronneberger,Lindsay Willmore, Andrew J Ballard, Joshua Bambrick, et al. Accurate structure prediction of biomolec-ular interactions with alphafold 3. Nature, pp. 13, 2024.",
  "Gabriel Cardoso, Yazid Janati El Idrissi, Sylvain Le Corff, and Eric Moulines. Monte carlo guided diffusionfor bayesian linear inverse problems, 2023": "Sitan Chen, Sinho Chewi, Jerry Li, Yuanzhi Li, Adil Salim, and Anru R Zhang. Sampling is as easy as learningthe score: theory for diffusion models with minimal data assumptions. In International Conference onLearning Representations, 2023. Hyungjin Chung, Jeongsol Kim, Michael Thompson Mccann, Marc Louis Klasky, and Jong Chul Ye. Diffu-sion posterior sampling for general noisy inverse problems. In The Eleventh International Conference onLearning Representations, 2022a. Hyungjin Chung, Byeongsu Sim, Dohoon Ryu, and Jong Chul Ye. Improving diffusion models for inverseproblems using manifold constraints. Advances in Neural Information Processing Systems, 35:2568325696,2022b. Hyungjin Chung, Jeongsol Kim, Sehui Kim, and Jong Chul Ye. Parallel diffusion models of operator andimage for blind inverse problems. In Proceedings of the IEEE/CVF Conference on Computer Vision andPattern Recognition, pp. 60596069, 2023. Giannis Daras, Yuval Dagan, Alex Dimakis, et al. Score-guided intermediate level optimization: Fast langevinmixing for inverse problems. In Proceedings of the 39th International Conference on Machine Learning(ICML), 2022. Alexander Denker, Francisco Vargas, Shreyas Padhy, Kieran Didi, Simon Mathis, Vincent Dutordoir, Ric-cardo Barbano, Emile Mathieu, Urszula Julia Komorowska, and Pietro Lio. Deft: Efficient finetuning ofconditional diffusion models by learning the generalised h-transform. arXiv preprint arXiv:2406.01781,2024.",
  "Berthy T Feng and Katherine L Bouman. Efficient bayesian computational imaging with a surrogate score-based prior. arXiv preprint arXiv:2309.01949, 2023": "Berthy T Feng, Jamie Smith, Michael Rubinstein, Huiwen Chang, Katherine L Bouman, and William TFreeman.Score-based diffusion models as principled priors for inverse imaging.arXiv preprintarXiv:2304.11751, 2023. Marc Anton Finzi, Anudhyan Boral, Andrew Gordon Wilson, Fei Sha, and Leonardo Zepeda-Nez. User-defined event sampling and uncertainty quantification in diffusion models for physical dynamical systems.In International Conference on Machine Learning, pp. 1013610152. PMLR, 2023.",
  "Solutions to the above reverse SDE will sample our target measure pc0 = pdata(x0|y) at final time. Therefore,we want to study how our algorithm approximates solutions of xc": "Intermediate Gaussian SDE:Instead of bounding the distance of solutions to the above conditionedreverse SDE to our algorithm, we will instead introduce an intermediate process, which we will later use ina triangle inequality. The process will be a Gaussian process. Therefore, we denote it with a superscript G. The process xGt will be defined analogous to Equation 2, but assuming that it is started in N(x0; 0, 0)instead of pdata. Since the forward SDE in Equation 2 is linear, all of the marginals of xGt , which we denoteby pGt , will also be Gaussian. Again, we define a conditioned version of x, called xG,c analogous to Equation 18, i.e. xG,c0will be distributedas xG0 conditioned on y = y, i.e. pG(x0|y = y). Since we have a linear observation model, pG(x0|y = y) isstill a Gaussian, and therefore xG,c will also be a Gaussian process. Its reverse SDE zG,c is defined throughEquation 19, just that every appearance of p will also have a superscript G,",
  "In the last equality, we used thatdPdPG (x[0,t]) = exp((x0)),": "where we denoted by P the path measure induced by Equation 18. Therefore, also their marginals pt = PGtand pt = Pt have relative densities, which are given by integrating out the density to time t, as we did inEquation 26. By assumption exp() is bounded from above and below by M and 1/M respectively, and by Equation 26the same holds for exp(t). Therefore, by Equation 25, pt|0 is absolutely continuous with respect to pGt|0with a density that is bounded from above and below by M 2 and 1/M 2 respectively. We now obtain",
  "(32)": "here we used that we can upper bound the operator norm of a positive semidefinite matrix by its trace fromthe third to the fourth line. We denoted by mxt0|t and 0|t the mean and covariance of pG(xG0 |xGt = xt).From the second to last to the last line we used that the mxt0|t depends on xt linearly, and the magnitude ofthe linear dependence can be bounded uniformly in t (see Equation 44). The integral in the last line is thevariance of a standard normal random variable, which evaluates to dx. Furthermore,",
  "ti1 + xt2dt": "Since the expectation is regarding a Gaussian random variable xt, we can make it finite as long as we picki = ti+1 ti small enough. By setting t0 = 0 and t1 > 0, we can show equivalence on [t0, t1]. We can theniterate this procedure, to get equivalence on [t1, t2] and so on. Since the lowest and highest eigenvalues oft are bounded from below and above respectively, and mt is also bounded, the i can be bounded frombelow. Therefore, we get equivalence on [0, T] this way in at most T/ steps.",
  "The matrix vt": "t (Idx + vtxts(xt, t)) needs to be inverted to calculate the log likelihood, and therefore mustbe both symmetric and positive definite for all time, which puts the requirement that s(xt, t) can be writtenas the negative gradient of a potential, however it is only approximated as such s(xt, t) log pxt(xt).",
  "E.1Computational Complexity": "Let N is the number of noise scales, and let dy is the dimensions of the observation, dx are the dimensionof the image and Ts is the time complexity of evaluating the score network. Computing a vector-Jacobianproduct has time complexity Ts. Then, the time complexity of TMPD (not including fast matrix-vectorproducts) is O(N(d3y + Tsdy + Ts + Ts)). GDM comes at a smaller computational cost due to needing onlyO(1) vector-jacobian-products, instead of O(dy), resulting in a time complexity of O(N(Ts + Ts)). DPScomes at the same complexity as GDM. Whereas TMPD requires calculating the Jacobian which has memory complexity of atleast O(dxdy). This istoo large for high resolution image problems where the dimension of the observation is large. In comparison,",
  "DTMPDxt log py|t(y|xt) xtm0|tH(H vttdiag(xtm0|t)H + 2yIdy)1(y Hm0|t)": "Whilst this approximation does not require a linear solve, taking out the O(Nd3y) time complexity term,we would still like to take out the O(NTsdy) time complexity and O(dxdy) memory complexity term fromcalculating and storing the Jacobian, respectively, since this is too large for solving high resolution imageapplications. A further approximation approximates the diagonal of the Jacobian by the row sum of theJacobian which only requires one vector-jacobian product and brings the memory and time complexity ofDTMPD down to that of GDM and DPS. We use this approximation in the image experiments and findthat in practice it is only (1.5 0.1) slower than GDM and DPS across all of our experiments. Therow sum will be a good approximation of the diagonal when the Jacobian is approximately diagonal, whichhappens when there is small linear correlation between observation pixels, which we found to work well forsuper-resolution and inpainting. In inpainting, we have further improved the accuracy of the rowsum byinstead of calculating the vector-Jacobian product evaluated at H1, masking out values of the vector inthe vector-Jacobian product using the inpainting observation operator since they wont contribute to thediagonal values of the of the variance. However, heuristics can be circumvented by noting in the definition of Equation 11 does not require theinverse but rather solving a system of linear equations with right hand side y Hm0|t(xt). A very recentwork (Rozet et al., 2024), uses the natural choice of the conjugate gradient (CG) method to solve this linearsystem noting that HC0|tH + 2yI is symmetric positive definite (SPD) and is therefore compatible withthe conjugate gradient (CG) method. The CG method is an iterative algorithm to solve linear systems ofthe form Mv = b where SPD matrix M and vector b are known. Importantly, the CG method only requiresimplicit access to M through an operator that performs the matrix-vector product Mv given a vector v. Inour case, the linear system to solve is y Hm0|t = (HC0|tH + 2yI)v = Hvt xtm0|tHv + 2yIv. Thevector-jacobian product vHxtm0|t can be cheaply evaluated. In practice, there is no restriction on thescore network to be the gradient of a potential, and the gradient of the score need not be SPD. Due to thisand numerical errors, Rozet et al. (2024) observed that CG method becomes unstable after a large number ofiterations and fails to reach an exact solution. Fortunately, the authors find that using very few iterations (1to 3) of the CG method as part of the computation of the posterior score approximation leads to significantimprovements over using heuristics for the covariance. Rozet et al. (2024) have successfully applied the CGmethod to non-sparse H, such as accelerated MRI (where H is the composition of the Fourier transform andfrequency subsampling).",
  "E.2Gaussian": "When the data distribution p0(x0) is a (multivariate) Gaussian, then the reverse SDE is a linear SDE and wecan calculate all of the terms needed to sample from the target posterior using diffusion explicitly. Moreover,we can sample from the target posterior using a direct or implicit method such as Cholesky decomposition.In this simple example, we compare samples from the direct method to various conditional diffusion methods(TMPD, GDM, DPM), by plotting a sample estimate of the L2 Wasserstein distance between the sampleand the target Gaussian measures (Givens & Shortt, 1984) by using the analytical mean and covariance ofthe target distribution and empirical estimate of the mean and covariance of the sample distribution. Togenerate p0(x0), we use an equally spaced grid of vectors ui [5.0, 5.0]2 for i 1, 2, ..., 322, pick a Matern5/2 kernel for the covariance function k(ui, uj) =1 +",
  "|ui uj|2exp": "3|ui uj|whichdefines the prior p0(x0) = N(m0, C0) covariance C0 R10241024 where C0ij = k(ui, uj) and we define themean as a zero vector m0 = 0 R1024. To compute analytically the distribution of p0|y(x0|y), we sampley = Hx0 + z, z N(0, 2yIdy) and use the standard Gaussian formula to calculate the mean and covarianceof x0|y which in this case are a complete description of p0|y(x0|y). The L2 Wasserstein estimate is plottedover an increasing sample size N and for y = 0.1 in .",
  "dxdyTMPDDTMPDGDMDPSTMPDDTMPDGDMDPSTMPDDTMPDGDMDPS": "811.5 0.51.5 0.51.5 0.45.7 2.21.4 0.51.4 0.51.2 0.45.6 2.10.9 0.30.9 0.30.9 0.30.9 0.3820.7 0.33.2 1.40.4 0.36.2 0.80.9 0.32.7 1.10.5 0.36.2 2.40.9 0.21.8 0.81.0 0.31.2 0.4840.3 0.30.6 0.40.1 0.1-0.3 0.20.7 0.40.1 0.08.4 3.10.6 0.20.9 0.50.2 0.10.3 0.28012.7 0.72.7 0.72.9 1.49.1 1.32.3 0.72.3 0.72.1 1.14.7 1.81.5 0.71.5 0.71.8 0.81.9 0.98021.0 0.53.3 1.00.8 0.72.2 0.91.2 0.53.3 1.00.8 0.76.0 2.11.1 0.22.2 1.01.3 0.51.5 0.58040.3 0.10.9 0.50.1 0.0-0.4 0.21.0 0.50.1 0.14.4 1.60.9 0.21.0 0.40.4 0.20.5 0.380013.1 0.73.1 0.73.2 1.06.8 1.22.9 0.62.9 0.62.8 0.76.4 1.51.5 0.41.5 0.41.3 0.31.3 0.380021.3 0.43.6 1.20.8 0.57.4 0.91.3 0.33.2 1.10.8 0.46.4 1.91.2 0.31.9 0.51.1 0.31.1 0.380040.3 0.20.9 0.60.6 0.5-0.4 0.20.9 0.60.1 0.05.8 1.40.9 0.21.1 0.50.4 0.20.4 0.2",
  "E.3GMM": "For a given dimension dx, we consider p0 a mixture of 25 Gaussian random variables. The components havemean i,j := (8i, 8j, ..., 8i, 8j) Rdxfor(i, j) 2, 1, 0, 1, 22 and unit variance. We have set the associatedunnormalized weights i,j = 1.0. We have set 2 = 104. Note that pt(xt) =pt|0(xt|x0)p0(x0)dx0. As p0(x0) is a mixture of Gaussians, pt(xt) is also a mixture ofGaussians with means ti,j and unitary variances. Therefore, using automatic differentiation libraries,we can calculate xt log pt(xt). We chose max = 500.0 and min = 0.1. We use 1000 timesteps for thetime-discretization. For the pair of dimensions and chosen observation noise standard deviation (dx, dy, y)the measurement model (y, H) is drawn as follows: H: We first draw H N(0dydx, Idydx) and compute the SVD decomposition of H = USV.Then, we sample for (i, j) 2, 1, 0, 1, 22, si,j according to a uniform in . Finally, we setH = Udiag(si,j(i,j)2,1,0,1,22)V.",
  "where = (Idx +12 HH)1": "Euler-Maruyama solver To compare the posterior distribution estimated by each algorithm with the targetposterior distribution, we use 104 slices for the SW distance and compare 1000 samples of the continuousSDEs defined by the TMPD, DTMPD, Song et al. (2023) and Chung et al. (2022a) approximations obtainedusing 1000 Euler-Maruyama time-steps with 1000 samples of the true posterior distribution. indicatesthe Central Limit Theorem (CLT) 95% confidence intervals obtained by considering 20 randomly selectedmeasurement models (H) for each setting (dx, dx, y). DDPM compares 1000 samples of TMPD-D, GDM-D and DPS-D which are obtained using 1000denoising steps and is the extended version of . We follow Cardoso et al. (2023) and compute the slicedWasserstein distance using Wasserstein-1 distance. shows the first two dimensions of the estimatedposterior distributions corresponding to the configurations (80, 1) and (800, 1) from for one of therandomly generated measurement model (H). These illustrations give us insight into the behaviour of thealgorithms and their accuracy in estimating the posterior distribution. We observe that TMPD-D (and theEuler-Maruyama method TMPD) is the only method that covers the modes of the posterior distribution.Finally, a direct comparison to Cardoso et al. (2023) using their original experimental setup is shown in, which shows competitive performance for posterior sampling compared to Sequential Monte-Carlo,an exact sampling method.",
  "dxdyTMPD-DDTMPD-DGDM-DDPS-DTMPD-DDTMPD-DGDM-DDPS-DTMPD-DDTMPD-DGDM-DDPS-D": "811.6 0.51.8 0.62.6 0.94.7 1.51.4 0.51.8 0.72.2 0.94.7 1.60.9 0.30.9 0.21.5 0.45.2 1.3820.7 0.33.3 1.52.1 1.01.8 1.50.9 0.32.7 1.11.6 0.61.5 0.90.9 0.21.7 0.82.3 0.43.5 1.2840.3 0.30.4 0.23.8 2.30.7 0.60.3 0.20.5 0.23.8 2.20.8 0.60.6 0.20.9 0.51.8 0.32.5 0.98012.7 0.72.8 0.93.2 1.05.6 1.82.3 0.72.6 0.92.9 0.85.1 1.81.5 0.71.4 0.61.6 0.56.9 1.88021.0 0.53.2 1.12.8 1.33.2 1.91.2 0.53.2 1.12.7 1.23.1 1.91.1 0.22.1 1.01.4 0.23.9 1.28040.3 0.10.7 0.40.6 0.41.2 1.10.4 0.20.8 0.40.6 0.41.0 1.10.9 0.30.9 0.40.9 0.21.7 0.680013.1 0.73.7 0.73.5 1.15.8 1.62.9 0.63.4 0.73.3 0.95.7 1.61.5 0.41.4 0.42.0 0.46.8 1.080021.4 0.43.5 0.73.1 1.13.5 1.71.3 0.33.4 0.72.7 0.93.1 1.41.2 0.32.0 0.42.0 0.54.7 1.380040.4 0.20.7 0.50.4 0.21.4 1.00.4 0.20.8 0.50.4 0.21.3 0.90.9 0.21.1 0.50.6 0.20.9 0.4",
  "E.4Inpainting and super-resolution": "Since the DPS-D method was derived and tuned specifically for VP-SDE, we look at the VP-SDE experimentsin Section E.4.1 separately from the VE-SDE experiments in Section E.4.2. In each comparison, we use thesame score network for each method and the same sampling or discretization numerical method. All methodsare discretized using 1000 denoising steps. For the Markov chain methods we use DDPM and for the SDEmethods we use an Euler-Maruyama discretization. In contrast to DPS(-D) GDM(-D), we observe therobustness of our method across both SDEs and inpainting and super-resolution observation maps.",
  "Imagenet 256256 For VP-SDE, our Imagenet 256256 experiment compares SNIPS (Kawar et al., 2021)to diffusion methods (DDPM) DTMPD-D, DPS-D and GDM-D, and results are shown in": "FFHQ 256256 For VP-SDE, our FFHQ 256256 experiment compares diffusion methods (DDPM)DTMPD-D to DPS-D and GDM-D, and results are shown in .Fig 3, Fig 4 and Fig 5 are avisual summary of , plotting the LPIPS, SSIM and FID metrics against increasing noise for differentobservation maps. Some uncurated samples that were used to generate are shown in Fig 6, 7 and 8.We observe that all methods can successfully produce high quality reconstructions in the low noise regime",
  "ProblemMethodFID LPIPS MSE PSNR SSIM": "y = 0.01DTMPD-D29.60.230 0.0341.60e-03 7.74e-0428.4 1.90.784 0.0464 bicubicDPS-D31.40.234 0.0481.90e-03 1.07e-0327.8 2.20.776 0.062super-resolutionGDM-D29.70.198 0.0371.56e-03 8.72e-0428.6 2.10.809 0.051 y = 0.05DTMPD-D32.70.304 0.0432.90e-03 5.64e-0326.0 1.70.699 0.0604 bicubicDPS-D29.30.280 0.0512.90e-03 5.73e-0326.0 1.80.719 0.066super-resolutionGDM-D45.10.311 0.0473.08e-03 5.79e-0325.7 1.70.682 0.062 y = 0.1DTMPD-D38.00.348 0.0484.33e-03 4.72e-0324.0 1.60.635 0.0664 bicubicDPS-D30.90.318 0.0514.06e-03 5.38e-0324.4 1.60.664 0.069super-resolutionGDM-D119.60.589 0.0471.10e-02 5.56e-0319.7 1.00.376 0.055 y = 0.2DTMPD-D45.60.401 0.0497.03e-03 2.56e-0321.8 1.50.559 0.0714 bicubicDPS-D38.10.385 0.0617.59e-03 3.50e-0321.6 1.80.570 0.081super-resolutionGDM-D295.70.780 0.0335.65e-02 5.20e-0312.5 0.40.117 0.035",
  "but, visually, only DPS-D and DTMPD-D successfully produce high quality reconstructions in the high noiseregime": "Whereas DPS-D requires a hyperparameter search, there are no hyperparameters for DTMPD-D. For GDM-D we use the algorithm and default hyperparameters as described in Song et al. (2023). For DPS-D we usethe algorithm in the codebase provided by the authors Chung et al. (2022a) and we use their suggestedhyperparameters for this task, such as step-size and using static-thresholding (clipping the denoised imageat each step to a range ) whereas DTMPD-D does not require any hyperparameter tuning or static-thresholding. CIFAR10 6464 Our CIFAR-10 6464 experiment compares TMPD to DPS and GDM, and also com-pares diffusion methods (DDPM) in and score-based methods (discretized with Euler-Maruyama)in . Various samples used to produce the figures in Tables 8 and 9 are shown in Fig 10 and 11.",
  ": box mask inpainting FID vs LPIPS (left) and SSIM (right) using the VP-SDE on FFHQ-1kvalidation dataset for increasing observation noise": "then calculating the posterior variance, to give unstable solutions for the algorithm given in Song et al.(2023). To make the method stable, we instead plug the GDM posterior score approximation into a DDIMsampler in a similar way to Algorithm 1, which, for the VPSDE, brings the algorithm GDM-D closer toour method; we are then able to choose r2t = vt/(t + vt) for both VP DDIM and VP-SDE methods. For DDPM we use the step-size constant suggested in Chung et al. (2022a) for inpainting, i = /y Hm0|t, where we tune over the suggested range of [0.1, 1.0] in Chung et al. (2022a) across LPIPS,MSE, PSNR and SSIM, as shown in for each inverse problem (each line in the Tables 8 and 9).",
  "E.4.2VE-SDE": "FFHQ 256256 For VE-SDE, our FFHQ 256256 experiment compares diffusion methods (DDPM)DTMPD-D to DPS-D and GDM-D, and results are shown in . Fig 12 and Fig 13 are a visualsummary of , plotting the LPIPS, SSIM and FID metrics against increasing noise. Some uncuratedsamples that were used to generate are shown in Fig 14 and 15. Since the DPS-D and PiGDMalgorithms were developed for the VP-SDE, the method is not performant for the VE-SDE, and the GDM",
  "y = 0.1DTMPD-D39.70.241 0.0460.004 0.00325.2 2.40.799 0.048box mask inpaintingGDM-D42.10.279 0.0400.003 0.00125.1 1.60.716 0.052": "For GDM(-D), we are able to use the hyperparameter r2t = vt/(1 + vt) as suggested by Song et al. (2023),which is calculated by assuming the data distribution p0(x0) is a standard normal and then calculating theposterior variance, for both VE DDIM and VE-SDE methods, but note some instability for the GDMVE-SDE method for small noise, as shown in Fig 10. For DDPM we use the step-size constant suggested in Chung et al. (2022a) for inpainting, i = /y Hm0|t, where we tune over the suggested range of [0.1, 1.0] in Chung et al. (2022a) across LPIPS,MSE, PSNR and SSIM, as shown in for each inverse problem (each line in the Tables 12 and 13)."
}