{
  "Abstract": "One of the primary objectives in modern artificial intelligence researches is to empoweragents to effectively coordinate with diverse teammates, particularly human teammates.Previous studies focused on training agents either with a fixed population of pre-generatedteammates or through the co-evolution of distinct populations of agents and teammates.However, it is challenging to enumerate all possible teammates in advance, and it is costly,or even impractical to maintain such a sufficiently diverse population and repeatedly interactwith previously encountered teammates. Additional design considerations, such as priori-tized sampling, are also required to ensure efficient training. To address these challenges andobtain an efficient human-AI coordination paradigm, we propose a novel approach calledConcord. Considering that human participants tend to occur in a sequential manner, wemodel the training process with different teammates as a continual learning framework,akin to how humans learn and adapt in the real world. We propose a mechanism basedon hyper-teammate identification to prevent catastrophic forgetting while promoting for-ward knowledge transfer. Concretely, we introduce a teammate recognition module thatcaptures the identification of corresponding teammates.Leveraging the identification, awell-coordinated AI policy can be generated via the hyper-network. The entire frameworkis trained in a decomposed policy gradient manner, allowing for effective credit assignmentamong agents. This approach enables us to train agents to cooperate with teammates oneby one, ensuring that agents can coordinate effectively with concurrent teammates with-out forgetting previous knowledge. Our approach outperforms multiple baselines in variousmulti-agent benchmarks, either with generated human proxies or real human participants.",
  "Introduction": "Cooperative Multi-Agent Reinforcement Learning (MARL) has made significant progress in recent years,enabling multiple agents to work together towards a shared goal in diverse domains such as active voltagecontrol (Wang et al., 2021a) and dynamic algorithm configuration (Xue et al., 2022b). However, buildingAI agents that can effectively coordinate with unseen teammates, especially human teammates, remains asignificant challenge (Klien et al., 2004; Mutlu et al., 2013; Strouse et al., 2021; Koster et al., 2022; Yuanet al., 2023c). Previous approaches focused on building effective behavior models from human data, which",
  ": This figure provides a visual comparison between our proposed setting of continual coordinationlearning and typical scenarios of classic Human-AI coordination methods": "is inefficient for complex problems (Kidd & Breazeal, 2008) and may raise privacy concerns (Pan et al.,2019). Recent approaches (Treutlein et al., 2021; Mirsky et al., 2022; Fosong et al., 2022) show remarkablecoordination abilities in a wide range of tasks such as Overcooked (Carroll et al., 2019), but still suffer fromover-fitting to their training teammates and struggle to coordinate effectively with unseen agents (Mahajanet al., 2022). Thus, the challenge of building AI agents capable of generalizing to unseen teammates in theopen-world (Zhou, 2022) remains. Training with different teammates is a promising approach to tackling the mentioned issue, involving thegeneration of diverse teammates and an efficient training paradigm. To achieve the former, one approach isto use hand-crafted policies (Xie et al., 2021; Papoudakis et al., 2021), special object regularizer (Derek &Isola, 2021; Lupu et al., 2021), or Population-Based Training (PBT) (Strouse et al., 2021; Xue et al., 2022a;Zhao et al., 2023). Regarding the training paradigm, a naive way is self-play (Tesauro, 1994; Silver et al.,2018), where the agent iteratively improves via playing against itself. Fictitious Co-Play (FCP) (Heinrichet al., 2015; Strouse et al., 2021) trains agents to be the best response to both the fully-trained agentsand their checkpoints.These approaches have demonstrated significant progress in benchmarks such asOvercooked (Carroll et al., 2019) and Hanabi (Lupu et al., 2021), offering promising prospects for human-AIcoordination and cooperation. However, the aforementioned methods pre-generate various teammates andnecessitate access to all of them during training, which might be unfeasible in the real world. On the onehand, the task of enumerating all potential teammates in advance poses significant challenges. Maintaining asufficiently diverse population has already placed a substantial demand on computing and storage resources,incurring considerable costs. Furthermore, if agents are required to cooperate with an unseen teammate,learning from scratch with all previous seen teammates (especially human participants) would be even morewasteful, considering the limitations of global time differences and economic costs.On the other hand,learning a generalized agent via interactions with a population of teammates requires meticulous design toensure efficient training, such as prioritized sampling (Zhao et al., 2023). To address the aforementioned challenges, we draw inspiration from the manner in which humans learnto coordinate with diverse teammates. Human participants, with whom agents are trained to cooperate,typically emerge sequentially.Instead of learning to cooperate with all agents simultaneously, humanscontinually adapt to new teammates while retaining knowledge from previous interactions (Hadsell et al.,2020). In accordance with this idea, we propose a new coordination learning setting and formulate it as aMulti-Agent Continual Markov Decision Process (MACMDP), where the controlled agents are trained tocoordinate effectively with different teammates that appear sequentially. offers a visual comparisonbetween this novel problem setting and the conventional scenarios found in previous works. We observethat naively applying single-agent continual learning methods to MARL is inefficient, and typical MARLapproaches are inadequate for continual learning scenarios (RQ1 in Sec. 4.2). Therefore, we propose a generalframework called Concord (abbreviation for Continual coordination) to enable efficient continual trainingof AI agents and solve the MACMDP. Concretely, we construct a teammate representation space wherethe teammates are represented as latent embeddings, and we introduce a recognition module that enablesrecognizing teammates through few-shot interaction. We then utilize the learned teammate embeddingsbased on a hyper-network (Ha et al., 2017; Oswald et al., 2020) to generate the policy parameters of eachcontrolled agent. Finally, we use value decomposition to facilitate credit assignment among agents, enabling",
  ": Workflow diagram of Multi-Agent Continual Markov Decision Process": "our framework to handle multiple controlled agents rather than only one. To evaluate the effectiveness ofConcord, we conduct extensive experiments on various cooperative multi-agent benchmarks including Over-cooked (Carroll et al., 2019) and StarCraft Multi-Agent Challenge benchmark (SMAC) (Samvelyan et al.,2019). We compare Concord against multiple methods in different scenarios, and the results demonstratethat Concord significantly outperforms these methods, achieving superiority across a range of evaluationmetrics. Our main contributions are:",
  "Problem Formalization": "The purpose of human-AI coordination is for AI agents to cooperate well with diverse human teammates.Previous works rely on a fixed policy population, involving extensive training of AI agents, but this approachis computationally expensive and does not align with real-world scenarios where humans are encounteredsequentially. Currently, there is limited research on training AI agents to work well with humans in thecontinual setting. To address this gap, we introduce a problem formulation for continual multi-agent coor-dination learning, so that agents can learn to cooperate with unseen human teammates continually. We formalize the continual multi-agent coordination learning problem as a Multi-Agent Continual MarkovDecision Process (MACMDP) consisting of a tuple M := {I, S, A, P, R, , , , T}, where I is the set of AIagents, S and A are the state and action space, respectively. P is the transition function, R is the rewardfunction, [0, 1) is the discount factor and T is the continual learning length. Besides, is the teammatetype space, and is an indicator function which means that () (1 T) denotes the teammatepolicy encountered at the -th stage. For the sake of brevity, we denote the policy determined by in the-th stage as H . These teammates are encountered sequentially, as depicted in .In human-AI coordination tasks,cooperating with different humans can be defined as different tasks, and each human teammate policy Hcan be represented by a task indicator .In MACMDP, AI agents can not interact with the previoushuman teammates, which corresponds to the assumption that the agent can not interact with previousenvironments in the common continual reinforcement learning setting (Khetarpal et al., 2022), but areexpected to remember how to cooperate with all previous human teammates. When handling the task ,",
  "Published in Transactions on Machine Learning Research (10/2024)": "Number of episodes for recognizing human Average reward per episode OracleConcord EWC CLEAR NaiveConcord0 Average reward per episode zeroshotfewshot : (a) The performance comparison with different episode numbers for the recognizer. (b) Theperformance of different algorithms in zero-shot testing and the performance of Concord algorithm in few-shot testing. : Comparisons of distinct algorithms in all 4 layouts on average coordination performance with 5random seeds. Concord (ID) is that the agents are given identities, which can be seen as Concord equippedwith an Oracle identification mechanism. The best mean value on each problem except for Oracle is high-lighted in bold. The symbols +, , and indicate that the result is significantly superior to, inferior to,and almost equivalent to Concord, respectively, according to the Wilcoxon rank-sum test with a significancelevel of 0.05.",
  "Method": "In this section, we introduce the proposed Concord framework. We first introduce the teammate-adaptiveAI framework in Sec. 3.1, which includes three main components, i.e., human-aware actor critic, teammaterepresentation and recognition, and anti-forgetting mechanism. Then, we give the overview of our algorithmin Sec. 3.2, which includes the pseudo-code of the two phases and a brief diagram illustration.",
  "Teammate-adaptive AI Framework": "AI agents are anticipated to identify different teammates and adapt accordingly, enabling them to acquire theability to coordinate continually. Additionally, the agents should be able to remember previous teammateswhile learning to coordinate with new ones. We here propose a comprehensive pipeline (depicted in andApp. A) that involves designing a human-aware actor-critic structure for policies training, building teammaterepresentations for teammates capturing, and implementing an anti-forgetting mechanism to address theforgetting problem during continual learning. Human-Aware Actor-CriticIn a human-AI coordination scenario, the decision-making process of theoverall multi-agent system is jointly determined by both the AI agents and the humans. To enable AI agentsto identify different teammates, we introduce a teammate-adaptive structure, an extension and variant ofhyper-network (Ha et al., 2017; Oswald et al., 2020). This structure is specifically designed for continualhuman-AI coordination tasks and can generate actor networks that can effectively coordinate with specificteammates by leveraging consistent teammate representations. Concretely, the parameters of the AI actornetworks are obtained through Ai= f hyper(z), where f hyperrepresents the hyper-network parameterizedby . In essence, we incorporate the ability to adapt to different teammates into the hyper-network, so thatit can generate appropriate actor networks for cooperating with the human teammates based on the inputof different teammate representations z. More details are reported in App. A.1. To better assess the contribution of AI to the overall performance, we here design a linearly decomposedcentralized critic like DOP (Wang et al., 2021b). Therefore, we can create individual critics for the un-controllable human agents, and model the global Q function by combining the individual Q values of bothhuman and AI agents. This approach enables the global Q function to more accurately reflect the jointdecision-making process of human-AI coordination, and facilitate credit assignment between AI and hu-mans, allowing us to isolate the contribution of the AI agents and provide more precise learning signalsfor updating A. The critic and actor networks are updated through minimizing the critic loss Lcritic =",
  "Recognizer": ": The overall framework of Concord.During the training phase, a human-aware actor-criticis trained for AI to coordinate with humans effectively, where the AI actors are generated via a hyper-network. Teammate representations are trained via backpropagation, while generated by the recognizer inthe test phase. When testing, the AI first interacts with human to collect few trajectories, then obtains thecorresponding teammates representation for decision-making.",
  "the parameters of the critic network as , and those of the target network as": "Teammate Representation LearningTo enable our method to adapt to different teammates, as men-tioned above, we use one hyper-network that can generate adaptive actor networks by inputting differentteammate representations.Here, the teammate representations serve the crucial role of recognizing thebehavior patterns of different human teammates. To adopt this teammate representation scheme, we havetwo necessary prerequisites: 1) the teammate representations can help the hyper-network generate AI actorthat can cooperates well with human; 2) the teammate representations should be obtainable with a minimalamount of interaction with humans during the test phase. To fulfill the first prerequisite, we design an end-to-end update scheme for teammate representations. Inparticular, we maintain a separate teammate representation for each human team encountered during thecontinual learning process This teammate representation z is then fed into the hyper-network to obtainthe parameters of the AI actor network A which is expected to coordinate effectively with H . Thus, wedirectly update the hyper-network and teammate representations with the gradients back-propagated fromoptimizing the actor loss Lactor. However, we hold an assumption that the identity of the teammates isunknown during testing. Thus, although we have learned representations for each team, we do not knowwhich one to utilize during testing, which leads to the second prerequisite above. In terms of the second prerequisite, we additionally design a recognizer network that can reconstruct learnedteammate representations from limited interaction data. This recognizer network is typically implementedas a trajectory-encoder that takes the trajectories of human teammates as input and outputs the predictionof the corresponding teammate representations. For training the recognizer network, we encode trajectories",
  "11i=1 f hyper(zi) f hyper(zi)22 in addition to the actor loss to avoid f hypers forgetting the": "previous cooperation knowledge, where f hyperdenotes the hyper-network obtained after the ( 1)-th task.In summary, we propose an anti-forgetting mechanism that effectively safeguards against the forgettingof past knowledge in both the hyper-network and the recognizer network through the preservation of theupdated hyper-network from the previous round and a small memory pool containing few-shot interactionsand teammate representations of previous human teammates.",
  "Overall Algorithm": "Finally, we provide the overall description of the procedure of our approach under a continual coordinationlearning setting. The main parts of our approach Concord can be decomposed into the training phase andthe test phase. The pseudo-codes for these two phases are respectively shown in Alg. 1 and Alg. 2. A briefdiagram illustrating the main idea of our approach can also be found in App. A.2. During the training phase, we conduct cooperation learning with human teammates one (team) by one (team)from H1 to HT , where we learn the hyper-network and the recognizer network together. As detailed from lines5 to 13 of Alg. 1, during the collaborative training with the -th human team, we firstly optimize the human-aware actor-critic architecture by minimizing the Lcritic and Lactor losses, where the teammate representationz is concurrently updated in an end-to-end manner with the back-propagated gradient zLactor. Whilefor the hyper-network f hyper, we update it with the actor loss Lactor plus one regularization loss Lreg, whichmeans that the actual loss function for the hyper-network is",
  "i=1f hyper(zi) f hyper(zi)22,(1)": "where reg is the coefficient balancing these two loss terms. We optimize the critic network, hyper-networkand teammate representation in lines 11, 12 and 13 of Alg. 1, respectively. Following the training process ofhyper-network and teammate representation, in line 15 we update with the latest hyper-network, and inline 17 we retain a certain amount of trajectories to buffer B. Subsequently, from lines 18 to 21, we furtheroptimize the recognizer network via reconstructing the representations of all previously encountered humanteammates, which means that we update the f encvia minimizing the reconstruction loss i=1 Lreconi, whereLreconiindicates the reconstruction loss for the teammate representation of the i-th human team.Aftercompleting these steps, we proceed to the collaborative training with the next human team. In brief, thecollaborative training with each group of human teammates is decomposed of two separate stages, where wetrain the hyper-network and teammate representations at the first stage and update the recognizer networkat the second stage.",
  ": end for": "When it comes to the test phase, in line 1 of Alg. 2, we first allow a few interaction with the humans. The,as detailed in lines 2 and 3 respectively, the sampled human trajectories are fed into the recognizer networkf encto help attain the predicted teammate representation, which is then inputted into the hyper-networkf hyperto obtain the actor network for the AI policy. The final actor network here is employed to coordinatewith the humans.",
  "Experiments": "In this section, we first conduct experiments on the widely-used Overcooked (Carroll et al., 2019) to verifywhether Concord can effectively coordinate with human teammates under the continual setting. Then, weevaluate Concords ability to coordinate with multiple teammates through experiments on SMAC (Samvelyanet al., 2019). The human teammates involved in these experiments consist of both human-like models andreal human participants. To comprehensively analyze the performance of various algorithms in the context of continual human-AIcoordination, we introduce the Single Coordination Performance Pi(t), where t {1, 2, ...T} and Pi(t)represents the episode reward when coordinating with the i-th human team after completing training with",
  "Baselines and Environments": "To completely evaluate the performance of Concord, we introduce the following baselines for comparison.First of all, we adapt EWC (Kirkpatrick et al., 2017) and CLEAR (Rolnick et al., 2018), which are twoclassic continual learning algorithms, to our experiment setting. Then, we additionally design two otherbaselines: Vanilla and Oracle. Vanilla does not do any extra design for the continual learning process, whileOracle is always allowed for collaborative training with all human teammates. Here, to address the potentialunfamiliarity of some readers with the field of continual learning, we have provided additional descriptionsfor the EWC and CLEAR algorithms to enhance the readers understanding of the specific processes of EWCand CLEAR, as well as how we adapt them in our context. EWCElastic Weight Consolidation (EWC) is a regularization-based technique that selectively reduces theplasticity of weights to alleviate catastrophic forgetting. The technique Fisher information matrix evaluatesthe importance of all weights and uses that for gradient updates. There are two variants of EWC: OfflineEWC keeps a model with a Fisher matrix for every previous task, which causes costly storage. Online EWCuses only one Fisher matrix that is computed based on all the previous tasks. Here, we choose Online EWCas our benchmark. CLEARWhen training, CLEAR samples a mini-batch from the current task and another replay mini-batch. The samples in the replay mini-batch are selected with the same probability as all previous tasks.CLEAR leverages behavioral cloning from replay mini-batch to enhance stability. Specifically, two additional",
  ": Four layouts of the Overcooked environment, with each one emphasizing different coordinationstrategies": "loss terms are added to induce behavioral cloning between the network and its past self. (1) the KL divergencebetween the historical policy distribution and the present policy distribution, (2) the 2 norm of the differencebetween the historical and present value functions. Unlike the original paper, we use Cross Entropy lossinstead of KL loss to guide the agent to choose the same action as its past self under the same state becausewe find it more effective. Additionally, we dont add any constraints to criticism for easier learning. OracleHere, we would like to explain why the multi-task learning approach can be seen as Oracle. 1)Multi-task learning (Oracle) can access all the previous models in the training process. However, Concord(and other continual learning methods) can only be trained with the previous teammates once in the process.2) Multi-task learning (Oracle) knows which human team it is coordinating with in the test phase. However,Concord (and other continual learning methods) has to identify the teammates by some specific mechanisms. In the preceding discussion, we introduced the baseline algorithms compared in our study. In the followingpart, we will proceed to discuss the environments utilized in our experiments. Specifically, our experimentsare conducted in two widely recognized multi-agent cooperation environments: Overcooked (Carroll et al.,2019) and SMAC (Samvelyan et al., 2019). OvercookedOvercooked is an environment proposed for coordination challenges. In this environment,two players control one kitchen chef to cook and serve soup. Specifically, they are required to put specificingredients in one pot, make soup and deliver it, for example, in Coord. Ring layout, the players need toput three onions in a pot, collect an onion soup from the pot after 20-time steps, and deliver the dish toa counter. The agents will receive 20 points for each dish served, and the goal is to serve as many dishesas possible within the allotted time. Moreover, we select 4 different layouts in our experiments, which areshown in . StarCraft Multi-Agent Challenge (SMAC)SMAC is a popular benchmark for cooperative multi-agent reinforcement learning, which contains multiple combat scenarios. In this environment, the AI agentsand human teammates control specific ally units and are expected to coordinate to defeat the enemy unitscontrolled by the built-in AI. Specifically, at the beginning of each episode, the ally units are spawned onthe fixed regions on the map.At each timestep, the ally agents can select actions from the action set{no-op, move[right, left, up, down], stop, attack[enemy_id]}, and then a global reward will begiven according to the damage caused to enemy units. Extra rewards of 10 and 200 will be given if oneenemy unit is killed or the ally units win the combat. We conduct experiments on three SMAC maps in thispaper as shown in , which are 3m, 2z1s, and 4m, respectively. On all these three maps, there exist thesame number and type of units on both ally and enemy sides, where 3m and 4m signify the presence of 3marines and 4 marines, respectively, and 2z1s indicates 2 zealots and 1 stalkers.",
  "Continual Coordination with Human-like teammates": "First, we evaluate continual coordination ability with human-like teammates on Overcooked (Carroll et al.,2019), where different layouts present a unique challenge that can be overcome through effective coordination.The players are required to put three onions in a pot, collect an onion soup from the pot after 20 time stepsand deliver the dish to a counter. The agents will receive 20 points for each dish served, and the goal is toserve as many dishes as possible within the allotted time. We run each algorithm for 5 random seeds andeach seed was tested 32 times. The final results were obtained after averaging these results. We investigatethe following research questions (RQs). RQ1: How does Concord perform compared to other methods?As can be seen from ,Vanilla achieves the most inferior coordination ability in almost all scenarios, indicating a specific consid-eration for the tasks where teammates appear sequentially. Other successful approaches for single agentcontinual learning, like EWC, and CLEAR, also suffer from performance degradation in the involved bench-marks, demonstrating the necessity of specific design for MARL. The Oracle method, where we train all thetasks simultaneously, can be seen as an upper bound of performance on the related benchmarks, acquiringsuperiority over all baselines. Our approach Concord, obtains comparable performance to Oracle, indicatingthe efficiency of all the designed modules. Besides, according to Wilcoxon rank-sum test with significancelevel 0.05, Concord is significantly better than EWC, CLEAR and Vanilla, except for CLEAR on ManyOrders and EWC on Open Asy. Advantages, which further strengthen our results. Furthermore, we also show the comparisons of different methods on forgetting and forward transfer in Tab. 1.Concord achieves the minimum forgetting on all the four layouts, validating the effectiveness of the proposedanti-forgetting mechanism.Besides, the forward transfer values of Concord on all the layouts are best,",
  ": Single coordination performance of Vanilla (left) and Concord (right) on the Asym. Adv. layout.The value at i-th row and j-th in the matrix is Pi(j)": ": Comparisons of distinct algorithms in all 4 layouts on forgetting and forward transfer metrics. Thebest and second best values on each layout are colored in red and blue, respectively. The symbols +, and indicate that the result is significantly superior to, inferior to, and almost equivalent to Concord,respectively, according to the Wilcoxon rank-sum test with significance level 0.05.",
  "Vanilla-46.92 11.57 80.94 9.19 -109.83 20.48 60.82 7.33 -68.36 10.14 67.61 4.16 -123.89 17.91 109.94 32.31": "indicating that Concord can coordinate well with unseen teammates. Vanilla is worst, demonstrating thenecessity of mechanism for continual learning. Specifically, we compare the single coordination performanceof Concord and Vanilla on the Asym. Adv. layout of Overcooked environment in . As expected,Vanilla exhibits a considerable amount of forgetting, as indicated by the lower triangle of the matrix, andhas poor forward transfer ability, as reflected by the upper triangle of the matrix. While Concord performssignificantly better than Vanilla. More results on other layouts can be found in -16 of App. C.1. RQ2: How does the teammate representation mechanism work?As an important aspect of Con-cord, the learning of teammate representations helps AI agents prevent catastrophic forgetting and effectivelyadapt to different human teammates. The human recognizer encodes a representation embedding for eachhuman team based on their behavior patterns.Using this embedding, an appropriate actor network isgenerated and utilized to effectively cooperate with that human team. We demonstrate the differences inbehavior patterns among distinct human-like models and the corresponding relationship between embeddingand behavior pattern in . Two main observations can be made: 1) The behavior patterns of human-likemodels are diverse and can be characterized through four key steps. Visualization results indicate that thesemodels exhibit varying behavior patterns.The output representations from the recognizer network alsodiffer significantly, forming distinct clusters. 2) Similar behavior patterns have similar embedding represen-tations, demonstrating the effectiveness of our representation mechanism in identifying teammates behaviorpatterns. For instance, the behavior patterns of models 3 and 4 are identical, and their embedding represen-tations are similarly aligned. This pattern of similarity also holds for models 6 and 11, as well as 8 and 12.Additionally, models 5, 6, 8, 11, and 12, which all utilize the right pot for cooking, show embeddings thatare clustered on the same side of a dividing line, in contrast to other models. To further investigate the impact of teammate representations on AI agent behavior, we also present thebehavior patterns displayed by AI actors generated by the hyper-network in response to different teammaterepresentations. The findings indicate that the AI agent can adapt its behavior to different teammates,",
  "Onion: CWDish: CCWSoup: CCWDeliver: CCW": ": The t-SNE (Van der Maaten & Hinton, 2008) projection of different behavior patterns. Completinga delivery task requires several key steps, including placing an onion into the pot, taking a dish, taking a readysoup, and delivering the soup, and each step has three directions for human-like models: always clockwise(CW), always counterclockwise (CCW), and bi-direction (Bi-D). Then, the behavior pattern of a teammatecan be defined as the directions used in the key steps. Average reward per episode ConcordConcord w/o Human CriticConcord (Average over Tasks)Concord w/o Human Critic (Average over Tasks) P&PP&MP&EM&MM&EE&E0.0 0.2 0.4 0.6 0.8 1.0 ConcordEWCCLEARVanillaMemory Decay",
  "(a) Study on value decomposition(b) Experimental results on SMAC": ": (a) Performance comparison between Concord with and without value decomposition. (b) Ex-perimental results on the map 4m. The gray shaded bar is used to denote the degree of forgetting, whichequals to the highest test win rate during training minus the final test win rate. The x-axis represents thecombination of two teammates, e.g., P&P means both teammates are poor (P) models. demonstrating varied targeted actions accordingly.Notably, the agent exhibits similar behaviors wheninteracting with teammates who have comparable behavior patterns. For instance, when comparing thebehaviors with the 3-rd and 4-th teammates, only minor differences are observed in how the AI agent placesthe onion into the pot.",
  "Continual Coordination between Multiple Human-like teammates and Multiple AI Agents": "In the previous experiments, there was only one agent and one teammate in the environment.Multi-player human-AI coordination, i.e., the coordination between multiple agents and multiple teammates, is anunderstudied yet important setting in practical applications. In this section, to test the generalization abilityof Concord, we use three maps in the SMAC environment (Samvelyan et al., 2019) to assess the coordinationability between multiple human-like teammates and multiple AI agents. We use DOP (Wang et al., 2021b)to train the human-like teammates. Three checkpoints with low, medium, high win rates during the trainingprocess were selected to simulate poor (P), medium (M) and expert (E) human players, respectively. Then,we train the agent via different algorithms to continually coordinate with single human-like teammate (maps3m and 2s1z) or multiple human-like teammates (map 4m).(b) presents the results on map 4m,from which we can see that the gray shaded area of Concord is smallest, indicating that Concord generallyhas better anti-forgetting performance over other baselines. Results on maps 3m and 2s1z are reported inApp. C.6.",
  "Continual Coordination with Humans": "Finally, we conduct experiments on the Open Asy. Advantages layout to investigate the ability of coordinat-ing with real humans. We first collect around 160 human-human trajectories (for a total of 64k environmenttimesteps) and partition them into two subsets, splitting each trajectory into two single-agent trajectories.Based on the collected human data, we construct 8 sets of human proxy models through behavior cloning.Then we train different methods by using these human proxy models as the teammates that appear sequen-tially. Note that the AI agents obtained by different methods will play with the real human participants inthe test phase, not the human proxy models. More details about the construction of human proxy modelsand ethical statement concerning this experiment are provided in App. D.1 and D.2, respectively. Main results. We show the single coordination performance after training in (a). Among thesealgorithms, Concord is the closest to oracle in terms of performance, and outperforms CLEAR and EWCbaselines with most teammates, which is consistent with the results in .2. The Vanilla methodperforms the worst and shows catastrophic forgetting, which again illustrates the importance of the anti-forgetting in continual human-AI coordination. Human preference. To check the preference of AI agents obtained by different algorithms of human players,we additionally introduce a popular subjective metric, i.e., human preference (Strouse et al., 2021; Yu et al.,2023), to evaluate different methods, with detailed explanation in App. D.3. As shown in (b), Concordis better than the three baselines and comparable with Oracle. For instance, the percentage of volunteers who",
  "(a) Main results(b) Human preference": ": (a) Single coordination performance results with humans. (b) Human preference results for rowmethod over column method. The value at i-th row and j-th column is the difference in the percentage ofhumans who prefer i-th method compared to those who prefer j-th method more. prefer Concord method is 62.5% higher than those who prefer CLEAR. This shows that Concord is relativelymore preferred by humans, demonstrating the value of Concord for practical human-AI coordination.",
  "Related Work": "Many real-world problems consist of multiple interactive agents, which can often be represented as a Multi-Agent Reinforcement Learning (MARL) problem (Busoniu et al., 2008; Zhang et al., 2021). Additionally,when these agents share a common objective, it falls under the category of cooperative MARL (Oroo-jlooy & Hajinezhad, 2022), which has exhibited significant advancements in diverse domains such as pathfinding (Sartoretti et al., 2019), active voltage control (Wang et al., 2021a), and dynamic algorithm con-figuration (Xue et al., 2022b). Numerous approaches have been proposed to facilitate agent coordination,including policy-based methods such as MADDPG (Lowe et al., 2017) and MAPPO (Yu et al., 2022), value-based techniques like VDN (Sunehag et al., 2018) and QMIX (Rashid et al., 2018), and other approacheslike the transformer architecture (Wen et al., 2022). These methods have demonstrated remarkable coor-dination abilities across a wide range of tasks, including SMAC, Hanabi, and GRF (Yu et al., 2022). Inaddition to the mentioned approaches and their variants, numerous other methods have been proposed toinvestigate cooperative MARL, such as efficient communication strategies (Zhu et al., 2022), offline policydeployment (Zhang et al., 2023), model learning in MARL (Wang et al., 2022a), policy robustness in thepresence of perturbations (Guo et al., 2022), and training paradigms like CTDE (centralized training withdecentralized execution) (Lyu et al., 2021), and more. Building AIs that can cooperate and coordinate with different teammates or human remains a fundamentalrequirement and challenge for AI (Dafoe et al., 2021). This ability is essential for numerous applications, in-cluding human-machine communication (Guzman & Lewis, 2020), cooperative autonomous vehicles (Toghiet al., 2021), and assistive robot control (Losey et al., 2022). A typical approach utilizes techniques likemodelling (Albrecht & Stone, 2018) to capture others intentions or behavior or (repeatedly) build an ef-fective behavior model over human data and plan with the human model (Sheridan, 2016). These methodsrequire an expensive and time-consuming data-collection process. Benefiting from the success of cooper-ative MARL (Wong et al., 2022), many related approaches like ad-hoc teamwork (AHT) (Mirsky et al.,2022), zero-shot coordination (ZSC) (Treutlein et al., 2021), few-shot teamwork (FST) (Fosong et al., 2022)have been developed recently. AHT is the research problem of designing agents that can coordinate withnew teammates without prior coordination (Stone et al., 2010), including teammates type inference (Bar-rett & Stone, 2015; Chen et al., 2020), changing point detection (Ravula et al., 2019), partial observationsolving (Gu et al., 2021), adversarial training (Fujimoto et al., 2022), robustness training (Rahman et al.,2022), etc. The goal of ZSC is to train the agent(s) that can coordinate effectively with a wide range of",
  "Conclusion and Future Work": "Recognizing the importance and practicality of human-AI coordination, this study takes a significant stridetowards addressing this challenge in a continual manner.We begin by formulating the problem as aMACMDP, where teammates are encountered sequentially. Subsequently, a mechanism based on hyper-teammate identification is proposed to avoid catastrophic forgetting while promote forward knowledge trans-fer for multi-agent coordination. To the best of our knowledge, our proposed method, Concord, is the firstto address the human-AI coordination problem via continual training. Experiments on various multi-agentbenchmarks validate the effectiveness of Concord, demonstrating its strong ability to continually coordinatewith generated human-like models or real human participants across a range of evaluation metrics. Currently,Concord has the following two main limitations. Although Concord alleviates the issue of storage overheadcompared to previous methods, it still necessitates a certain amount of storage to prevent forgetting. Be-",
  "Stefano V Albrecht and Peter Stone. Autonomous agents modelling other agents: A comprehensive surveyand open problems. Artificial Intelligence, 258:6695, 2018": "Samuel Barrett and Peter Stone.Cooperating with unknown teammates in complex domains: A robotsoccer case study of ad hoc teamwork. In Proceedings of the Twenty-Ninth AAAI Conference on ArtificialIntelligence, pp. 20102016, 2015. Lucian Busoniu, Robert Babuska, and Bart De Schutter. A comprehensive survey of multiagent reinforcementlearnin. IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews), 38(2):156172, 2008. Micah Carroll, Rohin Shah, Mark K Ho, Tom Griffiths, Sanjit Seshia, Pieter Abbeel, and Anca Dragan.On the utility of learning about humans for human-AI coordination. In Advances in Neural InformationProcessing Systems 32, pp. 51755186, 2019. Francisco M Castro, Manuel J Marn-Jimnez, Nicols Guil, Cordelia Schmid, and Karteek Alahari. End-to-end incremental learning. In Proceedings of the European conference on computer vision (ECCV), pp.233248, 2018. Rujikorn Charakorn, Poramate Manoonpong, and Nat Dilokthanakul. Generating diverse cooperative agentsby learning incompatible policies. In The 11th International Conference on Learning Representations, 2023. Shuo Chen, Ewa Andrejczuk, Zhiguang Cao, and Jie Zhang. Aateam: Achieving the ad hoc teamwork byemploying the attention mechanism. In The Thirty-Fourth AAAI Conference on Artificial Intelligence,pp. 70957102, 2020.",
  "Ted Fujimoto, Samrat Chatterjee, and Auroop Ganguly. Ad hoc teamwork in the presence of adversaries.arXiv preprint arXiv:2208.05071, 2022": "Jean-Baptiste Gaya, Thang Doan, Lucas Caccia, Laure Soulier, Ludovic Denoyer, and Roberta Raileanu.Building a subspace of policies for scalable continual learning. arXiv preprint arXiv:2211.10445, 2022a. Jean-Baptiste Gaya, Laure Soulier, and Ludovic Denoyer. Learning a subspace of policies for online adapta-tion in reinforcement learning. In The 10th International Conference on Learning Representations, 2022b.",
  "Cory D Kidd and Cynthia Breazeal. Robots at home: Understanding long-term human-robot interaction.In IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 32303235, 2008": "James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu,Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophicforgetting in neural networks. Proceedings of the national academy of sciences, 114(13):35213526, 2017. Glen Klien, David D Woods, Jeffrey M Bradshaw, Robert R Hoffman, and Paul J Feltovich. Ten challengesfor making automation a team player\" in joint human-agent activity. IEEE Intelligent Systems, 19(6):9195, 2004. Raphael Koster, Jan Balaguer, Andrea Tacchetti, Ari Weinstein, Tina Zhu, Oliver Hauser, Duncan Williams,Lucy Campbell-Gillingham, Phoebe Thacker, Matthew Botvinick, et al. Human-centred mechanism designwith democratic AI. Nature Human Behaviour, 6(10):13981407, 2022. Dhireesha Kudithipudi, Mario Aguilar-Simon, Jonathan Babb, Maxim Bazhenov, Douglas Blackiston, JoshBongard, Andrew P Brna, Suraj Chakravarthi Raja, Nick Cheney, Jeff Clune, et al. Biological underpin-nings for lifelong learning machines. Nature Machine Intelligence, 4(3):196210, 2022. Saurabh Kumar, Aviral Kumar, Sergey Levine, and Chelsea Finn. One solution is not all you need: Few-shotextrapolation via structured maxent RL. In Advances in Neural Information Processing Systems 33, pp.81988210, 2020. Niklas Lauffer, Ameesh Shah, Micah Carroll, Michael D. Dennis, and Stuart Russell. Who needs to know?Minimal knowledge for optimal coordination.In Proceedings of the 40th International Conference onMachine Learning, pp. 1859918613, 2023.",
  "Anuj Mahajan, Mikayel Samvelyan, Tarun Gupta, Benjamin Ellis, Mingfei Sun, Tim Rocktschel, andShimon Whiteson. Generalization in cooperative multi-agent systems. arXiv preprint arXiv:2202.00104,2022": "Marc Masana, Xialei Liu, Bartlomiej Twardowski, Mikel Menta, Andrew D Bagdanov, and Joost van deWeijer. Class-incremental learning: survey and performance evaluation on image classification. arXivpreprint arXiv:2010.15277, 2020. Reuth Mirsky, Ignacio Carlucho, Arrasy Rahman, Elliot Fosong, William Macke, Mohan Sridharan, PeterStone, and Stefano V Albrecht.A survey of ad hoc teamwork research.In European Conference onMulti-Agent Systems, pp. 275293, 2022.",
  "Johannes Von Oswald, Christian Henning, Joo Sacramento, and Benjamin F. Grewe. Continual learningwith hypernetworks. In The 8th International Conference on Learning Representations, 2020": "Xinlei Pan, Weiyao Wang, Xiaoshuai Zhang, Bo Li, Jinfeng Yi, and Dawn Song. How you act tells a lot:Privacy-leaking attack on deep reinforcement learning. In Proceedings of the 18th International Conferenceon Autonomous Agents and MultiAgent Systems, pp. 368376, 2019. Georgios Papoudakis, Filippos Christianos, and Stefano Albrecht. Agent modelling under partial observ-ability for deep reinforcement learning. In Advances in Neural Information Processing Systems 34, pp.1921019222, 2021.",
  "Arrasy Rahman, Elliot Fosong, Ignacio Carlucho, and Stefano V Albrecht. Towards robust ad hoc teamworkagents by creating diverse training teammates. arXiv preprint arXiv:2207.14138, 2022": "Tabish Rashid, Mikayel Samvelyan, Christian Schroeder, Gregory Farquhar, Jakob Foerster, and ShimonWhiteson. QMIX: Monotonic value function factorisation for deep multi-agent reinforcement learning. InProceedings of the 35th International Conference on Machine Learning, pp. 42954304, 2018. Manish Ravula, Shani Alkoby, and Peter Stone.Ad hoc teamwork with behavior switching agents.InProceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, pp. 550556,2019.",
  "David Rolnick, Arun Ahuja, Jonathan Schwarz, Timothy P. Lillicrap, and Greg Wayne. Experience replayfor continual learning. In Advances in Neural Information Processing Systems 32, pp. 348358, 2018": "Mikayel Samvelyan, Tabish Rashid, Christian Schrder de Witt, Gregory Farquhar, Nantas Nardelli, TimG. J. Rudner, Chia-Man Hung, Philip H. S. Torr, Jakob N. Foerster, and Shimon Whiteson. The Starcraftmulti-agent challenge. In Proceedings of the 18th International Conference on Autonomous Agents andMultiAgent Systems, pp. 21862188, 2019. Guillaume Sartoretti, Justin Kerr, Yunfei Shi, Glenn Wagner, TK Satish Kumar, Sven Koenig, and HowieChoset. Primal: Pathfinding via reinforcement and imitation multi-agent learning. IEEE Robotics andAutomation Letters, 4(3):23782385, 2019.",
  "Thomas B Sheridan. Humanrobot interaction: status and challenges. Human factors, 58(4):525532, 2016": "David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, MarcLanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, et al.A general reinforcement learningalgorithm that masters chess, shogi, and go through self-play. Science, 362(6419):11401144, 2018. Peter Stone, Gal Kaminka, Sarit Kraus, and Jeffrey Rosenschein. Ad hoc autonomous agent teams: Col-laboration without pre-coordination. In Proceedings of the Twenty-Fourth AAAI Conference on ArtificialIntelligence, pp. 15041509, 2010.",
  "Gerald Tesauro.Td-gammon, a self-teaching backgammon program, achieves master-level play.Neuralcomputation, 6(2):215219, 1994": "Behrad Toghi, Rodolfo Valiente, Dorsa Sadigh, Ramtin Pedarsani, and Yaser P Fallah. Cooperative au-tonomous vehicles that sympathize with human drivers. In IEEE/RSJ International Conference on Intel-ligent Robots and Systems, pp. 45174524, 2021. Johannes Treutlein, Michael Dennis, Caspar Oesterheld, and Jakob Foerster. A new formalism, method andopen issues for zero-shot coordination. In Proceedings of the 38th International Conference on MachineLearning, pp. 1041310423, 2021.",
  "Ke Xue, Yutong Wang, Cong Guan, Lei Yuan, Fu Haobo, Fu Qiang, Chao Qian, and Yang Yu. Heterogeneousmulti-agent zero-shot coordination by coevolution. arXiv preprint arXiv:2208.04957, 2022a": "Ke Xue, Jiacheng Xu, Lei Yuan, Miqing Li, Chao Qian, Zongzhang Zhang, and Yang Yu.Multi-agentdynamic algorithm configuration. In Advances in Neural Information Processing Systems 35, pp. 2014720161, 2022b. Chao Yu, Akash Velu, Eugene Vinitsky, Jiaxuan Gao, Yu Wang, Alexandre Bayen, and Yi Wu.Thesurprising effectiveness of PPO in cooperative multi-agent games. In Advances in Neural InformationProcessing Systems 35, pp. 2461124624, 2022. Chao Yu, Jiaxuan Gao, Weilin Liu, Botian Xu, Hao Tang, Jiaqi Yang, Yu Wang, and Yi Wu. Learningzero-shot cooperation with humans, assuming humans are biased. In The 11th International Conferenceon Learning Representations, 2023.",
  "(a)(b)": ": (a) Network architecture of the hyper-network and actor network. (b) Network architecture ofthe recognizer network. T in the figure is short for T H , indicating the horizon of the human trajectory H . Hyper-NetworkThe hyper-network in our method is used to generate the parameters of the AI actorby inputting the teammate representation to it.Thus, the network architecture determines the size ofparameters generated by the hyper-network. We model the actor network with three layers as shown in(a), including a fully-connected layer, followed by a 64 bit GRU, and further followed by anotherfully-connected layer that outputs a probability distribution over local actions. In our design, the parametersof this actor network is completely generated by the hyper-network. We model the hyper-network as threemulti-layer perceptions (MLPs), each generating the parameters of one layer of the actor network, which isalso shown in (a). We set each MLP with two hidden layers of 20 neurons and ELU non-linearityfunction, and the input dimension of each MLP network equals to the dimension of teammate representation,which is set 32 throughout all our experiments. Recognizer NetworkThe recognizer network (see (b)) is utilized to reconstruct the teammaterepresentation z from a few of human interaction trajectories. Specifically, the recognizer network is builtas a RNN-based trajectory encoder, which takes the human trajectory as input. For each human trajectory H , the recognizer will output a predicted teammate representation for each timestep t, which we denoteas zt, with the trajectory information before t. Then we update the recognizer network via minimizing thefollowing reconstruction loss:",
  "t=0T 1tzt z22,(2)": "which is a weighted sum of the 2 distances between every zt and z, where we utilize one hyper-parameter to control the weights. On one hand, by minimizing the 2 distance between each zt and z, we ensure therecognizers ability to reconstruct the representation z from variable-length trajectory information. On theother hand, we assign larger weights to the zt loss terms with larger t. The reason for this practice is thatduring the final test phase, only the predicted teammate representation at the last timestep zT 1is utilizedas the input of the subsequent networks, which means that we hope that zt with larger t should be moreaccurate. From a different perspective, the reconstruction losses for zt where t is smaller than T 1 areutilized as auxiliary losses to facilitate the recognizer network training. In practice, the hyper-parameter isset as 0.994 and 0.92 in Overcooked and SMAC, respectively. Above, we have explained how the recognizer",
  "(1)": ": A high-level workflow showing the main idea of our approach. We use z to denote the initializedrepresentation and z for the learned teammate representation. During the recognizer training, each taskstores a small amount of data in the Replay Buffer, where N is the number of trajectories per teammate,and supervised learning is performed to align the behavioral trajectories and teammate representations. network generates the reconstructed teammate representation with one human trajectory. However, we aimto utilize multiple trajectories to recognize teammates in our algorithm. In practice, we firstly generaterepresentation with each human trajectory and then compute the mean vector of these representations.",
  "A.2Overall Workflow of Concord": "Following the workflow of the multi-agent continual coordination learning in , the main idea of ourapproach is depicted in . In more details, Concord can be decomposed into two phases, the trainingphase and the test phase. During the training phase, we conduct the continual coordination learning, andduring the test phase, we recognize the involved human teammates and let the AI agents to cooperate withthem. The overall flow of these two phases are respectively shown in Alg. 1 and Alg. 2 in the maintext.",
  "A.3Storage Requirement During Continual Learning": "To address the potential forgetting problem of the hyper-network and recognizer network, Concord needs tostore some variables when learning continually. Specifically, our algorithm requires storing the following twotypes of variables: a. The parameters of the hyper-network at the last round, denoted as , are used to compute Lreg.This variable always occupies static storage space, which is not relevant to the continual learninglength T. Therefore, the storage cost for is considered acceptable. b. A small buffer B is utilized to store a few of human trajectories and teammate representations forprevious teammates. For each encountered teammate policy H , we store 36 human trajectoriesand one representation vector with dimension of 32. The final storage cost for these variables growslinearly with the continual learning length T. In practice, we find that the storage cost for thesevariables are relatively acceptable because we only store a few of human trajectories and the storagecost for teammate representations is small.",
  "Hyper-parameterValue": "Discount factor 0.99RMSprop learning rate for actor / critic0.0005 / 0.0001Adam learning rate for hyper and recognizer network0.001Batch size for actor learning32Buffer size for critic learning5000Max/Min value for -greedy exploration0.5 / 0.05Step number of exploration decay period500kEmbedding dimension for teammate representation (Demb)32Regularization loss weight (reg)2.0Episode number for teammate recognition2Episode number for training / validating recognizer network32 / 4",
  "B.2Construction of Human-like Teammates": "To evaluate the performance of coordination with human-like teammates, we should first construct a setof teammates.Building AIs that can cooperate with humans remains a fundamental challenge (Dafoeet al., 2021), as it requires human-like teammates under the property of high behavior diversity. We hereuse Maximum Entropy Population-based training (MEP) (Zhao et al., 2023) and Hidden-utility Self-Play(HSP) (Yu et al., 2023) algorithms to model a diversity of human-like behavioral policies, for their provendiverse agent ability in the Overcook environment. MEP trains partners with population entropy bonus topromote the pairwise diversity between partners and the individual diversity of partners themselves. HSPexplicitly models human biases as hidden reward functions in the self-play objective thus can generate anaugmented policy pool with biased policies.We use the code released by the HSP algorithm1 to trainMEP and HSP policies, and the number of our human-like models is defined as 12 (among them, 8 MEPpolicies and 4 HSP policies), to simulate real human behavior diversity and preference. As shown in ,the diverse teammates allow the agent to learn more universal coordination behaviors, and therefore havestronger continual human-AI coordination performance.",
  "(a) Coord. Ring(b) Asymm. Adv.(c) Many Orders(d) Open Asymm. Adv": ": Illustration of diversity of teammates. We plot the distribution of teammates actions p { | s}by t-SNE (Van der Maaten & Hinton, 2008) projection, where each point represents the t-SNE results ofteammates action distribution in each episode, and different teammates have different action distributions,thus the obtained teammates can be seen as diverse.",
  "C.1Additional Results on Overcooked Environment": "In Sec. 4.2 of the manuscript, we compare the detailed single coordination performance of Concord and Vanillaon the Asym. Adv. layout of Overcooked environment. To further analyze the situations on several otherlayouts, we present additional experimental results on the other three layouts here. -16 demonstratethat Concord continues to suffer less forgetting and achieves better forward transfer performance.",
  "C.2Additional Analysis": "One important part of Concord is to utilize the recognizer network to recognize the teammates througha few of interaction trajectories during the test phase. To analyze whether such a few-shot mechanism isnecessary and examine how the number of episodes used for teammate recognition influences the algorithmperformance, we conduct ablation studies on the layout Asym. Adv. of Overcooked environment. We firstprovide the experimental results under different numbers of episodes for recognizing teammates, and thenwe further investigate the zero-shot coordination ability of different methods. Moreover, we also providefurther analysis, including comparison with given teammate ID, ablation studies and sensitivity analysis ofthe recognition mechanism, as well as sensitivity analysis of the agent actor network architecture in thissection.",
  ": Single coordination performance of Vanilla (left) and Concord (right) on the Open Asy. Adv.layout. The value at i-th row and j-th in the matrix is Pi(j)": "Number of episodes for recognition.The selection of the number of episodes for human recognitionis a critical hyper-parameter in Concord. As depicted in (a), utilizing 2 episodes yields a notableimprovement compared to using only 1 episode. Although employing more episodes necessitates additionaldata, the impact is not substantial. Consequently, the number of episodes for human recognition has beenset to 2 in our experiments. Few-shot and zero-shot recognition.We ablate the few-shot mechanism for different methods, as shownin (b). For the zero-shot version of our approach, we refrain from pre-sampling certain trajectoriesto recognize teammates. Instead, we utilize in-episode trajectory information to estimate the teammaterepresentation, which is subsequently inputted into the hyper-network. Additionally, to better differentiatebetween the methods, we evaluate their generalization ability in this ablation study by conducting testswith an additionally generated teammate policy. As we can see from the results, Concord achieves betterperformance than other methods under zero-shot setting, while only Concord with few-shot mechanismachieves comparable performance to the Oracle baseline.This result demonstrates the necessity of thefew-shot mechanism. Comparison with given teammates ID.To investigate the influence of known teammates, we ad-ditionally test the performance of Concord (ID), which can be seen as Concord equipped with an Oracleidentification mechanism. As show in Tab. 4, we can see that Concord (ID) shows a slight advantage overConcord, but has no significant differences. This means that the identification accuracy of our identification",
  "mechanism is effective. Besides, the performance comparison between Concord (ID) and Oracle validatesthe effectiveness of other parts of Concord besides our identification module": "Ablation study for anti-forgetting mechanism.The anti-forgetting mechanism is also an importantcomponent of our method. To verify the effectiveness of this module, we design additional ablation experi-ments concerning this part. Specifically, we remove this anti-forgetting mechanism and conduct experimentson Overcooked environment, with the final results presented in Tab. 5. The results show that when theanti-forgetting mechanism is removed, our method suffers a significant performance drop, demonstrating theeffectiveness of the anti-forgetting mechanism. Sensitivity analysis of replay buffer size.To analyze the sensitivity of the replay buffer size in termsof addressing the forgetting issue, we add a sensitivity analysis in Tab. 6. It can be observed that Con-cord achieves relatively good performance across the ranges of buffer size for each teammate from 8 to 64.Note that Concord distinguishes itself from conventional replay-based methods to prevent any potentialmisunderstanding. In our approach, the replay buffer is solely employed to prevent the forgetting of team-mate recognition rather than to help learn solving previous tasks. An example of conventional replay-basedmethods in our experiments is CLEAR. Additionally, to further illustrate the role of teammate recognition, we conducted an experiment using randomIDs. Unlike Concord, which identifies teammates by inputting trajectories into a recognizer network, RandomID randomly selects one representation from all stored teammate representations during the testing phase.We can observe that the performance of Concord given the random ID is worse than standard Concord. Thisdemonstrates that our teammate recognition module can accurately identify teammates and leverage theirinformation to achieve better collaboration.",
  "ConcordAblationConcordAblationConcordAblationConcordAblation": "Episode Reward123.28 3.0185.25 10.23160.08 13.4550.12 8.76134.76 4.0181.13 11.94189.68 5.7190.99 10.87Forg.-1.31 4.23-20.15 2.52-16.11 6.85-90.45 18.96-1.27 6.69-60.28 2.71-6.83 4.59-100.08 20.31F. Transfer103.09 13.0278.84 11.79119.59 9.4159.89 7.65100.85 3.1468.87 5.82162.29 6.94122.24 17.52 : The sensitivity analysis of replay Buffer. The best mean results for Episode Reward are in bold.Here, Forg.represents the metric of Forgetting, F. Transfer represents the metric of Forward transfer,Random Rep. represents the identification mechanism with randomly generated representations, RandomID represents the identification mechanism that randomly selects one teammate representation from allpreviously stored teammate representations, and Oracle ID represents Concord with an oracle identificationmechanism.",
  "Episode RewardForg.F. TransferEpisode RewardForg.F. Transfer": "4109.97 0.98-3.67 5.6299.69 1.3181.97 7.32-64.62 6.01108.57 14.178117.72 1.97-7.78 1.85113.63 2.72128.61 13.87-51.54 13.05131.87 8.3116122.16 1.86-2.01 1.13114.81 1.72136.19 13.61-28.36 15.45143.57 8.4624126.51 1.940.61 4.21117.09 2.22165.94 3.64-2.73 8.24136.42 9.8332124.91 1.96-1.93 4.92113.57 2.56166.41 4.47-12.46 9.71131.01 5.0448126.27 1.45-0.27 0.65113.94 2.03167.75 13.96-5.79 16.45148.96 4.9264111.94 0.481.84 2.14104.21 2.08166.86 4.76-13.12 3.77123.81 16.62 Random Rep.36.92 6.94-1.58 21.6819.45 5.6433.27 8.15-20.36 14.8339.36 14.17Random ID87.63 9.58-1.27 1.1485.32 11.92110.61 9.98-5.42 0.9392.75 7.34Oracle ID126.34 2.861.76 2.6488.45 0.25169.27 6.39-3.36 3.12109.79 2.02 Sensitivity analysis of the size of Ai.The default configuration of the agents network is a 3-layerMultilayer Perceptron (MLP) with a hidden size of 64.We aim to assess the agents performance as afunction of the size of Ai. To do this, we modified the network structure to produce agents with varyingAisizes and then compared their performances. Concord (32) denotes an agent with a hidden layer sizeof 32, whereas Concord (4 layers) refers to a 4-layer MLP with each hidden layer sized at 64. Our findingsindicate that the parameters generated by our hyper-network are adequately expressive to accomplish thetask. A slight reduction in the number of parameters does not significantly degrade performance. In contrast,as we continue to increase the size of the network parameters, performance slightly declines. This may bedue to the larger parameter size requiring more data for training.",
  "C.3Discussions with Zero-Shot Coordination (ZSC) Methods": "In human-AI collaboration, the classic approach (i.e., Zero-Shot Coordination, ZSC (Strouse et al., 2021;Zhao et al., 2023; Yu et al., 2023)) is to pre-generate all the diverse teammates and allow the AI agent tosimultaneously train with them. In many real-world applications, humans do not collaborate with robots inthis way. We do not want an AI to stop updating after collaborating with humans. Instead, as more and morepeople collaborate with the agent, the robot should continue to learn and improve its collaboration skills.This is the desired capability for an agent in human-AI collaboration scenarios. Thus, the training approachof ZSC has several drawbacks, such as necessitating access to all of them during training is unfeasible andenumerating all potential teammates in advance is very hard, as we discussed in our paper. In the aforementioned realistic scenarios, we hope that after the agent collaborates with a human to completea task, it will retain the ability to collaborate with them and improve its ability to cooperate with otherteammates in the future. These two requirements correspond to the two important indicators in continuallearning: anti-forgetting and forward transfer.We frame this challenge within the context of continuallearning and seek solutions accordingly. The essence of ZSC is multi-task learning, which allows the AI agent to train with multiple teammatessimultaneously. This differs from our continual scenario where we believe that teammates arrive in sequenceand the agent can only train with the current team, not past ones. Besides, the focus of ZSC methods is toprovide a set of good partners for teammate training, while our work proposes a new problem formulation.We can also use ZSC techniques, as we did when constructing human-like teammates in our experiments. Here, we conduct additional experiments to enhance the comparison between our setup and ZSC, which isakin to multi-task learning. In our experimental design, teammates are introduced sequentially, and bothConcord and ZSC are allocated the same training budget.However, unlike Concord, ZSC is permittedto train with past teammates. When a new teammate (here we consider 2-player scenarios for simplicity)arrives, there should be a strategy to allocate the training timesteps between the new teammate and oldones. We propose two strategies, respectively Uniform and Binomial strategies. The Uniform strategyimplies that all teammates are allocated an equal number of training timesteps during the current trainingsession. The Binomial strategy indicates that the new teammate and all old teammates each receive halfof the timesteps. It is worth noting that if the new teammate uses all the timesteps without training with theold teammates, the ZSC method is indeed the Vanilla method we discussed in our paper. This comparisonmore clearly differentiates our setup from ZSC. Note that this setup is not fair to us, as we can not retrainwith past teammates.",
  "C.4Discussions with Meta RL Methods": "Our paper proposes a new formulation MACMDP, where the agents learn to cooperate with different team-mates continually. Another domain investigating the agents adaptability is meta RL. However, althoughcontinual RL and meta RL involves the changes of task distribution, but they are two completely differentproblem formulations. Continual RL emphasizes maintaining performance on both old and new tasks in achanging environment, while meta RL learns how to learn and focuses on improving the learning efficiencyon one new task. We stress that Concord follows continual RL setting, and our evaluation phase involvesonly testing with teammates that have already been encountered, rather than involving coordination withunseen teammates like in meta RL. Though our Concord follows continual RL, a setting different from meta RL, it shares certain similarities inpractice with context-based meta RL method, as both involve learning task representations. However, EWCand CLEAR do not have such recognization mechanism. To strengthen baseline, we introduce PEARL, aclassic context-based meta RL method, to our problem setting. We design two variants to adapt to our setting, PEARL and PEARL w/ Anti-Forg. PEARL, compared toVanilla, adds an encoder that processes trajectory data to model teammate representations. This encoder isoptimized using reinforcement learning losses and a standard normal distribution regularization, as describedin the original PEARL study. PEARL w/ Anti-Forg mitigates the challenge of forgetting in continual learningby utilizing a small buffer to store trajectories from previous training stages like us. When training with thecurrent teammate, it regularize the outputs of the current AI policy and encoder networks to be close to thoseof the last-round networks on the old data. This is a typical practice in regularization-based methods (Castroet al., 2018; Douillard et al., 2020). From the results, we can find that PEARL performs poor in our experiments, which reveals the significantdifferences between meta RL and our problem setting, where meta RL trains the policy on a set of tasksduring the meta-train phase, while PEARL needs to face the tasks one-by-one in our problem setting.Besides, though PEARL w/ Anti-Forg performs slightly better than PEARL, our Concord still exhibitssignificant superiority over it. The reason why PEARL w/ Anti-Forg does not achieve ideal results mightbe that to constrain the output of the policy network to be close to the old network often requires storingmore old data. Thus, when both storing 64 trajectories, our method shows better performance, reflectingthe advantage of employing the hyper-network.",
  "C.5Experiments with More Continual Teams": "An interesting questions is to investigate the performance when we have more teams to be continuallycoordinate. In this section, we double the teams to 24 to show that. For better presentation, we comparedVanilla and Concord and plotted the entire single coordination performance, which can be used to obtainall other metrics. The results can be found in . We found that, even though having more teammatesbrings significant challenges to continual learning, our Concord method still performs well and significantlyoutperforms Vanilla.",
  ": Single coordination performance of Vanilla (left) and Concord (right) on the Asym. Adv. layoutwith more partners. The value at i-th row and j-th in the matrix is Pi(j)": "PoorMediumExpert0.0 0.2 0.4 0.6 0.8 1.0 ConcordEWCCLEARVanillaMemory Decay PoorMediumExpert0.0 0.2 0.4 0.6 0.8 1.0 : Experimental results on SMAC maps 3m and 2z1s. The gray shaded bar is used to denote thedegree of forgetting, which equals to the highest test win rate during training minus the final test win rate. further validate the effectiveness of value decomposition when there exist multiple AI agents cooperatingwith multiple human-like teammates, we further conduct value decomposition study on different SMACmaps, of which the results are reported in Tab. 10-12. Similar to the results in Overcooked, Concord withvalue decomposition also achieves better performance on different SMAC maps, which further significantlydemonstrates the effectiveness of our value decomposition.",
  "C.7Additional Results on SMAC Environment": "In Sec. 4.2 of the manuscript, we compare Concord with other methods on the SMAC map 4m.Here,we further provide the experimental results on the other two SMAC maps in to demonstrate theconsistency of the conclusion across different maps. From the results, we can observe that the Concordmethod still exhibits the smallest gray shaded region in these two maps, indicating that generality of theanti-forgetting ability of our approach.",
  "Concord0.810.820.910.910.920.990.8933Concord w/o Human Critic0.750.750.890.890.950.980.8683": "introduction to the basic gameplay and experimental procedures.All volunteers were fully informed oftheir rights, and the experiments were approved by the relevant department. The Overcooked game wasremotely deployed on a server accessible through the volunteers browsers. Using this setup, we were ableto collect trajectories of human players interacting with each other, as shown in . We collect around160 human-human trajectories (for a total of 64k environment timesteps) and partition the these into twosubsets, splitting each trajectory into two single-agent trajectories. Based on the collected human data, weconstruct 8 sets of human proxy models through behavior cloning. Then we train different methods by usingthese human proxy models as the teammates that appear sequentially. Note that the AI agents obtainedby different methods will play with the real human participants, not the human proxy models.In ourpreliminary experiments, we observed that human models trained through behavior cloning outperformedthose trained with Generative Adversarial Imitation Learning (GAIL) (Ho & Ermon, 2016).Hence, wedecided to employ the former throughout our experiments.",
  "In terms of the experiments that has human participation, we claim that we have employed effective practiceto avoid possible ethical issues": "Test introductionBefore the experiments, we informed the volunteers of the specific details of the entireexperiments, including that they would be arranged to play the game with AI agents, and that their playingdata during test and the final test results would be utilized for the academic research of this work. All thecollected data is only for experiment purposes and we only make the testing results public in this paper.The volunteers voluntarily choose whether to participate in the experiment, and all participating volunteerswere informed of and agreed to our practice. Possible Risk for Humans and Broader ImpactThe main risks for the volunteers are twofold: thepotential leakage of their personal information and the time cost. Regarding the former, we only invitedthe volunteers to participate in our experiments without the requirement for any unnecessary personalinformation. Additionally, we will keep the volunteers identity information completely confidential. Asfor the latter, we first adjusted the frequency of the AI decision-making in the experiment to make the",
  "coordination process between humans and AI more comfortable. Besides, we provided appropriate materialcompensation for the volunteers participation in the experiments": "Furthermore, the experimental results are encouraging in the sense that we demonstrate Concord is a promis-ing method for dealing with Human-AI coordination in multi-agent systems via continual learning. It is notyet at the application stage and does not have a broader impact. However, this work learns one by onecoordination via continual learning, making Concord more practical in real-world applications.",
  "D.3Definition of the Human Preference Metric": "In Sec. 4.3 of the main paper, we validate our approach to conduct continual coordination with real hu-mans. To provide a more comprehensive comparison of our approach with other baselines, we present theexperimental results in two different dimensions. Among them, the main results are in the form of the singlecoordination performance, which is an objective metric. To further consider the humans subjective feelings,we additionally include one metric we refer to as Human preference in the main paper (Strouse et al.,2021; Yu et al., 2023). To calculate the human preference, some human participants have tasks as judges.Note the real human participants in our experiments we recruited have two tasks: Players. We first collect some trajectories of them, then construct human proxy models based onthese trajectories.Then we train different methods by using these human proxy models as thepartners that appear sequentially.When testing, the obtained agents by different methods willplay with the real human participants, not the human proxy modes. The final single coordinationperformance is shown in (a).",
  "Judges. We assign a different participant to each player as a judge, which will judge the preferencesin the game between trained agents and the corresponding human player": "In terms of the formal definition of this metric, the human preference for method A over method B can becalculated as follows. Let N be the total number of human players participating in the experiment, NA bethe number of human players who rank A over B, and NB be the number of those who rank B over A. Then,Human preference for method A over method B is computed as NA",
  "N NB": "N . The human players will first playwith the trained agents by different algorithms. The game replays are recorded for the following judgingprocess, where each algorithm is evaluated against with a different algorithm. There is 5 different algorithms(i.e., Oracle, Concord, EWC, CLEAR, and Vanilla), thus there is total 10 rounds of evaluations. In eachevaluation, human judges are asked to watch the replays of the two algorithms and vote for the algorithmthat is preferred. For example, let N = 8 be the number of human judges, A and B are the two methodscompared in a round, NA = 3 be the number of human players who rank A over B, and NB = 5 be thenumber of those who rank B over A. Then, Human preference for method A over method B is computed asNAN NB"
}