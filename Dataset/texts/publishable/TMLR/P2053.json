{
  "Abstract": "Graphs serve as generic tools to encode the underlying relational structure of data. Oftenthis graph is not given, and so the task of inferring it from nodal observations becomesimportant. Traditional approaches formulate a convex inverse problem with a smoothnesspromoting objective and rely on iterative methods to obtain a solution. In supervised settingswhere graph labels are available, one can unroll and truncate these iterations into a deepnetwork that is trained end-to-end. Such a network is parameter efficient and inheritsinductive bias from the optimization formulation, an appealing aspect for data constrainedsettings in, e.g., medicine, finance, and the natural sciences. But typically such settings careequally about uncertainty over edge predictions, not just point estimates. Here we introducenovel iterations with independently interpretable parameters, i.e., parameters whose values -independent of other parameters settings - proportionally influence characteristics of theestimated graph, such as edge sparsity. After unrolling these iterations, prior knowledge oversuch graph characteristics shape prior distributions over these independently interpretablenetwork parameters to yield a Bayesian neural network (BNN) capable of graph structurelearning (GSL) from smooth signal observations. Fast execution and parameter efficiencyallow for high-fidelity posterior approximation via Markov Chain Monte Carlo (MCMC) andthus uncertainty quantification on edge predictions. Informative priors unlock modeling toolsfrom Bayesian statistics like prior predictive checks. Synthetic and real data experimentscorroborate this models ability to provide well-calibrated estimates of uncertainty, in testcases that include unveiling economic sector modular structure from S&P500 data andrecovering pairwise digit similarities from MNIST images. Overall, this framework enablesGSL in modest-scale applications where uncertainty on the data structure is paramount.",
  "Introduction": "Graphs serve as a foundational paradigm in machine learning, data science, and network science for modelingcomplex systems, capturing intricate relationships in data that range from social networks to gene interactions;see e.g., (Kolaczyk, 2009; Hamilton, 2020). In computational biology, accurate graph structures can offerinsights into gene regulatory pathways, enhancing our ability to treat diseases at the genetic level (Cai et al.,2013). In finance, the ability to recover precise financial networks can be useful for risk assessment and marketstability (Marti et al., 2021). In geometric deep learning (Bronstein et al., 2017), the lack of observed graphstructure underlying data often limits the use of efficient learning models such as graph neural networks(GNNs); see e.g., (Cosmo et al., 2020). Yet, despite their importance, existing methodologies for graphstructure learning (GSL) from multivariate nodal data face significant limitations.",
  "Published in Transactions on Machine Learning Research (09/2024)": "introduce an optimization algorithm that is simpler than existing methods, without sacrificing estimationperformance. We unroll this algorithm producing the first true NN for GSL from smooth signals. We leverageindependent interpretability to incorporate prior information about sparsity characteristics of sought graphsinto prior distributions over unrolled network parameters, producing the first BNN for GSL from smoothsignal observations. We lean into the advantages of unrollings - low parameter dimensionality, fast compilerand auto-grad friendly layers, as well as the empirical need for shallow networks - to perform posteriorapproximation using MCMC sampling. Doing so yields high-quality and well-calibrated uncertainty estimatesover edge predictions, as demonstrated via comprehensive experiments with synthetic and real datasets. The primary limitation of our approach lies in its scalability to moderate- and large-sized graphs and datasets.The computational and memory requirements to achieve asymptotically exact posterior inference throughMCMC sampling are substantial, owing to the necessity for numerous forward network passes per posteriorsample and the requirement to store the entire data set in memory. Promising directions of future workinclude exploration of non-asymptotically exact posterior inference methods, namely variational inference,which typically requires much less computation and allows for mini-batch training and thus use of largerdatasets (Jospin et al., 2022). As we produce distributions over output graphs, future work can exploreBayesian decision analysis with the introduction of a utility function, or even using DPG as a module withina larger system, where propagation of uncertainty over the inferred graph is important.",
  "Towards interpretability and uncertainty quantification: Desiderata and contributions": "In composite inverse problems with multiple regularizers, understanding how each regularization parametervalues affect desirable solution traits (e.g., image sharpness or number of graph edges) may be complex andoften beyond the modelers knowledge. This obscurity necessitates a naiveand consequently costlymulti-dimensioanl grid search for suitable parameter values; see e.g., the numerical study in (Dong et al., 2016,Section V-B). When the relationship between the regularization parameters and a solution characteristic isclear, we say the parameters are interpretable with respect to (w.r.t.) the output characteristic. When anoutput characteristic is influenced by a single parameter, independent of all others, we call this parameterindependently interpretable w.r.t. the output characteristic. Such instances make interpretability actionable,namely they allow prior knowledge on the solution characteristic to be incorporated onto the value of theindependently interpretable parameter. A key contribution of this work is: i) in recognizing optimizationproblems with independently interpretable parameters as ripe for the adoption of unrollings, which inheritthis interpretability because they approximate inverse problem solutions; and ii) in leveraging the Bayesianframework to seamlessly incorporate such prior knowledge into the parameters of the unrolled network. An issue unrollings often face (within GSL and beyond) is their tendency to produce layers with pre-activation outputs that are nonlinear functions of the parameters; e.g., parameter products and parameters indenominators (Monga et al., 2021). This prevents the deep network from being a true neural network (NN),i.e., a function composed of layers, where each layer consists of affine transformations of data or intermediateactivations followed by non-linear functions applied pointwise (Bishop, 1995). To address this issue, networkdesigners may opt for reparameterization, but at the expense of parameter interpretability and often degradedempirical performance (Monga et al., 2021; Shrivastava et al., 2020). Unrollings can be impeded by nuisanceparameters - parameters not of direct interest, but which must be accounted for - like a step-size. Nuisanceparameters are typically a vestige of the chosen optimization algorithm rather than being intrinsic to theproblem formulation. Nuissance parameters can also undermine training stability (see Appendix A.1.1) andsince they lack a clear connection to specific solution characteristics they hinder informative prior modeling.The preceding discussion motivates the need for innovative GSL techniques to produce true NN unrollingsamenable to incorporation of prior knowledge on their parameters.",
  "In , we develop a novel optimization algorithm for GSL from smooth signals (Algorithm 1),which is step-size free and parameterized to yield independent interpretability w.r.t. edge sparsity": "In , we unroll Algorithm 1 to produce the first strict unrolling for GSL from smooth signals,which results in a true NN. This is to be contrasted with existing supervised-learning approaches toGSL, where GLAD (Shrivastava et al., 2020) and Unrolled PDS (Pu et al., 2021) are not true NNs,and GDN (Wasserman et al., 2023) is not a strict unrolling because it resorts to gradient truncationand reparameterization. The proposed unrolled NN is used to define our BNN, dubbed DPG sinceAlgorithm 1 is a dual-based proximal gradient method (Beck & Teboulle, 2014). In , we introduce a methodology to integrate prior knowledge into BNN prior distributions,specifically for networks derived from unrolled optimization algorithms for inverse problems withindependent interpretability. We show this approach unlocks classical Bayesian modeling tools likepredictive checking, which we fruitfully apply to DPG. High-fidelity parameter posterior inferencevia Hamiltonian Monte Carlo (HMC) sampling enables the first instance of a model for GSL fromsmooth signals, capable of producing estimates of uncertainty over edge predictions. In , we validate DPGs ability to produce high-quality and well-calibrated uncertaintyestimates from synthetic data, stock price time series (S&P500), as well as graphs learnt from imagesof MNIST digits. The reliability of these estimates is underscored by notable Pearson correlationsbetween predictive uncertainty and error: 0.70 for the stock data and 0.62 for the MNIST digits.Additional experimental evidence is provided in the appendices.",
  "Related Work": "A recent body of work addresses the GSL task via algorithm unrollings under varying data assumptions.Noteworthy contributions include GLAD (Shrivastava et al., 2020), which unrolls alternating-minimizationiterations for Gaussian graphical model selection, Unrolled PDS (Pu et al., 2021) which unrolls primal-dualsplitting (PDS) iterations for the GSL problem from smooth signals, and GDN (Wasserman et al., 2023)which unrolls linearized proximal-gradient iterations for a network deconvolution task that posits a polynomialrelationship between the observed pairwise vertex distance matrix and the latent graph. Here we deal with theGSL problem from observations of smooth signals as in (Pu et al., 2021), but the optimization algorithm weunroll is different (cf. DPG in Algorithm 1 versus PDS), more compact and devoid of step-sizes. Additionally,none of the previous GSL unrollings result in true NNs, provide estimates of uncertainty on their adjacencymatrix predictions, nor leverage the interpretability of network parameters in the modeling process. Building a probabilistic model to connect observed network data to latent graph structure has a long historyin the computer and social network analysis communities; see e.g., (Coates et al., 2002; Butts, 2003; Kolaczyk,2009). Gibbs sampling was used mostly as a means to efficiently explore the resulting network posterior ratherthan quantify the uncertainty the model placed on particular edges. Since then, Bayesian methods havebeen used with alternate network models (e.g., directed acyclic graphs specifying the structure of Bayesiannetworks), types of observed data (e.g., information cascades and protein-protein interactions), and posteriorapproximation approaches; see (Shaghaghian & Coates, 2016; Gray et al., 2020; Jiang & Kolaczyk, 2011;Williamson, 2016; Pal & Coates, 2019; Deleu et al., 2022). Such approaches are typically transductive - tyingthemselves to a single training graph - and require expensive joint inference of the model and the latentgraph structure. Our approach only requires inference of the BNN model parameters and thus is naturallyinductive, i.e., able to generalize to new nodes, or entirely new graphs. Some recent works build on suchgraph distribution modeling approaches in an approximate Bayesian manner, to incorporate the uncertaintyin an observed graph for downstream tasks with GNNs (Zhang et al., 2019; Pal et al., 2020). Others forgomodeling the distribution of the observed graph but take approximate Bayesian approaches to modeling withGNNs. For instance, (Opolka & Li, 2022) uses a deep graph convolutional Gaussian process with variationalposterior approximation for link prediction, and (Sevilla & Segarra, 2023) pre-trains a score-matching GNNfor use in annealed Langevin diffusion to draw approximate samples from the network posterior. All suchBayesian GNN approaches require (partial) observation of graph structure, and rely on approximate inferencemethods due to large dimensionality. Our DPG approach uses no observed graph structure (except for graphlabels during training) and allows for high-fidelity posterior approximation. To the best of our knowledge,BNNs have so far not been used for GSL with uncertainty quantification. More broadly, unrolling-inspired Bayesian deep networks have recently found success in uncertainty quan-tification for computational imaging (Barbano et al., 2020; Zhang et al., 2021; Ekmekci & Cetin, 2022).The inductive bias provided by the original iterations lead to gains in data efficiency, but still have limitedparameter interpretability and high dimensionality leading to naive priors and coarse posterior approximation.This exciting line of work inspired some crucial ideas in this paper, cross-pollinating benefits to GSL andwith the added value of overcoming the aforementioned Bayesian modeling and inference challenges.",
  "Model-based Formulation and Optimization Preliminaries": "Let G(V, E, A) be an undirected graph, where V = {1, . . . N} are the vertices (or nodes), E V V are theedges, and A RNN+is the symmetric adjacency matrix collecting the non-negative edge weights. For(i, j) / E we have Aij = 0. We exclude the possibility of self loops, so that A is hollow meaning Aii = 0, for alli V. For the special case of unweighted graphs that will be prominent in our models, then A {0, 1}NN. In this paper, we consider that A is unknown and we want to estimate the latent graph structure from nodalmeasurements only1. To this end, we acquire graph signal observations x = [x1, . . . , xN] RN, where xidenotes the signal value (i.e., a nodal attribute or feature) at vertex i V. When P such signals are availablewe construct matrix X = [x1, . . . , xP ] RNP , where each row xi RP , i = 1, . . . , N, of X represents a 1This is different to the link prediction task, where one is given measurements of edge status for a training subset of nodepairs (plus, optionally, node attributes), and the transductive goal is to predict test links from the same graph(Kolaczyk, 2009)",
  "Graph structure learning from smooth signals": "Given X assumed to be smooth on G, a popular model-based GSL approach is to minimize the Dirichletenergy in (1) w.r.t. A; see e.g., (Hu et al., 2013; Dong et al., 2016; Kalofolias, 2016; Kalofolias & Perraudin,2017). The inverse problems posed in these works can be unified under the general composite formulation",
  "A = arg minAA{A E1,1 + h(A)} ,(2)": "where the feasible set is A := {A RNN : diag(A) = 0, Aij = Aji 0, i, j V}, i.e., hollow, symmetric,non-negative matrices. The regularization term h(A) typically promotes desired structure on the estimatededge set (e.g., sparsity, no isolated nodes) and can be used to avoid the trivial solution A = 0. We henceforthuse h(A) = 1log(A1) +",
  "A2F (, 0 are regularization parameters), which excludes the possibilityof isolated nodes and has achieved state-of-the-art results (Kalofolias, 2016)": "It is convenient to reformulate (2) in an unconstrained, yet equivalent form. We start by compactly representingvariable A and data matrix E with their vectorized upper triangular parts a, e RN(N1)/2+, implicitlyenforcing symmetry and hollowness, while also halving the problem dimension. To enforce non-negativity theindicator function I{a 0} = {0 if a 0 else } is included in the objective. Finally, we substitute thenodal degrees d = A1 with the vectorized equivalent d = Sa, where S {0, 1}NN(N1)/2 is a fixed binarymatrix that maps vectorized edge weights to degrees. The resulting optimization problem is given by",
  "a22 + I{a 0},(3)": "which is convex and admits a unique optimal solution; see e.g., (Saboksayr & Mateos, 2021). Next, wecomment on the role of the regularization parameters , and their interpretability properties. We then offera brief discussion on optimization algorithms to tackle problem (3). These ingredients will be essential tobuild a BNN model for supervised GSL in Sections 4 and 5.",
  "Independent interpretability of regularization parameters. Definition 1 formalizes the notion ofindependent interpretability of regularization parameters in inverse problems such as (3)": "Definition 1. Let x(1, . . . , n) X be the solution to an inverse problem that depends on regularizationparameters 1, . . . , n. Consider some scalar function of the solution f : X R. In general, the value f(x)depends on all 1, . . . , n. When f(x) depends solely on a single regularization parameter i, then we saythat i is independently interpretable w.r.t. f(x). The weights and are not independently interpretable w.r.t. to relevant graph characteristics, frustratingstraightforward interpretation of their effect on the solution a(e, , ). Specifically, for fixed , increasing leads to denser edge patterns, as we have (quadratically) increased the relative cost of large edge weights.Indeed, the sparsest graph is obtained for = 0. But in general, many interesting graph characteristics, e.g.,sparsity, connectivity, diameter, and edge weight magnitude, are non-trivial functions of both and ; seealso (Dong et al., 2016) for a similar issue.",
  "We can map from the former parameterization to the latter by first scaling e by = 1/, solving (3) withe using = = 1, and finally scaling the recovered edges by the constant =": "/; we refer the readerto (Kalofolias, 2016; Kalofolias & Perraudin, 2017) for a proof of the equivalence claim. Due to the separablestructure of the right-hand-side of (4), any GSL algorithm would require a single input parameter , and theobtained solution a(e, 1, 1) can then be scaled by > 0. All in all, the sparsity level of a is determinedsolely by , making independently interpretable w.r.t. sparsity. Indeed, this satisfies Definition 1 with theidentifications x(1, . . . , n) a(e, 1, 1), X RN(N1)/2+, i , and f(x) a(e, 1, 1)0, where 0 counts the number of non-zero elements of its vector argument. Moreover, is interpretable w.r.t. edgeweight magnitude, but not independently so, as larger produces smaller weights [see (bottom-left)].",
  "Optimization algorithms": "Problem (3) has a favorable structure that has been well documented, and several efficient optimizationalgorithms were proposed to obtain a solution a(e, , ) with O(N 2) complexity per iteration. Specifically, aforward-backward-forward PDS algorithm was first proposed in (Kalofolias, 2016). PDS introduces a step-sizeparameter which must be tuned to yield satisfactory empirical convergence properties, thus increasing theoverall computational burden. We find that effective step-size values tend to lie on a narrow interval beyondwhich PDS exhibits divergent behavior, further frustrating tuning; see Appendix A.1.1 for a supportingdiscussion. GSL algorithms based on the alternating-directions method of multipliers (Wang et al., 2021)or majorization-minimization (Fatima et al., 2022) have been developed as well. Recently, (Saboksayr &Mateos, 2021) introduced a fast dual proximal gradient (FDPG) algorithm to solve (3), which is devoid ofstep-size parameters and different from all three previous approaches it comes with global convergencerate guarantees. For this problem, the strongest convergence results to date are in (Wang et al., 2023). Our starting point in this work is the FDPG optimization framework, but different from (Saboksayr &Mateos, 2021) we: (i) develop a solver for the (, )-parameterization of (3); and (ii) turn-off the Nesterov-typeacceleration from the proximal-gradient iterations used to solve the dual problem of (3). This yields a dualproximal gradient (DPG) method, tabulated under Algorithm 1. In a nutshell, during iterations k = 1, 2, . . .Algorithm 1 updates the vectorized adjacency matrix estimate ak RN(N1)/2+, an auxiliary vector of nodaldegrees dk RN+, as well as dual variables k RN used to enforce the variable splitting constraint d = Sa. Anaive DPG implementation incurs O(N 2) computational and memory complexities, and we note all nonlinearoperations involved (i.e., ReLU() = max(0, ), ()2, and ()) are pointwise on their vector arguments.As a result of the design choices (i)-(ii), the DPG algorithm requires the fewest operations per iterationand the fewest number of parameters among existing solvers of (3), and is devoid of any uninterpretablenuisance parameters, e.g., step-sizes. FDPG was only considered on the original (, )-parameterization of(3); by instead opting for DPG iterations to solve the (, )-parameterization of (3), we reveal independentinterpretability of w.r.t. sparsity of the optimal graphs. Next, we will unroll Algorithm 1 to produce a GSL NN which inherits its advantages - namely simple, efficient,minimally parameterized layers, with independent interpretability - forming the backbone of our BNN.",
  "Graph Structure Learning from Smooth Signals with Bayesian Neural Networks": "So far we have described a model-based approach to (point) estimation of graphs from smooth signals. Inthis work, we assume a labeled training dataset is available. We aim to construct a BNN model to produceuncertainty estimates on graph predictions for unseen test data. Our BNN approach for GSL in a nutshell.Here, we restrict ourselves to binary graphs a {0, 1}N(N1)/2; weighted graphs only require a change to the ensuing likelihood function.We denoteall training data as T = {Te, Ta} = {e(t), a(t)}Tt=1, an unseen test sample as (e, a), and the collection of",
  "m=1p(a | e, (m)),(5)": "where we use M Monte Carlo draws from the posterior (m) p( | T ) to approximate the intractableintegral. We can then designate our edge-wise point and uncertainty estimates as the first two moments ofthe posterior predictive marginals p(ai | e, T ), respectively, for each candidate edge indexed by i V V. Roadmap. In .1, we first discuss the development of a GSL NN which takes in the vectorizedEuclidean distance matrix e of nodal observations X and assigns a probability to all possible edges. We doso by unrolling Algorithm 1, producing an GSL NN with an independently interpretable parameter w.r.t.sparsity of graph outputs. Treating the model parameters of this unrolled NN as stochastic, choosing alikelihood function (.2), and setting appropriate priors (), we produce a BNN. For inferencewe use HMC (.3), an unusual choice in BNNs made viable by the low dimensionality and fastexecution of our NN, stemming from its origins as an unrolling. Pushing the test inputs through each suchGSL NN and averaging the resulting predictive distributions provides provides the approximate (5) posteriorpredictive, from which we derive edge-wise point and uncertainty estimates as elaborated in .4.",
  "Algorithm unrolling: Iterative optimization as a neural network blueprint": "Unrollings (also known as deep unfoldings) use iterative algorithms as templates of neural architecturesthat are trained to approximate solutions of optimization problems. Iterations are truncated and mappedto NN layers, while optimization (e.g., regularization and step-size) parameters are turned into learnableweights. Originating from convergent iterative procedures, unrolled NNs inherit many desirable properties;namely, a low number of parameters, fast layer-wise execution, and favorable generalization in low-dataenvironments (Monga et al., 2021). Typical architectural design choices include whether to replace intermediateoperators with more expressive (possibly pre-trained) learnable modules - making them no longer strictunrollings - and whether a common set of parameters should be used in all layers, or, to decouple theseparameters across layers. Both decisions are context dependent and often driven by the amount of dataavailable as well as complexity considerations. Deviating from strict unrollings with shared parameters cannaturally lead to gains in expressive power, but often at the cost of inductive bias and training stability. Unrolling DPG iterations. Pioneered by (Gregor & LeCun, 2010) to efficiently learn sparse codes fromdata, algorithm unrolling ideas are recently gaining traction for GSL as well. Starting from the formulationin .1, (Pu et al., 2021) unrolls a PDS solver of the (, )-parameterization (3). Unrolled PDS has noindependently interpretable parameters, is not a true NN (pre-activation outputs are nonlinear functions ofthe parameters), and includes a nuisance step-size parameter arguably a shortcoming as gradient-based",
  "p = (D (e) b1) (0, 1)N(N1)/2,(6)": "with parameters = {, , b}. To see that Unrolled DPG is a true NN, note that is only involved in a linearfunction e of the input data. Likewise, and b are only involved in an affine mapping of the activationsD (e). All non-linear operations (specifically squaring, square root, and max) are pointwise functions ofintermediate activations. Going back to the design considerations mentioned at the beginning of this section,here we keep the unrolling strict and share parameters across layers to retain independent interpretabilityof , minimize parameter count, and simplify upcoming Bayesian inference. Tradeoffs arising with modelexpansion using multiple input and output channels per layer are discussed in .1. All in all, unrolled DPG is the first true NN for GSL from smooth signals, and the first strict unrolling forGSL which produces a true NN. For the various reasons laid out in the preceding discussion, Unrolled DPGis of independent interest as a new model for point estimation of graph structure in a supervised setting. Aswe show next, it will be an integral component of the stochastic model used to construct a BNN to facilitateuncertainty quantification for adjacency matrix predictions.",
  "Stochastic model": "Here we specify a stochastic model for the random variables of interest, namely the binary adjacency matrixa, nodal data entering via the Euclidean distance matrix e, and the BNN weights . The Bayesian posteriorfrom which we wish to sample satisfies p( | T ) p(Ta | Te, )p(), and a first step in BNN design is tospecify the likelihood p(a | e, ) and the prior p(). An i.i.d. assumption on the training data T allows thelikelihood to factorize over samples p(Ta | Te, ) = Tt=1 p(a(t) | e(t), ). Moreover, assuming edges within agraph sample a(t) are mutually conditionally independent given parameters leads to further likelihoodfactorization as p(Ta | Te, ) = Tt=1N(N1)/2i=1p(a(t)i| e(t), ). We model ai | e, Bernoulli(pi), wherethe success (or edge i V V presence) probability is given by the output of the Unrolled DPG network, i.e.,",
  "Inference": "We aim to generate M Monte Carlo draws from the posterior (m) p( | T ) p(Ta | Te, )p(). Inmodern BNNs based on overparameterized deep networks, high dimensionality and parameter unidentifiabilityare prevalent. This translates to highly multi-modal posteriors which make sampling challenging (Izmailovet al., 2021). Even when posterior geometries are more benign, the computational and memory requirementsfor evaluating the network and its gradients render the generation of Monte Carlo posterior samples impractical.As a typical workaround one can resort to posterior approximation using inexpensive mini-batch methodssuch as mean-field variational inference or stochastic-gradient MCMC (Izmailov et al., 2021). In contrast, we",
  "now argue that generation of high-fidelity Monte Carlo posterior samples using HMC is uniquely compatiblewith our proposed BNN; for implementation details see .1": "For starters, Unrolled DPGs repetitive layers simplify model construction and allow straightforward com-pilation in automatic differentiation software, significantly boosting runtime efficiency. The low parametercount of Unrolled DPG facilitates the use of forward-mode auto-differentiation; this eliminates the need tostore intermediate activations and perform a backward pass, thus it ensures memory usage does not increasewith model depth D. Moreover, as Unrolled DPG can effectively approximate inverse problem solutions withrelatively shallow depths (see ), selecting a smaller D significantly reduces runtime complexity. Forsmall- to moderately-sized graphs and datasets, our computational and memory requirements for networkand gradient evaluation are low; see for GSL problem instances where inference is attainable in< 2 minutes on a M2 MacBook laptop. The limited parameter count also aids in sampling efficiency as weavoid the curse of dimensionality that complicates sampling in higher-dimensional models. Finally, the abilityto place informative priors over independently interpretable parameters (as we do in ) is knownto smoothen the posterior geometry, especially in data scarce regimes where the likelihood and prior are ofcomparable magnitude (Gelman et al., 1995).",
  "Prediction": "By conditioning on data T and integrating out model parameters , we obtain a predictive distributionon unseen graph adjacency matrices a given nodal signals e. As the integral is intractable, we use theM posterior samples obtained via HMC in the inference stage to approximate the predictive distributionp(a | e, T ). This approximation process is summarized in (5). . Importantly, we can generate samples from the posterior predictive by randomly drawing from the samplingdistribution with each parameter sample plugged in, i.e., a(m) p(a | e, (m)). Given the form of our BNNsstochastic model as introduced in .2, such a draw reduces to sampling a Bernoulli distribution foreach possible edge. Randomly drawing from the sampling distribution is critical as it accounts for both formsof uncertainty in posterior predictive quantities, namely, sampling uncertainty and estimation uncertainty(Gelman et al., 1995). Using these posterior predictive samples, we can approximate the mean (pred. mean)and standard deviation (pred. stdv.) of the edge-wise marginals of the posterior predictive as",
  "Bayesian Modeling of Unrolling-Based BNNs with Independent Interpretability": "In this section, we present a method for Bayesian modeling for BNNs which use a network produced byunrolling an optimization algorithm to solve an inverse problem with independent interpretability. Weinstantiate these ideas on the GSL problem (3) - where solutions are undirected graphs - and use priorknowledge over the sparsity of such graphs to shape prior distributions over the corresponding independentlyinterpretable parameter in Unrolled DPG introduced in .1.",
  "Prior modeling": "For Bayesian models, priors over unobserved quantities play two main roles: encoding information germaneto the problem being analyzed and aiding the ensuing Bayesian inferences. Despite the name, a prior can ingeneral only be interpreted in the context of the likelihood with which it will be paired. There are manytypes of priors, and we here we list the common ones in order of degree in which it is intended to affect theinformation in the likelihood: non-informative, reference, structural, regularizing, weakly informative, andstrongly informative (Gelman et al., 2017). The more domain specific information present, the further to the",
  "), denotedRG 1": "3 , and their corresponding analytic Euclidean distance matrices as inputs defined in . SeeAppendix A.4 for similar analysis on other random graph distributions. Prior modeling and upcoming priorpredictive checks require only the distance matrices e, and we use only 5 such distance matrices from T . Suppose we have prior beliefs on the sparsity related characteristics of recovered graphs, namely sparsity itself(= 1 edge density) or the number of connected components. For example, suppose we believe recoveredgraphs should have edge densities around [.05, .5] and 5 connected components. Since Unrolled DPGapproximates solutions to (3) - and determines such sparsity related characteristics independently of otheroptimization parameters - we can simply run Algorithm 1 to convergence using the 5 inputs on a set ofdiscretized values across several orders of magnitude, as demonstrated in (top-left). We observe produces solutions with sparsity characteristics consistent with our prior beliefs. Choosing",
  "Predictive checking": "In Bayesian statistics, predictive checks evaluate a models efficacy in capturing data characteristics likemeans, standard deviations, and quantiles (Gelman et al., 1995). Predictive checks leverage the existence of amodel of the joint distribution p(T , ) between data and parameters. This model allows us to draw samplesfrom the data marginal - called replicated data - by drawing samples from the joint and simply dropping theparameter samples. Prior predictive checks draw replicated data from the joint before conditioning on dataT , while posterior predictive checks do so after. In both cases, we apply the chosen statistic capturing ourdata characteristic of interest to the replicated data, producing a histogram. The goal in prior predictivechecking is to shape priors that encompass all plausible data sets, while still guiding the model towards datasets we deem more likely apriori. Thus prior predictive checking often consists of visual inspection of thehistogram to ensure this holds, and adjusting the location and/or scale of prior distributions until it does.The goal in posterior predictive checking is to subjectively validate model fit; here we compare the statistic",
  "on the replicated data (a histogram) to the same statistic applied to the actual data T (a single scalar). Awell-fit model should have a histogram tightly concentrated around the real data statistic": "In typical BNNs, the lack of parameter interpretability prevents effective use of predictive checking. (Wenzelet al., 2020) highlights this issue by demonstrating that an isotropic Gaussian prior on NN weights (ResNet-20)fails to generate data consistent with prior expectations in image classification tasks. But the lack of parameterinterpretability prevents the use of these findings to modify the prior for improved alignment. Below, weinstantiate predictive checking for DPG, with average edge density as our test statistic. Example (Prior predictive check over DPG parameters). To perform a prior predictive check weuse the 5 inputs from Te and draw 104 replicated data sets arep. (right) shows the histogram ofaverage edge densities of replicated data sets under the prior settings laid out in .1, which we callthe original prior. We observe that such a prior succeeds in producing data sets that coincide with thosewe deem more likely apriori, but falls short of encompassing all plausible data sets, namely those with edgedensities of 0.9. To address this, we lean on independent interpretability of w.r.t. the sparsity of graphsolutions: decreasing the location of p() will increase edge densities (lower sparsity) of graph solutions,and so we lower p()s location parameter from 0 to 1 2, with all other parameters of the prior distributionremaining the same. We call this new prior the altered prior. Repeating the prior predictive check with thisaltered prior we now observe 12% of replicated data sets with edge densities in [.75, 1], as opposed to 4%before, and we confirm through visual inspection that data sets with all possible edge densities are indeedproduced. The altered prior thus encompasses all plausible data sets while still preferentially generatingdata sets we feel are more likely apriori.",
  "Example (Posterior predictive check over DPG parameters). Now, we repeat this procedure, butdraw replicated data sets from the joint after conditioning on T . We perform inference drawing M = 104": "posterior samples; for each posterior sample we draw a single replicated data set. In (right) we plotthe histogram of the average edge densities of these replicated data sets, denoted posterior, and compareagainst the average edge density of the graph labels in T , denoted labels. Because all outcomes are tightlydistributed around the mean edge density of the real data, we can have confidence the model parametershave fit appropriately.",
  "Experiments": "We have introduced DPG, the first BNN for GSL from smooth signals, which is capable of providing estimatesof uncertainty of its edge predictions. We showed DPG can effectively incorporate prior information into theprior distribution over its parameters. In this section, we evaluate DPG across synthetic and real datasets,and introduce other baseline models for comparison, including a more expressive variant of DPG.",
  "Models, metrics, and experimental details": "Strict unrollings: DPG and PDS. In the following experiments, the prior used for the DPGs parameters{, , b} is the altered prior as developed in : Lognormal(1/2, 4), Lognormal(2, 2), andb Lognormal(1, 2). We similarly refer to the BNN with Unrolled PDS as its NN model - and no change tothe stochastic model - as PDS. As none of PDSs parameters {, , , b} have independent interpretability,we cannot use the prior modeling techniques outlined in . Attempting HMC on PDS producessignificant number of divergent simulated Hamiltonian trajectories, a phenomenon known to be caused byhigh posterior curvature (Betancourt, 2016). This makes HMC run slowly, produce poorly mixed chains,and ultimately non-performant PDS models. Indeed, we find that values of step-size which producedivergent behavior (very low likelihood) are close those which are most performant (very high likelihood),indicating high-curvature in the likelihood function, and thus the posterior; see (left). Because is not interpretable, it is unclear how to shape p() to reduce posterior curvature without first running adiscrete search. Indeed, to find a performant PDS model amenable to efficient HMC, we first run such adiscrete search, fix to a found performant value of 0.1, and then set priors , LogNormal(0, 10) andb N(0, 103). Both model-based algorithms in (Wang et al., 2023) and FDPG Saboksayr & Mateos (2021)come with faster convergence rate guarantees than DPG in Algorithm 1 - a characteristic found not to be",
  "through a sigmoid: 1": "CCj=1 j20j (aD[:, j])) b1. We can now run inference on these stochastic headparameters {1, 1, . . . , C, C, b} keeping MAP fixed. We denote this model as DPG-MIMO-E. All MIMOmodels use C = 4: C = 8 offered negligible performance gains and posed (partially stochastic) inferencechallenges, while C = 2 was less performant than C = 4. Metrics. To provide summaries of our models predictive accuracy and quality of uncertainty we use twoproper scoring rules: the Negative Log-Likelihood (NLL) p(a | e, T ) and the Brier Score1|E|E|T [p(a |e, ) a]2). Beyond proper scoring rules, we use Expected Calibration Error (ECE) (Guo et al., 2017),which measures the correspondence between predicted probabilities and empirical accuracy, and Error, definedas the percentage disagreement between a thresholded pred. mean E|T [a | e, T ] > 0.5 and the actual labela. See Appendix A.2 for full details. Hyperparameters and inference details. Utilizing NumPyros NUTS implementation of HMC (Phanet al., 2019), our experiments run 4 chains in parallel, each chain taking 500 warm-up steps before generating1000 samples, accumulating M = 4000 total samples. We use depth D = 200 for all models unless otherwisespecified, as we did not observe significant improvements in predictive performance beyond this depth. Wechoose a0 = 1",
  "Synthetic data evaluation": "Generative smooth signal model. We build on (Dong et al., 2016)s work on probabilistic smooth graphsignal generation to validate our methods using synthetic data. Let L = UU be the eigendecomposition ofthe graph Laplacian L = diag(A1) A, with diagonal eigenvalue matrix having associated eigenvalues isorted in increasing order, ui (i-th column of U) is the eigenvector of L with eigenvalue i. Further take L to be the Moore-Penrose pseudoinverse of L, with eigenvalues i := 1iwhen i > 0, and i := 0 otherwise.(Dong et al., 2016) proposed that smooth signals can be generated from a colored Gaussian distributionas x = +",
  "graphs. Error bars are scaled by .05 for compact visual effect": ": Qualitative i.i.d. generalization. Left: For a random test sample we show the label a, and estimatedmean (pred. mean) and standard deviation (pred. stdv) of the edge-wise marginal posterior predictivep( ai | e, T ). Comparing pred. mean to the label a adds qualitative evidence that the model is well fit to thedata. Right: The edge-wise uncertainty estimate (pred. stdv.) and error |ai E|T [ai|e, T ]| have a strongpositive Pearson correlation = 0.91.",
  "Lx0. When = 0, this procedure produces signals such that the power onthe i-th frequency component is x2i (k = 1": "2, = 2i). Thus the expected power E[x2i ] = i is inverselyproportional to the associated eigenvalue, concentrating energy on the low frequencies of the Laplacianspectrum. We construct the data matrix X = [x1, . . . , xP ] RNP , where xp are drawn i.i.d. from N(0, L)as described above. Recalling the definition of the Euclidean distance matrix E in and using thefact that xi xi (k = P 2 , = 2Lii) and E[xi xj] = Pp=1 E[xipxjp] = Pp=1 Cov(xip, xjp) = Lij, (i, j), wecan show E[E] = 1diag(L) + diag(L)1 2L which we call the analytic distance matrix. We henceforthuse E[E] and its finite sample approximations from P signals for evaluation.",
  "Evaluation of i.i.d. generalization. Here, we train and test DPG, PDS, and DPG-MIMO-E on N = 20RG 1": "3 graphs and their corresponding analytic distance matrices. We use T = 50 graphs for training and100 for testing. We present the results in . DPG and PDS perform comparably; although DPGhas marginally better accuracy and calibration. Significant improvement is found in the more expressiveDPG-MIMO-E across all metrics, showing our simple 3-parameter model can be effectively expanded. Thesemodels are similarly performant across graph distributions (see the experiments reported in and alsoin Appendix A.2); we show a single graph ensemble here for brevity. depicts estimates of the first two moments of the posterior predictive distributions produced by DPGon a random test sample. We observe well-calibrated confidence and uncertainty, plus a strong correlationbetween error magnitude and uncertainty; indeed a useful uncertainty estimate should be a proxy for error.",
  ": Detection of covariate shift across models. Using RG 1": "3 graphs with N = 20 nodes, we fit models onanalytic Euclidean distance matrices and evaluate on corrupted distance matrices in a test set. Fewer signalsP used tends to increase corruption magnitude. Log-scaling unintentionally makes error bars appear to growwith P. Predictive uncertainty under distribution shift. We evaluate two forms of distribution shift: corruptionsto the noiseless analytic distance matrix E[E] and label mismatch. To investigate corruptions, we trainusing T = 50 RG 1",
  "(N = 20) labels and their analytic distance matrix, and test on 100 RG 1": "3 samples whichinstead use P signals to compute an Euclidean distance matrix E. Fewer samples P tend to produce largercorruption magnitudes, i.e., deviations from E[E]. shows that indeed, all models display lowerpredictive accuracy and higher uncertainty on the increasingly shifted input data, with the simpler modelsoutperforming in the presence of larger corruption. To investigate label mismatch, models are fit to the sametraining data as above - T = 50 RG 1 3 (N = 20) labels with analytic distance matrix input - but now testedon 100 test samples from the following different N = 20 random graph distributions: (i) random geometricgraphs with connectivity radius 1",
  "BA1": "DPG10.3317.5333.0120.911.292.294.662.103.4252.06171.3816.60PDS10.3217.7033.4320.801.292.334.802.103.7552.87173.5316.60DPG-MIMO9.1917.1340.4913.371.191.744.622.063.5054.61161.7111.24DPG-MIMO-E8.3512.6637.778.351.071.527.821.322.4029.1729.072.84 Scaling to larger graphs. Direct inference in large graph settings is challenged by substantial memorydemands. Specifically, providing gradients for HMC via naive backpropagation necessitates storing allintermediate outputs, while full Bayesian inference mandates holding the full data set in memory, cumulativelyleading to a memory footprint of O(DN 2T). Adopting forward-mode auto-differentiation partially mitigatesthis issue by removing depth dependency, effectively reducing memory complexity to O(N 2T). The timecomplexity for MCMC also presents scalability challenges: each forward pass has time complexity ofO(DN 2T), and each posterior sample requires multiple forward passes. Attempting to instead performtransfer learning where inference occurs with smaller graphs to then test on larger graphs still faces anobstacle. The objective function (3) contains the terms 2ae = 2 N(N1)/2i=1aiei,",
  "In light of this, we perform an adjusted transfer learning experiment by fitting the empirical growth trendsof MAP DPG parameter estimates on N = {20, 50, 100, 200} ER 1": "4 graphs and their corresponding analyticdistance matrices. By using a moderate depth D = 200 and T = 10 training samples, MAP inference onlarger graphs with N = 200 nodes takes 1 hour on a M2 MacBook laptop. Using this empirical parameterfit we can extrapolate parameter values and perform transfer learning on 100 ER 1 4 test graphs of size upto N = 1000; again done locally without any GPU. The transfer learning experiment reveals an expectedbut graceful decay in performance in NLL as N increases. Further information and plots on these scalingexperiments are available in Appendix A.3.",
  "Ablation studies": "Prior modeling. To inspect the influence of informative prior modeling on DPG, we define model DPG-Uwhich replaces DPGs informative priors with the following uninformative priors: log U106, 106,, b N(0, 103I). What we find is that in data-rich regimes an informative prior mostly acts to improveefficiency of posterior inference, e.g., when fitting to T = 50 RG 1 3 graphs with N = 20 nodes (using analyticdistance matrices) and testing on 100 i.i.d. samples, it reduces the time needed to generate M = 4000 samplesby a factor of 7.8 and increases the effective sample size - a measure of the number of independent sampleswith the same estimation power as the observed correlated samples (Gelman et al., 1995) - for (ESS) bya factor of 5. In data poor regimes it also tends to help slow down performance degradation, e.g., wheninstead using T = 2 with the same graph data, NLL and ECE are 37% and 13.7% better with the prior thanwithout. See in Appendix A.4 for the full numerical results. Partial stochasticity.We conducted an ablation study investigating the benefits of adding partialstochasticity to the DPG-MIMO model (C = 4, D = 200) using MAP point estimates; the partially stochasticsetup is described in .1. The training and test sets are identical to the i.i.d. generalization experimentabove. Our experiments showed that introducing partial stochasticity in DPG-MIMO-E markedly improvedNLL, BS, error, and ECE by 9.1%, 10.3%, 9.0%, and 31.4%, respectively.",
  "Real data evaluation": "Quantifying uncertainty in networks learnt from S&P500 stock prices. To verify that DPG providesuseful measures of uncertainty on edge predictions based on real data, we first use the financial datasetpresented in (de Miranda Cardoso et al., 2021). The time series consist of S&P500 daily stock prices fromthree sectors (Communication Services, Utilities, and Real Estate), comprising a total of N = 82 stocks.The data spans the period from Jan. 3rd 2014 to Dec. 29th 2017, yielding P = 1006 daily observations.We divided these stocks equally into training and testing sets. For both sets, we created a log-returns datamatrix, denoted as X RN/2P . Specifically, Xi,j = log SPi,j log SPi,j1, where SPi,j signifies the closingprice of the i-th stock on the j-th day. From each matrix (train and test), we derived a matrix of Pearsoncorrelation coefficients . We take the input to be E 11 ||, thus yielding measure of dissimilarity. Wealso constructed a binary label graph, assigning an edge weight value of 1 for stocks within the same sectorand 0 for pairs in different sectors. Thus, our training and testing datasets each contain a single sample.Nodes from the same sector are numbered contiguously creating a block diagonal adjacency matrix structure,displayed with the red outline in (left). We use DPG with depth D = 200 and identify a high-performing parameter value range for via predictivechecking as in .2. Such values aligned closely with those found in the synthetic experiments, and sowe keep the priors unchanged. We run Bayesian parameter inference on a M2 MacBook laptop which takesabout 2 minutes; we visualize the input, empirical mean, and standard deviation of the posterior predictiveon the test sample in (left). Notably, while our mean recovery is robust, there are areas where it",
  "Concluding summary, limitations, and the road ahead": "In this work we identify independent interpretability of (regularization) parameters in inverse problemsas a key property, which allows one to incorporate prior information on solution characteristics into priordistributions over the NN parameters of an unrolled optimization algorithm. We investigate an inverseproblem with this independent interpretability property in the context of GSL from smooth signals, and",
  "Yann LeCun, Lon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to documentrecognition. Proc. IEEE, 1998": "Gautier Marti, Frank Nielsen, Mikoaj Bikowski, and Philippe Donnat.A review of two decades ofcorrelations, hierarchies, networks and clustering in financial markets. Prog. Inf. Geom. Theory Appl., 2021. Gonzalo Mateos, Santiago Segarra, Antonio G. Marques, and Alejandro Ribeiro. Connecting the dots:Identifying network structure via graph signal processing. IEEE Signal Process. Mag., 36(3):1643, 2019.",
  "end forReturn: ak+1": "Parameter tuning in the convergent setting. The optimization parameters , and of Algorithm 2 arenot interpretable, and so performing parameter tuning incurs a O(K3), where K is the number of discretizationvalues used for each parameter. This is in contrast to Algorithm 1 which has only two parameters and. Since is independently interpretable w.r.t sparsity, we can first tune to produce outputs with desiredsparsity level, then tune for appropriate edge weight scale, reducing reducing tuning costs to O(K). Step size frustrates Bayesian PDS. Algorithm 2 has a nuisance step size parameter which must beproperly tuned for convergent and performant iterations; problematically such values are a functions of and values; see (Kalofolias, 2016). The step size introduces several issues for use as a learned parameterin a BNN. First, we empirically find that the values which produce convergent iterations of Algorithm 2(within 5 104 iterations) are very close to values which produce divergent iterations, as shown in (left). This presents the practical problem of producing NaNs during Bayesian inference, as the samplerwill be drawn toward such unstable values of , causing the unrolling to diverge. Second, the values of which produce divergent unrollings are themselves a function of , , and the unrolling depth, frustratingsimple solutions to prevent divergence, e.g., setting p() to be some fixed closed interval. Third, the products and ensure the Unrolled PDS is not a neural network; this causes problems in practice as productsof parameters - each of which can vary over many orders of magnitude - can cause problematic gradients.These observations help explain why we could not find any prior configuration over the three parameters{, , } in PDS which produced convergent posterior sampling over multiple depths. Fixing to a scalarvalue ameliorates these three issues, indeed this was the only configuration which produced a performant",
  ": Left: PDS Step Size . Running both DPG and PDS iterations with = = 1 and = 1/ = 1, =": "/ = 1 for 5 104 iterations, we plot the mean and standard deviation of the normalized 2 distancebetween the output of PDS vs DPG. The value of resulting in the most faithful PDS solution is close tovalues which yield divergent iterations, problematic for its use as our NN function. Right: PDS vs DPGinference time using D = 200 on T = 50 RG 1",
  "graphs running NUTS with 4 chains (in parallel), each takingM = 1500 samples": "PDS model. We found = 0.1 worked best. Even so, care has to be taken in setting the priors for and: log and log U106, 106produced all divergent paths - recall this naive setting for log in DPGworked with no issue in the Ablation Studies of .3. A successful avenue we followed to produce aperformant PDS model was to use , LogNormal(0, 10) and b N(0, 103). Comparing Bayesian inference time of PDS vs DPG. Above we show that finding a range of reasonablevalues of the optimisation parameters can be cubic in PDS while linear in DPG, owing to independentinterpretability. Further, PDS takes more than twice as long in performing inference as the DPG (and PDSwith stochastic took an order of magnitude longer). This may be due to the increased complexity of PDSiterates and the fact that both and occur inside PDS iterations, compared to only for DPG iterates,increasing the complexity of the computational graph required to compute parameter gradients. See (right) showing the difference in inference runtime for PDS and DPG, both having depth D = 200 usingT = 50 RG 1",
  "The NLL is simply 1 log p(a | e, T ). A lower NLL indicates better predictive performance, as it suggeststhat the model assigns higher probabilities to the observed outcomes": "Brier Score. The Brier Score is used to assess the accuracy of probabilistic predictions. It measures themean squared difference between the predicted probability and the actual outcome. A lower Brier Scoreindicates better calibration and accuracy of the probabilistic predictions. Recall the edge-wise probabilitiesoutput by the model for parameters (m) are denoted p(m) = ((m)D(m)(e)b(m)). Then the Brier Score is",
  "m=1p(m) a22(10)": "Complementary metrics. The NLL is particularly useful for evaluating the overall fit of a probabilistic model,emphasizing the accuracy of the assigned probabilities, especially for less likely events. In contrast, the BrierScore provides a more intuitive measure of predictive accuracy and calibration, making it easier to interpretthe models probabilistic predictions in practical scenarios.Numerical Concerns. Evaluating log p(a|e, (m)) can cause numerical issues which stem from underflow incomputing log a(m) or log (1 a(m)). for some edges. This becomes an issue in the covariate shift experimentsin , where the data becomes very noisy and the model can confidently predict the wrong label. Whenthis underflow occurs, the log evaluates to . To solve this we use the softplus parametrization of the loglikelihood: softplus(y ((m)D(m)(e) b(m))), where y = 2y 1, see Equation (10.13) in (Murphy, 2023).",
  "Error. We take the error to be the percentage of incorrectly predicted edges between our thresholded pred.mean E|T [a | e, T ] > 0.5 and the true label a": "Calibration for GSL. We follow the calibration procedure laid out in (Guo et al., 2017). To fit into thisframework, we take our prediction to be the pred. mean thresholded at 0.5, i.e., E|T [a | e, T ] > 0.5. Thusthe confidence, i.e., the probabilities associated with the predicted label, will thus always be in [0.5, 1]. Wewill thus construct M uniform bins Im = (.5 + .5(m1)",
  "graphs": "chains with different starting points in the parameter space, discard the first 500 iterations of each simulation,and monitor relevant diagnostic criteria - namely the Potential Scale Reduction Factor (PSRF) r and EffectiveSample Size (ESS). We do not perform thinning on the resulting samples. Unless otherwise stated all inferencewas achieved without issue, namely with that r 1 and neff 1000 for each parameter. For a visualizationof successful HMC inference with DPG, see which displays the trace plots and bivariate kerneldensity estimates of DPG with D = 30 on RG 1",
  "graphs using analytic distance matrices": "MAP vs MLE. Due to nonconvexity in the loss, initialization is important in finding MAP and MLE ofparameters. We did not alter the default initialization provided in NumPyro with MAP, as it worked withoutissue. We found MLE to be more dependant on initialization; we initialized the parameters of MLE modelsto the median value of the priors set in , i.e. , , b 0.32, 100, 1. Log-normal parameterization. Let Z be a standard normal variable and let and > 0 be two realnumbers, then the distribution of the random variable X = e+Z is called a log-normal distribution withparameters and , i.e. X Lognormal(, 2). Parameters and represent the mean and standarddeviation of log X, not X itself. While the normality of log X is true regardless of base, we will be performingmodeling using powers of 10. Thus in the main body when we specify a distribution Lognormal(, 2),it implies the location parameter used is ln 10, while the scale parameter is unchanged. For example,Lognormal(2, 4) would imply location = ln 102 and scale = 2, and thus would be invoked as a call to theprobabilistic programming language distribution, here NumPyro, as numpyro.distributions.LogNormal(loc= ln 102, scale = 2).",
  "Scaling via transfer learning in the convergent setting. In (3), terms 2ae,": "2 a22 have N(N 1)/2components while term 1log(Sa) has N components, thus the former terms will tend to dominate thelatter as N grows (using fixed , ). We thus create a new problem where each term is normalized by itsnumber of components; we use the notation , to indicate the scale invariant parameter values",
  "= a(e, .5(N 1), ).(12)": "Intuitively, (12) tells us to scale by O(N). This makes sense, as itself scales the objective term which growsat a factor of O(N) slower than the other (finite) terms. This should help correct the differing componentsizes as the problem dimension N grows. Eq. (12) formulates the solution to the size normalized problem withrespect to the solutions of original problem (3). This shows that we can use the original solution procedures,",
  ": Transferring to Larger Graphs. Using T = 20 ER 1": "4 graphs of varying sizes, we run a grid search tofind optimal for the structure recovery task using Algorithm 1 for each graph size, and show the F1 scorefor each . We also plot the line corresponding to Equation 13 which extrapolates performant values fromthe Ni = 20 setting. This line coincides with empirically observed highly performant values in problemsetting for both smaller and significantly larger graphs. now with scaled , to produce solutions. To do so, suppose we find optimal parameter values i, i to theoriginal problem (3) with problem size Ni. To find the proper parameter values for a different problem sizeN, we can solve for the scale invariant parameter values, then re-scale to the appropriate size as",
  "that (N) resemble logarithmic growth, (N) are linear, while b(N) show a sort of power law decay": ":Left: Computing MAP estimates of DPG at increasing graph sizes shows improving recoveryperformance (solid lines). Extrapolating from the trends using the approach outlined in Appendix A.3, wecan perform transfer learning larger graph sizes (dotted lines) with gradual decay in performance. Right:The analytic distance matrix E get smoother for increasingly sized ER 1",
  ", ER 1": "4 , and BA1 from left to right. Weinspect the edge weight distributions recovered running the held out data through Algorithm 1 for discretizedvalues of . The recovered edge weights are non-negative, and so the relevant scaling of and b is a functionof the median and maximum values. analytic distance matrices. We additionally visualize the parameter posterior marginals. We find that pastT 50, i.i.d. generalization did not significantly improve, while past T 102, the parameter posteriormarginals converge. Prior ablation study. In sparse data regimes DPG is more performant and has more efficient inference thanDPG with un-informative priors (DPG-U). Indeed when the lack of data weakens the likelihood relative tothe prior, a useful prior should maintain efficient Bayesian inference (Gelman et al., 2017). illustratesthis using RG 1",
  ": Impact of training set size on i.i.d. generalization performance. We use DPG with D = 200on RG 1": "3 graphs with analytic distance matrices to estimate generalization. The leftmost plot shows thegeneralization performance converge after 50 samples on these same test graphs (error bars indicate.05stdv for compact visualization). The rightmost plots depict posterior marginals for the 3 parameters -, , and b - for increasing sizes of training data. The observed reduction in variance of the posterior is areflection of lower epistemic uncertainty. The posterior shows little change after 102 samples."
}