{
  "Abstract": "Sparse regression methods have been widely used in many elds for their statistical eective-ness and high interpretability. However, there are few sparse regression methods with skewnoise, although statistical modeling using skewness is becoming more important, e.g., inthe medical eld. The Azzalinis skew-normal distribution and its extensions are well-usedfor skew noise. Such skew regression methods have a severe problem with statistical inter-pretability because they model neither mean, median, nor mode. To overcome this problem,we propose a novel sparse regression method based on mode-invariant skew-normal noise.The regression model is easy to interpret in the proposed method because it always modelsa mode regardless of skewness. The proposed method is simple to implement and optimize,suggesting it is highly scalable to other machine-learning methods. We also provide theo-retical guarantees of the proposed method for the average excess risk and the estimationerror. Numerical experiments on articial and real-world data demonstrate that the pro-posed method performs signicantly better and is more stable than other existing methodsfor various skew-noise data.",
  "Introduction": "Starting with Lasso (Tibshirani, 1996), an 1 regularization regression method is becoming popular forpredictive modeling with relevant features. It has gained signicant attention in many elds because of itsability to handle high-dimensional data eciently and its exhaustive studies of methodological extensionsand theoretical properties. (For details, e.g., see Bhlmann & Van De Geer (2011); Hastie et al. (2015).) Aswe have more data available and need more reasonable models, an 1 regularization regression method willlikely continue to be more essential for high-dimensional data analysis. The Lasso is based on the squared error; in other words, it focuses on a symmetric noise. In real-worlddata analysis, however, we need to treat skewness.For example, statistical modeling using skewness isindispensable in the medical eld. Hossain & Beyene (2015) explored an application of the skew-normaldistribution in the analysis of microRNA (miRNA) data, which often does not follow a normal distribution.Their ndings, derived from both simulations and real miRNA dataset analyses, demonstrated that the skew-normal distribution could enhance the detection of dierentially expressed miRNAs. This enhancement isparticularly noticeable when the data is signicantly skewed. Shaei et al. (2020) introduced an automatedstain normalization framework for histopathology images that uses a mixture of multivariate skew-normaldistributions to capture both symmetric and non-symmetric observations. The method can model multi-modal and multiple correlated distributions, regardless of their non-symmetry, and has shown consistent and",
  "Published in Transactions on Machine Learning Research (09/2024)": ": Comparison of four methods with four dierent types of simulation noises in terms of ve evaluationmeasures. From top to bottom, each row results from Data 5, Data 6, Data 7, and Data 8, respectively. Therst column plots the probability density function of the noise. The second through fth columns are boxplots of some scores. The last column is the mean F1 score for each threshold (horizontal axis). In all gures,the proposed method, Chen+ (Chen et al., 2014), Lasso, and Lasso with the Yeo-Johnson transformationare in red, green, blue, and yellow, respectively. All experiments were conducted for 50 runs with dierentrandom seeds.",
  "Revealing partial convexity, we indicate that its optimization and implementation are relativelystraightforward, although the proposed method involves complicated non-linear transformations": "The proposed method is theoretically guaranteed in terms of its excess risk and estimation errorby non-asymptotic analysis under mild assumptions usually adopted in the theoretical analysis ofLasso-type problems. This paper is organized as follows. In , the proposed method is introduced. In , thetheoretical guarantees of the proposed method are provided by non-asymptotic analysis in terms of theexcess risk and the estimation error. Such theoretical studies for skew regression models have not beenshown so far. In , numerical experiments on articial and real-world data are demonstrated. Theresults obtained in that study indicate that the proposed method can serve as a viable alternative to Lasso.In , concluding remarks are given.",
  "exp(2)": "Remark 1. The above function slightly diers from that proposed in Fujisawa & Abe (2015) because theFisher information matrix is non-singular at = 0 when we use the above function but singular when we usethe function proposed in Fujisawa & Abe (2015). The non-singularity is usually necessary to obtain sometheoretical properties. A related proposition appears in Proposition 2.",
  "NNn=1 XnXn is positive denite, ( | DN) is strongly convex for": "The proof is given in Appendix B. Using this theorem, we develop a problem-specic optimization methodbased on the block coordinate descent method. This method allows us to separate the optimization probleminto convex and non-convex and take advantage of the convenient property described in Theorem 1. The update algorithm is summarized in Algorithm 1. We update , , and alternately. First, we x (, )and optimize . We employ the Majorization-Minimization (MM) algorithm using the Taylor expansionwith acceleration techniques (Jamshidian & Jennrich, 1997; Sun et al., 2016). The derivation of this MMalgorithm is given in Appendix C. As shown in line 8 of Algorithm 1, we can rewrite the update as anotherLasso-type problem and then use well-known software, e.g., the sklearn.linear_model package of Python.Next, we x (, ) and optimize , and nally x (, ) and optimize . Although we need to considerthe non-convexity for and , their optimization problems are at most one variable. Hence, we can easilyuse trustable software, e.g. the scipy.optimize package of Python. In this paper, we employ the L-BFGSalgorithm (Liu & Nocedal, 1989), which is an iterative method for solving non-linear optimization problems.In particular, for the inequality constraint > 0, we can utilize the L-BFGS-B algorithm (Byrd et al., 1995;Zhu et al., 1997), which extends the L-BFGS algorithm to handle bounded constraints.",
  ": end while": "The proof is given in Appendix D. Here, we describe how to set initial values in this paper. The regressionparameter started from the estimated coecients of the Lasso model for the same data. Using the residualsof its model, we initialized with their sample standard deviation and with 1 or 1 corresponding to thesign of their sample skewness.",
  "Related Work": "The most famous skew distribution is the Azzalinis skew-normal distribution (Azzalini, 1985), implementedin current standard scientic computing software. Many skew distributions were developed using the Azza-linis basic idea (Azzalini & Capitanio, 1999; Branco & Dey, 2001; Sahu et al., 2003; Liseo & Loperdo, 2003;Gonzlez-Faras et al., 2004; Arellano-Valle & Genton, 2005; Arellano-Valle & Azzalini, 2006). The closestmethod to the proposed method is Chen et al. (2014), which assumes a regression model for the locationparameter of the Azzalinis skew-normal distribution. However, the location parameter in these skew distri-butions is neither mean, median, nor mode; the mean, median, and mode have complicated combinations ofall the location, scale, and skewness parameters. Even if the location is modeled by X for the Azzalinisskew-normal distribution, it is dicult to understand the role of features. The proposed modeling makesit easy to understand the role of features via the modeling of mode. The proposed modeling is suitablefor an interpretable one in a high-dimensional setting, but the modelings using the Azzalinis skew-normaldistribution are not.",
  "Theoretical Results": "This section provides theoretical guarantees for the proposed method by non-asymptotic analysis. First, wepresent some basic properties of the rst, second, and third derivatives of ( | y) = log f(y | ). Next,using them, we show that the excess risk has a quadratic margin. Finally, we show the upper bounds of theaverage excess risk and the estimation error, and then we verify the favorable properties of the proposedmethod. All proofs of propositions and theorems in this section are given in Appendices E, F, G, and H.",
  "where C1,2, C1,1, C1,0 are some positive constants": "The score functions of regression parameters and are bounded by the rst and second-order polynomialsof |y|. This is the same property as the normal linear regression model. We may expect that the scorefunction of skew parameter is bounded by a third-order polynomial of |y| because the skewness usuallydepends on the third moment. However, it is bounded by a second-order polynomial of |y|. Proposition 1implies that the proposed model is easy to treat as well as .",
  "Proposition 2. Let 0(X) := [X0, 0, 0]. The Fisher information matrix I(0(X)) is uniformlypositive denite for any X, in other words, infX min(I(0(X))) > 0, where min(A) is the smallesteigenvalue of A": "If the model is simple, we can easily prove a similar proposition to Proposition 2 because the Fisher in-formation matrix is easily calculated. However, the model (1) is not simple, and it is not easy to verifyProposition 2, as seen in a devised proof of Appendix E.3. We found that the constant factor proposedby Fujisawa & Abe (2015) does not satisfy Proposition 2 at = 0. This is why a constant factor isslightly modied, as described in Remark 1, and then we can prove Proposition 2.",
  "We set L = 6 and assume 0 < (L, S0) 1": "The squared loss of the simple linear regression model for a low-dimensional case is strongly convex be-cause the Hessian matrix of the squared loss concerning the regression parameter vector is SN and it ispositive denite. This property provides the strength of the least squared estimator. However, for the high-dimensional case N < P, SN may not be of full rank; in other words, its smallest eigenvalue may be zero,and then the squared loss is not strongly convex. The non-asymptotic analysis reveals that if the smallesteigenvalue of SN under the restricted condition SC0 1 LS01 is positive, such as Assumption 2, we canshow theoretical guarantees of the 1 regularized estimator (Bhlmann & Van De Geer, 2011; Hastie et al.,2015). In Assumption 2, we set L = 6 to simplify later notations. We can replace it with any constant greaterthan one by adjusting how to choose the regularization parameter. Based on this assumption, we have thefollowing important theorem.",
  ".(25)": "The orders of the average excess risk (24) and the estimation error (25) are (log N)4 and (log N)2 larger thanthose of the ordinary Lasso, respectively. These are the same as in some well-known complicated models,such as the linear mixed eects models with 1 regularization (Schelldorfer et al., 2011). A similar result isalso seen in Corollary 2.",
  "Precision + Recall,(28)": "where Precision and Recall are dened by TP/(TP + FP) and TP/(TP + FN), respectively, and TP, FP,and FN mean the true positive, false positive, and false negative, respectively. The positivity (negativity)of the estimated coecient is dened by |p|/ > T for a threshold T (horizontal axis). Asthe feature selection is more successful at each threshold, the F1 score is larger. This means that a bettermethod presents an upper curve. As a result of , the proposed method outperformed the comparative methods in all the cases,indicating better prediction and feature selection with a smaller number of features. We see that Chen+is inferior to Lasso in Data 3. This might be because the Azzalinis skew-normal distribution could notadequately model such skewed noise. shows box plots of MSEs for 100 trials regarding Data 2 with P = 100 xed and various samplesizes N. The regularization coecient was tuned by 5-fold cross-validation with 80% training and 20%validation data. The Lasso is a benchmark against the proposed method. The proposed method presentedstable behaviors for N < P as well as N > P. In Appendix I.2, we also show the results with a linear regression model without regularization, assuming theAzzalinis skew-normal distribution, skew-t distribution, and skew-Cauchy distribution for noise (Arellano-Valle & Azzalini, 2013; Azzalini & Arellano-Valle, 2013; Azzalini & Salehi, 2020), on Data 1 through Data 4.",
  "Normal and Azzalinis Skew-Noises": "We explore both normal and Azzalinis skew-normal noises under the same conditions as in .1.Let the random variable following the Azzalinis skew-normal be denoted by SN A(, , ).The noisee was generated from additional four types of distribution: N(0, 1) (Data 5), SN A(0, 1, 2) (Data 6),SN A(0, 1, 4) (Data 7), and SN A(0, 1, 6) (Data 8).Notably, the Azzalinis skew-normal noise is not",
  "mode-invariant, i.e., its location parameter does not coincide with its mode, which signicantly violatesthe assumptions of the proposed method": "The results are shown in . In Data 5, the four methods presented similar behaviors, as expected.In Data 6, Data 7, and Data 8, the proposed method outperformed the other methods with the dierencebecoming more signicant as increased. Surprisingly, Chen+ presented similar results to Lasso, althoughChen+ was slightly worse. This would be because the objective function of Chen+ is hard to optimize dueto troublesome non-convexity with complicated combinations of all parameters. Similarly, for Data 3 (skewdata), Chen+ was worse than Lasso. In comparison, the proposed method can present stable behaviorsbecause the objective function is convex for many variables RP , although it is non-convex for only twovariables , R, as shown in Theorem 1.",
  ": Results of MTP (N = 274, P = 1142).MSE(y)Sparsity( )Size( )": "Proposed8.43 101 (1.65 101)9.91 101 (2.05 103)2.09 101 (5.45 100)Chen+10.8 101 (1.88 101)9.72 101 (4.58 103)5.93 101 (7.47 100)Lasso23.4 101 (84.9 101)9.82 101 (6.66 103)4.25 101 (13.9 100)Lasso + YJ11.6 101 (3.84 101)9.82 101 (6.66 103)4.25 101 (13.9 100) and P = 1142 features, where the outcome is the melting point of drug-like compounds (Karthikeyan et al.,2005). These datasets have many features compared to their sample size and may hold skewed noise, asdescribed later. We compared the proposed method with Chen+ (Chen et al., 2014), Lasso (Tibshirani,1996), and Lasso+YJ, as in the previous section. We used 80% of the samples for training and the remaining 20% for testing. Then, we tuned each regular-ization coecient with 5-fold cross-validation using 20% of the training samples (i.e., 16% of all samples)as validation data. These samples were generated randomly, and 30 trials were conducted with dierentrandom seeds. We calculated MSE(y), Sparsity of , and Model Size( ) and evaluated their performance. and show the results of PDGFR and MTP, respectively, with the means and standard devi-ations (in parentheses) of the evaluation measures. For both datasets, the proposed method simultaneouslyachieved the smallest prediction error and the smallest model size. In particular, for MTP, the proposedmethod reduced the model size to less than half when compared to the other methods. Regarding the resid-uals of the test data after training with Lasso, the mean of the normalized skewness of the residuals was0.81 for PDGFR and 0.75 for MTP. In Appendix I.3, we also applied the proposed method to more easily interpretable data, specically theEngineering Graduate Salary (EGS) prediction data (Aggarwal et al., 2016). EGS has fewer features thanthe two datasets above, and the meanings of all features are specically provided. These properties allowus to consider the validity of the estimated active features. The result shows that the proposed methodoutperformed the comparative methods and could select more reasonable features.",
  "Conclusion": "In this paper, we have proposed a novel sparse regression model whose noise is assumed to follow the mode-invariant skew-normal distribution. The proposed method always models a mode regardless of skewnessand is highly adaptable and interpretable to skew noise. We have also shown that the proposed method isstraightforward to implement and optimize, and it has theoretical guarantees for the average excess risk andthe estimation error by non-asymptotic analysis. The results of numerical experiments demonstrated thatthe proposed method achieved both skewness modeling and statistical interpretability simultaneously, evenfor high-dimensional data, highlighting its signicant eectiveness. As a next challenge, we focus on extending to the mode-invariant skew-t distribution. To our knowledge,no study has explicitly dealt with this distribution besides its basic idea being mentioned in Fujisawa &Abe (2015). The rst problem would be to prove the positive deniteness of the Fisher information matrix,whose proof could be complicated, such as in the proof of Proposition 2. Its optimization algorithm couldalso be problematic. However, these may be overcome in terms of t distribution being a scale mixture of",
  "Adelchi Azzalini and Reinaldo B Arellano-Valle. Maximum penalized likelihood estimation for skew-normaland skew-t distributions. Journal of statistical planning and inference, 143(2):419433, 2013": "Adelchi Azzalini and Antonella Capitanio. Statistical applications of the multivariate skew normal distribu-tion. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 61(3):579602, 1999. Adelchi Azzalini and Mahdi Salehi. Some computational aspects of maximum likelihood estimation of theskew-t distribution. Computational and Methodological Statistics and Biostatistics: Contemporary Essaysin Advancement, pp. 328, 2020.",
  "Hironori Fujisawa and Toshihiro Abe. A family of skew distributions with mode-invariance through trans-formation of scale. Statistical Methodology, 25:8998, 2015": "Graciela Gonzlez-Faras, J Armando Dominguez-Molina, and Arjun K Gupta. The closed skew-normaldistribution. Skew-elliptical distributions and their applications: a journey beyond normality, pp. 2542,2004. Rajarshi Guha and Peter C Jurs. Development of linear, ensemble, and nonlinear models for the predictionand interpretation of the biological activity of a set of pdgfr inhibitors. Journal of chemical informationand computer sciences, 44(6):21792189, 2004. Songhua Hao, Jun Yang, and Christophe Berenguer. Degradation analysis based on an extended inversegaussian process model with skew-normal random eects and measurement errors. Reliability Engineering& System Safety, 189:261270, 2019.",
  "Jrg Schelldorfer, Peter Bhlmann, and SARA VAN DE GEER. Estimation for high-dimensional linearmixed-eects models using 1-penalization. Scandinavian Journal of Statistics, 38(2):197214, 2011": "Sobhan Shaei, Amir Safarpoor, Ahad Jamalizadeh, and Hamid R Tizhoosh. Class-agnostic weighted nor-malization of staining in histopathology images using a spatially constrained mixture model. IEEE trans-actions on medical imaging, 39(11):33553366, 2020. Umberto Simola, Xavier Dumusque, and Jessi Cisewski-Kehe. Measuring precise radial velocities and cross-correlation function line-prole variations using a skew normal density. Astronomy & Astrophysics, 622:A131, 2019.",
  "BProof of Theorem 1": "Let f(u | ) := (r(u)). Let l( | u) := log f(u | ). For = 0, since the optimization problem can beregarded as an ordinary least squares method, this theorem clearly holds. Therefore, in the following, weconsider the case = 0. From u = (y )/ and = X, since",
  "as t": "Next, we regard Algorithm 1 as the block coordinate descent (BCD) method for the three blocks (, , )and consider its convergence based on Theorem 4.1 (c) of Tseng (2001). When using the cyclic rule (see of Tseng (2001)) to update the BCD method for B blocks, Theorem 4.1 (c) of Tseng (2001) statesthe following. Theorem 4.1 (c) of Tseng (2001). Assume that the level set 0 := | ( | DN) ((0) | DN)iscompact and that is continuous on 0. If (1, . . . , B) has at most one minimum in b for b = 2, . . . , B1,then every cluster point of {(t)}t:=(B1) mod B is a coordinatewise minimum point of . In addition, if is regular at , then is a stationary point of . We assume that there always exists (yn, Xn) DN such that yn Xn > 0 after updating by the MMalgorithm, and also, there always exists (yn, Xn) DN such that yn Xn < 0. Under this assumption, itholds that ( | DN) as 0, , , or . Let dom and dom be the eective domain of and , respectively. We can see that ( | DN) is continuousin dom and the level set 0 is compact. From Lemma 3.1 of Tseng (2001), is regular at any dom since dom is open and is Gteaux-dierentiable on dom . Therefore, from Theorem 4.1 (c) of Tseng(2001), if ( | DN) has at most one minimum in , then Theorem 2 holds. We will prove that this necessarycondition holds.",
  "Let un := ynXn": ". For ynXn > 0, un (0, ) decreases monotonically as increases. Since (un) > 0for un (0, ) from (82), (un) is a strictly decreasing function for . For yn Xn < 0, un (, 0)increases monotonically as increases. Since (un) < 0 for un (, 0) from (82), (un) is also a strictlydecreasing function for . For yn Xn = 0, (un) = (0) = 0.",
  "is also the only solution satisfying( | DN) = 0 and the only minimum of ( | DN). The proof iscomplete": "Note that from Theorem 1, if SN is positive denite, ( | DN) is strongly convex for and has only oneminimum in . In this case, can be optimized rst by swapping lines 2 to 9 and line 10 in Algorithm 1,and then every cluster point in a sequence of after updating is a stationary point.",
  "FProof of Theorem 3": "If Condition 1, 2, and 3 in .4.1.1 of Bhlmann & Van De Geer (2011) holds, the proposition holdsimmediately from Lemma 9.1 of Bhlmann & Van De Geer (2011). These Condition 1, 2, and 3 correspondto Proposition 3, 2, and 4, which were already shown, respectively. The proof is complete.",
  "GProof of Theorem 4": "If Condition 1, 2, and 3 in .4.1.1 and Condition 4 in .4.2 of Bhlmann & Van De Geer(2011) holds, the proposition holds immediately from Theorem 9.1 of Bhlmann & Van De Geer (2011).These Condition 1, 2, and 3 correspond to Proposition 3, 2, and 4, which were already shown, respectively.Condition 4 correspond to Assumption 2. The proof is complete.",
  "HProof of Theorem 5": "Theorem 5 is similar to Corollary 9.1 of Bhlmann & Van De Geer (2011). The dierence is a model. Theformer model is a regression model with the mode-invariant skew-normal noise, which belongs to a classof generalized linear models, but the latter model is a nite mixture model. The proof of Corollary 9.1 ofBhlmann & Van De Geer (2011) is based on two parts; one is a model-specic part, and the other is notrelated to a model. The model-specic part consists of Proposition 9.4 and Lemma 9.3 of Bhlmann & VanDe Geer (2011). We prove these propositions for generalized linear models. Using them, we can apply thesame proof technique as in Bhlmann & Van De Geer (2011) to prove Theorem 5. Proposition 9.4 and Lemma 9.3 of Bhlmann & Van De Geer (2011) show how to obtain an upper boundof the score function and how to set MN. For our sparse regression model with the mode-invariant skew-normal noise, the upper bound of the score function is obtained in Proposition 1, which corresponds toProposition 9.4 of Bhlmann & Van De Geer (2011).In this case, following the approach described in",
  ".(133)": "This measure is based on a weighted sum of all coecients, evaluating how important a particular coecientis to an overall sparsity. We have Sparsity() = 0 for the least sparse case where all coecients have thesame value, and Sparsity() = 1 for the most sparse case where there exist few non-zero coecients underthe situation P . Therefore, the measure can evaluate sparsity of .",
  "I.2Comparison with Other Methods": "Adding to .1, we also compared the proposed model with a linear regression model without regular-ization, assuming the Azzalinis skew-normal distribution, skew-t distribution, and skew-Cauchy distributionfor noise (Arellano-Valle & Azzalini, 2013; Azzalini & Arellano-Valle, 2013; Azzalini & Salehi, 2020). Thesemethods are denoted by skew-N, skew-t, and skew-C, respectively. The experimental setting and dataare the same as in .1. Four evaluation measures, i.e., MSE(y), MSE( ), Sparsity( ), and Model Size( ), are shown in .The denitions of these measures are the same as in .1. As a result of , the proposedmethod outperformed the comparative methods in all cases, indicating better prediction with a smallernumber of features. Note that the model size for skew-N, skew-t, and skew-C is always 100 due to noregularization term, and the results of Proposed, Chen+, and Lasso are the same as in .",
  "I.3Real-World Financial Data": "We applied the proposed method to the Engineering Graduate Salary (EGS) prediction data (Aggarwalet al., 2016), which provides engineering graduates employment outcomes with standardized assessmentscores. EGS has fewer features than the two medical datasets in .3, and the meanings of all featuresare specically given. These properties allow us to consider the validity of the estimated active features. Forsimplicity, we used only numerical type features (see for details). The data used here consisted ofN = 3998 and P = 25. Other experimental conditions were the same as in .3. shows the result of EGS with the means and standard deviations (in parentheses) of the evaluationmeasures. The proposed method simultaneously achieved the smallest prediction error and the smallestmodel size. Regarding the residuals of the test data after training with Lasso, the mean of the normalizedskewness of the residuals was 2.24. We also obtained interesting results. shows box plots of the estimated coecients for each method.The top 10 coecients are picked out based on the averaged absolute values over 30 trials. For example,a graduates quantitative ability (Quant) ranked rst for the proposed method, while a graduation year(12graduation) ranked rst for Lasso and Lasso+YJ. In the proposed method, 12graduation rankedfourth with about half an eect.It may sound strange that the most relevant factor for salary is thegraduation year. As the box plots indicate, the coecient of this feature was sometimes zero. Moreover, anEnglish score (English) and a college tier (CollegeTier) ranked second and third in the proposed method.From these points, the proposed method could select more reasonable features.",
  "FeatureDescription": "10percentageOverall marks obtained in grade 10 examinations12graduationYear of graduation (senior year high school)12percentageOverall marks obtained in grade 12 examinationsCollegeIDUnique ID identifying the college which the candidate attendedCollegeTierTier of college (computed from the average AMCAT scores)CollegeGPAAggregate GPA at graduationCollegeCityIDA unique ID to identify the city in which the college is locatedCollegeCityTierThe tier of the city in which the college is locatedGraduationYearYear of graduation (Bachelors degree)EnglishScore in AMCATs English sectionLogicalScore in AMCATs Logical ability sectionQuantScore in AMCATs Quantitative ability sectionDomainScore in AMCATs Domain moduleComputerProgrammingScore in AMCATs Computer programming sectionElectronicsAndSemiconScore in AMCATs Electronics & Semiconductor Engineering sectionComputerScienceScore in AMCATs Computer Science sectionMechanicalEnggScore in AMCATs Mechanical Engineering sectionElectricalEnggScore in AMCATs Electrical Engineering sectionTelecomEnggScore in AMCATs Telecommunication Engineering sectionCivilEnggScore in AMCATs Civil Engineering sectionconscientiousnessScore in one of the sections of AMCATs personality testagreeablenessScore in one of the sections of AMCATs personality testextraversionScore in one of the sections of AMCATs personality testnueroticismScore in one of the sections of AMCATs personality testopeness_to_experienceScore in one of the sections of AMCATs personality test"
}