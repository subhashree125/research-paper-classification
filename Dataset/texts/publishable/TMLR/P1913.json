{
  "Abstract": "Given the growing need for automatic 3D content creation pipelines, various 3D repre-sentations have been studied to generate 3D objects from a single image. Due to its su-perior rendering efficiency, 3D Gaussian splatting-based models have recently excelled inboth 3D reconstruction and generation. 3D Gaussian splatting approaches for image to 3Dgeneration are often optimization-based, requiring many computationally expensive score-distillation steps. To overcome these challenges, we introduce an Amortized1 Generative3D Gaussian framework (AGG) that instantly produces 3D Gaussians from a single image,eliminating the need for per-instance optimization. Utilizing an intermediate hybrid repre-sentation, AGG decomposes the generation of 3D Gaussian locations and other appearanceattributes for joint optimization. Moreover, we propose a cascaded pipeline that first gen-erates a coarse representation of the 3D data and later upsamples it with a 3D Gaussiansuper-resolution module. Our method is evaluated against existing optimization-based 3DGaussian frameworks and sampling-based pipelines utilizing other 3D representations, whereAGG showcases competitive generation abilities both qualitatively and quantitatively whilebeing several orders of magnitude faster. Project page:",
  "Introduction": "The rapid development in virtual and augmented reality introduces increasing demand for automatic 3Dcontent creation. Previous workflows often require tedious manual labor by human experts and need specific 1Our use of the term \"amortized\" originates from its original meaning: to gradually reduce or write off the initial cost ofan asset over time. To avoid potential confusion, we would like to clarify that we are not directly employing the standardamortized optimization/inference framework.",
  "Resolution": ": Overview of our AGG framework. We design a novel cascaded generation pipeline that produces 3DGaussian-based objects without per-instance optimization. Our AGG framework involves a coarse generatorthat predicts a hybrid representation for 3D Gaussians at a low resolution and a super-resolution modulethat delivers dense 3D Gaussians in the fine stage.",
  "software tools. A 3D asset generative model is essential to allow non-professional users to realize their ideasinto actual 3D digital content": "Much effort has been put into image-to-3D generation as it allows users to control the generated content.Early methods study the generation of simple objects Yu et al. (2021) or novel view synthesis of limitedviewpoints Li et al. (2021); Shih et al. (2020); Xu et al. (2022a). Later on, with the help of 2D text-to-imagediffusion models, score distillation sampling Poole et al. (2022) has enabled 360-degree object generation forin-the-wild instances Xu et al. (2022b); Melas-Kyriazi et al. (2023); Tang et al. (2023b). More recently, theadvances in collecting 3D datasets Deitke et al. (2023); Wu et al. (2023b); Yu et al. (2023) have supportedthe training of large-scale 3D-aware generative models Liu et al. (2023b;a); Gao et al. (2022) for betterimage-to-3D generations. Various 3D representations have been studied as the media to train 3D generative models.Many priorworks have explored generating explicit representations, such as point clouds Zeng et al. (2022), voxels Zhouet al. (2021); Schwarz et al. (2022), and occupancy grid Wu et al. (2023a). Implicit representations likeNeRF Mildenhall et al. (2020) and distance functions Lionar et al. (2023) are popular for easy optimization.Although effective for content generation Yang et al. (2023); Erko et al. (2023); Or-El et al. (2022), slowrendering and optimization hinder the wide adoption of these representations. As a result, researchers havelooked into more efficient volumetric representations Mller et al. (2022); Chan et al. (2022) to support thetraining of larger generative models Chen et al. (2023a); Karnewar et al. (2023) and the costly optimization inscore distillation Xu et al. (2022b); Melas-Kyriazi et al. (2023). More recently, 3D Gaussian splatting Kerblet al. (2023); Tang et al. (2023a); Chen et al. (2023b); Yi et al. (2023b) has attracted great attention dueto its high-quality real-time rendering ability. This owes to the visibility-aware rendering algorithm thatfacilitates much less optimization time cost compared with NeRF-based variants Liu et al. (2023b); Pooleet al. (2022). Such an efficient renderer enables supervising the 3D Gaussians through loss functions definedon the 2D renderings, which is not suitable for other representations. Despite the exciting progress, how to properly generate 3D Gaussians Kerbl et al. (2023) remains a lessstudied topic. Existing works Tang et al. (2023a); Chen et al. (2023b); Yi et al. (2023b) focus on the per-instance optimization-based setting. Though 3D Gaussians can be optimized with adaptive density controlto represent certain geometry, initialization remains critical for complex object structures. An amortizedpipeline is highly desired to produce the 3D Gaussians in one shot. Such a network can learn a shared 3Dunderstanding of images that generalizes to unseen objects of similar categories to the training set. This willfurther reduce the need for test-time optimization, trading off the computation cost of the inference stagewith the training stage. However, building such a feed-forward pipeline is challenging for two major reasons. First, 3D Gaussians relyon adaptive density control Kerbl et al. (2023) to represent complex geometry, but this leads to a dynamicnumber of 3D Gaussians, making it hard to predict them in an amortized training setting. Second, the3D Gaussians require curated initialization Yi et al. (2023b); Chen et al. (2023b) to be updated properly",
  "Published in Transactions on Machine Learning Research (10/2024)": "Dejia Xu, Yifan Jiang, Peihao Wang, Zhiwen Fan, Humphrey Shi, and Zhangyang Wang. Sinnerf: Trainingneural radiance fields on complex scenes from a single image. In European Conference on Computer Vision,pp. 736753. Springer, 2022a. Dejia Xu, Yifan Jiang, Peihao Wang, Zhiwen Fan, Yi Wang, and Zhangyang Wang. Neurallift-360: Liftingan in-the-wild 2d photo to a 3d object with 360 {\\deg} views. arXiv preprint arXiv:2211.16431, 2022b. Qiangeng Xu, Zexiang Xu, Julien Philip, Sai Bi, Zhixin Shu, Kalyan Sunkavalli, and Ulrich Neumann. Point-nerf: Point-based neural radiance fields. In Proceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition, pp. 54385448, 2022c.",
  "Our contributions can be summarized as follows,": "We study single image-to-3D Gaussian in the amortized setting. Unlike existing works that operateon individual objects, we build a novel cascaded generation framework that instantly presents 3DGaussians in one shot. Our AGG network first generates coarse Gaussian predictions through a hybrid representation thatdecomposes geometry and texture. With two separate transformers predicting the geometry andtexture information, the 3D Gaussian attributes can be optimized jointly and stably.",
  "Image-to-3D Generation": "Numerous research studies have been conducted on generating 3D data from a single image. Early attemptssimplify the challenges by either generating objects of simple geometry Yu et al. (2021) or focusing onsynthesizing novel view images of limited viewpoints Li et al. (2021); Shih et al. (2020); Xu et al. (2022a).Later on, various combinations of 2D image encoder and 3D representations are adopted to build 3Dgenerative models, such as on 3D voxels Girdhar et al. (2016); Choy et al. (2016); Yagubbayli et al. (2021),point clouds Yang et al. (2019); Fan et al. (2017); Zeng et al. (2022); Wu et al. (2023a); Lionar et al. (2023),meshes Gao et al. (2022); Kanazawa et al. (2018); Wang et al. (2018), and implicit functions Erko et al.(2023); Yang et al. (2021). More recently, with the help of score distillation sampling from a pre-trainedtext-to-image diffusion model, great efforts have been put into generating a 3D asset from a single imagethrough optimization Xu et al. (2022b); Melas-Kyriazi et al. (2023); Seo et al. (2023); Tang et al. (2023b);Deng et al. (2023); Tang et al. (2023a). Among them, DreamGaussian utilizes 3D Gaussians as an efficient3D representation that supports real-time high-resolution rendering via rasterization.In addition tothese optimization-based methods, building feed-forward models for image-to-3D generation avoids thetime-consuming optimization process, and a large generative model can learn unified representation forsimilar objects, leveraging prior knowledge from the 3D dataset better.The advances in large-scale 3Ddatasets Deitke et al. (2023); Wu et al. (2023b); Yu et al. (2023) largely contributed to the design of better",
  "Implicit 3D Representations": "Neural radiance field (NeRF) Mildenhall et al. (2020) implements a coordinate-based neural network torepresent the 3D scenes and demonstrates outstanding novel view synthesis abilities. Many following workshave attempted to improve NeRF in various aspects. For example, MipNeRF Barron et al. (2021a) andMipNeRF-360 Barron et al. (2021b) introduce advanced rendering techniques to avoid aliasing artifacts.SinNeRF Xu et al. (2022a), PixelNeRF Yu et al. (2021), and SparseNeRF Wang et al. (2023) extends theapplication of NeRFs to few-shot input views, making single image-to-3D generation through NeRF a morefeasible direction. With the recent advances in score distillation sampling Poole et al. (2022), numerousapproaches have looked into optimizing a neural radiance field through guidance from diffusion priors Xuet al. (2022b); Melas-Kyriazi et al. (2023); Seo et al. (2023); Tang et al. (2023b); Deng et al. (2023). Whilesuitable for optimization, NeRFs are usually expressed implicitly through MLP parameters, which makesit challenging to generate them via network Erko et al. (2023). Consequently, many works Lorraine et al.(2023); Chan et al. (2022); Bhattarai et al. (2023) instead choose to generate hybrid representations, whereNeRF MLP is adopted to decode features stored in explicit structures Chan et al. (2022); Chen et al. (2022);Mller et al. (2022); Fridovich-Keil et al. (2023); Yi et al. (2023a); Xu et al. (2022c); Barron et al. (2023).Among them, ATT3D Lorraine et al. (2023) builds an amortized framework for text-to-3D by generatingInstant NGP Mller et al. (2022) via score distillation sampling.While focusing on generating explicit3D Gaussians, our work draws inspiration from hybrid representations and utilizes hybrid structures asintermediate generation targets, which can be later decoded into 3D Gaussians.",
  "Explicit 3D Representations": "Explicit 3D representations have been widely studied for decades.Many works have attempted to con-struct 3D assets through 3D voxels Girdhar et al. (2016); Choy et al. (2016); Yagubbayli et al. (2021), pointclouds Yang et al. (2019); Fan et al. (2017); Zeng et al. (2022); Wu et al. (2023a); Lionar et al. (2023) andmeshes Gao et al. (2022); Kanazawa et al. (2018); Wang et al. (2018). Since pure implicit radiance fieldsare operationally slow, often needing millions of neural network queries to render large-scale scenes, greatefforts have been put into integrating explicit representations with implicit radiance fields to combine theiradvantages. Many works looked into employing tensor factorization to achieve efficient explicit represen-tations Chan et al. (2022); Chen et al. (2022); Fridovich-Keil et al. (2023); Yi et al. (2023a); Mller et al.(2022). Multi-scale hash grids Mller et al. (2022) and block decomposition Tancik et al. (2022) are intro-duced to extend to city-scale scenes. Another popular direction focuses on empowering explicit structureswith additional implicit attributes. Point NeRF Xu et al. (2022c) utilizes neural 3D points to render acontinuous radiance volume efficiently. Similarly, NU-MCC Lionar et al. (2023) uses latent point featuresas representation, specifically focusing on completing shapes. 3D Gaussian splatting Kerbl et al. (2023)introduces point-based -blending along with an efficient rasterizer and has attracted great attention dueto their outstanding ability in reconstruction Kerbl et al. (2023) and generation Tang et al. (2023a); Chenet al. (2023b); Yi et al. (2023b) tasks. A concurrent work, Hu et al. (2024), addresses efficient point cloudrendering through a generalizable neural network converting point clouds to 3D Gaussians. However, existing3D generation works present 3D Gaussians through optimization, while our work takes one step further andfocuses on building an amortized framework that generates 3D Gaussians without per-instance optimization.",
  "Background: 3D Gaussian Splatting (3DGS)": "3D Gaussian splatting is a recently popularized explicit 3D representation that utilizes anisotropic 3D Gaus-sians. Each 3D Gaussian G is defined with a center position R3, covariance matrix , color informationc R3 and opacity R1. The covariance matrix describes the configuration of an ellipsoid and isimplemented via a scaling matrix S R3 and a rotation matrix R R33.",
  "j=1(1 jG(xj)),(3)": "An efficient tile-based rasterizer enables fast forward and backward pass that facilitates real-time rendering.The optimization method of 3D Gaussian properties is interleaved with adaptive density control of Gaussians,namely, densifying and pruning operations, where Gaussians are added and occasionally removed. In ourwork, we follow the convention in 3D Gaussian splatting to establish the Gaussian attributes but identifyspecific canonicalization designed for single image-to-3D task. We first reduce the degree of the sphericalharmonic coefficients and allow only diffuse colors for the 3D Gaussians. Additionally, we set canonicalisotropic scale and rotation for the 3D Gaussians, as we find these attributes extremely unstable during theoptimization process.",
  "Coarse Hybrid Generator": "In the coarse stage, our AGG framework utilizes a hybrid generator to produce a hybrid representation as anintermediate generation target. Initially, the input image is encoded using a vision transformer. Subsequently,a geometry and a texture generator are constructed to map learnable queries into location sequences andtexture fields individually. This hybrid representation is then decoded into explicit 3D Gaussians, whichenables efficient high-resolution rendering and facilitates supervision via multi-view images.The overallarchitecture of our proposed coarse hybrid generator is illustrated in .Our model is trained viarendering loss defined on multi-view images and is warmed up with Chamfer distance loss using 3D Gaussianpseudo labels. Encoding Input RGB InformationSingle image-to-3D is a highly ill-posed problem since multiple 3Dobjects can align with the single-view projection. As a result, an effective image encoder is greatly neededto extract essential 3D information. Previous work Liu et al. (2023b;c) mainly incorporate CLIP imageencoder Radford et al. (2021), benefiting from the foundational training of Stable diffusion Rombach et al.(2022). While effective for text-to-image generation, CLIP encoder is not well-designed for identifying featurecorrespondences, which is crucial for 3D vision tasks. As a result, we use the pre-trained DINOv2 transformeras our image encoder, which has demonstrated robust feature extraction capability through self-supervisedpre-training. Unlike previous works Tumanyan et al. (2022); Xu et al. (2022a) that only use the aggregatedglobal [CLS] token, we choose to incorporate patch-wise features as well.",
  "!'()*+\"!": ": Architecture of our coarse hybrid generator. We first use a pre-trained DINOv2 image encoder toextract essential features and then adopt two transformers that individually map learnable query tokens toGaussian locations and a texture field. The texture field accepts location queries from the geometry branch,and a decoding MLP further converts the interpolated plane features into Gaussian attributes (e.g., opacity,color). Geometry PredictorThe location information of 3D Gaussian is expressed via their 3D means following3DGS Kerbl et al. (2023), which is a three-dimensional vector for each point. Thus, we adopt a transformer-based network for predicting the location sequence. The transformer inputs are a set of learnable queriesimplemented with a group of learnable position embeddings. Each query will correspond to one 3D Gaussianthat we generate. Before feeding into the transformer network, the positional embeddings are summed withthe global token [CLS] extracted from the DINOv2 model.The sequence of queries is then modulatedprogressively by a series of transformer blocks, each containing a cross-attention block, a self-attentionblock, and a multi-layer perception block. These components are interleaved with LayerNorm and GeLUactivations. The cross-attention blocks accept DINOv2 features as context information. The final layer of ourgeometry predictor involves an MLP decoding head, converting the hidden features generated by attentionmodules into a three-dimensional location vector. Texture Field GeneratorJoint prediction of texture and geometry presents significant challenges. Aprimary challenge is the lack of direct ground truth supervision for texture in 3D space. Instead, textureinformation is inferred through rendering losses in 2D. When geometry and texture information are decodedfrom a shared network, their generation becomes inevitably intertwined. Consequently, updating the pre-dicted location alters the supervision for rendered texture, leading to divergent optimization directions fortexture prediction. To resolve this, a distinct transformer is employed to generate a texture field. Our texture field is implementedusing a triplane Chan et al. (2022), complemented by a shared decoding MLP head. The triplane accepts3D location queries from the geometry branch and concatenates interpolated features for further processing.Utilization of this texture field facilitates decomposed optimization of geometry and texture information.More importantly, texture information is now stored in structured planes. Incorrect geometry predictions donot impact the texture branchs optimization process, which receives accurate supervision when geometryqueries approximate ground truth locations. Supervision Through 2D RenderingsThanks to the efficient rasterizer for 3D Gaussians, we can applysupervision on novel view renderings during the training. The 3D Gaussians we generate are rendered fromrandomly selected novel views, and image-space loss functions are calculated against ground truth renderingsavailable from the dataset. Concretely, we render the scene into RGB images and corresponding foreground",
  "Gaussian Super Resolution": "Although our hybrid generator is effective in the coarse generation stage, generating high-resolution 3DGaussians requires many learnable queries which are computationally expensive due to the quadratic cost ofself-attention layers. As a result, we instead utilize a second-stage network as a super-resolution module tointroduce dense 3D Gaussians into our generation. Since coarse geometry is obtained from the first stage,the super-resolution network can focus more on refining local details. For simplicity, we use a lightweightUNet architecture with the efficient point-voxel layers Liu et al. (2019), as shown in . Latent Space Super-ResolutionAs mentioned earlier, the 3D Gaussians require curated locations toupdate the other attributes properly. This leads to the key challenge for the super-resolution stage, whichis to introduce more point locations for the super-resolved 3D Gaussians. Inspired by previous works in 2Dimage Shi et al. (2016) and point cloud super resolution Qian et al. (2021), we perform feature expandingas a surrogate of directly expanding the number of points. Through a point-voxel convolutional encoder, wefirst convert the stage one coarse 3D Gaussians into compact latent features that can be safely expandedthrough a feature expansion operation: rearranging a tensor of shape (B, N, C r) to a tensor of shape(B, N r, C). The expanded features are then decoded through a point-voxel convolutional decoder thatpredicts the Gaussian locations and other attributes. Incorporating RGB informationWhile the first-stage network may capture the rough geometry ofobjects, the texture field may converge into blurry results due to the oscillating point locations and roughgeometry. Consequently, utilizing the abundant texture information from the input image is crucial for thesuper-resolution network to generate plausible details. For this purpose, we introduce RGB features intothe bottleneck of the UNet architecture. Specifically, we adopt cross-attention layers before and after thefeature expansion operation. Image features are fed through cross-attention layers to modulate the latentpoint-voxel features.",
  "Adaptive Density Control": "The original 3D Gaussians involve special density control operations to move them toward their desired 3Dlocations. However, in the amortized training setting, it is non-straightforward to adaptively clone, split, orprune the Gaussians according to the gradients they receive. This is because these operations change thenumber of points, making it hard for an amortized framework to process. To this end, we use a fixed number of 3D Gaussians for each object, avoiding the burden for the generatornetwork to determine the number of points needed. Moreover, since we do not have access to clone andsplit operations, training the amortized generator leads to sharp 3D Gaussians with unpleasant colors thatare trying to mimic fine-grained texture details. Once the predictions become sharp, they get stuck in thelocal minimum, and therefore, fewer 3D Gaussians are available to represent the overall object, leading toblurry or corrupted generations. To overcome the issue, we empirically set canonical isotropic scales androtations for the 3D Gaussians on all objects to stabilize the training process.",
  "Initialization": "Proper initialization plays an important role in the original optimization-based 3D Gaussian splatting.Though Gaussian locations are jointly optimized with other attributes, it is observed that optimizationupdates often prefer reducing the opacity and scale instead of moving the Gaussians directly. By doing so,Gaussians are eliminated from the wrong location after reaching the pruning threshold. Instead, the adaptivedensity control can then clone or split Gaussians at proper locations according to their gradients. Albeit thesimplicity of pruning and cloning operations for optimization algorithms, they are non-trivial to implementwith amortized neural networks. To overcome the issues, we warm up the generator with 3D Gaussian pseudo labels.We use a few 3Dobjects and perform multi-view reconstruction to obtain 3D Gaussians for each object. Since multiple setsof 3D Gaussians can all be plausible for reconstructing the same 3D ground truth object, the 3D Gaussiansare only considered pseudo labels where we use their attributes to initialize our network layers properly. Due to these Gaussians being stored in random orders as sets, we cannot use L1 reconstruction loss as ingenerative frameworks such as LION Zeng et al. (2022). Alternatively, we utilize the Chamfer distance lossto pre-train our network. Specifically, we first calculate the point matching correspondences according to thedistance of Gaussians 3D means and then minimize the L1 difference of each attribute, including location,opacity, and color. The formulation can be summarized as:",
  "Baseline Methods": "We compare with two streams of work. One involves the sampling-based 3D generation that uses different3D representations, such as Point-E and One-2345. Point-E Nichol et al. (2022) includes a large diffusiontransformer that generates a point cloud. Their work performs the generation in the world coordinates,where objects are always canonically placed as the original orientation in the dataset. One-2345 Liu et al.(2023a) leverages Zero123 Liu et al. (2023b) and utilizes SparseNeuS Long et al. (2022) to fuse informationfrom noisy multi-view generations efficiently. We also compare with DreamGaussian Tang et al. (2023a)sfirst stage, where 3D Gaussians are optimized with SDS Poole et al. (2022) from Zero123 Liu et al.(2023b). Both One-2345 and DreamGaussian require the elevation of the input image at inference time. ForOne-2345, we use official scripts to estimate the elevation. We provide DreamGaussian with ground truthelevation extracted from the camera pose in the testing dataset.",
  "Dataset": "Our model is trained on OmniObject3D dataset Wu et al. (2023b), which contains high-quality scans ofreal-world objects. The number of 3D objects is much fewer than Objaverse Deitke et al. (2023). Alongsidepoint clouds, the OmniObject3D dataset provides 2D renderings of objects obtained through Blender. Thisallows us to implement rendering-based loss functions that supervise the rendering quality in 2D space. Weconstruct our training set using 2,370 objects from 73 classes in total. We train one model using all classes.The test set contains 146 objects, with two left-out objects per class.",
  "Qualitative and Quantitative Comparisons": "We provide visual comparisons and quantitative analysis between our methods and existing baselines in and Tab. 1. As shown in the novel view synthesis results, our model learns reasonable geometryunderstanding and produces plausible generations of texture colors. Point-E Nichol et al. (2022) producesreasonable geometry but presents unrealistic colors, possibly because the model was trained on large-scalesynthetic 3D data. Both One-2345 Liu et al. (2023a) and DreamGaussian Tang et al. (2023a) can generatecorrupted shapes, which might be because the novel views synthesized from the Zero-123 diffusion model Liuet al. (2023b) are multi-view inconsistent. We use CLIP distance defined on image embeddings to reflect high-level image similarity on multi-viewrenderings, as well as PSNR, SSIM, and LPIPS Zhang et al. (2018a) to measure the visual quality comparedto the ground truth 3D object. For Point-Es outputs, we only use CLIP distance because they are producedin canonical orientation and not aligned with the ground truth 3D object. As shown in Tab. 1, our methodobtains competitive numbers in terms of image quality. Additionally, our AGG network enjoys the fastestinference speed. In comparison, both Point-E and One-2345 adopt an iterative diffusion process when producing their finalresults, while DreamGaussian leverages a costly optimization process through score distillation sampling.",
  "Ablation Study": "We conduct thorough experiments to validate the effectiveness of our proposed components using the available3D ground truth from the dataset.We compare with multiple variants of the model.Our full modelconditions on fixed DINO feature. We explored fixed CLIP features and fine-tuned DINO features in (a,b). In comparison, fixed DINO feature in (d) eases training, while other options degrade perceptualquality in the novel-view renderings. Then we study the effect of the warmup set in (c). This set easesthe optimization for geometry prediction. Without the warmup set, it is ineffective to rely on rendering lossfor accurate geometry prediction. We observe a large drop in PSNR and LPIPS in Tab. 2(c) compared to(d). Finally, we remove the image conditioning in stage 2. As shown in (e) and Tab. 2(e), imageconditioning helps stage 2 model overcome unexpected color shifts. As shown in Tab. 2(f) and (f),the full model delivers the best generation results both qualitatively and quantitatively when rendered fromnovel viewpoints.",
  "Conclusion": "In this work, we make the first attempt to develop an amortized pipeline capable of generating 3D Gaussiansfrom a single image input. The proposed AGG framework utilizes a cascaded generation pipeline, comprisinga coarse hybrid generator and a Gaussian super-resolution model. Experimental results demonstrate thatour method achieves competitive performance with several orders of magnitude speedup in single-image-to-3D generation, compared to both optimization-based 3D Gaussian frameworks and sampling-based 3Dgenerative models utilizing alternative 3D representations. Jonathan T Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman, Ricardo Martin-Brualla, and Pratul PSrinivasan. Mip-nerf: A multiscale representation for anti-aliasing neural radiance fields. In Proceedingsof the IEEE/CVF International Conference on Computer Vision, pp. 58555864, 2021a.",
  "Zilong Chen, Feng Wang, and Huaping Liu.Text-to-3d using gaussian splatting.arXiv preprintarXiv:2309.16585, 2023b": "Christopher B Choy, Danfei Xu, JunYoung Gwak, Kevin Chen, and Silvio Savarese. 3d-r2n2: A unifiedapproach for single and multi-view 3d object reconstruction.In Computer VisionECCV 2016: 14thEuropean Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part VIII 14, pp.628644. Springer, 2016. Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt,Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse: A universe of annotated 3d objects. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1314213153,2023.",
  "Ziya Erko, Fangchang Ma, Qi Shan, Matthias Niener, and Angela Dai. Hyperdiffusion: Generating implicitneural fields with weight-space diffusion. arXiv preprint arXiv:2303.17015, 2023": "Haoqiang Fan, Hao Su, and Leonidas J Guibas. A point set generation network for 3d object reconstructionfrom a single image. In Proceedings of the IEEE conference on computer vision and pattern recognition,pp. 605613, 2017. Sara Fridovich-Keil, Giacomo Meanti, Frederik Rahbk Warburg, Benjamin Recht, and Angjoo Kanazawa.K-planes: Explicit radiance fields in space, time, and appearance.In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition, pp. 1247912488, 2023. Jun Gao, Tianchang Shen, Zian Wang, Wenzheng Chen, Kangxue Yin, Daiqing Li, Or Litany, Zan Gojcic,and Sanja Fidler. Get3d: A generative model of high quality 3d textured shapes learned from images.Advances In Neural Information Processing Systems, 35:3184131854, 2022. Rohit Girdhar, David F Fouhey, Mikel Rodriguez, and Abhinav Gupta. Learning a predictable and gen-erative vector representation for objects. In Computer VisionECCV 2016: 14th European Conference,Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part VI 14, pp. 484499. Springer, 2016. Yicong Hong, Kai Zhang, Jiuxiang Gu, Sai Bi, Yang Zhou, Difan Liu, Feng Liu, Kalyan Sunkavalli,Trung Bui, and Hao Tan.Lrm: Large reconstruction model for single image to 3d.arXiv preprintarXiv:2311.04400, 2023. Yueyu Hu, Ran Gong, Qi Sun, and Yao Wang. Low latency point cloud rendering with learned splatting. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 57525761,2024.",
  "Heewoo Jun and Alex Nichol.Shap-e:Generating conditional 3d implicit functions.arXiv preprintarXiv:2305.02463, 2023": "Angjoo Kanazawa, Shubham Tulsiani, Alexei A Efros, and Jitendra Malik. Learning category-specific meshreconstruction from image collections. In Proceedings of the European Conference on Computer Vision(ECCV), pp. 371386, 2018. Animesh Karnewar, Andrea Vedaldi, David Novotny, and Niloy J Mitra.Holodiffusion: Training a 3ddiffusion model using 2d images. In Proceedings of the IEEE/CVF Conference on Computer Vision andPattern Recognition, pp. 1842318433, 2023.",
  "Diederik P Kingma and Jimmy Ba.Adam:A method for stochastic optimization.arXiv preprintarXiv:1412.6980, 2014": "Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao,Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al.Segment anything.arXiv preprintarXiv:2304.02643, 2023. Jiahao Li, Hao Tan, Kai Zhang, Zexiang Xu, Fujun Luan, Yinghao Xu, Yicong Hong, Kalyan Sunkavalli, GregShakhnarovich, and Sai Bi. Instant3d: Fast text-to-3d with sparse-view generation and large reconstructionmodel. arXiv preprint arXiv:2311.06214, 2023.",
  "Zhijian Liu, Haotian Tang, Yujun Lin, and Song Han. Point-voxel cnn for efficient 3d deep learning. Advancesin Neural Information Processing Systems, 32, 2019": "Xiaoxiao Long, Cheng Lin, Peng Wang, Taku Komura, and Wenping Wang. Sparseneus: Fast generalizableneural surface reconstruction from sparse views. In European Conference on Computer Vision, pp. 210227.Springer, 2022. Jonathan Lorraine, Kevin Xie, Xiaohui Zeng, Chen-Hsuan Lin, Towaki Takikawa, Nicholas Sharp, Tsung-YiLin, Ming-Yu Liu, Sanja Fidler, and James Lucas. Att3d: Amortized text-to-3d object synthesis. arXivpreprint arXiv:2306.07349, 2023. Luke Melas-Kyriazi, Iro Laina, Christian Rupprecht, and Andrea Vedaldi. Realfusion: 360deg reconstructionof any object from a single image. In Proceedings of the IEEE/CVF Conference on Computer Vision andPattern Recognition, pp. 84468455, 2023. Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and RenNg. Nerf: Representing scenes as neural radiance fields for view synthesis. In European conference oncomputer vision, pp. 405421. Springer, 2020.",
  "Alex Nichol, Heewoo Jun, Prafulla Dhariwal, Pamela Mishkin, and Mark Chen.Point-e: A system forgenerating 3d point clouds from complex prompts. arXiv preprint arXiv:2212.08751, 2022": "Maxime Oquab, Timothe Darcet, Tho Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, PierreFernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visualfeatures without supervision. arXiv preprint arXiv:2304.07193, 2023. Roy Or-El, Xuan Luo, Mengyi Shan, Eli Shechtman, Jeong Joon Park, and Ira Kemelmacher-Shlizerman.Stylesdf: High-resolution 3d-consistent image and geometry generation. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition, pp. 1350313513, 2022.",
  "Guangcong Wang, Zhaoxi Chen, Chen Change Loy, and Ziwei Liu. Sparsenerf: Distilling depth ranking forfew-shot novel view synthesis. arXiv preprint arXiv:2303.16196, 2023": "Nanyang Wang, Yinda Zhang, Zhuwen Li, Yanwei Fu, Wei Liu, and Yu-Gang Jiang. Pixel2mesh: Generating3d mesh models from single rgb images. In Proceedings of the European conference on computer vision(ECCV), pp. 5267, 2018. Chao-Yuan Wu, Justin Johnson, Jitendra Malik, Christoph Feichtenhofer, and Georgia Gkioxari. Multiviewcompressive coding for 3d reconstruction.In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition, pp. 90659075, 2023a. Tong Wu, Jiarui Zhang, Xiao Fu, Yuxin Wang, Jiawei Ren, Liang Pan, Wayne Wu, Lei Yang, Jiaqi Wang,Chen Qian, et al.Omniobject3d: Large-vocabulary 3d object dataset for realistic perception, recon-struction and generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition, pp. 803814, 2023b.",
  "Brent Yi, Weijia Zeng, Sam Buchanan, and Yi Ma. Canonical factors for hybrid neural fields. In Proceedingsof the IEEE/CVF International Conference on Computer Vision, pp. 34143426, 2023a": "Taoran Yi, Jiemin Fang, Guanjun Wu, Lingxi Xie, Xiaopeng Zhang, Wenyu Liu, Qi Tian, and XinggangWang. Gaussiandreamer: Fast generation from text to 3d gaussian splatting with point cloud priors. arXivpreprint arXiv:2310.08529, 2023b. Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa. pixelnerf: Neural radiance fields from one orfew images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,pp. 45784587, 2021. Xianggang Yu, Mutian Xu, Yidan Zhang, Haolin Liu, Chongjie Ye, Yushuang Wu, Zizheng Yan, ChenmingZhu, Zhangyang Xiong, Tianyou Liang, et al. Mvimgnet: A large-scale dataset of multi-view images. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 91509161,2023. Xiaohui Zeng, Arash Vahdat, Francis Williams, Zan Gojcic, Or Litany, Sanja Fidler, and Karsten Kreis.Lion: Latent point diffusion models for 3d shape generation. arXiv preprint arXiv:2210.06978, 2022.",
  "A.1Training": "The model is trained with Adam optimizer Kingma & Ba (2014). Rendering loss is enforced at 128 128resolution. We first train the coarse hybrid generator for ten epochs. The learning rate is set to 1e 4maximum, with a warmup stage for three epochs and followed by a cosine annealing learning rate scheduler.During the warmup epochs, we set the loss weight of Lchamfer to ten and Lrendering to one. Later, we graduallyreduce the weight of Lchamfer and increase the weight of Lrendering until the weight of Lrendering is ten andthat of Lchamfer is one. Then, we freeze the parameters for the coarse hybrid generator and only train theGaussian super-resolution module. We use Lrendering and optimize for five epochs, with a learning rate setto 1e 4. Finally, we unfreeze all the parameters in both modules and train with rendering loss Lrenderingfor three epochs, with a learning rate set to 1e 5. For Lrendering, we set 1 = 2 for all epochs. The parameters in DINOv2 Oquab et al. (2023) vision encoderare kept fixed in all epochs. The input image is 256 256 resolution, and the DINOv2 encoder producesimage features of 1 325 768. In each iteration, we render the generated 3D Gaussians into eight novelviews where Lrendering are calculated against the ground truth correspondingly. The warmup set consists often objects per class, resulting in 730 objects in total.",
  "A.2Inference": "At inference time, given an in-the-wild image, we first extract image features using the DINOv2 encoder.Then, we send the features sequentially into the coarse hybrid generator and the Gaussian super-resolutionnetwork. Our model does not require access to the camera pose of the input image at inference time. Thisis because, during training, we ensure that the object is reconstructed in view space. In other words, the 3Dasset and the input image are always aligned, and the supervision signals are rotated accordingly.",
  "B.1Metrics Calculation": "Since the objects generated by the baseline methods are not guaranteed to be aligned with the groundtruth 3D assets in the OmniObject3D Wu et al. (2023b) dataset, we cannot use pixel-wise reconstructionmetrics to measure the generation quality. Moreover, the ill-posed nature of image-to-3D generation makesit harder to evaluate the 3D generation results quantitatively. Therefore, we follow the practice in previousworks Xu et al. (2022b); Tang et al. (2023a) and leverage CLIP distance to measure the semantic quality ofthe generation results. Specifically, for each object, we first normalize the scale of the object, and then usea fixed set of cameras to obtain novel view renderings. Finally, we calculate the average spherical distancebetween the CLIP Radford et al. (2021) image embedding of the input image and the novel view images.The runtime of our method is measured on the test set using an A100 GPU. The runtime of baselines isobtained from DreamGaussian Tang et al. (2023a).",
  "B.2Baseline Configurations": "Point-E Nichol et al. (2022) consists of a transformer-based diffusion model that generates point cloudsthrough sampling. We render the generated point cloud using Open3D Zhou et al. (2018) library and set thepoint size to 10. As for One-2345 Liu et al. (2023a), we render the generated mesh via the original Blenderscript provided by the authors. To render the results obtained by DreamGaussian Tang et al. (2023a), weuse the authors script to first export the Gaussians to mesh and then render the mesh.",
  "DAdditional Comparisons with Concurrent Works": "As suggested by the Reviewer JHwb, weve added comparisons with concurrent works that appear in CVPR2024, TPS Zou et al. (2024) and Splatter Image Szymanowicz et al. (2024). Please kindly note that theseworks are implemented independently. To obtain the results from these concurrent methods, we use the authors original implementations providedin their Github repos and render the generated 3D Gaussians directly. As shown in Tab. A, we quantitativelycompare our proposed methods with other methods. Our work shows competitive performance in PSNRand SSIM, while outperforming most baselines in LPIPS and CLIP Distance that emphasize more on humanperceptual quality. We further provide side-by-side comparisons that qualitatively demonstrate our models superiority.Asshown in Fig. A, Splatter Image Szymanowicz et al. (2024) tends to produce results that are more blurry,and TPS Zou et al. (2024) suffers at generating realistic back-view results and side-view results. In compar-ison, our method outperforms these comparison methods in terms of geometry and appearance quality onchallenging novel views.",
  "EBroader Impacts": "Existing 3D content creation pipelines usually require tedious work that an expert must do. On the positiveside, the advent of image-to-3D generation frameworks lowers the barrier and makes 3D content creationaccessible to a broader audience. Individuals and small businesses with limited resources can now producehigh-quality 3D models without the need for extensive training or expensive software. This democratiza-tion can spur innovation, allowing more people to participate in industries such as gaming, virtual reality,education, and product design. However, these easily generated 3D models could lead to a saturation of content, potentially devaluingthe work of skilled professionals. Additionally, ethical considerations regarding the use of copyrighted orproprietary images to generate 3D models must be addressed. Ensuring that creators rights are respectedand that there is a clear framework for the ethical use of this technology is crucial."
}