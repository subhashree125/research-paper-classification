{
  "Abstract": "While adversarial training methods have significantly improved the robustness of deep neuralnetworks against norm-bounded adversarial perturbations, the generalization gap betweentheir performance on training and test data is considerably greater than that of standardempirical risk minimization. Recent studies have aimed to connect the generalization prop-erties of adversarially trained classifiers to the min-max optimization algorithm used in theirtraining. In this work, we analyze the interconnections between generalization and optimiza-tion in adversarial training using the algorithmic stability framework. Specifically, our goalis to compare the generalization gap of neural networks trained using the vanilla adversar-ial training method, which fully optimizes perturbations at every iteration, with the freeadversarial training method, which simultaneously optimizes norm-bounded perturbationsand classifier parameters. We prove bounds on the generalization error of these methods,indicating that the free adversarial training method may exhibit a lower generalization gapbetween training and test samples due to its simultaneous min-max optimization of classifierweights and perturbation variables. We conduct several numerical experiments to evaluatethe train-to-test generalization gap in vanilla and free adversarial training methods. Ourempirical findings also suggest that the free adversarial training method could lead to asmaller generalization gap over a similar number of training iterations. The paper code isavailable at",
  "Introduction": "Deep neural networks (DNNs) have attained state-of-the-art results in various supervised learning tasks incomputer vision, speech recognition, and natural language processing. However, it is widely recognized thatDNNs are susceptible to minor adversarially designed perturbations to their input data, commonly regardedas adversarial attacks (Szegedy et al., 2013; Goodfellow et al., 2014). Adversarial examples are typicallygenerated by optimizing a norm-constrained perturbation leading to the maximum classification loss forinput data. A standard approach to defending against adversarial attacks is adversarial training (AT) (Madryet al., 2017), according to which the DNN classifier is learned using adversarially perturbed training data.AT methods have significantly improved the robustness of DNNs against norm-bounded perturbations. Inrecent years, several variants of AT methods have been developed to accelerate and facilitate the applicationof AT to large-scale models and datasets (Kannan et al., 2018; Shafahi et al., 2019; Wong et al., 2020). While AT algorithms offer significant robustness against standard norm-bounded attacks, the generalizationgap between their performance on training and test data has been found to be considerably greater thanthat of DNNs trained by standard empirical risk minimization (ERM) (Schmidt et al., 2018; Raghunathanet al., 2019). To study the overfitting phenomenon in adversarial training, the recent literature has focused",
  "Published in Transactions on Machine Learning Research (11/2024)": ": Robust accuracy of ResNet18 models adversarially trained by vanilla, fast, and free algorithmsagainst square attack on CIFAR10. The left figure applies L2 attacks of radius ranging from 64 to 192, andthe right figure applies L attacks of radius ranging from 1 to 9. : Robust accuracy against transferred attacks designed for another independently trained robustmodel. The left figure applies L2 attacks of radius ranging from 64 to 192, and the right figure applies Lattacks of radius ranging from 1 to 9. bers of training samples n. We randomly sampled a subset from the CIFAR-10 training dataset of sizen {10000, 20000, 30000, 40000, 50000}, and adversarially trained ResNet18 models on the subset for a fixednumber of iterations. As shown in , the generalization gap of free AT is notably decreasing fasterthan vanilla AT with respect to n, which is consistent with our theoretical analysis. More experimentalresults are discussed in the Appendix B.4. Generalization analysis of friendly adversarial training (Zhang et al., 2020) with adaptive attacksteps: We also perform numerical experiments to compare vanilla AT with friendly adversarial training(Friendly-AT) (Zhang et al., 2020), which applies an adaptive number of steps and an adaptive trainingperturbation radius train. As shown in , by choosing a sufficiently large train its generalizationimprovement over vanilla AT is similar to that of Fast-AT. On the other hand, a smaller train results inPGD AT-like numerical results, showing the trade-off in generalization-optimization accuracy explored byFriendly-AT. The theoretical analysis of Friendly-AT will be an interesting future direction for our work.",
  "j=1max h(w, ; xj, yj)": "The generalization adversarial risk Egen(w) of model parameter w is defined as the difference between pop-ulation and empirical risk, i.e., Egen(w) := R(w) RS(w). For a potentially randomized algorithm A whichtakes a dataset S as input and outputs a random vector w = A(S), we can define its expected generalizationadversarial risk over the randomness of a training set S and stochastic algorithm A, e.g. under mini-batch",
  "Adversarial Training": "In standard applications of adversarial training, the perturbation set is usually an L2-norm or L-normbounded ball of some small radius (Szegedy et al., 2013; Goodfellow et al., 2014). To robustify a neuralnetwork, the standard methodology AVanilla is to train the network with (approximately) perfectly perturbedsamples, both in practice (Madry et al., 2017; Rice et al., 2020) and in theoretical analysis (Xing et al., 2021;Xiao et al., 2022b), which is formally defined as follows:",
  ": end for": ": Robust generalization performance of the TRADES and Free-TRADES algorithms for ResNet18models adversarially trained against L2 and L attacks on CIFAR-10 and CIFAR-100. We set TRADEScoefficient = 1/6, free steps m = 4, and other details following from B.1. We run five independent trialsand report the mean and standard deviation.",
  "P(g) := arg ming 2.(2)": "Despite the significant robustness gained from AVanilla, it demands significant computational costs for train-ing. The Free adversarial training algorithm AFree (Shafahi et al., 2019) is proposed to avoid the overheadcost, by simultaneously updating the model weight parameter w when performing PGD attacks. AFree is em-pirically observed to achieve comparable robustness to AVanilla, while it can considerably reduce the trainingtime (Shafahi et al., 2019; Wong et al., 2020).",
  "Stability-based Generalization Bounds for Free AT": "In this section, we provide generalization bounds on vanilla, free, and fast adversarial training algorithms.While previous works mainly focus on theoretically analyzing the stability behaviors of vanilla adversarialtraining under the scenario that h(w, ; x) is convex in w (Xing et al., 2021; Xiao et al., 2022b), or h(w, ; x)is concave or even strongly-concave in (Lei et al., 2021; Farnia & Ozdaglar, 2021; Yang et al., 2022;Ozdaglar et al., 2022), our analysis focuses on the nonconvex-nonconcave scenario: without assumptions onthe convexity of h(w, ; x) in w or concavity of h(w, ; x) in . We defer the proof of Theorems 2 and 4 tothe Appendix A.1 and A.2. Throughout the proof, we assume that Assumptions 1 and 2 hold.Theorem 2 (Stability generalization bound of AVanilla). Assume that h(w, ) satisfies Assumptions 1 and2 and is bounded in , and the perturbation set is an L2-norm ball of some constant radius , i.e., = { : |||| }. Suppose that we run AVanilla in Algorithm 1 for T steps with vanishing step sizew,t c/t. Let constant Vanilla := c, then",
  "Vanilla": "Vanilla+1 = (1) is non-vanishing even when we are giveninfinity samples. This implication is also confirmed by the following lower bound from the work of Xinget al. (2021) and Xiao et al. (2022b):Theorem 3 (Lower bound on stability; Theorem 1 in Xing et al. (2021), Theorem 5.2 in Xiao et al. (2022b)).Suppose = { : |||| }. Assume w(S) is the output of running AVanilla on the dataset S with mini-batchsize b = 1 and constant step size w 1/ for T steps. There exist some loss function h(w, ; x) which isdifferentiable and convex with respect to w, some constant > 0, and some datasets S and S that differ inonly one sample, such that",
  ".(6)": "This lower bound indicates that AVanilla could lack stability when the attack radius = (1), hence thealgorithm may result in significant generalization error from the stability perspective. Note that the lowerbound in equation 6 is not inconsistent with Theorem 2, in which the step-size is assumed to be vanishingw,t c/t and thus the lower bound is not directly applicable under that assumption. However, this constantgeneralization gap could be reduced by free adversarial training.Theorem 4 (Stability generalization bound of AFree). Assume that h(w, ) satisfies Assumptions 1 and2 and is bounded in , and the perturbation set is an L2-norm ball of some constant radius , i.e., = { : |||| }. Suppose we run AFree in Algorithm 2 for T/m steps with vanishing step size w,t c/mtand constant step size . If the norm of gradient h(w, ; x) is lower bounded by 1/ for constant > 0with probability 1 during the training process, let constant Free := c(1 + c/m + )m1, then",
  "Free+1 /n.(8)": "Therefore, by controlling the step size of the maximization step, we can bound the coefficient Free andthus control the generalization gap of AFree, where a lower can result in a smaller generalization gap. Remark 3. One technical contribution of this work is to perform the stability analysis where the maximiza-tion variable is re-initialized after every m iterations where a new mini-batch of data is used. Our theoreticalresults suggest that as long as m is bounded by O(/c), the generalization risk will not change signifi-cantly with a greater m value, which is not implied by the standard bounds in the existing works (Farnia &Ozdaglar, 2021) in the literature. Also, our theoretical analysis considers the normalized gradient (insteadof the vanilla gradient) for the gradient ascent step of solving the maximization sub-problem and mini-batchstochastic optimization for updating min and max variables at every iteration, which are not analyzed in theprevious literature (Farnia & Ozdaglar, 2021). We note that Theorem 3 (Xing et al., 2021; Xiao et al., 2022b) gives a lower bound (T/n) on the algorithmicstability of vanilla adversarial training. On the other hand, comparing equation 8 with equation 5 suggeststhat AFree can generalize better than AVanilla for any T = O(n), since",
  "Furthermore, when T = O(n), equation 8 gives Egen(AFree) = O1/n": "Free+1, which implies that thegeneralization gap of AFree can be bounded given enough samples. If the number of iterations T is fixed, onecan see that the generalization gap of AFree has a faster convergence to 0 than AVanilla. Therefore, neuralnets trained by the free AT algorithm could generalize better than the vanilla adversarially-trained networksdue to their improved algorithmic stability. Our theoretical results also echo the conclusion in Schmidt et al.(2018) that adversarially robust generalization requires more data, since Free increases with respect to . Remark 4. We note that the Free-AT method in Algorithm 2 follows the update rule of a projected gradientdescent ascent (Projected GDA) which has been widely studied in the optimization literature. To the bestof our knowledge, a tight convergence rate for Projected GDA applied to a general nonconvex-nonconcaveoptimization problem is still an open question in the community (Lin et al., 2020; Li et al., 2022). As a tightconvergence rate for GDA nonconvex-nonconcave optimization is not available in the literature, we relied onnumerical experiments to verify the improvement in the generalization gap by Free-AT method. Note that ournumerical experiments use standard selections of stepsizes and other hyperparameters for the AT algorithms,and their numerical results suggest the standard hyperparameter selection leads to a lower generalization gapfor Free-AT compared to Vanilla-AT.",
  "We also provide theoretical analysis for the fast adversarial training algorithm AFast in Theorem 5, whoseproof is deferred to Appendix A.3": "Theorem 5 (Stability generalization bound of AFast). Assume that h(w, ) satisfies Assumptions 1 and2 and is bounded in , and the perturbation set is an L2-norm ball of some constant radius , i.e., = { : |||| }. Suppose that we run AFast in Algorithm 3 for T steps with vanishing step size w,t c/tand constant step size . If the norm of gradient h(w, ; x) is lower bounded by 1/ for some constant > 0 with probability 1 during the training process, let constant Fast := c(1 + ), then",
  "Fast+1 /n.(10)": "Therefore, by controlling the step size of the maximization step, we can bound the coefficient Fast andthus control the generalization gap of AFast, where a lower can result in a smaller generalization gap.Comparing equation 10 with equation 5 also suggests that for any T = O(n), AFast can generalize better",
  "Numerical Results": "In this section, we evaluate the generalization performance of vanilla, fast, and free adversarial trainingalgorithms in a series of numerical experiments. We first demonstrate the overfitting issue in vanilla adver-sarial training and show that free or fast algorithms can considerably reduce the generalization gap. Wedemonstrate that the smaller generalization gap could translate into greater robustness against score-basedor transferred black-box attacks. To examine the advantages of free AT, we also study the generalizationgap for different numbers of training samples. Experiment Settings: We conduct our experiments on datasets CIFAR-10, CIFAR-100 (Krizhevsky &Hinton, 2009), Tiny-ImageNet (Le & Yang, 2015), and SVHN (Netzer et al., 2011). Following the standardsetting in Madry et al. (2017), we use ResNet18 (He et al., 2016) for CIFAR-10 and CIFAR-100, ResNet50for Tiny-ImageNet, and VGG19 (Simonyan & Zisserman, 2014) for SVHN to validate our results on a diverseselection of network architectures. For vanilla adversarial training algorithm, since the inner optimization",
  "LTrain Acc.95.696.070.336.952.943.236.732.2Test Acc.20.020.317.315.022.424.524.924.5Gen. Gap75.675.753.021.930.518.711.87.7": "task max h(w, ; x) is computationally intractable for neural networks which are generally non-concave,we apply standard projected gradient descent (PGD) attacks (Madry et al., 2017) as a surrogate adversary.For free and fast algorithms, we adopt AFree and AFast defined in Algorithms 2 and 3, following from Shafahiet al. (2019); Wong et al. (2020). Robust Overfitting during Training Process: We applied L2-norm attack of radius = 128/255 andL-norm attack of radius = 8/255 to adversarially train ResNet18 models on CIFAR-10. For the vanillaalgorithm, we used a PGD adversary to perturb the image. For the free algorithm, we applied the learningrate of adversarial attack = with free step m as 2, 4, 6, 8, and 10.1 The other implementation detailsare deferred to Appendix B.1. We trained the models for 200 epochs and after every epoch, we tested themodels robust accuracy against a PGD adversary and evaluated the generalization gap. The numericalresults on CIFAR-10 and CIFAR-100 are presented in , and the training curves are plotted in . Further numerical results on SVHN and Tiny-ImageNet are deferred to in Appendix B.1. Based on the empirical results, we observe the significant overfitting in the robust accuracy of the vanillaadversarial training: the generalization gap is above 30% against L2 attack and 50% against L attack.On the other hand, the free AT algorithm has less severe overfitting and reduced the generalization gap to20%. Although the free AT algorithm applies a weaker adversary, it achieves comparable robustness on testsamples to the vanilla AT algorithm against the PGD attacks by lowering the generalization gap. Additionalnumerical results for different numbers of free AT steps and on other datasets are provided in Appendix B.1. Robustness Evaluation Against Black-box Attacks: To study the consequences of the generalizationbehavior of the free AT algorithm, we evaluated the robustness of the adversarially-trained networks againstblack-box attack schemes where the attacker does not have access to the parameters of the target models(Bhagoji et al., 2018). We applied the square attack (Andriushchenko et al., 2020), a score-based methodologyvia random search, to examine networks adversarially trained by the discussed algorithms as shown in . We also used adversarial examples transferred from other independently trained robust models as shownin . More experiments on different datasets are provided in Appendix B.2. We extensively observe the improvements of the free algorithm compared to the vanilla algorithm againstdifferent black-box attacks, which suggests that its robustness is not gained from gradient-masking (Athalyeet al., 2018) but rather attributed to the smaller generalization gap. Furthermore, our numerical findingsin Appendix B.3 indicate that the adversarial perturbations designed for DNNs trained by free AT couldtransfer better to an unseen target neural net than those found for DNNs trained by vanilla and fast AT.",
  "Generalization Gap for Different Numbers of Training Samples: To examine our theoretical re-sults in Theorems 2 and 4, we evaluated the robust generalization loss with respect to different num-": "1Throughout this work, we use Vanilla-m to denote the vanilla AT algorithm with m PGD-adversary iterations, andVanilla without specification means Vanilla-10 by default. We also use Free-m to denote the free AT algorithm with mfree steps, and Free without specification means Free-4 by default. It is worth noting that Vanilla-1 is equivalent to fast ATalgorithm.",
  "FreeTRADES": "Our theoretical results suggest that the improved generalization in the free AT algorithm could follow fromits simultaneous min-max optimization updates. A natural question is whether we can extend these resultsto other adversarial training methods. Here we propose the FreeTRADES algorithm, a combination ofthe free AT algorithm and another well-established adversarial learning algorithm, TRADES (Zhang et al.,2019), and numerically evaluate the proposed FreeTRADES method.",
  "nnj=1 max h(w, ; xj, yj). Therefore,a natural idea gained from our theoretical analysis is to apply simultaneous updates to the adversarial attack and model weight w, which is stated in Algorithm 4": "We performed several numerical experiments to compare the performance of TRADES and FreeTRADESalgorithms.The results demonstrated in show that FreeTRADES could considerably improvethe generalization gap while attaining a comparable (sometimes better) test performance to TRADES,which indicates that other adversarial training algorithms different from vanilla AT can also benefit fromsimultaneous optimization updates. Furthermore, we note that Free-AT and FreeTRADES also have abetter generalization performance on the clean data than Vanilla-AT and TRADES. Our theoretical resultssuggest that the training process of free AT is algorithmically more stable than vanilla AT, therefore the freeAT could be similarly expected to generalize better on clean data than vanilla AT. The numerical results",
  "Conclusion": "In this work, we studied the role of min-max optimization algorithms in the generalization performanceof adversarial training methods.We focused on the widely-used free adversarial training method and,leveraging the algorithmic stability framework we compared its generalization behavior with that of vanillaadversarial training. Our generalization bounds suggest that not only can the free AT approach lead to afaster optimization compared to the vanilla AT, but also it leads to a lower generalization gap between theperformance on training and test data. We note that our theoretical conclusions are based on the upperbounds following the algorithmic stability-based generalization analysis, and an interesting topic for futurestudy is to prove a similar result for the actual generalization gap under simple linear or shallow neuralnet classifiers. Another future direction could be to extend our theoretical analysis of the simultaneousoptimization updates to other adversarial training methods such as ALP (Kannan et al., 2018). The work of Farzan Farnia is partially supported by a grant from the Research Grants Council of the HongKong Special Administrative Region, China, Project 14209920, and is partially supported by a CUHK DirectResearch Grant with CUHK Project No. 4055164.",
  "Maksym Andriushchenko and Nicolas Flammarion. Understanding and improving fast adversarial training.Advances in Neural Information Processing Systems, 33:1604816059, 2020": "Maksym Andriushchenko, Francesco Croce, Nicolas Flammarion, and Matthias Hein. Square attack: a query-efficient black-box adversarial attack via random search. In European conference on computer vision, pp.484501. Springer, 2020. Sanjeev Arora, Rong Ge, Yingyu Liang, Tengyu Ma, and Yi Zhang.Generalization and equilibrium ingenerative adversarial nets (gans). In International conference on machine learning, pp. 224232. PMLR,2017. Anish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients give a false sense of security:Circumventing defenses to adversarial examples. In International conference on machine learning, pp.274283. PMLR, 2018.",
  "Yu Bai, Tengyu Ma, and Andrej Risteski. Approximability of discriminators implies diversity in gans. arXivpreprint arXiv:1806.10586, 2018": "Raef Bassily, Vitaly Feldman, Cristbal Guzmn, and Kunal Talwar. Stability of stochastic gradient descenton nonsmooth convex losses. Advances in Neural Information Processing Systems, 33:43814391, 2020. Arjun Nitin Bhagoji, Warren He, Bo Li, and Dawn Song. Practical black-box attacks on deep neural networksusing efficient query mechanisms. In Proceedings of the European conference on computer vision (ECCV),pp. 154169, 2018.",
  "Yunwen Lei. Stability and generalization of stochastic optimization with nonconvex and nonsmooth problems.In The Thirty Sixth Annual Conference on Learning Theory, pp. 191227. PMLR, 2023": "Yunwen Lei, Zhenhuan Yang, Tianbao Yang, and Yiming Ying. Stability and generalization of stochasticgradient methods for minimax problems. In International Conference on Machine Learning, pp. 61756186.PMLR, 2021. Haochuan Li, Farzan Farnia, Subhro Das, and Ali Jadbabaie. On convergence of gradient descent ascent: Atight local analysis. In International Conference on Machine Learning, pp. 1271712740. PMLR, 2022.",
  "Leslie Rice, Eric Wong, and Zico Kolter. Overfitting in adversarially robust deep learning. In InternationalConference on Machine Learning, pp. 80938104. PMLR, 2020": "Ludwig Schmidt, Shibani Santurkar, Dimitris Tsipras, Kunal Talwar, and Aleksander Madry. Adversariallyrobust generalization requires more data. Advances in neural information processing systems, 31, 2018. Ali Shafahi, Mahyar Najibi, Mohammad Amin Ghiasi, Zheng Xu, John Dickerson, Christoph Studer, Larry SDavis, Gavin Taylor, and Tom Goldstein. Adversarial training for free! Advances in Neural InformationProcessing Systems, 32, 2019.",
  "Jiancong Xiao, Ruoyu Sun, and Zhi-Quan Luo. Pac-bayesian spectrally-normalized bounds for adversariallyrobust generalization. Advances in Neural Information Processing Systems, 36:3630536323, 2023b": "Jiancong Xiao, Qi Long, Weijie Su, et al. Bridging the gap: Rademacher complexity in robust and standardgeneralization. In The Thirty Seventh Annual Conference on Learning Theory, pp. 50745075. PMLR,2024a. Jiancong Xiao, Jiawei Zhang, Zhi-Quan Luo, and Asuman E. Ozdaglar. Uniformly stable algorithms foradversarial training and beyond. In Forty-first International Conference on Machine Learning, 2024b.URL",
  "Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146, 2016": "Bohang Zhang, Du Jiang, Di He, and Liwei Wang.Rethinking lipschitz neural networks and certifiedrobustness: A boolean function perspective.Advances in neural information processing systems, 35:1939819413, 2022. Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric Xing, Laurent El Ghaoui, and Michael Jordan. Theo-retically principled trade-off between robustness and accuracy. In International conference on machinelearning, pp. 74727482. PMLR, 2019. Jingfeng Zhang, Xilie Xu, Bo Han, Gang Niu, Lizhen Cui, Masashi Sugiyama, and Mohan Kankanhalli.Attacks which do not kill training make adversarial learning stronger. In International conference onmachine learning, pp. 1127811287. PMLR, 2020.",
  "thus completes the proof": "Another important observation is that similar to Lemma 3.11 in Hardt et al. (2015), mini-batch gradientdescent typically makes several steps before it encounters the one example on which two datasets in stabilityanalysis differ. Lemma 2. Suppose hmax(w; x) := max h(w, ; x) is bounded as hmax .By applying mini-batchgradient descent for two datasets S, S that only differ in only one sample s, denote by wt and wt the outputafter t steps respectively and define d(w)t:= wt wt. Then for any x and t0,",
  "nw,tL": "Proof. We first bound the difference between j and j.At step t, let Bt, Bt denote the mini-batchesrespectively. By equation 26, we have (g) = P(g) if ||g|| 1/. Since is convex, the projectionP() is 1-Lipschitz. So for all j such that xj B we havej j =P(j + (gj)) P(j + (gj))",
  "B.1Robust Overfitting During Training Process": "Implementation Details:For the training process of networks, we follow the standards in the literature(Madry et al., 2017; Rice et al., 2020). We apply mini-batch gradient descent with batch size b = 128.Weight decay is set to be 2104. We adopt a piecewise learning rate decay schedule, starting with 0.1 anddecaying by a factor of 10 at the 100th and 150th epochs, for 200 total epochs. For the vanilla algorithm,we apply PGD adversaries with 1, 7, or 10 iterations and set the step size as , /4, or /4 respectively. Forthe free algorithm, since it repeats m iterations at each step, we use 200/m epochs to match their trainingiterations for fair comparison. For the adversaries, we consider both L2-norm attack of radius = 128/255,and L-norm attack of radius = 8/255 (except for Tiny-ImageNet = 4/255). We extend the experiments in using different free steps m, various neural network architectures, andother datasets. We use ResNet18 for CIFAR-10 and CIFAR-100, ResNet50 for Tiny-ImageNet, and VGG19for SVHN. We apply both L2 and L adversaries and provide the plots of training curves for CIFAR-10 andCIFAR-100 in . We can observe that the vanilla training suffers from robust overfitting, while thefree algorithm with moderate free steps generalizes better and achieves comparable robustness to the vanillaalgorithm.",
  "(c) Accuracy of adversarially trained ResNet50 models against L2 and L transferred attacks on Tiny-ImageNet": ": Robust accuracy of models adversarially trained by vanilla, fast, and free algorithms againsttransferred attacks designed for other independently trained robust models on CIFAR-10, CIFAR-100, andTiny-ImageNet. The left figure applies L2 attacks of radius ranging from 64 to 192, and the right figureapplies L attacks of radius ranging from 1 to 9.",
  "B.3Transferability": "We further investigated the transferability of the adversarial examples designed for the models trained bythe mentioned adversarial training algorithms. We computed the adversarial perturbations designed for therobust models and used them to attack other standard ERM-trained models. We test the transferabilityof models trained by vanilla, fast, and free algorithms. In , we transfer the attacks to a standardResNet18 model on CIFAR-100 and a standard ResNet50 model on Tiny-ImageNet. In , we transferthe attacks to various standard models including ResNet18, ResNet50, and Wide-ResNet34 (Zagoruyko &Komodakis, 2016) on CIFAR-10. Our numerical results suggest that the better generalization performance of the free algorithm could resultin more transferable adversarial perturbations, which could be more detrimental to the performance of otherunseen neural network models.",
  "(b) Test accuracy of a standard ResNet50 target model against transferred attacks from adversariallytrained models on Tiny-ImageNet": ": Test accuracy of standard ResNet18 and ResNet50 target models against transferred attacks frommodels adversarially trained by vanilla, fast, and free algorithms on CIFAR-100 and Tiny-ImageNet. Theleft figure applies L2 attacks of radius ranging from 64 to 192, and the right figure applies L attacks ofradius ranging from 4 to 12.",
  "(c) Test accuracy of a standard Wide-ResNet34 target model against transferred attacks from adversar-ially trained ResNet18 models": ": Test accuracy of standard ResNet18, ResNet50, and Wide-ResNet34 target models against trans-ferred attacks from ResNet18 models adversarially trained by vanilla, fast, and free algorithms on CIFAR-10.The left figure applies L2 attacks of radius ranging from 64 to 192, and the right figure applies L attacksof radius ranging from 4 to 12.",
  "B.4Generalization Gap for Different Numbers of Training Samples": "We perform additional experiments to study the relationship between the number of training samples andthe generalization gap. We randomly sampled a subset from the CIFAR-10 and CIFAR-100 training datasetsof size n {10000, 20000, 30000, 40000, 50000}, and adversarially trained ResNet18 models on the subset fora fixed number of iterations. The results are demonstrated in .",
  "B.5Hyperparameter Choices and Generalization Behavior with Early Stopping": "For the fast adversarial training algorithm, we applied the learning rate of adversarial attack = 7/255for L attack and = 64/255 for L2 attack over 200 training epochs, following from Andriushchenko &Flammarion (2020). We note that our hyperparameter selection is different from the original implementationof fast AT (Wong et al., 2020), which applied the attack step size = 10/255 for L attack over 15 trainingepochs with early stopping. We clarify that we do not use early stopping in our implementation to test thegeneralization behavior of fast AT over the same 200 number of epochs as we choose for PGD and free AT,where the choice of 200 epoch numbers follows from the reference Rice et al. (2020) studying overfittingin adversarial training. Regarding the choice of attack step size, we observed that setting the fast attacklearning rate as = 10/255 will lead to fast ATs catastrophic overfitting over the course of 200 trainingepochs. The learning curves of fast AT with learning rate = 7/255 and = 10/255 for L attack over200 training epochs are demonstrated in . The catastrophic overfitting phenomenon in fast AT hasbeen similarly observed and reported in the literature (Andriushchenko & Flammarion, 2020; Kim et al.,2021; Huang et al., 2023).",
  ": Learning curves of fast AT with attack learning rate = 7/255 and = 10/255 for a ResNet18model adversarially trained against L attacks on CIFAR-10": "We also note that as the stability framework gives a generalization bound in terms of the min and maxsteps, our theoretical analysis is also applicable if early stopping is employed. From the learning curves in, we note that throughout the whole training process, the generalization performance of free ATis consistently better than vanilla AT, which is consistent with our theoretical results. For instance, reports the generalization gaps of the models trained by vanilla, fast (with = 10/255), and free ATalgorithms if the training process is stopped at the 50th epoch. : Robust training, testing, and generalization performance of the models trained by vanilla, fast(with = 10/255), and free AT algorithms against L attack on CIFAR-10, where the training process isstopped at the 50th epoch.",
  "B.6Soundness of Lower-bounded Gradient Norm Assumption in Theorem 4": "In Theorem 4 we make an assumption that the norm of gradient h(w, ; x) is lower bounded by 1/for some constant > 0 with probability 1 during the training process.We note that the assumptionis only required for the points within the -distance from the training data. To address the soundness ofthis assumption, we have numerically evaluated the gradient norm over the course of free-AT training onCIFAR-10 and CIFAR-100 data, indicating that the minimum gradient norm on training data is constantlylower-bounded by O(103) in those experiments, i.e., = O(103) is an upper bounded constant. We trained ResNet18 networks on CIFAR-10 and CIFAR-100 datasets, applying L2 adversary and setting thefree step m as 4 and 6, and we recorded the gradient norm h(w, j; xj, yj)2 of every sample throughoutthe training process. The heatmaps are plotted in ."
}