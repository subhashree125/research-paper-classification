{
  "Abstract": "Regression is a powerful tool to accurately predict the outcome metric of a system given aset of parameters, but has traditionally been restricted to methods which are only appli-cable to a specific task. In this paper, we propose OmniPred, a framework for traininglanguage models as universal end-to-end regressors over (x, y) data from arbitrary formats.Using data sourced from Google Vizier, one of the largest proprietary blackbox optimizationdatabases in the world, our extensive experiments demonstrate that language models arecapable of very precise numerical regression using only textual representations of mathe-matical parameters and values, and if given the opportunity to train at scale over multipletasks, can significantly outperform traditional regression models.",
  "Introduction": "Regression is a fundamental task for experimental design, in many domains such as hyperparameter tuning,computer software, industrial engineering, and chemical discovery. The goal of regression is to predict ametric y of a general system given a set of input features x. Such regressors can later be used for variousapplications, such as offline optimization (Kumar et al., 2022; Trabucco et al., 2022), online optimization (Caiet al., 2020), low-cost benchmarking (Zela et al., 2022; Eggensperger et al., 2015) and simulation (Mendiset al., 2019; Hashemi et al., 2018; Kaufman et al., 2021).",
  ": Overview of our method. Using heterogenous offline (x, y) evaluation data collected from a varietyof sources, we train a LM-based regressor": "In recent years, large language models (LLMs) have emerged as powerful tools for processing textual rep-resentations at scale over massive heterogeneous datasets to represent complex relationships between inputfeatures and output labels. Given that LLMs have been shown to be effective for a variety of tasks beyondnatural language processing, such as coding (Li et al., 2022), symbolic mathematics (Lewkowycz et al., 2022),and scientific reasoning (Singhal et al., 2022), it is reasonable to wonder: Can language models be used fornumeric regression?",
  "Published in Transactions on Machine Learning Research (12/2024)": "Yujia Li, David H. Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rmi Leblond, Tom Ec-cles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien de Mas-son dAutume, Igor Babuschkin, Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, AlexeyCherepanov, James Molloy, Daniel J. Mankowitz, Esme Sutherland Robson, Pushmeet Kohli, Nandode Freitas, Koray Kavukcuoglu, and Oriol Vinyals. Competition-level code generation with alphacode.CoRR, abs/2203.07814, 2022.",
  "Related Work and Motivation": "Traditional regression methods have widely used statistical techniques such as Gaussian Processes (GPs),tree-based methods, and multilayer perceptrons (MLPs), to predict a scalar objective given a fixed-lengthfeature vector, commonly seen in tabular data settings. Multitask (Bonilla et al., 2007) and contextual(Krause & Ong, 2011) variants have been further proposed for transfer learning purposes, but still requirefixed-length tensor representations of x, and can thus only use previous x from the same input space.Additional recent works utilizing deep learning-based regressors include Transformers (Hollmann et al.,2023; Huang et al., 2020; Garg et al., 2022), recurrent neural networks (Hashemi et al., 2018), graph neuralnetworks (Lukasik et al., 2020; Gao et al., 2023), and deep-hierarchical GPs (Fan et al., 2024), which allowlength-independence. Even so, a frequent issue is still the reliance on tensor representations of (x, y). Tensor representations are commonly problem-dependent, where each tensor element may need to be in areasonable numerical range (e.g. in ) as inputs to a model. Thus to represent x, every categoricalfeature must be one-hot embedded against user-provided choices, and scalar features may need to be rescaledagainst user-provided bounds. Dynamic yet minor input space changes such as new bounds or additionalcategories, are incompatible with this static representation. To represent y, a raw objective in R may alsoneed to be rescaled, which can be problematic at test-time when encountering outlier y-values. Dealingwith this issue leads to implementing complicated nonlinear warpings (Daimon, 2011; Yeo & Johnson, 2000),many of which are also data-dependent (e.g. require storing min/max values from training data).",
  ": Comparisons between the flexibilties of different typical regressors": "These issues are summarized in . In principle, an ideal regressor should process x and output y, bothin absolute terms, independent of changing external statistics or search constraints. For example, if theunderlying relationship is y = x2, then the regressors prediction at x = 2 should be the same regardless ifthe constraint is x or x . One way to accomplish this is to represent x with an independentuniversal tokenizer without problem-specific numeric transformations (Zhou et al., 2023). This immediately",
  "unlocks a large amount of transferrability when dealing with variable-length inputs and additional contextualmetadata": "Previous works in the token-based, or effectively text-to-text paradigm, have mostly been in reinforcementlearning from human feedback (Ziegler et al., 2019), where regression over textual responses (the x), alsoknown as reward modelling, has been crucial to the success of recent interactive LLMs such as ChatGPT(OpenAI, 2022) and Gemini (Google, 2024). While such works have shown that LLMs are able to imitatehuman ratings in the form of ordinal variables and their softmax probabilities (Bradley & Terry, 1952), theyhave not shown whether LLMs are capable of regression over precise numeric-based data where y R. This is because the overwhelming current focus has been on subjective human-based feedback needed fordetermining aspects such as creativity, safety, and personality, which contain high aleatoric uncertainty anddo not require high-precision measurements. Much less attention has been given towards feedback fromcomplex and natural systems common to experimental design.Given multiple works (Hendrycks et al.,2021; Nogueira et al., 2021) demonstrating their brittle and unreliable numeric abilities, it is non-obviousthat language models are capable of high-precision numerical prediction over token-based representations.This is a crucial technical challenge which our paper resolves in the quest for a general-purpose predictor.",
  "Preliminaries and Problem Definition": "Since regression is commonly used in blackbox optimization settings, we adopt standard terminology (Golovinet al., 2017; Liaw et al., 2018) from the field. For a given task T = (X, f, D, m), we assume there is a mappingf : X R for which we obtain trials (x, y) from evaluating an input x, selected from a (possibly implicit)input space X. We define a study as an offline collection of trials D = {xs, ys}Ts=1. To distinguish betweendifferent tasks, there may be observable task-level metadata m, which can additionally characterize the taskand potentially even describes the behavior of the corresponding mapping f(). The goal of standard regression is to obtain a distribution mapping s : X P(R) such that s(x) accuratelyapproximates f(x) over some distribution of inputs x X, provided that a training set Dtrain is given. Inour particular case, we also provide our language model with multi-task training data {Dtrain1, Dtrain2, ...}from other tasks {T1, T2, ...}. While these extraneous tasks contain different objectives f1, f2, . . . and mayeven have different input spaces from each other, training on such additional extraneous data may still leadto transferrability, especially for similar tasks. A common way to measure the accuracy of predictors (deterministic or probabilistic) is to compute thegap between a final pointwise prediction against the true objective value y.For probabilistic regressorss : X P(R), we may aggregate by e.g. taking the median or mean of the distribution. Since differentstudies can have vastly different objective scales (e.g. CIFAR10 accuracies are within while syntheticobjectives are within ), we must therefore normalize the difference based on per-study statistics, i.e.for a specific task, we define the study error as a normalized mean absolute error (MAE):",
  "Language Model": "We use a standard language model in which the model observes a prompt and decodes a response. For theregression setting, naturally these correspond to x and y respectively. However, in order to allow multi-tasktraining, the task-specific metadata m must be appended to the prompt in order to distinguish betweendifferent tasks, and thus for a given task, the prompt is actually (x, m). For simplicity, we train a relatively small 200M parameter T5 encoder-decoder (Raffel et al., 2020) fromscratch to avoid any confounding effects from typical generative language pre-training. We wish to learn a",
  ": Textual representations used for OmniPred. <> represents a single custom token. Input spaceand x is the same as in . Example y-tokenization represents a value of 725 101 = 72.5": "Training: We apply standard Prefix-LM training (Raffel et al., 2020), in which for a given (prompt, response)pair, cross-entropy losses are only computed over response tokens. Pairs are sampled from training data overmultiple tasks. One could additionally make the loss more metric-aware by weighting specific tokens oreven reinforce with non-differentiable scores, although we maintain simplicity in this paper for now by usinguniform cross-entropy. Thus the model will implicitly learn numeric distances from training data. Sampling and Decoding: Through regular temperature decoding, we can repeatedly sample y s(x),to approximate the prediction distribution over R. To remain robust to strong outliers, instead of usingempirical mean, our Aggregate() function is defined as the empirical median, since we found it leads tolower error from ablations in .1. Since the model may need to predict over unseen regions of theinput space, we can also assess the models uncertainty by observing the concentration of sampled predictionsy and additionally specific log probabilities across every decoded token. Online Finetuning: To adapt to an unseen task Tu common for Bayesian optimization settings (Wistuba& Grabocka, 2021), the model can further be quickly finetuned online over the taskss corresponding trainingdata Dtrainu, optionally using LoRA (Hu et al., 2022). Finetuning may also help to refocus over seen data,when the model is not fully optimized against a specific study, e.g. if the pretraining dataset was too large.",
  "Data": "Many industries possess large collections of metric data from experiments or systems. However, such datais typically not open-sourced as official training datasets for research. A natural dataset to use may comefrom multiple hyperparameter optimization trajectories, which tend to have (x, y) evaluations from expensiveexperiments, expressed as blackbox objectives y = f(x).",
  ": Common example of a (possibly nested) space and suggestions x in OSS Vizier": "Task-level metadata m consists of a title, username, description, objective name, and optional free-form text.Since the OSS Vizier API is meant to provide an optimization service for users, there can be many sourcesof transferrability due to user-specific settings. These include: A single user or team regularly tuning similar experiments. Multiple different users tuning similar experiments (e.g. training ResNets on CIFAR10). Similar parameters used across different experiments (e.g. learning rate). Metadata m describing the nature of the objective function.",
  "Datasets": "BBOB (Shifted): For precise controlled experiments where we can generate synthetic datasets and per-form online evaluations, we create a multi-task version of the BBOB benchmark (ElHara et al., 2019)containing 24 different synthetic functions, by applying random domain shifts c to transform a vanilla syn-thetic f(x) into f(x c), and ranging the dimension over . Thus each task T is parameterized bym = (function class, dimension, shift), and the corresponding objective can be seen as f(x, m), allowingevaluation over unseen m. For a specific task Ti, we minimize the in-study training data size Dtrainibutfreely vary inter-study training data {Dtrainj}j=i from different tasks {Tj}=i. Thus traditional regressors(e.g. MLPs) which can only train from a single Dtrainiwill struggle to regress the corresponding fi underthis limited data condition. In contrast, the LM may perform better as it will have seen trials from othertasks whose functions share similarities with fi.PropertyStatistic",
  "# StudiesO(70M+)# TrialsO(120B+)# Distinct UsersO(14K)": ": Relevant statistics on the real world database.We provide order estimates as there may be numerousways to define e.g. legitimate studies or trials. SeeAppendix D for further details and data breakdown. Real World Data: To investigate metric predic-tion over a rich variety of tasks, we will use datacollected by Google Vizier (Golovin et al., 2017).Because we are not limited to training on fully com-pleted trajectories over flat input spaces, we cantrain on more data than just the 750K studies usedin the OptFormer (Chen et al., 2022), as seen from.",
  "Space size: Approximate cardinality of a space X is exponential with respect to parameter count,and thus large input spaces will naturally be less explored": "While we apply practical processing steps such as (1) setting a maximum initial trial limit per study and (2)randomly shuffling the trials and then (3) deciding on a fixed train/validation/test splitting ratio (default0.8/0.1/0.1), we cannot fully control whether each D saturates its space X, or essentially how easy thetask is. Instead, we use a baseline regressor trained only on Dtrain and evaluated on corresponding Dtest asa proxy metric of the difficulty of the task.",
  "Simultaneous Regression": "In , we visually present how a BBOB-trained model captures the overall shape of analytical functionsof vastly different objective scales with high precision. Furthermore, the model is capable of expressinguncertainty estimates via i.i.d. prediction samples. StepEllipsoidal-4D Shifts: [-0.50,1.04,-1.79,3.38]x1=-3.78, x2=3.29, x3=-1.29 StepEllipsoidal-4D Shifts: [-0.50,1.04,-1.79,3.38] x0=3.65, x2=2.57, x3=0.30 StepEllipsoidal-4D Shifts: [-0.50,1.04,-1.79,3.38]x0=-0.04, x1=-0.99, x3=-4.18 StepEllipsoidal-4D Shifts: [-0.50,1.04,-1.79,3.38]x0=-1.16, x1=2.87, x2=3.00 Weierstrass-4D Shifts: [-3.79,2.68,-3.78,-1.62] x1=0.12, x2=4.11, x3=0.22 Weierstrass-4D Shifts: [-3.79,2.68,-3.78,-1.62] x0=4.02, x2=4.38, x3=2.48 Weierstrass-4D Shifts: [-3.79,2.68,-3.78,-1.62]x0=-0.68, x1=-0.69, x3=4.20 Weierstrass-4D Shifts: [-3.79,2.68,-3.78,-1.62] x0=2.24, x1=1.61, x2=4.11 x0 RosenbrockRotated-4D Shifts: [3.54,3.74,2.63,2.57]x1=-1.76, x2=-0.40, x3=-1.61 x1 RosenbrockRotated-4D Shifts: [3.54,3.74,2.63,2.57]x0=-0.92, x2=-1.58, x3=0.79 x2 RosenbrockRotated-4D Shifts: [3.54,3.74,2.63,2.57]x0=-0.74, x1=-1.29, x3=1.13 x3 RosenbrockRotated-4D Shifts: [3.54,3.74,2.63,2.57]x0=0.45, x1=0.77, x2=1.59 TargetPrediction Objective Value (y) : Model prediction samples over selected 4D BBOB functions with unseen shifts. Empirical mode (bolded)and min/max are shown from 10 samples. Over all BBOB functions, we vary the coordinate value xi while keepingothers xj=i fixed. In for a model trained over real world data, we present an analogous visualization over hand-selected studies with drastically different input spaces, representative of objectives tuned commonly in realworld settings. These include standard machine learning (e.g. image classification and language modeling),",
  "Multi-task Transferrability": "In this subsection, we demonstrate the models ability to transfer learn, i.e. improve accuracy over a specifictask using knowledge gained from other similar but non-equivalent tasks, in contrast to single-task regres-sors (described in Appendix C) which only observe training data from the task being evaluated. Note thatsingle-task baselines such as MLPs are incapable of multi-task training when tasks have different dimension-alities. In , we clearly see that the models accuracy improves with more tasks seen in training and eventu-ally outperforms all traditional baselines. For AutoML studies, the error is averaged from a fixed subset ofencountered studies. For BBOB, we can further demonstrate the models inter-study generalization capa-bilities over metadata m (as opposed to x) by evaluating on unseen tasks with new shifts not encounteredduring training. # of Training Studies 0.15 0.20 0.25 0.30 0.35",
  ": Lower () is better. Comparisons betweenmodels trained on original vs anonymized data, acrossBBOB-Shifted and AutoML test trials. FAIL meansthe model failed to even train": "To verify whether the model is performing trans-fer learning by reading textual cues, in we compare results against the case when data isanonymized using a study-dependent hash func-tion. For BBOB, we hash metadata m which orig-inally displayed (function class, dimension, shift).For AutoML, we hash parameter names and stringvalues. Each study can still be uniquely identifiedand trained on, but the model can no longer ob-serve useful correlations from common textual clues.Interestingly, the model fails to train over the fullanonymized BBOB dataset, a case when the data istoo large and heterogeneous. In , we further see that for the model, multi-task training consistently improves over single-tasktraining, and in regimes with relatively lower input space saturation (i.e.low trial to parameter countratio) from training data, multi-task models outperform traditional baselines over several different domains.Interestingly, a single-task model trained from scratch remains a competitive choice and for certain domainssuch as AutoML, can even outperform all other single-task baselines. We hypothesize this is due to language-based representations being more appropriate for the conditional structures of these domains (e.g. AutoML). BBOBBid SimulationAutoMLInit2WinitProtein DesignV-AI (Tab)V-AI (Text)",
  ": Lower () is better. Mean study errors ofpretrained models and their corresponding finetunedversions": "We first examine the conditions in which finetun-ing may be beneficial. In , we finetune vari-ous pretrained models over AutoML studies. Whilethere is negligible benefit in finetuning the AutoMLmodel on its data again, we see that a model pre-trained over the entire real world dataset is ableto finetune to the same level of accuracy as a pre-trained AutoML model, while a BBOB-pretrainedmodel leads to significantly worse results than evena single-task model. This suggests that knowledgeobtained from pretraining can have a large (positive or negative) influence on transferrability over specificdomains such as AutoML. We further examine this effect by evaluating over unseen tasks, i.e. those which were newly created afterthe original training set was scraped, and can contain studies from new users and objectives. In ,",
  "Effect of Sampling": "The LM can output extreme outliers in its y-prediction, usually due to an inaccurate prediction on theexponent token or significant digits. While such issues do not occur once the model has nearly perfectlyregressed on a task (e.g. BBOB), they occur frequently on realistic tasks with nontrivial error (e.g. AutoML)and thus require techniques for correction. One obvious method to reduce error is to increase sample count,as demonstrated in . # of Samples 0.00 0.05 0.10 0.15 0.20 0.25 0.30",
  "Uncertainty Calibration": "Although the main metric used throughout our work is based on pointwise prediction, an important abilityfor regressors is to express uncertainty when they are unable to provide an accurate prediction. This isparticularly useful in applications such as Bayesian optimization, where uncertainty can be used as anexploration proxy. In this section, we examine whether the model can quantify uncertainty even if we didnot calibrate or tune any of the models for such purposes. StepEllipsoidal-4D Shifts: [-0.50,1.04,-1.79,3.38]x1=-0.79, x2=0.47, x3=-4.18 StepEllipsoidal-4D Shifts: [-0.50,1.04,-1.79,3.38]x0=-1.16, x2=4.00, x3=0.00 StepEllipsoidal-4D Shifts: [-0.50,1.04,-1.79,3.38]x0=-1.33, x1=-0.73, x3=-4.22 StepEllipsoidal-4D Shifts: [-0.50,1.04,-1.79,3.38]x0=3.15, x1=-0.84, x2=0.53 DifferentPowers-4D Shifts: [1.88,-0.94,1.74,-3.89]x1=0.19, x2=-1.86, x3=2.87 DifferentPowers-4D Shifts: [1.88,-0.94,1.74,-3.89]x0=-2.48, x2=-2.07, x3=-1.04 DifferentPowers-4D Shifts: [1.88,-0.94,1.74,-3.89] x0=0.68, x1=3.71, x3=1.52 DifferentPowers-4D Shifts: [1.88,-0.94,1.74,-3.89] x0=1.84, x1=4.10, x2=0.78 Weierstrass-4D Shifts: [-3.79,2.68,-3.78,-1.62] x1=-4.38, x2=2.44, x3=4.20 Weierstrass-4D Shifts: [-3.79,2.68,-3.78,-1.62] x0=2.24, x2=4.40, x3=3.80 Weierstrass-4D Shifts: [-3.79,2.68,-3.78,-1.62] x0=0.24, x1=-4.55, x3=4.87 Weierstrass-4D Shifts: [-3.79,2.68,-3.78,-1.62] x0=2.65, x1=-0.94, x2=1.54 Objective Value (y)",
  "Pearson, Kendall-Tau, SpearmanRegressorUncertainty MetricAutoMLBBOB": "Gaussian ProcessPredicted SD0.254, 0.230, 0.3070.018, 0.068, 0.048LM w/ mean aggregationSample SD0.560, 0.487, 0.6250.360, 0.366, 0.454LM w/ median aggregationHarrell-Davis SE0.525, 0.412, 0.5390.360, 0.293, 0.380 : Higher () is better. Rank correlation between quantified uncertainty (SD = standard deviation,SE = standard error) and actual error over studies with at least 10 test trials (all BBOB studies and 641AutoML studies)",
  "Study Size vs. Multi-task Gain": "Intuitively, as a tasks space becomes more saturated with trials, single-task training becomes more sufficientfor accurate prediction. In , we plot the gains from multi-task LM training over the single-task MLPbaseline to validate this hypothesis. Gains are maximized at roughly 50 training trials and diminish as thenumber of training trials increases. Note that maximal gains do not occur with 0 trials, as presumablysome training trials are still needed to identify the structure of a task. # Training Trials 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00 (MLP Multitask LM) Error",
  "Discussion: Limitations and Extensions": "In this work, our emphasis was to demonstrate the promise of applying language modeling to general-purposeregression, and thus our design choices remained relatively simple to avoid confounding factors. We list somelimitations of our specific design, which opens many more potential areas of exploration. In-Context Learning (ICL): By design, we maximized the allowed prompt length to allow flexibility inrepresenting (x, m) and thus did not use in-context regression, since a single (x, m) prompt could be 10K+tokens long for some applications. While online finetuning in principle allows infinite amounts of data to beabsorbed at inference time, it is worth investigating ICL methods which allow arbitrarily long prompts aswell. Current methods (Chen et al., 2022) which require significant input compression are only applicable totabular-like xs. Additional use of ChatGPT and other service-based chat APIs (Vacareanu et al., 2024) relyon regression as an emergent property of expensive LLM-based training, making their serious use difficult,especially as such services cannot absorb large amount of user-defined offline (x, y) data which often containthe most relevant information for finetuning. Hallucinations: By giving the model the freedom to sample y-values over approximately all of R, wildlyinaccurate outlier predictions are now possible.This can be exacerbated by a wrong prediction over asignificant float token (e.g. leading digit or exponent). Although for convenience, we used an unweightedcross-entropy loss in which all float tokens are of equal importance, prediction accuracy can be improved byweighting more significant tokens, making the training loss more aware of numerical distances over R. Prompt-Side Numeric Tokenization: In this work, we directly represented numeric parameter valuesfrom x into the default human readable format (e.g. 1234.5 is serialized simply to 1234.5) to be consistent",
  "Impact Statement": "This research addresses the ability to regress metrics against textual data. Since any textual metadata maybe collected, user-specific information may be used, which raises privacy concerns. This would be particularlytrue on sensitive topics (e.g. predicting metrics related to personal protected characteristics). Our research does not involve such sensitive topics for regression, as it is performed over blackbox opti-mization data. We followed ethical guidelines as all users in our proprietary dataset have consented to havetheir tuning data saved and used for offline analysis. The proprietary real world dataset does not containany sensitive personal information other than usernames. Furthermore, we do not release any of the trainedcheckpoints, as it may be possible to reverse-engineer parts of the training data, which can lead to privacyviolations and data leakage.",
  "Conclusion": "Our OmniPred framework is a first step towards a universal regressor, capable of performing high-precisionpredictions over objectives of any scale from vastly different input spaces and applications. Its simple andscalable design allows transfer learning from large amounts of offline diverse evaluations, while its single-taskvariant can still perform competitively against a wide variety of gold-standard baselines. Furthermore, itis capable of adapting to unseen data through finetuning, while still transferring knowledge from previousdata. This research lays the groundwork for exciting new potential expansions in the field of experimentaldesign.",
  "Franois Charton. Linear algebra with transformers. Trans. Mach. Learn. Res., 2022, 2022": "Tianqi Chen and Carlos Guestrin. Xgboost: A scalable tree boosting system. In Balaji Krishnapuram, MohakShah, Alexander J. Smola, Charu C. Aggarwal, Dou Shen, and Rajeev Rastogi (eds.), Proceedings of the22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 785794.ACM, 2016. Yutian Chen, Xingyou Song, Chansoo Lee, Zi Wang, Richard Zhang, David Dohan, Kazuya Kawakami, GregKochanski, Arnaud Doucet, MarcAurelio Ranzato, Sagi Perel, and Nando de Freitas. Towards learninguniversal hyperparameter optimizers with transformers. In NeurIPS, 2022.",
  "Stphane dAscoli, Pierre-Alexandre Kamienny, Guillaume Lample, and Franois Charton. Deep symbolicregression for recurrent sequences. CoRR, abs/2201.04600, 2022": "Katharina Eggensperger, Frank Hutter, Holger H. Hoos, and Kevin Leyton-Brown. Efficient benchmarkingof hyperparameter optimizers via surrogates. In Blai Bonet and Sven Koenig (eds.), Proceedings of theTwenty-Ninth AAAI Conference on Artificial Intelligence, pp. 11141120. AAAI Press, 2015. Ouassim Ait ElHara, Konstantinos Varelas, Duc Manh Nguyen, Tea Tusar, Dimo Brockhoff, NikolausHansen, and Anne Auger. COCO: The large scale black-box optimization benchmarking (bbob-largescale)test suite. ArXiv, abs/1903.06396, 2019.",
  "Zhou Fan, Xinran Han, and Zi Wang. Transfer learning for bayesian optimization on heterogeneous searchspaces. Transactions on Machine Learning Research, 2024. ISSN 2835-8856": "Yanjie Gao, Xianyu Gu, Hongyu Zhang, Haoxiang Lin, and Mao Yang. Runtime performance predictionfor deep learning models with graph neural network. In 45th IEEE/ACM International Conference onSoftware Engineering: Software Engineering in Practice, SEIP@ICSE 2023, Melbourne, Australia, May14-20, 2023, pp. 368380. IEEE, 2023. Shivam Garg, Dimitris Tsipras, Percy Liang, and Gregory Valiant. What can transformers learn in-context?A case study of simple function classes. In Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave,K. Cho, and A. Oh (eds.), Neural Information Processing Systems, 2022. Daniel Golovin, Benjamin Solnik, Subhodeep Moitra, Greg Kochanski, John Karro, and D. Sculley. Googlevizier: A service for black-box optimization. In Proceedings of the 23rd ACM SIGKDD InternationalConference on Knowledge Discovery and Data Mining, pp. 14871495. ACM, 2017.",
  "Nate Gruver, Marc Finzi, Shikai Qiu, and Andrew Gordon Wilson. Large language models are zero-shottime series forecasters. CoRR, abs/2310.07820, 2023": "Milad Hashemi, Kevin Swersky, Jamie A. Smith, Grant Ayers, Heiner Litz, Jichuan Chang, ChristosKozyrakis, and Parthasarathy Ranganathan. Learning memory access patterns. In Jennifer G. Dy and An-dreas Krause (eds.), International Conference on Machine Learning, ICML 2018, volume 80 of Proceedingsof Machine Learning Research, pp. 19241933. PMLR, 2018. Marton Havasi, Rodolphe Jenatton, Stanislav Fort, Jeremiah Zhe Liu, Jasper Snoek, Balaji Lakshmi-narayanan, Andrew Mingbo Dai, and Dustin Tran. Training independent subnetworks for robust pre-diction. In International Conference on Learning Representations, ICLR, 2021. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, andJacob Steinhardt. Measuring mathematical problem solving with the MATH dataset. In Joaquin Van-schoren and Sai-Kit Yeung (eds.), Proceedings of the Neural Information Processing Systems Track onDatasets and Benchmarks, 2021. Noah Hollmann, Samuel Mller, Katharina Eggensperger, and Frank Hutter. Tabpfn: A transformer thatsolves small tabular classification problems in a second. In International Conference on Learning Repre-sentations, ICLR, 2023. Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, andWeizhu Chen.Lora: Low-rank adaptation of large language models.In International Conference onLearning Representations, ICLR, 2022.",
  "Xin Huang, Ashish Khetan, Milan Cvitkovic, and Zohar S. Karnin. Tabtransformer: Tabular data modelingusing contextual embeddings. CoRR, abs/2012.06678, 2020": "Yiding Jiang, Behnam Neyshabur, Hossein Mobahi, Dilip Krishnan, and Samy Bengio. Fantastic generaliza-tion measures and where to find them. In International Conference on Learning Representations, ICLR,2020. Samuel J. Kaufman, Phitchaya Mangpo Phothilimthana, Yanqi Zhou, Charith Mendis, Sudip Roy, AmitSabne, and Mike Burrows. A learned performance model for tensor processing units. In Alex Smola, AlexDimakis, and Ion Stoica (eds.), Proceedings of Machine Learning and Systems (MLSys). mlsys.org, 2021.",
  "Andreas Krause and Cheng Ong. Contextual gaussian process bandit optimization. In Advances in NeuralInformation Processing Systems, 2011": "Taku Kudo and John Richardson. Sentencepiece: A simple and language independent subword tokenizerand detokenizer for neural text processing. In Proceedings of the 2018 Conference on Empirical Methodsin Natural Language Processing: System Demonstrations, pp. 6671, 2018. Aviral Kumar, Amir Yazdanbakhsh, Milad Hashemi, Kevin Swersky, and Sergey Levine. Data-driven offlineoptimization for architecting hardware accelerators. In International Conference on Learning Representa-tions, ICLR, 2022. Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay V. Ramasesh,Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra. Solving quantitative reasoning problems with language models. In Sanmi Koyejo,S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh (eds.), Advances in Neural InformationProcessing Systems, 2022.",
  "Richard Liaw, Eric Liang, Robert Nishihara, Philipp Moritz, Joseph E. Gonzalez, and Ion Stoica. Tune: Aresearch platform for distributed model selection and training. CoRR, abs/1807.05118, 2018": "Jovita Lukasik, David Friede, Heiner Stuckenschmidt, and Margret Keuper. Neural architecture performanceprediction using graph neural networks. In Zeynep Akata, Andreas Geiger, and Torsten Sattler (eds.),Pattern Recognition - 42nd DAGM German Conference, DAGM GCPR, volume 12544 of Lecture Notesin Computer Science, pp. 188201. Springer, 2020. Joe Mellor, Jack Turner, Amos J. Storkey, and Elliot J. Crowley. Neural architecture search without training.In Marina Meila and Tong Zhang (eds.), International Conference on Machine Learning, ICML, volume139, pp. 75887598. PMLR, 2021. Charith Mendis, Alex Renda, Saman P. Amarasinghe, and Michael Carbin. Ithemal: Accurate, portable andfast basic block throughput estimation using deep neural networks. In Kamalika Chaudhuri and RuslanSalakhutdinov (eds.), International Conference on Machine Learning, ICML, volume 97 of Proceedings ofMachine Learning Research, pp. 45054515. PMLR, 2019.",
  "OpenAI. Introducing chatgpt. 2022": "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou,Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer.J. Mach. Learn. Res., 21:140:1140:67, 2020. Karan Singhal, Shekoofeh Azizi, Tao Tu, S. Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales,Ajay Kumar Tanwani, Heather Cole-Lewis, Stephen Pfohl, Perry Payne, Martin Seneviratne, Paul Gamble,Chris Kelly, Nathaneal Schrli, Aakanksha Chowdhery, Philip Andrew Mansfield, Blaise Agera y Arcas,Dale R. Webster, Gregory S. Corrado, Yossi Matias, Katherine Chou, Juraj Gottweis, Nenad Tomasev,Yun Liu, Alvin Rajkomar, Joelle K. Barral, Christopher Semturs, Alan Karthikesalingam, and VivekNatarajan. Large language models encode clinical knowledge. CoRR, abs/2212.13138, 2022. Xingyou Song, Sagi Perel, Chansoo Lee, Greg Kochanski, and Daniel Golovin. Open source vizier: Dis-tributed infrastructure and API for reliable and flexible blackbox optimization. In Isabelle Guyon, MariusLindauer, Mihaela van der Schaar, Frank Hutter, and Roman Garnett (eds.), International Conferenceon Automated Machine Learning, AutoML, volume 188 of Proceedings of Machine Learning Research, pp.8/117. PMLR, 2022. Xingyou Song, Qiuyi Zhang, Chansoo Lee, Emily Fertig, Tzu-Kuo Huang, Lior Belenki, Greg Kochanski,Setareh Ariafar, Srinivas Vasudevan, Sagi Perel, and Daniel Golovin. The vizier gaussian process banditalgorithm, 2024. Brandon Trabucco, Xinyang Geng, Aviral Kumar, and Sergey Levine.Design-bench:Benchmarks fordata-driven offline model-based optimization. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, CsabaSzepesvri, Gang Niu, and Sivan Sabato (eds.), International Conference on Machine Learning, ICML,volume 162 of Proceedings of Machine Learning Research, pp. 2165821676. PMLR, 2022. Robert Vacareanu, Vlad-Andrei Negru, Vasile Suciu, and Mihai Surdeanu.From words to numbers:Your large language model is secretly A capable regressor when given in-context examples.CoRR,abs/2404.07544, 2024.",
  "B.2Local Training": "During local training, data comes from a single studys limited trials (at most 1000). The training set sizecan be lower than the batch size (256), and thus we must define one epoch as seeing the training data once,i.e. only one gradient step if training size batch size, but multiple gradient steps otherwise. We use the same settings from pretraining for consistency, but allow a maximum of 30 epochs. For earlystopping, validation loss is now measured over the entire validation set instead of sampled batches. Furtherspecific changes:",
  "B.3Inference": "At inference time, we perform temperature sampling with a temperature of 1.0. We restrict the logits toonly decode the custom floating point tokens for representing y-values. To maximize batch size for a 1x1TPU V3, we generate 64 samples and select the empirical median of these floating point samples as our finalprediction when computing prediction error.",
  "C.2Data Processing for Flat Space": "A flat space is where every trial in the study specifies every parameter configured in the space. In this case,we convert the parameters into the unit hypercube d. For numeric parameters, we scale all values intothe range , by default using linear scaling unless (reverse)-log scaling was configured. For CATEGORICALparameters, we use a one-hot encoding.",
  "C.3Data Processing for Conditional Space": "A conditional space occurs when one parameter may be unused, depending on its parent parameters value.Conditional spaces commonly appear in AutoML settings where different model classes require a different setof parameters to be tuned. Another common use case is when we wish to optimize a numeric hyperparameterin the log scale but include 0 in the search (e.g. dropout rate, regularization coefficient), i.e. {UNUSED}[l, u]where l > 0.",
  "C.4Regressor Baselines": "Below, we list out the specific implementation details of our regressor baselines. One nuanced issue is ofhyperparameter tuning the regressors themselves, which could affect results. In order to be reasonably fairto all regressors (including our own OmniPred which has its own hyperparameters), for each regressor, weused a reasonable fixed set of hyperparameters for consistency throughout all experiments. We emphasize that our papers contributions are mostly on regression using flexible string representationsand large-scale multi-task training, and do not claim to replace widely accepted baselines in single-task,apples-to-apples comparisons.",
  "The algorithm then uses L-BFGS to obtain the MAP estimate of , and": "One caveat here is that this model requires a non-linear preprocessing on the observations and thus predictsy in the preprocessed space. This preprocessing is found to be critical to achieving stable regression acrossVizier studies, which have a wide variety of value ranges. Since the preprocessing is non-linear, we cannotobtain the predictive distribution over the raw observation in a closed form. Instead, we take 1000 samplesfrom the GP, apply the inverse of the preprocessor, and then take the mean.",
  "Although tree-based methods do not generally require rescaling, we still applied consistent x-preprocessing(in particular to deal with optional log or reverse-log scaling)": "Multilayer Perceptron: The base architecture consists of a 2-layer ReLU dense network of hidden size256 with a final scalar output.y-values are normalized using tf.keras.layers.Normalization whichsubtracts mean and divides by standard deviation computed empirically from the training data. Trainingwas performed with an Adam optimizer using learning rate 102, full batch training over 100 epochs, andmean squared error.",
  "D.1Study Preprocessing": "Since Google Vizier is a service in which users control evaluations, much of the raw study data can bequite chaotic. We apply certain preprocessing techniques to make the data more conducive to training andevaluation. Removing bad trials: Users may ignore or fail to evaluate a proposed x.Furthermore, during sometrials, the y-objective may be denoted with a special infeasible value (e.g. if a high batch size led to GPUout-of-memory errors, or if training encountered NaNs). We remove such trials from consideration in ourwork, although one can extend our y-tokenization to support infeasibility in later works. Trial count hard limit: Some raw studies can contain trials in upwards of 105 trials, which could dominatethe data distribution. We therefore apply a hard limit and only consider the first 103 trials per study. Filtering specific users: There are specific human and automated power users which produce orders ofmagnitude more studies than the average user. Some automated users in particular are simply automaticunit tests involving the use of OSS Vizier. We disregard studies from these users to prevent them fromdominating the data distribution.",
  "D.2Real World Data Descriptions": "(Overall) Entire Database: No filters were applied. All studies from the database were exported onMarch 31, 2023. Finetuning experiments involving unseen studies consist of studies created after this date. Bid Simulation: Contains hyperparameters for proprietary bid simulation. The simulator estimates howadvertisements might have performed in terms of key metrics such as cost, impressions, clicks, and conversionvolume.",
  "Protein Design: Each space consists of 50+ parameters, each of which denotes a categorical proteinbuilding block": "Vertex AI (Tabular and Text): A Vertex AI platform for automated ML model selection and trainingfor tabular or text data. For tabular data, Vertex AI searches over a tree of model and optimizer types,their hyperparameters, data transformation, and other components in the ML pipeline. For text, Vertex AItrains an ML model to classify text data, extract information, or understand the sentiment of the authors.For more information, see:#train-models-with-minimal-ml-expertise."
}