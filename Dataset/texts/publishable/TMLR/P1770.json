{
  "Abstract": "Multimodal Variational Autoencoders (VAEs) represent a promising group of generativemodels that facilitate the construction of a tractable posterior within the latent space givenmultiple modalities. Previous studies have shown that as the number of modalities increases,the generative quality of each modality declines. In this study, we explore an alternativeapproach to enhance the generative performance of multimodal VAEs by jointly modeling thelatent space of independently trained unimodal VAEs using score-based models (SBMs). Therole of the SBM is to enforce multimodal coherence by learning the correlation among thelatent variables. Consequently, our model combines a better generative quality of unimodalVAEs with coherent integration across different modalities using the latent score-based model.In addition, our approach provides the best unconditional coherence. The code can be foundat",
  "Introduction": "The real-world data often has multiple modalities such as image, text, and audio, which makes learning frommultiple modalities an important task. Multimodal VAEs are a class of multimodal generative models thatare able to generate multiple modalities jointly. Learning from multimodal data is inherently more challengingthan from unimodal data, as it involves processing multiple data modalities with distinct characteristics. In order to learn the joint representation of these modalities, previous approaches generally preferred toencode them to latent distribution that governs the data distribution across different modalities. In general,we expect the following properties from a multimodal generative model: Multiway conditional generation: Given the presence of certain modalities, it should be feasible togenerate the absent modalities based on the existing ones (Shi et al., 2019; Wu & Goodman, 2018). Theconditioning process should not be limited to specific modalities; rather, any modality should serve as a basisfor generating any other modality. Unconditional generation: If we have no modality present to condition on, we should be able to samplefrom the joint distribution so that the generated modalities are coherent (Shi et al., 2019). Coherence inthis case means that the generated modalities represent the same concept that is expressed in the differentmodalities we have. Conditional modality gain: When additional information is provided to the model through the observationof more modalities, there should be an enhancement in the performance of the generated absent modalities.In other words, the performance should consistently improve as the number of observed (given) modalitiesincreases.",
  "Published in Transactions on Machine Learning Research (12/2024)": "auxiliary encoder using contrastive learning objective so that the similarity between the zs coming from thesame pair of multimodal data are maximized and the similarity of the pairs of zs coming from unrelatedpairs are minimized. MLD by Bounoua et al. (2024) is trained using the same experimental setup used bythe SBM models, the same train/val/test split, and the same architecture using the code provided here3",
  "Methodology": "Assuming the each data point consists of M modalities: x1:M = (x1, x2, , xM), our latent variable modeldescribes the data distribution as p(x1:M) = z1:M p(x1:M|z1:M)p(z1:M), where z1:M = (z1, z2, , zM) andzk is the latent vector corresponding to the kth modality. In contrast to common multimodal VAE setups",
  "q(z1:M|x1:M).(1)": "To simplify the joint generative model p(x1:M|z1:M) and the joint recognition model q(z1:M|x1:M), we assumetwo conditional indenpendencies: 1) Given an observed modality xk, its corresponding latent variable zkis independent of other latent variables, i.e., xk have enough information to describe zk: zk zk|xk. 2)Knowing the latent variable zk of kth modality is enough to reconstruct that modality: xk xk, zk|zk.Using these conditional independencies the generative and recognition models factorize as:",
  "k qk(zk|xk)||p(z1:M))and since recognition model is constant w.r.t. , the step II becomes max Ep(x1:M)E": "k q(zk|xk) log p(z1:M),which is equivalent to maximum likelihood training of the parametric prior using sampled latent variablesfor each data points. And during inference, we only need samples from p(z1:M), thus we can parameterizes(z1:M) z1:M log(p(z1:M)) as the score model and train s(z1:M) using score matching (Hyvrinen &Dayan, 2005). In practice, the score matching objective does not scale to the large dimension which isrequired in our setup, and other alternatives such as denoising score-matching (Vincent, 2011) and slicedscore-matching (Song et al., 2020a) have been proposed. Here we use denoising score matching in the continuous form that diffuses the latent samples into noisedistribution and can be written in a stochastic differential equation (SDE) form (Song et al., 2020b). Theforward process will be an SDE of dz = f(z, t)dt + g(t)dw where w is the Brownian motion. To reversethe noise distribution back to the latent distribution, we use the trained score model and sample iterativelyfrom the reverse SDE dz =f(z, t) g(t)2z log pt(z)dt + g(t)dw. The score network which approximatesz log pt(z) is trained using eq. 4. We use the unweighted version where (t) = 1.",
  "where qk(zk|xk) is the recognition model for modality k": "In order to sample from q(zu|xo), following eq. 6, we first sample zo for all observed modalities, and thensample zu from p(zu|zo) by fixing zo which is the latent representation of the observed modalities andupdating the unobserved ones during sampling. When all modalities are missing, we will update all modalitiesin the sampling step.",
  "Contrastive guidance": "Another way to enhance the conditional performance of the score-based model is to leverage contrastiveguidance from an auxiliary encoder network E(x). This encoder maps the input data x from all modalities toa common embedding space, enabling conditional alignment through contrastive learning objectives (Rombachet al., 2021; Tang et al., 2023). The resulting embeddings capture semantic information that can guide thescore-based model during the sampling process. Specifically, we incorporate secondary encoder embeddings E(x) as an additional conditioning input to thescore model s(z(t), t, E(x)). The model can use this aligned representation which will be useful in providingguidance. We show this method is supportive as an additional guidance method on the datasets in theexperiments section. Compared to the energy-based coherence guidance, which is trained independently ofthe score model and added during sampling time, this method requires the secondary encoders to be presentwhen training the score model. We train the secondary encoders, E(x), using contrastive learning similar toRadford et al. (2021) where the similarity between representations from the same pairs of data are maximizedand the similarity between representations from unrelated pairs are minimized.",
  "Related Works": "Our work is heavily inspired by earlier works on deep multimodal learning (Ngiam et al., 2011; Srivastava& Salakhutdinov, 2014). Ngiam et al. (2011) use a set of autoencoders for each modality and a sharedrepresentation across different modalities and trained the parameters to reconstruct the missing modalitiesgiven the present one. Srivastava & Salakhutdinov (2014) define deep Boltzmann machine to representmultimodal data with modality-specific hidden layers followed by shared hidden layers across multiplemodalities, and use Gibbs sampling to recover missing modalities. Suzuki et al. (2017) approached this problem by maximizing the joint ELBO and additional KL terms betweenthe posterior of the joint and the individuals to handle missing data. Tsai et al. (2019) propose a factorizedmodel in a supervised setting over model-specific representation and label representation, which capture theshared information. The proposed factorization is q(z1:M, zy|x1:M, y) = q(zy|x1:M) Mk=1 q(zk|xk), where zyis the latent variable corresponding to the label.",
  "Similar to our setup p(x1:M|z) =": "k pk(xk|z), but q(z|x1:M) is handled differently. Wu & Goodman(2018) use a product of experts to describe q: qPoE(z|x1:M) = p(z) Mk=1 qk(z|xk). Assuming p(z) and eachof qk(z|xk) follow a Gaussian distribution, qPoE can be calculated in a closed form, and we can optimize themultimodal ELBO accordingly. To get a good performance on generating missing modality, Wu & Goodman(2018) sample different ELBO combinations of the subset of modalities. Moreover, the sub-sampling proposedby Wu & Goodman (2018) results in an invalid multimodal ELBO (Wu & Goodman, 2019). The MVAEproposed by(Wu & Goodman, 2018) generates good-quality images but suffers from low cross-modalcoherence. To address this issue Shi et al. (2019) propose constructing q(z|x1:M) as a mixture of experts:qMoE(z|x1:M) =1M",
  "xm q(z|xm), where q(z|xm) =": "km q(z|xk) and m is a subset of modalities.The number of mixture components grows exponentially as the number of modalities increases. MoPoEhas better coherence than PoE, but as discussed by Daunhawer et al. (2022) sub-sampling modalitiesin mixture-based multimodal VAEs result in loss of generation quality of the individual modalities. Toaddress this issue, more recently and in parallel to our work, Palumbo et al. (2023) introduce modality-specific latent variables in addition to the shared latent variable.In this setting the joint probabilitymodel over all variables factorizes as p(x1:M, z, w1:M) = p(z)",
  "k p(xk|z, wk)p(wk) and q factorizes asqMMVAE+(z, w1:M|x1:M) = qz(z|x)": "k qk(wk|xk). Using modality-specific representation is also exploredby Lee & Pavlovic (2021), however, the approach proposed by Palumbo et al. (2023) is more robust tocontrolling modality-specific representation vs shared representation. But since the shared component is amixture of experts of individual components, it can only use one of them at a time during inference whichlimits its ability to use additional observations that are available as the number of given modalities increase. Sutter et al. (2020) propose an updated multimodal objective that consists of a JS-divergence term instead ofthe normal KL term with a mixture-of-experts posterior. They also add additional modality-specific termsand a dynamic prior to approximate the unimodal and the posterior term. Though these additions providesome improvement, there is still a need for a model that balances coherence with quality (Palumbo et al.,2023). Wolff et al. (2022) propose a hierarchical multimodal VAEs for the task where a generative model of theform p(x, g, z) and an inference model q(g, z|x1:M) containing multiple hierarchies of z where g holds theshared structures of multiple modalities in a mixture-of-experts form q(g|x1:M).They argue that themodality-exclusive hierarchical structure helps in avoiding the modality sub-sampling issue and can capturethe variations of each modality. Though the hierarchy gives some improvement in results, the model is stillrestricted in capturing the shared structure in g discussed in their work. Suzuki & Matsuo (2023) introduce a multimodal objective that avoids sub-sampling during training by notusing a mixture-of-experts posterior to avoid the main issue discussed by Daunhawer et al. (2022). Theypropose a product-of-experts posterior multimodal objective with additional unimodal reconstruction termsto facilitate cross-modal generations. Hwang et al. (2021) propose an ELBO that is derived from an information theory perspective that encouragesfinding a representation that decreases the total correlation. They propose another ELBO objective whichis a convex combination of two ELBO terms that are based on conditional VIB and VIB. The VIB termdecomposes to ELBOs of previous approaches. The conditional term decreases the KL between the jointposterior and individual posteriors. They use a product-of-experts as their joint posterior. Finally, parallel to our work, Xu et al. (2023) propose diffusion models to tackle the multimodal generationwhich uses multi-flow diffusion with data and context layer for each modality and a shared global layer. Also,concurrent to our work, Bounoua et al. (2024) proposed a latent diffusion model for multimodal data using a",
  ": Extended PolyMnist Dataset": "We study our proposed methods and selected baselines using anextended version of PolyMNIST (Sutter et al., 2021) as well ashigh-dimensional CelebAMask-HQ (Lee et al., 2020) datasets.Extended PolyMNIST is ideal for observing the performancepatterns in the presence of a different number of modalities,which is also used by previous works, while CelebAMask-HQtests the behavior of the methods in a high-dimensional setting.The experiments on the CelebAMask-HQ demonstrate thatmultimodal VAEs, in general, can be used to solve structuredprediction problems. Finally, we study the robustness of mul-timodal VAEs in the presence of adversarial attacks. Otherexperiments including an audio modality are available in the Appendix section A.6.",
  "Extented PolyMnist": "The original PolyMNIST introduced by Sutter et al. (2021) has five modalities and in order to study thebehavior of the methods on a larger number of modalities, we extended the number of modalities to ten. shows samples of the Extended PolyMNIST data. We compare our methods SBM-VAE and SBM-RAE, which substitute individual variational autoencoder witha regularized deterministic autoencoder (Ghosh et al., 2020); see Appendix A.1 for the details of SBM-RAE,as well as their counterparts with energy-based coherence guidance SBM-VAE-C and SBM-RAE-C plusSBM-VAE-T which uses contrastive guidance with baselines of MVAE (Wu & Goodman, 2018), MMVAE (Shiet al., 2019), MoPoE (Sutter et al., 2021), MVTCAE (Hwang et al., 2021), and MMVAE+ (Palumbo et al.,2023). We added SBM-RAE to show that our approach can be used with any auto-encoder structure and isnot limited to only a variational auto encoder, unlike the baselines.",
  ": Unconditional samples from 10modalities using SBM-VAE. Columns repre-sent different samples from each modality": "Further, we study the scalability of methods by changing thenumber of trained modalities from two to ten and then evalu-ating unconditional FID. Similar to unconditional coherence,for unconditional FID we dont use coherence guidance. Theoutcome is shown in . The performance of our proposedmethods is consistent and superior in the presence of differentnumbers of data modalities. Conditional generation.We run similar experiments tostudy the behavior of the methods for conditional generation.In the conditional generation, we assume we are generatingthe first modality given the rest. The selection of the firstmodality is to remain consistent as we increase the numberof modalities from two to ten. shows the outcome.The SBM models achieve the best performance in generationquality in both conditional and unconditional generations as wescale the number of modalities. This shows that our approachcan scale to a high number of modalities without compromise.Specifically, the SBM-VAE variants show consistent FID andcoherence, for which the complexity of the sampling from prior remains the same as we increase the numberof data modalities. We also study, how the conditional coherence and FID change with the number of the observed modalityin a single model trained with 10 modalities. For an accurate conditional model, we expect as we observemore modalities, the accuracy of conditional generation will improve without a decrease in quality. shows the FID score of the last modality as we increase the given modalities and demonstratesthe conditional coherence of the predicted images on the last modality given a different number of observedmodalities. Our models show consistent FID while also having increasing accuracy. The figure shows the",
  ": a) Conditional FID and b) Conditional accuracy of the generated first modality given the rest ofmodalities as the number of modalities the models are trained on increases as shown on the x-axis": "deterioration in quality quantified by increasing FID as more modalities are observed for the comparedmethods except for the SBM models and MMVAE+. SBM-based multimodal VAEs and MMVAE+ bothare equipped with a way to capture modality-specific representation, so increasing the number of observedmodalities does not affect their generation quality. The SBM exploits the additional modalities to generatebetter and more accurate images as we increase the number of modalities without losing quality. Similarexperiments have been reported for predicting the first modality in the Appendix section (see and). The plain score model suffers from low coherence when we have only one or two observed modalities out ofthe ten. But with the help of the coherence guidance, we can guide the model towards the correct outputand that helps increase the coherence of SBM-VAE and SBM-RAE when the number of observed modalitiesis low. In addition to that, increasing the number of sampling steps from 100 to 1000 also helps when afew modalities are observed. We demonstrate the effect of coherence guidance and the number of inferencesteps in . The SBM-VAE-T model also shows a high coherence even when the number of observedmodalities is low. Further details as well as more generated samples and more results on other modalitieshave been reported in Appendix A.4.",
  "CelebAMask-HQ": "The images, masks, and attributes of the CelebAMask-HQ dataset can be treated as different modalitiesof expressing the visual characteristics of a person. A sample from the dataset is shown in . Theimages and masks are resized to 128 by 128. The masks are either white or black where all the masks given",
  "in the CelebAMask-HQ except the skin mask are drawn on top of each other as one image. We follow thepre-processing of Wu & Goodman (2018) by selecting 18 out of 40 existing attributes as described": "We compare SBM-VAE, SBM-VAE-C, SBM-VAE-T, SBM-RAE, SBM-RAE-C, SBM-RAE-T from the SBMvariants and MoPoE by Sutter et al. (2021), MVTCAE by Hwang et al. (2021), MMVAE+ by Palumboet al. (2023), and MLD by Bounoua et al. (2024) as baselines. See Appendix A.9 for the experimental setupsof these methods. We evaluate the generation quality of the image modality using FID score and generation accuracy of maskand attribute modalities using sample-average F1 score. shows how our methods compare with thebaselines in the presence of one or two observed modalities. We have also reported the performance of asupervised model trained to predict attributes and masks directly given the images. shows theunconditional image generation performance of the models. SBM-VAE and SBM-RAE variants generatehigh-quality images compared to the baselines in both conditional and unconditional settings, while on themask modalities, MoPoE and MVTCAE achieve a better F1 score on mask prediction. We can note that thecontrastive guidance is very helpful and the top-performing model when predicting the attributes conditionally.We can also observe that conditional guidance consistently improves the coherence of score-based models.The SBM models achieve superior performance when predicting the image modality consistently and alsoperform comparatively or better in the other modalities, showing the effectiveness of the method in differentmodalities.",
  "Robustness to Adversarial Attacks": "In this section, we study how multimodal VAEs and SBM-VAEs perform in the presence of a blackboxadversarial attack. We choose blackbox attack because the inference algorithm for SBM is not end-to-enddifferentiable. We apply the blackbox method by Papernot et al. (2017) to generate the adversarial examplesusing a supervised model that classifies the attributes from the images of the CelebAHQ dataset and thenfeed these perturbed images to both multimodal VAEs based on MoPoE, MVTCAE, MMVAE+ as well asour SBM-VAE-C and the contrastive guided SBM-RAE-T. The perturbed images are created using the FastGradient Sign Method (FGSM) Goodfellow et al. (2015) with =0.05. We then use the adversarial imagesinstead of the original images and predict the attributes. As reported in , SBMVAE-C is much less affected by the adversarial attacks as its performancedecreased by approximately 4% while MoPoEs and MVTCAEs performance dropped by 19.1% and 13.7%,respectively. This shows that our method is more robust to blackbox adversarial attacks. The robustness ofSBM-based VAEs is because of the inference procedure which causes purification Nie et al. (2022) on thelatent representation of the perturbed modality, by mapping back the noisy latent representation to theoriginal latent manifold. On the other hand, SBM-RAE-T is affected the most as the secondary conditioningencoder directly takes the images which will be affected when given an adversarial input.",
  "Conclusion and Discussion": "Multimodal VAEs are an important tool for modeling multimodal data. In this paper, we provide a differentmultimodal posterior using score-based models. Our proposed method learns the correlation of latent spacesof unimodal representations using a joint score model in contrast to the traditional multimodal VAE ELBO.We show that our method can generate better-quality samples and, at the same time, preserve coherenceamong modalities. We have also shown that our approach is scalable to multiple modalities. We also introducecoherence guidance to improve the coherence of the generated modalities. The coherence guidance effectivelyimproves the coherence of our score-based models. In conclusion, while our score-based multimodal VAEapproach offers certain advantages, it has a trade-off. Unlike traditional multimodal VAEs, our model requiresa computationally expensive sampling procedure during inference. This ultimately forces a balance betweenthe quality and coherence of predictions and the computational resources needed. This limitation highlightsan area for future research. Broader Impact: We believe this model can be used in real-world applications",
  "Imant Daunhawer, Thomas M. Sutter, Kieran Chin-Cheong, Emanuele Palumbo, and Julia E Vogt. On thelimitations of multimodal VAEs. In International Conference on Learning Representations, 2022. URL": "Prafulla Dhariwal and Alexander Quinn Nichol. Diffusion models beat gans on image synthesis. In MarcAurelioRanzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan (eds.),Advances in Neural Information Processing Systems 34: Annual Conference on Neural InformationProcessing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pp. 87808794, 2021. URL Partha Ghosh, Mehdi S. M. Sajjadi, Antonio Vergari, Michael Black, and Bernhard Scholkopf.Fromvariational to deterministic autoencoders. In International Conference on Learning Representations, 2020.URL Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples.In Yoshua Bengio and Yann LeCun (eds.), 3rd International Conference on Learning Representations,ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL Michael Gutmann and Aapo Hyvrinen.Noise-contrastive estimation: A new estimation principle forunnormalized statistical models. In Yee Whye Teh and Mike Titterington (eds.), Proceedings of theThirteenth International Conference on Artificial Intelligence and Statistics, volume 9 of Proceedings ofMachine Learning Research, pp. 297304, Chia Laguna Resort, Sardinia, Italy, 1315 May 2010. PMLR.URL Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trainedby a two time-scale update rule converge to a local nash equilibrium. In I. Guyon, U. Von Luxburg,S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural InformationProcessing Systems, volume 30. Curran Associates, Inc., 2017. URL HyeongJoo Hwang, Geon-Hyeong Kim, Seunghoon Hong, and Kee-Eung Kim. Multi-view representationlearning via total correlation objective. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, andJ. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems, volume 34, pp. 1219412207. Curran Associates, Inc., 2021. URL",
  "Aapo Hyvrinen and Peter Dayan. Estimation of non-normalized statistical models by score matching.Journal of Machine Learning Research, 6(4), 2005": "Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua Bengio andYann LeCun (eds.), 3rd International Conference on Learning Representations, ICLR 2015, San Diego,CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL Diederik P. Kingma and Max Welling. Auto-encoding variational bayes. In Yoshua Bengio and Yann LeCun(eds.), 2nd International Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April14-16, 2014, Conference Track Proceedings, 2014. URL",
  "Cheng-Han Lee, Ziwei Liu, Lingyun Wu, and Ping Luo. Maskgan: Towards diverse and interactive facialimage manipulation. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020": "Mihee Lee and Vladimir Pavlovic.Private-shared disentangled multimodal vae for learning of latentrepresentations. In 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops(CVPRW), pp. 16921700, 2021. doi: 10.1109/CVPRW53098.2021.00185. Jiquan Ngiam, Aditya Khosla, Mingyu Kim, Juhan Nam, Honglak Lee, and Andrew Y. Ng. Multimodal deeplearning. In Proceedings of the 28th International Conference on International Conference on MachineLearning, ICML11, pp. 689696, Madison, WI, USA, 2011. Omnipress. ISBN 9781450306195.",
  "Kushagra Pandey, Avideep Mukherjee, Piyush Rai, and Abhishek Kumar. Diffusevae: Efficient, controllableand high-fidelity generation from low-dimensional latents. CoRR, abs/2201.00308, 2022. URL": "Nicolas Papernot, Patrick D. McDaniel, Ian J. Goodfellow, Somesh Jha, Z. Berkay Celik, and AnanthramSwami. Practical black-box attacks against machine learning. In Ramesh Karri, Ozgur Sinanoglu, Ahmad-Reza Sadeghi, and Xun Yi (eds.), Proceedings of the 2017 ACM on Asia Conference on Computer andCommunications Security, AsiaCCS 2017, Abu Dhabi, United Arab Emirates, April 2-6, 2017, pp. 506519.ACM, 2017. doi: 10.1145/3052973.3053009. URL Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, GirishSastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learningtransferable visual models from natural language supervision. In Marina Meila and Tong Zhang (eds.),Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021,Virtual Event, volume 139 of Proceedings of Machine Learning Research, pp. 87488763. PMLR, 2021. URL",
  "Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjrn Ommer. High-resolutionimage synthesis with latent diffusion models, 2021": "Yuge Shi, Siddharth Narayanaswamy, Brooks Paige, and Philip H. S. Torr. Variational mixture-of-expertsautoencoders for multi-modal deep generative models. In Hanna M. Wallach, Hugo Larochelle, AlinaBeygelzimer, Florence dAlch-Buc, Emily B. Fox, and Roman Garnett (eds.), Advances in NeuralInformation Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019,NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pp. 1569215703, 2019. URL Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution.In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence dAlch-Buc, Emily B. Fox,and Roman Garnett (eds.), Advances in Neural Information Processing Systems 32: Annual Confer-ence on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancou-ver, BC, Canada, pp. 1189511907, 2019. URL",
  "Jannik Wolff, Rahul G Krishnan, Lukas Ruff, Jan Nikolas Morshuis, Tassilo Klein, Shinichi Nakajima, andMoin Nabi. Hierarchical multimodal variational autoencoders, 2022. URL": "Mike Wu and Noah D. Goodman. Multimodal generative models for scalable weakly-supervised learning. InSamy Bengio, Hanna M. Wallach, Hugo Larochelle, Nicol Cesa-Bianchi, and Roman Garnett (eds.), Ad-vances in Neural Information Processing Systems 31: Annual Conference on Neural Information ProcessingSystems 2018, NeurIPS 2018, December 3-8, 2018, Montral, Canada, pp. 55805590, 2018. URL",
  "A.2Model Architectures and Experimental setups for Extended PolyMNIST": "The extended PolyMnist dataset was updated from the original PolyMnist dataset by Sutter et al. (2020) withdifferent background images and ten modalities. It has 50,000 training set, 10,000 validation set, and 10,000test set. The VAEs for each modality are trained with an initial learning rate of 0.001 using a value of 0.1where all the prior, posterior, and likelihood are Gaussians. This also applies to all multimodal VAEs exceptMMVAE+ which uses Laplace distribution instead of Gaussian. The RAE for each modality was trainedusing the mean squared error loss with the norm of ||z||22 regularized by a factor of 105 and a Gaussiannoise added to z before feeding to the decoder with mean 0 and variance of 0.01 where the hyperparametervalues were tuned using the validation set. The encoders and decoders for all models use residual connectionsto improve performance and are similar in structure to the architecture used in Daniel & Tamar (2021)except for MMVAE+ were we used the original model because the models performance doesnt generalize todifferent neural net architecture. For MMVAE+, modality-specific and shared latent sizes are each 32 andthe model was trained similarly to the code provided by the paper 2 with the IWAE estimator with K=1.The detailed architecture can be found by referring to the code that is attached. We used latent size of 64 forour models and MMVAE+ and we increased the latent dimension of the other baselines to 64 n where n isthe number of modalities because they dont have any modality specific representation. We chose the best value for each model using the validation set. For MoPoE and MMVAE, of 2.5 was chosen, for MMVAE+5, for MVAE 1 and for MVTCAE 0.1. The neural net of SBM-VAE and SBM-RAE is a UNET network where we resize the latent size to 8x8. ForSBM-VAE, samples are taken from the posterior at training time and the mean of posterior is taken atinference time. For SBM-RAE, the zs are taken directly. We use a learning rate of 0.0002 with the Adamoptimizer (Kingma & Ba, 2015). The detailed hyperparameters are shown in table 4. We use the VPSDEwith 0 = 0.1 and 1=5 with N = 100 and the PC sampling technique with Euler-Maruyama sampling andlangevin dynamics. For modalities less than 10, we use 0 of 1, the others hyperparameters remain the same.The energy-based model is a simple MLP with the softplus activation. We follow equation 7 to train themodels. The classifier used for evaluation is composed of three convolutional layers followed by ReLU activationsand two fully connected layers where the last one outputs 10 logits for classification. Its trained using thecross-entropy loss using the training data composed of all modalities and achieves an average accuracy 98.7on the test set of all modalities.",
  "A.4.2 Ablation": "In , we discuss how of the unimodal VAE affects the conditional FID and the conditional coherenceof SBM-VAE. As the score model is trained on the samples from the posterior, samples that have goodreconstruction are important for the score model to learn well. This is also evident in the result as lower values have better result than higher values as VAEs with lower values have better reconstruction. Thescore model also learns the gap created in unconditional sampling due to this trade-off as the score modeltransforms normal Gaussian noise to posterior samples during inference. The table shows a score model onthe first two modalities of Extended PolyMnist trained using different values of 0.1,0.5, and 1 and theiraverage conditional result.",
  "A.5Mode coverage": "In this section, in addition to FID, we discuss an evaluation technique for unconditional generation. Wegenerate 10,000 unconditionally generated images from the multimodal Extended PolyMnist dataset andcount how many digits are generated from the 10,000 images for each digit. A good model should generateall digits uniformly covering all the distribution of the dataset. shows the counts of each modality",
  "A.5.1Fine-tunning the generative models using missing modalites": "We can further finetune the generative model (decoder) to increase the overall coherence. During training,we assume all the modalities are present. This condition is necessary for training p in the described setup.However, we are interested in conditional queries of p(.|xo), we can achieve a tighter lower bound by furtheroptimizing the p(xu|zu) for sample from q(zu|zo, xo). We update the parameters of decoders to maximizethe conditional log-likelihood:",
  "k=1log pk(xku|zku)zku q(.|zo, xo)(11)": "For each training example in the batch, we randomly drop each modality with probability p. Eq. 11 willincrease the likelihood of the true assignment of the dropped modalities given the observed modalities. Weevaluate this experiments on a different score model trained using NCSN Song & Ermon (2019) and withunimodal VAEs with =0.5.",
  "A.5.2Additional evaluation metrics": "In this section, we want to include additional metric for the extended PolyMnist. In addition to using aclassifier and comparing accuracy predicted using the classifier, here we will use cosine similarity and measurehow latent variable z of each modality is recovered. We will the cosine similarity of each recovered z with trueencoded z for SBM-VAE-C. For comparison, we also include MVTCAE, which uses the product of experts. shows the result of this section. The SBM model recovers the latent z that is conditionally generatedand it has a higher cosine similarity with the ground truth encoded z.",
  "Accuracy": "SBM-VAE SBM-VAE-p03 SBM-VAE-p05 SBM-VAE-p07 : Plot of finetuning experiment usingdifferent p values on how it affects conditional co-herence (different score model). The conditionalaccuracy of the last modality generated by incre-menting the given modality at a time. The x-axisshows how many modalities are given to generatethe modality and the y-axis shows the accuracy ofthe generated modality.",
  "A.7Computational Efficiency": "As discussed in the conclusion section as one limitation, SBM variant models take longer time duringinference compared to the other baselines. To show how the models are different during inference in terms ofcomputational efficiency, we evaluate the time it takes to generate conditional samples from batch of 256extended PolyMnist data. We first start with a model trained on 2 modalities and increase the modalities themodel is trained on to ten modalities. We compare MoPoE which uses a mixture of product of experts forinference and SBMVAE. We use A100 GPU for computing the time the models take. shows thetime the models take in seconds. SBM-VAE takes almost a constant time for all modalities, while MoPoEstarts increasing exponentially as the number of modalities increases. For the CelebMaskHQ dataset, asimilar batch of 256 takes approximately 95 seconds for inference while MoPoE takes approximately 0.05seconds. Here MoPoE has a clear edge as we use 1000 timesteps for this dataset on the score-based model.But one advantage of the score-based model is that the inference timesteps can be significantly decreasedwithout compromising that much quality. If we use 100 timesteps, the time required will decrease 10 folds toaround 9.5 seconds, but the performance drops very slightly. For 100 timesteps, the model retains the sameF1 performance for the attribute and mask, and only the FID of the image modality drops by a few points. Number of Modalities",
  "A.8Video-Audio Dataset": "In this section, we want to show that our model can be scaled to higher dimensional datasets and modalities.We use partial samples, approximately 100K, from the soundnet dataset (Aytar et al., 2016) and train it ontwo modalities by separating the video and the audio as two modalities from each video. We follow the setupof CoDi (Tang et al., 2023) and use pre-trained image and audio auto-encoders. The audio is changed tomelspectogram and encoded using an encoder to get zaud. We also use an autoencoder for the image andencode the image to zimg from a size of 256x256. To reconstruct the audio from the latent representation,",
  "A.9CelebMaskHQ Experimental Setup": "The CelebMaskHQ dataset is taken from Lee et al. (2020) where the images, masks, and attributes are thethree modalities. All face part masks were combined into a single black-and-white image except the skin mask.Out of the 40 attributes, 18 were taken from it similar to the setup of Wu & Goodman (2018). The encoderand decoder architectures are similar to Daniel & Tamar (2021) except MMVAE+ for the same reason as inPolyMNIST. A latent size of 256 was used for SBM models and MMVAE+, and a latent size of 1024 was usedfor MVTCAE and MoPoE. For MMVAE+, modality-specific and shared latent sizes are each 128 trainedwith IWAE estimator with K=1. For SBM-VAE, the image VAE was trained using Gaussian likelihood,posterior, and prior with = 0.1. The same applies for the mask VAE but with = 1. The attribute VAEuses Gaussian prior and posterior with a bernoulli likelihood. This applies to all other baselines with theexception of MMVAE+ which uses laplace likelihood and prior. We select the best for the baselines from[0.1,0.5,1,2.5,5]. MoPoE use of 0.1, MVTCAE 0.5 and MMVAE+ 5. For SBM-RAE, values of 104, 105,104 and Gaussian noises of mean 0 and variance of 0.001, 0.001, 0.1 were added to z before feeding to thedecoder for the image, mask, and attribute modality respectively and the best performing one was selected. The score-based models use a UNET architecture with the latent size reshaped into a size of 16x16. Wetake the mean of the posterior during training and inference time for SBM-VAE where as the z were takenduring both times for SBM-RAE. shows the detailed hyperparameters used for the score models.DiffuseVAE hyperparameters and models are the same ones used in Pandey et al. (2022) with formulation1 for the 128x128 CelebHQ dataset. The energy-based model uses an MLP network. We train 3 pairs ofmodels for each combination of modality. The supervised classifier for the attribute modality put for referencehas a similar architecture to the encoder, and the features are projected for classification and trained withcross-entropy loss. The mask classifier uses a UNET architecture with an input dimension of 3 for the imageand an output dimension of 1 to predict the masks.",
  "A.10.1Training with missing modaliites": "Multimodal models are trained on paired data coming from all modalities. Sometimes, some modality maybe missing from a data sample. In those cases, its important to make multimodal data trainable in thesecircumstances. We add this capability to the score-based model by using masking. Specifically, we maskout parts of the output score produced from the missing data. The missing data will be initially filledwith noise. To simulate the performance of the model on missing training data, we take the CelebMaskHQdataset and removing some portion of the data from the dataset. For example, we remove 20% of the datafrom each modality randomly and mix them. Note that multimodal data points that are not full are muchlarger than 20% as we first remove 20% from each modality randomly and then mix them. We evaluate theperformance of the model as shown in . Overall, MVTCAE has good performance when trainingdata is incomplete. SBMVAE has comparative outputs with the best Image FID.",
  "A.10.2High Quality Image generation for CelebAHQ": "In our setup, we use a normal variational autoencoder which makes it suitable for baseline comparison andtraining compute requirements. But since variational autoencoders suffer from low-quality and blurry images,compared to other SOTA generative models such as diffusion models (Dhariwal & Nichol, 2021), in orderto get higher-quality, we can do two things. The first is to use a high-quality pre-trained auto-encoder andtrain the score model on it which is possible because we have independent two phase training stages. Thesecond is to further increase the quality of the output of the decoder using DiffuseVAE model (Pandey et al.,2022). We illustrate the latter case here where the generated samples from SBM-VAE are fed into DiffuseVAEas shown in . The DiffuseVAE helps in generating high-quality images from the low-quality oneswithout changing the image characteristics. As the figure shows, the quality of the images is much betterwhile preserving the attributes and the masks given to generate them."
}