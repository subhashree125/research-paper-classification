{
  "Abstract": "In this article, we introduce audio-visual dataset distillation, a task to construct a smalleryet representative synthetic audio-visual dataset that maintains the cross-modal seman-tic association between audio and visual modalities. Dataset distillation techniques haveprimarily focused on image classification. However, with the growing capabilities of audio-visual models and the vast datasets required for their training, it is necessary to exploredistillation methods beyond the visual modality. Our approach builds upon the foundationof Distribution Matching (DM), extending it to handle the unique challenges of audio-visualdata. A key challenge is to jointly learn synthetic data that distills both the modality-wise information and natural alignment from real audio-visual data. We introduce a vanillaaudio-visual distribution matching framework that separately trains visual-only and audio-only DM components, enabling us to investigate the effectiveness of audio-visual integrationand various multimodal fusion methods. To address the limitations of unimodal distilla-tion, we propose two novel matching losses: implicit cross-matching and cross-modal gapmatching. These losses work in conjunction with the vanilla unimodal distribution match-ing loss to enforce cross-modal alignment and enhance the audio-visual dataset distillationprocess. Extensive audio-visual classification and retrieval experiments on four audio-visualdatasets, AVE, MUSIC-21, VGGSound, and VGGSound-10K, demonstrate the effectivenessof our proposed matching approaches and validate the benefits of audio-visual integrationwith condensed data. This work establishes a new frontier in audio-visual dataset distil-lation, paving the way for further advancements in this exciting field.Source code andpre-trained model:",
  "Introduction": "Dataset distillation aims to learn a condensed dataset such that it retains most of the essential informationof the entire training data. Recent progress in dataset distillation techniques, such as gradient matching(Zhao et al., 2020; Zhao & Bilen, 2021), trajectory matching (Cazenavette et al., 2022; Wu et al., 2023; Liuet al., 2023), and distribution matching (Zhao & Bilen, 2023; Zhao et al., 2023; Wang et al., 2022) have",
  "spaces": ": Vanilla audio-visual dataset distillation using distribution matching (A) The synthetic audio andvisual data are learned by minimizing the distribution discrepancy between real and synthetic data in thesesampled unimodal embedding spaces. (B) Mean test accuracy plot of VGGS-10k dataset for different imagesper class (IPC) settings. We observe that joint audio-visual can improve performance in data distillation.(C) Feature distribution plot of real audio()-visual() and synthetic audio()-visual() data for vanilladistribution matching(DM) and Our approach. We observe that independently distilled audio-visual syn-thetic data in DM is pulled towards their modality centers while our additional joint audio-visual lossesbetter cover the real distribution. achieved remarkable performance on the image datasets. For example, DATM (Guo et al., 2023) achieveslossless distillation using merely 1/5 and 1/10 original sizes of CIFAR100 (Krizhevsky et al., 2009) andTinyImageNet (Le & Yang, 2015) respectively. However, their potential in other domains remains largelyunderexplored. With the recent advancements in audio-visual learning (Zhao et al., 2018; Tian et al., 2018; Lin et al., 2019;Gao et al., 2018; Wei et al., 2022), the size of audio-visual datasets (Chen et al., 2020c; Xu et al., 2016;Gemmeke et al., 2017) have significantly increased, which leads to the heavy storage and computationalcost of training on these datasets. In this work, we investigate the extension of the dataset distillation tothe audio-visual domain. Unlike image distillation (Wang et al., 2018; Cazenavette et al., 2022), audio-visual distillation presents unique challenges: preserving complex cross-modal correlations and addressingthe complexities of high-resolution images and the additional audio modality. Audio-visual integration has proven beneficial for various audio-visual tasks, such as audio-visual eventlocalization (Tian et al., 2018; Wu et al., 2019; Lin et al., 2019), audio-visual sound separation (Zhao et al.,2018; Gao et al., 2018; Tian et al., 2021), and audio-visual action recognition (Kazakos et al., 2019; Zhanget al., 2022; Nagrani et al., 2020). This success raises an important question: Is audio-visual integration stilleffective when applied to distilled audio and visual data? To explore this, we propose audio-visual datasetdistillation, which aims to learn a smaller, yet representative synthetic audio-visual dataset that is usefulfor audio-visual learning tasks, distilled from the original large dataset. The task is inherently challenging.An effective audio-visual dataset distillation approach not only needs to condense data from two differentmodalities, but also preserve the natural cross-modal association between them to ensure the effectivenessof the synthetic multimodal data. To explore this new problem, we use audio-visual event recognition as the main proxy task and build ourapproach on top of Distribution Matching (DM) (Zhao & Bilen, 2023). We first propose a vanilla audio-visual distribution matching approach in which we separately train the visual-only and audio-only DM (asshown in A). This vanilla approach allows us to evaluate the effectiveness of audio-visual integrationwith condensed data and analyze the impact of different multimodal fusion methods on audio-visual event",
  "Published in Transactions on Machine Learning Research (10/2024)": "Classwise alignment. Our three losses aim at learning a classwise distribution matching (vanilla audio-visual DM loss) and classwise alignment matching (Joint Matching and Modality Gap Matching loss). Ourdistilled audio-visual data does not have any instance-wise constraints (we observed some performance degra-dation on adding such constraints) and hence training an attention-based fusion which relies on the pairedaudio-visual correspondence between the training examples, falls short in performance. Spatial Distortion. Since all the image and audio-spectrogram pixel values are learnable parameters andno additional constraint is added to preserve the spatial information from the original initialized data, thedistilled audio-visual data loses spatial information like object boundaries and frequency boundaries. Hencewhile training, the distilled audio gets confused about which part of the distilled image to look at.",
  "Related Work": "Audio-Visual Learning. Videos consist of naturally co-occurring audio and visual signals. To exploit thesynchronized and complementary information in the two modalities, several audio-visual learning problemshave been explored, such as self-supervised representation learning (Arandjelovic & Zisserman, 2017; 2018;Aytar et al., 2016; Lin et al., 2023; Gong et al., 2022; Owens & Efros, 2018; Hu et al., 2019), audio-visualsound separation (Zhao et al., 2018; Gan et al., 2020; Gao et al., 2018; Tian et al., 2021; Zhou et al., 2020),audio-visual action recognition (Zhang et al., 2022; Lee et al., 2020; Nagrani et al., 2020), audio-visualnavigation (Chen et al., 2021; 2020a;b), audio-visual event localization (Tian et al., 2018; Wu et al., 2019;Lin et al., 2019), etc. Audio-visual integration has generally demonstrated the ability to improve modelperformance compared to unimodal models in these tasks. In this work, we investigate the validity of jointaudio-visual integration in the context of dataset distillation. We employ audio-visual event recognition(Tian et al., 2018; Lin et al., 2023) and cross-modal retrieval (Surs et al., 2018; Kushwaha & Fuentes, 2023;Wu et al., 2023) as proxy to evaluate the effectiveness of condensed data. Dataset Distillation. The traditional method to reduce the training set size is coreset selection (Castroet al., 2018; Welling, 2009), which heuristically selects a subset of training data. A recent approach, datasetdistillation or condensation aims to learn a smaller dataset while still preserving the essential informationfrom the large training dataset.Unlike coreset selection methods, data condensation methods are notlimited by the subset of selected real data. It has been shown to benefit several tasks like efficient neuralarchitecture search (Zhao & Bilen, 2023; Zhao et al., 2020), continual learning (Gu et al., 2023; Sangermanoet al., 2022; Zhao et al., 2023), federated learning (Xiong et al., 2023; Huang et al., 2023), and privacypreservation (Vinaroz & Park, 2023; Dong et al., 2022). The problem was first introduced in (Wang et al.,2018), which learns condensed data with meta-learning techniques and has since been significantly improved",
  "In this section, we describe the problem formulation, introduce the proxy task for evaluating distilled syn-thetic data, and revisit distribution matching for visual-only dataset distillation": "Problem Formulation. Let xai and xvi denote the audio waveform and video frame of the i-th sample,respectively, with xavi= (xai , xvi ) and yi as the corresponding ground truth category label. Given a largeaudio-visual training set T = {xavi , yi}|T |i=1, our audio-visual dataset distillation task aims to learn a smaller,yet representative synthetic set S = {savi , yi}|S|i=1, where savi= (sai , svi ). This dataset S, with significantlyfewer samples |S| |T |, should encapsulate the essential information contained in T . The ultimate goal isfor models trained on each T and S to perform similarly on unseen test data:",
  "where D is the real test data, is the loss function (i.e. cross-entropy), is a neural network parameterizedby , and T and S are networks trained on T and S respectively": "Task.Following image dataset distillation methods (Sachdeva & McAuley, 2023; Yu et al., 2023), weuse audio-visual event recognition task as a proxy to investigate the effectiveness of audio-visual datasetdistillation. The task involves predicting the event category of a short video clip, characterized by audiowaveform xa and video frame xv. We employ an audio-visual network, illustrated in , to integrate datafrom both modalities. Specifically, the model uses a visual encoder to extract feature f v from xv and anaudio encoder extracts feature f a from the audio spectrogram ma transformed from input audio xa. Theseextracted features are then fused to feature f av by a fusion module for predicting the class-event probabilityp. We utilize cross-entropy loss as the objective function LCE = |C|i=1 yi log(pi), where |C| denotes thetotal number of event categories. Through this task, we will investigate whether integrating synthetic audio-visual data can enhance recognition in dataset distillation and to what extent different fusion methods affectthe model performance.",
  ": Audio-visual event recognition": "Revisit Distribution Matching.Dataset distilla-tion has demonstrated remarkable success in compressinglarge image datasets into smaller synthetic ones for visualrecognition tasks.Distribution matching (DM) (Zhao& Bilen, 2023) stands out as one of the prominent ap-proaches, which aims to generate synthetic data thatclosely resembles the distribution of real samples in thefeature space. This is achieved by minimizing the feature distance between the distributions of real and",
  "Forward pass Backpropagation": ": The proposed audio-visual dataset distillation framework.The synthetic audio-visual featuredistribution and real audio-visual feature distribution are matched using three main components: Vanillaaudio-visual distribution matching, Implicit Cross-Matching (ICM), and Cross-modal Gap Matching (CGM).The vanilla distribution matching loss ensures the alignment of the same modality matching while ICM andCGM facilitate cross-modal matching.Ra, Rv, Sa, Sv are the real audio, real visual, synthetic audio,synthetic visual distribution respectively. Maximum Mean discrepancy (MMD) (Gretton et al., 2012) isused as a measurement for distribution matching.The distilled image and audio spectrogram pairs areshown at an intermediate state and hence slightly modified from the initialized real data. synthetic samples, ensuring that the synthetic data effectively captures the essential characteristics of theoriginal dataset. Specifically, it uses randomly initialized neural networks as feature extractors and minimizesthe spatial distribution using an empirical estimate of maximum mean discrepancy (MMD) (Gretton et al.,2012). DM maps each training image xv Rd to a lower dimensional space using a family of parametricfunctions v : Rd Rd where d d. Here, v can be implemented using neural networks with randomweights. It also augments data by applying differential Siamese augmentation A() (Zhao & Bilen, 2021) toreal and synthetic data, where is the augmentation parameter. To this end, we will solve the followingoptimization problem:",
  "(1)": "where Pv is the distribution of network parameters. By minimizing the discrepancy between two distribu-tions in various embedding spaces by sampling v, we can learn the synthetic visual data Sv. Upon generatingthe synthetic dataset Sv, we will use it as training data to train our visual recognition model, optimizing itwith the cross-entropy loss. This trained model will then predict class labels for real test image samples.",
  "Audio-Visual Dataset Distillation": "In this section, we will explore dataset distillation approaches capable of simultaneously learning syntheticaudio and visual data for audio-visual event recognition. We will first introduce a vanilla model that extendsvisual distribution matching and then present two novel approaches tailored to this task, utilizing jointdistribution matching. Our approach is illustrated in . Vanilla Audio-Visual Distribution Matching. We build our audio-visual dataset distillation approachon the top of DM. To tackle the new multimodal task, one naive approach is to use DM to condense audio",
  "However, distilling modalities separately fails to capture the natural interplay between real audio-visual data,so we introduce two joint matching losses to better align the distilled synthetic data": ": Feature distribution plot ofreal audio()-visual() and syntheticaudio()-visual() data at an inter-mediate training stage of the vanillaaudio-visualdistributionmatching.The audio and visual features are ex-tracted from randomly initialized au-dio and visual networks, respectively.We can observe the strong modalitygap between the two modalities. Distribution Matching Between Joint Spaces.To enforcecross-modal alignment during distribution matching, one straight-forward approach is to introduce a cross-modal distribution match-ing loss. This loss function will minimize the discrepancy betweenthe distributions of real audio data and synthetic visual data, aswell as between real visual data and synthetic audio data. However,this approach has limitations due to the use of randomly initializedmodels v and a for mapping audio and visual data into sep-arate embedding spaces in DM. As shown in , there exists astrong modality gap between the two modalities. Since only the syn-thetic data is learnable, real data remains fixed, and model weightsare randomly initialized and not trainable, directly matching onemodality to another cannot mitigate the modality gap and can leadto instability during training (refer to Appendix A for a detailedexplanation of the failure exploration). This is in contrast to realdata, where audio-visual associations naturally exist.To addressthese challenges, we propose two new learning losses that encour-age cross-modal alignment in a joint embedding space, rather thanmerely performing cross-modal matching in individual unimodal au-dio and visual spaces. Implicit Cross-matching (ICM). Given the inherent challengesof achieving separate cross-modal distribution matching, we proposea novel approach that bypasses the difficulty by introducing a joint audio-visual distribution matching loss.This loss function effectively aligns the joint audio-visual distributions between real (Dr) and synthetic (Ds)data, enabling implicit cross-modal distribution matching. The loss is formally defined for each class asfollows:",
  "LavICM = ||Dr Ds||2,(6)": "Optimizing this loss term effectively compels our model to learn synthetic audio-visual data {Sa, Sv} thatclosely resembles and represents the real dataset {Ta, Tv}. Here, the loss term in Eq. 6 can be re-written asLavICM = ||( Ra + Rv) ( Sa + Sv)||2 = ||( Ra Sv) + ( Rv Sa)||2 ||( Ra Sv)||2 + ||( Rv Sa)||2. Thisformulation reveals that the loss implicitly enforces cross-modal matching between real audio and syntheticvisual data, as well as between real visual and synthetic audio data.",
  "LavMGM = ||Dav Dva||2(9)": "This addition ensures that the synthetic data closely represent the corresponding real data without misalign-ing the existing matches of Sa Ra and Sv Rv enforced in unimodal DM and Joint Matching. Witha simple re-writing, we can obtain LavICM = ||( Ra + Sv) ( Rv + Sa)||2 = ||( Ra Rv) ( Sa Sv)||2. Wecan see that it will help to align the modality gap between real and synthetic data to strengthen the jointaudio-visual distribution matching.",
  "Lavfinal = Lavbase + ICM LavICM + CGM LavCGM.(10)": "Here ICM and CGM are the weights for implicit cross-matching and cross-modal gap matching lossesrespectively. These three loss terms work collaboratively to enhance the audio-visual dataset distillationprocess. Their combined effect ensures that the synthetic data closely corresponds to the real data, aligningunimodal, cross-modal, and modality gap distributions effectively. Training algorithm. We train the synthetic data for K iterations and for each iteration, we sample randommodel a and v for audio and visual embedding. We then randomly sample real audio-visual data batchand audio-visual synthetic data batch and augmentation parameter for each class. We calculate the meandiscrepancy between each modality individually and implicit cross-matching and cross-modal gap-matchingrepresentations for each class and then sum them as loss L. We update the synthetic data by backpropagatingL for each class with learning rate . The overall training algorithm is shown in Appendix Algorithm 1. Improved initialization and storage. Herding (Welling, 2009) is a coreset selection method that greedilyselects data points to minimize the coreset center and the original data center.It has shown superiorperformance among the coreset selection methods in several previous dataset distillation research (Sachdeva& McAuley, 2023; Yu et al., 2023). We adopt herding as it aligns the initial synthetic data distribution withthe real data, offering an advantage over random initialization. The factor technique (Zhang et al., 2023;Kim et al., 2022; Zhao et al., 2023) aims to increase the number of representative features extracted fromsynthetic data without any additional cost. Specifically, given a factor parameter l each synthetic image isfactored into l2 mini-examples and then up-sampled to its original size in training. By combining herdinginitialization and factor technique, we can further improve our joint audio-visual distribution matching.(More details are provided in Appendix B)",
  "IPC129.602.3326.401.1033.771.6534.721.279.970.8336.542.521033.601.3531.631.9641.711.2740.491.8310.110.3543.851.752038.933.5235.231.1646.591.3446.051.7411.101.8849.012.44": "clip of the train/test split and have around 165k/13k samples respectively. For exploratory analyses andexperimental setup of this novel task, we randomly selected a subset of 10 classes from VGGSound with8808 train videos and 444 test videos. This subset is referred to as VGGS-10k. MUSIC-21: (Zhao et al., 2019) comprises synchronized audio-visual recordings featuring 21 distinct musicalinstruments. For our study, we focus exclusively on the solo performances subset and segment each video clipinto discrete, non-overlapping windows of one second. We randomly partition this subset into train/val/testsplits of 146,908/7,103/42,440 samples, respectively. AVE: (Tian et al., 2018) consists of 4,143 video clips spanning over 28 event categories.We segmenteach clip into non-overlapping one-second windows aligned with the synchronized annotations, resulting intrain/val/test splits of 27,726/3,288/3,305 samples, respectively. Implementation Details. Following previous distillation methods (Zhao et al., 2023; Zhao & Bilen, 2023;Cazenavette et al., 2022), we use a ConvNet architecture (Zhao et al., 2020) for both audio and visualinputs. The audio ConvNet consists of 3 blocks with convolution, normalization, ReLU, and pooling layers.For larger image inputs (224224), we use 5 such blocks. We use a learning rate of 0.2 and an SGD optimizerwith a momentum of 0.5. Our synthetic data is initialized with Herding-selected audio-visual (AV) pairs andtrained with a batch size of 128. For IPC 1 and 10, we set ICM and CGM to 10, and for IPC 20, we setthem to 20. Audio is sampled at 11kHz and transformed into 128 56 log mel-spectrograms. Baselines. For DM (Zhao & Bilen, 2023), we use the same learning rate, optimizer, and AV pair batch size.For DC (Zhao et al., 2020), DSA (Zhao & Bilen, 2021), and MTT (Cazenavette et al., 2022), we extendimage-only distillation to independently learn audio and visual distilled data. We randomly select AV pairsfor the random selection method. For Herding, we greedily select samples closest to the cluster center andfollow (Wu et al., 2023) to get AV data by concatenating AV features extracted from models trained on thewhole dataset. More details are in Appendix C. Evaluation. We report the mean accuracy and standard deviation of 3 runs where the model is randomlyinitialized and trained for 30 epochs using the learned synthetic data. Each run consists of 5000 iterationsand we use a similar setup as (Tian & Xu, 2021) to train the audio-visual event recognition models. Forreference, we also report the performance of models trained on the whole dataset under the same trainingconditions.",
  "Experimental Results": "Audio-visual integration with distilled data is still helpful. Audio-visual integration has consistentlydemonstrated superior performance over unimodal data across various tasks for real data.We aim tofurther investigate whether this advantage extends to audio-visual distilled data for different data distillationapproaches. To explore this, we employed audio-visual event recognition as a benchmark to compare theperformance of unimodal distilled data and audio-visual distilled data. Using the VGGS-10K dataset andan ensemble model trained on individually learned audio-visual data, we evaluate several data distillationmethods and different synthetic data sizes. The results, shown in Tab. 1, clearly demonstrate that audio-visual integration consistently outperforms unimodal modalities in most cases. This observation suggeststhat effective audio-visual integration remains beneficial even for distilled data. Multimodal Fusion. Audio-Visual fusion plays a crucial role in the performance of multimodal models.We investigated whether different audio-visual fusion strategies influence the performance of models trainedon distilled synthetic data.To address this question, we compared several standard audio-visual fusionapproaches: Sum, Concatenation (Concat), Ensemble, and audio-guided visual attention (Attention) (Pianet al., 2023).Tab. 2 presents the audio-visual recognition accuracy across different synthetic data sizesettings using separately learned VGGS-10K data with DM. From Tab. 2, we can see that the ensemblemethod consistently outperforms other approaches in all image-per-class settings, followed by a comparableperformance of Concatenation and Sum, and the lowest performance of attention fusion. The comparativelylow fusion results in attention fusion can be accounted for classwise alignment losses, spatial distortions (asvisualized in ), and a larger number of trainable model parameters (More details are in the Appendix).Consequently, we employ the ensemble fusion as the default. Furthermore, these results can also demonstratethat audio-visual integration with synthetic data is still helpful when employing different fusion methods. Comparison with Data Distillation Baselines. Tab. 3 compares the performance of our audio-visualdistillation method with other baselines across four datasets and three images-per-class (IPC) values. Ouraudio-visual data distillation approach consistently outperforms vanilla audio-visual distillation with DM,demonstrating the effectiveness of incorporating joint matching losses to strengthen cross-modal alignment.In addition, similar to previous image-only distillation methods (Cazenavette et al., 2022), we observe di-minishing returns as IPC increases. For instance, in MUSIC-21, theres a significant performance jump from44.02% to 68.07% when moving from 1 to 10 IPC, while the improvement from 10 to 20 IPC is more modest,reaching 70.30%. Interestingly, a simple heuristic method like herding becomes competitive starting at IPC10 and may even outperform DM and MTT in some cases. This could be attributed to the increase in",
  "OursDMMTT": ": t-SNE distribution plot of synthetic audio-visual data (IPC=10) learned by DM and Ours(l=1),with same initilization. (green, blue), (red, black) and (purple, yellow) points are the real (audio, visual)points for the first 3 classes of VGGS-10k. The synthetic (audio, visual) data is represented by (, ). Weobserve that our synthetic audio and visual distributions better resemble the real distributions. the number of learning parameters due to the high-resolution images. Scaling in DM proved more feasiblecompared to MTT, DC, and DSA. In fact, fitting visual-only distillation using MTT was not possible evenwith minimal configurations. This is due to large memory consumption when unrolling optimization throughSGD steps Cui et al. (2022); Zhou et al. (2022); Cui et al. (2023) for large datasets and higher IPC. Ablation Study. We developed two novel audio-visual distribution matching losses aimed at more effec-tively distilling essential audio-visual correlations into synthetic data.Additionally, we also use herdinginitialization and factor technique. To validate the contributions of these components, we conducted anablation study by systematic addition. The ablation study results on VGGS-10K and AVE are shown inTab. 4, from which we can see that each of the proposed parts has a positive influence on the final results.These findings unequivocally demonstrate the effectiveness of our proposed ICM and CGM losses and herdinginitialization and factor technique in helping the audio-visual data distillation process.",
  ":Visualization of dis-tilled data under different IPCs": "Visualization. To showcase our distilled data, we plot the learned au-dio and visual data in at different IPCs. We observe that with anincreased IPC, the synthetic data remains perceptually closer to the origi-nal audio-visual sample. The artifacts/patches in the synthetic data ariseas the approach distills information from the entire training data into afew synthetic samples. In addition, the spectrogram and image pixels arelearnable parameters, making artifacts more prominent in settings withsmaller IPCs. Despite this, the model achieves high accuracy on unseenreal test data, indicating that these artifacts do not negatively impactperformance. Furthermore, we compare the data distribution of the firstthree classes of VGGS-10k in . It illustrates how our approach bettercaptures the underlying distribution of the real data.",
  ": Audio-visual retrieval exam-ples.We observe close alignment ofaudio queries with top visual resultsand vice versa": "Audio-Visual Retrieval. We have demonstrated that audio-visualdistilled data facilitates learning effective audio-visual representa-tions for event recognition.To further examine the audio-visualalignment, we explore whether distilled data could help learn a well-coordinated audio-visual space for cross-modal retrieval. Since ouraudio-visual distillation model focuses on semantic alignment ratherthan instance-level alignment, we evaluate audio-visual retrieval ina class-wise setting. Following (Gong et al., 2022), we create a re-trieval test set by uniformly sampling a subset of five audio-visualsamples per class from the original test split. We train the audio andvisual ConvNet (Surs et al., 2018) with ArcFace margin loss (Denget al., 2019). The shared classifier and margin loss help to learn ajoint-modal embedding space with angular margins between classes.We train the model from scratch using the distilled data of IPC=20 and the same learning setting as theclassification model. We use these trained audio and visual components to get the corresponding representation of test samplesand calculate the class retrieval recall at rank 1, 5, and median rank based on the cosine similarity. Theresults of audio-to-visual and visual-to-audio retrieval, in Tab. 6, demonstrate that our losses help distillaudio-visual alignment (from real data) and hence our method outperforms DM in almost all scenarios. visualizes the top-3 cross-modal retrieval from the VGG-10k test set. We observe that the retrievedmodalities from the same class are closely aligned when trained with our audio-visual distilled data.5Conclusion and DiscussionIn this paper, we explore a new task of multimodal distillation using audio-visual data. To evaluate thedistilled data, we use audio-visual event recognition as the proxy task. To tackle this audio-visual datasetdistillation problem, we first introduce a vanilla AVDM method that could learn audio- and visual-onlydistilled data and simply fuse the information for event recognition.We further improve the quality ofcondensed data by introducing two new losses to strengthen cross-modal alignment. We further show theimproved alignment using the classwise cross-modal retrieval task. Experimental results on four audio-visualdatasets show that our approach outperforms other methods consistently and audio-visual integration withcondensed data is still helpful. This provides a new direction in the dataset distillation domain. As the first attempt on this new task, our work has some limitations and introduces extents that are beyondthe scope of a single paper but represent compelling directions for future research: (1) Compression-accuracytradeoff: While our approach significantly reduces training data size, it does so at the cost of a performancedrop (w.r.t whole data). Future work could explore methods to improve accuracy, potentially leveraging pre-trained weights (see Appendix F); (2) Short video segments: Our exploration focuses on short video segments.Extending our approach to long-form videos is a promising avenue for future research (see Appendix G);(3) Design for other audio-visual tasks: our approach learns synthetic data capturing class-level alignmentbetween audio and visual modalities but lacks the instance-level detail needed for more complex audio-visual tasks. Future research will focus on creating synthetic data with improved instance-level alignmentto enhance training for complex tasks such as audio-visual sound source localization. (4) Large datasets:Although our approach on VGGSound demonstrates better scalability and outperforms the baselines, the",
  "Broader Impact Statement": "Given that audio-visual datasets may contain personally identifiable information, improper handling, storage,or sharing of distilled datasets could lead to privacy breaches and unauthorized access or misuse of sensitivedata. Addressing these privacy concerns through the development of robust safeguards would be crucial. This work was supported in part by an Amazon Research Award Fall 2023. Any opinions, findings, andconclusions or recommendations expressed in this material are those of the authors and do not reflect theviews of Amazon.",
  "Yusuf Aytar, Carl Vondrick, and Antonio Torralba. Soundnet: Learning sound representations from unla-beled video. Advances in neural information processing systems, 29, 2016": "Francisco M Castro, Manuel J Marn-Jimnez, Nicols Guil, Cordelia Schmid, and Karteek Alahari. End-to-end incremental learning. In Proceedings of the European conference on computer vision (ECCV), pp.233248, 2018. George Cazenavette, Tongzhou Wang, Antonio Torralba, Alexei A Efros, and Jun-Yan Zhu. Dataset distilla-tion by matching training trajectories. In Proceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition, pp. 47504759, 2022. Changan Chen, Unnat Jain, Carl Schissler, Sebastia Vicenc Amengual Gari, Ziad Al-Halah, Vamsi KrishnaIthapu, Philip Robinson, and Kristen Grauman. Soundspaces: Audio-visual navigation in 3d environ-ments. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020,Proceedings, Part VI 16, pp. 1736. Springer, 2020a. Changan Chen, Sagnik Majumder, Ziad Al-Halah, Ruohan Gao, Santhosh Kumar Ramakrishnan, and Kris-ten Grauman. Learning to set waypoints for audio-visual navigation. arXiv preprint arXiv:2008.09622,2020b.",
  "Chun-Yin Huang, Ruinan Jin, Can Zhao, Daguang Xu, and Xiaoxiao Li. Federated virtual learning onheterogeneous data with local-global distillation. arXiv preprint arXiv:2303.02278, 2023": "Evangelos Kazakos, Arsha Nagrani, Andrew Zisserman, and Dima Damen. Epic-fusion: Audio-visual tempo-ral binding for egocentric action recognition. In Proceedings of the IEEE/CVF International Conferenceon Computer Vision, pp. 54925501, 2019. Jang-Hyun Kim, Jinuk Kim, Seong Joon Oh, Sangdoo Yun, Hwanjun Song, Joonhyun Jeong, Jung-Woo Ha,and Hyun Oh Song. Dataset condensation via efficient synthetic-data parameterization. In InternationalConference on Machine Learning, pp. 1110211118. PMLR, 2022.",
  "Noveen Sachdeva and Julian McAuley. Data distillation: A survey. arXiv preprint arXiv:2301.04272, 2023": "Mattia Sangermano, Antonio Carta, Andrea Cossu, and Davide Bacciu.Sample condensation in onlinecontinual learning.In 2022 International Joint Conference on Neural Networks (IJCNN), pp. 0108.IEEE, 2022. Didac Surs, Amanda Duarte, Amaia Salvador, Jordi Torres, and Xavier Gir-i Nieto. Cross-modal embed-dings for video and audio retrieval. In Proceedings of the european conference on computer vision (eccv)workshops, pp. 00, 2018. Yapeng Tian and Chenliang Xu.Can audio-visual integration strengthen robustness under multimodalattacks?In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp.56015611, 2021. Yapeng Tian, Jing Shi, Bochen Li, Zhiyao Duan, and Chenliang Xu. Audio-visual event localization inunconstrained videos. In Proceedings of the European conference on computer vision (ECCV), pp. 247263, 2018. Yapeng Tian, Di Hu, and Chenliang Xu. Cyclic co-learning of sounding object visual grounding and soundseparation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,pp. 27452754, 2021.",
  "Yu Wu, Linchao Zhu, Yan Yan, and Yi Yang. Dual attention matching for audio-visual event localization.In Proceedings of the IEEE/CVF international conference on computer vision, pp. 62926300, 2019": "Yuanhao Xiong, Ruochen Wang, Minhao Cheng, Felix Yu, and Cho-Jui Hsieh. Feddm: Iterative distributionmatching for communication-efficient federated learning. In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition, pp. 1632316332, 2023. Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: A large video description dataset for bridging videoand language. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.52885296, 2016.",
  "preprint arXiv:2006.05929, 2020": "Ganlong Zhao, Guanbin Li, Yipeng Qin, and Yizhou Yu. Improved distribution matching for dataset con-densation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,pp. 78567865, 2023. Hang Zhao, Chuang Gan, Andrew Rouditchenko, Carl Vondrick, Josh McDermott, and Antonio Torralba.The sound of pixels. In Proceedings of the European conference on computer vision (ECCV), pp. 570586,2018.",
  "AFailure Cross-Modal Distribution Matching": "In this section, we delve into two direct cross-modal matching strategies that, contrary to intuition, do not en-hance the alignment of audio-visual synthetic data within the audio-visual distribution matching framework.We explored the integration of alignment between audio and visual distilled data by experimenting withtwo simple cross-modal losses: Synthetic Cross-Modal Matching (SCMM) and Real-Synthetic Cross-ModalMatching (RSCMM). Synthetic Cross-Modal Matching(SCMM): Given the natural synchronization in real audio-visual datait is logical to hypothesize that implementing a loss function to bring synthetic audio and visual distributionscloser would improve the alignment of synthetic audio-visual data. Precisely, we show the SCMM loss in Eq.11:",
  "Lavcombined = Lavbase + LavSCMM + LavRSCMM(15)": "We do an ablation study to show the effect of different loss functions using one or more losses in Tab. 7.We observe a negative effect of adding these direct cross-modal losses and observe a severe drop in accuracy.This could be explained by the modality gap that is created by using randomly initialized feature extractors(a, v). To visualize the effect of these losses we plot the distribution of the first three classes of VGGS-10kdata (as shown in ) and observe that these losses lead to unstable training with the synthetic dataunable to cover the real audio-visual distribution.",
  "BHerding and Factor technique": "Herding iteratively selects data points (features) that are closest to the cumulative mean of previouslyselected points, aiming to create a subset that closely represents the overall data distribution. We use thepseudocode in to get the data subset of IPC size for each class. mean = torch.mean(features, dim=0, keepdim=True)idx_selected = []idx_left = np.arange(features.shape).tolist()for i in range(IPC):det = mean*(i+1) - torch.sum(features[idx_selected],dim=0)dis = distance(det, features[idx_left])idx = torch.argmin(dis)idx_selected.append(idx_left[idx])del idx_left[idx]",
  "Algorithm 1 Audio-Visual Dataset Distillation": "Require: Initial set of synthetic samples S for C classes, where each class c is represented by a subset Sc of synthetic data , deep neuralnetwork a parameterized with a and probability distribution over Pa, v parameterized with v and probability distributionover Pv , differentiable augmentation A parameterized with , augmentation parameter distribution , training iterations K,learning rate , loss weights ICM, CGM.Input: Real training set Tfor k = 0, . . . , K 1 do",
  "CDetailed model architecture and hyperparameters": "Following previous distillation methods (Zhao et al., 2023; Zhao & Bilen, 2023; Cazenavette et al., 2022),we use the ConvNet architecture model (Zhao et al., 2020) for both audio and visual data inputs. Theaudio ConvNet consists of 3 blocks with each block consisting of a 128-kernel convolution layer, instancenormalization, ReLU, and average pooling layer. The last average pooling layer is replaced by an adaptiveaverage pooling layer with (7,7) spatial filter to match the dimension of visual embedding (of size 6272) tofacilitate joint matching. For the large image input, we use 5 such blocks. We use the same audio and visualmodel architecture for all the experiments. For our approach, we use a learning rate of 0.2 for audio and image synthetic data and SGD optimizerwith a momentum of 0.5. We initialize our synthetic data with Herding-selected AV pairs(elaborated in thecoreset selection section) and use a real audio-visual pair batch size of 128 at each iteration. For images per",
  "Ours (l=2)Ours (l=1)": ": Performance comparison between Distribution Matching(DM) and our approach with differentfactor (l) values, across varying training steps for VGGS-10k dataset with IPC=10. The left and right plotsdiffer in initialization i.e. herding-based and random initialization respectively. We observe that herdinginitialization gives a better initialization performance and our approach performs better than DM at differentfactor values. class(IPC) 1, and 10 we keep ICM=CGM=10, while for IPC 20 we keep ICM=CGM=20. Following theprevious works(Zhang et al., 2023; Kim et al., 2022; Zhao et al., 2023) we keep the factor parameter l to 2.We sample audio at 11kHz and transform them into log mel-spectrograms using a hop length of 200 and 128mel banks, resulting in 128x56 size. The whole dataset indicates training on the entire real training set. During evaluation, the initial learning rate for the audio model is kept 1e-3, the visual part is kept 1e-4 andfor the classifier layers are kept at 1e-4. The learning rates are lowered by multiplying by 0.1 after every 10epochs.",
  "C.1Baseline implementations": "Trajectory Matching(MTT). Contrary to DM, MTT involves a range of hyperparameters that requireoptimization.For MTT, we retain 20 expert trajectories for each modality independently and conducta hyperparameter search (memory-constrained) across all synthetic images per class (IPC) and datasetconfigurations.For the Audio-visual MTT, we separately train distilled data for audio-only and visual-only MTT and then combine these datasets using an ensemble fusion to obtain AV results.The besthyperparameters for each configuration are detailed in Tab. 9. DC and DSA. The original methods are proposed for image-only dataset distillation. For audio-visualdatatset distillation, we extend the method to independently learn synthetic data for both modalities. Furtherwe use this distilled data for evaluation. We follow the optimal hyperparameters suggested by the authors(Zhao et al., 2020; Zhao & Bilen, 2021) and due to large image resolution, it does not scale to visual-onlydistillation.",
  "DAudio-Visual Fusion": "Audio-visual fusion is critical for multimodal model performance. We examined how different fusion strategiesaffect models trained on distilled synthetic data. We compared several fusion approaches: Sum, Concate-nation (Concat), Ensemble, and audio-guided visual attention (Attention) (Pian et al., 2023). The fusionfunction takes in audio feature f a and visual feature f v to get fused feature f av. Here are the formulationsof the fusion functions:Sum: It directly sums the audio and visual modality features: f av = f a + f v.",
  "------": "Concatenation: It directly concatenates the audio and visual modality features: f av = [f a; f v].Ensemble: To get the ensemble prediction pav the predictions from the audio and visual modality featuresare averaged. Given the audio and visual learnable projection matrices W a and W v respectively, we candefine pav as",
  "pav = pa + pv": "Attention: Audio-guided visual attention has shown to be an effective mechanism to learn correlationsbetween audio and visual features adaptively (Pian et al., 2023; Li et al., 2021; Tian et al., 2018). Givenlearnable projection matrices W a, W v, U a, U v and as the Hadamard product, we can formally defineattention fusion f av as:",
  "EAudio-visual retrieval": "The framework for training Audio-visual retrieval using distilled data is shown in . We alternativelytrain audio and visual encoder with ArcFace loss (Deng et al., 2019). The loss can be formulated as in eq.16. Where is the angle between the audio (or visual) embeddings and mean class embeddings, m is theadditive angular margin penalty and s is the scaling factor. For our case, we keep s = 3.0 and m=0.2.",
  "IPC139.072.9840.411.8139.622.031048.922.4354.991.7356.191.622049.481.6856.990.8058.041.68": "Ablation study of loss weights ICM, CGM. We do an abla-tion study over the effect of loss weights ICM, CGM for differentIPCs for VGGS-10k dataset and show the results in Tab. 11. Fromthe ablation, we observe that the highest accuracy for IPC 1, 10, 20occur at ICM = CGM = 10, 20, 20 respectively. Subsequently, wechoose ICM = CGM= 10 for IPC 1 and ICM = CGM = 20 forIPC 20 for all other datasets. However, we observe ICM = CGM= 10 working better for IPC 10 for other datasets (especially biggerdatasets) and hence we keep it 10 for IPC 10. Due to limited com-putational resources, we do not show ablation over other datasets.",
  "HAdditional Visualizations": "Figures 12 and 13 visualize the synthetic data for VGGS-10k for IPC=1 with l = 1 and l = 2, respectively.We observe that all the audio-visual distilled data consists of repeating patterns and is more prominent inthe visual data. Due to the repetitive patterns in (l = 1) we can ignore the fine-grained details andachieve better performance by utilizing the storage (as in (l = 2)) without increasing the syntheticsize. We also observe similar patterns in when trained with DM. We further show that the databecomes less far away from the original initialized real data in Figures 15, where we visualize VGGS-10k withIPC=10. ,17,18 visualizes the audio-visual data learnt through our method for IPC=1 and VGG,Music-21 and AVE datasets, respectively."
}