{
  "Abstract": "In real-world applications, one often encounters ambiguously labeled data, where differentannotators assign conflicting class labels. Partial-label learning allows training classifiers inthis weakly supervised setting, where state-of-the-art methods already show good predictiveperformance.However, even the best algorithms give incorrect predictions, which canhave severe consequences when they impact actions or decisions.We propose a novelrisk-consistent nearest-neighbor-based partial-label learning algorithm with a reject option,that is, the algorithm can reject unsure predictions. Extensive experiments on artificial andreal-world datasets show that our method provides the best trade-off between the number andaccuracy of non-rejected predictions when compared to our competitors, which use confidencethresholds for rejecting unsure predictions. When evaluated without the reject option, ournearest-neighbor-based approach also achieves competitive prediction performance.",
  "Introduction": "Real-world data is often noisy, for example, human annotators might assign different class labels to the sameinstance. In partial-label learning (PLL; Hllermeier & Beringer 2005; Liu & Dietterich 2012; Zhang & Yu2015; Xu et al. 2023), training instances can have multiple labels, known as candidates, of which only one iscorrect. While in some cases it is possible to sanitize such data, cleaning is costly, especially for large-scaledatasets. Instead, one wants to predict the class labels of unseen instances having sets of candidates only, thatis, without knowing the exact ground-truth labels of the training data. PLL algorithms allow for handlingsuch ambiguously labeled data. However, even the best algorithms give incorrect predictions. These errors can have severe consequences whenthey impact actions or decisions. Consider, for example, safety-critical domains such as the classification ofmedical images (Yang et al., 2009; Lambrou et al., 2011; Senge et al., 2014; Kendall & Gal, 2017; Reamaroonet al., 2019) or the control of self-driving cars (Xu et al., 2014; Varshney & Alemzadeh, 2017; Hubmann et al.,2017; Shafaei et al., 2018; Michelmore et al., 2020). One option to limit fallacies is to employ so-called rejectoptions, which allow one to abstain from certain predictions if unsure and, instead, let humans decide on thelabel of an instance or the actions to take (Mozannar et al., 2023). Naturally, there is a trade-off arisingbetween the number and accuracy of non-rejected predictions. In the supervised setting, reject options havealready been studied, both, for multi-class classification (Charoenphakdee et al., 2021; Cao et al., 2022; Maoet al., 2024; Narasimhan et al., 2024) and regression tasks (Zaoui et al., 2020; Cheng et al., 2023). In the weakly supervised PLL setting, obtaining sensible reject options is more challenging than in thesupervised case as ground truth is not available. Still, a reject option allows for mitigating misclassifications",
  "Published in Transactions on Machine Learning Research (01/2025)": "compares the computation and simulation of the expected probability masses. The values of thecomputation and simulation are indiscernible. Note that the expected probability mass of any single-itemset converges towards zero as k increases. The more sets one intersects with in (4), the more likely it is toproduce the empty set. As shown in Lemma 4.5 (part (ii)), the expected probability mass of y is greaterthan the mass of yc, independently from the number of neighbors k N.",
  "Experiments. Extensive experiments on artificial and real-world data support our claims. We makeour code and data openly available.1": "Theoretical analysis. We analyze Dst-Pll, give a closed-form expression of its expected decisionboundary under mild assumptions, and prove its risk consistency. The runtime analysis shows thatthe proposed methods runtime is dominated by k-nearest-neighbor search, which has an averagetime complexity of O(dk log n), with d features and n training instances. Structure of the paper. We discuss related work in , define the problem setting in , andpropose our method in . features experiments and concludes. All proofs andadditional experiments are in the appendices.",
  "Related Work": "Imperfect data often renders the application of supervised methods challenging. Weakly supervised learningtackles this setting and encompasses a variety of problem formulations (Bylander, 1994; Hady & Schwenker,2013; Ishida et al., 2019) including PLL. We discuss related work regarding PLL in .1. .2elaborates on related work in dealing with uncertainty in ML through the lens of reject options.",
  "Partial-Label Learning": "The early PLL approaches transfer common supervised learning frameworks to the PLL context: Grand-valet (2002) proposes a logistic regression formulation, Jin & Ghahramani (2002) propose an expectation-maximization strategy, Hllermeier & Beringer (2005) propose a k-nearest-neighbors method, Nguyen &Caruana (2008) propose an extension of support-vector machines, and Cour et al. (2011) introduce an averageloss formulation allowing for the use of any supervised method. However, these approaches (i) cannot modelthe relevance of candidate labels in the labeling process or (ii) are not robust to non-uniform noise in thecandidate sets. We emphasize that (i) and (ii) stem from two different sources of uncertainty: (i) arises fromthe lack of knowledge about the candidate label relevancies of the PLL model (epistemic uncertainty) and(ii) arises from the random label noise in the candidate sets and is inherent to the PLL problem (aleatoricuncertainty).",
  "Reject Options": "Recently, much attention has been given to the study of reject options (Mozannar et al., 2023; Mao et al.,2024; Narasimhan et al., 2024). A reject option allows one to abstain from predictions and defer them tohumans rather than making uncertain and possibly harmful decisions. There are two common strategies in rejecting predictions in the supervised setting: The confidence-basedand the classifier-rejector approach (Ni et al., 2019; Cao et al., 2022). The confidence-based strategy usesa threshold on the models confidence in order to accept or reject predictions. Common model choices forquantifying the confidences are Bayesian methods (Kingma & Welling, 2014; Kendall & Gal, 2017) andensembles (Lakshminarayanan et al., 2017; Wimmer et al., 2023). In contrast, the classifier-rejector approachjointly learns the classifier and rejector (Ni et al., 2019; Mao et al., 2024), which can have beneficial theoreticalproperties. However, the classifier-rejector approach is less flexible than the confidence-based strategy as it iscoupled with the concrete loss formulation of the classifier. We propose an extension of the confidence-basedstrategy for partial-label learning in .2. Calibration methods (Naeini et al., 2015; Guo et al., 2017; Ao et al., 2023) are also related to the confidence-based rejection strategy as both are used to make statements about the certainty of predictions. While rejectoptions provide a binary decision, calibration methods modify the predicted confidences such that they alignwith the observed accuracies. In this sense, both approaches are orthogonal and cannot directly be compared.In our work, we focus on reject options.",
  "Partial-Label Learning (PLL) with Reject Option": "Let X = Rd denote a d-dimensional real-valued feature space and Y = [l] := {1, . . . , l} the finite set of3 l N classes. A partial-label learning training dataset D = {(xi, si) | i [n]} of n instances containsfeature vectors xi X and a set of candidate labels si Y for each i [n]. All instances i have an unknownground-truth label yi Y, and yi si. Further, the candidate labels si can be partitioned into si = {yi} ziwith yi / zi, that is, zi Y \\ {yi} are the false-positive labels.",
  "nni=1 L(g(xi), si)": "As misclassifications can be quite harmful, we look at the possibility of rejecting predictions, that is,abstaining from making these predictions and, instead, deferring the decisions to humans. In the PLL setting,a reject option g : X 2Y associated with a trained classifier g either returns gs prediction (accept) orabstains from making any prediction at all (reject), that is, g(x) {, {g(x)}} for x X, with g(x) = denoting a reject. We then define the rejection probability r(g) = PX(g(X) = ) and the expected errorerr(g) = E(X,S)PXS[L(g(X), S)1{g(X)=}] of accepted predictions. Naturally, a trade-off arises betweenthe number and accuracy of accepted predictions, which leads us to the risk",
  "Dempster-Shafer Theory (DST)": "Dempster-Shafer theory (DST; Dempster 1967; Shafer 1986) allows for dealing with uncertainty by assigningprobability mass to sets of events without specifying the probabilities of individual labels; incorrect labelsdo not obtain any probability mass. DST builds upon two core quantities, so-called belief and plausibility.Informally, belief collects all evidence that supports a hypothesis and plausibility collects all evidence thatdoes not contradict a hypothesis. We argue that DST is a perfect fit for partial-label learning as one mayinterpret the ambiguous candidate sets as evidence regarding a label hypothesis. We further exploit beliefand plausibility to inform our reject option, taking into account the difference between the supporting andnon-conflicting evidence. In contrast, existing PLL approaches (Hllermeier & Beringer, 2005; Cour et al.,2011; Liu & Dietterich, 2012; Zhang & Yu, 2015; Ni et al., 2021; Xu et al., 2023) initially assign someprobability mass to each label candidate and subsequently refine them. By doing so, most probability massis first allocated to labels that are certainly incorrect, as only one candidate is the true label. This knownmethod renders handling the noise coming from incorrect candidate labels challenging. With these intuitions, we now recall DST formally. In DST, a basic probability assignment (bpa) m : 2Y assigns probability mass to subsets of Y. m satisfies m() = 0 and AY m(A) = 1. This differs fromstandard probability as P(Y) = 1 for any P M+1 (Y, 2Y) but m(Y) 1. Also, the mass allocated tonon-intersecting sets does not necessarily add up to the mass allocated to the union, that is, one can havem({1, 2}) = m({1}) + m({2}). In this sense, DST allows for more flexibility as one can allocate mass on theset {1, 2} without needing to specify any mass for {1} and {2} if uncertain. The sets A Y with m(A) > 0are called focal sets of m. The mass allocated to the set of all possible alternatives m(Y) can be interpretedas the degree of ignorance; it is the mass not supporting a specific alternative within Y. The basic probability assignment m does not induce a single probability measure P on (Y, 2Y) but rather aset of probability measures Cm(Y, 2Y), which is called credal set (Abelln et al., 2006; Cuzzolin, 2021). Theprobability measures P Cm(Y, 2Y) are restricted by imposing lower and upper bounds, which are calledbelief and plausibility, respectively. They are defined as follows:",
  "Cm(Y, 2Y) :=P M+1 (Y, 2Y) | belm(A) P(A) plm(A) for all A Y M+1 (Y, 2Y).(3)": "Further, DST provides rules to combine m-s from multiple sources (Dempster, 1967; Yager, 1987a;b). Thisis beneficial in the PLL setting as there is several conflicting evidence about the class labels within aneighborhood of instances. Estimating a credal set Cm(Y, 2Y) from such a neighborhood allows us to constructan effective reject option, which we detail in .2. Several methods already leverage DST in supervised learning (Mandler & Schmann, 1988; Denoeux, 1995;Tabassian et al., 2012; Sensoy et al., 2018; Denoeux, 2019; Tong et al., 2021). We consider the nearestneighbor approach by Denoeux (1995) to be most closely related to our approach. Here, basic probabilityassignments are constructed from the nearest neighbors of an instance. Then, Dempsters rule is used tocombine them into a single bpa. Their analysis is, however, not transferable to our case because they onlyhave singletons or the full label space as focal sets, making set intersections in the combination rule easyto handle. In this sense, we examine a more general setting since we allocate probability mass to arbitrarysubsets.",
  "Our Method: DST-PLL": "This section introduces our novel partial-label learning method Dst-Pll. Based on the labeling informationof an instances nearest neighbors, we construct basic probability assignments within Dempster-Shafertheory. These bpas inform the prediction and rejection decisions as discussed in .1 and .2,respectively. Regarding the reject option, we propose a novel variation of the confidence-based rejectionstrategy: The confidence threshold is adaptively selected on a per-instance basis dependent on the amount ofincorrect label noise. The more noise from incorrect labels there is, the more confident the model needs to beto accept a prediction. Algorithm 1 outlines Dst-Pll, which we summarize in the following. We denote by NNk(x) X 2Y theset of the k-nearest neighbors of instance x with their associated candidate labels. To predict the class labelof an instance x (Line 11), the algorithm first transforms information from xs neighbors NNk(x) into bpasmi (Lines 37), collects these into evidence set E (Line 8), and combines the bpas into m using Yagers rule(Line 10; Yager 1987a;b). .1 elaborates on these steps. .2 elaborates on how we extract ourreject option from m (Line 12). .3 shows that the resulting classification rule is risk-consistent. Weanalyze our algorithms runtime in .4.",
  "Making Predictions": "Basic probability assignments. Following the standard assumption that neighboring instances in featurespace are also close in label space, we combine the evidence from the k-nearest neighbors (xi, si) NNk(x) ofa given instance x X with its candidate labels s Y (s = Y if x is a test instance). When looking at a neighboring instance (xi, si) NNk(x), there are generally two cases: either (i) (xi, si)provides information about the correct label of x or (ii) (xi, si) is irrelevant for finding the correct label of x.To address (i), we allocate probability mass on the candidates si of neighbor xi. To address (ii), we allocateprobability mass on the full label space Y indicating uncertainty about the correct label of x. More formally, for fixed i [k], the candidate labels si do not provide any valuable information if they supportall (s si) or none (s si = ) of the labels in s; we use a bpa of mi(s) = 1 (Line 4). We set mi(A) = 1/2 ifA = s or A = s si, else mi(A) = 0 (Line 6), where 1/2 equally weights evidence. We later elaborate furtheron this choice and demonstrate the application of the proposed classification rule in Example 4.1. Note thatwe make the common assumption that the true label of instance x is always in s (Cour et al., 2011; Liu &Dietterich, 2012; Lv et al., 2020; Ni et al., 2021). While our definition of the mi-s is similar to (Denoeux,",
  "), we target a more general setting as our focal sets can be arbitrary subsets instead of only singletons orthe full label set": "The bpa mi has the following four effects on belief and plausibility as defined in (2): (i) A set of candidates Ahas maximal belief, that is, belmi(A) = 1, if it covers s, that is, s A. (ii) A set of candidates A is plausible,that is, plmi(A) > 0, if it supports at least one of the candidate labels in s, that is, A s = . (iii) There is agap, that is, belmi(A) < plmi(A), if A supports some candidate in s si but does not cover all candidates ins si or supports some candidate in s but does not cover all candidates of s. (iv) Class labels y s si aremaximally plausible, that is, plmi({y}) = 1. Evidence weighting. Our definition of the mi-s (Algorithm 1, Line 6) also permits a more general view,that is, mi(A) = if A = s, mi(A) = 1 if A = s si, and mi(A) = 0 otherwise, for some (0, 1).However, without further assumptions, one cannot know how relevant the information from a particularneighbor is. The setting of = 1/2, which we use, weights supporting and conflicting evidence of all neighborsequally. In other words, if a neighbors evidence excludes some candidate labels from consideration, it is ofequal importance compared to supporting some candidate labels. Therefore, we set = 1/2. Evidence combination. Given the set E = {mi | i [k]}, we combine all mi-s using Yagers rule (Yager,1987a;b). Dempsters original rule (Dempster, 1967) enforces m() = 0 by normalization, which is criticized forits unintuitive results when facing high conflict (Zadeh, 1984). Instead, Yagers rule first collects overlappingevidence in q : 2Y and creates a valid bpa m : 2Y by",
  "Proposed Reject Option": "To limit the impact of misclassification, our method provides a reject option g, that is, the algorithmcan abstain from individual predictions if unsure (Algorithm 1, Line 12). Our formulation builds on theconfidence-based rejection strategy (.2), that is, := conf(g) with conf(g) being themodels confidence and the confidence threshold. We adapt this setting to the PLL context bychanging the confidence threshold m based on the amount of noise present. Recall from (3) that the belief and plausibility regarding m act as a lower and upper bound of the probabilitymass, respectively. The intuition of our reject option is as follows. If the lower bound (belief) on theprobability mass of our predicted label exceeds the maximal upper bound (plausibility) on the probabilitymass regarding any other label, we can safely make the prediction. In other words, if there is a class label different from the predicted one that is quite plausible (highmaxys\\{y} pl m({y})), we require a high belief mass in order to be certain about the prediction, that is, thebelief must satisfy bel m({y}) > maxys\\{y} pl m({y}). If, instead, there is no other plausible candidate label,we can be sure of our prediction with less belief mass.",
  "withy := arg maxys m({y}).(5)": "In other words, we instantiate the models confidence with the models belief mass conf(g) = bel m({y}) ofthe predicted instance y and the confidence threshold m based on the amount of noise regarding other labelsy = y, that is, m = maxys\\{y} pl m({y}). Note that the dependence on m allows for setting the thresholdadaptively. (5) satisfies several desirable properties, which we collect in Theorem 4.2 and elaborate on in the following.Theorem 4.2. Let Y be the label space, x X the instance of interest, s Y its candidate labels (s = Y ifx is a test instance), g(x) our algorithms prediction, and m the resulting probability mass as determined byAlgorithm 1. Then, the following hold:",
  "Based on these considerations, we define the accept and reject regions of our method as follows": "1. Accept. When m > 0, we are in the accept region and g(x) = {y}: The lower-bound probabilityof the class label y is greater than the upper-bound probability of any other class label y = y, that is,P({y}) > P({y}) for all P C m(Y, 2Y) and y = y. 2. Reject. When m 0, we are in the reject region and g(x) = : The lower-bound probability ofthe class label y is less than or equal to the upper-bound probability of another class label y = y.There exists P C m(Y, 2Y) and y = y with P({y}) P({y}).",
  "Consistency": "Our classification rule yields a risk-consistent classifier, which we demonstrate in the following. As is commonin the literature (Cour et al., 2011; Liu & Dietterich, 2012; Feng et al., 2020; Lv et al., 2020) and required toobtain statistical guarantees in the PLL setting, we fix a label (noise) distribution (Assumption 4.4) thatpermits further analysis of the proposed algorithm. Appendix D experimentally verifies that Assumption 4.4is satisfied on real-world datasets. Assumption 4.4. Let x X be the instance of interest with hidden true label y Y and l = | Y | 3classes. Its k partially-labeled neighbors are (xi, si) NNk(x). Label yc Y \\{y} denotes the class label thatco-occurs most frequently with label y in xs neighborhood. We assume that the true label dominates theneighborhood, that is,",
  "with p1, p2, p3 (0, 1), p1 + p2 + p3 = 1, and p1 p2 p3 > 0": "Because p1 + p2 > p3, Assumption 4.4 implies PXY (Y = y | X = xi) > PXY (Y = y | X = xi), that is, pointsthat are close in feature space are likely to have similar class labels. We note that this assumption is relatedto the ambiguity degree condition by Cour et al. (2011) as both make sure that the noise labels do notoverwhelm the PLL algorithm. Assumption 4.4 enforces this as p1 p2 p3 > 0. Having all cases spelledout as in Assumption 4.4 benefits the proof of Lemma 4.5.",
  "In the following, we elaborate on the four cases (i) (iv)": "If a candidate set contains all labels (s = Y), no labels (s = ), or does not allow to be partitionedinto true label y and false-positive labels z, it is not a valid candidate set (Hllermeier & Beringer,2005; Cour et al., 2011; Liu & Dietterich, 2012; Zhang & Yu, 2015). (i) handles this pathologicalsetting by assigning zero probability.",
  "In (iii), xs label y is also the label of neighbor xi. Label yc is not part of the candidate set si ofneighbor xi. There are 2l2 sets s that contain y but not yc": "In (iv), neighbor xi has different labels altogether and is irrelevant for explaining the class label ofinstance x. There are 2l1 1 sets that do not contain the label y (excluding the empty set). Eachof the |s| candidates can be the neighbors correct label. Theorem 4.6, which establishes risk-consistency, hinges on the following intermediate results in Lemma 4.5.The fact that the mi-s can have arbitrary focal sets renders the direct application of Yagers rule (4) in our",
  "(ii) EP[ m({y}) | X = x] > EP[ m({yc}) | X = x]": "As the probability mass of the hidden true label y is positive in expectation by (i), we can reduce our analysisto the first case of our decision rule (Algorithm 1, Line 11). (ii) shows how Assumption 4.4 propagates whenapplying Yagers rule on all k neighbors mi-s. The label that dominates the neighborhood obtains the highestprobability mass. To our knowledge, it is the first time that such a result has been shown for arbitrary focalsets in m. Denoeux (1995) analyze the special case when all focal sets are singletons or the full label space. To put Theorem 4.6 into context, we recall the concept of risk consistency (Devroye et al., 1996). The Bayesclassifier is defined by g = arg ming:XY R(g); it has the least overall risk. Let gn be the classifier trainedby Alg. 1 with n instances. Its empirical risk R(gn) can be computed by substituting the expectation with asample mean. It is risk-consistent if R(gn) R(g) for n almost surely. Theorem 4.6 establishes therisk consistency of the proposed classifier. Theorem 4.6. Assume the setting of Assumption 4.4, let gn : X Y be the classifier trained by Algorithm 1with n training instances, g : X Y the Bayes classifier, and x X a fixed instance with unknown truelabel y. Then, the following hold:",
  "Runtime Complexity": "We decompose the overall runtime of our approach in Algorithm 1 into k-times querying one nearest neighborand creating its bpa (Lines 29), the cost of Yagers rule (Line 10), as well as extracting predictions inLines 11-12. Using the ball-tree data structure (Omohundro, 1989), querying one neighbor takes O(d log n)time on average. In the worst case, query time is O(dn). One builds a ball-tree in O(dn log n) time. Weconstruct a bpa mi by storing its focal sets within a hash map and combine all mi-s as defined in (4). Thereare at most min2k, 2lfocal sets of m: Each mi has at most two focal sets producing 2k combinationsand there are at most 2l possible subsets of Y. We take the minimum as both are upper bounds. Lookingup a focal set in the hash-map requires O(l) time as the key length is variable. Extracting a predictionand the reject option then requires Ol2time. Combining the above yields a worst-case complexity ofOdkn + lmin2k, 2l+ max(k, l). Since k and l are constant, the nearest-neighbor search dominates.The average total runtime of the search is O(dk log n).",
  "Experimental Setup": "Data. Following the default protocol (Cour et al., 2011; Zhang & Yu, 2015; Lv et al., 2020; Xu et al., 2023),we conduct several experiments using datasets for supervised learning with added artificial noise as well asexperiments on real-world partially-labeled data. We repeat all experiments five times to report averages andstandard deviations. For the supervised datasets, we use the ecoli (Horton & Nakai, 1996), multiple-features(Duin, 2002), pen-digits (Alpaydin & Alimoglu, 1998), semeion (Buscema & Terzi, 2008), solar-flare (Dodson& Hedeman, 1989), statlog-landsat (Srinivasan, 1993), and theorem datasets (Bridge et al., 2013) from theUCI repository (Bache & Lichman, 2013). These datasets contain between 336 and 10 992 instances each.Also, we use the popular MNIST (LeCun et al., 1999), KMNIST (Clanuwat et al., 2018), and FMNISTdatasets (Xiao et al., 2018), which contain 60 000 images each similar to other datasets like Cifar-10 andCifar-100 (Krizhevsky, 2009). For the partially labeled data, we use the bird-song (Briggs et al., 2012),flickr (Huiskes & Lew, 2008), yahoo-news (Guillaumin et al., 2010), and msrc-v2 datasets (Liu & Dietterich,2012). They contain between 1755 and 22 762 instances. Noise Generation. We use three noise generation strategies to partially label the supervised datasets:uniform, class-dependent, and instance-dependent noise. Uniform noise (Liu & Dietterich, 2012) adds threeuniform random noise labels to a fraction of 70 % of all instances. Class-dependent noise (Cour et al.,2011) randomly partitions all class labels into pairs and adds the partner label as noise to 70 % of allinstances having the other label. Instance-dependent noise (Zhang et al., 2021) first trains a supervisedclassifier g : X M+1Y, 2Y. Given an instance x with true label y, a flipping probability of y(x) :=gy(x)/ maxyY \\{y} gy(x) for y = y determines which noise labels to randomly pick.",
  "Results": "Prediction Performance. shows the average test-set accuracies and standard deviations over allUCI datasets with class- and instance-dependent noise, MNIST datasets with class- and instance-dependentnoise, and the real-world datasets. The algorithms with the highest accuracies as well as the algorithms withnon-significant differences using a paired t-test with level = 0.05 are emphasized. Our approach (Dst-Pll)performs comparably to the other methods. We note that none of the methods is best across all settings. Forexample, Cc performs best in four out of five settings but is significantly outperformed by our approach onthe real-world experiments. Reject Option. To compare our reject option with the other methods, we use a threshold of m > 0 forour proposed approach (Algorithm 1, Line 12), a confidence threshold of 90 % for classifiers outputting aprobability distribution over the class labels, and a threshold of 50 % of all votes for Pl-Knn to not reject aprediction, which is in line with the reject option by Hellman (1970). shows the average empirical reject-option risk and standard deviation across all experiments forvarying . We compute R(g) = err(g) + r(g) by using ground-truth information to calculate thenon-reject error err(g) and counting the number of rejects to calculate the reject rate r(g). The algorithmswith the lowest risks as well as the algorithms with non-significant differences using a paired t-test with level",
  "Class-dep.Inst.-dep.Class-dep.Inst.-dep": "Pl-Knn (2005)81.2 ( 13.9)75.8 ( 11.1)92.2 (4.1)84.8 (7.2)53.4 ( 10.9)Pl-Svm (2008)62.3 ( 16.3)43.8 ( 16.4)67.5 (9.8)47.8 (8.2)39.1 (9.6)Ipal (2015)79.3 ( 17.1)75.3 ( 18.3)92.9 (4.3)88.5 (6.6)58.7 (9.8)Pl-Ecoc (2017)63.7 ( 13.1)66.5 ( 12.5)64.3 ( 14.3)51.7 ( 10.7)46.2 ( 10.2)Proden (2020)81.6 ( 14.0)78.1 ( 13.2)93.9 (4.4)88.2 (6.0)64.2 (8.2)Cc (2020)81.3 ( 14.2)78.8 ( 13.6)93.9 (4.5)89.6 (5.7)49.2 ( 29.7)Valen (2021)79.7 ( 15.3)75.6 ( 12.7)91.8 (4.2)83.4 (8.4)63.6 (9.7)Pop (2023)81.5 ( 14.0)78.1 ( 13.1)93.9 (4.5)88.1 (6.0)63.6 (8.4)CroSel (2024)79.9 ( 17.2)78.4 ( 14.5)94.2 (4.5)88.9 (6.7)46.3 ( 28.9)",
  "= 0.00 = 0.05 = 0.10 = 0.15 = 0.20": "Pl-Knn (2005)0.11 ( 0.17)0.15 ( 0.18)0.18 ( 0.19)0.21 ( 0.20)0.25 ( 0.21)Pl-Svm (2008)0.19 ( 0.27)0.23 ( 0.28)0.27 ( 0.29)0.31 ( 0.30)0.35 ( 0.31)Ipal (2015)0.19 ( 0.17)0.20 ( 0.18)0.21 ( 0.19)0.22 ( 0.20)0.23 ( 0.21)Pl-Ecoc (2017)0.25 ( 0.27)0.29 ( 0.27)0.34 ( 0.28)0.38 ( 0.28)0.43 ( 0.28)Proden (2020)0.12 ( 0.11)0.13 ( 0.11)0.14 ( 0.12)0.15 ( 0.12)0.16 ( 0.13)Cc (2020)0.16 ( 0.18)0.16 ( 0.19)0.17 ( 0.20)0.18 ( 0.20)0.19 ( 0.21)Valen (2021)0.20 ( 0.14)0.20 ( 0.14)0.20 ( 0.14)0.20 ( 0.14)0.21 ( 0.14)Pop (2023)0.12 ( 0.11)0.13 ( 0.11)0.14 ( 0.12)0.15 ( 0.12)0.16 ( 0.13)CroSel (2024)0.15 ( 0.20)0.16 ( 0.21)0.17 ( 0.21)0.18 ( 0.21)0.19 ( 0.22)",
  "AlgorithmsFraction of rejects ( std.)Non-rejected test accuracy ( std.)": "Pl-Knn (2005)50.19 % ( 20.98 %)91.23 % ( 10.12 %)Pl-Svm (2008)50.19 % ( 20.98 %)74.40 % ( 19.77 %)Ipal (2015)50.19 % ( 20.98 %)83.52 % ( 16.08 %)Pl-Ecoc (2017)50.19 % ( 20.98 %)73.92 % ( 17.11 %)Proden (2020)50.19 % ( 20.98 %)94.03 % (8.23 %)Cc (2020)50.19 % ( 20.98 %)90.13 % ( 17.89 %)Valen (2021)50.19 % ( 20.98 %)86.81 % ( 13.04 %)Pop (2023)50.19 % ( 20.98 %)94.02 % (8.20 %)CroSel (2024)50.19 % ( 20.98 %)89.71 % ( 18.67 %)",
  "(a) Ecoli (inst.-dep. noise)(b) KMNIST (inst.-dep. noise)(c) msrc-v2": ": Trade-off between the fraction of rejected predictions and the accuracy of non-rejected predictionsfor three experiments: Ecoli with instance-dependent noise, KMNIST with instance-dependent noise, andthe real-world dataset msrc-v2. We show the trade-off curves for varying confidence (0 to 1) and m (-1 to 1)thresholds. We highlight the points corresponding to a threshold of m = 0 for our method, a confidencethreshold of 90 % for methods with a probability output, and a threshold of 50 % of all votes for Pl-Knn.We refer to Appendix D.4 for all reject trade-off curves across all experimental settings. = 0.05 are emphasized. When misclassification is costly ( 0.2), our method provides the significantlybest trade-off compared to our competitors. In contrast, when rejecting predictions is costly ( > 0.2), themethods in are to be preferred. shows the non-rejected test accuracy of all methods across all experimental settings for a fixed fractionof rejects. To obtain the results, we tune the confidence thresholds such that each competitor rejectsa similar number of instances as our proposed approach. Our approach uses the threshold > 0, for whichwe have proved several desirable guarantees in Theorem 4.2. Our method achieves superior test-set accuracyon the non-rejected predictions. shows the reject trade-off for varying confidence (0 to 1) and m (-1 to 1) thresholds on the ecoliand KMNIST datasets with instance-dependent noise as well as on the msrc-v2 real-world dataset. Thex-axes show the fractions of predictions that are rejected. The y-axes show the accuracies of predictions thatare not rejected. The plots show (fraction of rejects, non-rejected test-set accuracy)-pairs corresponding todifferent settings of the thresholds. Most methods have monotonic growth: The more predictions are rejected,the more accurate are non-rejected predictions. Also, it is desirable to be close to the top-left corner of theplots as one wants to achieve high accuracy while rejecting as few predictions as possible. Note that the point(1, 1) cannot be observed as the test-set accuracy is undefined if all predictions are rejected. Given a desired rejection rate r(g), also allows for numerically finding the appropriate value of that minimizes R(g). Varying in (1), while having err(g) and r(g) fixed, yields a straight line in showing all possible trade-offs.",
  "Conclusions": "When misclassification is costly, reject options provide a principled way of alleviating the consequences ofincorrect predictions. In this work, we have presented a novel nearest-neighbor-based partial-label learningalgorithm that can reject predictions if uncertain. We have demonstrated the desirable properties of theproposed reject option both from a theoretical (Theorem 4.2) and practical perspective. Our wide range ofexperiments showed the effectiveness of our classification rule and reject option on supervised datasets withadded artificial noise and partially labeled real-world datasets.",
  "Tom Bylander. Learning linear threshold functions in the presence of classification noise. In Conference onComputational Learning Theory, pp. 340347, 1994": "Vivien Cabannes, Alessandro Rudi, and Francis R. Bach. Structured prediction with partial labelling throughthe infimum loss. In International Conference on Machine Learning, volume 119, pp. 12301239, 2020. Yuzhou Cao, Tianchi Cai, Lei Feng, Lihong Gu, Jinjie Gu, Bo An, Gang Niu, and Masashi Sugiyama.Generalizing consistent multi-class classification with rejection to be compatible with arbitrary losses. InAdvances in Neural Information Processing Systems, 2022. Nontawat Charoenphakdee, Zhenghang Cui, Yivan Zhang, and Masashi Sugiyama. Classification withrejection based on cost-sensitive classification. In International Conference on Machine Learning, volume139, pp. 15071517, 2021.",
  "Stephen C. Hora. Aleatory and epistemic uncertainty in probability elicitation with an example fromhazardous waste management. Reliability Engineering and System Safety, 54(2-3):217223, 1996": "Paul Horton and Kenta Nakai. A probabilistic classification system for predicting the cellular localizationsites of proteins. In International Conference on Intelligent Systems for Molecular Biology, pp. 109115,1996. Constantin Hubmann, Marvin Becker, Daniel Althoff, David Lenz, and Christoph Stiller. Decision making forautonomous driving considering interaction and uncertain prediction of surrounding vehicles. In IntelligentVehicles Symposium, pp. 16711678, 2017.",
  "Li-Ping Liu and Thomas G. Dietterich. A conditional multinomial mixture model for superset label learning.In Advances in Neural Information Processing Systems, pp. 557565, 2012": "Jiaqi Lv, Miao Xu, Lei Feng, Gang Niu, Xin Geng, and Masashi Sugiyama. Progressive identification oftrue labels for partial-label learning. In International Conference on Machine Learning, volume 119, pp.65006510, 2020. Eberhard Mandler and Jrgen Schmann. Combining the classification results of independent classifiersbased on the Dempster-Shafer theory of evidence. Machine Intelligence and Pattern Recognition, 7:381393,1988. Anqi Mao, Mehryar Mohri, and Yutao Zhong. Predictor-rejector multi-class abstention: Theoretical analysisand algorithms. In International Conference on Algorithmic Learning Theory, volume 237, pp. 822867,2024. Rhiannon Michelmore, Matthew Wicker, Luca Laurenti, Luca Cardelli, Yarin Gal, and Marta Kwiatkowska.Uncertainty quantification with statistical guarantees in end-to-end autonomous driving control.InInternational Conference on Robotics and Automation, pp. 73447350, 2020. Hussein Mozannar, Hunter Lang, Dennis Wei, Prasanna Sattigeri, Subhro Das, and David A. Sontag. Whoshould predict? Exact algorithms for learning to defer to humans. In International Conference on ArtificialIntelligence and Statistics, volume 206, pp. 1052010545, 2023.",
  "A.1Proof of Theorem 4.2": "Part (i). Given an instance x X with its candidate set s Y and its associated prediction g(x) as describedin Algorithm 1, we assume that g(x) has been picked randomly from arg maxAs m(A) (Algorithm 1, Line 11,second case), so maxys m({y}) must be zero because we are not in the first case in Line 11. Therefore,",
  "independent of the choice of y Y. Therefore, m 0": "Part (ii).Given an instance x X with its candidate set s Y and its associated prediction g(x)as described in Algorithm 1, we assume that m > 0.By the contraposition of part (i), g(x) = ywith y = arg maxys m({y}). From (5) and m > 0, it follows that bel m({y}) > maxys\\{y} pl m({y}),which is equivalent to bel m({y}) > pl m({y}) for all y s \\ {y}. For all P C m(Y, 2Y), it holds thatbel m(A) P(A) pl m(A) for all A Y by (3). Therefore, P({y}) pl m({y}) < bel m({y}) P({y}) for ally s \\ {y}. Part (iii). (): Given an instance x X with its candidate set s Y and its associated predictiong(x) as described in Algorithm 1, we assume that m({g(x)}) > 1/2. Because maxys m({y}) > 0, weare in the first case in Line 11 of Algorithm 1. Therefore, g(x) = y with y = arg maxys m({y}). As",
  "AY, A{y}= m(A) < 1/2. Hence, pl m({y}) < bel m({y}) for all y s with y = y. Therefore, m = bel m({y}) maxys\\{y} pl m({y}) > 0": "(): In the following, we provide a counter-example. Let s = Y = {1, 2, 3} and m be defined by m(A) = 0.4if A = {1}, m(A) = 0.3 if A = {1, 2} or A = {1, 3}, else m(A) = 0. Then, y = 1 is our prediction since it hasthe highest probability mass. The prediction is not rejected because m = 0.4 0.3 = 0.1 > 0. However,m({y}) < 1/2.",
  ",": "the proposition yields (ii), in (iii) we rearrange the sum and collect both products, and (iv) expands thefractions. In (v), we rearrange the factor within the product and apply the (i)-s, which hold by similarcalculations as (i), and Lemma B.1 with k = k + 1 implies (vi). These derivations and the observation thatthe base case guarantees the inequality to be strict conclude the proof.",
  "2l2> 1 2l2 > 1,": "where (i) holds as Assumption 4.4 guarantees that p2 p3 > 0. The last statement is satisfied for l 3,which shows that the dominance assumption w.r.t. the label distribution propagates through the computationof belief when using Yagers rule. We note that the result is independent of p1 as a candidate set can notdistinguish the belief in y and yc when these co-occur.",
  "In (iii), we then compute the expected value by taking into account all possible combinations of setsA1, . . . , Ak Y producing the intersection {y}": "Term (iv) enumerates all cases where kr=1 Ar = {y}. To produce this intersection, label y needsto be contained in all sets Ar. At most k 1 sets can be the full label space, that is, h sets satisfyAr = Y. The label yc with which y is most often confused has to be missing in at least one set Ar,that is, i sets Ar satisfy yc Ar. All remaining labels y3, . . . , yl Y also have to be missing in atleast one set Ar each, that is, ja sets Ar satisfy ya+2 Ar, respectively.",
  "kht.(8)": "To produce the intersection {yc} in Yagers rule, all k h sets with which we intersect need to contain yc.The variable t denotes how many out of k h sets contain the true label y. Therefore, y and yc co-occur in aset in t cases, with which the probability (1",
  "2l11p3)kht is associated according to Assumption 4.4": "Simulation. To simulate the expected probability masses, we randomly draw k candidate sets si accordingto the label distribution in Assumption 4.4, apply Algorithm 1, and report the belief of y and yc. We repeatthis 100 000 times and average the results. For l = 3, y = 1, and yc = 2, we obtain the label distribution",
  "D.1Applicability of Assumption 4.4": "Given a fixed instance x, Assumption 4.4 describes the label distribution of the neighbors candidate sets.Thereby, we assume that the true label y dominates the neighborhood and that label yc co-occurs most oftenwith the true label y. Otherwise, we assume uniform noise among the remaining noise labels. demonstrates that those assumptions are largely satisfied on the four real-world datasets (see.3). In most cases, the true label y occurs most often in the neighborhood of instance x. There areone or two other labels with which y is commonly confused, which we model by yc. Apart from that, allremaining noise labels are distributed uniformly.",
  "As mentioned in .1, we consider ten commonly used PLL approaches. We choose their parametersas recommended by the respective authors": "Pl-Knn (Hllermeier & Beringer, 2005): For all non-MNIST datasets, we use k = 10 neighborsas recommended by the authors. For the MNIST datasets, we use the hidden representation of avariational auto-encoder as instance features and use k = 20. The variational auto-encoder has a768-dimensional input layer (flat MNIST input), a 512-dimensional second layer, and 48-dimensionalbottleneck layers for the mean and variance representations. The decoder uses a 48-dimensional firstlayer, a 512-dimensional second layer, and a 768-dimensional output layer with sigmoid activation.Otherwise, we use ReLU activations between all layers. Binary cross-entropy is used as a reconstructionloss. We choose the AdamW optimizer for training.",
  "Pl-Ecoc (Zhang et al., 2017): We use L = 10 log2(l) and = 0.1 as recommended": "Proden (Lv et al., 2020): For a fair comparison, we use the same base models for all neural-network-based approaches. We use a standard d-300-300-300-l MLP (Werbos, 1974) for the non-MNISTdatasets with ReLU activations, batch normalizations, and softmax output. For the MNIST datasets,we use the LeNet-5 architecture (LeCun et al., 1998). We choose the Adam optimizer for training.",
  "Valen (Xu et al., 2021): We use the same base models as mentioned above for Proden": "Pop (Xu et al., 2023): We use the same base models as mentioned above for Proden. Also, we sete0 = 0.001, eend = 0.04, and es = 0.001. We abstain from using the data augmentations discussed inthe paper for a fair comparison. CroSel (Tian et al., 2024): We use the same base models as mentioned above for Proden. We use10 warm-up epochs using Cc and cr = 2. We abstain from using the data augmentations discussedin the paper for a fair comparison. Dst-Pll (our proposed approach): Similar to Pl-Knn and Ipal, we use k = 10 neighbors for thenon-MNIST datasets. For the MNIST datasets, we use the hidden representation of a variationalauto-encoder as instance features and use k = 20. The architecture of the variational auto-encoder isthe same as described above for Pl-Knn.",
  "D.3Parameter Sensitivity": "shows the sensitivity of the number of neighbors k regarding the test-set performance, the fraction ofconfident / non-rejected predictions, and the non-rejected prediction performance. The shaded areas indicatethe standard deviation regarding the 5-fold cross-validation. As for default k-nearest neighbor classification,changes of k have a relatively large impact. We show parameter sensitivity for each of the real-world datasetsseparately. Naturally, different datasets have different optimal parameter settings. The configuration k = 10,which is also recommended within Pl-Knn (Hllermeier & Beringer, 2005) and Ipal (Zhang & Yu, 2015),provides a good trade-off between the number of confident predictions and how accurate confident predictionsare. Indeed, this setting produces a good number of confident predictions on most datasets (; centerplot). At the same time, it produces a good MCC performance of confident predictions on most datasets(; right plot).",
  ": Sensitivity of k regarding the test-set MCC score, the fraction, and the MCC score of confident /non-rejected predictions": "When increasing k, our methods behavior on different datasets can be assembled into two groups. On thebird-song, mir-flickr, and yahoo-news datasets, increasing k past ten neighbors also increases the amountof irrelevant labeling information from those neighbors. Therefore, our approach produces less confidentpredictions. At the same time, the MCC score of confident predictions remains at roughly the same level.This is because irrelevant labeling information from neighbors increases at most at the same rate as k.In contrast, on the lost and msrc-v2 datasets, the MCC score of confident predictions drops sharply at acertain point while the number of confident predictions decreases similarly. This is because irrelevant labelinginformation increases more rapidly than k: The decrease of confidence in predictions is slower than theincrease of irrelevant candidate labels.",
  "D.4Reject Trade-off Curves": "(i) (xxxiv) shows the reject trade-off for varying confidence (0 to 1) and m (-1 to 1) thresholdsand augments by considering all datasets and noise generation strategies. The x-axes show thefractions of predictions that are rejected. The y-axes show the accuracies of predictions that are not rejected.The plots show (fraction of rejects, non-rejected test-set accuracy)-pairs corresponding to different settingsof the thresholds. In most cases, our method provides a better trade-off between the number of rejectedpredictions and the accuracy of the non-rejected predictions. summarizes all plots by showing theaverage empirical risks across all experimental settings and for different trade-off parameters . We recallthat our method provides the significantly best trade-offs for [0, 0.2]."
}