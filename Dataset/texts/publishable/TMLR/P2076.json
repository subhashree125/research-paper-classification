{
  "Abstract": "Recent advancements in equivariant deep models have shown promise in accurately predictingatomic potentials and force fields in molecular dynamics simulations.Using sphericalharmonics (SH) and tensor products (TP), these equivariant networks gain enhanced physicalunderstanding, like symmetries and many-body interactions. Beyond encoding physicalinsights, SH and TP are also crucial to represent equivariant polynomial functions. In thiswork, we analyze the equivariant polynomial functions for the equivariant architecture, andintroduce a novel equivariant network, named PACE. The proposed PACE utilizes edgebooster and the Atomic Cluster Expansion (ACE) technique to approximate a greater numberof SE(3) Sn equivariant polynomial functions with enhanced degrees. As experimented incommonly used benchmarks, PACE demonstrates state-of-the-art performance in predictingatomic energy and force fields, with robust generalization capability across various geometricdistributions under molecular dynamics (MD) across different temperature conditions. Ourcode is publicly available as part of the AIRS library",
  "Introduction": "Deep learning has led to notable progress in computational quantum chemistry tasks, such as predictingatomic potentials and force fields in molecular systems (Zhang et al., 2023). Fast and accurate predictionof energy and force is desired, as it plays crucial roles in advanced applications such as material designand drug discovery. However, it is insufficient to rely solely on learning from data, as there are physicschallenges that must be taken into consideration. For example, to better consider symmetries inherent in3D molecular structures, equivariant graph neural networks (GNNs) have been developed in recent years.By using equivariant features and equivariant operations, SE(3)-equivariant GNNs ensure equivariance ofpermutation, translation and rotation. Thus, their internal features and predictions transform accordingly asthe molecule is rotated or translated. Existing equivariant GNNs can specialize in handling features witheither rotation order = 1 (Schtt et al., 2021; Jing et al., 2021; Satorras et al., 2021; Du et al., 2022; 2023;",
  "Published in Transactions on Machine Learning Research (08/2024)": "Algorithm 1 Code sketch of PACE network. The PACE architecture comprises embedding layers, twodistinct message passing (MPNN) layers, and an output layer. The proposed edge booster is integrated intothe first MPNN layer to generate the boosted edge message m1ij. In the polynomial many-body interactionmodule, we utilize additional self-interactions to produce different atomic bases Aiv. Require: Zi, |N(i)|, pi, rij, rij, E One-hot encoding of node by atomic types Zi, the number of neighbornodes |N(i)|, node positions pi, edge orientation vector rij, edge distance rij, the averaged total energy ofthe training set E.",
  "Symmetries": "Incorporating physical symmetries into machine learning models is pivotal for tackling quantum chemistrychallenges. This is because numerous quantum properties of molecules inherently exhibit equivariance orinvariance to symmetry transformations. For instance, if we rotate a molecule in 3D space, forces actingon atoms rotate accordingly while the total energy of the molecule remains invariant. Besides rotation, thepermutation of atoms within a molecule represents another type of symmetry. In such scenarios, the forces onatoms obey permutation equivariance, while the molecular energy remains invariant to atomic permutations.To summarize, atomic forces can be characterized as SE(3)-equivariant to rotational transformations andSn-equivariant to atomic permutations, reflecting the respective symmetry groups G associated with thesesymmetry operations. Concurrently, molecular energy exhibits SE(3)-invariance and Sn-invariance. Mathematically (Bronstein et al., 2021), given a group G and group action , we say a function f mappingfrom source domain Q to target domain Y is G-equivariant if f(g q) = g f(q), q Q, g G. Similarly,if f(g q) = f(q) holds, we say f is G-invariant. Due to the intrinsic SE(3) and Sn equivariant andinvariant symmetries in quantum chemistry, it is natural to encode these symmetries directly into the modelarchitectures for effectively approximating the function f.",
  "Polynomial Functions in Equivariant GNNs": "Building on this concept of learning a powerful invariant or equivariant function f, various equivariant networkshave been developed. These networks (Thomas et al., 2018; Batatia et al., 2022b; Yu et al., 2023b) aredesigned for mapping 3D molecular structures to inherently symmetric properties, such as energy, force, andthe Hamiltonian matrix. Typically, these networks capture directions r and distances r by using real sphericalharmonics Y (r) combined with radial basis functions (rbf). Owing to their equivariance property, sphericalharmonics are advantageous for encoding geometric information and predicting the physical properties ofmolecules. It is noteworthy that spherical harmonics, as partially discussed in Dusson et al. (2022), can also serve asproper features for representing SE(3) invariant and equivariant polynomial functions with the help of tensorproducts. Here, we illustrate how spherical harmonics represent polynomial functions using a specific example.We start with a polynomial function without SE(3) invariance and equivariance constrain. In this polynomialfunction denoted as P D(x, y, z) : R3 R1, D is the highest degree of the polynomial terms, and r = (x, y, z).By observing Equations (1) to (3), we can find polynomial terms of P D(x, y, z) with D 2 in Y =0, Y =1, andY =1 Y =1, where denotes tensor product, C is the coefficients of the spherical harmonics, and Y =1(r)t represents the tensor product of Y =1(r) with itself, repeated t times. Thats to say, the polynomial functionsP D2(x, y, z) can be represented as a linear combination of entries in Y =0, Y =1, and Y =1 Y =1.",
  "there exists a linear function mapping from Y =0(r), Y =1(r), Y =1(r)2 into P D2SE(3)[k](x, y, z), 1 k K,": "which then comprise P D2SE(3)(x, y, z). Given that both the input features and polynomial functions are SE(3)equivariant, the corresponding linear mapping also retains this equivariance. From this, we can concludethat for any polynomial function P D2SE(3)(x, y, z), there exists an equivariant linear function mapping fromY =0(r), Y =1(r), and Y =1(r)2 into this function. This principle of linear universality is also discussed inDym & Maron (2020).",
  "(3)": "As previously elucidated, spherical harmonics and their tensor products provide a way to approximatethe polynomial functions, ensuring the preservation of SE(3)-equivariance. In existing equivariant graphnetworks (Batzner et al., 2022; Batatia et al., 2022b; Liao & Smidt, 2023), spherical harmonics usually serve ascomponents of edge messages to provide SE(3) equivariant features. For Sn permutation equivariance, GraphNeural Networks (GNNs)(Kipf & Welling, 2017; Velikovi et al., 2018; Gao & Ji, 2019; Liu et al., 2020; Caiet al., 2021; Chen et al., 2024) employ a message passing scheme to aggregate neighboring messages for eachcentral node. Therefore, by adhering to the aggregation of this message passing scheme, SE(3)-equivariantfeatures can successfully attain Sn equivariance. Specifically, edge messages are constructed using sphericalharmonics Y =1(rij), leading to aggregated equivariant features that are analogous to",
  "Atomic Energy with Local Environment for Equivariant Polynomial Functions": "The approximation of polynomial functions in point cloud networks has been explored previously, as detailedin .1 (Maron et al., 2019; Keriven & Peyr, 2019; Dym & Maron, 2020). In this series of analyses,the architectures capacity to approximate invariant or equivariant polynomial functions is demonstrated,using 3D coordinates (p1, p2, , pn) as inputs. However, the analysis for equivariant networks in predictingsymmetric physical properties is still underexplored. In energy and force prediction tasks, atomic clusterexpansion (ACE) is a widely used technique for approximating atomic energy. ACE decomposes the totalenergy Ei into a sum of individual atomic energies Ei for each atom i, and then uses local environments tolearn these atomic energies, formulated as",
  "+ ,(7)": "where i = (rij1, , rijN ) denotes N edges in the atomic environment, rij denotes edge direction anddistance from atom i to atom j, denotes the single edge basis function, and c represents the coefficients.Therefore, in the context of approximating atomic energy, as opposed to their use in point cloud networks,the polynomial function of node features fxi here focuses on the N-edge system i rather than on atomiccoordinates. The research by Dym & Maron (2020) introduces the concept of D-spanning function sets to denote thecapability of covering SE(3) Sn equivariant polynomial function with the highest degree D. Expandingupon their work, a D-spanning function for approximating atomic energy within an N-bond system is definedas",
  "Motivation of PACE": "Inspired by the goal of approximating SE(3) Sn equivariant functions with full spanning and higherpolynomial degrees for the atomic energy based on the local atomic environment, in this work, we proposean equivariant network PACE following the message passing scheme. To briefly summarize using the sameexample from Equation (4), PACE incorporates a novel edge booster that leverages the tensor product ofspherical harmonics from rtij , t max to construct edge messages analogous to rtij , t max N boost. Hence,the aggregated equivariant features in PACE become",
  "Universality Analysis": "Universality is a powerful property for neural networks that can approximate arbitrary functions. While Zaheeret al. (2017); Maron et al. (2019); Keriven & Peyr (2019) study the universality of permutation invariantnetworks, several works have recently studied the rotational equivariant networks. Dym & Maron (2020) takesuse of the proposed tensor representation to build D-spanning family and shows that Tensor Field Networks(TFN) (Thomas et al., 2018) is proved to be a universal equivariant network capable of approximating arbitraryequivariant functions defined on the point coordinates of point cloud data. Furthermore, GemNet (Gasteigeret al., 2021) uses the conclusion in Dym & Maron (2020), and is proved to be a universal GNN with directededge embeddings and two-hop message passing.",
  "Equivariant Graph Neural Networks": "In recent years, equivariant graph neural networks have been developed for 3D molecular representation learn-ing, as they are capable of effectively incorporating the symmetries required by the specific task (Ruddigkeitet al., 2012; Chmiela et al., 2017; Yu et al., 2023a; Khrabrov et al., 2022). Existing equivariant 3D GNNs canbe broadly classified into two categories, depending on whether they utilize order = 1 equivariant featuresor higher order > 1 equivariant features. Methods belonging to the first category (Satorras et al., 2021;Schtt et al., 2021; Deng et al., 2021; Jing et al., 2021; Thlke & Fabritiis, 2022) achieve equivariance byapplying constrained operations on order 1 vectors, such as vector scaling, summation, linear transformation,vector product, and scalar product. The second category of methods (Thomas et al., 2018; Fuchs et al., 2020;Liao & Smidt, 2023; Batzner et al., 2022; Batatia et al., 2022a;b; Brandstetter et al., 2021) predominantlyemploys tensor products (TP) to preserve higher-order equivariant features, with some works (Luo et al.,2024) focusing on accelerating tensor product computations.",
  "Atomic Cluster Expansion": "Molecular potential and force field are crucial physical properties in molecular analysis. To approximate theseproperties, the atomic cluster expansion (ACE) (Drautz, 2019; Kovcs et al., 2021) is used to approximatethe atomic potential. Recently, several neural networks aiming to predict atomic potential and forces havebeen developed to consider many-body interactions by incorporating ACE into their model architectures.Specifically, BOTNet (Batatia et al., 2022a) takes multiple message passing layers to encode the many-body",
  ", t1, , tvmax max N boost, spanning a D-spanning fam-": "ily with D = max {max N boost, vmax}, as explained in .4. During our experiments, we set max = 3and vmax = 3 following previous work (Batatia et al., 2022b; Musaelian et al., 2023) for a fair comparsion.Although both MACE and PACE can form a D-spanning family with D = 3, our PACE model can stillapproximate a greater number of polynomial functions with D > 3 in this scenario. Moreover, by usingadditional self-interactions, PACE needs fewer channels to approximate the same amount of positions inD-spanning functions.",
  "The Proposed PACE": "The node features x0i are initialized through a linear transformation applied to its atomic type. Edgesconstructed based on cutoff distance have orientations rij denoted by spherical harmonics Y (rij), andpairwise distances rij embedded using a learnable radial basis function R. The proposed PACE comprises twodistinct message passing layers followed by an output layer. The illustration of PACE architecture is providedin . In this section, we will introduce the architecture of our proposed PACE model as well as thecorresponding equivariant polynomial functions from the perspective of irreducible representations. Detailedinformation about the irreducible representations used in equivariant GNNs can be found in Appendix A.1.As shown in Appendix B.1, the irreducible representations can also represent a D-spanning family. Theorem 4.1. For any D-spanning function Q(t)K (i) appeared in QDK and for any position P = (p1, p2, , pk)in tensor representation, where pk R3 denotes the element position, if there exists w1 and irreps1 such thatQ(t)K (i)(P) = m wm1 irrepsm1 , and the set of irreps1 forms a D-spanning family.",
  "The First Layer": "Following the message passing neural network, the first step is to build edge messages from neighboring nodesto the central node. As discussed in .4, the construction of these edge messages for node pairs inequivariant neural networks generally requires a tensor product to combine edge spherical harmonics andnode features at each layer. Diverging from this conventional design, the first layer of PACE uses the edgebooster two consecutive tensor products, which means N boost = 2, to construct the edge message as",
  "i": ": An architecture overview of PACE. A: The first message passing layer. The initial node features x0iand x0j are concatenated and transformed by an MLP. Then, a tensor product is applied to the transformednode features and edge spherical harmonics Sphij with an RBF and MLP transformed edge distance aslearnable weights. Next, its output is further combined with Sphij using another tensor product, and theweights involve the initial node features and edge distance. These operations in the yellow box comprisethe edge booster, aiming to enhance the models expressiveness. The obtained edge-boosted messagesare aggregated to form the atomic base. Then, an update module comprising a polynomial many-bodyinteraction module and a skip connection is used to update the features of central nodes. B: The secondmessage passing layer. A tensor product is applied to the edge message and updated node features thatare obtained from the first layer. The learnable weights of the tensor product are based on the initial nodefeatures and distance of each neighboring pair. Then, edge messages are aggregated and node features areupdated similarly to those in the first layer. C: An example of 4-body interaction in ACE. We aim to fit v1(rij1) v2(rij2) v3(rij3) using Aiv1 Aiv2 Aiv3, where {} denotes the atomic base in ACE.D: Polynomial many-body interaction module. The atomic base Ai is fed to multiple self-interaction layersseparately to produce different Aiv. Then, tensor contraction is performed to produce ai. E: Output. Theinvariant part of node features produced by both layers are transformed and summed to predict the deviationfrom the total molecular energy to its average.",
  "Next, Ai is processed by a polynomial many-body interaction module to produce features ai. The correspondingequivariant polynomial form of ai is Q(t)K=3(i) =": "j1,j2,j3Ni rt1ij1 rt2ij2 rt3ij3 , t1, t2, t3 6 for P D(i)with D 3 as shown in Appendix B.2.2. These features are then combined with the initial node features toupdate for each node. Details of the polynomial many-body interaction module are described in .3,and the design of the first layer is illustrated in A.",
  "W vccAic + b = 0W vccAic > 0,(20)": "where is the rotation order of irreps, c is the channel index for Ai and c is the channel index for Ai. Then,we use tensor contraction (Batatia et al., 2022b) with generalized Clebsch-Golden to fuse multiple atomicbases. Tensor contraction is illustrated in Appendix A.4 As shown in Equation (25) in Appendix, in the case of two irreps, Clebsch-Gordan coefficients C3m31m1,2m2 areused to maintain equivariance when fusing two irreps with rotation orders 1 and 2 to the output 3, andthe triplet (1, 2, 3) is defined as a path. When fusing N irreps, the generalized Clebsch-Gordan coefficientsused to maintain the equivariance can be defined as",
  "where W is the path weight, a is the intermediate irreps, and v N+ starts from N to 1 to incorporateN-body interactions. Note that C = 1 and L0 = 0 when v = 1, and a = 0 when v = N": "In contrast to MACE (Batatia et al., 2022b) applying tensor contraction to the atomic base Ai, our PACEfirst takes multiple self-interactions to distinguish the atomic base from the same Ai to different Aiv fordifferent body orders before applying the contraction. According to Darby et al. (2023), the output oftensor contraction in MACE is defined as the tensor decomposed product basis while the one obtained inPACE is called tensor sketched product basis. Although Darby et al. (2023) has demonstrated that thetensor-decomposed basis can consistently reconstruct the tensor-sketched basis with a factor of vmax timeschannels, we implement the latter one in PACE to improve the model expressiveness without increasing thenumber of channels.",
  "Summary of Key Contributions": "Edge BoosterThe first key contribution of our work is the edge booster, which is computed by Equa-tions (12) to (16) and illustrated as the yellow-boxed operations in A. Our edge booster leveragesconsecutive tensor products on spherical harmonics which helps enhance an equivariant models abilityto approximate higher-degree polynomial functions. This idea is shown in Equation (10) and the overallpolynomial degree is shown in Appendix B.2.2. There might be various implementations to build an edgebooster following the idea of Equation (10). However, we choose to design our edge booster as describedby Equations (12) to (16) for two reasons. First, our edge-boosted message only considers spherical harmonicsand atomic types of nodes. Note that the computation of our edge booster does not involve node featuresthat are updated layer by layer. Such a design follows our motivation to create an independent edge boostermodule that can produce an edge-boosted message to replace the original edge spherical harmonics used ineach layer. Based on this design, we only need to compute the edge booster once in the first layer and we canreuse the edge-boosted message in all other layers. As shown in A&B, the edge-boosted message m1ijis used in both layers. Hence, the design of our edge booster also considers computational costs. In summary,the result of our edge booster can be used in each layer to help approximate more polynomial functions withhigher degree in an efficient manner. A detailed cost analysis of edge booster is provided in Appendix D.2.3. Additional Self-interaction LayersThe second enhancement comes from the additional self-interactionlayers used in the polynomial many-body interaction module. In .3, we use the tensor contractionproposed in MACE (Batatia et al., 2022b) to approximate many-body interactions, but we apply additionalself-interactions (SI) to the atomic base Ai so that we use different inputs of the tensor contraction. In MACE,the atomic base Ai is directly fed into the tensor contraction, while the atomic base Ai in our design is firstfed to multiple self-interaction layers separately to produce different atomic bases Aiv. The functionalityof these self-interactions is to increase the ability to approximate more positions in D-spanning functions.",
  "UracilE1.10.63.05.1-0.40.40.60.50.3F6.64.217.621.43.83.13.21.82.12.0": "Specifically, to approximate n different positions in D-spanning function, PACE without SI needs vmaxnchannels, while PACE with SI only needs n channels. This is because the SI in the polynomial many-bodyinteraction module enables every single channel in PACE to approximate one position in D-spanning functions.The corresponding proof is shown in Appendix B.2.2. The total number of self-interactions equals the maxcorrelation order vmax, where vmax corresponds to (body order 1). In our implementations, we use threeself-interactions because the highest body order we consider in each layer is 4. Although vmaxn channelswithout self-interactions can achieve the same expressiveness theoretically, we find adding these additionalself-interactions can significantly enhance the empirical performance.",
  "Experiments": "We conduct experiments on three molecular dynamics simulation datasets, the revised MD17 (rMD17), 3BPAand AcAc datasets. The proposed PACE is trained using these datasets to predict both the invariant energyof the entire molecule and the equivariant forces acting on individual atoms. Among our baselines (Kovcset al., 2021; Christensen et al., 2020; Bartk et al., 2010; Smith et al., 2017; Gasteiger et al., 2021; Batzneret al., 2022; Batatia et al., 2022a; Musaelian et al., 2023; Batatia et al., 2022b), NequIP, BOTNet, Allegroand MACE are all equivariant graph neural networks (GNNs) with rotation order > 1. In particular,BOTNet, Allegro, and MACE incorporate many-body interactions, while ACE is a parameterized physicalmodel that does not belong to the class of neural networks. Our experiments are implemented with PyTorch1.11.0 (Paszke et al., 2019), PyTorch Geometric 2.1.0 (Fey & Lenssen, 2019), and e3nn 0.5.1 (Geiger & Smidt,2022). In experiments, we train models on a single 11GB Nvidia GeForce RTX 2080Ti GPU and Intel XeonGold 6248 CPU.",
  "functional theory (DFT). These structures capture the diverse conformational space of the molecules and arevaluable for studying their quantum properties": "Setup. In our experiments, we use officially provided random splits. Next, we use the same splitting seed asMACE to further divide the training set into a training set comprising 950 structures and a validation setcomprising 50 structures. Then, we perform our evaluations on the test set with 1000 structures. Trainingdetails are provided in Appendix C.2. Results. summarizes the performance of our proposed method in comparison to baselines on all tenmolecules in the rMD17 dataset. Mean absolute errors (MAE) are employed as the evaluation metric forboth energy and force predictions. It is worth noting that our PACE demonstrates state-of-art performancein energy prediction across all molecules. Specifically, we achieved significant improvements of 33.3%, 33.3%and 30.8% on Benzene, Toluene, and Paracetamol, respectively. In terms of force prediction, PACE achievesstate-of-the-art performance on nine out of the ten molecules, exhibiting substantial improvements of 26.7%,16.7% and 15.4% on Toluene, Paracetamol, and Azobenzene, respectively. Besides, we attain the second-beston Uracil. The comparison of algorithm efficiency and the affect from rotation order can be found inAppendix D.2 and D.3.",
  "The 3BPA & AcAc Datasets": "Dataset. The 3BPA dataset (Kovcs et al., 2021) is also generated through molecular dynamic simulationsemploying Density Functional Theory (DFT). Unlike rMD17, this dataset is specifically focused on a singleflexible molecule, namely the 3BPA molecule. 3BPA is characterized by three freely rotating angles, whichprimarily induce structural changes at varying temperatures. The AcAc dataset (Batatia et al., 2022a) is alsogenerated through molecular dynamic simulation using Density Functional Theory (DFT). Similar to 3BPA,this dataset specifically focuses on a single flexible molecule, Acetylacetone. Given that many real-worldapplications involve temperature fluctuations, evaluating a models robustness and extrapolation capabilitiesunder varying temperature conditions is crucial for predicting molecular behavior accurately. 3BPA and AcAcdatasets are frequently employed to assess a models out-of-distribution (OOD) generalization capability toMD geometric distributions across different temperatures for the same molecule.",
  "PACE (-SI)E0.95 (0.01)4.0 (0.2)F5.7 (0.1)21.3 (1.0)": "Setup. For both 3BPA and AcAc, We follow MACE to train our model using a training set consisting of450 structures and a validation set comprising 50 structures. Both the training and validation sets weresampled at 300K. Both 3BPA and AcAc have two test sets sampled at 300K and 600K. 3BPA has anothertest set sampled at 1200K. Moreover, 3BPA has an additional test set, including optimized conformationswith dihedral slices, where two dihedral angles are fixed and the third angle varies from 0 to 360 degrees.This test set probes regions of the potential energy surface (PES) that are distant from the training set. Weprovide training details in Appendix C.2. Results. and summarize the performance of our proposed method in the 3BPA and AcAcdatasets, respectively. Here, root-mean-square error (RMSE) is used as the evaluation metric. On 3BPAdataset, the proposed PACE shows comparable performance to MACE, while outperforming other baselinemethods significantly. On AcAc dataset, our PACE demonstrates significant performance improvement fromall baselines. Moreover, we use the model trained on 3BPA to conduct molecular dynamic simulations andthe comparison of PACE-generated trajectories and the ground-truth trajectories is shown in Appendix D.1.",
  "Ablation Studies": "The first ablation study aims to demonstrate the effectiveness of the proposed edge booster, designed tofacilitate a higher polynomial degree. In this experiment, we remove the second tensor product, namely theedge booster, from the first layer of PACE. As indicated in , this comparison reveals that the edgebooster, which enhances expressiveness, indeed imrpoves the models performance in force field prediction. The second ablation study is for demonstrating the effectiveness of using different atomic bases Aiv fordifferent body orders in many-body interaction. In this experiment, we remove additional self-interactionoperations and directly use Ai in symmetric contraction in the polynomial many-body interaction module. Thecomparison shown in justifies that different atomic bases Aiv produced by additional self-interactionscan improve the expressiveness and model performance without increasing the number of channels.",
  "Conclusion": "In this work, we introduced PACE, a new equivariant network for atomic potential and force field predictions.Our PACE is designed from two perspectives, increasing the ability to approximate more positions in D-spanning functions, as well as expanding the space to cover a greater number of higher degree polynomialequivariant functions. The integration of the edge booster and Atomic Cluster Expansion (ACE) techniquewith additional self-interactions allows PACE to approximate SE(3) Sn equivariant polynomial functionswith higher degrees, thereby improving the accuracy in prediction. The comprehensive experimental resultsand analyses provide compelling evidence for the efficacy of the proposed PACE model. In the future, we aimto advance the development of models capable of approximating polynomial functions of even higher degreeswhile simultaneously maintaining high computational efficiency.",
  "Albert P Bartk, Mike C Payne, Risi Kondor, and Gbor Csnyi. Gaussian approximation potentials: Theaccuracy of quantum mechanics, without the electrons. Physical review letters, 104(13):136403, 2010": "Ilyes Batatia, Simon Batzner, Dvid Pter Kovcs, Albert Musaelian, Gregor NC Simm, Ralf Drautz,Christoph Ortner, Boris Kozinsky, and Gbor Csnyi. The design space of E(3)-equivariant atom-centeredinteratomic potentials. arXiv preprint arXiv:2205.06643, 2022a. Ilyes Batatia, David P Kovacs, Gregor Simm, Christoph Ortner, and Gbor Csnyi. Mace: Higher orderequivariant message passing neural networks for fast and accurate force fields.Advances in NeuralInformation Processing Systems, 35:1142311436, 2022b. Simon Batzner, Albert Musaelian, Lixin Sun, Mario Geiger, Jonathan P Mailoa, Mordechai Kornbluth, NicolaMolinari, Tess E Smidt, and Boris Kozinsky. E (3)-equivariant graph neural networks for data-efficient andaccurate interatomic potentials. Nature communications, 13(1):2453, 2022.",
  "Lei Cai, Jundong Li, Jie Wang, and Shuiwang Ji. Line graph neural networks for link prediction. IEEETransactions on Pattern Analysis and Machine Intelligence, 44(9):51035113, 2021": "Pei Chen, Soumajyoti Sarkar, Leonard Lausen, Balasubramaniam Srinivasan, Sheng Zha, Ruihong Huang,and George Karypis. Hytrel: Hypergraph-enhanced tabular data representation learning. In Advances inNeural Information Processing Systems, 2024. Stefan Chmiela, Alexandre Tkatchenko, Huziel E Sauceda, Igor Poltavsky, Kristof T Schtt, and Klaus-RobertMller. Machine learning of accurate energy-conserving molecular force fields. Science advances, 3(5):e1603015, 2017.",
  "Ralf Drautz. Atomic cluster expansion for accurate and transferable interatomic potentials. Physical ReviewB, 99(1):014104, 2019": "Weitao Du, He Zhang, Yuanqi Du, Qi Meng, Wei Chen, Nanning Zheng, Bin Shao, and Tie-Yan Liu. Se (3)equivariant graph neural networks with complete local frames. In International Conference on MachineLearning, pp. 55835608. PMLR, 2022. Weitao Du, Yuanqi Du, Limei Wang, Dieqiao Feng, Guifeng Wang, Shuiwang Ji, Carla Gomes, and Zhi-MingMa. A new perspective on building efficient and expressive 3d equivariant graph neural networks. arXivpreprint arXiv:2304.04757, 2023. Genevieve Dusson, Markus Bachmayr, Gbor Csnyi, Ralf Drautz, Simon Etter, Cas van der Oord, andChristoph Ortner. Atomic cluster expansion: Completeness, efficiency and stability. Journal of Computa-tional Physics, 454:110946, 2022.",
  "Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. InInternational Conference on Learning Representations, 2017": "Dvid Pter Kovcs, Cas van der Oord, Jiri Kucera, Alice EA Allen, Daniel J Cole, Christoph Ortner, andGbor Csnyi. Linear atomic cluster expansion force fields for organic molecules: beyond rmse. Journal ofchemical theory and computation, 17(12):76967711, 2021. He Li, Zun Wang, Nianlong Zou, Meng Ye, Runzhang Xu, Xiaoxun Gong, Wenhui Duan, and Yong Xu.Deep-learning density functional theory hamiltonian for efficient ab initio electronic-structure calculation.Nature Computational Science, 2(6):367377, 2022.",
  "Yi Liu, Limei Wang, Meng Liu, Yuchao Lin, Xuan Zhang, Bora Oztekin, and Shuiwang Ji. Spherical messagepassing for 3d molecular graphs. In International Conference on Learning Representations, 2022": "Shengjie Luo, Tianlang Chen, Yixian Xu, Shuxin Zheng, Tie-Yan Liu, Liwei Wang, and Di He.Onetransformer can understand both 2D & 3D molecular data. In International Conference on LearningRepresentations, 2023. URL Shengjie Luo, Tianlang Chen, and Aditi S Krishnapriyan. Enabling efficient equivariant operations in thefourier basis via gaunt tensor products. In International Conference on Learning Representations, 2024.",
  "Haggai Maron, Ethan Fetaya, Nimrod Segol, and Yaron Lipman. On the universality of invariant networks.In International conference on machine learning, pp. 43634371. PMLR, 2019": "Albert Musaelian, Simon Batzner, Anders Johansson, Lixin Sun, Cameron J Owen, Mordechai Kornbluth,and Boris Kozinsky. Learning local equivariant representations for large-scale atomistic dynamics. NatureCommunications, 14(1):579, 2023. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen,Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, ZacharyDeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, andSoumith Chintala. PyTorch: An Imperative Style, High-Performance Deep Learning Library. Advances inNeural Information Processing Systems, 32, 2019. Lars Ruddigkeit, Ruud Van Deursen, Lorenz C Blum, and Jean-Louis Reymond. Enumeration of 166 billionorganic small molecules in the chemical universe database gdb-17. Journal of chemical information andmodeling, 52(11):28642875, 2012.",
  "Philipp Thlke and Gianni De Fabritiis. Equivariant transformers for neural network based molecularpotentials. In International Conference on Learning Representations, 2022": "Nathaniel Thomas, Tess Smidt, Steven Kearnes, Lusann Yang, Li Li, Kai Kohlhoff, and Patrick Riley. Tensorfield networks: Rotation-and translation-equivariant neural networks for 3D point clouds. arXiv preprintarXiv:1802.08219, 2018. Oliver Unke, Mihail Bogojeski, Michael Gastegger, Mario Geiger, Tess Smidt, and Klaus-Robert Mller.SE (3)-equivariant prediction of molecular wavefunctions and electronic densities. Advances in NeuralInformation Processing Systems, 34:1443414447, 2021a. Oliver T Unke, Stefan Chmiela, Michael Gastegger, Kristof T Schtt, Huziel E Sauceda, and Klaus-RobertMller. Spookynet: Learning force fields with electronic degrees of freedom and nonlocal effects. Naturecommunications, 12(1):7273, 2021b.",
  "Petar Velikovi, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio.Graph attention networks. In International Conference on Learning Representations, 2018": "Chengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He, Yanming Shen, and Tie-YanLiu. Do transformers really perform badly for graph representation? Advances in Neural InformationProcessing Systems, 34:2887728888, 2021. Haiyang Yu, Meng Liu, Youzhi Luo, Alex Strasser, Xiaofeng Qian, Xiaoning Qian, and Shuiwang Ji. QH9: Aquantum hamiltonian prediction benchmark for QM9 molecules. arXiv preprint arXiv:2306.09549, 2023a. Haiyang Yu, Zhao Xu, Xiaofeng Qian, Xiaoning Qian, and Shuiwang Ji. Efficient and equivariant graphnetworks for predicting quantum Hamiltonian. In Proceedings of the 40th International Conference onMachine Learning, 2023b.",
  "Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Russ R Salakhutdinov, and Alexander JSmola. Deep sets. Advances in neural information processing systems, 30, 2017": "Linfeng Zhang, Jiequn Han, Han Wang, Roberto Car, and EJPRL Weinan. Deep potential moleculardynamics: a scalable model with the accuracy of quantum mechanics. Physical review letters, 120(14):143001, 2018a. Linfeng Zhang, Jiequn Han, Han Wang, Wissam Saidi, Roberto Car, et al. End-to-end symmetry preservinginter-atomic potential energy model for finite and extended systems. Advances in neural informationprocessing systems, 31, 2018b. Xuan Zhang, Limei Wang, Jacob Helwig, Youzhi Luo, Cong Fu, Yaochen Xie, ..., and Shuiwang Ji. Artificialintelligence for science in quantum, atomistic, and continuum systems. arXiv preprint arXiv:2307.08423,2023.",
  "A.1Spherical Harmonics and Irreducible Representation": "To encode geometric information of molecules into SE(3)-equivariant features, spherical harmonics are usedfor its equivariance property. Specifically, we use real spherical harmonic basis functions Y to encode anorientation rij between a node pair. If the molecule is rotated by a rotation matrix T in 3D coordinatesystem, then we have:qY (T rij) = (D(T)q)Y (rij),(24) where [0, L] is the degree, q with size 2 + 1 denotes coefficients of spherical harmonics, and the WignerD-matrix D(T) with size (2 + 1) (2 + 1) specifies the corresponding rotation acting on coefficientsq. In practice, equivariant features are often represented by irreducible representations (irreps), whichcorrespond to the coefficients of real spherical harmonics. Concretely, irreducible representations are formedby the concatenation of multiple type- vectors, where [0, L]. More detailed explanation about irreduciblerepresentations can be found in Liao & Smidt (2023).",
  "A.2Tensor Product": "Equivariant networks using higher order > 1 equivariant features (Thomas et al., 2018; Fuchs et al., 2020;Liao & Smidt, 2023; Batzner et al., 2022; Batatia et al., 2022a;b) predominantly employs tensor products(TP) to preserve higher-order equivariant features. Tensor product operates on irreducible representations uof rotation order 1 and v of rotation order 2, yielding a new irreducible representation of order 3 as",
  "m2=2C(3,m3)(1,m1),(2,m2)u1m1v2m2,(25)": "where C denotes the Clebsch-Gordan (CG) coefficients (Griffiths & Schroeter, 2018) and m N denotes them-th element in the irreducible representation. Here, 3 satisfies |1 2| 3 1 + 2, and 1, 2, 3 N.High-order equivariant 3D GNNs commonly use tensor products on the irreducible representations of neighbornodes and edges to construct messages as",
  "v1v2v3c(3)v1v2v3v1 (rij1) v2 (rij2) v3 (rij3) + ,(27)": "where i = (rij1, , rijN ) denotes the N bonds in the atomic environment, is the single bond basisfunction and c is the coefficients. The computational complexity of modeling many-body potential increasesexponentially with number of neighbors. To reduce the complexity, ACE further makes use of density trickto calculate the atomic energy via atomic base Aiv =",
  "A.4Tensor Contraction in Polynomial Many-Body Interaction Module": "In the polynomial many-body interaction module of PACE, we apply tensor contraction to different atomicbases for encoding the interaction of multiple nodes around the central node. The tensor contraction whichis originally proposed in (Batatia et al., 2022b). However, illustrates the computation of tensorcontraction described in Equation (22). This figure demonstrates an example of 4-body interactions with themax correlation order vmax = 3, where different atomic bases Ai1, Ai2, Ai3 are fed as inputs. Intuitively, thisexample is the computation similar to Ai1 Ai2 Ai3 + Ai1 Ai2 + Ai1, where Ai1 Ai2 Ai3 approximates4-body interactions, Ai1 Ai2 approximates 3-body interactions, and Ai1 approximates 2-body interactions.For easier understanding, this example only has 1 hidden channel, and only shows one output LM, that isL3 = 0, M3 = 0. Generally, the complete final output irreducible representation is the concatenation of allrequired L3M3. Lets delve deeper into the contract weights box and contract features box at the top of this figure. Thepath here denotes a coupled path tuple (1, 2, L2, 3, L3). The purpose of using such coupled paths is forcomputing consecutive tensor products as a whole instead of applying tensor products one by one. For eachpath, there is a generalized CG coefficient matrix with shape of (max2, max2, max2, max2, 2 L3 + 1). Notethat L3 = 0 in this example. The top contrast weights box executes the first step to take a weighted sumof the generalized CG matrices with learnable weights over the paths. Then, the contract features boxbelow applies a dot product on this summed generalized CG matrix and atomic base Ai3 along the dimensiondenoted as 3, m3. The output of this step is denoted as aL3M3i2,L2M2. Then, aL3M3i2,L2M2 is further contractedwith Ai2 and Ai1 to obtain the final result with similar contract features operations. This flow considersthe 4-body interactions. To further consider 3-body and 2-body interactions, similar contract weightsoperations are applied over and . Then, the results of contracted weights are added to aL3M3i2,L2M2 andaL3M3i1,L1M1 (represented by arrows pointing from right to left). The summation of generalized CG and theaddition of contracted weights to aLvMviv,LvMv is for efficient computation of various paths and various bodyorder interactions simultaneously. As mentioned this example is computing something similar to Ai1 Ai2 Ai3 +Ai1 Ai2 +Ai1 for many-bodyinteractions. However, they are not exactly equivalent, because two tensor products in Ai1 Ai2 Ai3consider the coupled path with generalized CG, which is beyond the original tensor product.",
  "A.5Defination": "Definition A.1. (D-spanning). For D N+, let Ffeat be a subset of CG(R3N, W Nfeat). We say that Ffeat isD-spanning, if there exist f1, , fK Ffeat, such that every polynomial R3N RN of degree D which isinvariant to translations and equivariant to permutations, can be written as p(X) = Kk=1 k (fk(X)), wherek : Wfeat R are all linear functionals, and k : Wfeat R are the functions defined by element-wiseapplications of k. When discussing the expressiveness of equivariant networks, equivariant polynomial functions are commonlyused to analyze these networks (Dym & Maron, 2020; Segol & Lipman, 2019). According to Lemma 1 fromDym & Maron (2020), any continuous G-equivariant function can be uniformly approximated on compactsets by G-equivariant polynomials. Consequently, the ability of a model to approximate equivariant functionsincreases with its capacity to cover more equivariant polynomials. In this context, a D-spanning familyrefers to a set of functions that span the polynomial function space up to degree D. This implies that anySnpermutation equivariant and rotation invariant polynomial function up to degree D can be represented as",
  "l1m1": ": Illustration of tensor contraction in the polynomial many-body interaction module. In this figure,it demonstrates an example of 4-body interactions with vmax = 3 and final L3 = 0, M3 = 0. Note that thecontract weights operation learns weighted summation over all paths [v], where [v] = (1, 2, L2, , v, Lv).",
  "Lemma B.1. The linear combination of elements for spherical harmonics with radial basis functionR(rij)Ym(rij), where 0 max, N and m , m Z, can represent any polynomialfunction Pmax(rij)": "Proof. When the radial basis function is set to R(rij) = rij, N+, real spherical harmonics with radialbasis function R(rij)Ym(rij) represent homogeneous polynomials P(rij) with degree . Moreover, by usingmulti-layer perceptrons (MLPs) R0(rij), it can approximate any polynomial with respect to the pairwisedistance rij (Hornik, 1991; Hornik et al., 1989). We first define the space of homogeneous polynomialsHn(R3), and space of harmonic homogeneous polynomials Yn(R3). When we constrain the function to be onS2, Yn Y(S2) and is composed of spherical harmonics. From the Lemma 4.1 in Atkinson & Han (2012), ahomogeneous polynomial p(x) Hn can be decomposed as p(x) = h(x)+|x|2q(x), where h Yn and q Hn2for n 2. When considering polynomial functions constrained to S2, we have p( x",
  "|x|), whereh is in the space spanned by spherical harmonics of degree n. The dimension of homogeneous polynomials is(n+1)(n+2)": "2. For n = 1, the dimension of homogeneous polynomials space is 3, and the dimension of sphericalharmonics space is also 3. For n = 2, the dimension of homogeneous polynomial space is 6, and the dimensionof spherical harmonic space is 5. Then, with the constrain x2 + y2 + z2 = 1 denoting the input (x, y, z)on S2, which is independent with spherical harmonics Y =2(r). Therefore, homogeneous polynomials onS2 can be linear combination of spherical harmonics for n = 2. Thus, q( x",
  "= Q(t1,t2, ,tv1)(P1, P2, Pv1)Qtv(Pv)(40)": "Above all, the output of equivariant base can represent Q(t1,t2, ,tv)(i) with tj1 v, j [v]. Since it canselect any (t1, t2, , tv) within (t1, t2, , tv)1 v, the constructed D-spanning family QDK with K = vand D = v. Then, the set of the output equivariant features is also a D-spanning family with D = v.",
  "C.1Data Preprocessing": "The raw data and splits for the rMD17 dataset are downloaded from The raw data is provided in Numpy .npz format, withenergies in units of kcal and forces in units of kcal/. We convert the units for energies and forces to eV andeV/, and save the data in .xyz format. The 3BPA and AcAc datasets, already in .xyz format with energiesin eV and forces in eV/, are directly downloaded from Next, we extract nuclear charges, conformation coordinates, energies, and forces from the .xyz files andencapsulate all this information into a PyTorch Geometric dataset. Subsequently, we perform anotherpre-processing step to obtain one-hot encoding for each atom and construct a molecular graph using a radius",
  "C.2Experiment Details": "For all molecules in, we use 2 GNN layers with 256 hidden channels and max = 3. For multi-layer perceptions(MLPs), SiLU is used as nonlinearity. During training, we train each model for 5000 epochs using a batchsize of 5 and a learning rate of 0.01. Besides, we use Adam-AMSGrad optimizer with default parameters of1 = 0.9, 2 = 0.999, = 108, and without weight decay. Moreover, an exponential moving average with aweight of 0.99 is used in training. For rMD17, edges are built for pairwise distances within a radius cutoff. Edge features are initiated byeither Exponential Bernstein radial basis functions (Unke et al., 2021b) or bessel functions with a smoothedpolynomial cutoff (Gasteiger et al., 2020). shows our options for molecules in rMD17 dataset. Thetraining loss considers both energy and force with a weight ratio of 9: 1000. In addition, we use a rotationorder of 3 for the hidden irreducible representations outputted by PACEs polynomial many-body interactionmodule. For both 3BPA and AcAc, edges are built using a radius cutoff of 5 A, and edge features are initiatedwith 4 Bessel basis functions. The ratio of energy and force in loss is 15:1000. Moreover, the rotation order ofthe hidden irreducible representations is 2.",
  "C.3Pseudocode of PACE": "In this subsection, we provide pseudocode for implementing our PACE. The initial node features are embeddedbased on atomic types, edge directions are encoded by spherical harmonics, and edge lengths are encoded byradial basis functions. These node and edge features are then fed into the first message passing layer. In thislayer, an edge booster is first applied to produce boosted messages along the edges. These boosted messagesare then aggregated around the central node. Next, the aggregated message and initial node features areprocessed by a polynomial many-body interaction module, followed by a skip connection with self-interactionto update the node features. In the second message passing layer, a tensor product is applied to the updatednode features and the boosted messages obtained from the first layer to update edge messages. As in the firstlayer, edge messages are aggregated and further processed by the polynomial many-body interaction moduleto update the node features. Finally, the total energy and forces acting on atoms are calculated based on theupdated node features and atom coordinates. The first key contribution of our method is the use of an edge booster to increase the number of thehigher-degree polynomial functions that our model can approximate. The edge booster is implemented inthe first PACE layer using two consecutive tensor products. Initially, a tensor product is applied to theedge spherical harmonics and the initial node features to generate the boosted message with N boost = 1.Subsequently, another tensor product is applied to the edge spherical harmonics and the previously obtainedboosted message, resulting in a boosted message with N boost = 2. The final boosted message is obtained byconcatenating these boosted messages with N boost = 1 and N boost = 2. This boosted message is used in bothPACE layers without repetitive computation, taking computational costs into consideration. The second key",
  ": Illustration of radial distribution functions (RDF) of MD trajectories. Values are averaged over fiveMD simulations with five initial molecular structures. The shell thickness dr = 0.05 is used": "contribution of our method is the incorporation of additional self-interactions in the polynomial many-bodyinteraction module. These self-interactions produce different atomic bases, which serve as inputs for symmetriccontraction. The function of these self-interactions is to enhance the models ability to approximate morepositions in D-spanning functions. In our implementation, the MLP applied to invariant features consists of a 2-layer non-linear transformationwith SiLu activation. Detailed information about the tensor product and self-interaction applied to equivariantfeatures can be found in the TFN (Thomas et al., 2018) and e3nn (Geiger & Smidt, 2022) libraries. For a morecomprehensive explanation of the algorithm used for symmetric contraction, please refer to MACE (Batatiaet al., 2022b).",
  "D.1Molecular Dynamic Simulation": "To further assess the ability of PACE to simulate realistic structures and dynamics, we conduct MolecularDynamics (MD) simulations on the 3BPA dataset using PACE. These simulations are implemented with theLangevin dynamics provided by the ASE library, using 1000 timesteps with 1 fs for each timestep. We randomlyselect an initial structure from the testing set to generate MD trajectories with PACE. Correspondingly,we extract the ground truth MD trajectory, starting from the same initial structure, from the testing set.The total radial distribution functions (RDF) of the ground truth MD trajectories and PACE-generatedMD trajectories are visualized in . The results of the MD simulations further affirm that our PACEmodel not only achieves state-of-the-art (SOTA) performance in force field prediction but also holds practicalvalue in realistic simulations.",
  "D.2.1Time Complexity of PACE": "shows the time complexity for several key components of existing equivariant model architectures. Inthis table, C is the number of channels, L is the maximum rotation order of the equivariant features, and vdenotes the correlation order. Our PACE model requires three tensor products (TPs) to obtain the edgefeatures and two polynomial many-body interaction modules on the aggregated node features. Each tensorproduct has a time complexity of O(CL6). Compared to the many-body interaction module used in MACEwith a time complexity of O(C2L + CL4v+2), our polynomial many-body interaction module has a timecomplexity of O(NCL6 + ECL4v+2), where N denotes the number of nodes, E denotes the number of edges.",
  "D.2.2Training Time and Memory of PACE": "presents a comparative analysis of training time and memory consumption between our proposedPACE method and the baseline methods. Results are reported in seconds per epoch and MB as units. Aconsistent batch size of 5 is used for each method, with average times calculated over 10 epochs, wherevalidation occurs once every 2 epochs. In these experiments, Allegro is configured with 5 layers and a rotationorder of 3, and NequIP with 3 layers and a rotation order of 3. MACE is configured with 2 layers and arotation order of 2. PACE is configured as reported in Appendix C.2. The results show that compared toMACE, PACE offers enhanced expressiveness and superior performance, albeit with a justifiable increase in",
  "D.2.3Cost Analysis of Edge Booster": "As summarized in .5, the proposed edge booster (EB) module enhances our PACE models abilityto approximate higher-degree polynomial functions. Instead of integrating the EB module into each PACElayer, we employ it solely in the first layer of our PACE network. The edge-boosted message generated bythe EB is then utilized to replace the original spherical harmonics in every PACE layer. Moreover, the EBmodule only takes spherical harmonics and atomic types of nodes as inputs without considering the updatednode features. This design choice aims to minimize the computational cost associated with approximatingmore higher-degree polynomial functions. Accordingly, in we provide a quantitative analysis of thecomputational cost incurred by our edge booster. This analysis of runtime and memory consumption is conducted using the AcAc dataset. Results are reportedin seconds per epoch and megabytes (MB). A batch size of 5 is used, and the average time is calculated over10 epochs, with validation occurring once every 2 epochs. Our PACE model comprises 2 layers. In the firstexperiment, we remove the EB module and use the original spherical harmonics for tensor products in bothPACE layers. The second experiment employs the EB module only in the first layer, which corresponds tothe architecture of our PACE. In the third experiment, we add an additional EB module in the second layer.Here, the second-layer EB module uses the updated node features output by the first layer, instead of thenode embeddings based on atomic types. The results demonstrate that the EB module with two consecutivetensor product operations is computationally expensive. Compared to incorporating the EB module intoboth layers, the design of using only one EB module in the first layer, and without considering updated nodefeatures, can efficiently reduce the time and memory cost by 30.5% and 9.7%, respectively.",
  "D.3Rotation Order": "As described in Appendix C.2, for the rMD17 dataset, PACE utilizes a rotation order of 3 for the hiddenirreducible representations outputted by its polynomial many-body interaction module, contrasting withMACEs use of a rotation order of 2 for encoding many-body interactions. Hence, we conduct furtherexperiments to determine if the enhanced performance of PACE is solely due to this increased rotation order.The results in indicate that while the higher rotation order of hidden representations does contributeto improved model performance, it is not the sole factor in PACEs superior performance.",
  "Training time12.117.825.6Memory208030763408": ": Results of experiments on rotation order of hidden irreducible representation of polynomial many-body interaction module. PACE uses max = 3 for rMD17 dataset. Mean absolute errors (MAE) are reportedfor both energy (E) and force (F) predictions, with meV and meV/ A as units, respectively. Bold numbershighlight the best performance.",
  "D.4Hyperparameter Exploration": "In this subsection, we conduct experiments on the AcAc dataset to investigate our models sensitivityto different hyperparameter settings. presents the performance curves corresponding to varioushyperparameter choices. From top to bottom, the results pertain to four hyperparameters: radius cutoff,number of channels, number of radial basis functions (RBFs), and maximum rotation order. Each data pointrepresents the average performance over three runs with different random seeds. It is widely believed that the radius cutoff can impact model performance, as it determines the connectivitywithin the 3D molecular graph. Our results indicate that a cutoff of approximately 4-5 A yields the bestperformance for our model. Next, the results in the second row indicate that the model performance remainsrelatively stable with the number of channels ranging from 100 to 300. Having too few channels reduces themodels capacity, while having too many channels does not provide additional benefits. The results in thethird row demonstrate that the models performance remains relatively stable, with optimal performanceoccurring when using around 4 radial basis functions. Using too few RBFs will reduce the impact of edgedistance. In contrast, the last row shows that the maximum rotation order for spherical harmonics andirreducible representations has a significant impact on model performance, which is consistent with trendsobserved in previous studies (Batzner et al., 2022; Batatia et al., 2022b). However, it is important to notethat higher rotation orders introduce greater computational costs. Consequently, existing models typicallyuse a maximum rotation order of three. The experimental results shown in demonstrate that ourPACE model exhibits reasonable stability and robustness to variations in hyperparameter choices."
}