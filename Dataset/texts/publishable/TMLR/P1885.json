{
  "Abstract": "While K-means is known to be a standard clustering algorithm, its performance may becompromised due to the presence of outliers and high-dimensional noisy variables. Thispaper proposes adaptively robust and sparse K-means clustering (ARSK) to address thesepractical limitations of the standard K-means algorithm. For robustness, we introduce aredundant error component for each observation, and this additional parameter is penal-ized using a group sparse penalty. To accommodate the impact of high-dimensional noisyvariables, the objective function is modified by incorporating weights and implementing apenalty to control the sparsity of the weight vector. The tuning parameters to control therobustness and sparsity are selected by Gap statistics. Through simulation experiments andreal data analysis, we demonstrate the proposed methods superiority to existing algorithmsin identifying clusters without outliers and informative variables simultaneously.",
  "Introduction": "Identifying clustering structures is recognized as an important task in pursuing potential heterogeneityin datasets.While there are a variety of clustering methods, K-means clustering (Forgy, 1965) is themost standard clustering algorithm and is widely used in many scientific applications. However, real-worlddatasets present significant difficulties, such as the presence of outliers and high-dimensional noisy variables.Regarding the outlier issues, there are some attempts to robustify the standard K-means such as trimmedK-means (Cuesta-Albertos et al., 1997) and robust K-means (Klochkov et al., 2021). On the other hand,high-dimensional noisy characteristics are typically addressed through clustering approaches that focus onsparsity for variable selection. In particular, Witten & Tibshirani (2010) proposed a lasso-type penalty tothe variable weights incorporated in the clustering objective function. However, the efficacy of this algorithmmight be lost if the dataset contains a significant number of outliers. While several methods address eitherof the two aspects (the existence of outliers and noisy variables), practically useful methods to addressboth aspects simultaneously are scarce. One exception is the method proposed by Kondo et al. (2016).However, this approach requires the assumption of a trimming level in advance. In contrast, Brodinovet al. (2019) introduces an approach beginning with eliminating outliers identified by the model. However,this methodology potentially leads to reduced sample size and may result in information loss. Moreover,the excessive variations in gradient magnitudes may destabilize when optimizing the variable weight processunless all outliers are eliminated in advance. In addition to clustering methods based on objective functions, probabilistic clustering using mixture modelsis also popular. It is typically done by fitting multivariate Gaussian mixtures. While several approaches",
  "Published in Transactions on Machine Learning Research (11/2024)": "Through 100 repetitions, the results of the TPR and TNR are presented in Tables 3.3 and 3.3. As theproportion of outliers increases from 0 to 0.2, both the TPR and TNR of most approaches demonstrate adecreasing trend across different scenarios, suggesting that the presence of outliers can negatively influencethe variable selection performance. However, the TPR and TNR of each type of ARSK approach variantremain relatively high and stable across various data scenarios, demonstrating ARSKs superior variableselection capabilities in the presence of outlier contamination in datasets. For example, in the independentcase with p = 500 and q = 50, the TPR of soft-soft-ARSK, soft-SCAD-ARSK, SCAD-soft-ARSK, andSCAD-SCAD-ARSK remain above 0.797, 0.810, 0.798, and 0.810, respectively, even when = 0.2. Similarly,their TNR values stay above 0.977, 0.980, 0.978, and 0.981 in the same scenario. These results emphasizethe superior ability of soft-ARSK and SCAD-ARSK methods to accurately identify informative and non-informative variables in the presence of outliers, making them valuable tools for handling contaminatedclustering datasets. In contrast, during the simulation process, we observed that traditional trimmed approaches such as WRCKand RSKC often struggled to maintain their variable selection performance under high outlier proportions.In many cases, these methods incorrectly assigned non-zero weights to variables that originally had zeroweights, leading to an increased number of false positives. This limitation makes it challenging for WRCKand RSKC to effectively detect informative variables in the presence of outliers, emphasizing the need formore effective methods like ARSK.",
  "Conventional K-means algorithm": "Suppose that we observe a p-dimensional vector, Xi,: = (Xi1, . . . , Xip), for i = 1, . . . , n, and that we areinterested in clustering Xi,:. Given the number of clusters, K, the conventional K-means algorithm is tofind the optimal clustering by minimizing the following objective function:",
  "i,ickXi,: Xi,:2,(1)": "where Xi,: Xi,:2 is the squared value of L2-distance between two observed vectors. Here ck is a set ofindices representing a cluster of observations and nk is the size of ck, such that Kk=1ck = {1, . . . , n} andKk=1 nk = n, where ck ck = for k = k. The objective function (1) can be easily optimized by iterativelyupdating cluster means and assignment (e.g. Lloyd, 1982).",
  "(3)": "where E is an n p error matrix, which is a collection of Eij and Eij is an additional location parameterfor each Xij such that Eij will be non-zero values for outlying observations Xij to prevent an undesirableeffects from outliers. While the number of elements of E is the same as the number of observations, it canbe assumed that E is sparse in the sense that most elements of E are 0. That is, most elements are notoutliers. To stably estimate E, we consider penalized estimation for Eij when optimizing the function (3).Such approaches are used in the robust fitting of regression models (e.g. She & Owen, 2011; Katayama &Fujisawa, 2017).",
  "ik(Xij Eij) and Eij is the optimized value of Eij from the previous iteration": "As previously discussed, the variable E plays a crucial role in robustness. If an observation does not fall intoany cluster, vector Ei,: elements will exhibit large values. The penalty P1(Ei,:2; 1) controls the robustnessof the model. We also note that under 1 , all Ei,: equal a p-dimensional zero vector. As a result, theobjective function (6) will be equivalent to traditional K-means clustering. In the minimization problem (6), as previously mentioned, we employ the group lasso penalty and groupSCAD penalty (Tibshirani, 1996; Antoniadis & Fan, 2001). The optimizers are obtained by applying thethresholding function corresponding to the group penalty to Xi,: k,:. These two types of group penaltyfunctions correspond to two distinct categories of thresholding functions: the group lasso penalty functioncorresponds to the multivariate soft-threshold operator, where Ei,: can be calculated through",
  "sgn(x)(|x| 2)+if |x| 22{(a 1)x a2 sign(x)} /(a 2)if 22 |x| < a2xif a2 |x|": "The parameter 2 has a crucial role in controlling the sparsity in the variable weight vector. A higher valueof 2 results in increased sparsity of the variable vector w. Specifically, a higher weight value wj indicatesgreater importance of the jth variable in the clustering process. Conversely, when wj is equal to zero, itsignifies that the jth variable does not contribute to the clustering. In order to gain a deeper comprehension of the algorithmic process we have proposed, we summarize theaforementioned procedures in a pseudocode in Algorithm 1. Moreover, the computation complexity of Titeration of Algorithm 1 can be represented O(nptTK), assuming that the number of iterations in the 4thstep is O(t).",
  "Adaptation of the tuning parameters by the robust Gap statistics": "There are two tuning parameters in the proposed algorithm: 1 and 2. The tuning parameter 1 is crucialfor finding the error matrix and impacts the detection of outliers, and the tuning parameter 2 plays acrucial role in filtering out variables contributing to the clustering process. We here provide the robust Gapstatistics to determine these parameters.",
  ": Output: w, E,": "The Gap statistics were originally proposed in Tibshirani et al. (2001) for selecting the number of clusters inthe K-means algorithm. Note that the Gap statistics is constructed as a function of the between-cluster sumof squares, D = pj=1 wj(n1 ni=1ni=1(Xij Xij)2 Kk=1 n1k i,ick(Xij Xij)2), for the originaldataset. However, since D is sensitive to outliers, it is not suitable for selecting the tuning parameters underexistence of outliers. Hence, we propose a robust version of Gap statistics by adding the error part to eachobservation when calculating the DR2,1. As we mentioned before, the error part can outweigh the influenceof the outlier, so that we define Gap2,1 as",
  "and the optimal value of 2 and 1 corresponding to the largest value of Gap2,1. Note that DR2,1 is a": "weighted robust between-cluster sum of squares defined in (3), and DR(b)2,1 is a version with bth permuteddatasets.Here, B denotes the number of permuted datasets generated by randomly selecting from theoriginal dataset. Increasing the number of permuted datasets improves the accuracy of selecting the trueparameters. The between-cluster sum of squares, taking account of the variable weight, is adopted in Witten& Tibshirani (2010), so our version used in (10) can be regarded as its robust extension.",
  "Experimental setup": "In this section, we explore the ability of the proposed clustering method. We consider that each observationxi is generated independently from a multivariate normal distribution, given that the observation belongsto cluster k. Specifically, for an observation xi in cluster k, we have xi N(k,:, p), where k,: Rp denotes the mean vector for cluster k, and p Rpp represents the covariance matrix. Each elementof k,: is independently sampled from from either U(6, 3) or U(3, 6). To simulate scenarios involvingoutliers, we introduce a contaminated error distribution with a multivariate normal mixture, representedas (1 )N(k,:, p) + N(k,: + bj, p) where j (1, . . . , p) and represents the proportion ofoutliers for each cluster. In the simulation study, we consider two types of p. In the first scenario, theassumption is made that all variables are independent. In another scenario, we assume there is a correlationstructure among variables. To introduce randomness to the appearance of outliers, bj is generated with arandom number from one of two uniform distributions: U(13, 7) or U(7, 13). In this setup, we assumethat some explanatory variables are significantly associated with the response, while the remaining variablesare redundant for variable selection. To mimic this situation, we randomly set q elements of k,: to non-zerovalues while the remaining elements are set to zero. Consequently, a successful method should accuratelyidentify the q positions where the weights wj corresponding to the true significant variables are non-zerowhile ensuring that the weights wj of the remaining variables are set to zero. In order to evaluate the approachs clustering accuracy capability, we employ the clustering error rate (CER)(Rand, 1971). The CER measures the extent to which the models predicted partition C for a set of nobservations is consistent with the true labels C.",
  "Simulation 1: selection of turning parameter": "In this subsection explores the new Gap statistics capability to directly search for the tuning parameters ofrobustness and variable vector sparsity using Algorithm 2. Initially, we considered 3 clusters, each containing50 observations, number of variables as p = 50, with q = 5, and all variables are independent, i.e., p = Ip.A proper of hyperparameters 1 and 2 should make the model accurately identify the structure of theclustering data in different contamination levels for in {0, 0.1, 0.2, 0.3}. The proposed tuning parametersearch strategy generates both the robustness parameter 1 and the variable selection tuning parameter 2using exponential decay. We consider the number of permuted datasets to be 25 (i.e., B = 25). We employeddifferent thresholding function types, where the first thresholding function is used to control model sparsity,and the second thresholding function is used to control model robustness. The tuning parameter searchprocess is repeated 30 times for 30 different datasets, and the resulting table is presented in .2.",
  "SCAD-SCAD = 00.090 (0.333)4.272 (0.881) = 0.113.33 (1.258)5.833 (3.125) = 0.224.10 (5.065)4.500 (2.068) = 0.344.00 (4.2110)7.187 (1.223)": ": Using the robust Gap statistic to select the optimal tuning parameters for the RSKC algorithm basedon soft-soft-thresholding, soft-SCAD-thresholding, SCAD-soft-thresholding, and SCAD-SCAD-thresholding,we report the average and its standard error (in parentheses) of the number of detected outliers and thenumber of detected informative variables. .2 summarizes the results of the evaluation measures. Overall, a good tuning parameter 1 shouldaccurately identify the number of outliers under different contamination levels. As the contamination level increases from 0 to 0.3, we observe that the four thresholding methods demonstrate varying outlier detectioncapabilities across different contamination levels. At = 0.1, the detected number of outliers ranges from13.33 to 15.56, with soft-soft-thresholding achieving the highest average of 15.56 (standard error: 6.425).When increases to 0.2, the detected number of outliers falls between 24.10 and 28.73, with soft-soft andsoft-SCAD-thresholding showing similar performance. This demonstrates that the Gap statistics can helpthe ARKC method determine 1 to some extent. However, when the contamination level reaches 0.3, thesoft-soft-thresholding ARSK method detects 33.10 outliers on average, which is lower than the SCAD-SCAD-thresholding ARSK method (44.00 outliers) and deviates from the true number of outliers. Regarding the selection of the tuning parameter 2, a good tuning parameter should accurately identifythe number of informative variables under different contamination levels. When the data is clean ( = 0),",
  "Simulation 2: comparison with other methods": "In this subsection, we present the findings of a comprehensive simulation study conducted to investigatethe cluster data structure and properties by the ARSK algorithm based on soft-thresholding and SCAD-thresholding. The study extensively applied Gap statistics to estimate optimal parameter settings and com-pared with several benchmark approaches, including the original K-means (KC), PCA-K-means (PCA-KC)- a mixed approach that combines Principal Component Analysis (PCA) with the original K-means algo-rithm, as initially proposed by Ding & He (2004). It is noted that PCA is a widely recognized dimensionalityreduction technique that transforms high-dimensional data into a lower-dimensional space. We also consid-ered Trimmed K-means (TKM), which executes standard K-means clustering and subsequently eliminatesa predetermined percentage of points exhibiting the greatest Euclidean distance from their respective clus-ter centroids. Furthermore, we employ Robust and Sparse K-means (RSKC), whose fundamental premiseinvolves adapting the SK-means algorithm of Witten & Tibshirani (2010) in conjunction with TrimmedK-means.Lastly, we apply the Weighted Robust K-means (WRCK) method, which shares similaritieswith RSKC but distinguishes itself through the incorporation of observation-specific weights in its objec-tive function. These weights are reflective of each observations isolation level relative to its surroundingneighbourhood. Considering the mixture error model mentioned at the beginning, we also generated data for 3 clusters, eachcontaining 50 observations. We set the number of variables as p = 50, with q = 5, and p = 500, with q = 50.Meanwhile, we consider in {0, 0.1, 0.2} for specifying the distribution of outliers. As previously stated,we study two types of covariance matrices: one with p = Ip and the another generated according to themethod proposed by Hirose et al. (2017) as",
  "where the Q denotes a pp random rotation matrix satisfying QT = Q1 and the t are randomly generatedform an Uniform distribution U(0.1, 1)": "In order to evaluate the clustering accuracy and outlier detection capability of each approach, we employthe clustering error rate (CER) as described above. To assess the outlier detection performance, the outliersidentified by each model are assigned to the (K + 1)-th cluster group. .3 presents the results of 100simulations for the scenario where all variables are independent, i.e., p = Ip, while .3 summarizesthe results of 100 simulations for the scenario where variables exhibit a certain correlation structure. In Tables 3.3 and 3.3, the results show that KC, PCA-KC, and TKM approaches, which lack robustness forhigh dimensions or outliers, are the worst performers, particularly when the dimensionality p and proportionof outliers increase. For instance, in the independent case with p = 500 and q = 50, the CER of KCincreases from 0.050 to 0.341 as increases from 0 to 0.2.Similarly, the CER of PCA-KC and TKM",
  "= 0 = 0.1 = 0.2 = 0 = 0.1 = 0.2": "TPRRSKC ( = 0.1)0.917 (0.013)0.839 (0.125)0.761 (0.236)0.813 (0.125)0.934 (0.092)0.802 (0.294)RSKC ( = 0.2)0.705 (0.132)0.608 (0.260)0.860 (0.015)0.695 (0.232)0.903 (0.057)0.884 (0.072)WRCK0.813 (0.126)0.604 (0.212)0.734 (0.126)0.613 (0.213)0.932 (0.074)0.812 (0.126)soft-soft-ARSK0.953 (0.086)0.860 (0.175)0.769 (0.042)0.974 (0.031)0.843 (0.071)0.733 (0.095)soft-SCAD-ARSK0.940 (0.105)0.855 (0.181)0.806 (0.177)0.968 (0.022)0.849 (0.071)0.773 (0.109)SCAD-soft-ARSK0.928 (0.096)0.830 (0.177)0.772 (0.169)0.973 (0.029)0.877 (0.051)0.813 (0.063)SCAD-SCAD-ARSK0.928 (0.104)0.918 (0.120)0.758 (0.186)0.968 (0.036)0.874 (0.053)0.793 (0.058) TNRRSKC ( = 0.1)0.680 (0.387)0.730 (0.325)0.864 (0.226)0.776 (0.218)0.632 (0.363)0.743 (0.206)RSKC ( = 0.2)0.837 (0.232)0.935 (0.102)0.794 (0.400)1 (0)0.136 (0.454)0.542 (0.436)WRCK0.697 (0.437)0.954 (0.083)0.803 (0.231)1 (0)0.476 (0.376)0.644 (0.306)soft-soft-ARSK1 (0)0.961 (0.085)0.985 (0.026)0.998 (0.006)0.977 (0.011)0.984 (0.015)soft-SCAD-ARSK1 (0)0.876 (0.080)0.997 (0.008)0.999 (0.001)0.978 (0.012)0.991 (0.008)SCAD-soft-ARSK1 (0)0.978 (0.035)0.992 (0.017)0.983 (0.010)0.977 (0.011)0.983 (0.010)SCAD-SCAD-ARSK1 (0)0.968 (0.020)0.994 (0.013)0.997 (0.005)0.986 (0.010)0.981 (0.009)",
  ": When variables are correlated, the average values of the CER and their standard errors (in paren-theses) based on 100 Monte Carlo replications": "( = 0.1) increase from 0.058 to 0.300 and 0.128 to 0.302, respectively, under the same scenario. Conversely,RSCK, WRCK, and ARSK perform exceptionally well in high-dimension and high-contamination scenarios.However, the effectiveness of the RSCK approach heavily depends on the choice of the parameter . When equals the true outlier proportion, RSCK performs impressively, with CER close to 0. For example, inthe correlated case with p = 500 and q = 50, the CER of RSKC ( = 0.1) is 0 when = 0.1. However,the approach loses effectiveness if is incorrectly specified, leading to higher CER values. In the samescenario, the CER of RSKC ( = 0.2) is 0.122 when = 0.1. The WRCK approach, based on the densityclustering method, generally outperforms RSCK when outliers are present. Still, it may erroneously classifymore normal data points as outliers when no contamination exists. In contrast, the ARSK approach, withdifferent combinations of thresholding functions (e.g., soft-soft, soft-SCAD, SCAD-soft, and SCAD-SCAD),consistently maintains a low Clustering Error Rate (CER) across all simulated data scenarios. In summary, the simulation results demonstrate the effectiveness and robustness of the ARSK method inclustering high-dimensional datasets, particularly in the presence of outliers and variable correlations. Acrossdiverse data scenarios, the ARSK variants consistently outperform traditional methods, including KC, PCA-KC, TKM, RSKC, and WRCK, highlighting the distinct advantages of the ARSK approach. Both In .3 and .3, we observe that the CER of KC is worse than ARSK when there are nooutliers in the dataset. Moreover, when = 0.1, the CER of the TKM method is significantly higher thanthat of the RSCK method. We attribute this result to the presence of noisy variables, which contribute tothe increased CER. Therefore, it is essential to identify these noisy variables to reduce their impact.",
  "UCI data application": "In this subsection, we apply ARSK with different thresholding configurations to real-life datasets and compareit with the benchmark methods previously mentioned in the simulation study. We consider the dataset allfrom the UCI Machine Learning Repository (Dua & Graff (2019)). The datasets evaluated include variousapplications, such as glass identification, breast cancer diagnosis, acoustic signal classification, spam basedetection, mortality analysis, and Parkinsons disease detection. The real-life dataset comprises data with awide range of dimensions, ranging from low to high. As in the previous analysis, we also applied Gap statisticsto estimate the hyperparameter. Due to the different units of the continuous variables, we normalize all ofcontinuous variables before conducting the study. summarizes the result of the real-life dataset comparison. The reported values are CERs for eachapproach. In order to better illustrate the performance of that method, we have emphasized the first andsecond best CER values in bold when applied to a certain real-life dataset. While traditional methods such",
  ": Comparison of CERs of different algorithms for the real-world data. The smallest and secondsmallest CER values are shown in bold": "as KC, PCA-KC, and TKM demonstrate effectiveness in certain contexts, they generally yield higher CERsacross the datasets compared to the proposed ARSK algorithm and its variants. For instance, in the glassand Breast Cancer dataset, the CER values obtained by KC, PCA-KC, and TKM are considerably higherthan those of the best-performing ARSK. In seven real-world datasets, our ARSK method ranked amongthe top two best methods in six of the datasets. Therefore, the proposed ARSK algorithm consistentlyachieves superior performance with lower CERs across most of the real datasets, particularly when usingthe multivariate soft-threshold operator to obtain the error matrix E. ARSK demonstrates competitiveperformance in this regard.Furthermore, .4.1 summarizes the number of zero elements in thevariable weight vector of each method, where ARKC adopted a combination of thresholding correspondingto the minimum CER. .4.1 shows that when analyzing ultra-high-dimensional data, the variableweight vector predicted by ARSK exhibits strong sparsity.",
  ": The number of zero elements in the variable weight vector of each method, with results rangingfrom low-dimension to high-dimension dataset": "This comparative analysis emphasizes the ARSK algorithms success in significantly reducing classificationerror rates across diverse real-world datasets, showcasing the versatility and adaptability of our proposedclustering method. Its different configurations (e.g., soft-soft, soft-SCAD, SCAD-soft, and SCAD-SCAD)allow it to handle various data structures and sizes effectively. The consistent performance across differentreal-world datasets underscores the broad applicability of our approach in various fields.",
  "Analyzing the group structure of recognition of handwritten digits": "The dataset was downloaded from Kaggle, a data science competition platform.It consists of digitizedimages of the numbers 0 to 9 handwritten by 13 subjects. The images were divided into 64 non-overlappingblocks of 44 bits. For each block, we observe the number of black pixels. Hence, each image is representedby p = 64 variables recording the count of pixels in each block, taking integer values between 0 and 16. Itsrow names identify the true digit in the image, and the column names identify the position of each block inthe original bitmap.",
  ": The heatmap of the variable weight vectorobtained by SCAD-soft ARSK": "Our focus is to detect an entire variable weight and group structure, i.e., ten groups. Given the superiorperformance of SCAD-soft ARSK in subsection 3.4.1, we apply RSKC ( = 0.1), RSKC ( = 0.2), WRCK,and SCAD-soft ARSK to this dataset. Although the precise contribution of individual pixels to the cluster-ing process remains indeterminate, visual inspection of the real images of handwritten digits data (0 and 9)as shows that the pixels situated in both the initial and terminal columns exhibit uniform white",
  ": The number of zero elements within the weight vector derived from each method": "colouration. Thus, these 16 pixels can be reasonably deemed redundant and inconsequential for the clus-tering. The optimal robustness and sparsity parameter for ARSK is also selected using the Gap statistic.The evaluation of the resulting clustering solution is presented in .4.2. We also examine the finalvariable weights obtained by the sparse k-means-based algorithms. .4.2 illustrates each methodsnumber of zero elements within the weight vector derived. The analysis of the weight values reveals thatWRCK is ineffective in achieving sparsity within the variable weight vector, as evidenced by the exclusivepresence of nonzero elements throughout. Correspondingly, it also exhibits the highest CER. As shown in.4.2, the performance of RSKC and ARSK is similar, with CER values around 0.8. However, theRSKC ( = 0.2) produces weight vectors with only 6 zero elements, indicating a moderate sparsity level.In contrast, the proposed ARSK approach generates weight vectors with 13 zero elements that are close to16. We also presented a heatmap, in , visualization of the variable weight vector derived from theSCAD-soft ARSK algorithm as , wherein the intensity of the chromatic scale correlates positivelywith the magnitude of the weights. We can conclude that these findings correspond with those shown in. These results demonstrate that ARSK exhibits both sparsity and accuracy of feature selectioncapabilities.",
  "Concluding remarks": "In this paper, we propose an Adaptively Robust and Sparse K-means Clustering algorithm designed to handlemultiple tasks within a unified framework for a comprehensive analysis of contaminated high-dimensionaldatasets.Our approach demonstrates greater flexibility in handling outliers within real-world datasets.We also proposed a modified Gap statistic with an alternating optimization algorithm to search for tuningparameters.This approach significantly reduces the computational time required for parameter tuningcompared to traditional methods. The experimental results demonstrate the significant capabilities of theproposed approach in revealing group structures in high-dimensional data. Our model not only performs wellin detecting outliers but also identifies significant variables directly, leading to a more thorough understandingof complex clustering datasets. In our current study, we assume that the number of clusters is known and consistent with the traditionalK-means method. Additionally, Chen & Witten (2023) proposed a test for detecting differences in meansbetween each cluster estimated from K-means clustering. This technique can be applied to the modifieddataset Xij Eij, where noise variables (i.e., wj = 0) are removed to help determine which clusters shouldbe retained. Furthermore, we have consistently utilized a single penalty for outliers in the objective function (4). Thisimplements a uniform approach to detecting outliers across various groups. However, given the complexityand diversity of the data, a more sophisticated strategy may be required. This might involve using distinctgroup penalty functions for different groups. Incorporating this approach into unsupervised learning presentschallenges in selecting group penalty functions. These extensions are left to our future work."
}