{
  "Abstract": "The issue of group fairness in machine learning models, where certain sub-populations orgroups are favored over others, has been recognized for some time. While many mitigationstrategies have been proposed in centralized learning, many of these methods are not directlyapplicable in federated learning, where data is privately stored on multiple clients. To addressthis, many proposals try to mitigate bias at the level of clients before aggregation, which wecall locally fair training. However, the effectiveness of these approaches is not well understood.In this work, we investigate the theoretical foundation of locally fair training by studying therelationship between global model fairness and local model fairness. Additionally, we provethat for a broad class of fairness metrics, the global models fairness can be obtained usingonly summary statistics from local clients. Based on that, we propose a globally fair trainingalgorithm that optimizes the fairness-regularized empirical loss. Real-data experimentsdemonstrate the promising performance of our proposed approach for enhancing fairnesswhile retaining high accuracy compared to locally fair training methods.",
  "Introduction": "As edge devices such as mobile phones and wearable devices have been heavily involved in our daily life,leveraging the enormous data collected by those devices and their computational resources to train machinelearning models has attracted increasing research interest. One challenge is that datasets collected by differentdevices are often forbidden to be shared due to communication costs and privacy concerns. Thus, classicalcentralized learning, where data is gathered and stored in a central database, is not suitable. To addressthose challenges, federated learning (McMahan et al., 2017; Konen`y et al., 2016) has been proposed to trainmodels in a decentralized manner. In federated learning, a global model is distributed to multiple clients, oredge devices, which update the model using their own data and send the updated model back to a centralserver. The server then aggregates the updated models to obtain a new global model and the process isrepeated. While significant progress has been made in the theory and application of federated learning (Li et al., 2020b),most research has focused on improving the prediction accuracy of the global model. As these models areincreasingly being used in areas that have a direct impact on peoples lives, such as healthcare, finance, andcriminal justice (Berk, 2019; Barocas & Selbst, 2016; Becker, 2010), the ethical implications of these models",
  "Published in Transactions on Machine Learning Research (09/2024)": "where gk(t | ) is the stochastic gradient of Hk, represents the stochastic batches of datasets, St is arandomly selected subset of clients with cardinality M (in which client k is selected with probability nk/n),and t is the step size. We make the following technical assumptions often used in the optimization literature, e.g., Li et al. (2020c);Wang et al. (2020) and the references therein. For two vectors u and v, u, v = uTv is the inner productof u and v, and v = (vTv)1/2 is the 2 norm of v. The gradient operator is with respect to the modelparameter throughout this subsection.",
  "Our contributions are three-fold as summarized below": "1. We formulate the definitions of group-based and proper group-based fairness metrics. For propergroup-based metrics, the global fairness value can be expressed as a function of fairness-relatedstatistics calculated by local clients solely. This property enables us to calculate the global fairnessvalue without directly accessing local datasets. In particular, those fairness-related statistics are notlocal fairness values, distinct from all existing works. 2. Under our proposed group-based fairness notion, we investigate the relationship between the fairnessof local and global models. We find that global fairness and local fairness do not imply each other ingeneral. Nevertheless, for proper group-based fairness defined in , the global fairness valueis controlled by the local fairness values and the data heterogeneity level. This result explains thesuccess of LFT methods in the setting of near-homogeneous clients for common fairness metrics, suchas demographic parity and equal opportunity. It supports the results in Chu et al. (2021) that localfairness implies global fairness for homogeneous clients. Our work also provides a fairness analysisbased on data heterogeneity, complementing the information theory driven analysis presented inHamman & Dutta (2023). 3. We propose a globally fair training method named FedGFT for proper group-based metrics. FedGFTgoes beyond LFT by directly solving a regularized objective function consisting of the empiricalprediction loss and a penalty term for fairness. Additionally, it applies to clients with arbitrary dataheterogeneity. Numerical experiments on multiple datasets show that FedGFT significantly reducesthe bias of the global model while retaining high prediction accuracy.",
  "Related Work": "Fair federated learning methods. For bias mitigation methods in federated learning, one popular approachis locally fair training (LFT). For example, Abay et al. (2020)(LRW), Chu et al. (2021), and Ezzeldin et al.(2021) (FairFed) propose to train local models by applying centralized bias mitigation methods, such asreweighing the dataset to balance the group distribution (Kamiran & Calders, 2012), and adding a constraintor a penalizing term of fairness on the optimization objective function (Zafar et al., 2017; Kamishima et al.,2012).",
  "Federated Learning": "There is a large body of literature on federated learning (Li et al., 2020a) since proposed by (Konen`y et al.,2016; McMahan et al., 2017). It aims to train a global machine learning model while keeping the trainingdata privately on edge devices, also named local clients. Suppose there are K clients in total, and the k-thclient owns nk training dataX(i)k , Y (i)knki=1, where X is the predictor and Y is the response. Let l(, ) be a",
  "Here, Lk() is the empirical risk of the k-th client, f(; ) is a parameterized model": "The original idea of federated learning is training the model on each client using its local dataset for severalupdating steps, then aggregating the local models on the central server to obtain a global model, and repeatingthe above procedure until meeting the terminating conditions. More specifically, at each communicationround t, the server first propagates the parameters t of the current global model to the clients. Then, eachclient will perform E epochs of local updates to get t,Ek, k = 1, . . . , K. Finally, the server will aggregatet,Eks to a new global model with parameter t+1.",
  "Group Fairness": "There are many different interpretations of fairness (Caton & Haas, 2020; Mehrabi et al., 2021; Li et al.,2021; Yue et al., 2023; Papadaki et al., 2022). In this paper, we focus on group fairness, which ensuresthat the model will not have discriminatory behavior towards certain groups. For simplicity, we consider abinary classification task with the outcome Y {0, 1}, and sensitive group A {0, 1}. There are two majorcategories of group fairness quantification (Corbett-Davies & Goel, 2018). The first category is based onthe classification parity, which means a measure of the prediction error is equal across different groups. Forexample, statistical parity (Kamishima et al., 2012; Feldman et al., 2015), also known as demographic parity,requires that the distribution of the prediction Y conditional on the sensitive group is the same. In otherwords, P(Y = 1|A = 0) = P(Y = 1|A = 1). Another example is equal opportunity (Hardt et al., 2016), whichrequires the same true positive rate across groups, i.e., P(Y = 1|A = 0, Y = 1) = P(Y = 1|A = 1, Y = 1). Thesecond category is calibration (Pleiss et al., 2017). A model is well-calibrated or achieves test fairness if thetrue outcome is independent of the group given the predicted value. We note that different fairness definitionsmay be incompatible; actually, it is impossible to achieve multiple fairness goals simultaneously (Kleinberget al., 2016).",
  "Problem Formulation": "This paper considers a binary classification task with outcome Y {0, 1}. Suppose the predictors X =(X1, . . . , Xp)T Rp are p-dimensional variables. Without loss of generality, we assume the first predictorto be the sensitive attribute as A = X1 {0, 1}, and other predictors are non-sensitive. We consider aheterogeneous scenario that there are K clients (K > 1), and the k-th clients training dataX(i)k , Y (i)knki=1is IID generated from a distribution Dk. The empirical distribution ofX(i)k , Y (i)knki=1 is denoted as Dk. Ourgoal is to learn a function f : Rp from data, where f(X) is regarded as the predicted probabilityof P(Y = 1 | X). The accuracy of the learned function f is evaluated by the prediction risk E{l(f(X), Y )},where E denotes expectation, and l(, ) is a loss function, such as the cross entropy loss. As for the fairnessmeasure, we define the following group-based fairness metrics.",
  ",": "where ai(f, D) and bi(f, D) are some expectations on the event {A = i}. Accordingly, a proper group-basedfairness metric is one such that bis are degenerated with respect to f. We believe that by using the sametechniques as the current work, we can derive similar results as Theorem 2, 4, and 7.",
  "Well-CalibrationP(Y = 1, Y = 1, A = 0)P(Y = 1, A = 0)P(Y = 1, Y = 1, A = 1)P(Y = 1, A = 1)": "for example, it is the difference of the probability being predicted as positive for individuals from differentgroups. In other words, when A is a binary group variable taking values in 0 and 1, the statistical disparity ismeasured by |P(Y = 1|A = 0) P(Y = 1|A = 1)|, where Y is the prediction by model f. We thus proposethat the model performance given a specific group s can be typically expressed as a conditional expectationon the event that A = s, namely E(g(Y )|A = s) for any evaluation function g. In the case of statisticaldisparity, g(Y ) = 1Y =1 and 1() is the indicator function. Then, why our Definition 1 takes form of the difference of two ratios? There are two critical reasons. First,the Bayes theorem shows that E(g(Y )|A = s) can always be written as the ratio of two expectations, thereforea group fairness metric can be naturally written as the form in Definition 1. Second, the increase in thedegree of freedom (the additionally introduced b and d in Definition 1) enables a finer grid analysis on therelationship between global and local fairness. In particular, we will show in the following sections that whileglobal fairness and local fairness are not aligned for general group-fairness metric, they are closely related if band d are irrelevant to the model f. In short, it is the first time a theoretical formulation is given to group-based fairness metrics. Clearly, a smallerF(f, D) indicates higher model fairness and smaller model bias. Definition 1 includes many common measures,such as the following three. We can verify this by checking , with full details in supplementarydocument.",
  "Well-Calibration. F(f, D) = |P(Y = 1|A = 0, Y = 1) P(Y = 1|A = 1, Y = 1)|": "In practice, since the true underlying distribution is typically unknown, we take the empirical estimationF(f, Dk) as a surrogate for the local fairness of the k-th client, and use F(f, D) as the global fairness, whereD = Ki=1 wk Dk, wk = nk/n, n = ki=1 nk. Recall that the learned function f is expected to be both accurate (with respect to the classification task) andfair (with respect to the sensitive group A). Locally fair training is one approach to extend bias mitigationmethods from the centralized setting to the federated learning setting. Essentially, it minimizes the bias ofeach local client at each communication round and expects that the aggregation of locally fair models willyield a globally fair model. To better understand the effectiveness of locally fair training methods, we aregoing to study the following two fundamental questions in next sections:",
  ". Is there an algorithm that directly targets improving global fairness?": "The answer to the first question is local fairness does not imply global fairness in general. Nevertheless,for a proper group-based fairness metric, which is defined in , we show that global fairness canbe controlled by local fairness and data heterogeneity. To our best knowledge, this is the first work tosystematically study the relationship between local and global fairness. As for the second question, wepropose such an algorithm called FedGFT in .",
  "Locally Fair Training": "This section explores the relationship between local and global fairness, which helps us in analyzing thelocally fair training methods. The idea of minimizing the biases of local models is appealing at the firstglance, based on an intuition that global fairness will be guaranteed if all local models are fair. Chu et al.(2021) proved that this intuition is true for homogeneous clients and a special fairness metric named accuracydisparity. However, we show that it does not hold in general. All proofs are included in Appendix. Theorem 2 (In general, Global = local). Suppose F is a group-based fairness metric. For any 0 C 1,there exist a model f and local data distributions { Dk, k = 1, . . . , K} such that F(f, Dk) = 0 for all k, andF(f, D) C. Conversely, for any 0 C 1, there also exists another set of f and { Dk, k = 1, . . . , K} suchthat F(f, D) = 0, and F(f, Dk) C for all k.",
  "Corollary 3. Suppose F is a group-based fairness metric, then F(f, D) cannot be written as a linearcombination of {F(f, Dk), k = 1, . . . , K}": "Theorem 2 means that locally fair models do not imply a fair global model and vice versa. Therefore, locallyfair training methods are not always effective in general. Additionally, Corollary 3 indicates that globalfairness cannot be obtained from a simple average of local fairness. Simpsons paradox (Blyth, 1972; Bickelet al., 1975) is an excellent example to illustrate that the local property cannot represent that of the global,as shown in . Suppose a college has two departments, A and B, which accept applications from highschool students. Here, gender is the sensitive group, and each department is considered a client. Althoughthe acceptance rate is the same between males and females (i.e., SP is zero) for both departments, the overallacceptance rate is significantly biased toward males.",
  "The major factor that may lead to the failure of LFT is data heterogeneity, as revealed in the following twotheorems": "Theorem 4. Suppose F is a group-based fairness metric, f is non-degenerated, and D = Ki=1 wkDk, wherewk is the aggregation weight. A necessary and sufficient condition for F(f, D) = 0 holds for any wk is thatthere exists a constant C such that ak/bk = ck/dk = C for all k, where gk = g(f, Dk) for g {a, b, c, d}. Theorem 4 implies that if the global model is perfectly unbiased regardless of sample sizes of clients, thenall local models are also unbiased. Note that this is assured if the data distributions of different clientsare homogeneous. Inspired by Theorem 4, a natural idea to evaluate data heterogeneity is the maximumdifference of ak/bk (also ck/dk) among all clients. However, those quantities involve the global model f, whichis unknown before the training. Thus, we introduce the following concepts to decouple with f. Definition 5 (Proper Group-based fairness metrics). A group-based fairness metric F(f, D) is proper if thecorresponding b(f, D) and d(f, D) are degenerated with respect to f. In other words, there exist a function b",
  "Theorem 7 (Near IID, local implies global). Suppose F is proper, the data heterogeneity coefficient ofclients data is , and F(f, Dk) for all k, then F(f, D) +": "Theorem 7 shows that the global fairness is upper-bounded by the local fairness and data heterogeneity levelfor proper group-based fairness metrics. This upper bound is tight when data heterogeneity level is small.On the one hand, it justifies the success of locally fair training methods in the region of near-homogeneoussituations; on the other hand, it implies that locally fair training may fail when data distributions are highlydifferent. Thus, together with Theorem 2, we provide a fundamental understanding of the first question askedin . Furthermore, the proper group-based fairness metrics provide the possibility to calculate theglobal fairness value using information from local clients. In the next section, we will utilize this observationand propose a globally fair training algorithm, which answers the second question in .",
  "nkn Lk() + J(F(f(; ); D)),(2)": "where J() is a regularization function and is a penalty parameter for the fairness term. Without thefairness regularization, Eq. 2 is reduced to Eq. 1, where the gradient of the global objective function canbe calculated or estimated by aggregating the gradients of local objective functions Lk. FedAvg (McMahanet al., 2017) is inspired by this observation, which performs the gradient descent algorithm on each local clientand then aggregates local models. Thus, to generalize federated learning algorithms to fairness-regularizedobjective Eq. 2, the challenge is how to obtain the gradient of global fairness using summary statistics fromlocal clients. However, as we showed in Corollary 3, the global fairness value cannot be simply represented bylocal fairness in general.",
  "Update model parameters by any FL algorithm with local objective Eq. 3endendReturn t,Ek": "method on the server side and the optimization tool on the client side remain the same, FedGFT adapts thelocal objective function to the fairness regularization. Moreover, FedGFT also applies to the situation whereclients are purely from one group (for example a client with all points from Group A, and another client withall points from group B), which is not allowed for LFT.Remark 9. Many fairness metrics are not differentiable. Taking SP for example, ak = P(Y = 1, A = 1) =nki=1 1f(X(i)k )>0.51Ak(i)=0 is not differentiable. To employ gradient-based optimization method, a common",
  "strategy is using a surrogate, such as the softmax score nki=1 f(X(i)k )1Ak(i)=0": "Furthermore, we prove that FedGFT will converge to a stationary point when we use gradient-basedoptimization tools. The complete statement and proof are included in the supplementary document.Theorem 10 (Covergence analysis). Suppose the local clients apply one-step stochastic gradient descent tooptimize Eq. 3, and the global server updates the global model by averaging a random subset of local models.Let t be the parameter of the global model at round t. Under mild assumptions, for a step-size sequence{t, t = 0, . . . , T 1}, we have",
  "Datasets. We use the following three datasets in this subsection": "1. Adult dataset (Dua & Graff, 2017). This dataset contains the income level and demographic attributesof 48842 people. We train a logistic regression model to predict a binary response Income (high orlow) with 14 continuous and categorical predictors. The predictor Race (white or non-white) isconsidered the sensitive group. 2. COMPAS dataset (Angwin et al., 2016). This dataset includes ten demographic attributes of 6172criminal defendants and whether they recidivate in two years. A logistic model is trained to predictrecidivism, and gender is the sensitive variable. 3. CelebA dataset (Liu et al., 2015). This dataset contains 202, 599 face images, and each image has 40binary attributes. We train a ResNet18 model (He et al., 2016) targeting at classifying the Smilingattribute (yes or no), and take Male (yes or no) as the sensitive attribute. To speed up the training",
  "process, we randomly select 10, 000 images for training and 6, 000 for testing in each independentexperiment": "Setup. There are six methods in comparison: baseline method FedAvg (McMahan et al., 2017), state-of-the-art fairness-aware FL FairFed (Ezzeldin et al., 2021) and FedFB (Zeng et al., 2021), LFT methods locallyreweighing LRW (Abay et al., 2020) and FedLFT, and our proposed globally fair method FedGFT. Foreach dataset, we first randomly split the original dataset into three parts, training, validation, and test dataset.The training dataset is further split into disjoint subsets, serving as local datasets of clients. The steps ofdividing the training dataset are detailed as follows. First, we generate the proportion of each combination ofthe group variable A and response variable Y for each client from a Dirichlet distribution Dir(). A larger implies more homogeneous clients. Then, we randomly assign the corresponding proportion of data pointsto each client. Throughout this section, takes values in 0.5, 5, and 100. After training is done, the testaccuracy (using zero-one loss) and fairness metric of the global model is calculated on the test dataset. Notethat we conduct the experiments using both SP and EOP as the fairness metric, respectively. For eachcombination of dataset, learning method, and , we run 20 independent experiments. The default hyper-parameters are listed in . The penalty parameter for FairFed is chosen from{0.1, 1, 10} with cross-validation, and for FedGFT is chosen from {10, 20, 50}. Also, the regularizationfunction used by FedGFT is J(x) = x2. Results. Experiment results are summarized in . We can see that FedGFT has the smallest biasamong almost all situations, while the accuracy drop by using FedGFT is negligible. Notably, FedLFT andFedFB achieve similar fairness performance as FedGFT but experience markedly degraded accuracy. LRWis worse than FedGFT except for EOP in the CelebA dataset. While the accuracy of FedGFT is within twostandard errors compared to the best method, FedGFT significantly decreases the bias even in the highlyheterogeneous case. Discussion on the choice of hyper-parameters for FedGFT. The three datasets used in this paperdiffer in learning difficulty, which influences the choice of proper hyper-parameters. We also conduct theexperiments that vary each hyper-parameter with communication round T = 50, which ensures that themodel achieves convergence at the end of training. On the COMPAS dataset, we observe that the learningrate and number of epochs will jointly determine the performance of FedGFT. When the learning rate andnumber of epochs is increased, it will first accelerate model convergence (less communication round), sincethere are more local updating steps in each round and the step size is increased. However, when the learningrate or the number of epochs is too large, it will impede model training instead. In particular, a large learningrate will cause the local model to oscillate around the minimizer and hence hard to converge, which is aknown issue of the classical neural network training using an optimizer with a fixed learning rate. On theother hand, a large number of epochs can amplify the difference among local models at each communication",
  "FedFB76.6 (0.3)1.6 (1.5)NA54.2 (0.0)0.1 (0.0)NANANANA": "round, causing slow convergence for the global model. Nevertheless, according to the above analysis, wemay accommodate the choice of hyper-parameter by using an optimizer with shrinking learning rate and anobjective that can address data heterogeneity. Moreover, experimental results with different combination ofhyper-parameters consistently show that FedGFT outperforms other methods. Lastly, it is recommended tochoose those hyper-parameters through cross-validation. Discussion on the required conditions of FedGFT. In our originally proposed FedGFT, there aretwo assumptions: (1) the fairness metric is proper, (2) all clients are involved in each federated round. Here,a proper group-based fairness metric plays an essential role for FedGFT, while the second condition canbe relaxed. Specifically, for cross-device federated learning, we can apply FedGFT as long as clients areindependently sampled in each round. Recall that our ultimate goal is optimizing the penalized objective Eq.(2), and FedGFT shows that its gradient can be written as the linear combination of gradients from localclients. When clients are sampled independently, then we can replace the whole gradient with its stochasticcounterpart, namely the combination of gradients from those independently sampled clients. This is akin tothe relationship between gradient descent algorithm and stochastic gradient descent algorithm.",
  "Performance under the Extreme Case of Data Heterogeneity": "In this subsection, we performed experiments under the scenario where each clients data are purely fromone group. In this case, local fairness is not well-defined. Therefore, locally fair training is not applicable,and our proposed FedGFT is preferred instead. With other settings the same as above subsection, weconduct experiments on the COMPAS dataset to corroborate our algorithms effectiveness. The results aresummarized in . From the results, FedGFT still mitigates the bias compared to FedAvg, though theaccuracy-fairness trade-off is worse than the situation where the clients have data from both groups.",
  "Performance under Different Client Heterogeneity Levels": "In this subsection, we investigate the learning algorithms performance given the following two scenarios:1. there are many more clients. 2. The number of data points owned by each client is highly unequal. Weconduct experiments on the COMPAS dataset and compare all six methods except FedFB. For the first scenario, with other setting the same as in Subsection 6.1, the number of clients is 100. That is,each client has tens of data points on average. The results are reported in . Compared to",
  "where only 10 clients are involved, FedGFT is still the algorithm with the smallest bias. Moreover, theaccuracy drop by FedGFT is similar or smaller to other fairness-aware algorithms": "Next, we consider the scenario where each client can have different number of data points. With other settingthe same as in Subsection 6.1, we modify the data spliting step as follows. First, we generate the proportionof data points for each client from a Dirichlet distribution Dir(). Then, for each combination of sensitiveattribute A and response Y , we randomly assign the corresponding proportion of data points to each client.It means that the data is completely homogeneous to each client, which is the most favorable scenario forlocally fair training methods. The parameter takes values in 0.5, 5, and 100, and a higher indicates moreevenly distributed numbers of data points. For example, the median ratio of the largest and smallest numberof data points is around 400 when = 0.5, while this ratio is 1.3 for = 100. The results of clients with varying numbers of data points are reported in . Clearly, FedGFT has thesmallest bias even in the homogeneous setting, with a comparable accuracy to FedAvg. The results, togetherwith the experiments under different data heterogeneity levels, demonstrate that FedGFT is a robust andeffective fairness-aware federated learning algorithm.",
  "Conclusion": "In this work, we proved that the fairness of the global model in federated learning is upper-bounded bythe fairness of local models and the data heterogeneity level for proper group-based fairness metrics, thusproviding theoretical support for locally fair training methods. Nevertheless, locally fair training may failin highly heterogeneous cases. We also proposed a globally fair training method called FedGFT for propergroup-based fairness metrics, which directly minimizes the fairness-penalized empirical loss of the globalmodel and can be easily incorporated with existing FL algorithms. Experiments on three real-world datasetsshowed that the proposed method can significantly reduce the model bias while retaining a similar predictionaccuracy compared to the baseline. Limitations There are several problems not fully addressed and will be interesting future work. For example,the calibration is not a proper fairness metric, thus, how to generalize our results to calibration, or moregenerally, group-based metrics, is of interest. It is also worth thinking about how to generalize the proposedmethod regarding multiple group variables. Moreover, an examination of the proposed method on large-scaledatasets is desired. For instance, Ding et al. (2021) proposed to retire the Adult dataset and use a suite oflarger datasets derived from US Census surveys for fair machine learning research.",
  "Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In Proc.ICCV, December 2015": "Pranay K Lohia, Karthikeyan Natesan Ramamurthy, Manish Bhide, Diptikalyan Saha, Kush R Varshney,and Ruchir Puri. Bias mitigation post-processing for individual and group fairness. In Proc. ICASSP, pp.28472851. IEEE, 2019. Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. Communication-efficient learning of deep networks from decentralized data. In Artificial intelligence and statistics, pp.12731282. PMLR, 2017.",
  "P(A = 0)": "Therefore, let a(f, D) = P(Y = 1, A = 0), b(f, D) = P(A = 0), then P(Y = 1|A = 0) = a(f, D)/b(f, D).Similarly, we have P(Y = 1|A = 1) = c(f, D)/d(f, D), where c(f, D) = P(Y = 1, A = 1), d(f, D) = P(A = 1).Thus, SP is a group-based fairness metric. Furthermore, it is clear that b(f, D) and d(f, D) are independentof f, hence SP is also a proper group-based fairness metric.",
  "BMissing Proofs in": "Proof of Theorem 2. We first prove that fair local models do not imply a fair global model. Let gkand g be the abbreviations of g(f, Dk) and g(f, D) for g {a, b, c, d} (see Definition 1), respectively. Notethat the summation in g (f, k wkDk) can be taken out because g is an expectation by Definition 1. Forexample, a(f, D) can be written as",
  "where wk = nk/n": "Note that wk can take an arbitrary value in as long as Dks are properly chosen. Furthermore, accordingto Definition 1, gks are arbitrarily manipulable as well. Next, we will construct gks and wks that satisfyEq. 4. In particular, we consider quantities with a1/b1 = c1/d1 = 1, w1 = (1 + C)/2, and ak/bk = ck/dk = 0and wk = (1 w1)/(K 1) for k = 2, . . . , K. By simple calculation, when b1, d2, . . . , dK converge to one andd1, b2, . . . , bK converge to zero, F(f, D) converges to w1, which is larger than C. It immediately implies thatthere exists a proper choice satisfying Eq. 4.",
  "where x means the floor of a number x": "Proof of Corollary 3. If the claim is false, then there exists a sequence of constants {vk, k = 1, . . . , K},such that F(f, D) = Kk=1 vkF(f, Dk) always holds. Now, evoking Theorem 2, we know it is possible thatF(f, D) > 0 with F(f, Dk) = 0 for all k, which is a contradiction, and thus concludes the proof.",
  "k wkbk)(": "k wkdk) on the both hand sides and rearranging the above equation yields thatwTMw = 0, where w = (w1, . . . , wK)T and M RKK is a matrix with (i, j)-th element Mij = aidj bicj.Thus, wTMw = 0 for any w is equivalent to that aidj bicj = 0 for all 1 i, j K, which completes theproof.",
  "Assumption 18. The objective function is lower bounded, L := inf L() >": "Remark 19. Assumptions 14, 15, and 16 are standard in optimization literature, which ensure that the SGDupdate produces a sufficiently large decrease in the function value, leading to the convergence. Assumption 17ensures the convergence with data heterogeneity. Larger B indicate more severe data heterogeneity, andB = 1 corresponds to the homogeneous case.Remark 20. If we use GD instead of SGD, then the update of local clients will be"
}