{
  "Abstract": "State-of-the-art semi-supervised learning (SSL) approaches rely on highly confident predic-tions to serve as pseudo-labels that guide the training on unlabeled samples. An inherentdrawback of this strategy stems from the quality of the uncertainty estimates, as pseudo-labels are filtered only based on their degree of uncertainty, regardless of the correctnessof their predictions. Thus, assessing and enhancing the uncertainty of network predictionsis of paramount importance in the pseudo-labeling process. In this work, we empiricallydemonstrate that SSL methods based on pseudo-labels are significantly miscalibrated, andformally demonstrate the minimization of the min-entropy, a lower bound of the Shannonentropy, as a potential cause for miscalibration. To alleviate this issue, we integrate a simplepenalty term, which enforces the logit distances of the predictions on unlabeled samples toremain low, preventing the network predictions to become overconfident. Comprehensiveexperiments on a variety of SSL image classification benchmarks demonstrate that the pro-posed solution systematically improves the calibration performance of relevant SSL models,while also enhancing their discriminative power, being an appealing addition to tackle SSLtasks. Code :",
  "Introduction": "Deep learning models have significantly advanced the state-of-the-art across a myriad of tasks (Masana et al.,2022; Minaee et al., 2021). Nonetheless, their success has been often contingent on the availability of largeamounts of labeled data. Having access to curated large training datasets, however, is not easy, and ofteninvolves a tremendous human labor, particularly in those domains where labeling data samples requiresexpertise, hindering the progress to address a broader span of real-world problems. Semi-supervised learning (SSL) (Chapelle et al., 2006) mitigates the need for large labeled datasets by pro-viding means of efficiently leveraging unlabeled samples, which are easier to obtain. This learning paradigmhas led to a plethora of approaches, which can be mainly categorized into consistency regularization (Bach-man et al., 2014; Laine & Aila, 2017) and pseudo-labeling (Lee et al., 2013; Xie et al., 2020b) methods.Indeed, state-of-the-art SSL approaches (Wang et al., 2023; Zhang et al., 2021; Sohn et al., 2020; Chenet al., 2023; Zheng et al., 2022) combine both strategies, obtaining promising results. The underlying ideaof these approaches follows the low-density and smoothness assumptions in SSL (Chapelle & Zien, 2005). Inparticular, incorporating pseudo-labels from unlabeled data points into the training process aids the deci-sion boundary to lie in low density regions. Furthermore, consistency regularization assumes that the same",
  "Published in Transactions on Machine Learning Research (09/2024)": "To quantify the discussion, we present our results in the . We also illustrate the logit distribution ofthe three methods under consideration (a) FreeMatch, (b) FreeMatch + Ours and (c) FreeMatch with logitsscaled by a temperature of 2. In these plots, it can be observed that the impact on the logit distribution ofour method, and systematically applying temperature scaling to all samples, is different, which empiricallysupports our hypothesis regarding their different behaviour.",
  "Semi-supervised learning": "Prevailing semi-supervised learning (SSL) approaches heavily rely on the concept of pseudo-labels (Lee et al.,2013; Shi et al., 2018) and consistency regularization (Bachman et al., 2014; Laine & Aila, 2017; Sajjadiet al., 2016; Tarvainen & Valpola, 2017; Miyato et al., 2018), where labels are dynamically generated forunlabeled data throughout the training process. Essentially, these methods exploit the role of perturbations,by stochastically perturbing the unlabeled images and enforcing consistency across their predictions. Thisconsistency is achieved by a pseudo-supervised loss, where the predictions over the strong perturbations aresupervised by the pseudo-labels obtained from the weak perturbations (Xie et al., 2020a; Wang et al., 2023;Zhang et al., 2021; Chen et al., 2023; Zheng et al., 2022; Yang et al., 2023; Xu et al., 2021; Zheng et al., 2022).This paradigm to use artificial labels facilitates the integration of unlabeled data into the learning process,thereby augmenting the training set and improving the model ability to generalize. To avoid introducingnoise in the pseudo-supervision process, these approaches retain a given pseudo-label only if the model",
  "Calibration": "Recent evidence (Guo et al., 2017; Mller et al., 2019; Mukhoti et al., 2020) has shown that deep networksare prone to make overconfident predictions due to miscalibrated output probabilities.This emerges asa byproduct of minimizing the prevalent cross-entropy loss, which occurs when the softmax predictionsfor all training samples fully match the ground-truth labels, and thus the entropy of output probabilities isencouraged to be zero. To mitigate the miscalibration issue, and to better estimate the predictive uncertaintyof deterministic models, two main families of approaches have emerged recently: post-processing (Guo et al.,2017; Ding et al., 2021; Tomani et al., 2021) and learning (Pereyra et al., 2017; Mller et al., 2019; Mukhotiet al., 2020; Liu et al., 2022; Cheng & Vasconcelos, 2022; Liu et al., 2023; Noh et al., 2023; Murugesanet al., 2023; Larrazabal et al., 2023; Park et al., 2023) approaches. Among the post-processing strategies,Temperature Scaling (TS) (Guo et al., 2017) has been a popular alternative, which manipulates logit outputsmonotonely, by applying a single scalar temperature parameter. This idea is further extended in (Dinget al., 2021), where a local TS per pixel is provided by a regression neural network. Nevertheless, despite thesimplicity of these methods, learning approaches have arisen as a more powerful choice, as in this scenario themodel adapts to calibration requirements alongside its primary learning objectives, optimizing both aspectssimultaneously. Initial attempts integrated learning objectives that maximize the entropy of the networksoftmax predictions either explicitly (Pereyra et al., 2017; Larrazabal et al., 2023), or implicitly (Mukhotiet al., 2020; Mller et al., 2019; Cheng & Vasconcelos, 2022). In order to alleviate the non-informativenature of simply maximizing the entropy of the softmax predictions, recent work (Liu et al., 2022; 2023)has presented a generalized inequality constraint, which penalizes logits distances larger than a pre-definedmargin.",
  "Problem statement": "In the semi-supervised learning scenario, the training dataset is composed of labeled and unlabeled datapoints. In this setting, let DL = {(xi, yi)}NLibe the labeled dataset and DU = {xi}NUithe unlabeled dataset,where NL and NU represent the number of labeled and unlabeled samples, respectively, and NL << NU.Furthermore, xi Rd is a d-dimensional training sample, with yi {0, 1}K its associated ground truth(only for labeled data points, i.e., xi DL) that assigns one of the K classes to the sample. The objectiveis, given a batch of labeled and unlabeled samples, to find an optimal set of parameters of a deterministic",
  "iDUyi log p(y|(xi))(2)": "where yi is the one-hot encoding of the arg max of the softmax probabilities for the weak augmented version,i.e., arg max(p(y|(xi)). To avoid that samples with high uncertainty, and possibly incorrect predictions,intervene in the optimization of the term in eq. (2), a common strategy is to retain only discrete pseudo-labelswhose largest class probability fall above a predefined threshold (Lee et al., 2013; Wang et al., 2023; Sohnet al., 2020; Zhang et al., 2021). This results in the following objective:",
  "Revisiting the calibration of semi-supervised models": "We now introduce a series of observations revealing several intrinsic properties of semi-supervised methodsbuilt up on pseudo-labels generation, which allows us to motivate a calibration technique tailored for pseudo-label based SSL. Observation 1. Semi-supervised learning degrades the calibration performance. An importantbody of literature on SSL relies on pseudo-labeling to leverage the large amount of unlabeled samples. Toachieve this, a very common strategy is to generate weak augmentations of each unlabeled image, whosepredictions serve as supervision for their strong augmentation counterpart, as presented in eq. (3). Pseudo-labeling (Lee et al., 2013) is indeed closely related to entropy regularization (Grandvalet & Bengio, 2004),which favors a low-density separation between classes, a commonly assumed prior for semi-supervised learn-ing. While minimizing the entropy of the predictions can actually improve the discriminative performanceof neural networks, it inherently favours overconfident predictions, which is one of the main causes of mis-calibration. brings empirical evidence about this observation, where we can observe that despitebringing performance gains, in terms of accuracy, pseudo-label based SSL methods naturally degrade thecalibration properties of a supervised baseline trained with a few labeled samples. Observation 2. Pseudo-labeling in SSL indeed minimizes min-entropy on unlabeled points. Theuse of a hard label makes pseudo-labeling closely related to entropy minimization (Grandvalet & Bengio,2004). Nevertheless, the different transformations that unlabeled images follow in modern SSL methodsproduce different probability distributions for the weak and strong versions of the same image, where the",
  ",(4)": "where si = p(y|xi) is used for simplicity, and the superscripts w and s denote weak and strong transfor-mations, respectively. We now split the unlabeled dataset into DU, which contains the unlabeled sampleswhose predicted class from weak and strong augmentations are different, i.e., arg max(swi ) = arg max(ssi)and DU, containing the samples whose predicted class from weak and strong augmentations are the same.Thus, we can decompose the right-hand term in eq. (4) into two terms, one acting over DU and one overDU:",
  "Density": "Target Class: 5FreeMatch : Observation 3.These plots depict the Kernel Density Estimation of the logit distributionsobtained by (left) the supervised baseline trained with eq. (1) and (right) FreeMatch on STL-10, for thesamples belonging to class 5. We can observe that, even for non-target classes (k=5), the logit magnitudesin FreeMatch are larger, which translates to higher overconfidence in both correct and incorrect predictions.We select STL-10 due to its number of classes (10 vs. 100 in CIFAR-100).",
  "Our solution": "Based on our findings, especially in Observation 3, it becomes evident that an effective strategy foraddressing miscalibration within the SSL scenario involves controlling the magnitude of predicted logitsfor unlabeled samples. Furthermore, from Observation 2 we can derive that the underlying mechanismmagnifying the miscalibration issue stems from the hidden min-entropy term on the data points in DU ,which represents the majority of unlabeled samples. Thus, we resort to an inequality constraint that imposesa controllable margin on the logit distances of predictions in samples from DU . This constraint, which drawsinspiration from (Liu et al., 2022), takes the following form d(l) m, where d(l) = (maxj(lj) lk)1kK RK represents the vector of logit distances between the winner class and the rest (only for samples in DU )and m a K-dimensional vector defining the margin values, with all elements equal to m R++. Note thatwhile (Liu et al., 2022) was proposed in the fully supervised learning scenario, in this work we only enforcethe constraint on a subset of samples, which is motivated by the observations presented in 3.2. Furthermore,this choice is also supported empirically in the ablation studies presented in the experiments.",
  "kmax(0, maxj (li,j) li,k mk)(8)": "where the second term, i.e., the non-linear ReLU penalty, prevents logit distances from exceeding a prede-termined margin m, and R+ is a blending hyperparameter which controls the contribution of the CEloss and the corresponding penalty. The intuition behind this penalty term is simple. For the winner logitswhere the distance with the remaining logits is above the margin m, a gradient will be back-propagated toenforce those values to decrease. As a result, the whole logit magnitudes will decrease, potentially alleviatingthe miscalibration issue in the set of unlabeled samples LU , which dominate the SSL training.",
  "Experiments": "Datasets. We resort to the recent Unified Semi-supervised Learning Benchmark for Classification (USB)(Wang et al., 2022), which compiles a diverse and challenging benchmark across several datasets. In partic-ular, we focus on three popular datasets: CIFAR-100 (Krizhevsky & Hinton, 2010), which has significantvalue as a standard for fine-grained image classification due to its wide range of classes and detailed objectdistinctions; STL-10 (Coates et al., 2011), which is widely recognized for its limited sample size and ex-tensive collection of unlabeled data, rendering it a challenging scenario of special significance in the contextof SSL; and EuroSAT (Helber et al., 2019), containing 10 unique fine-grained categories related to earthobservation and satellite imagery analysis, and important challenges such as high variability and imbalanceclasses. Last, we also conduct further experiments in the long-tailed version of CIFAR-100. Architectures.We have prioritized Vision Transformers (ViT) over Convolutional Neural Networks(CNNs), for three main reasons related to discriminative performance, quality of uncertainty estimates,generalization and transfer learning capabilities. First, the emergence of ViTs has proven these models tooutperform their CNNs counterparts (Raghu et al., 2021; Cai et al., 2022).Second, from a calibrationstandpoint, ViTs have also shown to be better calibrated than CNNs (Minderer et al., 2021; Pinto et al.,2021). Hence, due to their superior discriminative and calibration performance, they pose a more challengingscenario to evaluate the effectiveness of the proposed strategy. And last, the fine-tuning capabilities of ViTsenable effective transfer learning across diverse visual tasks and datasets. This capability is particularlyadvantageous in scenarios where labeled data is scarce, as it allows leveraging pre-trained representationslearned from large-scale datasets, significantly minimizing the amount of training iterations while maintain-ing consistent performance2. More concretely, we employ a ViTSmall (Gani et al., 2022) with a patch sizeof 2 and an image size of 32 for CIFAR-100 and EuroSAT, in accordance with the standard in USB, and aViT-Small with an image size of 96 for STL10.",
  "We use same settings for all models and benchmarks to provide a fair comparison": "Training, evaluation protocol and metrics. While several works use different amounts of labeled datain their experiments, we perform due diligence and follow the settings proposed in USB (Wang et al., 2022)for training. For each method and configuration, we perform three runs with different seeds, select the bestcheckpoint and report their mean and standard deviation, following the literature. We report error rates forthe accuracy performance and the expected calibration error (ECE), following the literature in calibrationof supervised models. Implementation details are discussed in Appendix.",
  "Results": "Main results. The proposed strategy is model agnostic, and can be integrated on top of any SSL approachbased on pseudo-labels, enabling substantial flexibility. For the empirical evaluation, we selected three pop-ular and relevant approaches that resort to hard pseudo-labels: FixMatch (Sohn et al., 2020), FlexMatch(Zhang et al., 2021) and FreeMatch (Wang et al., 2023) and assess the impact of adding our simple solutionduring training. Discriminative performance (table 1): we observe that in 16 out of the 18 different settings,adding the penalty in eq. (8) brings improvement gains compared to the original versions of each method,which are only trained with eq. (4). Note that these gains are sometimes substantial, improving the originalmethod by up to 4% (e.g., FixMatch in CIFAR-100(200) and EuroSAT(20) or FlexMatch in EuroSAT(20)).Furthermore, the three approaches combined with our penalty achieve very competitive performance com-pared to existing SSL literature, typically yielding state-of-the-art results. Calibration performance (table 2):Similarly, including the penalty term systematically enhances the calibration performance of the three ana-lyzed approaches, whose improvements are typically significant (up to 6-7% in several cases). An interesting observation is that, MixMatch (Berthelot et al., 2019), a consistency regularization basedapproach, yields surprisingly well calibrated models. Indeed, MixMatch has several components that haveshown to improve calibration, such as ensembling predictions and MixUp, which may explain the obtainedvalues.Nevertheless, it is noteworthy to mention that the discriminative performance compared to our",
  "FreeMatch + Ours14.861.2210.350.682.820.812.630.703.740.903.501.02": "We would like to stress that our goal is not to propose a novel state-of-the-art SSL approach, but to shed lightabout an important issue in prevalent pseudo-labeling based SSL methods, and present a simple yet efficientsolution that can improve their performance. These results demonstrate that integrating the penalty in eq. (8)during training appears as an appealing strategy to improve both accuracy and calibration performance ofpseudo-label SSL approaches. Lastly, we follow (Wang et al., 2022) and employ the Friedman rank (Friedman, 1937; 1940) to fairlycompare the performance of different methods across various settings.This metric can be defined asrankF =1mmi=1 ranki, with m being the number of evaluation settings (m = 12 in our case, 2 metrics 6 datasets), and ranki the rank of a method in the i-th setting. Hence, the lower the rank obtained,the better the method. These rankings, which are depicted in fig. 4, show that our modified versions pro-vide competitive performances considering accuracy (Error), ECE and both (All), with our FreeMatch andFlexMatch versions presenting the best alternatives across overall methods. On the impact on the logits. We now analyze in more detail the impact of incorporating the constraintduring training, particularly on the logit distribution. In fig. 5, we depict the kernel density estimation of thelogits distribution, which was negatively affected by SSL methods compared to a fully-supervised baseline(Observation 3). From these figures, we can observe two interesting findings that will have a positive effect",
  ": Radar plots for different calibration approaches based on FreeMatch. The range of the radar plotfor ECE is changed for better visualization (CIFAR-100: , EuroSAT: , STL-10: )": "In which samples to impose the penalty? A natural question that arises from our analysis is whetherconstraining the logit distances should also be applied to the samples in DU . We argue that, indeed, it is onlybeneficial to the samples in DU . While we formally exposed that the third term in eq. (8) minimizes the min-entropy, the second term has some sort of corrective effect, where pseudo-labels from weak augmentationscorrect the predictions obtained with the strong augmented versions. As labels are different, enforcing apenalty on the logit distances might indeed have a counterproductive effect. More concretely, trying tosatisfy the constraint in samples from DU , where hard predictions are from different classes, may actuallyimpede the network to learn semantically meaningful features that can bring discriminative capabilities tothe model. To support these arguments empirically, we report in table 4 the performance of the three SSLmethods - FixMatch, FlexMatch and FreeMatch when the constraint is enforced across different scenarios. Wecan clearly see that, regardless of the configuration or model used, including the penalty on the samples fromDU has a detrimental effect, with discriminative and calibration results suffering a substantial degradation. : Impact of the penalty term in the different unlabeled subsets, DU and DU for the threerelevant pseudo-labeling SSL methods studied in this work. The proposed approach is shadowed in gray andbest result in bold.",
  "FreeMatch23.922.0218.271.9516.180.3811.560.5321.361.6214.861.4816.090.8010.350.8327.441.0820.861.4818.820.6712.390.78": "Can we add our calibration strategy to other methods?. We have evaluated the impact of incorpo-rating the proposed term into relevant SSL approaches that resort to hard pseudo-labels during training. In-deed, our motivation stems from the observations that under this scenario (i.e., the standard pseudo-labelingprocess in SSL), there is a hidden min-entropy objective that dominates the training. While we have demon-strated that our approach effectively improves both the discriminative and calibration performance of thesemodels, we further assess its impact on approaches that use soft pseudo-labels, i.e., pseudo-labels on weakaugmentations are not converted to one-hot encoded vectors. For these approaches, the reasoning in eq. (5)and eq. (6) does not entirely hold in these cases. More concretely, the recent work SimMatch (Zheng et al.,2022) uses this strategy3, where the direct softmax prediction over the weak augmented samples is used assupervisory signal for its strong augmented counterpart. On the other hand, for SoftMatch (Chen et al.,2023), the weighted thresholding mask prevents overly confident predictions, distinguishing these methodsfrom those relying entirely on hard pseudo-labels. The results from this experiment are reported in ,which show that adding the proposed penalty has a positive effect favouring better calibrated models even",
  "SoftMatch + Ours21.720.3116.820.7414.530.3615.800.3211.700.8610.600.46": "Long-tailed experiments. In table 6 we report error rate and ECE on CIFAR100-LT to asses the effect ofthe proposed calibration when the class population is imbalanced. Across all cases both the method accuracyand the ECE improve consistently. It is noteworthy to mention that for all these experiments we use thesame margin as in CIFAR-100, without any adaptation to the specific setting.",
  "Additional experiments with detailed analysis across methods, comparison to additional approaches andadditional settings are reported in Appendix": "Scalability and Computational Costs. The proposed approach boils down to a simple penalty term,which can be straightforwardly integrated on existing methods by modifying the loss function, whose compu-tational cost is negligible. Indeed, our learning approach does not involve architectural changes, avoiding anyincrease in model complexity. Thus, the strategy proposed in this work further improves the discriminativeand calibration performance of existing SSL methods without incurring additional overheads.",
  "Conclusion": "In this work we have raised awareness of the miscalibration problem induced by pseudo-label training, whichis one of the most popular approaches for SSL. We demonstrated that the unsupervised loss used by pseudo-label methods is dominated by the min-entropy term, a lower bound of the Shannon entropy, and identified itas a potential source of miscalibration. We then proposed a simple solution based on enforcing a fixed marginconstraint between the winner class and its contenders. Our solution on popular SSL datasets and methodsyields consistent calibration improvements, whereas we also found consistent gains in terms of predictiveaccuracy, typically outperforming the state-of-the-art for SSL.",
  "Philip Bachman, Ouais Alsharif, and Doina Precup. Learning with pseudo-ensembles. Advances in neuralinformation processing systems, 27, 2014": "David Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital Oliver, and Colin A Raffel.Mixmatch: A holistic approach to semi-supervised learning. Advances in Neural Information ProcessingSystems, 32, 2019. Zhaowei Cai, Avinash Ravichandran, Paolo Favaro, Manchen Wang, Davide Modolo, Rahul Bhotika,Zhuowen Tu, and Stefano Soatto. Semi-supervised vision transformers at scale. In Alice H. Oh, AlekhAgarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Sys-tems, 2022.",
  "Olivier Chapelle, Bernhard Schlkopf, and Alexander Zien (eds.). Semi-Supervised Learning. The MIT Press,2006": "Hao Chen, Ran Tao, Yue Fan, Yidong Wang, Jindong Wang, Bernt Schiele, Xing Xie, Bhiksha Raj, andMarios Savvides. Softmatch: Addressing the quantity-quality tradeoff in semi-supervised learning. In TheEleventh International Conference on Learning Representations, 2023. Jiacheng Cheng and Nuno Vasconcelos. Calibrating deep neural networks by pairwise constraints. In Pro-ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1370913718,2022. Adam Coates, Andrew Ng, and Honglak Lee. An analysis of single-layer networks in unsupervised featurelearning. In Proceedings of the fourteenth international conference on artificial intelligence and statistics,pp. 215223. JMLR Workshop and Conference Proceedings, 2011. Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. Randaugment: Practical automated dataaugmentation with a reduced search space. In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition Workshops, pp. 702703, 2020. Zhipeng Ding, Xu Han, Peirong Liu, and Marc Niethammer.Local temperature scaling for probabilitycalibration. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 68896899, 2021.",
  "Samuli Laine and Timo Aila. Temporal ensembling for semi-supervised learning. In International Conferenceon Learning Representations, 2017": "Agostina J Larrazabal, Csar Martnez, Jose Dolz, and Enzo Ferrante. Maximum entropy on erroneouspredictions: Improving model calibration for medical image segmentation. In International Conference onMedical Image Computing and Computer-Assisted Intervention, pp. 273283. Springer, 2023. Dong-Hyun Lee et al. Pseudo-label: The simple and efficient semi-supervised learning method for deep neuralnetworks. In Workshop on challenges in representation learning, ICML, volume 3, pp. 896. Atlanta, 2013.",
  "Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollr.Focal loss for dense objectdetection. In Proceedings of the IEEE international conference on computer vision, pp. 29802988, 2017": "Bingyuan Liu, Ismail Ben Ayed, Adrian Galdran, and Jose Dolz. The devil is in the margin: Margin-basedlabel smoothing for network calibration. In Proceedings of the IEEE/CVF Conference on Computer Visionand Pattern Recognition, pp. 8088, 2022. Bingyuan Liu, Jrme Rony, Adrian Galdran, Jose Dolz, and Ismail Ben Ayed. Class adaptive networkcalibration. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,pp. 1607016079, 2023. Charlotte Loh, Rumen Dangovski, Shivchander Sudalairaj, Seungwook Han, Ligong Han, Leonid Karlinsky,Marin Soljacic, and Akash Srivastava. Mitigating confirmation bias in semi-supervised learning via efficientbayesian model averaging. Transactions on Machine Learning Research, 2023. Marc Masana, Xialei Liu, Bartomiej Twardowski, Mikel Menta, Andrew D Bagdanov, and Joost Van De Wei-jer. Class-incremental learning: survey and performance evaluation on image classification. IEEE Trans-actions on Pattern Analysis and Machine Intelligence, 45(5):55135533, 2022. Shervin Minaee, Yuri Boykov, Fatih Porikli, Antonio Plaza, Nasser Kehtarnavaz, and Demetri Terzopoulos.Image segmentation using deep learning: A survey. IEEE transactions on pattern analysis and machineintelligence, 44(7):35233542, 2021. Matthias Minderer, Josip Djolonga, Rob Romijnders, Frances Hubis, Xiaohua Zhai, Neil Houlsby, DustinTran, and Mario Lucic. Revisiting the calibration of modern neural networks. Advances in Neural Infor-mation Processing Systems, 34:1568215694, 2021. Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, and Shin Ishii. Virtual adversarial training: a regular-ization method for supervised and semi-supervised learning. IEEE transactions on pattern analysis andmachine intelligence, 41(8):19791993, 2018. Jishnu Mukhoti, Viveka Kulharia, Amartya Sanyal, Stuart Golodetz, Philip Torr, and Puneet Dokania.Calibrating deep neural networks using focal loss. Advances in Neural Information Processing Systems,33:1528815299, 2020.",
  "Mamshad Nayeem Rizve, Kevin Duarte, Yogesh S Rawat, and Mubarak Shah. In defense of pseudo-labeling:An uncertainty-aware pseudo-label selection framework for semi-supervised learning, 2021. URL": "Mehdi Sajjadi, Mehran Javanmardi, and Tolga Tasdizen. Regularization with stochastic transformationsand perturbations for deep semi-supervised learning. Advances in neural information processing systems,29, 2016. Weiwei Shi, Yihong Gong, Chris Ding, Zhiheng MaXiaoyu Tao, and Nanning Zheng. Transductive semi-supervised deep learning using min-max features. In Proceedings of the European Conference on ComputerVision (ECCV), pp. 299315, 2018. Kihyuk Sohn, David Berthelot, Nicholas Carlini, Zizhao Zhang, Han Zhang, Colin A Raffel, Ekin DogusCubuk, Alexey Kurakin, and Chun-Liang Li. Fixmatch: Simplifying semi-supervised learning with con-sistency and confidence. Advances in Neural Information Processing Systems, 33, 2020. Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna.Rethinking theinception architecture for computer vision. In Proceedings of the IEEE conference on computer vision andpattern recognition, pp. 28182826, 2016. Antti Tarvainen and Harri Valpola. Mean teachers are better role models: Weight-averaged consistencytargets improve semi-supervised deep learning results. Advances in neural information processing systems,30, 2017. Christian Tomani, Sebastian Gruber, Muhammed Ebrar Erdem, Daniel Cremers, and Florian Buettner.Post-hoc uncertainty calibration for domain drift scenarios. In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pp. 1012410132, 2021. Yidong Wang, Hao Chen, Yue Fan, Wang Sun, Ran Tao, Wenxin Hou, Renjie Wang, Linyi Yang, Zhi Zhou,Lan-Zhe Guo, et al. USB: A unified semi-supervised learning benchmark for classification. Advances inNeural Information Processing Systems, 35:39383961, 2022. Yidong Wang, Hao Chen, Qiang Heng, Wenxin Hou, Yue Fan, Zhen Wu, Jindong Wang, Marios Savvides,Takahiro Shinozaki, Bhiksha Raj, Bernt Schiele, and Xing Xie. Freematch: Self-adaptive thresholding forsemi-supervised learning. In The Eleventh International Conference on Learning Representations, 2023.",
  "FreeMatch + Ours21.361.6216.090.804.301.463.500.7013.181.618.571.05": "In the main paper, we included several relevant SSL methods to compare both error and calibration perfor-mance. We did not include a considerably large set of approaches because we need to run three times eachmethod in each setting (so that each evaluated method means 36=18 runs, with an average of 10 hoursper run). Furthermore, we would like to stress again that our goal in this work is not to provide a novelstate-of-the-art SSL method, but investigate the miscalibration issue of pseudo-labeling SSL, identify thesource of the problem, and provide a solution that can enhance the calibration of this family of methods.Having said this, the discriminative performance of additional methods is provided in (Wang et al., 2022),which is used to complement the discriminative comparison in , whose results can be found in of this appendix.",
  "BDataset details": "We provide detailed insights into the datasets utilized in our experiments, whose configuration is stronglyinspired by the recent USB (A Unified Semi-supervised Learning Benchmark for Classification) benchmark(Wang et al., 2022). For CIFAR-100, a renowned benchmark for fine-grained image classification, we consid-ered two label settings: 2 labeled samples and 4 labeled samples per class for each of the 100 classes,resultingin a total of 50,000 training samples and 10,000 samples for testing. Each image in CIFAR-100 is sizedat 3232 pixels. STL-10, known for its limited sample size and extensive unlabeled data, offers a uniquechallenge. We employed two label settings as well: 4 labeled samples and 10 labeled samples per class forall 10 classes, and an additional 100,000 unlabeled samples for training, along with 8,000 samples for test-ing. Each STL-10 image measures 9696 pixels. Lastly, EuroSAT, based on Sentinel-2 satellite images,features two label settings: 2 labeled samples per class and 4 samples per class for 10 classes. With a totalof 16,200 training samples, including labeled and unlabeled images and 5,400 testing samples, EuroSATimages are sized at 6464. While literature includes SVHN and CIFAR-10 datasets, we do not use themfor evaluation, as state-of-the-art SSL methods already achieve a performance comparable to that of fullysupervised training on these datasets. The selected datasets offer diverse challenges and settings, allowingfor a comprehensive evaluation of semi-supervised learning methods across various domains. For the long-tailed CIFAR-100 experiments, we investigate two subsets of the dataset, with N1 represent-ing the number of samples in the minority class and M denoting the number of samples in the majority class.The imbalance ratio is controlled by l and u, where both parameters are set to -10, 10 or 15, ensuring aconsistent level of label imbalance across categories. In the first setting, we set the imbalance ratio of labeledsamples l to 10 and the imbalance ratio of unlabeled samples u to 10, with the number of labeled samples(N1) set to 150 and the number of unlabeled samples (M) set to 300. In the second setting, we maintain thesame label imbalance ratio (l = 10) and number of labeled samples (N1 = 150), but we set the unlabeledsample imbalance ratio u to -10. Finally, in the third setting, we increase the l to 15 while keeping theu at 15, with N1 and M set to 150 and 300, respectively. This configuration introduces a higher degree oflabel imbalance, with the minority class having 15 times fewer samples than the majority class. We followthe same setup as authors in (Wang et al., 2022) for the classic setting using WideResnet. These settingsallow us to comprehensively evaluate the performance of machine learning models under varying degrees oflabel imbalance and class distribution scenarios within the CIFAR100-LT dataset.",
  "CImplementation details": "Method-dependent hyperparemeters. In the context of pseudo-labeling methods, while FixMatch and Flex-Match rely on a single threshold hyperparameter for pseudo-label selection, FreeMatch introduces additionalhyperparameters such as a pre-defined threshold , unlabeled batch ratio , unsupervised loss weight wu,fairness loss weight wf, and EMA decay . To avoid unfair comparisons across methods, we use the defaultvalues for all these hyperparameters, which are reported in their respective papers. Specific-case: BAM. In our comparison with BAM (Loh et al., 2023), we encountered difficulties in hyper-parameter tuning. BAM employs the quantile Q over the batch to determine the threshold for pseudo-labelselection, which serves as a key hyperparameter. In the cited paper, Q = 0.75 for the CIFAR-100 benchmarkand Q = 0.95 for the CIFAR-10 benchmark were chosen. Additionally, BAM incorporates a separate Adamoptimizer for the Bayesian Neural Network (BNN) layer, with a fixed learning rate of 0.01. Notably, forthe sharpening temperature parameter in BaM-UDA, BAM opts for t = 0.9 as opposed to t = 0.4 utilizedin UDA. These hyperparameter selections are specific to both the dataset and the method employed, thuspresenting a challenge when evaluating the method across different datasets. For our experiments, we choseto focus on CIFAR-100 due to the inherent complexity involved in conducting extensive hyperparametersearches, not only concerning the dataset or method, but also regarding the optimizer and learning rate forthe introduced BNN layer. Margin Selection with Limited Fine-tuning: We stress that in the majority of cases, we did not extensivelyfine-tune the margin hyperparameter. Across various experiments, settings and datasets, the chosen marginvalues remained consistent without significant adjustments. Specifically, for the CIFAR-100 and EuroSAT",
  "DModel choices": "In this work we have selected three relevant SSL approaches based on hard pseudo-labels, as our findingsare closely related to these approaches. More concretely, we demonstrate empirically our observations inFixMatch (Sohn et al., 2020), FlexMatch (Zhang et al., 2021) and FreeMatch (Wang et al., 2023), as they arepopular SSL methods, some of them published recently (i.e., FreeMatch has been published in 2023). Thus,for the and , we have shown the effect of adding the proposed term in all the three approaches.Similarly, the impact of our term over the three methods is evaluated in (i.e., CIFAR-100-LT), as weconsider that long-tailed classification represents an important task, and assessing the performance of thethree approaches will undoubtedly strengthen the message that our solution can improve both classificationand calibration performance across a general family of SSL approaches. Regarding the ablation studies toevaluate the impact of several choices, we have used FreeMatch, the most recent approach among the three,in all the ablations (Fig 6 and ), except when comparing to the concurrent work in BAM (Lohet al., 2023). The reason is that authors in (Loh et al., 2023) only consider FixMatch in their experiments,despite BAM being a work published in 2023. Thus, the code4 is all constrained to the use of FixMatch andCIFAR-100, making it difficult to evaluate with other SSL approaches and datasets. In fact, we attempted tointegrate FreeMatch on the BAM framework, but the results obtained were suboptimal. We believe that, thesensitivity to hyperparameters (e.g., just the choice of the backbone for the Bayesian layer in BAM requiresdifferent learning rates) could be the main cause for the low performance of BAM in SSL approaches otherthan FixMatch. Therefore, in order to avoid an unfair comparison (due to the suboptimal performance ofBAM in other SSL methods) we decided to compare to it based only on FixMatch, following their originalwork.",
  "ENumerical values for results in radar plots": "For the radar plots shown in the main paper, we present here () the individual quantitative results,for a more detailed comparison across methods. As a reminder, FreeMatch (Wang et al., 2023) is used as abaseline strategy, and the different calibration strategies (i.e., Focal Loss (Lin et al., 2017), Label Smoothing(Szegedy et al., 2016), and ours) are applied over the DU subset.",
  "j=1 explj / ) can indeed modify the shape of the softmax distribution,": "leading to smoother distributions when > 1. Nevertheless, compared to the proposed solution, scaling thesoftmax values during training (before using the softmax probabilities into the cross-entropy loss) presentsseveral drawbacks. 1) No mechanism to control the importance of softened distributions: System-atically scaling the softmax predictions might be suboptimal in several cases: e.g., reduction of the gradientmagnitude, loss of sparsity in the predictions, impaired learning of hard examples, or even magnifying themiscalibration issue, as strong softmax smoothing can lead to meaningless uniform distributions. Whileapplying TS on the softmax values will always introduce a risk of these drawbacks to appear, the use ofthe proposed constrained term acts as a regularizer of the main objective, whose importance can be con-trolled by a blending hyperparameter , which introduces more flexibility during training. 2) Difficultyon finding the optimal scaling factor. Softmax distributions at the beginning of the training (mostlyuniform) are very different from those observed typically when the network converges (peaky distributionswith one category dominating the softmax vector). Thus, the scaling factor to control the logit distanceshould also evolve during training (e.g., at the beginning, one might not want to scale the softmax at all,as this could magnify all the problems presented in previous point). For instance, a scaling value of 2 doesnot result to the same logit distances (or their equivalent softmax predictions) on an almost uniform thanon a peaky distribution. On the other hand, the proposed penalty acts on the absolute logit values, enforc-ing the same constraint across the training (i.e., just penalize those predictions whose logit distances arelonger than the given margin), which is easier to adjust. 3) Different behaviour. Based on the previouspoint, at the beginning of the training, when the logit distribution is more uniform, the penalty term willnot apply to most predictions, allowing the cross-entropy, pseudo cross-entropy and min-entropy terms toguide the training. However, it will start to increasingly act as regularizer as logit distributions becomesharper, controlling the logits magnitudes from becoming larger and larger, but allowing their distances tobe large enough to have discriminative power. On the other hand, scaling the softmax from the beginningwill reduce the softmax values of all the samples systematically, which may impede the network to properlylearn. Thus, we can argue that the proposed constrained term offers more control over the logit/softmaxdistributions than simply scaling the softmax predictions, more flexibility as it offers a mechanism to controlthe importance of the constraint, and a different behaviour, particularly at the beginning of the training.",
  "GEvaluating our method in a classic setting": "As stated in the main paper, we advocate for the use of Visual transformers over standard convolutionalneural networks due to their multiple advantages. Nevertheless, to show that our approach is also applicableto convolutional deep models, we evaluate our method against the baseline FreeMatch as well as UPS (Rizveet al., 2021) with WideResNet-28 as the backbone architecture. The results obtained from comparing ourmethod are reported in the below. While UPS was initially evaluated on higher amounts of labeleddata, semi-supervised learning has evolved to use much lesser data in recent settings, which echoes morerealistic scenarios.Thus, we have evaluated UPS under these newer settings, alongside state-of-the-artmethods. In particular, for a scenario where 4000 labeled samples are available, and UPS achieves an errorrate of 40.77 (See in (Rizve et al., 2021)). Thus, it is reasonable to think that its performance isdegraded as the amount of labeled samples decreases. Having validated the performance of UPS, we caneasily observe that our method, as well as the baseline FreeMatch, significantly outperform UPS in terms ofboth error rate, and Expected Calibration Error (ECE).",
  "We include additional comparisons between our proposed method and the original SSL approaches": "Convergence analysis. First, as depicted in , we assess the impact of adding our approach to FreeMatchin terms of convergence. Notably, our method, denoted as FreeMatch (Ours), demonstrates superior per-formance in terms of accuracy, outperforming the baseline FreeMatch method within the initial 50, 000iterations. In contrast, the original method reaches convergence around 170, 000 iterations, making it con-siderably slower if one looks at discriminative performance. Additionally, our approach consistently achieveslower values of Expected Calibration Error (ECE) throughout the iterative process, highlighting its potentialto enhance the calibration performance. This contrasts with the original FreeMatch, whose predictions seemto be better calibrated at the first iteration, and these are degraded (i.e., ECE increases) with the training.Thus, adding our method exhibits accelerated convergence rates compared to the original approach, reachingcompetitive performance levels in a fraction of the time.",
  ": Comparison of convergence of Accuracy (left) and ECE (right) over iterations for STL10 datasetwith 40 labeled samples": "Logits distribution. We now provide additional insights into the effects of our approach across the logitdistribution. More concretely, in fig. 9 and fig. 10 we present the kernel density estimation of the logitsdistribution for each class on the STL10 dataset for FreeMatch and FreeMatch plus our method (denoted asOurs). This analysis expands on Observation 3 from the main paper, highlighting the impact of SSL on thecalibration of the model across a diverse range of classes. Specifically, while the original FreeMatch provideslarger logit ranges, as well as larger logit values even for incorrect classes, adding our approach limits theselogit increases across all the classes. As the logit range decreases, the resulting softmax probabilities for thepredicted classes will be smaller, alleviating the problem of overconfident predictions. This is particularlyimportant for wrong predictions, as we expect a well-calibrated model to not be overconfident when incorrectclasses are predicted. Reliability plots. Last, we depict several reliability plots for both the semi-supervised learning (SSL) methodsthat were discussed in the main paper and our modified version, to highlight the calibration improvementsbrought by our approach (fig. 11 and fig. 12)."
}