{
  "Abstract": "Multi-modal large language models (MLLMs) are trained based on large language models(LLM), with an enhanced capability to comprehend multi-modal inputs and generate textualresponses. While they excel in multi-modal tasks, the conventional view within the machinelearning community has often undervalued/overlooked their capabilities in pure naturallanguage processing. This paper aims to get out of the box and showcase an intriguingcharacteristic of multi-modal trained LLMs our preliminary results suggest that visualinstruction tuning, a prevailing strategy to integrate vision knowledge into the LLMs,unexpectedly and interestingly helps models attain both improved truthfulness and ethicalalignment in the pure NLP context. For example, a visual-instruction-tuned LLaMA2 7Bmodel surpasses the performance of the LLaMA2-chat 7B model, fine-tuned with over onemillion human annotations, on TruthfulQA and Ethics benchmarks. Similarly, the latestLLaMA3 series also shows consistent performance gains by 0.6% on average following visual-instruction tuning. Another example is that two versions of proprietary model GPT-4V-turbo,which incorporates visual information, surpasses its LLM-only counterpart GPT-4-turbo byaround 1.6% on both aspects. Further analysis reveals that the improved alignment can beattributed to the superior instruction quality inherent to visual-text data. By presentingthose findings, we advocate for a broader exploration into visual-text synergies, positingthat such multi-modal interactions could be pivotal in advancing alignment research. Inreleasing our code at we aspire tofoster further exploration into the intrinsic value of visual-text synergies and, in a broaderscope, multi-modal interactions in alignment research.",
  "Introduction": "Enhancing truthfulness and reducing hallucinations of Large Language Models (LLMs) is one the paramountchallenges in the domain of artificial intelligence. This paper introduces a new perspective on this researchtopic, advocating for the integration of multi-modal data into LLM training as a strategy to significantlyimprove their truthfulness and alignment with human values. Our stance is informed by empirical evidence demonstrating the beneficial impact of diverse data sources onLLM capabilities. For example, the inclusion of code data has been shown to improve the reasoning ability ofLLMs (Ma et al., 2024). Building upon this premise, this paper aims to explore the potential benefits of aneven more diverse data source multi-modal data, particularly images, in enhancing the capabilities of LLMs. Most modern MLLMs leverage LLMs as their core, setting the aim to bridge the gap between language andvisual tokens (Liu et al., 2023b; Li et al., 2023a; Ye et al., 2023). While language tokens often capture muchof the real-world context, visual information is essential to share richer real-world details that connect to thefactual knowledge in human experience (Harnad, 1990; Bisk et al., 2020; Tu et al., 2023b), particularly in",
  "Multi-Modal Tuning": ": Visual instruction tuning substantially improves the truthfulness and ethics of LLMs. We observetuning LLMs with only 80k multi-modal data can yield stronger results on truthfulness and ethics than thosewith over one million human-annotated RLHF data. Note that these LLMs employ images only during thevisual instruction tuning and are tested without images for NLP tasks. areas related to truthfulness and ethics. Consequently, guiding LLMs to integrate and process visual tokensenhances the models performance in dimensions such as ethics and truthfulness. Our claim is firmly groundedin our experimental evidence. In our preliminary explorations, we tune LLaMA series models (Touvron et al.,2023a;b) with the visual instruction data from LLaVA (Liu et al., 2023b;a). The results of these experimentsare intriguing: for a vanilla LLaMA2 7B model, visual instruction tuning can register impressive scores of46.0% on TruthfulQA-mc (+7.1%) (Lin et al., 2022) and 65.4% on Ethics (+19.6%) (Hendrycks et al.,2020), depending on the specific tuning approach. It is particularly noteworthy that, even without engineeringefforts that explicitly elicit ethical or truthful behaviors, the performance of the visual instruction-tunedmodel already outperforms that of the LLaMA2-chat 7B variant, which is heavily tuned with over a millionhuman annotations (Touvron et al., 2023b). We hypothesize that, in real-world scenarios, visual signals can enhance language models (LMs) in three keyways: by providing enhanced contextual grounding through explicit clues (e.g., settings, object relationships,and implied actions), integrating implicit knowledge that, while not explicitly stated in the text, is crucial forethical decision-making, and mitigating bias by incorporating diverse, real-world depictions that reflect abroader range of human experiences and cultural practices. In proposing this novel perspective, we aim to spur a possible paradigm upgrade or even a complete shiftto the ongoing dialogue within the machine learning community. We contend that broadening the datadiversity for LLMs, beyond traditional text-based inputs, is a pivotal step towards developing models thatmore accurately reflect, interpret, and respond to the complexities of real-world information (Ma et al., 2023).This paper seeks to engage the community in a discussion about this evolving approach, underlining itspotential impact on the ethical and responsible aspects of AI development. In summary, our insights accentuate the promise of visual instruction tuning in fostering the ethical andtruthful alignment of LLMs. It is our hope that this paper will serve as a catalyst for a new wave of research,one that embraces the rich possibilities offered by multi-modal data and paves the way for more aligned andresponsible AI systems.",
  "visual information. We strictly adhere to the setups in LLaVA (Liu et al., 2023b) for fine-tuning LLMs onvisual instruction tuning data": "Model Architecture. We incorporate the pre-trained visual branch of CLIP ViT-L/14 (Radford et al.,2021) as our vision encoder. Additionally, a trainable linear layer is employed to project visual tokens intothe language embedding space. Regarding the choice of LLM, we take the widely recognized open-sourcedLLaMA models (Touvron et al., 2023a;b; Geng & Liu, 2023) for this study. Specifically, our investigationfocuses on the following six models, containing three latest LLMs and their corresponding instruction-tunedvariants:",
  "Pre-trained LLM: OpenLLaMA-3B (Geng & Liu, 2023), LLaMA-7B (Touvron et al., 2023a), LLaMA2-7B (Touvron et al., 2023b)": "Instruction-tuned LLM: OpenAlpaca-3B (Su et al., 2023), LLaMA2-chat-7B (Touvron et al., 2023b),the Vicuna family (Vicuna-7B, Vicuna-v1.5-7B, Vicuna-v1.5-13B) (Zheng et al., 2023), the LLaMA-3-Instruct family (LLaMA3-8B, LLaMA-3.1-8B, LLaMA-3.2-11B) (Dubey et al., 2024), the OpenAIsGPT-4 family (GPT-4-turbo, GPT-4V-turbo) (Achiam et al., 2023a;b). As listed above, our study is centered on two model scales: 3B and 7B. While the 3B LLaMA model is sourcedfrom the OpenLM project (Geng & Liu, 2023), the 7B LLaMA models are directly released by Meta (Touvronet al., 2023a;b); additionally, our investigation extends to the instruction-tuned variants of these base LLMs.Concretely, OpenAlpaca-3B is fine-tuned on the Alpaca data (Taori et al., 2023) using OpenLLaMA-3B as itsbackbone; Vicuna-7B is the v1.1 model from FastChat (Zheng et al., 2023), which is crafted upon LLaMA-7Band employs 125K conversational data from ShareGPT (ShareGPT) during tuning; LLaMA2-chat-7B iswell-engineered for human alignment, undergoing its training on publicly available instruction datasets andone million human-annotated examples using RLHF techniques. The LLaMA3 series is the most recentlyreleased open-weight model family with powerful NLP abilities. For LLaMA3.2-11B, we only take the LLMpart in the model for experiments. Note that we test 7B/8B LLM variants by default, and indicate 3B modelsby the suffix -3B. Training Procedure. The MLLM training unfolds in two stages. First, we exclusively tune the weight ofthe vision-language connector, with both the visual encoder and the LLM remaining frozen. In the secondphase, we fine-tune the weights of both the connector and the LLM. Data-wise, we adhere to the protocols setby LLaVA (Liu et al., 2023b): the connector is initially trained using 595k image-text pairings filtered fromCC3M (Changpinyo et al., 2021); the subsequent stage that requires LLM training utilizes 158k instructions-following data from LLaVA with 80k unique images, which contains image-grounded conversation, imagedescriptions, and image-based complex reasoning tasks. To investigate the factors driving the improvementsin visual instruction tuning, we also explore tuning the model using only text-based instruction data. Weutilize three types of text-only data (sampled to equal sizes): visual instruction tuning data without images,Alpaca data (Taori et al., 2023), and Orca data (Lian et al., 2023). The Alpaca dataset is derived fromprompting OpenAIs GPT model with a variety of real-world questions and scenarios, while the Orca datasetcomprises FLAN-augmented examples (Longpre et al., 2023), which have been shown to empower open-source13B LLMs to excel across multiple benchmarks. (Lian et al., 2023). As for the training strategy, we probe theeffects of both full fine-tuning and LoRA fine-tuning (Hu et al., 2021). Evaluation Protocols. We conduct our evaluation using a publicly available and widely used pipeline (Gaoet al., 2021).Specifically, for the Ethics benchmark, we use accuracy as the evaluation metric.ForTruthfulQA, we follow the official repository and use Rouge and/or BLEU accuracy for generation tasks,along with single-true (mc1) and multi-true (mc2) metrics for question-answering. Other NLP tasks areevaluated with accuracy or F1 score, following the original work. For multi-modal tasks, we use accuracy forquestion-answering benchmarks and CIDEr (Vedantam et al., 2015) for generation tasks. Further details onmulti-modal benchmark metrics will be presented in .4.",
  "LLaMA3.166.9%60.6%36.7%54.0%LLaMA3.2*68.5% (+1.6%)63.0% (+2.4%)37.7% (+1.0%)54.9% (+0.9%)": ": Comparison on the original LLMs and the multi-modal fine-tuned ones on Ethics (Hendrycks et al.,2020) and TruthfulQA (Lin et al., 2022). -ft represents full parameter fine-tuning and -lora indicates LoRAtuning. We report Rouge-L accuracy for TruthfulQA-gen and accuracy for the rest. Note that LLaMA3.2*denotes a model with visual capabilities built on LLaMA3.1. Note that we take the Delphi (Jiang et al., 2021)and the Trustworthy-Alignment (TA) (Zhang et al., 2024) to provide baseline numbers for these two tasks.",
  "Truthfulness and Ethics of MLLMs": "We report the evaluation results on the TruthfulQA and Ethics benchmarks, designed for measuring LLMstruthfulness and ethical alignment. During this evaluation, we utilize the weights exclusively from thevisual-instruction-tuned LLMs, intentionally omitting the visual encoders and vision-language connectorsintroduced during the fine-tuning process. The results are presented in table 1. Visual Instruction Tuning Improves Truthfulness and Ethics. Our observations suggest that, ratherunexpectedly, visual instruction tuning tends to enhance the truthfulness of LLMs. A compelling observationemerges when comparing between LLaMA2 and LLaMA3 variants: visual-instruction-tuned models, especiallyLLaMA2 with MM-lora, surpass the LLaMA2-chat model in performance metrics on both TruthfulQA-mc1(32.1% vs. 29.5%) and TruthfulQA-mc2 (46.0% vs. 44.6%). Furthermore, as one of the leading open-weight",
  "LLMs, the LLaMA3 series shows noticeable performance gains when visual instruction data is integrated,with average improvements of 0.6% and 0.9% on the Ethics and TruthfulQA-gen, respectively": "From table 1, we also observe visual instruction tuning leads to substantial improvements on the Ethics task.Echoing the trend in the TruthfulQA evaluations, visual-instruction-tuned models, specifically the MM-ftversions of both LLaMA2, consistently outpace their instruction-tuned counterparts, such as LLaMA2-chatand Alpaca-3B. For example, the performance enhancements observed for LLaMA2 on the Ethics taskamounted to increments of 19.6%, outperforming LLaMA2-chat and Alpaca-3B by margins of 6.9% and11.3%. Another straightforward observation is that, models with larger parameter scale generally performbetter in these two aspects (e.g., 13B vs. 7B vs. 3B LLMs), as stronger base LLMs are more capable duringthe multi-modal tuning process. For more recent LLMs like the Vicuna-v1.5 and LLaMA3 family, we also have the observation that thevisual-instruction-tuned MLLMs perform better than its language-only counterparts by 2.2% and 0.8% acrossall Vicuna-v1.5 and LLaMA3 models on Ethics and TruthfulQA, respectively. While finetuning the LLMpart in MLLM with full parameter activation leads to better multi-modal performance, which has also beenvalidated by other works, it generally underperforms LoRA-tuned MLLMs in TruthfulQA task (i.e., 2.2%,2.4%, and 2.6% improvement of LoRA tuning compared with finetuning for Vicuna-1.5-7B, Vicuna-1.5-13B,LLaMA-3.1, respectively). But LoRA-tuned MLLM lags behind finetuned ones on Ethics by an averageof 7.2% across 7 model variants. This observation suggests that the ethics aspect aligns better with themulti-modal objective in visual instruction tuning than the truthfulness aspect as discussed in .1. For recent LLMs like Vicuna-v1.5 and the LLaMA3 family, we observe that visual-instruction-tuned MLLMsoutperform their language-only counterparts, with gains of 2.2% and 0.8% on the Ethics and TruthfulQAbenchmarks, respectively, across all Vicuna-v1.5 and LLaMA3 models. While finetuning the full LLMcomponent in MLLMs enhances multi-modal performance (Liu et al., 2023b; Li et al., 2023a)it generallyunderperforms compared to LoRA-tuned MLLMs on the TruthfulQA task. Specifically, LoRA tuning yieldsimprovements of 2.2%, 2.4%, and 2.6% over finetuning for Vicuna-1.5-7B, Vicuna-1.5-13B, and LLaMA-3.1,respectively. However, LoRA-tuned MLLMs fall behind finetuned ones by an average of 7.2% on the Ethicsbenchmark across seven model variants. This suggests that alignment with the multi-modal objective invisual instruction tuning may be stronger for ethics-related dimensions than for truthfulness, we will diveinto this aspect and discuss more later. It should be noted that the employed visual instruction tuning data (that requires LLM parameter update)is the 158k dataset derived from LLaVA (Liu et al., 2023b), which does not contain special designsfor aligning models to human preferences. Remarkably, despite this, visual instruction tuning is ableto yield empirical advantages that surpass those from RLHF, which heavily utilizes a substantial corpus ofhuman-annotated data dedicated to LLM alignment. This observation strongly attests to the potential thatvisual instruction tuning holds in addressing AI alignment challenges. However, it is not a silver bullet our experiments also show that visual instruction tuning is limited atenhancing the alignment of models previously fine-tuned via instruction tuning (e.g., models like Vicuna,LLaMA2-chat), indicating variability in its efficacy.",
  "Ethics": "TruthfulQA-mc2 MM-LLaMA2-chat-ft LLaMA2-chatFull 80KRandom 20KConversations 20KDetails 20KReasoning 20K : Results of different data components on Ethics and TruthfulQA of visual-instruction-tuned LLMs.We utilize 20K of different forms of data (Conversation, Details, Reasoning), and additionally sample 20Kdata out of the original 80K training instances (Random 20K) for comparison. We also report model performance of two versions of proprietary GPT-4-turbo series on these two NLP tasksin table 2. The GPT-4V model is regarded as an upgrade of GPT-4-turbo, with the visual understandingability. The GPT-4V demonstrates improved performance on Ethics by an average of 0.9%, as well as on theTruthfulQA generation task under BLEU accuracy, further supporting our claim in bringing visual knowledgeto enhance LLMs ethical and truthful awareness. Effects of Modalities in Visual Instruction-Tuning Data on LLM Alignment. Next, we seek tounderstand how different modalities in the visual instruction data contribute to the alignment of LLMs.Specifically, we design a set of ablations where we only utilize the text part of the visual instruction tuningdata to tune the LLMs, and draw a comparison with the models tuned with both the visual inputs and thecorresponding texts. As shown in fig. 2, we observe that models with text-only visual instruction tuning can largely attaincomparable alignment performance with the vanilla visual instruction tuning baseline where both images andtexts are used. While additionally including visual inputs yields seemingly modest alignment improvements,",
  ": Results of three MLLMs on Ethics (left) and TruthfulQA-mc1 (right) during MLLM visualinstruction tuning": "we stress that these gains are consistent across different LLMs, tuning methods, and alignment tasks. Forexample, this can be verified across three model variants, resulting in an average accuracy improvement of2.5% across three sub-tasks presented in fig. 2. This finding supports the idea that visual instruction tuning can improve model performance, even whentrained on GPT-4-generated data. Notably, these two techniquesvisual instruction tuning and GPT-4 datatrainingcan be used simultaneously. Additionally, we conduct ablation experiments with text-only data inthe later section to better demonstrate the efficacy of visual instruction tuning. This observation leads to our hypothesis that there exists a promising avenue in leveraging visual data toconstruct enhanced instruction-tuning datasets. Although textual information plays a significant role inalignment, it is crucial to recognize that this text is inherently grounded in its corresponding real-world visualcontent; therefore, utilizing such paired information is integral to ensuring strong alignment in LLMs. Thesefindings underscore the multifaceted benefits of visual data: it not only enhances alignment quality but alsocontributes significantly to the creation of more accurate instruction-tuning datasets. Types of Visual Instruction Data Matters. We further extend our investigation to understand howvarying types of visual instruction-tuning data affect LLM alignment. Specifically, we utilize data fromLLaVA (Liu et al., 2023b), which categorizes visual instruction tuning data into three groups: Conversation,Details, and Reasoning. Each group comprises 20k data points, sampled from the original training splits.For a fair comparison, we also take a uniform sample of 20k from the full 80k visual instructions to form thebaseline group. We tune LLaMA2 and LLaMA2-chat with each data group (of 20k data points) separately,and report the results in fig. 3. Our analysis reveals that, in general, conversational data has a greater impact on improving LLMs performanceon the Ethics task, resulting in an improvement of 15% on MM-LLaMA2-ft and 3% on MM-LLaMA2-chat-ft. Conversely, reasoning and details data tend to be more effective in improving performance on TruthfulQA,yielding gains of more than 2% and 6% on these two models. This suggests that a targeted approach,leveraging the unique strengths of each data type, can facilitate more nuanced and effective instruction tuningfor LLM alignment. Imperfect Match Between Multi-Modal and NLP Objectives. The incorporation of visual informationhas shown to benefit the ethical and truthful aspects of LLMs. In fig. 4, we present the model performanceon the Ethics and TruthfulQA tasks during the multi-modal LLM finetuning stage. At the initial stage ofvisual instruction tuning, there is a noticeable improvement in most models for both aspects within the first1000 training steps. Specifically, the scores on Ethics task continue to increase as more visual knowledgeis incorporated, indicating a well-aligned training objective between visual instruction training and ethicalawareness. However, the alignment between incorporating visual perception into LLMs and enhancing modeltruthfulness may not be optimal, as the scores for truthfulness degenerate with more training steps considered(e.g., two LLaMA2 models achieve their highest TruthfulQA-mc1 scores at the 1000th training step). Thisfinding is consistent with our previous observation that the models awareness of truthfulness shows lessimprovement compared to its ethical alignment.",
  ": Performances of both the vanilla LLMs and visual-instruction-tuned LLMs on five NLP capabilitiesbenchmarks. Note that LLaMA3.2* denotes a model with visual capabilities built on LLaMA3.1": "By analyzing the trajectory of model performance on these tasks, we observe that the optimization goalsbetween multi-modal ability and the improved truthfulness and ethics are not perfectly aligned. ThoughLLMs trained on full visual tuning steps surpass the vanilla LLMs on ethics and truthfulness, the sametraining budgets designed for multi-modal tasks might not be optimal for models NLP abilities.",
  "Standard NLP Abilities": "Given these LLMs are further fine-tuned with multi-modal data, it might be intuitively expected that theirstandard NLP capabilities could degrade. Such a phenomenon is commonly referred to as catastrophicforgetting (Kirkpatrick et al., 2017) or in the AI alignment community the alignment tax (Christiano,2019; Jensen et al., 2023). Interestingly, contrary to these assumptions, our results presented in table 3 show that MM-lora (markedin the gray background) results in only an average 0.17% performance decrease across five NLP capabilitybenchmarks and four models, after applying visual instruction tuning. More notably, in certain instances,MM-lora even modestly improves performance on these benchmarks. However, the visually-tuned LLaMA3series shows an average score drop of 4.5% across these tasks while maintain high scores on Ethics andTruthfulQA in . Notably, LLaMA3.2 a visually-enhanced LLM tuned by Meta (Dubey et al., 2024) demonstrates highly consistent performance with our trained models on Ethics, TruthfulQA, and otherNLP benchmarks, further supporting our claim that visual instruction tuning enhances LLMs in ethics andtruthfulness.",
  ": Results of LLaMA2 finetuned on Alpaca data, text-only visual instruction data (text-VI), visualinstruction tuning data (VI), and Orca data": "Additionally, MLLMs with a more advanced or larger LLM component tend to perform better on NLPbenchmarks. For instance, the multi-modal-tuned Vicuna-v1.5-13B outperforms its 7B variant and the v1.3counterpart by an average of 5.3% and 15.8% across five NLP tasks, respectively. In conjunction with the insights from .1, these observations altogether highlight the ability ofvisual-instruction-tuned LLMs in both maintaining the strong capability on standard NLP benchmarks andaligning better with human values, not to mention the additional capability of recognizing visual inputs. Suchfindings pave new avenues for both academic exploration and practical implementations within multi-modaldomains. We believe these insights should catalyze further investigations into the tuning of LLMs withmulti-modal interactions.",
  "Tuning on Different Vision-Language Data": "To better explain the benefit of visual text tuning in the process, we present the results of LLaMA2 finetunedon Alpaca data (Taori et al., 2023), text-only visual instruction data (text-VI), visual instruction data (VI),and Orca data (Lian et al., 2023) in . To keep the fair comparison, we randomly sample 80K datafrom Alpaca and Orca data respectively for the training. We can observe that the visual instruction tuning 1) surpasses Alpaca and text-VI data tuned models inmost cases, i.e., average 6.7% and 2.8% improvements over Alpaca on two benchmarks, and 0.2%, -1.5%over text-VI; 2) but lags behind the Orca-tuned model by 1.1% and 16.3% on Ethics and TruthfulQAbenchmarks. Considering that the Orca dataset includes Chain-of-Thought (CoT) and complex, nuancedinstruction-following data from a diverse array of tasks within the FLAN collection (Longpre et al., 2023),the observed performance gain is reasonable. The CoT reasoning and complex instruction-following examplesin Orca offer richer contextual understanding and problem-solving patterns. This observation indicates theinsight that upgrading text quality within the visual instruction data could further enhance the modelsethical and truthful reasoning, opening a promising avenue for refining these abilities.",
  "Analysis on Multi-Modal Benchmarks": "We hereby test the visual-instruction tuned models on recent multi-modal benchmarks, where five tasks aredeployed: Unicorn benchmark (Tu et al., 2023a) dedicates evaluating the MLLM ability in safety scenarios,we take two OODCV-VQA tasks and Sketchy-VQA tasks for testing whether models can well handle OODvisual/text input and sketch images, respectively. MME (Fu et al., 2023) consists of two evaluation aspects, i.e.,cognition (CS) and perception (PS) with total 14 VQA tasks;1 MSCOCO (Lin et al., 2014) and Flickr30k (Younget al., 2014) captioning tasks are commonly used benchmarks in the field of image caption generation. Wereport the zero-shot CIDEr (Vedantam et al., 2015) scores (with three text-only QA examples) on the test setfrom the Karpathy split (Karpathy & Fei-Fei, 2015). POPE (Li et al., 2023c) is used to evaluate the level ofobject hallucinations in MLLMs, which consists of three versions of balanced yes/no VQA tasks consideringobjects in the given image. It is built upon MSCOCO-2017 dataset (Lin et al., 2014). Additionally, We alsomake use of the image corruptions proposed in ImageNet-C (Hendrycks & Dietterich, 2019) to measure theperformance of the MLLMs on corrupted images for MSCOCO task (denoted as MSCOCO-C).2. 1We exclude landmark and artwork tasks to accelerate the evaluation process.2For corrupted images, we report the average results of tested models on four noises (gaussian noise, defocus blur, contrast,brightness) across three severity levels (1, 3, 5)",
  "ModelsUnicornoodcv / sketch MMECSMMEPSCOCO Flickr30kPOPER / A / P": "MM-LLaMA-ft45.2 / 80.9199.3510.559.227.165.7 / 57.8 / 59.9MM-Vicuna-ft58.4 / 82.7270.7625.257.524.676.5 / 66.5 / 73.8MM-LLaMA2-ft55.0 / 83.6237.1661.365.131.665.0 / 55.4 / 56.3MM-LLaMA2-lora52.3 / 79.6200.0395.052.026.250.8 / 50.4 / 50.6MM-LLaMA2-chat-ft54.5 / 80.9234.6805.457.426.769.8 / 57.9 / 60.3MM-LLaMA2-chat-lora53.1 / 82.8228.6709.843.423.065.9 / 56.8 / 59.2MM-Vicuna-v1.5-7B58.4 / 80.4320.4 1182.0 98.562.889.3 / 79.7/ 85.5MM-Vicuna-v1.5-13B59.7 / 87.8287.9 1213.3 84.051.387.4 / 78.7 / 84.1 : Performances of our MLLM family on five widely employed multi-modal benchmarks. We test modelson oodcv and sketch sub-tasks in the Unicorn benchmark (Tu et al., 2023a) and Random (R), Adversarial(A), and Popular (P) in POPE (Li et al., 2023c). Enhanced MLLMs Expand NLP Capabilities. Is a better visual reasoner also a better NLP task solver?In fig. 5, we illustrate the correlation between model performance in multi-modal and NLP tasks. Theresults reveal positive correlations across tasks from different domains, suggesting that the visual reasoningabilities of the eight models analyzed contribute to their improved performance on NLP benchmarks. Notably,the average coefficient of determination score across these eight scenarios is 0.482, indicating a moderatecorrelation. In specific cases, such as pairing MME (PS) with Ethics and MMLU, both of the coefficient scoresexceed 0.72, indicating a very strong correlation between these tasks. This evident correlation explains ourfinding of stronger MLLMs can lead to expanded NLP capacities. But unlike the misaligned objectivesobserved between MLLM multi-modal and NLP abilities during training in Sec. 3.1, this finding might not besurprising, as MLLMs rely heavily on language to reasoning and expression. It is plausible that an improvedLLM (i.e. better training data, larger model scale) could enhance the expressive abilities of an MLLM,resulting in a close correlation between abilities across different modalities. Aligned LLMs vs. Unaligned LLMs on Multi-Modal Benchmarks. In table 5, MLLMs incorporatingaligned LLMs (e.g., Vicuna, LLaMA2-chat) have demonstrated top performance in comprehensive andchallenging tasks such as Unicorn, MME, and POPE. Specifically, MM-Vicuna-ft and MM-LLaMA-chat-ftoutperform their corresponding vanilla MLLM counterparts by an average of 164.9 on MME and 7.5% onPOPE. Despite the incorporation of text-aligned LLMs, MLLMs still exhibit unexpected shortcomings whencompared to models that use vanilla LLMs, particularly when evaluated on three traditional vision-texttasks. For instance, MLLMs show an average drop of 4.2 CIDEr across two captioning tasks. This couldbe attributed to the nature of the captioning task itself, which is more generation-focused and less aboutfollowing instructions as seen in more challenging QA tasks. As a result, MLLMs that use unaligned LLMsmay perform better on captioning tasks, as they are less constrained by alignment requirements and arebetter suited for the generation-centric nature of captioning.",
  ": Performances of the MLLM family on MSCOCO (Lin et al., 2014) with corrupted visual inputs": "Need for Studying Multi-Modal Alignments. Although text-aligned models like Vicuna and LLaMA2-chat have proven effective, their MLLM variants perform poorly on corrupted images, as shown in table 6.Not only do these models underperform compared to MLLMs that do not use instruction-tuned LLMs, butthey also exhibit a performance drop of over 17% when evaluated on corrupted images, compared to cleanones. This decline is even greater than the drops observed in MM-LLaMA-ft and MM-LLaMA2-ft. Thissuggests that while visual instruction tuning enhances the truthfulness and ethical behavior of LLMs in thelanguage domain, these MLLMs still face their unique challenges in the multi-modal context, especially whendealing with corrupted or imperfect visual inputs (Tu et al., 2023a; Dong et al., 2023).",
  "Related Work": "Alignments. The alignment of AI systems to human values is an important topic for todays advanced AIsystems, from testing model robustness to out-of-distribution shifts (Hendrycks & Dietterich, 2019; Hendryckset al., 2021a; Zhao et al., 2022) to adversarial attacks (Hendrycks et al., 2021b; Eykholt et al., 2018; Xieet al., 2020), many works have been proposed. The recent development of LLMs has revolutionized naturallanguage processing and has been widely adopted in various applications. Thus, concerns regarding thehonesty and truthfulness of these models have also emerged, prompting alignment researchers to investigatethe ethical implications and potential risks associated with their deployment. TruthfulQA (Lin et al., 2022)is proposed to measure how LLMs imitate human misconceptions. And Ethics (Hendrycks et al., 2020) isused to assess a language models knowledge of basic concepts of morality. Advanced techniques for aligning language models with human preference are also popular these days, fromRLHF (Ouyang et al., 2022) to DPO (Rafailov et al., 2023) optimization, the alignment training paradigmshave shifted fast recently. Reinforcement learning as one of the most popular solutions to enhance the truthfuland ethical awareness of LLMs have long been discussed. Recent works explored various approaches for suchpurpose, including progressive rewarding (Gao et al., 2024), optimized RL reward function (Bai et al., 2022),and applications as well as benchmarking (Zhang et al., 2024; Li et al., 2024). The concept of LLM alignmenthas also gradually switched from human-supervised (Ouyang et al., 2022) to the paradigm of incorporatingother AI model supervisions (Lee et al., 2023), and to most recently the employment of weak signals (Burnset al., 2023). Given the popularity of the use of large language models, adversarial attacks on LLMs have alsobeen explored (Zou et al., 2023). In this work, we present our findings on how visual instruction tuning canhelp the LLMs align with human values, our results show impressive performance boost on these datasetswithout explicit prompting such behaviors. Multi-Modal and Large Language Models. In light of the rapid evolvement of large language models(LLMs), recent studies about multi-modal systems have turned their focuses from incorporating fine-grainedmulti-modal data (Liang et al., 2021; Tu et al., 2023b) to integrating powerful LLMs with few-shot capability.More recently, some instruction-tuned MLLMs have emerged, showing excellent generalization ability inunseen VL tasks (Zhu et al., 2023; Liu et al., 2023b; Ye et al., 2023; Li et al., 2023a; Dai et al., 2023). Forexample, MiniGPT4 (Zhu et al., 2023) is built upon QFormer (Li et al., 2023a) and Vicuna (Zheng et al.,2023) and only activates the linear layer connecting the vision encoder and LLM. LLaVA (Liu et al., 2023b;a)projects the output of a vision encoder to word tokens and trains both the VL connector and the LLM onsynthetic data. mPLUG-owl (Ye et al., 2023) tunes LLaMA with a query-based VL connector using bothtext-only and vision-language instruction data. InstructBLIP (Dai et al., 2023) uses BLIP2 (Li et al., 2023a)",
  "Published in Transactions on Machine Learning Research (12/2024)": "Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica.Judgingllm-as-a-judge with mt-bench and chatbot arena, 2023. 3, 11 Yiyang Zhou, Chenhang Cui, Jaehong Yoon, Linjun Zhang, Zhun Deng, Chelsea Finn, Mohit Bansal, andHuaxiu Yao. Analyzing and mitigating object hallucination in large vision-language models. In NeurIPS2023 Workshop on Instruction Tuning and Instruction Following, 2023. 12",
  "Discussion, Conclusion, and Future Work": "More Aligned Objectives between Multi-Modal and NLP Abilities. Our exploration shows thattraining on multi-modal instruction tuning data can also benefit the LLMs factual accuracy and ethics.In fig. 4, we have shown that these alignment-focused metrics improve while training proceeds on multi-modal.However, current multi-modal data is not designed for alignment, the main focus is still eliciting languagemodels with multi-modal perception. Our results demonstrate a promising new avenue for developing modelsto understand and interact with the world more truthfully, and also suggest the need for exploration toidentify appropriate tasks that can effectively improve these two aspects simultaneously. And we hope ourpaper could inspire discussions on this direction. Exploring the Training Framework. In our pilot study, we have shown the results of using image-basedinstruction fine-tuning data to support our findings that leveraging multi-modal interactions could yield morealigned models. Based on our results, it is reasonable to assume that introducing multi-modal data to thepre-training stage could also yield more aligned models. For example, the Gemini family models could be aninteresting case for study (Gemini Team, 2023). Understanding how to instruction fine-tune the base modelfor multi-modal ability and alignment is another direction worthy of exploration. Our study explores fullparameter fine-tuning as well as LoRA parameter efficient fine-tuning. It can be beneficial to study howvaries types of parameter-efficient fine-tuning techniques helps (Kopiczko et al., 2024; Zhao et al., 2024).Besides the training techniques, the training data can also be explored, how should we create a mixture ofdata for fine-tuning, how to determine the ratio of multi-modal data to text-only data (Ye et al., 2023; Liuet al., 2023a), and how to extend to other modalities other than images. These exploration could help gives amore practical and comprehensive guide. Conclusion. In this study, we offer preliminary findings that underscore the potential of enhancing thetruthfulness and ethical alignment of LLMs through visual instruction tuning. Remarkably, even withoutprompts tailored for truthfulness or ethical behaviors, our approach to tuning LLM weights using visualinstruction datasets yielded significant improvements in both the TruthfulQA and Ethics benchmarks.Notably, such improvements are even stronger than that of RLHF, which tunes LLMs with a huge corpus ofhuman-aligned data points. The follow-up analysis demonstrates the importance of instruction data qualityfor improving aligned values in MLLMs, as well as specific types of data models employed for applying todifferent alignment tasks. Future Work. In light of our findings, we advocate for future research endeavors to focus on devisinginnovative methodologies for crafting visual instruction data that can more effectively align LLMs. Exploringnovel MLLM architectures could also be a fruitful avenue. We hope fostering LLM interactions with real-worldenvironments may emerge as a pivotal strategy for achieving superior model alignment.",
  "Paul Christiano. Current work in ai alignment. Effective Altruism, 2019. 8": "Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, BoyangLi, Pascale Fung, and Steven Hoi. Instructblip: Towards general-purpose vision-language models withinstruction tuning. ArXiv, abs/2305.06500, 2023. URL 11 Yinpeng Dong, Huanran Chen, Jiawei Chen, Zhengwei Fang, Xiao Yang, Yichi Zhang, Yu Tian, Hang Su,and Jun Zhu. How robust is googles bard to adversarial image attacks? arXiv preprint arXiv:2309.11751,2023. 11 Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman,Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprintarXiv:2407.21783, 2024. 3, 8 Kevin Eykholt, Ivan Evtimov, Earlence Fernandes, Bo Li, Amir Rahmati, Chaowei Xiao, Atul Prakash,Tadayoshi Kohno, and Dawn Song. Robust physical-world attacks on deep learning visual classification. InProceedings of the IEEE conference on computer vision and pattern recognition, pp. 16251634, 2018. 11 Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Zhenyu Qiu, Wei Lin, JinruiYang, Xiawu Zheng, et al. Mme: A comprehensive evaluation benchmark for multimodal large languagemodels. arXiv preprint arXiv:2306.13394, 2023. 9 Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding,Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, Jason Phang, Laria Reynolds, Eric Tang, Anish Thite,Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot language model evaluation, September2021. URL 3",
  "Dawid Jan Kopiczko, Tijmen Blankevoort, and Yuki Markus Asano. Vera: Vector-based random matrixadaptation. In ICLR, 2024. 12": "Harrison Lee, Samrat Phatale, Hassan Mansoor, Kellie Lu, Thomas Mesnard, Colton Bishop, Victor Carbune,and Abhinav Rastogi. Rlaif: Scaling reinforcement learning from human feedback with ai feedback. arXivpreprint arXiv:2309.00267, 2023. 11 Tony Lee, Haoqin Tu, Chi Heem Wong, Wenhao Zheng, Yiyang Zhou, Yifan Mai, Josselin Somerville Roberts,Michihiro Yasunaga, Huaxiu Yao, Cihang Xie, et al. Vhelm: A holistic evaluation of vision language models.arXiv preprint arXiv:2410.07112, 2024. 12",
  "Wing Lian, Bleys Goodson, Eugene Pentland, Austin Cook, Chanvichet Vong, and \"Teknium\". Openorca:An open dataset of gpt augmented flan reasoning traces. 2023. 3, 9": "Zujie Liang, Huang Hu, Can Xu, Chongyang Tao, Xiubo Geng, Yining Chen, Fan Liang, and Daxin Jiang.Maria: A visual experience powered conversational agent. In Proceedings of the 59th Annual Meeting ofthe Association for Computational Linguistics and the 11th International Joint Conference on NaturalLanguage Processing (Volume 1: Long Papers), pp. 55965611, 2021. 11 Stephanie Lin, Jacob Hilton, and Owain Evans. Truthfulqa: Measuring how models mimic human falsehoods.In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1:Long Papers), pp. 32143252, 2022. 2, 4, 11 Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollr, andC Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer VisionECCV 2014: 13thEuropean Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pp. 740755.Springer, 2014. 9, 11",
  "Yuxi Ma, Chi Zhang, and Song-Chun Zhu. Brain in a vat: On missing pieces towards artificial generalintelligence in large language models. arXiv preprint arXiv:2307.03762, 2023. 2": "Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang,Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions withhuman feedback. Advances in Neural Information Processing Systems, 35:2773027744, 2022. 11 Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from naturallanguage supervision. In International conference on machine learning, 2021. 3",
  "Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, andTatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. 2023. 3, 9": "Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothe Lacroix,Baptiste Rozire, Naman Goyal, Eric Hambro, Faisal Azhar, Aurlien Rodriguez, Armand Joulin, EdouardGrave, and Guillaume Lample. Llama: Open and efficient foundation language models. arXiv preprintarXiv:2302.13971, 2023a. 2, 3 Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation andfine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023b. 2, 3 Haoqin Tu, Chenhang Cui, Zijun Wang, Yiyang Zhou, Bingchen Zhao, Junlin Han, Wangchunshu Zhou,Huaxiu Yao, and Cihang Xie. How many unicorns are in this image? a safety evaluation benchmark forvision llms. arXiv preprint arXiv:2311.16101, 2023a. 9, 10, 11, 12",
  "Haoqin Tu, Yitong Li, Fei Mi, and Zhongliang Yang. Resee: Responding through seeing fine-grained visualknowledge in open-domain dialogue. arXiv preprint arXiv:2305.13602, 2023b. 1, 11": "Ramakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image descriptionevaluation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 45664575,2015. 3, 9 Cihang Xie, Mingxing Tan, Boqing Gong, Jiang Wang, Alan L Yuille, and Quoc V Le. Adversarial examplesimprove image recognition. In Proceedings of the IEEE/CVF conference on computer vision and patternrecognition, pp. 819828, 2020. 11 Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu,Pengcheng Shi, Yaya Shi, et al.mplug-owl: Modularization empowers large language models withmultimodality. arXiv preprint arXiv:2304.14178, 2023. 1, 11, 12 Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. From image descriptions to visual denotations:New similarity metrics for semantic inference over event descriptions. Transactions of the Association forComputational Linguistics, 2:6778, 2014. 9",
  "Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned audio-visual language model forvideo understanding. arXiv preprint arXiv:2306.02858, 2023a. 12": "Letian Zhang, Xiaotong Zhai, Zhongkai Zhao, Xin Wen, and Bingchen Zhao. What if the tv was off?examining counterfactual reasoning abilities of multi-modal language models.In Proceedings of theIEEE/CVF International Conference on Computer Vision (ICCV) Workshops, 2023b. 12 Pan Zhang, Xiaoyi Dong Bin Wang, Yuhang Cao, Chao Xu, Linke Ouyang, Zhiyuan Zhao, Shuangrui Ding,Songyang Zhang, Haodong Duan, Hang Yan, et al. Internlm-xcomposer: A vision-language large model foradvanced text-image comprehension and composition. arXiv preprint arXiv:2309.15112, 2023c. 12 Zongmeng Zhang, Yufeng Shi, Jinhua Zhu, Wengang Zhou, Xiang Qi, Peng Zhang, and Houqiang Li.Trustworthy alignment of retrieval-augmented large language models via reinforcement learning. arXivpreprint arXiv:2410.16843, 2024. 4, 11 Bingchen Zhao, Shaozuo Yu, Wufei Ma, Mingxin Yu, Shenxiao Mei, Angtian Wang, Ju He, Alan Yuille, andAdam Kortylewski. Ood-cv: a benchmark for robustness to out-of-distribution shifts of individual nuisancesin natural images. In European Conference on Computer Vision, pp. 163180. Springer, 2022. 11"
}