{
  "Abstract": "Nonlinear dimensional reduction with the manifold assumption, often called manifold learn-ing, has proven its usefulness in a wide range of high-dimensional data analysis. The signif-icant impact of t-SNE and UMAP has catalyzed intense research interest, seeking furtherinnovations toward visualizing not only the local but also the global structure informationof the data. Moreover, there have been consistent eorts toward generalizable dimensionalreduction that handles unseen data. In this paper, we rst propose GLoMAP, a novel man-ifold learning method for dimensional reduction and high-dimensional data visualization.GLoMAP preserves locally and globally meaningful distance estimates and displays a pro-gression from global to local formation during the course of optimization. Furthermore,we extend GLoMAP to its inductive version, iGLoMAP, which utilizes a deep neural net-work to map data to its lower-dimensional representation. This allows iGLoMAP to providelower-dimensional embeddings for unseen points without needing to re-train the algorithm.iGLoMAP is also well-suited for mini-batch learning, enabling large-scale, accelerated gra-dient calculations.We have successfully applied both GLoMAP and iGLoMAP to thesimulated and real-data settings, with competitive experiments against the state-of-the-artmethods.",
  "Introduction": "Data visualization, which belongs to exploratory data analysis, has been promoted by John Tukey since the1970s as a critical component in scientic research (Tukey, 1977). However, visualizing high-dimensionaldata is a challenging task. The lack of visibility in the high-dimensional space gives less intuition about whatassumptions would be necessary for compactly presenting the data on the reduced dimension. A commonassumption that can be made without specic knowledge of the high-dimensional data is the manifold as-sumption. It assumes that the data are distributed on a low-dimensional manifold within a high-dimensionalspace. Nonlinear dimensional reduction (DR) with the manifold assumption, often called manifold learning,has proven its usefulness in a wide range of high-dimensional data analysis (Meil & Zhang, 2023). Variousresearch eorts have been made to develop DR tools for data visualization, and several leading algorithmsinclude MDS (Cox & Cox, 2008), Isomap (Tenenbaum et al., 2000), t-SNE (Van der Maaten & Hinton,2008), UMAP (McInnes et al., 2018), PHATE (Moon et al., 2017), PacMAP (Wang et al., 2021), and manyothers. It is not uncommon to perform further statistical analysis on the reduced dimensional data, includingvisualized data, through methods such as regression (Cheng & Wu, 2013), functional data analysis (Dai &Mller, 2018), classication (Belkina et al., 2019), and generative models (Qiu & Wang, 2021).",
  "Published in Transactions on Machine Learning Research (12/2024)": "Figure S36: (a) Parametric UMAP: The majority runs of parametric UMAP resulted such a shape, so wecherry picked the best one for comparison as in . (b-d) Depending on the run or choice of thehyperparameters, GLoMAP and iGLoMAP may result in a twisted visualization.However, we did notobserve any case having two separated partial visualizations.",
  "of transductive learning. By this, we can formulate an eective stochastic gradient descent algorithm forlearning the inductive map": "Our proposed global distance itself, constructed through a new local distance, also can be seen as an advancein the array of global preservation algorithms. MDS keeps the metric information between all the pairs of datapoints, and thereby aims to preserve the global geometry of the dataset. Recognizing that Euclidean distancesbetween non-neighboring points may not always be informative in high-dimensional settings, Isomap appliesMDS instead to geodesic distance estimates computed via the shortest path search. Eorts to rene thisgeodesic distance estimator include improving the Euclidean distances among nearby points to better reectthe local manifold, for instance, through a conformal embedding (Silva & Tenenbaum, 2002), a sphereletsargument (Li & Dunson, 2019), or the tangential Delaunay complex (Arias-Castro & Chau, 2020). On therened local distances, these approaches apply the shortest path search to estimate global distances. As ourlocal distance estimator is in a closed form, the distance computation is a scalable alternative to Li & Dunson(2019) and Arias-Castro & Chau (2020). Similar to our approach, the local distance in Silva & Tenenbaum(2002) is computed by multiplying a closed-form local rescaling constant to the Euclidean distances. Ascompared in Section S9 in Appendix, when our distance is instead used under the framework of Silva &Tenenbaum (2002), the visualization result shows separation between clusters similar to . Now we discuss related works or compare our framework with the previous works.The seminal t-SNEand UMAP preserve the distance among neighboring data points. It is known that these methods maylose global information, leading to a misunderstanding of the global structure (Coenen & Pearce, 2019;Rudin et al., 2022). To overcome the loss of global information, much eort has been made, for example,by good initialization (McInnes et al., 2018; Kobak & Berens, 2019; Kobak & Linderman, 2021) or by anEuclidean distance preservation between selected non-neighboring points (Fu et al., 2019; Wang et al., 2021).Our proposed GLoMAP improves this global structure preservation by estimating the global distances withadopting a UMAP-like loss function. When it comes to the local distances, both UMAP uses rescale factorsthat may enable local adaptability of estimated distances. Our local approximation diers from that ofUMAP as our local distance estimator is in a closed form and has a consistent property. Additionally, UMAPadopts a fuzzy union to allow their inconsistent coexistence in a single representation of the data manifold,while we use the shortest path search seeking a coherent global metric. Many eorts have been made toextend existing visualization methods to utilize DNNs for generalizability or for handling optimization withlower computational cost, especially when the data size is large (Van Der Maaten, 2009; Gisbrecht et al.,2015; Pai et al., 2019; Sainburg et al., 2021). Our sampling schemes and particle-based algorithm may benetthese approaches in stabilizing the optimization.",
  "The main contributions of this paper are as follows:": "(1) We introduce GLoMAP, a novel dimensionality reduction method that captures both global andlocal structures.The key inventions include locally-adaptive global distances and an annealing-like process for scaling. These innovations enable GLoMAP to produce a progression from globalstructure to local details during optimization, a feature not present in previous methods. (2) We introduce iGLoMAP, an inductive version of GLoMAP, which avoids the need to re-run theentire algorithm when a new data point is introduced.Our particle-based algorithm leveragesthe advantages of inductive formulation while maintaining the stability of optimization, similar totransductive learning. (3) We establish the consistency of our new local geodesic distance estimator, which serves as thebuilding block for the global distance used in both the GLoMAP and iGLoMAP algorithms. Theglobal distance is shown to be an extended metric. (4) We demonstrate the usefulness of both GLoMAP and iGLoMAP by applying them to simulated andreal data and conducting comparative experiments with state-of-the-art methods. All implementa-tions are available at",
  "Background and the Related Work": "In manifold learning, the key design components of DR are: (a) What information to preserve; (b) How tocalculate it; (c) How to preserve it. From this perspective, in this section, we review several techniques, suchas Isomap, MDS, t-SNE, UMAP in common notation, and discuss their limitations and our improvements.We also discuss PacMAP and PHATE in Section S2. Let x1, . . . , xn X Rp denote the original dataset. Write X = [x1, . . . , xn] Rnp. Our goal is tond embeddings z1, . . . , zn Rd, where d p and zi is the corresponding representation of xi in the low-dimensional space. Under many circumstances, d = 2 is used for visualization. Write Z = [z1, . . . , zn] Rnd.",
  "Global methods": "MDS is a family of algorithms that solves the problem of recovering the original data from a dissimilaritymatrix. Dissimilarity between a pair of data points can be dened by a metric, by a monotone function onthe metric, or even by a non-metric functions on the pair. For example, in metric MDS, the objective is tominimize stress, which is dened by stress(z1, ..., zn) = i=j(dz(zi, zj)f(dx(xi, xj)))21/2 for a monotonicrescaling function f. Depending on how we see the input space geometry, the choices of dissimilarity measuresfor dx and dz would vary. The common choice of the L2 distance corresponds to an implicit assumptionthat the input data is a linear isometric embedding of a lower-dimensional set into a high-dimensional space(Silva & Tenenbaum, 2002). Isomap can be seen as a special case of metric MDS where the dissimilarity measure is the geodesic distance,i.e., Isomap tries to preserve the geometry of the data manifold by keeping the pairwise geodesic distances.For this, Isomap rst constructs a K-nearest neighbor (KNN) graph where the edges are weighted by theL2 distance among the KNNs. Then it estimates the geodesic distance by applying a shortest path searchalgorithm, such as Dijkstras algorithm (Tenenbaum et al., 2000).The crux of Isomap is the use of ashortest path search to approximate global geodesic distances, with the underlying assumption that themanifold is at and without intrinsic curvature. There have been multiple attempts to improve the geodesicdistance estimator under more relaxed conditions. For instance, Isomap has been extended to conformalIsomap (Silva & Tenenbaum, 2002) to accommodate manifolds with curvature. Both their approach andours construct global distances by nding the shortest path over locally rescaled distances (although we usedierent rescalers). Our work extends this locally adaptive global approach beyond the MDS framework,addressing the crowding problem identied by Van der Maaten & Hinton (2008). Similarly, other eortshave also sought to handle general manifolds by calibrating local distances.For example, Arias-Castro& Chau (2020) employed the tangential Delaunay complex and Li & Dunson (2019) used a sphereletsargument with the decomposition of a local covariance matrix near each data point. In our work, we usea computationally ecient local distance estimator albeit the global distance construction becomes moreheuristic. Nevertheless, we believe that these distances from previous work could also be integrated into ourframework, providing better theoretical understanding of the chosen global distance, pending improvementsin ecient computation.",
  "Local methods": "t-SNE, as introduced by Van der Maaten & Hinton (2008), is one of the most cited works in manifoldlearning for visualization. Instead of equally weighting the dierence between the distances on the input andembedding spaces, t-SNE adaptively penalizes this distance gap. More specically, for each pair (xi, xj),pij denes a relative distance metric in the high-dimensional space, where the relative distance qij in thelow-dimensional space is optimized to match pij according to the Kullback-Leibler (KL) divergence. For any",
  "i=j pij log pij": "qij . The weight matrix ofa graph, such as (pij) or (qij), which encodes proximity, is called an anity matrix. To handle datasets withheterogeneous density, Van Assel et al. (2024) extends t-SNE by maintaining constant entropy for each rowin the anity matrix using an optimal transport framework, dening a doubly stochastic matrix (normalizedby both rows and columns) for both input and visualization space. Another highly cited visualization technique is UMAP (McInnes et al., 2018), which is often considered anew algorithm based on t-SNE (Rudin et al., 2022), known for being faster and more scalable (Becht et al.,2019). UMAP achieves a signicant computational improvement by changing the probability paradigm oft-SNE to Bernoulli probability on every single edge between a pair. This removes the need for normalizationin t-SNE in both distributions {pij} and {qij} that require summations over the entire dataset. Anotherinnovation brought about by UMAP is a theoretical viewpoint on the local geodesic distance as a rescale ofthe existing metric on the ambient space (Lemma 1 in McInnes et al. (2018)). McInnes et al. (2018) furtherdevelops a local geodesic distance estimator near xi dened as",
  "dumapi(xi, xj) = for j Ni or j = i,(xi xj i)/iotherwise,(2)": "where Ni is the index set of the KNN of xi, i is the distance between xi and its nearest neighbor, and i isan estimator of the rescale parameter for each i. In our work, we address the previously unexplained choicesof i and i in (2) by proposing a new local geodesic distance estimator and theoretically establishing itsconsistency. To embrace the incompatibility between local distances, as indicated by dumapi(xi, xj) = dumapj(xi, xj),McInnes et al. (2018) treats the distances as uncertain, so that those incompatible dumapi(xi, xj) anddumapj(xi, xj) may co-exist by a fuzzy union. UMAP denes a weighted graph with the edge weight, orthe membership strength, for each edge (i, j) as ij = pij + pji pijpji, where pij = exp{dumapi(xi, xj)}.Meanwhile, on the low-dimensional embedding space, the edge weight for each edge (i, j) is dened byqij =1+azi zj2b1, where a and b are hyperparameters. Then, UMAP tries to match the membershipstrength of the representation graph of the embedding space to that of the input space by using a fuzzy setcross entropy, which can be seen as a sum of KL divergences between two Bernoulli distributions such as",
  "(1 qij).(3)": "In UMAP, the global structure is preserved through a good initialization, e.g., through the Laplacian Eigen-maps initialization (McInnes et al., 2018). In our work, we seek a visualization method that does not dependon initialization for global preservation. Another dierence of our work is the way to merge the incompatiblelocal distances. We alter the fuzzy union of UMAP by rst taking the maximum among nite local distancesand then merging the local information by the shortest path search, thereby obtaining global distances.",
  "Methodology": "In this section, we rst present a new framework, called GLoMAP, which is a transductive algorithm fornonlinear manifold learning. GLoMAP consists of three primary phases: (1) global metric computation; (2)representation graph construction by information reduction; (3) optimization of the low-dimensional embed-ding by a stochastic gradient descent algorithm. During the optimization process, the data representation",
  "Global metric computation": "Recall that {xi}ni=1 denotes the original data in Rp and {zi}ni=1 are the corresponding embeddings in Rd.Our goal is to construct two graphs GX and GZ that, respectively, represent the geometry on the inputdata manifold and the embedding space. We then formulate an objective function as a dissimilarity measurebetween the graphs GX and GZ to project the input space geometry onto the embedding space.Thekey information to construct the representation graph GX of the input space is the global distance matrixbetween all pairs of data points.The global distance will be determined based on the local distances,where the locality is dened by the K-nearest neighbor Ki = {X(k)(xi)}Kk=1 for each xi. When we obtainthe estimate of the local geodesic distance by a rescaled L2 distance, two neighboring points can have twopossible rescalers because each of the two points provides its own local view. Therefore, we handle thisambiguity by considering the minimum of the two rescalers, which corresponds to taking the maximum ofthe two distances. Consequently, the local distance estimate between xi and xj is given by",
  "k=1x X(k)(x)2.(5)": "The theoretical rationale for the choice of the local normalizing constant 2x will be provided in under the local Euclidean assumption. Using dloc as the local building block, we can construct a globaldistance matrix. We construct a weighted graph, denoted by Gloc, in which xi and xj are connected by anedge with the weight dloc(xi, xj) if dloc(xi, xj) is nite. Given the weighted graph (Gloc, dloc), we apply ashortest path search algorithm, e.g., Dijkstras algorithm, to construct the global distance between any twodata points. Note that dloc(xi, xj) < when xj Ki or xi Kj. Therefore, the shortest path search onGloc can be seen as an undirected shortest path search on a KNN graph, where the distances are locallyrescaled. As a result, for any two data points x and y, the search givesdglo(x, y) = minP dloc(x, u1) + + dloc(up, y),(6) where P varies over the graph path between x and y. This process is outlined in Algorithm 1. In line 4,D2, sets the distances to zero for all but the K-nearest neighbors, indicating disconnections on the neighborgraph. Consequently, the elementwise-max operation in Line 8 yields the intended dloc in equation 4. Notethat for the distance of the disconnected elements (nodes), a shortest path search algorithm assigns . Thisimplies that when two points are disconnected based on the local graph Gloc, they are regarded to be ondierent disconnected manifolds, so that the global distance is dened as . The search for the shortest path on a neighbor graph with Euclidean distances is well studied by Tenenbaumet al. (2000).However, with our locally adaptive building blocks, comprehending the properties of theresultant global distance becomes challenging. Therefore, the distance construction in equation 6 is heuristicin its nature. In .2, the construction of the global distance will be explained through the lens ofthe coequalizer on extended pseudometric spaces. The eectiveness of this approach in capturing the globalstructure is demonstrated in Examples 1 and 2, as well as in .",
  ":end for": "partially preserved even from the start, as shown in Figure S31 in Section S8. Furthermore, only within 5epochs, all level of clusters are identied. For the sensitivity of e, see Figure S32 in Section S8. This guredemonstrates es impact, similar to that in the transductive case, but with a much milder eect. Remark 1. The two main hyperparameters of our method are the negative weight e and the temperingschedule of . Smaller e leads to tighter clustering, while larger values disperse clusters more, as demon-strated in Examples 1 and 2 (Figures S11, S12, and S32 in Section S8).We typically x e = 1, andrecommend e = 0.1 for a tighter clustering and e = 10 for more relaxed cluster shapes. For tempering,since the impact s varies between datasets, we normalize the distances throughout this paper, aiming for amedian of 3. Our standard practice begins with = 1, reducing to = 0.1. This range may not suit everydataset, and thus we provide guidance for adjusting . Our method evolves from random noise to global, thenlocal shapes. If it remains noisy for a long time, e.g., over half the epochs, it suggests a too high initial ,giving insucient time for local detail development. In such cases, it is advisable to reduce the starting ; in",
  "Proof See Section S1.1": "We apply stochastic gradient descent (SGD) to optimize the loss in (10) with respect to Z, decaying thelearning rate and temperature . The tempering through decreasing shifts the focus from global to local,thereby catalyzing the progression of visualization from a global to a local perspective in a single optimizationprocess. For a more in-depth discussion of tempering , refer to Section S5.1. To stabilize SGD, we adopttwo optimization techniques from McInnes et al. (2018). First, the gradient of each summand of Liglo(Z)is clipped by a xed constant c.Second, the optimizer rst updates Z w.r.t.the negative term, thenagain updates w.r.t. the positive term, which is calculated with the updated Z. The described transductiveGLoMAP algorithm is in Algorithm 2. This stochastic optimization approach is highly sought after, especially in our global distance context. Theoptimization of Lglo(Z) in (9) is challenging because the number of pairs with nonzero ij can be O(n2).Note that although the loss formulation Lglo(Z) in (9) resembles that of t-SNE and UMAP, they do not havethe same computational burden because the number of all pairs with non-zero ij is O(nK) = O(n), andthus their schemes consider all such pairs at once or sequentially. Therefore, we optimize Lglo(Z) throughLiglo(Z) in equation 10 with a mini-batch sampling.The caution may arise as each iteration does notnecessarily involve the entire dataset. This problem is handled by an inductive formulation in the followingsection. Example 1. We apply GLoMAP to two exemplary simulated datasets with both global and local structures.The spheres dataset (Moor et al., 2020) has ten inner clusters and an eleventh spread on a large outer shell.Moor et al. (2020) found that UMAP identies all inner clusters but not the outer shell one. The hierarchicaldataset (Wang et al., 2021) features a three-layer tree with ve child clusters per parent, across ve trees.",
  ".3iGLoMAP: The particle-based inductive algorithm": "To achieve an inductive dimensional reduction mapping, we parameterize the embedding vector using amapper Q : X Rd, where Q can be modeled by a deep neural network, and the resulted embeddings arezi = Q(xi) for i = 1, . . . , n. Then the loss in equation 9 becomes Lglo(Q(x1), . . . , Q(xn)) and is optimizedwith respect to Q. Due to Proposition 1, we can use an unbiased estimator of Lglo(Q(x1), . . . , Q(xn))with Lglo dened in equation 10 for stochastic optimization. We develop a particle-based algorithm to stablyoptimize the inductive formulation. The idea is that the evaluated particle z = Q(x) is rst updated in atransductive way (as updating z in Algorithm 2), and then Q is updated accordingly by minimizing thesquared error between the original and updated z. The proposed particle-based inductive learning is inAlgorithm 3. Note that this particle-based approach does not increase any computational cost in handlingthe DNN compared to a typical deep learning optimization; the DNN is evaluated/dierentiated only onetime over the entire mini-batch. At the same time, it individually regularizes the gradient of each pair dueto the transductive step in Line 12 in Algorithm 3. In our implementation, the optimizer for is set as theAdam optimizer (Kingma & Ba, 2015) with its default learning hyperparameters, i.e., = (0.9, 0.999) withlearning rate decay 0.98. The Adam optimizer is renewed every 20 epochs. In the iGLoMAP algorithm, the entire dataset is aected by every step of stochastic optimization throughthe mapper Q. This is in contrast to the previous transductive GLoMAP algorithm, where an embeddingupdate of a pair of points does not aect the embeddings of the other points. Due to the generalizationproperty of the mapper, we empirically found that the iGLoMAP algorithm needs fewer iterations than thetransductive GLoMAP algorithm, as will be shown in the following example. Note that the particle-basedalgorithm of iGLoMAP is distinguished from two stage-wise algorithms that rst complete the transductiveembedding and then train a neural network to learn the embedding (for example, Duque et al. (2020)). Insuch cases, the transductive embedding stage cannot enjoy the generalization property of the neural networkduring training. Example 2. We apply iGLoMAP on the spheres and hierarchical datasets in Example 1 with the samehyperparameter settings. As a mapper, we train a basic fully connected neural network as described in. The visualization results of the training set and also the generalization performance on the testset are presented in . In the spheres dataset, the ten inner clusters remain distinct from the outershell (colored purple), and this separation is still evident in the generalization. For the hierarchical dataset,the test set is generated with dierent level of random corruption (in noise level, micro level, or meso level)from the training set. As these levels increase, the generalization appears dierent, yet various levels ofclusters can still be observed. We make some other empirical observations. First, as mentioned above, theiGLoMAP algorithm needs fewer iterations compared to the transductive GLoMAP algorithm. We trainedit for 150 epochs, in contrast to the 300 epochs for the transductive algorithm. Second, we found that evena small starting does not hinder the discovery of global structures; for an analogous gure to Figure S10,see Figure S29 in Section S8. These ndings suggest that the deep neural network might inherently encodesome global information. For example, with iGLoMAP on the hierarchical dataset using the default K = 15neighbors, which is generally too few for meso level cluster connectivity, the macro and meso clusters are",
  "Theoretical Analysis": "One of the key innovations of GLoMAP compared with existing methods is the construction of the distancematrix. Roughly speaking, our distance is designed to combine the best of both worlds: like Isomap, thedistances are globally meaningful by adopting its shortest path search; like t-SNE and UMAP, the distance islocally adaptive because the shortest path search is conducted over locally adaptive distances. In .1,we establish that our local distances are consistent geodesic distance estimators under the same assumptionutilized in UMAP. Then, in .2, we provide theoretical insights into our heuristic, which constructs asingle global metric space by coherently combining local geodesic estimates. Recall that a geodesic distancedM, given a manifold M, is dened by dM(x, y) = infs{length(s)}, where s varies over the set of (piecewise)smooth arcs connecting x to y in M.",
  "Local geodesic distance": "This section aims to justify the local geodesic distance estimator on a small local Euclidean patch of themanifold M. Consider a dataset in Rp, distributed on a d-dimensional Riemannian manifold M 1, whered p. We make the following assumptions on (M, g) and the data distribution. A1. Assume that, for a given point u M, there exists a geodesically convex neighborhood Uu Msuch that (Uu, g) composes a d-dimensional Euclidean patch with scale u > 0, i.e., g = ug, whereg is a Euclidean metric restricted to a d-dimensional subspace.",
  "A2. Assume that M is compact. The data are then assumed to be i.i.d. samples drawn from a uniformdistribution on M": "A1 characterizes a local neighborhood around a xed point on M that near the point u, M behaves locallyas a d-dimensional Euclidean space (up to scaling). A2 assumes that the data are uniformly distributed onthe manifold. We assume compactness to ensure nite volume, allowing us to dene a uniform distribution.Alternatively, we could assume that the manifold M has nite volume under the metric g. A2 is benecialfor theoretical reasons, as pointed out in the work of Belkin and Nuyogi on Laplacian eigenmaps (Belkin& Niyogi, 2001; 2003) and also observed by McInnes et al. (2018). Assumptions A1 and A2 together aresimilar to those of UMAP, which interprets a denser neighborhood as a local manifold approximation with alarger u and a sparser neighborhood as one with a smaller u. Intuitively, this brings an eect of projecting(attening) the manifold locally onto a Euclidean space. Now we present a distance theorem that connectsthe local geodesic distance with the L2 distance.Theorem 1. Denote x P the uniform probability distribution on M that satises A2.",
  "(a) Assume A1 regarding a point u M and its open convex neighborhood Uu. For any pair (x, y) thatbelongs to Uu and u > 0, dM(x, y) = ux y2": "(b) Assume A1 and A2 regarding a point u M with its neighbor Uu and u. Let V = vol(M). Assume,for an m , B2(u, GP,m(u)) Uu, where GP,t : x M inf{r > 0; P(B2(x, r)) t} andB2(x, r) is an L2 ball2 centered at x with radius r. Then, u = C/P,m(u), where C is a universalconstant that does not depend on u or u but only on d, m, V , and",
  "G2P,tdt,(11)": "1Given manifold M, a Riemannian metric on M is a family of inner products, {, u : u M}, on each tangent space,TuM, such that , u depends smoothly on u M. A smooth manifold with a Riemannian metric is called a Riemannianmanifold.The Riemannian metric is often denoted by g = (gu)uM.Using local coordinates, we often use the notationg = di=1dj=1 gijdxi dxj, where gij(u) = (",
  "Proof See Section S1.2": "Theorem 1 (a) marginally generalizes Lemma 1 from McInnes et al. (2018), which originally introduced theconcept of viewing the local geodesic distance as a rescale of the existing metric on the ambient space. InTheorem 1 (b), we connect u to DTM (Chazal et al., 2011) by noticing that under the uniform distributionassumption, the probability of a set is closely related to its volume. By this result, for any pair (x, y) in ageodesically convex Ux, we have 3",
  "P,m(x)x y2.(12)": "This formulation of dM(x, y) through DTM P,m(x) is important because its unbiased estimator is identicalto x in equation 5. Therefore, by the result in Theorem 1 (b), we know that x/C is an unbiased estimatorof 1/x. Now, by using x, we dene a local geodesic distance estimator of dM(x, y)/C as",
  "Theorem 2 demonstrates that dx(x, y) converges in probability to 1": "C dM(x, y), where C is a universal constantindependent of the local center xs choice. Thus, dx(x, y) estimates the geodesic distance up to a constantmultiplication. Note that for dimension reduction, the scale of the input space is relatively unimportant,and thus, we regard C = 1. Consequently, by establishing an equivalence relationship dM(x, y) = xy2",
  "Construction of global metric space in extended-pseudo-metric spaces": "When we apply the local geodesic distance estimation as described in equation 13, treating each data point asthe local center, we eectively obtain n dierent local approximations of the data manifold. In this section,our goal is to coherently glue these approximations together. By treating these as n dierent metric systemson the shared space X with |X| = n, we employ an equivalence relation analogous to Spivaks coequalizerin extended-pseudo-metric spaces. This method presents an alternative to the fuzzy union approach used inMcInnes et al. (2018), with both methods aiming to merge local information amidst potential inconsistenciesamong local assumptions. While the fuzzy union encapsulates the global manifold by aggregating all localdistances into a single set, our shortest path approach intricately interweaves these distances to form a unied(extended) metric system. Let A be the set of all indices of data points X, and for all a A, let Xa = (X, da) denote the space denedthrough the localized metric da of UMAP in equation 2 or our approach in equation 13. As shown in McInneset al. (2018), this localized space Xa for any a A is an extended-pseudo-metric space. Metaphoricallyspeaking, the points in Xa that are within a nite distance from a under da can be thought of as composingan island centered around a (See Xa in Figure S28, the red-colored part). We dene fa as an operator thatmaps from X to Xa. Now, we simply merge these extended-pseudo-metric spaces into one as XA =",
  "dY ([z], [z]) = inf(dZ(p1, q1) + dZ(p2, q2) + + dZ(pn, qn)),(15)": "where the inmum is taken over all pairs of sequences (p1, ..., pn), (q1, ..., qn) of elements of Z, such thatp1 z, qn z, and pi+1 qi for all 1 i n 1. According to Spivak (2009), when (Z, dZ) is a extended-pseudo-metric space, (Y, dY ) is another extended-pseudo-metric space and satises the universal property ofa coequalizer. His construction of coequalizer through equivalence relation can provide in our setting a toolto merge two possibly-conicting distances into one coherent distance. In the above diagram let Z = XAand let gi be the function that maps X to the subset in XA that is induced by Xai, where we enumerate Athrough A = {a1, ..., an}. First, consider g1 and g2. The coequalizer diagram equation 14 regarding g1 andg2 merges the subspaces in XA that correspond to Xa1 and Xa2 into a smaller space (Y, dY ), eliminating theredundancy discussed above. As illustrated in Figure S28, the disconnected two inherently-identical pointsnow are identied as identical, and the two islands are linked, forming a larger island (in the sense that morepoints are connected with nite distance to others). Under the design of Spivak (2009) in equation 15, two possibly inconsistent local distances are merged intoone by taking the minimum of them. We consider instead to take the maximum between the two as follows.Dene a nite max operator (a maximum among nite elements), denoted by",
  "fmaxaA{f(a)} :=maxaAf {f(a)},if Af = ,,otherwise,(16)": "where Af = {a A|f(a) < }. Recalling that Z = XA, we dene the local building block as df(p, q) :=fmaxxp,yqdZ(x, y)to reconcile the incompatibility. Then, the merged distance on Y in equation 15 isreplaced asdY ([z], [z]) = inf(df(p1, q1) + df(p2, q2) + + df(pn, qn)).(17) Now, we consider all g1, ..., gn. We extend the equivalence relation [], dening x x if there exist somea A and indices i, j such that gi(a) = x and gj(a) = x. In this case, dY in equation 17 satises theconditions of an extended-pseudo-metric.",
  "Proof See Section S1.3.1": "On Y , dY denes the distances between all pairs in the dataset without inconsistencies among distances.Now, the following theorem says that the global distance of GLoMAP is a special case of the metric dY inequation 17, given that two neighboring points are neighbors to each other. Furthermore, it states that inthis case, dY is an extended metric, which is a stronger notion than an extended pseudo-metric. Theorem 3. Assume x Ky i y Kx for x, y X, where Kx is the K-nearest neighbor of x. Consider(Xa, da) constructed by the local geodesic distance estimator dened in equation 13 with Ux approximated byKx. Then dY in equation 17 is identical to dglo in equation 6 and an extended metric.",
  "Proof See Section S1.3.2": "Remark 2. If a minimum operator is used instead of the fmax operator, the distance in equation 17 corre-sponds to the distance dened by Spivak (2009) as in equation 15. The dierence between these two distancesis subtle, yet the modication in equation 17 reects our view that when each of two points has a dierentlocal scale, the smaller scale should be employed to dene the distance between them. In other words, thelarger local distance should be used. Conversely, the fuzzy union approach of UMAP results in a nal lo-cal distance that is shorter than either of the two conicting local distances.4 Therefore, our approach andthat of McInnes et al. (2018) reect dierent philosophical perspectives. We believe that the impact of thisphilosophical divergence becomes signicant in practical applications. For instance, in the Spheres datasetin Examples 1 and 2, points on the outer shell seem very distant from the perspective of the inner smallclusters, while from the outer shells perspective, the inner clusters do not appear comparatively far. Webelieve that the choice of perspective signicantly inuences the visualization outcome, as demonstrated in, where GLoMAP aligns with the former perspective.",
  "Numerical Results": "In this section, we compare our approach with existing manifold learning methods and demonstrate itseectiveness in preserving both local and global structures. We consider two scenarios that allow for a faircomparison: 1) cases where lower-dimensional points are smoothly transformed into a high-dimensional space,with the goal being to recover the lower-dimensional points; and 2) scenarios where label information revealingthe data structure is available.We provide the GLoMAP and iGLoMAP implementation as a Pythonpackage. The detailed learning congurations are in Section S4. Our analysis begins with transductivelearning, followed by inductive learning.",
  "Transductive Learning": "We compare GLoMAP with existing manifold learning methods, including Isomap, t-SNE, UMAP, PaCMAP,and PHATE, which inherently employ transductive learning.First, we consider a number of three-dimensional datasets to visually compare the DR results, while also quantitatively measuring the perfor-mance. Second, we study DR for three high-dimensional datasets that inherently form clusters so that theKNN classication result can be used to measure the DR performance. 4As described in .2, the membership strength given by a fuzzy union is dened by ij = pij + pji pijpji,where pij = exp{dumapi(xi, xj)} and pji = exp{dumapj(xi, xj)}. Hence, it can be seen as ij = exp{x} for some x min{dumapi(xi, xj), dumapj(xi, xj)}.",
  "Manifolds embedded in three dimensional spaces": "Dataset and performance measure. We study DR for three datasets: S-curve, Severed Sphere, andEggs, shown in . All three datasets are obtained by embedding data points on a two-dimensionalbox into a three-dimensional space using a smooth function. Detailed data descriptions are presented inSection S3.We adopt two performance measures.The rst one is the correlation between the originaltwo-dimensional and the dimensional reduction L2 distances. The second one is the KL-divergence betweendistance-to-measure (DTM) type distributions, as used in Moor et al. (2020), which is dened by",
  "We vary from 0.001 to 10. A larger focuses more on global preservation, while a smaller focuses moreon local preservation": "Results. The visualization results are shown in , where GLoMAP demonstrates clear and infor-mative results. Across all three datasets, we observe rectangular shapes and color alignments, indicative ofsuccessful preservation of both global and local structures. Isomap and GLoMAP consistently demonstratevisually compatible recovery across all three datasets. PacMAP also achieves this in the Severed Spheres andEggs datasets. The visualization by PacMAP of the S-curve demonstrates local color alignment althoughit shows an S-shape. A similar pattern (but with a U-shape) is observed with t-SNE and UMAP in theS-curve. Although the global connectivity of the Eggs dataset is not displayed by t-SNE and UMAP, theydemonstrate local structure preservation by local color alignments for each egg shape. Consistent with thesevisual observations, the quantitative measures, plotted in Figure S30, reveal that GLoMAP has the compet-itive results overall. Depending on the run, the visualization of GLoMAP of the S-curve and Eggs datasetcan be twisted (Figure S36 (c) and (d)). Even when it happens, the performance measures are similar, asthe overall global shape is quite similar.",
  "High dimensional cluster structured datasets": "Dataset.We consider the case where the label information revealing the data structure is available. Weapply GLoMAP on hierarchical data (Wang et al., 2021), Spheres data (Moor et al., 2020), and MNIST(LeCun et al., 2010). The hierarchical data have ve macro clusters, each macro cluster contains ve mesoclusters, and each meso cluster contains 5 micro clusters. Therefore, the 125 clusters can be seen as 25 mesoclusters, where 5 meso clusters also compose one macro cluster. One micro cluster contains 48 observations,and the dimension of the hierarchical dataset is 50. The Spheres data by (Moor et al., 2020) are consistedof 10000 points in 101 dimensional space. Half of the data is uniformly distributed on the surface of a largesphere with radius 25, composing an outer shell. The rest reside inside the shell relatively closer to theorigin, composing 10 smaller Spheres of radius 5. Detailed data descriptions are presented in Section S3.The MNIST database is a large database of handwritten digits 0 9 that is commonly used to train variousimage processing systems. The MNIST database contains 70, 000 28 28 grey images with the class (label)information, and is available at The MNIST dataset is regardedas the most important and widely used real dataset for evaluating the data structure preservation of a DRmethod (Van der Maaten & Hinton, 2008; McInnes et al., 2018; Wang et al., 2021). Since in the later section,iGLoMAP generalizes to unseen data points, we use 60, 000 images for training (and in the later section,the other 10, 000 images for generalization). The baseline methods are also applied to the 60, 000 trainingimages.5 Performance measure. As a performance measure, we use KNN classication accuracy as described inthe followings: Since a data observation comes with a label, we can calculate the classication precision bytting a k-nearest neighbor (KNN) classier on the embedding low-dimensional vectors (based on the L2distance). For the hierarchical dataset, the KNN can measure both local and global preservation; The KNNbased on the micro labels shows local preservation, while the KNN based on the macro labels demonstrateglobal preservation. For the Spheres, KNN is a local measure for the small inner clusters, and a proxy forglobal separation between the outer shell and the inner clusters. For the MNIST, the KNN measures localpreservation. If we assume that the data points within the same class have a stronger proximity than betweenclasses, the KNN classication accuracies on the DR results are a reasonable measure of local preservation.",
  ": The KNN measures for increasing K": "Note that the labels of the MNIST dataset allow global interpretation by human perceptual understandingsof the similarity between numbers. For example, a handwritten 9 is often confused with a handwritten 4. Results. The comparison of the visualization with other leading DR methods is presented in andperformance measure in . On the Spheres, the proposed GLoMAP shows very intuitive visualizationresults similar to what one would probably draw on a paper based on the data description. The inner tenclusters are enclosed by the other points that make up a large disk. Similarly, for the hierarchical dataset,we can clearly identify all levels of the hierarchy structure. We can see that although the inner ten clustersare less clearly identied, Isomap also gives the global shape, such that the outer points are well spreadout. For methods such as t-SNE, PaCMAP, and UMAP, the outer shell points of the Spheres dataset stickwith the inner points making ten clusters. For the hierarchical dataset, no baseline method catches thenested clustering structure; where either the global structure or the meso level clusters is missed. Thesevisual observations are corroborated by the KNN classication plot in , which demonstrates theeective KNN classication performance on GLoMAPs DR. For the hierarchical dataset, only GLoMAPshows almost perfect meso and macro level classication. Also, for MNIST, GLoMAP achieves more than97% of the KNN accuracy numbers, demonstrating GLoMAPs competence in local preservation.",
  "Inductive Learning": "We apply iGLoMAP and compare it with other leading parametric visualization methods, including Para-metric UMAP (PUMAP) (Pai et al., 2019), TopoAE (Moor et al., 2020), and DIMAL (Sainburg et al.,2021), which can be seen as a parametric Isomap. These methods employ deep neural networks for map-ping; they are detailed in further detail in Section S2. For iGLoMAPs mapper, we utilize a fully connectedReLU network with three hidden layers, each of width 128. Following each hidden layer, we incorporatea batch normalization layer and ReLU activation. The networks nal hidden layer is transformed into 2dimensions via a linear layer. For the other methods, we either employ the default networks provided orreplicate the network designs used in iGLoMAP. Specically, for MNIST, PUMAP, and TopoAE are giventhe congurations (including hyperparameters and network designs) recommended by the original authors. The DR results are presented in .6 iGLoMAP exhibits similar visual qualities to GLoMAP butwith more clearly identiable global and meso-level clusters for the hierarchical and MNIST datasets. Forinstance, from the MNIST results in , we observe that similar numbers form groups, such as (7,9,4),(0,6), and (8,3,5,2), while the ten distinct local clusters corresponding to the ten-digit numbers are evident.The generalization capability of iGLoMAP is depicted in for MNIST and in Figure S33 in SectionS8 for the S-curve, Severed, and Eggs datasets, demonstrating almost identical visualizations for unseendata points.Interestingly, the enhanced performance of parametric UMAP over (transductive) UMAP,despite sharing the same framework, reinforces our conjecture in Example 2 that incorporating DNNs aidsin preserving global information. Nonetheless, PUMAPs performance on the spheres dataset serves as acautionary example, indicating that the application of DNNs is not a universal solution for bridging thegap in global information representation. On the Eggs dataset, PUMAP often displayed a more pronounceddisconnection than that shown in , as shown in Figure S36 (a) in Section S8. We also observed thatdepending on the specic run, the Eggs and S-curve visualizations of iGLoMAP can appear twisted, as inFigure S36 (b) in Section S8. The numerical performance metrics are presented in Figures S34 and S35 in",
  "Ery Arias-Castro and Phong Alain Chau.Minimax estimation of distances on a surface and minimaxmanifold learning in the isometric-to-convex setting. arXiv preprint arXiv:2011.12478, 2020": "Etienne Becht, Leland McInnes, John Healy, Charles-Antoine Dutertre, Immanuel WH Kwok, Lai GuanNg, Florent Ginhoux, and Evan W Newell. Dimensionality reduction for visualizing single-cell data usingUMAP. Nature biotechnology, 37(1):3844, 2019. Mikhail Belkin and Partha Niyogi. Laplacian eigenmaps and spectral techniques for embedding and cluster-ing. In Proceedings of the 14th International Conference on NIPS: Natural and Synthetic, pp. 585591,Cambridge, MA, USA, 2001. MIT Press.",
  "Mikhail Belkin and Partha Niyogi. Laplacian eigenmaps for dimensionality reduction and data representation.Neural Computation, 15(6):13731396, 2003. doi: 10.1162/089976603321780317": "Anna C Belkina, Christopher O Ciccolella, Rina Anno, Richard Halpert, Josef Spidlen, and Jennifer ESnyder-Cappione. Automated optimized parameters for t-distributed stochastic neighbor embedding im-prove visualization and analysis of large datasets. Nature communications, 10(1), 2019. Lars Buitinck, Gilles Louppe, Mathieu Blondel, Fabian Pedregosa, Andreas Mueller, Olivier Grisel, VladNiculae, Peter Prettenhofer, Alexandre Gramfort, Jaques Grobler, Robert Layton, Jake VanderPlas, Ar-naud Joly, Brian Holt, and Gal Varoquaux. API design for machine learning software: experiences fromthe scikit-learn project. In ECML PKDD Workshop: Languages for Data Mining and Machine Learning,pp. 108122, 2013.",
  "Xiongtao Dai and Hans-Georg Mller.Principal component analysis for functional data on riemannianmanifolds and spheres. The Annals of Statistics, 46(6B):33343361, 2018": "Andrs F Duque, Sacha Morin, Guy Wolf, and Kevin Moon. Extendable and invertible manifold learning withgeometry regularized autoencoders. In 2020 IEEE International Conference on Big Data, pp. 50275036.IEEE, 2020. Cong Fu, Yonghui Zhang, Deng Cai, and Xiang Ren.Atsne: Ecient and robust visualization on gputhrough hierarchical optimization. In Proceedings of the 25th ACM SIGKDD International Conference onKnowledge Discovery & Data Mining, pp. 176186, 2019.",
  "Michael Moor, Max Horn, Bastian Rieck, and Karsten Borgwardt. Topological autoencoders. In Internationalconference on machine learning, pp. 70457054. PMLR, 2020": "Gautam Pai, Ronen Talmon, Alex Bronstein, and Ron Kimmel. Dimal: Deep isometric manifold learningusing sparse geodesic sampling. In 2019 IEEE Winter Conference on Applications of Computer Vision(WACV), pp. 819828. IEEE, 2019. F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer,R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay.Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:28252830, 2011. Yixuan Qiu and Xiao Wang. ALMOND: Adaptive latent modeling and optimization via neural networksand langevin diusion. Journal of the American Statistical Association, 116(535):12241236, 2021. doi:10.1080/01621459.2019.1691563. URL",
  "j=1(1 ij) log (1 qij).(S2)": "We rst rewrite the rst term in (S2) by using an expectation form. We say i i when i follows a discretedistribution over {1, ..., n} with probability (1, ..., n).Likewise, we say j|i j|i when j|i follows adiscrete distribution over {1, ..., n} with probability (1|i, ..., n|i) where j|i :=ij",
  "Proof. In this proof, we make use of the limiting distribution theorem regarding the empirical DTM inChazal et al. (2017). First, we restate the theorem": "Theorem 4 (Theorem 5 in Chazal et al. (2017)). Let P be some distribution in Rd. For some xed x,assume that Fx is dierentiable at F 1x (m), for m (0, 1), with positive derivative F x(F 1x (m)). DenePn,m(x) = Kk=1 x X(k)(x)2/K for K = mn. Then we have",
  "t)= ctd/2,": "for some constant c > 0. Therefore, Fx(t) is dierentiable for t GP,m(x), and its derivative is alwayspositive as long as t > 0. Note that F 1x (m) > 0 when m (0, 1). Therefore, we can apply Theorem 4 inour setting, so that 2P,m(x) 2x = op(1). Therefore,",
  ". Since each df is non-negative, by denition dY is non-negative": "2. For a sequence in A(x, y), its reverse exists and is also a sequence in A(y, x). Now, the symmetry ofdf makes the path distance of any ordered sequence {pi, qi}ni=1 A(x, y) the same as that of the reversedsequence {pi, qi}ni=1 A(y, x), where pj = qn+1j and qj = pn+1j. In other words,",
  "infA(x,y) df(p1, q1) + + df(pn, qn) =infA(y,x) df(p1, q1) + + df(pn, qn)": "3.Let dY (x, y) < . Choose any w Y .If any of dY (x, w) or dY (w, y) is , then the inequality istrue.Now, consider both dY (x, w) and dY (w, y) to be nite.Then, there exist minimizing sequences{pi, qi}ni=1 A(x, w) and {pi, qi}ni=1 A(w, y) such that dY (x, w) = df(p1, q1)+ df(pn, qn) and dY (w, y) =df(p1, q1) + df(pn, qn) with [qn] = [p1] = w. Note that when we merge the two paths into one, we havea path connecting x and y. Now, similarly to A(x, y), dene A2(x, y) as a set of all sequences {pi, qi}2ni=1 ofelements of XA, that satisfy pi+1 qi for all 1 i n 1 and p1 x, q2n y.",
  "S1.3.2Proof of Theorem 3": "We have g1, ..., gn, and each x X is mapped to a distinct point gi(x) XA, i.e., gi(x) = gj(x) if i = j.Therefore, for each p XA, we can uniquely identify the index i and x X s.t. p = gi(x), i.e., x = g1i(p).Since the image of gi does not overlap i.e., gi(X) gj(X) = for i = j, we can dene g1, which is a unied",
  "dloc(x, u1) + + dloc(up, y)= dglo(x, y)": "with dglo(x, y) dened in equation 6, where x = g1(p1) X and y = g1(qn) X. The second equality isdue to g1(p) = g1(q) for p q. Note that the x, y Y are identied by x, y X, so with a slight abuseof notation, we can say dY (x, y) = dglo(x, y). Now, we show that dY in this case is an extended-metric. Given Proposition 2, we only need to show thatdY (x, y) = 0 i x = y. Since da satises da(x, y) = 0 i x = y, we have df(p, q) = 0 i p = q. Given this,consider x, y Y . It is obvious that dY (x, y) = 0 if x = y; we can take pi and qi so that pi = qi and [pi] =[qi] = x for i = 1, ..., n, which gives 0 dY (x, y) df(p1, q1)+df(p2, q2)+ +df(pn, qn) = 0. If dY (x, y) = 0,there must exist a sequence {pi, qi}ni=1 A(x, y) such that df(p1, q1)+df(p2, q2)+ +df(pn, qn) = 0. Sinceif pi = qi, then df(pi, qi) > 0, we have pi = qi for i = 1, ..., n. Since pi+1 qi by the denition of A(x, y)and since [qi] = [pi], we conclude that x = [p1] = = [pn] = y.",
  "In this section, we discuss additional related work": "PaCMAP(Wang et al., 2021) propose PaCMAP to preserve both the local and global structure. The lossof PaCMAP is derived from an observational study of state-of-the-art dimension reduction methods suchas t-SNE (Van der Maaten & Hinton, 2008) and UMAP (McInnes et al., 2018). The global preservationof PaCMAP is based on initialization through PCA and some optimization scheduling techniques.Ourdevelopment of global distances may be adopted by PacMAP instead of its current use of Euclidean distance. PHATE(Moon et al., 2017) propose PHATE to preserve a diusion-based information distance, wherethe long-term transition probability catches the global information. Intuitively, PHATE generates a randomwalk on the data points with the transition probability matrix P where P[i|j] exixj for some globalrescaler .. After t walks, a large transition probability pt[i|j] where P T = P will imply that i and jare relatively closer than those pairs of low transition probability. The diusion-based design of PHATEis especially eective to visualize biological data, which has some development according to time, such assingle-cell RNA sequencing of human embryonic stem cells. Distance to Measure.Distance to measure (DTM) was rst introduced by Chazal et al. (2011) and itslimiting distributions are investigated in (Chazal et al., 2017). These works are in the context of topologicaldata analysis, where DTM is a robust alternative of distance to support (DTS). DTS is a distance from a pointx to the support of the data distribution, not a distance between two points. DTS is used to reconstruct thedata manifold, for example by r-oset (a union of radius r-balls centered at the data points), or together withpersistent topology (increasing r) to infer topological features such as Betti numbers. DTM is introducedas an alternative to DTS because the empirical estimator of DTS depends on only one observation in thedataset, which might be sensitive to outliers. These works do not share the same context with our work",
  ", 3": "2 ). Also, let u be uniform samples from (0, 2) of the samesize. We can think that the original data in the lower dimensional space (2-dimensional) is a matrix [t, y].Now we transform this data matrix into higher dimension (3-dimensional). The rst dimension x is denedby x = sin t, where the sin is an element-wise sin operator. The second dimension is simply y = u. Thethird dimension is dened by a vector z = sin(t) (cos(t) 1). The resulting 3-dimensional data is a matrix[x, y, z]. We use n = 6000. Severed SphereThe Severed Sphere dataset is also obtained by the Python package scikit-learn (Pe-dregosa et al., 2011).Let u be a vector of uniform samples from (0.55, 2 0.55). Also, let t be avector of uniform samples from (0, 2). The 2-dimensional data matrix [t, y] is transformed into threedimension rst by transform, and then by point selection.First, the transformation is [x, y, z] =[sin(t) cos(u), sin(t) sin(u), cos(u)]. Now only the rows of the data matrix such that",
  "< t <78are se-lected for t t. We use n = 6000": "EggsThe Eggs data is made up of 5982 data points in the three-dimensional space. It has 12 half-spheres(empty inside) and a rectangle with 12 holes.When we locate the 12 half-spheres on the holes on therectangle, we obtain the shape of a two-dimensional surface, where the half spheres and the boundariesof the wholes are seamlessly connected. On the rectangular surface, the points on the at area are littlesparser than in the curved area; we uniformly distribute 2130 points on the rectangle andremove the points on the 12 circles with radius 1, centered at [(13, 2), (13, 2), (8, 2), (3, 2), (2, +2),(7, 2), (12, 2), (8, +2), (3, 2), (2, 2), (7, +2), (12, 2)]. Depending on the random uniform generation,the amount of points removed can vary. Therefore, in our setting, we use a xed random generating seednumber so that the training set is always the same, and always the number of remaining points is 1266.Each half-sphere of radius 1 has 348 uniformly distributed points.Therefore, the dataset in total has12 348 + 1266 = 5982. HierarchyThe hierarchy dataset is developed by Wang et al. (2021). The ve macro centers are generatedfrom N(0, 1002I50) where I50 is a 50x50 identity matrix. We call each macro center as Mi for i = 1, ..., 5.Now, for each macro center, 5 meso centers are generated from N(Mi, 1000I50). We call the meso centers asMij where j = 1, .., 5. Now, for each meso center, 5 micro centers are generated from N(Mij, 100I50). Wecall the micro centers Mijk, where k = 1, ..., 5. Now for each micro center, a local cluster is generated fromN(Mijk, 10I5). In our experiment, each local cluster consists of 48 points so that in total n = 6000. SpheresThe spheres dataset is designed by Moor et al. (2020). It has one large outer sphere on whichhalf of the dataset is uniformly distributed and ten smaller inner spheres on which the remaining points areuniformly distributed. Let the center an inner sphere be denoted by ui, where i = 1, ..., 10. Then, ui isgenerated from N(0, 0.5I101). Now, 5% of the dataset is uniformly distributed on the sphere of radius onecentered at ui. The large outer sphere is centered at the origin and has a radius of 25. The 50% of the dataset is uniformly distributed on this outer sphere. In our case, we use n = 10000.",
  "S4Learning Congurations": "Learning Conguration for Example 1.We apply UMAP and GLoMAP, where for both methods, thenumber of neighbors is set as K = 15 (default) for the spheres, and as K = 250 for the hierarchical dataset.The latter is large to make the graph connected at least within each of the macro-level clusters. We x alle to 1 (default), while is scheduled to decrease from 1 to 0.1 (default). Learning Conguration for .Typically, we set e at 1 (default) to showcase performancewithout tuning. However, for MNIST, we opt for tighter clustering by setting e at 0.1. In the case ofiGLoMAP applied to the S-curve, the shape initially remains linear, then expands suddenly at the end.To address this, we adjusted the starting to be four times smaller as outlined in Remark 1. For similarreasons, we made adjustments on the Eggs and Severed datasets, allowing shapes to form earlier and providingsucient time for detail development. In the MNIST dataset, where both GLoMAP and iGLoMAP tendto linger in the noise, we reduced the starting by a quarter. Apart from these specic cases, we maintainthe default schedule (from 1 to 0.1). All other learning hyperparameters are set to their defaults in theiGLoMAP package (learning rate decay = 0.98, Adams initial learning rate = 0.01, initial particle learningrate = 1, number of neighbors K=15, and mini-batch size=100). GLoMAP was optimized for 300 epochs(500 epochs for MNIST), and iGLoMAP was trained for 150 epochs.",
  "S5.1The eect of tempering": "In Example 1, it seems tempering is vital to discover the hierarchical clustering structure in the data. FigureS10 (a) depicts the eect of decreasing temperatures on (d) = exp{d/}. For a given d, a decreasing reduces (d), requiring the corresponding pair on the visualization space to be more distanced. This makesthe local clusters stay close at the early stage while forming the global shape, and separates them at the latestages, forming the detailed local structures. The same progression from global shape to detailed localizationdoes not happen without the tempering (when is kept as a constant) as Figure S10 (b-d). A larger xed ( = 1) nds global shapes, and smaller xed s nd more local shapes while missing the global clusters.",
  "S5.2The eect of the aesthetics parameter e": "For Example 1, to see the eect of the aesthetics parameter e in controlling the attractive and repulsiveforces, we vary e for a wide range in Figure S11 and S12. We observe that similar development patternsoccur across a wide range of e values. However, a smaller e more strongly condenses data points within thecluster, and vice versa. For an extremely large e, the cluster shape may not appear, suggesting a possibilitythat optimization did not occur properly.",
  "S5.3Computational cost reduction": "While we have reduced the computational cost for local distance estimation, managing the global distances,conversely, increases the cost.Besides the shortest path search, the neighbor sampling scheme (line 9in Algorithm 2) incurs costs comparable to the normalization of t-SNE; The sampling involves summingmembership scores over n distances, given that all data points are connected. The cost of the samplingscheme can be reduced by instead considering the K-nearest neighbor distance matrix of Dglo, for example,with K = 20K. To further reduce the computational cost, we could also ignore the dierence between(1 ij) values, but consider them all as 1. This saves time in retrieving the ij values from the n n(sparse) matrix. Since (i) is only an n1 vector, it does not take much time to look up. In such a case, theloss function is L =",
  "S6Additional Experiments": "In this section, we discuss the performance of our methods on an interesting dataset, called Fishbowl. Inaddition, we also empirically investigate the sensitivity of iGLoMAP against the capacity choice of deepneural networks for the mapper. Fishbowl Data.It is an interesting problem how a dimensional reduction method performs on a dataset,which is not homeomorphic to R2, for example, a sphere. In such a case, there is no way to map ontothe two dimensional space while preserving all the pairwise distances in the data. Now, we slightly changethe problem. We consider that we are given data on a half-sphere. This is the famous crowding problem,and in this case also, there is no way to map onto the two dimensional space while preserving all thepairwise distances. Intuitively, we may want to smash (or, stretch) the half sphere to make it at on the",
  "Figure S15: The shbowl dataset (n=6000)": "two dimensional space. Now, we turn our attention to a dataset on something between a sphere and ahalf-sphere, called a shbowl (Silva & Tenenbaum, 2002), which has more than half of the sphere preserved.We generate a uniformly distributed dataset on a shbowl F = {(x, y, z) R3|x2 + y2 + z2 = 1, z }for {0.5, 0.6, 0.7, 0.8, 0.9, 0.95, 0.975, 1}. The data is illustrated in Figure S15. When = 1, the data areequivalent to a uniform sample on a sphere. Although there is no consensus on what type of visualization mustbe ideal, in this case the exact pairwise distance is impossible. All learning parameters and hyperparametersare set to their default values, except for the initial value for iGLoMAP, which was reduced to 0.5. Thisadjustment was necessary because, under the default value, iGLoMAP remains in a linear (non-meaningful)state until very late (see Remark 1). The results of GLoMAP and iGLoMAP are shown in Figure S25. ForGLoMAP, we see some recovery of the original disk until 0.95. Then, when 0.975 (when the shbowlis closer to a sphere), GLoMAP simply shows a side image. For iGLoMAP, the at disc shape is made until = 0.975. The result on a sphere is in Figure S25 (e) for GLoMAP and Figure S25 (j) for iGLoMAP. Sensitivity analysis on the deep neural network capacityIn order to see the impact of networksize, we use the same fully connected deep neural network (four layers) with varying network width. Thenetwork width (which controls the network size) varies in {2q|q = 2, 4, 6, 8, 10, 11}, of which the range isintentionally chosen to be extremely wide. We enumerate the corresponding capacities as 0 to 5. Width 4 isimpractically small; When networks are this small, the network may not be expressive. Width 2048 is theextreme opposite. Note that width 4 has 66 parameters to learn in the network, width 16 has 646, width 64",
  "Figure S16: The result of GLoMAP on the shbowl dataset. The number on each panel indicates the heightof sh bowl (the threshold on the z-axis)": "has 8,706, width 256 has 133,122, width 1024 has 2,105,346, and width 2048 has 8,404,994. We recall thatin , we used width 64 for all simulation datasets. We set the hyperparameters of iGLoMAP sameas in .2, except halving the starting to 0.5 for Spheres since for smaller networks, the expansionof visualization is observed too late. The visualizations of the results are in Figure S17. We see that increasing the network size improves thevisualization quality from width 4 to width 64. On the other hand, among larger networks, we see very similarresults, concluding that for a good range of network widths the visualizations are fairly similar. Therefore,we see that for a wide range of network width (of the fully connected deep neural network that we use here),the performance of iGLoMAP is stable. Those visual inspections are supported by the numerical measuresshown in Figures S18 and S19, which generally show slightly better performance of a larger network. Thisresult implies that we should avoid using too small networks, but much less caution is needed about usingtoo large networks.",
  "S7Additional Measures": "In Van Assel et al. (2024), the Silhouette coecient (Rousseeuw, 1987) and the trustworthiness score (Venna& Kaski, 2001) are used to assess the quality of dimensional reduction. The Silhouette coecient measuresboth the cohesion within clusters and separation between clusters, dened as ba/ max(a, b), where a is themean intra-cluster distance, and b is the mean nearest-cluster distance. The trustworthiness score quantieshow well the K-nearest neighbors in the low-dimensional representation aligns with the rank of among theEuclidean distances in the input space. Both scores are computed using the Scikit-learn Python packageBuitinck et al. (2013). The full results are shown in Figure S27 and S26, while Table S1 shows the case when n = 6000 with K = 5for trustworthiness measure. In Figure S27, For the Hierarchical dataset, when K is small, Isomap performspoorly under the Trustworthiness measure, while for larger K values, UMAP exhibits lower performance.GLoMAP demonstrates competitive performance overall. This consistent competence is also shown in theSilhouette coecient as in Figure S26. For the Spheres dataset, however, GLoMAP performs the worst onthis Trustworthiness measure and improves on the Silhouette coecient. The corresponding visualization ispresented in Figure S21, which we believe remains acceptable or even desirable.",
  "S9Comparison with C-Isomap": "To see the usefulness of our distance in equation 6, we plug-in our global distance into the Isomap framework,i.e., apply an MDS onto our global distance matrix. As a baseline, we choose C-Isomap (Silva & Tenenbaum,2002) among many Isomap variants to improve the local distance because C-Isomap is a scalable option asit also provides a closed-form local distance estimator. C-Isomap applies shortest path search over the localdistances and then Multidimensional Scaling (MDS). Since MDS does not allow any pair having as thedistance, we imputed 1.5 times of the maximum of pairwise distances for such pairs. As shown in FigureS38, for C-Isomap, when K is large then we see separation between the purple and the other classes. WhenK is small, the smaller classes are doted, and each class may compose multiple clusters in visualizationdepending on the size of . On the other hand, when C-Isomap is using our distance, we can see separation. Our GLoMAP corresponds to Algorithm 1 + Algorithm 2. Our choice of designing the loss similar to thatof UMAP (as in Algorithm 2) was also signicant, and novel if we see it from the perspective of Isomap.When comparing to , we can see that using Algorithm 2 instead of MDS brings a better clusteringeect for those ten inner clusters.",
  "Figure S30: The global preservation measures. Top: KL (smaller is better), Bottom: distance correlation(larger is better)": "Figure S31: iGLoMAP on the hierarchical dataset using the default K = 15 neighbors, which is generallytoo few for meso level cluster connectivity. However, the macro and meso clusters are still preserved fromthe start. This suggests the deep neural network might inherently encode some location information. (a)The random initialization of the deep neural network. It shows many levels of clusters. (b) At the epoch 5,already all the levels of clusters are found."
}