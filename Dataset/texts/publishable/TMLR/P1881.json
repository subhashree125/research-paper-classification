{
  "Abstract": "Policy gradient methods are a vital ingredient behind the success of modern reinforcementlearning. Modern policy gradient methods, although successful, introduce a residual errorin gradient estimation. In this work, we argue that this residual term is significant andcorrecting for it could potentially improve sample-complexity of reinforcement learningmethods. To that end, we propose log density gradient to estimate the policy gradient,which corrects for this residual error term. Log density gradient method computes policygradient by utilising the state-action discounted distributional formulation. We first presentthe equations needed to exactly find the log density gradient for a tabular Markov DecisionProcesses (MDPs). For more complex environments, we propose a temporal difference (TD)method that approximates log density gradient by utilizing backward on-policy samples.Since backward sampling from a Markov chain is highly restrictive we also propose a min-maxoptimization that can approximate log density gradient using just on-policy samples. We alsoprove uniqueness, and convergence under linear function approximation, for this min-maxoptimization. Finally, we show that the sample complexity of our min-max optimization tobe of the order of m1/2, where m is the number of on-policy samples. We also demonstratea proof-of-concept for our log density gradient method on gridworld environment, andobserve that our method is able to improve upon the classical policy gradient method by aclear margin, thus indicating a promising novel direction to develop reinforcement learningalgorithms that require fewer samples.",
  "Introduction": "Policy gradient (PG) methods are a vital ingredient behind the success of modern reinforcement learning (Silveret al., 2017; John Schulman et al., 2023; Haarnoja et al.; Kakade, 2001). The success of PG methods stemsfrom their simplicity and compatibility with neural network-based function approximations (Sutton et al.,1999; Baxter & Bartlett, 2001). Although modern policy gradient methods like PPO and TRPO, haveachieved excellent results in various on-policy tasks (Schulman et al., 2017; 2015), they require extensivehyper-parameter tuning. Additionally, it has been shown by Ilyas et al. (2020) that the estimation errorbetween policy gradient estimated by the methods like PPO and the true policy gradient increases significantlyduring the training process. Classically in reinforcement learning, two types of problems are well known andwell studied: discounted reward and average reward, see for example Sutton & Barto (2018). It is well knownthat for the discounted reward scenario, the Bellman operator is a contraction, and solving the Bellmanequation iteratively is a straightforward numerical procedure. However, the same contraction property failsto hold for the average reward scenario. Therefore, classical policy gradient methods typically approximategradient of the policy using Q-function estimated with discount factor strictly less than 1, which leads to a",
  "Average Rewards": "Classical Policy Gradient (10 x 10)Log Density Gradient (10 x 10)Classical Policy Gradient (5 x 5)Log Density Gradient (5 x 5) : For the average reward scenario, performance of classical policy gradient (blue) algorithm ascompared to log density gradient (green) algorithm over a n n gridworld environment, for n = 5, 10. Weobserve that log density gradient algorithm consistently converges to better policy performance. Theoreticalcalculated solutions are used for implementation. error in gradient estimation (Morimura et al., 2010). In this paper, we empirically demonstrate that thiserror in indeed significant, see for instance, . We further propose a novel algorithm to estimate policygradient that corrects for this residual error, which could potentially lead to sample efficient reinforcementlearning, thus enabling their deployment over a wide variety of complex scenarios. We call our method, logdensity gradient. We show that log density gradient method can be used to estimate the policy gradientfor all values of discounting factor including the average reward scenario. Log density gradient methodis based on the average state-action stationary distribution formulation of reinforcement learning, whichallows for the estimation of policy gradient as a multiplication of the gradient of log density and the rewardfunction (Nachum et al., 2019; Uehara et al.). This separation results in an improved correlation with thetrue policy gradient and requires fewer hyperparameters. We show that our method is consistent with theclassical policy gradient theorem (Sutton, 1988) and also prove convergence properties and sample complexity. Our main contributions are as follows. 1. A novel method to provably calculate policy gradient by using theaverage state-action discounted formulation for all values of the discounting factor. We will show that policygradient estimated in this manner for average reward scenario will correct for the residual error in policygradient estimation, which is widely ignored in empirical implementations of policy gradients (as shown in). 2. A model-free Temporal Difference (TD) method for approximating policy gradient. We provideproof of contraction as well as convergence. However, there is a major drawback that it requires samples fromthe backward Markov chain (described in detail in the paper) which motivates the next contribution. 3. Amin-max optimization which yields the gradient of log density for all values of the discounting factor includingthe average reward scenario and a model free TD method to implement it, with proof of convergence. We alsoshow that this min-max optimization has a closed form solution under linear function class assumptions, thusenabling their practical use with linear MDP problems (Zhang et al., 2022). We additionally show samplecomplexity of the order O(m1/2) for the projected version of the proposed TD method, where m is thenumber of on-policy samples. Our method is competitive with the sample complexity of classical vanillapolicy gradient methods (Yuan et al., 2022). starts with problem formulation and motivation behind this paper. , discusses prior workin policy gradient methods, temporal difference methods, and min-max problems in off-policy evaluationand compares our work with existing works to situate our paper in the literature. Our main contributionsare discussed in detail starting from which starts with rigorously defining log density gradient.Additionally we also propose a TD approach to estimate log density gradient under strict reversibilityassumptions, and we describe the issue caused by this assumption. In section 6, to overcome this issue to",
  "Notation: we let ()T denote matrix transpose, and let e represent the vector of ones, the size of whichwould be clear from context": "We define Markov Decision Process (MDP) as a 6-tuple of (S, A, P, r, , d0). Here, S is a finite state space ofthe MDP, A is a finite action space, P is the transition probability matrix, r is the reward function and d0is the initial distribution. The reinforcement learning problems is optimise for a policy : S (A) thatmaximizes J(), defined as",
  "t=0tr(st, at), s0 d0, at (|st), st+1 P(|st, at)], for = 1,": "where is the discounting factor which accounts for the impact of future rewards in present decisionmaking. When = 1, J1() the scenario is called the average reward formulation. Most practical problemsin reinforcement learning typically aim to solve for an optimal policy = arg max J1()(See Haarnoja et al.). Modern reinforcement learning methods aim to parameterise policy with a set of parameters Rn, where n is the dimensions of the parameter space. We refer to such paremterisation as . This kindof parameterisation enables us search for optimal set of parameters instead of a search over S A whichin practice could be very large. We define",
  "= r(s, a) + EsP(|s,a),a(|s)[Q (s, a)](1b)": "Where, equation 1b is called the Bellman Equation. Bellman equation is popularly used to estimate the Q-function using just empirical data collected on the MDP. Q-function approximation methods typically use < 1for stable estimation of the Q-function. We also similarly define value function V (s) = Ea(|s)[Q (s, a)]. Modern RL algorithms generally solve for by estimating the gradient of policy performance J() withrespect to policy parameters . This is also commonly referred to as the policy gradient theorem J1()(Sutton et al., 1999) which says",
  "Published in Transactions on Machine Learning Research (10/2024)": "In the remainder of this section, we will focus on linear function approximation, and provide an update ruleto solve equation 17 under linear function approximation. For that we choose a feature map : S A Rd and parameters , Rdn that need to be learnt, so that we can approximate the optima of equation 17,w(s, a) and f (s, a) with T (s, a), and T (s, a) respectively, for each state action pair (s, a). The updaterule is",
  "(5)": "In this paper, we prove that this residual error is significant. What more, we also propose another methodto exactly obtain the policy gradient, for all values of the discounting factor including = 1 which we callas the log density gradient. Our estimation of log density gradient utilises average state-action discounteddistributional formulation of a reinforcement learning problem which re-states J() as expectation underd(Nachum et al., 2019; Uehara et al.) as",
  "J() = E(s,a)d [ log d (s, a) r(s, a)].(6)": "We refer to log das the log density gradient. A key advantage of log density gradient is that it wouldallow us to approximate policy gradient for average reward scenarios in a provable manner. In this work, weshow that log density gradient can be approximated even under average reward scenarios ( = 1).",
  "Survey of Related Work and Comparison": "In this section we will discuss existing studies in policy gradient methods including the framework of logdensity gradient first introduced by Morimura et al. (2010). We also briefly discuss density ratio learningmethods which have been very popular in off-policy evaluation. A short discussion on Temporal Difference(TD) learning methods may be found in Appendix 9.1.",
  "Policy Gradient Methods": "Literature survey: Policy gradient methods are a widely studied topic in reinforcement learning. Oneof the earliest works in this area proposed a closed-form solution for evaluating policy gradients called thepolicy gradient theorem (Sutton et al., 1999). Initially implementations for policy gradient methods usedepisodic estimates to update policy parameters (Williams, 1992) and GPOMDP (Baxter & Bartlett, 2001).Unfortunately this way of implementing policy gradient suffered from high variance, thus inhibiting scalabilityto large problem spaces (Schulman et al., 2016). To address this problem, actor-critic methods approximatethe Q-function or advantage function using an additional neural network, which are then used to update thepolicy (Mnih et al., 2016; Schulman et al., 2016). Furthermore policy gradient methods are also designed tobe compatible with deterministic policies (Lillicrap et al., 2016; Silver et al., 2014). Recently Trust regionmethods, such as Trust Region Policy Optimization (TRPO) Schulman et al. (2015) and Proximal PolicyOptimization (PPO) Schulman et al. (2017) have been introduced which update policies while ensuringmonotonic performance improvement. To the best of our knowledge, Log density gradient has only been",
  "discussed in Morimura et al. (2010) in which a TD method to estimate log density gradient for averagereward scenarios by using reversible backward Markov chain is proposed": "Comparison: In our paper, we re-introduce the idea of log density gradient introduced by Morimura et al.(2010) for estimating gradient in the average reward scenario. Morimura et al. (2010) was also the firstwork to find out the residual error in policy gradient approximation (Proposition 2). Additionally, this workproposes estimating log density gradient specifically for average reward scenarios ( = 1) using a TD update,with additional extensions for linear function approximation. Our work not only fixes many technical gaps evident in the theory of log density gradient as proposed byMorimura et al. (2010) but also builds on them to make log density gradient practical. We first define logdensity gradient (equation 10) over a range of discounting factor , which also includes the averagereward scenario. Using Lemma 2 and 3 we then prove mathematical conditions under which the log densitygradient is unique and can be exactly calculated. We further use this relation to propose a TD form ofupdates (equation 14) for log density gradient estimation for all values of discounting factor asagainst the average reward scenario proposed by Morimura et al. (2010). In Lemma 4 we further provethat these TD updates converge to a unique solution for all values of discounting factor except 1. Thus,effectively demonstrating that TD-updates proposed by Morimura et al. (2010) does not converge to thetrue log density gradient, further limiting their use for large scale problems. Additionally, to make log densitygradient estimation viable for practical problems, we propose a min-max optimization approach (equation 17)that allows us to estimate log density gradient using empirical samples. We also demonstrate that under linearfunction approximation settings, this min-max optimization not only has a closed form but also converges toa unique solution (Theorem 1). Under weighted updates, as proposed in algorithm 1 we also show a boundon sample complexity of log density gradient estimation of the order of O( 1n).",
  "Density Ratio Learning": "Literature survey: Off-policy evaluation estimates the performance of a target policy using an offlinedataset generated by a behavior policy (Voloshin et al., 2019; Katdare et al.). This is done by estimatingthe average state-action density ratiod d , which allows approximation of the target policys performance. Inthis work, we are primarily interested in the DICE class of off-policy evaluation algorithms (Zhang et al.,2020b; Nachum et al., 2019; Zhang et al., 2020a). These algorithms typically approximate the divergence(for some fdivergence of their choice) between the two distributions d and d in their convex dual form,eliminating the need to obtain samples from d, which results in a min-max form optimization problem. Comparison: Inspired by the DICE class of algorithms we too propose a min-max form of estimating thelog density gradient. We show that such a method of estimating log density gradient converges to the truepolicy under linear function approximations assumptions. We also show that the sample complexity of suchan estimator is of the order (n1/2), with n being the number of on-policy samples.",
  "Before we begin the technical exposition we would like to give a high level idea of the building blocks involved,and how each step connects to the next": "To that end, we finally propose a min-max version of log density gradient which allows to approximate log d using empirical samples. Our experimental further show that this manner of policy gradientestimation can potentially make reinforcement learning sample efficient, thus helping them scale. In , we first propose a model based approach to approximate the log density gradient for tabularscenarios. To that end, we propose an optimisation problem, the solution to which is the log density gradient.Then we prove some properties about the solution of the optimisation problem, in particular. Next we suggesta temporal difference type method to solve the optimisation problem using samples from the controlledMarkov chain. We prove certain uniqueness and convergence properties about the solution obtained fromthe TD method. However, the TD method works under a very restrctive assumption: it requires backwardsamples from the Markov chain. To circumvent this assumption, we proceed to the next section.",
  "Log Density Gradient": "In this section, we introduce log density gradient approach to policy gradient estimation. By deriving thegradient of log density, we demonstrate that traditional policy gradient methods are not well-suited atapproximating policy gradient through average reward scenarios. The log density gradient approach addressesthis limitation, allowing us to estimate both the average reward and discounted reward formulations. We firstpropose a model-based approach to log density gradient estimation which relies on Bellman-type recursionto exactly calculate policy gradient. We then extend this to a Temporal difference (TD) version of policygradient estimation which will allow us to estimate gradient only using on-policy samples.",
  "s,ad (s, a)P(s|s, a) (7)": "A detailed proof for the same can be found in the Appendix 9.3. Note that the Bellmam flow equation 7equation is similar to the classical Bellman contraction except that it is reversed. Unlike the classical Bellmancontraction, the Bellman flow equation shows us the relationship of the next state s with previous state-actionpair (s, a). In that sense, the Bellman flow equation is reversed and requires samples from a reversed MDP toestimate d accurately. In the following Lemma, we present an optimisation problem, the solution of which issame as the solution of equation 7. This optimisation problem will be used in later development.",
  "s w(s) 1)2 is redundantfor < 1, and only becomes useful for average reward scenarios wherein = 1 thus helping ensure uniqueness": "Recall that we are interested in estimating the log density gradient, which is the gradient of the log of densitywith the policy parameters log d . Similar to the Bellman flow equation equation 7, the log densitygradient also follows a similar recursion which can be obtained by taking the gradient of the Bellman flowequation 7 with respect to the policy parameters as follows:",
  "E(s,a)d [w(s, a)]2": "is redundant for < 1 and only becomes useful for average reward scenarios = 1. For a finite state andaction space, the proof follows from the fact that the optimal solution to equation 11 requires solving fora linear equation A w = b. The remaining part of the proof demonstrates that this matrix A is invertiblefor . It is worth reiterating that, once we have an estimate log d , we can use this estimate toapproximate the policy gradient using equation 6. We will now recall two important properties of log densitygradient.",
  "J() = E(s,a)d [Q (s, a) log (a|s)],": "Detailed proof for this proposition can be found in Appendix 9.2. In essence this results shows us thatlog density gradient approach to policy gradient estimates the exact same gradient but using a differentformulation. Additionally log density gradient formulation allows us to estimate policy gradient for averagereward formulations. It is well known that, the classical policy gradient theorem is in-sufficient to estimategradient for average reward scenarios. This is because the Bellman equation for reward case ( = 1) isnot a contraction, making it harder to approximate Q-function using neural networks. As a compromise,reinforcement learning practioners typically approximate policy gradient for average reward scenario usinga discounting factor slightly less than 1 and use that to update policy. Our next result shows that such away to approximating policy gradient ignores a residual error, which is not necessarily small. Correctingfor this residual error, can not improve policy gradient estimation but also potentially help improve samplecomplexity of reinforcement learning algorithms.",
  "Temporal Difference Log Density Gradient": "In this section, we begin to estimate log density gradient algorithm using just on-policy samples. To that end,as a first step, we propose a temporal difference approach to log density gradient estimation.We refer toour method as TD(0) method. 1 To get an update equation for our TD(0) method, consider a re-arrangedversion of equation 10 for log density gradient,",
  "Min-Max Log Density Gradient": "In this section, we propose min-max optimization approach to evaluate log density gradient for all values ofthe discounting factor including the average reward scenario ( = 1). Doing so removes the need for samplesfrom backward distribution, which was limiting scalability of log density gradient method described in theprevious section. Min-max optimizations also allow us to use a large variety of function classes like neuralnetworks to approximate log density gradient. Let us return to the loss function that we initially propose in equation 11. Classical machine learning algorithmsusually posit loss function as Empiricial Risk Minimization (ERM), which allows us to approximate the lossusing samples from that distribution. In order to bring our key optimization in an ERM form, consider amodified form of the optimization proposed in equation 11 (the modification is that (s, a) is divided byd (s, a) where the ergodicity assumption ensures this operation is well defined),",
  "t+1 = t + t((Tt t t))(18d)": "where, t := (st, at) is the feature encountered at time t, gt := log (at|st), and t := t(st, at)for (st, at) d , st P(|st, at), at (at|st). We first re-write the updates in equation 18 in form ofdt = [t, t, Tt ] so that the updates can be written in matrix form dt+1 = dt + t(Gt+1dt + ht+1), where,Gt+1, ht+1 are as follows,",
  "B0": "Here, each column of R|S||A|n is the feature vector (s, a), for each (s, a) S A and e Rn is a vectorof 1s at every element. We can similarly write A = D(I P)T , B = DGT , C = DT andEp[] := E(s,a)d ,sP(|s,a),a(|s)[]. We can now prove the convergence of linear function approximationunder the following key assumptions.Assumption 1.1. The matrix has linearly independent columns.",
  "The detailed proof is provided in Appendix 9.8. The proof is similar to (Zhang et al., 2020b, Theorem 2)and invokes theorem 2.2 Borkar & Meyn (2000)": "We provide a sample complexity analysis for a projected version of the update rule equation 18. To that end,we propose Algorithm 1 called the Projected Log Density Gradient. We choose closed, bounded and convexsets X Rdn, Y Rdn, Z R1n and define a projection operator X, Y , Z that project our variablest, t, t onto X, Y, Z respectively. Moreover, we choose a learning rate {t}mt=1 where we run the algorithmfor m steps. The details of the choice of learning rate are found in Appendix 9.9.Theorem 2. Under assumptions 1 for (, , ) obtained from Algorithm 1 after m steps, the optimality gapg(, ) (defined below) is bounded with probability 1 as follows,",
  "Experiments": "In this section, we present a proof of concept for our log density gradient estimation on two sets ofenvironments 5 5 and 3 3 gridworld environment (Towers et al., 2023). For the gridworld experiments, weapproximate log density gradient by using linear function approximation (Algorithm 2). Here, the featuresare : S A R|S||A| such that it maps every state to the corresponding standard basis vector. Our resultsfor 5 5 are in and for 3 3 in . We compare our algorithm against 3 different baselines. The first is theoretical log density gradient asdescribed in Lemma 3. The second baseline implements REINFORCE algorithm, which is the practicalrendition of the policy gradient theorem (Williams, 1992). The third is theoretical policy gradient methodwhich exactly computes the classical policy gradient theorem, as in equation 4b (Sutton et al., 1999). We observe in that both log density gradient approaches are more sample efficient than both policy gradientapproaches. This is because policy gradient methods approximate the gradient for average reward scenarios( = 1) by estimating a Q-function for a discounting factor less than 1. Moreover, we observe that ourmethod tends to outperform REINFORCE with much reduced variance. Our approach is always very closein performance to the theoretical log density gradient which serves to validate correctness of our algorithm.In 5 5 gridworld we also observe our algorithm to outperforms theoretical log density gradient. This isbecause, theoretical log density gradient suffers from some numerical computation issues arising from averagereward scenarios.",
  "Conclusion and Future Work": "We present log density gradient algorithm that estimates policy gradient using state-action discountedformulation of a reinforcement learning problem. We observe that policy gradient estimated in this manner,corrects for a residual error common in many reinforcement learning tasks. We show that with a knownmodel, we can exactly calculate the gradient of the log density by solving two sets of linear equations. Wefurther propose a TD(0) algorithm to implement the same, but it needs samples from the backward Markovchain, which becomes too restrictive. Therefore, we propose a min-max optimization that estimates logdensity gradient using just on-policy samples. We not only prove theoretical properties like convergence anduniqueness but also experimentally demonstrate that our method is sample efficient as compared to classicalpolicy gradient methods like REINFORCE. This approach looks promising, and further studies of log densitygradient will focus on scaling their performance to complex tasks.Limitations: Currently most of our experimental results require Linear function approximation to estimatelog density gradient. This is because under linear function approximation conditions, our min-max optimizationequation 17 becomes a quadratic program and thus has a closed form solution. To scale log density gradient",
  "Katdare, P., Liu, S., and Campbell, K. D. Off environment evaluation using convex risk minimization. In2022 International Conference on Robotics and Automation, ICRA 2022": "Lillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., Silver, D., and Wierstra, D. Continuouscontrol with deep reinforcement learning. In 4th International Conference on Learning Representations,ICLR, 2016. Liu, B., Liu, J., Ghavamzadeh, M., Mahadevan, S., and Petrik, M. Finite-sample analysis of proximalgradient TD algorithms. In Meila, M. and Heskes, T. (eds.), Proceedings of the Thirty-First Conference onUncertainty in Artificial Intelligence, UAI 2015."
}