{
  "Abstract": "Large language models have demonstrated remarkable few-shot in-context learning (ICL)capabilities, adapting to new tasks with few-shots demonstrations. However, the efficacyof ICL is highly dependent on the selection of these demonstrations. Recent developmentshave introduced retrieval-based in-context learning (RetICL), which dynamically retrievesdemonstrations tailored to each input query. This approach leverages existing databases andretrieval systems, enhancing efficiency and scalability while mitigating biases inherent inmanual example selection. Given the promising results and growing interest in RetICL, wepresent a comprehensive survey of this field. Our review encompasses: design choices forICL demonstration retrieval models, retrieval training procedures, inference strategies andcurrent applications of RetICL. In the end, we explore future directions for this emergingtechnology.",
  "Introduction": "Few-shot in-context learning (ICL) is the ability of large language models (LLMs) to perform a new task whena few input-output examples, or demonstrations, for the new task are given alongside the actual task input.Importantly, the model parameters do not have to be fine-tuned towards the new task. ICL is popularizedby the work on pre-trained large language models, which can perform ICL without being trained to doso (Brown et al., 2020), though smaller language models can also be explicitly trained to perform ICL (Minet al., 2022a). ICL presents several advantages over the conventional methodology for adapting language models to adownstream task, which typically involves initial pre-training followed by subsequent fine-tuning. Onesignificant merit of ICL is the circumvention of fine-tuning, which might not always be possible due to limitedaccess to the model parameters or constraints on computational resources (Brown et al., 2020). Furthermore,ICL avoids common issues associated with fine-tuning, such as overfitting (Ying, 2019; Kazemi et al., 2023a).Compared to parameter-efficient fine-tuning methods (PEFT) (Hu et al., 2021; Dettmers et al., 2023; Lesteret al., 2021), ICL is computationally cheaper and remain the model parameters unchanged thus preservingthe generality of the LLMs.",
  "Published in Transactions on Machine Learning Research (10/2024)": "Adina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge corpus for sentenceunderstanding through inference. In Proceedings of the 2018 Conference of the North American Chapter ofthe Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers),pp. 11121122, 2018. Tomer Wolfson, Mor Geva, Ankit Gupta, Matt Gardner, Yoav Goldberg, Daniel Deutch, and Jonathan Berant.Break it down: A question understanding benchmark. Transactions of the Association for ComputationalLinguistics, 8:183198, 2020.",
  "Few-shot In-context Learning for Language Models": "Language models (LMs) (Zhao et al., 2023; Rosenfeld, 2000; Jurafsky, 2021; Radford et al., 2019; Raffel et al.,2020; Lewis et al., 2019; Touvron et al., 2023) are probabilistic models that assign probabilities to sequencesof words and are essential components in many tasks. Let s represent a sequence of words (e.g., a sentence)and w1, w2, . . . , wn represent the tokens in the sequence. Based on the chain rule, the probability p(s) can bedecomposed into the following product of probabilities:",
  "k=1p(wk | w1, . . . , wk1)": "where each element in the product corresponds to the probability of a token given the previous tokens. Basedon the above decomposition, an LM can be constructed by learning the probability of the next token giventhe previous ones. Earlier LMs were mostly based on N-gram models (Jurafsky, 2021) and has been used in computingapproximations to English word sequences (Shannon, 1948) and speech recognition system (Baker, 1990;Jelinek et al., 1975; Baker, 1975; Bahl et al., 1983; Jelinek, 1990).N-gram models are based on the",
  "In DomainTC, SAContrastive LearningTop-k": ": Comparison with Related Work. Abbreviation for Evaluation Tasks: CodeGen (code generation),SA (sentiment analysis), Text (Table to Text generation), QA (question answering), SP (semanticparsing), DST (Dialogue State Tracking), D2T (Data-to-Text), Summ ( Summarization), CSR ( commonsensereasoning), RC (reading comprehension), NLI (natural language inference), CR (Coreference Resolution),MathR (mathematical reasoning), PD (paraphrase detection), TQA (Table Question Answering), TC (TopicClassification), StoryGen (Story Generation), MT (Machine Translation) Markovian assumption that the next token only depends on the recent context (Markov, 1913). Based on thisassumption, p(wk | w1, . . . , wk1) is approximated, e.g., by p(wk | wk2, wk1) in the case of a bi-gram model;p(wk | wk2, wk1) is then approximated statistically based on the number of times wk appeared after wk2in a large corpora of text, wk1 divided by the total number of times wk2, wk1 appeared in the corpora. With the advent of word embeddings (Bengio et al., 2000; Mikolov et al., 2013), neural approaches to languagemodeling gained more popularity, in which a neural network is used to predict the next token probability.The use of powerful neural networks such as feedforward network (Bengio et al., 2000), recurrent neuralmodel (Mikolov et al., 2012), specifically, long-short term memory (LSTM) models (Hochreiter & Schmidhuber,1997) and Transformer models (Vaswani et al., 2017) allowed for predicting the next token probability basedon a much longer and a variable length context, thus enabling better estimation of p(wk | w1, . . . , wk1). The increased power of neural LMs led to a new learning paradigm for NLP problems. Historically, thedominant learning paradigm for NLP problems was to train models on task-specific data from scratch.Consequently, for each new task, the model had to learn everything from scratch. This often resulted in poorgeneralization, especially in the cases where previously unobserved vocabulary was observed at the test time.In the subsequent paradigm, an LM was first pre-trained on a large corpora of text making it learn abouthow language works and gain a vast amount of knowledge about the world (Petroni et al., 2019; Lin et al.,2020; Sung et al., 2021; Yuan et al., 2023); the pre-trained LM (PLM) was then further finetuned on datafrom the new tasks (Sarzynska-Wawer et al., 2021; Devlin et al., 2018) thus teaching the general PLM thespecifics of the new task. This paradigm often resulted in faster learning and higher predictive performance.It was later shown that further finetuning a PLM on multiple tasks leads to better transfer of knowledgeacross tasks and may lead to better performance on new tasks (Raffel et al., 2020).",
  "In-Context Learning": "As the scale of the PLMs and the scale of the datasets on which these models were pre-trained increased leading to pre-trained Large Language Models (LLMs), it was discovered that pre-trained LLMs (hereafter,referred to as LLMs for brevity) have a remarkable capability of learning in-context from a few demonstrations(Brown et al., 2020). That is, LLMs were shown to be able to adapt to new tasks by only seeing a fewexamples of the new task in their input, as opposed to needing additional training data or fine-tuning. Thisis typically referred to as few-shot in-context learning. In the following, we will illustrate this formally. Let T be a task and q T represent a sample query from this task for which we would like to find an answerusing an LLM. In the case of few-shot learning, we find or construct multiple demonstrations {d1, . . . , dk}where each demonstration di = (qi, ai) contains a query qi T and the answer ai to that query, and feed aninput of the form",
  "Demonstration 1: Query: q1, Answer: a1...Demonstration k: Query: qk, Answer: akDemonstration k + 1: Query: q, Answer:": "The demonstrations serve as a few examples of the task helping the LLMs learn both the input and labelspace as well as the mapping from the inputs to the labels (Wei et al., 2023; Pan, 2023) both in context(without any weight updates), so a similar mapping can be applied to q. Few-shot learning is a remarkablecapability of LLMs, showcasing their generalization ability to rapidly adapt to a wide range of tasks withexplicit instruction. While LLMs show strong few-shot learning capabilities off-the-shelve, it has been shownthat warming them up by finetuning them on few-shot data from multiple tasks will further boost theirfew-shot learning capability (Min et al., 2022a; Chen et al., 2022; Radford et al., 2019). Another remarkable ICL capability of LLMs is to learn from in-context instructions: finetuning LLMs oninstructions from multiple tasks makes them learn to follow instructions for new tasks (Ouyang et al., 2022;Longpre et al., 2023; Zhang et al., 2023). In this case, commonly known as instruction tuning, the LLM isfinetuned on data of the type IT , qT , a where IT represents the instructions for a task T describing how thetask should be performed, qT represents a query from task T and a represents the answer. The finetuning isperformed on data from multiple tasks and multiple queries from each task. It is also possible to combineinstructions with few-shot demonstrations, in which case an example prompt is as follows:",
  "[Task instructions][Prompt 2]": "Benefits of ICL:Compared to the aforementioned approach of utilizing LLMs which involves pre-trainingfollowed by fine-tuning, ICL offers several key advantages. Firstly, fine-tuning may not always be feasible dueto restricted access to the LLM, inadequate computational resources, or inadequately labeled data (Brownet al., 2020), whereas ICL requires fewer resources, less data, and is easier to serve through API calls.Additionally, ICL avoids the issues commonly associated with fine-tuning, such as overfitting or shocks (Ying,2019; Kazemi et al., 2023a), as it does not modify the models parameters, allowing it to remain general.",
  "What Makes for Good Demonstrations?": "Several works try to provide theoretical justifications and insights into how LLMs learn from a few in-contextdemonstrations (Xie et al., 2021; Garg et al., 2022; Von Oswald et al., 2023). However, the exact reasonsbehind this capability are still largely unclear making it difficult to select optimal few-shot demonstrations.Fortunately, various empirical results show the effect of the few-shot demonstrations on the predictive accuracyof the LLMs and provide suggestions on the best practices for preparing them. They also show the brittlenessof the LLMs in the choice, format, and order of the few-shot demonstrations. Here, we describe some of themore prominent ones. Number of Demonstrations:LLMs generally benefit from more demonstrations, but as the number ofdemonstrations increases the rate of improvement typically decreases (Brown et al., 2020; Ye et al., 2023b;Min et al., 2022b). Generation tasks have been shown to benefit from an increased number of demonstrationsmore than classification tasks (Li et al., 2023b). Toward increasing the number of demonstrations, one barrieris the maximum context size of the LLM. While the size of the context has been increasing over time withnewer LLMs (Team et al., 2024; Chen et al.; Reid et al., 2024; Peng et al., 2023a; Gu & Dao, 2023; Li et al.,2024), it may still be problematic for datasets with long input texts or classification datasets with manyclasses. Demonstration Formatting:Various works have shown that the formatting and wording of the promptscan play a crucial role in the performance of the LLM (Jiang et al., 2020; Shin et al., 2020; Kojima et al.;Yang et al., 2023). For example, Kojima et al. shows that simply adding Lets think step by step to theprompt makes LLMs reason step by step and solve substantially more problems, and Weller et al. (2023)show that adding According to Wikipedia to the prompt makes them more factual. Moreover, Min et al.(2022b) shows that besides the text formatting, the label space and the distribution of the input text in thedemonstrations are also of immense importance. Order of Demonstrations:The order of demonstrations has been shown to substantially affect the modelperformance. For example, Lu et al. (2022b) show that on some tasks, the model performance can rangefrom near-random to state-of-the-art depending on the order of the prompts, and Zhao et al. (2021) showthat answers appearing toward the end of the prompt are more likely to be predicted by the model. Diversity of Demonstrations:Another important factor in the success of few-shot learning is thediversity of the demonstrations (Li et al., 2022b). Zhang et al. (2022b) propose to select a diverse set ofquestions as few-shot examples. Ma et al. (2023) propose a fairness metric for selecting demonstrations whichencourages selecting diverse few-shot demonstrations that produce a near uniform predictive distribution fora semantic-free input. These works have shown that the diversity of demonstrations is crucial for achievingbetter in-context learning (ICL) performance. Chain of Thought (CoT):It has been shown that including a rationale for the answer significantlyimproves model performance, especially for models that are larger than a certain size (Suzgun et al., 2022).The rationale is commonly known as chain of thought (CoT) (Wei et al., 2022). In the case of CoT prompting,the demonstrations are typically formatted as:",
  "Query : qi,Rationale : ri,Answer : ai": "with the rationale appearing before the final answer. Naik et al. (2023) found that the diversity of reasoningpath is important as well, therefore they propose DiversePrompting where an LLM is prompted to generatediverse reasoning path to solve a problem. Several works have investigated the reason behind the efficacy ofCoT prompting and how to improve the prompts and rationales (Wang et al., 2022a; Lanham et al., 2023).",
  "Retrieval Objectives: Similarity and Diversity": "Various retrieval objectives for selecting and tailoring in-context examples for LLMs have been explored (Luoet al., 2023; Rubin et al., 2022; Ye et al., 2023a; Dalvi et al., 2022; Cheng et al., 2023; Li et al., 2023b).There are two primary retrieval objectives for selecting demonstrations: similarity and diversity. Similarityinvolves selecting demonstrations most akin to the query and can be based on language similarity (termmatching (Luo et al., 2023; Rubin et al., 2022; Agrawal et al., 2022; Ye et al., 2023a; Dalvi et al., 2022) orsemantic matching (Rubin et al., 2022; Li & Qiu, 2023b; Wang et al., 2023a; Liu et al., 2022)), structuralaspects (sentence structure (Poesia et al., 2021; Levy et al., 2022; Drozdov et al., 2022), reasoning structure (Fuet al., 2022)). Most studies focus on language similarity, with fewer addressing structural similarity, oftendue to the challenges in extracting a querys structure in many tasks (Levy et al., 2022). Beyond similarity,some work has found that the diversity of demonstrations is important. The motivations for diversity includeavoiding repetitive demonstrations (Zhang et al., 2022b), bringing different perspectives (Yu et al., 2023),and maximizing the demonstrations coverage of the test query, in terms of covering either its words orsyntactic structures (Levy et al., 2022). Measuring the diversity of multiple demonstrations is a majortechnical challenge. Ye et al. (2023a) applied determinantal point processes (DPP) a probabilistic modelto measure the negative interaction (Kulesza et al., 2012), to measure the diversity.Levy et al. (2022)found that diversity and coverage are important when the model is unfamiliar with the output symbolsspace. It is noteworthy that researchers have found that ICL benefits more from demonstrations with highercomplexity in some scenarios (Fu et al., 2022), where they define the complexity in terms of the query lengthor reasoning steps. However, Fu et al. (2022) employed heuristic rules to define complexity and pre-selecteddemonstrations accordingly. Their research revealed that using a similarity-based retriever led to improvedperformance in a specific mathematical reasoning task. This might indicate that combining similarity andcomplexity considerations could be a promising strategy for enhancing the approach to reasoning tasks.",
  "This section explores various strategies for employing a retriever to gather k demonstrations. We divide theseinto three distinct methodologies": "Top-k RetrievalThis is the simplest and most popular retrieval strategy (Liu et al., 2022; Rubin et al.,2022; Li et al., 2023b; Luo et al., 2023; Gao et al., 2023). To obtain k demonstrations, given a query, theretriever ranks the demonstrations and then selects the top-k demonstrations. Thus, each demonstration ischosen independently of the others. This method is straightforward and fast, however, it might not yield thebest combination of k demonstrations as these demonstrations might be homogeneous. Clustering RetrievalTo mitigate the issue of homogeneity in one-hot retrieval, clustering retrievalapproaches (Li et al., 2022a; Zhang et al., 2022b; Li & Qiu, 2023b) categorize all demonstrations into ksub-groups aiming to group similar demonstrations together. Then given a query, the retriever picks the mostsimilar demonstration from each sub-group resulting in a final set of k demonstrations. The core principle of",
  "Retrieval Corpus": "The retrieval corpus forms a pool of demonstrations that the retriever can access. Using annotated data isone of the most straightforward ways to construct the retrieval corpus. This setting assumes that trainingdata related to a task is available, and thus can be used as the retrieval corpus. Under this setting, there arethree main ways to construct the corpus that we will discuss individually below. In-DomainIn this setting, an in-domain training set, independently and identically distribution (IID) withthe test queries, is available and serves as the retrieval corpus. Most existing work take the full training setas the corpus. However, to be more annotation efficient, Hongjin et al. (2022) uses only a subset M of thetraining set N which includes the most representative and diverse ones, where |M| << |N|. One questionthat remains unanswered from the work of Hongjin et al. (2022) is how the predictive performance is affectedas a function of retrieving from a subset M instead of the entire training set N. While there is no follow-upwork to answer this question, the closest comparison we find is the results in Ye et al. (2023a) where a similarsetup as Hongjin et al. (2022) is used except that they use the entire training set as the retrieval corpus, andreport lower performance on the SST-5 dataset (compare the in Hongjin et al. (2022) and in (Ye et al., 2023a)). While there might be other differences (e.g. the number of the demonstrations andthe templates being used to do the inference) between the two setups that may affect the final performance,this comparison implies that retrieving from a carefully selected subset might have comparable results toretrieving from the entire training set. Mix-DomainThe previous scenario has one individual retrieval corpus for different tasks. Assuming thatwe want to test model performance on two tasks, then in the in-domain setting, there will be two retrievalcorpora separately. Furthermore, the in-domain setting assumes that the model has knowledge about whichtask the test question belongs to such that when it comes to the retrieval phase, it knows which corpus toselect the demonstrations from. However, this assumption does not hold in several real-world applicationsof LLMs. In the mix-domain setting (Wang et al., 2023a), the retrieval corpus is constructed from thecombination of all tasks. At the inference time, given a question, the retriever will retrieve demonstrationsfrom this mixed corpus; the demonstrations can come from the same domain as the test question or fromother tasks. However, the authors have not discussed whether the mix-domain approach is more beneficialthan the single-domain approach. Li et al. (2023b) propose a unified retriever trained on a mixture of 40 tasks.This unified retriever can be used for different tasks during inference. The authors have compared the unifiedretriever with the single-domain retriever (Rubin et al., 2022) and demonstrated improved performance.Its worth noting that while the unified retriever is trained on mix-domain data, the actual selection ofdemonstrations during inference depends on the specific implementation.",
  "Off-the-shelf Demonstration Retrievers": "To achieve the retrieval objectives outlined above, researchers have explored various types of demonstrationretrievers Robertson et al. (2009); Reimers & Gurevych (2019); Liu et al. (2019); Yang et al. (2019); Ni et al.(2021); Santhanam et al. (2021). A typical demonstration retriever encodes examples from the retrieval corpusand the query into some vector representations, and then a similarity measure (e.g. cosine similarity) iscalculated between candidate demonstration embeddings and the query embedding to locate the most relateddemonstrations Rubin et al. (2022); Li & Qiu (2023b); Wang et al. (2023a). Given the limited understandingof the underlying mechanism through which retrieved demonstrations enhance the performance of LLMs,initial research efforts focused on a heuristic evaluation of readily available retrievers for this task (Liuet al., 2022; Zhang et al., 2022b). Subsequent research endeavors explored the design and developmentof learning-based retrievers specifically customized for retrieving demonstrations (Luo et al., 2023; Chenget al., 2023; Li et al., 2023b). This section reviews representative off-the-shelf models and we will discuss thelearning-based models in . Term-based SimilarityBM25 (Robertson et al., 2009) is one of the most popular term-based scoringmethods due to its simplicity and effectiveness in producing relevant results. It takes into account bothterm frequencies and document lengths. It has been empirically demonstrated in various works (Luo et al.,2023; Rubin et al., 2022; Agrawal et al., 2022; Ye et al., 2023a; Dalvi et al., 2022) that using BM25 to selectsimilar examples as few-shots in ICL can help improve the performance of many LLM inference tasks. WhileBM25 has become a standard baseline model in the field, it is not without its limitations. Due to its heavy",
  "Fine-tuned Demonstrations Retrievers": "Although off-the-shelf retrievers have shown some promise in retrieving demonstrations for LLMs, the retrieveddemonstrations given by the off-the-shelf retrievers might not represent the nature of the task and how thetask should be solved in general. Therefore, it might lead to sub-optimal performance. Researchers thushave started to explore learning-based methods to further push the boundaries. A typical objective whendesigning a good demonstration retriever is: if an LLM finds a demonstration useful when being used as anillustrative example, the retriever should be encouraged to rank the demonstration higher. This allows us totrain models directly relying on signals from query and output pairs in the task of interest, without humanannotations. To develop a demonstration retriever, the majority of approaches utilize current dual encodermodels (Karpukhin et al., 2020; Ni et al., 2021). The key variations lie in the methods of gathering trainingdata and formulating training objectives. We will explore these aspects in more detail in the subsequentsections.",
  "Collecting Training Data for Demonstration Retriever": "Based on LLMs SignalsA popular approach to collecting training examples is to use the supervisorysignals from LLMs. In this case, a typical paradigm is to first employ some filtering mechanisms (Chenget al., 2023) or unsupervised retrievers (e.g. BM25 and SBERT) (Luo et al., 2023) as the initial retriever,this step can help limit the pool size for mining the right training data. Then a scoring LLM, which serves asa proxy for the inference LLM, is used to score each candidate demonstration d. Here the score is definedas s(e) = p(a|d, q) which is the conditional probability of output answer a given the input query q anddemonstration d. Another approach is to train a smaller reward model that can provide more fine-grainedsupervision for dense retrievers. For example, Wang et al. (2023a) proposed to finetune a cross-encoder modelserving as a teacher model for training the retriever. Once a score is obtained, a retriever can be trained that predicts these scores directly (Ye et al., 2023a).Alternatively, the candidate demonstrations can be ranked for each query based on their scores, consideringthe top-ranked demonstrations as positive examples that help the LLM get to the right answer and thebottom-ranked ones as negative examples that mislead the LLM towards the wrong answers; then a retrievercan be trained which separates positive examples from negative examples (Rubin et al., 2022; Cheng et al.,2023; Luo et al., 2023). There are different strategies for choosing the scoring LLM. Ideally, one uses the inference LLM itself asthe scorer in order to perfectly reflect its preferences (Li et al., 2023b; Shi et al., 2022). However, trainingretrievers requires large amounts of labeled data, and it may be expensive use very large models for labeling.Consequently, for scoring one may gravitate towards utilizing smaller models, especially those within thesame model family as the inference LLM (Luo et al., 2023; Cheng et al., 2023; Rubin et al., 2022). Model-FreeOne approach to collecting training data for demonstration retriever is to directly measurethe similarity between the labels of the candidate demonstrations and the label of the query, and use thissimilarity as a proxy of the importance of a demonstration (Hu et al., 2022; Poesia et al., 2021). For instance,Hu et al. (2022) explored a dialogue context where labels are structured as a sequence of stages. The similaritybetween a querys label and a demonstrations label is determined by calculating the average F1 scores ofthese two labels. This method adopts a heuristic approach (i.e. stage changes), presuming that the similaritymetric can closely resemble the preference for good demonstrations from an LLM, and it often necessitatesdomain-specific expertise for design.",
  "log(1 + esim(qi,dj)sim(qi,di))": "where sim(q, d) is the relavance between a candidate demonstration d and the input q. In the list-wise rankingobjective, retriever can benefit from the full ranking of the candidate set to make accurate predictions forthe most relevant demonstrations. However, obtaining the full ranking list and calculating the loss functionon top of it might be very expensive and time-consuming. Additionally, the model is trained to discern therelative preferences between examples without explicitly determining whether an example can serve as anabsolute good demonstration. InfoNCE LossAnother widely adopted training procedure is contrastive learning using the InfoNCEloss (Rubin et al., 2022; Cheng et al., 2023; Luo et al., 2023). When positive and negative examples can becorrectly identified, InfoNCE loss is an effective loss function because it can take advantage of the supervisorylabels to produce a representation that sets apart the useful examples for demonstration retrieval. In thisapproach, each training instance is given in the form of < qi, d+i , di,1, ...di,k >. Here d+i is a selected positiveexample concerning the input qi, and the negative examples consist of one hard negative example di,1 and krandom examples from the other instances in the same mini-batch. Then the typical contrastive loss can bedefined as",
  "The random negative examples from the same mini-batch are called in-batch negatives. They are typicallyselected from both the positive examples and hard negative examples of other instances": "Distillation by KL DivergenceYe et al. (2023a) claims that although the InfoNCE loss has been foundeffective in training demonstration retrievers and can learn which examples might be superior to others, it hasthe same treatment for all negative examples and the predicted scores from LLM are not fully utilized. As analternative to train a demonstration retriever using positive and negative examples, Shi et al. (2022) proposedto train the retriever by directly distilling the LLMs scoring function. More specifically, the retriever modelis designed to produce ranking scores that match the usefulness of a demonstration to help with the LLMinference; this is done by minimizing the KL-divergence between the top K examples score distribution fromscoring LLM and the ranking score distribution produced by the retriever",
  "pretriever(dk)": "Multiple ObjectivesThe training signals, especially the LLM scores, are typically rich enough to definemultiple training objectives that can be ensembled. For instance, Wang et al. (2023a) proposed to train thedemonstration retriever model with combined objectives: (1) knowledge distillation from the trained rewardmodel which can capture the preferences of LLMs over the retrieved candidates (2) InfoNCE-based contrastiveloss to incorporate the in-batch negatives. More specifically, the resulting loss function is as follows:",
  "Lcombined = Lcont + Ldistill": "Here is a constant that controls the relative importance of the two losses. They claimed that with themulti-objective function, both the absolute scores and supervised signals are taken into consideration. Anotherexample is Li et al. (2023b), who trains a universal retriever with both the list-wise ranking loss and InfoCNEloss. Both losses show additive effects on the model.",
  "Summary": "Here, we summarize the advantages and disadvantages of various retriever models. The off-the-shelf retrieversare easy to use without any downstream task finetuning and typically demonstrate stronger performance thanrandom demonstrations. One exception is in commonsense reasoning tasks where Zhang et al. (2022b) and Yeet al. (2023a) found that for these tasks, random demonstrations are consistently better than retrieval-basedmethod. Cheng et al. (2023) also show that retrieved demonstrations harm commonsense reasoning andcoreference resolution tasks. Among the three categories of off-the-shelf retrievers, sparse retrievers such asBM25 are more index-efficient. This feature becomes particularly valuable when dealing with large volumesof demonstrations and limited hardware memory, making BM25 a preferable choice under such circumstances.In contrast, sentence-embedding similarity-based methods and dual-encoder-based retrieval systems, whichare trained on language tasks, excel in capturing more semantically focused retrieval. Regarding performance,Luo et al. (2023) compared BM25 with dual encoder (GTR) across 5 tasks, and they found that the averageperformance of these two is very similar (within 0.5% difference), and BM25 outperformed the dual encoderin some tasks and vice versa. In another study, Ye et al. (2023a) observed a similar trend highlighting thatno single retriever consistently outperforms others across different tasks. Both Rubin et al. (2022) and Liet al. (2023b) found that BM25 is better than SBERT on semantic parsing tasks, while Li et al. (2023b)found that SBERT is better than BM25 on sentiment analysis tasks. Nevertheless, fine-tuned retrievers demonstrate superior performance compared to their off-the-shelf counter-parts. The main drawback of fine-tuned retrievers lies in the high cost of obtaining training data. Additionally,the common practice of employing task-specific retrievers complicates the system and limits its generalizability,though there were recent works attempting to address this. For instance, Li et al. (2023b) trained a universalretriever that shows stronger performance than task-specific demonstration retriever on most of the tasks. The choice of training objectives for fine-tuning a retriever depends on the specific goals, dataset characteristics,availability of labeled positive and negative examples, and computational constraints. For general retrievaltasks, when absolute relevance score is important, KL divergence is the ideal solution since it is optimized fordistribution alignment. If there is a way to clearly differentiate positive examples from negative examples,InfoNCE is computationally cheap and works well, but as it does not emphasis absolute relevance, the orderingof the retrieved documents may be not as accurate. List-wise ranking loss optimizes the entire ranking order",
  "Applications": "The effectiveness of retrieval-based ICL has been showed in four categories of tasks: 1) natural languageunderstanding, 2) reasoning, 3) knowledge-based QA, and 4) Text generation. We discuss each categorybelow. Natural language understanding tasks that benefit from RetICL include sentiment analysis (SA) (Socher et al.,2013; Zhang et al., 2015; Go et al., 2009), paraphrase detection (PD) (Dolan et al., 2004; Zhang et al., 2019),reading comprehension (RC) (Rajpurkar et al., 2016; Khashabi et al., 2018; Clark et al., 2019; Khashabiet al., 2018; Clark et al., 2019; Mihaylov et al., 2018), and natural language inference (NLI) (Williams et al.,2018; Wang et al., 2018; Bowman et al., 2015a; De Marneffe et al., 2008). Specially, RetICL shows noticeableimprovements on SA and NLI tasks (Liu et al., 2022; Ye et al., 2023a). Reasoning tasks that benefit from RetICL include mathematical reasoning (Cobbe et al., 2021; Lu et al., 2022a;Ling et al., 2017), commonsense reasoning (CSR) (Talmor et al., 2019; Zellers et al., 2019; Bisk et al., 2020;Roemmele et al., 2011),and ethical Reasoning (Jiang et al., 2021). Such tasks are usually accompanied byCoT. Luo et al. (2023) demonstrate that retrieved demonstrations can be combined with the CoT techniqueto further enhance performance in mathematical reasoning tasks, showing that RetICL improves on top ofCoT. Zhang et al. (2022b) emphasize the importance of diversity for these tasks. The iterative retrievalstrategy, as noted by (Scarlatos & Lan, 2023), shows the most significant improvement in mathematicalreasoning. Conversely, some studies have found that retrieval-based demonstrations perform worse thanrandom demonstrations in commonsense reasoning tasks, such as CMSQA. In Knowledge-based QA, external knowledge is required to answer the question (Berant et al., 2013;Kwiatkowski et al., 2019; Joshi et al., 2017; Clark et al., 2018). To tackle such tasks, the state-of-the-art systems usually retrieve relevant passages that might contain the answer to the question, and then feedsuch passages and questions together to a language model to generate the answer. Liu et al. (2022) shows thatusing retrieval-based ICL (sentence semantic similarity-based retriever with GPT-3) is almost comparable toa fine-tuned method. Ye et al. (2023a) shows that BM25 achieve 10+% improvement on open-domain QA. Text generation tasks such as code generation (CodeGen) (Zelle & Mooney, 1996; Lin et al., 2018), semanticparsing (SP) (Wolfson et al., 2020; Li et al., 2021; Andreas et al., 2020), text-to-SQL (Shi et al., 2022),Table-to-text (Text) generation (Parikh et al., 2020), Data-to-Text (D2T) (Nan et al., 2021; Dueket al., 2019) benefit from RetICL (Poesia et al., 2021; Rubin et al., 2022; Hu et al., 2022; Hongjin et al.,2022; Shi et al., 2022; Ye et al., 2023a; Agrawal et al., 2023). Rubin et al. (2022) shows that the retrieveddemonstrations significantly outperform random demonstrations (e.g. BM25 is 25+% better than random,and EPR is 30% better than random). Apart from different types of tasks, Hongjin et al. (2022) shows that in scenarios with limited trainingdata, RetICL outperforms fine-tuning a model on such sparse data. Furthermore, leveraging data from ahigh-resource domain can enhance performance in a low-resource domain, as seen in cross-lingual contexts (Shiet al., 2022; Nie et al., 2022; Cheng et al., 2023).",
  "Future Directions": "Retrieve Demonstrations From Raw TextMuch research assumes the availability of annotated samplesthat can be utilized as a retrieval corpus. Yet, when faced with a novel task, it is often the case that no suchtraining dataset exists. While there are preliminary efforts to create pseudo demonstrations from open-endedcorpora like Wikipedia (Lyu et al., 2022), the proposed method is restricted to classification tasks and thelabel to the demonstrations are randomly assigned. A potential approach to obtain pseudo demonstrations forgeneration tasks is the approach of Wan et al. (2023), where they assume a set of unlabelled queries available(without ground truth labels), and use LLMs to generate chain-of-thoughts and answers and then applyself-consistency (Wang et al., 2022c) to select high-quality demonstrations to form pseudo demonstrationspool. Employing this method of generating answers with sentences retrieved from a free-form corpus couldpotentially create high-quality pseudo demonstrations. Demonstration Retriever DesignCurrent demonstration retriever architectures are not significantlydifferent from those used for raw text, with most efforts focusing on constructing the downstream training data.An open question remains: How can retrievers be designed specifically for demonstrations rather than rawtext? A potential direction could involve training a task-aware retriever with instructions Asai et al. (2023),such that the fine-tuned retriever learns to tailor itself to instructional content. The instruction retrievershould not only understand the semantic instructions but also retrieve the most relevant demonstrationsfrom the database that follow these instructions. Retriever Training MethodsIn , we explore various methods for training a retriever tosearch demonstrations. These methods largely depend on using an LLM to identify positive and negativedemonstrations for a given question. This approach, while innovative, comes with significant computationaldemands. Moreover, the ambiguity in choosing what constitutes a positive or negative demonstration raisesconcerns about the quality of the training data for the retriever (Hashimoto et al., 2023). Addressing thesechallenges is crucial for the development of more efficient and reliable retriever training methods.",
  "Conclusion": "This survey concentrates on few-shots In-Context Learning (ICL) using retrieved examples for large languagemodels, a key aspect of Retrieval-Augmented Generation (RAG). We outline various retrieval strategies,diverse retrieval models, retrieval pools, techniques for training demonstration retrievers, and applications.Based on the comprehensive understanding of current trends, we suggest several promising future paths forenhancing the efficacy and functionality of this approach. Abdalghani Abujabal, Rishiraj Saha Roy, Mohamed Yahya, and Gerhard Weikum. ComQA: A community-sourced dataset for complex factoid question answering with paraphrase clusters. In Proceedings of the2019 Conference of the North American Chapter of the Association for Computational Linguistics: HumanLanguage Technologies, Volume 1 (Long and Short Papers), pp. 307317, Minneapolis, Minnesota, June 2019.Association for Computational Linguistics. doi: 10.18653/v1/N19-1027. URL Rishabh Agarwal, Avi Singh, Lei M Zhang, Bernd Bohnet, Stephanie Chan, Ankesh Anand, Zaheer Ab-bas, Azade Nova, John D Co-Reyes, Eric Chu, et al. Many-shot in-context learning. arXiv preprintarXiv:2404.11018, 2024.",
  "Sweta Agrawal, Chunting Zhou, Mike Lewis, Luke Zettlemoyer, and Marjan Ghazvininejad. In-contextexamples selection for machine translation. arXiv preprint arXiv:2212.02437, 2022": "Sweta Agrawal, Chunting Zhou, Mike Lewis, Luke Zettlemoyer, and Marjan Ghazvininejad. In-contextexamples selection for machine translation. In Findings of the Association for Computational Linguistics:ACL 2023, pp. 88578873, 2023. Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc,Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model forfew-shot learning. Advances in neural information processing systems, 35:2371623736, 2022.",
  "AI Anthropic. The claude 3 model family: Opus, sonnet, haiku. Claude-3 Model Card, 1, 2024": "Akari Asai, Timo Schick, Patrick Lewis, Xilun Chen, Gautier Izacard, Sebastian Riedel, Hannaneh Hajishirzi,and Wen-tau Yih. Task-aware retrieval with instructions. In Findings of the Association for ComputationalLinguistics: ACL 2023, pp. 36503675, 2023. Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe,Yonatan Bitton, Samir Gadre, Shiori Sagawa, et al. Openflamingo: An open-source framework for traininglarge autoregressive vision-language models. arXiv preprint arXiv:2308.01390, 2023.",
  "Yoshua Bengio, Rjean Ducharme, and Pascal Vincent. A neural probabilistic language model. Advances inneural information processing systems, 13, 2000": "Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. Semantic parsing on freebase from question-answer pairs. In Proceedings of the 2013 conference on empirical methods in natural language processing,pp. 15331544, 2013. Amanda Bertsch, Maor Ivgi, Uri Alon, Jonathan Berant, Matthew R Gormley, and Graham Neubig. In-contextlearning with long-context models: An in-depth exploration. arXiv preprint arXiv:2405.00200, 2024. Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. Piqa: Reasoning about physicalcommonsense in natural language. In Thirty-Fourth AAAI Conference on Artificial Intelligence, 2020. Samuel Bowman, Gabor Angeli, Christopher Potts, and Christopher D Manning. A large annotated corpusfor learning natural language inference. In Proceedings of the 2015 Conference on Empirical Methods inNatural Language Processing, pp. 632642, 2015a. Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. A large annotatedcorpus for learning natural language inference. In Proceedings of the 2015 Conference on Empirical Methodsin Natural Language Processing (EMNLP), pp. 632642, 2015b. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.Advances in neural information processing systems, 33:18771901, 2020.",
  "Christopher J.C. Burges. From ranknet to lambdarank to lambdamart: An overview. Technical report,Microsoft Research Technical Report MSR-TR-2010-82, 2010": "Daniel M. Cer, Mona T. Diab, Eneko Agirre, Iigo Lopez-Gazpio, and Lucia Specia. Semeval-2017 task 1:Semantic textual similarity - multilingual and cross-lingual focused evaluation. CoRR, abs/1708.00055,2017. URL Mingda Chen, Jingfei Du, Ramakanth Pasunuru, Todor Mihaylov, Srini Iyer, Veselin Stoyanov, and Zor-nitsa Kozareva.Improving in-context few-shot learning via self-supervised training.arXiv preprintarXiv:2205.01703, 2022. Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya Jia. Longlora: Efficientfine-tuning of long-context large language models. In The Twelfth International Conference on LearningRepresentations. Daixuan Cheng, Shaohan Huang, Junyu Bi, Yuefeng Zhan, Jianfeng Liu, Yujing Wang, Hao Sun, Furu Wei,Denvy Deng, and Qi Zhang. Uprise: Universal prompt retrieval for improving zero-shot evaluation. arXivpreprint arXiv:2303.08518, 2023. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, PaulBarham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modelingwith pathways. Journal of Machine Learning Research, 24(240):1113, 2023. Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang,Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. arXivpreprint arXiv:2210.11416, 2022.",
  "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectionaltransformers for language understanding. arXiv preprint arXiv:1810.04805, 2018": "Bill Dolan, Chris Quirk, and Chris Brockett. Unsupervised construction of large paraphrase corpora: exploitingmassively parallel news sources. In Proceedings of the 20th international conference on ComputationalLinguistics, pp. 350es, 2004. Andrew Drozdov, Nathanael Schrli, Ekin Akyrek, Nathan Scales, Xinying Song, Xinyun Chen, OlivierBousquet, and Denny Zhou. Compositional semantic parsing with large language models. In The EleventhInternational Conference on Learning Representations, 2022. Andrew Drozdov, Honglei Zhuang, Zhuyun Dai, Zhen Qin, Razieh Rahimi, Xuanhui Wang, Dana Alon, MohitIyyer, Andrew McCallum, Donald Metzler, et al. Parade: Passage ranking using demonstrations with largelanguage models. arXiv preprint arXiv:2310.14408, 2023. Ondej Duek, David M Howcroft, and Verena Rieser. Semantic noise matters for neural natural languagegeneration. In Proceedings of the 12th International Conference on Natural Language Generation, pp.421426, 2019.",
  "Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, and Tushar Khot. Complexity-based prompting formulti-step reasoning. arXiv preprint arXiv:2210.00720, 2022": "Feng Gao, Qing Ping, Govind Thattai, Aishwarya Reganti, Ying Nian Wu, and Prem Natarajan. Transform-retrieve-generate: Natural language-centric outside-knowledge visual question answering. In Proceedings ofthe IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 50675077, June2022. Lingyu Gao, Aditi Chaudhary, Krishna Srinivasan, Kazuma Hashimoto, Karthik Raman, and Michael Ben-dersky. Ambiguity-aware in-context learning with large language models. arXiv preprint arXiv:2309.07900,2023. Tianyu Gao, Adam Fisch, and Danqi Chen. Making pre-trained language models better few-shot learners.In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11thInternational Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 38163830,2021a.",
  "Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprintarXiv:2312.00752, 2023": "Jiafeng Guo, Yixing Fan, Qingyao Ai, and W Bruce Croft. A deep relevance matching model for ad-hocretrieval. In Proceedings of the 25th ACM international on conference on information and knowledgemanagement, pp. 5564, 2016. Suchin Gururangan, Mike Lewis, Ari Holtzman, Noah A Smith, and Luke Zettlemoyer. Demix layers:Disentangling domains for modular language modeling. In Proceedings of the 2022 Conference of the NorthAmerican Chapter of the Association for Computational Linguistics: Human Language Technologies, pp.55575576, 2022.",
  "Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. Retrieval augmented languagemodel pre-training. In International conference on machine learning, pp. 39293938. PMLR, 2020": "Kazuma Hashimoto, Karthik Raman, and Michael Bendersky. Take one step at a time to know incrementalutility of demonstration: An analysis on reranking for few-shot in-context learning.arXiv preprintarXiv:2311.09619, 2023. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and JacobSteinhardt. Measuring mathematical problem solving with the math dataset. In Thirty-fifth Conference onNeural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021.",
  "Sepp Hochreiter and Jrgen Schmidhuber. Long short-term memory. Neural computation, 9(8):17351780,1997": "SU Hongjin, Jungo Kasai, Chen Henry Wu, Weijia Shi, Tianlu Wang, Jiayi Xin, Rui Zhang, Mari Ostendorf,Luke Zettlemoyer, Noah A Smith, et al. Selective annotation makes language models better few-shotlearners. In The Eleventh International Conference on Learning Representations, 2022. Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora:Low-rank adaptation of large language models. In International Conference on Learning Representations,2021. Yushi Hu, Chia-Hsuan Lee, Tianbao Xie, Tao Yu, Noah A Smith, and Mari Ostendorf. In-context learning forfew-shot dialogue state tracking. In Findings of the Association for Computational Linguistics: EMNLP2022, pp. 26272643, 2022. Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu,Armand Joulin, Sebastian Riedel, and Edouard Grave. Atlas: Few-shot learning with retrieval augmentedlanguage models. Journal of Machine Learning Research, 24(251):143, 2023.",
  "Alex Kulesza, Ben Taskar, et al. Determinantal point processes for machine learning. Foundations andTrends in Machine Learning, 5(23):123286, 2012": "Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti,Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. Natural questions: a benchmark forquestion answering research. Transactions of the Association for Computational Linguistics, 7:453466,2019. Tamera Lanham, Anna Chen, Ansh Radhakrishnan, Benoit Steiner, Carson Denison, Danny Hernandez,Dustin Li, Esin Durmus, Evan Hubinger, Jackson Kernion, et al. Measuring faithfulness in chain-of-thoughtreasoning. arXiv preprint arXiv:2307.13702, 2023.",
  "Itay Levy, Ben Bogin, and Jonathan Berant. Diverse demonstrations improve in-context compositionalgeneralization. arXiv preprint arXiv:2212.06800, 2022": "Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, VesStoyanov, and Luke Zettlemoyer. Bart: Denoising sequence-to-sequence pre-training for natural languagegeneration, translation, and comprehension. arXiv preprint arXiv:1910.13461, 2019. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, HeinrichKttler, Mike Lewis, Wen-tau Yih, Tim Rocktschel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems, 33:94599474, 2020. Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh,Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. Solving quantitative reasoningproblems with language models. Advances in Neural Information Processing Systems, 35:38433857, 2022.",
  "Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, and Weizhu Chen. Making largelanguage models better reasoners with step-aware verifier. arXiv preprint arXiv:2206.02336, 2022b": "Bill Yuchen Lin, Seyeon Lee, Rahul Khanna, and Xiang Ren. Birds have four legs?! numersense: Probingnumerical commonsense knowledge of pre-trained language models. arXiv preprint arXiv:2005.00683, 2020. Xi Victoria Lin, Chenglong Wang, Luke Zettlemoyer, and Michael D Ernst. Nl2bash: A corpus and semanticparser for natural language interface to the linux operating system.In Proceedings of the EleventhInternational Conference on Language Resources and Evaluation (LREC 2018), 2018. Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. Program induction by rationale generation:Learning to solve and explain algebraic word problems. In Proceedings of the 55th Annual Meeting of theAssociation for Computational Linguistics (Volume 1: Long Papers), pp. 158167, 2017. Jiachang Liu, Dinghan Shen, Yizhe Zhang, William B Dolan, Lawrence Carin, and Weizhu Chen. Whatmakes good in-context examples for gpt-3? In Proceedings of Deep Learning Inside Out (DeeLIO 2022):The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, pp. 100114,2022.",
  "Tom Mikolov et al. Statistical language models based on neural networks. 2012": "Aristides Milios, Siva Reddy, and Dzmitry Bahdanau. In-context learning for text classification with manylabels. In Proceedings of the 1st GenBench Workshop on (Benchmarking) Generalisation in NLP, pp.173184, 2023. Sewon Min, Mike Lewis, Luke Zettlemoyer, and Hannaneh Hajishirzi. Metaicl: Learning to learn in context.In Proceedings of the 2022 Conference of the North American Chapter of the Association for ComputationalLinguistics: Human Language Technologies, pp. 27912809, 2022a. Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettle-moyer. Rethinking the role of demonstrations: What makes in-context learning work? arXiv preprintarXiv:2202.12837, 2022b.",
  "Ercong Nie, Sheng Liang, Helmut Schmid, and Hinrich Schtze. Cross-lingual retrieval augmented promptfor low-resource languages. ArXiv, abs/2212.09651, 2022": "Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann,Amanda Askell, Yuntao Bai, Anna Chen, et al. In-context learning and induction heads. arXiv preprintarXiv:2209.11895, 2022. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang,Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions withhuman feedback. Advances in Neural Information Processing Systems, 35:2773027744, 2022.",
  "Jane Pan. What in-context learning learns in-context: Disentangling task recognition and task learning.Masters thesis, Princeton University, 2023": "Ankur Parikh, Xuezhi Wang, Sebastian Gehrmann, Manaal Faruqui, Bhuwan Dhingra, Diyi Yang, andDipanjan Das. Totto: A controlled table-to-text generation dataset. In Proceedings of the 2020 Conferenceon Empirical Methods in Natural Language Processing (EMNLP), pp. 11731186, 2020. Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Stella Biderman, Huanqi Cao,Xin Cheng, Michael Chung, Leon Derczynski, et al. Rwkv: Reinventing rnns for the transformer era. InFindings of the Association for Computational Linguistics: EMNLP 2023, pp. 1404814077, 2023a. Yingzhe Peng, Xu Yang, Haoxuan Ma, Shuo Xu, Chi Zhang, Yucheng Han, and Hanwang Zhang. Icd-lm: Con-figuring vision-language in-context demonstrations by language modeling. arXiv preprint arXiv:2312.10104,2023b.",
  "Claude Elwood Shannon. A mathematical theory of communication. The Bell system technical journal, 27(3):379423, 1948": "Peng Shi, Rui Zhang, He Bai, and Jimmy Lin. Xricl: Cross-lingual retrieval-augmented in-context learningfor cross-lingual text-to-sql semantic parsing. In Findings of the Association for Computational Linguistics:EMNLP 2022, pp. 52485259, 2022. Taylor Shin, Yasaman Razeghi, Robert L Logan IV, Eric Wallace, and Sameer Singh. Autoprompt: Elicitingknowledge from language models with automatically generated prompts. arXiv preprint arXiv:2010.15980,2020. Suzanna Sia and Kevin Duh. In-context learning as maintaining coherency: A study of on-the-fly machinetranslation using large language models. In Proceedings of Machine Translation Summit XIX, Vol. 1:Research Track, pp. 173185, 2023. Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, andChristopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. InProceedings of the 2013 conference on empirical methods in natural language processing, pp. 16311642,2013.",
  "Xingchen Wan, Ruoxi Sun, Hanjun Dai, Sercan O Arik, and Tomas Pfister. Better zero-shot reasoning withself-adaptive prompting. arXiv preprint arXiv:2305.14106, 2023": "Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. Glue: Amulti-task benchmark and analysis platform for natural language understanding. In Proceedings of the2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pp. 353355,2018. Boshi Wang, Sewon Min, Xiang Deng, Jiaming Shen, You Wu, Luke Zettlemoyer, and Huan Sun. To-wards understanding chain-of-thought prompting: An empirical study of what matters. arXiv preprintarXiv:2212.10001, 2022a.",
  "Liang Wang, Nan Yang, and Furu Wei. Learning to retrieve in-context examples for large language models.arXiv preprint arXiv:2307.07164, 2023a": "Xinyi Wang, Wanrong Zhu, and William Yang Wang. Large language models are implicitly topic models:Explaining and finding good demonstrations for in-context learning. arXiv preprint arXiv:2301.11916,2023b. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H Chi, Sharan Narang, Aakanksha Chowdhery,and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. In The EleventhInternational Conference on Learning Representations, 2022c.",
  "Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain ofthought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022": "Jerry Wei, Jason Wei, Yi Tay, Dustin Tran, Albert Webson, Yifeng Lu, Xinyun Chen, Hanxiao Liu,Da Huang, Denny Zhou, et al. Larger language models do in-context learning differently. arXiv preprintarXiv:2303.03846, 2023. Orion Weller, Marc Marone, Nathaniel Weir, Dawn Lawrie, Daniel Khashabi, and Benjamin Van Durme.\" according to...\" prompting language models improves quoting from pre-training data. arXiv preprintarXiv:2305.13252, 2023.",
  "Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanation of in-context learningas implicit bayesian inference. In International Conference on Learning Representations, 2021": "Chenyan Xiong, Zhuyun Dai, Jamie Callan, Zhiyuan Liu, and Russell Power. End-to-end neural ad-hocranking with kernel pooling. In Proceedings of the 40th International ACM SIGIR conference on researchand development in information retrieval, pp. 5564, 2017. Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, andColin Raffel. mt5: A massively multilingual pre-trained text-to-text transformer. CoRR, abs/2010.11934,2020. URL",
  "Xu Yang, Yongliang Wu, Mingzhuo Yang, Haokun Chen, and Xin Geng. Exploring diverse in-contextconfigurations for image captioning. Advances in Neural Information Processing Systems, 36, 2024": "Yinfei Yang, Daniel Cer, Amin Ahmad, Mandy Guo, Jax Law, Noah Constant, Gustavo Hernndez brego,Steve Yuan, Chris Tar, Yun-Hsuan Sung, Brian Strope, and Ray Kurzweil. Multilingual universal sentenceencoder for semantic retrieval. CoRR, abs/1907.04307, 2019. URL Michihiro Yasunaga, Armen Aghajanyan, Weijia Shi, Richard James, Jure Leskovec, Percy Liang, Mike Lewis,Luke Zettlemoyer, and Wen-Tau Yih. Retrieval-augmented multimodal language modeling. In InternationalConference on Machine Learning, pp. 3975539769. PMLR, 2023.",
  "Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. Automatic chain of thought prompting in largelanguage models. In The Eleventh International Conference on Learning Representations, 2022b": "Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang,Junjie Zhang, Zican Dong, et al. A survey of large language models. arXiv preprint arXiv:2303.18223,2023. Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. Calibrate before use: Improving few-shotperformance of language models. In International Conference on Machine Learning, pp. 1269712706.PMLR, 2021."
}