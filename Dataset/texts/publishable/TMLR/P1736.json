{
  "Abstract": "For many practical problems, it is important to measure similarity between graphs. Thiscan be done via graph kernels. One particular application where the choice of a graph kernelis essential is assessing the quality of graph generative models. However, despite the vastnumber of graph kernels available in the literature, only basic kernels are usually consideredfor generative model evaluation. In this paper, we fill this gap and analyze how differentgraph kernels perform as an ingredient in the pipeline of generative model performanceevaluation. To conduct a detailed analysis, we propose a framework for comparing graphkernels in terms of which high-level structural properties they are sensitive to: heterogeneityof degree distribution, the presence of community structure, the presence of latent geometry,and others. For this, we design continuous transitions between random graph models thataffect a particular property and measure which graph kernel is sensitive to the correspondingchange. We show that using such diverse models with the corresponding transitions is crucialfor evaluation: many kernels can successfully capture some properties and fail on others.We also found some well-known kernels that show good performance in our experiments buthave been previously overlooked in the literature on evaluating graph generative models.",
  "Introduction": "Many real-world objects can be represented as graphs: social and citation networks, molecules, the Internet,transportation networks, and so on. For a number of practical problems, it is important to measure similarityor distance between graphs, e.g., graph classification, graph clustering, or evaluating graph generative models. A number of graph kernels have been proposed in the literature to evaluate the similarity betweengraphs (Kriege et al., 2020; Nikolentzos et al., 2021). Graph kernels are often based on some graph statisticslike node degrees, shortest path lengths, small subgraph counts, and so on. Thus, different kernels capturedifferent graph properties. However, it is not obvious which properties are captured by a given kernel. Thismakes it challenging to choose a suitable kernel for a particular problem. One important application of graph kernels is evaluating the performance of graph generative models. Toevaluate such a model, one needs a measure that compares a generated set of graphs with a reference set,which is a non-trivial task (OBray et al., 2022; Thompson et al., 2022). A standard approach is to convertall graphs to some vector representations or to compute similarities (kernel values) for pairs of graphs andthen aggregate the values into a similarity measure.The most widespread measure of (dis)similarity ismaximum mean discrepancy (MMD) which measures the distance between two sets of elements given the",
  "Published in Transactions on Machine Learning Research (12/2024)": "some well-known kernels that show good performance in our experiments have been previously overlooked inthe literature on this subject. Examples of such kernels include Shortest Path and Pyramid Match kernels.Second, we observe that the wide diversity of the considered structural properties is critical for a thoroughevaluation: it is often the case that a particular graph kernel is sensitive to some characteristics while beinginsensitive to others. The results shown in can be useful when deciding which graph kernel is most suitable when evaluatinggenerative models in a particular application. For example, our experiments show that the popular Weisfeiler-Lehman kernel is insensitive to geometry. In certain applications, geometry can be especially relevant, e.g., forthe application of molecular modeling because atoms have spatial locations. In this case, our analysis showsthat using the WL kernel is not a good option. Let us also note that kernels with different properties canpotentially be combined with each other by, e.g., averaging their values. Indeed, if one kernel is sensitive toa particular characteristic while another kernel is sensitive to another characteristic, then their combinationis expected to be sensitive to both of them. However, a detailed analysis of such combinations is beyond thescope of the current paper.",
  "Various approaches can be used to measure similarity or dissimilarity between graphs. Two major researchdirections are concentrated on graph distances and graph kernels": "Graph distances measure dissimilarity between graphs and are supposed to satisfy the metric axioms. How-ever, the positivity axiom is usually violated in the sense that the distance between two different graphscan be equal to zero. Indeed, if we guarantee that D(G, G) = 0 if and only if G and G are isomorphic,then computing such distance is at least as hard as graph isomorphism testing, which is infeasible for mostapplications. In turn, a graph kernel is a symmetric, positive semidefinite function defined on the space of graphs. Thisfunction can be expressed as an inner product in some Hilbert space. A survey of many graph kernels canbe found in, e.g., Kriege et al. (2020); Nikolentzos et al. (2021).",
  "where deg(u) be the degree of u": "Weisfeiler-Leman (WL) subtree kernelwas proposed in Shervashidze et al. (2011) and is based onthe WL color refinement procedure. This procedure works as follows. Initially, all nodes have their labels(or one fixed label for unlabelled graphs). At each iteration, a nodes label is replaced by another labelidentifying a multiset of labels of its neighbors. The procedure stops when it converges. Based on that, theWL kernel for l iterations is",
  "where wliG1(u) is the label at i-th iteration of the WL procedure": "The Shortest Path (SP) kernelwas introduced in Borgwardt & Kriegel (2005). Each shortest path isdescribed by the following triplet: its length and labels of starting and ending nodes. Then, the graph isrepresented by a vector fG, where each coordinate is the frequency of a particular triplet in a graph. Then,the kernel is a scalar product of fG and fG. In our setting, where each node has the same label, the SPkernel compares the shortest-path histograms. Thus, the vector fG can be interpreted as a signature of thetopology described by the network. The Graphlet kernelwas proposed in Prulj (2007). Graphlets are small connected subgraphs of a graph.Consider graphlets of size k. We assign an index to each graphlet and let fG be a vector such that its i-thentry is equal to the frequency of occurrence of i-th graphlet in G. Then, the kernel can be computed as thescalar product of fG and fG.",
  "To make the computation of this kernel feasible, graph invariants can be employed to encode each rootedsubgraph. Then, these invariants can be compared instead of graph isomorphism checking": "NetLSDtreats a graph as a dynamical system and simulates heat and wave diffusion processes on nodesand edges of a given graph, followed by measuring system conditions at fixed timestamps (Tsitsulin et al.,2018). More formally, let j be the j-th smallest eigenvalue of the normalized Laplacian of a graph G. Fora timestamp t, we define the heat trace ht and wave trace wt of a graph G as follows:",
  "Here t > 0 for the heat trace and t [0, 2) for the wave trace": "Then, the heat trace signature and wave trace signature of G are defined as the sequences of the correspondingtraces at different timestamps, i.e., h(G) = {ht}tTh and w(G) = {wt}tTw. As in the original article, weuse 250 log-spaced time stamps between 102 and 102 for Th and 250 equally-spaced time stamps between0 and 2 for Tw, respectively. Finally, the NetLSD distance (heat or wave) between two graphs G and G can be computed as any distancemeasure between the corresponding signatures. In our comparison, we use NetLSD representations to obtaina graph kernel. For this, we measure the cosine similarity between the graph representations. Pyramid Match kernelproposed in Nikolentzos et al. (2017) first embeds the vertices of each graphinto a low-dimensional vector space using the eigenvectors of the d largest (in magnitude) eigenvalues of thegraphs adjacency matrix. Since the signs of these eigenvectors are arbitrary, it replaces all their componentsby their absolute values.Each vertex is thus a point in the d-dimensional unit hypercube.To find anapproximate correspondence between the sets of vertices of two graphs, the kernel maps these points tomulti-resolution histograms, and compares the emerging histograms with a weighted histogram intersectionfunction, see Nikolentzos et al. (2017) for more details.",
  "Kernels for evaluating graph generative models": "To evaluate a graph generative model, one needs to compare a set of graphs produced by the model withsome reference set of graphs (not available to the model during training). Typically, the comparison consistsof two steps. First, each graph is described as a point in a vector space, or for each pair of graphs theirsimilarity is defined. Then, two sets of points or two sets of similarity values are compared. Arguably themost widely-used approach is to define a kernel K(, ) for measuring the similarity of graphs and then usemaximum mean discrepancy (MMD) to measure the distance between two sets of graphs G1 and G2:",
  "|Gi|": "GGi f(G). Thus, MMD is indeed a distance.Besides MMD, there are other approaches to compare distributions: Frchet Distance (Heusel et al., 2017),Improved Precision & Recall (Kynknniemi et al., 2019), Density & Coverage (Naeem et al., 2020). Overall,there are many ways to compare two distributions of graphs. The work by OBray et al. (2022) is the most relevant to our study. The authors consider graphs without nodefeatures and labels and compare several typically used MMD-based measures. To define a kernel, OBray",
  "Comparing graph kernels": "In this section, we describe the proposed framework for comparing graph kernels. The main questions wewant to address is What structural characteristics are captured by what kernel?. With captured, we meanthat the kernel can distinguish two sets of graphs if they differ in that particular aspect. To measure thesensitivity of a kernel to a graph characteristic, we consider a sequence of graph generators where, at eachstep, the considered graph characteristic is more present. To that end, we consider graph generators thatinclude a step parameter which we vary between 0 and 1 to increase the value of the characteristic. Wedescribe below which graph generators we consider and how we interpolate between them. In all of our generators, we keep the number of nodes n constant, and we also preserve the expected numberof edges m (except when the target characteristic is density). We use the Erds-Rnyi (ER) random graphmodel as baseline generator. In the Erds-Rnyi model, each edge is added independently with probabilityp = m/n2. As we discussed above, previous studies (OBray et al., 2022; Thompson et al., 2022) measured the sensitivityof graph kernels by perturbing a given (real) dataset. Instead, we use random graph generators since theyallow us to gradually increase or decrease specific characteristics and thus conduct a detailed analysis ofkernels sensitivity to various structural shifts.",
  "DensityDensity is the simplest graph characteristic. We model this by varying the probability p in thebasic Erds-Rnyi model": "Degree heterogeneityThe degree distribution of an ER graph differs from what is observed in real-worldnetworks: ER graphs have Binomially distributed degrees, leading to a variance that is lower than the meandegree. However, many real-world networks have a much more heterogeneous degree distribution, where thevariance is often many times larger than the mean degree. Many networks even seem to have power-lawdegree distributions, leading to many hubs and a high degree variance (Barabsi & Bonabeau, 2003). Severalgenerative models incorporate degree heterogeneity by prescribing an (expected) degree sequence, as can bedone with the Configuration Model and the Chung-Lu model (Chung & Lu, 2002; Van der Hofstad, 2016).We use the Chung-Lu model because it is a generalization of ER and is simple to work with. The inputto this model is the vector of the expected degrees (w1, . . . , wn)T . Given the expected degrees, an edgebetween two nodes u and v is added with probabilitywuwv",
  "i wi independently of all other edges. We sample the": "prescribed degrees from a Pareto distribution with power-law exponent 2 and scale parameter chosento ensure that the expected number of edges equals m. In the limit , this is equivalent to the ERmodel, while small finite values lead to degree sequences with high variance. ClusteringIn many real-world networks, nodes with common neighbors are more likely to be connectedto each other (Watts & Strogatz, 1998; Van der Hofstad, 2016). This phenomenon is often referred to asclustering (Holland & Leinhardt, 1971; Peixoto, 2022), and it results in an abundance of triangles, which isoften quantified by the clustering coefficient. Social networks and many other real graphs are well-knownfor having a high degree of clustering (Holland & Leinhardt, 1971; McPherson et al., 2001).",
  "There are two main mechanisms that are used to explain and model clustering: community structure andlatent geometry. In our analysis, we treat these as two different graph characteristics": "Community structureMany real-world networks contain groups of nodes that are more densely con-nected to each other than to the rest of the network. In network science, these groups are referred to ascommunities (Fortunato, 2010), and they often have a natural interpretation, like friend groups in socialnetworks or subject areas in citation networks. In addition, the presence of community structure can ex-plain the clustering that is observed in real networks: the presence of densely connected groups leads to anincreased number of triangles. The simplest generative model for community structure is the Planted Partition (PP) model (Holland et al.,1983), where we are given a partition of the network nodes into communities, and node pairs of the samecommunity connect with probability pin, while node pairs of different communities connect with probabilitypout. We consider two communities of size n/2 each, and we parameterize pin, pout as",
  "n2(1 + ) 2n,(2)": "so that the expected number of edges equals m, while pin/pout = . This parametrization of the PP modelreduces to the ER model for = 1, and the (expected) global clustering coefficient increases monotonouslywith . Latent geometryCommunity structure explains clustering by assuming a certain (finite) set of types,and that nodes of the same type have a higher likelihood of connecting than nodes of different types. TheRandom Geometric (RG) model generalizes this notion by considering a continuous type space and assumingthat nodes whose types are similar to each other have a larger probability of connecting to each other. Incitation networks, for example, papers tend to cite papers on related topics. This continuous type space can",
  "Interpolating graph characteristics": "For each graph characteristic, we define an interpolation between two graph generators, so that the strengthof that characteristic changes monotonously along the transition. We parametrize each of these interpolationsby a step parameter , so that = 0 leads to the left generator, while = 1 leads to the rightgenerator.We make use of the fact that most of the generating models introduced in .1 aregeneralizations of the ER model. In these cases, we simply parameterize a transition away from the ERmodel. The ER model is chosen with p = m/n2so that the expected number of edges equals m. Forthe geometric models used for latent geometry and complementarity, we will take a different approach tointerpolate between generators.",
  "Density (ER(p))We use the ER model with the edge probability p( + 1). Thus, when changes from0 to 1, the edge probability increases from p to 2p": "Heterogeneity (ERCL)We use the Chung-Lu (CL) model with weights drawn from a Pareto distri-bution with power-law exponent 1 + 1/ and scale chosen such that the expected number of edges equals m.Note that = 0 leads to a power-law exponent , i.e., all the weights are constant and the corresponding",
  "CL model is equivalent to the ER model. The values > 0.5 give weight distributions with infinite variance,which leads to a high variance in the degree distribution": "Communities (ERPP)We use the PP model with pin, pout as given in equation 2 for = 1 +c(n, m),where c(n, m) is a constant so that = 0 leads to pin = pout = p, while = 1 leads to every vertexhaving (in expectation) one neighbor outside its community. This value is derived in Appendix A.2. Latent geometry (ERTorus)For this transition, we take the mixture of an ER graph and a RG torusgraph: we generate a graph from each generator and denote their adjacency matrices by A(ER) and A(RG).We construct the adjacency matrix A as follows: for each node pair with indices 1 i < j n we draw anindependent Bernoulli random variable Bij with success probability and set Aij = A(ER)ij+ Bij(A(RG)ij",
  "A(ER)ij). Hence, = 0 leads to A = A(ER), while = 1 leads to A = A(RG). We complete the adjacencymatrix symmetrically: Aij = Aji for i > j and Aii = 0": "Dimensionality (TorusCircle)To interpolate between a two- and one-dimensional torus, we considera torus of height h = 1 and connection radius r(h, m) as derived in Appendix A.1. This way, = 0 leadsto a torus on [0, 1)2 while h 0 leads to a one-dimensional torus (a circle). Complementarity (ERSC)For this transition, we take the same approach as for the interpolationERTorus: we construct a graph by taking a mixture of graphs generated by the ER and SC models, suchthat = 0 results in an ER graph, while = 1 results in a SC graph.",
  "Measuring the sensitivity of a kernel": "We now explain how we quantify the sensitivity of a kernel w.r.t. an interpolation between graph generators.We consider a discretization of the interpolation such that {0, 1} .For , let Gdenote a set of g graphs sampled independently from the interpolation generator at step . Furthermore,let MMD(G1, G2; K) denote the MMD value between G1 and G2 w.r.t. the kernel K. We compare each to each endpoint {0, 1}. For each endpoint {0, 1}, we quantify the sensitivity of the kernelK w.r.t. the interpolation as the Spearman correlation between MMD(G, G; K) and | |. If this valueis close to 1, it means that the MMD values tend to increase when transitioning away from . But if thisvalue is close to zero, it indicates that there is no clear monotone relation between and MMD(G1, G2; K).This leads to two different correlation coefficients r0 and r1, corresponding to the different values of . See for an illustration. To ensure the independence of the different MMD values that are used to compute a correlation coefficient,we sample two different sets of graphs for each MMD value. Between each pair of s that we compare, we",
  "Setup": "We follow the framework described in the previous section. In most of our experiments, we consider graphswith n = 50 nodes and (in expectation) m = 190 edges. We discretize the interpolation interval by = {0.0, 0.1, . . . , 1.0}. Thus, we have || = 11 steps in our interpolation. We consider sets of g = 100graphs and compute s = 30 different MMD values for each pair of interpolation steps that we compare. In this work, we mostly focus on small graphs since evaluating graph generative models is typically appliedto small networks: e.g., in molecular modeling, the graphs consist of a relatively small number of atoms.Additionally, let us note that not all kernels are scalable: for instance, the graphlet kernel has a prohibitivelyhigh computational cost. Thus, for smaller graphs, we can include all popular graph kernels in our experi-ments. However, for the completeness of the study, we also consider larger graphs with n = 1000 nodes and(in expectation) m = 3800 edges, so that the average degree is the same in both experiments.",
  "In our experiments, we use the following kernels and representations:": "Node degree histogram kernel (Degree); Shortest Path (SP) kernel (Borgwardt & Kriegel, 2005); Weisfeiler-Leman (WL) kernel (Shervashidze et al., 2011) with l = 5; Weisfeiler-Lehman optimal assignment (WL-OA) kernel (Kriege et al., 2016) with l = 5; Graphlet kernel (Prulj, 2007) with k = 3 and k = 4 (referred to as Graphlet-3 and Graphlet-4,respectively);",
  "For most graph kernels, we use the GraKeL python library (Siglidis et al., 2020). Following Nikolentzos et al.(2021), we normalize the kernel values as K(G1, G2)/": "K(G1, G1)K(G2, G2). Since our research comparesgraph kernels as ingredients of evaluation measures, we have to fix their hyperparameters before running theexperiments. For the considered kernels, we use their default hyperparameters listed above (that are eitherthe default parameters of the implementation or the most commonly used parameters in the literature).For the Graphlet kernel, however, the considered values of k are limited by 4 due to high computationalcomplexity of the kernel. For this kernel, we consider and compare its performance for two parameter values:k = 3 and k = 4. For NetLSD and RandGIN we use the implementation provided by the authors withthe default parameters. In both cases, we use cosine similarity to convert graph representations to kernelvalues. Thus, for all the obtained kernels we have K(G, G) = 1. Our code and experiments are availableat",
  "Results and discussion": "The results are shown in and the respective computation times are reported in Appendix B. We seethat most kernels perform well on some of the interpolations while performing badly on others. This showsthe importance of using different interpolations for assessing the sensitivity of kernels. Recall that in thisaspect, our framework extends the works by OBray et al. (2022); Thompson et al. (2022).",
  "RandGIN0.9380.9470.1320.5270.0670.285": "We note that some transitions are easier to detect by all the kernels: for heterogeneity, all kernels showrelatively good and stable performance. In contrast, for communities, geometry, dimensionality, and com-plementarity the difference in performance between different kernels is large: some kernels have correlationslarger than 0.97, while others may have near-zero performance, which suggests that these kernels are com-pletely insensitive to these characteristics. Finally, while most kernels perform well on the density transition,this turns out to be challenging for NSPDK. Let us now go over each kernel individually and summarize theresults. The Degree histogram kernelis added to our analysis as it is similar to the degree kernel used inprevious studies OBray et al. (2022); Thompson et al. (2022). As expected, this kernel can perfectly detectthe transformations of density and heterogeneity since they directly affect the degree distribution. However,the degree histogram kernel is not sensitive to all other transformations. This shows the limitations of usingsimple graph characteristics to compare graph distributions and motivates the analysis of more advancedkernels that we conduct in our study. The Weisfeiler-Leman kerneland the related WL-OA kernel are considered to be powerful graphkernels. However, in our experiments, we observe that they only perform well on interpolations that are welldetected by the degree histogram kernel. Indeed, WL and WL-OA are the best kernels for heterogeneity,which is expected as the first step of the WL procedure is based on node degrees. Similarly, both of themcan detect the density transformation well. In contrast, for all the remaining interpolations, the Spearmancorrelation coefficient is quite small. Based on these results, we can assume that WL-based kernels are mostlysensitive to the degree distributions. However, it is worth noting that both WL and WL-OA dominate thedegree histogram kernel for all interpolations since they are based on more advanced graph statistics thatgo beyond immediate neighbors.",
  "Comparing WL and WL-OA, we note that the latter dominates for all considered transformations. Thisconfirms that the procedure of node matching improves the sensitivity of this kernel": "The Shortest Path kernelhas quite stable performance. Indeed, its correlation values always exceed0.9 (in fact, it is the only kernel with this property), even for the most difficult transitions communi-ties, geometry, dimensionality, complementarity. Interestingly, SP is not among the best kernels for simpletransitions (heterogeneity and density), while still having good performance. Notably, SP is the best fordimensionality, which is arguably the most subtle transition. This can be explained by the fact that short-est path lengths capture both local and global information of the graphs, making this kernel sensitive tonon-trivial transitions.",
  ": The distribution of shortest path lengths for different graph generators, -1 corresponds to discon-nected node pairs": "shows the shortest path distribution for different graph generators. It can be clearly seen that allmodels are distinguishable. In particular, we see that the SP kernel is sensitive to dimensionality: the Circleresults in much longer path lengths than the Torus. The Graphlet kernelis another good option in terms of the overall performance: it is among thebest-performing kernels for all interpolations but dimensionality, where the difference with the SP kernelis noticeable. The poorer performance for dimensionality may be explained by the fact that the graphletkernel is not able to capture the global properties of graphs which are affected when we vary the dimension(e.g., its diameter). additionally illustrates the difficulty of the dimensionality transition: it turnsout that for small values of (when the height h is not too small), the distribution of graphlets does notchange much, and thus the MMD values are close to zero up to = 0.7. We also see that increasing thekernel sizes from 3 to 4 significantly improves the performance for dimensionality since this change makesthe considered neighborhoods larger. For all other interpolations, increasing graphlet sizes does not lead tonoticeable improvements. The Neighborhood Subgraph Pairwise Distance kernelwas previously used in the analysisby Thompson et al. (2022) and has shown reasonably good results in their experiments. In contrast, ouranalysis with diverse transformations shows certain shortcomings of this kernel. In particular, NSPDK turnsout to be insensitive to community structure and density. Other interpolations that are hard to detect forthis kernel are dimensionality and complementarity. We hypothesize that such poor performance for someof the transitions can be explained by a particular graph invariant used to compare two rooted subgraphs(as the exact graph isomorphism is infeasible). Unfortunately, this graph invariant also makes this kernelharder to theoretically analyze or intuitively explain.",
  ": Scatter plots of the MMD values of the Graphlet-3 kernel for the toruscircle interpolation": "The Pyramid Match kernelis another kernel that seems to be sensitive to all the characteristics.Similarly to the graphlet kernel, PM does not perform so well on dimensionality. Let us also note that PMis dominated by Graphlet-4 for each interpolation. On the other hand, this kernel is scalable and thus itis a good option for larger graphs where the Graphlet kernel cannot be applied. Similarly to NSPDK, thepyramid match kernel has a complex construction procedure and thus its performance is hard to intuitivelyinterpret. Still, our diverse transformations allow us to get some insights into such complex graph kernels. The NetLSD representationsare also sensitive to most of the properties. On almost all the transitions,NetLSD-wave outperforms NetLSD-heat. The notable exception is Dimensionality, which is arguably themost challenging interpolation. Here, NetLSD-heat is the second-best, while the performance of NetLSD-wave is quite poor. We illustrate the difference between NetLSD-heat and NetLSD-wave for this transitionin , where each cell shows the MMD value for two samples of graphs: corresponding to steps 1 and2. As expected, in both cases we see near-zero MMD values on the diagonal 1 = 2 since the distributionsare the same. For a good kernel, we also expect that MMD increases monotonically as we move away fromthe diagonal while keeping 1 (or 2) fixed since the distributions become more different. In particular, for2 = 1, we expect the distance to monotonically increase when we move from 1 = 1 to 1 = 0. This isindeed the case for NetLSD-heat, but surprisingly not the case for NetLSD-wave. 0.5 0.5",
  ": NetLSD-wave traces for Dimensionality": "b shows that the distance between graphsamples for 1 = 0.6 and 2 = 1 is larger than thedistance between 1 = 0 and 2 = 1, which is quiteunexpected.To additionally illustrate this phe-nomenon, shows the NetLSD-wave tracesfrom which the graph representations are obtained.One can see that the trace for = 0 is indeed closerto = 1.0 than = 0.6.Taking into accountquite complex nature of NetLSD-wave graph repre-sentation, we currently cannot explain this behavior.However, our framework allows one to detect suchpeculiar properties of graph representations. Random GIN representationswere proposed in Thompson et al. (2022) in the context of evaluatinggraph generative models. We note that there are some differences in our setup. First, our graphs do nothave node features, while being able to process such features is one of the main advantages of RandGIN.Second, Thompson et al. (2022) use GIN representations to compute measures like precision, recall, or FrchetDistance. Instead, we use RandGIN to construct a graph kernel that can be used with the MMD framework:we do this to compare all kernels and representations in the unified setup. Our results show that when",
  "Heterogeneity(ERCL)Communities(ERPP)Geometry(ERTorus)Dimensionality(TorusCircle)Complementarity(ERSC)": "Degree0.140.150.130.140.14SP1.131.131.131.051.13WL0.150.190.180.190.19WL-OA1.161.271.111.031.18Graphlet-311.328.707.685.979.00Graphlet-4150.8685.3770.0835.7591.08NSPDK41.3740.4636.7024.8338.48PM0.630.640.620.600.63NetLSD-heat0.740.780.730.890.81NetLSD-wave0.880.890.851.040.96RandGIN0.430.460.430.440.45 : The average computation times (in seconds) for comparing two sets of g = 100 graphs, eachconsisting of n = 50 vertices. The shown times are averaged over the || = 11 interpolation steps. Theexperiments were conducted on a laptop with AMD Ryzen 7 8840HS CPU and 16GB RAM.",
  "Experiments on larger graphs": "Results for scalable kernels on larger graph datasets with 1000 nodes are shown in . To reduce thecomputation time of these experiments, we set the number of packs to s = 10 instead of 30. We see that theseresults are consistent with those discussed above for smaller graphs. In particular, NetLSD-heat is betterthan NetLSD-wave for all transitions but Dimensionality, while NetLSD-heat is the best scalable kernel forDimensionality transition.",
  "Conclusion": "In this paper, we analyze the expressivity of graph kernels in the context of evaluating the performance ofgraph generative models. We first note that previous research on this subject used only a limited set ofgraph kernels. We significantly extend this set and consider a list of long-known graph kernels. To conduct adetailed analysis of the kernels, we develop a framework that allows us to check whether a kernel is insensitiveto a particular high-level structural graph property. For this, we carefully design an experimental setup basedon interpolations between random graph generators differing in a particular structural property. The results of our experiments confirm the main points of our paper. First, it is important to consider graphkernels beyond those usually considered for evaluating the performance of graph generative models. Indeed,",
  "Limitations": "It is important to keep in mind that it can be challenging (or even impossible) to change one graph character-istic keeping all other properties fixed. While we attempt to isolate the characteristic of interest, it is likelythat some other properties are affected to some extent. However, let us note that if a kernel turns out to beinsensitive to a particular interpolation this implies that it is insensitive to the underlying characteristics.Also, for clustering we consider two different ways to increase it, thus reducing the possible bias from aparticular generator. Another limitation of our work is that we only consider graphs without attributes. Our general frameworkeasily extends to attributed graphs, but the main challenge would be to design meaningful graph genera-tors for this scenario: there are many ways to combine standard graph generators with different attributedistributions. Additionally, our study considers relatively small graphs. Our main motivation is that evaluating graphgenerative models using graph kernels is typically applied to such small networks. For example, in molecularmodeling, the graphs consist of a relatively small number of atoms. Other applications where small graphsare relevant include modeling ego networks in sociology.Additionally, we note that not all kernels arescalable: for instance, the graphlet kernel has a prohibitively high computational cost on large networks. Weconsider smaller graphs so that we can include all popular graph kernels in our experiments. Finally, we note that our work is an empirical study of the sensitivity of graph kernels to some structuralshifts. Theoretical analysis supporting the obtained conclusions would be beneficial. However, for somekernels, such an analysis would be trivial, e.g., the degree histogram is clearly sensitive to shifts in the degreedistribution, but not sensitive to shifts in other characteristics that keep the degree distribution unchanged.On the other hand, for most of the kernels, such an analysis seems intractable due to their complex structures.",
  "A.1Random geometric models": "For a two-dimensional torus with width 1 and height h 1, we find the radius r that leads to m edges inexpectation. Two nodes are connected when their distance is smaller than r, i.e., whenever the second nodeis inside a circle with radius r around the first node. If h 2r, then the surface area of this circle is simplyr2, while the total area of the torus is h. Therefore, the expected edge density is r2. Solvingn2"
}