{
  "Abstract": "The celebrated message-passing updates for graph neural networks allow representing large-scale graphs with local and computationally tractable updates. However, the updates sufferfrom backtracking, i.e., a message flowing through the same edge twice and revisiting thepreviously visited node. Since the number of message flows increases exponentially withthe number of updates, the redundancy in local updates prevents the graph neural networkfrom accurately recognizing a particular message flow relevant for downstream tasks. Inthis work, we propose to resolve such a redundancy issue via the non-backtracking graphneural network (NBA-GNN) that updates a message without incorporating the messagefrom the previously visited node. We theoretically investigate how NBA-GNN alleviates theover-squashing of GNNs, and establish a connection between NBA-GNN and the impressiveperformance of non-backtracking updates for stochastic block model recovery. Furthermore,we empirically verify the effectiveness of our NBA-GNN on the long-range graph benchmarkand transductive node classification problems.",
  "svnhU3pqlAENYmUqQjpXf09kLNR6EvqmM2Q40sveTPzP6YXHiZiJIUIeKLRUEqKcZ0lgIdCAUc5cQxpUwt1I+YopxNFmVTAju8surpF2ruvVq/aZWaVzmcRTJMTkhZ8Ql56RBrkmTtAgnKXkmr+TNerJerHfrY9FasPKZI/IH1ucPqXKTGQ=</latexit>Step 2": ": Comparison of two types of message flow.Step 1: Message flow from node i to node j. Step2: The simple update includes the message flow fromnode j back to node i (left) at step 1, while the non-backtracking update removes this redundant messageflow (right). Recently, graph neural networks (GNNs) (Kipf &Welling, 2017; Hamilton et al., 2017; Xu et al.,2019) have shown great success in various applica-tions, such as molecular property prediction (Gilmeret al., 2017) and community detection (Bruna & Li,2017). Such success can be largely attributed to themessage-passing structure of GNNs, which providesa computationally tractable way of incorporatingthe overall graph through iterative updates based onlocal neighborhoods. However, the message-passingstructure also brings challenges due to the paral-lel updates and memoryless behavior of messagespassed along the graph. In particular, the message flow in a GNN is proneto backtracking, where the message from vertex i tovertex j is reincorporated in the subsequent messagefrom j to i, e.g.the left image of step 2 in .Since the message-passing iteratively aggregatesthe information, the GNN inevitably encounters an exponential surge in the number of message flows,proportionate to the vertex degrees.This issue is compounded by backtracking, which accelerates thegrowth of message flows with redundant information.",
  "Published in Transactions on Machine Learning Research (09/2024)": "Lemma 8. Suppose we have a SBM with parameters defined in Proposition 8. Then, there exists a functionof the eigenvectors of the non-backtracking matrix that can accurately recover the original community indexof vertices. The proof for Lemma 8 can be found in Appendix D.3 In the following, we will demonstrate that the non-backtracking GNN can compute an approximation to the top K eigenvectors mentioned in Lemma 8. Toachieve this, we first require the following lemma: Lemma 9. Assuming that the non-backtracking matrix B has a set of orthonormal eigenvectors i withcorresponding eigenvalues 1 > ... > K K+1 ... 2m, then there exists a sequence of convolutionallayers in the non-backtracking GNNs capable of computing the eigenvectors of the non-backtracking matrix. For the proof of Lemma 9, please refer to Appendix D.4. With this lemma in mind, we can observe that asequence of convolutional layers, followed by a multilayer perceptron, can approximate the function outlinedin Lemma 8, leveraging the universal approximation theorem. This argument leads to Proposition 4.",
  "Related works": "Non-backtracking Algorithms.Many classical algorithms have considered non-backtracking updates(Newman, 2013; Kempton, 2016).Belief propagation (Pearl, 1982) infers the marginal distribution onprobabilistic graphical models, and has demonstrated success for tree graphs (Kim & Pearl, 1983) and graphswith large girth (Murphy et al., 1999). Moreover, Mah et al. (2004) and Aziz et al. (2013) suggested graphkernels between labeled graphs utilizing non-backtracking walks, and Krzakala et al. (2013) first used it fornode classification. Furthermore, the non-backtracking has been shown to yield better spectral separationproperties, and its eigenspace contains information about the hidden structure of a graph model (Bordenaveet al., 2015; Stephan & Massouli, 2022). Non-backtracking and GNNs.We also note that there have been similar approaches of applying non-backtracking to GNNs. Chen et al. (2018) first used the non-backtracking operator in GNNs, though theydo not prevent backtracking and only target community detection tasks. Also, Chen et al. (2022) havecomputed a non-redundant tree for every node to eliminate redundancy, but inevitably suffers from highcomplexity. We emphasize the distinction between prior works and our NBA-GNN in Appendix A from thelens of (i) computational complexity and (ii) empirical performance. Over-squashing of GNNs.When a node receives information from a k-hop neighbor node, an expo-nential number of messages pass through node representations with fixed-sized vectors. This leads to theloss of information known as over-squashing (Alon & Yahav, 2021), and has been formalized in terms ofsensitivity (Topping et al., 2022; Di Giovanni et al., 2023). Hence, sensitivity is defined as the Jacobian of afinal node feature at a GNN layer to the initial node representation and can be upper bounded via the graphtopology. Stemming from this, graph rewiring methods alleviate over-squashing by adding or removing edgesto compute an optimal graph (Topping et al., 2022; Black et al., 2023; Di Giovanni et al., 2023). Anotherline of work uses global aspects, e.g., Transformers have been applied to consider global aspects to avoidover-squashing (Ying et al., 2021; Kreuzer et al., 2021; Rampek et al., 2022; Shirzad et al., 2023). Expressivity of GNNs for the SBM.Certain studies focus on analyzing the expressive power of GNNsusing variations of the SBM (Holland et al., 1983). Fountoulakis et al. (2023) established conditions for theexistence of graph attention networks (GATs) that can precisely classify nodes in the contextual stochasticblock model (CSBM) with high probability. Similarly, Baranwal et al. (2023) investigated the effects ofgraph convolutions within a network on the XOR-CSBM. The preceding works primarily focused on theprobability distribution of node features, such as the distance between the means of feature vectors. Onthe other hand, Kanatsoulis & Ribeiro (2023) analyzed the expressivity utilizing linear algebraic tools andeigenvalue decomposition of graph operators.",
  "Motivation from Sensitivity Analysis": "We first explain how the conventional message-passing updates are prone to backtracking. To be specific,consider a simple, undirected graph G = (V, E) and let N(i) denote the set of neighbor nodes of the node i.Each node i V is associated with a feature xi. Then, the conventional graph neural networks (GNNs), i.e.,message-passing neural networks (MPNNs) (Gilmer et al., 2017), iteratively update the node-wise hiddenfeature at the t-th layer h(t)ias follows:",
  "h(t1)s(t1),(2)": "where h(0)i= xi, W(i) denotes the set of T-step walks ending at node i, and s(t) denotes the t-th nodein the walk s W(i). Intuitively, this equation shows that a GNN with T layers recognize the graph viaaggregation of random walks with length T. Our key observation from Equation (2) is on how the feature h(T )iis insensitive to the information from an initial node feature h(0)j , due to the information being squashedby the aggregation over the exponential number of walks W(i). A similar analysis has been conducted onhow a node feature h(T )iis insensitive to the far-away initial node feature h(0)j= xj, i.e., the over-squashingphenomenon of GNNs (Topping et al., 2022).",
  ": Two non-backtracking walks (right) are sufficientto express information contained in a walk with backtrack-ing transition (left)": "Redundancy of walks with backtracking.In particular, a walk s randomly sampled fromW(i) is likely to contain a transition that back-tracks, i.e., s(t) = s(t+2) for some t < T. Thewalk s would be redundant since the informa-tion is contained in two other walks in W(i):s(0), . . . , s(t+1) and s(0), . . . , s(t)s(t+1), s(t+2) = s(t), s(t+3), . . . s(T), as illustrated in Fig-ure 2. This leads to the conclusion that non-backtracking walks, i.e., walks that do not con-tain backtracking transitions, are sufficient toexpress the information in the walks W(i). Since the exponential number of walks in W(i) causes the GNNto be insensitive to a particular walk information, it makes sense to design a non-backtracking GNN that issensitive to the constrained set of non-backtracking walks. Relation to Over-squashing.Finally, we point out an intriguing motivation for our work in termsof over-squashing. Di Giovanni et al. (2023) analyzed the lower bound for the Jacobian obstruction thatmeasures the degree of over-squashing in terms of access time with respect to a simple random walk. Theyconcluded that the degree of over-squashing, i.e., the size of Jacobian obstruction, is higher for a pair of nodeswith longer access time. Hence, for a GNN architecture robust to over-squashing, one could (i) propose arandom walk that has shorter access time for a pair of nodes in the graph, and (ii) design a GNN that alignswith the random walk. Since non-backtracking random walks have been empirically shown and believed togenerally yield faster access time than simple random walks (Lin & Zhang, 2019; Fasino et al., 2023), onecould aim to design a GNN architecture that aligns with the non-backtracking random walks. Access Time of Random Walks.However, to the best of our knowledge, there is no formal proof ofscenarios where non-backtracking random walks yield a shorter access time. As a motivating example, weprovide a theoretical result comparing the access time of non-backtracking and simple random walks for treegraphs. Since non-backtracking random walks do not guarantee a walk of a certain length, we make use ofbegrudgingly backtracking random walks (Rappaport et al., 2017), which modifies non-backtracking randomwalks to remove dead ends for tree graphs. For the full proof, please refer to Appendix B. Proposition 1. Given a tree G = (V, E) and a pair of nodes i, j V, the access time of begrudginglybacktracking random walk is equal to or smaller than that of a simple random walk. The equality holds ifand only if the walk length is 1.",
  "(b) Computation graph of NBA-GNN": ": Computation graph of typical GNN and NBA-GNN predicting node 0. (a) Redundant messagesincrease the size of the computation graph, proportional to the number of layers. (b) NBA-GNN assignsa pair of features for each edge and updates them via non-backtracking message passing.By reducingredundant messages, it results in a simplified computation graph compared to typical GNNs.",
  "h(t+1)ji= (t)h(t)ji,(t) h(t)kj, h(t)ji: k N(j) \\ {i} ,(3)": "where (t) and (t) are backbone-specific non-linear update and permutation-invariant aggregation functionsat the t-th layer, respectively. For example, (t) and (t) are multi-layer perceptron and summation over a setfor the graph isomorphism network (Xu et al., 2019, GIN), respectively. Given the update in Equation (3),one can observe that the message h(t)ij is never incorporated in the message h(t+1)ji , and hence the updateis free from backtracking. Note that line graph neural networks (Chen et al., 2018, LGNN) also appliednon-backtracking operators in GNNs. However, it does not address the issue of redundant messages, as itcontinues to use the adjacency matrix. Our key contribution is the use of the non-backtracking operatorspecifically to tackle the message redundancy issue. Please refer to Appendix A for a detailed comparison. Initialization and Node-wise Aggregation of Messages.The message at the 0-th layer h(0)ij is ini-tialized by encoding the node features xi, xj, and the edge feature eij using a non-linear function . Afterupdating hidden features for each edge based on Equation (3), we apply a permutation-invariant poolingover all the messages for graph-wise predictions. Since we use hidden features for each edge, we constructthe node-wise predictions at the final T-th layer as follows:",
  "hi = h(T )ji : j N(i), h(T )ij : j N(i),(4)": "where is a non-linear aggregation function with different weights for incoming edges j i and outgoingedges i j, is a non-linear aggregation function invariant to the permutation of nodes in N(i). We providea computation graph of NBA-GNN in b to summarize our algorithm. Begrudgingly Backtracking Update.While the messages from our update are resistant to backtrack-ing, a message h(t)ji may get trapped in node i for the special case when N(i) = {j}.To resolve thisissue, we introduce a simple trick coined begrudgingly backtracking update (Rappaport et al., 2017) thatupdates h(t+1)ijusing h(t)ji only when N(i) = {j}. We empirically verify the effectiveness of begrudginglybacktracking updates in .3. Implementation.To better understand our NBA-GNN, we provide an example of non-backtrackingmessage-passing updates with a GCN backbone (Kipf & Welling, 2017), coined NBA-GCN. The messageupdate at the t-th layer of NBA-GCN can be written as follows:",
  "Sensitivity Analysis on Over-squashing": "While Chen et al. (2018) initially introduced the non-backtracking operator in GNNs, they did not explore itstheoretical implications. Hence, we first analyze how NBA-GNNs alleviate the over-squashing issue. A well-known quantification to assess the over-squashing effect is the sensitivity bound presented in Proposition 2,i.e., the Jacobian of node-wise output for another initial node feature. Note that Topping et al. (2022)assumes the node features and hidden representations as scalars for better understanding. Proposition 2 (Sensitivity bounds). (Topping et al., 2022) Assume an MPNN defined in Equation (1).Let two nodes i, j V with distance T. If(t)1 and(t)1 for 0 t < T, then the sensitivitybound can be defined as the following:h(T )j",
  "where A denotes the degree-normalized adjacency matrix": "Now, we show that the sensitivity bound for non-backtracking GNNs can be defined as following. Follow-ing Topping et al. (2022), we assume the node features and hidden representations are scalar for betterunderstanding. Lemma 1 (Sensitivity bounds of NBA-GNNs). Consider two nodes i, j V with a random walkdistance T given a (T 1)-layer NBA-GNN as described in Equation (3) and Equation (4).Suppose(t)1 , 1 ,(t)1 , 1 , and 1 for 0 t < T. Then the following holds:hjxi",
  "For d-regular graphs, ( C BT 1 C)j,i decays slower by O(dT ), while ( AT )j,i decays with O((d + 1)T )": "We provide the full proof in Appendix C, based on comparing the degree-normalized number of non-backtracking and simple walks from node i to node j. To the best of our knowledge, we are the first tocompare the degree of over-squashing between GNNs aligned with different types of random walks. Hence,Proposition 3 indicates that NBA-GNNs has a larger sensitivity bound compared to conventional GNNs andsuffers less from over-squashing, experimentally shown in and Figures 4a and 4b. Moreover, for thesensitivity bound of d-regular graphs, consider the case of multiplying the power of B or A to a one-hotvector. Since every entry is always identical and smaller in B, all entries from the resulting vector from thenon-backtracking matrix will have larger values than those from the adjacency matrix.",
  "Expressive Power of NBA-GNN on SBMs": "In the literature on the expressive capabilities of GNNs, comparisons with the well-known k-WL test arecommon.However, since the k-WL test only focuses on graph isomorphism, i.e.graph level tasks, itis inadequate for measuring the expressive power in node classification tasks.Furthermore, due to thesubstantial performance gap between the 1-WL (equivalent to 2-WL) and 3-WL tests, many algorithms fallinto the range between these two tests, making it more difficult to compare them with each other (Huang &Villar, 2021; Wang et al., 2023). It is also worth noting that comparing GNNs with the WL test does notalways accurately reflect their performance on real-world datasets. To address these issues, several studies have turned to spectral analysis of GNNs. From a spectral viewpoint,GNNs can be seen as functions of the eigenvectors and eigenvalues of the given graph. NT & Maehara (2019)showed that GNNs operate as low-pass filters on the graph spectrum, and Balcilar et al. (2020) analyzed theuse of various GNNs as filters to extract the relevant graph spectrum and measure their expressive power.Moreover, Oono & Suzuki (2020) argue that the expressive power of GNNs is influenced by the topologicalinformation contained in the graph spectrum. The eigenvalues and the corresponding adjacency matrix eigenvectors play a pivotal role in establishingthe fundamental limits of community detection in SBM, as evidenced by Yun & Proutire (2019).Theadjacency matrix exhibits a spectral separation property, and an eigenvector containing information about theassignments of the vertex community becomes apparent (Lei & Rinaldo, 2015). Furthermore, by analyzingthe eigenvalues of the adjacency matrix, it is feasible to determine whether a graph originates from theErdsRnyi (ER) model or the SBM (Erds et al., 2013; Avrachenkov et al., 2015). However, these spectralproperties are particularly salient when the average degree of the graph satisfies (log n). For graphs withaverage degrees o(log n), vertices with higher degrees predominate, affecting eigenvalues and complicatingthe discovery of the underlying structure of the graph (Benaych-Georges et al., 2019). In contrast, the non-backtracking matrix exhibits several advantageous properties, even for constant-degreecases.In Stephan & Massouli (2022), the non-backtracking matrix demonstrates a spectral separationproperty and establishes the presence of an eigenvector containing information about vertex communityassignments, when the average degree only satisfies (1) and no(1). Furthermore, Bordenave et al. (2015)have demonstrated that by inspecting the eigenvalues of the non-backtracking matrix, it is possible to discernwhether a graph originates from the ER model or the SBM, even when the graphs average degree remains",
  "GatedGCN0.5864 0.00770.3420 0.00130.2873 0.0219+ NBA0.6429 0.0062+10%0.2539 0.0011+26%0.3910 0.0010+36%+ NBA+LapPE0.6982 0.0014+19%0.2466 0.0012+28%0.3969 0.0027+38%": "constant. This capability enhances NBA-GNNs performance in both node and graph classification tasks,especially in sparse settings. These lines of reasoning lead to the formulation of the following propositions. Proposition 4. (Informal) Assume the average degree in the stochastic block model satisfies the conditionsof being at least (1) and no(1). In such a scenario, NBA-GNN can map from graph G to node labels. Proposition 5. (Informal) Suppose we have a pair of graphs with a constant average degree, one generatedfrom the stochastic block model and the other from the ErdsRnyi model. In this scenario, NBA-GNN iscapable of distinguishing between them. Proposition 4 suggests that even if a given graph is too sparse to extract node class information usinga GNN with the adjacency matrix, NBA-GNN can still successfully classify the nodes with a probabilityapproaching 1. Similarly, Proposition 5 extends this argument to the graph classification problem. Therationale behind these valuable properties of NBA-GNNs in sparse scenarios lies in the fact that the non-backtracking matrix Bk exhibits similarity to the k-hop adjacency matrix, while Ak is mainly influencedby high-degree vertices. This enables NBA-GNNs to extract valuable information from the spectrum of thenon-backtracking matrix, aiding in the recovery of the hidden structure of the graph. For these reasons,NBA-GNNs would outperform traditional GNNs in both node and graph classification tasks, particularlyin sparse graph environments. These propositions integrate prior work on the non-backtracking matrix intoGNNs, contributing to a deeper understanding of the expressive power within GNN structures. Such anapproach has the potential to advance the field by introducing new perspectives and methodologies withinthe GNN framework. For an in-depth exploration of this argument, please refer to Appendix D.",
  "Long-Range Graph Benchmark": "The long-range graph benchmark (Dwivedi et al., 2022, LRGB) considers a set of tasks that require learn-ing long-range interactions.We validate our method using three datasets from the LRGB benchmark:Peptides-func (graph classification), Peptides-struct (graph regression), and PascalVOC-SP (node classi-fication). We adopt performance scores from Dwivedi et al. (2022) for GNNs and from each baseline paper:",
  "NBA-GatedGCN+LapPE0.6982 0.00140.2466 0.00120.3969 0.0027": "subgraph based GNNs (Abu-El-Haija et al., 2019; Michel et al., 2023; Giusti et al., 2023), graph Trans-formers (Kreuzer et al., 2021; Rampek et al., 2022; Shirzad et al., 2023; He et al., 2023), graph rewiringmethods (Gasteiger et al., 2019; Gutteridge et al., 2023), and state space model (Wang et al., 2024). ForNBA-GNNs and NBA-GNNs with begrudgingly backtracking, we report the one with better performance.Furthermore, LapPE, i.e., Laplacian positional encoding (Dwivedi et al., 2023), is applied as it enhances theperformance of NBA-GNNs in common cases. As one can see in , NBA-GNNs show improvement regardless of the combined backbone GNNs, i.e.,GCN (Kipf & Welling, 2017), GIN (Xu et al., 2019), and GatedGCN (Bresson & Laurent, 2018), aligningwith our results in Proposition 3. Specifically, NBA-GCN outperforms GatedGCN, (i) confirming that simplyupdating both node and edge features does not lead to performance improvement and (ii) showing that non-backtracking is a key component for solving long-range interaction tasks. Furthermore, when compared to avariety of recent baselines in , at least one NBA-GNN shows competitive performance with the bestbaseline, except for the state space model for PascalVOC-SP. It is also noteworthy that the improvement ofNBA-GNNs is higher in dense graphs, where PascalVOC-SP has an average degree of 8 while Peptides-funcand Peptides-struct have an average degree of 2.",
  "ModelCoraCiteSeerPubMedTexasWisconsinCornell": "NBA-GCN0.87220.00950.75850.01750.88260.00440.71080.07960.74710.03860.61080.0614NBA-GraphSAGE0.87020.00830.75860.02130.88710.00440.72700.09050.77650.05080.64590.0691NBA-GAT0.87220.01200.75490.01710.88290.00430.66220.05140.70590.05620.58380.0558 GatedGCN0.84770.01560.73250.01920.86710.00600.61080.06520.58240.06410.52160.0987EGNN0.87690.01250.75670.02210.87690.00280.65950.05270.67840.04070.59460.0573CensNet0.86480.01380.75160.01620.87530.00760.64050.05100.66080.04630.61620.0707 datasets (Texas, Wisconsin, and Cornell) (Pei et al., 2019). We use three conventional GNN architectures- GCN (Kipf & Welling, 2017), GraphSAGE (Hamilton et al., 2017), and GAT (Velikovi et al., 2018) -and one spectral GNN architecture, ChebNet (Defferrard et al., 2016), as the backbone of NBA-GNN in Ta-ble 3. The results indicate that the non-backtracking update improves the performance of all GNN variants.Furthermore, shows significant enhancements in heterophilic datasets. Given that related nodes inheterophilic graphs are often widely separated (Zheng et al., 2022), the ability of NBA-GNN to alleviateover-squashing plays a vital role in classifying nodes in such scenarios.",
  "Ablation Studies": "In this section, we conduct ablation studies to empirically verify our framework. For simplicity, we use BAfor backtracking GNNs and BG for begrudgingly backtracking GNNs. All experiments are averaged over 3seeds, and hyper-parameters for GCN from Tnshoff et al. (2023) are used for a fair comparison. Non-backtracking vs. Backtracking.We first verify whether the performance improvements indeedstem from the non-backtracking updates. To this end, we compare our NBA-GNN with a backtrackingvariant, coined BA-GNN. To be clear, BA-GNN allows backtracking update prohibited in NBA-GNN, i.e.,using h()ij to update h(+1)jiin Equation (3). From Figures 4a and 4b, NBA-GCN consistently outper-forms the BA-GCN and GCN regardless of the number of layers, aligning with the results of Proposition 3.Intriguingly, one can also observe that BA-GCN outperforms the nave backbone, i.e., GCN, consistently.",
  "GAT64.80265051.0GAT+NBA416.852906440.0": "Begrudgingly Backtracking Updates in Sparse Graphs.Additionally in c, we investigatethe effect of begrudgingly backtracking in sparse graphs, i.e., Peptides-func, and Peptides-struct. Onecan see that begrudgingly backtracking is effective in Peptides-func, and shows similar performance inPeptides-struct (Note that the PascalVOC-SP does not have a vertex with degree one). Non-backtracking & Edge Features Updates.From another point of view, NBA-GNN can be in-terpreted as an architecture only updating edge-wise features using backbone GNN layers.Therefore,we conduct additional experiments to validate that the performance improvement truly comes from non-backtracking rather than updating edge features in . We consider three GNN architectures that alsoupdate edge features: GatedGCN (Bresson & Laurent, 2018), EGNN (Gong & Cheng, 2019), and CensNet(Jiang et al., 2020). As seen in , NBA-GNN outperforms these models on most datasets, showing theeffectiveness of non-backtracking over edge feature updates. It is noteworthy that NBA-GNN only updatesedge features derived from initial node features, while EGNN and CensNet update both node and edgefeatures from initial node and edge features. We report detailed implementation in Appendix E.3.3.",
  "In this section, we analyze the space and time complexity of some baselines and NBA-GNN. All experimentswere conducted on a single RTX 3090": "Space Complexity.NBA-GNNs generate messages for each edge considering directions and pass thesemessages in a non-backtracking matter. This process requires 2|E| messages, and (davg 1)|E| connectionsamong messages where davg is the average degree of the graph. Although this may seem substantial, ithas not been a bottleneck in practice and can be mitigated by adjusting the batch size. Moreover, this isa relatively less computation compared to DRew, which requires computation over k-hop neighbors (withk being the number of layers). We report the memory usage for the transductive node classification taskin and the LRGB task in (DRew and PathNN could not fit into a single GPU due to",
  "memory overflow for PascalVOC-SP). NBA-GNN shows less memory usage compared to graph Transformerarchitectures, although it consumes more memory than MPNNs": "Time Complexity.We also report the average test time per epoch for LRGB in . Every experimenthas been conducted with the largest batch size that fits the GPU. NBA-GNN shows competitive timecompared to other baselines, though it does suffer in graphs with a high average degree. Preprocessing Time Complexity.Finally, we investigate the time complexity of preprocessing inRFGNN (Chen et al., 2022), PathNN-SP (Michel et al., 2023), DRew (Gutteridge et al., 2023), and NBA-GNN on the Peptides-func dataset. To be specific, RFGNN requires k-depth non-redundant tree, PathNN-SP needs the single shortest path between nodes, and DRew demands the k-hop neighbors information.NBA-GNN requires the computation of the non-backtracking edge adjacency, which can be computed inO(|E|2), even O(davg|E|) if the data is provided in the form of an adjacency list. We denote b as the branch-ing factor, k as the number of layers, and davg as the average degree of nodes in a graph. Inevitably, previousworks are dependent on the number of layers, while NBA-GNN remains irrelevant to the number of lay-ers. In , one can see that NBA-GNN shows superiority in both theoretical time complexity andpractical computation time (RFGNN codes were not reported by the authors).",
  "Conclusion": "We have introduced a message-passing framework applicable to any GNN architectures to alleviate over-squashing. As theoretically shown, NBA-GNNs mitigate over-squashing in terms of sensitivity and enhancetheir expressive power for both node and graph classification tasks on SBMs. Additionally, we have demon-strated that NBA-GNNs achieve competitive performance on the LRGB benchmark, and show improvementsover conventional GNNs across transductive node classification tasks, even in heterophilic datasets.",
  "Acknowledgements": "S. Park and S. Ahn were supported by Institute of Information & communications Technology Planning& Evaluation (IITP) grant funded by the Korea government (MSIT) (No.RS-2019-II191906, ArtificialIntelligence Graduate School Program(POSTECH)), the National Research Foundation of Korea (NRF)grant funded by the Korea government (MSIT) (No.2022R1C1C1013366), and Basic Science ResearchProgram through the National Research Foundation of Korea (NRF) funded by the Ministry of Educa-tion(2022R1A6A1A0305295413). N. Ryu and S. Yun were supported by Institute of Information & com-munications Technology Planning & Evaluation (IITP) grant funded by the Korea government (MSIT)",
  "Florent Benaych-Georges, Charles Bordenave, and Antti Knowles. Largest eigenvalues of sparse inhomoge-neous erdsrnyi graphs. The Annals of Probability, 47(3):16531676, 2019": "Mitchell Black, Zhengchao Wan, Amir Nayyeri, and Yusu Wang.Understanding oversquashing in gnnsthrough the lens of effective resistance. In International Conference on Machine Learning, pp. 25282547.PMLR, 2023. Charles Bordenave, Marc Lelarge, and Laurent Massouli. Non-backtracking spectrum of random graphs:community detection and non-regular ramanujan graphs.In 2015 IEEE 56th Annual Symposium onFoundations of Computer Science, pp. 13471357. IEEE, 2015.",
  "Paul W Holland, Kathryn Blackmond Laskey, and Samuel Leinhardt. Stochastic blockmodels: First steps.Social networks, 5(2):109137, 1983": "Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, andJure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. Advances in neuralinformation processing systems, 33:2211822133, 2020. Weihua Hu, Matthias Fey, Hongyu Ren, Maho Nakata, Yuxiao Dong, and Jure Leskovec.Ogb-lsc: Alarge-scale challenge for machine learning on graphs. In Thirty-fifth Conference on Neural InformationProcessing Systems Datasets and Benchmarks Track (Round 2), 2021.",
  "Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. InInternational Conference on Learning Representations, 2017": "Devin Kreuzer, Dominique Beaini, Will Hamilton, Vincent Ltourneau, and Prudencio Tossou. Rethinkinggraph transformers with spectral attention.Advances in Neural Information Processing Systems, 34:2161821629, 2021. Florent Krzakala, Cristopher Moore, Elchanan Mossel, Joe Neeman, Allan Sly, Lenka Zdeborov, and PanZhang. Spectral redemption in clustering sparse networks. Proceedings of the National Academy of Sci-ences, 110(52):2093520940, 2013.",
  "Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference onLearning Representations, 2018": "Pierre Mah, Nobuhisa Ueda, Tatsuya Akutsu, Jean-Luc Perret, and Jean-Philippe Vert. Extensions ofmarginalized graph kernels. In Proceedings of the twenty-first international conference on Machine learning,pp. 70, 2004. Gaspard Michel, Giannis Nikolentzos, Johannes F Lutzeyer, and Michalis Vazirgiannis. Path neural networks:Expressive and accurate graph neural networks. In International Conference on Machine Learning, pp.2473724755. PMLR, 2023. Kevin P Murphy, Yair Weiss, and Michael I Jordan. Loopy belief propagation for approximate inference:an empirical study. In Proceedings of the Fifteenth conference on Uncertainty in artificial intelligence, pp.467475, 1999.",
  "A.1Line Graph Neural Networks": "Line Graph Neural Networks (LGNN) (Chen et al., 2018) were the pioneers in applying the non-backtrackingoperator to GNNs. However, it does not resolve redundancy issues, as it employs both an adjacency matrixand non-backtracking matrix in a layer. In other words, our main contribution over LGNN is in considerationof the non-backtracking operator specifically for the message redundancy problem, in a end-to-end manner.Notably, the computational complexity of LGNN is acknowledged to be close to |V | log(|V |) by its authors.",
  "LGNN0.42020.377887.76197.10NBA-GCN+LapPE0.97240.72070.54151.18": "In our experiments on Peptides-func, we compare the performance and complexity of LGNN and NBA-GCN+LapPE, as detailed in . Given that LGNN was initially proposed using only node degreesfor node features, we incorporated an additional node feature encoder, similar to our NBA-GNN setup.Specifically, we used a batch size of 64, hidden dimension of 24, and 4 layers for LGNN. Refer to Appendix E.2for the hyperparameters of NBA-GCN. The drawbacks of LGNN in terms of time and space complexitybecome evident when compared to NBA-GNN, while requiring space and time for computing both simplerandom walks and non-backtracking walks, but result in poor performance.",
  "A.2Redundancy-free Graph Neural Network": "Redundancy-Free Graph Neural Network (RFGNN) (Chen et al., 2022) shares the common motivation withNBA-GNN, aiming to reduce redundant messages in the computation graph.RFGNN achieves this byconstructing a tree for every node, coined Truncated ePath Tree (TPT). TPT ensures that there are norepeated nodes along the simple path from the root to the leaf, except for the root node which can appeartwice in the path.This approach differs significantly from NBA-GNN, which eliminates redundancy byidentifying non-backtracking edge adjacency.",
  "|V t1|!, where": "|V | is the number of nodes and t is the number of GNN layers. In contrast, NBA-GNN achieves superiorcomplexity with space complexity O(2|E|) and time complexity O(davg|E|), which remains irrelevant tothe number of layers. The preprocessing process time complexity, involved in finding non-backtrackingedge adjacency, is O(|E|2). This scalability allows NBA-GNN to handle larger graph dataset compared toRFGNN. Furthermore, in theoretical aspects, our work distinguishes itself by providing the sensitivity upperbound of non-backtracking updates, rather than comparing the relative inference of paths. : Average test accuracy of LGNN, NBA-GNN and BP on two sparse SBMs, where n represents thenumber of vertices, C is the number of classes, and p and q denote edge probabilities within and acrosscommunities, respectively.",
  "A.3Stochastic Block Models": "We also conduct a comparative analysis for our method on two sparse Stochastic Block Models (SBMs)with distinct parameters for Belief propagation (BP), and LGNN: (a) a binary assortative SBM (n = 400,C = 2, p = 20/n, q = 10/n), and (b) a 5-community dissociative SBM (n = 400, C = 5, p = 0, q =18/n). To be specific, n denotes the number of vertices, C denotes the number of classes, and p and q denoteedge probabilities within and across communities, respectively. illustrates the average test accuracyacross 100 graphs. Notably, BP, known for achieving the information-theoretic threshold, exhibited the bestperformance, consistent with expectations. Additionally, NBA-GNN outperforms LGNN in both scenarios.",
  "BProofs for .1": "In this section, we present the findings discussed in .1. Similar to Di Giovanni et al. (2023), weconcentrate on the relationship between over-squashing and access time of non-backtracking random walks.Our study establishes that the access time using a begrudgingly backtracking random walk (BBRW) issmaller than that of a simple random walk (SRW) between two nodes, in tree graphs. Also, it is noteworthythat the gap between these two access times increases as the length of the walk grows. In the following, we will compare the access times between BBRW and SRW. First, we show that on the treegraph, the access time equals the sum of access times between neighboring nodes. Note that the access timebetween neighboring nodes, which is the cut-point, can be represented in terms of return time. The formulasfor return time in Fasino et al. (2023) and Lemma 6 allow us to derive the formulas for access time betweenneighboring nodes. Finally, we derive and compare the access time of BBRW and SRW, i.e., Proposition 1. Proposition 1. Given a tree G = (V, E) and a pair of nodes i, j V, the access time of begrudginglybacktracking random walk is equal to or smaller than that of a simple random walk. The equality holds ifand only if the walk length is 1.",
  "B.1.2Access Time": "Consider a SRW starting at a node i V. Let Ti denote the time when a SRW first arrived at node i,Ti := min{n 0|xn = i}, and T i be the time when a random walk first arrived at node i after the first step,T i := min{n > 0|xn = i}. With slight abuse of notation, we define access time t(i, j) from i to j, accesstime t(i j, k) from i to k where x1 = j, and return time t(i; G) in a graph G as follows:",
  "t(i; G) := E[T i|x0 = i]": "Similarly, we denote the access time from i to j, access time from i to k with where x1 = j and return timeof BBRW as t(i, j), t(i j, k) and t(i; G), respectively. Note that the first step of BBRW shows the samebehavior as the SRW since there is no previous node on the first step. Finally, for a tree graph G and pair of nodes i, j in Proposition 1, we denote the unique paths between i andj as (v0, . . . , vN) with i = v0, j = vN. We also let N denote the distance between the nodes i and j.",
  "B.2.1Decomposition of Access Time": "First, we show that the access time is equal to the sum of access times between neighboring nodes. When arandom walker travels from v0 to vN, it must pass through all nodes vn on the paths. Intuitively, We canconsider the entire time taken as the summation of the time intervals between when the walker arrived atvn and when it arrived at vn+1.",
  "n=0t(vn, vn+1)": "Proof. Given a unique path (v0, . . . , vN) between v0 and vN, any walk between (v0, vN) can be decomposedinto a series of N 1 walks between (v0, v1), (v1, v2), . . . (vN1, vN) such that the walk between (vn, vn+1)does not contain a node vn+1 except at the end point. The expected length of each walk is t(vn, vn+1).",
  ". The walk transitions from node i to j based on transitioning with respect to the edge (i, j) E withprobability1di . The walk terminates": "2. The walk fails to reach j and continues the walk in the subtree Gi until arriving at the node i again.Note that the walk cannot arrive at node j without arriving at node i in prior. In other words, thewalk continues for the return time of i concerning the graph Gi. The two scenarios imply that every time the walk arrives at node i, the walk terminates with probability1di . Then the number of trials follows the geometric distribution and consequently, the average number oftrials is di. In other words, the walk falls into the scenario of type 2, for di 1 times on average. We haveto traverse at least one edge (i, j). The expected total penalty is the product of the average failure penaltyand the average number of failures. Thus,",
  "B.3.1Decomposition of Access Time": "We start by decomposing the access time t(i, j) similar to the one in Lemma 2. However, a key differenceexists in the BBRW random walk. When the walk first arrives at node vn, it previously passed the nodevn1. Therefore, we cannot return to vn1 upon the first failure to reach vn+1.",
  "Proof. Consider the tree as a rooted tree where the root is i. In the following, we will use mathematicalinduction based on the tree height of i": "First, consider the base case where the height of the tree is 1. Then, whatever we choose as the next nodex1 from x0 = i, we return to i at the second transition (i.e., x2 = i) since all the neighbors of i are leaf node.Since di = |E| for tree with height 1, t(i; G) = 2 = 2|E|",
  "di": "Now, assume that the lemma holds for the tree with a height less than k 1. It suffices to show that thelemma also holds for a tree with height k + 1 and its root i. From the same perspective in the proofs of theLemma 3, we can view the random walk returning to i as follows:",
  "C.1Preliminaries": "Let G be a graph with a set of n vertices V, a set of m edges E V2. We use xi to denote the node-wise featurefor i V, and di for the degree of node i V. The adjacency matrix A Rnn encodes the connectivityof graph G. For node i V, we define the set of incident outgoing edges of i as N +e (i), the set of incidentincoming edges of i as N e (i), and let Ne(i) = N +e (i) N e (i) be the set of all incident edges of node i. Also,recall the non-backtracking matrix B {0, 1}2|E|2|E| and the incidence matrix C R2|E||V|:",
  "To assess the expressive capabilities, we initially make an assumption about the graphs, considering that itis generated from the Stochastic Block Model (SBM), which is defined as follows:": "Definition 1. Stochastic Block Model (SBM) is generated using parameters (n, K, , P), where n de-notes the number of vertices, K is the number of communities, = (1, ..., K) represents the probability ofeach vertex being assigned to communities V1, ..., VK, and Pij denotes the probability of an edge (v, w) Ebetween v Vi and w Vj. Numerous studies have focused on the problem of achieving exact recovery of communities within the SBM.However, these investigations typically address scenarios in which the average degree is at least on the orderof (log n) (Abbe, 2017). It is well-established that the information-theoretic limit in such cases can bereached through the utilization of the spectral method, as demonstrated by Yun & Proutire (2016). Incontrast, when dealing with a graph characterized by the average degree much smaller, specifically o (log n),recovery using the graph spectrum becomes a more challenging endeavor. This difficulty arises due to the factthat the n1o(1) largest eigenvalues and their corresponding eigenvectors are influenced by the high-degreevertices, as discussed in Benaych-Georges et al. (2019). However, real-world benchmark datasets often fall within the category of very sparse graphs. For example,the citation network dataset discussed in Sen et al. (2008) has an average degree of less than three. In suchscenarios, relying solely on an adjacency matrix may not be an efficient approach for uncovering the hiddengraph structure. Fortunately, an alternative strategy is available by utilizing a non-backtracking matrix.",
  "n, b": "n.In this case, we have two eigenvalues 1 > 2 of ndiag()P, and the eigenvector 2 corresponding to 2,where v-th component is set to (v). Then, for any n larger than an absolute constant, the eigenvalues 1and 2 of the non-backtracking matrix B satisfies |ii| = o(1), and all other eigenvalues of B are confined",
  "T 2, T {0, 1}2mn, Tei = 1{e2 = i} and {0, 1}n2, ij = 1 if the vertex i is in the j-thcommunity, and 0 otherwise": "The proposition above highlights that the non-backtracking matrix possesses a spectral separation property,even in the case of very sparse graphs. Moreover, the existence of an eigenvector 2 such that 2, 2 = 1o(1)suggests that this eigenvector contains information about the community index of vertices. The foundationfor these advantageous properties of the non-backtracking matrix B in sparse scenarios can be attributedto the observation that the matrix Bk shares similarities with the k-hop adjacency matrix, while Ak ispredominantly influenced by high-degree vertices. Consequently, we can establish the following lemma:",
  "(BH) = c + o(1)and|2(BH)| c + o(1)": "Proposition 9 informs us that by examining the distribution of eigenvalues, we can discern whether a graphoriginates from the ErdsRnyi model or the SBM. Leveraging Lemma 9, we can obtain the top twonormalized eigenvectors of the non-backtracking matrix using convolutional layers, denoted as 1 and 2.Applying the non-backtracking convolutional layer to these vectors yields resulting vectors with 2-normscorresponding to i(B). Consequently, we can distinguish between the two graphs, G and H, by examiningthe output of the convolutional layer in the non-backtracking GNN.",
  "D.3Proof of Lemma 8": "Proof. Let us revisit the matrix T, defined as Tei = 1{e2 = i}, and its pseudo-inverse denoted as T + =D1T , where D is a diagonal matrix containing the degrees of vertices on the diagonal. Considering thedefinition of 2 as provided in Proposition 8, we can deduce the label of vertex v. Specifically, if (T +2)v > 0,it implies that the vertex belongs to the first class; otherwise, it belongs to the other class. Additionally, we are aware that (T +2 T +2)i 2 = O(f(a, b)) as indicated in Proposition 8, consideringthe property that the sum of each row of T + is equal to 1. Consequently, by examining the signs of elementsin the vector T +2, we can classify nodes without encountering any misclassified ones.",
  "D.4Proof of Lemma 9": "Proof. Suppose we have f arbitrary vectors x1, ..., xf and a matrix X = [x1, ..., xf] R2mf, which hasx1, ..., xf as columns. Without loss of generality, we assume that f 2m and xv2 = 1. Let xj = 2mi=1 c(j)i ifor 1 j f. We will prove the lemma by showing that if we multiply X by B and repeatedly apply theGramSchmidt orthnormalization to the columns of the resulting matrix, the j-th column of the resultingmatrix converges towards the direction of j. Further, we will show that there exists a series of convolutionallayers equivalent to this process.",
  "E.2.1Dataset Statistics": "From LRGB (Dwivedi et al., 2022), we experiment for 3 tasks: graph classification (Peptides-func), graphregression (Peptides-struct), and node classification (PascalVOC-SP). We provide the dataset statistics in. Note that for PascalVOC-SP, we use SLIC compactness of 30, and edge weights are based only onsuper-pixels coordinates following the recent work (Gutteridge et al., 2023).",
  "E.3.1Dataset statistics": "We conducted experiments involving three citation networks (Cora, CiteSeer, and Pubmed in Sen et al.(2008)), and three heterophilic datasets (Texas, Wisconsin, and Cornell in Pei et al. (2019)), focusing ontransductive node classification. Our reported results in are the averages obtained from 10 differentseed runs to ensure robustness and reliability. For the citation networks, we employed the dataset splitting procedure outlined in Yang et al. (2016). Incontrast, for the heterophilic datasets, we randomly divided the nodes of each class into training (60%),validation (20%), and testing (20%) sets. We provide more details of dataset statistics in .",
  "E.3.2Experiment Details": "The training duration spanned 1,000 epochs for citation networks and 100 epochs for heterophilic datasets.Following training, we selected the best epoch based on validation accuracy for evaluation on the test dataset.We used the AdamW optimizer (Loshchilov & Hutter, 2018) with a learning rate of 3e-5. The models hid-den dimension and dropout ratio were set to 512 and 0.2, respectively, consistent across all datasets, afterfine-tuning these hyperparameters on the Cora dataset. Additionally, we conducted optimization for thenumber of convolutional layers within the set {1, 2, 3, 4, 5}.The results revealed that the optimal num-ber of layers is typically three for most of the models and datasets. However, there are exceptions, suchas CiteSeer-GatedGCN, PubMed-{GraphSAGE, GraphSAGE+NBA+PE}, Wisconsin-{GraphSAGE+NBA,GraphSAGE+NBA+PE} and Cornell-{GraphSAGE, GraphSAGE+NBA, GraphSAGE+NBA+PE}, wherethe optimal number of layers is found to be four.Furthermore, for Cora-{GraphSAGE+NBA+PE,GAT+NBA}, CiteSeer-GraphSAGE+NBA, the optimal number of layers is determined to be five.",
  "E.3.3Baseline implementation": "In .3, we compared NBA-GNNs with several baselines to verify the effectiveness of non-backtrackingupdates. Three baselines that update edge features were considered: GatedGCN (Bresson & Laurent, 2018),EGNN (Gong & Cheng, 2019), and CensNet (Jiang et al., 2020).Though these baselines update edgefeatures, the six datasets used for transductive node classification do not have initial edge features. Basedon each paper and its code, we used the pairwise cosine similarities between corresponding node features asedge features for CensNet, and encoded each directed edge into a vector v {0, 1}3 for EGNN. Additionally,we utilize the attention-based EGNN model, referred to as EGNN(A) in the original paper."
}