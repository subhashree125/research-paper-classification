{
  "Abstract": "Recent work in adversarial robustness suggests that natural data distributions are localized,i.e., they place high probability in small volume regions of the input space, and that thisproperty can be utilized for designing classiers with improved robustness guarantees for2-bounded perturbations. Yet, it is still unclear if this observation holds true for moregeneral metrics. In this work, we extend this theory to 0-bounded adversarial perturbations,where the attacker can modify a few pixels of the image but is unrestricted in the magnitudeof perturbation, and we show necessary and sucient conditions for the existence of 0-robustclassiers. Theoretical certication approaches in this regime essentially employ voting overa large ensemble of classiers. Such procedures are combinatorial and expensive or requirecomplicated certication techniques. In contrast, a simple classier emerges from our theory,dubbed Box-NN, which naturally incorporates the geometry of the problem and improvesupon the current state-of-the-art in certied robustness against sparse attacks for the MNISTand Fashion-MNIST datasets.",
  "Introduction": "It is by now well known that adversarial attacks aect Machine Learning (ML) systems that can potentiallybe used for security sensitive applications. However, despite signicant eorts on robustifying ML modelsagainst adversarial attacks, it has been observed that their performance on most tasks under adversarialperturbation is not close to human levels. This motivated researchers to obtain theoretical impossiblityresults for adversarial robustness Shafahi et al. (2018); Dohmatob (2019); Dai & Giord (2022), which statethat for general data distributions, no robust classier exists against adversarial perturbations, even when theadversary is limited to making small p-norm-bounded perturbations. However, such results are seemingly inconict with the fact that humans can classify most natural images quite well under small p-norm-boundedperturbations. Even more, there is a rich literature on certied robustness, e.g., Zhang et al. (2018); Cohenet al. (2019); Pal & Vidal (2020); Fischer et al. (2020); Jeong & Shin (2020); Jia et al. (2022); Pfrommer",
  "et al. (2023); Salman et al. (2022); Eiras et al. (2022); Pal & Sulam (2023), where the goal is to obtain andanalyze methods with provable guarantees on their robustness under adversarial attacks": "Pal et al. (2023) recently provided a solution to this apparent conict, noting that existing impossibilityresults become vacuous when the data distribution is such that a large probability mass is concentratedon very small volume in the input space, a property they call (C, , )-concentration. This characterizationimplies that at least 1 probability mass is found in a region of volume at most Cen for small 0and large . As an example, this property dictates that sampling a random 224 224 dimensional image isextremely likely to not be a natural image. This property is intuitively satised for natural datasets likeImageNet, and Pal et al. (2023) formally show that whenever a classier robust against small 2-boundedattacks exists for a data distribution (e.g., humans for natural images), this distribution must be concentrated.This shows that indeed, robust classiers against 2 attacks can be obtained for natural image distributions,and there is no impossibility. While these results are encouraging, attacks that are bounded in Euclidean norm have nice analyticalproperties that facilitated the results in Pal et al. (2023). In this work, we seek to understand if similarnotions can provide insights on provable defenses against sparse adversarial attacks (bounded in their 0distance) where the adversary is limited to modifying a few pixels on the image, but those pixels can bemodied in an unbounded fashion. Even though for humans it seems trivial to correctly classify a naturalimage corrupted in a few pixels, this problem has stood out as a particularly hard task for machine learningmodels. The dierence is extreme: Su et al. (2019) demonstrated that adversarially modifying a single pixelleads to large performance degradation of many state of the art image recognition models. Standard ideasfor improving robustness, like adversarial training, seem to be empirically ineective against sparse attacks.Since then, researchers have resorted to enumerating a large number of subsets of the input pixels, and takinga majority vote over the class predicted from each subset, as a means of obtaining classiers robust to sparseattacks. The resultant methods (Levine & Feizi, 2020b) are expensive, and need probabilistic certicates dueto the combinatorial blow-up in the number of subsets needed as the number of attacked pixels increases.Follow-up work by Jia et al. (2022) has employed complicated certication schemes to reduce the slack inthese certicates, while still remaining computationally expensive. Most recently, Hammoudeh & Lowd (2023)carefully selected these subsets to speed up the certicate computation. However, none of these existingmethods utilizes the geometry of the underlying data distribution highlighted by our results. Departingfrom this stream of research, we propose a classier that closely utilizes this underlying geometry to obtainrobustness certicates. As a result, we provide a classier that is lighter and simpler than all existing works,and an associated certication algorithm with 0 certicates that are better than prior work. Our proof techniques extend results in Pal et al. (2023) to sparse adversarial attacks. In practice, one canalways project the pixel values to lie in some predened range, say , before classication, so we canconsider adversarial perturbations to lie within n without any loss of generality. In other words, ouradversary at power is allowed to modify an image from x to x such that x x0 , x 1. Thetechniques in Pal et al. (2023) break down under such an adversary, as their rst assumption is to restrictattention to adversarial perturbations v such that x + v cannot lie -close to the boundary of the imagedomain. In our case, the geometry of the problem is radically dierent: even a perturbation of size 1 issucient to take any image to the boundary of the domain n (simply perturb any pixel to 1). As aresult, although we are motivated by Pal et al. (2023), our theory and certication algorithms are markedlydierent from those in that work. In the above setting, we show that whenever there exists a classier robust to adversarial modication of afew entries in the input, the underlying data distribution places a large mass, i.e., localizes, on low-volumesubsets of the input space. We further show that the converse holds too, albeit with a strengthening of thelocalization condition; i.e., we show that when the data distribution localizes on low-volume subsets of theinput space, and these subsets are suciently separated from one another, then a robust classier exists.These results suggest that such underlying geometry in natural image distributions should be exploited forconstructing classiers robust against 0 attacks. Indeed, we then propose a simple classier, called BoxNearest Neighbors (Box-NN), that utilizes this underlying geometry by having decision regions that areunions of axis-aligned rectangular boxes in the input space. Such a classier naturally allows for 0 robustnesscerticates that improve upon prior work for certied defenses in a wide regime.",
  "To summarize, we make the following contributions in this work:": "1. In we show that if a data-distribution p dening a multi-class classication problem admitsa robust classier whose error is at most under sparse adversarial perturbations to pixels, thenthere is a subset S of volume at most Ce2/n and a class k such that the class conditional qk placesa large mass qk(S) 1 on S, i.e., qk is (C, 2/n, )-localized. The constant C captures certaingeometric properties of the robust classier, and will be claried later. 2. In , we show that a stronger notion of localization, which ensures that the class conditionaldistributions are suciently separated with respect to the 0 distance, is sucient for the existence ofa robust classier. In fact, this result generalizes to any distance d, showing the existence of a robustclassier w.r.t. perturbations bounded in distance d whenever the data distribution p is stronglylocalized with respect to d. 3. In , we propose a classier certiably robust against sparse adversarial attacks, calledBox-NN, and derive certicates of 0 robustness for it. We then provide empirical evaluation on theMNIST and the Fashion-MNIST datasets, and demonstrate that Box-NN obtains state-of-the-artresults in certied 0 robustness.",
  "Existence of an 0-Robust Classier implies Localization": "We will take our data domain to be n, to mimic the standard natural image classication tasks1, i.e.,X = {x: x 1}. We will take our label domain to be Y = {1, 2, . . . , K}, and assume that we have aclassication task dened by a joint data distribution p over X Y. The marginal distribution over theclasses will be denoted by pY . The conditional distribution pX|Y =k for each class k Y will be denoted byqk2. Further, we will say that the classes are balanced when pY (k) = 1",
  "Similarly, we dene a classier f to be (, )-robust with respect to a distance d if the robust risk againstperturbations at a distance bounded by is at most , i.e., Rd(f, )": "For the rest of this section, we will assume that p denes a task for which one can obtain a classier fsuch that R0(f, 0) , where 0 is a non-negative integer denoting the maximum number of co-ordinatesthat an adversary can perturb. Given such an f, we will show that p should satisfy the special property oflocalization. In other words, we will obtain a necessary condition for 0 robustness. This special property of(C, , )-localization3 is similar to Pal et al. (2023, Denition 2.2), with a slight modication: Denition 2.1 (Localized Distribution, modication of Pal et al. (2023)). A probability distribution q overa domain X Rn is said to be (C, , )-localized if there exists a subset S X such that q(S) 1 butVol(S) C exp(). Here, Vol denotes the standard Lebesgue measure on Rn, and q(S) denotes the measureof S under q. Denition 2.1 is similar to Pal et al. (2023, Denition 2.2) but it removes the explicit dimension of theproblem, i.e., n, from the volume constraint. This allows one to state the results in Pal et al. (2023), aswell as ours, under the same denition. Additionally, we rename the property from concentration in Palet al. (2023) to localization, in order to distinguish ourselves from the well known notion of concentration ofmeasure. These two notions are related and, before proceeding, we compare them in more detail. 1Albeit with a scaling natural images are typically stored with each pixel value in .2The conditional density is dened in a standard fashion as qk(x) = p(x, k)/pY (k).3Here, the quantity C, similar to and , is a parameter of localization, and controls the extent of localization. C can bedetermined directly given access to a distribution p, or indirectly via Theorem 2.2 given access to a robust classier.",
  "Published in Transactions on Machine Learning Research (10/2024)": "Improving Initialization.We initialize by using a set of boxes dened from the data. This is done byrst drawing a subset T of size M uniformly at random from the training data-points, and then initializing with axis-aligned boxes centered at these data-points, as {(B(x 0.1, x + 0.1), y): (x, y) T}, where thescalar is added to, or subtracted from, every co-ordinate of the vector. Having described all the tricks used foroptimizing Box-NN, we now proceed to performing an empirical evaluation in . For the interestedreader, we also ablate over the training strategies mentioned above in Appendix B.2.",
  ", localized for any": "From the above we can see that keeping , a < 1 xed, qa becomes more localized as the dimension nincreases. Similarly, keeping , n xed, qa becomes more localized as a gets closer to 0. In this sense, thelocalization parameters depend on the scale of the support of the underlying distribution. In contrast, as measure concentration depends on an underlying metric, the concentration parameters areindependent of the scale of the support when the metric is invariant to scaling. As an example, for X equippedwith the hamming metric, d0(x, x) = x x0, the concentration function for the distribution qa can beshown to be",
  ".(2)": "Armed with the above denition, we will now derive a necessary condition for 0-robustness in terms oflocalization, by using a measure-concentration result w.r.t. the 0 distance due to Talagrand (1995). Theorem 2.2. If there exists a non-trivial (, )-robust classier f with respect to the 0 distance for a datadistribution p, then at least one of the class conditionals q1, q2, . . . , qK must be (C, 2/n, )localized accordingto Denition 2.1. Further, if the classes are balanced, then all the class conditionals are (Cmax, 2/n, K)-localized. Here, C and Cmax are constants dependent on f.",
  "Discussion on Theorem 2.2.A few comments are in order for the above result": "1. Theorem 2.2 demonstrates that whenever a non-constant 0 robust classier exists for a data distribution,this distribution must be localized. This could be instantiated for real data sets like ImageNet to obtaininteresting observations about the underlying distribution. For instance, humans are robust to perturbationof a few pixels to any image in ImageNet. Then, Theorem 2.2 tells us that ImageNet is localized. Note,however, that the localization parameters (i.e., C, , for the human classier) are unknown.",
  ". The localization parameters in Theorem 2.2 are dierent than the concentration parameters in Pal et al": "(2023, Theorem 2.1). Specically, Pal et al. (2023, Theorem 2.1) shows that (C, n, )-concentrationis a necessary condition for 2-robustness under Denition 2.1, and we will now show that (C, 2/n, )-localization is a necessary condition for 0-robustness. This demonstrates that the existence of a classierrobust to 0 classier implies a dierent kind of localization of the data distribution than robustness to2 perturbations. While Pal et al. (2023) assume that their data lies in a unit 2 ball with adversarialperturbation strength , we assume that our data lies in a unit ball and with perturbationstrength {0, 1, 2, . . . , n}. As such a direct comparison of the parameters is not immediate as our workdeals with objects very dierent from Pal et al. (2023). 3. Theorem 2.2 suggests that for obtaining 0 robust classiers, we should try to nd and classify over thesets that the distribution localizes on. This is a signicant departure from the existing literature on0-robust classiers Levine & Feizi (2020a); Jia et al. (2022); Hammoudeh & Lowd (2023), and indeed, wewill obtain a classier in that respects such geometry.",
  "3d-Strong Localization implies Existence of a d-Robust Classier": "Localization of the data distribution ensures that each class conditional concentrates on a small volumesubset of X. However, as noted in Pal et al. (2023), these subsets might intersect too much, in which casethere might not exist a classier with low standard risk, i.e., R0(f, 0). Hence, one cannot expect localizationto be sucient for the existence of a classier with low robust risk, i.e., R0(f, ) with > 0. However, ifthese subsets were separated enough, then one can expect to use them to build a robust classier. Indeed, wewill now formalize this intuition to obtain a condition stronger than localization, which will be shown to besucient for the existence of a robust classier. Denition 3.1 (d-Strongly Localized Distributions, generalizing Pal et al. (2023)). A distribution p is saidto be (, , )-strongly-localized with respect to a distance d, if each class conditional distribution qk localizes",
  "Theorem 3.2. If p is (, , )-strongly localized with respect to a distance d, then there exists a classier fsuch that Rd(f, ) +": "Proof. At a high level, we will construct a classier g that predicts the label k over an -expansion of the setSk on which the class conditional qk localizes. We will then shave o some regions from each Sk to ensureg is well dened. For the rest of the input space X we will predict an arbitrary label, as we incur at most in robust risk. Our construction of the robust classier f is same as that in Pal et al. (2023), extended togeneral d. However, bounding the robust risk of f needs technical innovations, since we are bounding therobust risk with respect to a general distance d, as opposed to the 2 norm in Pal et al. (2023). For each k {1, 2, . . . , K}, let Sk be the set over which the conditional density qk is localized, i.e., qk(Sk) 1.Dene S+ to be the -expansion of the set S, as S+ = {x: x S, d(x, x) }. Dene Ck to be the-expanded version of the localized region Sk but removing the -expanded version of all other regions Sk, as",
  "qk exp2/n,(7)": "where qk = 2 supx qk(x) depends on the class conditional qk. When qk is localized, qk can grow faster thanexp2/n, making the lower bound vacuous. This implies that for localized data-distributions there is noimpossibility, and there is a wide class of high-dimensional classication problems for which robust classiersexist. We now provide a concrete example. Example 3.1. Let us consider a problem with 2 classes dened by the distribution p such that P(Y = 0) =P(Y = 1) = 1/2, the class conditional q1 = P(X|Y = 1) = Unif(B(1, )), and similarly q2 = P(X|Y =2) = Unif(B(1, )). For this distribution, q1 = q2 = exp(n), and the lower bound (7) becomes vacuousfor n as1 qk exp2/n= 1 2 exp(2/n + n) 0. Even though Example 3.1 is quite simple, the construction of small balls in the input space containingmost of the mass of the distribution is quite general, and depicts a wide class of data-distributions whereexisting impossibility results are vacuous. We will now demonstrate that these general theoretical ideas leadto practical 0 robust classiers.",
  "-Adversarially Robust Classication via the Box-NN classier": "In this section, our aim will be to derive a 0-robust classier by utilizing the geometry exposed by Theorem 3.2.To this end, we will rst investigate how a robust classier looks like for a simple 2-class problem in 3-dimensions. This will motivate a general form of a classier whose decision regions are axis-aligned cuboids,or boxes. Finally, we will generalize this classier to obtain a 0-robust classier and derive corresponding 0certicates.",
  "Development and Robustness Certication": "Consider n = 3, and say there are two classes, cat and dog, dening conditional distributions q1 and q2,strongly localized over S1 and S2 respectively, such that q1(S+12 ) = 0 and q2(S+11 ) = 0. In such a situation,Theorem 3.2 (invoked with = 1) constructs a robust classier fA as the following:",
  ": S1 is the green shaded region around xdog, where the class dog is localized, and S2 is the orangeshaded region around xcat, where the class cat is localized": "From , we see that the classier fA is robust to 1-pixel perturbations whenever x S1 or x S2, asTheorem 3.2 predicts. More importantly, we see that a perturbation of a single pixel of any xcat S2 lieswithin the union of the orange cuboids. In other words, {x 3 : xx0 1, x S1} = S+11 Orange,and similarly for the dog class. Furthermore, we see that the intersection of these orange cuboids is given bythe cube Bcat. We can see that for any x Bcat, no single-pixel perturbation v can take x + v outside theorange region Orange, and similarly for the dog class. However, Bcat, Bdog are very eciently described,they are simply axis-aligned polyhedra enclosing S2 and S1 respectively. This motivates our modied classierfB,",
  "dog,if x B+1dogcat,if x B+1catcat,otherwise": "While fB is ecient to describe, it ignores a large portion of the input region outside the green and the orangecuboids, i.e., X \\ B+1dog B+1cat, by making the constant prediction cat in this region. We can further extend fBto attempt to correctly classify those regions as well, by computing 0 distances to our boxes Bcat, Bdog, as",
  "wheredist(x, S) = minvv0 sub. to x + v S(8)": "gives the minimum number of pixel changes needed to get from x to S. While solving (8) is computationallyhard for general S, the following lemma shows that for our axis-aligned boxes B, (8) can be computedeciently, in closed form. The proofs of all our results can be found in Appendix A.",
  "which can be computed in O(n) operations": "For real data distributions, however, having a single box per class would be overly simplistic and not providegood accuracy. Thus, we generalize fC to our Box-NN classier operating on boxes B = {B1, B2, . . . , BM},such that we have an label ym {1, 2, . . . , K} associated with each Bm. Our Box-NN classier is thendened asBox-NN(x, B) = ym, where m = arg minmdist(x, Bm). Note that, so far, we have not described how these boxes B are learned from data. This will be the subjectof .2 and onward. We can now obtain a 0 robustness certicate for Box-NN via the followingTheorem.",
  "Then, with margin(x)def= d2 d1, we have Box-NN(x, B) = Box-NN(x, B) whenever x x0 <margin(x)/2": "Key Intuition.Our robust classier Box-NN is essentially a generalization of the nearest-neighbor classierto a nearest-box classier, specically suited to 0 metrics. This simple form turns out to be the right choice,in the sense of the theoretical motivation of our previous section, for defending against sparse perturbations.As we will shortly see, Box-NN also empirically produces better certicates than prior work in severalregimes.",
  "Learning Box-NN from Data": "In this section, we are concerned with learning boxes {Bm} and their associated labels {ym}, such thatBox-NN obtains a high accuracy under sparse adversarial perturbations. For the rest of this section, we willrefer to the classier Box-NN as f, with the learnable parameters = {ak, bk, yk}Mk=1 following the notationin Lemma 4.1. The quantity we are interested in maximizing is the robust accuracy, dened as 1 R0(f, ) following ournotation in . As we do not have access to the data distribution, we will instead be concerned withmaximizing the empirical robust accuracy RobustAcc(f, ) dened over a set of samples {xi, yi}Ni=1 given by",
  "i=11[f(xi) = yi] 1[C(xi) ].(10)": "We will take a gradient based optimization approach to maximize (10) over . However, since the gradientsof 1[] are zero almost everywhere (and discontinuous otherwise), we will progressively relax the indicators in(10). To this end, we maximize the integral of CertAcc(f, ) over all 0 instead of treating it point-wise4,leading to the objective",
  "margin(x) = minm dist(x, Bm) minm: ym=ym dist(x, Bm)": "The gradient of min w.r.t. its input (c1, . . . , cM) is extremely sparse5, and hence a very small number ofparameters i are updated at each step of gradient descent using gradients of (11). As a result, optimizationis extremely slow. We remedy this by using a soft approximation to min which has dense gradients,",
  "where is a parameter that approximately controls the sparsity of the gradients. The function min is equalto min in the limit , and reduces to the average when = 0": "Furthermore, we nd that for many data points xi, a small number of boxes m contribute a lot to the nalloss due to large distances dist(xi, Bm). As a result, learning is slow for parameters corresponding to theremaining boxes. To prevent such imbalance, we clip the certicates to 50. With these approximations, weobtain",
  "minminmdist(x, Bm) minm: ym=ymdist(x, Bm), 50.(13)": "Relaxing Indicator Functions.Now observe that L2 is still a function of indicator functions, due to thedist function in (13), which was derived in Lemma 4.1 to be dist(x, B(a, b)) = ni=1 1 (xi [ai, bi]). Again,as the gradients of 1[] are zero almost everywhere, we perform a conical approximation to 1 (xi [ai, bi])which has non-zero gradients:",
  "Empirical Evaluation": "In this section, we will briey describe existing methods for probabilistic 0 certication, (Levine & Feizi,2020b) and (Jia et al., 2022) as well as deterministic 0 certication (Hammoudeh & Lowd, 2023), and thenempirically compare our (deterministic) 0 certied defense Box-NN to these approaches. Levine & Feizi (2020b) and Jia et al. (2022) extend the technique of randomized smoothing (Cohen et al.,2019) to randomized ablation (RA), where given any classier f (e.g., a neural network), they produce asmoothed classier by zeroing out pixels uniformly at random:",
  "RA(x) = arg maxkPvUnif(V) (f(x v) = k) ,(15)": "where V = {v {0, 1}n : v0 = } is the discrete set of all binary vectors of length n having exactly ones, and denotes the Hadamard product. For this construction in (15), a counting argument leads to therobustness certicate in Levine & Feizi (2020b), which we compare to in . A more complicated analysisbased on the Neyman-Pearson lemma leads to a tighter certicate in Jia et al. (2022), which is also includedin our comparison in (left), denoted by RAB . Both these certicates are randomized, i.e., they holdwith a condence 1 , where , are hyper-parameters that trade-o benign accuracy to robustness, andcan be chosen empirically. According to standard practice, we x = 0.05 and produce plots for varying .The interested reader can refer to Appendix B.3 for a description of the certication procedures developed inLevine & Feizi (2020b); Jia et al. (2022).",
  "Certified Accuracy": "RA10RA20RA40RA60BoxNN : Comparison of Randomized Ablation (Levine & Feizi, 2020b) to our method Box-NN on theMNIST (left) and FashionMNIST (right) datasets. In each gure, the dotted lines correspond to dierenthyperparameter settings . Details in text. More recently, given any classier f, Hammoudeh & Lowd (2023) produce a determinstic 0 certied classierg by partitioning the set of pixels {1, 2, . . . , n} into disjoint partitions V, and then producing the majorityprediction of f over V:FPA(x) = Majority{f(xV )}V V,(16) where f(xV ) is dened as the prediction of f obtained after zeroing out the pixels in x not in V . Hammoudeh& Lowd (2023) then produce a certicate by counting the dierence in the votes of the majority label to the",
  "FMNISTBox-NN22RA Levine & Feizi (2020b)16": "runner-up label in (16). In (right), we compare to the best performing strategy for constructing V in(Hammoudeh & Lowd, 2023) named strided and denoted by FPAA , where equally spaced pixels are selectedfor each partition, i.e., V = {j : j t 1 mod }1t=0 . Here is a hyper-parameter as earlier, and we vary to produce the plots in . Note that (Hammoudeh & Lowd, 2023) also obtain an improved certicate byusing an aggregation more complicated than the majority vote (called FPA with run-o elections), which iscompared to in , where it is denoted by FPAB . The interested reader can refer to Appendix B.3 formore details. Results Recall from .2 Eq. (10) that the certied accuracy of a classier g against -boundedadversarial perturbations, CertAcc(g, ), can be obtained given a point-wise certicate C for g. For each ofthe methods described so far, we plot CertAcc against using the corresponding robust classier g and thecerticate C over samples from the test set of the datasets mentioned. A commonly used metric for comparing certied accuracy curves adopted in the literature (Levine & Feizi,2020b; Jia et al., 2022; Hammoudeh & Lowd, 2023) is the median certied radius, which is the largestperturbation strength under which a classier is certied to have atleast 50% robust accuracy. As can beseen in , our method Box-NN outperforms all existing methods under all hyperparameter settings onthis metric. The median certied radius captures a small slice of the full certied accuracy curve, which provides a completepicture. Observe that the dotted curves in Figs. 2 and 3 remain lower than our red curve except at smallattack strengths. This shows that Box-NN is able to produce better certicates at most radii, and trades-o",
  "Conclusion, Limitations and Future Work": "In this work, we developed a theoretical to exploit properties of the data distribution for robustness againstsparse adversarial attacks. We showed that data localization the property that a data distribution p placesmost of its mass on very small volume sets in the input space characterizes the existence of a 0-robustclassier for p. Following this theory, we developed a defense against sparse adversarial attacks, and deriveda corresponding robustness certicate. We showed that this certicate empirically improves upon existingstate-of-the-art in several broad regimes. The primary limitation of our work is the diculty in eciently learning classiers that have axis-aligneddecision regions. While we are able to successfully employ several optimization tricks for datasets like MNISTand Fashion MNIST, the task becomes harder on more complicated datasets, even though the geometryrequired for the underlying data-distribution remains the same due to our general theoretical results. Theseoptimization diculties mostly stem from the strict requirement of axis-aligned boxes for our distancecomputation in Lemma 4.1. In the future, we hope to trade-o eciency in the distance computation infavor of richer decision boundaries that can be learnt eciently and generalize well.",
  "B.2Ablation Study": "We provide an ablation study on the various strategies used to eectively train BoxNN developed in.2. For each of the following certied accuracy plots in our ablation study, we x all the otherparameters at the values specied in (these values correspond to those reported in the main text, andare explained in the following description), and vary the parameter specied. The red curve is the same in allthe plots, and corresponds to the BoxNN plots in the main text.",
  ": Ablation over the choice of the relaxation for the indicator function": "2. Initialization. For each box, we choose the location and size at initialization. For location, we ndthat centering boxes initially on training data points is always much better than random initialization,and hence we omit this from our ablation. We ablate on the size of the box , such that the boxesat initialization are dened as {(B(x , x + ), y): (x, y) T}, for a subset T of size M chosenuniformly at random from the training data, as mentioned in .2. The resultant ablation isshown in .",
  ": Ablation over the number of boxes M": "4. Clipping Certicates Recall that in (13), we clipped the robustness certicates in the optimizationobjective to = 50 pixels to prevent a few boxes from dominating the loss. We ablate over thisclipping value in . 5. Optimizer. We ablate over a few choices of the gradient-based optimizer for our problem: (a) vanillaSGD with a learning rate of 0.02, (b) SGD with a learning rate of 0.02, a momentum of 0.9, and aweight decay of 0.0005, and (c) Adam with a learning rate of 0.001, and standard decay factors, in. 6. Approximation of min. Recall that we replace the function min by an approximation to obtainnon-sparse gradients to aid the optimization process. We describe the approximation min in Eq. (12),and we ablate over {0, 0.5, 1, }, where recall that min0 is same as the average function, and",
  "RA(x) = arg maxkPvUnif(V) (f(x v) = k) ,((15) revisited)": "where V = {v {0, 1}n : v0 = } is the discrete set of all binary vectors of length n having exactly ones, and denotes the Hadamard product. The method to produce a certicate of 0 robustness inLevine & Feizi (2020a) follows the technique developed in Cohen et al. (2019), which is to bound thechange in the probability assigned to the top class k in (15) when x is perturbed. In other words, lettingpk(x) = PvUnif(V) (f(x v) = k) for k {1, 2, . . . , K}, and k = arg maxk pk(x), Levine & Feizi (2020a)show that",
  "Thus, the greatest quantity rRA that satises the if condition in (29) becomes the 0 robustness certicatefor the classer RA at the input point x": "However, obtaining of the probability pk(x) exactly is computationally infeasible, due to the exponentiallylarge size of V, and hence evaluation of the classier RA and the certied radius rRA is computationallyinfeasible. Nevertheless, the technique to produce such randomized smoothing certicates in practice isto obtain a high condence lower bound on pk using several samples from Unif(V) following standardstatistical estimation literature, and use this lower bound for computing rRA (Cohen et al., 2019, Sec 3.2.2). Certication procedure for RAB (Jia et al., 2022).Jia et al. (2022) produce a tighter analysis of thecerticate of robustness for the classier dened in (15). Their primary contribution is to develop techniquesthat can provide 0 robustness certicates for the top-k predictions where k can be greater than 1. For thepurposes of comparison to our work, we are only concerned for the special case of k = 1 in their certicate(Jia et al., 2022, Theorem 1). As Jia et al. (2022) remark, in this case, the form of the certicate is identicalto Levine & Feizi (2020a), with the important dierence that Jia et al. (2022) utilize the fact that each ofthe probabilities pk(x) are integer multiples of1(n), by incrementing the empirically computed lower bounds",
  "greater than rRA, resulting in a higher certied accuracy curve": "Certication procedures for FPA (Hammoudeh & Lowd, 2023).As opposed to the randomizeddefenses above, Hammoudeh & Lowd (2023) build a classier that is simply a majority vote among severalsub-classiers, each looking at a subset of the pixels of the full input image. More formally, partitioning theset of pixels {1, 2, . . . , n} into disjoint partitions V, recall that the classier certied in Hammoudeh & Lowd(2023) isFPAA(x) = arg maxk|{V V : f(xV ) = k}| , ,((16) rewritten) where f(xV ) is the prediction obtained from a classier that zeroes out the pixels of x not in V . Deningthe number of votes for class k as nk = |{V V : f(xV ) = k}| for k {1, 2, . . . , K}, the winning class ask = arg maxk nk, and the runner-up class as kru = arg maxk=k nk, Hammoudeh & Lowd (2023) present therobustness certicate by a simple counting:",
  "For evaluating the above certicate rFPAA for any input x, one needs to compute nk for all the classes. Thisis easily done by counting the votes for each class from each classier f(xV ), V V": "Hammoudeh & Lowd (2023) then note that the above certicate discards the information from all theclassiers which did not predict k or kru, and remedy this in their improved certicate rFPAB. For this, theidea is to rst determine k and kru, and then get all the classiers f(xV ) to vote for one of these two classes.The predicted label is the majority winner of this vote, i.e.,",
  "FPAB(x) = Majority{votef(xV )(k, kru)}V V.(31)": "While the above description of a two-round voting is quite general, the actual voting scheme in Hammoudeh& Lowd (2023) is implemented by setting f to be a neural network classier, and the vote decided by whichclass among {k, kru} is assigned a large probability by the network. Evaluation of the certicate rFPAB nowbecomes much more complicated due to the complicated relationship between the label predicted and nk.Nevertheless, rFPAB is typically larger than rFPAA, and we produce a comparison to our method BoxNN.Specically, in , we compare BoxNN against the numbers reported in Hammoudeh & Lowd (2023,) (FPA with run-o elections). We observe that BoxNN improves upon the median certiedrobustness against FPAB, and more generally mirrors the observations in with respect to otherbaselines."
}