{
  "Abstract": "We find that, simply via a sampling-and-voting method, the performance of large languagemodels (LLMs) scales with the number of agents instantiated. Also, this method, termed asAgent Forest, is orthogonal to existing complicated methods to further enhance LLMs, whilethe degree of enhancement is correlated to the task difficulty. We conduct comprehensiveexperiments on a wide range of LLM benchmarks to verify the presence of our finding, andto study the properties that can facilitate its occurrence. Our code is publicly available at:",
  "Introduction": "Although large language models (LLMs) demonstrate remarkable capabilities in variety of applications (Zhaoet al., 2023), such as language generation, understanding, and reasoning, they struggle to provide accurateanswers when faced with complicated tasks. To improve the performance of LLMs, some of recent studiesfocus on ensemble methods (Wang et al., 2023b; Wan et al., 2024) and multiple LLM-Agents collaborationframeworks (Du et al., 2023; Wu et al., 2023). In these works, multiple LLM agents are used to improve the performance of LLMs. For instance, LLM-DebateDu et al. (2023) employs multiple LLM agents in a debate form. The reasoning performance is improvedby creating a framework that allows more than one agent to debate the final answer of arithmetic tasks.They show performance improvements compared to using one single agent. Similarly, CoT-SC (Wang et al.,2023b) generates multiple thought chains and picks the most self-consistent one as the final answer. Thereasoning performance is improved by involving more thought chains compared to chain-of-thought (CoT)(Wei et al., 2022) which employs a single thought chain. Incidentally, from the data analysis of these works,we can notice the effects of putting multiple agents together, to some extent, can lead to a performanceimprovement in certain problems. For example, in of .3 of LLM-Debate Du et al. (2023),the authors have reported a preliminary curve: the accuracy of a math problem increases with the numberof debating agents (although the number was simply increased from 1 to 7). Also, in Wang et al. (2023b),involving more chain-of-thought pipelines (termed as a sample-and-marginalize decoding procedure), can",
  "Ensemble Size": "Accuracy (%) llama-70B (single) gpt-3.5-turbo (single) gpt-4 (single)Accuracy Curves In GSM8K llama-13Bllama-70Bgpt-3.5-turbo : The accuracy increases with ensemble size across Llama2-13B, Llama2-70B and GPT-3.5-Turbo inGSM8K. When the ensemble size scales up to 15, Llama2-13B achieves comparable accuracy with Llama2-70B.Similarly, When the ensemble size scales up to 15 and 20, Llama2-70B and GPT-3.5-Turbo achieve comparableaccuracy with their more powerful counterparts. The error bars represent the standard error. lead to a performance gain. We realize that the LLM performance may likely be improved by a brute-forcescaling up of the number of agents instantiated. However, since the scaling property of raw agents is notthe focus of these works, the scenarios/tasks and experiments considered are limited. So far, there lacks adedicated in-depth study on such a phenomenon. Hence, a natural question arises: Does this phenomenongenerally exist? To answer the research question above, we conduct the first comprehensive study on the scaling property ofLLM agents. To dig out the potential of multiple agents, we propose to use a simple(st) sampling-and-votingmethod, which involves two phases. First, the query of the task, i.e., the input to an LLM, is iterativelyfed into a single LLM, or a multiple LLM-Agents collaboration framework, to generate multiple outputs.Subsequently, majority voting is used to determine the final result. The procedure is inspired by that of theCoT-SC, but it does not rely on designing complex CoT paths. In fact, it can be used as a plug-in to furtherenhance CoT-based methods, as will be shown in our evaluations. Our method is termed as Agent Forest,a tribute to the classic Random Forest (Breiman, 2001). The experiments are conducted by using various LLMs of different sizes on diverse datasets covering reasoningand generation. The result indicates that LLM performance can generally be improved by increasing theensemble size, i.e., the number of agents, across a wide range of tasks. Surprisingly, a brute-force ensemble ofsmaller LLMs can achieve comparable or superior performance to larger LLMs, with a nutshell shown in, which will be further expanded in later sections. Moreover, by combining our method with otherexisting methods, we find the performance can be further improved. By comparing with the performance ofcomplicated methods, the result shows that employing our method solely can achieve comparable performancein most cases. This implies that comparable performance can be achieved without the need for additionalhandcraft prompt design or complex collaboration frameworks. Additionally, the experimental results indicate that there are greater performance improvements whenaddressing difficult tasks and when using weaker models. To understand the reasons behind these performanceimprovements, we analyze the influence of problem difficulty on the effectiveness of our method. We classifydifficulty into three dimensions: the inherent difficulty, the length of reasoning steps, and the prior probabilityof the correct answer. Through a series of experiments, we adjust these dimensions and observe their effectsindependently. We observe and summarize a few properties, based on which, we further develop optimizationstrategies that can intrigue the power of More Agents.",
  "Published in Transactions on Machine Learning Research (10/2024)": "Krivokon, Will Rusch, Marc Pickett, Kathleen S. Meier-Hellstern, Meredith Ringel Morris, Tulsee Doshi,Renelito Delos Santos, Toju Duke, Johnny Soraker, Ben Zevenbergen, Vinodkumar Prabhakaran, MarkDiaz, Ben Hutchinson, Kristen Olson, Alejandra Molina, Erin Hoffman-John, Josh Lee, Lora Aroyo, RaviRajakumar, Alena Butryna, Matthew Lamm, Viktoriya Kuzmina, Joe Fenton, Aaron Cohen, RachelBernstein, Ray Kurzweil, Blaise Agera y Arcas, Claire Cui, Marian Croak, Ed H. Chi, and Quoc Le.Lamda: Language models for dialog applications. CoRR, abs/2201.08239, 2022. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, HakanInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit SinghKoura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, XavierMartinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein,Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian,Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan,Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurlien Rodriguez, RobertStojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models.CoRR, abs/2307.09288, 2023.",
  "Related Work": "Related works can be categorized into three parts: 1) LLM self-ensemble Wang et al. (2023b), whichattempts to harness multiple outputs from homogeneous LLMs to assemble the final answer; 2) heterogeneousLLM ensemble, which focuses on combining heterogeneous LLMs through supervised learning to improveperformance across various downstream applications; and 3) multiple LLM agents collaboration, whichimproves performance through interactions among LLM agents. We discuss these works below. LLM Self-Ensemble. CoT-SC Wang et al. (2023b) harnesses diverse chain-of-thought Wei et al. (2022)prompts to elicit a variety of reasoning processes from a single LLM and select the final answer throughmajority voting. Fu et al. (2023); Li et al. (2023b); Cobbe et al. (2021b); Thoppilan et al. (2022); Lin et al.(2023) can be considered as the extensions of CoT-SC. These methods mainly focus on reasoning tasks andexclusively investigate the compatibility with CoT. In contrast, our method not only validates effectiveness inreasoning tasks but also in generation tasks. Moreover, our method is compatible with a broader range ofmethods, such as prompt engineering (including CoT) and multiple LLM agents collaboration. Very recently,Lu et al. (2024) proposes a method named Blended that utilizes multiple LLMs for chat scenarios. In contrast,Blended focuses on utilizing the power of multiple LLMs, whereas our focus is on the scaling trend of addingmore LLMs. Also, Blended is only for limited chat scenarios evaluated via human annotations. Furthermore,we explore orthogonality with other methods. Heterogeneous LLM Ensemble.Wan et al. (2024) conducts a supervised LLM fusion framework todistill multiple heterogeneous LLMs into a single model and surpasses each of these LLMs. Jiang et al.(2023) introduces a supervised ensembling framework based on multiple heterogeneous LLMs. Chen et al.(2023b) proposes a sequential inference method for LLMs that halts when the output quality is deemedadequate. Wang et al. (2023a) addresses the fusion-of-experts problem by integrating outputs from modelswith distinct knowledge domains through supervised learning. Shnitzer et al. (2023) and Lu et al. (2023)select the most suitable LLM for new tasks by training a reward-guided router. These approaches primarilyemploy supervised learning, necessitating task-specific annotated data, and exhibit limited generalizability.In contrast, our method is unsupervised, without the need for additional training data. Multiple LLM Agents Collaboration.Du et al. (2023); Liang et al. (2023); Xiong et al. (2023) explorevarious multiple LLM agents interaction architectures, with employing static debate-style engagements amongLLMs for enhanced reasoning . Liu et al. (2023) enables agents to interact for multiple rounds in a dynamicarchitecture. Li et al. (2023a); Hong et al. (2023); Wu et al. (2023); Chen et al. (2023c;a) offer severalmulti-agent frameworks that enable the development of LLM applications or enhance task-solving capabilities.However, these methods primarily focus on the interaction structures between LLM agents, rather than therelationship between the number of agents and performance. We also select representative methods Du et al.(2023); Shinn et al. (2023) to combine with our method, achieving further enhancements.",
  "VotingLLM": "Multiple LLM collaboration framework : Illustration of Agent Forest. The two-phase process begins by feeding the task query, either aloneor combined with prompt engineering methods, into LLM agents to generate answers. Subsequently, majorityvoting is applied to these answers to determine the final answer. Specifically, an LLM agent refers to a singleLLM or a multiple LLM-Agents collaboration framework.",
  ": end for14: A arg maxsiS V (si)15: return A": "Sampling. Let x represent the task query and M denote an LLM. In this phase, we generate N samplesby solely querying the LLM M N times with each sample represented as s = M(x) or by integrating withother methods fM with N times executions where each sample is denoted as s = fM(x). We obtain a set ofsamples S = {s1, s2, ..., sN} at the end of this phase. Voting. Let A represent the final answer. In this phase, we employ majority voting to consolidate theresponse sample set S into the final answer A. This involves calculating the cumulative similarity for eachsample relative to the others, denoted as V (si) = Nj=1,j=i sim(si, sj). For open-ended generation taskssuch as code generation, the BLEU score proposed by Papineni et al. (2002) is utilized to quantify similarity.Conversely, for close-ended tasks like multiple-choice questions, similarity is measured by occurrence frequency.The sample that exhibits the highest cumulative similarity is then chosen as the final answer denoted asA = arg maxsiS V (si).",
  "Experimental Setup": "We separate the experimental setup (this section) with evaluations (next section), to introduce the coverageof scenarios/tasks compared with the most related works (for examining the comprehensiveness of our work),the backbone language models we adopted (for examining the applicability of our work), and the methodscombined with ours (for examining the compatibility and orthogonality of our work).",
  "TasksOur method is evaluated on the following task:": "Arithmetic Reasoning. Similar to Wang et al. (2023b); Fu et al. (2023); Du et al. (2023), we selectthe GSM8K Cobbe et al. (2021a) as one of the test sets. Additionally, we select the more challengingMATH dataset Hendrycks et al. (2021b), which is used by Wu et al. (2023). General Reasoning. Similar to Du et al. (2023); Jiang et al. (2023), we select the MMLU Hendryckset al. (2021a). Additionally, we select the dataset from the chess state tracking task (Chess) 1, whichis used by Du et al. (2023); Zhang et al. (2023). Code Generation. Similar to Liu et al. (2023), we select the HumanEval Chen et al. (2021). Toimplement our method, we compute the BLEU score Papineni et al. (2002) among all pairs ofgenerated candidate answers. The answer with the highest cumulative BLEU score is then selectedas the final output.",
  "Ours": "Language models adoptedWe evaluate our method using language models of different scales from theLlama2 Touvron et al. (2023) and GPT series OpenAI (2022). Specifically, we evaluate two versions ofLlama2-Chat2, optimized for conversational use cases through alignment techniques, with model sizes of 13Band 70B parameters. Additionally, we include GPT-3.5-Turbo and GPT-4 in our evaluation.",
  "Methods enhanced by our methodTo examine the comparability of our method, we study theintegration of various typical methods from two distinct categories with our method:": "Prompt Engineering. Various prompt engineering methods are considered to conduct comprehensiveexperiments. We evaluate Chain-of-Thought prompting (CoT) Wei et al. (2022), Zero-Shot Chain-of-Thought prompting (Zero-Shot Cot) Kojima et al. (2022), and more sophisticated methods such asSolo Performance Prompting (SPP) Wang et al. (2023c). Initially, these methods are applied with asingle LLM query. We then increase the number of queries and employ majority voting to determinethe most consistent answer as the final response. Multiple LLM Agents Collaboration. We select LLM-Debate Du et al. (2023) denoted as Debate,and self-reflection Shinn et al. (2023) denoted as Reflection. Within these methods, we generatemultiple samples by iteratively operating these methods and using majority voting to produce thefinal answer.",
  ": The accuracy scales with the ensemble size of our method across different tasks with various LLMs.The error bars represent the standard error": ": Our method generally enhances performance across all tasks and LLMs. The bolded instancesindicate that smaller LLMs outperform the larger LLMs. Single denotes that the LLM is queried onlyonce. GPT-4 is used only for comparison with other methods, hence it only presents Single results. Oursdenotes our method where the ensemble size is 40. The error bars represent the standard error.",
  "Generalizability": "and show that our method generally enhances performance across all tasks and LLMs byincreasing the ensemble size. Specifically, in arithmetic reasoning tasks, the accuracy gains range from 12%to 24% on the GSM8K and from 6% to 10% on the MATH. In general reasoning tasks, the accuracy gainsrange from 1% to 4% on the Chess and from 5% to 11% on the MMLU. In code generation task, the accuracygains range from 4% to 9% on HumanEval. Surprisingly, our method enables a smaller LLM to outperform alarger counterpart by simply scaling up the ensemble size. For instance, the enhanced Llama2-13B modelachieves 59% accuracy on the GSM8K dataset, outperforming the Llama2-70B model, which scores 54%.Additional statistical results are presented in Appendix B.2.",
  "Compatibility": "shows that by integrating our method with other methods, the performance can be further improvedacross different LLMs and tasks, despite these methods have different implementations. To be specific, inarithmetic reasoning tasks, our method enhances these methods to further improvement, yielding increasesbetween 10% and 21% on the GSM8K dataset, and between 1% and 15% on the MATH dataset. In generalreasoning tasks, integration with other methods generally achieves performance gains ranging from 1% to13% in the Chess task and from 1% to 11% in the MMLU task. In code generation task, when combined withother methods, gains range from 2% to 7%. However, two notable exceptions are observed when integratedwith the debate method with the Llama2-13B and Llama2-70B models, which result in failed cases. Thisfailure in performance is attributed primarily to the noise generated by referencing the answers of otheragents during the debate process. The synthesized responses, which incorporate input from multiple agents,disrupt the coherence of the code logic, leading to the observed performance degradation. All accuracy curvesare provided in the Appendix B.1.",
  "Robustness": "We conduct ablation studies to evaluate the impact of changes in various hyperparameters on the finalperformance. The experiment is conducted by altering the temperature T Ficler & Goldberg (2017) andthe nucleus probability p Radford et al. (2019), using the GPT-3.5-Turbo model over an average of 20 runs.As shown in , scaling up ensemble size improves the LLM performance consistently across differenttasks, despite the variation of these hyperparameters.",
  "Token usage": "We record the token usage for different methods, with the presenting the token usage for a single agent.Given that our method scales up by increasing the ensemble size, the token usage increases proportionallywith the number of agents or when combined with other methods. When addressing specific tasks, one cantrade a higher token budget for improved performance. More details are presented in Appendix B.3",
  "Pswhere Pm and Ps arethe performances (accuracy) with our method and a single LLM query, respectively. The results are shown in": "It is noteworthy that the relative performance gain is more substantial with increasing task difficulty.Specifically, we observe that within the same task, the smaller model, Llama2-13B, gains ranging from28%-200%, but only 8%-16% over GPT-3.5-Turbo. Moreover, the more challenging task MATH yields gainsof 34%-200%, in contrast to only 16%-69% on the easier task GSM8K.",
  "GSM8K (easy)693716MATH (hard)20012034": "To further analyze this correlation in detail, we categorize the difficulty of a given task into three orthogonaldimensions: 1) the inherent difficulty of the task; 2) the number of steps required to solve the task; 3) theprior probability of the correct answer. To investigate these dimensions, we conduct experiments that canisolate each dimension. And then, we delve into each dimension in detail.",
  "TRUEFALSETRUEFALSEFALSEFALSE": "Prior Probability of the correct answer Step 1 Step 2 Step 3 Step 4 Step 5 easyhard Inherent difficulty : Illustration of three dimensions for a given task. Nodes represent steps, while dashed lines indicatealternative potential steps. The depth of nodes represents the number of steps, and the color intensityrepresents the level of inherent difficulty.",
  "Inherent Difficulty": "Property 1: Gains increase then decrease by rising the inherent difficulty. We investigate the inherentdifficulty by varying I from 10 to 400, while keeping the values of S and K constant across four groups ofdifferent values, from small to large, respectively. (left) shows an initial uptick in performance gainswith increases in I, indicating that our method can significantly enhance performance in line with risinginherent difficulty. The most notable gains are seen at I = 100 and I = 200, consistent across all S andK settings. Yet, at I = 400, gains taper off, implying that excessive complexity may exceed the modelsreasoning capabilities, leading to diminishing returns for our method under extreme task difficulty. I (Inherent Difficulty) Percentage %",
  "Absolute Performance": "I=10 S=1I=10 S=4I=200 S=1I=200 S=4 : (Left) The relative performance gains increase and then decrease with rising inherent difficulty.(Middle) The relative performance gains increase with the number of steps. (Right) The absolute performanceincreases with the prior probability. We analyze each dimension by fixing the other two dimensions. Theerror bars represent the standard error.",
  "Number of Steps": "Property 2.1: Gains increase with the number of steps. We analyze the number of steps by isolating S.We tune S from 1 to 8, while keeping the values of I and K constant across four groups of different values,ranging from small to large, respectively. (middle) shows that as the number of steps increases,there is a corresponding increase in performance gain. Additionally, we find that when I and K are increased(which indicates a higher difficulty), the performance gains are more significant, e.g., 4%-18% gains over{I = 10, K = 2} compared to 16%-48% over {I = 100, K = 4}. Property 2.2: Agent Forest increases the performance for each step. We conduct a fine-grained analysisfor each step of a given task. We explicitly prompt the language model to output the result of each step.Subsequently, we utilize Agent Forest at each step to derive the answer for that step. (left) showsthat although each step has equal inherent difficulty, the accumulation of errors from previous steps lead toa decrease in accuracy as the number of steps increases. However, our method mitigates the performancedecrease encountered with increasing steps.",
  "Prior Probability": "Property 3: The performance increases with the prior probability. We investigate the influence of priorprobability on performance by tuning the parameter K, while maintaining constant values for I and K. AsK represents the number of intervals, the prior probability is defined as 1/K. We vary K from 4 to 32, whichequivalently alters the prior probability from 1/4 to 1/32. Through four experimental groups illustrated in (right), each characterized by different configurations of I and S, we find that as the prior probabilityincreases, so does the performance.",
  "Derivation. Based on Property 3, we propose Hierarchical Agent Forest can further enhance the performance": "As the performance is related to the prior probability, decomposing low-probability tasks into multiplehigh-probability subtasks and addressing them hierarchically can boost performance. Moreover, subtaskswith varying prior probabilities can be addressed using different models. Additionally, cost savings can beachieved by using simpler, less expensive models for the easier, higher-probability subtasks. In our experiments, the task is to solve the problem with K = 32. GPT-3.5-Turbo is used in homogeneouscombination experiment and GPT-3.5-Turbo and GPT-4 are used in heterogeneous combination experiment.The results are presented in (right). In homogeneous combination experiment, by employing the hierarchical method, we start with K = 8 toobtain an intermediate answer and then find the solution with K = 32, focusing on intervals identified bythe intermediate answer. This method enhances the performance from 21% to 31%, demonstrating that thehierarchical method can further enhance the performance. In heterogeneous combination experiment, GPT-3.5-Turbo is used for generating the intermediate answerwith K = 8, and GPT-4 is then employed to solve for the final answer with K = 32. In (right),compared with the result of GPT-4 with K = 32, the hierarchical method improves performance from 35% to47%, suggesting the deployment of different LLMs at the corresponding level of problem-solving can improvethe performance in a cost-effective manner.",
  "Conclusions and Future Work": "In this paper, we report that more agents is all you need, i.e., simply adding more instantiated LLM agentsis what you need to obtain a better LLM performance in processing complex tasks, without botheringcomplicated methods, such as CoT pipelines, multi-agent collaboration frameworks, etc. We have conductedthe first comprehensive study in the literature to understand such a scaling law, including when it holdsand how to facilitate its occurrence. The results indicate that Agent Forest can generally improve the performance of LLMs by increasing theensemble size. Importantly, this method is orthogonal to different existing methods, which can lead to furtherimprovements when combined with them. Furthermore, we observe that the performance gains are influenced by the difficulty of the task. To explorethis correlation, we isolate and analyze three dimensions of task difficulty: the inherent difficulty, the lengthof reasoning steps, and the prior probability of a correct answer. We find that: 1) the performance gainsincrease then decrease by rising the inherent difficulty; 2) performance gains increase with the number ofsteps; and 3) performance increases with the prior probability. Based on these properties, we also developways to boost the effectiveness of More Agents. Considering that each input remains the same when we increase the number of agents, the sampling phasecan be optimized to reduce the cost. Nonetheless, such a challenge of escalating costs commonly exists inworks requiring multiple LLM calls Wang et al. (2023b); Du et al. (2023). This aligns with recent findingsas discussed in Kapoor et al. (2024) that current benchmarks focus narrowly on accuracy, leading to costlyagents. Additionally, the lack of adequate holdout sets and standardization in evaluation practices furthercomplicates the development of cost-effective agents. Addressing these issues can enhance the practicalefficiency of AI agents in real-world scenarios. We leave it as a future work to optimize.",
  "Lingjiao Chen, Matei Zaharia, and James Zou. Frugalgpt: How to use large language models while reducingcost and improving performance. CoRR, abs/2305.05176, 2023b": "Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Pond de Oliveira Pinto, Jared Kaplan,Harrison Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger,Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder,Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet,Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, ArielHerbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin,Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, JoshuaAchiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati,Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and WojciechZaremba. Evaluating large language models trained on code. CoRR, abs/2107.03374, 2021. Weize Chen, Yusheng Su, Jingwei Zuo, Cheng Yang, Chenfei Yuan, Chen Qian, Chi-Min Chan, Yujia Qin,Yaxi Lu, Ruobing Xie, Zhiyuan Liu, Maosong Sun, and Jie Zhou. Agentverse: Facilitating multi-agentcollaboration and exploring emergent behaviors in agents. CoRR, abs/2308.10848, 2023c. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, MatthiasPlappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Trainingverifiers to solve math word problems. CoRR, abs/2110.14168, 2021a. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, MatthiasPlappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Trainingverifiers to solve math word problems. CoRR, abs/2110.14168, 2021b.",
  "Jessica Ficler and Yoav Goldberg. Controlling linguistic style aspects in neural language generation. CoRR,abs/1707.02633, 2017": "Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, and Tushar Khot. Complexity-based prompting formulti-step reasoning. In The Eleventh International Conference on Learning Representations, ICLR 2023,Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt.Measuring massive multitask language understanding.In 9th International Conference on LearningRepresentations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021a. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, andJacob Steinhardt. Measuring mathematical problem solving with the MATH dataset. In Joaquin Vanschorenand Sai-Kit Yeung (eds.), Proceedings of the Neural Information Processing Systems Track on Datasetsand Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual, 2021b. Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, StevenKa Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, Lingfeng Xiao, and Chenglin Wu. Metagpt: Metaprogramming for multi-agent collaborative framework. CoRR, abs/2308.00352, 2023. Dongfu Jiang, Xiang Ren, and Bill Yuchen Lin. Llm-blender: Ensembling large language models with pairwiseranking and generative fusion. In Anna Rogers, Jordan L. Boyd-Graber, and Naoaki Okazaki (eds.),Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long",
  "Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large languagemodels are zero-shot reasoners. In NeurIPS, 2022": "Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. CAMEL:communicative agents for \"mind\" exploration of large scale language model society. CoRR, abs/2303.17760,2023a. Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, and Weizhu Chen. Making languagemodels better reasoners with step-aware verifier. In Anna Rogers, Jordan Boyd-Graber, and NaoakiOkazaki (eds.), Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics(Volume 1: Long Papers), pp. 53155333, Toronto, Canada, July 2023b. Association for ComputationalLinguistics. Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Zhaopeng Tu, andShuming Shi. Encouraging divergent thinking in large language models through multi-agent debate. CoRR,abs/2305.19118, 2023. Lei Lin, Jia-Yi Fu, Pengli Liu, Junchen Wan, Fuzheng Zhang, Zhongyuan Wang, Di Zhang, and Kun Gai.Ask one more time: Self-agreement improves reasoning of language models in (almost) all scenarios. CoRR,abs/2311.08154, 2023.",
  "Tal Shnitzer, Anthony Ou, Mrian Silva, Kate Soule, Yuekai Sun, Justin Solomon, Neil Thompson, andMikhail Yurochkin. Large language model routing with benchmark datasets. CoRR, abs/2309.15789, 2023": "Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng,Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, YaGuang Li, Hongrae Lee, Huaixiu Steven Zheng, AminGhafouri, Marcelo Menegali, Yanping Huang, Maxim Krikun, Dmitry Lepikhin, James Qin, Dehao Chen,Yuanzhong Xu, Zhifeng Chen, Adam Roberts, Maarten Bosma, Yanqi Zhou, Chung-Ching Chang, Igor",
  "Hongyi Wang, Felipe Maia Polo, Yuekai Sun, Souvik Kundu, Eric P. Xing, and Mikhail Yurochkin. Fusingmodels with complementary expertise. CoRR, abs/2310.01542, 2023a": "Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V. Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery,and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. In TheEleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5,2023. OpenReview.net, 2023b. Zhenhailong Wang, Shaoguang Mao, Wenshan Wu, Tao Ge, Furu Wei, and Heng Ji. Unleashing cognitivesynergy in large language models: A task-solving agent through multi-persona self-collaboration. CoRR,abs/2307.05300, 2023c. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le,and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. In NeurIPS,2022. Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang Zhu, Beibin Li, Li Jiang,Xiaoyun Zhang, and Chi Wang. Autogen: Enabling next-gen LLM applications via multi-agent conversationframework. CoRR, abs/2308.08155, 2023.",
  "Jintian Zhang, Xin Xu, and Shumin Deng. Exploring collaboration mechanisms for llm agents: A socialpsychology view. arXiv preprint arXiv:2310.02124, 2023": "Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, BeichenZhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang,Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. A survey oflarge language models. arXiv preprint arXiv:2303.18223, 2023.",
  "A.1Common Settings": "In all experiments involving GPT-3.5-Turbo presented in , we utilize the model versiongpt-3.5-turbo-0613. In , the notation GPT-4 corresponds to the model version gpt-4-0613. For theexperiments conducted with GPT-3.5-Turbo in , we employ the model version gpt-3.5-turbo-1106with the JSON mode enabled. Similarly, GPT-4 in this context refers to gpt4-1106-Preview operating inJSON mode.",
  "A.2Experiments on Arithmetic Reasoning Tasks": "For the implementation of Agent Forest to arithmetic reasoning tasks within the GSM8K and MATH datasets,we execute the initial sampling phase by Algorithm 1. Samples are extracted from the responses by matchingboxed{{X}}, where X denotes a numerical value or a mathematical expression. In the subsequent voting phase, to evaluate the similarity between samples, as outlined in Algorithm 1, weemploy mathematical equivalence comparisons for each sample. The most probable sample is chosen as thefinal answer. This answer is subjected to a comparison of mathematical equivalence with the ground truth toascertain the correctness of the result.",
  "A.3Experiments on General Reasoning Tasks": "For general reasoning tasks, as encountered in the MMLU and Chess datasets, Agent Forest is appliedfollowing Algorithm 1 during the sampling phase. Samples are extracted by matching the pattern (X or(X), where X corresponds to the options A, B, C, or D in MMLU task and the chessboard position inChess task. During the voting phase, we calculate similarity by counting the frequency of each options occurrence withinthe samples. The most frequently occurring option is then chosen as the final answer. This selected answer iscompared with the ground truth to determine the accuracy of the result.",
  "In the code generation task, we apply the Agent Forest method to produce Python code using the HumanEvaldataset. During the sampling phase, we extract Python code snippets from the models responses": "In the voting phase, we compute the BLEU score using sacreBLEU Post (2018) to evaluate the similaritybetween each of the generated samples. The sample with the highest cumulative BLEU score is selected as thefinal answer. This method ensures that the final output is the most representative and accurate piece of codeas determined by consensus through similarity scoring among the samples.",
  "B.2Statistical Test Results": "In this section, we performed a one-way ANOVA to determine if there are significant differences in accuracyacross different ensemble sizes (1, 10, 20, 30, 40), across 10 independent runs. The detailed p-values areprovided in , where all p-values are less than 0.05, demonstrating that the differences in accuracybetween ensemble sizes are significant."
}