{
  "Abstract": "This paper introduces consistency models to the problem of sequential decision-making.Previous work applying diffusion models to planning within a model-based reinforcementlearning framework often struggles with high computational cost during the inference pro-cess, primarily due to their reliance on iterative reverse diffusion processes. Consistencymodels, known for their computational efficiency, have already shown promise in reinforce-ment learning within the actor-critic algorithm. Therefore, we combine guided consistencydistillation with a continuous-time diffusion model in the framework of Decision Diffuser.Our approach, named Consistency Planning, combines the robust planning capabilities ofdiffusion models with the speed of consistency models. We validate our method on Gymtasks in the D4RL framework, demonstrating that, when compared to its diffusion modelcounterparts, our method achieves more than a 12-fold increase in speed without any lossin performance.",
  "Introduction": "In recent years, significant strides have been made in high-resolution image generation through the advance-ment of diffusion-based generative models. Similarly, in offline reinforcement learning (RL) settings, derivingeffective policies from pre-existing offline datasets can be simplified to the task of developing a probabilisticmodel for trajectory prediction, an area where diffusion-based generative models have proven to be highlysuccessful. Existing models such as Diffuser (Janner et al., 2022) and Decision Diffuser (Ajay et al., 2022)underscore the efficacy of applying diffusion models to planning within model-based RL frameworks. InDiffuser, a diffusion model is trained on the trajectories in offline datasets, and then a separate classifiermodel is trained to predict the cumulative rewards of trajectory samples. During the inference process, thediffusion model, combined with classifier guidance, is employed to sample trajectories with high returns.Likewise, Decision Diffuser introduces a conditional diffusion model with state sequences as input, utilizingthe return as a conditioning variable for classifier-free guidance during sampling. Moreover, by incorporatingonly the state sequenceexcluding the action sequenceDecision Diffuser trains an extra inverse dynamicmodel to infer actions. Parallel to these developments, diffusion models have been adapted to model-free reinforcement learningscenarios, as illustrated by Diffusion-QL (Wang et al., 2022) and further enhanced in the efficient diffusionpolicy (EDP) (Kang et al., 2024). Diffusion-QL, utilizing a denoising diffusion probabilistic model (DDPM)",
  "Published in Transactions on Machine Learning Research (12/2024)": ": The performance of Consistency Planning, Diffuser, and previous model-free algorithms in theMaze2D environment, which tests long-horizon planning due to its sparse reward structure. ConsistencyPlannings performance is comparable to that of Diffuser, with diffusion steps N = 2. All results are derivedfrom the data provided in Janner et al. (2022).",
  "Diffusion Models in Reinforcement Learning": "Diffusion models offer a versatile approach for data augmentation in reinforcement learning. SynthER (Luet al., 2024) employs unguided diffusion models to enhance both offline and online RL datasets, subsequentlyutilized by model-free off-policy algorithms. Although this approach boosts performance, SynthERs relianceon unguided diffusion to approximate the behavior distribution faces challenges due to distributional shift.Similarly, MTDiff (He et al., 2024) implements unguided data generation in multitask environments. Additionally, diffusion models have been adapted for training world models. For instance, Alonso et al.(2023) use diffusion to train world models, achieving precise predictions of future observations. However,this method does not model entire trajectories, leading to compounded errors and lack of policy guidance.In a related effort, Rigter et al. (2023) integrate policy guidance to enhance a diffusion world model in onlineRL. Jackson et al. (2024) concentrate on offline RL, providing a theoretical framework and rationale for thetrajectory distribution shaped by policy guidance. Diffusion models have also been adapted for policy representation in RL, capturing the multi-modal distri-butions in offline datasets. Specifically, Diffusion-QL (Wang et al., 2022), applies the diffusion model withinthe framework of both Q-learning and Behavior Cloning (BC) for policy representation. However, the mainlimitation of Diffusion-QL is that it demonstrates computational inefficiency due to the necessity of process-ing both forward and backward through the entire Markov chain during training. To alleviate these issues,Kang et al. (2024) introduce action approximation, eliminating the need to execute the denoising processduring the training process. Diffusion models have been employed in recent studies for human behavior imitation learning (Pearce et al.,2023) and trajectory generation in offline RL. Trajectories that include states and actions are generated byDiffuser (Janner et al., 2022), using an unconditional diffusion model, guided by a reward function trainedon noisy state-action pairs. Decision Diffuser (Ajay et al., 2022) models the trajectories with the datasetusing a unified, conditional generative model, avoiding separate training a classifier for reward functions.",
  "Reinforcement Learning Problem Setting": "The sequential decision-making problem is defined as a Markov decision process (MDP): M={S, A, P, R, , d0}, where S and A are the state space and the action space respectively, P : S A Srepresents the transition function, R : S A S R denotes the reward function, [0, 1) is the discountfactor, d0 is the initial state distribution. The goal of RL is to learn policy (a|s) to maximize the expectedsum of discounted rewards Ek=0kr (sk, ak).",
  "Consistency Models": "Diffusion models operate by introducing Gaussian perturbations to transform data into noise, followed bygenerating data samples through a series of sequential denoising steps. Song et al. (2020) introduce a stochas-tic differential equation (SDE) framework that ensures the maintenance of the desired distribution as samplex evolves over time. The consistency models proposed by Song et al. (2023) recover the original data sam-ple by solving a corresponding probability flow ordinary differential equation (ODE): dxt dt = t logt pt(x),where pt(x) = pdata(x) N(0, t2I), pdata(x) represents the original data distribution, t [0, T] is the timeperiod. The data generation process in this framework reverses along the trajectory {xt}t[,T ] of the ODE,starting from random initial samples xT N0, T 2Iwhere is a minimal constant close to 0 to addressnumerical issues at the boundary. To accelerate the sampling process in diffusion models, the consistency model significantly reduces the numberof steps required for sampling compared to the original diffusion model, without substantially compromisingthe models performance. This is achieved by approximating a parameterized consistency function, f :(xt, t) x, which maps a noisy sample xt at step t back to the original sample x. This approach differs from the diffusion model, which utilizes a step-by-step denoising function p (xt1 | xt),for the reverse diffusion process. Slightly different from the original consistency model, this paper focuseson a conditional distribution, so the consistency function is modified to f(xt, t, c), where c denotes thecondition variable.",
  "Planning with Consistency Model": "This paper explores the integration of consistency models, which are trained by distillation from a pre-traineddiffusion model, into the planning architecture of Decision Diffuser. The original consistency models drawfrom the principles of score-based diffusion models (Song et al., 2020; Karras et al., 2022), making directdistillation from the discrete-time model used in Decision Diffuser ineffective. In the following, we discuss howwe use consistency models for the trajectory optimization process. .1 details the training processof the diffusion model, followed by an explanation in .2 of how guided consistency distillation isapplied, and how the consistency model is integrated with the Decision Diffuser framework during inference.",
  "xti+1() := (sk, sk+1, . . . , sk+H1)ti+1.(1)": "In this notation, k indicates the timestep of a state within a trajectory , H represents the planning horizon,and ti+1 is the timestep in the diffusion sequence. Consequently, xti+1() is defined as a noisy sequence ofstates, represented as a two-dimensional array where each column corresponds to a different timestep of thetrajectory. In training process, the sub-sequence ti [, T] follows the Karras boundary schedule (Karraset al., 2022):",
  "where = 0.002, tN = 80, and = 7": "Acting with inverse dynamics.To derive actions from the states generated by the diffusion model,we employ an inverse dynamics model (Agrawal et al., 2016; Pathak et al., 2018), denoted as h, trainedusing the same dataset as the diffusion model. Actions can be obtained via the inverse dynamics model byextracting the consecutive state sk and sk+1 at diffusion timestep t0:",
  ",(4)": "where ptrain is a log-normal distribution using the design choice from Karras et al. (2022), is sampledfrom a Bernoulli distribution with probability p. Namely, the condition information c() is ignored withprobability p, which is manifested by the condition information being an empty set . We employ returns R() under trajectories as the conditioning information c(), normalized such thatR() . We map it into a latent variable c Rh using a multi-layer perceptron. In cases whereR() = , the components of c are set to zero. During the inference time, sampling trajectories with highreturns corresponds to conditioning on R() = 1.",
  "f(x, , c, t) = cskip(t)x + cout(t)F(x, , c, t),(5)": "where F is a free-form neural network with an output that matches the dimensionality of x, cskip() = 1and cout() = 0 so that f satisfies boundary condition f(x, , c, ) x. During the distillation process,the guidance scale and n are sampled uniformly from the intervals [min, max] and {1, , N 1},respectively. The trajectory and returns tuple (x, c) are sampled from the dataset. Then, x,tnis estimatedby employing an ODE solver :",
  ": end while": "Consistency Model Inference. During the inference process, we first observe a state s in the environmentand sample an initial trajectory xT . Then, our consistency model, conditioned on returns c, guidance scale and history of last C states observed, iteratively predicts the denoised trajectories from the noisy inputsxtn() x() + t2n 2z along the probability flow ODE trajectory at step n [N], with Gaussian noisez N(0, I). For the single-step version of Consistency Inference, {tn | n = 0, 1} = {, T}. Finally, we extractstates (sk, sk+1) from denoised trajectory and get the action ak via our inverse dynamics model h. Thealgorithm of Consistency Planning is provided in Algorithm 2 and visualized in . For the architectureand implementation details, please refer to Appendix.",
  "Offline Reinforcement Learning": "The diffusion model, inverse dynamics model, and consistency model are trained using publicly availableD4RL datasets and subsequently evaluated on a range of Gym tasks, including HalfCheetah, Hopper,Walker2d, and Maze2D, within the D4RL benchmark suite (Fu et al., 2020). These tasks are character-ized by continuous state and action spaces and are conducted under offline reinforcement learning settings.Details of the dataset size are provided in Appendix. We compare the performance of our method with those of both behavior-cloning methods, i.e., Consistency-BC (C-BC) (Ding & Jin, 2023), Diffusion-BC (D-BC) (Wang et al., 2022), actor-critic methods, i.e.,Consistency-AC (C-AC) (Ding & Jin, 2023), Diffusion-QL (D-QL) (Wang et al., 2022) algorithms, andmodel-based methods, i.e., Diffuser (Janner et al., 2022), Decision-Diffuser (DD) (Ajay et al., 2022) in . For evaluation, results for our method correspond to the average over 150 planning seeds. By default, ourconsistency model applies the number of denoising steps N = 2 with saturated performance on most tasks,while the diffusion policy uses N = 5 (Wang et al., 2022), Diffuser and Decision Diffuser use N = 20 andN = 40, respectively (Janner et al., 2022; Ajay et al., 2022). For the performance metric, we use normalizedaverage returns (Fu et al., 2020) for all tasks. shows that although our method achieves a slightly lower average score (82.2) than Diffusion-QL(87.9) and Consistency-AC (85.1), as a model-based planning model, it outperforms its diffusion counterparts,i.e., Diffuser (75.3) and Decision Diffuser (81.8), with the reduction of denoising steps in the inference stages. However, it is worth noting that our model exhibits a more pronounced performance gap in the half-cheetah environment compared to the hopper and walker2d environments compared with Diffusion-QL andConsistency-AC. This can be attributed to several factors. While our model employs classifier-free guidance,it lacks the value function estimation provided by Q-networks in Consistency-AC and Diffusion-QL. TheseQ-network-based methods explicitly optimize for cumulative rewards, allowing them to perform better inenvironments like half-cheetah, where subtle variations in action sequences can have a significant impacton long-term returns. Moreover, half-cheetah presents a higher degree of control complexity and a moredynamic action space compared to hopper and walker2d. The need for precise coordination among multiplejoints makes it more challenging for generative models like ours, which rely purely on the consistency modeland classifier-free guidance, to generate optimal action sequences. In contrast, Q-network-based methodsare better equipped to navigate this complexity by explicitly evaluating state-action pairs in terms of theirfuture rewards. To validate the long-horizon planning capabilities of Consistency Planning, we conduct an evaluation in theMaze2D environment (Fu et al., 2020), where the task involves navigating to a specific goal location, with areward of 1 assigned only upon reaching the goal. Since it requires hundreds of steps to reach the goal, eventhe most advanced model-free algorithms struggle with effective credit assignment and consistently reachingthe goal. As shown in , Consistency Planning achieved scores exceeding 100 across all maze sizes,indicating that it outperforms the reference expert policy. To assess the computational efficiency of Consistency Planning and its diffusion model counterparts, DecisionDiffuser, we conduct experiments to measure inference time (ms per sample) in the hopper-medium-expert-v2on our server. Both the consistency model and the diffusion model, as generative models based on probabilityflow, have computational times that are directly dependent on the number of denoising steps N. The resultsin show that N = 2 for Consistency-Planning, and N = 20 for Decision Diffuser, are the valueswhere each algorithm achieves its saturated performance in hopper-medium-expert. The mean and standarddeviation of results are calculated over five random seeds.",
  "Ablation Study": "Inverse dynamics. To evaluate the role of inverse dynamics in our approach, we conducted a compara-tive analysis with Consistency Planning models that exclude the inverse dynamics component, where thetrajectories consist of both state and action sequences. The performance results are presented in .Our findings indicate that incorporating inverse dynamics leads to better performance across all three hop-per environments, consistently outperforming the variant without inverse dynamics. This demonstrates theeffectiveness of inverse dynamics in improving the models ability to generate action sequences, contributingenhanced planning performance. Guidance Scale. In our experiments, we examine the impact of guidance scale on the performance of ourconsistency models during planning. The guidance scale controls the weight of the guidance signal duringthe planning process, and its effect on performance can vary significantly depending on the characteristicsof the offline dataset. To assess the sensitivity of our model to , we conducted experiments across differenttypes of datasets, including hopper medium-expert, hopper medium, and hopper medium-replay. For the hopper medium-expert dataset, our results in indicate that varying the guidance scale does not lead to substantial differences in performance. This can be attributed to the relatively high-qualitydata in the medium-expert dataset. As a result, the model performs well across a wide range of values,and the influence of the guidance signal is less pronounced. However, when applied to hopper medium and medium-replay datasets, we observe a marked difference inperformance based on the choice of . Specifically, larger guidance scales (e.g., = 0.8) consistently resultin better outcomes compared to smaller scales (e.g., = 0). This is particularly evident in the mediumsetting, where the dataset contains suboptimal trajectories alongside more desirable ones. In such cases, thestronger guidance provided by larger helps the model better navigate the diverse quality of trajectories,ultimately leading to more consistent and improved performance.On the other hand, smaller valuesmay not provide sufficient signal strength to guide the model away from suboptimal decisions, resulting indegraded performance.",
  "Conclusion": "By combining the score-based diffusion model proposed by Karras et al. (2022), one-stage guided distillation(Luo et al., 2023), and conditional model-based generative model for sequential decision making (Ajay et al.,2022), the consistency model in this paper achieves comparable performance in gym tasks with its diffusionmodel counterparts, Diffuser and Decision Diffuser, and obtains a significant speedup during inference inoffline settings. While our work increases the inference speed compared with its diffusion models counterpart(Ajay et al., 2022), there still exists some potential challenges of applying the approach to scenarios wherequick, real-time decision making is critical. One key challenge is scalability and model complexity. As consistency models are applied to larger and morecomplex environments, such as autonomous driving or stock market trading, the number of variables andpossible future states grows exponentially. Managing this complexity in real-time, while ensuring consistentand reliable outcomes, presents a significant challenge without the use of advanced computational techniques.Another challenge involves data availability. Effective real-time decision-making requires access to up-to-dateand accurate data streams. In an online setting, the Consistency Planning method requires completing anentire episode to collect the trajectory before updating the model, which can lead to suboptimal results indynamic environments. This data collection process introduces delays that may hinder the effectiveness ofreal-time applications. Future work should include: 1) combining improved techniques in training consistency models (Song &Dhariwal, 2023), such as designing a changing weighting function and noise schedule more suitable forreinforcement learning scenarios; 2) combining the consistency inference process with changing guidanceschedule (Ma et al., 2023) to improve the quality of trajectory sampling; and 3) investigating online learningstrategies to reduce delays and improve performance in real-time scenarios.",
  "Scott Fujimoto and Shixiang Shane Gu. A minimalist approach to offline reinforcement learning. Advancesin neural information processing systems, 34:2013220145, 2021": "Will Grathwohl, Kuan-Chieh Wang, Jrn-Henrik Jacobsen, David Duvenaud, and Richard Zemel. Learningthe stein discrepancy for training and evaluating energy-based models without sampling. In InternationalConference on Machine Learning, pp. 37323747. PMLR, 2020. Haoran He, Chenjia Bai, Kang Xu, Zhuoran Yang, Weinan Zhang, Dong Wang, Bin Zhao, and XuelongLi. Diffusion model is an effective planner and data synthesizer for multi-task reinforcement learning.Advances in neural information processing systems, 36, 2024.",
  "Erik Nijkamp, Mitch Hill, Song-Chun Zhu, and Ying Nian Wu. Learning non-convergent non-persistentshort-run mcmc toward energy-based model. Advances in Neural Information Processing Systems, 32,2019": "Deepak Pathak, Parsa Mahmoudieh, Guanghao Luo, Pulkit Agrawal, Dian Chen, Yide Shentu, Evan Shel-hamer, Jitendra Malik, Alexei A Efros, and Trevor Darrell. Zero-shot visual imitation. In Proceedings ofthe IEEE conference on computer vision and pattern recognition workshops, pp. 20502053, 2018. Tim Pearce, Tabish Rashid, Anssi Kanervisto, Dave Bignell, Mingfei Sun, Raluca Georgescu, Sergio ValcarcelMacua, Shan Zheng Tan, Ida Momennejad, Katja Hofmann, et al.Imitating human behaviour withdiffusion models. arXiv preprint arXiv:2301.10677, 2023.",
  "Marc Rigter, Jun Yamada, and Ingmar Posner. World models via policy-guided trajectory diffusion. arXivpreprint arXiv:2312.08533, 2023": "Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, KamyarGhasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in neural information processingsystems, 35:3647936494, 2022. Laura Smith, J Chase Kew, Tianyu Li, Linda Luu, Xue Bin Peng, Sehoon Ha, Jie Tan, and Sergey Levine.Learning and adapting agile locomotion skills by transferring experience. arXiv preprint arXiv:2304.09834,2023. Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learningusing nonequilibrium thermodynamics. In International conference on machine learning, pp. 22562265.PMLR, 2015.",
  "AAppendix": "is a full list of dataset size. We use the latest version of dataset in D4RL (Fu et al., 2020). Ourexperiments utilize D4RL datasets comprising three distinct data types for the Hopper, HalfCheetah, andWalker environments: medium-expert, medium, and medium-replay. The medium-expert datasets containtrajectories generated through a combination of medium-level and expert-level policies, offering a blendof both optimal and suboptimal actions. The medium datasets are produced exclusively by medium-levelpolicies and therefore exhibit a greater proportion of suboptimal actions compared to the medium-expertdatasets. The medium-replay datasets represent the replay buffer of a medium-level agent, encompassing adiverse range of suboptimal actions along with exploration noise. The results in show the relationship between the computational time, denoised steps N and corre-sponding performance of Consistency Planning and Decision Diffuser on the task hopper-medium-expert-v2.The results in show the inference time and corresponding performance of Consistency Planning andDecision Diffusuer in different datasets, i.e, hopper-mr, hopper-m, halfcheetah-me and walker2d-me, withdiffusion steps N = 100 for Decision Diffuser and N = 2 for Consistency Planning. The choice of diffusionsteps in depends on the default training hyperparamters in Ajay et al. (2022) which claims all taskssuffer no performance loss with N = 100. Each cell contains the mean and standard deviation over 5 ran-dom seeds in the same server. As demonstrated in , we achieved more than 12-fold increase in speedwithout any loss in performance with N=2 for Consisitency Planning and N=20 for Decision Diffuser."
}