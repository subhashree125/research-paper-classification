{
  "Abstract": "An evolving area of research in deep learning is the study of architectures and inductive biasesthat support the learning of relational feature representations. In this paper, we address thechallenge of learning representations of hierarchical relationsthat is, higher-order relationalpatterns among groups of objects. We introduce relational convolutional networks, a neuralarchitecture equipped with computational mechanisms that capture progressively morecomplex relational features through the composition of simple modules. A key component ofthis framework is a novel operation that captures relational patterns in groups of objectsby convolving graphlet filterslearnable templates of relational patternsagainst subsetsof the input.Composing relational convolutions gives rise to a deep architecture thatlearns representations of higher-order, hierarchical relations. We present the motivation anddetails of the architecture, together with a set of experiments to demonstrate how relationalconvolutional networks can provide an effective framework for modeling relational tasks thathave hierarchical structure.",
  "Introduction": "Objects in the real world rarely exist in isolation; modeling the relationships between them is essential toaccurately capturing complex systems. As increasingly powerful machine learning models advance towardsbuilding internal world models, it becomes crucial to explore natural inductive biases to enable efficientlearning of relational representations. The computational challenge lies in developing the components requiredto construct robust, flexible, and progressively more complex relational representations.",
  ": A variant of arelational match-to-sampletask": "An important component of relational cognitive processing in humans is anability to reason about higher-order relational patterns between groups ofobjects. To illustrate this, it is instructive to consider experimental tasks fromthe cognitive psychology literature that probe abstract relational reasoningability. Consider, for example, the task depicted to the right in whichis a variant of a relational match-to-sample task (Ferster, 1960; Webb et al.,2021). The subject is presented with a source triplet of objects and severaltarget triplets, with each triplet having a particular relational pattern. The taskis to match the source to a target triplet with the same relational pattern (inthis case, the source has an ABA pattern that matches the second target).This task requires going beyond reasoning about pairwise relations; the subjectmust reason about each triplet of objects as a group, determine its relationalpattern, then compare it to those of the target triplets, inferring the abstractrule in the process. The ability to infer generalizable abstract rules in such tasksis believed to be unique to humans (Fagot et al., 2001).",
  "Published in Transactions on Machine Learning Research (09/2024)": "These properties of context normalization make it a confounder in the evaluation of relational architectures.In particular, for small context windows especially, context normalization symbolically encodes the relevantinformation. Experiments on relational architectures should evaluate the architectures ability to learn thoserelations from data. Hence, we do not use context normalization in our experiments.",
  "Related Work": "To place our framework in the context of previous work, we briefly discuss related forms of relational learningbelow, pointing first to the review of relational learning inductive biases by Battaglia et al. (2018). Graph neural networks (GNNs) are a class of neural network architectures which operate on graphs andprocess relational data (e.g., Niepert et al., 2016; Kipf and Welling, 2017; Schlichtkrull et al., 2018; Velikoviet al., 2018; Kipf et al., 2018; Xu et al., 2019). A defining feature of GNN models is their use of a form ofneural message-passing, wherein the hidden representation of a node is updated as a function of the hiddenrepresentations of its neighbors on a graph (Gilmer et al., 2017). Typical examples of tasks that GNNs areapplied to include node classification, graph classification, and link prediction (Hamilton, 2020). In GNNs, the relations are given to the model as input via edges in a graph. In contrast, our architecture,as well as the relational architectures described below, operate on collections of objects without any relationsgiven as input. Instead, such relational architectures must infer the relevant relations from the objectsthemselves. Still, graph neural networks can be applied to these relational tasks by passing in the collectionof objects along with a complete graph.",
  ", ,": ": Proposed architecture for relational convo-lutional networks. Hierarchical relations are modeledby iteratively computing pairwise relations betweenobjects and convolving the resultant relation tensorwith graphlet filters representing templates of relationsbetween groups of objects. Several works have proposed architectures with theability to model relations by incorporating an atten-tion mechanism (e.g., Vaswani et al., 2017; Velikoviet al., 2018; Santoro et al., 2018; Zambaldi et al.,2018; Locatello et al., 2020). Attention mechanisms,such as self-attention in Transformers (Vaswani etal., 2017), model relations between objects implicitlyas an intermediate step in an information-retrievaloperation to update the representation of each objectas a function of its context. There also exists a growing literature on neural ar-chitectures that aim to explicitly model relationalinformation between objects. An early example is therelation network proposed by Santoro et al. (2017),which produces an embedding representation for aset of objects based on aggregated pairwise rela-tions. Shanahan et al. (2020) proposes the PrediNetarchitecture, which aims to learn relational repre-sentations that are compatible with predicate logic.Kerg et al. (2022) proposes CoRelNet, a simple ar-chitecture based on similarity scores that aims todistill the relational inductive biases discovered inprevious work into a minimal architecture. Altabaaet al. (2024) and Altabaa and Lafferty (2024b) ex-plore relational inductive biases in the context ofTransformers, and propose a view of relational in-ductive biases as a type of selective informationbottleneck which disentangles relational informa-tion from object-level features. Webb et al. (2024) provides a cognitive science perspective on this idea,arguing that a relational information bottleneck may be a mechanism for abstraction in the human mind.",
  "Multi-Dimensional Inner Product Relation Module": "A relation function maps a pair of objects x, y P X to a vector that represents the relationship between them.For example, a relation may represent comparisons along different attributes of the two objects, such asx has the same color as y, x is larger than y, and x is to the left of y. In principle, this can be modeledby an arbitrary learnable function on the concatenation of the two objects feature representations. Forexample, Santoro et al. (2017) use multilayer perceptrons (MLPs) to model relations by processing theconcatenated feature vectors of object pairs. However, this approach lacks crucial inductive biases. While itis theoretically capable of modeling relations, it imposes no constraints to ensure that the learned pairwisefunction reflects meaningful relational patterns. In particular, it entangles the feature representations of thetwo objects without explicitly comparing their features. Following previous work (e.g., Vaswani et al., 2017; Webb et al., 2021; Kerg et al., 2022; Altabaa et al.,2024), we propose modeling pairwise relations between objects via inner products of feature maps. Thisintroduces added structure to the pairwise function that explicitly incorporates a comparison operation(the inner product). The advantage of this approach is that it provides added pressure to learn explicitlyrelational representations, disentangling relational information from attributes of individual objects, andinducing a geometry on the object space X. For example, in the symmetric case, the inner product relationrpx, yq \u0010 xpxq, pyqy satisfies symmetry, positive definiteness, and induces a pseudometric on X. The triangleinequality of the pseudometric expresses a transitivity propertyif x is related to y and y is related to z,then x must be related to z. More generally, we can allow for multi-dimensional relations by having multiple encoding functions, eachextracting a feature to compute a relation on. Furthermore, we can allow for asymmetric relations by having",
  "rpx, yq \u0010 px1pxq, 1pyqy , . . . , xdrpxq, drpyqyq ,(1)": "where 1, 1, . . . , dr, dr are learnable functions. The intuition is that, for each dimension, the encodersextract, or filter out, particular attributes of the objects and the inner products compute similarity acrosseach attribute. A relation, in this sense, is similarity across a particular attribute. In the asymmetric case,the attributes extracted from the two objects are different, resulting in an asymmetric relation where oneattribute of the first object is compared with a different attribute of the second object. For example, this canmodel relations of the form x is brighter than y (an antisymmetric relation). Altabaa and Lafferty (2024a) analyzes the function approximation properties of neural relation functions ofthe form of Equation (1). In particular, the function class of inner products of neural networks is characterizedin both the symmetric case and the asymmetric case. In the symmetric case (i.e., \u0010 ), it is shownthat inner products of MLPs are universal approximators for symmetric positive definite kernels. In theasymmetric case, inner products of MLPs are universal approximators for continuous bivariate functions.The efficiency of approximation is characterized in terms of a bound on the number of neurons needed toachieve a particular approximation error. To promote weight sharing, we can have one common non-linear map shared across all dimensions togetherwith different linear projections for each dimension of the relation. That is, r : X X Rdr is given by",
  "kPrdrs ,(2)": "where the learnable parameters are and W k1 , W k2 , k P rdrs. The non-linear map : X Rd may be anMLP, for example, and W k1 , W k2 are dproj d matrices. The class of functions realizable by Equation (2) isthe same as Equation (1) but enables greater weight sharing. The Multi-dimensional Inner Product Relation (MD-IPR) module receives a sequence of objects px1, . . . , xnqas input and models the pairwise relations between them by Equation (2), returning an n n dr relationtensor, Rri, js \u0010 rpxi, xjq, describing the relations between each pair of objects.",
  "Relational Convolutions with Discrete Groups": "In this section, we formalize a relational convolution operation which processes pairwise relations betweenobjects to produce representations of the relational patterns within groups of objects. Suppose that wehave a sequence of objects px1, . . . , xnq and a relation tensor R describing the pairwise relations betweenthem, obtained by an MD-IPR layer via Rri, js \u0010 rpxi, xjq. The key idea is to learn a template of relationsbetween a small set of objects, and to convolve the template with the relation tensor, matching it againstthe relational patterns in different groups of objects. This transforms the relation tensor into a sequenceof vectors, each summarizing the relational pattern in some group of objects. Crucially, this can now becomposed with another relational layer to compute higher-order relationsi.e., relations on relations. Fix some filter size s n, where s is a hyperparameter of the relational convolution layer. One filter of sizes is given by the graphlet filter f1 P Rssdr. This is a template for the pairwise relations between a group ofs objects. Since pairwise relations can be viewed as edges on a graph, we use the term graphlet filter torefer to a template of pairwise relations between a small set of objects. Let g rns be a group of s objectsamong px1, . . . , xnq. Then, denote the relation sub-tensor associated with this group by Rrgs :\u0010 rRri, jssi,jPg.We define the relational inner product between this relation subtensor and the filter f1 by",
  "\u000e P Rnf .(4)": "This vector summarizes various aspects of the relational pattern within a group, captured by several differentfilters1. Each filter corresponds to one dimension. This is reminiscent of convolutional neural networks, whereeach filter gives us one channel in the output tensor. For a given group g rns, the relational inner product with a graphlet filter, xRrgs, fyrel, gives us a vectorsummarizing the relational patterns inside that group. Let G be a set of groupings of the n objects, each ofsize s. The relational convolution between a relation tensor R and a relational graphlet filter f is defined asthe sequence of relational inner products with each group in G",
  "R f \u0015 pxRrgs, fyrelqgPG \u0011z1, . . . , z|G|P R|G|nf(5)": "In this section, we assumed that G was given. If some prior information is known about what groupingsare relevant, this can be encoded in G. Otherwise, if n is small, G can be all possible combinations of sizes. However, when n is large, considering all combinations will be intractable. In the next subsection, weconsider the problem of differentiably learning the relevant groups.",
  "Relational Convolutions with Group Attention": "In the above formulation, the groups are discrete. Having discrete groups can be desirable for interpretabilityif the relevant groupings are known a priori or if considering every possible grouping is computationallyand statistically feasible. However, if the relevant groupings are not known, then considering all possiblecombinations results in a rapid growth of the number of objects at each layer. In order to address these issues, we can explicitly model and learn the relevant groups. This allows us tocontrol the number of objects in the output sequence of a relational convolution such that only relevantgroups are considered. We propose modeling groups via an attention operation. 1We have overloaded the notation x, yrel, but will use the convention that a collection of filters is denoted by a bold symbol(e.g., f vs fi) to distinguish between the two forms of the relational inner product.",
  "(6)": "where xgi is the i-th object retrieved in the g-th group, qgi is the query for retrieving the i-th object in theg-th group, keypxjq is the key associated with the object xj, and is a temperature scaling parameter. The key for each object is computed as a function of its position, features, and/or context. For example,to group objects based on their position, the key can be a positional embedding, keypxiq \u0010 PEi. To groupbased on features, the key can be a linear projection of the objects feature vector, keypxiq \u0010 Wkxi. To groupbased on both position and features, the key can be a sum or concatenation of the above. Finally, computingkeys after a self-attention operation allows objects to be grouped based on the context in which they occur.",
  "gPrngs .(8)": "Overall, relational convolution with group attention can be summarized as follows: 1) learn ng groupingsof objects, retrieving s objects per group; 2) compute the relation tensor of each group using an MD-IPRmodule; 3) compute a relational convolution with a learned set of graphlet filters f, producing a sequence ofng vectors each describing the relational pattern within a (learned) group of objects. Computing input-dependent queries.In the simplest case, the query vectors are simply learnedparameters of the model, representing a fixed criterion for selecting the ng groups. The queries can alsobe produced in an input-dependent manner. There are many ways to do this. For example, the inputpx1, . . . , xnq can be processed with some sequence or set embedder (e.g., through a self-attention operation)producing a vector embedding that can be mapped to different queries tqgi ui,g using learned linear maps. Entropy regularization. Intuitively, we would ideally like the group attention scores in Equation (6) to beclose to discrete assignments. To encourage the model to learn more structured group assignments, we add anentropy regularization to the loss function, Lentr \u0010 png sq1",
  "g,i Hpgi,q, where Hpgi,q \u0010": "j gij logpgijqis the Shannon entropy.As a heuristic, this regularization can be scaled by a factor proportional tologpn_classesq{ logpnq so that it doesnt dominate the underlying tasks loss. Sparsity regularization inneural attention has been explored in several previous works, including through entropy regularization (e.g.,Niculae and Blondel, 2017; Martins et al., 2020; Attanasio et al., 2022). Symmetric relational inner products. So far, we considered ordered groups. That is, the relationalpattern computed by the relational inner product xRrgs, fyrel for the group p1, 2, 3q is different from the groupp2, 3, 1q. In some scenarios, symmetry in the representation of the relational pattern is a useful inductivebias. To capture this, we define a symmetric variant of the relational inner product that is invariant to theordering of the elements in the group. This can be done by pooling over all permutations in the group. Inparticular, we suggest max-pooling or average-pooling, although any set-aggregator would be valid. We definethe permutation-invariant relational inner product as",
  "Deep Relational Architectures by Composing Relational Convolutions": "A relational convolution block (including a MD-IPR module) is a simple neural module that can be composedto build a deep architecture for learning iteratively more complex relational feature representations. Following the notation in , let n denote the number of objects and d the object dimension at layer .A relational convolution block receives as input a sequence of objects of shape n d and returns a sequenceof objects of shape n1 d1 representing the relational patterns among groupings of objects. The outputdimension d1 corresponds to the number of graphlet filters nf, and is a hyperparameter. The sequencelength n1 corresponds to the number of groups, and is |G| in the case of given discrete groups (.1)or a hyperparameter ng in the case of learned groups via group attention (.2). Each compositionof a relational convolution block computes relational features of one degree higher (i.e., relations betweenrelations). A common recipe for building modern deep learning architectures is by using residual connections (He et al.,2016) and normalization (Ba et al., 2016). This can be achieved for relational convolutional networks by fixingthe number of groups ng and number of filters nf hyperparameters to be the same across all layers, suchthat the input shape and output shape remain the same. Then, letting H denote the hidden representationat layer , the overall architecture becomes H1 \u0010 NormpH W1RelConvBlockpHqq, where W1 is alinear transformation that controls where information is written to in the residual stream. This ResNet-stylearchitecture allows for the hidden representation to encode relational information at multiple layers ofhierarchy, retaining the information at shallower layers. Additionally, we can insert MLP layers to process therelational representations before the next relational convolution layer. In this paper, we limit our explorationto relatively shallow networks of the form H1 \u0010 RelConvBlockpHq.",
  "Experiments": "In this section, we empirically evaluate the proposed relational convolutional network architecture (abbreviatedRelConvNet) to assess its effectiveness at learning relational tasks. We compare this architecture to severalexisting relational architectures as well as general-purpose sequence models. The common input to all modelsis a sequence of objects X \u0010 px1, . . . , xnq P Rnd. We evaluate against the following baselines. Transformer (Vaswani et al., 2017). The Transformer is a powerful general-purpose sequence model.It consists of alternating self-attention and multi-layer perceptron blocks. Self-attention performsan information retrieval operation, which updates the internal representation of each object as afunction of its context. Dot product attention is computed via X1 SoftmaxppXWqqpXWkqqWvX,and the MLP is applied independently on each objects internal representation. The attention scorescomputed as an intermediate step in dot-product attention can perhaps be thought of as relationsthat determine what information to retrieve. PrediNet (Shanahan et al., 2020). The PrediNet architecture is an explicitly relational architectureinspired by predicate logic. At a high-level, the PrediNet architecture computes j relations betweenk pairs of objects. The k pairs of objects are selected via a learned attention operation. The jrelations refer to a difference between j-dimensional embeddings of the selected objects. Moreprecisely, for each head h P rks, a pair of objects Eh1 , Eh2 P Rd is retrieved via an attention operation,and the final output of PrediNet is a set of difference relations given by Dh \u0010 Eh1 Ws Eh2 Ws. CoRelNet (Kerg et al., 2022). The CoRelNet architecture is proposed as a minimal relationalarchitecture distilling the core inductive biases that the authors argue are important for relationaltasks.The CoRelNet module simply computes inner products between object representationsand applies Softmax normalization, returning an n n similarity matrix. That is, the objectsX \u0010 px1, . . . , xnq are processed independently to produce embeddings Z \u0010 pz1, . . . , znq, and the",
  "similarity matrix is computed as R \u0010 SoftmaxpZZq. The similarity matrix R is then flattened andpassed through an MLP to produce the final output": "Graph Neural Networks. Graph neural networks are a class of neural network architectureswhich operate on graphs-structured data. A graph neural network typically receives two inputs:a graph described by a set of edges, and feature vectors for each node in the graph. GNNs canbe described through the unifying framework of neural message-passing. Under this framework,graph-structured data is processed through an iterative message-passing operation given by hpl1qiUpdatephplqi , thplqj ujPNpiqq, where hp0qi xi. That is, each nodes internal representation is iterativelyupdated as a function of its neighborhood. Here, Update is parameterized by a neural network, andthe variation between different GNN architectures lies in the architectural design of this update process.We use Graph Convolution Networks (Kipf and Welling, 2017), Graph Attention Networks (Velikoviet al., 2018), and Graph Isomorphism Networks (Xu et al., 2019) as representative GNN baselines. CNN. As a non-relational baseline, we test a regular convolutional neural network which processes theraw image input. The central modules in the baselines above receive an object-centric representationas input. That is, a sequence of vector embeddings produced by a small CNN each corresponding toone of the n objects in the input. Here, instead, a deeper CNN model processes the raw image inputrepresenting the entire scene in an end-to-end manner.",
  "Relational Games": "The relational games dataset was contributed as a benchmark for relational reasoning by Shanahan et al.(2020). It consists of a family of binary classification tasks for identifying abstract relational rules betweena set of objects represented as images. The object images depict simple geometric shapes and consist ofthree different splits with different visual styles for evaluating out-of-distribution generalization, referred toas pentominoes, hexominoes, and stripes. The input is a sequence of objects arranged in a 3 3 grid.Each task corresponds to some relationship between the objects, and the target is to classify whether therelationship holds among the objects in the input or not (see ). In our experiments, we evaluate out-of-distribution generalization by training on the pentominoes objectsand evaluating on the hexominoes and stripes objects. The input to the models is presented as a sequenceof 9 objects, with each object represented as a 12 12 3 RGB image. All models share the commonarchitecture px1, . . . , xnq CNN tu MLP y, where tu indicates the central module being tested.In all models, the objects are first processed independently by a CNN with a shared architecture. Theprocessed objects are then passed to the central module of the model. The final prediction is produced byan MLP with a shared architecture. In this section, we focus our comparison on four models: RelConvNet(ours), CoRelNet (Kerg et al., 2022), PrediNet (Shanahan et al., 2020), and a Transformer (Vaswani et al.,2017)2. The pentominoes split is used for training, and the hexominoes and stripes splits are used to testout-of-distribution generalization after training. We train for 50 epochs using the categorical cross-entropyloss and the Adam optimizer with learning rate 0.001, 1 \u0010 0.9, 2 \u0010 0.999, \u0010 107, and batch size 512. Foreach model and task, we run 5 trials with different random seeds. Appendix A describes further experimentaldetails about the architectures and training setup. Out-of-distribution generalization. reports model performance on the two hold-out objectsets after training. On the hexominoes objects, which are similar-looking to the pentominoes objects usedfor training, RelConvNet and CoRelNet do nearly perfectly. PrediNet and the Transformer do well on thesimpler tasks, but struggle with the more difficult match pattern task. The stripes objects are visuallymore distinct from the objects in the training split, making generalization more difficult. We observe anoverall drop in performance for all models. The drop is particularly dramatic for CoRelNet3. The separationbetween RelConvNet and the other models is largest on the match pattern task of the stripes split (themost difficult task and the most difficult generalization split). Here, RelConvNet maintains a mean accuracy 2The GNN baselines failed to learn the relational games tasks in a way that generalizes, often severely overfitting. For clarityof presentation, we defer results on the GNN baselines to Appendix A.3The experiments in Kerg et al. (2022) on the relational games benchmark use a technique called context normalization (Webbet al., 2020) as a preprocessing step. We choose not to use this technique since it is an added confounder. We discuss this choicein Appendix C.",
  ": Out-of-distribution generalization on hold-out object sets. Bar heights indicate the mean over 5trials and the error bars indicate a bootstrap 95% confidence interval": "of 87% while the other models drop below 65%. We attribute this to RelConvNets ability to naturallyrepresent higher-order relations and model groupings of objects. The CNN baseline learns the easier same,between, and occurs tasks nearly perfectly, but completely fails to learn the more difficult xoccurs andmatch pattern tasks. This hard boundary suggests that explicit relational architectural inductive biases arenecessary for learning more difficult relational tasks. Data efficiency. We observe that the relational inductive biases of RelConvNet, and relational modelsmore generally, grant a significant advantage in sample-efficiency. shows the training accuracyover the first 2,000 batches for each model. RelConvNet, CoRelNet, and PrediNet are explicitly relationalarchitectures, whereas the Transformer is not. The Transformer is able to process relational informationthrough its attention mechanism, but this information is entangled with the features of individual objects(which, for these relational tasks, is extraneous information). The Transformer consistently requires thelargest amount of data to learn the relational games tasks. PrediNet tends to be more sample-efficient.RelConvNet and CoRelNet are the most sample-efficient, with RelConvNet only slightly more sample-efficienton most tasks. On the match pattern task, which is the most difficult, RelConvNet is significantly more sample-efficient.We attribute this to the fact that RelConvNet is able to model higher-order relations through its relationalconvolution module. The match pattern task can be thought of as a second-order relational taskit involvescomputing the relational pattern in each of two groups, and comparing the two relational patterns. Therelational convolution module naturally models this kind of situation since it learns representations of therelational patterns within groups of objects.",
  "ModelRelConvNetCoRelNetPrediNetTransformerCNN": ": Training curves, up to 2,000 batch steps, for each relational games task. Solid lines indicate themean over 5 trials and the shaded regions indicate a bootstrap 95% confidence interval. Note that this isin-distribution (i.e., on the pentominoes split). Group 1 Group 2 Group 3 Group 4 Group 5 Group 6 Group 7 Group 8",
  ": Learned groups in the match pattern tasksby a 2-layer RelConvNet with group attention": "Learning groups via group attention. Next, weanalyze RelConvNets ability to learn useful group-ings through group attention in an end-to-end man-ner. We train a 2-layer relational convolutional net-work with 8 learned groups and a graphlet size of3. We group based on position by using positionalembeddings for keypxiq. In , we visualizethe group attention scores gij (see Equation (6))learned from one of the training runs.For eachgroup g P rngs, the figure depicts a 3 3 grid repre-senting the objects attended to in that group. Sinceeach group contains 3 objects, we represent the valuepgijqiPr3s in the 3-channel HSV color representation.We observe that 1) group attention learns to ignore the middle row, which contains no relevant information;and 2) the selection of objects in the top row and the bottom row is structured. In particular, group 2considers the relational pattern within the bottom row and group 8 considers the relational pattern in thetop row, which is exactly how a human would tackle this problem. We refer to for an explorationof the effect of entropy regularization on group attention. We find that entropy regularization is necessary forthe model to learn and causes the group attention scores to converge to interpretable discrete assignments.",
  "Set: Grouping and Compositionality in Relational Reasoning": "Set is a card game that forms a simple-to-describe but challenging relational task. The objects are a set ofcards with four attributes, each of which can take one of three possible values. Color can be red, green, orpurple; number can be one, two, or three; shape can be diamond, squiggle, or oval; and fill can be solid,striped, or empty. A set is a triplet of cards such that each attribute is either a) the same on all three cards,or b) different on all three cards. In Set, the task is: given a hand of n 3 cards, find a set among them. a depicts a positive andnegative example for n \u0010 5, with indicating the set in the positive example. This task is deceptivelychallenging, and is representative of the type of relational reasoning that humans excel at but machinelearning systems still struggle with. To solve the task, one must process the sensory information of individualcards to identify the values of each attribute, and then reason about the relational pattern in each triplet of",
  "cards. The construct of relational convolutions proposed in this paper is a step towards developing machinelearning systems that can perform this kind of relational reasoning": "In this section, we evaluate RelConvNet on a task based on Set and compare it to several baselines. The taskis: given a collection of n \u0010 5 images of Set cards, determine whether or not they contain a set. All modelsshare the common architecture px1, . . . , xnq CNN tu MLP y, where tu indicates the central modulebeing tested. The CNN embedder is pre-trained on the task of classifying the four attributes of the cardsand an intermediate layer is used to generate embeddings. The output MLP architecture is shared across allmodels. Further architectural details can be found in Appendix A. In Set, there exists813\u0010 85 320 triplets of cards, of which 1 080 are a set. We partition the sets intotraining (70%), validation (15%), and test (15%) sets. The training, validation, and test datasets are generatedby sampling n-tuples of cards such that with probability 1{2 the n-tuple does not contain a set, and withprobability 1{2 it contains a set among the corresponding partition of sets. Partitioning the data in this wayallows us to measure the models ability to learn the rule and identify new unseen sets. We train for 100epochs with the same loss, optimizer, and batch size as the experiments in the previous section. For eachmodel, we run 10 trials with different random seeds. When using the default optimizer hyperparameters as in the previous experiment without hyperparametertuning, we find that RelConvNet is the only model able to meaningfully learn the task in a manner thatgeneralizes to unseen sets. In particular, we observe that many baselines severely overfit to the trainingdata, failing to learn the rule and generalize (see Appendix B.1). Although RelConvNet did not requirehyperparameter tuning, we carry out an extensive hyperparameter sweep for all other baselines individuallyin order to validate our conclusions against the best-achievable performance for each baseline. We ran a total",
  ": The relational convolution layer producesrepresentations that separates sets from non-sets": "In we analyze the geometry of the represen-tations learned by the relational convolution layer.We consider all triplets of cards, compute the rela-tion subtensor using the learned MD-IPR layer, andplot the relational inner product with the learnedgraphlet filter f P Rssdrnf . The result is a nf-dimensional vector for each triplet of cards.Weperform PCA to plot this in two dimensions, andcolor-code each triplet of cards according to whetheror not it forms a set. We find that the relationalconvolution layer learns a representation of the re-lational pattern in groups of objects that separatessets and non-sets. In particular, the two classesform clusters that are linearly separable even whenprojected down to two dimensions by PCA. Thisexplains why RelConvNet is able to learn the taskin a way that generalizes while the other models arenot. In Appendix E we expand on this discussion,and further analyze the representations learned by the MD-IPR layer, showing that the learned relationsmap to the color, number, shape, and fill attributes. It is perhaps surprising that models like GNNs and Transformers perform poorly on these relational tasks,given their apparent ability to process relations through neural message-passing and attention, respectively.We remark that GNNs operate in a different domain compared to relational models like RelConvNe, PrediNet,and CoRelNet. In GNNs, the relations are an input to the model, received in the form of a graph, and areused to dictate the flow of information in a neural message-passing operation. By contrast, in relationalconvolutional networks, the input is simply a set of objects without relationsthe relations need to be inferredas part of the feature representation process. Thus, GNNs operate in domains where relational information isalready present (e.g., analysis of social networks, biological networks, etc.), whereas our framework aims tosolve tasks that rely on relations but those relations need to be inferred end-to-end. This offers a partialexplanation for the inability of GNNs to learn this taskGNNs are good at processing network-style relationswhen they are given as input, but may not be able to infer and hierarchically process relations when theyare not given. In the case of Transformers, relations are modeled implicitly to direct information retrievalin attention, but are not encoded explicitly in the final representations. By contrast, RelConvNet operateson collections of objects and possesses inductive biases for learning iteratively more complex relationalrepresentations, guided only by the supervisory signal of the downstream task.",
  "Summary": "In this paper, we proposed a compositional architecture and framework for learning hierarchical relationalrepresentations via a novel relational convolution operation. The relational convolution operation we proposehere is a convolution in the sense that it considers a patch of the relation tensor, given by a subset ofobjects, and compares the relations within it to a template graphlet filter via an appropriately-defined innerproduct. This is analogous to convolutional neural networks, where an image filter is compared againstdifferent patches of the input image. Moreover, we propose an attention-based mechanism for modelinguseful groupings of objects in order to maintain scalability. By alternating inner product relation layers andrelational convolution layers, we obtain an architecture that naturally models hierarchical relations.",
  "Discussion on relational inductive biases": "In our experiments, we observed that general-purpose sequence models like the Transformer struggle tolearn tasks that involve relational reasoning in a data-efficient manner. The relational inductive biases ofRelConvNet, CoRelNet, and PrediNet result in significantly improved performance on the relational gamestasks. These models each implement different kinds of relational inductive biases, and are each designed withdifferent motivations in mind. For example, PrediNets architecture is loosely inspired by the structure ofpredicate logic, but can be understood as ultimately producing representations of pairwise difference-relations,with pairs of objects selected by an attention operation. CoRelNet is a minimal relational architecturethat consists of computing an n n inner product similarity matrix followed by a softmax normalization.RelConvNet, our proposed architecture, provides further flexibility across several dimensions. Like CoRelNet,it models relations as inner products of feature maps, but it achieves greater representational capacity bylearning multi-dimensional relations through multiple learned feature maps or filters. More importantly, therelational convolutions operation enables learning higher-order relations between groups of objects. This is incontrast to both PrediNet and CoRelNet, which are limited to pairwise relations. Our experiments showthat the inductive biases of RelConvNet result in improved performance in relational reasoning tasks. Inparticular, the Set task, where RelConvNet was the only model able to generalize non-trivially, demonstratesthe necessity for explicit inductive biases that support learning hierarchical relations.",
  "Limitations and future work": "The tasks considered here are solvable by modeling only second-order relations. In the case of the relationalgames benchmark of Shanahan et al. (2020), we observe that the tasks are saturated by the relationalconvolutional networks architecture. While the contains set task demonstrates a sharp separation betweenrelational convolutional networks and existing baselines, this task too only involves second-order relations. Amore thorough evaluation of this architecture, and future architectures for modeling hierarchical relations,would require the development of new benchmark tasks and datasets that involve a larger number of objectsand higher-order relations. This is a subtle and non-trivial task that we leave for future work. The modules proposed in this paper assume object-centric representations as input. In particular, the tasksconsidered in our experiments have an explicit delineation between different objects. In more general settings,object information may need to be extracted from raw stimulus explicitly by the system (e.g., a natural imagecontaining multiple objects in apriori unknown positions). Learning object-centric representations is an activearea of research (Sabour et al., 2017; Greff et al., 2019; Locatello et al., 2020; Kipf et al., 2022), and is relatedbut separate from learning relational representations. These methods produce a set of embedding vectors,each describing a different object in the scene, which can then be passed to the central processing module(e.g., a relational processing module such as RelConvNet). In future work, it will be important to explore howwell RelConvNet integrates with methods for learning object-centric representations in an end-to-end system.",
  "Code and Reproducibility": "The project repository can be found here: Itincludes an implementation of the relational convolutional networks architecture, code and instructions forreproducing our experimental results, and links to experimental logs. This work is supported by the funds provided by the National Science Foundation and by DoD OUSD (R&E)under Cooperative Agreement PHY-2229929 (The NSF AI Institute for Artificial and Natural Intelligence).",
  "Altabaa, Awni and John Lafferty (2024b). Disentangling and Integrating Relational and Sensory Informationin Transformer Architectures. arXiv: 2405.16727 [cs.LG] (cited on page 3)": "Altabaa, Awni, Taylor Whittington Webb, Jonathan D. Cohen, and John Lafferty (2024). Abstractors andrelational cross-attention: An inductive bias for explicit relational reasoning in Transformers. In: TheTwelfth International Conference on Learning Representations (cited on page 3). Attanasio, Giuseppe, Debora Nozza, Dirk Hovy, and Elena Baralis (2022). Entropy-based Attention Regu-larization Frees Unintended Bias Mitigation from Lists. In: Findings of the Association for ComputationalLinguistics: ACL 2022 (cited on page 6).",
  "[cs, stat] (cited on page 2)": "Carpenter, Patricia A, Marcel A Just, and Peter Shell (1990). What one intelligence test measures: atheoretical account of the processing in the Raven Progressive Matrices Test. In: Psychological review. Englund, CE, DL Reeves, CA Shingledecker, DR Thorne, KP Wilson, and FW Hegge (1987). Unified tri-service cognitive performance assessment battery (UTC-PAB) I. Design and Specification of the Battery.In: Naval Health Research Center Report. San Diego, California. Fagot, Jol, Edward A Wasserman, and Michael E Young (2001). Discriminating the relation betweenrelations: the role of entropy in abstract conceptualization by baboons (Papio papio) and humans (Homosapiens). In: Journal of Experimental Psychology: Animal Behavior Processes (cited on page 1).",
  "Frank, Stefan L, Rens Bod, and Morten H Christiansen (2012). How hierarchical is language use? In:Proceedings of the Royal Society B: Biological Sciences (cited on page 28)": "Gilmer, Justin, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, and George E. Dahl (2017). NeuralMessage Passing for Quantum Chemistry. In: International Conference on Machine Learning. PMLR(cited on page 2). Greff, Klaus, Raphal Lopez Kaufman, Rishabh Kabra, Nick Watters, Christopher Burgess, Daniel Zoran, LoicMatthey, Matthew Botvinick, and Alexander Lerchner (2019). Multi-Object Representation Learning withIterative Variational Inference. In: Proceedings of the 36th International Conference on Machine Learning.Ed. by Kamalika Chaudhuri and Ruslan Salakhutdinov. Proceedings of Machine Learning Research. PMLR(cited on page 13).",
  "Sabour, Sara, Nicholas Frosst, and Geoffrey E Hinton (2017). Dynamic routing between capsules. In:Advances in neural information processing systems (cited on page 13)": "Santoro, Adam, Ryan Faulkner, David Raposo, Jack Rae, Mike Chrzanowski, Theophane Weber, DaanWierstra, Oriol Vinyals, Razvan Pascanu, and Timothy Lillicrap (2018). Relational Recurrent NeuralNetworks. In: Advances in Neural Information Processing Systems. Curran Associates, Inc. (cited onpage 3). Santoro, Adam, David Raposo, David G Barrett, Mateusz Malinowski, Razvan Pascanu, Peter Battaglia,and Timothy Lillicrap (2017). A simple neural network module for relational reasoning. In: Advances inneural information processing systems (cited on pages 3, 28). Schlichtkrull, Michael, Thomas N Kipf, Peter Bloem, Rianne Van Den Berg, Ivan Titov, and Max Welling(2018). Modeling relational data with graph convolutional networks. In: The Semantic Web: 15thInternational Conference, ESWC 2018, Heraklion, Crete, Greece, June 37, 2018, Proceedings 15. Springer(cited on page 2). Shanahan, Murray, Kyriacos Nikiforou, Antonia Creswell, Christos Kaplanis, David Barrett, and MartaGarnelo (2020). An Explicitly Relational Neural Network Architecture. In: Proceedings of the 37thInternational Conference on Machine Learning. Ed. by Hal Daum III and Aarti Singh. Proceedings ofMachine Learning Research. PMLR (cited on pages 2, 3, 7, 8, 13). Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ukasz Kaiser,and Illia Polosukhin (2017). Attention is all you need. In: Advances in neural information processingsystems (cited on pages 3, 7, 8).",
  "Xu, Keyulu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka (2019). How Powerful are Graph NeuralNetworks? In: International Conference on Learning Representations (cited on pages 2, 8)": "Zaheer, Manzil, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Russ R Salakhutdinov, and AlexanderJ Smola (2017). Deep sets. In: Advances in neural information processing systems (cited on page 28). Zambaldi, Vinicius, David Raposo, Adam Santoro, Victor Bapst, Yujia Li, Igor Babuschkin, Karl Tuyls,David Reichert, Timothy Lillicrap, Edward Lockhart, et al. (2018). Deep reinforcement learning withrelational inductive biases. In: International conference on learning representations (cited on page 3). Zeiler, Matthew D and Rob Fergus (2014). Visualizing and understanding convolutional networks. In:Computer VisionECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014,Proceedings, Part I 13. Springer (cited on page 2).",
  "A.1Relational Games (.1)": "The pentominoes split is used for training, and the hexominoes and stripes splits are used to test out-of-distribution generalization after training. We hold out 1000 samples for validation (during training) and 5000samples for testing (after training), and use the rest as the training set. We train for 50 epochs using thecategorical cross-entropy loss and the Adam optimizer with learning rate 0.001, 1 \u0010 0.9, 2 \u0010 0.999, \u0010 107.We use a batch size of 512. For each model and task, we run 5 trials with different random seeds.contains text descriptions of each task in the relational games dataset in the experiments of .1. contains a description of the architectures of each model (or shared component) in the experiments. reports the accuracy on the hold-out object sets (i.e., the numbers depicted in of the maintext). Figures 10 and 11 explore the effect of entropy regularization in group attention on learning using thematch pattern task as an example.",
  "A.2Set (.2)": "We train for 100 epochs using the cross-entropy loss. RelConvNet uses the Adam optimizer with learningrate 0.001, 1 \u0010 0.9, 2 \u0010 0.999, \u0010 107. The baselines each use their own individually-tuned optimizationhyperparameters, described in Appendix B. We use a batch size of 512. For each model and task, we run5 trials with different random seeds. contains a description of the architecture of each model inthe contains set experiments of .2. reports the generalization accuracies on the hold-outsets (i.e., the numbers depicted in b of the main text). explores the effect of differentRelConvNet hyperparameters on the models ability to learn the the Set task.",
  "Cross-entropy LossGroup Attention Entropy": ": Trade-off between task loss and group attention entropy. RelConvNet models are trained on thematch pattern task in the Relational Games benchmark varying the entropy regularization level. The overallmodel loss is Lloss Lentr, where Lloss \u0010 CrossEntropypy, yq is the task loss (blue line), Lentr is the entropyregularization term for the group attention scores (orange line) as defined in .2, and is a scalingfactor. Different lines correspond to different values of . When \u0010 0 (no entropy regularization) the modelfails to learn the task. A small amount of regularization is enough to guide the model to a good solution.Increasing causes smaller group attention entropy at convergence, approaching discrete assignments.",
  "Eect of Entropy Regularization on Group Attention": ": Effect of group attention entropy regularization. Group attention entropy (left) and baselinecross-entropy loss (right) of a relational convolutional network model trained on the match pattern taskwith different levels of entropy regularization. The overall model loss is Lloss Lentr, where Lloss is thetask loss, Lentr is the entropy regularization term, and is a scaling factor. Different lines correspond todifferent values of . Without entropy regularization, the model fails to learn the task. With sufficient entropyregularization, the model is able to learn the task and group attention converges towards discrete assignments.The group attention entropy starts at log 9 \u0013 2.2 (the entropy of a uniform distribution) and decreases overthe course of training. Expectedly, larger values cause the entropy to decrease faster, converging towards asmaller value. When is too large, the entropy regularization overwhelms the base cross-entropy loss andresults in converging to a worse cross-entropy loss. Intuitively, one needs to strike a balance such that boththe entropy regularization and the cross-entropy loss guide the evolution of the group attention map.",
  "RelConvNetRelConvNet (dr = 1)RelConvNet (Symmetry Relations = False)": ": Exploring the effect of multi-dimensional relations and symmetric relations in RelConvNet.RelConvNet models matching the architecture described in are trained on the Set task. We testtwo variants: 1) set the relation dimension to be dr \u0010 1 (instead of dr \u0010 16), and 2) remove the symmetryinductive bias (i.e., W1 \u0018 W2 in Equation (2)). We find that with dr \u0010 1 (which is analogous to CoRelNetssingle-dimensional similarity matrix), the model struggles to find good solutions. In 10 different runs withrandom seeds, one run was able to find a good solution reaching an accuracy of 98.5%, whereas the other runswere stuck below 65%. This suggests that having multi-dimensional relations yields a more robust model withmultiple different avenues for finding good solutions during the optimization process. In the case of the modelwith the asymmetric relations (i.e., lacking a symmetry inductive bias), the model is able to fit the trainingdata, but fails to generalize. This suggests the symmetry is an important inductive bias for certain tasks.",
  "BHyperparameter sweep for baseline models": "In order to ensure that we compare RelConvNet against the best-achievable performance by each baselinearchitecture, we carry out an extensive hyperparameter sweep over combinations of architectural hyperparam-eters and optimization hyperparameters. In particular, as seen in Appendix B.1, the baseline models severelyoverfit on the Set task, fitting the training data but failing to generalize to unseen sets. Hence, we explorewhether it is possible to avoid or alleviate overfitting through an appropriate choice of hyperparameters. In , we vary the number of layers in the baseline models to select an optimal configuration of eacharchitecture. We find that increased depth beyond 2 layers is generally detrimental on this task. Based onthese results, we choose the optimal number of layers as 2 for the Transformer, GCN, GIN baselines and 1 forthe GAT baseline. In , we vary the level of weight decay. Expectedly, larger weight decay results in decreased trainingaccuracy. Generally, weight decay has a small effect on validation performance (e.g., no discernable effect inCoRelNet or CNN). For some models, some choices of weight decay result in improved validation performance.Based on these results, we use a weight decay of 0 for CoRelNet/CNN, 0.032 for Transformer/GAT/GIN,and 1.024 for PrediNet/GCN/LSTM. In , we explore the effect of the learning rate schedule, comparing a cosine decay schedule againstour default constant learning rate. For most models, there is no significant difference, with a constant learningrate sometimes slightly better. On the GAT model, however, the cosine learning rate schedule results insignificantly improved performance. Based on these results, we use a cosine learning rate schedule for GATand a constant learning rate for all other models.",
  "ModelTransformerGCNGATGIN": ": Hyperparameter sweep over number of layers in baseline architectures. Transformers and GNNs(e.g., GCN, GAT, GIN) are compositional deep learning architectures. Here, we explore the effect of depth(i.e., number of layers) on task performance for these baselines. The plots show the maximum trainingand validation accuracy reached throughout training for each depth (5 trials with different random seeds).Generally, we find that generalization performance drops with increasing depth. The optimal depth for theTransformer, GCN, and GIN models is 2 layers, and the optimal depth for the GAT model is 1-layer. Theperformance drop with depth can perhaps be attributed to increased difficulty of training and overfittingdue to limited data and a lack of relational inductive biases. The AdamW optimizer is used with a constantlearning rate of 103.",
  "ModelCoRelNetPrediNetTransformerGCNGATGINLSTMCNNLR SchedConstantCosine": ": Hyperparameter sweep over learning rate schedule (constant vs cosine decay). We explore theeffect of the learning rate schedule on model performance, comparing a constant learning rate against a cosinedecay schedule. For most models, there is no significant difference, with a constant learning rate sometimesslightly better. On the GAT model, however, the cosine learning rate schedule results in significantly improvedperformance.",
  "CDiscussion on use of TCN in evaluating relational architectures": "In .1 the CoRelNet model of Kerg et al. (2022) was among the baselines we compared to. In thatwork, the authors also evaluate their model on the relational games benchmark. A difference between theirexperimental set up and ours is that they use a method called context normalization as a preprocessingstep on the sequence of objects. Context normalization was proposed by Webb et al. (2020). The proposal is simple: Given a sequence ofobjects, px1, . . . , xmq, and a set of context windows W1, . . . , WW t1, . . . , mu which partition the objects,each object is normalized along each dimension with respect to the other objects in its context. That is,pz1, . . . , zmq \u0010 CNpx1, . . . , xmq is computed as,",
  "j,for t P Wk": "where \u0010 p1, . . . , dq, \u0010 p1, . . . , dq are learnable gain and shift parameters for each dimension (initializedat 1 and 0, respectively, as with batch normalization). The context windows represent logical groupingsof objects that are assumed to be known. For instance, (Webb et al., 2021; Kerg et al., 2022) consider arelational match-to-sample task where 3 pairs of objects are presented in sequence, and the task is to identifywhether the relation in the first pair is the same as the relation in the second pair or the third pair. Here,the context windows would be the pairs of objects. In the relational games match rows pattern task, thecontext windows would be each row. It is reported in (Webb et al., 2021; Kerg et al., 2022) that context normalization significantly accelerateslearning and improves out-of-distribution generalization. Since (Webb et al., 2021; Kerg et al., 2022) usecontext normalization in their experiments, in this section we aim to explain our choice to exclude it. Weargue that context normalization is a confounder and that an evaluation of relational architectures withoutsuch preprocessing is more informative.",
  "CNpx, yq \u0010 psignpx yq, signpy xqq": "In particular, what context normalization does when there are two objects is, along each dimension, output 0if the value is the same, and 1 if it is different (encoding whether it is larger or smaller). Hence, it makes thecontext-normalized output independent of the original feature representation. For tasks like relational games,where the key relation to model is same/different, this preprocessing is directly encoding this informationin a symbolic way. In particular, for two objects x1, x2, context normalized to produce z1, z2, we havethat x1 \u0010 x2 if and only if xz1, z2y \u0010 0. This makes out-of-distribution generalization trivial, and does notproperly test a relational architectures ability to model the same/different relation.",
  "DHigher-order relational tasks": "As noted in the discussion, the tasks considered in this paper are solvable by modeling second-order relationsat most. One of the main innovations of the relational convolutions architecture over existing relationalarchitectures is its compositionality and ability to model higher-order relations. An important directionof future research is to test the architectures ability to model hierarchical relations of increasingly higherorder. Constructing such benchmarks is a non-trivial task which requires careful thought and consideration.This was outside the scope of this paper, but we provide an initial discussion here which may be useful forconstructing such benchmarks in future work.",
  "x1 ^ ppx2 _ x3q ^ pp x3 ^ x4q _ px5 ^ x6 ^ x7qqq": "Evaluating this logical expression (in this form) requires iteratively grouping objects and computing therelations between them. For instance, we begin by computing the relation within g1 \u0010 px3, x4q and therelation within g2 \u0010 px5, x6, x7q, then we compute the relation between the groups g1 and g2, etc. For atask which involves logical reasoning of this hierarchical form, one might imagine the group attention inRelConvNet learning the relevant groups and the relational convolution operation computing the relationswithin each group. Taking inspiration from logical reasoning with such hierarchical structure may lead tointeresting benchmarks of higher-order relational representation. Sequence modeling. In sequence modeling (e.g., language modeling), modeling the relations betweenobjects is usually essential. For example, syntactic and semantic relations between words are crucial toparsing language. Higher-order relations are also important, capturing syntactic and semantic relationalfeatures across different locations in the text and across multiple length-scales and layers of hierarchy (seefor example some relevant work in linguistics Frank et al., 2012; Rosario et al., 2002). The attentionmatrix in Transformers can be thought of as implicitly representing relations between tokens. It is possiblethat composing Transformer layers also learns hierarchical relations. However, as shown in this work andprevious work on relational representation, Transformers have limited efficiency in representing relations.Thus, incorporating relational convolutions into Transformer-based sequence models may yield meaningfulimprovements in the relational aspects of sequence modeling. One way to do this is by cross-attending to athe sequence of relational objects produced by relational convolutions, each of which summarizes the relationswithin a group of objects at some level of hierarchy. Set embedding. The objective of set embedding is to map a collection of objects to a euclidean vectorwhich represents the important features of the objects in the set (Zaheer et al., 2017). Depending on what theset embedding will be used for, it may need to represent a combination of object-level features and relationalinformation, including perhaps relations of higher order. A set embedder which incorporates relationalconvolutions may be able to generate representations which summarize relations between objects at multiplelayers of hierarchy. Visual scene understanding. In a visual scene, there are typically several objects with spatial, visual,and semantic relations between them which are crucial for parsing the scene. The CLEVR benchmark onvisual scene understanding (Johnson et al., 2017) was used in early work on relational representation (Santoroet al., 2017). In more complex situations, the objects in the scene may fall into natural groupings, and thespatial, visual, and semantic relations between those groups may be important for parsing a scene (e.g.,objects forming larger components with functional dependence determined by the relations between them).Integrating relational convolutions into a visual scene understanding system may enable reasoning aboutsuch higher-order relations.",
  "EGeometry of representations learned by MD-IPR and Relational Convolutions": "In this section, we explore and visualize the representations learned by MD-IPR and RelConv layers. Inparticular, we will visualize the representations produced by the RelConvNet model trained on the Settask described in .2. Recall that the MD-IPR layer learns encoders 1, 1, . . . , dr, dr. In thismodel dr \u0010 16, i \u0010 i (so that learned relations are symmetric), and each i is a linear transformation todproj \u0010 4-dimensional space. The representations learned by a selection of 6 encoders is visualized in .For each of the 81 possible Set cards, we apply each encoder in the MD-IPR layer, reduce to 2-dimensionsvia PCA, and visualize how each encoder separates the 4 attributes: number, color, fill, and shape. Observe,for example, that Encoder 0 disentangles color and shape, Encoder 2 disentangles fill, and Encoder 3disentangles number. Next, we visualize, we explore the geometry of learned representations of relation vectors. That is, the innerproducts producing the 16-dimensional relation vector for each pair of objects. For each812pairs of Setcards, we compute the 16-dimensional relation vector learned by the MD-IPR layer, reduce to 2 dimensionsvia PCA, and visualize how the learned relation disentangles the latent same/different relations among thefour attributes. This is shown in . We see some separation of the underlying same/different relationsamong the four attributes, even with only two dimensions out of 16. Finally, we visualize the representations learned by the relational convolution layer. Recall that this layerlearns a set of graphlet filters f P Rssdrnf which form templates of relational patterns against whichgroups of objects are compared. In our experiments, the filter size is s \u0010 3 and the number of filters isnf \u0010 16. Hence, for each group g of 3 Set cards, the relational convolution layer produces a 16-dimensionalvector, xRrgs, fyrel P Rnf , summarizing the relational structure of the group. Of the813possible triplets ofSet cards, we create a balanced sample of sets and non-sets. We then compute xRrgs, fyrel and reduce to 2dimensions via PCA. strikingly shows that the representations learned by the relational convolutionlayer very clearly separate triplets of cards which form a set from those that dont form a set."
}