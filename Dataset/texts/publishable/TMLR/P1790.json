{
  "Abstract": "Deep hierarchical variational autoencoders (VAEs) are powerful latent variable generativemodels.In this paper, we introduce Hierarchical VAE with Diffusion-based VariationalMixture of the Posterior Prior (VampPrior). We apply amortization to scale the VampPriorto models with many stochastic layers. The proposed approach allows us to achieve betterperformance compared to the original VampPrior work and other deep hierarchical VAEs,while using fewer parameters. We empirically validate our method on standard benchmarkdatasets (MNIST, OMNIGLOT, CIFAR10) and demonstrate improved training stabilityand latent space utilization.",
  "Introduction": "Latent variable models (LVMs) parameterized with neural networks constitute a large group in deep gener-ative modeling (Tomczak, 2022). One class of LVMs, Variational Autoencoders (VAEs) (Kingma & Welling,2014; Rezende et al., 2014), utilize amortized variational inference to efficiently learn distributions over vari-ous data modalities, e.g., images (Kingma & Welling, 2014), audio (Van Den Oord et al., 2017), or molecules(Gmez-Bombarelli et al., 2018). The expressive power of VAEs can be improved by introducing a hierarchyof latent variables. The resulting hierarchical VAEs, such as ResNET VAEs (Kingma et al., 2016), BIVA(Maale et al., 2019), very deep VAE (VDVAE) (Child, 2021), or NVAE (Vahdat & Kautz, 2020) achievestate-of-the-art performance on images in terms of the negative log-likelihood (NLL). However, hierarchicalVAEs are known to have training instabilities (Vahdat & Kautz, 2020). To mitigate these issues, varioustricks were proposed, such as gradient skipping (Child, 2021), spectral normalization (Vahdat & Kautz,2020), or softmax parameterization of variances (Hazami et al., 2022). In this work, we propose a differentapproach that focuses on two aspects of hierarchical VAEs: (i) the structure of latent variables, and (ii) theform of the prior for the given structure. We introduce several changes to the architecture of parameteriza-tions (i.e. neural networks) and the model itself. As a result, we can train a powerful hierarchical VAE withgradient-based methods and ELBO as the objective without any hacks. In the VAE literature, it is a well known fact that the choice of the prior plays an important role in theresulting VAE performance (Chen et al., 2017; Tomczak, 2022).For example, VampPrior (Tomczak &Welling, 2018), a form of the prior approximating the aggregated posterior, has been shown to consistentlyoutperform VAEs with a standard prior and a mixture prior. In this work, we extend the VampPrior to deephierarchical VAEs in an efficient manner. We propose utilizing a non-trainable linear transformation likeDiscrete Cosine Transform (DCT) to obtain pseudoinputs. Together with our architecture improvements,we can achieve state-of-the-art performance, high utilization of the latent space, and stable training of deephierarchical VAEs.",
  "Hierarchical Variational Autoencoders": "Let us consider random variables x X D (e.g. X = R). We observe N xs sampled from the empiricaldistribution r(x). We assume that each x has L corresponding latent variables z1:L = (z1, . . . , zL), wherezl RMl and Ml is the dimensionality of each variable. We aim to find a latent variable generative modelwith unknown parameters , p(x, z1:L) = p(x|z1:L)p(z1:L). In general, optimizing latent-variable models with nonlinear stochastic dependencies is non-trivial. A possiblesolution is approximate inference, e.g., in the form of variational inference (Jordan et al., 1999) with afamily of variational posteriors over latent variables {q(z1:L|x)}. This idea is exploited in VariationalAuto-Encoders (VAEs) (Kingma & Welling, 2014; Rezende et al., 2014), in which variational posteriors arereferred to as encoders. As a result, we optimize a tractable objective function called the Evidence LowerBOund (ELBO) over the parameters of the variational posterior, , and the generative part, , that is:",
  "p(z) = Ex [q(z|x)] .(3)": "The main problem with the optimal prior in Eq. 3 is the summation over all N training datapoints. Since Ncould be very large (e.g., tens or hundreds of thousands), using such a prior is infeasible due to potentiallyvery high memory demands. As an alternative approach, Tomczak & Welling (2018) proposed VampPrior,a new class of priors that approximate the optimal prior in the following manner:",
  "K": "k (u uk) is the distribution ofu in the form of the mixture of Diracs deltas, and {uk}Kk=1 are learnable parameters (we will refer to themas pseudoinputs as well). K is a hyperparameter and is assumed to be smaller than the size of the trainingdataset, K < N. Pseudoinputs are randomly initialized before training and are learned along with modelparameters by optimizing the ELBO objective using a gradient-based method. In follow-up work, Egorov et al. (2021) suggested using a separate objective for pseudoinputs (a greedyboosting approach) and demonstrated the superior performance of such formulation in the continual learningsetting. Here, we will present a different approximation to the optimal prior instead.",
  "Ladder VAEs (a.k.a. Top-down VAEs)": "We refer to models with many stochastic layers L as deep hierarchical VAEs. They can differ in the waythe prior and variational posterior distributions are factorized and parameterized.Here, we follow thefactorization proposed in Ladder VAE (Snderby et al., 2016) that considers the prior distribution over thelatent variables factorized in an autoregressive manner:",
  "VampPrior for hierarchical VAE": "Directly using the VampPrior approximation (see Eq. 4) for deep hierarchical VAE can be very computation-ally expensive since it requires evaluating the variational posterior of all latent variables for K pseudoinputsat each training iteration. Thus, (Tomczak & Welling, 2018) proposed a modification in which only the toplatent variable uses VampPrior, namely:",
  "where r(u) =1K": "k (u uk) with learnable pseudoinputs {uk}Kk=1. In this approach, there are a fewproblems: (i) how to pick the best number of pseudoinputs K, (ii) how to train pseudoinputs, and (iii) howto train the VampPrior in a scalable fashion. The last issue results from the first two problems and the factthat the dimensionality of pseudoinputs is the same as the original data, i.e., dim(u) = dim(x). Here, we propose a different prior parameterization to overcome all these three problems. Our approachconsists of three steps in which we approximate the VampPrior at all levels of the deep hierarchical VAE.We propose to amortize the distribution of pseudoinputs in VampPrior and use them to directly conditionthe prior distribution:p(z1:L) = Exq,(z1:L|x) Ex,r(u|x)p(z1:L|u),(8) where we use r(u) = Ex[r(u|x)]. Using r(u|x), which is cheap to evaluate for any input, we avoid theexpensive procedure of encoding pseudoinputs along with the inputs x. Amortizing the VampPrior solvesthe problem of picking K and helps with training pseudoinputs. To define conditional distribution, we treat pseudoinputs u as a result of some noisy non-trainable trans-formation of the input datapoints x. Let us consider a transformation f : X D X P , i.e., u = f(x) + ,where is a standard Gaussian random variable and is the standard deviation. Note that by applying f,e.g., a linear transformation, we can lower the dimensionality of pseudoinputs, dim(u) < dim(x), resultingin better scalability. As a result, we get the following amortized distribution:",
  "Return: ux RcDD": "The crucial part then is how to choose the transformation f. It is a non-trivial choice since we require thefollowing properties of f: (i) it should result in dim(u) < dim(x), (ii) u should be a reasonable representationof x, (iii) it should be easily computable (e.g., fast for scalability).We have two candidates for suchtransformations. First, we can consider a downsampled version of an image. Second, we propose to use adiscrete cosine transform. We will discuss this approach in the following subsection. Moreover, the outlined amortized VampPrior in the previous two steps seems to be a good candidate forefficient and scalable training. However, it does not seem to be suitable for generating new data. Therefore,we propose including pseudoinputs as the final level in our model and use a marginal distribution r(u) thatapproximates r(u). Here, we propose to use a diffusion-based model for r(u).",
  "DCT-based pseudoinputs": "The first important component in our approach is the form of the non-trainable transformation from theinput to the pseudoinput space. We assume that for u to be a reasonable representation of x means that ushould preserve general patterns (information) of x, but it does not necessarily contain any high-frequencydetails of x. To achieve this, we propose to use a discrete cosine transform1 (DCT) to convert the input intoa frequency domain and then filter the high-frequency component. DCTDCT (Ahmed et al., 1974) is a widely used transformation in signal processing for image, video, andaudio data. For example, it is part of the JPEG standard (Pennebaker & Mitchell, 1992). For instance,consider a signal as a 3-dimensional tensor x RcDD. DCT is a linear transformation that decomposeseach channel xi on a basis consisting of cosine functions of different frequencies: uDCT,i = CxiC, where",
  "2k": "Our transformationWe use the DCT transform as the first step in the procedure of computing pseu-doinputs. Let us assume that each channel of x is D D. We select the desired size of the context d < Dand remove (crop) D d bottom rows and right-most columns for each channel in the frequency domainsince they contain the highest-frequency information. Finally, we perform normalization using the matrix Sthat contains the maximal absolute value of each frequency. We calculate this matrix once (before trainingthe model) using all the training data: S = maxxDtrain |DCT(x)|. The complete procedure is described inAlgorithm 1 and we denote it as fdct. In the frequency domain, a pseudoinput has a smaller spatial dimension than its corresponding datapoint.This allows us to use a small prior model and lower the memory consumption. However, we noticed em-pirically that conditioning the amortized VampPrior (see Eq. 8) on the pseudoinput in the original domainmakes training easier. Therefore, the pseudo-inverse is applied as a part of the TopDown path. First, westart by multiplying the pseudoinput by the normalization matrix S. Afterward, we pad each channel withzeros to account for the \"lost\" high frequencies. Lastly, we apply the inverse of the Discrete Cosine Trans-form (iDCT). We denote the procedure for converting a pseudoinput from the frequency domain to the datadomain as f dct and describe it in Algorithm 2.",
  "In the variational posteriors, we use the amortization of r(u) outlined in Eq. 8": "Let us consider a Ladder VAE (a TopDown VAE) with three levels of latent variables.We depict thegraphical model of this latent variable model in a with the inference model on the left and thegenerative model on the right. Note that inference and generative models use shared parameters in theTopDown path, denoted by the blue arrows. In b we show a graphical model of our proposed model that additionally contains pseudoinputs u.The generation model (b right) is conditioned on the pseudoinput variable u at each level. Thisformulation is similar to the iVAE model (Khemakhem et al., 2020) in which the auxiliary random variableu is used to enforce the identifiability of the latent variable model. However, unlike iVAE, we do not treat uas a separate observed variable. Instead, we use a non-trainable transformation to obtain it during trainingand introduce the pseudoinput prior distribution p(u) with learnable parameters to sample u at test time.This allows us to get unconditional samples from the model.",
  "L(x, , , ) = Eq(z1:L|x) ln p(x|z1:L) Eq(u|x)DKL [q(z1:L|x)p(z1:L|u)] DKL [r(u|x)r(u)] .(15)": "The ELBO for our model gives a straightforward objective for training an approximate prior by minimizingthe Kullback-Leibler between r(u|x) and the amortized VampPrior r(u) = Ex[r(u|x)]. However, calculatingthe KL-term with r(u) is computationally demanding and, in fact, using r(u) as the prior does not help ussolve the original problem of using the VampPrior. Therefore, in the following section, we propose to use anapproximation r(u) instead.",
  "Diffusion-based VampPrior": "Even though pseudoinputs are assumed to be much simpler than the observed datapoint x (e.g., in terms oftheir dimensionality), a very flexible prior distribution r(u) is required to ensure the high quality of the finalsamples. Since we cannot use the amortized VampPrior directly, following (Vahdat et al., 2021; Wehenkel &Louppe, 2021), we propose to use a diffusion-based generative model (Ho et al., 2020) as the prior and theapproximation of r(u). Diffusion models are flexible generative models and, in addition, can be seen as latent variable generativemodels (Kingma et al., 2021). As a result, we have access to the lower bound on its log-likelihood function:",
  "Model Details: Architecture and parameterization": "The model architecture and parameterization are crucial to the scalability of the model. In this section, wediscuss the specific choices we made. The starting point for our architecture is the architecture proposedin VDVAE (Child, 2021). However, there are certain differences. We schematically depict our architecturein a. We consider a hierarchical TopDown VAE with L stochastic layers, namely, latent variablesz1, . . . , zL. We assume that each latent variable has the same number of channels, but they differ in spatialdimensions: zl Rchlwl. We refer to different spatial dimensions of the latent space as scales.",
  "Bottom-up": "The bottom-up part corresponds to calculations of intermediary variables dependent on x. We follow theimplementation of Child (2021) for it. We start from the bottom-up path depicted in a (left), whichis fully deterministic and consists of several ResNet blocks (see c). The input is processed by Nencblocks at each scale, and the output of the last resnet block of each scale is passed to the TopDown path ina (right). Note that here Nenc is a separate hyperparameter that does not depend on the number ofstochastic layers L.",
  "The TopDown path depicted in a (right) computes the parameters of the variational posterior andthe prior distribution starting from the top latent variable zL": "The first step is the pseudoinput block shown in d. Using the deterministic function fdct, it createsthe pseudoinput random variable from the input x (see Algorithm 1) that is used to train the Diffusion-basedVampPrior r(u). At test time, a pseudoinput is sampled using this unconditional prior. The pseudoinputsample is then converted back to the input domain (see Algorithm 2) and used to condition prior distributionsat all levels p(z1:L|u).",
  "(d)Pseudoinputblock": ": A diagram of the DVP-VAE: TopDown hierarchical VAE with the diffusion-based VampPrior.(a) A BottomUp path (left) and a TopDown path (right). (b) A TopDown block that takes features from theblock above hdec, encoder features henc (only during training) and a pseudoinput u as inputs. (c) A singleResnet block. (d) A single pseudoinput block. Next, the model has L TopDown blocks depicted in b. Each TopDown block takes deterministicfeatures from the corresponding scale of the bottom-up path denoted as henc, the output of the pseudoinputblock u, and deterministic features from the TopDown block above hdec as inputs. Our implementation ofthis block is similar to the VDVAE architecture, but there are several differences that we summarize below:",
  "Latent Aggregation in Conditional Likelihood": "The last important element of our architecture is the aggregation of latents. Let us denote samples fromeither the variational posteriors q(z1:L|x) during training or the prior p(z1:L|u) during generating newdata as z1, . . . , zL. Furthermore, let h1 be the output of the last TopDown block. These deterministicfeatures are computed as a function of all samples z1, . . . , zL. Therefore, it can be used to calculate thefinal likelihood value. However, we observe empirically that in such parameterization some layers of latentvariables tend to be completely ignored by the model. Instead, we propose to enforce a strong connectionbetween the conditional likelihood and all latent variables by explicitly conditioning on all of the sampledlatent variables, namely:",
  "Related Work": "Latent prior in VAEsThe original VAE formulation uses the standard Gaussian distribution as a priorover the latent variables. This can be an overly simplistic choice, as the prior minimizing the EvidenceLower bound is given by the aggregated posterior (Hoffman & Johnson, 2016; Tomczak & Welling, 2018).Furthermore, using a unimodal prior with multimodal real-world data can lead to non-smooth encoder ormeaningless distances in the latent space Bozkurt et al. (2019). More flexible prior distributions proposed inthe literature include the Gaussian Mixture Model (Jiang et al., 2016; Nalisnick et al., 2016; Tran et al., 2022),the autoregressive normalizing flow (Chen et al., 2017), the autoregressive model (Gulrajani et al., 2016;Sadeghi et al., 2019), the rejection sampling distribution with the learned acceptance function (Bauer & Mnih,2019), the diffusion-based prior (Vahdat et al., 2021; Wehenkel & Louppe, 2021). The VampPrior (Tomczak& Welling, 2018) proposes using an approximation of the aggregated posterior as a prior distribution. Theapproximation is constructed using learnable pseudoinputs to the encoder. This work can be seen as anefficient extension of the VampPrior to deep hierarchical VAE, which also utilizes a diffusion-based priorover the pseudoinputs. Auxiliary Variables in VAEsSeveral works consider auxiliary variables u as a way to improve the flexi-bility of the variational posterior. Maale et al. (2016) use auxiliary variables with one-level VAE to improvethe variational approximation while keeping the generative model unchanged. Salimans et al. (2015) useMarkov transition kernel for the same expressivity purpose. The authors treat intermediate MCMC samplesas an auxiliary random variable and derive evidence lower bound of the extended model. Ranganath et al.(2016) introduce hierarchical variational models. They increase the flexibility of the variational approxi-mation by imposing prior on its parameters. In this setting, it assumes that the latent variable z and theauxiliary variable u are not conditionally independent and the variational posterior factorizes, for example,as follows:q(u, z|x) = q(u|x)q(z|u, x).(19)",
  "q(u, z|x) = q(u|x)q(z|x).(20)": "Khemakhem et al. (2020) consider the non-identifiability problem of VAEs. They propose to use auxiliaryobservation u and use it to condition the prior distribution. This additional observation is similar to thepseudoinputs that we consider in our work. However, we define a way to construct u from the input andlearn a prior distribution to sample it during inference, while Khemakhem et al. (2020) require u to beobserved both during training and at the inference time. Similarly to our work, (Klushyn et al., 2019) consider hierarchical prior p(z|u)p(u). However, they treatu rather as a second layer of latent variables and learn a variational posterior in the form q(u, z|x) =q(u|z)q(z|x). Latent Variables AggregationThere are different ways in which the conditional likelihood p(x|z1:L)can be parameterized. In LadderVAE (Snderby et al., 2016), where TopDown hierarchical VAE was origi-nally proposed, the following formulation is used:",
  "DVP-VAE (ours)877.10(0.05)89.07(0.10)20M282.73Attentive VAE (Apostolopoulou et al., 2022)1577.6389.50119M162.79CR-NVAE (Sinha & Dieng, 2021)1576.93131M302.51": "VDVAE (Child, 2021)39M452.87OU-VAE (Pervez & Gavves, 2021)581.1096.0810M33.39NVAE (Vahdat & Kautz, 2020)1578.01302.91BIVA(Maale et al., 2019)678.4191.34103M153.08VampPrior (Tomczak & Welling, 2018)278.4589.76LVAE (Snderby et al., 2016)581.74102.11IAF-VAE(Kingma et al., 2016)79.10123.11 Note that deterministic features depend on all the latent variables. However, we propose to use a moreexplicit dependency on latent variables in Eq. 18. Our idea bears some similarities with Skip-VAE (Dienget al., 2019). Skip-VAE proposes to add a latent variable to each layer of the neural network parameterizingdecoder of the VAE with a single stochastic layer. In this work, instead, we add all the latent variablestogether to parameterize conditional likelihood.",
  "Main Quantitative and Qualitative Results": "We report all results in , where we compare the proposed approach with other hierarchical VAEs.We observe that DVP-VAE outperforms most of the VAE models3 while using fewer parameters than othermodels. For instance, on CIFAR10, our DVP-VAE requires 20M weights to beat Attentive VAE with about6 times more weights. Furthermore, because of the smaller model size, we were able to obtain all the resultsusing a single GPU. We show the unconditional samples in (see Appendix D for more samples).The top row of each image shows samples from the diffusion-based VampPrior (i.e., pseudoinputs), while thesecond row shows corresponding samples from the VAE. We observe that, as expected, pseudoinputs definethe general appearance of an image, while a lot of details are added later by the TopDown decoder. Thiseffect can be further observed in where we plot the reconstructions using different numbers of latentvariables. In the first row, only a pseudoinput corresponding to the original image is used (i.e., u r(u|x))while the remaining latent variables are sampled from the prior with low temperature. Each row below usesmore latent variables from the variational posterior grouped by the scales. Namely, the second row uses thepseudoinput above and all the 4 4 latent variables from the variational posterior, then the third row usesadditionally 8 8 latent variables, and so on.",
  "Test BPD2.872.872.86": "normalization (Vahdat & Kautz, 2020), or softmax parameterization of variances (Hazami et al., 2022). Weuse the Adamax version of the Adam optimizer (Kingma & Ba, 2015) following Hazami et al. (2022), as itdemonstrated much better convergence for the model with a mixture of discretized logistic likelihood. First, we observe a consistent performance improvement as we increase the model size and the number ofstochastic layers. In , we report test performance and the percentage of active units (see .3for details) for models of different stochastic depths trained on the CIFAR10 dataset. We train each modelfor 500 epochs, which corresponds to less than 200k training iterations. Additionally, we report gradientnorms and training and validation losses for all four models in Appendix B. To demonstrate the advantage of the proposed architecture, we compare our model to a closest deep hier-archical VAE architecture: Very Deep VAE(Child, 2021). For this experiment, we chose hyperparametersclosest to in Child (2021) (CIFAR-10). That is, our model has 45 stochastic layers and a comparablenumber of trainable parameters. Furthermore, following Child (2021), we train this model with a batch sizeof 32, a gradient clipping threshold of 200, and an EMA rate of 0.9998. However, in DVP-VAE, we were ableto eliminate gradient skipping and gradient smoothing. We report the difference in key hyperparameters andtest performance in . We also add a comparison with Efficient-VDVAE (see in Hazami et al.(2022)). We observe that DVP-VAE achieves comparable performance within much fewer training iterationsthan both VDVAE implementations. Latent Aggregation Increases Latent Space UtilizationNext, we test the claim that latent variableaggregation discussed in Sec. 4.3 improves latent space utilization. We use Active Units (AU) metric (Burdaet al., 2015), which can be calculated for a given threshold as follows:",
  ": Samples from the pseudoinputs prior u r(u)(top row) and corresponding samples from the model x p(x|z1:L, u = u) (other rows).Columns corresponds tomodels trained with different random seeds": "Here Ml is the dimensionality of the stochastic layer l, [B] is the Iverson bracket, which equals 1 if B is trueand 0 otherwise, and Var stands for variance. Following Burda et al. (2015), we use the threshold = 0.01.The higher the share of active units, the more efficient the model is in utilizing its latent space. We report results in and observe that the model with latent aggregation always attains more than90% of active units. Furthermore, latent aggregation considerably improves the utilization of the latentspace if we compare it with exactly the same model but with the conditional likelihood parameterized usingdeterministic feature from the TopDown path (see Eq. 22). Amortized VampPrior Improves BPDFurther, we test how the proposed amortized VampPrior im-proves model performance as measured by the negative log-likelihood. We report results in andobserve that DVP-VAE always has a better NLL metric compared to the deep hierarchical VAE with thesame architecture and number of stochastic layers. Due to additional diffusion-based prior over pseudoinputs,DVP-VAE has slightly more trainable parameters. However, because of the small spatial dimensionality ofthe pseudoinputs, we were able to keep the size of the two models comparable. Pseudoinputs type, size and priorWe conduct an extensive ablation study regarding pseudoinputs.First, we train VAE with two types of pseudoinputs: DCT and Downsampled images. Moreover, we vary thespatial dimensions of the pseudoinputs between 3 3 and 11 11. We expect that a smaller pseudoinputssize will be an easier task for the prior r(u), but will constitute a poorer approximation of an optimal prior.The larger pseudoinput size, on the other hand, results in a better optimal prior approximation since moreinformation about the datapoint x is preserved. However, it becomes harder for the prior to achieve goodresults since we keep the prior model size fixed. In we observe that the DCT-based transformationperforms consistently better across various sizes and datasets.",
  "Published in Transactions on Machine Learning Research (11/2024)": "Trainable PseudoinputsIn the VampPrior, the optimal prior is approximated using learnable pseudoin-puts. In this work, on the other hand, we propose to use fixed linear transformation instead. To furtherverify whether a fixed transformation like DCT is reasonable, we checked a learnable linear transformation.We present in that the learnable linear transformation of the input exhibits unstable behavior interms of the quality of learned pseudoinput. The top row of (a) shows samples from the trainedprior and the corresponding samples from the decoder. We observe that only one out of four models withlearnable pseudoinputs was able to learn a visually meaningful representation of the data (seed 0), whichalso resulted in very high variance of the results (rows below). For other models (e.g., Seed 1 and Seed 2),the same pseudoinput sample corresponds to completely different datapoints. This lack of consistency motivates us to use a non-trainable transformation for obtaining pseudoinputs.In (b), we show the expected behavior of sampling semantically meaningful pseudoinputs that isconsistent across random seeds.",
  "Vamp500430s29Gb20.5M1.5750500s38Gb21.3M1.81000OOM>40Gb22.1M": "ScalabilityHere, we study how DVP-VAE scalesas we increase model size and input size in compari-son with VampPrior. For this, we implement Vamp-Prior as proposed by Tomczak & Welling (2018),where the top latent variable is trained with theVampPrior and the other layers with the conditionalGaussian (see Eq. 7).We use the same architec-ture as in main experiments for MNIST (32 channels)and CIFAR10 (see ). Additionally, we trainmodel with doubled number of channels on MNIST(64 channels). In , we report the training time (second perepoch), GPU memory utilization and total numberof trainable parameters.We observe that Vamp-Prior almost always utilizes significantly more mem-ory and requires longer training time.The differ-ence is less visible on a small model and small inputsize (MNIST, 32 channels). However, as we doublenumber of channels, both training time and memoryutilization for VampPrior grows much faster. As aresult, for a bigger input size (CIFAR10 dataset) amodel with more than 750 pseudoinputs does not fit into a single A100 GPU. For 500 pseudoinputs, memoryutilization is already 2.5 times higher for VampPrior compared to DVP-VAE.",
  "Limitations": "One limitation of DVP-VAE is the long sampling time reported in (sec. per 1000 images). It isa direct consequence of using a diffusion-based prior. While diffusion is a powerful generative model thatallows us to achieve outstanding performance, it is known to be slow at sampling. In our experiments, we use50 diffusion steps to generate a pseudoinput, however, there are efficient distillation techniques (Salimans &Ho, 2022; Geng et al., 2024) that can be applied to mitigate this issue and reduce the number of diffusionsteps to just a single forward pass through the model. We leave this optimization for future work. Furthermore, within DVP-VAE, we add an extra learnable block to the model, namely, the diffusion-basedprior over pseudoinputs, as a result, additional modelling choices should be made. We provide an ablationstudy to show the effect of these choices on the performance of the model.",
  "Conclusion": "In this work, we introduce DVP-VAE, a new class of deep hierarchical VAEs with the diffusion-based Vamp-Prior. We propose to use a VampPrior approximation which allows us to use it with hierarchical VAEs withlittle computational overhead. We show that the proposed approach demonstrate competitive performance interms of the negative log-likelihood on three benchmark datasets with much fewer parameters and stochasticlayers compared to the best performing contemporary hierarchical VAEs.",
  "Zhengyang Geng, Ashwini Pokle, and J Zico Kolter. One-step diffusion distillation via deep equilibriummodels. Advances in Neural Information Processing Systems, 36, 2024": "Rafael Gmez-Bombarelli, Jennifer N Wei, David Duvenaud, Jos Miguel Hernndez-Lobato, BenjamnSnchez-Lengeling, Dennis Sheberla, Jorge Aguilera-Iparraguirre, Timothy D Hirzel, Ryan P Adams,and Aln Aspuru-Guzik. Automatic chemical design using a data-driven continuous representation ofmolecules. ACS central science, 2018. Ishaan Gulrajani, Kundan Kumar, Faruk Ahmed, Adrien Ali Taiga, Francesco Visin, David Vazquez, andAaron Courville. Pixelvae: A latent variable model for natural images. arXiv preprint arXiv:1611.05013,2016.",
  "ADiffusion probabilistic models": "Diffusion Probabilistic Models or Diffusion-based Deep Generative Models (Ho et al., 2020; Sohl-Dicksteinet al., 2015) constitute a class of generative models that can be viewed as a special case of the HierarchicalVAEs (Huang et al., 2021; Kingma et al., 2021; Tomczak, 2022; Tzen & Raginsky, 2019). Here, we followthe definition of the variational diffusion model (Kingma et al., 2021). We use the diffusion model as a priorover the pseudoinputs u.",
  "BTraining Stability: depth": "We report the 2-norm of the gradient for each iteration training for models of different stochastic depth in. We observe very high gradient norms for the first few training iterations but no spikes at laterstages of training. This happens because we did not initiate the KL-term to be zero at the beginning of thetraining and it tends to be large at the initialization step for very deep VAEs. However, we did not observeour model diverge since the gradient is clipped to reasonable values (200 in these experiments) and after thefirst few gradient updates, the KL-term goes to reasonable values. Moreover, we plot training and validationlosses in ."
}