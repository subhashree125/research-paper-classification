{
  "Abstract": "Offline black-box optimization aims to maximize a black-box function using an offline datasetof designs and their measured properties. Two main approaches have emerged: the forwardapproach, which learns a mapping from input to its value, thereby acting as a proxy toguide optimization, and the inverse approach, which learns a mapping from value to inputfor conditional generation. (a) Although proxy-free (classifier-free) diffusion shows promisein robustly modeling the inverse mapping, it lacks explicit guidance from proxies, essentialfor generating high-performance samples beyond the training distribution. Therefore, wepropose proxy-enhanced sampling which utilizes the explicit guidance from a trained proxyto bolster proxy-free diffusion with enhanced sampling control. (b) Yet, the trained proxy issusceptible to out-of-distribution issues. To address this, we devise the module diffusion-basedproxy refinement, which seamlessly integrates insights from proxy-free diffusion back into theproxy for refinement. To sum up, we propose Robust Guided Diffusion for Offline Black-boxOptimization (RGD), combining the advantages of proxy (explicit guidance) and proxy-freediffusion (robustness) for effective conditional generation. RGD achieves state-of-the-artresults on various design-bench tasks, underscoring its efficacy. Our code is here.",
  "Introduction": "Creating new objects to optimize specific properties is a ubiquitous challenge that spans a multitude offields, including material science, robotic design, and genetic engineering. Traditional methods generallyrequire interaction with a black-box function to generate new designs, a process that could be financiallyburdensome and potentially perilous (Hamidieh, 2018; Sarkisyan et al., 2016). Addressing this, recent researchendeavors have pivoted toward a more relevant and practical context, termed offline black-box optimization(BBO) (Trabucco et al., 2022; Krishnamoorthy et al., 2023). In this context, the goal is to maximize ablack-box function exclusively utilizing an offline dataset of designs and their measured properties. There are two main approaches for this task: the forward approach and the reverse approach. The forwardapproach entails training a deep neural network (DNN), parameterized as J(), using the offline dataset. Oncetrained, the DNN acts as a proxy and provides explicit gradient guidance to enhance existing designs. However,this technique is susceptible to the out-of-distribution (OOD) issue, leading to potential overestimation ofunseen designs and resulting in adversarial solutions (Trabucco et al., 2021). The reverse approach aims to learn a mapping from property value to input. Inputting a high value intothis mapping directly yields a high-performance design. For example, MINs (Kumar & Levine, 2020) adoptsGAN (Goodfellow et al., 2014) to model this inverse mapping, and demonstrate some success. Recentworks (Krishnamoorthy et al., 2023) have applied proxy-free diffusion1 (Ho & Salimans, 2022), parameterizedby , to model this mapping, which proves its efficacy over other generative models. Proxy-free diffusion",
  "Published in Transactions on Machine Learning Research (12/2024)": "0.5, 1.0, 1.5, 2.0, and 2.5, considering 1.0 as the default. Similarly, for the learning rate , we explore valuesof 2.5e3, 5.0e3, 0.01, 0.02, and 0.04, with the default set to = 0.01. Results are normalized by comparingthem with the performance obtained at default values. As depicted in Figures 5, 6, and 7, RGD demonstrates considerable resilience to hyperparameter variations.The Ant task, in particular, exhibits a more marked sensitivity, with a gradual enhancement in performanceas these hyperparameters are varied. The underlying reasons for this trend include: (1) An increase in thenumber of diffusion steps (T) enhances the overall quality of the generated samples. This improvement, inconjunction with more effective guidance from the trained proxy, leads to better results. (2) Elevating thecondition (y) enables the diffusion model to extend its reach beyond the existing dataset, paving the way forsuperior design solutions. However, selecting an optimal y can be challenging and may, as observed in theTFB10 task, sometimes lead to suboptimal results. (3) A higher learning rate () integrates an enhancedguidance signal from the trained proxy, contributing to improved performances. In contrast, the discrete nature of the TFB10 task seems to endow it with a certain robustness to variations inthese hyperparameters, highlighting a distinct behavioral pattern in response to hyperparameter adjustments. Diffusion step t 0.990 0.995 1.000 1.005 1.011 Score ratio AntTF10",
  ": Motivation of explicit proxy guidance": "Nevertheless, the inverse approach, proxy-free diffu-sion, initially designed for in-distribution generation,such as synthesizing specific image categories, faceslimitations in offline BBO. Particularly, it strugglesto generate high-performance samples that exceedthe training distribution due to the lack of explicitguidance2. Consider, for example, the optimizationof a two-dimensional variable (xd1, xd2) to maxi-mize the negative Rosenbrock function (Rosenbrock,1960): y(xd1, xd2) = (1 xd1)2 100(xd2 x2d1)2,as depicted in . The objective is to steer theinitial points (indicated in pink) towards the high-performance region (highlighted in yellow). Whileproxy-free diffusion can nudge the initial points closerto this high-performance region, the generated points(depicted in blue) fail to reach the high-performanceregion due to its lack of explicit proxy guidance. To address this challenge, we introduce a proxy-enhanced sampling module as illustrated in (a). Itincorporates the explicit guidance from the proxy J(x) into proxy-free diffusion to enable enhanced controlover the sampling process. This module hinges on the strategic optimization of the strength parameter toachieve a better balance between condition and diversity, per reverse diffusion step. This incorporation notonly preserves the inherent robustness of proxy-free diffusion but also leverages the explicit proxy guidance,thereby enhancing the overall conditional generation efficacy. As illustrated in , samples (depictedin red) generated via proxy-enhanced sampling are more effectively guided towards, and often reach, thehigh-performance area (in yellow). Forward diffusion Reverse diffusionFinal design Diffusion-based proxy refinement Proxy-enhanced sampling (optimizing ) (a) (b) Diffusion distributionProxy Probability flow ODE ... : Overview of RGD: Module (a) incor-porates proxy guidance into proxy-free diffusionto enable enhanced sampling control; Module (b)integrates insights from proxy-free diffusion backinto the proxy for refinement. Yet, the trained proxy is susceptible to out-of-distribution(OOD) issues.To address this, we devise a modulediffusion-based proxy refinement as detailed in (b).This module seamlessly integrates insights from proxy-freediffusion into the proxy J(x) for refinement.Specif-ically, we generate a diffusion distribution p(y|x) onadversarial samples x, using the associated probabilityflow ODE 3. This distribution is derived independently ofa proxy, thereby exhibiting greater robustness than theproxy distribution on adversarial samples. Subsequently,we calculate the Kullback-Leibler divergence between thetwo distributions on adversarial samples, and use thisdivergence minimization as a regularization strategy tofortify the proxys robustness and reliability. To sum up, we propose Robust Guided Diffusion for Of-fline Black-box Optimization (RGD), a novel frameworkthat combines the advantages of proxy (explicit guidance)and proxy-free diffusion (robustness) for effective condi-tional generation. Our contributions are three-fold:",
  "x = arg maxxX J(x).(1)": "In this equation, J() is the unknown objective function, and x X is a possible design. In this context, thereis an offline dataset, D, that consists of pairs of designs and their measured properties. Specifically, each xdenotes a particular design, like the size of a robot, while y indicates its related metric, such as its speed.",
  "Diffusion Models": "Diffusion models, a type of latent variable models, progressively introduce Gaussian noise to data in theforward process, while the reverse process aims to iteratively remove this noise through a learned scoreestimator Ho et al. (2020). In this work, we utilize continuous time diffusion models governed by a stochasticdifferential equation (SDE), as presented in Song et al. (2021). The forward SDE is formulated as:",
  "dx =f(x, t) g(t)2x log p(x)dt + g(t)d w,(5)": "with x log p(x) representing the score of the marginal distribution at time t, and w symbolizing the reverseWiener process. The score function x log p(x) is estimated using a time-dependent neural network s(xt, t),enabling us to transform noise into samples. For simplicity, we will use s(xt), implicitly including t.",
  "Guided Diffusion": "Unconditional diffusion models capture the natural data distribution, while guided diffusion seeks to producesamples with specific desirable attributes, falling into two categories: proxy diffusion (Dhariwal & Nichol,2021) and proxy-free diffusion (Ho & Salimans, 2022). While these were initially termed classifier diffusionand classifier-free diffusion in classification tasks, we have renamed them to proxy diffusion and proxy-freediffusion, respectively, to generalize to our regression context. Proxy diffusion combines the models scoreestimate with the gradient from the proxy distribution, providing explicit guidance in line with the forwardapproach. However, it can be interpreted as a gradient-based adversarial attack. In proxy-free diffusion, guidance is not dependent on proxy gradients, which enables an inherent robustnessof the sampling process. Particularly, it models the score as a linear combination of an unconditional and aconditional score. A unified neural network s(xt, y) parameterizes both score types. The score s(xt, y)approximates the gradient of the log probability xt log p(xt|y), i.e., the conditional score, while s(xt)estimates the gradient of the log probability xt log p(xt), i.e., the unconditional score. The score functionfollows:",
  "s(xt, y, ) = (1 + )s(xt, y) s(xt).(6)": "Within this context, the strength parameter specifies the generations adherence to the condition y, which isset to the maximum value ymax in the offline dataset following Krishnamoorthy et al. (2023). Optimization of balances the condition and diversity. Lower values increase sample diversity at the expense of conformityto y, and higher values do the opposite.",
  "Related Work": "Offline black-box optimization. A recent surge in research has presented two predominant approachesfor offline BBO. The forward approach deploys a DNN to fit the offline dataset, subsequently utilizinggradient ascent to enhance existing designs. Typically, these techniques, including COMs (Trabucco et al.,2021), ROMA (Yu et al., 2021), NEMO (Fu & Levine, 2021), BDI (Chen et al., 2022b; 2023b), IOM (Qiet al., 2022a) and Parallel-mentoring (Chen et al., 2023a), are designed to embed prior knowledge withinthe surrogate model to alleviate the OOD issue. The reverse approach (Kumar & Levine, 2020; Chan et al.,2021) is dedicated to learning a mapping from property values back to inputs. Feeding a high value into thisinverse mapping directly produces a design of elevated performance. Additionally, methods in Brookes et al.(2019); Fannjiang & Listgarten (2020) progressively tailor a generative model towards the optimized designvia a proxy function and BONET (Mashkaria et al., 2023) introduces an autoregressive model trained onfixed-length trajectories to sample high-scoring designs. Recent investigations (Krishnamoorthy et al., 2023)have underscored the superiority of diffusion models in delineating the inverse mapping. However, researchon specialized guided diffusion for offline BBO remains limited. This paper addresses this research gap. Guided diffusion. Guided diffusion seeks to produce samples with specific desirable attributes. Contempo-rary research in guided diffusion primarily concentrates on enhancing the efficiency of its sampling process.Meng et al. (2023) propose a method for distilling a classifier-free guided diffusion model into a more efficientsingle model that necessitates fewer steps in sampling. Sadat et al. (2023) introduce the Dynamic CFG,which initially compels the model to depend more on the unconditional score and progressively shifts towardsthe standard CFG. However, unlike our method, it does not optimize the strength parameter. Kynknniemiet al. (2024) restrict the guidance to a specific range of noise levels, which improves both sampling speed andquality. Wizadwongsa & Suwajanakorn (2023) introduce an operator splitting method to expedite classifierguidance by separating the update process into two key functions: the diffusion function and the conditioningfunction. Additionally, Bansal et al. (2023) presents an efficient and universal guidance mechanism thatutilizes a readily available proxy to enable diffusion guidance across time steps. In this work, we explorethe application of guided diffusion in offline BBO, with the goal of creating tailored algorithms to efficientlygenerate high-performance designs.",
  "Method": "In this section, we present our method RGD, melding the strengths of proxy and proxy-free diffusion foreffective conditional generation. Firstly, we describe a newly developed module termed proxy-enhancedsampling. It integrates explicit proxy guidance into proxy-free diffusion to enable enhanced sampling control,as detailed in .1. Subsequently, we explore diffusion-based proxy refinement which incorporatesinsights gleaned from proxy-free diffusion back into the proxy, further elaborated in .2. The overallalgorithm is shown in Algorithm 1.",
  ": end for16: Return x = x0": "As discussed in .3, proxy-free diffu-sion trains an unconditional model and condi-tional models. Although proxy-free diffusioncan generate samples aligned with most con-ditions, it traditionally lacks control due tothe absence of an explicit proxy. This is par-ticularly significant in offline BBO where weaim to obtain samples beyond the training dis-tribution, as the offline dataset is inherentlylimited. Therefore, we require explicit proxyguidance to achieve enhanced sampling con-trol. This module is outlined in Algorithm 1,Line 8- Line 16. Optimization of . Directly updating thedesign xt with proxy gradient suffers from theOOD issue and determining a proper condi-tion y necessitates the manual adjustment ofmultiple hyperparameters (Kumar & Levine,2020). Thus, we propose to introduce proxyguidance by only optimizing the strength pa-rameter within s(xt, y, ) in Eq. (6). Asdiscussed in .3, the parameter bal-ances the condition and diversity, and an optimized could achieve a better balance in the sampling process,leading to more effective generation.",
  "xt() = solver(xt+1, s(xt+1, y, )),(7)": "where the solver is the second-order Heun solver (Sli & Mayers, 2003), chosen for its enhanced accuracythrough a predictor-corrector method. A proxy is then trained to predict the property of noise xt at time stept, denoted as J(xt, t). By maximizing J(xt(), t) with respect to , we can incorporate the explicit proxyguidance into proxy-free diffusion to facilitate improved sampling control in the balance between conditionand diversity. This maximization process is:",
  "(t),(10)": "where s(xt) is the estimated unconditional score at time step t, and (t)2 and (t) are the variance and themean coefficient of the perturbation kernel at time t, as detailed in equations (32-33) in Song et al. (2021).For a more detailed derivation, refer to the Appendix B. Consequently, we express",
  "Diffusion-based Proxy Refinement": "In the proxy-enhanced sampling module, the proxy J() is employed to update the parameter to enableenhanced control. However, J() may still be prone to the OOD issue, especially on adversarial samples (Tra-bucco et al., 2021). To address this, we refine the proxy by using insights from proxy-free diffusion. Theprocedure of this module is specified in Algorithm 1, Lines 3-7. Diffusion Distribution. Adversarial samples are identified by gradient ascent on the proxy as per Eq. (3) toform the distribution q(x). We utilize a vanilla proxy to perform 300 gradient ascent steps, identifying sampleswith unusually high prediction scores as adversarial. This method is based on the limited extrapolationcapability of the vanilla proxy, as demonstrated in in COMs (Trabucco et al., 2021). Consequently,these samples are vulnerable to the proxy distribution. Conversely, the proxy-free diffusion, which functionswithout depending on a proxy, inherently offers greater resilience against these samples, thus producing a morerobust distribution. For an adversarial sample x q(x), we compute p(x) and p(x|y) using the probabilityflow ODE associated with the SDE, as detailed in Appendix D of Song et al. (2021). The implementationcan be accessed here. We estimate p(y) using Gaussian kernel-density estimation. The diffusion distributionregarding y is:",
  "p(x),(12)": "which demonstrates inherent robustness over the proxy distribution p(y|x). Yet, directly applying diffusiondistribution to design optimization by gradient ascent is computationally intensive and potentially unstabledue to the demands of reversing ODEs and scoring steps. Proxy Refinement.We opt for a more feasible approach: refine the proxy distribution p(y|x) =N(J(x), (x)) by minimizing its distance to the diffusion distribution p(y|x). The distance is quantifiedby the Kullback-Leibler (KL) divergence:",
  "Tasks. Our experiments encompass a variety of tasks, split into continuous and discrete categories": "The continuous category includes four tasks: (1) Superconductor (SuperC): The objective here is to engineera superconductor composed of 86 continuous elements. The goal is to enhance the critical temperature using17, 010 design samples. This task is based on the dataset from Hamidieh (2018). (2) Ant Morphology (Ant):In this task, the focus is on developing a quadrupedal ant robot, comprising 60 continuous parts, to augmentits crawling velocity. It uses 10, 004 design instances from the dataset in Trabucco et al. (2022); Brockmanet al. (2016). (3) DKitty Morphology (DKitty): Similar to Ant Morphology, this task involves the design ofa quadrupedal DKitty robot with 56 components, aiming to improve its crawling speed with 10, 004 designs,as described in Trabucco et al. (2022); Ahn et al. (2020). (4) Rosenbrock (Rosen): The aim of this task is tooptimize a 60-dimension continuous vector to maximize the Rosenbrock black-box function. It uses 50000designs from the low-scoring part (Rosenbrock, 1960). For the discrete category, we explore three tasks: (1) TF Bind 8 (TF8): The goal is to identify an 8-unitDNA sequence that maximizes binding activity. This task uses 32, 898 designs and is detailed in Barreraet al. (2016). (2) TF Bind 10 (TF10): Similar to TF8, but with a 10-unit DNA sequence and a larger poolof 50, 000 samples, as described in (Barrera et al., 2016). (3) Neural Architecture Search (NAS): This taskfocuses on discovering the optimal neural network architecture to improve test accuracy on the CIFAR-10dataset, using 1, 771 designs (Zoph & Le, 2017). Evaluation. In this study, we utilize the oracle evaluation from design-bench (Trabucco et al., 2022).Adhering to this established protocol, we analyze the top 128 promising designs from each method. Theevaluation metric employed is the 100th percentile normalized ground-truth score, calculated using the formulayn =yymin ymaxymin , where ymin and ymax signify the lowest and highest scores respectively in the comprehensive,yet unobserved, dataset. In addition to these scores, we provide an overview of each methods effectivenessthrough the mean and median rankings across all evaluated tasks. Notably, the best design discovered in theoffline dataset, designated as D(best), is also included for reference. For further details on the 50th percentile(median) scores, please refer to Appendix C.",
  "Method Comparison": "Our approach is evaluated against two primary groups of baseline methods: forward and inverse approaches.Forward approaches enhance existing designs through gradient ascent. This includes: (i) Grad: utilizes simplegradient ascent on current designs for new creations; (ii) ROMA (Yu et al., 2021): implements smoothnessregularization on proxies; (iii) COMs (Trabucco et al., 2021): applies regularization to assign lower scores toadversarial designs; (iv) NEMO (Fu & Levine, 2021): bridges the gap between proxy and actual functionsusing normalized maximum likelihood; (v) BDI (Chen et al., 2022b): utilizes both forward and inversemappings to transfer knowledge from offline datasets to the designs; (vi) IOM (Qi et al., 2022b): ensuresconsistency between representations of training datasets and optimized designs. Inverse approaches focus on learning a mapping from a designs property value back to its input. High propertyvalues are input into this inverse mapping to yield enhanced designs. This includes: (i) CbAS (Brookeset al., 2019): CbAS employs a VAE model to implicitly implement the inverse mapping. It gradually tunesits distribution toward higher scores by raising the scoring threshold. This process can be interpreted asincrementally increasing the conditional score within the inverse mapping framework. (ii) Autofocused CbAS",
  "Experimental Configuration": "In alignment with the experimental protocols established in Trabucco et al. (2022); Chen et al. (2022b), wehave tailored our training methodologies for all approaches, utilizing a three-layer MLP architecture for allinvolved proxies.For methods such as BO-qEI, CMA-ES, REINFORCE, CbAS, and Auto.CbAS that donot utilize gradient ascent, we base our approach on the findings reported in Trabucco et al. (2022). Weadopted T = 1000 diffusion sampling steps, set the condition y to ymax, and initial strength as 2 in linewith Krishnamoorthy et al. (2023). To ensure reliability and consistency in our comparative analysis, eachexperimental setting was replicated across 8 independent runs, unless stated otherwise, with the presentationof both mean values and standard deviations. These experiments were conducted using a NVIDIA GeForceV100 GPU. We have detailed the computational overhead of our approach in Appendix E to provide acomprehensive view of its practicality.",
  "Results and Analysis": "In Tables 1 and 2, we showcase our experimental results for both continuous and discrete tasks. To clearlydifferentiate among the various approaches, distinct lines separate traditional, forward, and inverse approacheswithin the tables For every task, algorithms performing within a standard deviation of the highest score areemphasized by bolding following Trabucco et al. (2021). We make the following observations. (1) As highlighted in , RGD not only achieves the top rank butalso demonstrates the best performance in six out of seven tasks, emphasizing the robustness and superiorityof our method. (2) RGD outperforms the VAE-based CbAS, the GAN-based MIN and the Transformer-based",
  "RGD0.974 0.0030.694 0.0180.825 0.0632.0/152/15": "BONET. This result highlights the superiority of diffusion models in modeling inverse mappings comparedto other generative approaches. (3) Upon examining TF Bind 8, we observe that the average rankings forforward and inverse methods stand at 10.3 and 6.0, respectively. In contrast, for TF Bind 10, both methodshave the same average ranking of 8.7, indicating no advantage. This notable advantage of inverse methods inTF Bind 8 implies that the relatively smaller design space of TF Bind 8 (48) facilitates easier inverse mapping,as opposed to the more complex space in TF Bind 10 (410). (4) RGDs performance is less impressive on NAS,where designs are encoded as 64-length sequences of 5-category one-hot vectors. This may stem from thedesign-benchs encoding not fully capturing the sequential and hierarchical aspects of network architectures,affecting the efficacy of inverse mapping modeling.",
  "Ablation Studies": "In this section, we present a series of ablation studies to scrutinize the individual contributions of distinctcomponents in our methodology. We employ our proposed approach as a benchmark and methodically excludekey modules, such as the proxy-enhanced sampling and diffusion-based proxy refinement, to assess theirinfluence on performance. These variants are denoted as w/o proxy-e and w/o diffusion-b r. Additionally, weexplore the strategy of directly performing gradient ascent on the diffusion intermediate state with a learningrate of 0.1, referred to as direct grad update. The results from these ablations are detailed in . Our analysis reveals that omitting either module results in a decrease in performance, thereby affirming theimportance of each component. The w/o diffusion-b r variant generally surpasses w/o proxy-e, highlightingthe utility of the proxy-enhanced sampling even with a basic proxy setup. Conversely, direct grad updatetends to produce subpar results across tasks, likely attributable to the proxys limitations in handlingout-of-distribution samples, leading to suboptimal design optimizations. To further dive into the proxy-enhanced sampling module, we visualize the strength ratio /0where 0represents the initial strengthacross diffusion steps t. This analysis is depicted in for two tasks:Ant and TF10. We observe a pattern of initial decrease followed by an increase in across both tasks.This pattern can be interpreted as follows: The decrease in facilitates the generation of a more diverseset of samples, enhancing exploratory capabilities. Subsequently, the increase in signifies a shift towardsintegrating high-performance features into the sample generation. Within this context, conditioning on themaximum y is not aimed at achieving the datasets maximum but at enriching samples with high-scoringattributes. Overall, this adjustment of effectively balances between generating novel solutions and honingin on high-quality ones.",
  ": The proxy distribution overestimates theground truth, while the diffusion distribution closelyaligns with it, demonstrating its robustness": "In addition, we visualize the proxy distribution alongside the diffusion distribution for a sample x fromthe Ant task in , to substantiate the efficacy of diffusion-based proxy refinement.The proxydistribution significantly overestimates the ground truth, whereas the diffusion distribution closely alignswith it, demonstrating the robustness of diffusion distribution. For a more quantitative analysis, we computethe expectation of both distributions and compare them with the ground truth. The mean of the diffusiondistribution is calculated as Ep(y|x)[y] = Ep(y|x)p(y|x)p(y|x)y. The MSE loss for the proxy distribution is 2.88,while for the diffusion distribution, it is 0.13 on the Ant task. Additionally, we evaluate this on the TFB10task, where the MSE loss for the proxy distribution is 323.63 compared to 0.82 for the diffusion distribution.These results further corroborate the effectiveness of our proposed module. Furthermore, we (1) investigate the impact of replacing our trained proxy model with alternative approaches,specifically ROMA and COMs, (2) analyze the performance with an optimized condition y and (3) explore asimple annealing approach of . For a comprehensive discussion on these, readers are referred to Appendix F,where the results further highlight the effectiveness of our trained proxy and the adaptation strategy.",
  "Hyperparameter Sensitivity Analysis": "This section investigates the sensitivity of RGD to various hyperparameters. Specifically, we analyze theeffects of (1) the number of diffusion sampling steps T, (2) the condition y, and (3) the learning rate of theproxy-enhanced sampling. These parameters are evaluated on two tasks: the continuous Ant task and thediscrete TFB10 task. Our method is generally robust to these hyperparameters.For a detailed discussion,see Appendix G.",
  "Conclusion": "In conclusion, we propose Robust Guided Diffusion for Offline Black-box Optimization (RGD). The proxy-enhanced sampling module adeptly integrates proxy guidance to facilitate improved sampling control, whilethe diffusion-based proxy refinement module leverages proxy-free diffusion insights for proxy improvement.Empirical evaluations on design-bench have showcased RGDs outstanding performance, further validatedby ablation studies on the contributions of these novel components. We discuss the broader impact andlimitation in Appendix I. Michael Ahn, Henry Zhu, Kristian Hartikainen, Hugo Ponte, Abhishek Gupta, Sergey Levine, and VikashKumar. Robel: robotics benchmarks for learning with low-cost robots. In Conf. on Robot Lea. (CoRL),2020. Arpit Bansal, Hong-Min Chu, Avi Schwarzschild, Soumyadip Sengupta, Micah Goldblum, Jonas Geiping, andTom Goldstein. Universal guidance for diffusion models. In Proc. Comp. Vision. Pattern. Rec.(CVPR),2023.",
  "BProxy Training": "We follow Eq.(33) from Song et al. (2021) where p(xt|x0) = N(xt; (t)x0, 2(t)I). Given this, we can samplext from x0 using: xt = (t)x0 + (t). To recover x0 from xt, we need to know , which approximates as (t) s(xt). Using this approximation, we derive x0 = xt(t)",
  "s.t.() = arg minL(, ).(19)": "Within this context, Dv represents the validation dataset sampled from the offline dataset. The inneroptimization task, which seeks the optimal (), is efficiently approximated via first-order gradient descentmethods. We use batch optimization, with each batch containing 256 training samples and 256 validationsamples. The bi-level optimization process updates the hyperparameter with a single iteration for both theinner and outer levels.",
  "DEvaluation of Median Scores": "While the main text of our paper focuses on the 100th percentile scores, this section provides an in-depthanalysis of the 50th percentile scores. These median scores, previously explored in Trabucco et al. (2022),serve as an additional metric to assess the performance of our RGD method. The outcomes for continuoustasks are detailed in , and those pertaining to discrete tasks, along with their respective rankingstatistics, are outlined in . An examination of highlights the notable success of the RGDapproach, as it achieves the top rank in this evaluation. This finding underscores the methods robustnessand effectiveness.",
  "Overall cost3581.34908.82388.23191.5": "In this section, we analyze the computational overhead of our method. RGD consists of two core components:proxy-enhanced sampling (proxy-e sampling) and diffusion-based proxy refinement (diffusion-b proxy r).Additionally, RGD employs a trained proxy and a proxy-free diffusion model, whose computational demandsare denoted as proxy training and diffusion training, respectively. indicates that experiments can be completed within approximately one hour, demonstrating efficiency.The diffusion-based proxy refinement module is the primary contributor to the computational overhead,primarily due to the usage of a probability flow ODE for sample likelihood computation. However, as this is aone-time process for refining the proxy, its high computational cost is offset by its non-recurring nature. Wehave also compared the time costs of various competitive methods for the 86-dimension continuous SuperCtask. Our method (RGD) requires approximately 3581.3 seconds, Auto.CbAS requires 425.2 seconds, MINrequires 921.4 seconds, BONET requires 673.2 seconds, DDOM requires 460.7 seconds, and BDI requires 618.4seconds. For the discrete 64-dimension NAS task, our method (RGD) takes approximately 3191.5 seconds,while Auto.CbAS takes 389.3 seconds, MIN takes 879.1 seconds, BONET takes 485.3 seconds, DDOM takes103.4 seconds, and BDI takes 498.0 seconds. The evaluation of any mentioned method in NAS entails trainingthe CIFAR-10 dataset over 20 epochs for 128 architectural designs, accumulating a total of 153.6 hours. Incomparison, the one-hour computation time of our method appears negligible. This comparative analysisillustrates the computational overhead of RGD relative to other methods. The computational bottleneck of our method is the diffusion-based proxy refinement module. When weremove this module, this adjustment significantly reduces computational overhead: from 3581.3 seconds to476.7 seconds on SuperC, and from 3191.5 seconds to 95.3 seconds on NAS, rendering our method moreefficient than the comparison methods. Following this adjustment, we recalculate the rankings based on theresults presented in Tables 1, 2, and 3. The new ranking, as shown in the , reaffirms that our methodcontinues to hold its position as the top-performing method in terms of ranking. In contexts such as robotics or bio-chemical research, the most time-intensive part of the production cycle isusually the evaluation of the unknown objective function. Therefore, the time differences between methodsfor deriving high-performance designs are less critical in actual production environments, highlighting RGDspracticality where optimization performance are prioritized over computational speed. This aligns with recentliterature (A.3 Computational Complexity in Chen et al. (2022a) and A.7.5. Computational Cost in Chenet al. (2023b)) indicating that in black-box optimization scenarios, computational time is relatively minorcompared to the time and resources dedicated to experimental validation phases.",
  "FFurther Ablation Studies": "In this section, we extend our exploration to include alternative proxy refinement schemes, namely ROMAand COMs, to compare against our diffusion-based proxy refinement module. The objective is to assessthe relative effectiveness of these schemes in the context of the Ant and TFB10 tasks. The comparativeresults are presented in . Our investigation reveals that proxies refined through ROMA and COMsexhibit performance akin to the vanilla proxy and they fall short of achieving the enhancements seen withour diffusion-based proxy refinement. We hypothesize that the diffusion-based proxy refinement, by aligningclosely with the characteristics of the diffusion model, provides a more relevant and impactful signal. Thisalignment improves the proxys ability to enhance the sampling process more effectively. Additionally, we contrast our approach, which adjusts the strength parameter , with the MIN method thatfocuses on identifying an optimal condition y. The MIN strategy entails optimizing a Lagrangian objectivewith respect to y, a process that requires manual tuning of four hyperparameters. We adopt their methodologyto determine optimal conditions y and incorporate these into the proxy-free diffusion for tasks Ant and TF10.The normalized scores for Ant and TF10 are 0.950 0.017 and 0.660 0.027, respectively. The outcomes",
  "No proxy0.940 0.0040.657 0.006Vanilla proxy0.961 0.0110.667 0.009COMs0.963 0.0040.668 0.003ROMA0.953 0.0030.667 0.003Ours0.968 0.0060.694 0.018": "fall short of those achieved by our method as detailed in . This discrepancy likely stems from thecomplexity involved in optimizing y, whereas dynamically adjusting proves to be a more efficient strategyfor enhancing sampling control. Last but not least, we explore simple annealing approaches for . Specifically, we test two annealing scenariosconsidering the default as 2.0: (1) a decrease from 4.0 to 0.0, (2) an increase from 0.0 to 4.0, both modulatedby a cosine function over the time step (t) and (3) a high constant = 4. We apply these strategies to theAnt Morphology and TF Bind 10 tasks, and the results are as follows:",
  "IBroader Impact and Limitation": "Broader impact. Our research has the potential to significantly accelerate advancements in fields such asnew material development, biomedical innovation, and robotics technology. These advancements could leadto breakthroughs with substantial positive societal impacts. However, we recognize that, like any powerfultool, there are inherent risks associated with the misuse of this technology. One concerning possibility is theexploitation of our optimization techniques to design objects or entities for malicious purposes, including thecreation of more efficient weaponry or harmful biological agents. Given these potential risks, it is imperativeto enforce strict safeguards and regulatory measures, especially in areas where the misuse of technology couldlead to significant ethical and societal harm. The responsible application and governance of such technologiesare crucial to ensuring that they serve to benefit society as a whole. Limitation. We recognize that the benchmarks utilized in our study may not fully capture the complexitiesof more advanced applications, such as protein drug design, primarily due to our current limitations inaccessing wet-lab experimental setups. Moving forward, we aim to mitigate this limitation by fosteringpartnerships with domain experts, which will enable us to apply our method to more challenging and diverseproblems. This direction not only promises to validate the efficacy of our approach in more complex scenariosbut also aligns with our commitment to pushing the boundaries of what our technology can achieve.",
  "NotationsDescriptions": "Proxy parametersDiffusion model parametersJ()Learned mean()Learned standard deviationNGaussian distributionp(y|x)Proxy distributionxParticular designyProperty of designxAdversarial designsq(x)Adversarial distributiontTime step of diffusion samplingTTotal number of diffusion stepsGradient optimization step on designMTotal number of gradient optimization stepsxtNoisy sample at time step tf(, t)Drift coefficientg()Diffusion coefficientwStandard Wiener processwReverse Wiener processx log p(x)Score of the marginal distributions()Learned score function of xtxd1/xd2Two-dim variable of RosenbrockXDesign spacedDesign dimensionOptimized strength parameterStrength parameterDOffline dataset(t)2Variance of perturbation kernel(t)Mean of perturbation kernelHyperparameter of KL loss"
}