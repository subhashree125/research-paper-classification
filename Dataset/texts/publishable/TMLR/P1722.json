{
  "Abstract": "Min-max optimization problems, also known as saddle point problems, have attractedsignificant attention due to their applications in various fields, such as fair beamforming,generative adversarial networks (GANs), and adversarial learning. However, understandingthe properties of these min-max problems has remained a substantial challenge. This studyintroduces a statistical mechanical formalism for analyzing the equilibrium values of min-max problems in the high-dimensional limit, while appropriately addressing the order ofoperations for min and max. As a first step, we apply this formalism to bilinear min-maxgames and simple GANs, deriving the relationship between the amount of training data andgeneralization error and indicating the optimal ratio of fake to real data for effective learning.This formalism provides a groundwork for a deeper theoretical analysis of the equilibriumproperties in various machine learning methods based on min-max problems and encouragesthe development of new algorithms and architectures.",
  "Introduction": "Min-max optimization problems, also known as saddle point problems, are well-known classical optimizationproblems extensively studied in the context of zero-sum games (Wald, 1945; Von Neumann & Morgenstern,1947). These problems have diverse applications across various fields, such as game theory, machine learning,and signal processing. In game theory, min-max problems arise in zero-sum games where one players gaincorresponds to anothers loss. Several methods have been proposed to find the min-max value or equilibriumpoints in these games (Demyanov & Pevnyi, 1972; Maistroskii, 1977; Bruck, 1977; Lions, 1978; Nemhauser& Wolsey, 1988; Freund & Schapire, 1999). In machine learning, min-max games are relevant for traininggenerative adversarial networks (GANs) (Goodfellow et al., 2020; Arjovsky et al., 2017), Additionally, inadversarial learning, these problems are employed to train models that are robust to adversarial attacks byoptimizing a worst-case perturbed loss function (Szegedy et al., 2013; Goodfellow et al., 2014b; Papernotet al., 2016; Madry et al., 2017), Despite the widespread application of min-max optimization problems, several challenges still need to beaddressed, including understanding the usefulness of these min-max formulations, evaluating the convergenceproperties of the algorithms, and conducting sensitivity analyses of min-max values. A promising approachto addressing these issues is to analyze the typical-case behavior of min-max problems by examining themin-max value averaged over random instances drawn from distributions that capture realistic settings,referred to as randomized instance ensembles. Statistical-mechanical approaches, which have demonstratedtheir effectiveness in analyzing the typical-case behavior of randomized instance ensembles of optimizationand constraint-satisfaction problems (Mzard & Parisi, 1986; Fontanari, 1995), provide a powerful formalismfor such analyses. Extending this formalism to analyze the typical-case behavior of min-max values thuspresents a potential direction for further research, although this has not yet been fully explored.",
  "Published in Transactions on Machine Learning Research (1/2025)": "The results for are shown in (Right). The optimal ratio is r = 1/2, indicating that using fakedata approximately equal to half of the real data is effective when the dataset approaches infinity. At r = 1,a phase transition occurs, suggesting that the model changes from a phase of effective learning phase to onewhere fake data becomes dominant. Beyond this point, for r 1, the model fails to learn any meaningfulsignal w, and the generalization error is 1. Furthermore, when r = 1/2, the generalization error scales as g 1, which represents the optimalasymptotic behavior for a model-matched scenario. These results demonstrate the critical role of the ratior in determining learning performance. Therefore, tuning the ratio r according to the available real datais crucial for achieving optimal performance. In practice, it is known that in training GANs, the stabilityof learning can deteriorate depending on the ratio r of fake to real data. This theoretical analysis providesinsights into the importance of the ratio r and is expected to contribute to improving learning algorithms.",
  "Related Work": "The replica method, which is employed in this study, is a non-rigorous but powerful heuristic approach instatistical physics (Edwards & Anderson, 1975; Mzard et al., 1987; Mezard & Montanari, 2009). It hasbeen proven to be a valuable method for high-dimensional machine-learning problems. Previous studies haveinvestigated the relationship between dataset size and generalization error in supervised learning, includingsingle-layer (Gardner & Derrida, 1988; Opper & Haussler, 1991; Barbier et al., 2019; Aubin et al., 2020) andmulti-layer (Aubin et al., 2018) neural networks, as well as kernel methods(Dietrich et al., 1999; Bordelonet al., 2020; Gerace et al., 2020). In unsupervised learning, the replica method has also been applied todimensionality reduction techniques such as the principal component analysis (Biehl & Mietzner, 1993; Hoyle& Rattray, 2004; 2007), and to generative models such as energy-based models (Decelle et al., 2018; Ichikawa& Hukushima, 2022) and denoising autoencoders (Cui & Zdeborov, 2023). However, the dataset-sizedependence of GANs has not been previously analyzed, which this study aims to address. Related to our work, a statistical mechanical formalism for addressing min-max problems has been proposed(Varga, 1998). However, the treatment of the inverse temperature limit differs from our approach, and it haslimitations in accurately handling the order of the min and max operations. In the context of adversariallearning, which involves a non-convex and concave min-max problem, Tanner et al. (2024) analyzes a tractablesetting where the internal maximization can be solved. By reducing such cases to standard optimizationproblems, they apply the replica method and approximate message passing to explore the core phenomenologyobserved in the adversarial robustness. Even in cases where the internal maximization cannot be explicitlysolved, the formalism discussed here provides a basis for further analysis and potential extensions to morecomplex scenarios. NotationHere, we summarize the notations used in this study. We use the shorthand expression [N] ={1, 2, . . . , N}, where N N.Id Rdd denotes a d d identity matrix, and 1d denotes the vector(1, . . . , 1) Rd and 0d denotes the vector (0, . . . , 0) Rd. For a matrix A = (Aij) Rdk and a vectora = (ai) Rd, we use the shorthand expressions dA = di=1kj=1 dAij and da = di=1 dai, respectively. Thenotation Odx,dy(1) describes the asymptotic order of a function with respect to the parameters dx and dy.Specifically, a function f(dx, dy) is said to be Odx,dy(1) if it remains bounded as dx and dy grow large (ortend toward some specified limit), independently of dx and dy. The standard Gaussian measure is definedas Dz = dzez2/2/(2)n/2. The notation extrxf(x) represents the evaluation of a function f(x) at itsextremum with respect to the variable x. Specifically, this shorthand implies locating and evaluating f(x) atpoints where its gradient xf(x) = 0.",
  "Statistical Physics Formalism for Min-Max Optimization Problems": "This section introduces a statistical-mechanical formalism that models min-max problems as a virtual two-temperature system from a statistical mechanics perspective. Min-max problems are formally expressedas(A) = minxX maxyY V (x, y; A),s.t. x X Rdx,y Y Rdy,(1) where V (, ) : Rdx Rdy R is a bivariate function; x Rdx and y Rdy are the optimization variables; Xand Y are the feasible sets; A is a parameter characterizing the problem, e.g., graph G. We introduce thefollowing Boltzmann distribution to analyze min-max problems for a given bivariate function V (x, y; A) inEq. (1), with virtual inverse temperatures min R and max R:",
  "Y dyemaxV (x,y;A),": "where Z(min, max, A) is the normalization constant, also known as the partition function. Hereafter, we referto it as the partition function. In this context, a two-temperature system is particularly important because itallows us to distinguish between the opposing optimization objectives inherent in min-max problems, similarto the different thermal behaviors in statistical mechanics.",
  "By taking the limit max + followed by min +, the distributionlimmin+limmax+ pmin,max(x; A)": "concentrates on a uniform distribution over the min-max values, assuming that well-defined min-max valuesexist for V (, ) and the min-max value is bounded over feasible sets X and Y. Note that the order of theselimits is crucial because the min and max operations cannot be interchanged in non-convex and non-concavemin-max problems (Razaviyayn et al., 2020), i.e., minxX maxyY V (x, y; A) = maxyY minxX V (x, y; A).While a similar formulation has been used in the previous work (Varga, 1998), they simultaneously take thelimits of both min and max with a fixed ratio min/max = Omin,max(1), which does not fully capture thedistinct effects of min and max operations in non-convex settings. Such an approach generally does not yieldaccurate results when the function V (x, y; A) is non-convex with respect to x and y. Statistical-mechanical approaches have demonstrated their effectiveness in analyzing the typical-case behaviorspecifically, the properties of the optimal value averaged over the instances that follow a distribution p(A) foroptimization and constraint-satisfaction problems (Mzard & Parisi, 1986; Fontanari, 1995). These analyseshave succeeded in providing insights into different aspects of combinatorial optimization, unlike worst-caseanalysis. This work also focuses on evaluating the typical cases of min-max problems characterized by arandom parameter A. Our main objective is to calculate the logarithm of Z(min, max, A) averaged over therandom variables A in the limit max followed by min :",
  "At this point, note that these transformations are purely algebraic identities without assuming the parameters or p to be integers": "Following the idea of the replica method (Edwards & Anderson, 1975; Parisi, 1979; 1983; Zdeborov &Krzakala, 2016; Gabri, 2020), we then proceed under the assumption that and p are natural numbers.Specifically, rather than addressing Eq (4) directly real values and p, one calculates the average of the -thand p-th powers for , p N, then performs an analytic continuation to , p R for this expression, andfinally takes the limits +0, min + and max +. Based on this replica trick, the calculationsimplifies to the replicated partition function Z(min, max) as an approximation of Z(min, max):",
  ",(6)": "up to the first order of to take the +0 limit on the right- hand side of Eq. (4). This computationis a standard procedure in the statistical physics of interaction systems including random variables, and isgenerally accepted as exact, although rigorous proof has not yet been provided. Specifically, the mathematicalrigor of the method remains limited due to the unproven uniqueness of the analytic continuation, an issuenoted for the moment problem (Tanaka, 2007). As noted in , the replica method has providedvarious results in high dimensional statistics and machine learning as well. Additionally, before taking the limits, min and max , the concept of finite inverse temperaturesmin and max corresponds to scenarios where neither the minimum nor the maximum is fully achieved, acommon situation in the min-max algorithms. This approach provides valuable insights into cases whereneither extreme is fully realized or both are only partially optimized. Exploring novel algorithms based onthis finite-temperature generalization of min-max problems represents an intriguing direction for future work.Furthermore, in game theory, this formalism can be interpreted as a framework for modeling games underrelaxed assumptions of complete rationality, where players x and y are assumed to behave with boundedrationality rather than adhering strictly to classical models of fully rational behavior (Von Neumann &Morgenstern, 1947). In the following sections, we apply this formalism to a fundamental and significant bilinear min-max game,demonstrating that the analytic continuation of p in the replica method is a rigorous operation. We thenanalyze the minimal model of GANs as a more practical example.",
  "Bilinear Min-max Games": "This min-max formalism introduces two replica parameters: , associated with the randomness of A, and p,related to the dual structure of min-max problems. The analytic continuation with respect to the replicaparameter is widely recognized as effective and is frequently employed in the statistical mechanics ofoptimization. However, the analytic continuation of the replica parameter p has not yet been explored.While establishing its mathematical validity presents challenges, this study eliminates the influence of thereplica parameter associated with the randomness of A and rigorously demonstrates that the analyticcontinuation with respect to p holds for fundamental bilinear min-max games. Specifically, we show thatthe free energy density derived using the replica trick in Eq. (6), as explained in , is equivalent tothe exact expression in Eq. (5) derived without analytic continuation of and p for bilinear min-max games(Tseng, 1995; Daskalakis et al., 2017). Bilinear games are regarded as a fundamental example for studying new min-max optimization algorithmsand techniques (Daskalakis et al., 2017; Gidel et al., 2019; 2018; Liang & Stokes, 2019). Mathematically,",
  "dxdyxWxyy + xbx + yby,": "where W = (Wxx, Wyy, Wxy, bx, by).For simplicity, we assume Wxx = wxx1dxdx Rdxdx, Wxy =wxx1dxdy Rdxdy, Wyy = wyy1dydy Rdydy, bx = bx1dx Rdx, and by = by1dy Rdy. The followingresults can be readily extended to the matrices Wxx, Wyy, and Wxy with a limited number of eigenvalues ofOdx,dy(1). For a detailed discussion, refer to Appendix B. In this setting, the analytically continued free energy density f(min, max; W ) calculated using replicatedpartition function Z(min, max) in Eq. (6) coincides with the exact free energy density f(min, max; W )from the partition function Z(min, max) in Eq. (5).",
  "Generative Adversarial Networks": "Generative adversarial networks (GANs) (Goodfellow et al., 2020) aim to model high-dimensional probabilitydistributions based on training datasets. Despite significant progress in practical applications (Arjovskyet al., 2017; Lucic et al., 2018; Ledig et al., 2017; Isola et al., 2017; Reed et al., 2016), several issues areyet to be resolved, including how the amount of training data influences generalization performance andhow sensitive GANs are to specific hyperparameters. This section analyzes the relationship between theamount of training data and generalization error. Additionally, we conduct a sensitivity analysis on the ratioof fake data generated by the generator to the amount of training data, which is critical for the training ofGANs. Our analysis employs a minimal setup that captures the intrinsic structure and learning dynamicsof GANs (Wang et al., 2019). We consider the high-dimensional limit, where the number of real and fakesamples, n and n, respectively, and the dimension d are large while remaining comparable. Specifically, weanalyze the regime in which n, n, d while maintaining a comparable ratio, i.e., = n/d = (d0) and = n/d = (d0), commonly referred to as sample complexity.",
  "n,(8)": "where w Rd is a learnable parameter, z R is a latent variable drawn from a standard normal distributionp(z) = N(z; 0, 1), n is a noise vector whose components are i.i.d. from the standard normal distributionN(n; 0d, Id), and R is a scalar parameter to control the strength of the noise.",
  "v2 +2 w2,(11)": "and D = {z}n=1 are the latent values of the fake data. The last two terms are regularization terms, where and control the regularization strength. This value function defined in Eq. (11) is a general form thatincludes various types of GANs. Specifically, when = and L 1, it represents a Wasserstein GANs(WGANs) (Arjovsky et al., 2017) and, when (x) = log (x) and (x) = log(1 (x)) with beingthe sigmoid function, it corresponds to the Vanilla GANs, which minimize the JS-divergence (Goodfellowet al., 2014a). As we assumes a linear discriminator, V (w, v; D, D) can be expressed as a function of linearcombinations vx/",
  "+ 1": "Learning CurveFor simplicity, we set = r and = = = = 1. Our analysis focuses on how thegeneralization error depends on while varying the ratio r, as generating fake data from the generator isgenerally much easier than collecting real data. (Left) shows the dependence of generalization erroron sample complexity for various values of the ratio r. The results demonstrate a sharp decline in thegeneralization error as the ratio r increases. However, when r becomes large, the generalization error increasesin the region where is large, eventually leading to a phase where no learning occurs, and the generalizationerror equals 1. This implies that as increases, the learning becomes dominated by only fake data. In contrast, for smaller r, real data consistently dominates the objective function V (w, v; D, D), resulting ina steady decrease in generalization error. However, the reduced influence of the fake data component in theobjective function, which drives the learning of the generator, requires a significantly larger amount of realdata for effective generator training. Asymptotic Generalization ErrorWe next analyze the asymptotic behavior of the generalization errorwhen the sample complexity becomes sufficiently large. The asymptotic behavior of the generalization erroras a function of is given by",
  "Conclusion": "This study introduces a statistical mechanical formalism to analyze high-dimensional min-max optimizationproblems, focusing on the critical order of min and max operations in non-convex scenarios. Our goal wasto perform a sensitivity analysis of equilibrium values, providing new insights into their properties andgeneralization performance. We applied this approach to a simple min-max game, evaluated the generalization performance of GANs,and derived the optimal ratio of fake to real data for effective learning. This successful application notonly validates the approach but also opens the way for extending this formalism to more complex min-maxproblems and broader applications, suggesting a promising direction for significant advancements in machinelearning and optimization. We thank T. Takahashi, K. Okajima, Y. Nagano, and K. Nakaishi for useful discussions and suggestions.This work was supported by JST Grant Number JPMJPF2221 and JPSJ Grant-in-Aid for Scientific ResearchNumber 23H01095. Additionally, YI was supported by the WINGS-FMSP program at the University ofTokyo.",
  "Martin Arjovsky, Soumith Chintala, and Lon Bottou. Wasserstein generative adversarial networks. InInternational Conference on Machine Learning, pp. 214223. PMLR, 2017": "Benjamin Aubin, Antoine Maillard, Florent Krzakala, Nicolas Macris, and Lenka Zdeborov. The committeemachine: Computational to statistical gaps in learning a two-layers neural network. Advances in NeuralInformation Processing Systems, 31, 2018. Benjamin Aubin, Florent Krzakala, Yue Lu, and Lenka Zdeborov. Generalization error in high-dimensionalperceptrons: Approaching bayes error with convex optimization. Advances in Neural Information ProcessingSystems, 33:1219912210, 2020. Jean Barbier, Florent Krzakala, Nicolas Macris, Lo Miolane, and Lenka Zdeborov. Optimal errors andphase transitions in high-dimensional generalized linear models. Proceedings of the National Academy ofSciences, 116(12):54515460, 2019.",
  "Gauthier Gidel, Hugo Berard, Gatan Vignoud, Pascal Vincent, and Simon Lacoste-Julien. A variationalinequality perspective on generative adversarial networks. arXiv preprint arXiv:1802.10551, 2018": "Gauthier Gidel, Reyhane Askari Hemmat, Mohammad Pezeshki, Rmi Le Priol, Gabriel Huang, SimonLacoste-Julien, and Ioannis Mitliagkas. Negative momentum for improved game dynamics. In The 22ndInternational Conference on Artificial Intelligence and Statistics, pp. 18021811. PMLR, 2019. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, AaronCourville, and Yoshua Bengio. Generative adversarial nets. Advances in Neural Information ProcessingSystems, 27, 2014a.",
  "Yuma Ichikawa and Koji Hukushima. Dataset size dependence of rate-distortion curve and threshold ofposterior collapse in linear vae. arXiv preprint arXiv:2309.07663, 2023": "Yuma Ichikawa and Koji Hukushima.Learning dynamics in linear VAE: Posterior collapse threshold,superfluous latent space pitfalls, and speedup with KL annealing. In Sanjoy Dasgupta, Stephan Mandt,and Yingzhen Li (eds.), Proceedings of The 27th International Conference on Artificial Intelligence andStatistics, volume 238 of Proceedings of Machine Learning Research, pp. 19361944. PMLR, 0204 May2024.",
  "Niels Ipsen and Lars Kai Hansen. Phase transition in pca with missing data: Reduced signal-to-noise ratio,not sample size! In International Conference on Machine Learning, pp. 29512960. PMLR, 2019": "Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image translation with conditionaladversarial networks. In Proceedings of the IEEE conference on computer vision and pattern recognition,pp. 11251134, 2017. Christian Ledig, Lucas Theis, Ferenc Huszr, Jose Caballero, Andrew Cunningham, Alejandro Acosta, AndrewAitken, Alykhan Tejani, Johannes Totz, Zehan Wang, et al. Photo-realistic single image super-resolutionusing a generative adversarial network. In Proceedings of the IEEE conference on computer vision andpattern recognition, pp. 46814690, 2017.",
  "Marc Potters and Jean-Philippe Bouchaud. A First Course in Random Matrix Theory: For Physicists,Engineers and Data Scientists. Cambridge University Press, 2020": "Meisam Razaviyayn, Tianjian Huang, Songtao Lu, Maher Nouiehed, Maziar Sanjabi, and Mingyi Hong.Nonconvex min-max optimization: Applications, challenges, and recent theoretical advances. IEEE SignalProcessing Magazine, 37(5):5566, 2020. Scott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran, Bernt Schiele, and Honglak Lee. Generativeadversarial text to image synthesis. In International Conference on Machine Learning, pp. 10601069.PMLR, 2016.",
  "ADerivation of Theorem 4.1 proof": "In this section, we provide the derivation proof of Theorem 4.1. The derivation begins with the calculation ofthe free energy density without the analytic continuation of p = min/max to p N. The free energy densityin Eq. (2) is connected to the effective Hamiltonian, Heff(x; W ), which is defined through the relationship:",
  "CEvaluation of Functions of the Optimal Value of Min-Max Problems": "In this section, we present a method for evaluating the expected value EA[G(x(A))] over a set of probleminstances A, where x(A) = argminxX maxyY V (x, y; A) represents the min-max optimal solution. Thisanalysis provides insights into how Eq. (2) functions as a generating function. We also demonstrate that thisapproach can be directly applied to evaluate the generalization error in , particularly Eq. (13).",
  "dED w(D) w2": "To compute this, we augment the free energy calculation by adding the term (w2 2ww). Thisadjustment is incorporated into the calculation by modifying the term (wa)2 in the exponent of the d-thpower expression in Eq. (14) to ( + )(wa)2 2(wwa). Since these terms are quadratic, the Gaussianintegration remains straightforward. The remaining calculation follows the same procedure in the main textand is thus omitted for brevity. Eventually, we obtain the following form:"
}