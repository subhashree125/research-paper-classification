{
  "Abstract": "Backdoor attacks facilitate unauthorized control in the testing stage by carefully injectingharmful triggers during the training phase of deep neural networks. Previous works havefocused on improving the stealthiness of the trigger while randomly selecting samples toattack. However, we find that random selection harms the stealthiness of the model. Inthis paper, we identify significant pitfalls of random sampling, which make the attacks moredetectable and easier to defend against. To improve the stealthiness of existing attacks, weintroduce a method of strategically poisoning samples near the models decision boundary,aiming to minimally alter the models behavior (decision boundary) before and after back-dooring. Our main insight for detecting boundary samples is exploiting the confidence scoresas a metric for being near the decision boundary and selecting those to poison (inject) theattack. The proposed approach makes it significantly harder for defenders to identify theattacks. Our method is versatile and independent of any specific trigger design. We providetheoretical insights and conduct extensive experiments to demonstrate the effectiveness ofthe proposed method.",
  "Published in Transactions on Machine Learning Research (11/2024)": "To further validate, we consider a naive defense: ignore data points (x, y) from the training set if x xy,mean2 where xy,mean := mean{x|(x, y) data}.We follow and test on Cifar10 andResNet18. We remove samples with a l2 distance in the raw space larger than 18, which is about 12% oftotal samples. Then we train the model on the filtered dataset and test the success rate. We also considerBadNet and Blended as the backbone for illustration.",
  "Is there a better sampling strategy to enhance the stealthiness of backdoors?": "To investigate this question, we follow the common understanding to assume that the attackers can accessthe training data while maybe only allowed to manipulate a part of the training data. For example, theattackers may contribute malicious data to publicly sourced datasets via uploading their own data online (Liet al., 2022). Besides improving the trigger pattern, they also need a sampling strategy to determine thedata to update. To better understand the behavior of the backdoor attacks, in .1, we investigate the latent space ofthe backdoored model to take a closer look at the random sampling strategy. We draw two findings from thevisualizations in . First, most randomly chosen samples are close to the center of their true classesin the latent space. Second, the closer a sample is from its true class on the clean model, the farther it getsfrom the target class on the backdoored model (.1). These two observations reveal an importantconcern about the stealthiness of the random sampling strategy, where the randomly sampled data pointsmay be easily detected as outliers. To gain a deeper understanding, we further build a theoretical analysis ofSVM in the latent space (.3) to demonstrate the relation between the random sampling strategy andattack stealthiness. Moreover, our observations suggest an alternative to random samplingit is better toselect samples closer to the decision boundary. Our preliminary studies show that these boundary samplescan be manipulated to be closer to the clean samples from the target class and can greatly enhance theirstealthiness under potential outlier detection (see c and 1d). Inspired by the above observations, we propose a novel method called confidence-driven boundary sam-pling (CBS). Specifically, we identify boundary samples with low confidence scores based on a surrogatemodel trained on the clean training set. Intuitively, samples with lower confidence scores are closer to theboundary between their own class and the target class in the latent space Karimi et al. (2019) comparedto random samples. Therefore, this strategy makes it more challenging to detect attacks. Moreover, oursampling strategy is independent from existing attack approaches, making it exceptionally versatile. It canbe easily integrated with various backdoor attacks, offering researchers and practitioners a powerful toolto enhance the stealthiness of backdoor attacks without requiring extensive modifications to their existingmethods or frameworks. Extensive experiments combining proposed confidence-based boundary samplingwith various backdoor attacks illustrate the advantage of the proposed method over random sampling.",
  "Samplings in backdoor attacks": "Despite the development of triggers in backdoor attacks, the impact of poisoned sample selection is also at-tracting more and more attention. Xia et al. (2022) proposed a filtering-and-updating strategy (FUS) to selectsamples with higher contributions to the injection of backdoors by computing the forgetting event (Tonevaet al., 2018) of each sample. For each iteration, poison samples with low forgetting events will be removed,and new samples will be randomly sampled to fill out the poisoned training set.Han et al. (2023); Liet al. (2023); Xia et al. (2023) followed this line and also adopted the forgetting score for sample selection.Wu et al. (2023); Zhu et al. (2023) leverages masks and l2 distance in representation space respectively toimprove the effectiveness of the backdoor. Though these works can improve the success rate of backdoorattacks via sample selection, they ignore the backdoors ability to resist defenses, known as the stealthinessof backdoors. To the best of our knowledge, we are the first to study the stealthiness problem from thesampling perspective.",
  "Threat model": "We follow the commonly used threat model for the backdoor attacks (Gu et al., 2017; Doan et al., 2021b).We assume that the attacker can access the clean training set and modify a proportion of the trainingdata.Then, the victim trains his own models on this data, and the attacker has no knowledge of thistraining procedure. In a real-world situation, the attacker can access some clean datasets, and modify aproportion of them by inserting triggers. Then they upload the poisoned data to the Internet and victimsunknowingly download and use it for training (Gu et al., 2017; Chen et al., 2017). Note that many existingbackdoor attacks (Nguyen & Tran, 2021; Turner et al., 2019; Saha et al., 2020) have already adopted thisassumption, and our proposed method does not demand additional capabilities from attackers beyond whatis already assumed in the context of existing attack scenarios. Furthermore, our method, detailed in , addresses practical scenarios where attackers are limited to poisoning samples from a specific subset, notthe entire dataset, with empirical results in .5. For example, an attacker might only control theirown data and not have access to alter public datasets.",
  "In the following, we introduce a general pipeline, which is applicable to a wide range of backdoor attacks.The pipeline consists of two components": "(1) Poison sampling. Let Dtr = {(xi, yi)}ni=1 denote the set of n clean training samples, where xi X iseach individual input sample with yi Y as the true class. The attacker selects a subset of data U Dtr,with p = |U|/|Dtr| as the poison rate, where the poison rate p is usually small.",
  "T(U) = {(x, y)|x = Gt(x), y = S(x, y), (x, y) U}(1)": "where Gt : X X is the attacker-specified poisoned image generator with trigger pattern t, which satisfiesthe following constraints: Gt(x) = x and d(Gt(x), x) where d is some distance function such as l2/ldistance, and S indicates the attacker-specified target label generator. After training the backdoored modelf(; b), where b denote parameters of the backdoored model, on the poisoned set, the injected backdoorwill be activated by trigger t. For any given clean test set Dte, the accuracy of f(; b) evaluated on trigger-embedded dataset T(Dte) is referred to as success rate, and attackers also expect to see high accuracy onany clean samples without triggers.",
  "Revisit random sampling": "Visualization of Stealthiness. Random sampling selects samples to be poisoned from the clean trainingset with the same probability and is commonly used in existing attacking methods. However, we suspectthat such unconstrained random sampling is easy to detect as outliers of the target class in the latentspace. To examine the sample distribution in the latent space, we first conduct TSNE (Van der Maaten &Hinton, 2008) visualizations for the backdoored model of (1) clean samples of the target class, and (2) thepoisoned samples from other classes but labeled as the target class. We consider these poisoned samplesare obtained by two representative attack algorithms, BadNet (Gu et al., 2017) and Blend (Chen et al.,2017) both of which apply random sampling, on CIFAR10 (Krizhevsky et al., 2009), in a and 1b.In detail, the visualizations show the latent representations of samples from the target class, and the colorsred and blue indicate poisoned and clean samples respectively. One can see a clear gap between poisonedand clean samples. For both attacks, most of the poisoned samples form a distinct cluster outside the clean",
  ": The left two figures depict the distribution of do when samples are Randomly selected by BadNet and Blend. Theright two figures shows the relationship between do and dt for BadNet and Blend": "Relation between Stealthiness & Random Sampling. In our study, we also observe the potentialrelation between random sampling and the stealthiness of backdoors 1. To elaborate, we further calculatethe distance from each selected sample (without trigger) to the center2 of their true classes computed on theclean model, which is denoted as do. As seen in a and 2b, random sampling is likely to select samplesthat are close to the center of their true classes. However, we find do may have an obvious correlation with thedistance between the sample and the target class which we visualize in the previous . Formally, wedefine the distance between each selected sample (with trigger) and the center of the target class computedon the backdoored model as dt. From c and 2d, we observe a negative correlation between dt and do 3, indicating that samplescloser to the center of their true classes in the clean model tend to be farther from the target class afterpoisoning and thus easier to detect. These findings imply that random sampling often results in the selectionof samples with weaker stealthiness. Our observations also suggest that samples closer to the boundary maylead to better stealthiness and motivate our proposed method.",
  "Confidence-driven boundary sampling (CBS)": "One key challenge for boundary sampling is how to determine which samples are around the boundaries.Though we can directly compute the distance from each sample to the center of the target class in thelatent space and choose those with smaller distances, this approach can be time-consuming, as one needs tocompute the center of the target class first and then compute the distance for each sample. This problemcan be more severe when the datasets size and dimensionality grow. Consequently, a more efficient andeffective method is in pursuit. 1 We provide a detailed discussion of stealthiness\" in Appendix 8.5.2A formal definition of do and dt is shown in Appendix 8.53We also include a discussion of the relationship between raw input space and latent space in Appendix 8.11.",
  "|sc(f(x; ))y sc(f(x; ))y| ,(2)then (x, y) is noted as -boundary sample with target y": "To explain Definition 4.1, since sc(f(x; ))y represents the probability of classifying x as class y, then whenthere exists another class y, for which sc(f(x; ))y sc(f(x; ))y, it signifies that the model is uncertainabout whether to classify x as class y or class y. This uncertainty suggests that the sample is positionednear the boundary that separates class y from class y (Karimi et al., 2019). The proposed Confidence-driven boundary sampling (CBS) method is based on Definition 4.1.Ingeneral, CBS selects boundary samples in Definition 4.1 for a given threshold . Since we assume the attackerhas no knowledge of the victims model, we apply a surrogate model like what black-box adversarial attacksoften do (Chakraborty et al., 2018). In detail, a pre-trained surrogate model f(; ) is leveraged to estimateconfidence scores for each sample, and -boundary samples with pre-specified target yt are selected forpoisoning. The detailed algorithm is shown in Algorithm 1 5. Note that the threshold is closely related topoison rate p in .2, and we can determine so that |U(yt, )| = p |Dtr|. Since we claim that oursampling method can be easily adapted to various backdoor attacks, we provide an example that adapts oursampling methods to Blend (Chen et al., 2017), where we first select samples to be poisoned via Algorithm 1and then blend these samples with the trigger pattern t to generate the poisoned training set.",
  ": Backdoor on SVM": "where let 2 = 0 for simplicity. Assume that each class contains n samples. We consider a simple attack thatselects one single sample x from class C1, add a trigger to it to generate a poisoned x, and assign a label asclass C2 for it. Let C1, C2 denote the poisoned data, and we can obtain a new backdoored decision boundaryof SVM on the poisoned data. To study the backdoor effect of the trigger, we assume x = x + t/t wheret/t, denote the direction and strength of the trigger, respectively. To explain this design, we assume that the trigger introduces a feature to the original samples (Khaddajet al., 2023), and this feature is closely related to the target class while nearly orthogonal to the predictionfeatures7. In addition, we assume t is fixed for simplicity, which means this trigger is universal and we arguethat this is valid because existing attacks such as BadNet (Gu et al., 2017) and Blend (Chen et al., 2017)inject the same trigger to every sample.To ensure the backdoor effect, we further assume (2 1)T t 0,otherwise the poisoned sample will be even further from the target class (shown as the direction of the greendashed arrow) and lead to subtle backdoor effects. We are interested in two questions: (Q1) Are boundarysamples harder to detect? (Q2) How do samples affect the backdoor performance? To investigate (Q1), we adopt the Mahalanobis distance (Mahalanobis, 2018) between the poisoned samplex and the target class C2 as an indicator of outliers. A smaller distance means x is less likely to be anoutlier, indicating better stealthiness. For (Q2), we estimate the success rate by estimating the volume (orarea in 2D data) of the shifted class C1 to the right of the backdoored decision boundary. This is becausewhen triggers are added to every sample, the whole class will shift in the direction of t, shown as the orangedashed circle in . The following series of theorems and propositions answer the above two questions.We begin with the Manahalnobis distance: Theorem 4.2 (Mahalanobis distance). Assume x = x + t/t2 := x + a for some arbitrary x and sometrigger t and strength . Also assume 2 = 0, (2 1)T x 0. Denote 2 and S2 as the sample meanand covariance matrix of the poisoned data with label C2. Then the Mahalanobis distance between x and thetarget class C2 is defined asd2M(x, C2) = (x 2)T S12 (x 2).",
  "cos(a, x 1) x 1/2 r/2": "The proof of Theorem 4.3 can be found in Appendix 8.1. To figure out the attack success rate, we directlycalculate the area of incorrect classification.Remark 4.4 (Existence of hard margin). Theorem 4.3 describes how the attack on x affects the attacksuccess rate.However, the existence of a hard margin depends on the selected sample x and trigger t,which in turn determines whether an attack is effective. We provide for a better illustration. Forthe trigger t, we need tT (1) > 0, which indicates that the trigger t moves the clean sample x (fromC1) towards the target cluster C2.To guarantee a hard margin, we require the poisoned sample x tofall into the shaded area, which is determined by f1, f2 and C1. f1, f2 are the common tangent lines toC1, C2, and are two extreme hyperplane that separates two clusters. Formally, we can define the area asx {x| ({x + a 12 r} {f1(x + a) < 0} {f2(x + a) > 0}) {f1(x + a) > 0} {f2(x + a) < 0}}.These conditions indicate that samples closer to the decision boundary are more likely to result in a hardmargin in the poisoned data and guarantee an effective attack.",
  ": An illustrating figure for the existence of hard margin. The shaded area represents the region ofx where a hard margin exists": "On the other hand, unlike random sampling, since CBSrelies on the estimate of the confidence, we providethe following results to illustrate the impact of a finite sample size on CBSand random sampling. We firstpresent Theorem 4.5 below to explain the change of the SVM margin under the finite-sample scenario usingclean data: Theorem 4.5 (Finite-sample scenario). Under the same conditions as Theorem 4.2, consider the classifica-tion with clean data. Assume there are n samples from C1 and n samples from C2. Take n = (log n)/n.With probability at least",
  "Experiment": "In this section, we conduct experiments to validate the effectiveness of CBS, and show its ability to boostthe stealthiness of various existing attacks. We evaluate CBS and baseline samplings under no-defense andvarious representative defenses in .2 and 5.3. In particular, we select poisoned samples from thewhole training data in these three sections and provide results when only partial data is accessible in .5 to validate the effectiveness of our approach in a broad and practical scenario. In .6, we willprovide more empirical evidence to illustrate that CBS is harder to detect and mitigate. We also directreaders to additional experiments regarding larger datasets and more defenses in the Supplementary for amore comprehensive evaluation.",
  "Adapt-patch2.20.76.60.514.30.310.92.313.41.416.20.9": "To mitigate this limitation, we design two strategies.The general idea is to balance the effectiveness-stealthiness trade-off to control the performance change of different defense methods. In the first strategy,we set a lower threshold for confidence score during selection, i.e. 1 |sc(f(xi; ))yi sc(f(xi; ))yt| 2.The second strategy is to mix the boundary samples with some random samples. We conduct experimentsto test if these strategies can improve the performance on undefended models.We test with ResNet18model, Cifar10 dataset, and backbone attacks are BadNet and Blended. For the first strategy, we set thethreshold as [0.07, 0.25] (about 100 samples within this interval and a good balance between effectiveness andstealthiness), and for the second strategy, we select 70% boundary samples and 30% random samples. Thepoison rate is 0.2%, i.e. 100 poisoned samples. We report the results of both undefended and 4 representativedefenses in . It is clear that both strategies can improve the attacking performance under no defenses and can achievecomparable performance with random selections. The performance under defenses is slightly reduced becausethe poisoned samples are easier to detect by defense methods. Nonetheless, the performance still significantlyoutperforms the baselines.",
  "LC0.90.22.70.86.20.62.50.82.10.76.70.5": "Blend (Chen et al., 2017) which applies the image blending to interpolate the trigger with samples; andAdaptive backdoor12 (Qi et al., 2022) which introduces regularization samples to improve the stealthiness ofbackdoors, as backbone attacks. We include 4 representative defenses: Spectral Signiture (SS) (Tran et al.,2018) and STRIP (Gao et al., 2019) which are outlier-detection-based defenses, Anti-Backdoor Learning(ABL) (Li et al., 2021a) and Neural Cleanser (NC) (Wang et al., 2019) which are not detection-baseddefenses. We follow the default settings for backbone attacks and defenses. For CBS, we set = 0.2 and thecorresponding poison rate is 0.2% applied for Random and FUS, to guarantee that poisoning rates are thesame for all sampling methods. We retrain victim models on poisoned training data from scratch via SGDfor 200 epochs with an initial learning rate of 0.1 and decay by 0.1 at epochs 100 and 150. Then we comparethe success rate which is defined as the probability of classifying samples with triggers as the target class.We repeat every experiment 5 times and report average success rates (ASR) as well as the standard error ifnot specified 13. Results on Cifar10 are shown in , and results on Cifar100 and Tiny-ImageNet areshown in the Appendix. Performance comparison. Generally, CBS enhances the resilience of backbone attacks against variousdefense mechanisms. It achieves notable improvement compared to Random and FUS without a significantdecrease in ASR when no defenses are in place. This is consistent with our analysis in .3. Wenotice that CBS has the lowest success rate when no defenses are active, which is consistent with ouranalysis in Remark 4.814. Nonetheless, CBSstill achieves commendable performance, with success ratesexceeding 70% and even reaching 90% for certain attacks. It is important to note that the effectiveness ofCBS varies for different attacks and defenses. The improvements are more pronounced when dealing withstronger defenses and more vulnerable attacks. For instance, when facing SS, a robust defense strategy,CBS significantly enhances ASR for nearly all backbone attacks, especially for BadNet. In this case, CBS canachieve more than a 20% increase compared to Random and a 15% increase compared to FUS. Additionally,its worth mentioning that CBS consistently strengthens resistance against detection-based (first two) andnon-detection-based defenses (the other two).This further supports the notion that boundary samplesare inherently more challenging to detect and counteract. While the improvement of CBS on VGG16 isslightly less pronounced than on ResNet18, it still outperforms Random and FUS in nearly every experiment,indicating that CBS can be effective even on unknown models.",
  "Performance of CBS in Type II backdoor attacks": "Attacks & Defenses. We consider 2 representative attacks in this categoryHidden-trigger (Saha et al.,2020), which adds imperceptible perturbations to samples to inject backdoors, and Clean-label (LC) (Turneret al., 2019), which leverages adversarial examples to train a backdoored model.We follow the defaultsettings in the original papers and adapt l2-norm bounded perturbation (perturbation size 6/255) for LC.We test all attacks against three representative defenses that are applicable to these attacks. We includeNC, SS, Fine Pruning (FP) (Liu et al., 2018), Anti-Backdoor Learning (ABL) (Li et al., 2021a). We set = 0.3 for CBS and p = 0.2% for Random and FUS correspondingly. For every experiment, a source classand a target class are randomly chosen, and poisoned samples are selected from the source class. The successrate is defined as the probability of misclassifying samples from the source class with triggers as the targetclass. Results on dataset Cifar10 and Cifar100 are presented in . We include additional results onTiny-ImageNet in the Supplementary for a further illustration. Performance comparison. As detailed in , CBS displays an enhanced capacity to withstand variousdefense mechanisms, akin to Type I attacks, while sacrificing a marginal degree of success rate. Notably,in the presence of defenses, CBS consistently surpasses both Random and FUS strategies in performance,demonstrating its adaptability across various challenging conditions. This is particularly evident when tack-ling susceptible attack strategies like BadNet, where CBS not only achieves substantial gainsoutperformingRandom by upwards of 10% and FUS by more than 5%but also maintains smaller standard errors. Thesesmaller errors reflect CBSs stability, which is critical in real-world applications where consistent performanceis crucial.",
  "Performance of CBS in Type III backdoor attacks": "Attacks & Defenses. We consider 3 Representative attacks in this categoryLira (Doan et al., 2021b)which involves a stealthy backdoor transformation function and iteratively updates triggers and model pa-rameters; WaNet (Nguyen & Tran, 2021) which applies the image warping technique to make triggers morestealthy; Wasserstein Backdoor (WB) (Doan et al., 2021a) which directly minimizes the distance betweenpoisoned and clean representations. Note that Type III attacks allow the attackers to take control of thetraining process. Though our threat model does not require this additional capability of attackers, we followthis assumption when implementing these attacks. Therefore, we directly select samples based on ResNet18and VGG16 rather than using ResNet18 as a surrogate model. We conduct 3 representative defenses thatare applicable for this type of attacksNC, STRIP, FP. We follow the default settings to implement theseattacks and defenses. We set = 0.37 which matches the poison rate p = 0.1 in the original settings ofbackbone attacks. Results on Cifar10 and Cifar100 are presented in . Performance comparison. Except for the common findings in previous attacks, where CBS consistentlyoutperforms baseline methods in nearly all experiments, we observe that the impact of CBS varies whenapplied to different backbone attacks. Specifically, CBS tends to yield the most significant improvements whenapplied to WB, while its effect is less pronounced when applied to WaNet. For example, when confrontingFP and comparing CBS with both Random and FUS, we observed an increase in ASR of over 7% on WB.In comparison, the increase on WaNet amounted to only 3%, with Lira showing intermediate results. Thisdivergence may be attributed to the distinct techniques employed by these attacks to enhance their resistanceagainst defenses. WB focuses on minimizing the distance between poisoned samples and clean samples fromthe target class in the latent space. By selecting boundary samples that are closer to the target class, WBcan reach a smaller loss than that optimized on random samples, resulting in improved resistance. Theutilization of the fine-tuning process and additional information from victim models in Lira enable a moreprecise estimation of decision boundaries and the identification of boundary samples. WaNet introducesGaussian noise to some randomly selected trigger samples throughout the poisoned dataset, which maydestroy the impact of CBS if some boundary samples move away from the boundary after adding noise. Theseobservations suggest that combining CBS with proper trigger designs can achieve even better performance,and it is an interesting topic to optimize trigger designs and sampling methods at the same time for morestealthiness, which leaves for future exploration.",
  "CBS with Partial Backdoor": "We investigate scenarios where attackers can only manipulate partial training data. Specifically, we conductexperiments on the ResNet18 model using the Cifar10 dataset, employing the BadNet attack method withvarious sampling strategies. We designate different subset rates (10%, 5%, 1%) of the training set as accessibleto the attacker, who can only poison this fraction of the data. From their accessible data, attackers inserttriggers into 10% of the samples. The effectiveness of these attacks, under different defense mechanisms,is evaluated. Our findings, presented in , demonstrate that our method effectively enhances thestealthiness of backdoor attacks, even with limited data access. This underscores the practical potential ofour approach in real-world situations where attackers cannot access the entire training dataset.",
  "Ablation study": "Impact of . Threshold is one key hyperparameter in CBS to determine which samples are around theboundary, and to study the impact of , we conduct experiments on different . Since the size of the poisonedset generated by different is different, we fix the poison rate to be 0.1% (50 samples), and for large thatgenerates more samples, we randomly choose 50 samples from it to form the final poisoned set. We consider = 0.1, 0.15, 0.2, 0.25, 0.3, and conduct experiments on model ResNet18 and dataset Cifar10 with BadNetas the backbone. Results of ASR under no defense and 5 defenses are shown in . It is obvious thatthe ASR for no defenses is increasing when is increasing. We notice that large (0.25,0.3) has higher ASRwithout defenses but relatively small ASR against defenses, indicating that the stealthiness of backdoorsis reduced for larger . For small (0.1), ASR decreases for either no defenses or against defenses. Theseobservations suggest that samples too close or too far from the boundary can hurt the effect of CBS, and aproper is needed to balance between performance and stealthiness. Impact of confidence. Since our core idea is to select samples with lower confidence, we conduct ex-periments to compare the influence of high-confidence and low-confidence samples.In detail, we selectlow-confidence samples with = 0.2 and high-confidence samples with = 0.915. We still conduct experi-ments on ResNet18 and Cifar10 with BadNet, and the ASR is shown in . Note that low-confidencesamples significantly outperform the other 2 types of samples, while high-confidence samples are even worsethan random samples. Therefore, these results further support our claim that low-confidence samples canimprove the stealthiness of backdoors.",
  "Conclusion": "In this paper, we highlight a crucial aspect of backdoor attacks that was previously overlooked. We find thatthe choice of which samples to poison plays a significant role in a models ability to resist defense mechanisms.To address this, we introduce a confidence-driven boundary sampling approach, which involves carefullyselecting samples near the decision boundary. This approach has proven highly effective in improving anattackers resistance against defenses. It also holds promising potential for enhancing the robustness of allbackdoored models against defense mechanisms.",
  "Khoa Doan, Yingjie Lao, and Ping Li. Backdoor attack with imperceptible input and latent modification.Advances in Neural Information Processing Systems, 34:1894418957, 2021a": "Khoa Doan, Yingjie Lao, Weijie Zhao, and Ping Li. Lira: Learnable, imperceptible and robust backdoorattacks. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 1196611976,2021b. Alhussein Fawzi, Seyed-Mohsen Moosavi-Dezfooli, Pascal Frossard, and Stefano Soatto. Empirical study ofthe topology and geometry of deep networks. In Proceedings of the IEEE Conference on Computer Visionand Pattern Recognition, pp. 37623770, 2018. Yansong Gao, Change Xu, Derui Wang, Shiping Chen, Damith C Ranasinghe, and Surya Nepal. Strip:A defence against trojan attacks on deep neural networks. In Proceedings of the 35th Annual ComputerSecurity Applications Conference, pp. 113125, 2019.",
  "Tianyu Gu, Brendan Dolan-Gavitt, and Siddharth Garg. Badnets: Identifying vulnerabilities in the machinelearning model supply chain. arXiv preprint arXiv:1708.06733, 2017": "Xingshuo Han, Yutong Wu, Qingjie Zhang, Yuan Zhou, Yuan Xu, Han Qiu, Guowen Xu, and Tianwei Zhang.Backdooring multimodal learning. In 2024 IEEE Symposium on Security and Privacy (SP), pp. 3131.IEEE Computer Society, 2023. Jonathan Hayase, Weihao Kong, Raghav Somani, and Sewoong Oh. Spectre: Defending against backdoorattacks using robust statistics. In International Conference on Machine Learning, pp. 41294139. PMLR,2021.",
  "Ya Le and Xuan Yang. Tiny imagenet visual recognition challenge. CS 231N, 7(7):3, 2015": "Shaofeng Li, Minhui Xue, Benjamin Zi Hao Zhao, Haojin Zhu, and Xinpeng Zhang. Invisible backdoorattacks on deep neural networks via steganography and regularization. IEEE Transactions on Dependableand Secure Computing, 18(5):20882105, 2020. Yige Li, Xixiang Lyu, Nodens Koren, Lingjuan Lyu, Bo Li, and Xingjun Ma.Anti-backdoor learning:Training clean models on poisoned data. Advances in Neural Information Processing Systems, 34:1490014912, 2021a.",
  "Tim Pearce, Alexandra Brintrup, and Jun Zhu. Understanding softmax confidence and uncertainty. arXivpreprint arXiv:2106.04972, 2021": "Xiangyu Qi, Tinghao Xie, Yiming Li, Saeed Mahloujifar, and Prateek Mittal. Revisiting the assumption oflatent separability for backdoor defenses. In The eleventh international conference on learning represen-tations, 2022. Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, AndrejKarpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet Large ScaleVisual Recognition Challenge. International Journal of Computer Vision (IJCV), 115(3):211252, 2015.doi: 10.1007/s11263-015-0816-y.",
  "Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition.arXiv preprint arXiv:1409.1556, 2014": "Hossein Souri, Liam Fowl, Rama Chellappa, Micah Goldblum, and Tom Goldstein. Sleeper agent: Scalablehidden trigger backdoors for neural networks trained from scratch.Advances in Neural InformationProcessing Systems, 35:1916519178, 2022. Di Tang, XiaoFeng Wang, Haixu Tang, and Kehuan Zhang. Demon in the variant: Statistical analysis of{DNNs} for robust backdoor contamination detection. In 30th USENIX Security Symposium (USENIXSecurity 21), pp. 15411558, 2021. Mariya Toneva, Alessandro Sordoni, Remi Tachet des Combes, Adam Trischler, Yoshua Bengio, and Ge-offrey J Gordon. An empirical study of example forgetting during deep neural network learning. arXivpreprint arXiv:1812.05159, 2018.",
  "Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine learningresearch, 9(11), 2008": "Bolun Wang, Yuanshun Yao, Shawn Shan, Huiying Li, Bimal Viswanath, Haitao Zheng, and Ben Y Zhao.Neural cleanse: Identifying and mitigating backdoor attacks in neural networks. In 2019 IEEE Symposiumon Security and Privacy (SP), pp. 707723. IEEE, 2019. Baoyuan Wu, Hongrui Chen, Mingda Zhang, Zihao Zhu, Shaokui Wei, Danni Yuan, and Chao Shen. Back-doorbench: A comprehensive benchmark of backdoor learning. Advances in Neural Information ProcessingSystems, 35:1054610559, 2022.",
  "r2 1[x 22 r]": "Both classes have n samples. Assume x C1, and a trigger is added to x such that x = x + t/t2 := x + a.Then define the poisoned data as C1 = C1/{x} and C2 = C1 {x}. Then we train a backdoored SVM onthe poisoned data. The following theorem provides estimations for Mahalanobis distance which serves as theindicator of outliers, and success rate.",
  "As shown in a (for Random) and 7b (for CBS), for a given sample x (red point), x = x + t": "t2 := x + a(blue point), where T1 t 0. Since C2 is not changed, the backdoored decision boundary (the bold black line)is determined by x and C1. Specifically, the decision boundary is determined by x and center 1 Connectthe center of C1 with x and we obtain an interaction point on C1, which is 1 + rx1",
  "where we assume this decision boundary is not overlapped with C2. During the inference, triggers will beadded to samples in C1, which means that the circle of C1 will shift by t": "t2 (denoted as C1) as shown ina and 7b, then the yellow area will be misclassified as C2. Thus the success rate without any defensesis determined by the area of the yellow area. Since the circle of C1 is fixed, we only need to compare thedistance from the center of C1 to the backdoored decision boundary, which is the bold green line in aand 7b. Notice that 1 x is orthogonal to the decision boundary defined in Eq.6, thus the length of thegreen bold line is the length of c1 c1 in the direction of 1 x where c1 is the center of C1, thus the distance",
  "(b) CBS": ": Illustrating figures for SVM under Random and CBS. The red point is a sample x from C1, and theblue one is the triggered sample x. The grey dashed line and black bold line represent the decision boundaryof clean and backdoored SVM respectively. We are interested in the Mahalanobis distance between x andthe target class C2. The yellow area is in proportion to the success rate and the length of the green boldline is positively correlated with the area of the yellow part. It is obvious that CBShas smaller Mahalanobisdistance and smaller area of yellow. Proof of Theorem 4.5. Denote F as the set of all linear functions f which can separate C1 and C2, and definex as the point of tangency of the common tangent line to two clusters and x C1, as shown in .",
  "Note that all the above sets have no overlap with each other, and Figure": "Denote |C| as the area of a region C, then it is easy to see that for all C {Ci,0, Ci,j,k}, |C| = ((log n)2/n).Suppose there are n samples in C1, then since all examples are i.i.d. sampled uniformly from C1, we havefor some constant c, for all C {Ci,0, , Ci,j,k},",
  "Implementations for samplings": "We implement Random with a uniform distribution on Dtr. We implement CBS according to Algorithm 1in the main paper, and the surrogate model is trained via SGD for 60 epochs with an initial learning rateof 0.01 and decreases by 0.1 at epochs 30,50. We implement FUS according to its original settings, i.e. 10overall iterations and 60 epochs for updating the surrogate model in each iteration, and the surrogate modelis pre-trained the same as in CBS.",
  "Type I attacks:": "BadNet (Gu et al., 2017). BadNet is the first work exploring the backdoor attacks, and it attaches asmall patch to the sample to create the poisoned training set. Then this training set is used to train abackdoor model. We implement it based on the code of work (Qi et al., 2022) and following the defaultsetting. Blend (Chen et al., 2017). Blend incorporates the image blending technique, and blends the selectedimage with a pre-specified trigger pattern that has the same size as the original image. We implement thisattack based on the code of work (Qi et al., 2022), and following the default setting, i.e. mixing ratio = 0.2. Adaptive backdoor (Qi et al., 2022). This method leverages regularization samples to weaken the rela-tionship between triggers and the target label and achieve better stealthiness. We implement two versions ofthe method: Adaptive-blend and Adaptive-patch. During the implementation, we consider the conservatismratio of = 0.5 and mixing ratio = 0.2 for adaptive-blend; conservatism ratio = 2/3 and 4 patches forAdaptive-patch.",
  "Type II attacks:": "Hidden-trigger (Saha et al., 2020). This attacking method first attaches the trigger to a sample andthen searches for an imperceptible perturbation that achieves a similar model output (measured by l2 norm)as the triggered sample. We follow the original settings in work (Saha et al., 2020), i.e. placing the triggerat the right corner of the image, setting the budget size as 16/255, optimizing the perturbation for 10000iterations with a learning rate of 0.01 and decay by 0.95 for every 2000 iterations. Label-consistent (LC) (Turner et al., 2019). This attacking method leverages GAN or adversarialexamples to create the poisoned image without changing the label. We implement the one with adversarialexamples bounded by l2 norm. We set the budget size as 600 to achieve a higher success rate.",
  "Type III attacks:": "Lira (Doan et al., 2021b). This method iteratively learns the model parameters and a trigger generator.Once the trigger generator is trained, attackers will finetune the model on poisoned samples attached withtriggers generated by the generator, and release the backdoored model to the public. Our implementationis based on the Benchmark (Wu et al., 2022). WaNet (Nguyen & Tran, 2021). WaNet incorporates the image warping technique to inject invisibletriggers into the selected image. To improve the poisoning effect, they introduce a special training mode",
  "Spectral Signature (SS) (Tran et al., 2018). This defense detects poisoned samples with strongerspectral signatures in the learned representations. We remove 1.5 p of samples in each class": "Activation Clustering (AC) (Chen et al., 2018). This defense is based on the clustering of activationsof the last hidden neural network layer, for which clean samples and poisoned samples form distinct clusters.We remove clusters with sizes smaller than 35% for each class. SCAn (Tang et al., 2021). This defense leverages an EM algorithm to decompose an image into itsidentity part and variation part, and a detection score is constructed by analyzing the distribution of thevariation. SPECTRE (Hayase et al., 2021). This method proposes a novel defense algorithm using robust covari-ance estimation to amplify the spectral signature of corrupted data. We also remove 1.5 p of samples ineach class.",
  "Fine Pruning (FP) (Liu et al., 2018). This is a model-pruning-based backdoor defense that eliminatesa models backdoor by pruning these dormant neurons until a certain clean accuracy drops": "Neural Cleanse (NC) (Wang et al., 2019). This is a trigger-inversion method that restores triggers byoptimizing the input domain. It is based on the intuition that the norm of reversed triggers from poisonedsamples will be much smaller than clean samples. Anti-Backdoor Learning (ABL) (Li et al., 2021a). This defense utilizes local gradient ascent to isolate1% suspected training samples with the smallest losses and leverage unlearning techniques to train a cleansedmodel on poisoned data.",
  "In this section, we provide detailed algorithms for CBS and its application on Blend (Chen et al., 2017)": "As shown in Algorithm 1 in the main paper, CBS first pretrain a surrogate model f(; ) on the clean trainingset Dtr for E epochs; then f(; ) is used to estimate the confidence score for every sample; for a given targetyt, samples satisfying |sc(f(xi; ))yi sc(f(xi; ))yt| are selected as the poison sample set U. As shown in Algorithm 2, the poison sample set U is first selected via Algorithm 1; then for each sample inU, a trigger is blended to this sample with a mixing ratio via x = t + (1 ) x and generate thepoisoned training set Dp.",
  "Discussion of computation overhead of CBS": "Since CBS selects samples based on confidence score of a clean model, it is necessary to analyze its computationoverhead compared with simple random selection. Suppose the sample size is N, poison rate is r. For therandom selection, we need to randomly select rN samples from the dataset and the time complexity isO(rN). For the proposed method, we need to first compute the confidence score for each sample and thenselect those smaller than . Suppose the average time for computing the confidence score for one sample ist, and the time complexity for the proposed method is O(N(t + 1)). Therefore, both complexities are in thelinear order of sample size N. When the inference time is shorter, CBS is more efficient.",
  "We provide more details for the discussions in .1": "Additional figures.To supplement , we present the distributions of distance do for differentselections of samples. In specific, in , we present the kernel density curve of each category for aclearer comparison, and this is why there are curves in the negative region. In the real data of distances, allvalues are non-negative. The original histograms can be found in a and b. Formal definition of do and dt.We also provide the formal definition of distances do and dt forclearer understanding.Assume sample x comes from the class y and the target class is yt, model fmapping from input space to the latent space, classifier g mapping from latent space to label space.We then can define the center of samples in class y and yt in the latent space as:f(x)y,mean :=1",
  "(x,y)Dtr f(x). A larger distance means thepoisoned sample is separated from the target class and therefore easy to detect, and vice versa": "In the proposed method, since we take the model output before the last linear layer as the latent representa-tion (for example ResNet18), selection based on the confidence score is equivalent to that directly based onthe latent representation. To provide some more details, we visualize two classes in Cifar10. In the Figures10a 10b, there are two clusters with different colors. The blue one is the target class, the green one is theoriginal class, and the red points are the poisoned samples. These verify our statements. In the randomselection case, since the poisoned samples are far away from the blue cluster, with the label as the blue class,they are more likely to be identified by the defender. For boundary selection, the poisoned samples are lesslikely to be detected. Besides the formal definition above, we would like to clarify that from a more general perspective, thestealthiness\" of an attack is to what extent the attack can be defended.Since defenses are based ondifferent insights, for instance, Spectral Signature (SS) is based on outlier-detection while anti-backdoor(ABL) is based on the fact that poisoned sample is learned faster than clean data, it is hard to find a formaldefinition for stealthiness\" accommodating all defenses. In our experiments, we test different defenses andthe reduction of success rate measure the stealthiness\" (smaller reduction means better resistance againstdefenses thus more stealthiness).",
  "Due to the page limit of the main text, we present details and comprehensive experimental results in thissection": "Attacks & Defenses. We consider 3 Representative attacks in this categoryLira (Doan et al., 2021b)which involves a stealthy backdoor transformation function and iteratively updates triggers and model pa-rameters; WaNet (Nguyen & Tran, 2021) which applies the image warping technique to make triggers morestealthy; Wasserstein Backdoor (WB) (Doan et al., 2021a) which directly minimizes the distance betweenpoisoned and clean representations. Note that Type III attacks allow the attackers to take control of thetraining process. Though our threat model does not require this additional capability of attackers, we followthis assumption when implementing these attacks. Therefore, we directly select samples based on ResNet18and VGG16 rather than using ResNet18 as a surrogate model. We conduct 5 representative defenses that",
  "In this section, we provide additional experimental results": "Type I attacks. We include additional defenses: Activation Clustering (AC) (Chen et al., 2018), SCAn(Tang et al., 2021), SPECTRE (Hayase et al., 2021), Fine Pruning (FP) (Liu et al., 2018). We also conductexperiments on Cifar100. Results of Type I attacks on Cifar10 and Cifar100 datasets are shown in and 7 respectively. CBS has similar behavior on Cifar100improve the resistance against various defenseswhile slightly decrease ASR without defenses. Type II attacks. We also include additional defenses: Spectral Signature (SS) (Tran et al., 2018). Theresults of all defenses on model ResNet18, VGG16 and datasets Cifar10, Cifar100 are presented in .Detailed analysis is shown in .3 in the main paper. Clean accuracy.We also report the accuracy of clean samples (without triggers) on the backdooredmodels of all three sampling methods.Results are shown in .It is clear that CBS can reach abetter clean accuracy than other selections, which makes it more stealthy.To intuitively explain this, sincethe poisoned samples are around the original decision boundary, they do not severely change the decisionboundary, thus preserving a high clean accuracy when no attack is injected in the testing data. Tiny-ImageNet dataset.Except for Cifar10 and Cifar100, we also conduct experiments on a largerdataset Tiny-ImageNet (Le & Yang, 2015).We also train model ResNet18 for 60 epochs to select thepoisoned samples for each sampling method. Results are shown in . These results demonstrate thatCBSis applicable to larger datasets and consistently improves the stealthiness of different attacks. ImageNet-1k dataset.We also consider a large-scale dataset, ImageNet-1k (Russakovsky et al., 2015),which has 1000 object classes and more than 1,200,000 images. We test representative backbone attacks fromall three types and representative defenses. Since FUS is time-consuming and not eligible for large datasets,we only compare CBS with the random baseline. Results are shown in . CBS is still effective evenon such a big dataset.",
  "Additional discussion on effectiveness-stealthiness tradeoff": "As discussed in Theorem 4.2 and Remark 4.8, there exists an effectiveness-stealthiness tradeoff for attacks.In general, when selecting samples closer to the decision boundary, it is harder to detect but will sacrificesome poisoning effect when facing no defenses. This is also verified by our empirical results in , and , where CBS has the worst performance when there is no defense.",
  "Ablation study on poisoning rate": "We conduct additional experiments on Cifar10 dataset, ResNet18 model and backbone attack BadNet totest how different poisoning rate affect the proposed method. We report the success rate against undefendedmodels and three representative defenses in . According to the results, poisoning more samplescan increase the success rate against undefended models, and slightly increase the poisoning effect againstdefenses. We notice that while the poisoning rate is increasing, the improvement of the poisoning effectagainst defenses becomes minor.This can be because when the number of poisoned samples increases,samples that are farther from the boundary are included. These samples can achieve better performancewhen there is no defense but are easy to detect. This also highlights the importance of a proper sampleselection strategy in backdoor attacks.",
  "WB77.577.378.273.573.674.3": "compare ResNet18 and VGG16, and these two models have similar clean accuracy on the Cifar10 dataset(95.5% and 93.6% respectively). We fix class 1 as the target class and select 100 samples using CBSfromboth models. We notice that 67% of selected samples are the same, which is quite a large proportion. Thiscan explain why our method can transfer well from ResNet18 to VGG16.",
  "Discussion on the difference between raw input space and latent space": "CBS is based on the observation that poisoned samples can be separate from the target class in the latentspace, which raises a question of whether detecting outliers in the raw space can defend against such anattack. To investigate, we compare the distance between each sample and the center of its class in both",
  "No defense91.893.696.798.4SS18.720.223.524.2ABL27.431.333.134.8NC23.724.628.430.6": "raw input space and latent space, i.e. draw(x) = x xy,mean2 and dlatent(x) = f(x) f(x)y,mean2where f(x) denote the latent representation w.r.t the model f. We use the backdoored ResNet18 model andCifar10 dataset. Then we plot two distances in the same figure to check their relationship, shown in . Based on the result, there is no obvious relationship between them, and the outliers in the raw spacecan have a very small distance to the center in the latent space. This indicates that when filtering out theoutliers based on distances in the raw space, the poisoned samples can still be preserved.",
  "Blend84.2": "As shown in , the success rate after the naive defense does not drop much, which suggests that thisdefense can not effectively defend the proposed attack. Due to the difference between the raw input spaceand the latent space, the outliers in the raw input space may not be the actual poisoned samples."
}