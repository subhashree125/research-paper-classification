{
  "Abstract": "Fine-tuning the pre-trained model with active learning holds promise for reducing annotationcosts. However, this combination introduces significant computational costs, particularlywith the growing scale of pre-trained models. Recent research has proposed proxy-basedactive learning, which pre-computes features to reduce computational costs. Yet, this ap-proach often incurs a significant loss in active learning performance, sometimes outweighingthe computational cost savings. This paper demonstrates that not all sample selection dif-ferences result in performance degradation. Furthermore, we show that suitable trainingmethods can mitigate the decline of active learning performance caused by certain selec-tion discrepancies. Building upon detailed analysis, we propose a novel method, alignedselection via proxy, which improves proxy-based active learning performance by updatingpre-computed features and selecting a proper training method. Extensive experiments vali-date that our method improves the total cost of efficient active learning while maintainingcomputational efficiency. The code is available at",
  "Introduction": "Training effective deep neural networks typically requires large-scale data (Deng et al., 2009; He et al., 2016;Liu et al., 2021). However, the high cost of acquiring data, especially annotated data, poses a significantchallenge for practitioners (Budd et al., 2021; Norouzzadeh et al., 2021). Active learning and the pre-trainingfine-tuning paradigm are widely adopted strategies to address this challenge. Active learning reduces thedemand for extensive annotation by iteratively selecting and labeling the most informative samples (Renet al., 2021). Additionally, the pre-training fine-tuning method leverages large-scale unsupervised pre-trainingto create a powerful foundational model, enabling exceptional performance on downstream tasks with onlya limited amount of labeled data (Chen et al., 2020a; Grill et al., 2020; Caron et al., 2021; He et al., 2022).To further minimize labels demands, researchers have turned their attention to active fine-tuning, whichfine-tunes the pre-training model with labeled samples selected by active learning strategies (Xie et al., 2023;Chan et al., 2021; Bengar et al., 2021). However, as the scale of pre-training models continues to grow, thesample selection time of active learning, which often is overlooked in traditional active learning, becomes achallenge (Coleman et al., 2019). As illustrated in fig. 1, larger pre-training models lead to higher accuracy,but also significantly increase the time required for active learning sample selection. To address this issue, recent research has introduced an efficient active learning framework known as Selectionvia Proxy based on pre-trained features (SVPp) (Zhang et al., 2024). SVPp begins by forwarding the entire",
  "Published in Transactions on Machine Learning Research (11/2024)": "Number of Labels Active Learning Gain (%) Standard (w/o replace)Proxy (w/o replace)Replace with A1 Replace with A2Replace with B1Replace with B2 : The impact of different regions on SVPp active learning performance. Replacing samples selectedby the proxy model with those from regions A1, A2, B1, B2. The active learning gain refers to the differencein accuracy between active learning and the random baseline on CIFAR-10. 0.4k 0.6k 0.8k 1.0k 1.2k 1.4k 1.6k 1.8k 2.0k 2.5k 3.0k 3.5k 4.0k 4.5k 5.0k 5.5k 6.0k ALL Number of Labels Proportion of Samples by Difficulty (%)",
  "Related Work": "Active learning is a technique that minimizes the number of labels needed for model training by selectivelyrequesting annotations for samples. Numerous active learning strategies have been proposed, primarily in-cluding uncertainty sampling (Lewis & Catlett, 1994; Scheffer et al., 2001; Gal et al., 2017; Kirsch et al.,2019; Woo, 2022), diversity sampling (feature space coverage) (Sener & Savarese, 2018; Mahmood et al.,2022), their combinations (Yang et al., 2017; Ash et al., 2020; Shui et al., 2020), and some learning-basedapproaches (Sinha et al., 2019; Yoo & Kweon, 2019; Tran et al., 2019). With the advancement of large-scaleunsupervised pre-training and fine-tuning, an increasing number of researchers have turned their attentionto active fine-tuning, which fine-tuning the pre-trained model with samples selected by active learning strate-gies. Given the success of the pre-training fine-tuning mode, most works of active fine-tuning have focusedon developing active learning strategies that can work with extremely limited annotations (Hacohen et al.,2022; Yehuda et al., 2022; Wen et al., 2023). Furthermore, researchers have validated the performance of ex-isting active learning methods when applied to pre-training fine-tuning mode, demonstrating that traditionalmargin sampling exhibits strong performance (Zhang et al., 2024; Emam et al., 2021). Efficient active learning. Most active learning methods operate in a multi-round mode, iteratively trainingmodels and selecting samples to label. This mode needs significant training time and costs. In the contextof active fine-tuning, this phenomenon becomes more pronounced, as larger models are better equipped toeffectively leverage data in unsupervised pre-training (Chen et al., 2020b; Oquab et al., 2023). Increasing the active learning batch size, which refers to the number of samples selected per active learningiteration, is one solution to enhance computational efficiency (Citovsky et al., 2021; Zhang et al., 2022).However, it weakens the performance of active learning, and there remains the considerable computationalcost associated with training the entire network multiple times. Therefore, recent research has exploredsingle-shot active learning based on pre-trained features (Xie et al., 2023).However, as the number oflabels increases, the effectiveness of this method gradually diminishes in comparison to other standard activelearning strategies. Another solution to boost active learning efficiency is to use less computationally intensive models or trainingmethods as proxy tasks for sample selection, known as Selection via Proxy (SVP) (Coleman et al., 2019).Typical proxy tasks include reducing model complexity, such as using models like ResNet-8 or ResNet-14,early stopping, or training linear classifiers or MLPs based on pre-trained features (Zhang et al., 2024).However, these proxy tasks often have an impact on active learning performance and may result in savingsin computation costs not outweighing the increase in annotation costs. As a result, practitioners are oftenfaced with a trade-off between computational efficiency and the overall cost.",
  "Selection via Proxy based on Pre-computed Features": "We briefly review the Selection via Proxy framework based on pre-computed features, SVPp, as shown infig. 3. We denote the labeled set as L, the unlabeled set as U. The pre-trained neural network backbone isdefined as f(; p) : X Rd, where p represents weights of the pre-trained model, X is the data space andRd is feature space. And we define the predictor as h(; h) : Rd Rc, where h is weights of the predictor",
  "LogME-PED": "LogME-PED is an efficient metric for selecting pre-trained models, which outputs a score reflecting theperformance of fine-tuned models. LogME uses evidence as a metric to assess the compatibility between agiven pre-trained feature and the labels of the training samples (You et al., 2021), as shown in equation 1,where F denotes the pre-trained features of the labeled set and w denotes the parameters of the linearclassifier. For specific calculation methods, refer to (You et al., 2021).",
  "p(y|F) =p(w)p(y|w, F)dw(1)": "PED is a physically inspired model, that simulates feature learning dynamics through multiple iterations (Liet al., 2023). The approach draws an analogy between the optimization objective during fine-tuning and theconcept of potential energy in physics. In fine-tuning, adjusting the model parameters forces the featuresof different classes to be distinguished, akin to the movement of objects influenced by forces in physics.Following this intuition, PED models the features of different classes as separate small balls, where the meanand standard deviation of each classs features correspond to the position and radius of the ball, respectively.The force between balls is proportional to their overlapping length. In this physical model, the positions andforces of the objects are updated iteratively until convergence. Finally, the updated features are regardedas the features after fine-tuning the pre-trained model. Then, LogME is computed based on these updatedfeatures to assess the performance of the fine-tuned model.",
  "Observation and Analysis": "Since practitioners utilizing proxy models for efficient active learning aim to reduce sample selection timewhile minimizing the loss of active learning gains (between standard active learning based on fine-tunedmodels and SVPp), the exact overlap between samples selected by the proxy model and those selected bythe fine-tuned model is not their primary concern. Therefore, in this section, we first investigate whichdifferences in sample selection lead to the degradation of SVPp active learning performance.",
  "Impact of Sample Selection Differences on SVPp Performance": "We categorize the samples selected by both the proxy model and the fine-tuned model into different regionsbased on whether the models can correctly predict the samples and the active learning metrics (such asmargin (Scheffer et al., 2001), entropy (Lewis & Catlett, 1994), etc.). As shown in fig. 4, the x and y axesrepresent whether the proxy model and the fine-tuned model can correctly predict the samples, respectively.The closer a sample is to the origin, the more likely it is to be selected in active learning. For uncertainty-based active learning strategies (e.g., margin, entropy, confidence (Wang & Shang, 2014)) and strategiescombining uncertainty and diversity (e.g., BADGE (Ash et al., 2020)), a smaller distance from the originindicates higher uncertainty, such as a smaller margin predicted by the model. For distance-based activelearning strategies (e.g., coreset (Sener & Savarese, 2018)), a smaller distance from the origin indicates agreater distance between the sample and the labeled set. Most effective active learning strategies for fine-tuning pre-trained models are uncertainty-based or combine uncertainty with diversity (where the latterextends the former by selecting diverse samples from the uncertainty candidates) (Zhang et al., 2024; Emamet al., 2021), this paper will use uncertainty-based active learning as an example for analysis. Standard active learning based on the fine-tuned model selects samples from regions O, A, and B, whileefficient active learning based on the proxy model selects samples from regions O, C, and D. In other words,compared to the fine-tuned model, the proxy model misses samples from regions A1, A2, B1, and B2 whileadditionally selecting samples from regions C and D. : Sample Selection Discrepancy: Proxy vs. Fine-tuned Models. For instance, in uncertainty-basedactive learning strategies, the two axes represent the confidence of predictions for the proxy model andthe fine-tuned model, with the positive half-axis indicating correct predictions and the negative half-axisindicating incorrect predictions. AL strategy selects samples with the lowest prediction confidence, meaningthe proxy model chooses samples from regions O, C, and D, while the fine-tuned model selects samples fromregions O, A, and B. Depending on whether the proxy model correctly predicts the samples, regions A andB are further divided into subregions A1, A2, B1, and B2. Both the proxy model and the fine-tuned modelconfidently predict the remaining white region, hence they do not select samples from it. Empirical EvidenceWe conducted replacement experiments to demonstrate the impact of missing sam-ples from regions A1, A2, B1, and B2 on SVPp active learning performance. In this section, we used one ofthe state-of-the-art strategies, margin sampling, with fine-tuned pre-trained models. A total of 400k sampleswere selected over 16 rounds, with the first 10k samples selected randomly. The experiments were conductedon ImageNet, with detailed settings provided in sec. 6.1. Specifically, during each round of sample selection, we replaced the tail samples selected by the proxy model(i.e., those with the largest margins) with samples from regions A1, A2, B1, and B2. As shown in fig. 5, weobserved that not all differences in sample selection caused performance degradation in SVPp active learning.Replacing samples with those from regions A2 and B2 improved SVPp active learning performance, replacingsamples from region A1 had no effect, while replacing samples from region B1 decreased performance. Based",
  "on these observations, to improve the performance of SVPp active learning, we should focus on the sampleselection differences from regions A2 and B2": "To investigate why samples from regions A1 and B1 fail to improve SVPp performance, we analyzed thedifficulty of samples across different regions. Recent studies suggest that hard-to-learn samples are likelyto undermine active learning performance (Karamcheti et al., 2021). A1 and B1 regions, compared to A2and B2, likely contain a higher proportion of difficult samples.This is because the fine-tuned modelspredictions on these samples show high uncertainty, while the proxy models predictions on them are wrong.To validate this hypothesis, we utilized Dataset Maps (Swayamdipta et al., 2020) to assess sample difficulty.Dataset Maps depict sample learnability by analyzing training dynamics from models trained on the entiredataset, using metrics such as average confidence for the correct class and prediction accuracy across trainingepochs to quantify difficulty. FollowingKaramcheti et al. (2021), we categorized sample difficulty basedon mean confidence levels: very hard (<0.25), hard (>=0.25 and <0.5), medium (>=0.5 and <0.75), andeasy (>=0.75). As illustrated in fig. 6, regions A1 and B1 indeed contain a higher proportion of very hardsamples. 20k 30k 40k 50k 60k 70k 80k 90k 100k150k200k250k300k350k400k All Number of Labels Proportion of Samples by Difficulty (%)",
  "Role of Region A2 and B2 in Improving SVPp Performance": "Those missed samples (Region A2, B2) that can improve SVPp performance share a common characteristic:the proxy model can confidently predict them correctly, while the fine-tuned model shows high uncertainty inits predictions. This indicates that these samples are well-distinguished using pre-trained features, whereasthey are harder to differentiate in the fine-tuned features. Including these samples in the next round of activelearning will help the fine-tuned model distinguish them. A question arises: are the pre-trained featuremanifold and the fine-tuned feature manifold (training with samples from regions A2 and B2)similar?",
  "Discussions": "The previous section discussed that not all differences in sample selection between SVPp and standard activelearning result in performance differences (only those from regions A2 and B2 do). Moreover, when usingLP-FT to train the model, the sample selection differences from regions A2 and B2 do not cause a declinein SVPp active learning performance. Next, we discuss the limitations of the above analysis.",
  "(b) Active Learning Gain": ": Observations on region C and D: (a) the proportion of region C and D, (b) comparison of activelearning gain when selecting only samples from region C or D. The active learning gain refers to the differencein accuracy between active learning and the random baseline. As shown in fig. 9a, with more samples selected during active learning, the proportion of samples fromregion C selected by the proxy model increases, while the proportion from region D decreases. This alignswith intuition: as the number of annotations increases, the features of the fine-tuned model become betterat distinguishing between sample categories than those of the proxy model, increasing correctly predictedsamples (samples from region C). Additionally, as shown in fig. 9b, selecting samples from region C resultsin an active learning gain of less than 0, meaning it performs worse than random selection. These twoobservations imply that as more samples are selected during active learning, the proxy model selects moresamples from region C, causing its active learning performance to decline. As the proxy models activelearning performance declines, it may lead to all missed regions (A1, A2, B1, B2) contributing to performancedifferences between SVPp and standard active learning. In summary, when LP-FT training achieves performance comparable to fine-tuning and the proportion ofsamples from region C is small, using LP-FT can prevent the decline in active learning gains caused bySVPp. The next section discusses how to adjust proxy model-based active learning to build an efficientactive learning framework when these conditions are not met.",
  "Method": "This section first addresses the issue of the increasing proportion of region C samples, which makes LP-FT insufficient to prevent the decline in SVPp active learning performance. Region C refers to samplesthat the fine-tuned model can predict correctly with high confidence, while the proxy model struggles todifferentiate them (high uncertainty). This indicates that the fine-tuned models features are more suitablefor distinguishing these samples. To address this, we recommend fine-tuning the pre-trained model as thenumber of region C samples increases. Updating the pre-computed features enhances the proxy modelsdiscriminative capacity, thereby reducing the proportion of region C samples. Additionally, when updatingpre-computed features, we switch the model training method from LP-FT to fine-tuning. Continuing withLP-FT when fine-tuned features outperform pre-trained features can lead to suboptimal results by hinderingnecessary modifications to the pre-trained model.",
  "Results": "Our method is validated on four datasets: ImageNet-1k (Deng et al., 2009), CIFAR-10 (Krizhevsky et al.,2009), CIFAR-100 (Krizhevsky et al., 2009) and Oxford-IIIT Pet dataset (Parkhi et al., 2012) with fivetypical active learning strategies. The experiment setup is clarified in sec. 6.1. We propose the new efficientactive learning evaluation metric in sec. 6.2. The results are shown in sec. 6.3 and sec. 6.4. The ablationstudy is shown in sec. 6.5. The detailed study of the impact of the position of updating features is shownin sec. 6.6. Additionally, given the efficiency of our framework, decreasing the amount of sample selected ineach active learning iteration to alleviate the redundant problem of batch active learning strategy is feasible.The results are shown in appendix F.",
  "Experiment Setup": "Active learning strategies. To evaluate the compatibility of our proposed ASVP with existing activelearning strategies, four typical active learning strategies are included: (1) Margin: uncertainty-based sam-pling, one of the SOTA in the context of fine-tuning pre-trained models (Scheffer et al., 2001), (2) Confidence:uncertainty-based sampling (Wang & Shang, 2014), (3) Coreset: diversity-based sampling (Sener & Savarese,2018), (4) BADGE: combination of uncertainty and diversity (Ash et al., 2020), (5) ActiveFT(al), featurespace coverage with features from fine-tuned model (Xie et al., 2023). Baseline. (1) Standard active learning: training the whole model with fine-tuning (FT) or linear-probingthen fine-tuning (LP-FT) (Kumar et al., 2022) in each active iteration, (2) SVPp (Zhang et al., 2024):selecting samples via MLP proxy model based on pre-trained features and training the final model with FT,(3) ActiveFT(pre) (Xie et al., 2023): a single-shot active learning strategy that selects all sample in singleiteration based on pre-trained features. Implementations. We utilized the ResNet-50 model (He et al., 2016), pre-trained on ImageNet using theBYOL-EMAN (Cai et al., 2021) method in all our experiments. The training hyper-parameters of the twomodel training approaches, fine-tune (FT) and LP-FT, are provided in appendix A. For the baseline method(SVPp) and our method (ASVP), we employed 2-layer MLPs as proxy models, where its architecture isLinear + BatchNorm + ReLU + Linear. The hidden layer width matched the input feature dimensions.We performed multiple active learning iterations until the model performance approached that of trainingon the entire dataset (upper bound).Specifically, for the ImageNet, CIFAR-10, CIFAR-100, and Petsdatasets, we conducted 16, 18, 22, and 10 active learning iterations, respectively. The query step was setto 200 samples per iteration for the Pets dataset.For ImageNet, CIFAR-10, and CIFAR-100, we usedvarying query steps: for ImageNet, we queried 10,000 samples per iteration until reaching 100,000 samples,after which we queried 50,000 samples per iteration. For CIFAR-10, we queried 200 samples per iterationuntil reaching 2,000 samples, then increased to 500 samples per iteration. For CIFAR-100, we queried 400samples per iteration until reaching 6,000 labeled samples, then switched to 2,000 samples per iteration. Thedifference in LogME-PED scores between consecutive active learning iterations is set as 1 for the thresholdof updating pre-computed features.",
  "Metric": "Efficient active learning methods often exhibit a lower accuracy compared to standard active learning ap-proaches. Understanding the additional annotation costs needed to compensate for the accuracy gap causedby efficient active learning allows practitioners to weigh the computational cost savings against increasedlabeling expenses, aiding their decision to utilize these methods. To this end, we propose a new evaluation metric: equivalent saving amount. Let N1 represent the numberof samples selected by the active learning strategy, and let A be the accuracy achieved by a model trainedwith these N1 labeled samples. We utilize interpolation to estimate the number of samples, N2, required toachieve accuracy A for a random baseline. We refer to N2 as the equivalent number of non-AL labels inthe subsequent discussions. The number of saved samples is N2 N1 and sample saving ratio is N2N1 N2.Subsequently, we compute the average sample saving ratio across all iterations of active learning to assessthe performance of the active learning strategy. Furthermore, the overall cost of active learning, includingannotation and training costs, equals N1 Pl + Ctr, where Pl denotes the unit price of annotation and Ctrrepresents the training cost of active learning.",
  "Sample Saving Ratio and Overall Cost": "The average sample savings ratios are presented in table 1, with each experimental setting repeated threetimes to report both the average and standard deviation. The table also illustrates the total cost requiredfor active learning to reach the accuracy achieved by the random baseline at the last active learning iteration(i.e., the accuracy achieved when randomly selecting 400k, 6000, 20000, and 2000 samples from ImageNet,CIFAR-10, CIFAR-100, and Pets datasets, respectively). The training cost is assessed based on AWS EC2P3 instances (Zhang et al., 2024), while the annotation cost is evaluated using AWS Mechanical Turk,",
  ": The accuracy of the standard method, SVPp, and our method ASVP on (a) ImageNet, (b)CIFAR-10, (C) CIFAR-100 and (d) Pets using margin sampling": "Our method consistently outperforms the efficient active learning method, SVPp, in terms of sample savingratio and overall cost reduction. It surpasses the single-shot active learning strategy, ActiveFT(pre), whencombined with most active learning strategies. In most cases, our approach offers similar or greater overallcost savings compared to standard active learning.This alleviates practitioners concerns about overallcosts when using efficient active learning. Additionally, in standard active learning methods, the lack ofa clear criterion for choosing the proper training method may lead to suboptimal results. Specifically, thestandard active learning with FT outperforms the standard active learning with LP-FT on CIFAR-10 andPets, while on ImageNet and CIFAR-100, the standard (LP-FT) method demonstrates better performance.",
  "In contrast, our approach, by switching training methods, ensures a relatively favorable performance inpractical applications": "Compared to the single-shot active learning method ActiveFT(pre), our approach is compatible with variousexisting active learning strategies and exhibits lower dependence on the quality of pre-trained features.The reliance of ActiveFT(pre) on the quality of pre-trained features leads to suboptimal results. This isparticularly evident in datasets like Pets, where the pre-trained feature quality is low.",
  "Computational Time": "We define the average training time for the fine-tuned and the proxy models in each active learning iterationas Ttr,full and Ttr,proxy, respectively. The time for processing all unlabeled samples through the whole pre-trained model and the proxy model to compute active learning metrics and select samples is denoted as Tf,fulland Tf,proxy, respectively. The time spent on forwarding all samples through the model to record featuresis represented by Tpre. The number of active learning iteration is given by Nal. The total active learningtime for standard active learning, Ts,full, is Nal (Ttr,full + Tf,full). The total time for SVPp, Ts,svpp, isNal(Ttr,proxy+Tf,proxy)+Tpre. For ASVP, the initial pre-computed features are obtained from a pre-trainedmodel. When updating these pre-computed features is necessary, the model is fine-tuned on labeled data, andthe pre-computed features are refreshed accordingly. Let Npre denote the number of times the ASVP featuresare pre-computed, then, the time required for pre-computing features is NpreTpre+(Npre1)Ttr,full. Thetotal time required by ASVP, denoted as Ts,asvp, is Nal(Ttr,proxy+Tf,proxy)+NpreTpre+(Npre1)Ttr,full. The computation time of standard active learning, SVPp, and ASVP on ImageNet is presented in table 2.The results on other datasets are shown in appendix B. The time for ASVP mainly arises from updatingpre-computed features, as the computation time for the proxy model, an MLP, is significantly lower thanthat of the fine-tuned model. This means that increasing the number of active learning iterations Nal has amarginal impact on ASVPs total time. Therefore, this opens up the potential to improve the diversity ofselected samples by reducing the number of samples chosen in each active learning iteration.",
  "Ablation Study": "Our ablation study was conducted on CIFAR-10, CIFAR-100, and Pets, using 10, 15, and 10 active learningiterations, respectively, with each iteration querying 200, 400, and 200 samples. All other settings followthose described in sec. 6.1. The results with margin sampling are shown in , while additional ablationstudies with other active learning strategies are provided in Appendix C. Across all three datasets, updatingpre-computed features consistently led to a noticeable performance boost. Meanwhile, switching trainingmethods improved performance on all datasets except for the Pets dataset, suggesting that our method maynot precisely determine the optimal point for switching training methods. However, considering the practicalscenario where we cannot pre-determine whether using FT or LP-FT for training is better, our approachstill aids in selecting a preferable training method. : Ablation Study. Comparison of the average sample saving ratio of only updating pre-computedfeatures (training the final model using fine-tuning), solely switching training methods (using pre-trainedfeatures), and the complete ASVP approach (both updating pre-computed features and switching trainingmethods) under different datasets with margin sampling.",
  "Influence from Position of Updating Features": "We examined the impact of the position of updating pre-computed features on CIFAR-10. The averagesavings ratio is presented in table 4. The choice of updating pre-computed features does indeed affect thefinal performance of ASVP. Nevertheless, across the six different positions explored in the experiments,ASVP consistently outperforms the method relying solely on pre-training features (SVPp). Furthermore,our proposed method for determining the update location based on LogME-PED proves to be effective. OnCIFAR-10, this method selects the update of pre-computed features at 600 labels, ranking as the second-bestamong the six experimental positions, as shown in table 4. The positions of updating pre-computed featuresestimated through LogME-PED are outlined in appendix D.",
  "Conclusion": "Active learning on pre-trained models offers label efficiency but requires significant computational time,whereas current efficient active learning methods tend to compromise a notable fraction of active learningperformance, resulting in increased overall costs. To address this challenge, this paper attributes the reducedperformance of SVPp to (1) the absence of samples necessary to preserve fine-tuned features similar to thepre-trained features and (2) an increased selection of redundant samples due to fine-tuned features becomingbetter than pre-trained features as the annotation increases. Building upon this analysis, we propose a simpleand effective method, ASVP, to improve the effectiveness (in terms of accuracy) of efficient active learningwith comparable or marginally increased computational time. Additionally, we introduce the sample savingsratio as a metric to assess the effectiveness of efficient active learning, providing a straightforward measurefor labeling cost savings. Experiments show that our proposed ASVP saves similar or greater total costs inmost cases while maintaining computational efficiency.",
  "Limitations and Future Directions": "Due to limited computational resources, this paper evaluates efficient active learning methods labeling andtraining costs using a ResNet-50 model with a specific pre-training method, BYOL-EMAN (Grill et al., 2020;Cai et al., 2021). Although ResNet-50 serves as a practical choice for our experiments and provides a reliablebaseline, larger models achieve higher accuracy and are often more helpful in practical applications (Zhaiet al., 2022).Existing research shows that larger models exhibit better label efficiency, requiring fewerlabeled samples to reach the same accuracy as smaller models (Chen et al., 2020b). This implies that inactive learning, using larger models may reduce labeling costs while increasing training costs. Therefore,the total cost savings achieved by both standard and efficient active learning methods are closely tied tomodel size. Evaluating the impact of model size on total cost savings, and exploring efficient active learningmethods that can effectively reduce sample selection time and total cost (labeling and training costs) in thecontext of larger models, are important directions for future research. Furthermore, when transferring to downstream datasets (used for active learning), different pre-trainedmodels exhibit varying performance gaps between fine-tuning and linear probing. A smaller gap suggestshigher-quality pre-trained features, making it easier for proxy models based on these features to selectsamples similar to those chosen by fine-tuned models. This means that proxy model-based active learningmay compromise less on performance compared to standard active learning. Therefore, exploring how toselect suitable pre-trained models is another interesting direction for future research. Jordan T. Ash, Chicheng Zhang, Akshay Krishnamurthy, John Langford, and Alekh Agarwal. Deep batchactive learning by diverse, uncertain gradient lower bounds. In International Conference on LearningRepresentations, 2020. Javad Zolfaghari Bengar, Joost van de Weijer, Bartlomiej Twardowski, and Bogdan Raducanu.Reduc-ing label effort: Self-supervised meets active learning. In Proceedings of the IEEE/CVF InternationalConference on Computer Vision, pp. 16311639, 2021.",
  "Samuel Budd, Emma C Robinson, and Bernhard Kainz. A survey on active learning and human-in-the-loopdeep learning for medical image analysis. Medical Image Analysis, 71:102062, 2021": "Zhaowei Cai, Avinash Ravichandran, Subhransu Maji, Charless Fowlkes, Zhuowen Tu, and Stefano Soatto.Exponential moving average normalization for self-supervised and semi-supervised learning. In Proceedingsof the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 194203, 2021. Mathilde Caron, Hugo Touvron, Ishan Misra, Herv Jgou, Julien Mairal, Piotr Bojanowski, and ArmandJoulin.Emerging properties in self-supervised vision transformers.In Proceedings of the IEEE/CVFinternational conference on computer vision, pp. 96509660, 2021.",
  "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. InProceedings of the IEEE conference on computer vision and pattern recognition, pp. 770778, 2016": "Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollr, and Ross Girshick. Masked autoencodersare scalable vision learners. In Proceedings of the IEEE/CVF conference on computer vision and patternrecognition, pp. 1600016009, 2022. Siddharth Karamcheti, Ranjay Krishna, Li Fei-Fei, and Christopher D Manning. Mind your outliers! inves-tigating the negative impact of outliers on active learning for visual question answering. In Proceedingsof the 59th Annual Meeting of the Association for Computational Linguistics and the 11th InternationalJoint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 72657281, 2021.",
  "David D Lewis and Jason Catlett. Heterogeneous uncertainty sampling for supervised learning. In Machinelearning proceedings 1994, pp. 148156. Elsevier, 1994": "Xiaotong Li, Zixuan Hu, Yixiao Ge, Ying Shan, and Ling-Yu Duan. Exploring model transferability throughthe lens of potential energy. In Proceedings of the IEEE/CVF International Conference on ComputerVision, pp. 54295438, 2023. Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swintransformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVFInternational Conference on Computer Vision, pp. 1001210022, 2021.",
  "Rafid Mahmood, Sanja Fidler, and Marc T Law. Low-budget active learning via wasserstein distance: Aninteger programming approach. In International Conference on Learning Representations, 2022": "Mohammad Sadegh Norouzzadeh, Dan Morris, Sara Beery, Neel Joshi, Nebojsa Jojic, and Jeff Clune. Adeep active learning system for species identification and counting in camera trap images. Methods inecology and evolution, 12(1):150161, 2021. Maxime Oquab, Timothe Darcet, Tho Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, PierreFernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visualfeatures without supervision. arXiv preprint arXiv:2304.07193, 2023.",
  "Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer.Scaling vision transformers.InProceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1210412113,2022": "Jifan Zhang, Yifang Chen, Gregory Canal, Arnav Mohanty Das, Gantavya Bhatt, Stephen Mussmann,Yinglun Zhu, Jeff Bilmes, Simon Shaolei Du, Kevin Jamieson, et al.Labelbench: A comprehensiveframework for benchmarking adaptive label-efficient learning. Journal of Data-centric Machine LearningResearch, 2024. Renyu Zhang, Aly A Khan, Robert L Grossman, and Yuxin Chen.Scalable batch-mode deep bayesianactive learning via equivalence class annealing. In The Eleventh International Conference on LearningRepresentations, 2022.",
  "AHyperparameters": "Fine-tuning, linear probing then fine-tuning (LP-FT), and proxy model training all utilized the SGD withmomentum optimizer, with a momentum value of 0.9. For fine-tuning and LP-FT, the batch size in thetraining stage is 128 and the batch size in the sample selection stage of the active learning is 512. The hyper-parameters of fine-tuning are shown in table 5. For ImageNet, we adopted the fine-tuning pa-rameters following the prior work (Cai et al., 2021). Setting the learning rates separately for the classifierhead and the backbone. For CIFAR-10, CIFAR-100, and Pets, we followed prior work (Cai et al., 2021) toset the learning rate of the classifier as 0.1 and to search other hyper-parameters. The hyper-parameters aredetailed in table 5. The learning rate of the backbone was searched from (0.01, 0.05, 0.1 0.3) and the weightdecay from (0, 0.0001). Additionally, to maintain consistency in training iterations between fine-tuning andLP-FT, we set the number of fine-tuning epochs as the sum of linear probing and fine-tuning epochs inLP-FT training.",
  "ImageNet0.10.01500.0001CIFAR-100.10.11300CIFAR-1000.10.051500Pets0.10.11400": "For the LP-FT hyper-parameters, we conducted a search within the following ranges: (0.01, 0.1) for theclassifiers learning rate, (0.01, 0.05, 0.1, 0.3) for the backbones learning rate, (0.05,0.1,0.5) for learning ratein linear-probing (LP) stage, and (0, 0.0001) for weight decay. Fine-tuning (FT) training epochs were set at120. For ImageNet, linear probing (LP) training epochs were searched from (5, 10), while for other datasets,LP training epochs were explored from (5, 10, 20, 30). Given the numerous hyper-parameters in LP-FT, we",
  "BComputation Time": "The time for standard active learning, SVPp and ASVP on V100 GPU is presented in table 7. The activelearning setup, outlined in sec. 6.1, performs a total of 10 active learning iterations on CIFAR-10 and Pets,where 200 samples are selected for each iteration. Regarding CIFAR-100, it spans 15 iterations, selecting400 samples per iteration.",
  "CAblation Study": "The results of ablation experiments with various active learning strategies are shown in table 8, table 9,table 10. Specifically, feature alignment refers to whether the ASVP method updates pre-computed featuresusing fine-tuned model features, while training alignment indicates the selection of the final model train-ing method (FT or LP-FT) based on LogME-PED scores. In all ablation experiments, feature alignmentconsistently demonstrates notable improvements. Training alignment, except for the Pets dataset, exhibitsenhancements.",
  "EResults of other Active Learning Strategies": "The accuracy and the equivalent non-active learning label amount of all active learning strategies on Ima-geNet, CIFAR-10, CIFAR-100 and Pets are shown in the fig. 15-fig. 22. The equivalent non-active learninglabel amount refers to the number of samples selected by a random baseline to achieve the same accuracyas active learning.",
  "(c) Confidence": ": The equivalent number of non-active learning label amounts comparison of the standard method,SVPp, and our method ASVP on ImageNet using (a) Margin, (b) BADGE and (c) Confidence sampling.The equivalent non-active learning label amount refers to the number of samples selected by a randombaseline to achieve the same accuracy as active learning. Number of Labels Accuracy (%)",
  "(e) Coreset": ": The equivalent number of non-active learning label amounts comparison of the standard method,SVPp, and our method ASVP on the Pets using (a) Margin, (b) BADGE, (c) Confidence, (d) ActiveFT(al)and (e) Coreset sampling. The equivalent non-active learning label amount refers to the number of samplesselected by a random baseline to achieve the same accuracy as active learning. Sample Selection Time (hrs) Overall Cost ($)",
  ": The accuracy of the standard method, SVPp, and our method ASVP on the Pets using (a)Margin, (b) BADGE, (c) Confidence, (d) ActiveFT(al) and (e) Coreset sampling": "strategy tends to result in selecting some redundant samples. Given the efficiency of our method, we canalleviate this problem. We conducted experiments on ImageNet, randomly selecting an initial pool of 10,000samples. Subsequently, we performed 9, 45, 180, and 900 active learning iterations using margin sampling,selecting a total of 100,000 samples. The accuracy of the final model and the total sample selection time forboth FT and LP-FT training are presented in table 12. The results show a general improvement in activelearning performance with an increase in the number of iterations. However, beyond 180 iterations, the",
  "GReplacement Experiment on CIFAR-10": "We conducted replacement experiments on CIFAR-10 to demonstrate the impact of replacing missing samplesfrom regions A1, A2, B1, and B2 on SVPp active learning performance. As shown in , similar tothe ImageNet replacement experiments, replacing samples from regions A1 and B1 results in little change inactive learning performance, whereas replacing samples from regions A2 and B2 improves performance. We also used Dataset Maps to illustrate the difficulty distribution of samples from different regions. TheDataset Map calculates the average and standard deviation of the models confidence for the correct class(need true labels) using training dynamics from the entire dataset. Due to the simplicity of CIFAR-10, theDataset Map shows very high mean confidence for almost all samples. Therefore, the difficulty based onmean confidence, as used in .1, would categorize nearly all samples as easy. Instead, we classified sample difficulty based on the standard deviation of the models confidence in thecorrect class. Specifically, we defined the difficulty levels as follows: very hard (std. >= 0.15), hard (std.>= 0.1 and std. < 0.15), medium (std. >= 0.05 and std. < 0.10), and easy (std. < 0.05). The resultingdistribution of different difficulty levels across regions is shown in , where the proportion of eachdifficulty level within the entire dataset is displayed in the last column."
}