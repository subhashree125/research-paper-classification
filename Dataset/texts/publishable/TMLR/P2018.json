{
  "Abstract": "Transferring knowledge from a source domain to a target domain in the absence of sourcedata constitutes a formidable obstacle within the field of source-free domain adaptation,often termed hypothesis adaptation. Conventional methodologies have depended on a ro-bustly trained (strong) source hypothesis to encapsulate the knowledge pertinent to thesource domain. However, this strong hypothesis is prone to overfitting the source domain,resulting in diminished generalization performance when applied to the target domain. Tomitigate this issue, we advocate for the augmentation of transferable source knowledge viathe integration of multiple (weak) source models that are underfitting. Furthermore, wepropose a novel architectural framework, designated as the Hierarchical Feature Ensem-ble (HiFE) framework for Few-Shot Hypotheses Adaptation, which amalgamates featuresfrom both the strong and intentionally underfit source models. Empirical evidence fromour experiments indicates that these weaker models, while not optimal within the sourcedomain context, contribute to an enhanced generalization capacity of the resultant modelfor the target domain. Moreover, the HiFE framework we introduce demonstrates superiorperformance, surpassing other leading baselines across a spectrum of few-shot hypothesisadaptation scenarios.",
  "(c) (d)": ": Different approaches to improve the performance of Few-shot Hypothesis Adaptation (FHA). (a)Conventional FHA. (b) Enhancing target performance by increasing the number of target samples.(c)Typical ensemble approaches to improve target performance by increasing the number of source domains.(d) Improving target performance under resource constraints with only one source domain and a limitednumber of target samples by generating multiple source hypotheses from the given source domain.",
  "Introduction": "Domain adaptation (DA) (Ben-David et al., 2010) refers to the study of leveraging labeled data in a sourcedomain (SD) to obtain a predicted model for a given target domain (TD) where labels are insufficient orunavailable. Conventional DA methods (Ahmed et al., 2021; Jiang et al., 2021; Kang et al., 2019; Sukhijaet al., 2016; Wang et al., 2019) pose a potential risk of exposing private information caused by accessingthe source data. To mitigate this concern, recent studies have introduced source-free DA, also referred toas hypothesis adaptation (HA) (Liang et al., 2020; Li et al., 2020; Yang et al., 2021; Yi et al., 2023), whichleverages a source model to encode the knowledge from the SD rather than the source data. Recently, few-shot HA (FHA) (Chi et al., 2021; Yazdanpanah & Moradi, 2022), which operates effectively in scenarios withlimited labeled data from the TD, has emerged as an appealing approach to address data scarcity ( (a)). To improve the FHA performance, recent approaches have attempted to increase the size of targetsamples via data labeling or generation ( (b)) or gather multiple source domains to generate moresource hypotheses ( (c) (Ahmed et al., 2021; Shu et al., 2022; Li et al., 2024)). However, the abovementioned approaches require additional effort to label the target data or to collecthypotheses from different source domains. Moreover, these approaches rely on best-performing strong sourcemodels from the SD, which may overfit the SD and subsequently perform worse on the TD after adaptation.This overfitting issue has been demonstrated by a pilot experiment in which models with varying accuraciesfrom the digit dataset SVHN were adapted to the target task Mnist using a straightforward fine-tuningapproach (details are provided in Appendix A). In this experiment, under-fitted weak source models mayexhibit superior performance compared to the strong model after adaptation (). This indicates thatsome weak hypotheses, although suboptimal for the SD, may contain underlying knowledgethat could be beneficial for the TD. Inspired by this experiment, we propose addressing the FHAproblem by generating multiple source hypotheses with varying source accuracies from a single SD ((d)). Notably, the routine training of source models naturally produces a series of weak intermediate modelsthat are often overlooked and discarded. These models are easily obtainable, as reaching out to the sourceprovider to acquire additional weak models incurs minimal additional cost. A new problem: FHAW. Thus, we propose to study a new problem called few-shot hypotheses adaptationwith weak models (FHAW). Unlike previous single-source FHA approaches (Chi et al., 2021; Liang et al., 2020)",
  "Published in Transactions on Machine Learning Research (09/2024)": "Moslem Yazdanpanah and Parham Moradi. Visual domain bridge: A source-free domain adaptation forcross-domain few-shot learning. In Proceedings of the IEEE/CVF Conference on Computer Vision andPattern Recognition, pp. 28682877, 2022. Li Yi, Gezheng Xu, Pengcheng Xu, Jiaqi Li, Ruizhi Pu, Charles Ling, A Ian McLeod, and Boyu Wang.When source-free domain adaptation meets learning with noisy labels. arXiv preprint arXiv:2301.13381,2023. Kaichao You, Yong Liu, Jianmin Wang, and Mingsheng Long. Logme: Practical assessment of pre-trainedmodels for transfer learning. In International Conference on Machine Learning, pp. 1213312143. PMLR,2021. Yang Zou, Zhiding Yu, BVK Kumar, and Jinsong Wang. Unsupervised domain adaptation for semanticsegmentation via class-balanced self-training. In Proceedings of the European Conference on ComputerVision (ECCV), pp. 289305, 2018.",
  "Main contributions. Our contributions are three-fold:": "To the best of our knowledge, this is the initial investigation into the FHAW problem. FHAW holdspractical relevance in numerous private data-based scenarios, as source providers are inclined to offer\"redundant\" weak models instead of disclosing sensitive datasets. Our work introduces a fresh perspectiveto encode the source knowledge in the absence of the source data. We propose a new framework to aggregate all source hypotheses at the feature level to address the FHAWproblem. We are the first to apply a hierarchical ensemble at the feature level in hypotheses adaptation.We effectively alleviate the over-fitting problem by the design of WRU and improve the generalization ofthe final hypothesis by incorporating feature DeCL loss under the few-shot setting. The comprehensive evaluation of the proposed HiFE methodology, conducted over an array of benchmarkdatasetsincluding Mnist, SVHN, USPS, CIFAR-10, STL-10, Amazon, DSLR, Webcam, and VisDA-C-has established that our approach achieves performance on par with or exceeding current SOTA methodsin various FHA tasks. Notably, as detailed in , the HiFE method surpasses the SOTA by anaverage accuracy of 4.3% in the digit dataset task USPS Mnist. Similarly, in the task of adaptingDSLR to Webcam datasets, as shown in , HiFE outperforms the SOTA by 3.6% in accuracy.",
  "This section presents a brief overview of the literature about traditional domain adaptation, hypothesisadaptation, multi-hypotheses adaptation, and ensemble methods for hypothesis adaptation": "Domain adaptation (DA). Traditional DA is a subfield of machine learning that focuses on learning ahypothesis for a TD when labeled data is insufficient or unavailable by leveraging labeled data from an SD.Numerous DA methods have been proposed for various tasks such as object classification (Liang et al., 2018),object detection (Hsu et al., 2020), and semantic segmentation (Zou et al., 2018). Existing approaches forDA can mainly be categorized into two classes: feature-based DA and instance-based DA. The former aimsto learn a domain-invariant representation by minimizing the domain discrepancy in a shared space (Kanget al., 2019; Long et al., 2017). For example, Gradually Vanishing Bridge (Cui et al., 2020) uses bi-directionalgeneration to learn domain-invariant representations. The latter minimizes the discrepancy by re-weightingthe source samples for better training. Despite the success achieved by these methods, they require access tosource data during the learning process, which incurs significant costs in terms of data transfer and storageas well as risks related to personal information leakage.",
  "Problem Definition": "We address the problem of few-shot hypotheses adaptation with weak models, where several pre-trainedsource hypotheses, including one strong and some weak hypotheses, are given. Let X Rd be an inputspace and Y := {1, . . . , C} be the label space, where C is the number of classes. To formalize the problemclearly, some definitions are presented as follows. Definition 1. (Expected and empirical risk). Given a data distribution P over X Y, let H = {h : X Y}be the hypothesis space and h H with the parameter , then the expected and empirical risks aredefined as",
  "Addressing FHAW in Principle": "We will present a theoretical view based on the PAC-Bayesian framework (Germain et al., 2009; McAllester,1999; Masegosa, 2020) to demonstrate why we propose to incorporate multiple weak hypotheses for FHA andwhy our HiFE framework works. In the PAC-Bayesian framework, each hypothesis h has prior knowledgeof the hypothesis space , and this prior distribution is updated to a posterior distribution afterfeeding samples D to h. In FHAW, multiple models {hi}Mi=1 are given with i i, = {i}Mi=1 and() = Mi=1 i(i). For a given sample (x, y), we apply the cross-entropy loss (, x, y) = log p(y|x, ). Abound theorem proposed by Deng et al.for the model ensemble is restated below, and some other previousrelated theorems are shown in Appendix C.Theorem 1. (Model ensemble error bound (Deng et al., 2023)).Given a data distribution P over X Y,a set of model parameters {i}Mi=1 with associated prior {i}Mi=1, where i is defined over i with i(i) N(0, 2I), a (0, 1], a real number c > 0, and i(i) is a Dirac-delta distribution centered around i withi(i) = i(i), then we have that the E()(L()) is upper bounded by",
  "WRU": ": The architecture of HiFE framework. Each source hypothesis consists of a feature encoder and aclassifier. We train a model with a hierarchical feature ensemble module to merge features from all sourceencoders. In this module, features are grouped according to the cosine similarity, and WRU merges thegrouped features to generate new features for the next layer. Each WRU leverages skip connections to avoidover-fitting. Besides, we apply the decorrelation learning (DeCL) strategy by adding a correction penaltyterm to the loss function to encourage feature diversity. The target classifier ct, initializing with the averageof all the source classifiers, is fixed during the training. Only the parameters of the strong encoder gs andthe hierarchical feature ensemble module are updated.",
  "Few-shot Hypotheses Adaptation via Hierarchical Feature Ensemble": "To aggregate knowledge from both strong and weak source hypotheses, we introduce the HiFE framework,depicted in . HiFE hierarchically merges features induced by all the source hypotheses. We assumeeach source hypothesis has been embedded with its specific discriminative knowledge about the SD. Hence,during the aggregation, we use a feature de-correction learning module, making the features as mutuallyindependent as possible at each level to increase the representative power of the intermediate features. Wedescribe the design insights of HiFE in .1 and .2.",
  "Hierarchical Feature Ensemble": "Ensemble learning is widely recognized as an effective approach for combining multiple learning methodsand improving overall performance (Beven & Binley, 1992; Kuczera & Parent, 1998).While ensemblemethods have been utilized for HA in past research (Ahmed et al., 2021), ensemble learning at the featurerepresentation level has received relatively less attention. However, prior research has shown that hierarchicalfeature representations can significantly enhance classification accuracy. We propose a hierarchical featureensemble-based approach for FHAW to leverage such benefits. Specifically, our method involves mergingsource features that contain knowledge of the SD using a hierarchical feature ensemble module before feedingthem to the final classifier. To simplify the feature extraction process with the source hypotheses, we follow (Motiian et al., 2017) and(Ahmed et al., 2021) to decompose each hypothesis h into two modules: a feature encoder g : X Rd and aclassifier c : Rd RC, where d denotes the dimension of the output feature. Thus, we have h(x) = c(g(x)),",
  "i=1i fi,(1)": "where {i}Ki=0 are the learnable weights. We add the batch normalization and ReLu layers after the weightedsum for better performance. If the dimension of fi is not equal to that of the output of FC, we can makea linear projection of fi by extending i to a square matrice Wi to match the dimension. The applicationof WRU allows us to preserve some source knowledge and learn new information from the target samplessimultaneously. In our approach, we follow the aforementioned principle to determine which features to merge and then usethe WRU to merge the features based on the first batch of input data. Once we pass the first batch of inputdata, the merging network is built. This merging network then remains fixed, and all subsequent samplesshare the same merging network. Such a setting ensures stable fine-tuning on target samples.",
  "Decorrelation Learning": "It has been commonly agreed that diversity is a success factor of ensemble algorithms. Different opinions frommultiple classifiers are expected to reduce the generalization error. Traditional decorrelation learning (DeCL)methods encourage diversity explicitly by adding a correlation penalty term to the final error function (Liu& Yao, 1999; Shi et al., 2018; Wang et al., 2010). When it comes to feature ensemble, learning the featureswith good discriminative power is also essential for various high-level vision tasks (Wen et al., 2016; Chenget al., 2018). To promote the learning of features widely distributed across the feature space and embedvarious forms of source knowledge, we apply DeCL in the feature space to encourage independence betweenfeatures in each layer. In this regard, we introduce a cosine similarity penalty to decrease feature correlationand encourage feature diversity (see (c)). Specifically, we calculate the pairwise square values of",
  "j=i+1cos(xil, xjl )2,(2)": "where Nl is the number of features at layer l and cos(xil, xjl ) = (xil xjl )/(xil xjl ). Furthermore, toenable the adaptation of the ensemble network to the TD, we incorporate the knowledge of TD by fittingthe network to the labeled target data. To accomplish this, we adopt the standard cross-entropy loss, whichis defined as follows,LCLS = E(xt,yt)PT [CE(ct(A(xt)), yt)],(3) where CE() denotes the cross-entropy loss and A(xt) refers to the output of the feature ensemble modulewhen fed xt to the source encoders. To summarize, we train the ensemble network using joint supervisionthat combines the target supervised loss (Equation (3)) and a feature DeCL penalty term (Equation (2))with a hyper-parameter to trade off the two aspects (Equation (4)). The target supervised loss guides thenetwork in learning the knowledge from the target samples, while the feature DeCL loss promotes mutualindependence amongst features in each layer, thereby increasing the feature diversity and preserving thedistinct discriminative knowledge of each source hypothesis.",
  "Datasets. We conduct experiments on various standard DA benchmarks to evaluate our approach1": "Digits. We choose three-digit datasets, i.e., Mnist (M), USPS (U), and SVHN (S) for our experiments.Following (Motiian et al., 2017; Chi et al., 2021), we experiment with different numbers of target samplesfrom 1 to 7 per class. Office. We use three domains of the office datasets (Saenko et al., 2010): Amazon (A), DSLR (D), and Web-cam (W). Each domain contains 31 object classes in the office environment. We conduct several experimentswith different numbers of target samples per class ranging from 1 to 5. Image classification. We use two image classification benchmarks CIFAR-10 (CF) (Krizhevsky, 2009) andSTL-10 (ST) (Coates et al., 2011). Each benchmark consists of 10 classes of objects, and nine classes areoverlapped. We remove the non-overlapped classes (monkey and frog) and reduce the tasks to a 9-classclassification problem following the procedure in (Shu et al., 2018). As the two domains are more complexthan digits, we increase the number of target samples to 15 and 20 for each class. VisDA-C. VisDA-C (Peng et al., 2017) is a demanding large-scale benchmark designed primarily for the12-class synthesis-to-real object recognition task. The source domain comprises 152,000 synthetic imagescreated by rendering 3D models from different angles and with different lighting conditions, whereas thetarget domain includes 72,000 real object images sourced from Microsoft COCO. We randomly choose 10%of the target data set (7200 images) as the testing set. Since the domain gap between the synthesis andreal-object images is large, we experiment on a larger number (10, 30, and 50) of the target samples. Baseline methods. In the context of the novel FHAW problem setting, we establish our baseline compar-isons by adapting and refining several established approaches in the field. We conducted a comprehensiveevaluation against four existing methods for HA and their respective variations. Initially, SHOT (Liang et al.,2020), a hypothesis transfer learning framework tailored for unsupervised HA, served as a foundation. In our",
  "Result Analysis": "Results of digit classification tasks. We evaluate the effectiveness of our approach on six closed-setadaptation tasks for digit classification. These tasks are by pairwise combinations of the three domainsS, M, and U. We report the results of three tasks in (more results can be found in AppendixD). Firstly, as shown in , there exist some weak hypotheses that can perform better than the stronghypothesis after adaptation (see the comparison of SHOT-best and SHOT-strong), supporting our motivationof adopting the weak hypotheses. Moreover, incorporating the weak hypotheses allows our proposed HiFE tooutperform SHOT-strong. For instance, compared with the average accuracy of SHOT-strong (77.3%), HiFEleads to higher average accuracy (88.7%) in the task S M. Additionally, despite some weak hypotheseswith bad adaptation performance (see SHOT-worst), HiFE can largely avoid the severe negative transfer andachieve the best performance than previous ensemble approaches. For example, HiFE outperforms the SOTA(TOHAN-ens) by 4.3% in the average accuracy of U M task. Notably, we can observe that our approachnot only maintains competitive accuracy but also decreases the standard deviation (std) of accuracy acrossdifferent target samples. This reduction in std is paramount as it indicates a more consistent and reliableperformance of our HiFE.",
  "Multiple": "SHOT-ens99.20.499.20.399.30.499.50.399.20.399.3TOHAN-ens99.20.299.40.399.30.299.40.499.50.299.4DECISION99.50.6 98.90.699.30.399.20.499.20.199.2Bi-ATEN99.40.6 99.60.4 99.40.699.30.399.60.499.5HiFE (ours) 99.50.6 99.60.1 99.60.1 99.60.3 99.90.299.6 : Classification accuracystandard deviation(%) on six adaptation tasks of office datasets. A, D, andW are abbreviations of Amazon, DSLR, and Webcam. The suffixes of -best and -worst refer to the best andworst results after adapting each single source hypothesis. The suffixes -strong and -ens refer to the resultafter adapting the strong hypothesis and the ensemble of all adapted hypotheses, respectively. Results ofSHOT (Liang et al., 2020), TOHAN (Chi et al., 2021), DECISION (Ahmed et al., 2021) and our HiFE arepresented. The highest accuracy is marked in bold.",
  "W AMultiple": "SHOT-ens55.11.258.21.659.91.460.81.161.21.159.0TOHAN-ens56.51.060.10.960.41.261.20.862.50.760.1DECISION54.12.154.91.855.61.656.51.258.11.255.8Bi-ATEN56.21.958.41.658.92.261.51.361.71.159.3HiFE (ours)62.52.565.11.564.41.264.30.964.80.864.2 : Classification accuracystandard deviation (%) on three adaptation tasks of office datasets. A, D,and W are abbreviations of Amazon, DSLR, and Webcam. The suffix -ens refers to the result of the ensembleof all adapted hypotheses. Results of SHOT (Liang et al., 2020), TOHAN (Chi et al., 2021), DECISION(Ahmed et al., 2021), Bi-ATEN (Li et al., 2024), and our HiFE are presented. The bold value represents thehighest accuracy.",
  "Ablation Studies": "Ablation study on the feature DeCL loss. We study the advantage of our training loss by incorporatingfeature DeCL loss LDeCL in Equation (4) with different values ranging from 0 to 1.0 with the digit datasets.In this context, = 0 corresponds to training the network using only the supervised loss LCLS, while = 1corresponds to training the network using only the feature DeCL loss LDeCL. As shown in , ouroptimal results generally occur at = 0.1, which yields an average improvement of 1.4% compared to theresult obtained when no feature DeCL loss is used ( = 0). Notably, even when the model is trained solelyusing the feature DeCL loss LDeCL ( = 1.0), it still achieves an average improvement of 10.9% comparedto the accuracy before the adaptation (WA), demonstrating the effectiveness of the feature DeCL loss. We also visualize the correlation matrices of the features after the merge at the first layer of the task U Mwhen = 0.1. As depicted in , as the training progresses, the feature DeCL loss guides the decreaseof most of the correlation values between the four features, thereby increasing feature diversity. Ablation study on the number of weak hypotheses. We conduct an ablation study to analyze theimpact of the number of weak hypotheses on the final performance, providing insights into choosing a propernumber of weak hypotheses to balance the cost and performance. We do this experiment in an adaptationtask from domain Mnist to USPS. We select varying numbers (from 2 to 6) of source hypotheses from themodels provided in .1 and make sure the accuracy range of each group of source models is thesame. For each experiment, the selected weak hypotheses started from h5 and ended with h11, ensuring the",
  "Average79.280.680.179.178.176.975.469.959": ": Ablation study on the feature decrrelation learning loss balance parameter in Equation (4). M,U, and S are abbreviations of MNIST, USPS, and SVHN. WA indicates the accuracy of the model withoutthe adaptation. The bold value represents the highest accuracy (%). selected accuracy range was [60, 95). The results are presented in Exp11 Exp15 of . As shownin , when the number of weak hypotheses is less than 3, the average accuracy (94.2% in Exp11)is lower than that when the number of weak hypotheses is greater than 3. Moreover, the average accuracyremains consistent when the number of weak hypotheses exceeds 3 (see the comparison of the result ofExp13 Exp15). Ablation study on the accuracy range of weak hypotheses. To investigate the impact of the accuracyrange of weak hypotheses on the final performance, we leverage the source models presented in .1and do the digit adaptation task from Mnist to USPS using source models with varying accuracy ranges.We conducted experiments from Exp21 to Exp24 as outlined in . Our results indicate that asthe accuracy range increases, the performance after adaptation improves. It is important to note that weaksource models with an accuracy lower than 55% may harm the final performance. Our comparison of Exp21and Exp25 revealed that using weak hypotheses with such low accuracy resulted in worse performance than",
  "Emax(d) e = Emax": ": The correlation matrixes of the features after the merge at the first layer on digit task U Mwhen the value is set to 0.1. (a) (d) shows the results over different training stages with e, Emax being thecurrent and maximum number of epochs. As the training continues, we observe that most of the correlationvalues between the four features in this layer decrease (the darker the color, the lower the correspondingcorrelation value).",
  "Accuracy Range1357": "Exp112[60, 95)h5, h1123h1292.01.2 94.60.7 94.50.5 95.60.594.2Exp123[60, 95)h5, h8, h1123h1292.31.0 94.80.6 95.70.7 96.10.794.7Exp134[60, 95)h5, h7, h9, h1123h1292.71.1 95.20.6 96.10.5 96.70.295.2Exp145[60, 95)h5, h7, h8, h9, h1123h1292.61.0 95.30.7 96.00.5 96.60.495.1Exp156[60, 95)h5, h6, h7, h8, h9, h1123h1292.80.9 95.10.5 96.20.7 96.80.595.2 Exp213[40, 55)h1, h2, h323h1291.10.7 93.80.6 94.50.8 95.20.893.7Exp223[55, 70)h4, h5, h623h1291.50.7 94.10.6 95.30.6 96.00.894.2Exp233[70, 85)h7, h8, h923h1292.20.6 94.70.8 95.70.8 96.70.694.8Exp243[80, 95)h9, h10, h1123h1292.40.8 94.80.4 96.10.8 96.70.695.0Exp250--11h1291.50.6 94.20.6 94.20.9 95.50.793.9 Exp317[60, 95)h51124h1293.01.4 95.30.5 96.00.3 96.70.395.3Exp327[60, 95)h51143h1293.21.1 94.90.6 96.20.5 96.60.495.2Exp337[60, 95)h51181h1292.31.2 93.11.0 95.40.8 95.60.594.1Exp347[60, 95)h511/1h1292.11.0 92.50.9 94.80.6 95.10.793.6 Exp418[60, 95)h51128h1293.00.9 95.30.5 96.00.3 96.70.395.3Exp428[60, 95)h51128h11,1293.20.7 95.60.7 95.80.8 96.90.595.4Exp438[60, 95)h51128h1012 92.50.6 95.10.7 95.40.5 96.10.494.8Exp448[60, 95)h51128h51291.00.8 93.10.6 93.80.6 94.10.393.0Exp458[60, 95)h51128h51088.10.9 89.10.8 89.5 1.1 89.70.789.1 : Classification accuracystandard deviation (%) on digit adaptation task Mnist USPS withvarying parameters including the number of weak hypotheses (HN), the accuracy ranges of the weak hy-potheses, the number of input features to each WRU (FN), the layer number (LN) in the hierarchical featureensemble module and the updated hypotheses during fine-tuning (UH). h12 is the strong hypothesis. adaptation without weak hypotheses. To summarize, our approach, HiFE, performs well when incorporatinga larger number of weak hypotheses, provided that these hypotheses are not of very low accuracy. In ourexperiments, we selected 7 weak hypotheses as our base setting to balance performance and computationalcost. In practical applications, the selection of weak hypotheses can be guided by the available computationalresources and the performance of each weak hypothesis on the SD. Ablation study on the hierarchical layer number. To investigate the impact of the layer number in theensemble module on the final performance, we have experimented with varying numbers of input features fedinto WRU, which subsequently alters the number of merge layers within the hierarchical feature ensemblemodule. we conducted experiments from Exp31 to Exp34, modifying the number of input features foreach WRU. The results are presented in . Notably, in Exp34, we utilized a simple weighted featuresum to merge all source features rather than using HEFM with WRU for feature aggregation. When we setthe number of input features for each WRU to 2 or 4, with the corresponding layer number in the ensemblemodule to be 4 and 3, respectively, the adaptation average accuracy is similar. However, when the numberof input features for each WRU increases to 8, we apply one WRU to merge the eight source encoders atonce, and the adaptation average accuracy decreases to 94.1%. Ablation study on the updated hypotheses. In our HiFE approach, we aggregate multiple hypotheses.During the fine-tuning stage, we fix all the weak hypotheses and only update the parameters in the stronghypotheses. We conducted experiments from Exp41 to Exp44, modifying which hypotheses are updatedduring the fine-tuning stage. Exp41 is our default setting, where only the last strong hypothesis h12 isupdated and it achieves good performance. When we update all the weak and strong hypotheses together(Exp43), or only update all the weak hypotheses (Exp44), the performance deteriorates. Notably, whenwe update the last two models h11 and h12 (Exp42), we achieve the best performance. This is because h12tends to overfit to the SD while h11 is better suited for the TD (see the comparison of the result of adaptationof h11 (SHOT-best) and h12 (SHOT-strong) of task M U in in Appendix Sec D). In practice, thefew-shot target samples are insufficient to guide the fine-tuning of both weak and strong hypotheses together.Additionally, it is unrealistic to determine which source hypothesis is best for the target domain beforehand.Therefore, the most suitable approach is to only update the final strong hypotheses while keeping all otherweak hypotheses unchanged among the source encoders. By designing and updating the hierarchical featureensemble module, we can merge all these models to obtain the best final model.",
  "Limitations and Future Work": "HiFE leverages multiple source hypotheses with varying accuracy levels from the SD to improve the per-formance of models in the TD. By exploiting the diversity of source models, HiFE has the potential toenhance the generalization capabilities of the adapted models. However, the additional hypotheses result inincreased model transfer and storage costs. Moreover, the increase in the number of parameters of the targetmodel leads to higher computational costs. Nonetheless, we argue that the benefits of leveraging multiplesource models with different strengths outweigh the costs associated with processing additional hypotheses,particularly when source data is absent for transfer. With the growing need to address privacy concerns andmitigate data-sharing challenges in real-world applications, opting for weak models simplifies collaborationbetween source providers and users. For future research, it would be beneficial to investigate methods for generating weak hypotheses with higherdiversity. Although the current experiments obtained weak hypotheses in the same run as generating thefinal strong hypotheses for simplicity, there is potential for improvement by obtaining weak hypothesesthrough different random seeds, hyperparameter choices, or training on different subsets of the source data.By increasing the diversity of weak hypotheses, we could obtain better performance after adaptation andfurther improve the effectiveness of the proposed approach.",
  "Conclusion": "In this paper, we investigate the potential of utilizing weak source hypotheses for domain adaptation andintroduce a new problem setting termed few-shot hypotheses adaptation with weak models. To tacklethis problem, we design a new framework called HiFE, which leverages an array of readily available weakhypotheses to improve the adaptation performance of a strong source hypothesis. As a result, HiFE signifi-cantly mitigates the occurrence of over-fitting under the few-shot setting and achieves the SOTA performanceacross various adaptation tasks. This research introduces an innovative perspective for addressing the FHAproblem in scenarios where the source data is inaccessible and the target data is limited. Additionally, thisresearch shed light on the use of a weak source model to boost the practical application of transfer learningin scenarios where data privacy concerns are on the rise.",
  "Jiuwen Cao, Zhiping Lin, Guang-Bin Huang, and Nan Liu. Voting based extreme learning machine. Infor-mation Sciences, 185(1):6677, 2012": "Gong Cheng, Ceyuan Yang, Xiwen Yao, Lei Guo, and Junwei Han.When deep learning meets metriclearning: Remote sensing image scene classification via learning discriminative cnns. IEEE Transactionson Geoscience and Remote Sensing, 56(5):28112821, 2018. Haoang Chi, Feng Liu, Wenjing Yang, Long Lan, Tongliang Liu, Bo Han, William Cheung, and James Kwok.Tohan: A one-step approach towards few-shot hypothesis adaptation. In Advances in Neural InformationProcessing Systems, volume 34, pp. 2097020982, 2021. Adam Coates, Andrew Ng, and Honglak Lee. An analysis of single-layer networks in unsupervised featurelearning. In Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,pp. 215223. JMLR Workshop and Conference Proceedings, 2011. Shuhao Cui, Shuhui Wang, Junbao Zhuo, Chi Su, Qingming Huang, and Qi Tian. Gradually vanishing bridgefor adversarial domain adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision andPattern Recognition, pp. 1245512464, 2020.",
  "Danruo Deng, Guangyong Chen, Furui Liu, Jinpeng Li, Jianye Hao, and Heng Pheng-Ann.A simpleand provable method to adapt pre-trained model across domains with few samples, 2023. URL": "Thomas G Dietterich. Ensemble methods in machine learning. In Multiple Classifier Systems: First In-ternational Workshop, MCS 2000 Cagliari, Italy, June 2123, 2000 Proceedings 1, pp. 115. Springer,2000. Ning Ding, Yixing Xu, Yehui Tang, Chao Xu, Yunhe Wang, and Dacheng Tao. Source-free domain adaptationvia distribution estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition, pp. 72127222, 2022.",
  "Dirk Eilers, Felippe Schmoeller Roza, and Karsten Roscher. Ensemble-based uncertainty estimation withoverlapping alternative predictions. In Deep Reinforcement Learning Workshop NeurIPS 2022, 2022": "Pascal Germain, Alexandre Lacasse, Franois Laviolette, and Mario Marchand. Pac-bayesian learning oflinear classifiers. In Proceedings of the 26th Annual International Conference on Machine Learning, pp.353360, 2009. Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. Rich feature hierarchies for accurate objectdetection and semantic segmentation. In Proceedings of the IEEE Conference on Computer Vision andPattern Recognition, pp. 580587, 2014.",
  "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. InProceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 770778, 2016": "Han-Kai Hsu, Chun-Han Yao, Yi-Hsuan Tsai, Wei-Chih Hung, Hung-Yu Tseng, Maneesh Singh, and Ming-Hsuan Yang. Progressive domain adaptation for object detection. In Proceedings of the IEEE/CVF winterconference on applications of computer vision, pp. 749757, 2020. Pin Jiang, Aming Wu, Yahong Han, Yunfeng Shao, Meiyu Qi, and Bingshuai Li. Bidirectional adversar-ial training for semi-supervised domain adaptation.In Proceedings of the Twenty-Ninth InternationalConference on International Joint Conferences on Artificial Intelligence, pp. 934940, 2021. Guoliang Kang, Lu Jiang, Yi Yang, and Alexander G Hauptmann.Contrastive adaptation network forunsupervised domain adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision andPattern Recognition, pp. 48934902, 2019. James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu,Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophicforgetting in neural networks. Proceedings of the National Academy of Sciences, 114(13):35213526, 2017.",
  "Xinyao Li, Jingjing Li, Fengling Li, Lei Zhu, and Ke Lu. Agile multi-source-free domain adaptation. InProceedings of the AAAI Conference on Artificial Intelligence, pp. 1367313681, 2024": "Jian Liang, Ran He, Zhenan Sun, and Tieniu Tan. Aggregating randomized clustering-promoting invariantprojections for domain adaptation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 41(5):10271042, 2018. Jian Liang, Dapeng Hu, and Jiashi Feng. Do we really need to access the source data? source hypothesistransfer for unsupervised domain adaptation.In International Conference on Machine Learning, pp.60286039. PMLR, 2020.",
  "Sebastian Ruder. An overview of gradient descent optimization algorithms. arXiv preprint arXiv:1609.04747,2016": "Andrei A Rusu, Neil C Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick, KorayKavukcuoglu, Razvan Pascanu, and Raia Hadsell.Progressive neural networks.arXiv preprintarXiv:1606.04671, 2016. Kate Saenko, Brian Kulis, Mario Fritz, and Trevor Darrell. Adapting visual category models to new domains.In Proceedings of the European Conference on Computer Vision (ECCV), pp. 213226. Springer, 2010.",
  "APilot Study on the FHA Process From SVHN to Mnist": "The strong source model may overfit the SD and perform worse on the TD after the adaptation. To justifyour assumption, we empirically design an experimental task that adapts source models with varying sourceaccuracies from the digit dataset SVHN to the target dataset Mnist. shows that the weak sourcehypotheses (e.g., Model-4 [source acc=76.0%]) could perform better than the strong source models (e.g.,Model-1 [source acc = 92.2%]) on the TD, indicating that weak hypotheses can generalize better on the TD.",
  "BImplementation Details": "Training target hypotheses with HiFE. As we mentioned in the manuscript , we generate12 source models {hi|12i=1}. Among these models, we used {hi|12i=5} (8 models) in the experiments shown in.2. In our HiFE, we set the number of input features for each WRU to two, leading to a four-layerfeature ensemble structure. The network uses the PyTorch framework on a PC with four NVIDIA 2080ti GPUs. We trained the sourcehypothesis using a stochastic gradient descent (SGD) optimizer with a momentum value of 0.5 with thelearning rate initialized to 1e-2 and decreased to 1e-5 step by step. During the adaptation, we adopt SGDwith Nesterov momentum (Ruder, 2016) with a momentum value of 0.9. Following (Liang et al., 2020),we insert a batch normalization layer and a weight normalization layer before the end of each encoder andclassifier, respectively.",
  "CReview of the Theorems": "In this section, we present a brief overview of the related theorems of Theorem 1, which is derived under thePAC-Bayes framework (McAllester, 1999). The PAC-Bayes theory provides data-dependent generalizationbounds over the generalization error E()(L() of a model with parameter under i.i.d. data. In recentyears, a PAC-Bayes bound (Germain et al., 2009) has been widely adopted because it can apply to generalunbounded losses (e.g., log-loss). We restate here this PAC-Bayes bound as follows:",
  "Accuracy rank": ": This figure depicts the FHA process from SVHN to Mnist. Four source models were generatedusing the training data from SVHN and fine-tuned with different quantities of samples from Mnist. They-axis indicates the performance rank after adaptation, with the highest accuracy in the TD ranked first.Model-4, despite having the lowest source accuracy (76%), exhibits notably superior performance on the TDcompared to Model-1, which has the highest source accuracy (92.9%). Theorem 2. (PAC-Bayes bounds (Germain et al., 2009; Masegosa, 2020; McAllester, 1999)).Given adata distribution P over X Y, a hypothesis space , a prior distribution over , for any (0, 1] and > 0, with probability at least 1 over samples D P n, we have for all posterior ,",
  "where P,(, n) = lnE()EDP n[e(L()L(,D))]": "Based on the second-order Jensen inequalities ((Needham, 1993)), the second-order oracle bound with tighterPAC-Bayes bounds is derived. (Deng et al., 2023) further extends this theory to ensemble models as shownin Theorem 3. (Deng et al., 2023) also shows a second-order PAC-Bayesian bound over the performance ofthe posterior predictive distribution of the averaging ensemble model, which is shown in Theorem 4. Theorem 3. (Second-order oracle bound ((Deng et al., 2023; Masegosa, 2020; Needham, 1993))).Givena data distribution P, a set of model parameters {i}Mi=1, for any distribution {i}Mi=1 over {i}Mi=1 satisfiesthat",
  "Lemma 2. If there exists an input sample x PX such that hi(x) = hj(x), we then have V(()) > 0((Deng et al., 2023))": "Our method proposes to add multiple weak hypotheses for training, thereby improving the diversity of thesource model. From the above theorem, our approach intends to look for a tighter upper bound of theexpected loss. To further analyze the term DKL(i i) in Theorem 4 when providing n training samples from the targetdomain, we assume i Rdi, i(i) N(0, 2I), and i(i) is a Dirac-delta distribution centered around iwith i(i) = i(i), i [M], then we have",
  "EMore Results on Office Datasets": "The complete results of the closest-set adaptation tasks with office datasets are presented in . Ouranalysis reveals that HiFE outperforms SHOT-strong, indicating that incorporating weak hypotheses can leadto improved performance. Regarding multi-hypotheses adaptation results, HiFE achieved average accuracyimprovements of 4.1% and 3.6% over the SOTA on W A and D A tasks, respectively. These resultsdemonstrate that HiFE can circumvent the potentially severe negative transfer induced by weak sourcehypotheses (as observed in the SHOT-worst result) and outperform previous model ensemble approaches.",
  "FExperiments on Multi-domain Multi-hypotheses Adaptation Scenario": "In our manuscript, HiFE is designed for adapting source models from one single source domain to a targetdomain. Individuals may wonder whether HiFE still works if the source models come from multiple domains.To answer this question, we do experiments on office datasets (Amazon (A), DSLR (D), Webcam (W)). Wetrain one strong and seven weak hypotheses for each domain in our single-source multi-model adaptationexperiment. To adopt models from multiple domains, we randomly choose one strong and seven weak hy-potheses among the 16 hypotheses from domains D and W and adapt them to A. As shown in ,HiFE outperforms the SOTA with 4.4% average improvement in this multi-domain and multi-model adap-tation task. Moreover, compared with the single-source domain adaptation tasks D A and W A, HiFEachieves 1.0% and 2.4% improvement on the multi-source domain adaptation task D, W A, respectively.",
  "D AHiFE (ours)61.81.064.70.767.20.666.81.067.50.965.6W AHiFE (ours)62.52.565.11.564.41.264.30.964.80.864.2": ": Classification accuracystandard deviation (%) on multi-domain multi-hypotheses adaptationtask from the office dataset DSLR (D), Webcam(A) to Amazon (A). SHOT-ens (Liang et al., 2020), TOHAN-ens (Chi et al., 2021) refers to the result of the ensemble of all corresponding adapted source hypotheses.The highest accuracy (%) is marked in bold. We also list the results of single-source adaptation tasks D Aand W A of HiFE for further comparison.",
  "GHiFE for Partial Few-Shot Hypothesis Adaptation": "We extend HiFE to a partial few-shot hypothesis adaptation scenario. The office dataset contains threedomains, including Amazon (A), DSLR (D), and Webcam (W), with each domain containing images of 31object categories. We use images from the first 17 classes as the target domain and images from all 31 classesas the source domain. In this way, we obtain six partial domain adaptation tasks. HiFE is also better thanthe SOTA in the partial few-shot hypothesis adaptation scenario (see details in ).",
  "SHOT-ens55.92.0591.860.01.664.20.9651.260.8TOHAN-ens59.40.760.30.862.11.964.21.370.61.463.3DECISION54.51.255.91.958.03.060.02.263.62.058.4HiFE (ours)67.11.8 72.01.0 71.32.1 71.92.1 72.20.970.9": ": Classification accuracies (%) on six partial few-shot hypotheses adaptation tasks of office datasets.A, D, and W are abbreviations of Amazon, DSLR, and Webcam. Results of SHOT (Liang et al., 2020),TOHAN (Chi et al., 2021), DECISION (Ahmed et al., 2021) and our HiFE are presented. The bold valuerepresents the highest accuracy (%).",
  "HComplexity Analysis": "We compare the time complexity and parameter size of our framework HiFE, a single model, and theensemble of all models using DECISION in .The experiments are conducted using the digitdataset for the SVHN to Mnist task with a LeNet model backbone. The batch size, the number of targetsamples per class, and the number of pre-trained models are set to 128, 7, and 8, respectively.HiFE,being a multi-hypotheses adaptation approach, incurs additional computational costs compared to the singlemodel baseline. Specifically, the cost is approximately 8 times higher, corresponding to the number of pre-trained models. While HiFE has only slightly more parameters than the ensemble method DECISION, itis significantly more efficient in its usage. Overall, HiFE strikes a better balance between performance andefficiency for multi-hypotheses adaptation."
}