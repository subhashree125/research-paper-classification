{
  "Abstract": "Graph-based representations and message-passing modular policies constitute prominent ap-proaches to tackling composable control problems in reinforcement learning (RL). However,as shown by recent graph deep learning literature, such local message-passing operators cancreate information bottlenecks and hinder global coordination. The issue becomes moreserious in tasks requiring high-level planning. In this work, we propose a novel methodol-ogy, named Feudal Graph Reinforcement Learning (FGRL), that addresses such challengesby relying on hierarchical RL and a pyramidal message-passing architecture. In particular,FGRL defines a hierarchy of policies where high-level commands are propagated from thetop of the hierarchy down through a layered graph structure. The bottom layers mimicthe morphology of the physical system, while the upper layers correspond to higher-ordersub-modules. The resulting agents are then characterized by a committee of policies whereactions at a certain level set goals for the level below, thus implementing a hierarchicaldecision-making structure that can naturally implement task decomposition. We evaluatethe proposed framework on a graph clustering problem and MuJoCo locomotion tasks; simu-lation results show that FGRL compares favorably against relevant baselines. Furthermore,an in-depth analysis of the command propagation mechanism provides evidence that theintroduced message-passing scheme favors learning hierarchical decision-making policies.",
  "Introduction": "Although reinforcement learning (RL) methods paired with deep learning models have recently led to out-standing results, e.g., see (Silver et al., 2017; Degrave et al., 2022; Wurman et al., 2022; Ouyang et al., 2022),achievements have come with a severe cost in terms of sample complexity. A possible way out foresees em-bedding inductive biases into the learning system, for instance, by leveraging the relational compositionalityof the tasks and physical objects involved (Battaglia et al., 2018; Hamrick et al., 2018; Zambaldi et al., 2018;Funk et al., 2022a). Within this context, physical systems can be often decomposed into a collection ofdiscrete entities interconnected by binary relationships. In such cases, graphs emerge as a suitable repre-sentation to capture the systems underlying structure. When processed by message-passing graph neuralnetworks (GNNs) (Gilmer et al., 2017; Bacciu et al., 2020; Bronstein et al., 2021), these graph representa-tions enable the reuse of experience and the transfer of models across agents (Wang et al., 2018): node-levelmodules (policies) can easily be applied to graphs (systems) with different topologies (structures). However,while conceptually appealing, the use of modular message-passing policies also bears some concerns in terms",
  "Published in Transactions on Machine Learning Research (12/2024)": "MLPs. In Tab. 3 are reported the values of the hyperparameters used in our experiments. For each baseline,we tuned the number of hidden units and CMA-ES step size by performing a grid search on the averagereturn. Note that for the MuJoCo benchmarks the best configuration of the MLP baseline and that of FGNNresults in a similar number of learnable parameters. For the graph clustering experiment we used the sameset of hyperparameters, except for the aggregation function of subordinate nodes in the feudal models wherewe used the average instead of the sum. The hyperparameters used for the comparison with PPO (Appendix C.2) are the following: we used Adamoptimizer (Kingma & Ba, 2014) with a learning rate of 3 106 and hidden layers with tanh non-linearities for both actor and critic; discount factor and clipping value are fixed to 0.99 and 0.2, respec-tively; policy is updated for 10 epochs with batch size of 64 and the updating horizon is 2048 time steps; theinitial action standard deviation is 0.6 and it decays every 104 episodes of 0.05, up to a minimum of 0.2. Weperformed a grid search on such hyperparameters, focusing mainly on learning rate, hidden layers, updatingepochs, and updating horizon. : Hyperparameters used in each model, where the marker indicates those that are not part ofthe architecture. For the total number of parameters, in the non-modular baseline (MLP) we reported themaximum among the four environments of the MuJoCo benchmark. We remark that in the modular models,the number of parameters does not depend on the environment, but since in the feudal architectures itdepends on the hierarchical height, in FGNN we reported the number corresponding to the maximum one,i.e., 3 levels.",
  "Related Works": "Several RL methods rely on relational representations. Zambaldi et al. (2018) embed relational inductivebiases into a model-free deep RL architecture by exploiting the attention mechanism. Sanchez-Gonzalezet al. (2018) use GNNs to predict the dynamics of simulated physical systems and show applications of suchmodels in the context of model-based RL. Other works adopt GNNs in place of standard, fully-connected,feed-forward networks to learn policies and value functions for specific structured tasks, such as physicalconstruction (Hamrick et al., 2018; Bapst et al., 2019). Moreover, GNNs have been exploited also in thecontext of multi-agent systems (Jiang et al., 2020) and robotics (Funk et al., 2022a;b). Ha & Tang (2022)provide an overview of deep learning systems based on the idea of collective intelligence, i.e., systems wherethe desired behavior emerges from the interactions of many simple (often identical) units. More related to our approach, NerveNet (Wang et al., 2018) relies on message passing to propagate infor-mation across nodes and learn an actuator-level policy. Similarly to NerveNet, the shared modular policies(SMP) method (Huang et al., 2020) learns a global policy that is shared across the limbs of a target agentand controls simultaneously different morphologies. The agents structure is encoded by a tree where anarbitrary limb acts as a root node. Information is propagated through the tree in two stages, from root toleaves and then backward. Kurin et al. (2020), however, show that constraining the exchange of informationto the structure of the system being controlled can hinder performance. This issue is a well-known problemin graph machine learning: the best structure to perform message passing does not necessarily correspondto the input topology (Topping et al., 2022; Rusch et al., 2023). Graph pooling (Ying et al., 2018; Bianchiet al., 2020; Grattarola et al., 2022; Bianchi & Lachi, 2024) tackles this problem by clustering nodes andrewiring the graph to learn hierarchical representations. Our work can be seen as introducing a similar idea ingraph-based RL. Moreover, in FGRL, the hierarchical structure corresponds to the structure of the decision-making process. A different research line on composable RL leverages Transformers (Vaswani, 2017) to learnuniversal policies designed for handling multiple morphologies simultaneously (Trabucco et al., 2022; Guptaet al., 2022; Xiong et al., 2023), possibly encoding geometric symmetries in the model (Chen et al., 2023).In this context, Xiong et al. (2024) propose an approach to improve the efficiency of existing multi-task RLmethods by generating policies using a morphology-conditioned hypernetwork (Ha et al., 2017). FRL (Dayan & Hinton, 1992) has been extended to the deep RL setting with the introduction of FeUdalNetworks (FUN) (Vezhnevets et al., 2017) and explored in the context of multi-agent systems (Ahilan &Dayan, 2019). However, none of the previous works match FRL with a hierarchical graph-based architectureto learn modular policies, as we do here instead.",
  "Preliminaries": "Markov decision processesA Markov decision process (MDP) (Sutton & Barto, 2018) is a tupleS, A, P, R where S Rns is a state space, A Rna is an action space, P : S A S is a Marko-vian transition function and R : S A R is a payoff (reward) function. We focus on the episodic RLsetting where the agent acts in the environment for a fixed number of time steps or until it receives a ter-mination signal from the environment. The objective is to learn a parameterized stochastic policy thatmaximizes the total expected reward received in an episode. We focus on environments where the staterepresentation can be broken down into sub-parts, each part mapped to a node of a graph structure.",
  "Feudal Graph Reinforcement Learning": "A physical system can often be described as a set of entities and relationships among these entities. As acase study, we consider structured agents (simplified robots), as those typically used in continuous controlRL benchmarks (Tassa et al., 2018). Such agents are modeled as made of several joints and limbs thatcorrespond to actuators of a system to control. The following section provides a method for extracting andexploiting hierarchical graph-based representations in RL.",
  "Torso": "Low arm (2) Up arm (1)Up arm (2) Low arm (1) Thigh (2) Shin (2) Thigh (1) Shin (1) : Constructing the agent graph G1 for Humanoidenvironment. Blue squares in the agents morphology rep-resent the joints of the agent and are not mapped to nodes,differently from the green labels which, instead, refer tothe limbs and constitute the nodes of G1. A structured agent with K limbs can be repre-sented as an undirected graph G1. The subscriptdenotes the level in the hierarchy and will be con-textualized in the next subsection. In this setup,each i-th limb with i {1, . . . , K} is mapped toa node whose feature vector sti S contains in-formation regarding its state at time t (e.g., theposition, orientation, and velocity of the node);an edge between two nodes indicates that the cor-responding limbs are connected. Each limb canbe paired with an actuator and outputs are asso-ciated with control actions ati A. Limbs withno associated action act as auxiliary hub nodesand simply account for the morphology of theagent; in practice, representations of such nodesare simply not mapped to actions and are discarded in the final processing steps. provides an exampleof the graph extraction process for the Humanoid environment from MuJoCo, in which the torso node actsas a simple hub for message passing.",
  "Building the Feudal Hierarchy": "The core idea of FGRL consists of exploiting a multi-level hierarchical graph structure G to model andcontrol a target system by leveraging a pyramidal decision-making architecture. In this framework, eachi-th node in the hierarchy reads from the representation of subordinate (child) nodes C(i) and assigns themgoals; in turn, it is subject to goals imposed by its supervisor (parent) nodes P(i) through the same goal-assignment mechanism. Each node has no access to state representations associated with the levels above,as the structure is layered. We identify three types of nodes: 1. Manager: It is a single node at the highest level of the hierarchy, i.e., it has no supervisor. Itreceives the reward directly from the environment and is responsible for coordinating the entirehierarchy.",
  "Learning Architecture": "Given the introduced hierarchical setup, this subsection illustrates the learning architecture and the proce-dure to generate actions starting from the environment state. In particular, we generate the initial repre-sentations of nodes in G starting from raw observations, in a bottom-up fashion. Subsequently, to take fulladvantage of both the feudal structure and the agents morphology, information is propagated across nodesat the same level as well as through the hierarchical structure. Finally, (sub-)managers set goals top-downthrough G, while workers act according to the received commands. We now break this process down into astep-by-step procedure, a visual representation of which is reported in . State representationThe environment state is partitioned and mapped to a set {si}Ki=1 of K local nodestates, each corresponding to an actuator. We omit the temporal index t as there is no ambiguity. Addi-tional (positional) node features {fi}Ki=1 can be included in the representation, thus generating observationvectors {xi}Ki=1, where, for each i-th limb, xi is obtained by concatenating si and fi. Starting from theobservation vectors, the initial representation h0i of each i-th node in G is obtained recursively as:",
  "lh(h0j),i Glh, lh {2, . . . , Lh}(2)": "where W1 Rdhdx is a learnable weight matrix, lh a (trainable) differentiable function and Aggr ageneric aggregation function. The superscript of h0i indicates the stage of the information propagation. Inpractice, the initial state representation of each worker is obtained through a linear transformation of thecorresponding observation vector, while, for (sub-)managers, representations are initialized bottom-up witha many-to-one mapping obtained by aggregating recursively representations at the lower levels of G.",
  "Environment": "Hierarchical graph Sub-managers Workers Manager : Learning architecture given the hierarchical graph G and the graphs Glh for the Walker environ-ment. Trainable functions are reported in red and hierarchical operations are represented with dashed lines:in G, information flows bottom-up, while goals are assigned top-down. Propagation layerInformation across nodes is propagated through message passing. Nodes at the lh-thlevel can read from representations of neighbors in Glh and subordinate nodes in Glh1, i.e., those correspond-ing to the lower level. We combine these 2 information flows in a single message-passing step. In particular,starting from the initial representations h0i , each round lr {1, . . . , Lr} of message passing updates thecorresponding state representation hlri as:",
  "lr,lh2(hlr1i, hlr1j, eij) ,(3)": "where the message function lr,lh1regulates the exchange of information among nodes at the same hierarchylevel, lr,lh2conveys, conversely, information from the subordinate nodes, and lr,lh updates the represen-tation. We comment that workers (nodes at the lowest level in the hierarchy) only receive messages fromneighbors in G1, while the top-level manager simply reads from its direct subordinates. We remark that, ingeneral, for each round lr and hierarchy level lh we can have different message and update functions. How-ever, multiple steps of message passing can lead to over-smoothing in certain graphs (Rusch et al., 2023).In such cases, the operator in Eq. 3 can be constrained to only receive messages from neighbors at the samelevel. Note that information nonetheless flows bottom-up at the initial encoding step (Eq. 2) and top-downthrough goals, as discussed below. Goal generationState representations are used to generate goals (or commands) in a recursive top-downfashion through G. In particular, each supervisor i Glh, with lh {2, . . . , Lh}, sends a local goal gij toeach subordinate node j C(i) as:",
  ",otherwise(4)": "where the superscript Lr denotes the last round of message passing (see Eq. 3). We remark that the top-level manager has no supervisor: goals here are directly generated from its state representation, whichencompasses the global state of the agent. All goal (or command) functions lh used by (sub-)managerscan be implemented, for example, as MLPs, while worker nodes do not have an associated goal-generationmechanism, but, instead, have a dedicated action-generation network.",
  "The action-generation function can be again implemented by, e.g., an MLP shared among workers. Weremark that actions associated with nodes that do not correspond to any actuator are discarded": "RewardsEach node of the hierarchy receives a different reward according to the associated goals andits role in the hierarchy.As already mentioned, the top-level manager coordinates the entire hierarchyand collects rewards directly from the environment. On the other hand, sub-managers and workers receiveintrinsic rewards that can be used either as their sole reward signal or added to the external one. As anexample, a dense reward signal for the i-th worker can be generated as a function of the received goals andthe state transition:",
  ",(6)": "where fR is a score function and si denotes the subsequent state. We remark that since goals are learnedand are (at least partially) a function of si, designing a proper score function fR is critical to avoid de-generate solutions. Nevertheless, the reward scheme outlined in Appendix D.2 and empirically validated inAppendix C.3 can be easily applied to many tasks without any prior knowledge. At each step, rewards ofnodes belonging to the same level lh are combined and then aggregated over time to generate a cumulativereward (or return) Rlh, which is subsequently used as a learning signal for that level. ScalabilityMPNNs are inductive, i.e., not restricted to process input graphs of a fixed size. As a result,the number of learning parameters mainly depends on the depth of the hierarchy, which is a hyperparameterthat depends on the problem at hand. Nevertheless, we remark that, in graph deep learning literature,hierarchies with a small number of levels (e.g., 3 levels) have proved to be effective even when dealing withlarge graphs (Wu et al., 2022), so there is usually no need for using very deep hierarchies.",
  "Modularity": "Additional results including comparisons with anon-modular baseline are reported in Appendix C.2.Tab. 1 provides a summary of the salient propertiesof each architecture. Additional details are reportedin Appendix D. For FGNN, message-passing lay-ers simply propagate representations among neigh-bors and not across different levels; messages flowbottom-up at the encoding step only. In models ex-ploiting the hierarchical structure, the intrinsic re-wards of lower levels incentivize behaviors that align nodes with the assigned goals. More precisely, at eachtime step, the intrinsic signal of a node is defined as a function of the cosine similarity between its statesand the received commands (refer to Appendix D.2 for a detailed explanation). Optimization algorithmIn FGRL, each graph Glh represents a structured agent in which each nodeacts according to a shared policy that is specific for that level. Each level is trained independently from theothers as it maximizes its own reward signal. In principle, the model could also be trained end-to-end, i.e.,as a single network, but this would most likely result in the collapse of goals representations, and would notallow for having different reward signals for each level. We provide a sensitivity analysis of this multi-leveloptimization approach in Appendix C.3. In principle, each level-specific policy can be paired with a standard RL optimization algorithm, but thehierarchical paradigm introduces additional challenges in the training procedure. As an example, instabilitiesat a single level can hinder global performance and make credit assignment more difficult: good commandspropagated by the upper layers are not rewarded properly if workers fail to select the correct actions.We use Covariance Matrix Adaptation Evolution Strategy (CMA-ES) (Hansen & Ostermeier, 2001) as theoptimization method since evolutionary algorithms have proven to be competitive with policy gradientmethods and less likely to get stuck in such sub-optimal solutions (Salimans et al., 2017); we provide acomparison with a standard gradient-based algorithm in Appendix C.2. Finally, policies of intermediate levels can commit to sub-optimal behaviors if they receive the intrinsicreward as the only learning signal. Therefore, similarly to Vezhnevets et al. (2017), we add the externalreward to the intrinsic one at each level. Further details regarding both the policy optimization and rewardscheme can be found in Appendix D.1 and D.2, respectively.",
  "Graph Clustering Problem": "Given a graph with communities and N nodes per community with random labels, the objective is tocluster nodes so that only nodes belonging to the same community have the same label, i.e., are assignedto the same cluster. The agents receive a sparse reward based on the MinCut loss (Bianchi et al., 2020)at the end of each episode, with an additional reward bonus for eventually solving the problem. Furtherdetails regarding observation space, action space, and reward function can be found in Appendix B. Thehierarchical graph G of both FGNN and FDS is a 3-level graph that is built by assigning a sub-managerto each community; all the sub-managers are then connected to a single top-level manager. As a result,goals sent to workers and sub-managers are conditioned to be a -dimensional vector representing a targetassignment at the node level and community level, respectively. Furthermore, we let the top-level managerand sub-managers act on different time scales by keeping the selected goals fixed for 10 and 5 time steps,respectively. All baselines have access to static coordinates f R2 of each node.",
  "FGNNFDSGNNDSFGNNFDSGNNDSFGNNFDSGNNDSFGNNFDSGNNDS": ": Average return and standard deviation of the considered agents on the MuJoCo benchmarks (av-eraged over 4 runs). Each generation refers to a population of 64 episodes. To ease the visualization, theplots show a running average with returns normalized w.r.t. the maximum obtained values, that are: 4025(Humanoid), 2817 (Walker), 1918 (Half Cheetah), and 3175 (Hopper).",
  "MuJoCo Benchmarks": "We consider the modular variant of 4 environments adapted from the standard MuJoCo locomotion tasks namely Humanoid, Walker2D, Hopper, and Half Cheetah. For these variants, we use the same setupintroduced by Huang et al. (2020): differently from the original environments where states (actions) areglobal, here the state (action) space is a collection of local limb states (actions), one for each actuatorof the agent. The composable nature of such variants makes GNNs suitable candidates for tackling theseproblems (Wang et al., 2018; Huang et al., 2020). In particular, the rationale for selecting these environmentsas benchmarks is that empirical evidence (Kurin et al., 2020) shows the limitations of flat GNNs in achievinggood coordination and motivates the adoption of our hierarchical approach in these settings. If agents do notcrash, episodes last for 1000 time steps; environment rewards are defined according to the distance covered:the faster the agent, the higher the reward. Nodes in the base graph are the actuators of the physical agent:hence, in hierarchical models (FGNN and FDS), workers are clustered using a simple heuristic, i.e., groupingtogether nodes belonging to the same body part (see Appendix D.3 for details). We report the results for the 4 agents in . Returns obtained by the FGNN architecture in Humanoidand Walker support the adoption of message passing within the feudal paradigm in structured environ-ments, where the agent requires a higher degree of coordination. Indeed, hierarchical graph-based policiesoutperform the baselines by a larger margin in the more complex tasks. In Half Cheetah, FGNN and GNNachieve similar results. A possible explanation for this behavior can be found by considering the morphologyof the agent itself: this agent cannot fall, i.e., there is no need for high-level planning accounting to maintainbalance. Similarly, in Hopper (i.e., the agent with the simplest morphology), good locomotion policies canbe learned without a high degree of coordination, making the architecture of the more sophisticated modelsredundant. As one would expect, then, the performance of FGNN and GNN for this agent is comparablewith that of DS, i.e., the simpler modular variant. In general, FDS obtains subpar performance across envi-ronments, suggesting that, in modular architecture, the feudal paradigm might not be as effective withoutany other coordination mechanism. Analysis of generated goalsThe feudal paradigm relies on the goal-generation mechanism, that es-tablishes communication among different hierarchical levels by propagating goals. This key feature enablespolicy improvement on both a global and local scale: the top-level manager has to generate goals alignedwith the external return, while sub-managers must learn to break down the received goals into subtasks. To investigate whether commands are generated coherently, we run t-SNE (Van der Maaten & Hinton, 2008)on goal vectors received by pairs of nodes with symmetric roles in the morphology of a trained Walker agentand analyze their time evolution during an episode. More precisely, for each pair of nodes (e.g., left and rightknee) we collect the goals signals received in an episode, compute the one-dimensional t-SNE embeddingsfrom such sample, and then plot the embeddings corresponding to the goals sent to the left and right nodes.",
  "leftright": ":Analysis of goals received by sub-managers (orange and blue lines) and workers(green and red lines) in an episode of a Walkerenvironment (refer to for the hierarchicalgraph). Plots show a 50-step running average. Empirical observations emerging from the results in provide experimental evidence to support the effective-ness of the proposed goal-propagation mechanism. First,we highlight that commands sent by the top-level man-ager to the intermediate sub-managers responsible forlegs (purple and blue nodes in ) show a tempo-ral structure that is coherent with the oscillating patternsof worker nodes. Indeed, goals are generated using a top-down approach (refer to Eq. 4) where commands assignedto lower levels are a function of those received by up-per levels; as such, the temporal structure is preserved.Second, while curves associated with workers intersect attime steps corresponding to the actual steps taken by thephysical agent, goal representations received by left andright sub-managers appear less predictable, and the cor-responding t-SNE embeddings are more diverse. Indeed,the abstraction of commands increases with the depth ofthe hierarchy: goals at the upper levels are not directlymapped into physical actions and should provide super-vision to the levels below.Conversely, goals assignedby sub-managers to workers become more specific and,e.g., depend on the specific joints being controlled. Thisanalysis shows that propagated goals are meaningful andcapture salient aspects of the learned gait. Results showcoordination emerging from the interaction of nodes atdifferent levels and support the adoption of such an ar-chitecture to implement hierarchical control policies.",
  "Conclusions and Future Works": "We proposed a novel hierarchical graph-based reinforcement learning framework, named Feudal Graph Re-inforcement Learning. Our approach exploits the feudal RL paradigm to learn modular hierarchical policieswithin a message-passing architecture. We argue that hierarchical graph neural networks provide the propercomputational and learning framework to achieve spatiotemporal abstraction. In FGRL, nodes are orga-nized in a multilayered graph structure and act according to a committee of composable policies, each with aspecific role within the hierarchy. Nodes at the lowest level (workers) take actions in the environment, while(sub-)managers implement higher-level functionalities and provide commands at levels below, given a coarserstate representation. Experiments on graph clustering and MuJoCo locomotion benchmarks together withthe in-depth analysis of the learned behaviors highlight the effectiveness of the approach. There are several possible directions for future research. In the first place, the main current limitation ofFGRL, shared among several HRL algorithms (Vezhnevets et al., 2017), is in the inherent issues in jointly andefficiently learning the different components of the hierarchy in an end-to-end fashion. Solving this limitationwould facilitate learning hierarchies of policies less reliant on the external reward, allowing state-of-the-artoptimization methods to be integrated into the framework while ensuring stability in the training process.In this regard, different implementations of the intrinsic reward mechanism could be explored. Lastly, whilein this paper we addressed limitations of flat graph-based approaches, future directions might investigate theeffectiveness of the proposed framework in scenarios where credit assignment is more challenging. Preliminaryexperiments conducted in Sec. 5.2 show promising results in this sense.",
  "Peter Dayan and Geoffrey E Hinton. Feudal reinforcement learning. Advances in neural information pro-cessing systems, 5, 1992": "Jonas Degrave, Federico Felici, Jonas Buchli, Michael Neunert, Brendan Tracey, Francesco Carpanese, TimoEwalds, Roland Hafner, Abbas Abdolmaleki, Diego de Las Casas, et al. Magnetic control of tokamakplasmas through deep reinforcement learning. Nature, 602(7897):414419, 2022. Niklas Funk, Georgia Chalvatzaki, Boris Belousov, and Jan Peters. Learn2assemble with structured represen-tations and search for robotic architectural construction. In Conference on Robot Learning, pp. 14011411.PMLR, 2022a. Niklas Funk, Svenja Menzenbach, Georgia Chalvatzaki, and Jan Peters. Graph-based reinforcement learn-ing meets mixed integer programs: An application to 3d robot assembly discovery. In 2022 IEEE/RSJInternational Conference on Intelligent Robots and Systems (IROS), pp. 1021510222. IEEE, 2022b. Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural messagepassing for quantum chemistry. In International conference on machine learning, pp. 12631272. PMLR,2017.",
  "Nikolaus Hansen and Andreas Ostermeier. Completely derandomized self-adaptation in evolution strategies.Evolutionary computation, 9(2):159195, 2001": "Wenlong Huang, Igor Mordatch, and Deepak Pathak.One policy to control them all: Shared modularpolicies for agent-agnostic control.In International Conference on Machine Learning, pp. 44554464.PMLR, 2020. Jiechuan Jiang, Chen Dun, Tiejun Huang, and Zongqing Lu. Graph convolutional reinforcement learning.In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April26-30, 2020. OpenReview.net, 2020. URL",
  "Diederik P Kingma and Jimmy Ba.Adam:A method for stochastic optimization.arXiv preprintarXiv:1412.6980, 2014": "Vitaly Kurin, Maximilian Igl, Tim Rocktschel, Wendelin Boehmer, and Shimon Whiteson. My body is acage: the role of morphology in graph-based incompatible control. In International Conference on LearningRepresentations, 2020. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang,Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions withhuman feedback. Advances in Neural Information Processing Systems, 35:2773027744, 2022.",
  "T Konstantin Rusch, Michael M Bronstein, and Siddhartha Mishra. A survey on oversmoothing in graphneural networks. arXiv preprint arXiv:2303.10993, 2023": "Tim Salimans, Jonathan Ho, Xi Chen, Szymon Sidor, and Ilya Sutskever. Evolution Strategies as a Scal-able Alternative to Reinforcement Learning, September 2017. URL arXiv:1703.03864 arXiv:1703.03864 [cs, stat]. Alvaro Sanchez-Gonzalez, Nicolas Heess, Jost Tobias Springenberg, Josh Merel, Martin Riedmiller, RaiaHadsell, and Peter Battaglia. Graph networks as learnable physics engines for inference and control. InInternational Conference on Machine Learning, pp. 44704479. PMLR, 2018.",
  "John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimiza-tion algorithms. arXiv preprint arXiv:1707.06347, 2017": "David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, ThomasHubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al.Mastering the game of go without humanknowledge. nature, 550(7676):354359, 2017. Richard S. Sutton and Andrew G. Barto. Reinforcement learning: an introduction. Adaptive computationand machine learning series. The MIT Press, Cambridge, Massachusetts, second edition edition, 2018.ISBN 978-0-262-03924-6.",
  "Zheng Xiong, Jacob Beck, and Shimon Whiteson. Universal morphology control via contextual modulation.In International Conference on Machine Learning, pp. 3828638300. PMLR, 2023": "Zheng Xiong, Risto Vuorio, Jacob Beck, Matthieu Zimmer, Kun Shao, and Shimon Whiteson. Distillingmorphology-conditioned hypernetworks for efficient universal morphology control. In Forty-first Interna-tional Conference on Machine Learning, 2024. URL Zhitao Ying, Jiaxuan You, Christopher Morris, Xiang Ren, Will Hamilton, and Jure Leskovec. Hierarchi-cal graph representation learning with differentiable pooling. Advances in neural information processingsystems, 31, 2018. Chao Yu, Akash Velu, Eugene Vinitsky, Jiaxuan Gao, Yu Wang, Alexandre Bayen, and Yi Wu. The surprisingeffectiveness of ppo in cooperative multi-agent games. Advances in Neural Information Processing Systems,35:2461124624, 2022.",
  "AExtraction of the Hierarchical Graph": "In we report an example of extraction of the hierarchical graph G for the Humanoid environment;the base graph G1 is obtained directly from the agents morphology (see for reference) and the numberof layers of G is a hyperparameter. WorkersSub-managersManager Agent graphs Hierarchical graph Clustering Clustering Clustering Multi-level structure : Extraction of the hierarchical graph G starting from the workers graph G1 of the Humanoidenvironment. Hierarchical edges of G are represented as dashed lines and denote parent-child relationships.As shown in the second level, each node can have more supervisors.",
  "BGraph Clustering Environment": "The environment of the synthetic graph clustering problem is defined by a -community graph with Nnodes for each community, that is, a community graph with N nodes. The objective is to cluster thegraph so that each community has a unique label for all the nodes, different from other communities. Foreach simulation, we generate a (, N) graph with binary adjacency matrix A {0, 1}NN and degreematrix D = diag(A1N), under the assumption that there are no isolated nodes. Each node i has a staticfeature vector given by the fi R2 coordinates: nodes belonging to the same community are characterizedby similar features. At each time step t, the observation vector xti of each i-th node is the concatenationof the one-hot encoding of the current cluster assignment sti, its coordinates, and the normalized remainingtime steps before the end of the episode:",
  "xti = sti || fi|| 1 t/T(7)": "We remark that assignments are initialized at random at the beginning of each episode. The action is a 3-dimensional vector (left/no-op/right) that allows nodes to change cluster by possibly changing its label of atmost one index; periodic boundary conditions are applied on the label vector. We consider an episodic settingwhere episodes last for T = 50 time steps, unless the graph reaches one of the possible target configurationswhere nodes belonging to the same community have the same label. The sparse reward at the terminationtime step t T is given by:",
  "AD 1": "2 RNN is the normalized adjacency matrix, and D isthe degree matrix of A. The first two terms represent the negative of the MinCut loss (Bianchi et al., 2020):the first one promotes clustering among strongly connected nodes, while the second one prevents degeneratesolutions by ensuring similarity in size and orthogonality between different clusters. We remark that, forthis maximization problem, those terms are bounded in and , respectively: therefore, the bonusfactor (T t) strongly encourages solving the task with as few iterations as possible. We consider the taskas solved if the running average of the success rate percentages of the last 20 evaluation samples is greaterthan 95%.",
  "H(X) + H(Y ),(9)": "where H() and I(X; Y ) denote the entropy of the labels and mutual information, respectively. This metric isbounded in , where the boundaries represent perfect dissimilarity (NMI = 0) and similarity (NMI = 1),and is invariant under labels permutation. Hence, in our setting, we measure the NMI score of the predictedclustering w.r.t. a generic target configuration where nodes belonging to the same community have the samecluster-specific label. In Tab. 2 we report the maximum (blue) and minimum (red) values achieved by themodels in each configuration ( and 4 for reference).",
  "C.2Comparison with Non-modular Baselines": "The FGRL framework is specifically designed for learning modular policies. Therefore, the experiments con-ducted in Sec. 5.2 and 5.3 validate the effectiveness of the full proposed methodology (FGNN) in comparisonto other modular approaches, such as flat GNNs. Nevertheless, to better contextualize the results, here weprovide a comparison with an MLP (a non-modular policy) where each state and action are the concatena-tion of node-level observations and actions, respectively. Being non-modular, it cannot be directly applied todifferent morphologies and the model size increases with the number of nodes. We remark that, in Sec. 5.2and 5.3, all the models were trained using the same learning algorithm, i.e., CMA-ES (Hansen & Ostermeier,2001), to highlight the performance differences brought by each architectural variant rather than those com-ing from the learning algorithm. Specifically, we chose evolution strategies because, as mentioned in Sec. 5.1,learning a hierarchy of level-specific policies using independent reward signals is challenging. In this regard,this section provides a comparison against two different MLP baselines that use different learning algorithms:one is trained with CMA-ES to assess the difference in performance w.r.t. the composable FGNN, while theother uses Proximal Policy Optimization (PPO) (Schulman et al., 2017) and allows us to analyze the resultsin the broader context of gradient-based methods. For simplicity, we denote the two variants as MLP andPPO, respectively. Hyperparameters are reported in Appendix D.4 and the code for PPO was adapted froma public repository (Barhate, 2021). We chose PPO as a gradient-based algorithm because of its popularityand wide applicability in both single and multi-agent reinforcement learning problems (Yu et al., 2022). In general, evolution strategies rely on a selection process that evaluates a sample (population) of parametersfor each update step, resulting in worse sample efficiency compared to gradient-based methods that can",
  "FGNNMLPPPO (max)PPO (mean)FGNNMLPPPO (max)PPO (mean)FGNNMLPPPO (max)PPO (mean)FGNNMLPPPO (max)PPO (mean)": ": Comparison of the average return achieved with FGNN (4 runs) and MLP (4 runs) with averageand maximum value of PPO (5 runs) at the last training step in the 4 MuJoCo benchmarks. To ease thevisualization, the plots show a running average with returns normalized w.r.t. the maximum obtained values,that are: 4025 (Humanoid), 3125 (Walker), 2402 (Half Cheetah), and 3950 (Hopper). instead perform multiple updates with a smaller sample. Therefore, in we report the average returnsobtained with FGNN and MLP, together with the mean and maximum values among 5 independent seedsof PPO at the last training step, i.e., 2 107; we applied a running average to account for fluctuations. PPOhyperparameters were tuned on the Walker agent (as we similarly did for CMA-ES). In this scenario, theperformance of FGNN and MLP is comparable with PPO, highlighting that evolution strategies allow toachieve absolute returns that are competitive w.r.t. a widely used gradient-based method. Conversely, PPOappears to be less robust when the same hyperparameters are used to learn locomotion policies for othermorphologies. FGNN performs similarly or better than the MLP baseline in all the environments, except forHopper: as already mentioned, this agent has the simplest morphology and, as expected, a simple MLP isenough to learn a good policy. Furthermore, we qualitatively observed that FGNN learns more natural gaitsin Humanoid and Walker.",
  "C.3Analysis of the Multi-Level Optimization": "The proposed feudal framework relies on a multi-level hierarchy where nodes at each level Glh act according toa level-specific policy trained to maximize its own reward. Notably, the policy trained at level lh completelyignores rewards received at different levels: coordination completely relies on the message-passing and goal-generation mechanisms. This peculiar aspect implies that each policy can have its own policy optimizationroutine. To analyze the impact of such a multi-level structure and optimization routine, we compare results obtainedby a 3-level FGNN model in the Walker environment against 1) a 3-level FGNN model without intrinsicreward, 2) a 2-level FGNN model, and 3) a 3-level FGNN model where all the policies are jointly trained tomaximize the external reward only. We remark that the number of learnable parameters is the same for allthe 3-level variants. As shown in , our method achieves the highest average return, while baselines aremore likely to get stuck in sub-optimal policies (e.g., by learning to jump rather than to walk). Comparingthe full model (blue curve) with the variant without intrinsic reward (left, red curve) highlights the advantageof the hierarchical intrinsic reward mechanism. In the full 3-level FGRL model, the reward at each levelincentivizes workers and sub-managers to act accordingly to the received commands. This auxiliary taskturns out to be instrumental in avoiding sub-optimal gaits. The 2-level model (middle, green curve) performsakin to the variant without intrinsic reward, hence hinting at the benefits of using a deeper hierarchy foragents with complex morphologies. Lastly, for the variant where all levels are jointly optimized (right, blackcurve), empirical results show that the resulting agent does not only achieve a lower return but, surprisingly,it is also less sample efficient.",
  "levelssingle optimizer": ": Analysis of the 3-level FGNN model with variants that use a different reward choice (left), depthof the hierarchy (middle), and optimization scheme (right) in a Walker environment (4 seeds for eachvariant). Returns are reported with standard deviation and each generation refers to a population of 64episodes. To ease the visualization, the plot shows a running average with returns normalized w.r.t. themaximum obtained value, i.e., 2725.",
  "Manager CMA-ES (level Lh): functions Lh and Lh": "Experiments were run on a workstation equipped with AMD EPYC 7513 CPUs. Depending on the modeland environment, each seed can take from 30 minutes to 1 day for the graph clustering experiment andfrom 12 hours to 3 days for the MuJoCo benchmarks. Code to reproduce experiments is available online at",
  "D.2Intrinsic Reward": "In variants that take advantage of the hierarchical structure (FGNN and FDS), we define the intrinsic rewardof sub-managers and workers as a signal that measures their alignment with the assigned goals. At eachtime step t, for each hierarchical level we add the environment reward to the average intrinsic signal. Forboth environments, the reward scheme was inspired by Vezhnevets et al. (2017). Graph clustering problemIn this environment, each i-th node has a single (sub-)manager P(i) andpropagated goals represent a target label. At each time step, the reward of each i-th worker is defined as:",
  "D.3Hierarchical Graphs": "The graphs under analysis for the MuJoCo benchmarks have a low number of nodes, and each actuatorhas a proper physical meaning in the morphology of the agent. Thus, we decide to create the hierarchicalgraphs using heuristic. As an example, in the Walker agent we expect nodes of the same leg to be clusteredtogether, and the associated sub-manager to be at the same hierarchical level as that of the other leg: inthis way, the topology of the intermediate graph reflects the symmetry of the agent graph G1. The hierarchical graphs for the FGNN model are reported in . Notice that the Hopper agent has asimple morphology with no immediate hierarchical abstraction and where each actuator has a different role:as a consequence, a meaningful hierarchy cannot be trivially extracted, and results revealed no benefit inimplementing a 3-level hierarchy for this agent. We remark that hierarchical graphs used in the FDS variantare not reported because in all the environments empirical evidence did not show improvements as the depthof the hierarchy increased, leading to 2-level hierarchical graphs where all the workers are connected to asingle top-level manager (see Hopper in for an example)."
}