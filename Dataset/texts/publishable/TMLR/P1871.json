{
  "Abstract": "In the field of neuroimaging, accurate brain age prediction is pivotal for uncovering the com-plexities of brain aging and pinpointing early indicators of neurodegenerative conditions.Recent advancements in self-supervised learning, particularly in contrastive learning, havedemonstrated greater robustness when dealing with complex datasets. However, currentapproaches often fall short in generalizing across non-uniformly distributed data, prevalentin medical imaging scenarios. To bridge this gap, we introduce a novel contrastive loss thatadapts dynamically during the training process, focusing on the localized neighborhoods ofsamples. Moreover, we expand beyond traditional structural features by incorporating brainstiffnessa mechanical property previously underexplored yet promising due to its sensitiv-ity to age-related changes. This work presents the first application of self-supervised learningto brain mechanical properties, using compiled stiffness maps from various clinical studies topredict brain age. Our approach, featuring dynamic localized loss, consistently outperformsexisting state-of-the-art methods, demonstrating superior performance and paving the wayfor new directions in brain aging research.",
  "Introduction": "Aging causes significant changes in the structure and function of the brain. Magnetic Resonance Elastography(MRE) has recently emerged as a non-invasive technique to measure mechanical brain properties (Hiscoxet al., 2016), such as stiffness , that is, the resistance of a viscoelastic material to an applied harmonic force.Studies have shown a promising age sensitivity of whole-brain stiffness measurements (Hiscox et al., 2021),surpassing established neuroimaging age biomarkers, such as volume measurements (Sack et al., 2011). Withadvancing MRE protocols, improvements in resolution have enabled the study of stiffness changes in morelocalized brain regions, revealing regional age-related variability (Murphy et al., 2013; Arani et al., 2015;Takamura et al., 2020; Hiscox et al., 2018; Delgorio et al., 2021). Furthermore, clinical studies in patients",
  ": Overview of Contrastive Regression Losses. This details existing methods each employing distinctstrategies to refine the contrastive learning process for regression tasks": "with neurodegenerative diseases have revealed stiffness alterations exceeding those observed in healthy aging(Murphy et al., 2016; Hiscox et al., 2020a). However, current methods are limited to region-wide averagesand thus fail to exploit the rich information available in stiffness maps, which can be accessed throughnonlinear relationships at a voxel level. Brain age prediction leverages neuroimaging data through machine learning by casting it as a regressionproblem, wherein models are trained on healthy samples to establish a baseline trajectory for aging. Regres-sion is a statistical method used to predict a continuous outcomesuch as agebased on input data, in thiscase, brain imaging data. This approach is particularly promising for identifying deviations from normalaging processes that might indicate neurological conditions. This approach has traditionally been appliedto structural brain features using T1-weighted MRI (Baecker et al., 2021). More recently, the scope hasexpanded to include features obtained from resting-state fMRI, diffusion imaging and MRE are also beingexamined (Lund et al., 2022; Niu et al., 2020; Clements et al., 2023). Additionally, its potential in detectingand the prognosis of neurodegenerative disorders such as Alzheimers Disease (AD) is gaining significant at-tention (Lee et al., 2022). In parallel, modeling approaches have evolved from traditional regression methodsto state-of-the-art self-supervised learning (Zha et al., 2024; Dufumier et al., 2021; Barbano et al., 2023).Inspired by advancements in computer vision, self-supervised learning techniques, particularly contrastivelearning methods, have been effectively adapted for predicting brain age from structural MRI scans (Dufu-mier et al., 2021; Barbano et al., 2023). Contrastive learning, a technique that learns by comparing pairs ofexamples, enhances model performance by structuring the embedding space to bring samples with similarages closer together and push dissimilar samples further apart. Despite their potential, current methods often struggle with generalization, particularly across datasetscharacterized by non-uniform distributions. To address this limitation, we introduce a novel contrastiveloss that specifically focuses on localized sample neighborhoods. This method is distinctively designed toadapt dynamically across different stages of training, thereby enhancing model performance where traditionalapproaches falter. Furthermore, considering the demonstrated age sensitivity of mechanical properties com-pared to structural brain properties, this study is pioneering. It represents the first application of contrastivelearning to brain stiffness maps, highlighting a new direction in neuroimaging research. Our contributionsare summarized next. We introduce a adaptive neighborhoods approach, specifically tailored for the framework of contrastiveregression learning. Our new method is designed to address the major challenge of limited generalizabilityin medical image analysis, particularly within datasets characterized by non-uniform distributions.Byconcentrating on these challenging distributions, our approach not only enhances the robustness of themodels but also performance.",
  "Related Work": "Regression Task Regression is a statistical technique that establishes a relationship between a set ofindependent variables (X) and a dependent variable (Y ), through a function f : X Y .Regressionspecifically addresses continuous variables, with Y taking values in R. Known for its robust effectiveness,regression has made significant impacts across various domains including (Fanelli et al., 2011; Sun et al., 2012;Lathuilire et al., 2019). Recent decades have seen a surge in research aimed at advancing deep regressiontechniques, significantly enhancing their performance and applicability for example, the works of that (Gaoet al., 2018; Rothe et al., 2015; Li et al., 2021; Cao et al., 2020; Yang et al., 2021). Another interesting familyof techniques, which is the focus of this work, falls within the contrastive learning family. This perspectiveis particularly interesting due to its ability to enhance learning by emphasizing differences between samples,thereby improving model robustness and generalization (Zha et al., 2024; Barbano et al., 2023). Contrastive Learning. To construct semantically rich and structured representations, contrastive learn-ing has become a widely adopted method for self-supervised representation learning. This technique involvesdifferentiating between similar (positive) and dissimilar (negative) pairs of data samples xi and xk, withthe goal of adjusting the distances between representations in the embedding spacebringing similar itemscloser and pushing dissimilar ones apart (Chen et al., 2020b; Khosla et al., 2020). Contrastive learninginitially gained popularity through its applications in computer vision, where it demonstrated significantimprovements in tasks such as image classification and object detection. Early methods like SimCLR (Chenet al., 2020b) introduced a simple yet effective framework that maximized agreement between differentlyaugmented views of the same data sample. This approach utilized a contrastive loss function to bring repre-sentations of augmented pairs (positive pairs) closer while pushing apart representations of different samples(negative pairs). Building on these foundations, a notable advancement was the introduction of supervisedcontrastive learning (Khosla et al., 2020), which extended the contrastive loss to leverage label information,treating all samples with the same label as positives. This approach enhanced the performance of models byincorporating supervised information into the self-supervised learning framework. As we shift focus from classification to regression problems, the distinction between positive and negativepairs transitions to a continuous spectrum. This shift necessitates the models ability to discern varyingdegrees of similarity, represented as si,k = sim(f(xi), f(xk)), beyond mere categorical differentiation. Par-ticularly, we are interested in brain age prediction, which requires precise modeling of age-related changesin brain structure and function. In response to the challenge of integrating continuous labels such as age,recent advancements propose strategies such as the Y-Aware loss (Dufumier et al., 2021), which softens theboundary between positive and negative samples. Similarly, the work of that (Barbano et al., 2023) proposedthe Threshold and Exponential losses, which adjust the strength of alignment and repulsion based on thesimilarity between continuous labels. Another work introduced the Rank-N-Contrast loss (Zha et al., 2024),which employs a comparative ranking strategy among samples. This method ranks samples based on theirsimilarity to a given anchor, creating a ranking-based continuous spectrum of positive and negative pairs. Curriculum Contrastive Learning. Curriculum learning, which introduces training samples in a pro-gressively challenging manner, has been effectively applied to contrastive learning to improve model gener-alization. In this context, simpler pairs of samples are presented early in training, with more difficult pairsintroduced gradually. Chu et al. (Chu et al., 2021) proposed CuCo, a graph representation learning methodthat leverages curriculum contrastive learning to enhance performance on graph tasks. Similarly, Wang etal.(Wang et al., 2024) applied curriculum contrastive learning to self-supervised depth estimation underadverse weather conditions, showing improved robustness in challenging environments. These approachesdemonstrate that progressively increasing difficulty in sample pairs can help models learn more effectivelyin both graph and visual tasks, making curriculum contrastive learning a promising strategy for variousdomains.",
  "Published in Transactions on Machine Learning Research (10/2024)": "Rasmus Rothe, Radu Timofte, and Luc Van Gool. Dex: Deep expectation of apparent age from a singleimage. In Proceedings of the IEEE international conference on computer vision workshops, pp. 1015,2015. 3, 9 Ingolf Sack, Kaspar-Josche Streitberger, Dagmar Krefting, Friedemann Paul, and Jrgen Braun. The influ-ence of physiological aging and atrophy on brain viscoelastic properties in humans. PloS one, 6(9):e23451,2011. 1 Faria Sanjana, Peyton L Delgorio, Lucy V Hiscox, Theodore M DeConne, Joshua C Hobson, Matthew LCohen, Curtis L Johnson, and Christopher R Martens. Blood lipid markers are associated with hippocam-pal viscoelastic properties and memory in humans. Journal of Cerebral Blood Flow & Metabolism, 41(6):14171427, 2021. 8",
  "Problem Setup": "The primary challenge in brain age modeling lies in accurately mapping high-dimensional brain imaging datato a continuous age variable. Traditional contrastive learning methods are limited in their capacity to handlethe subtle variations in brain stiffness associated with aging due to their global approach. Our proposedtechnique introduces a dynamic, localized strategy, focusing on progressively capture age-related featureseffectively. Importantly, our adaptive neighborhoods method progressively reduces the number of repulsedsamples during training, unlike static nearest-neighbor methods like NNCLR (Dwibedi et al., 2021). Thisadjustment is critical for capturing the continuous nature of regression tasks, such as brain age prediction.Formally, in brain age modeling, our objective is to train a neural network capable of accurately mappingbrain images x X to their corresponding target ages y R, neural network capable of accurately mappingbrain images, where X Rn represents the space of high-dimensional brain imaging data (e.g., stiffnessmaps). The model comprises two key components: a feature encoder f : X Z, which transforms brainimages into an embedding space Z Rd, and an age predictor g : Z R, tasked with estimating the agefrom these features.",
  "Adaptive Neighborhoods": "The adaptive neighborhoods technique is a cornerstone of our methodology, designed to optimize the con-trastive learning framework specifically for the regression tasks inherent in brain age modeling. This sectionoutlines the detailed mechanics of this technique, including model components, operational processes, andalgorithmic strategies. To enhance the precision of contrastive learning, we introduce a adaptive neighborhoods approach thatprogressively explores varied scales within the embedding space, as depicted in . This methodologysystematically adjusts the selection of repulsion candidates, taking into account both their proximity andthe evolutionary stage of training. The selection process for repulsed samples is defined by:",
  "neighbors of fepoch(xi) based on d(fepoch(xk), fepoch(xi))}(1)": "This expression delineates the set of samples subject to repulsion, identified by their distance d: ZZ R+.Our approach progressively narrows the scope of nearest neighbors involved in the repulsion process, therebyfocusing the learning on increasingly localized neighborhoods within the embedding space. This adaptivemechanism is governed by two critical hyperparameters: the final count of nearest neighbors,NNnb,final,representing the ultimate scope of repulsion at the end of training, and the decrement frequency, NNstep size,which specifies the interval of epochs for adjustments in the neighbor count, as detailed in Algorithm 1. In datasets that show non-uniform distributions, especially those with multi-modal characteristics, it iscommon to find some target areas oversampled and others undersampled. This scenario is typical in neu-roimaging datasets (see ). Our dynamic localized strategy is designed to address this issue. It startsby segregating distinct groups and then the training objective evolves to focus exclusively on those groups.This process is illustrated in . Our approach aims to reveal more coherent representations throughoutthe dataset. Following the methodology proposed by (Barbano et al., 2023), we utilize kernel functions to determine thedegrees of positiveness, wi,k = K(yi yk) where 0 wi,k 1, between pairs of samples. This metric isdefined by the function K(yi yk) and reflects the age similarity between two samples. A higher value of",
  "NNnb batch size (NNnb decrement per step steps completed)NNnb max(NNnb, NNnb, final)": "wi,k suggests that the samples are close in age, prompting the algorithm to minimize the distance betweentheir representations in the embedding space. Conversely, a lower value indicates a significant age difference,thus leading to an increase in the distance between their embeddings. Each sample xi in a batch is compared against every other sample xk, with the embeddings being adjustedbased on their relative age similarities. The selection of nearest neighbors for this comparison, NNnb(epoch),dynamically changes as the training progresses. Initially, a larger set of neighbors is considered to establishbroad relationships. As training advances, this number is progressively reduced to focus on more immediateand relevant interactions, thus refining the learning towards localized features. The per-sample adaptiveneighborhoods loss reads:",
  ".(2)": "This expression calculates the contribution of each sample pair to the overall loss.It normalizes thesecontributions by the sum of positiveness weights for all comparisons within the batch, adjusting the impactof each pair based on their age-related similarity. The softmax function is then applied to these normalizedand adjusted similarity scores si,k, defined by s : Z Z R+, which are recalculated for each dynamicallydefined nearest neighbor set. What is the Intuition Behind Our Adaptive Neighborhoods? Weaddress the challenge of non-uniform data distributions in neuroimaging datasets.Traditional learningmodels often fail to adequately distinguish between different age groups when these groups are unevenlyrepresented in the training data. Our approach refines this by adjusting embeddings dynamically. This isdone via (2) that represents the per-sample loss function in a contrastive learning framework, which aims todynamically adjust the embeddings based on age-related similarities. The essence of this equation lies in itsability to modulate the degree of repulsion or attraction between samples within the same batch based ontheir age proximity, which is quantified by wi,k the positiveness weight. This formulation allows for adaptivelearning where the focus is progressively shifted toward more challenging or informative pairs, potentiallythose that are not well-aligned in age, thus encouraging the model to learn finer distinctions as trainingprogresses.",
  ".(3)": "This aggregated loss function is key for structuring the embedding space optimally, ensuring that sampleswith similar ages are located closer together while those with significant age differences are distanced. Sucha configuration not only enhances the accuracy of age prediction but also demonstrates the models abilityto learn semantically structured representations based on age-related patterns.",
  "Data Description": "We have assembled a dataset of 311 three-dimensional (3D) brain stiffness maps obtained from healthycontrol subjects (HC). These data were sourced from multiple clinical studies, all of which utilized highlysimilar Magnetic Resonance Elastography (MRE) protocols. This ensures consistency across the collecteddata. Although our dataset contains only 311 subjects, it represents one of the largest collections of brainstiffness maps, spanning a wide age range and offering a balanced distribution of male and female subjects.For detailed information on the individual studies and the data collection methods, please refer to .All datasets were collected in accordance with ethical standards, under protocols approved by the respectivelocal institutional review boards. The age distribution of all samples from each study is illustrated in . While age-related non-uniformityis evident in the dataset , further variability in brain coverage, as shown in Appendix , adds complexityto our analysis of stiffness maps. Following this, stiffness maps, depicted in , were processed to enhancequality and uniformity. Initially, each map underwent skull stripping using Freesurfer (Fischl, 2012), a stepto isolate the brain tissue from non-relevant anatomical structures. Subsequently, we applied a bias fieldcorrection to remove intensity gradients that could affect subsequent analyses.",
  "Age: 26": ": Comparison of Neuroimaging Modalities. Each row shows three orthogonal views (sagittal, coronal,and axial) of the brain images, highlighting the differences in mechanical and structural properties acrossdifferent ages. To address the issue of data heterogeneity across different studies, we performed affine registration of theimages to the MNI152 template.This registration was conducted at an isotropic resolution of 2 mm3 using ANTs (Avants et al., 2009), ensuring consistent orientation and scale among all datasets. Finally, wenormalized the quantitative stiffness images, setting their mean to zero and standard deviation to one acrossthe dataset.",
  "Evaluation Protocol": "Our evaluation strategy involved a 3D ResNet-18 model (33.5M parameters), which was pre-trained on theopenBHB dataset (Dufumier et al., 2022), containing over 5000 T1 3D MRI brain images from multiplescanning sites, using the best reported method from the OpenBHB challenge (Dufumier et al., 2022). Toenhance generalization to our stiffness dataset, we utilized quasi-raw images, ensuring uniform image pre-processing, and downsampled the structural MRI images to 2 mm3 isotropic resolution. We selected ResNet-18, the smallest variant of the ResNet architecture, to match the scale of our dataset. The pre-trainedResNet-18 underwent full fine-tuning (i.e. updating all weights) on our brain stiffness dataset, following an80:20 train-test split, over 50 epochs and a batch size of 32 using the Adam optimizer. This optimization",
  "Total-31141.021.9183:128": ": Age Distribution of Participants from Multi-Site MR Elastography Studies.Contribution tothe 311 healthy control (HC) stiffness brain maps of different clinical studies is highlighted in color. Thedistribution is bimodal, indicating two predominant age groups among the subjects. included an initial learning rate of 1104, decreased by 0.9 every 10 epochs, and a weight decay of 5105.Hyperparameters NNnb,final and NNstepsize were optimized via random search across 30 iterations. Ourimplementation is based on Barbano et al.(Barbano et al., 2023).As contrastive learning frameworksSimCLR and NNCLR require data augmentations, Gaussian Noise was applied for these methods. Ourmodels were trained using an NVIDIA A100-SXM-80GB GPU. Following the training of the representations,we employed a Ridge Regression estimator (Barbano et al., 2023) on top of the frozen encoder to predictage. As an evaluation metric, we calculated the mean absolute error (MAE) on the test set, averaging theresults across five random seeds.",
  "Results and Discussion": "We begin by evaluating the representations of stiffness maps learned using our adaptive neighborhoods lossas in (3) with Manhattan distance, NNnb,final = 14 and NNstepsize = 1, selected via random search, againstthose using current state-of-the-art contrastive classification and regression losses. illustrates the effectiveness of our proposed Dynamical Localized Repulsion approach in the contextof brain age prediction from stiffness maps, as evidenced by the Mean Absolute Error (MAE) metric. No-tably, our method significantly outperforms contrastive classification losses such as SimCLR (Chen et al.,",
  "Contrastive Regression Loss": "Rank-N-Contrast (Zha et al., 2024)5.266 0.587Y-Aware (Dufumier et al., 2021; Barbano et al., 2023)3.852 0.212Threshold (Barbano et al., 2023)4.420 0.503Exponential (Dufumier et al., 2021; Barbano et al., 2023)3.824 0.215Adaptive Neighborhoods (Ours)3.724 0.220 2020b) and NNCLR (Dwibedi et al., 2021), which achieve higher MAEs of 9.600 1.701 and 8.526 1.442,respectively. When comparing our method to existing state-of-the-art contrastive regression losses, our ap-proach demonstrates superior accuracy in predicting brain age. Upon careful examination, we can observethat Rank-N-Contrast (Zha et al., 2024) shows the highest MAE, suggesting it may be less adept at capturingthe nuanced patterns within the data necessary for precise age prediction. Y-Aware and Exponential (Du-fumier et al., 2021; Barbano et al., 2023) losses show improvements over Rank-N-Contrast. These methodsappear to better align with the underlying age-related changes in brain stiffness but still fall short comparedto our approach. Threshold (Barbano et al., 2023) loss offers a competitive performance, yet it does notachieve the same level of accuracy as our method. This could indicate that while it handles some aspectsof the data variability effectively, it might not fully capture the localized age-related changes as our methoddoes. Our proposed loss achieves the lowest MAE, underscoring its ability to more accurately model the age-related changes in brain stiffness. This suggests that our methods focus on localized sample neighborhoodsand its dynamic adaptation during the training process significantly contribute to its improved performance. To investigate the role of the distance norm for nearest neighbor selection of our adaptive neighborhoodsapproach, we conduct an ablation study, examining the impact of various distance norms on the modelsperformance, as detailed in a. The method shows robustness regarding the choice of distance norm.Our findings reveal that the Manhattan norm achieves the lowest MAE of 3.7240.220 years, outperformingthe Cosine (MAE = 3.748 0.142 years), Euclidean (MAE = 3.806 0.154 years), and Chebyshev norm(MAE = 3.842 0.196 years). In addition to the primary evaluation of the contrastive regression loss, we conducted a comparison of differentauxiliary loss functions to investigate their impact on the final performance. Specifically, we evaluated theMean Squared Error (MSE), Mean Absolute Error (L1), Huber loss, and DEX loss (Rothe et al., 2015).For this experiment, the encoder was first trained using LAdapN. The trained encoder was then frozen,and a separate predictor trained on top of the fixed representations using the auxiliary loss function. Theresults, shown in b, indicate that the MSE loss performs best with a MAE of 3.724 0.220 years.Both L1 and Huber losses also showed strong performance, 3.795 0.254 years and 3.973 0.199 yearsrespectively. However, the DEX loss performed significantly worse in this setup, producing a final MAE of5.7590.429 years, suggesting that traditional regression-based losses like MSE are better suited for this taskwhen combined with the contrastive learning representations compared to classification-based approacheslike DEX. Data augmentations play a prominent role in other self-supervised learning methods, as they often help mod-els learn more robust and generalizable representations. In the context of brain imaging data, augmentationcan be particularly important, but it must be applied with care due to the structural consistency of thedata. To investigate this, we conducted an ablation study to explore the impact of various augmentationson the models performance. Specifically, we applied four types of augmentationsNoise, Cutout, Rotation,and Flipduring training to examine how each one influences the learned representations and final predic-tions. The results, shown in c, indicate that Cutout provides the best performance with a MAE of",
  "(d) Ablation study for feature extraction shows featureextraction after non-linear projection mapping achieveslower MAE": ": Ablation studies show the impact of distance norms, regression losses, augmentations, and pro-jection mapping on model performance, with the Manhattan norm, MSE loss, Cutout augmentation, andprojection mapping achieving the best results, respectively. 3.6670.147 years. This may be due to the fact that Cutout selectively masks parts of the brain images whilepreserving the overall structural integrity, allowing the model to focus on the most informative areas withoutintroducing significant distortions. Noise also performed reasonably well, with a MAE of 3.9560.283 years,likely because it introduces minor variations that enhance the models robustness. Flip resulted in a slightlyhigher MAE of 4.238 0.192 years. This augmentation appears to be less harmful than others, likely dueto the natural symmetries in brain anatomy. In contrast, rotation had the worst performance, significantlydegrading the models accuracy with a MAE of 5.431 0.414 years. This is likely because brain images arepre-registered during preprocessing to ensure consistent orientation across subjects, meaning that rotationdisrupts this careful alignment. In previous self-supervised learning frameworks like SimCLR, features are first extracted and then trans-formed through a non-linear projection before computing the loss. Specifically, the encoder : X H mapsthe input data x X to feature vectors h H, where H Rd is the feature space. The projection function : H Z then maps these feature vectors to the embedding space. The overall function f : X Z,which maps the input x X directly to the embedding z Z, is the composition of the encoder andthe projection : z = f(x) = ((x)). To explore the impact of this projection step in our framework, we",
  "Conclusion": "We introduced a novel contrastive regression loss that adeptly prioritizes local regions within embeddingspaces and dynamically adjusts these regions throughout the training process. By applying this methodto brain stiffness maps obtained from Magnetic Resonance Elastography (MRE), we achieved superior pre-dictive performance in brain age estimation compared to established contrastive learning methods. Thisadvancement not only demonstrates our models efficacy but also underscores the potential of using localizeddynamic adjustments in the analysis of complex neuroimaging data. Significantly, our research marks thefirst application of self-supervised learning techniques to explore the mechanical properties of the brain, anarea previously uncharted in the literature. The implications of this are profound, opening up new avenuesfor understanding the structural changes associated with aging and potentially other neurological conditions.However, while our proposed contrastive learning method shows strong predictive performance in brain ageestimation, its validation remains limited to the domain of brain imaging. Broader testing across diversedatasets and comparison with other approaches, such as purely supervised learning, are needed to fullysubstantiate its generalizability. Thus, while promising, our method requires further exploration to confirmits broader applicability. Future work includes to extend our framework to include cohorts with neurologicaldiseases, such as Alzheimers and Parkinsons. This expansion is expected to provide deeper insights intothe progression and early diagnosis of these conditions, leveraging the detection capabilities of our model.Additionally, we aim to explore the integration of multimodal imaging data to enhance the robustness andaccuracy of our predictions, potentially leading to breakthroughs in personalized medicine and neuroimaginganalytics. Finally, as this is a first strategy to leverage non-nearest samples, we acknowledge that moreadvanced methods to introduce bias in the sampling strategy could be explored. This direction, includingadaptive or more dynamic reweighting mechanisms, is an exciting area for future work. JT acknowledges support from the Gates Cambridge Trust via the Gates Cambridge Scholarship. CJ ac-knowledges partial support from the National Institutes of Health grants R01-AG058853 and U01-NS112120.CBS acknowledges support from the Philip Leverhulme Prize, the Royal Society Wolfson Fellowship, theEPSRC advanced career fellowship EP/V029428/1, EPSRC grants EP/S026045/1 and EP/T003553/1,EP/N014588/1, EP/T017961/1, the Wellcome Innovator Awards 215733/Z/19/Z and 221633/Z/20/Z, CCMIand the Alan Turing Institute. GSKS acknowledges funding from the Wellcome Trust (065807/Z/01/Z)(203249/Z/16/Z), the UK Medical Research Council (MRC) (MR/K02292X/1), ARUK (ARUK-PG013-14),Michael J Fox Foundation (16238; 022159), and Infinitus China Ltd. LVH is supported by the WellcomeTrust (grant number: 226420/Z/22/Z). AAR gratefully acknowledges funding from the Cambridge Centrefor Data-Driven Discovery and Accelerate Programme for Scientific Discovery, made possible by a dona-tion from Schmidt Futures, ESPRC Digital Core Capability Award, and CMIH and CCIMI, University ofCambridge. Arvin Arani, Matthew C Murphy, Kevin J Glaser, Armando Manduca, David S Lake, Scott A Kruse,Clifford R Jack Jr, Richard L Ehman, and John Huston 3rd. Measuring the effects of aging and sex onregional brain stiffness with mr elastography in healthy older adults. Neuroimage, 111:5964, 2015. 1",
  "Brian B Avants, Nick Tustison, Gang Song, et al. Advanced normalization tools (ants). Insight j, 2(365):135, 2009. 7": "Lea Baecker, Rafael Garcia-Dias, Sandra Vieira, Cristina Scarpazza, and Andrea Mechelli. Machine learningfor brain age prediction: Introduction to methods and clinical applications. EBioMedicine, 72, 2021. 2 Carlo Alberto Barbano, Benoit Dufumier, Edouard Duchesnay, Marco Grangetto, and Pietro Gori. Con-trastive learning for regression in multi-site brain age prediction. In 2023 IEEE 20th International Sym-posium on Biomedical Imaging (ISBI), pp. 14. IEEE, 2023. 2, 3, 4, 8, 9, 15",
  "Wenzhi Cao, Vahid Mirjalili, and Sebastian Raschka. Rank consistent ordinal regression for neural networkswith application to age estimation. Pattern Recognition Letters, 140:325331, 2020. 3": "Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastivelearning of visual representations. In International conference on machine learning, pp. 15971607. PMLR,2020a. 9 Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastivelearning of visual representations. In International conference on machine learning, pp. 15971607. PMLR,2020b. 3, 8",
  "Guanyi Chu, Xiao Wang, Chuan Shi, and Xunqiang Jiang. Cuco: Graph representation with curriculumcontrastive learning. In IJCAI, pp. 23002306, 2021. 3": "Rebecca G Clements, Claudio Cesar Claros-Olivares, Grace McIlvain, Austin J Brockmeier, and Curtis LJohnson. Mechanical property based brain age prediction using convolutional neural networks. bioRxiv,pp. 202302, 2023. 2 Peyton L Delgorio, Lucy V Hiscox, Ana M Daugherty, Faria Sanjana, Ryan T Pohlig, James M Ellison,Christopher R Martens, Hillary Schwarb, Matthew DJ McGarry, and Curtis L Johnson. Effect of agingon the viscoelastic properties of hippocampal subfields assessed with high-resolution mr elastography.Cerebral Cortex, 31(6):27992811, 2021. 1 Peyton L Delgorio, Lucy V Hiscox, Ana M Daugherty, Faria Sanjana, Grace McIlvain, Ryan T Pohlig,Matthew DJ McGarry, Christopher R Martens, Hillary Schwarb, and Curtis L Johnson. Structurefunctiondissociations of human hippocampal subfield stiffness and memory performance. Journal of Neuroscience,42(42):79577968, 2022. 8 Peyton L Delgorio, Lucy V Hiscox, Grace McIlvain, Mary K Kramer, Alexa M Diano, Kyra E Twohy,Alexis A Merritt, Matthew DJ McGarry, Hillary Schwarb, Ana M Daugherty, et al. Hippocampal sub-field viscoelasticity in amnestic mild cognitive impairment evaluated with mr elastography. NeuroImage:Clinical, 37:103327, 2023. 8 Benoit Dufumier, Pietro Gori, Julie Victor, Antoine Grigis, Michele Wessa, Paolo Brambilla, Pauline Favre,Mircea Polosan, Colm McDonald, Camille Marie Piguet, et al. Contrastive learning with continuous proxymeta-data for 3d mri classification. In Medical Image Computing and Computer Assisted InterventionMICCAI 2021: 24th International Conference, Strasbourg, France, September 27October 1, 2021, Pro-ceedings, Part II 24, pp. 5868. Springer, 2021. 2, 3, 9, 15 Benoit Dufumier, Antoine Grigis, Julie Victor, Corentin Ambroise, Vincent Frouin, and Edouard Duchesnay.Openbhb: a large-scale multi-site brain mri data-set for age prediction and debiasing. NeuroImage, 263:119637, 2022. 7 Debidatta Dwibedi, Yusuf Aytar, Jonathan Tompson, Pierre Sermanet, and Andrew Zisserman. With a littlehelp from my friends: Nearest-neighbor contrastive learning of visual representations. In Proceedings ofthe IEEE/CVF International Conference on Computer Vision, pp. 95889597, 2021. 4, 9",
  "Lucy V Hiscox, Hillary Schwarb, Matthew DJ McGarry, and Curtis L Johnson. Aging brain mechanics:Progress and promise of magnetic resonance elastography. Neuroimage, 232:117889, 2021. 1": "Curtis L Johnson, Joseph L Holtrop, Matthew DJ McGarry, John B Weaver, Keith D Paulsen, John GGeorgiadis, and Bradley P Sutton. 3d multislab, multishot acquisition for fast, whole-brain mr elastographywith high signal-to-noise efficiency. Magnetic resonance in medicine, 71(2):477485, 2014. 16 Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron Maschinot,Ce Liu, and Dilip Krishnan. Supervised contrastive learning. Advances in neural information processingsystems, 33:1866118673, 2020. 3 Stphane Lathuilire, Pablo Mesejo, Xavier Alameda-Pineda, and Radu Horaud. A comprehensive analysisof deep regression. IEEE transactions on pattern analysis and machine intelligence, 42(9):20652081, 2019.3 Jeyeon Lee, Brian J Burkett, Hoon-Ki Min, Matthew L Senjem, Emily S Lundt, Hugo Botha, JonathanGraff-Radford, Leland R Barnard, Jeffrey L Gunter, Christopher G Schwarz, et al. Deep learning-basedbrain age prediction in normal aging and dementia. Nature Aging, 2(5):412424, 2022. 2 Wanhua Li, Xiaoke Huang, Jiwen Lu, Jianjiang Feng, and Jie Zhou. Learning probabilistic ordinal embed-dings for uncertainty-aware regression. In Proceedings of the IEEE/CVF conference on computer visionand pattern recognition, pp. 1389613905, 2021. 3 Martina J Lund, Dag Alns, Ann-Marie G de Lange, Ole A Andreassen, Lars T Westlye, and TobiasKaufmann. Brain age prediction using fmri network coupling in youths and associations with psychiatricsymptoms. NeuroImage: Clinical, 33:102921, 2022. 2 Matthew C Murphy, John Huston III, Clifford R Jack Jr, Kevin J Glaser, Matthew L Senjem, Jun Chen,Armando Manduca, Joel P Felmlee, and Richard L Ehman. Measuring the characteristic topography ofbrain stiffness with magnetic resonance elastography. PloS one, 8(12):e81668, 2013. 1 Matthew C Murphy, David T Jones, Clifford R Jack Jr, Kevin J Glaser, Matthew L Senjem, ArmandoManduca, Joel P Felmlee, Rickey E Carter, Richard L Ehman, and John Huston III. Regional brainstiffness changes across the alzheimers disease spectrum. NeuroImage: Clinical, 10:283290, 2016. 2",
  "Min Sun, Pushmeet Kohli, and Jamie Shotton. Conditional regression forests for human pose estimation. In2012 IEEE Conference on Computer Vision and Pattern Recognition, pp. 33943401. IEEE, 2012. 3": "Tomohiro Takamura, Utaroh Motosugi, Yu Sasaki, Takashi Kakegawa, Kazuyuki Sato, Kevin J Glaser,Richard L Ehman, and Hiroshi Onishi. Influence of age on global and regional brain stiffness in youngand middle-aged adults. Journal of Magnetic Resonance Imaging, 51(3):727733, 2020. 1 Jiyuan Wang, Chunyu Lin, Lang Nie, Shujun Huang, Yao Zhao, Xing Pan, and Rui Ai. Weatherdepth:Curriculum contrastive learning for self-supervised depth estimation under adverse weather conditions. In2024 IEEE International Conference on Robotics and Automation (ICRA), pp. 49764982. IEEE, 2024. 3",
  "where 1": "d represents the normalized inverse distance between the anchor and the sample, and is a hyper-parameter controlling the rate of decay in weighting for samples at greater distances. Thus, the repulsionterm in 3 is expanded to all samples, and non-nearest samples accordingly weighted. This formulation en-sures that as increases, the contributions from farther negative samples diminish more rapidly. In theextreme case where , only the nearest neighbors contribute to the loss 3. Conversely, when = 0,the weighting becomes uniform across all samples, recovering the exponential contrastive regression loss(Dufumier et al., 2021; Barbano et al., 2023). We performed an ablation study to investigate the impact ofdifferent values of on the models performance, as illustrated in . The results indicate that whilere-weighting the non-nearest samples provides some benefits, only one configuration with an intermediatevalue of = 1 marginally outperformed the approach of simply dropping non-nearest samples yielding aMAE of 3.6810.234 years. This suggests that re-weighting can help retain informative negative samples buthas a limited overall impact on improving model generalization. Further exploration of more sophisticatedre-weighting mechanisms could be beneficial in future work.",
  "A.2Brain Coverage of Stiffness Maps": ": Distribution of Brain Coverage in Stiffness Maps Across Studies. In Magnetic Resonance Elas-tography (MRE), achieving optimal brain coverage necessitates a balance between high spatial resolution,signal-to-noise ratio (SNR), and acceptable scan times (Johnson et al., 2014). The majority of existing brainMRE studies focus on deep brain structures, leading to limited coverage in certain areas. This limitation isillustrated here, where the brain coverage variability in our stiffness maps is visualized."
}