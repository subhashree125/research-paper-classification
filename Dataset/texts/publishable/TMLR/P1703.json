{
  "Abstract": "Interest is rising in Physics-Informed Neural Networks (PINNs) as a mesh-free alternativeto traditional numerical solvers for partial differential equations (PDEs). However, PINNsoften struggle to learn high-frequency and multi-scale target solutions. To tackle this prob-lem, we first study a strong Boundary Condition (BC) version of PINNs for Dirichlet BCsand observe a consistent decline in relative error compared to the standard PINNs. We thenperform a theoretical analysis based on the Fourier transform and convolution theorem. Wefind that strong BC PINNs can better learn the amplitudes of high-frequency componentsof the target solutions.However, constructing the architecture for strong BC PINNs isdifficult for many BCs and domain geometries. Enlightened by our theoretical analysis, wepropose Fourier PINNs a simple, general, yet powerful method that augments PINNs withpre-specified, dense Fourier bases. Our proposed architecture likewise learns high-frequencycomponents better but places no restrictions on the particular BCs or problem domains. Wedevelop an adaptive learning and basis selection algorithm via alternating neural net basisoptimization, Fourier and neural net basis coefficient estimation, and coefficient truncation.This scheme can flexibly identify the significant frequencies while weakening the nominalfrequencies to better capture the target solutions power spectrum. We show the advantageof our approach through a set of systematic experiments.",
  "Introduction": "Physics-informed neural networks (PINNs) (Raissi et al., 2019) are innovative, mesh-free approaches forsolving partial differential equations (PDEs). They offer alternatives to traditional mesh-based numericalmethods such as finite elements and finite volumes (Reddy, 2019). The optimization of PINNs involvessoftly constraining neural networks (NNs) through customized loss functions designed to adhere to thegoverning equations of physical processes. Researchers have successfully applied PINNs in various domainsfor example, they have been used to simulate the radiative transport equation (Mishra & Molinaro, 2021),which is crucial for radio frequency chip and material design (Chen et al., 2020; Liu & Wang, 2019). Further,",
  "Published in Transactions on Machine Learning Research (01/2025)": "Zhiping Mao, Ameya D. Jagtap, and George Em Karniadakis. Physics-informed neural networks for high-speed flows. Computer Methods in Applied Mechanics and Engineering, 360:112789, 2020. doi: 10.1016/j.cma.2019.112789. Levi D. McClenny and Ulisses Braga-Neto.Self-adaptive physics-informed neural networks using a softattention mechanism.Journal of Computational Physics, 474:111722, 2024.doi: 10.1016/j.jcp.2023.111722. Kevin Stanley McFall and James Robert Mahan. Artificial neural network method for solution of boundaryvalue problems with exact satisfaction of arbitrary boundary conditions. IEEE Transactions on NeuralNetworks, 20(8):12211233, 2009. doi: 10.1109/TNN.2009.2025588.",
  "where is the boundary of the domain and B is a general boundary-condition operator": "To solve the PDE, the PINN uses a deep NN, uN(x; ), to approximate the true solution u(x). For clarityin later sections, we follow the convention of Cyr et al. (2020) and define the output of the NN with widthW as a linear combination of a set of nonlinear bases such that",
  "j=1cjj(x; H),(3)": "where each j are nonlinear activation functions (such as Tanh) acting on the hidden layer outputs. Each cjfor j = 1, ..., w and H are the weights and biases in the last layer of the NN and hidden layers, respectively.Therefore, = {c, H} form the set of all network parameters. Then, finding the optimal network parameters involves minimizing the following composite loss function between boundary and residual loss terms,",
  "Strong Boundary Condition PINNs": "Equation 5 can approximately enforce various types of boundary conditions, including Dirichlet, Neumann,Robin, and periodic. However, prior analysis by Wang et al. (2021b; 2022) suggests that the instabilityin training PINNs likely arises from the competition between the weakly enforced boundary loss (5) andthe residual loss (6) terms during optimization. This competition likely occurs because both loss termsare minimized simultaneously during training. However, the optimization algorithm tends to prioritize theminimization of the residual loss, as it typically dominates the gradient of the combined loss function.Consequently, this can lead to a scenario where the residual loss converges effectively but at the expense ofthe boundary loss, which remains inadequately optimized. As a result, the boundary conditions may not besatisfied, leading to poor model performance on new, unseen data or data at the domains boundaries. One solution to mitigate the competition between the loss terms is to design a surrogate model that inherentlysatisfies the boundary conditions. This approach typically works by modifying the network architecture tosatisfy the boundary conditions exactly, thus eliminating the need for a boundary enforcement loss termand further reducing the computational cost. This method of exact imposition of boundary conditions hasbecome a standard approach in the PINN literature and is especially prevalent for Dirichlet and periodicboundary conditions (Lu et al., 2021; Yu et al., 2022; Wang et al., 2024a). This work primarily focuses on Dirichlet boundary conditions to simplify the analysis and implementationswhile addressing a significant and common scenario in physical systems.Dirichlet boundary conditionsprovide a clear and straightforward framework to demonstrate the effectiveness of the surrogate modelapproach, as they require the solution to take specific values at the boundaries, which is relatively easy toenforce exactly on simplified domains. To illustrate, we consider the 1D Dirichlet boundary condition definedas,",
  "u(x) = g(x) + (x)uN(x),(8)": "such that u(x) is the solution function, g(x) is the provided boundary function, uN(x) is the output ofthe neural network, and (x) is a composite distance function that zeroes out on the boundaries, ensuringthe boundary conditions are met without explicit penalty terms. By construction, u(a) = u(b) = g(x),and thus the boundary condition in Equation 7 is strictly and automatically satisfied.To estimate theparameters of u, we only need to minimize the residual loss Lr(). Most PINN surrogate model designsrely on distance functions based on the theory of R-functions (Sukumar & Srivastava, 2022), which aresmooth mathematical functions that encode Boolean logic and facilitate the combination of simple shapesto form complex geometries (Rvachev & Sheiko, 1995). Some meshfree Galerkin methods have utilized thecapability of R-functions to enforce boundary conditions smoothly and exactly to enhance the precision androbustness of numerical simulations for solving boundary-value problems (Shapiro & Tsukanov, 1999; Akin,1994; Tsukanov & Posireddy, 2011). x exp, = 0.75 exp, = 1.25 exp, = 2.25 exp, = 3.75 exp, = 5.75 exp, = 8.75 poly : This figure shows the polynomial distance function poly(x) and the exponential distance function exp(x)with different values. The curves for exp(x) correspond to different parameters, demonstrating how the shape ofexp(x) sharpens around the domain boundaries as increases. In this work, we compare the standard PINN to two strong BC PINNs using different distance functionsto enforce Dirichlet boundary conditions exactly. The first distance function we consider is the polynomialdistance function proposed by (Lu et al., 2021), defined as,",
  "poly(x) = (x a)(b x).(9)": "This function satisfies the Dirichlet boundary conditions by zeroing out at the x = a and x = b endpoints.While Lu et al. (2021) demonstrated promising results using this formulation, the polynomial distancefunction poly(x) remains static throughout the training process. The lack of adaptability limits the modelsability to handle different problem domains and boundary conditions, which can hinder performance whenfaced with varying complexities within the solution space.",
  "exp(x) = (1 ea(ax))(1 eb(xb)),(10)": "where a and b are parameters that can be pre-set or optimized during training. These parameters allowthe function to adjust to the characteristics of the problem domain dynamically, ensuring robust enforcementof boundary conditions across different scenarios. As we demonstrate in subsequent sections, this flexibilityimproves accuracy and enhances the efficiency of the training process by enabling the network to adaptto the complexity of the solution space.Allowing independent values for a and b enables additionalflexibility and allows the function to adapt asymmetrically to different boundary conditions. Although theproposed adaptive distance function in Equation 10 may appear incremental, its primary contribution liesin demonstrating a scalable and flexible framework for enforcing strong boundary conditions. This method",
  "eai(aixi) 1 ebi(xibi),(11)": "where ai, bi represent the boundaries in the i-th dimension, and ai, bi are the corresponding steepnessparameters. This form ensures that the boundary conditions are enforced independently in each dimension,making the approach easily scalable to higher dimensions in rectilinear domains. One potential strategy for adaptation for irregular domains is using Fourier extension methods, which allownon-periodic functions defined on arbitrary domains to be extended into periodic ones. Doing so can enforcethe boundary conditions in a more structured domain, allowing the network to handle irregularities better.Another promising direction is leveraging custom network layers that map irregular domains to rectilin-ear spaces, as demonstrated by methods such as the GOFNO network Liu et al. (2023). Extending thisframework to more complex boundary conditions, such as Neumann or Robin conditions, is an importantfuture direction. The adaptive distance function can be modified for Neumann boundary conditions to havenon-zero slopes at the boundaries, ensuring correctly enforced derivatives. This would require careful andnon-trivial adjustments to the distance function, but the general framework remains applicable. illustrates the behavior of the polynomial distance function poly(x) to the exponential distancefunction exp(x) with various values of . As varies, exp(x) adjusts its shape, showcasing its ability toconform to different problem domains and complexities. We expect this dynamic adaptability to enhance themodels performance by more precisely enforcing boundary conditions across diverse scenarios. We affirmthis through a numerical example in the next section.",
  "Boundary Condition Pathologies in PINNs": "Recent works have employed strong boundary enforcement strategies(Lu et al., 2021; Yu et al., 2022)but offer few insights into the specific surrogate model design process.The design and implementationof the surrogate models are often ad-hoc, with no systematic approach to their architectural design. Thiscomplicates their application and limits their effectiveness across boundary conditions and physical problems,especially when dealing with complex physics and domains.Moreover, the standard PINNs frequentlyencounter difficulties with high-frequency and multi-scale solutions, leading to inaccurate predictions (Wanget al., 2021c). We show that while the strong BC PINN model improves solution quality for problems withhigher frequency compared to the standard PINN, the solution quality nevertheless degrades with largerfrequencies. In the following, we empirically demonstrate how the standard PINN solution quality degradesas the actual frequency of the solution increases in both a linear 1D Poisson problem and a non-linear 1Dsteady-state Allen-Cahn problem. Specifically, we consider the fabricated solution u(x) = sin(kx) for a rangeof frequencies k and x . Therefore, by varying k, we can examinethe performance of the standard PINNs compared to the strong BC PINNs when the solution u includesdifferent frequency information.",
  "which gives the forcing function f(x) = k2 sin(kx)+sin(kx)(sin2(kx)1). Both are subject to the Dirichletcondition in Equation 7 where a = 0 and b = 2": "We test these problems using a standard PINN, which explicitly incorporates the weakly enforced boundaryloss and residual loss terms. We compare these results to two strong BC PINNs employing different distancefunctions (8) and show that this method exhibits slower solution degradation with increased frequency.Specifically, we tested two neural network architectures, each with 100 neurons per layer and depths of twoand four, and assessed model performance using the relative 2 error, defined as",
  "ni=1 y2i,": "where y is the predicted solution and y is the true solution. All networks used the Tanh activation and aretrained using 10K equally spaced collocation points sampled from the domain. Relative 2 errors are reportedon a separate testing set of 20K points. All experiments used 32-bit floating-point precision (float32) to ensurecomputational efficiency. Each model was trained for 100K iterations using the Adam optimizer (Kingma& Ba, 2015) with an initial learning rate of 103, decaying exponentially by 0.9 every 1000 iterations,followed by L-BFGS optimization until convergence (tolerance 109). For the strong BC PINN with exp,we initialized to 0.5 and jointly optimized it with all other parameters. Results are averaged over fiverandom trials and were conducted on a GeForce RTX 3090 GPU with CUDA version 12.3, running on Ubuntu20.04.6 LTS. Additionally, we implemented a second-order accurate Finite Difference Method (FDM) as abaseline for solving the 1D Poisson and steady-state Allen-Cahn equations using 1000 discretization pointsand central differences. in the Appendix provides detailed hyperparameters and experimental designinformation. k Relative 2 error",
  "D Poisson": "k Relative 2 error 1D Allen-Cahn Standard PINN (d-2)Standard PINN (d-4)Strong BC PINN poly (d-2) Strong BC PINN poly (d-4) Strong BC PINN exp (d-2) Strong BC PINN exp (d-4) FDM : Relative 2 error in predicting the solution u(x) = sin(kx) as a function of frequency k. The plots comparethe performance of the standard PINN, the strong BC PINN with polynomial boundary function poly, the strong BCPINN with exponential boundary function exp, and FDM for (left) 1D Poisson and (right) 1D steady-state Allen-Cahn equations for NN with 2 hidden layers (d-2) and 4 hidden layers (d-4) each with 100 neurons. The shadedregions represent the error variability.",
  "Fourier Analysis of Strong BC PINNs": "Both NNs and PINNs are known to exhibit spectral bias, meaning they readily capture low-frequencycomponents of a target function but struggle with high-frequency details (Basri et al., 2020; Rahaman et al.,2019; Xu et al., 2020; Wang et al., 2021c). As demonstrated earlier, this bias causes the solution qualityof PINNs to degrade as the target solutions frequency increases. However, strong BC PINNs appear tomitigate or delay this issue, achieving better performance for higher-frequency solutions. This section uses Fourier analysis to investigate why strong BC PINNs outperform standard PINNs inaddressing spectral bias. The infinite Fourier series expresses a periodic function u(x) with period L as:",
  "Numerical Fourier Analysis of Strong BC PINNs": "To analyze spectral bias, we compute the Fourier coefficients for several manufactured solutions: 1. A singlesine wave to demonstrate the baseline frequency handling of PINNs. 2. A combination of sine waves withdiffering frequencies to explore how multi-scale components are captured. 3. A Gaussian-modulated sinewave to evaluate performance on solutions with continuous spectral content. 4. A sine wave and polynomialcombination to examine behavior under non-homogeneous boundary conditions. For standard PINNs, the frequency spectrum typically shows a pronounced low-frequency peak, reflectingeffective learning of low-frequency components and a gradual decay for higher frequencies, often resultingin heavy tails. These heavy tails indicate residual noise and the networks difficulty in accurately filteringout high-frequency inaccuracies. By contrast, as we will show, strong BC PINNs exhibit faster decay inthe higher-frequency components, enabling better suppression of noise and improved representation of finedetails in the solution. We trained a standard PINN and a strong BC PINN for each example with four hidden layers, 100 nodesper layer, and Tanh activations. lists the specific training and hyperparameter details. We thencomputed the Discrete Fourier Transform (DFT) of the learned solutions u(x), which represents theirfrequency spectrum. For N equally spaced points xj sampled over the domain , the DFT is definedas:",
  "1": "Frequency Index k | u[k] | Standard PINNStrong BC PINN Frequency Index k | u[k] | B(kv) N(v) : Frequency spectrum of the learned solution (left) and the convolution operation (right) in the strong BCPINN and standard PINN. The ground-truth solution is sin(kx) with k = 15. The left graph shows frequency handlingby standard and strong BC PINNs, while the right demonstrates how convolution reduces high-frequency noise. (left) compares the frequency spectrum of a standard PINN and a strong BC PINN trained on the1D Poisson problem with the ground-truth solution u(x) = sin(kx), for x and k = 15. While thestandard PINN successfully captures the target frequency, it also retains significant high-frequency noise, asevident in the heavy tails of the spectrum. In contrast, the strong BC PINN exhibits much faster decay inhigher frequencies, effectively reducing high-frequency noise and improving accuracy. This indicates that thestrong BC PINN filters out unnecessary components more efficiently, enhancing the clarity and precision ofthe learned solution by isolating the dominant frequency. Frequency Index k | u[k] | True SolutionStandard PINNStrong BC PINN Frequency Index k",
  "Absolute Error": "Standard PINNStrong BC PINN :The frequency spectrum of the learned solutions (top) and absolute error of the Fourier coefficients(bottom) for the strong BC PINN and standard PINN compared to the ground truth. The ground-truth solution isu(x) = x2 + sin(16x). Finally, we explore the strong BC PINNs performance on non-homogeneous boundary conditions usingthe manufactured solution u(x) = x2 + sin(16x). illustrates the Fourier spectrum for this non-homogeneous boundary case. The polynomial term x2 introduces a low-frequency component that is notharmonically related to the oscillatory term sin(16x). The standard PINN struggles to capture this low-frequency component fully. At the same time, the strong BC PINN demonstrates improved performancein accurately representing both the polynomial boundary effect and the oscillatory component. This ex-ample underscores the strong BC PINNs effectiveness in handling non-homogeneous boundaries, as it canaccommodate mixed low- and high-frequency components in the solution. These results collectively illustrate the strong BC PINNs enhanced ability to manage various frequencycases, from simple single-frequency cases to continuous spectra and complex non-homogeneous boundaryconditions. The strong BC PINN demonstrates robustness and flexibility by effectively suppressing high-frequency noise and maintaining accuracy across various spectral components, making it suitable for a widearray of boundary-conditioned problems in applied settings.",
  "the Fourier transform of (x), meaning F1 ()= (x) where F1 denotes the inverse Fouriertransform": "Since |poly[n]| 1/n2, the amplitude of the frequencies of poly(x) decay quadratically fast with the increaseof the absolute frequency value. This rapid decay is beneficial for denoising as it effectively suppresses high-frequencies, leading to smoother approximations. However, it may result in the loss of fine details, especiallyif essential signal components are present at higher frequencies.Alternatively, |exp[n]| /(2 + n2),indicating that the frequency amplitudes follow Lorentzian decay controlled by the decay rate . Lorentziandecay is less aggressive in reducing high-frequency components, which helps preserve fine details and sharptransitions in the signal. This flexibility helps tune the decay based on signal characteristics. However, theslower decay rate may lead to less effective noise suppression.",
  "( ) uN() d(22)": "where denotes convolution, and uN is the Fourier transform of the NN. Suppose > 0, we know that( ) is obtained by first reflecting () about the y-axis, and then shifting the frequency left along thespectrum. Then, we integrate uN() weighted by ( ). The larger the frequency , the more is movedleft to obtain () resulting in a stronger weighting of the high-frequency components during integration.Since the frequency coefficients of decay quadratically fast (|[n]| 1/n2), the larger the portion of thetail is used, and the smaller the integration result. Specifically, with , the corresponding amplitude of u()decrease fast when increases2. See for an illustration in the discrete case. During training, theboundary function pushes the surrogate model in Equation 8 to weaken irrelevant high-frequency componentsto reduce large tails and better capture the frequency spectrum. 1While the analysis here is presented in the discrete Fourier series form for clarity and consistency with the numericalimplementation, the results can be extended to the continuous Fourier transform by replacing summations over discrete indicesn with integrals over the continuous frequency domain.2The same conclusion applies when we consider < 0.",
  "Fourier PINNs": "We introduce Fourier PINNs, a novel PINN architecture to overcome these challenges. Fourier PINNs seam-lessly handle diverse boundary conditions, like the standard PINNs, while emphasizing the true solutionfrequencies during trainingakin to the strong BC PINNs. Specifically, in order to flexibly yet compre-hensively capture the frequency spectrum, we first introduce a set of dense frequency candidates, {n}Kn=1,evenly sampled from the range [1, K]. From this set of frequency candidates, we define a set of trainableFourier bases uB as,",
  "u(x) = uN(x) + vec(x1)(x2)(25)": "where vec() denotes the vectorization, and are the coefficients of the Fourier bases.Note that thetensor-product of bases grows exponentially with the problem dimension, quickly becoming computationallyintractable. One potential solution is to generate more sparse Fourier bases, such as total-degree or hyperboliccross bases. Another option is to re-organize the coefficients into a tensor or matrix and introduce a low-rank decomposition structure to parameterize to save computational cost (Novikov et al., 2015). Theseexplorations are left for future work.",
  "Adaptive Basis Selection Training Algorithm": "Given that the ground-truth solution likely contains significantly fewer frequencies than the augmentedcandidate bases (uB), and our previous analysis showed that models often struggle to prioritize learningnecessary frequencies, we developed an adaptive learning and basis selection algorithm.This algorithmflexibly identifies meaningful frequencies while pruning the inconsequential onesallowing the model tofocus on learning and improving the bases that significantly contribute to the solution by inhibiting thelearning of unnecessary (and likely noisy) frequencies. To aid in identifying the significant frequencies, we add an L2 regularization term to the basis parametersw. While L2 regularization does not promote sparsity, in our specific case, we use L2 regularization witha pruning strategy, which indirectly promotes sparsity by pruning coefficients after driving them toward athreshold. Methods such as the SINDy (Sparse Identification of Nonlinear Dynamics) algorithm Zhang &Schaeffer (2019) have explored this strategy, where L2 regularization helps to stabilize the coefficients duringoptimization before pruningyielding sparse solutions. This combination effectively balances stability andsparsity, as shown in previous work Wang et al. (2021a). In this case, we chose not to use L1 regularizationdue to practical challenges, including the instability it can introduce during training, particularly whenoptimizing neural networks for complex systems. L2 regularization, combined with a careful pruning process,offers more stable convergence while still leading to sparse solutions after pruning the small coefficients. This additional regularization penalizes large coefficients for the basis functions, thereby promoting sparsityand encouraging the model to focus on learning the most significant frequencies. The full loss function weuse is similar to PINNs (see Equation 4) but with the addition of the L2 regularization term. Specifically,the loss function is,",
  "w2,(26)": "where w2 represents the L2 regularization of w and is the regularization strength defined by the userbefore initiating training. This regularization term helps prune or inhibit unnecessary frequencies, allowingthe model to focus more on learning and improving the bases that significantly contribute to the solution. Linear OperatorsOur adaptive basis selection algorithm optimizes the regularized loss function in Equa-tion 26 by building upon the hybrid least squares gradient descent method proposed by Cyr et al. (2020).Cyr et al. (2020) formulates the output of a neural network as a linear combination of nonlinear basis func-tions, as shown in Equation 3. Their algorithm alternates between optimizing the hidden weights (H) viagradient descent and the final layer coefficients (c) of the NN through the least squares problem of the form,",
  "arg minc Ac y22,(27)": "such that yi = L[u](xi) for the data points {xi}Mi=1, Aij = L[j(xi; H)] for j = 1, ..., W, and L is some linearoperator. Equation 27 extends to problems with multiple operators, such as those defined by Equation 1and Equation 2 when F and B are linear operators. Representing the output of Fourier PINNs in Equation 23 as a linear combination of both adaptive bases(i.e., the hidden layers of the NN) and an augmented Fourier series ensures our models compatibility withthe general approach of the alternating least squares and gradient descent optimization. While Cyr et al.(2020) hold the coefficients of the last layer in the NN (c) constant while optimizing the bases H throughgradient descent, we alternatively choose to jointly optimize both the Fourier PINNs basis coefficients w(which are comprised of both the NN basis coefficients c along with the Fourier basis coefficients a1, ..., aKand b1, ..., bK) and the adaptive bases H during the gradient descent step. During the least squares step,we solve a similar problem to Equation 27 modified to suit the Fourier PINN architecture. Specifically, wesolve the least squares problem of the form,",
  "L[j(xi; H)]for j = 1, ..., W,L[cos(2jxi)]for j = W + 1, ..., W + K,L[sin(2jxi)]for j = W + K + 2, ..., W + 2K.(29)": "We define Equation 28 for one operator L, but the method extends to multiple operators. Additionally, weextend our least-squares method to nonlinear operators, unlike many works that focus only on linear cases. Non-Linear OperatorsWhen the operator L is nonlinear, updating the coefficients w requires addi-tional care to ensure the method accurately handles the nonlinearities and achieves convergence. Nonlinearoperators can often be decomposed into a combination of linear and nonlinear components, simplifying theprocess of updating w during optimization. Specifically, we decompose the operator L as follows:",
  "G[j(xi; H)]for j = 1, ..., W,G[cos(2jxi)]for j = W + 1, ..., W + K,G[sin(2jxi)]for j = W + K + 2, ..., W + 2K.(33)": "The nonlinear terms si are included in the target vector such that yi = L[u](xi) si, which is updatediteratively and reduces the problem to a linear least-squares system.As this method incorporates thecurrent values of w into calculating the nonlinear components, the optimization process can be viewed asa form of fixed-point iteration. We note that for cases where L consists primarily of nonlinear terms ordoes not include a linear operator, the optimization problem might need to be handled using continuousoptimization algorithms such as L-BFGS, which effectively updates w based on the entire system to ensureconvergence. We integrate a custom basis pruning routine into the optimization algorithm to eliminate insignificant basesand refine model training. Our modified loss function defined in Equation 26 enhances this hybrid optimiza-tion algorithm with an L2 regularization term to identify and prioritize significant frequencies. Algorithm 1outlines our adaptive learning routine. The algorithm begins with a joint optimization of H and w toprovide a warm start to the model parameters.Next, we fix H while alternating between solving forthe coefficients w using the least squares method and truncating bases with coefficients below a predefinedthreshold. Specifically, bases corresponding to |wj| 103 are removed along with wj. After completingthe alternating solve and truncating steps for a set number of iterations, the algorithm jointly optimizes allremaining parameters and bases using the Adam optimization algorithm. We repeat this routine for a fixednumber of iterations, followed by L-BFGS optimization until the final convergence.",
  "Experiments": "This section comprehensively evaluates our methods across a diverse set of partial differential equation (PDE)problems. The test cases include the 1D and 2D Poisson equations, which serve as canonical examples oflinear elliptic PDEs, and the 1D and 2D steady-state Allen-Cahn equations, representing nonlinear ellipticPDEs with a nonlinear reaction term. To further challenge the models, we analyze the 1D one-way Waveequation, which incorporates a first-order time derivative, and the 1D non-steady-state Allen-Cahn equation,a stiff PDE with non-harmonic characteristics. We first describe the baseline methods for comparing FourierPINNs and then outline the experimental setup. Finally, we present and discuss the results, highlighting therelative performance of each method on the benchmark problems.",
  "(5) (Spectral) A single Fourier layer comprised of Fourier bases and trainable coefficients. Basis coeffi-cients are optimized through the same gradient descent routines as all other baselines3": "We do not test against the conceptually similar Fourier Neural Operators (FNOs) (Li et al., 2021), as theyare designed for inverse problems and rely on a data loss term. In contrast, our method focuses on solvingforward problems and does not require training data within the domain. Additionally, while the methodslisted above represent widely-used PINN architectures, we acknowledge the existence of alternative neural",
  "Hyperparameters and experimental details": "For each experiment, we trained the models in two stages: first using the Adam optimizer (Kingma & Ba,2015), followed by L-BFGS optimization (Liu & Nocedal, 1989) until convergence, with the tolerance set to109. Specific hyperparameters and training details are provided in the following tables: for the 1DPoisson and 1D Allen-Cahn experiments, for the 1D Wave equation experiments, for the 2DPoisson and 2D (steady-state) Allen-Cahn experiments, and for the 1D (non-steady-state) Allen-Cahnexperiment. All experiments use 32-bit floating-point precision (float32) to ensure computational efficiencyand consistency across models and were conducted on a GeForce RTX 3090 GPU with CUDA version 12.3,running on Ubuntu 20.04.6 LTS. In Fourier PINNs, we determined the range of frequency candidates forthe Fourier bases by setting a maximum frequency K and including all equally spaced frequencies in the set{1, 2, ..., K}. For the alternating optimization algorithm (Algorithm 1), we set the basis truncation threshold = 104 and the regularization strength parameter = 104. Optimization used the same number ofAdam iterations across experiments, with pauses at every 1000th iteration, to solve the LS problem for fiveiterations. RFF-PINNs required specifying the number and scales of Gaussian variances used to constructthe random features. To evaluate the effects of these hyperparameters, we tested 20 different settings, varyingthe number of scales (one, two, three, and five) and using variances suggested in (Wang et al., 2021c) alongwith randomly sampled values. provides the exact sets of scales. For W-PINN, we varied the weightof the residual loss from {101, 103, 104} and reported the configuration achieving the best results. ForA-PINN, we introduced a learned parameter for the activation function in each layer and updated theseparameters jointly. We ran every method for five random trials and reported the five number summariesobtained on seperate testing datasets for each experiment in boxplot form. All code is implemented usingthe PyTorch C++ library Paszke et al. (2019)4.",
  "D Numerical Experiments": "This section evaluates Fourier PINNs on three progressively challenging 1D Poisson and Allen-Cahn bench-marks, demonstrating their superior performance and robustness over the baseline methods. These bench-marks systematically assess the models ability to handle high-frequency, multi-scale, and hybrid-frequencysolutions for linear and non-linear elliptic PDEs. The analysis includes relative 2 error comparisons, conver-gence dynamics, and frequency spectrum decomposition to highlight the strengths of Fourier PINNs. in the Appendix lists each examples hyperparameters and experimental details.",
  "(a)": "RFF-PINN (20) RFF-PINN (1, 50) RFF-PINN (3, 20) RFF-PINN (1, 20, 194) RFF-PINN (1, 50, 189) RFF-PINN (1, 20, 49, 50, 100) RFF-PINN (1, 20, 50, 85, 100) RFF-PINN (1, 20, 104, 197, 199) RFF-PINN (6, 36, 67, 79, 136) Fourier PINN (K=150) Fourier PINN (K=200) Fourier PINN (K=250) Fourier PINN (K=300) Relative 2 Error RFF-PINN (1 scales)RFF-PINN (2 scales) RFF-PINN (3 scales)RFF-PINN (5 scales) Fourier PINN",
  "(d)": "Frequency Index k | u[k] | uN of Fourier PINNuB of Fourier PINN : (1D Poisson) Results for the multi-scale true solution u(x) = sin(x) + 0.1 sin(20x) + 0.05 cos(100x). (a)Boxplot showing the relative 2 errors for Fourier PINNs and the top-performing baseline methods. (b) Convergenceplots comparing the relative 2 error across iterations (top) and training time (bottom) for selected models. Verticallines indicate when each model reached an error of 103. (c) Frequency spectrum of the solution u(x) compared toPINN and Fourier PINN approximations. (d) Decomposition of the frequency components in Fourier PINNs betweenthe NN output and the Fourier layer output, highlighting differences in spectral representation. Multi-Scale Solution Results.The multi-scale problem introduces a combination of low- and high-frequency components, presenting a greater challenge for baseline methods. a shows the relative 2error distributions for Fourier PINNs and top baseline models, while b in the Appendix includesresults for all models. Fourier PINNs outperform most baseline methods; however, a subset of RFF-PINNconfigurations achieves slightly better results. These cases highlight the sensitivity of RFF-PINNs to scale",
  "Spectral": "PINN W-PINN A-PINN RFF-PINN (1) RFF-PINN (20) RFF-PINN (50) RFF-PINN (84) RFF-PINN (100) RFF-PINN (1, 50) RFF-PINN (3, 20) RFF-PINN (19, 71) RFF-PINN (39, 69) RFF-PINN (50, 100) RFF-PINN (1, 20, 194) RFF-PINN (1, 50, 189) RFF-PINN (20, 50, 100) RFF-PINN (38, 112, 119) RFF-PINN (44, 47, 165) RFF-PINN (1, 20, 49, 50, 100) RFF-PINN (1, 20, 50, 85, 100) RFF-PINN (1, 20, 104, 197, 199) RFF-PINN (6, 36, 67, 79, 136) RFF-PINN (50, 65, 83, 104, 139) Fourier PINN Relative 2 Error SpectralPINN W-PINNA-PINN RFF-PINN (1 scales)RFF-PINN (2 scales) RFF-PINN (3 scales)RFF-PINN (5 scales) Fourier PINN",
  "Iterations": "Relative 2 Error Training Time (s) Relative 2 Error PINNFourier PINN (K = 200) RFF-PINN (1, 20, 104, 197, 199)RFF-PINN (1, 50) : Convergence behavior of select models on the 2D Poisson problems, showing the mean relative 2 erroras a function of iterations (left) and training time (right). Results are presented for Equation 38 (left two plots) andEquation 39 (right two plots). Results. For the high-frequency solution (Equation 38), Fourier PINNs achieve an average relative 2 errorof 0.0384, outperforming all baseline methods, with the spectral method being the closest competitor at0.0423. The Appendix provides detailed results for all models in . Notably, other methods failto capture the high-frequency features, with errors exceeding 1. For the multi-scale solution (Equation 39),Fourier PINNs demonstrate a significant advantage, achieving an average relative 2 error of 0.00085, farsurpassing the performance of all other baselines. highlights the superior convergence behaviorof Fourier PINNs in both scenarios, underscoring their ability to handle challenging high-frequency andmulti-scale problems effectively.",
  "D Steady-State Allen-Cahn Problem": "Next, we examine the 1D steady-state Allen-Cahn equation, which is a nonlinear reaction-diffusion system.This problem serves as a more complex PDE benchmark for evaluating Fourier PINNs and baseline models.To ensure consistency, we test the same manufactured solutions used in the 1D Poisson case, defined byEquation 34, Equation 35, and Equation 36. Results. presents the relative 2 error distributions for Fourier PINNs and the top-performingbaseline models across all manufactured solutions, while detailed results for all models can be found in in the Appendix. Across all configurations, Fourier PINNs consistently achieve lower relative 2errors. Additionally, their faster convergence, as highlighted in , underscores the efficiency of theFourier PINN framework for solving the 1D steady-state Allen-Cahn problem. RFF-PINN (1, 50) RFF-PINN (3, 20) RFF-PINN (1, 20, 194) RFF-PINN (1, 50, 189) RFF-PINN (1, 20, 49, 50, 100) RFF-PINN (1, 20, 50, 85, 100) RFF-PINN (1, 20, 104, 197, 199) RFF-PINN (6, 36, 67, 79, 136) Fourier PINN (K=200) Relative 2 Error RFF-PINN (2 scales)RFF-PINN (3 scales) RFF-PINN (5 scales)Fourier PINN RFF-PINN (1, 50) RFF-PINN (3, 20) RFF-PINN (1, 20, 194) RFF-PINN (1, 50, 189) RFF-PINN (1, 20, 50, 85, 100) RFF-PINN (1, 20, 104, 197, 199) RFF-PINN (6, 36, 67, 79, 136) Fourier PINN (K=150) Fourier PINN (K=200) Fourier PINN (K=250) Fourier PINN (K=300) 1.65 102 1.7 102 1.75 102 1.8 102 1.85 102 1.9 102 1.95 102 2 102 Relative 2 Error RFF-PINN (2 scales)RFF-PINN (3 scales) RFF-PINN (5 scales)Fourier PINN",
  "u(x, t) = sin(x) cos(10t) + sin(2x) cos(20t).(43)": "This problem introduces a time-dependent component, testing the ability of models to handle first-timederivatives within a high-frequency context. We conducted an ablation study to assess the roles of basispruning and regularization by evaluating Fourier PINNs without these mechanisms. This study highlightsthe distinct contributions of basis pruning and regularization to model accuracy and computational efficiency. in the Appendix lists all hyperparameters and experimental details. shows the relative 2 error distributions for Fourier PINNs and top-performing baseline modelsacross all manufactured solutions. Detailed results for all models are provided in in the Appendix.As expected, Fourier PINNs with both basis regularization and pruning (denoted as \"reg + prune\") achievethe best results, while the configuration using neither (denoted as \"no reg + no prune\") yields the worstperformance. Fourier-PINN Without Basis Pruning. In configurations without basis pruning, all frequency candi-dates up to the maximum frequency K were allowed to contribute to the solution. Two variations weretested: with basis regularization (\"reg + no prune\") and without regularization (\"no reg + no prune\"). Bothconfigurations resulted in worse relative 2 errors than the pruned counterparts, although the regularizedversion performed slightly better than the unregularized one. Fourier-PINN Without Regularization. To further evaluate the impact of regularization, we testedconfigurations without the regularization term. Two setups were compared: with basis pruning (\"no reg +prune\") and without pruning (\"no reg + no prune\"). The configuration without regularization but with basispruning (\"no reg + prune\") achieved better relative 2 errors than the setup using regularization but no basispruning (\"reg + no prune\"). This result suggests that basis pruning has a slightly more significant influenceon solution quality than regularization.",
  "u(t, 1) = u(t, 1), ux(t, 1) = ux(t, 1)": "This problem is particularly challenging because the Allen-Cahn PDE introduces complex, multi-scale dy-namics driven by a combination of stiff reaction terms and spatial diffusion. Unlike problems with harmonicsolutions, the nonlinear terms generate sharp transitions and localized structures in both space and time,testing the models adaptability to varying frequency components and its robustness to sharp gradients. in the Appendix outlines the hyperparameter and experimental settings used in the example. In thisexperiment, we randomly sampled 8192 collocation points within the domain at each iteration. compares the convergence behavior of Fourier PINNs with baseline models (standard PINN and RFF-PINN).",
  "Results.As shown in , the Fourier PINN significantly outperforms both the standard PINNand RFF-PINN. The relative 2 error decreases steadily for Fourier PINNs, reaching values below 103": "after sufficient training iterations, while the baseline models plateau at higher error levels. Specifically, thestandard PINN struggles with stiffness, failing to reduce the error below 101 even after extended training.The RFF-PINN shows some improvement over the standard PINN but is highly sensitive to scale settings,resulting in slower convergence and larger errors compared to Fourier PINNs. Fourier PINNs leverage theaugmented Fourier layer structure to efficiently represent both smooth and sharp features, as evidenced bytheir rapid convergence and low final error. This experiment underscores the potential of Fourier PINNs tohandle challenging nonlinear PDEs with sharp transitions, where conventional PINN architectures struggle.",
  "Discussion": "The 1D and 2D problems results reveal that standard PINN, W-PINN, and A-PINN consistently fail toachieve reasonable solutions, with relative 2 errors remaining large ( 1.0) across all cases. This indicatesthat these methods struggle to capture high-frequency signals effectively. While RFF-PINNs achieve lowerrelative errors under specific scale configurations, they exhibit significant sensitivity to the choice of thenumber and range of scales. For most configurations (6070% of the 20 tested settings listed in ), RFF-PINNs failed with large solution errors. Notably, even in the single-scale solution case (a), successfulconfigurations often involved multi-scale settings (e.g., (1, 20, 194)), whereas all single-scale settings failed.These findings highlight the lack of a clear relationship between scale configurations and solution accuracy,posing a significant challenge for RFF-PINNs. In contrast, Fourier PINNs consistently delivered accurateresults, with relative 2 errors ranging from 103 to 104 across almost all test cases. A key advantageof Fourier PINNs is their robustness to the choice of the frequency range K. Provided K is sufficientlylarge, the method reliably selects the target frequencies, achieving comparable solution accuracy acrosscases. It is important to note that Fourier PINNs do not employ advanced loss re-weighting schemes (e.g.,NTK re-weighting used in RFF-PINNs or mini-max updates) in this study, as the focus was on evaluating",
  "jd=1Cj1,j2,...,jd A1(i1, j1) A2(i2, j2) Ad(id, jd),(45)": "where each factor matrix Aj Rnjrj maps the original tensor to a lower-dimensional representation. Thisdecomposition reduces the number of parameters to dj=1 njrj + dj=1 rj, allowing scalability as it growslinearly with d under fixed ranks rj.Another promising approach is Tensor-Train (TT) decomposition,where U is expressed as a chain of lower-order tensors, known as TT-cores:",
  "Jens Berg and Kaj Nystrm. A unified deep artificial neural network approach to partial differential equationsin complex geometries. Neurocomputing, 317:2841, 2018. doi: 10.1016/j.neucom.2018.06.056": "Shengze Cai, Zhiping Mao, Zhicheng Wang, Minglang Yin, and George Em Karniadakis. Physics-informedneural networks (PINNs) for fluid mechanics: a review. Acta Mechanica Sinica, 37(12):17271738, 2021.doi: 10.1007/s10409-021-01148-1. Francesco Calabr, Gianluca Fabiani, and Constantinos Siettos. Extreme learning machine collocation forthe numerical solution of elliptic pdes with sharp gradients. Computer Methods in Applied Mechanics andEngineering, 387, 2021. doi: 10.1016/j.cma.2021.114188.",
  "Yuyao Chen, Lu Lu, George Em Karniadakis, and Luca Dal Negro. Physics-informed neural networks forinverse problems in nano-optics and metamaterials. Optics Express, 28(8):1161811633, 2020": "Eric C. Cyr, Mamikon A. Gulian, Ravi G. Patel, Mauro Perego, and Nathaniel A. Trask. Robust trainingand initialization of deep neural networks: An adaptive basis viewpoint.In Proceedings of The FirstMathematical and Scientific Machine Learning Conference, pp. 512536. PMLR, August 2020.URL ISSN: 2640-3498. Shifei Ding, Han Zhao, Yanan Zhang, Xinzheng Xu, and Ru Nie. Extreme learning machine: algorithm,theory and applications. Artificial Intelligence Review, 44:103115, 2013. doi: 10.1007/s10462-013-9405-z. Gianluca Fabiani, Francesco Calabr, Lucia Russo, and Constantinos Siettos. Numerical solution and bi-furcation analysis of nonlinear partial differential equations with extreme learning machines. Journal ofScientific Computing, 89, 2021. doi: 10.1007/s10915-021-01650-5. Gianluca Fabiani, Ioannis G. Kevrekidis, Constantinos Siettos, and Athanasios N. Yannacopoulos. Ran-donets: Shallow networks with random projections for learning linear and nonlinear operators. Journal ofComputational Physics, 520:113433, 2025. doi: 10.1016/j.jcp.2024.113433.",
  "Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Proceedings of the3rd International Conference for Learning Representations (ICLR), 2015. URL": "Georgios Kissas, Yibo Yang, Eileen Hwuang, Walter R Witschey, John A Detre, and Paris Perdikaris.Machine learning in cardiovascular flows modeling: Predicting arterial blood pressure from non-invasive4d flow mri data using physics-informed neural networks. Computer Methods in Applied Mechanics andEngineering, 358:112623, 2020. doi: 10.1016/j.cma.2019.112623. Aditi Krishnapriyan, Amir Gholami, Shandian Zhe, Robert Kirby, and Michael W Mahoney. Characterizingpossible failure modes in physics-informed neural networks. Advances in Neural Information ProcessingSystems, 34:2654826560, 2021. Pola Lydia Lagari, Lefteri H Tsoukalas, Salar Safarkhani, and Isaac E Lagaris. Systematic constructionof neural forms for solving partial differential equations inside rectangular domains, subject to initial,boundary and interface conditions. International Journal on Artificial Intelligence Tools, 29(05):2050009,2020. Isaac E Lagaris, Aristidis Likas, and Dimitrios I Fotiadis. Artificial neural networks for solving ordinary andpartial differential equations. IEEE Transactions on Neural Networks, 9(5):9871000, 1998. Isaac E Lagaris, Aristidis C Likas, and Dimitris G Papageorgiou. Neural-network methods for boundaryvalue problems with irregular boundaries. IEEE Transactions on Neural Networks, 11(5):10411049, 2000. Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew Stu-art, and Anima Anandkumar.Fourier neural operator for parametric partial differential equations.In Proceedings of the 9th International Conference on Learning Representations (ICLR), 2021.URL",
  "Dong C. Liu and Jorge Nocedal. On the limited memory bfgs method for large scale optimization. Mathe-matical Programming, 45(13):503528, 1989. doi: 10.1007/BF01589116": "Ning Liu, Siavash Jafarzadeh, and Yue Yu. Domain agnostic fourier neural operators. In Proceedings of the37th Conference on Neural Information Processing Systems (NeurIPS 2023), volume 36, pp. 4743847450.Curran Associates, Inc., 2023. Lu Lu, Raphael Pestourie, Wenjie Yao, Zhicheng Wang, Francesc Verdugo, and Steven G Johnson. Physics-informed neural networks with hard constraints for inverse design. SIAM Journal on Scientific Computing,43(6):B1105B1132, 2021. doi: 10.1137/20M1360926. Liyao Lyu, Keke Wu, Rui Du, and Jingrun Chen. Enforcing exact boundary and initial conditions in thedeep mixed residual method. CSIAM Transactions on Applied Mathematics, 2(4):748775, 2021. doi:10.4208/csiam-am.SO-2021-0011.",
  "Clare D. McGillem and George R. Cooper. Continuous and Discrete Signal and System Analysis. HarcourtSchool, 3rd edition, 1991. ISBN 9780030747095": "Siddhartha Mishra and Roberto Molinaro. Physics informed neural networks for simulating radiative transfer.Journal of Quantitative Spectroscopy and Radiative Transfer, 270:107705, 2021. doi: 10.1016/j.jqsrt.2021.107705. Rambod Mojgani, Maciej Balajewicz, and Pedram Hassanzadeh.Kolmogorov nwidth and lagrangianphysics-informed neural networks: A causality-conforming manifold for convection-dominated pdes. Com-puter Methods in Applied Mechanics and Engineering, 404:115810, 2023. doi: 10.1016/j.cma.2022.115810.",
  "Alexander Novikov, Dmitrii Podoprikhin, Anton Osokin, and Dmitry P. Vetrov. Tensorizing neural networks.In Advances in Neural Information Processing Systems, volume 28, 2015": "Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen,Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deeplearning library. Advances in neural information processing systems, 32, 2019. Jaideep Pathak, Zhixin Lu, Brian Hunt, Michelle Girvan, and Edward Ott. Reservoir computing approachesfor solving nonlinear pdes. Physical Review Letters, 120(2):024102, 2018. doi: 10.1103/PhysRevLett.120.024102. Nasim Rahaman, Aristide Baratin, Devansh Arpit, Felix Draxler, Min Lin, Fred Hamprecht, Yoshua Bengio,and Aaron Courville. On the spectral bias of neural networks. In Proceedings of the 36th InternationalConference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pp. 53015310.PMLR, 2019.",
  "Ali Rahimi and Benjamin Recht. On randomized algorithms and random projection networks for functionapproximation. Advances in Neural Information Processing Systems, 21:12321240, 2008": "Maziar Raissi, Paris Perdikaris, and George Em Karniadakis. Physics informed deep learning (part i): Data-driven solutions of nonlinear partial differential equations. arXiv preprint arXiv:1711.10561, 2017. URL Maziar Raissi, Paris Perdikaris, and George E. Karniadakis. Physics-informed neural networks: A deep learn-ing framework for solving forward and inverse problems involving nonlinear partial differential equations.Journal of Computational Physics, 378:686707, 2019. doi: 10.1016/j.jcp.2018.10.045. Maziar Raissi, Alireza Yazdani, and George Em Karniadakis. Hidden fluid mechanics: Learning velocity andpressure fields from flow visualizations. Science, 367(6481):10261030, 2020. doi: 10.1126/science.aaw4741.",
  "Vadim Shapiro and Igor Tsukanov. Meshfree simulation of deforming domains. Computer-Aided Design, 31(7):459471, 1999. doi: 10.1016/S0010-4485(99)00044-7": "Justin Sirignano and Konstantinos Spiliopoulos. Dgm: A deep learning algorithm for solving partial differ-ential equations. Journal of Computational Physics, 375:13391364, 2018. doi: 10.1016/j.jcp.2018.08.029. N. Sukumar and Ankit Srivastava.Exact imposition of boundary conditions with distance functions inphysics-informed deep neural networks. Computer Methods in Applied Mechanics and Engineering, 389:114333, 2022. doi: 10.1016/j.cma.2021.114333. Luning Sun, Han Gao, Shaowu Pan, and Jian-Xun Wang.Surrogate modeling for fluid flows based onphysics-constrained deep learning without simulation data. Computer Methods in Applied Mechanics andEngineering, 361:112732, 2020. doi: 10.1016/j.cma.2019.112732. Matthew Tancik, Pratul Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Raghavan, Utkarsh Singhal,Ravi Ramamoorthi, Jonathan Barron, and Ren Ng. Fourier features let networks learn high frequencyfunctions in low dimensional domains. Advances in Neural Information Processing Systems, 33:75377547,2020. Igor Tsukanov and Sudhir R Posireddy. Hybrid method of engineering analysis: Combining meshfree methodwith distance fields and collocation technique. Journal of Computing and Information Science in Engi-neering, 11(3), 2011.",
  "Huan Wang, Can Qin, Yulun Zhang, and Yun Fu.Neural pruning via growing regularization.In 9thInternational Conference on Learning Representations (ICLR), 2021a. URL": "Sifan Wang, Yujun Teng, and Paris Perdikaris. Understanding and mitigating gradient pathologies in physics-informed neural networks.SIAM Journal on Scientific Computing, 43(5):A3055A3081, 2021b.doi:10.1137/20M1318043. Sifan Wang, Hanwen Wang, and Paris Perdikaris.On the eigenvector bias of fourier feature networks:From regression to solving multi-scale pdes with physics-informed neural networks. Computer Methods inApplied Mechanics and Engineering, 384:113938, 2021c. doi: 10.1016/j.cma.2021.113938.",
  "(a) 1D Allen-Cahn equation with true solution u(x) = sin(100x)": "PINN Strong BC PINN W-PINN A-PINN RFF-PINN (1, 50) RFF-PINN (3, 20) RFF-PINN (19, 71) RFF-PINN (39, 69) RFF-PINN (50, 100) RFF-PINN (1, 20, 194) RFF-PINN (1, 50, 189) RFF-PINN (20, 50, 100) RFF-PINN (38, 112, 119) RFF-PINN (44, 47, 165) RFF-PINN (1, 20, 49, 50, 100) RFF-PINN (1, 20, 50, 85, 100) RFF-PINN (1, 20, 104, 197, 199) RFF-PINN (6, 36, 67, 79, 136) RFF-PINN (50, 65, 83, 104, 139) Fourier PINN (K=150) Fourier PINN (K=200) Fourier PINN (K=250) Fourier PINN (K=300) Relative 2 Error SpectralPINN Strong BC PINNW-PINN A-PINNRFF-PINN (1 scales) RFF-PINN (2 scales)RFF-PINN (3 scales) RFF-PINN (5 scales)Fourier PINN"
}