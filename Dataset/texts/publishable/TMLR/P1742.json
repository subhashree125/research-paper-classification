{
  "Abstract": "The rapid adoption of large language models (LLMs) has led to a growing number of com-panies oering generative LLMs as callable services at varying costs. We nd that populargenerative LLM APIs, such as GPT-4, Gemini 1.5, and Claude 3.5, exhibit heterogeneouspricing structures, with fees that can dier by two orders of magnitude and heterogeneousperformance across tasks and input queries. This makes it challenging for users to decidewhich generative LLM APIs to utilize for their applications and budget. Motivated by thesendings, we propose FrugalGPT, an algorithmic framework that adaptively selects whichgenerative LLMs to use for dierent queries to reduce cost and improve accuracy.Our experiments demonstrate that, for a range of natural language tasks including news classi-cation, reading comprehension, and scientic question answering, FrugalGPT can matchthe performance of the best individual generative LLM (e.g., GPT-4) with up to a 98%cost reduction or improve the accuracy over GPT-4 by 4% at the same cost. The ideas andndings presented in this paper lay a foundation for using LLMs sustainably and eciently.",
  "Introduction": "We are currently witnessing a surge in the adoption of generative large language models (LLMs). The en-ticing potential of generative LLMs has led to a growing number of companies (such as AI21, Anthropic,Google, OpenAI, etc.) oering LLMs as callable services. Consequently, ML practitioners now frequentlybuild applications by invoking these foundation models. For example, Tweet sentiment analysis is an of-cial use case of ChatGPT (OpenAI, 2024a), Strabag uses Microsoft services to predict construction siterisks (Microsoft, 2024), and Stripe uses GPT-4 to detect fraudulent behavior (OpenAI, 2024b). However, practitioners often face challenges in deciding which generative LLM services to utilize for theirapplications and optimizing their budgets. The cost of generative LLM services can vary by up to two ordersof magnitude: for instance, the prompt cost for 10M tokens is $300 for OpenAIs GPT-4 Turbo but only $1.5for Googles Gemini 1.5 Flash 8B (as shown in ). Smaller LLMs are generally more aordable, buttheir performance is comparatively limited (as depicted in (d)). Larger LLMs like GPT-4 Turbo oerbetter performance but at the risk of escalating costs. In addition to their nancial burden, employing largerLLMs incurs signicant environmental and energy impacts (Bender et al., 2021; Wu et al., 2022; Schwartzet al., 2020).",
  ":Comparisons of dierent approaches to using LLM services.(a) The standard usage sends": "queries to a single LLM (e.g., GPT-4 Turbo), which can be expensive. (b) FrugalGPT, adaptively decideswhich LLMs to trigger for dierent user queries to reduce the inference cost. By optimizing the selectionof dierent LLM APIs (e.g., Gemini Flash, Llama 3 (70B), and GPT-4 Turbo), we can achieve substantialeciency gains. (c) LLM performance breakdown on HEADLINES (a nancial news dataset). Llama 3(70B) outperforms GPT-4 Turbo on 5% of queries and produces identical generations on 87% of queries.(d) FrugalGPT can reduce the inference cost by 98% while exceeding the performance of the best individualLLM (GPT-4 Turbo) on HEADLINES. This is because FrugalGPT successfully learns data subsets on whichinexpensive LLMs like Gemini Flash 8B are as good as or better than GPT-4 Turbo, and directs these datato the corresponding low-cost LLMs only. In this paper, we empirically demonstrate that for many of the tasks that generative LLMs are used for, it ispossible to evaluate a results quality using an inexpensive model. For example, we found that DistillBERTcan accurately predict the answer quality of many LLMs including GPT-4 Turbo and Llama 3 (70B) oncommon natural language tasks like classication and question answering. Furthermore, we nd that nogenerative LLM is universally\" superior to others. Take the task of classifying price sentiments from newsheadlines as an example (Sinha & Khandait, 2021). There are 5% of queries where Llama 3 (70B) is entirelyaccurate while GPT-4 Turbo makes errors, and 87% of queries where both models provide identical responses(as illustrated in (c)). Directing 92% (=87%+5%) of queries to Llama 3 (70B) and the remaining8% to GPT-4 Turbo is considerably more cost-eective and performant than relying solely on GPT-4 Turbo.These discoveries suggest the possibility of routing queries adaptively to dierent LLMs to both lower thecost and enhance the performance of LLM applications. Inspired by these ndings, we propose FrugalGPT, an algorithmic framework that adaptively determineswhich LLMs to use given a users budget. FrugalGPT rst learns a generation judger that assigns a scoreto indicate the quality of dierent LLMs generations for any given query. It then invokes a list of LLMssequentially until the judgers score for an answer surpasses a threshold. For example, FrugalGPT mayinitially call Google Gemini 1.5 Flash 8B to obtain an answer. If the judgers score for this answer is lowerthan a threshold of 0.9, Llama 3 (70B) is subsequently invoked to generate a new response. The judgers",
  "Published in Transactions on Machine Learning Research (12/2024)": ": Summary of commercial LLM APIs. We use 14 LLM APIs from 6 providers. The cost was retrievedin March 2023. The cost includes three components: input (proportional to the number of input tokens),output (proportional to the number of generated tokens), and a xed cost per request. LLMs costs candier by up to 2 orders of magnitude. For example, to process 1M input tokens, GPT-J from Textsynthcosts only $0.2, but OpenAIs GPT-4 costs $30.",
  "Related Works": "Model Ensembles. Model ensembles (Dong et al., 2020; Ganaie et al., 2022), which involve combiningmultiple ML models for prediction, have gained popularity in supervised learning (Garca-Pedrajas, 2009;Friedman, 2002), unsupervised learning (Yang et al., 2014), semi-supervised learning (Gupta et al., 2022), andweakly supervised learning (Diba et al., 2017). Recent work (Arora et al., 2022) shows that fusing multiplegenerations from GPT-J (Wang & Komatsuzaki, 2021) can compete with GPT-3s performance, and syn-thesizing multiple open-source LLMs generations leads to better performance than individual LLMs (Jianget al., 2023). Model ensembles typically require white-box access to multiple models for training, but LLMAPIs are often black-box. Moreover, model ensembles necessitate querying all models for any single query,thereby increasing costs. ML-as-a-Service and Cascades. Generative LLM APIs constitute a crucial component of the rapidlyexpanding machine-learning-as-a-service (MLaaS) industry. Recent studies have demonstrated the diversityof dierent ML APIs predictions (Buolamwini & Gebru, 2018; Koenecke et al., 2020; Chen et al., 2022b;a).The concept of using multiple services for speed is known as model cascade (Viola & Jones, 2004), which hasbeen applied in predictive ML domains such as pedestrian detection (Cai et al., 2015), analytic systems (Kanget al., 2017) and facial recognition (Li et al., 2015; Sun et al., 2013). Recent work (Chen et al., 2020; 2022c)builds a customized cascade for cost reduction, with a focus on classication ML APIs. However, theirapproach needs to estimate the performance of an ML API without querying it, based on simple signalssuch as labels from a proxy model.Such pre-query estimation is challenging for generative LLM APIs,",
  "whose outputs encompass a much larger space. FrugalGPT overcomes this by creating a post-query qualityestimator.Furthermore, for a given query, previous work invokes at most two APIs, while FrugalGPT": "allows invoking three or more given the vast number of LLM APIs. This renders it computationally morechallenging to nd the best calling strategies, and thus we also develop novel techniques to identify theoptimal strategies eciently (). Speculative Decoding. Speculative decoding has recently emerged for LLM inference acceleration withoutretraining or model architecture modication (Leviathan et al., 2023; Chen et al., 2023; Sun et al., 2023;Spector & Re, 2023; Liu et al., 2023). Its goal is to provide the same output as a large LLM at lower latency.It relies on inexpensive LLMs for most generations and switches to costly LLMs when necessary. However, itrequires access to the decoding module, which is not available for proprietary LLMs like GPT-4, and becauseit aims to give the same answer as the large LLM, it misses the opportunity to provide a better answer incases where the small LLM is more accurate. The remainder of the paper is organized as follows. We start by oering more context and the problemstatement in . We present how FrugalGPT works in . shows the empiricalbenets of FrugalGPT using real-world LLM APIs (including GPT-3, ChatGPT, and GPT-4). We discussfuture prospects in .",
  "i=1. Each fi() : P A is a function that, given a prompt p from the prompt": "space P, generates an answer from the answer distribution A. Note that to use LLM APIs, one has to converteach query q to some corresponding prompt rst. LLM APIs are associated with their own cost, typicallyconsisting of three components: a portion proportional to the length of the prompt, a portion proportionalto the length of the generated answer, and (sometimes) a xed cost per query. Formally, given a prompt p,the cost of using the ith API is ci(p) , ci,2fi(p) + ci,1p + ci,0, where ci,j, j = 0, 1, 2 are constants.",
  "An illustrative example.Adapting the case study provided by (Kaiser & Slowik, 2023), assume a small": "business operates a customer service using GPT-4. The company caters to 15,000 customers each month,with each customer asking three questions twice a week, totaling 360,000 queries per month. Suppose foreach question, its prompt averages 1800 tokens and the answer is around 80 tokens (as estimated by (Kaiser& Slowik, 2023)). Considering that the input and response costs of GPT-4 are $0.03 and $0.06 per thousandtokens, the total monthly cost amounts to 360 ($0.03 1800 + $0.06 80) $21.2K. Such a high cost isprohibitive for many small businesses.",
  "Problem statement: budget-aware LLM API usage.Our primary goal in this paper is leveraging": "LLM APIs within a budget constraint. Formally, this can be formulated as maximizing the overall taskperformance E(q,a)QA[r(q, a(s, q))], while ensuring the average cost is bounded by a user-dened valueb, i.e., E(q,a)QA[c(s, q)] b. Here, a denotes the correct answer to the query q, a(s, q) is the generatedanswer by some strategy s for query q, and c(s, q) is the associated cost for processing query q using strategys. The reward function r(, ) measures how closely the generated answer aligns with the user query.",
  "FrugalGPT Pipeline.FrugalGPT comprises three main components: an LLM router, an answer scorer,": "and a stop judger. Given a user query q, the LLM router is rst invoked to select an LLM to obtain itsresponse to the query. Next, the generation scorer takes the query, the answer, and the selected LLM asinput and generates a quality measurement as output. Based on the quality measurement and the invokedLLM service, the stop judger determines whether (i) to stop and return the answer, or (ii) to repeat theprocess of invoking the LLM router and generation scorer. The LLM router consists of two parts. First, given the previously invoked LLM service k, it selects the nextLLM service to use, denoted by k , (k), where : {?, 1, 2, , K} {?, 1, 2, , K} is a permutationof all LLM services (with ? representing no invocation). Second, it sends the query q to the kth serviceand obtains the generation fk(q) as output. Although the service permutation could depend on the inputquery in principle, our instantiation adopts a query-agnostic permutation () for simplicity. As an example,consider the case of three models: GPT-4, GPT-Neo, and GPT-J. In this case K = 3. The LLM router mayreturn GPT-Js generation for the rst time it is invoked. For the second and third time, it gives outputby GPT-Neo and by GPT-4, respectively. For the fourth time and beyond, it simply returns empty. Thiscorresponds to the permutation (GPT-J) = GPT-Neo, (GPT-Neo) = GPT-4, (GPT-4) = ?. Anotherinstance is that the LLM router rst invokes GPT-Neo, then GPT-J, and nally GPT-4. In this case, thepermutation becomes (GPT-Neo) = GPT-J, (GPT-J) = GPT-4, (GPT-4) = ?.",
  "Joint optimization of the FrugalGPT Pipeline.Conguring the LLM router and stop judger appro-": "priately is crucial to FrugalGPT. Technically, we need to congure (i) the LLM routers service permutationfunction () and (ii) the stop judgers threshold vector . Our goal is to maximize the expected rewardon the query distribution while satisfying the user budget. This problem can be formally modeled as thefollowing optimization problem:",
  "(1)": "Here, the objective is the expected performance (reward), the rst constraint ensures the average cost isbounded by the budget, the second constraint indicates that the stop judge returns the answer at the t-thiteration, and the last constraint indicates that the LLM router and the generation scorer are called repeatedlyfor previous iterations. L is a hyperparameter that controls the maximum number of LLM services to call fora query. Solving this problem is inherently challenging because the optimization space is vastly large. ()is a permutation function over all possible LLM services, and exhaustive search takes O(KL) computations.Moreover, even if () is xed, the problem is non-convex with respect to the threshold vector . In fact,we can show that even if the scorers are of high quality, the optimization problem is still NP-hard, formallystated as follows. We leave the proof in the appendix due to space constraints.Suppose the scorers are",
  "perfect, i.e., gi(q, a) > gi(q, a) r(q, a) > r(q, a) Then Problem (1) is an NP-hard problem": "To overcome this computational obstacle, we design a specialized optimizer for this problem. It (i) prunesthe search space of () by ignoring any consecutive selection of LLMs with small answer disagreement, and(ii) approximates the objective by interpolating it within a few samples. Search space pruning removes candidate permutation functions with relatively small maximum performanceimprovement, or MPI. Here, MPI is a function of two LLMs, k1, k2, that measures at most how manymistakes k2 incurs can be xed by k1. Formally, MPI(k1, k2) , Pr[r(q, fk1(q)) > r(q, fk2(q))]. Suppose k iscalled from the last iteration in the cascade. Then in the next iteration, calling any LLMs with small MPIwould not yield signicant performance gains and thus could be avoided. Inspired by this, we introduce thefollowing pruning condition",
  "for at most m 1 other values of k K}(2)": "That is to say, given the previously invoked LLM k, the next LLM to call must hold the top-m value of MPIwith respect to k. This reduces the search complexity from O(KL) to O(mL). In practice, we found thatm = 3 often suces to identify a high-quality cascade. Now suppose the function () is xed. The remaining step is to nd the optimal threshold vector . Thiscan be resolved via a two-stage approximation. First, we divide the search space L into a few equal-sizegrids. Next, within each grid, we approximate the objective by a quadratic function of the threshold vector,whose parameters are determined by the grid bound values. Then within each grid, we can leverage a QPsolver to nd the optimal solution. The nal solution is the best among all grids. The combination of theabove two techniques provides an ecient implementation with satisfactory performance, as demonstratedlater in .",
  "Experiments": "In this section, we present an empirical study on FrugalGPT. Our goals are four-fold: (i) understand whenand why FrugalGPT lowers the cost, (ii) quantify the cost savings attained by FrugalGPT while matching thebest individual LLM APIs performance, (iii) measure the trade-os between performance and cost enabledby FrugalGPT, and (iv) explore how dierent factors including data distribution shifts and scorers qualityaect FrugalGPT.",
  "h": ": A case study of FrugalGPT on the HEADLINES dataset. (a) The learned FrugalGPT on thisdataset with an overall budget of $6.5, one-fth of GPT-4s cost. FrugalGPT avoids querying GPT-4 as longas GPT-J and J1-L produce high-quality answers. (b) Sometimes GPT-4 makes a mistake, but FrugalGPTlearns to use the correct answers by J-1 and GPT-J. (c) Overall, FrugalGPT reduces the cost by 80%, whileimproving the accuracy by 1.5% compared to GPT-4. (d) The maximum possible improvement (MPI) foreach LLM pair, measuring how often one LLM (each row) makes a mistake while another (each column) iscorrect. Even for the best individual LLM, GPT-4, cheap LLMs (e.g., GPT-J) can be better on 6% of thedata. (e) FrugalGPT sends only 16.6% queries to GPT-4 and thus saves cost. with newer models would be discussed later. The details are summarized in . FrugalGPT has beendeveloped on top of these APIs and evaluated on a range of datasets belonging to dierent tasks, includingHEADLINES (Sinha & Khandait, 2021), OVERRULING (Zheng et al., 2021), COQA (Reddy et al., 2019),AGNEWS (Zhang et al., 2015) and SCIQ (Welbl et al., 2017). More details of the datasets and tasks canbe found in the Appendix. We focus on FrugalGPT with the hyperparameter L = 3, as this simplies theoptimization space and shows exciting results. Each dataset is randomly split into a training set (50%) tolearn FrugalGPT and a test set for evaluation (50%).",
  "A Case Study.We begin with a case study on the HEADLINES dataset. We set the budget to be $6.5,": "which is one-fth of GPT-4s cost. As depicted in (a), the learned FrugalGPT sequentially callsGPT-J, J1-L, and GPT-4. For any given query, it rst extracts an answer from GPT-J. If the score of thisanswer is greater than 0.96, the answer is accepted as the nal response. Otherwise, J1-L is queried. J1-Lsanswer is accepted as the nal response if its score is greater than 0.37; otherwise, GPT-4 is invoked toobtain the nal answer. Interestingly, this approach outperforms GPT-4 for numerous queries. For instance,given a headline \"Gold prices trade near 3-month high as Fed begins meeting\" from NASDAQ, FrugalGPTaccurately predicts that the price is going down, while GPT-4 provides an incorrect answer (as shown in(b)). Overall, FrugalGPT results in both accuracy gains and cost reduction, as illustrated in (c). This is partially because FrugalGPT only sends 16.6% queries to GPT-4 (see (e)).",
  "LLM diversity.Why can multiple LLM APIs potentially produce better performance than the best": "individual LLM? Similar to how ensemble methods can improve accuracy on many tasks, this is often dueto generation diversity: even an inexpensive LLM can sometimes correctly answer queries on which a moreexpensive LLM fails. Recall that we introduce maximum performance improvement ( MPI) in asan pruning metric. It also measures the generation diversity well: larger value of MPI indicates that onegenerative LLM give more responses dierent from another one. As shown in (d), MPI is indeedlarge for many pairs of generative LLMs. For instance, there are 6% queries where GPT-4 is incorrect butGPT-J (and GPT-C, J1-L, or Dolly) can give desired answers. This indicates the potential of combiningmultiple generative LLMs, and veries why FrugalGPT oers cost reduction without performance drops.",
  "Cost Savings.Subsequently, we examine if FrugalGPT can reduce costs while maintaining accuracy and,": "if so, by how much. displays the overall cost savings of FrugalGPT, which range from 50% to 98%.This is feasible because FrugalGPT identies the queries that can be accurately answered by smaller LLMsand, as a result, only invokes those cost-eective LLMs. Powerful but expensive LLMs, such as GPT-4, areutilized only for challenging queries detected by FrugalGPT.",
  "achieved by FrugalGPT, as illustrated in . Here we focus on three datasets due to space limitations;more results on other datasets can be found in the Appendix": "Several interesting observations can be made. First, the cost ranking of dierent LLM APIs is not xed.For instance, J1 is the second most expensive LLM on the HEADLINES dataset, while GPT-3 holds thatposition on the OVERRULING and COQA datasets. This is primarily due to the heterogeneous pricingmechanism: J1 incurs a high cost for each generated token but charges nothing for input tokens, whereasGPT-3 charges for both input and output tokens. Moreover, more expensive LLM APIs sometimes result inworse performance than their cheaper counterparts. For example, J1 is costlier than GPT-3 on HEADLINES,but its performance is inferior. These observations underscore the importance of aptly selecting LLM APIs,even in the absence of budget constraints. Next, we note that FrugalGPT enables smooth performance-costtrade-os across all evaluated datasets. This oers exible choices to LLM users and potentially helps LLMAPI providers save energy and reduce carbon emissions. In fact, FrugalGPT can simultaneously reduce costsand improve accuracy. For example, on the OVERRULING dataset, FrugalGPT achieves a 1% accuracygain while reducing costs by 73% compared to the best LLM API, GPT-4. This is likely because FrugalGPTintegrates knowledge from multiple LLMs. The example queries shown in further aid in understanding why FrugalGPT can simultaneouslyimprove performance and reduce costs. GPT-4 makes mistakes on some queries (e.g., the rst example inpart (a)), but some low-cost APIs provide correct predictions. FrugalGPT accurately identies those queriesand relies solely on the inexpensive APIs. For example, GPT-4 incorrectly infers no overruling from the legalstatement \"The time has come to reconcile and regularize our cases in this eld,\" as shown in (b).However, FrugalGPT accepts GPT-Js correct answer, avoiding the use of expensive LLMs and improvingoverall performance.Naturally, a single LLM API is not always correct; FrugalGPT overcomes this by",
  "Yes": "0.91 > 0.9GPT-J YesYes GPT-4No 1.0 > 0.9 GPT-4 When I [...] a littleblack-walnut shelf[...] Q: What was the shelf made of? GPT-3GPT-4J1 GPT-4 [...] told every Tuesday for theirstory time. [...]. Q:when did they have time free? 0.1 < 0.2 GPT-3J1 [..] Cap Winters [...]added a thousandgrey hairs to his head [...] Q: Did he have red hair? 0.8 > 0.2GPT-3 GPT-4 0.6 > 0.3 GPT-4 The textdoes not mention this.",
  "Accuracy": ": FrugalGPTs performance on the HEADLINES (left) and SCIQ (right) datasets using more recentmodels. In particular, we use eight proprietary models, GPT-4-Turbo, GPT-4o, and GPT-4o-mini oeredby OpenAI, Jamba 1.5 Large from AI21, Claude 3.5 Sonnet from Anthropic, Gemini 1.5 Pro, Gemini 1.5Flash, and Gemini 1.5 Flash 8B from Google, and two open-source models, LLama 3 (70B) and Gamma 2(9B) provided by Together AI. We use the same prompts and temperatures for all these experiments. OnSCIQ, we omit three models since their accuracy is much lower than other models, namely, LLama 3 70B(24.1%), Gemimi Pro (53.9%) and Gemini Flash 8B (43.6%). FrugalGPT can reduce the cost by more than90% while matching the best individual models performance on these datasets. performance and are widely used in many applications. Their prices are summarized in , collectedin November 2024. Compared to , the overall prices are indeed lower, but there is still a large pricedierence between these models. shows the performance-cost tradeo achieved by FrugalGPT using these new models on HEAD-LINES (left) and SCIQ (right). Overall, FrugalGPT consistently oers large cost savings. For example, tomatch GPT-4-Turbos performance, it requires less than 10% of the cost. It is also interesting to note thatexpensive models again are not necessarily better than cheap models. For example, GPT-4os performance isworse than GPT-4o-minis on this task, although the formers price is much higher. This further underscoresthe importance of choosing models carefully even if the budget is large.",
  "Comparison with a threshold baseline.To further understand the benets of FrugalGPT, we also": "compare it against a simple thresholding method. In particular, it rst invokes GPT-J, and computes thelog probability averaged over all output tokens. If the averaged log probability is larger than a threshold,GPT-Js response is accepted. Otherwise, GPT-4 is invoked. This can be seen as a variant of FrugalGPTwith L = 2, except that the scorer is simply the rst generators own judgment. We evaluate this methodwith dierent threshold values on the HEADLINES dataset, and show its performance in as theblue dashed line (the blue texts correspond to the threshold values).",
  ": Comparison of FrugalGPT with a simple threshold baseline on the HEADLINES dataset. Overall,we observe that FrugalGPT consistently outperforms the threshold method given dierent cost constraints": "how likely the answer is correct. Second, a threshold value in the middle (-0.8) leads to the best performance.This again veries our intuition that no single model is the best. Finally, FrugalGPT consistently outperformsthis threshold method. For example, to match GPT-4s performance, this simple baseline reduces 60% costwhile FrugalGPT reduces more than 90%. This is because FrugalGPT uses three models sequentially, andalso uses an optimized generation scorer.",
  "ber 2024.As of November 2024, Claude 3.5, GPT-4o, LLama 3 (70B), and Gamma 2 (9B) had dif-": "ferent versions, and in our experiments we used claude-3-5-sonnet-20240620, gpt-4o-2024-05-13,Meta-Llama-3-70B-Instruct-Turbo and gemma-2-9b-it oered by the corresponding providers, respec-tively. Overall, the prices are lower than those in 2023, but there is still a large price dierence betweenthese new models. For example, the generation cost of GPT-4 Turbo is 200x of Google Flash 8Bs cost.",
  "Evaluation using more recent models.While the main evaluation was conducted using models available": "in 2023, many new models with lower prices have been developed since 2024. To understand how FrugalGPTworks with these newly developed models, we evaluate its performance using a few more recent models onthe HEADLINES and SCIQ datasets. In particular, we use eight proprietary models, GPT-4-Turbo, GPT-4o, and GPT-4o-mini oered by OpenAI, Jamba 1.5 Large from AI21, Claude 3.5 Sonnet from Anthropic,Gemini 1.5 Pro, Gemini 1.5 Flash, and Gemini 1.5 Flash 8B from Google, and two open-source models,LLama 3 (70B) and Gamma 2 (9B) provided by Together AI. This is because these models show superior",
  "Performance Resilience to Data Distribution Shifts.A common challenge when deploying ML sys-": "tems in practice is data distribution shifts, i.e., the queries encountered during deployment dier from thosein development. To understand the robustness of FrugalGPT against data distribution shifts, we trained Fru-galGPT on the original HEADLINES training data and evaluated its performance on four testing datasetswith dierent distributions. Specically, we created these testing datasets by altering the distribution oflabels. For instance, in Variant 1, the label distribution is 33% (up), 17% (down), 17% (none), and 33%(neutral). Conversely, the original datasets label distribution is balanced (25% for each label). Details canbe found in in the Appendix. As depicted in (a) in the appendix, the performance of bothFrugalGPT and GPT-4 remains relatively consistent across dierent data distributions. Interestingly, whileusing only 10% of GPT-4s cost, FrugalGPT often delivers similar or superior performance compared toGPT-4 under several testing data distributions.",
  "Eects of Scorer Functions.The scorer plays a crucial role in FrugalGPT. Therefore, it is essential": "to study how the scorers quality impacts FrugalGPTs performance. In this regard, we focused on threebackbones for the scorer with varying numbers of parameters: ALBERT (11M), DistilBERT (67M), andBERT (110M). We trained the scorer on the HEADLINES dataset using dierent backbone models andcompared the performance of the resulting FrugalGPT, with a budget of 10% of GPT-4. As illustrated",
  "Discussions and Future Prospects": "The substantial cost of employing LLMs in real-world scenarios presents a considerable barrier to theirwidespread usage. In this paper, we discovered that for many tasks on which LLMs are commonly usedtoday, (i) small models can predict the quality of LLMs accurately, and (ii) no LLM is universally betterthan others. Based on these ndings, we introduce FrugalGPT, our approach to resolving the cost challenge.Our empirical ndings show that FrugalGPT can reduce costs by up to 98% while preserving the performanceof cutting-edge LLMs."
}