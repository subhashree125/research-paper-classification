{
  "Abstract": "From celebrity faces to cats and dogs, humans enjoy pushing the boundaries of art by blend-ing existing concepts together in new ways. With the rise of generative artificial intelligence,machines are increasingly capable of creating new images. Generative Adversarial Networks(GANs) generate images similar to their training data but struggle to blend images fromdistinct datasets.This paper introduces MiddleGAN, a novel GAN variant that blendsinter-domain images from two distinct input sets. By incorporating a second discriminator,MiddleGAN forces the generator to create images that fool both discriminators, thus cap-turing the qualities of both input sets. We also introduce a blend ratio hyperparameter tocontrol the weighting of the input sets and compensate for datasets of different complex-ities. Evaluating MiddleGAN on the CelebA dataset, we demonstrate that it successfullygenerates images that lie between the distributions of the input sets, both mathematicallyand visually. An additional experiment verifies the viability of MiddleGAN on handwrittendigit datasets (DIDA and MNIST). We provide a proof of optimal convergence for the neuralnetworks in our architecture and show that MiddleGAN functions across various resolutionsand blend ratios. We conclude with potential future research directions for MiddleGAN.",
  "Introduction": "Humans have always loved to create, especially visually. Using advanced photo manipulation tools suchas Photoshop, digital artists have produced highly realistic blended faces of well-known celebrities (EmmaTaggart). Simultaneously, with the rise of generative artificial intelligence (AI), computers are now creatingimages and other artistic works as well (Cetinic & She). One such method of image creation is GenerativeAdversarial Networks (GANs). GANs are machine learning models that are trained on a set of images andlearn to create new images (from random noise) that are similar to the training images. More formally,GANs learn to generate images that are in distribution with the images of the input domain. The use ofGANs to create images has been extensively explored over the last decade. However, traditional GANs arenot designed to blend two distinct sets of images together. In this paper, we present MiddleGAN, a novel variation of the traditional GAN, which takes as input twodomains of images and learns to create images that lie between the input domains. In essence, MiddleGANaims to create blended inter-domain images that contain features from both input domains. For example,given the input domains of male and female faces, MiddleGAN can produce images of human faces withboth masculine and feminine qualities, as shown in . Compared to existing state-of-the-art multi-discriminator GAN variations, such as FairGAN (Xu et al.) and D2GAN (Nguyen et al.), the dual-datasetinput and blended-image objective of MiddleGAN is novel. Instead of manually blending the faces of femaleand male celebrities, an artist could use a model such as MiddleGAN to create entirely new faces at the clickof a button. Artists and designers can use the blended features to create unique and original artworks. Bygenerating faces or figures that combine male and female traits, they can explore new aesthetic possibilitiesor challenge traditional gender representations in art and media. In addition, in industries like gaming andanimation, blending features from different domains (e.g., male and female) can allow for more nuancedcharacter creation. It could help generate characters with androgynous features or those that evolve acrossdifferent gender traits dynamically.",
  "Second, we extend the initial functionality of MiddleGAN to introduce a Blend Ratio, which controlsthe amount of emphasis placed on each input domains corresponding discriminator": "Third, through extensive evaluation on the CelebA dataset, we show that images generated byMiddleGAN are within the expected distribution (via t-SNE) and have both masculine and femininequalities when visually examined. The remainder of this paper is organized as follows: describes related work and the context ofthis project. describes the theoretical basis of MiddleGAN and its model architecture. discusses our implementation of MiddleGAN. details our extensive evaluation of MiddleGAN. discusses MiddleGAN and outlines opportunities for future work. concludes our paper.",
  "In this section, we present an overview of the original generative adversarial network (GAN) as well as severalsubsequent GANs as they relate to our current work": "Generative Adversarial Nets: Since the introduction of the first generative adversarial network over adecade ago, many novel variations of GANs have been introduced. However, at the start of them all is thework of Goodfellow et al. in the paper \"Generative Adversarial Nets\" (Goodfellow et al.). The original GANcontained one discriminator and one generator. These models are trained in parallel. The generator learns totake random noise as input and produce an image as output. The discriminator learns to distinguish betweenreal images and those created by the generator. By engaging in a minimax game, the generator learns toproduce better images while the discriminator learns to better differentiate between real and generatedimages. The goal of training is to create a generator model that can produce images, from uniform randomnoise, that are within the distribution of the original training images. In other words, \"GANs are a frameworkfor teaching a deep learning model to capture the training data distribution so we can generate new datafrom that same distribution\" (Tutorials). GANs have been used in a wide range of applications, ranging from image-to-image style transfer to domaintransfer tasks. CycleGAN focuses on \"unpaired image-to-image translation,\" while GP-GAN focuses on high-resolution composite image blending (Wu et al.; Zhu et al.). The work of Sankar et al. focuses on the useof GANs for domain adaptation, and the work of Rahman et al. focuses on how GANs can be leveraged fordomain generalization. DCGAN: While an important first step, the original GAN as proposed by Goodfellow et al. is notoriouslyhard to train and can often fail to converge (Nie & Patel). Due to these challenges, many improved GAN",
  "Published in Transactions on Machine Learning Research (12/2024)": "In our experiments, we also observed a reduction in mode collapse by incorporating an adaptive weighteddiscriminator Zadorozhnyy et al. (2021), which contributed to more consistent convergence across multipletraining runs. Initially, without the WGAN-GP loss function, mode collapse occurred frequently duringtraining. However, after introducing both the WGAN-GP loss and the adaptive weighted discriminator,mode collapse was significantly reduced, with very few occurrences. These findings show that while progresshas been made, there is still potential for further optimization to enhance stability.",
  "Theory": "In , we are going to provide detailed information on exactly how we achieve to obtain the inter-domain. In this Section (), we explore what is the potential usage of the inter-domain. To wit, theinter-domain, whose similarity to the two original domains is showcased by being equally distant (measuredvia Wasserstein distance) to the two original domains, can be used as an intermediate domain shown tofacilitate better domain adaptation and generalization: existing works Na et al. (2021); Wang et al. (2022)have already showcased that empirically, adapting from one original domain (source) to another (target) isuseful. Here, we theoretically prove that the inter-domain generated by MiddleGAN can indeed lower theerror difference over distribution and domain shift if we adapt from one original domain to the inter-domainthen to the other original domain, compared with the strategy in which the domain adaptation/generalizationhappens directly from one original domain (that is used as the source) to the other original domain (that isused as the target). The purpose of this proof is to showcase a potential usage for the generated syntheticinter-domain.",
  "Preliminaries": "Let X and Y represent the input and output spaces, respectively. Let X and Y be random variables takenfrom the input and output spaces. For a given domain (e.g., female faces), the distribution is denoted as over X Y. When considering only the sample distribution and not the joint sample-label distribution, wedenote the sample distribution of over the input space X as (X).",
  "Model Architecture & Design": "In this section, we provide an overview of the model architecture as well as the design of MiddleGAN.Additionally, we provide a theoretical basis for MiddleGAN, building on top of the original GAN. We showshow that there exists optimal parameters for both discriminators as well as an optimal solution for theparameters of the generator. Before we discuss MiddleGAN, we need to discuss the original GAN on which MiddleGAN is based. In theoriginal GAN (Goodfellow et al.), the generator G and the discriminator D engage in a minimax game inwhich G tries to minimize a value objective V (G, D) whereas D tries to maximize it. V (G, D) is defined inEquation 1, in which p is the distribution of the real samples and q is the distribution of the noise. A keyobservation obtained from Equation 1 is that Gs effort is to generate G(z) whereas z is an input noise suchthat G(z) will be in-distribution with the distribution of the real samples p.",
  "+ Ezq(z)[log(1 D(G(z)))](1)": "Based on the key observation that we obtain from Equation 1, in MiddleGAN we propose to employ twodiscriminators, Da and Db. Each discriminator is only aware of one domain of images. The first discriminator,termed Da, only knows of the images in Domain A. Similarly, the second discriminator, termed Db, onlyknows of the images in Domain B. Neither discriminator knows of the existence of the other domain ofimages, and is solely responsible for determining if an image came from its corresponding domain or not.The generator G engages in a two-way minimax game with the two discriminators. The samples it generateswill be in the middle of the feature space of both input domains. Below, we empirically prove that thegenerated samples in pm are represented by the features that are invariant across the input domains.",
  "Inputs": "LDB LDA : In this figure we describe how to use the MiddleGAN to generate fake, domain agnostic samples.As shown in the diagram, neither discriminator is directly aware of the others existence or the existence of asecond dataset. This is importance, as it allows each discriminator to focus solely on differentiating betweenimages in its own dataset and any other images, regardless of source. Black arrows indicate the forwardflow of information, while red arrows indicate backwards propagation. The dashed green line indicates thatthe generated images come from the trained generator.",
  "Beyond the Middle Feature Space": "With two discriminators, the generator G faces an additional challenge, as its success is determined by itsability to fool both discriminators. In order to do this, the generator must learn to produce images thathave features of both input domains. Simply put, the loss of the generator is the averaged loss from bothdiscriminators, as shown in Equation 8.",
  "(8)": "In Equation 8, the losses of both discriminators are weighed equally. This aims to ensure that the generatorG learns to produce images that are in the middle of both input domains. However, this is an artificiallimitation, as the weights for the losses from each discriminator do not have to be evenly weighted. Toexpand the flexibility of MiddleGAN, we introduced a new blending hyperparameter, the blend ratio, intoour model architecture.",
  "LG = LDA (BR) + LDB (1 BR)(9)": "The updated loss for the generator is shown in Equation 9. The blend ratio can range from 0 to 1, anddirectly controls the percent of weight placed on the loss of DA. A blend ratio of 0.5 equally weighs the lossesfrom both discriminators and replicates our initial implementation of MiddleGAN. A blend ratio of 0 wouldcompletely ignore Domain A and cause MiddleGAN to act like a traditional GAN, using only Domain B asinput. A blend ratio of 0.75 would result in the loss of discriminator A carrying three times the weight ofthe loss of discriminator B. The blend ratio vastly increases the range of images that MiddleGAN can learnto produce.",
  "Choosing the Blending Ratio": "In many generative models, the influence of domains is affected not only by the blend ratio but also byfactors such as the underlying data distributions and the number of samples. However, in MiddleGAN, theblend ratio directly determines how the losses from the two discriminators are weighted, enabling us to sys-tematically blend features from both domains without relying on assumptions about the data distributions.Since the discriminators are trained independently on their respective datasets, the blend ratio effectivelyserves as a weighted vote between the loss functions, providing more precise control over the blendingprocess than the raw characteristics of the data alone. We agree that manually selecting the appropriate blend ratio can be inefficient. To address this, we suggestusing grid search to systematically optimize the blend ratio. Our aim is to minimize metrics like Kernel",
  "Novelty of MiddleGAN": "At first glance, MiddleGAN might appear similar to a traditional GAN, but with two discriminators insteadof one. However, the second discriminator plays a pivotal role that sets MiddleGAN apart from standardGANs.While many GAN variants utilize multiple discriminators, MiddleGAN is uniquely designed togenerate inter-domain images that blend features from two distinct datasets, which is not the primary focus oftypical GANs. The second discriminator is not merely an incremental addition; it is crucial for ensuring thatthe generated images effectively combine characteristics from both input domains. This enables MiddleGANto function in a space between domain translation and image synthesis, providing a solution for generatingimages that exist between two feature distributionssomething that traditional GANs, like DCGAN orStarGAN, are not built to do. Additionally, the blend ratio hyperparameter enhances the models flexibility,allowing precise control over the influence of each domain. This feature makes MiddleGAN particularlyvaluable in scenarios where a balance between characteristics from two domains is required, representing asignificant advancement over conventional GAN approaches.",
  "Implementation": "Our implementation of MiddleGAN builds upon PyTorchs official implementation of DCGAN (Tutorials).This implementation was designed for use with the CelebA dataset, which sped up our initial developmenttime. However, this implementation had several limitations, most notably a fixed input image size of 64px.By dynamically computing the number of layers required by both the generator and discriminator modelsbased on the requested image size, we enabled our implementation of DCGAN to support any power-of-twoimage size. Next, we modified our implementation of DCGAN to add an additional discriminator and support two inputdomains. These changes formed the basis for MiddleGAN. We modified the model architecture so that ineach training epoch, images from both domain A and domain B were seen by the model. To account fordifferences in the sizes of the input domains, we leveraged the itertools librarys Cycle method to ensure thatthe length of the smaller dataset would appear to match that of the larger dataset (pyt, a). With the additionof the second discriminator, we simply adjusted the loss function for the generator to take into account thelosses of both discriminators when evaluated on generated images. At this stage, our implementation ofMiddleGAN could generate images, albeit with low quality. With the basic MiddleGAN implementation complete, we began preliminary testing and evaluation of themodel.We observed high training instability of the GAN and poor quality of generated images.Aftermaking these observations, we began to look into ways to stabilize the training of the GAN. In parallel, we were exploring the idea of a transformer-based implementation of MiddleGAN, based onTransGAN (Jiang et al.). While the paper authors provided a PyTorch implementation of TransGAN (git, b)(which itself was based on their earlier work, AutoGAN (git, a)), we opted for a cleaner re-implementation ofTransGAN (Sargn). Using TransGAN as our base model, we developed a low resolution (32px) transformer-based version of MiddleGAN. This implementation utilized the loss function from WGAN-GP (Gulrajaniet al.)to stabilize the training process.A sample of the images produced with our transformer-basedimplementation of MiddleGAN is shown in a.c and b were very intermixed,showing that generated samples were not clearly between the input distributions.Given these results,along with significantly longer training times we observed, we ultimately declined to pursue development",
  "(c) t-SNE Visualization": ": A sample of images generated with our transformer-based MiddleGAN implementation as well asassociated tSNE visualizations. The images were generated at a resolution of 32px by 32px with a blendratio of of 0.50. As shown by the t-SNE visualizations, the generated images fell within thedistribution of both input domains (as opposed to between it).",
  "of a transformer-based MiddleGAN for this project. We leave it to future research to better evaluate theviability and effectiveness of a transformer-based MiddleGAN": "While we ultimately declined to pursue a Transformer-based implementation of MiddleGAN, that work wasnot in vain, as we did observe that the loss function being used in TransGAN was leading to more stablemodel training. The loss function, WGAN-GP loss, is based on the work of Gulrajani et al. in ImprovedTraining of Wasserstein GANs (Gulrajani et al.). When we implemented WGAN-GP loss on our DCGAN-based MiddleGAN, we observed more stable training performance.Taking inspiration from the originalWGAN paper (Arjovsky et al.), we also modified our model to train the discriminators at a higher frequencythen the generator, which also led to improved quality in the output images.",
  "Experimental Setup": "Dataset: We primarily evaluated MiddleGAN using the CelebA dataset (Liu et al., 2015), which wasproposed by Liu et al. in 2015. The CelebA dataset contains over 200,000 annotated images of celebrityfaces.We selected this dataset due to its large size, extensive use in prior works, and separability intotwo domains (Male and Female). To create the two domains we used the boolean Male identifierpresent in the original dataset annotations1. By splitting on the Male identifier we created two smallerdatasets, which we designated Male (with approx. 84,000 images) and Female (with approx. 118,000images).In all experiments, the Female dataset was assigned domain A, and the Male dataset wasassigned domain B. These dataset-domain assignments were based on alphabetical ordering, but given theorder-agnostic structure of MiddleGAN with regards to input domains, we would anticipate identical resultsif the dataset-domain assignments were reversed. Image Generation: To evaluate MiddleGAN, we used the Male and Female datasets to train ninedifferent MiddleGAN models using a combination of three blend ratios (0.25, 0.50, and 0.75) and threeimage sizes (32px, 64px, and 128px). Aside from the blend ratio and Image Size, all models used the samehyperparameters as shown in . If our selection of a hyperparameter was heavily influenced by one 1All annotations in the CelebA dataset are booleans. As such, we made the assumption that any image that had a value of1 for the Male identifier was male, and a value of -1 was female. While this may not be 100% accurate, as a -1 valueis technically Not Male as opposed to Female, we do not expect that this assumption significantly impacted our results.",
  "(c) Generated Images": ": This figure shows the results of images from both input domains as well as images generated byMiddleGAN. We used an image size of 128px and a blend ratio of 0.5. The generated images have bothmasculine and feminine qualities. or more prior works, those works are noted in the Source Paper column. All models were trained on aNVIDIA A100 GPU, with an average per-model training time of around 12 hours. We generated 1000 imagesper model for use in our experiments. Feature Extraction & t-SNE: We elected to use the pre-trained ResNet101 model provided by PyTorchas the basis of our feature extractor (pyt, c; He et al.). We then fine-tuned the model to provide a binaryclassification for male and female faces from the original CelebA dataset.This resulted in 3 fine-tunedResNet101 models, one per Image Size. From there, we were able to extract feature vectors (length=2048)from the second-to-last layer of the ResNet101 model. We extracted feature vectors for each of the nine sets ofimages generated by MiddleGAN, as well as matching-size sets of images from both input domains. Roughlyspeaking, this means we extracted 3000 feature vectors for each of the nine models, which were evenlysplit between domain A images, domain B images, and generated images. Leveraging principal componentanalysis (PCA), we reduced the length of each extracted vector to 50 (skl, b). After performing PCA, weinput the feature vectors into t-SNE in order to better understand the relationship between the input imagesand generated images.",
  "Experimental Results": "We achieved positive results on all nine trained versions of MiddleGAN, across a variety of blend ratios andimage sizes. shows a sample of images from both input domains as well as images generated byMiddleGAN. An informal evaluation by the authors of the paper found the generated images to have bothmasculine and feminine qualities, which one would expect given the input domains of male and female faces.Beyond the mix of masculine and feminine qualities, we noted a diverse range of skintones and ethnicitiespresent in both the input images and generated images, which further shows the diversity of images thatMiddleGAN can generate. This is also an indicator that MiddleGAN did not suffer from mode collapse,which is when the generator starts producing the same output (or a small set of outputs) over and overagain (pro). While the training loss for the generator slowly increases over the course of the training, we continued to seeimprovements in the resulting imagery, as shown in , which shows samples generated by MiddleGANon fixed noise across training epochs. For this project, all training stopped after 200 epochs. We leave it tofuture work to evaluate if the quality of images generated by MiddleGAN could continue to improve after200 epochs.",
  "KID and FID Scores to Evaluate the Generated Images": "The FID score is commonly used to measure how closely the distribution of generated images aligns withthat of real images by comparing feature representations extracted from a pre-trained Inception network. Itevaluates both the realism and diversity of the generated images, with lower FID values indicating a bettermatch to real images. The KID score, in contrast, is an alternative metric that does not assume a Gaussiandistribution for the features and is particularly beneficial when working with smaller datasets, as it avoidsthe bias that FID can introduce with limited sample sizes. Its important to emphasize that FID and KID are typically applied in contexts where the goal is to generateimages that closely mirror the real image distribution. However, our objective in this work is different.MiddleGAN is designed to generate blended inter-domain images that incorporate features from two distinctdomainsmale and female faces. As a result, the generated images are expected to fall somewhere betweenthe two real distributions.Consequently, higher FID and KID values do not necessarily indicate poorperformance but instead reflect the intentional design of our model. With this understanding, we presentthe computed FID and KID scores in . Although the FID scores for the generated images compared to the male and female distributions are higherthan what is typical for GANs designed to replicate a single domain, this is due to the specific nature ofour task. Notably, FID(female, generated) < FID(female, male) and FID(male, generated) < FID(female,male), indicating that the generated images resemble both male and female faces more closely than male andfemale faces resemble each other. This demonstrates that MiddleGAN successfully captures the intermediatecharacteristics of both domains, producing the blended images as intended. The KID scores show a similar",
  "KID0.06550.07000.0940": ": The KID and FID scores to evaluate the generated images. It is worth highlighting that FID andKID are generally employed in tasks where the aim is to generate images that closely resemble the real imagedistribution. However, this differs from our objective in this work. MiddleGAN is specifically designed toproduce blended inter-domain images that combine features from two distinct domains (male and femalefaces). As a result, the generated images are expected to fall between these two real distributions. Therefore,higher FID and KID scores do not necessarily signify poor performance but rather reflect the intentionaldesign of our model.",
  "(f) t-SNE Visualization (128px)": ": The impact of different image sizes on the images generated by MiddleGAN, with a blend ratioof 0.5. While the generated images contain masculine and feminine qualities at all resolutions, the t-SNEvisualizations are noticeably different at lower resolutions. This is expected, as at low resolutions, there arenot enough pixels to clearly differentiate between masculine and feminine qualities. trend, further confirming that the generated images capture the diversity and overlap of features betweenthe two domains, rather than merely replicating features from one domain in isolation. In conclusion, whileFID and KID are traditionally used to assess how well generated images replicate real ones, in our case,these metrics reflect the inter-domain nature of the generated images, which are purposefully designed toblend features from both male and female faces. We hope this explanation clarifies our approach and therationale behind the FID and KID results.",
  "(c) Perplexity = 90": ": This figure shows the results of our t-SNE perplexity experiments. We used an image size of 128pxand a blend ratio of 0.5. We varied the perplexity from 5 to 90. In all cases, the generated images fell withinor between the input domain images, as expected. blend-ratio of 0.25 caused the generated images to largely overlap with the images from the Male domain. Asimilar yet reversed pattern was visible with a blend ratio of 0.75, which resulted in more feminine images. As previously discussed in , we modified the traditional DCGAN architecture to support any power-of-two image size by scaling the number of layers in the generator and discriminators. To assess the successof this modification, we evaluated MiddleGAN on three image sizes - 32px, 64px, and 128px. showsour results. Our results show that MiddleGAN can produce high quality images across a wide range of imagesizes. Additionally, despite having different image resolutions (and thus layers), all three models had similartraining losses and all had strong alignment when visualized with t-SNE. One key observation is that thegenerated samples fell within the distributions of the input domains at lower resolutions, while the generatedsamples fell between the distributions of the input domains at higher resolutions. We hypothesize that thisis because at lower resolutions, there are not enough pixels to distinguish between masculine and femininefaces.",
  "Blend Ratio Experiments": "The inclusion of a blend ratio in our model architecture enabled greater flexibility and variety in the rangeof images that MiddleGAN could produce. In this section we explore the impact that the blend ratio hadon generated images as well as the corresponding visualizations. We evaluated MiddleGAN on three blendratios - 0.25, 0.50, and 0.75. shows our results along with samples of the original input images asreference. When compared to a blend ratio of 0.50, the results shown for the other blend ratios are noticeably different.A blend ratio of 0.25 weighs the loss of discriminator A (which trains on Domain A, female images) at 25%and weighs the loss of discriminator B (which trains on Domain B, male images) at 75%. As such, theresulting images (shown in a) are noticeably more male than images generated with a blend ratioof 0.5, while still being more feminine than the original all-male images. The training loss is also different,as discriminator B shows much lower losses than discriminator A. This intuitively makes sense, as the lossof discriminator B is more heavily penalized during training, and thus a priority for the model to minimize.Lastly, the t-SNE visualization shows a shift in distribution, with the generated images falling both withinand between the male and female images, as shown in d. A blend ratio of 0.75 shows similar butinverse changes, with the images, training losses, and t-SNE visualizations all being biased towards femaleimages (shown in Figures 1c, and 1f).",
  "specific concern to us was the Perplexity hyperparameter, which past research has established can have adramatic effect on the visualizations that t-SNE generates (Wattenberg et al., 2016; skl, a)": "shows the results of three different perplexity experiments we performed, with perplexity valuesranging from 5 to 90. We selected this range of values based on the guidance provided in the scikit-learnimplementation of t-SNE, which recommends a value between 5 and 50 (skl, c). For these experiments, allinput images were 128px resolution images, with the generated images created with a blend ratio of 0.5. Ourresults show that perplexity did not have a major impact on the t-SNE visualizations we generated, whichis a positive result. We ultimately selected a perplexity value of 30 for all of our other t-SNE visualizationspresented in this paper, as we found it generated consistently good results.Coincidentally, the defaultPerplexity value in scikit-learns t-SNE implementation is 30, which supports our selection (skl, c).",
  "High Resolution Results": "While our evaluation of MiddleGANs ability to scale to different image sizes presented in .3 isthorough, we were eager to see if MiddleGAN could produce high quality images beyond 128px resolution.The push to go beyond 128px resolution images was prompted by our observation that the original WGAN-GP paper appeared to only test image generation up to 128px resolution (Gulrajani et al.). Given our modelsrequirement for power-of-two image sizes, the next possible image size for us to generate was 256px resolution.Our initial 256px resolution results are shown in . We believe these results, while preliminary, showa positive outcome with both the generated samples and the t-SNE visualization appearing as expected,even with the higher resolution.",
  "Use Case for the Generated Inter-Domain Images": "We present additional experiments using these images as training data for image classification tasks toevaluate their potential utility.Specifically, we applied the generated inter-domain images to a genderclassification task on the CelebA dataset. In the first experiment (Round 1), we trained a basic CNN to classify two genders (male and female) using atraining set that included inter-domain images. The model was then tested on a standard test set containingimages of both genders, achieving an accuracy of 89.51%. In the second experiment (Round 2), we expanded",
  "Accuracy89.51%94.58%": ": The classification results on the CelebA datasets testing set consisting of male and female faces,with and without the inter-domain images. With the data augmentation achieved via having the inter-domain images, we observe an increase in the classification accuracy score of 5.07%. This shows that theinter-domain images, when incorporated into the data augmentation strategy, can increase the performanceof the image classification model. the training set by adding a third categoryunsureto represent the inter-domain images that blendmale and female features. The model was tested on the same two-gender test set from Round 1, but with anadditional option: if the classifier identified an image as unsure, it could opt not to classify it strictly as maleor female. This approach led to an improved accuracy of 94.58%. These results indicate that incorporatingsynthetic inter-domain images can increase the flexibility and performance of image classification models byenabling them to better handle ambiguous cases, as observed in .",
  "Handwritten Digits Experiments": "Our evaluation of MiddleGAN for handwritten digit datasets followed a very similar process to that of theCelebA dataset. We first identified our datasets, trained MiddleGAN to generate images, and then visualizedthe results using t-SNE. We discuss our process and results below. Datasets: While our earlier experiments split a single dataset into two domains, our handwritten digitexperiments leverage two different datasets - each one serving as input ton one domain. We selected theMNIST dataset (LeCun et al., 2010), published in 2010, and the DIDA dataset (Kusetogullari et al., 2020b;a),published in 2020. The MNIST dataset contains over 70,000 black and white cropped and centered imagesof the digits 0 through 9. The DIDA dataset contains over 250,000 color images of the digits 0 through 9,sourced from Swedish historical documents from 1800 to 1940 - DIDA is claimed to be the largest historicalhandwritten digit dataset (Kusetogullari et al., 2020b;a). Unlike the CelebA experiments, in which thedomains were both relatively similar (human faces), the DIDA and MNIST datasets are visually distinct andhave different levels of complexity (the images in MNIST are at least 3 times less complex then DIDA dueto having a single channel of black and white color information). Image Generation: We trained MiddleGAN using the DIDA and MNIST datasets, specifically imagesfrom the class of 7 for both datasets. Samples of both input datasets are shown in . We utilizeda image size of 32px and a blend ratio of 0.60. Given that handwritten digits are significantly simpler thenhuman faces, we modified many of the hyperparameters shown in to better work for handwrittendigits. We selected a blend ratio of 0.60 after observing that a blend ratio of 0.50 caused MiddleGAN toonly produce MNIST-like images. By adjusting the blend ratio to 0.60 (with DIDA as domain A), we placedmore weight on the more complex dataset, which ensured a diverse quality of images were produced whenMiddleGAN was trained. This observation is important, as it means that the blend ratio can be used tocompensate for cases when two domains of dataset have radically different complexities. Feature Extraction & t-SNE: We elected to use the pre-trained GoogleLeNet model provided by PyTorchas the basis of our feature extractor (pyt, b; Szegedy et al., 2014). We opted for GoogleLeNet over ResNet101as we felt GoogleLeNet was better suited for the simpler task of handwritten digit recognition. We fine-tunedthe model to provide a binary classification for DIDA vs MNIST digits. From there, we were able to extractfeature vectors from the second-to-last layer of the GoogleLeNet model. We extracted feature vectors forthe images generated by MiddleGAN, as well as both input domains. After performing PCA, we input thefeature vectors into t-SNE in order to better understand the relationship between the input images andgenerated images.",
  ": This figure shows the generated images for the blended DIDA-MNIST dataset as well as theassociated t-SNE visualization": "the middle visually from a human perspective, the t-SNE visualization reveals that algorithmically thegenerated images fall between the DIDA and MNIST datasets. The results of this experiment emphasis theability of MiddleGAN to blend sharply different domains together in a way that will still result in middleimages when visualized with t-SNE.",
  "MiddleGAN vs. Interpolating Label Embedding for Blending Generation": "One might be prompted to ask the question: Even though the proposed method is feasible, it is possible tosimply train a GAN with two domains and interpolate the label embedding for blending generation. Whatwould be the advantage of MiddleGAN? We explain that the advantages of MiddleGAN are: MiddleGAN explicitly trains its generator to produce images that blend features from two input do-mains by employing two separate discriminators, ensuring that the generated image is adversariallyvalidated by both domain-specific discriminators. In contrast, label interpolation does not enforceinter-domain blending through adversarial training; instead, the generator is tasked with interpolat-ing the learned embeddings, which may not guarantee a successful (adversarially validated) blending,as there is no explicit supervision over the blend produced by the interpolation. MiddleGANs inter-domain images are designed to lie between the two distributions in the featurespace, as validated by t-SNE visualizations and supported by theoretical results. The architectureis specifically structured to ensure that the generated images fall within this intermediate space,facilitating a smooth transition between the domains. Label interpolation, on the other hand, maynot guarantee that the generated images occupy this in-between space in a controlled manner, asinterpolation is often heuristic and not explicitly tied to the true data distributions. While label interpolation is an appealing approach and we agree that it is possible to train a GANwith two domains and interpolate the label embeddings to generate blended images, this methodfaces a potential issue known as catastrophic forgetting Kirkpatrick et al. (2017). This occurs whenthe model, trained sequentially on two domains, tends to forget the features learned from the firstdomain as it adapts to the second. This happens because the weights learned during training on thefirst domain can be overwritten during subsequent training on the second domain. MiddleGAN, incontrast, mitigates this problem by updating the weights of both discriminators and the generatorsimultaneously during training. This concurrent update ensures that both domains are continuouslyconsidered, preventing the overwriting of domain-specific features and thus avoiding catastrophicforgetting.As a result, MiddleGAN is able to generate blended images that more reliably andcohesively capture features from both input domains.",
  "Comparision with Other Variations of the GAN as Baselines": "Traditionally, it is considered important to evaluate the performance of a new GAN variation by comparing itwith other baseline models (existing variations of the GAN). However, we argue that traditional GAN modelslike DCGAN or StarGAN are not suitable for direct comparison in this case. These models are designedto generate images within a single domain or perform domain-to-domain translation, while MiddleGAN isspecifically focused on generating inter-domain images that blend features from two distinct input sets. Since models like DCGAN and StarGAN are not designed to produce inter-domain images, comparing themwith MiddleGAN would not offer a meaningful assessment of the core contribution of our methodthe abilityto create blended images from distinct domains. Therefore, our experiments emphasize demonstrating theunique strengths of MiddleGAN in generating these images and applying them to tasks like classification,which traditional GANs are not equipped to address in the same way.",
  "Instability on Training": "GANs are indeed known for their instability during training.To mitigate this, we implemented severalstabilizing techniques, including the WGAN-GP loss function and adjustments to the training dynamics,such as increasing the discriminator-to-generator training ratio. These measures have resulted in noticeableimprovements in training stability in our experiments. However, we acknowledge that further investigationinto the stability of MiddleGAN is important. While our current approach has demonstrated stable trainingacross various tasks, including inter-domain image generation, a more thorough exploration of how themodel behaves under different hyperparameter settings or with additional discriminators would yield valuableinsights.",
  "Limitation of CelebA Dataset": "Throughout this work, we have primarily used the CelebA dataset with a split on the male attribute tocreate the male and female input domains. Although we achieved strong results using these domains,there are some inherent limitations of our approach and we caution against similar blending being blindlyapplied to future applications. Firstly, the CelebA dataset comprises images of celebrities, meaning ourMiddleGAN model learns to generate faces resembling celebrities rather than everyday people. While weexpect that MiddleGAN would perform equally well with photos of non-celebrities, it currently generatesnew celebrity-like faces.Secondly, and perhaps more importantly, MiddleGAN is learning stereotypicalmasculine and feminine traits rather than the true essence of being male or female. This could potentiallyreinforce gender stereotypes if misused in the future, despite the models ability to demonstrate the blendingcapabilities of MiddleGAN. We emphasize that MiddleGAN, like any tool, has the potential for artisticand other applications. However, it is crucial to remain mindful that certain blending combinations (e.g.,different races) may produce uncomfortable or inappropriate results.",
  "Expanding to N-Domains": "While MiddleGAN currently supports the blending of two-domains, this is not a hard limitation. In fact,MiddleGAN could be modified to support N-domains by modifying the training code to support one dis-criminator per input domain. However, while this is theoretically possible, we anticipate larger losses for,and the eventual collapse of, MiddeleGANs generator if too many different domains were included. Weanticipate this happening as the generator would be unable to create a single image that would satisfy alldiscriminators. Despite this anticipated challenge, we still believe that this area of research would be worthfuture exploration.",
  "Conclusion": "In this work, we introduced MiddleGAN, a novel variation of the traditional GAN capable of generatingimages in between two distinct input domains. MiddleGAN leverages two discriminators and one generatorto create images that appear to be in both domains, resulting in blended images. The theoretical basis forMiddleGAN was covered in detail, in additional to the empirical evaluation. We extensively evaluated thecapabilities of MiddleGAN on the CelebA dataset across three Blend Ratios (0.25, 0.50, and 0.75) and threeImage Sizes (32px, 64px, and 128px). Our evaluation with the CelebA dataset showed the blended natureof the generated images, which contained both masculine and feminine features. In our handwritten digitbased experiments, we revealed that with some adjustments of the Blend Ratio hyperparameter, MiddleGANcould handle datasets with radically different complexities (MNIST being low complexity and DIDA beinghigh complexity). We concluded our work by outlining future opportunities for MiddleGAN as well as thelimitations and caveats of MiddleGAN."
}