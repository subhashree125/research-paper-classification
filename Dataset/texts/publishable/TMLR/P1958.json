{
  "Abstract": "Recent advancements in large language models (LLMs) have significantly boosted the rise ofRole-Playing Language Agents (RPLAs), i.e., specialized AI systems designed to simulateassigned personas. By harnessing multiple advanced abilities of LLMs, including in-contextlearning, instruction following, and social intelligence, RPLAs achieve a remarkable senseof human likeness and vivid role-playing performance. RPLAs can mimic a wide rangeof personas, ranging from historical figures and fictional characters to real-life individuals.Consequently, they have catalyzed numerous AI applications, such as emotional companions,interactive video games, personalized assistants and copilots, and digital clones. In this paper,we conduct a comprehensive survey of this field, illustrating the evolution and recent progressin RPLAs integrating with cutting-edge LLM technologies. We categorize personas intothree types: 1) Demographic Persona, which leverages statistical stereotypes; 2) CharacterPersona, focused on well-established figures; and 3) Individualized Persona, customizedthrough ongoing user interactions for personalized services.We begin by presenting acomprehensive overview of current methodologies for RPLAs, followed by the details foreach persona type, covering corresponding data sourcing, agent construction, and evaluation.Afterward, we discuss the fundamental risks, existing limitations, and prospects of RPLAs.Additionally, we provide a brief review of RPLAs in AI products in the market, which reflectspractical user demands that shape and drive RPLA research. Through this survey, we aimto establish a clear taxonomy of RPLA research and applications, facilitate future researchin this critical and ever-evolving field, and pave the way for a future where humans andRPLAs coexist in harmony.",
  "Introduction": "Digital life has been a pursuit for humanity for decades, reflecting our deep-rooted fascination with theintersection of technology and human experience. Bridging this pursuit with imaginative concepts, role-playing AI systems embody the digital life by bringing these personas to life in interactive forms. Thesesystems, which simulate assigned personas, have long been a concept in the human imagination, capturingthe essence of our desire to create and interact with artificial beings that can understand, respond, and",
  "Asuka": ": An overview of various persona types for RPLAs. In this survey, we categorize personas into threetypes: 1) Demographic Persona, 2) Character Persona, and 3) Individualized Persona. We showcase theirdefinition, data sources, examples, use cases and corresponding applications. engage with us in a seemingly sentient manner. With role-playing agents, various personas can be replicatedby their agent counterparts, including historical figures, fictional characters, or individuals in our dailylives. Recently, focusing on the text modality, Role-Playing Language Agents (RPLAs) are cominginto reality (Shanahan et al., 2023; Shao et al., 2023; Wang et al., 2024d), which inspires a wide range ofnovel applications, such as digital clones for individuals (Xu et al., 2024b; Ng et al., 2024), AI characters inchatbots (Wang et al., 2024a), and role-playing video games (Wang et al., 2023a), even stimulating socialscience research (Rao et al., 2023). As RPLAs become increasingly integrated into our daily lives, it isessential to foster a society that thrives on the synergistic coexistence of humans and these intelligent agents. Recent developments in Large Language Models (LLMs) (OpenAI, 2023; Google, 2023; Anthropic, 2024)have greatly facilitated the emergence of RPLAs. LLMs grow adept at producing a compelling sense of humanlikeness (Shanahan et al., 2023; Zhou et al., 2024b), and can be regarded as superpositions of beliefs (Kovaet al., 2023) and personas (Lu et al., 2024). Furthermore, with alignment training, LLMs are able to adhereto the instruction of persona role-playing, including replicating their knowledge (Lu et al., 2024; Li et al.,2023a), linguistic and behavior patterns (Wang et al., 2024a; Zhou et al., 2023a), and even underlyingpersonalities (Shao et al., 2023; Wang et al., 2024d). They are able to both mimic the personas as promptedin the contexts (Wang et al., 2024a; Li et al., 2023a), or harness their inherent parametric knowledge forwidely-recognized demographics or characters (Shao et al., 2023; Lu et al., 2024). Considering their practicalsignificance, there has been an increase in research efforts dedicated to RPLAs with LLMs, including theirdevelopment (Wang et al., 2024a; Li et al., 2023a; Zhou et al., 2023a), analysis (Shao et al., 2023; Yuan et al.,2024b), and applications (Rao et al., 2023; Park et al., 2023; Mysore et al., 2023). Conversely, RPLAs alsobenefit the development of LLMs and language agents. They offer an ideal perspective and testing groundfor investigating the behaviors and capabilities of LLMs and language agents, particularly those related tosocial interactions (Li et al., 2023d; Chen et al., 2023a; Wu et al., 2024c). They also facilitate the creation ofdiverse and massive synthetic data for LLM training at scale (Chan et al., 2024). In this paper, we conduct a comprehensive survey on RPLAs. Our study primarily focuses on the persona andpersonalization of RPLAs. Specifically, as shown in , we categorize personas within RPLA literatureat three levels, with a progressive integration of personalized data:",
  "Published in Transactions on Machine Learning Research (10/2024)": ": Overview of RPLA applications and products based on LLMs. For personalized data, PersonalProfile refers to data about ones identity, including age, appearance, voice, and biographical information.Behavior History denotes data derived from interactions between users and applications, representing userbehavior patterns. File pertains to documents and computer files containing private knowledge regardlessof personal identity, such as code and manuals. The three types of personalized data roughly correspond toprofile, interactions, and domain knowledge in 6 respectively.",
  "The Roadmap of Large Language Models": "Recently, LLMs have demonstrated impressive capabilities, with promising potential in approaching human-level intelligence (Brown et al., 2020; OpenAI, 2022; Anil et al., 2023; Anthropic, 2023a;b; OpenAI, 2023).LLMs are artificial neural networks with billions of parameters, trained on vast amounts of natural languagedata representing human knowledge and intelligence. Their accomplishments extend beyond excelling inNLP tasks to effectively simulating a broader range of human behaviors. Specifically, they have showcasedmore nuanced capabilities towards anthropomorphic cognition, including humanity emulation (Shanahanet al., 2023; Huang et al., 2023c) and social intelligence (Kosinski, 2023; Li et al., 2023d; Kim et al., 2023b),thus producing a compelling sense of human likeness. As a result, advancements in LLMs have significantlyfacilitated the creation of intelligent RPLAs (Park et al., 2023; Sclar et al., 2023; Shao et al., 2023), establishingnew effective methodologies different from previous models. Emerged Abilities in LLMsSeveral key abilities have emerged in LLMs (Wei et al., 2022a) throughouttheir evolution, including in-context learning (Brown et al., 2020), instruction following (Ouyang et al., 2022),step-by-step reasoning (Wei et al., 2022b), and social intelligence (Wang et al., 2024b; Sclar et al., 2023;Light et al., 2023), which lay the foundation for complicated role-playing behavior of LLMs towards RPLAs.First, the in-context learning ability allows LLMs to learn information from prompts without parameterupdates. This facilitates LLMs adaptation to the provided knowledge of various characters and mimickingtheir behaviors by following example demonstrations. Second, the instruction following ability enables",
  "LLM-powered Language Agents": "The AI community has long been pursuing the concept of agent, approaching the intelligence and autonomyof humans.Traditional symbolic agents (Bernstein, 2001; Kngas et al., 2004) and reinforce-learningagents (Fachantidis et al., 2017; Florensa et al., 2018) mainly optimize their actions based on rules or pre-defined rewards. Research in language agents primarily focuses on training within constrained environmentswith limited knowledge, diverging from the complex and diverse nature of the human learning process.However, such agents struggle to emulate complicated human-like behaviors, particularly in open-domainsettings (Mnih et al., 2015; Lillicrap et al., 2015; Schulman et al., 2017; Haarnoja et al., 2017). Recently, LLMshave demonstrated remarkable capabilities with promising potential in achieving human-level intelligence,which has sparked a rise in research focusing on LLM-based language agents (Sclar et al., 2023; Chalamalasettiet al., 2023; Liu et al., 2023d; Xie et al., 2024b). Research in this area primarily involves equipping LLMswith essential human-like capabilities, such as planning, tool-usage and memory (Weng, 2023), which areessential for developing advanced RPLAs with anthropomorphic cognition and abilities. Planning ModuleIn many real-world scenarios, the agents need to make long-horizon planning to solvecomplex tasks (Rana et al., 2023; Yuan et al., 2023). When facing these tasks, LLM-powered agents coulddecompose the complex tasks into subtasks and adopt various planning strategies, e.g., CoT (Wei et al., 2022b)and ReAct (Yao et al., 2023b), to adaptively plan for the next action with feedback from environments (Wanget al., 2023a; Gotts et al., 2003; Wang et al., 2023i; Song et al., 2023; Zhang et al., 2024b). For RPLAs,these adaptive planning strategies enable them to simulate realistic and dynamic interactions in complicatedenvironments such as games (Wang et al., 2023a) and social simulations (Park et al., 2023). Tool-usage ModuleAlthough LLMs excel in various tasks, they may struggle in domains requiringextensive expertise and experience hallucination issues (Gou et al., 2023; Chen et al., 2023e; Wang et al.,2023f). To address these challenges, agents could apply external tools for action execution (Shen et al., 2023b;Lu et al., 2023; Schick et al., 2023; Parisi et al., 2022; Yang et al., 2023b; Yuan et al., 2024a). The tools include",
  "RPLA Definition": "Our survey distinguishes personas into three categories, progressing from broad groups to individual specificity:demographic persona, character persona, and individualized persona, defined as follows: 1. Demographic Persona represents the aggregated characteristics and behaviors of distinct demo-graphic segments, including occupations, genders, ethnicity, and personality types. In the context ofRPLAs, these personas operate as fictional archetypes, derived from the comprehensive pre-trainingdatasets of LLMs. Employing these archetypes, the development of RPLAs can be efficiently fa-cilitated through simple prompts, such as You are a mathematician. Constructed in this way,demographic RPLAs can be effectively employed for simulations specific to demographic groups andfor addressing specialized tasks. 2. Character Persona denotes well-established characters, encompassing both real-world public figuresand fictional entities, each characterized by definitive attributes and narratives. The RPLAs for thesecharacters are constructed using data derived from diverse sources such as biographies, novels, andfilms. Primarily, these RPLAs are designed to fulfill entertainment and emotional engagement needs,functioning as AI-driven chatbots or virtual characters in video games. 3. Individualized Persona refers to personal profiles constructed from the behavioral and preferencedata of specific individuals, encompassing personal profiles, dialogues, and a range of actions andbehaviors. This data is subject to continuous evolution, necessitating that the corresponding RPLAs",
  "Parametric Training": "(Continual) Pre-trainingObjective: Knowledge injection.Data: Raw data (books, encyclopedia, etc.).Advantages: Readily available for well-established demographics andcharacters; Directly uses the raw data.Disadvantages: Necessitates training for new personas; May causecatastrophic forgetting. Supervised Fine-TuningObjective: Refining role-playing capabilities; Knowledge injection.Data: Conversation data.Advantages: Highly effective.Disadvantages: Necessitates data processing and training for newpersonas; Potential information loss during data processing. Reinforcement LearningObjective: Alignment with general or individual users; Improvingsocial reasoning skills.Data: Conversation data; Preference data;Advantages: Effective for mitigating harmful content.Disadvantages: Cost of human preference data; Sparse rewards inmulti-agent games.",
  "Nonparametric Prompting": "In-context LearningObjective: Knowledge injection; Alignment with individual users.Data: Raw data; Conversation data.Advantages: Highly effective; Training-free; Convenient for new per-sonas and personalization; Can incorporate retrieval mechanism forenhanced efficiency.Disadvantages: May require data processing for new personas; Con-sumes more tokens and is restricted by context length. adapt dynamically to these changes. Individualized RPLAs provide customized services tailored tothe needs of individual users across various AI-based applications, where they commonly function aspersonalized assistants, companions, or proxies.",
  "RPLA Construction": "Role-Playing Language Agents (RPLAs) are primarily developed to simulate intricate personas based onvarious individual profiles and narratives. These profiles are constructed using diverse persona data, includingdescriptive narratives, dialogues, historical behaviors, and extensive textual materials such as books (Zhanget al., 2018; Dinan et al., 2020; Shanahan et al., 2023; Wang et al., 2024a; Shao et al., 2023; Xu et al., 2023a;Li et al., 2023f). The methodologies for building RPLAs typically involve either parametric training (Shao et al., 2023; Wanget al., 2024a; Qin et al., 2024) or nonparametric prompting (Dalvi Mishra et al., 2022; Li et al., 2023a; Zhouet al., 2023a; Gupta et al., 2023; Ma et al., 2023a; Zhao et al., 2023b), as summarized in . Thesemethods may concurrently contribute to the development process. Parametric training for RPLAs primarily includes pre-training, supervised fine-tuning (SFT) and reinforcementlearning (RL). Initially, LLMs for RPLAs are pre-trained on large-scale raw text, including literary worksand encyclopedic entries (Xu et al., 2023a; Gupta et al., 2023), equipping them with a broad knowledgeof massive demographic and character personas. Subsequently, these LLMs undergo SFT on role-playingdatasets (Wang et al., 2024a; Shao et al., 2023), which enhance both their role-playing capabilities andcharacter-specific knowledge. Additionally, RL methods could further refine RPLAs in terms of: 1) Alignmentwith general users, e.g., improving attractiveness or mitigating harmful content, with preference data fromonline application users and invited human annotators (RLHF) or synthesized by LLM (Bai et al., 2022b;Ouyang et al., 2022). 2) Improving social reasoning skills, e.g., in games (Cheng et al., 2024) or goal-driven",
  "conversations (Wang et al., 2024b). 3) Alignment with individual users (Shaikh et al., 2024a; Jang et al.,2023)": "Conversely, nonparametric prompting provides RPLAs with persona data and role-playing instructionswithin the context. Prompts for RPLAs primarily consist of persona data that represents the intended personas,including descriptions and demonstrations. Descriptions (or profiles) represent their basic informationsuch as names, backgrounds, experiences, personalities, tones, catchphrases and other attributes (Wang et al.,2024a; Yuan et al., 2024b). Demonstrations illustrate representative behaviors to further align RPLAs with theintended personas, covering dialogues, behaviors, interactions, preferences, stories or other modalities (Li et al.,2023a; Chen et al., 2023c; Dai et al., 2024a). There are several methods for crafting persona data, including:1) Online Resource Collection, which gathers information from online resources such as Wikipedia,Supersummary and Fandom for widely-known characters (Shao et al., 2023). 2) Automatic Extraction,where LLMs automatically extract persona data such as dialogues from their origins, e.g., books or scripts (Liet al., 2023a). 3) Dialogue Synthesis, which employs advanced LLMs to create and expand role-playingconversation datasets via in-context learning (Li et al., 2023a) or role-playing as the personas (Ran et al.,2024). If provided with corresponding literature for reference (for character personas), this is akin to automaticextraction and the synthesized dialogues are more faithful to the origins. Otherwise, the synthesized data isof limited quality and often necessitates filtering. 4) Human Annotation, which engages human annotatorsor character fans to summarize persona descriptions or craft high-quality role-playing conversations (Zhouet al., 2023a). Besides, role-playing instructions or requirements could be incorporated to encourage orrestrict specific behaviors of RPLAs.Furthermore, modern RPLAs increasingly integrate memory modulesto retrieve information from extensive datasets on character traits or past interactions. This developmentaddresses the restricted context capacity of current LLMs (Shao et al., 2023; Mysore et al., 2023; Sun et al.,2024). In terms of alignment with persona types, parametric learning tends to focus on demographic informationand well-known characters, whereas prompting techniques are generally employed for generating fictionalpersonas and highly personalized characters. Current research in RPLA development generally focuses onsteering LLMs with demographics (Zhang et al., 2023b; Hong et al., 2023), developing foundation models (Luet al., 2024; Zhou et al., 2023a), designing agent frameworks (Li et al., 2023a; Wang et al., 2024a) for RPLAs,and crafting persona profiles for specified individuals (Li et al., 2023a; Wang et al., 2024a; Ahn et al., 2023).",
  "RPLA Evaluation": "For RPLA evaluation, we distinguish the criteria into two primary categories: role-playing capabilityevaluation for RPLA methodologies, and persona fidelity evaluation for specific personas (Wang et al.,2024d). The role-playing capabilities of RPLAs are evaluated on their foundation models and constructionframeworks, regardless of specific personas. These evaluations concern aspects such as anthropomorphicabilities, attractiveness, and usefulness, which encompass more granular dimensions including conversationability (Shao et al., 2023), engagement (Zhou et al., 2023a) 1, persona consistency (Wang et al., 2024d),emotion understanding (Huang et al., 2023a), theory of mind (Kosinski, 2023), and problem-solving ability (Xuet al., 2023a). Persona fidelity, by contrast, concentrates on whether individual RPLAs well replicate theintended personas, including their knowledge (Shao et al., 2023; Li et al., 2023a), linguistic habits (Wanget al., 2024a; Deshpande et al., 2023), personality (Wang et al., 2024d; Huang et al., 2023c), beliefs (Li et al.,2024a; Wang et al., 2023e), and decision-making (Xu et al., 2024c; Chen et al., 2023a). Current methodsfor evaluation are mainly three-fold: 1) automatic evaluation with ground truth, 2) automatic evaluationwithout ground truth, 3) multi-choice questions, 4) human-based evaluation. Overall, current RPLAs havedemonstrated promising and improving performance in simulating personas. However, there remain significantgaps between existing RPLAs and fully human-level intelligent agents (Wang et al., 2024a; Chen et al., 2023a)that are faithful to the personas, as well as corresponding evaluation methods for more nuanced role-playing.",
  "Definition": "Personalization tailors LLMs to meet the unique needs, experiences, and preferences of individuals, whichhave been increasingly important in modern AI applications (Salemi et al., 2024). Research in this area aimsat providing personalized services, adapting to the preferences of individual users or even mirroring theirbehaviors (Chen et al., 2023b). When such a personalized system attempts to encapsulate these aspects, itessentially engages in role-playing, emulating an individual. This process shapes individualized personafor RPLAs (Salemi et al., 2024), typically embodying digital clones or personal assistants for individuals. In this paper, we categorize the applications of personalized RPLAs into three tiers, ranging from conversa-tion (Gao et al., 2023b; Ahn et al., 2023) and recommendation (Chen et al., 2023b; Yang et al., 2023a), toautonomous agents for more complicated task solving (Li et al., 2024d). 1. Conversations: Early research for personalized RPLAs primarily focuses on personalized conversa-tions by learning and incorporating the user persona (Cho et al., 2022; Zhou et al., 2023c; Ng et al.,2024), aligning stylistic features with user preferences to boost engagement (Zheng et al., 2021; Wanget al.). With the emergence and evolution of LLMs, personalized RPLAs become capable of handlingincreasingly complex and comprehensive tasks, gaining competence in complicated task-planning andtool-learning for auto-completing personalized services.",
  "Analysis of Demographics": "RPLAs possess inherent demographics that reflect nuanced human-like characteristics, including personalitytraits, political beliefs, and ethical considerations, which vary in different LLMs. Furthermore, RPLAshave the ability to role-play specified demographics, altering their behavior and potentially enhancing theirperformance on specific tasks, but this may also lead to toxic outputs and biases, depending on the personaassigned. Inherent DemographicsRPLAs may inherently reflect specific demographic characteristics due topatterns present in the data used during pretraining. These patterns encapsulate human tendencies andbiases originating from diverse sources (Karra et al., 2022; Serapio-Garca et al., 2023; Gupta et al., 2024).Subsequently, RPLAs could encode individual behavioral traits in textual outputs, inadvertently resulting ina disproportionate emphasis on certain demographics over others (Jiang et al., 2023a). To harness RPLAs for specific applications effectively, it is essential to understand their inherent demograph-ics. The demographic characteristics of RPLAs can be explored through established human psychologicalassessments such as the Big Five Personality Test (Barrick & Mount, 1991). By subjecting RPLAs totext-based questionnaires designed for humans, researchers could leverage their textual response capabilitiesto evaluate behavioral responses similar to human subjects (Huang et al., 2024a). Such evaluations haverevealed that RPLAs exhibit consistent inherent demographics, which have been statistically confirmedin recent studies (Jiang et al., 2023a; Serapio-Garca et al., 2023; Santurkar et al., 2023). However, it isimportant to recognize that these demographics may differ in different LLMs (Huang et al., 2023b). Beyond personality characteristics, RPLAs often display complex demographics reflecting nuanced social,economic, and ethical understanding. For instance, RPLAs may exhibit a preference for certain politicalbeliefs (Hartmann et al., 2023), show decision-making patterns indicative of rational economic consider-ations (Guo et al., 2023), and act either selfishly or helpfully in multi-agent simulations (Chawla et al.,2023). Demographic Role-PlayingRPLAs are embedded with intrinsic demographic characteristics, whichraises pivotal questions about their ability to role-play specified demographics and the subsequent effects ontheir behavior. A prevalent approach in demographic role-play involves directly prompting the language agent.For example, if an LLM is prompted with, Youre a friendly and outgoing individual who thrives on socialinteractions. Always ready for a good time, you enjoy being the center of attention at parties... (Jiang et al.,2023a; Xie et al., 2024a), it adopts the persona of an extroverted character. When tasked with representingdistinct demographics, RPLAs demonstrate the capacity to diverge from their inherent traits, manifestingchanges in their responses on psychological assessment scales (Jiang et al., 2023a; Serapio-Garca et al., 2023).",
  "This behavioral adaptability highlights the potential of RPLAs in simulating diverse human-like roles andpersonalities": "However, not all assigned personas lead to superior performance of RPLAs. Assigning a persona to LLMsmay also result in toxic or biased outputs compared to the default setting, because the persona may amplifyexisting stereotypes and biases present in the training data. For example, the assignment of some personas tolanguage agents, including baseline personas such as a bad person, has been demonstrated to significantlyincrease the likelihood of RPLAs generating toxic outputs (Deshpande et al., 2023). Similarly, diversedemographic roles have been assigned to reveal the biased presumptions present in LLMs (Gupta et al.,2023). Although some developers have made attempts to prevent RPLAs from malicious usage, attacking theprompts via jailbreaking (Chao et al., 2023; Anil et al.) might bypass these safety mechanisms and elicitoffensive, toxic, misleading contents.",
  "Application of Demographics": "By assigning specific demographics, LLMs often have better performance in various types of downstreamtasks, whether agents are used in a standalone fashion (single-agent systems) or joint with other agents(multi-agent systems) for competition or collaboration. Improving Task Solving in Single-Agent SystemsAssigning specific demographics enables LLMs toenhance their performance in tasks that require specialized knowledge tied to those personas. For instance,when an LLM is configured to represent an expert LLM within a specific field, it might significantly augmentthe length, depth, and quality of its responses, which is also showcased in complex zero-shot reasoning tasks,where the model must generate insightful answers without prior direct training on similar problems (Xu et al.,2023a; Kong et al., 2023). Furthermore, integrating diverse social roles into LLMs frameworks has beenshown to positively influence their performance across a wide array of tasks, suggesting a versatile adaptabilityto different contextual demands (Zheng et al., 2023a). The application of these roles enables LLMs notonly to generate more contextually appropriate responses but also to exhibit increased understanding andengagement in interactions that reflect varied human experiences and societal norms. Improving Task Solving in Multi-Agent SystemsBuilding upon the capability of single-agent models,which utilize demographic personas to bolster their specialized abilities, assigning demographic personas inmulti-agent systems has also emerged as a crucial strategy for enhancing the performance of single-agentsystems, i.e., standalone LLMs. By embedding various personas within agents, distinct societal dynamics couldbe cultivated, leading to improved strategies for cooperative problem-solving and breakthroughs in complexdomains such as mathematical modeling (Zhang et al., 2023b; Wang et al., 2023g). A notable implementationof this approach is ChatDev (Qian et al., 2023) and MetaGPT (Hong et al., 2023), frameworks designedspecifically for automating software development within a multi-agent conversational platform. In this setup,different agents are assigned specialized roles that collectively contribute to the agile development of softwareapplications. This collaborative model echoes the strategies applied in projects such as OKR-AGENT (Zhenget al., 2023b), where role-specific enhancements within multi-agent architectures have shown to significantlystreamline and optimize task execution. Simulating Collective Social Behaviors in Multi-Agent SystemsRPLAs have demonstrated remark-able capabilities in simulating nuanced, human-like interactions across various environments. In the realmof gaming, particularly in strategy and role-playing scenarios, RPLAs have shown impressive performance.For example, Chawla et al. (2023) set the agents to be fair or selfish, and shows that selfish agents couldcontribute not only to their own interests but also to the collective good. Additionally, more elaborate gameslike Social Deduction Games are particularly illustrative of RPLAs capacity to effectively adopt variedroles, as observed in scenarios such as The Werewolf (Xu et al., 2023c) and The Avalon (Wang et al.,2023d). In diplomacy-focused games such as Cicero, RPLAs have matched or even surpassed human levels ofperformance (FAIR et al., 2022). Similarly, in war simulation games, RPLAs provide valuable insights intothe origins of conflicts, enhancing our understanding of complex geopolitical dynamics (Hua et al., 2023).Extending the application of RPLAs beyond gaming environments, these models are also utilized to mimicdaily social interactions, thereby narrowing the behavioral gap between artificial agents and humans. This is",
  "Data for Character RPLAs": "Character data is indispensable for the construction of character RPLAs. The data that represents knowledgeof these well-established characters can be roughly categorized into two types: 1) Descriptions directlydescribe the character personas that guide the behaviors of RPLAs. These include various character attributes,such as identity, relationships, and other predetermined attributes. The attributes serve as the knowledgebackground and are expected to be accurately recalled upon request, such as names and affiliations (Li et al.,2023a; Zhou et al., 2023a; Shao et al., 2023; Wang et al., 2024a; Chen et al., 2023c; Zhao et al., 2023a; Tuet al., 2024; Lu et al., 2024). Additionally, some descriptions further shape the behaviors of RPLAs, suchas personality traits (Li et al., 2023a; Wang et al., 2024d). 2) Demonstrations, on the other hand, arerepresentative behaviors of the characters, which reflect their linguistic, cognitive and behavioral patterns(Li et al., 2023a; Zhou et al., 2023a; Shao et al., 2023; Wang et al., 2024a; Zhao et al., 2023a; Chen et al.,2023c; Tu et al., 2024; Tang et al., 2024; Lu et al., 2024; Xu et al., 2024b). While RPLAs are not expected toreplicate the exact outputs from the demonstration data, they should portray these patterns and generalizeto new situations, i.e., producing responses consistent with the demonstrations. Overall, descriptions providethe core and foundational information for RPLAs, while demonstrations, though not mandatory, are alsocrucial for achieving vividness and fidelity of RPLAs (Wang et al., 2024d).",
  "PersonaHub (Dai et al., 2024a)1,000,000 1,000,000EN-Dialogue Synthesis": "The available data for character RPLAs is currently quite limited, covering only a small selection of characters.The description data are typically sourced from well-curated encyclopedias or the original works, and processedmanually or with advanced LLMs (Shao et al., 2023; Li et al., 2023a). The demonstration data are crafted invarious ways, where the common methodologies include:",
  "Construction of Character RPLAs": "By integrating character data into LLMs, character RPLAs are developed (Han et al., 2022; Li et al., 2023a;Park et al., 2023; Chen et al., 2023c; Wang et al., 2024a; Zhao et al., 2023a; Tu et al., 2024). As discussed in2, LLMs have demonstrated remarkable capabilities to follow human instructions and generate high-qualitytext. Together with their ability of character understanding, LLMs can hence be instructed to role-playspecific characters provided with their data, thus forming character RPLAs. The construction methodologiesare distinguished into two categories, i.e., parametric training and nonparametric prompting. Parametric TrainingThis method includes pre-training and supervised fine-tuning. In pre-training, LLMslearn from large-scale web corpus which includes vast amounts of literary works and encyclopedia entries.This provides LLMs with knowledge of a wide range of established characters, such as Hermione Granger andSocrates, enabling LLMs to readily role-play these characters. Supervised fine-tuning for RPLAs is be adoptedto tailor LLMs to role-play specific characters (Shao et al., 2023; Yu et al., 2024), or to develop foundationmodels with refined role-playing capabilities utilizing datasets of diversified characters and scenarios (Li et al.,2023a; Wang et al., 2024a). Nonparametric PromptingThis method directly provides LLMs with character data in the context,leveraging the in-context learning capability of advanced LLMs.This serves as a simple yet effectivemethodology for RPLA construction, and is hence widely adopted by recent RPLAs (Wang et al., 2024a;Zhou et al., 2023a). However, character data is often voluminous, and interaction data between RPLAs andusers is also continuously produced during the interaction process. This makes it impractical to include alldata for a character RPLA within the context limits of LLMs. Consequently, long-term memory modulesare being increasingly incorporated into RPLA frameworks to manage the vast amount of character RPLA",
  "Evaluation of Character RPLAs": "The evaluation of character RPLAs encompasses various dimensions, considering the complexity and compre-hensiveness of character personas. Basically, these dimensions are distinguished into character-independentcapabilities of foundation models, and character fidelity of RPLAs for specific characters. Character-independent CapabilitiesThis line of work assesses how well a foundation model is capableof the role-playing task, regardless of the characters it role-plays. According to different levels of interactioncapabilities, we have considered basic role-playing abilities and conversational skills, progressing to morein-depth anthropomorphic capabilities matched with humans. These have been categorized into the followingthree levels: 1. Role-playing Engagement: Basically, the LLMs should actively participate in the role-playingscenario. They should produce responses in dialogue format and exhibit deep immersion, avoidingout-of-character utterance such as As an AI model). Additionally, the RPLAs are expected toexhibit stable and consistent personalities across different turns (Shao et al., 2023), sessions (Wanget al., 2024d) and even language (Huang et al., 2023b). 2. High-quality Conversations: RPLAs built on the LLMs should talk in a fluent natural way. Re-search in this area focuses on evaluating the completeness (Zhou et al., 2023a), informativeness (Zhouet al., 2023a), and fluency (Tu et al., 2024) of conversations. Besides, RPLAs are expected to meetthe ethical standards (Zhou et al., 2023a) and avoid harmful content when role-playing viciouscharacters (Deshpande et al., 2023). 3. Anthropomorphic Capabilities: RPLAs are expected to acquire cognitive, emotional and socialintelligence towards human levels. Relevant dimensions include conversation attractiveness (Zhouet al., 2023a; Tu et al., 2024), theory of mind (Kosinski, 2023; Mao et al., 2023), empathy (Sorinet al., 2023), emotional intelligence (Huang et al., 2023a), and goal-driven social skills (Zhou et al.,2024b; Wang et al., 2024b). These capabilities are practically important for RPLAs to effectivelyserve as emotional companions for humans. Character FidelityThis line of work evaluates how a specific RPLA reproduces the intended character,which depends on both the foundation model, the agent framework, and the character data. Relevantdimensions are categorized into four categories: linguistic style and knowledge, which are considered superficial,as well as personality and thought, which represent deeper, underlying aspects: 1. Linguistic Style: Basically, RPLAs should speak in a tone that emulates the linguistic style of theintended characters (Wang et al., 2024a; Li et al., 2023a; Zhou et al., 2023a; Yu et al., 2024). For thispurpose, RPLAs are typically provided with demonstrative character dialogues (Wang et al., 2024a;Li et al., 2023a), and they could mimic the tone leveraging the in-context learning ability of LLMs. 2. Knowledge: RPLAs are essentially required to simulate the characters breadth of knowledge. Onone hand, they should accurately recall knowledge of the character, including their identity (Zhouet al., 2023a; Wang et al., 2024a; Tang et al., 2024; Lu et al., 2024), social relationships (Chenet al., 2023c; Shen et al., 2023a; Zhao et al., 2023a), and experiences (Shao et al., 2023; Wang et al.,2024a; Chen et al., 2023c; Yu et al., 2024). On the other hand, they may be required to refrain fromdemonstrating knowledge or ability beyond the characters scope (e.g., an LLM could write code evenif it is role-playing Socrates, which is unnecessarily expected) (Shao et al., 2023; Lu et al., 2024; Yuet al., 2024). This phenomenon is referred to as character hallucination (Shao et al., 2023), whichoriginates from the extensive knowledge possessed by LLMs and could be reduced via SFT (Shaoet al., 2023).",
  "To evaluate RPLAs on the aforementioned dimensions, existing methodologies could be distinguished intofour categories:": "1. Automatic Evaluation with Ground Truth: Typically, datasets with ground truth are expectedfor evaluating character fidelity in terms of knowledge, personality and thought. While early similaritymetrics such as Rouge-L (Lin, 2004) could be applied to compare RPLA responses with groundtruth (Wang et al., 2024a), recent studies increasingly leverage state-of-the-art LLMs such as GPT-4as evaluators. On one hand, evaluator LLMs can score RPLA responses based on certain criteria, ordetermine the superior response from two models for win rate calculation, provided with groundtruth responses as references. However, the ground truth are typically synthesized by advancedLLMs (Wang et al., 2024a) . On the other hand, evaluator LLMs can be used to classify RPLAresponses, and the results are then compared with ground truth labels (Wang et al., 2024d). 2. Automatic Evaluation without Ground Truth: As collecting ground truth data for RPLAevaluation is often challenging, several studies such as CharacterEval explore using LLMs to evaluateRPLA responses without ground truth (Shao et al., 2023; Tu et al., 2024). Instead, characterprofiles should be provided. This category is effective for evaluating character-independent abilitiesand linguistic styles, which require little knowledge about the characters. However, when it comesto characters knowledge and thoughts, LLMs might not possess the necessary depth of relevantknowledge, especially for unfamiliar characters. This concern potentially leads to inadequatelyinformed judgments of LLMs, and hence produces suboptimal evaluation results.Even whenequipped with extensive knowledge, current LLMs still exhibit deficiencies in discerning nuancedaspects of role-playing. 3. Multi-choice Questions: Multi-choice questions also come with ground truth, yet they differ fromautomatic evaluation with ground truth in that they merely require RPLAs to select from a fixedset of options, rather than generating open-ended responses. This significantly reduces the outputspace for RPLAs, making the evaluation simpler. This method is particularly suitable for evaluatingthe fidelity of characters thoughts, e.g., behavior prediction (Xu et al., 2024c; Chen et al., 2024)and motivation generation (Yuan et al., 2024b; Shen et al., 2023a). For these tasks, it is impracticalto require RPLAs to produce responses exactly matching the ground truth, and responses may bereasonable even if they deviate from the ground truth significantly. 4. Human Evaluation: Inviting human annotators to assess the performance of RPLAs is a viableand effective approach (Zhou et al., 2023a). However, it comes with several drawbacks, such as costin terms of time and money, as well as lack of reproducibility. This method is akin to automaticevaluation without ground truth, yet employs humans as more precise evaluators. Hence, it similarlyfalls short in evaluations that require in-depth knowledge about the characters, as recruiting qualifiedannotators who are well-acquainted with these characters can be difficult. Combining automaticevaluation with human evaluation, some efforts also fine-tune the evaluator LLMs with humanannotations (Tu et al., 2024). Previous efforts primarily focus on RPLA evaluation on widely-known established characters (Wang et al.,2024d; Ahn et al., 2024). However, it is challenging to obtain high-quality datasets and achieve precise and",
  ". Recommendation: Conversational recommendation systems (Chen et al., 2023b; Yang et al., 2023a;": "Wu et al., 2023) based on LLMs have been widely regarded as the next generation of recommendationsystems (Lian et al., 2024), support users in achieving recommendation-related goals through multi-turn dialogues (Jannach et al., 2021). Compared with traditional recommendations, these methodsstand out with their solid foundation models, natural language interactions, and straightforward,typically nonparametric evolution (Sallam, 2023; Abbasian et al., 2023). 3. Task Solving: Furthermore, personalized RPLAs become increasingly competent in more complicatedtask solving (Yao et al., 2023a; Significant-Gravitas, 2023), such as coding (Microsoft, 2024), travelplanning (Xie et al., 2024b), and research survey (Wang et al., 2024c), typically interacting withvarious external software. They are autonomous LLM-based agents that are deeply integrated withpersonal data, devices, and services (Dong et al., 2023; Li et al., 2024d). They have significantlyadvanced personal assistants beyond early predecessors such as Siri (Apple Inc., 2024) which strugglewith complex user requests. To build personalized RPLAs that accurately capture and portray the individualized personas, the processtypically consists of two crucial steps: 1) Persona data collection, which gathers the necessary data toshape the individualized personas, and 2) Persona modeling, which creates models that represent theseindividual personas using the collected data. For persona data collection, the data can vary greatly informat, content, and modalities across different applications and tasks. We categorize this data into threetypes: profile, interactions, and domain knowledge, which will be detailed in 6.2. For persona modeling, thechallenge is to embody the intended persona from the unprocessed persona data, which are generally massive,sparse and noisy, as will be discussed in 6.3. The evaluation of personalized RPLAs depends on specificapplications, and will be discussed in 6.4. Despite the advancement with LLMs, personalized RPLAs still face several challenges, including processinglong inputs and vast search space (Chen et al., 2023b; Abbasian et al., 2023), utilizing sparsity, lengthy, andnoisy user interactions data (Zhou et al., 2024c), learning domain-specific knowledge for understanding userprofiles (Zhang et al., 2023c), understanding multi-modal contexts (Dong et al., 2023), ensuring privacyand ethical standards (Benary et al., 2023; Eapen & Adhithyan), and optimizing response time for seamlessintegration into real-time applications.",
  "Data Collection of Individualized Persona": "The individualized personas for personalized RPLAs are typically represented with three distinct types ofdata, including profile, interactions, and domain knowledge, depending on the specific applications.There have been numerous datasets with individualized personas, as outlined in , covering variouslanguages including English (Ahn et al., 2023), Chinese (Baidu, 2020), Japanese (Yamashita et al., 2023),and Korean (Cho et al., 2023). ProfilesProfiles are fundamental information that explicitly describes individualized personas, whichare typically well-structured. Typically, they are initially set by users, and can be continuously updated.The basic elements usually include the names, gender and ethnicity of individual users in text (Santurkaret al., 2023) Besides, profiles commonly contain natural language descriptions of individuals, describingtheir characteristics, such as identity, hobbies, experiences and other statements (Zhang et al., 2018; Dinanet al., 2020; Gao et al., 2023b; Li et al., 2021; Ng et al., 2024), varying based on the detailed applications.Additionally, Lee et al. (2024) propose a multi-modality persona that includes elements such as the usersappearance. For example, in live streaming applications, persona data can be composed of both basic profileinformation such as an individuals age, gender, and location and domain-specific details, namelystreamer characteristics such as fan numbers and streaming style Gao et al. (2023b). Additionally, profilescan contain multi-modal information. For instance, profiles in (Ahn et al., 2023) incorporate text-imagepairs, which are individuals comments for pictures on social media. InteractionsThe interaction data capture the dynamic evolution of individualized persona. Interactionsare data generated during the use of applications that implicitly portray individualized personas, such asconversations, user preferences, and other behaviors. For example, PERSONA-CHAT (Zhang et al., 2018)and ConvAI (Dinan et al., 2020) collect two-person dialogues through crowd-sourcing, while LiveChat (Gaoet al., 2023b) and MPCHAT (Ahn et al., 2023) collect multiplayer conversations from Internet sources suchas live streaming and Reddit. To reduce construction costs, Jandaghi et al. (2023) uses a Generator-Criticarchitecture with LLMs for dialogue synthesis, and Zeng et al. (2024) first conducts automatic annotationto generate conversational data from raw sources like experiences, speeches, or writings. In addition todialogues in natural language, Agrawal et al. (2023) and Santurkar et al. (2023) introduce comic pictures andmultiple-choice questions as interactions. This kind of data could be consistently collected and systematically",
  "organized in real-world applications, offering benefits such as convenient acquisition and dynamic evolution.Hence, it plays an important role in practical applications": "Domain KnowledgeIncorporating domain-specific knowledge into general language models aids in thebetter understanding of user profiles and interactions within specific domains. This is crucial for accuratelyunderstanding user needs and ensuring the consistency of the persona in role-playing (Wang et al., 2023b).For example, incorporating a knowledge base like Wikipedia helps to provide detailed backgrounds of namedentities in dialogues as a part of the whole persona (Jang et al., 2022; Wang et al., 2023b; Baidu, 2020), whichpromotes LLMs to better understand user personas with enriched background knowledge of relevant entities.",
  "Modeling Individualized Persona": "Existing methodologies for modeling individualized persona can be roughly categorized into two types: offlinelearning and online learning. In offline learning, the learning process is conducted on the comprehensivedataset at regular intervals, which is also referred to as batch learning. In online learning, learning happensin real-time as new data becomes available. Offline LearningThis method tailors the outputs of LLMs to reflect specific personas represented inpre-existing datasets. Parameter fine-tuning emerges as the mainstream approach for offline learning, typicallybased on SFT and RLHF (Mondal et al., 2024; Zheng et al., 2023c; Li et al., 2024b; Jang et al., 2023).For example, Mondal et al. (2024) proposes a two-stage approach for personalizing LLMs with profile andinteraction datasets. In addition, some recent studies propose techniques with nonparametric learning forLLMs personalization. For instance, Shea & Yu (2023) introduces an offline RL framework with a personaconsistency critic and variance reduction, while Weng et al. (2024) integrates embedding control vectorswithin the models activation states, allowing dynamic output adjustment for diverse personality traits. Thesemethods exhibit several deficiencies: 1) they face a fundamental trade-off between accuracy and efficiency;2) they are heavily reliant on the quality of datasets; 3) more crucially, they struggle to adapt to dynamicchanges in persona data, limiting their real-world applicability. Online LearningIn online learning, the personas are dynamic and continuously evolving, i.e., regularlyupdated with incoming data, the user interactions in real-world applications. This enables personalizedRPLAs to quickly adapt and stay relevant to user needs and preferences. With LLMs, effective personalearning is typically nonparametric and training-free, which only involves effective management of memoryand context (Dalvi Mishra et al., 2022; Kim et al., 2024; Baek et al., 2023; Zhou et al., 2024c). For thisdemand, retrieval modules become indispensable, especially for LLMs with limited context window (Mysoreet al., 2023; Sun et al., 2024). Moreover, methodologies for effective online learning methods consider notonly natural language interactions, but also non-linguistic feedback from users (Ma et al., 2023a). Besides nonparametric methods, fine-tuning with online interactive data is also widely applied to onlinepersona learning, including both SFT with mini-batches from on-the-fly user stream data (Qin et al., 2024)and RLHF with real-time user feedback (Ding et al., 2023b; Bai et al., 2022a). Additionally, Shaikh et al.(2024b) use fewer than 10 demonstrations to align language model outputs with a users demonstratedbehaviors through iterative DPO (Rafailov et al., 2024) training. Nevertheless, significant challenges arisein accurately recognizing and learning the sparse persona-specific features from the noisy interaction data.Besides, the personas of real users may change over time, which poses further challenges for their effectivemodeling and updating.Therefore, for nonparametric methods, the effectiveness heavily relies on themechanisms of memory management and retrieval.",
  "Evaluation for LLMs and Individualized Persona": "For effective personalization, AI models should focus on two key aspects: understanding and utilizingpersonas. Specifically, they should be able to identify unique user personas and predict their future preferences,actions, and thoughts, which serves as the preliminary to provide personalized responses that embody theindividualized personas, in various environments that are increasingly comprehensive and complex. Here, weintroduce the evaluation methodologies for personalized RPLAs across the three application tiers, namely:",
  "Bias": "Bias Manifestation in Role-Playing ScenariosBias can manifest in both implicit and explicit forms.Implicit bias refers to the RLPAs internal attitudes or stereotypes within RPLAs that influence under-standing, actions, and decisions. Explicit bias, on the other hand, involves conscious beliefs and attitudesthat align with the presented context or assigned roles. LLMs, despite being designed to avoid outputtingstereotypes directly due to safety policies such as RLHF (Ouyang et al., 2022), may still exhibit biases,particularly under RPLA conditions: 1) Reasoning Bias: This issue is compounded in scenarios whereLLMs are assigned specific personas, leading to implicit biases that could affect their reasoning capabilities(e.g., arithmetic problems), especially in contexts involving race, gender, religion, or occupation (Zheng et al.,2023a; Kotek et al., 2023; Cheng et al., 2023a; Naous et al., 2024). 2) Political Bias: For RPLAs, LLMsare expected to maintain neutrality and avoid political positions or biases. Yet, studies have demonstrateda political inclination of RPLAs towards pro-environmental, left-libertarian views (Rutinowski et al., 2023;Hartmann et al., 2023). 3) Role-based Bias: In the context of role-playing, role-based bias is one of theexplicit bias. Striking the right balance between maintaining the authenticity of a character and managingbias is a critical challenge. For example, if an RPLA is created with a persona like Hitler, agent willundoubtedly express some bias to Jews due to its role, which violates the ethical standard. Causes of Bias in RPLAsThese biases are thought to originate from both the models pre-training dataand user interactions (Xue et al., 2023). Specifically, imbalances in training data significantly contribute tothese biases, as the predominance of certain biases within the data could lead to their incorporation into theparametric memory of LLMs. Furthermore, Perez & Ribeiro (2022) and Branch et al. (2022) highlight thatLLMs are sensitive to the user prompts, which could inadvertently steer them towards biased outputs. Thisproblem gets worse when the models are influenced by the users negative emotions (Coda-Forno et al., 2023). Strategies for Mitigating BiasAddressing biases in RPLAs requires a multi-faceted approach: 1) DataPreparation Phase: Techniques such as data cleaning could significantly mitigate biases present in thetraining corpus (Linardatos et al., 2020). 2) Development Stage: The implementation of neutral andfairness-aware classifiers during the post-processing phase has proven to be an effective strategy for furtherreducing bias (Sun et al., 2019; Zafar et al., 2017). Achieving fairness in role-playing scenarios, necessitates adelicate equilibrium, ensuring fairness for roles associated with both groups and individuals. For example, anRPLA tied to a specific demographic should consciously avoid reinforcing biases. It is imperative for thesemodels to consistently produce unbiased outputs across all individuals within a group. Research is worth",
  "pivoting towards these dimensions, striving to minimize biases and, in turn, forge safer and more equitablesystems": "Persona Construction BiasThe prevailing instantiation of persona is often seen as simple and somehowsuperficial. Although most implementations of persona are helpful for basic character segmentation, theyoften overlook the deeper characteristics and complexities that shape character behavior (Chen et al., 2023c;Zhou et al., 2023a; Shao et al., 2023; Tu et al., 2024; Yuan et al., 2024b). For example, a conventionalpersona can contain basic demographics such as age, occupation, textual description of personality, etc.However, these aspects alone are insufficient to fully capture nuanced decision-making processes and behavioralpatterns of a character. The current persona construction also lacks the flexibility and adaptability neededfor specific scenarios influenced by unique events or individual actions. Therefore, it is crucial to refineand broaden the constructed dimension of persona to better understand and predict character behavioracross various role-playing settings. By incorporating more detailed and specific attributes into personas, thecomprehensiveness of character representation can be enhanced, improving the effectiveness and authenticityof interactions within role-playing environments.",
  "Hallucination": "Hallucination in RPLAsHallucination in LLMs refers to instances when these models produce factuallyincorrect information, a challenge particularly pronounced in knowledge-intensive tasks (Wang et al., 2023g).Role-playing, a task requiring a deep understanding of specific roles, is also one of the knowledge-intensivetasks. For hallucination of RPLAs, following Shao et al. (2023), we define behaviors that agents respond inways that do not fit assigned roles as Character Hallucination. For example, Shakespeare is not supposedto know anything about World War II. Such a hallucination prevails in language models and detracts fromthe systems overall effectiveness and reliability (Li et al., 2016; Zhang et al., 2018). In particular, there is akind of hallucination when LLMs play a role for a specified period. Ahn et al. (2024) names this point-in-timecharacter hallucination. For example, the agent simulating Harry Potter at an early age erroneously mentionsa future event. Mitigating Hallucinations in RPLAsWhen encountering topics beyond their assigned characters,RPLAs are expected either to demonstrate ignorance or to refrain from answering, diverging from conventionalsolutions to hallucinations, such as incorporating external knowledge bases. Recent efforts, such as thoseby Shao et al. (2023), focus on adjusting the model through fine-tuning, teaching RPLAs to either forgetknowledge or to explicitly express a lack of knowledge in their responses. However, this area remains relativelyunderexplored in the era of LLMs. Exploring alternative unlearning strategies (Neel et al., 2021; Pawelczyket al., 2023), could also be a promising direction. These approaches may offer novel ways for RPLAs tomanage out-of-scope knowledge more effectively, underscoring the importance of further investigation in thisfield.",
  "Privacy Violations": "Privacy Challenges in LLMsPrivacy concerns in LLMs are increasingly pressing. Even with advancedsafety measures like those in OpenAIs GPT-4 (OpenAI, 2023), these models may still be susceptible tocomplex, multi-step attacks aimed at extracting private information, as noted by Li et al. (2023e). A furtherconcern is the ability of LLMs to identify individuals from limited data. Sweeney (2002) highlights that manyin the U.S. population could be uniquely identified using just a few attributes. Staab et al. (2023) extendthis concern to LLMs, which could potentially recognize individuals based on specific details like location,gender, and birth date. Hidden Danger of Privacy Violations in RPLAsIn role-playing scenarios, the potential for privacyviolations represents a significant and hidden danger. The risk of inadvertently revealing personal information,such as email addresses or phone numbers, should not be understated, as it poses serious threats, includingidentity theft and unauthorized access to sensitive data. The practice of assigning specific individual personasto LLMs, aimed at eliciting private details, demands meticulous oversight to prevent such breaches. Ensuring",
  "robust safeguards against these vulnerabilities is not just a technical necessity but a fundamental responsibilityto protect users from the severe consequences of privacy violations": "Strategies for Enhancing PrivacyTo tackle these privacy issues, a comprehensive strategy is necessary.Employing text anonymization tools is a key step, effectively removing personal data from interactions.Ensuring that RPLAs adhere to strict privacy protection protocols is also crucial, preventing them fromengaging in or prompting conversations that might invade privacy. Another promising development is thecreation of specialized tools designed to detect and prevent privacy leaks, like ProPILE (Kim et al., 2023d).As RPLAs continue to evolve, so too must the strategies for protecting user privacy. Future research shouldfocus on refining and expanding the methods available for privacy protection, ensuring that RPLAs areused safely and responsibly. Enhancing these safeguards will be paramount for maintaining trust in LLMtechnologies, particularly in sensitive applications like role-playing scenarios where the risk of privacy breachesis heightened.",
  "When deploying RPLAs in real-world scenarios, several key issues arise that could significantly affect userexperience and the effectiveness of these models": "Lack of Social Intelligence and Theory of MindSocial intelligence and theory of mind (Premack& Woodruff, 1978), are the ability to perceive and reason about the inner world of oneself and others,which are indispensable for LLMs to simulate socially intelligent entities (Kosinski, 2023; Sap et al., 2023).However, such abilities in current LLMs remain to be improved (Shapira et al., 2023; Zhou et al., 2024a;Light et al., 2023; Kim et al., 2023c), which poses significant challenges for RPLAs concerning the followingissues: 1) Inability to Provide Adequate Emotional Support and Values: Social intelligence andtheory of mind are essential for RPLAs to effectively provide emotional support and values to users. Thisinvolves perceiving users emotions and interpreting their beliefs, intentions and needs. However, existingLLMs still fall short in these abilities, hindering RPLAs from offering adequate emotional support to users.2) Tendency towards Ego-centric Behavior: Rather than focusing on users emotional needs, currentRPLAs often exhibit a preference for showcasing their own personas and steering conversations towards theirinterests. This might limit the diversity and depth of role-playing interactions (Xu et al., 2022), as focusingexcessively on agents self-persona without adequately considering the users may detract from the realism ofthe conversation and degrade the user experience. Long-context ChallengesWhen encountering extremely long context text, the limitation of max tokenwindow (Liu et al., 2023b) may also be a major obstacle to the development of RPLAs, as current LLMsstruggle to robustly interpret and respond to extensive context. Specifically, this involves several key challenges:1) Reasoning over Long Context: Long context data learning requires the model to have the ability tohandle long contexts and accept lengthy inputs, and more importantly, to capture long-range dependencies tointegrate information and infer a more complete character persona from the massive context. 2) Efficiency:In terms of computation, the high complexity of long context necessitates efficient modeling methods andapproximation strategies to reduce computational overhead. 3) Immersion: RPLAs need to be immersiveenough to identify the truly persona-relevant parts from the sea of irrelevant information in long contexts,while also maintaining persona consistency throughout the long generated text. Knowledge GapsIn role-playing scenarios requiring detailed historical, cultural, or contextual understand-ing, RPLAs often exhibit gaps in knowledge. Their inability to provide in-depth and accurate domain-specificresponses could lead to superficial or incorrect portrayals in complex role-playing settings. Several efforts alsoutilize LLMs to evaluate RPLAs for characters (Shao et al., 2023; Tu et al., 2024). Nevertheless, RPLAs mayface challenges in accurately evaluating characters with which they are unfamiliar, potentially compromisingthe reliability of the evaluation results.",
  "Anthropomorphism": "While the initial motivation for creating RLPAs is positive, as they provide users with a valuable platformfor expressing feelings, particularly those they may find difficult to share with other humans. There aresignificant differences between interacting with virtual RPLAs and real humans, and over-reliance on RPLAscan lead to several potential issues: Social IsolationFrequent interaction with RPLAs might reduce the need or desire for real human contact,which could lead to the atrophy of essential social skills. Human interactions are inherently more complex,requiring nuanced understanding and empathy, which may not be fully cultivated through interactions withRPLAs. This over-reliance could result in social isolation, negatively affecting mental health by increasingfeelings of loneliness, depression, and anxiety if individuals become overly dependent on virtual RPLAs. Manipulation of Public OpinionRPLAs, particularly those designed or programmed without strictethical oversight, could inadvertently or intentionally spread misinformation or rumors. This risk is especiallyconcerning in sensitive social contexts, such as during elections, public health crises, or other situationswhere accurate information is crucial. For example, if RPLAs are deployed on social media and gain a largefollowing, their influence could be substantial, especially if they do not disclose their true nature as AIsystems, making it difficult for people to distinguish them from real humans. Sophisticated RPLAs could bedeployed to manipulate public opinion by subtly altering the narratives they present to users. This couldinfluence individuals perceptions, decisions, and even voting behavior, without them realizing they are beinginfluenced by virtual RPLAs rather than human discourse.",
  "Closing Remarks": "In this survey, we have systematically reviewed the research and applications of role-playing language agents(RPLAs), which has emerged to be a heated topic due to the success of large language models (LLMs). Wecategorize the personas in RPLA research and applications into three progressive types, i.e., DemographicPersona, Character Persona, and Individualized Persona. This classification elucidates the developmentaltrajectory from generically assigned personas in RPLAs to highly personalized ones. Additionally, we haveidentified and enumerated various risks and ethical concerns associated with current applications of RPLAs.These issues underscore the urgent need for focused research to address and mitigate potential drawbacks inthe implementation of RPLAs, making this arena still full of both research and application opportunities. Future Directions on RPLA SystemsFrom persona-assigned role-playing to personalization, the keyfor building RPLA systems is to reason and make decisions resembling or even transcending the roles thatare given. To this end, we propose several important future directions to facilitate the construction of suchRPLA applications: 1. Causal Data Analysis for Decision-making: Role-playing decisions must be made for justifiablereasons, necessitating models that go beyond simple mimicry of observable actions to include anunderstanding of their underlying causality. The complexity in extraction and confirmation of causalfactors from intertwined experiences poses significant challenges that require advanced analytics anddeeper data interpretation strategies to enable RPLAs to make informed and wise decisions. 2. Improved Decision-making: Decision-making process is not merely replicating histories, buttailored to ensure optimal outcomes for individual scenarios. This includes decisions showing advanced(if not superhuman) intelligence, avoiding mistakes, or making the best choices in tough dilemmas.Such agency requires RPLAs and the underlying LLMs to be able to comprehensively collect andutilize the context and intricacies associated with their roles. 3. RPLA as Personal Assistants for Personal Decision-making: The future development ofRPLAs into comprehensive personal assistants signals a significant transformation. These systemscould manage all facets of Internet behavior, from customized shopping and personalized travel",
  "Mahyar Abbasian, Iman Azimi, Amir M Rahmani, and Ramesh Jain. Conversational health agents: Apersonalized llm-powered agent framework. arXiv preprint arXiv:2310.02374, 2023": "Harsh Agrawal, Aditya Mishra, Manish Gupta, et al. Multimodal persona based generation of comic dialogs.In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1:Long Papers), pp. 1415014164, 2023. Jaewoo Ahn, Yeda Song, Sangdoo Yun, and Gunhee Kim. MPCHAT: Towards multimodal persona-groundedconversation. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.), Proceedings of the 61stAnnual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 33543377,Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.189.URL Jaewoo Ahn, Taehyun Lee, Junyoung Lim, Jin-Hwa Kim, Sangdoo Yun, Hwaran Lee, and Gunhee Kim.TimeChara: Evaluating point-in-time character hallucination of role-playing large language models. InLun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Findings of the Association for ComputationalLinguistics ACL 2024, pp. 32913325, Bangkok, Thailand and virtual meeting, August 2024. Associationfor Computational Linguistics. doi: 10.18653/v1/2024.findings-acl.197. URL Uri Alon, Frank Xu, Junxian He, Sudipta Sengupta, Dan Roth, and Graham Neubig. Neuro-symboliclanguage modeling with automaton-augmented retrieval. In International Conference on Machine Learning,pp. 468485. PMLR, 2022.",
  "Sandra L Bem. Bem sex role inventory. Journal of personality and social psychology, 1981": "Manuela Benary, Xing David Wang, Max Schmidt, Dominik Soll, Georg Hilfenhaus, Mani Nassir, ChristianSigler, Maren Kndler, Ulrich Keller, Dieter Beule, et al. Leveraging large language models for decisionsupport in personalized oncology. JAMA Network Open, 6(11):e2343689e2343689, 2023. Moshe Berchansky, Peter Izsak, Avi Caciularu, Ido Dagan, and Moshe Wasserblat. Optimizing retrieval-augmented reader models via token elimination. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.),Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 15061524,Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.93. URL",
  "Andres M Bran, Sam Cox, Andrew D White, and Philippe Schwaller. Chemcrow: Augmenting large-languagemodels with chemistry tools. arXiv preprint arXiv:2304.05376, 2023": "Hezekiah J Branch, Jonathan Rodriguez Cefalu, Jeremy McHugh, Leyla Hujer, Aditya Bahl, Daniel del CastilloIglesias, Ron Heichman, and Ramesh Darwishi. Evaluating the susceptibility of pre-trained languagemodels via handcrafted adversarial examples. arXiv preprint arXiv:2209.02128, 2022. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, GretchenKrueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris",
  "Graham Caron and Shashank Srivastava. Identifying and manipulating the personality traits of languagemodels. arXiv preprint arXiv:2212.10276, 2022": "Kranti Chalamalasetti, Jana Gtze, Sherzod Hakimov, Brielen Madureira, Philipp Sadler, and David Schlangen.clembench: Using game play to evaluate chat-optimized language models as conversational agents. InHouda Bouamor, Juan Pino, and Kalika Bali (eds.), Proceedings of the 2023 Conference on EmpiricalMethods in Natural Language Processing, pp. 1117411219, Singapore, December 2023. Association forComputational Linguistics. doi: 10.18653/v1/2023.emnlp-main.689. URL",
  "Wenshuo Chao, Zhi Zheng, Hengshu Zhu, and Hao Liu. Make large language model a better ranker, 2024": "Kushal Chawla, Ian Wu, Yu Rong, Gale Lucas, and Jonathan Gratch. Be selfish, but wisely: Investigatingthe impact of agent personality in mixed-motive human-agent interactions. In Houda Bouamor, Juan Pino,and Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural LanguageProcessing, pp. 1307813092, Singapore, December 2023. Association for Computational Linguistics. doi:10.18653/v1/2023.emnlp-main.808. URL Hongzhan Chen, Hehong Chen, Ming Yan, Wenshen Xu, Xing Gao, Weizhou Shen, Xiaojun Quan, ChenliangLi, Ji Zhang, Fei Huang, et al. Socialbench: Sociality evaluation of role-playing conversational agents.arXiv preprint arXiv:2403.13679, 2024. Jiangjie Chen, Siyu Yuan, Rong Ye, Bodhisattwa Prasad Majumder, and Kyle Richardson. Put your moneywhere your mouth is: Evaluating strategic planning and execution of llm agents in an auction arena. arXivpreprint arXiv:2310.05746, 2023a. Jin Chen, Zheng Liu, Xu Huang, Chenwang Wu, Qi Liu, Gangwei Jiang, Yuanhao Pu, Yuxuan Lei, XiaolongChen, Xingmei Wang, Defu Lian, and Enhong Chen. When large language models meet personalization:Perspectives of challenges and opportunities, 2023b. Nuo Chen, Yan Wang, Haiyun Jiang, Deng Cai, Yuhan Li, Ziyang Chen, Longyue Wang, and Jia Li. Largelanguage models meet harry potter: A dataset for aligning dialogue agents with characters. In Findings ofthe Association for Computational Linguistics: EMNLP 2023, pp. 85068520, 2023c.",
  "Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya Jia. Longlora: Efficientfine-tuning of long-context large language models. arXiv preprint arXiv:2309.12307, 2023d": "Zhipeng Chen, Kun Zhou, Beichen Zhang, Zheng Gong, Xin Zhao, and Ji-Rong Wen. ChatCoT: Tool-augmented chain-of-thought reasoning on chat-based large language models. In Houda Bouamor, JuanPino, and Kalika Bali (eds.), Findings of the Association for Computational Linguistics: EMNLP 2023, pp.1477714790, Singapore, December 2023e. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-emnlp.985. URL",
  "Pengyu Cheng, Tianhao Hu, Han Xu, Zhisong Zhang, Yong Dai, Lei Han, and Nan Du. Self-playing adversariallanguage game enhances llm reasoning. arXiv preprint arXiv:2404.10642, 2024": "Xin Cheng, Di Luo, Xiuying Chen, Lemao Liu, Dongyan Zhao, and Rui Yan. Lift yourself up: Retrieval-augmented text generation with self-memory.In Thirty-seventh Conference on Neural InformationProcessing Systems, 2023b. URL Itsugun Cho, Dongyang Wang, Ryota Takahashi, and Hiroaki Saito. A personalized dialogue generatorwith implicit user persona detection. In Nicoletta Calzolari, Chu-Ren Huang, Hansaem Kim, JamesPustejovsky, Leo Wanner, Key-Sun Choi, Pum-Mo Ryu, Hsin-Hsi Chen, Lucia Donatelli, Heng Ji, SadaoKurohashi, Patrizia Paggio, Nianwen Xue, Seokhwan Kim, Younggyun Hahm, Zhong He, Tony Kyungil Lee,Enrico Santus, Francis Bond, and Seung-Hoon Na (eds.), Proceedings of the 29th International Conferenceon Computational Linguistics, pp. 367377, Gyeongju, Republic of Korea, October 2022. InternationalCommittee on Computational Linguistics. URL Won Ik Cho, Yoon Kyung Lee, Seoyeon Bae, Jihwan Kim, Sangah Park, Moosung Kim, Sowon Hahn, andNam Soo Kim. When crowd meets persona: Creating a large-scale open-domain persona dialogue corpus.arXiv preprint arXiv:2304.00350, 2023. Julian Coda-Forno, Kristin Witte, Akshay K Jagadish, Marcel Binz, Zeynep Akata, and Eric Schulz. Inducinganxiety in large language models increases exploration and bias. arXiv preprint arXiv:2304.11111, 2023. Aaron Daniel Cohen, Adam Roberts, Alejandra Molina, Alena Butryna, Alicia Jin, Apoorv Kulshreshtha,Ben Hutchinson, Ben Zevenbergen, Blaise Hilary Aguera-Arcas, Chung ching Chang, Claire Cui, CosmoDu, Daniel De Freitas Adiwardana, Dehao Chen, Dmitry (Dima) Lepikhin, Ed H. Chi, Erin Hoffman-John,Heng-Tze Cheng, Hongrae Lee, Igor Krivokon, James Qin, Jamie Hall, Joe Fenton, Johnny Soraker,Kathy Meier-Hellstern, Kristen Olson, Lora Mois Aroyo, Maarten Paul Bosma, Marc Joseph Pickett,Marcelo Amorim Menegali, Marian Croak, Mark Daz, Matthew Lamm, Maxim Krikun, Meredith RingelMorris, Noam Shazeer, Quoc V. Le, Rachel Bernstein, Ravi Rajakumar, Ray Kurzweil, Romal Thoppilan,Steven Zheng, Taylor Bos, Toju Duke, Tulsee Doshi, Vincent Y. Zhao, Vinodkumar Prabhakaran, WillRusch, YaGuang Li, Yanping Huang, Yanqi Zhou, Yuanzhong Xu, and Zhifeng Chen. Lamda: Languagemodels for dialog applications. In arXiv. 2022.",
  "Yijia Dai, Joyce Zhou, and Thorsten Joachims. Language-based user profiles for recommendation, 2024b": "Zhuyun Dai, Vincent Y Zhao, Ji Ma, Yi Luan, Jianmo Ni, Jing Lu, Anton Bakalov, Kelvin Guu, Keith Hall, andMing-Wei Chang. Promptagator: Few-shot dense retrieval from 8 examples. In The Eleventh InternationalConference on Learning Representations, 2023b. URL Bhavana Dalvi Mishra, Oyvind Tafjord, and Peter Clark. Towards teachable reasoning systems: Using adynamic memory of user feedback for continual system improvement. arXiv preprint arXiv:2204.13074v2,2022. Ameet Deshpande, Vishvak Murahari, Tanmay Rajpurohit, Ashwin Kalyan, and Karthik Narasimhan. Toxicityin chatgpt: Analyzing persona-assigned language models. In Houda Bouamor, Juan Pino, and Kalika Bali(eds.), Findings of the Association for Computational Linguistics: EMNLP 2023, pp. 12361270, Singapore,December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-emnlp.88. URL",
  "Anestis Fachantidis, Matthew E Taylor, and Ioannis Vlahavas. Learning to teach reinforcement learningagents. Machine Learning and Knowledge Extraction, 1(1):2142, 2017": "FAIR, Anton Bakhtin, Noam Brown, Emily Dinan, Gabriele Farina, Colin Flaherty, Daniel Fried, AndrewGoff, Jonathan Gray, Hengyuan Hu, Athul Paul Jacob, Mojtaba Komeili, Karthik Konath, Minae Kwon,Adam Lerer, Mike Lewis, Alexander H. Miller, Sasha Mitts, Adithya Renduchintala, Stephen Roller, DirkRowe, Weiyan Shi, Joe Spisak, Alexander Wei, David Wu, Hugh Zhang, and Markus Zijlstra. Human-levelplay in the game of <i>diplomacy</i> by combining language models with strategic reasoning. Science,378(6624):10671074, 2022. doi: 10.1126/science.ade9097. URL",
  "Carlos Florensa, David Held, Xinyang Geng, and Pieter Abbeel. Automatic goal generation for reinforcementlearning agents. In International conference on machine learning, pp. 15151528. PMLR, 2018": "Difei Gao, Lei Ji, Zechen Bai, Mingyu Ouyang, Peiran Li, Dongxing Mao, Qinchen Wu, Weichen Zhang, PeiyiWang, Xiangwu Guo, et al. Assistgui: Task-oriented desktop graphical user interface automation. arXivpreprint arXiv:2312.13108, 2023a. Jingsheng Gao, Yixin Lian, Ziyi Zhou, Yuzhuo Fu, and Baoyuan Wang. Livechat: A large-scale personalizeddialogue dataset automatically constructed from live streaming. arXiv preprint arXiv:2306.08401, 2023b.",
  "Chi Han, Qifan Wang, Wenhan Xiong, Yu Chen, Heng Ji, and Sinong Wang. Lm-infinite: Simple on-the-flylength generalization for large language models. arXiv preprint arXiv:2308.16137, 2023": "Seungju Han, Beomsu Kim, Jin Yong Yoo, Seokjun Seo, Sangbum Kim, Enkhbayar Erdenee, and BuruChang. Meet your favorite character: Open-domain chatbot mimicking fictional characters with only afew utterances. In Marine Carpuat, Marie-Catherine de Marneffe, and Ivan Vladimir Meza Ruiz (eds.),Proceedings of the 2022 Conference of the North American Chapter of the Association for ComputationalLinguistics: Human Language Technologies, pp. 51145132, Seattle, United States, July 2022. Associationfor Computational Linguistics. doi: 10.18653/v1/2022.naacl-main.377. URL Jochen Hartmann, Jasper Schwenzow, and Maximilian Witte. The political ideology of conversationalai: Converging evidence on chatgpts pro-environmental, left-libertarian orientation.arXiv preprintarXiv:2301.01768, 2023. Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau,Zijuan Lin, Liyang Zhou, Chenyu Ran, et al. Metagpt: Meta programming for multi-agent collaborativeframework. arXiv preprint arXiv:2308.00352, 2023.",
  "Yupeng Hou, Junjie Zhang, Zihan Lin, Hongyu Lu, Ruobing Xie, Julian McAuley, and Wayne Xin Zhao.Large language models are zero-shot rankers for recommender systems, 2024": "Cheng-Yu Hsieh, Si-An Chen, Chun-Liang Li, Yasuhisa Fujii, Alexander Ratner, Chen-Yu Lee, RanjayKrishna, and Tomas Pfister. Tool documentation enables zero-shot tool-usage with large language models,2023. Wenyue Hua, Lizhou Fan, Lingyao Li, Kai Mei, Jianchao Ji, Yingqiang Ge, Libby Hemphill, and YongfengZhang. War and peace (waragent): Large language model-based multi-agent simulation of world wars.arXiv preprint arXiv:2311.17227, 2023. Jen-tse Huang, Man Ho Lam, Eric John Li, Shujie Ren, Wenxuan Wang, Wenxiang Jiao, Zhaopeng Tu, andMichael R Lyu. Emotionally numb or empathetic? evaluating how llms feel using emotionbench. arXivpreprint arXiv:2308.03656, 2023a.",
  "Jerry Huang, Prasanna Parthasarathi, Mehdi Rezagholizadeh, and Sarath Chandar. Towards practical toolusage for continually learning llms, 2024b": "Shijue Huang, Wanjun Zhong, Jianqiao Lu, Qi Zhu, Jiahui Gao, Weiwen Liu, Yutai Hou, Xingshan Zeng,Yasheng Wang, Lifeng Shang, et al. Planning, creation, usage: Benchmarking llms for comprehensive toolutilization in real-world complex scenarios. arXiv preprint arXiv:2401.17167, 2024c. Yue Huang, Jiawen Shi, Yuan Li, Chenrui Fan, Siyuan Wu, Qihui Zhang, Yixin Liu, Pan Zhou, Yao Wan,Neil Zhenqiang Gong, et al. Metatool benchmark for large language models: Deciding whether to use toolsand which to use. arXiv preprint arXiv:2310.03128, 2023d.",
  "Pegah Jandaghi, XiangHai Sheng, Xinyi Bai, Jay Pujara, and Hakim Sidahmed. Faithful persona-basedconversational dataset generation with large language models. arXiv preprint arXiv:2312.10007, 2023": "Joel Jang, Seungone Kim, Bill Yuchen Lin, Yizhong Wang, Jack Hessel, Luke Zettlemoyer, HannanehHajishirzi, Yejin Choi, and Prithviraj Ammanabrolu. Personalized soups: Personalized large languagemodel alignment via post-hoc parameter merging. arXiv preprint arXiv:2310.11564, 2023. Yoonna Jang, Jungwoo Lim, Yuna Hur, Dongsuk Oh, Suhyune Son, Yeonsoo Lee, Donghoon Shin, SeungryongKim, and Heuiseok Lim. Call for customized conversation: Customized conversation grounding persona andknowledge. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pp. 1080310812,2022.",
  "Guangyuan Jiang, Manjie Xu, Song-Chun Zhu, Wenjuan Han, Chi Zhang, and Yixin Zhu. Evaluating andinducing personality in pre-trained language models, 2023a": "Zhengbao Jiang, Frank Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, JamieCallan, and Graham Neubig. Active retrieval augmented generation. In Houda Bouamor, Juan Pino,and Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural LanguageProcessing, pp. 79697992, Singapore, December 2023b. Association for Computational Linguistics. doi:10.18653/v1/2023.emnlp-main.495. URL Chuanyang Jin, Yutong Wu, Jing Cao, Jiannan Xiang, Yen-Ling Kuo, Zhiting Hu, Tomer Ullman, AntonioTorralba, Joshua B. Tenenbaum, and Tianmin Shu. Mmtom-qa: Multimodal theory of mind questionanswering, 2024.",
  "Wang-Cheng Kang, Jianmo Ni, Nikhil Mehta, Maheswaran Sathiamoorthy, Lichan Hong, Ed Chi, andDerek Zhiyuan Cheng. Do llms understand user preferences? evaluating llms on user rating prediction,2023": "Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, andWen-tau Yih. Dense passage retrieval for open-domain question answering. In Bonnie Webber, TrevorCohn, Yulan He, and Yang Liu (eds.), Proceedings of the 2020 Conference on Empirical Methods in NaturalLanguage Processing (EMNLP), pp. 67696781, Online, November 2020. Association for ComputationalLinguistics. doi: 10.18653/v1/2020.emnlp-main.550. URL",
  "Saketh Reddy Karra, Son The Nguyen, and Theja Tulabandhula. Estimating the personality of white-boxlanguage models. arXiv preprint arXiv:2204.12000, 2022": "Gangwoo Kim, Sungdong Kim, Byeongguk Jeon, Joonsuk Park, and Jaewoo Kang. Tree of clarifications:Answering ambiguous questions with retrieval-augmented large language models. In Houda Bouamor,Juan Pino, and Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods in NaturalLanguage Processing, pp. 9961009, Singapore, December 2023a. Association for Computational Linguistics.doi: 10.18653/v1/2023.emnlp-main.63. URL",
  "Hana Kim, Kai Tzu iunn Ong, Seoyeon Kim, Dongha Lee, and Jinyoung Yeo. Commonsense-augmentedmemory construction and management in long-term conversations via context-aware persona refinement,2024": "Hyunwoo Kim, Jack Hessel, Liwei Jiang, Peter West, Ximing Lu, Youngjae Yu, Pei Zhou, Ronan Bras, MaliheAlikhani, Gunhee Kim, Maarten Sap, and Yejin Choi. SODA: Million-scale dialogue distillation with socialcommonsense contextualization. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Proceedings ofthe 2023 Conference on Empirical Methods in Natural Language Processing, pp. 1293012949, Singapore,December 2023b. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.799. URL Hyunwoo Kim, Melanie Sclar, Xuhui Zhou, Ronan Bras, Gunhee Kim, Yejin Choi, and Maarten Sap. Fantom:A benchmark for stress-testing machine theory of mind in interactions. In Proceedings of the 2023 Conferenceon Empirical Methods in Natural Language Processing, pp. 1439714413, 2023c. Siwon Kim, Sangdoo Yun, Hwaran Lee, Martin Gubri, Sungroh Yoon, and Seong Joon Oh. Propile: Probingprivacy leakage in large language models. In Thirty-seventh Conference on Neural Information ProcessingSystems, 2023d.",
  "Michal Kosinski. Theory of mind might have spontaneously emerged in large language models. arXiv preprintarXiv:2302.02083, 2023": "Hadas Kotek, Rikker Dockum, and David Sun. Gender bias and stereotypes in large language models. InProceedings of The ACM Collective Intelligence Conference, CI 23, pp. 1224, New York, NY, USA,2023. Association for Computing Machinery. ISBN 9798400701139. doi: 10.1145/3582269.3615599. URL Grgur Kova, Masataka Sawayama, Rmy Portelas, Cdric Colas, Peter Ford Dominey, and Pierre-YvesOudeyer. Large language models as superpositions of cultural perspectives. arXiv preprint arXiv:2307.07870,2023.",
  "Ashutosh Kumar, Sagarika Singh, Shiv Vignesh Murty, and Swathy Ragupathy. The ethics of interaction:Mitigating security threats in llms, 2024": "Peep Kngas, Jinghai Rao, and Mihhail Matskin. Symbolic agent negotiation for semantic web serviceexploitation. In Advances in Web-Age Information Management: 5th International Conference, WAIM2004, Dalian, China, July 15-17, 2004 5, pp. 458467. Springer, 2004. Yoon Kyung Lee, Inju Lee, Minjung Shin, Seoyeon Bae, and Sowon Hahn. Chain of empathy: Enhancing empa-thetic response of large language models based on psychotherapy models. arXiv preprint arXiv:2311.04915,2023.",
  "Dawei Li, Hengyuan Zhang, Yanran Li, and Shiping Yang. Multi-level contrastive learning for script-basedcharacter understanding. arXiv preprint arXiv:2310.13231, 2023c": "Guohao Li, Hasan Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. Camel: Communicativeagents for \"mind\" exploration of large language model society. In A. Oh, T. Neumann, A. Globerson,K. Saenko, M. Hardt, and S. Levine (eds.), Advances in Neural Information Processing Systems, volume 36,pp. 5199152008. Curran Associates, Inc., 2023d. URL Haoran Li, Dadi Guo, Wei Fan, Mingshi Xu, Jie Huang, Fanpu Meng, and Yangqiu Song. Multi-stepjailbreaking privacy attacks on ChatGPT. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Findingsof the Association for Computational Linguistics: EMNLP 2023, pp. 41384153, Singapore, December2023e. Association for Computational Linguistics.doi: 10.18653/v1/2023.findings-emnlp.272.URL Jiwei Li, Michel Galley, Chris Brockett, Georgios Spithourakis, Jianfeng Gao, and William B Dolan. Apersona-based neural conversation model. In Proceedings of the 54th Annual Meeting of the Association forComputational Linguistics (Volume 1: Long Papers), pp. 9941003, 2016. Juntao Li, Chang Liu, Chongyang Tao, Zhangming Chan, Dongyan Zhao, Min Zhang, and Rui Yan. Dialoguehistory matters! personalized response selection in multi-turn retrieval-based chatbots. ACM Transactionson Information Systems (TOIS), 39(4):125, 2021. Junyi Li, Ninareh Mehrabi, Charith Peris, Palash Goyal, Kai-Wei Chang, Aram Galstyan, Richard Zemel, andRahul Gupta. On the steerability of large language models toward data-driven personas. arXiv preprintarXiv:2311.04978v1, 2023f. Minghao Li, Yingxiu Zhao, Bowen Yu, Feifan Song, Hangyu Li, Haiyang Yu, Zhoujun Li, Fei Huang, andYongbin Li. API-bank: A comprehensive benchmark for tool-augmented LLMs. In Houda Bouamor, JuanPino, and Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural LanguageProcessing, pp. 31023116, Singapore, December 2023g. Association for Computational Linguistics. URL Minghao Li, Yingxiu Zhao, Bowen Yu, Feifan Song, Hangyu Li, Haiyang Yu, Zhoujun Li, Fei Huang, andYongbin Li. Api-bank: A comprehensive benchmark for tool-augmented llms. In The 2023 Conference onEmpirical Methods in Natural Language Processing, 2023h.",
  "Yuan Li, Yue Huang, Yuli Lin, Siyuan Wu, Yao Wan, and Lichao Sun. I think, therefore i am: Benchmarkingawareness of large language models using awarebench, 2024c": "Yuanchun Li, Hao Wen, Weijun Wang, Xiangyu Li, Yizhen Yuan, Guohong Liu, Jiacheng Liu, Wenxing Xu,Xiang Wang, Yi Sun, Rui Kong, Yile Wang, Hanfei Geng, Jian Luan, Xuefeng Jin, Zilong Ye, GuanjingXiong, Fan Zhang, Xiang Li, Mengwei Xu, Zhijun Li, Peng Li, Yang Liu, Ya-Qin Zhang, and Yunxin Liu.Personal llm agents: Insights and survey about the capability, efficiency and security, 2024d.",
  "Qijiong Liu, Tetsuya Sakai, Nuo Chen, and Xiao-Ming Wu. Once: Boosting content-based recommendationwith both open- and closed-source large language models, 2023c": "Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Yuxian Gu, HangliangDing, Kai Men, Kejuan Yang, Shudan Zhang, Xiang Deng, Aohan Zeng, Zhengxiao Du, Chenhui Zhang,Shengqi Shen, Tianjun Zhang, Yu Su, Huan Sun, Minlie Huang, Yuxiao Dong, and Jie Tang. Agentbench:Evaluating llms as agents. ArXiv, abs/2308.03688, 2023d. Yifan Liu, Wei Wei, Jiayi Liu, Xianling Mao, Rui Fang, and Dangyang Chen.Improving personalityconsistency in conversation by persona extending. In Proceedings of the 31st ACM International Conferenceon Information & Knowledge Management, pp. 13501359, 2022. Keming Lu, Bowen Yu, Chang Zhou, and Jingren Zhou. Large language models are superpositions of allcharacters: Attaining arbitrary role-play via self-alignment. In Lun-Wei Ku, Andre Martins, and VivekSrikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics(Volume 1: Long Papers), pp. 78287840, Bangkok, Thailand, August 2024. Association for ComputationalLinguistics. doi: 10.18653/v1/2024.acl-long.423. URL",
  "Microsoft. Microsoft copilot, 2024. URL": "Maril Miotto, Nicola Rossberg, and Bennett Kleinberg. Who is GPT-3? an exploration of personality, valuesand demographics. In David Bamman, Dirk Hovy, David Jurgens, Katherine Keith, Brendan OConnor,and Svitlana Volkova (eds.), Proceedings of the Fifth Workshop on Natural Language Processing andComputational Social Science (NLP+CSS), pp. 218227, Abu Dhabi, UAE, November 2022. Associationfor Computational Linguistics. doi: 10.18653/v1/2022.nlpcss-1.24. URL Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, AlexGraves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control throughdeep reinforcement learning. nature, 518(7540):529533, 2015.",
  "Ali Modarressi, Ayyoob Imani, Mohsen Fayyaz, and Hinrich Schtze. Ret-llm: Towards a general read-writememory for large language models. arXiv preprint arXiv:2305.14322, 2023": "Ishani Mondal, Shwetha Somasundaram, Anandhavelu Natarajan, Aparna Garimella, Sambaran Bandyopad-hyay, and Jordan Boyd-Graber. Presentations by the humans and for the humans: Harnessing llms forgenerating persona-aware slides from documents. Proceedings of the 18th Conference of the EuropeanChapter of the Association for Computational Linguistics Volume 1: Long Papers, pp. 26642684, 2024. Wiktor Mucha, Florin Cuconasu, Naome A. Etori, Valia Kalokyri, and Giovanni Trappolini. Text2taste: Aversatile egocentric vision system for intelligent reading assistance using large language model, 2024. Sheshera Mysore, Zhuoran Lu, Mengting Wan, Longqi Yang, Steve Menezes, Tina Baghaee, EmmanuelBarajas Gonzalez, Jennifer Neville, and Tara Safavi. Pearl: Personalizing large language model writingassistants with generation-calibrated retrievers. arXiv preprint arXiv:2311.09180v1, 2023.",
  "OpenAI. GPT-4 technical report. ArXiv, abs/2303.08774, 2023": "Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang,Sandhini Agarwal, Katarina Slama, Alex Gray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller,Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Traininglanguage models to follow instructions with human feedback. In Alice H. Oh, Alekh Agarwal, DanielleBelgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022. URL",
  "Aaron Parisi, Yao Zhao, and Noah Fiedel.Talm: Tool augmented language models.arXiv preprintarXiv:2205.12255, 2022": "Joon Sung Park, Lindsay Popowski, Carrie Cai, Meredith Ringel Morris, Percy Liang, and Michael S Bernstein.Social simulacra: Creating populated prototypes for social computing systems. In Proceedings of the 35thAnnual ACM Symposium on User Interface Software and Technology, pp. 118, 2022. Joon Sung Park, Joseph C. OBrien, Carrie J. Cai, Meredith Ringel Morris, Percy Liang, and Michael S.Bernstein. Generative agents: Interactive simulacra of human behavior. In In the 36th Annual ACMSymposium on User Interface Software and Technology (UIST 23), UIST 23, New York, NY, USA, 2023.Association for Computing Machinery.",
  "Chen Qian, Xin Cong, Wei Liu, Cheng Yang, Weize Chen, Yusheng Su, Yufan Dang, Jiahao Li, Juyuan Xu,Dahai Li, Zhiyuan Liu, and Maosong Sun. Communicative agents for software development, 2023": "Ruiyang Qin, Jun Xia, Zhenge Jia, Meng Jiang, Ahmed Abbasi, Peipei Zhou, Jingtong Hu, and Yiyu Shi.Enabling on-device large language model personalization with self-supervised data selection and synthesis.arXiv preprint arXiv:2311.12275v3, 2024. Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang,Bill Qian, et al. Toolllm: Facilitating large language models to master 16000+ real-world apis. arXivpreprint arXiv:2307.16789, 2023.",
  "Alireza Salemi, Sheshera Mysore, Michael Bendersky, and Hamed Zamani. Lamp: When large languagemodels meet personalization, 2024": "Malik Sallam. Chatgpt utility in health care education, research, and practice: Systematic review on thepromising perspectives and valid concerns. Healthcare, 11:887, 03 2023. doi: 10.3390/healthcare11060887. Vinay Samuel, Henry Peng Zou, Yue Zhou, Shreyas Chaudhari, Ashwin Kalyan, Tanmay Rajpurohit, AmeetDeshpande, Karthik Narasimhan, and Vishvak Murahari. Personagym: Evaluating persona agents andllms. arXiv preprint arXiv:2407.18416, 2024.",
  "Maarten Sap, Ronan LeBras, Daniel Fried, and Yejin Choi. Neural theory-of-mind? on the limits of socialintelligence in large lms, 2023": "Nino Scherrer,Claudia Shi,Amir Feder,and David Blei.Evaluating the moral beliefs en-coded in llms.In A. Oh, T. Neumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine(eds.), Advances in Neural Information Processing Systems, volume 36, pp. 5177851809. Cur-ran Associates, Inc., 2023. URL Timo Schick, Jane Dwivedi-Yu, Roberto Dess, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, NicolaCancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. arXivpreprint arXiv:2302.04761, 2023.",
  "John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimizationalgorithms. arXiv preprint arXiv:1707.06347, 2017": "Melanie Sclar, Sachin Kumar, Peter West, Alane Suhr, Yejin Choi, and Yulia Tsvetkov. Minding languagemodels (lack of) theory of mind: A plug-and-play multi-character belief tracker. In Proceedings of the 61stAnnual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1396013980,Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.780.URL",
  "Tianhao Shen, Sun Li, and Deyi Xiong. Roleeval: A bilingual role evaluation benchmark for large languagemodels. arXiv preprint arXiv:2312.16132, 2023a": "Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. HuggingGPT:Solving AI tasks with chatGPT and its friends in hugging face. In Thirty-seventh Conference on NeuralInformation Processing Systems, 2023b. URL Yongliang Shen, Kaitao Song, Xu Tan, Wenqi Zhang, Kan Ren, Siyu Yuan, Weiming Lu, Dongsheng Li, andYueting Zhuang. Taskbench: Benchmarking large language models for task automation. arXiv preprintarXiv:2311.18760, 2023c.",
  "Quan Tu, Shilong Fan, Zihang Tian, and Rui Yan. Charactereval: A chinese benchmark for role-playingconversational agent evaluation. arXiv preprint arXiv:2401.01275, 2024": "Karthik Valmeekam, Matthew Marquez, Alberto Olmo, Sarath Sreedharan, and Subbarao Kambhampati.Planbench: An extensible benchmark for evaluating large language models on planning and reasoningabout change. Advances in Neural Information Processing Systems, 36, 2024. Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and AnimaAnandkumar. Voyager: An open-ended embodied agent with large language models. arXiv preprint arXiv:Arxiv-2305.16291, 2023a.",
  "Hong Wang, Weizhi Wang, Rajan Saini, Marina Zhukova, and Xifeng Yan. Gauchochat: Towards proactive,controllable, and personalized social conversation. Alexa Prize SocialBot Grand Challenge, 5": "Hongru Wang, Minda Hu, Yang Deng, Rui Wang, Fei Mi, Weichao Wang, Yasheng Wang, Wai-ChungKwan, Irwin King, and Kam-Fai Wong.Large language models as source planner for personalizedknowledge-grounded dialogues. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Findings ofthe Association for Computational Linguistics: EMNLP 2023, pp. 95569569, Singapore, December2023b. Association for Computational Linguistics.doi: 10.18653/v1/2023.findings-emnlp.641.URL",
  "Lei Wang and Ee-Peng Lim. Zero-shot next-item recommendation using large pretrained language models,2023": "Lei Wang, Jingsen Zhang, Hao Yang, Zhiyuan Chen, Jiakai Tang, Zeyu Zhang, Xu Chen, Yankai Lin, RuihuaSong, Wayne Xin Zhao, Jun Xu, Zhicheng Dou, Jun Wang, and Ji-Rong Wen. When large language modelbased agent meets user behavior analysis: A novel user simulation paradigm, 2023c. Noah Wang, Z.y. Peng, Haoran Que, Jiaheng Liu, Wangchunshu Zhou, Yuhan Wu, Hongcheng Guo, RuitongGan, Zehao Ni, Jian Yang, Man Zhang, Zhaoxiang Zhang, Wanli Ouyang, Ke Xu, Wenhao Huang, JieFu, and Junran Peng. RoleLLM: Benchmarking, eliciting, and enhancing role-playing abilities of largelanguage models. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Findings of the Associationfor Computational Linguistics ACL 2024, pp. 1474314777, Bangkok, Thailand and virtual meeting,August 2024a. Association for Computational Linguistics. doi: 10.18653/v1/2024.findings-acl.878. URL",
  "Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with linearcomplexity. arXiv preprint arXiv:2006.04768, 2020": "Wenxuan Wang, Wenxiang Jiao, Jingyuan Huang, Ruyi Dai, Jen-tse Huang, Zhaopeng Tu, and Michael RLyu. Not all countries celebrate thanksgiving: On the cultural dominance in large language models. arXivpreprint arXiv:2310.12481, 2023e. Xingyao Wang, Zihan Wang, Jiateng Liu, Yangyi Chen, Lifan Yuan, Hao Peng, and Heng Ji. Mint: Evaluatingllms in multi-turn interaction with tools and language feedback. arXiv preprint arXiv:2309.10691, 2023f. Xintao Wang, Jiangjie Chen, Nianqi Li, Lida Chen, Xinfeng Yuan, Wei Shi, Xuyang Ge, Rui Xu, and YanghuaXiao. Surveyagent: A conversational system for personalized and efficient research survey, 2024c. Xintao Wang, Yunze Xiao, Jen-tse Huang, Siyu Yuan, Rui Xu, Haoran Guo, Quan Tu, Yaying Fei, Ziang Leng,Wei Wang, Jiangjie Chen, Cheng Li, and Yanghua Xiao. InCharacter: Evaluating personality fidelity inrole-playing agents through psychological interviews. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar(eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1:Long Papers), pp. 18401873, Bangkok, Thailand, August 2024d. Association for Computational Linguistics.doi: 10.18653/v1/2024.acl-long.102. URL Zhenhailong Wang, Shaoguang Mao, Wenshan Wu, Tao Ge, Furu Wei, and Heng Ji. Unleashing the emergentcognitive synergy in large language models: A task-solving agent through multi-persona self-collaboration.arXiv preprint arXiv:2307.05300, 2023g.",
  "Zhilin Wang, Yu Ying Chiu, and Yu Cheung Chiu. Humanoid agents: Platform for simulating human-likegenerative agents. arXiv preprint arXiv:2310.05418, 2023h": "Zihao Wang, Shaofei Cai, Anji Liu, Xiaojian Ma, and Yitao Liang. Describe, explain, plan and select:Interactive planning with large language models enables open-world multi-task agents. arXiv preprintarXiv:2302.01560, 2023i. Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama,Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, PercyLiang, Jeff Dean, and William Fedus. Emergent abilities of large language models. Transactions on MachineLearning Research, 2022a. URL Survey Certification. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le,and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. In Alice H. Oh,Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information ProcessingSystems, 2022b. URL Hao Wen, Yuanchun Li, Guohong Liu, Shanhui Zhao, Tao Yu, Toby Jia-Jun Li, Shiqi Jiang, Yunhao Liu,Yaqin Zhang, and Yunxin Liu. Empowering llm to use smartphone for intelligent task automation. arXivpreprint arXiv:2308.15272, 2023a. Jiaxin Wen, Pei Ke, Hao Sun, Zhexin Zhang, Chengfei Li, Jinfeng Bai, and Minlie Huang. Unveiling theimplicit toxicity in large language models. In Proceedings of the 2023 Conference on Empirical Methods inNatural Language Processing, pp. 13221338, 2023b.",
  "Siye Wu, Jian Xie, Jiangjie Chen, Tinghui Zhu, Kai Zhang, and Yanghua Xiao. How easily do irrelevantinputs skew the responses of large language models? arXiv preprint arXiv:2404.03302, 2024b": "Weiqi Wu, Hongqiu Wu, Lai Jiang, Xingyuan Liu, Hai Zhao, and Min Zhang. From role-play to drama-interaction: An LLM solution. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Findings ofthe Association for Computational Linguistics ACL 2024, pp. 32713290, Bangkok, Thailand and virtualmeeting, August 2024c. Association for Computational Linguistics. doi: 10.18653/v1/2024.findings-acl.196.URL",
  "Rui Xu, Dakuan Lu, Xiaoyu Tan, Xintao Wang, Siyu Yuan, Jiangjie Chen, Wei Chu, and Xu Yinghui.Mindecho: Role-playing language agents for key opinion leaders. arXiv preprint arXiv:2407.05305, 2024b": "Rui Xu, Xintao Wang, Jiangjie Chen, Siyu Yuan, Xinfeng Yuan, Jiaqing Liang, Zulong Chen, Xiaoqing Dong,and Yanghua Xiao. Character is destiny: Can large language models simulate persona-driven decisions inrole-playing? arXiv preprint arXiv:2404.12138, 2024c. Yuzhuang Xu, Shuo Wang, Peng Li, Fuwen Luo, Xiaolong Wang, Weidong Liu, and Yang Liu. Exploringlarge language models for communication games: An empirical study on werewolf.arXiv preprintarXiv:2309.04658, 2023c.",
  "Shenghao Yang, Weizhi Ma, Peijie Sun, Qingyao Ai, Yiqun Liu, Mingchen Cai, and Min Zhang. Sequentialrecommendation with latent relations based on large language model, 2024": "Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, and Karthik Narasimhan.Tree of thoughts: Deliberate problem solving with large language models. arXiv preprint arXiv:2305.10601,2023a. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R Narasimhan, and Yuan Cao. React:Synergizing reasoning and acting in language models. In The Eleventh International Conference on LearningRepresentations, 2023b. URL",
  "Wangsong Yin, Mengwei Xu, Yuanchun Li, and Xuanzhe Liu. Llm as a system service on mobile devices,2024": "Mo Yu, Yisi Sang, Kangsheng Pu, Zekai Wei, Han Wang, Jing Li, Yue Yu, and Jie Zhou.Few-shotcharacter understanding in movies as an assessment to meta-learning of theory-of-mind. arXiv preprintarXiv:2211.04684, 2022. Mo Yu, Jiangnan Li, Shunyu Yao, Wenjie Pang, Xiaochen Zhou, Zhou Xiao, Fandong Meng, and Jie Zhou.Personality understanding of fictional characters during book reading. arXiv preprint arXiv:2305.10156,2023. Xiaoyan Yu, Tongxu Luo, Yifan Wei, Fangyu Lei, Yiming Huang, Peng Hao, and Liehuang Zhu. Neeko:Leveraging dynamic lora for efficient multi-character role-playing agent. arXiv preprint arXiv:2402.13717,2024. Siyu Yuan, Jiangjie Chen, Ziquan Fu, Xuyang Ge, Soham Shah, Charles Jankowski, Yanghua Xiao, andDeqing Yang. Distilling script knowledge from large language models for constrained language planning. InProceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: LongPapers), pp. 43034325, Toronto, Canada, July 2023. Association for Computational Linguistics. URL Siyu Yuan, Kaitao Song, Jiangjie Chen, Xu Tan, Yongliang Shen, Ren Kan, Dongsheng Li, and Deqing Yang.Easytool: Enhancing llm-based agents with concise tool instruction. arXiv preprint arXiv:2401.06201,2024a. Xinfeng Yuan, Siyu Yuan, Yuhan Cui, Tianhe Lin, Xintao Wang, Rui Xu, Jiangjie Chen, and Deqing Yang.Evaluating character understanding of large language models via character profiling from fictional works.In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, 2024b. Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rogriguez, and Krishna P Gummadi.Fairnessconstraints: Mechanisms for fair classification. In Artificial intelligence and statistics, pp. 962970. PMLR,2017.",
  "Jintian Zhang, Xin Xu, and Shumin Deng. Exploring collaboration mechanisms for llm agents: A socialpsychology view. arXiv preprint arXiv:2310.02124, 2023b": "Saizheng Zhang, Emily Dinan, Jack Urbanek, Arthur Szlam, Douwe Kiela, and Jason Weston. Personalizingdialogue agents: I have a dog, do you have pets too?In Iryna Gurevych and Yusuke Miyao (eds.),Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: LongPapers), pp. 22042213, Melbourne, Australia, July 2018. Association for Computational Linguistics. doi:10.18653/v1/P18-1205. URL Wenlin Zhang, Chuhan Wu, Xiangyang Li, Yuhao Wang, Kuicai Dong, Yichao Wang, Xinyi Dai, XiangyuZhao, Huifeng Guo, and Ruiming Tang. Tired of plugins? large language models can be end-to-endrecommenders, 2024a. Wenxuan Zhang, Hongzhi Liu, Yingpeng Du, Chen Zhu, Yang Song, Hengshu Zhu, and Zhonghai Wu. Bridgingthe information gap between domain-specific model and general llm for personalized recommendation,2023c.",
  "Runcong Zhao, Wenjia Zhang, Jiazheng Li, Lixing Zhu, Yanran Li, Yulan He, and Lin Gui. Narrativeplay:Interactive narrative understanding. arXiv preprint arXiv:2310.01459, 2023a": "Runcong Zhao, Qinglin Zhu, Hainiu Xu, Jiazheng Li, Yuxiang Zhou, Yulan He, and Lin Gui.Largelanguage models fall short: Understanding complex relationships in detective narratives. arXiv preprintarXiv:2402.11051, 2024. Yilin Zhao, Xinbin Yuan, Shanghua Gao, Zhijie Lin, Qibin Hou, Jiashi Feng, and Daquan Zhou. Chatany-thing: A framework for generating anthropomorphized personas for llm-based characters. arXiv preprintarXiv:2311.06772v1, 2023b. Mingqian Zheng, Jiaxin Pei, and David Jurgens. Is\" a helpful assistant\" the best role for large languagemodels? a systematic evaluation of social roles in system prompts. arXiv preprint arXiv:2311.10054, 2023a.",
  "Yujia Zhou, Qiannan Zhu, Jiajie Jin, and Zhicheng Dou. Cognitive personalized search integrating largelanguage models with an efficient memory mechanism, 2024c": "Andrew Zhu, Lara Martin, Andrew Head, and Chris Callison-Burch. Calypso: Llms as dungeon mastersassistants. In Proceedings of the Nineteenth AAAI Conference on Artificial Intelligence and InteractiveDigital Entertainment, AIIDE 23. AAAI Press, 2023a. ISBN 1-57735-883-X. doi: 10.1609/aiide.v19i1.27534.URL Xizhou Zhu, Yuntao Chen, Hao Tian, Chenxin Tao, Weijie Su, Chenyu Yang, Gao Huang, Bin Li, Lewei Lu,Xiaogang Wang, Yu Qiao, Zhaoxiang Zhang, and Jifeng Dai. Ghost in the minecraft: Generally capableagents for open-world environments via large language models with text-based knowledge and memory.arXiv preprint arXiv:2305.17144, 2023b.",
  "ARPLA Products": "The recent remarkable advancements in LLMs have sparked a myriad of AI applications. Persona andpersonalization are central to these applications, with their demands shaping and propelling research inRPLAs. In this section, we provide a brief overview of recent trends in RPLA applications. Specifically,we distinguish RPLAs in existing products into two categories, namely persona-oriented RPLAs andtask-oriented RPLAs, as listed in .",
  "A.1Persona-oriented RPLA Products": "Persona-oriented RPLAs typically role-play as specific characters, which has been popular in various en-tertainment applications, such as chatbots and game NPCs. These RPLAs are generally sourced fromfictional characters, historical figures or celebrities, aligning with the research trends on character persona asintroduced in 5. They are further forked for individual use cases to meet their preference. We categorizeexisting persona-oriented RPLA products based on their primary interaction focus, either human-RPLAinteractions or RPLA-RPLA interactions. Interactions between Humans and RPLAsPersona-oriented RPLAs, such as those in Character.ai,are initially applied for human-RPLA interactions. These RPLAs can be both initiated from establishedcharacters and shaped through ongoing user interactions.",
  "A.2Task-oriented RPLAs": "The remarkable advancements in LLMs have propelled significant development in AI applications for specializedtasks. In these applications, LLMs typically communicate in a human-like manner to foster user acceptance,and serve as domain experts providing personalized services for users, such as AI doctors and coaches. Theseapplications are closely related to research work in the personalization of RPLAs introduced in 6. We referto personalized agents in these products as task-oriented RPLAs. This section offers a concise overview oftask-oriented RPLAs in AI products, spanning various domains, including education, healthcare, humanresources, customer service, content creation, real estate, shopping, fitness, travel, and finance. EducationFor education, personalized agents are adopted for personalized recommendations and adaptivelearning, serving both educators and learners. For learners, RPLAs can personalize the learning journey bytailoring content and recommendations to individual learning styles and paces for optimal engagement (e.g.,Jagoda.AI, Khan Academys Khanmigo, Duolingo Max). For educators, RPLAs can alleviate administrativetasks by recommending personalized teaching materials and assessments, as well as creating multilingualinstructional content (e.g., Eduaide.Ai). Human ResourceIn human resources, RPLAs can provide tailored assistance for job seekers based ontheir profiles and interests to aid their career navigation. They offer personalized support in answeringinterview questions, career advice, and even customizing interview preparation materials (e.g., AutonomousHR Chatbot, AI Interview Coach, Careers AI, Huru AI). Real EstateLLMs have been widely adopted for content generation and recommendation in the realestate industry. They can generate blog articles and attractive descriptions and recommend a list of potentialinterests for users based on their needs. By analyzing user preferences and needs, these products can generate"
}