{
  "Abstract": "This work studies the problem of time series analysis with generalist (or foundation) models,which are models trained across many data domains. Drawing inspiration from the widespreadsuccess of large language models, we consider the simple strategy of discretely tokenizingtime series data drawn from a myriad of datasets via self-supervision, then using the xedtokenization to solve a variety of tasks across many data domains. Canonically, time seriesmodels are either trained on a single dataset or built in a task-specic manner (e.g., aforecasting-only model), where many use patches of time as inputs to the model. As such,performant generalist, discrete representation time series models explored across manytasks are of value. Our method, TOkenized Time Series EMbeddings (TOTEM), producessuch generalist time series models with minimal or no ne-tuning while exhibiting strongzero-shot performance. We evaluate TOTEM extensively over nearly 500 experiments onthree commonly-studied time series tasks with real-world data: imputation (17 baselines,12 datasets), anomaly detection (19 baselines, 25 datasets), and forecasting (14 baselines,12 datasets). We conclude that TOTEM matches or outperforms existing state-of-the-artmodels in both the canonical specialist setting (i.e., training one model on one domain)as well as the generalist setting (i.e., training a single model on many domains), whichdemonstrates the ecacy of tokenization for general time series analysis. The open-sourceimplementation is available here: a videosummary is available here:",
  "Introduction": "Time series are a fundamental data modality, generalizing large classes of time-varying data from manydomains, like weather phenomena, electrical grid activity, or trac ow. Most commonly, time series analysisis rst restricted to one such domain, then to a specic task, like imputation (Luo et al., 2018; 2019; Talukderet al., 2022), anomaly detection (Xu et al., 2021; He & Zhao, 2019), or forecasting (Wu et al., 2021; Woo et al.,2022), among others. Though these domains and tasks are quite distinct, a natural question is whether it ispossible to design domain-agnostic models adaptable to any task. This question is the subject of our work. Generalist models are those trained on many data domains simultaneously (e.g., weather, electricity, trac,etc.), while specialist models are those trained on a single time series domain (e.g., weather only), as shown inA (Zhou et al., 2023; Wu et al., 2022; Nie et al., 2022). Both generalist and specialist models can betested in two ways: in-domain testing, where a model is tested on the same domain(s) on which it was trained,and zero-shot testing, where it is tested on dierent domain(s) (see B). Performing zero-shot testingon specialist models is not uncommon. For example, some works have studied zero-shot forecasting, where aforecaster trains on one dataset then predicts on a separate dataset (Zhou et al., 2023), or trains on a subsetof channels (which we call sensors) from one dataset then forecasts zero-shot on the remaining sensors in the",
  "Z": ": Anomaly Detection Visualization.The VQVAE architecture does not change for theanomaly detection task. The training data passed in must be clean such that the VQVAE can learn cleanrepresentations. At test time, when anomaly data is passed in with anomaly A% (in this case 25%), theworst A% reconstructed is set to the anomaly. : Specialist Anomaly Detection (). TOTEM has the highest AvgWins at 33.3% followed by ave-way tie between GPT2, TiNet, ATrans, ETS, and LogTr at 13.3%. Some prior methods use the test setas a validation set for early stopping of the learning algorithm, which can inate performance. We do notadopt this practice and train TOTEM for a set number of iterations.",
  "Related Work": "We categorize related works in three ways: whether they (i) study specialists or generalists, (ii) use patchedor discrete data representations, and (iii) train and evaluate models for multiple distinct time series tasks.Unlike TOTEM, no prior works study the use of discrete data representations for training generalists acrossmultiple tasks (see for a comparison). Specialist vs. Generalist Training. Historically, the specialist (i.e., single-domain) training paradigm ismost common amongst time series models (Zhou et al., 2023; Wu et al., 2022; Nie et al., 2022; Zhang & Yan,2022). These specialist models are primarily evaluated via in-domain testing, where the test set is a held-outset from the same training domain. Some concurrent and subsequent works have begun exploring generalisttime series foundation models, including forecasters from Google and Amazon (Das et al., 2023b; Ansariet al., 2024). We compare to the concurrent MOMENT model (Goswami et al., 2024) in limited evaluations(see Tables 13 and 20) as it also studies multiple tasks, and nd that TOTEM generally outperforms it. Patched vs. Discrete Data Representations. In order to pass time series data to a downstream model,it is necessary to choose some latent data representation. As in ViTs (Dosovitskiy et al., 2020), the prevailingstrategy is to patch time series data, either temporally (Liu et al., 2023; Zhang & Yan, 2022; Nie et al., 2022)or spatially Li et al. (2019); Zhou et al. (2021); Wu et al. (2021), then to linearly project the patches to somelatent embedding on which a model like a transformer or MLP can operate. We emphasize that patchedrepresentations are dynamic in the sense that the embedding associated with the patch is determined entirelyby the layers in the downstream model which project the patches to the embedding space. Therefore, patchedrepresentations are trained end to end. Patching is fundamentally at odds with tokenization, wherein a xed vocabulary of embeddings is deter-mined before training the downstream model, which then operates on the xed, tokenized representations.Tokenization (learned or otherwise) has been leveraged for training models in elds like language and visionmodeling (Gage, 1994; Radford et al., 2018; Van Den Oord et al., 2017; Esser et al., 2021; Rombach et al.,2022). Some prior work in time series modeling has explored discrete representations using binning (Rabanseret al., 2020b;a; Lin et al., 2007) or quantization (Baevski et al., 2020; Van Den Oord et al., 2017; Oord et al.,2016) in domain- or task-specic ways. Inspired by the success of vector quantized variational autoencoders(VQVAEs) in both audio and vision (Van Den Oord et al., 2017; Esser et al., 2021; Rombach et al., 2022), webuild on these works by showing that the VQVAE is also eective for learning discrete representations forgeneral time series modeling.",
  ": Related Work Overview. TOTEM is designed for generalist training using discrete tokenizationfor any task. No prior and concurrent/subsequent (C/S) works study all three at once": "Time Series Tasks. Prior works on time series modeling study a variety of tasks, like forecasting, anomalydetection, imputation, and classication. Many prior and concurrent works focus on a single task (Zhang& Yan, 2022; Nie et al., 2022; Xu et al., 2021; Ansari et al., 2024; Das et al., 2023b), with a few exploringmultiple specialist trained models on many tasks (Zhou et al., 2023; Wu et al., 2022). TOTEM is most closelyrelated to concurrent works like MOMENT (Goswami et al., 2024), which are focused on generalist modelswhich are eective on any one of the above tasks. For detail on each task, see Sections 3 and 4.",
  "Task Denitions": "This work considers three tasks: imputation, anomaly detection, and forecasting. In imputation, models intakea masked time series xm RSTin and impute the missing values to recover the reconstruction x RSTin.In anomaly detection, models intake a time series corrupted at a known level xcorr RSTin and predictwhich times are anomalous, y {0, 1}Tin. Lastly, in forecasting, models intake a time series x RSTin andpredict future values y RSTout, where Tin and Tout signify the durations of the preceding and succeedingtime series, respectively. A core design goal for TOTEM is to learn a representation suitable for any of thesethree tasks using the same architecture and without leveraging any task- or domain-specic knowledge.",
  "This section discusses TOTEMs key design features: a self-supervised training stage, exclusively-temporaltokenization, and no domain-specic data engineering": "Self-supervised Tokenizer Training. As described in , TOTEM learns a xed codebook of tokensover a multi-domain corpus of time series data independently from the training of any downstream model.This disentangles the choice of data representation from the choice of task-specic architecture and permitsthe learning of representations from a large, diverse set of data, which aids in zero-shot generalization. First, we elect to use a discrete, deterministic encoder to produce time series tokens. This decision is largelymotivated by large language models (and in particular, tokenization methods in NLP like byte pair encoding(BPE) (Gage, 1994; Radford et al., 2018)), in which a downstream model learns on a nite number of distincttokens. Moreover, in methods like BPE, the tokenization operation is lossless and reversible because it isdeterministic (though non-unique). This suggests that vector quantization-based models could be eectivefor tokenizing time series data. Two popular vector quantization methods are VQVAEs (Van Den Oord et al.,2017) and VQGANs (Esser et al., 2021). In this work, we choose to use a VQVAE, as VQGANs are morecommonly used for encoding images. Moreover, the use of VQVAEs has been studied in neural audio models(Oord et al., 2016; Van Den Oord et al., 2017), including followup works with audio-specic models (Baevskiet al., 2020), which suggests that they may be eective for modeling general time series. : Left. Specialist models can tok-enize along any of the E, S, or T dimensions.Right. Generalist models can only tok-enize along T, since the learned tokenizationmust apply to a diverse set of domains withany possible data dimensionality. Exclusively-Temporal Tokenization. A time series datasetconsists of E examples, S sensor channels, and T time steps,and can be formally expressed as {xj}Ej=1 RST . Prior workcommonly patches along either the sensor dimension (Li et al.,2019; Zhou et al., 2021; Wu et al., 2021; Liu et al., 2021), ortime dimension (Liu et al., 2023; Zhang & Yan, 2022; Nie et al.,2022). When training specialists, it is reasonable to tokenizeacross any combination of these or the example dimension (e.g.,in neuroscience data, it is common to group recordings by day,where the subject exhibits dierent behavior on a daily basis(Talukder et al., 2022)). However, in the generalist case, because the sensors associatedwith each domain have distinct semantic meanings, perform-ing sensor- or example-wise tokenization will capture domain-specic relations, hindering the tokenizers generalization. Thus,we choose to exclusively tokenize over the temporal dimension,such that the tokens represent univariate waveforms. Further,this is crucial for testing the tokenizer in the zero-shot regime, where the dimensionality of the testing domainmay dier signicantly from that of the training domain(s). Specically, TOTEM tokenizes time series datawith non-overlapping temporal chunks of length T/F, where F is some compression factor for downsamplingthe data.",
  "Published in Transactions on Machine Learning Research (12/2024)": ": Extra Anomaly Detection (). We present the the Adj. F1 metric the table (higher is better),then calculate the AvgWins . The selection criteria for the 15 datasets from (Wu & Keogh, 2021; Goswamiet al., 2024) was the following. First, based only on the names in (Goswami et al., 2024), it was oftenambiguous which data le was used. In these cases, we excluded the dataset. Second, we had dicultyverifying whether the default train/val/test ratios specied in the (Goswami et al., 2024) code matched whatwas reported. We found for the majority of datasets that the defaults resulted in test sets with no anomalies,when anomalies should be present. These were also excluded. From the results we could obtain, TOTEMmatches or beats all other methods.",
  ": TOTEM attens the sensor andexample dimensions and learns a discreterepresentation along the time dimension ina normalized space": "Though TOTEM is a VQVAE, the design of the encoder anddecoder dier substantially from the original model and similarworks like WaveNet, which use dilated convolutions (Oord et al.,2016; Van Den Oord et al., 2017).The dilations in thesearchitectures skip many time steps, allowing the convolutionallters to operate on a larger input area at a coarser scale,improving model eciency. However, this design decision ismotivated by the high sampling rates of digital audio waveforms,which is not a universal trait across time series domains (see ). In contrast, TOTEM uses a stack ofstrided 1D convolutions with a dilation of 1 such that it can account for every time step. Using a long input(e.g., 96 time steps for standard forecasting tasks) allows TOTEM to maintain a large receptive eld. Lastly,the use of RevIN allows TOTEM to remain eective by only learning a small set of normalized waveforms,and if the unnormalized reconstruction is required for a downstream task, the normalization parameters canalso be passed to the decoder (see ). Formally, TOTEM accepts a batch of univariate time series {xi RT }ESi=1 obtained by attening the sensorchannel of the multivariate data.An encoder E consisting of a stack of strided 1D convolutions thentemporally compresses the data by a total factor of F to recover a latent variable z = E(x) RT/FD,where D is the latent feature dimension. The latent variable z is then quantized into an element z of thecodebook C = {ci}Ki=1 consisting of K D-dimensional codewords ci RD following the relation z = c, where = arg mini ||z ci||22. The decoder D mirrors the encoders architecture, mapping the quantized embeddingz to a reconstructed time series x = D(z) RT .",
  "Forecasting Model Implementation": "In contrast with prior works, TOTEM is capable of solving the imputation and anomaly detectiontasks with the tokenizer alone (see Figures 11 and 12). Therefore, the only downstream model wemust design is the forecasting model. First, each sensors observations xi RTin are converted into asequence of Tin/F discrete tokens zi. The forecaster processes adds temporal positional embeddings tothese tokens, passing them through a transformer encoder consisting of a series of multi-head attentionlayers that attend along the time dimension to predict normalized measurements yi RTout for i = 1, ..., S.",
  "Experimental Setup": "This section explains the experimental setup for eachtask, including the baselines and datasets used forevaluation. The results and analyses are presented in. We compare to two families of approaches:methods designed for multiple tasks (multi-task),like TOTEM, and methods designed for a specic task (single-task). Many single-task methods havefrequently been adapted by others to tasks besides the ones for which they were originally designed, and inthose cases, we compare against the best reported results for the adapted model. For all tasks, we trained aGPT2 generalist baseline from scratch, and for forecasting, we additionally trained a GPT2 specialist.",
  "Imputation": "In imputation, models intake a masked time series xm RSTin, and then impute the signal x RSTin (see). We experiment with four canonical masking percentages at 12.5%, 25%, 37.5%, 50%, and reportthe resulting MSE and MAE. Specialists. A and D compare TOTEM to specialist baselines. All models are trained andevaluated on the same dataset (in-domain). TOTEM has the highest AvgWins with 52.1%, followed by GPT2at 35.4%, and TiNet at 18.8%. TOTEMs performance on m1 and h1 is lower, but since these datasets arethe minute and hour resampling of the same raw data respectively, we expect their results to be correlated.",
  "Baselines. In the main text, we compare TOTEM against 16 baselines, and in the Appendix A.3 anadditional 3 for a total of 19 baselines (see ). See A for a summary": "Datasets. For the in-domain testing regime, we test on 5 datasets, and for the zero-shot regime, we test onanother 5. For additional signal, we also test on 15 distinct anomaly detection datasets from Wu & Keogh(2021) in Appendix A.3 (see ). In total, we evaluate on 25 datasets. See B for a summary.",
  "Forecasting": "In forecasting, models intake a time series x RSTin and predict future readings y RSTout, where S is thesensor dimension and Tin, Tout signify the durations of the preceding and succeeding time series, respectively.All models have a lookback window of Tin = 96, with prediction lengths Tout = {96, 192, 336, 720}. Resultsfor baselines are from Liu et al. (2023). We run GPT2 with Tin = 96 as Zhou et al. (2023) originally useinconsistent dataset-specic lookback lengths. See for a summary. Specialists. and show that TOTEM achieves the highest AvgWins at 28.6% followedby iTrans at 26.8%. In particular, TOTEM has rst-place nishes in ve datasets while iTrans rst-placenishes are concentrated in only the electricity and trac datasets. Generalists. and compare the generalist-trained TOTEM and GPT2 models. TOTEMoutperforms GPT2 in both the in-domain (67.9% vs. 33.9%) and zero-shot (90.0% vs. 12.5%) regimes.TOTEMs AvgWins across both regimes show that tokens are a performant representation for forecasting.",
  "Task Selection": "In the time series literature, there are ve canonically studied tasks: imputation, anomaly detection, short-and long-term forecasting, and classication. In this work, we study imputation, anomaly detection, andlong-term forecasting. We exclude short-term forecasting and classication for the following reasons. Non-Standardized Baselines. The long-term forecasting task uses standardized input and output lengthsacross all datasets (in particular an input length of 96 timesteps and output lengths of 96, 192, 336, and 720timesteps), as enforced by a large body of existing work Liu et al. (2023); Wu et al. (2022); Liu et al. (2022b);Zhou et al. (2022) among others2. This allows us to fairly baseline TOTEM without rerunning thousands ofexperiments on dozens of models trained from scratch. In contrast, the short-term forecasting task typically uses non-standard and dataset-specic input and outputdimensionalities (see for details), which makes systematic, fair comparisons of TOTEM against priorworks extremely challenging in the generalist setting3. Thus, we exclude short-term forecasting from ourmain results. Leaky Baselines. In both classication and anomaly detection, the modern SOTA baselines are leaky (Zhouet al., 2023; Wu et al., 2022; Xu et al., 2021), where leakage is dened as using the test set as the validationset during training. In particular, the cited works that report SOTA results all use models that were trainedwith either early stopping or with the best model checkpoint on the validation (i.e., the test) set. We feltstrongly that we should not propagate faulty baselines, so we did not compare to these models in our work.Subsequent to the initial release of this paper, followup works have demonstrated on neural classicationtasks that TOTEM, when compared to baselines trained in a non-leaky manner, achieves SOTA performance(Chau et al., 2024a;b). For anomaly detection, the benchmark datasets used by Zhou et al. (2023); Wu et al. (2022); Xu et al. (2021)contain numerous aws besides training leakage awed (see Wu & Keogh (2021) for a detailed account).However, since Wu & Keogh (2021) released a large set of new, unawed benchmarks, we elected to compareTOTEM to both the awed and a subset of the unawed baselines (see the comparisons to Wu & Keogh(2021) in the Appendix). Because we nd that TOTEM convincingly achieves SOTA performance in bothcases, we report our results to establish an unawed baseline for future comparison.",
  "Main Results": "The primary goal of our experiments is to systematically evaluate TOTEM on multiple tasks simultaneouslyagainst new generalist benchmarks and strong specialist baselines (i.e., models trained on data from manydomains versus one domain). In particular, for each task, we report evaluations against (i) specialists onthe in-domain testing regime, (ii) generalists on the in-domain regime, and (iii) generalists on the zero-shotregime. We emphasize that no domain, sampling rate, or sensor dimension is shared between the trainingsets and zero-shot testing sets (see for additional dataset details). Throughout the main text, we report summary results. The full numerical results can be found throughoutthe Appendix. Moreover, all results are reported as the mean of 3 seeded runs, with standard deviationsavailable in the Appendix. Since evaluation metrics dier across tasks, () will denote a metric where loweris better and () will denote a metric where higher is better. Given the varied metrics, we calculate theaverage number of best results, or AvgWins, for each method and highlight the best method. For a summaryof training and testing domains, see ; for a comparison of generalist parameter counts and trainingtimes, see Section A.8; for additional architecture and training details, see Sections A.11 and A.12. 2Some methods utilize a 512-dimensional input, which makes consistent comparisons challenging; despite this eld-wideinconsistency, we include some of these results in Appendix 20. TOTEM outperforms other methods across lookback lengths96, 512 at 58.3% AvgWins , while the next best model is GPT2 at 8.3% AvgWins .3Despite this, we show that TOTEM and GPT2 outperform all other methods on a small set of short term forecasting lengthsand datasets in Appendix 18.",
  "W": "960.165 0.208 0.184 0.224 0.172 0.220 0.174 0.214 0.177 0.218 0.158 0.230 0.217 0.296 0.173 0.223 0.202 0.261 0.192 0.232 0.196 0.255 0.221 0.3061920.207 0.250 0.231 0.263 0.219 0.261 0.221 0.254 0.225 0.259 0.206 0.277 0.276 0.336 0.245 0.285 0.242 0.298 0.240 0.271 0.237 0.296 0.261 0.3403360.257 0.291 0.285 0.302 0.280 0.306 0.278 0.296 0.278 0.297 0.272 0.335 0.339 0.380 0.321 0.338 0.287 0.335 0.292 0.307 0.283 0.335 0.309 0.3787200.326 0.340 0.362 0.351 0.365 0.359 0.358 0.349 0.354 0.348 0.398 0.418 0.403 0.428 0.414 0.410 0.351 0.386 0.364 0.353 0.345 0.381 0.377 0.427",
  "E": "960.178 0.263 0.186 0.272 0.168 0.272 0.148 0.240 0.195 0.285 0.219 0.314 0.193 0.308 0.169 0.273 0.237 0.329 0.201 0.281 0.197 0.282 0.247 0.3451920.187 0.272 0.190 0.278 0.184 0.289 0.162 0.253 0.199 0.289 0.231 0.322 0.201 0.315 0.182 0.286 0.236 0.330 0.201 0.283 0.196 0.285 0.257 0.3553360.199 0.285 0.204 0.291 0.198 0.300 0.178 0.269 0.215 0.305 0.246 0.337 0.214 0.329 0.200 0.304 0.249 0.344 0.215 0.298 0.209 0.301 0.269 0.3697200.236 0.318 0.245 0.324 0.220 0.320 0.225 0.317 0.256 0.337 0.280 0.363 0.246 0.355 0.222 0.321 0.284 0.373 0.257 0.331 0.245 0.333 0.299 0.390",
  "m1": "960.320 0.347 0.328 0.363 0.338 0.375 0.334 0.368 0.329 0.367 0.404 0.426 0.379 0.419 0.386 0.398 0.364 0.387 0.355 0.376 0.345 0.372 0.418 0.4381920.379 0.382 0.368 0.382 0.374 0.387 0.377 0.391 0.367 0.385 0.450 0.451 0.426 0.441 0.459 0.444 0.398 0.404 0.391 0.392 0.380 0.389 0.439 0.4503360.406 0.402 0.400 0.404 0.410 0.411 0.426 0.420 0.399 0.410 0.532 0.515 0.445 0.459 0.495 0.464 0.428 0.425 0.424 0.415 0.413 0.413 0.490 0.4857200.471 0.438 0.462 0.440 0.478 0.450 0.491 0.459 0.454 0.439 0.666 0.589 0.543 0.490 0.585 0.516 0.487 0.461 0.487 0.450 0.474 0.453 0.595 0.550",
  "m2": "960.176 0.253 0.178 0.263 0.187 0.267 0.180 0.264 0.175 0.259 0.287 0.366 0.203 0.287 0.192 0.274 0.207 0.305 0.182 0.265 0.193 0.292 0.286 0.3771920.247 0.302 0.245 0.307 0.249 0.309 0.250 0.309 0.241 0.302 0.414 0.492 0.269 0.328 0.280 0.339 0.290 0.364 0.246 0.304 0.284 0.362 0.399 0.4453360.317 0.348 0.307 0.346 0.321 0.351 0.311 0.348 0.305 0.343 0.597 0.542 0.325 0.366 0.334 0.361 0.377 0.422 0.307 0.342 0.369 0.427 0.637 0.5917200.426 0.410 0.410 0.409 0.408 0.403 0.412 0.407 0.402 0.400 1.730 1.042 0.421 0.415 0.417 0.413 0.558 0.524 0.407 0.398 0.554 0.522 0.960 0.735",
  "h1": "960.380 0.394 0.379 0.397 0.384 0.402 0.386 0.405 0.414 0.419 0.423 0.448 0.376 0.419 0.513 0.491 0.479 0.464 0.386 0.395 0.386 0.400 0.654 0.5991920.434 0.427 0.438 0.427 0.436 0.429 0.441 0.436 0.460 0.445 0.471 0.474 0.420 0.448 0.534 0.504 0.525 0.492 0.437 0.424 0.437 0.432 0.719 0.6313360.490 0.459 0.474 0.448 0.491 0.469 0.487 0.458 0.501 0.466 0.570 0.546 0.459 0.465 0.588 0.535 0.565 0.515 0.479 0.446 0.481 0.459 0.778 0.6597200.539 0.513 0.496 0.475 0.521 0.500 0.503 0.491 0.500 0.488 0.653 0.621 0.506 0.507 0.643 0.616 0.594 0.558 0.481 0.470 0.519 0.516 0.836 0.699",
  "h2": "960.293 0.338 0.295 0.348 0.340 0.374 0.297 0.349 0.302 0.348 0.745 0.584 0.358 0.397 0.476 0.458 0.400 0.440 0.288 0.338 0.333 0.387 0.707 0.6211920.375 0.390 0.384 0.402 0.402 0.414 0.380 0.400 0.388 0.400 0.877 0.656 0.429 0.439 0.512 0.493 0.528 0.509 0.374 0.390 0.477 0.476 0.860 0.6893360.422 0.431 0.418 0.432 0.452 0.452 0.428 0.432 0.426 0.433 1.043 0.731 0.496 0.487 0.552 0.551 0.643 0.571 0.415 0.426 0.594 0.541 1.000 0.7447200.610 0.567 0.423 0.446 0.462 0.468 0.427 0.445 0.431 0.446 1.104 0.763 0.463 0.474 0.562 0.560 0.874 0.679 0.420 0.440 0.831 0.657 1.249 0.838",
  ": Imputation Summary. In all categories TOTEM has SOTA AvgWins . In the specialist TOTEMhas 52.1% AvgWins ; in generalist in domain TOTEM has 58.3%; in generalist zero shot TOTEM has 80.0%": "Since we only use 3 seeds, we run a non-parametric permutation test on the generalist models in AppendixA.6 to analyze the performance of TOTEM vs. GPT2 (), and TOTEM vs. PatchTOTEM (). We nd that TOTEM statistically signicantly (p 0.05) outperforms GPT2 in terms of AvgWinson all tasks for both the in-domain and zero-shot testing paradigms. Additionally, TOTEM outperformsPatchTOTEM in a statistically signicant (p 0.05) manner for in-domain and zero-shot testing.",
  ": Anomaly Detection Results. In all cases,TOTEM has SOTA AvgWins. Vs. specialists, TOTEMhas 33.3%; vs. generalists in-domain, TOTEM has80.0%; vs. generalists zero-shot, TOTEM has 73.3%": "In anomaly detection, models intake a corrupted timeseries xcorr RSTin and predict which times corre-spond to anomalies via a binary mask y {0, 1}Tin,where the amount of corruption is considered known,at A% (see ). We report Precision P (),Recall R (), and F1 Score (). In the main text,we compare against the awed baselines from priorwork (see .4) for ease of comparison. Wecompare against a subset of 15 correct baselinesfrom Wu & Keogh (2021) in . In both cases,we nd that TOTEM achieves SOTA results.",
  ":Discrete Token Ablation.Inall categories, the discrete token representation(TOTEM) has SOTA AvgWins over the patchrepresentation (PatchTOTEM)": "Tokens vs. Patches. The experiments in show that the combination of discrete tokenization andTOTEMs generalist architecture achieve SOTA performance. We now x the architecture while varyingonly the representation (TOTEM vs. PatchTOTEM) on a forecasting task to test what proportion of theperformance is attributable to tokenization. We nd that in all testing regimes used in the main results,TOTEM greatly outperforms PatchTOTEM, with 67.9% vs. 39.3% AvgWins in the specialist in-domainregime, 78.6% vs. 23.2% AvgWins in the generalist in-domain regime, and 67.5% vs. 35.0% AvgWins in thegeneralist zero-shot regime (see and ).",
  ": Discrete Token vs. Patcheswith MLP. For both the transformer (left)and MLP (right) the discrete token repre-sentation (TOTEM) outperforms the patchrespresentation (PatchTOTEM)": "Downstream Architecture Study. In & ,we explore the eect of discrete tokens vs. patches for each oftwo common downstream forecasting models: the transformerencoder introduced in .3 and , and an MLP(Ekambaram et al., 2023; Das et al., 2023a; Zeng et al., 2023).The MLP has 3-layers ReLU activations, uses dropout withp = 0.1 after the second layer, and concludes with a layernorm;this architecture is modeled after similar architectures in theliterature like Das et al. (2023a). The patch-based MLP takesin an uncompressed time series. We nd that for both the MLPand transformer architectures, the discrete token representationoutperforms the patch representation (in the transformer 67.9%to 39.3% AvgWins and MLP 66.1% to 37.5% AvgWins). Thisshows that TOTEMs strength in forecasting is not due to thestrength of the transformer forecaster, but because of the choiceto use discrete tokens. Codebook Size. In , we explore the eect of thecodebook size K on the VQVAEs reconstruction performance.As expected, we nd that as K increases from 32 to 256 to512, the reconstruction performance improves. However, fordownstream tasks like forecasting, it is more parsimonious to model interactions between fewer codewords.Thus, we elect to use K = 256 codewords, as the reconstruction performance is similar to that of K = 512.We note that the the average generalist codebook error (see D), is substantially lower than thecorresponding downstream forecasting error, demonstrating that a larger proportion of error is attributableto the diculty of the forecasting task rather than poor reconstruction. This provides evidence that time",
  "series can have a single unied representation across multiple domains, akin to BPE in language modeling.We note that this same trend holds for the specialist models as well": "Dataset Size Study. One natural question is whether TOTEMs strong generalization performance isdriven by the size of the dataset or the diversity of the training samples. We study this in a minimal settingby comparing the TOTEM generalist model against two TOTEM specialists trained on the two largestdomain-specic datasets: trac (10.2M examples) and electricity (5.8M examples). As expected, the resultsin show that the TOTEM generalist signicantly outperforms the two specialists in the zero-shotsetting. However, the electricity specialist outperformed the trac specialist even though the training datasetwas about half the size. This provides some preliminary evidence that simply training on more data isinsucient for achieving generalization - the types of data are also crucial. For related exploratory studies ongeneralist models, see Appendix A.7.",
  "Conclusion": "We present TOTEM: a simple, performant tokenizer that is designed to learn domain-agnostic discreterepresentations for time series data, paving the way for time series foundation models. TOTEM demonstratesstrong in-domain and zero-shot capabilities versus a large array of both generalist and specialist baselinesacross dozens of domains and datasets over hundreds of seeded experiments. Overall, TOTEM unlocksdomain generalization while performing at or above existing SOTA levels, demonstrating the potential ofadopting training and modeling techniques from language and vision modeling for time series modeling. There are many exciting directions for future work. First, our proposed architectural design decisions werevery simple, which suggests that there are many possible performant extensions. Further, while we havecollated millions of existing time series, TOTEMs promising initial results suggest that scaling up thegeneralist training dataset size by an order of magnitude or more could unlock true domain- and task-agnosticgeneralizability. Such followup works could allow a more systematic study of the relationships betweengeneralist data representations, token length, data size, and domain diversity.",
  "Broader Impact Statement": "There are no immediate ethical concerns that arise from our work. However, as with all data driven methods,certain societal consequences are important to be discussed, in this case surrounding time series modeling. Afew are reported below: Privacy Concerns. Time series data, especially when sourced from personal devices or applications, cancontain sensitive information about individuals, e.g. for health domains. In this work, no time series weresourced from personal devices. Misuse. Time series forecast models can be misused. For instance, if a model forecasts stock prices ormarket movements, it could be exploited for insider trading or other illegal nancial activities. In this work,we are focused on domains pertinent to scientic disciplines. Economic Impacts. Automated forecasts and decisions based on time series models can signicantly impactindustries and labor markets both positively and negatively. For instance, if a model can accurately predictweather patterns, it might aect farmers and their crop decisions, or if it can forecast energy consumption, itcould impact the energy sector.",
  "Wei Cao, Dong Wang, Jian Li, Hao Zhou, Lei Li, and Yitan Li. Brits: Bidirectional recurrent imputation fortime series. Advances in neural information processing systems, 31, 2018": "Cristian I Challu, Peihong Jiang, Ying Nian Wu, and Laurent Callot. Deep generative model with hierarchicallatent factors for time series anomaly detection. In International Conference on Articial Intelligence andStatistics, pp. 16431654. PMLR, 2022. Geeling Chau, Yujin An, Ahamed Raey Iqbal, Soon-Jo Chung, Yisong Yue, and Sabera Talukder. General-izability under sensor failure: Tokenization+ transformers enable more robust latent spaces. arXiv preprintarXiv:2402.18546, 2024a. Geeling Chau, Christopher Wang, Sabera Talukder, Vighnesh Subramaniam, Saraswati Soedarmadji, YisongYue, Boris Katz, and Andrei Barbu. Population transformer: Learning population-level representations ofintracranial activity. ArXiv, 2024b.",
  "Abhimanyu Das, Weihao Kong, Rajat Sen, and Yichen Zhou. A decoder-only foundation model for time-seriesforecasting. arXiv preprint arXiv:2310.10688, 2023b": "Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words:Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. Vijay Ekambaram, Arindam Jati, Nam Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam. Tsmixer:Lightweight mlp-mixer model for multivariate time series forecasting. In Proceedings of the 29th ACMSIGKDD Conference on Knowledge Discovery and Data Mining, pp. 459469, 2023. Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis.In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1287312883,2021.",
  "Yangdong He and Jiabao Zhao. Temporal convolutional networks for anomaly detection in time series. InJournal of Physics: Conference Series, volume 1213, pp. 042050. IOP Publishing, 2019": "Peter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup, and David Meger. Deepreinforcement learning that matters. In Proceedings of the AAAI conference on articial intelligence,volume 32, 2018. Taesung Kim, Jinhee Kim, Yunwon Tae, Cheonbok Park, Jang-Ho Choi, and Jaegul Choo. Reversible instancenormalization for accurate time-series forecasting against distribution shift. In International Conference onLearning Representations, 2021.",
  "Jessica Lin, Eamonn Keogh, Li Wei, and Stefano Lonardi. Experiencing sax: a novel symbolic representationof time series. Data Mining and knowledge discovery, 15:107144, 2007": "Minhao Liu, Ailing Zeng, Muxi Chen, Zhijian Xu, Qiuxia Lai, Lingna Ma, and Qiang Xu. Scinet: Timeseries modeling and forecasting with sample convolution and interaction. Advances in Neural InformationProcessing Systems, 35:58165828, 2022a. Shizhan Liu, Hang Yu, Cong Liao, Jianguo Li, Weiyao Lin, Alex X Liu, and Schahram Dustdar. Pyraformer:Low-complexity pyramidal attention for long-range time series modeling and forecasting. In Internationalconference on learning representations, 2021.",
  "Boris N Oreshkin, Dmitri Carpov, Nicolas Chapados, and Yoshua Bengio. N-beats: Neural basis expansionanalysis for interpretable time series forecasting. arXiv preprint arXiv:1905.10437, 2019": "Steven M Peterson, Satpreet H Singh, Benjamin Dichter, Michael Scheid, Rajesh PN Rao, and Bingni WBrunton. Ajile12: Long-term naturalistic human intracranial neural recordings and pose. Scientic data, 9(1):184, 2022. S Rabanser, T Januschowski, V Flunkert, D Salinas, and J Gasthaus. The eectiveness of discretization inforecasting: An empirical study on neural time series models. arxiv 2020. arXiv preprint arXiv:2005.10111,2020a. Stephan Rabanser, Tim Januschowski, Valentin Flunkert, David Salinas, and Jan Gasthaus. The eective-ness of discretization in forecasting: An empirical study on neural time series models. arXiv preprintarXiv:2005.10111, 2020b.",
  "Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understandingby generative pre-training. 2018": "Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjrn Ommer. High-resolutionimage synthesis with latent diusion models. In Proceedings of the IEEE/CVF conference on computervision and pattern recognition, pp. 1068410695, 2022. David Salinas, Valentin Flunkert, Jan Gasthaus, and Tim Januschowski. Deepar: Probabilistic forecastingwith autoregressive recurrent networks. International Journal of Forecasting, 36(3):11811191, 2020. Ikaro Silva, George Moody, Daniel J Scott, Leo A Celi, and Roger G Mark. Predicting in-hospital mortalityof icu patients: The physionet/computing in cardiology challenge 2012. In 2012 computing in cardiology,pp. 245248. IEEE, 2012. Sabera Talukder, Jennifer J Sun, Matthew Leonard, Bingni W Brunton, and Yisong Yue. Deep neuralimputation: A framework for recovering incomplete brain recordings. arXiv preprint arXiv:2206.08094,2022. Yusuke Tashiro, Jiaming Song, Yang Song, and Stefano Ermon. Csdi: Conditional score-based diusionmodels for probabilistic time series imputation. Advances in Neural Information Processing Systems, 34:2480424816, 2021.",
  "Ailing Zeng, Muxi Chen, Lei Zhang, and Qiang Xu. Are transformers eective for time series forecasting? InProceedings of the AAAI conference on articial intelligence, volume 37, pp. 1112111128, 2023": "Tianping Zhang, Yizhuo Zhang, Wei Cao, Jiang Bian, Xiaohan Yi, Shun Zheng, and Jian Li. Less is more:Fast multivariate time series forecasting with light sampling-oriented mlp structures. arXiv preprintarXiv:2207.01186, 2022. Yunhao Zhang and Junchi Yan. Crossformer: Transformer utilizing cross-dimension dependency for multi-variate time series forecasting. In The Eleventh International Conference on Learning Representations,2022. Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai Zhang. Informer:Beyond ecient transformer for long sequence time-series forecasting. In Proceedings of the AAAI conferenceon articial intelligence, volume 35, pp. 1110611115, 2021. Tian Zhou, Ziqing Ma, Qingsong Wen, Xue Wang, Liang Sun, and Rong Jin. Fedformer: Frequency enhanceddecomposed transformer for long-term series forecasting. In International Conference on Machine Learning,pp. 2726827286. PMLR, 2022.",
  "T": "960.523 0.303 0.471 0.311 0.593 0.321 0.395 0.268 0.544 0.359 0.522 0.290 0.587 0.366 0.612 0.338 0.805 0.493 0.649 0.389 0.650 0.396 0.788 0.4991920.530 0.303 0.479 0.312 0.617 0.336 0.417 0.276 0.540 0.354 0.530 0.293 0.604 0.373 0.613 0.340 0.756 0.474 0.601 0.366 0.598 0.370 0.789 0.5053360.549 0.311 0.490 0.317 0.629 0.336 0.433 0.283 0.551 0.358 0.558 0.305 0.621 0.383 0.618 0.328 0.762 0.477 0.609 0.369 0.605 0.373 0.797 0.5087200.598 0.331 0.524 0.336 0.640 0.350 0.467 0.302 0.586 0.375 0.589 0.328 0.626 0.382 0.653 0.355 0.719 0.449 0.647 0.387 0.645 0.394 0.841 0.523",
  "A.6Statistical Signicance": "Despite the fact that much prior and concurrent work only reports results on 1 seed Wu et al. (2022); Goswamiet al. (2024) and Zhou et al. (2023) (except for ), we perform a statistical analysis on our generalistresults. Our results are statistically signicant with 3 seeds using an exact one-sided permutation test. The exactone-sided permutation test repeatedly randomly permutes the reported metrics (e.g., MSE) between twocompeting methods (e.g., TOTEM vs.GPT2) and generates a distribution of all the possible metricassignments. This is an appropriate test because it returns a p-value indicating how rare it is to observe ourreported results relative to all the permuted outcomes. Importantly, this test is non-parametric, so it is validin a low-sample regime. The reason we expect this statistic to return p<=0.05, even with 3 seeds, is becausethe training algorithms in this setting are actually quite stable (unlike other areas of ML such as deep RL(Henderson et al., 2018)). We perform this analysis on the generalist models for each experimental trial/metric (e.g., for the MSE metricof the Weather dataset in the forecasting task), for all tasks (where we compare TOTEM vs. GPT2), and forthe token vs. patch analysis (where we compare TOTEM vs. PatchTOTEM). The following tables report the proportion of trials where the p-value is <= 0.05, i.e., where TOTEMstatistically signicantly outperforms the competing method (GPT2 24 or PatchTOTEM 25). While theresults in the main paper double-count the ties between method (e.g., if TOTEM and GPT2 tied, a win wascounted for both) to prove the strength of TOTEM, the tables below compare the percentage of experimentsin which TOTEM strictly outperforms the baseline (we also provide the reported win percentage from themain paper for ease of comparison). It is clear that even in the statistical setting, TOTEM still wins morethan baselines. : TOTEM vs. GPT2 Generalist Statistical Signicance. Here we compare the TOTEM andGPT2 generalists and calculate the proportion of trials where the p-value is <= 0.05, i.e., where TOTEMstatistically signicantly outperforms GPT2 (right column). These results are in line with those reported inthe main paper.",
  "(as in 5, 5, 6)no tiesno ties": "Imputation In Domain58.3%56.3%56.3%Imputation Zero Shot80.0%80.0%80.0%Anomaly Detection In Domain80.0%80.0%66.6%Anomaly Detection Zero Shot73.3%73.3%73.3%Forecasting In Domain67.9%66.1%62.5%Forecasting Zero Shot90.0%87.5%82.5% : Tokens vs. Patches Generalist Statistical Signicance. Here we compare the TOTEM andPatchTOTEM generalists and calculate the proportion of trials where the p-value is <= 0.05, i.e., whereTOTEM statistically signicantly outperforms PatchTOTEM (right column). These results are in line withthose reported in the main paper.",
  "A.7Further Exploration Details": "Generalist Codebooks. To further explore the capabilities of a generalist codebook data representationwe train models that utilize a general codebook but dataset-specic transformer forecasters, i.e., a TOTEMVQVAE trained on multiple domains with a forecaster trained only on electricity, . We comparethese mixed models to generalist and specialist models trained on the same domains. All models use the same",
  "codebook hyperparameters (number of codewords K = 256, compression factor F = 4, code dimensionalityD = 64) as well as the forecaster transformer architecture to ensure a fair comparison": "Since we are evaluating specialists, mixed-models, and a generalist on in-domain test data, one might expectthe TOTEM specialists to signicantly outperform all models in all domains. Surprisingly, this intuition is notcorrect. We nd that the fully-generalist model (right ) signicantly outperforms the mixed-models(middle ) in trac (T) and electricity (E). This performance is puzzling until considering the trainingsizes. The largest training set across domains belongs to trac (T) at 10.2M training examples. In dataset T,the fully generalist models achieves 100% AvgWins. The second-largest training set belongs to electricity(E) at 5.8M training examples, with 75% AvgWins for the fully-generalist model. Unfortunately, there is asharp drop o in training set sizes, with the rest of the data domains collectively comprising 1.6M trainingexamples. These results evoke questions. For instance: does training on the smaller datasets act like a form ofregularization? How does in-domain generalist performance scale with dataset size? We leave these excitingdirections for future work. The generalist codebooks performance across datasets highlights the potential ofunied, discrete, token representations for in-domain evaluations.",
  "Magnitude": "Weather - Codewords in Time Space : TOTEM Codebooks. We visualize all 256 codes for the generalist (All), and three specialists(Trac, Electricity, and Weather). The top row visualizes codes in the latent space, the bottom row visualizescodes in the decoded time space. We additionally highlight codeword pairs matched via low MSE betweenAll-Trac, All-Electricity, and All-Weather in the bottom row.",
  "A.11Architecture Details": "VQVAE. For imputation, anomaly detection, and forecasting the VQVAEs number of residual layers = 2,residual hidden size = 64, and block hidden size = 128 for all datasets. Each residual block has 2 non-causal,non-dilated 1D convolutional layers. The residual blocks are paired with additional non-causal, non-dilated1D convolutional layers, where the number of additional layers is determined by the desired compressionfactor. See for more hyperparameter details. : VQVAE Hyperparameters (A) Imputation generalist (All) and specialists. (B) Anomalydetection generalist (All) and specialists. The anomaly %s for all of the zero shot datasets are 2%. (C)Forecasting generalist (All) and specialists.",
  "Dataset LR Iter.BS # CW CW Dim. CF": "All1e-3 15000 4096256644Elec.1e-3 15000 4096256644Weather 1e-3 15000 4096256644Trac 1e-3 15000 4096256644ETTm1 1e-3 15000 4096256644ETTm2 1e-3 15000 4096256644ETTh1 1e-3 15000 4096256644ETTh2 1e-3 15000 4096256644 Downstream Forecaster. The downstream forecaster has two components the transformer encoder thatintakes codes and outputs a normalized time forecast, and the feedforward neural network that takes in timeand outputs predictions for the forecasts mean and standard deviation. The downstream forecaster is atransformer encoder with a model dimension = 64, hidden dimension = 256, number of heads = 4, number oflayers = 4. The transformer encoder applies a sin / cos positional embedding along the time dimension and",
  "K , 1": "K ], whereK is the number of codewords and D is the latent dimension. In all tasks there is a global normalization,and local normalization Kim et al. (2021); both are standard throughout prior work. In imputation weonly leverage global normalization, in anomaly detection and forecasting we utilize both global and localnormalization. In anomaly detection we evaluate the models we run, TOTEM and GPT2, with both localnormalized data and non-local normalized data for each method and report whichever schema leads to thebest performance. In forecasting the downstream model is a transformer encoder with 4 layers and 4 attentionheads and a feed-forward hidden dimension of 256. We train using Adam with a base learning rate of 0.0001and a one cycle learning rate scheduler in accordance with Nie et al. (2022) on A100s."
}