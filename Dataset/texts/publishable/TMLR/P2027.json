{
  "Abstract": "Teacher-Student Curriculum Learning (TSCL) is a curriculum learning framework that draws inspi-ration from human cultural transmission and learning. It involves a teacher algorithm shaping thelearning process of a learner algorithm by exposing it to controlled experiences. Despite its success,understanding the conditions under which TSCL is effective remains challenging. In this paper,we propose a data-centric perspective to analyze the underlying mechanics of the teacher-studentinteractions in TSCL. We leverage cooperative game theory to describe how the composition ofthe set of experiences presented by the teacher to the learner, as well as their order, influences theperformance of the curriculum that is found by TSCL approaches. To do so, we demonstrate thatfor every TSCL problem, an equivalent cooperative game exists, and several key components ofthe TSCL framework can be reinterpreted using game-theoretic principles. Through experimentscovering supervised learning, reinforcement learning, and classical games, we estimate the coopera-tive values of experiences and use value-proportional curriculum mechanisms to construct curricula,even in cases where TSCL struggles. The framework and experimental setup we present in this workrepresents a novel foundation for a deeper exploration of TSCL, shedding light on its underlyingmechanisms and providing insights into its broader applicability in machine learning.",
  "Introduction": "Controlling the sequence of tasks that a learning algorithm is exposed to through curriculum has been shown topotentially enhance learning efficiency (Elman, 1993; Krueger & Dayan, 2009; Bengio et al., 2009). One widely usedcurriculum framework, known as Teacher-Student Curriculum Learning (TSCL) (Graves et al., 2017; Matiisen et al.,2020), specifically gives a teacher algorithm the ability to control this sequence. While it is commonly understoodthat presenting tasks with increasing difficulty can improve learning, the underlying dynamics and structure of teacher-student interaction in this context are still relatively unexplored. Very few works have attempted to understand when, andhow TSCL works (Lee et al., 2021; Wu et al., 2020) while most have focused on providing algorithmic improvements tothe problem (Portelas et al., 2019a; Turchetta et al., 2020; Liu et al., 2020; Feng et al., 2021). In this paper, we proposea novel data-centric perspective (Ng, 2021) to understand and analyze TSCL algorithms. We begin by formalizing a general notion of units of experience to describe the teacher algorithms control objects(consumed by the learner). Subsequently, our approach draws inspiration from work on feature attribution (Patel et al.,2021), data valuation (Ghorbani & Zou, 2019; Yan & Procaccia, 2021) and explainability (Lundberg & Lee, 2017), andleverages tools from cooperative game theory (Von Neumann & Morgenstern, 1944; Shapley, 1952) to analyze how the",
  "Published in Transactions on Machine Learning Research (09/2024)": "Ariel Procaccia, Nisarg Shah, and Max Tucker. On the structure of synergies in cooperative games. Proceedings of theAAAI Conference on Artificial Intelligence, 28(1), Jun 2014. ISSN 2374-3468. doi: 10.1609/aaai.v28i1.8812. URL 2, 3 Sebastien Racaniere, Andrew Lampinen, Adam Santoro, David Reichert, Vlad Firoiu, and Timothy Lillicrap. Automatedcurriculum generation through setter-solver interactions. In International Conference on Learning Representations,2020. 10",
  "Cooperative Game Theory": "Cooperative Games. Cooperative games model problems where players interact to maximize collective gain (Roth,1988). In a (traditional) cooperative game in characteristic function form among a set of players U, denoted byG = U, v, the characteristic function v : 2U R associates to each coalition C 2U, belonging to the powerset 2U, areal number that represents the benefits produced by the players in C acting jointly. In a cooperative game, a solutionconcept represents a mechanism that produces allocation vectors R|U| (Shubik, 1981). Particularly, Shapleysvalue (Shapley, 1952) allocates to each player u U its average marginal contribution v(C + u) v(C) to coalitionsC U, where u U C",
  "|U|![v(C + u) v(C)](1)": "and uniquely satisfies the axioms of efficiency, null-player, symmetry, and linearity, which are generally considered tobe properties of a fair allocation mechanism (van den Brink & van der Laan, 1998). Generalized Cooperative Games. When the order in which players join determines coalitional worth, traditionalcooperative games and their solution concepts (e.g., Shapleys value) may produce unintuitive allocations (Nowak& Radzik, 1994). In these games, the generalized characteristic function v : P(2U) R assigns to every orderedcoalition C P(2U) in the powerset of permutations P(2U) its worth if members join in the permutation order. Nowak& Radzik (1994) and Sanchez & Bergantios (1997) extended Shapleys work and proposed solution concepts for thesegeneralized cooperative games. We focus on the former due to its intuitive formulation",
  "that averages, for all ordered coalitions C P(2U) where the unit u U is appended last, its marginal contribution tothe newly formed ordered coalition C : u": "Measures of Interactions. In a cooperative game, a measure of interaction (Grabisch & Roubens, 1999; Procacciaet al., 2014) computes players influences on other players outcomes. In particular, we leverage the value of a playerto another player (vPoP) (Hausken & Mohr, 2001). For the games above, vPoP constructs a matrix whose entries(ui, uj) R measure the influence player ui exerts over player uj. It measures how the Shapley value of a unitchanges in the absence of another. More precisely,",
  "|U|![(uj, C) (uj, C ui)](3)": "where (uj, C) is the Shapley value of unit uj (Eq. 1) in the cooperative game restricted to players in C. This matrixmarginal (ui) = j (ui, uj) corresponds to each players Shapley value. We extend vPoP to games in generalizedcharacteristic function form by applying Eq. 3 mutatis mutandis using Nowak & Radzik (1994) value to provide anordered pairwise interaction metric NR(ui, uj).",
  "i=1r(ui)Iui=u(4)": "and that estimate the average reward received by the algorithm in the iterations N uk k where the u-arm has beenpulled. Bandit algorithms, like the ones Graves et al. (2017) and Matiisen et al. (2020) use in their work, transform theestimated average contributions into arms interactions by deriving from estimated values a Boltzmann policy k (U)such that the probability of interaction is proportional to the value estimates:",
  "Experience to Control": "The TSCL framework commonly operates under the assumption that tasks presented to a learning algorithm caninfluence its learning dynamics. Modern iterative learning algorithms process tasks in discrete units. For instance,SL and RL algorithms operate over instances and transitions, respectively. But also, collections of these elementaryunits, such as batches or episodes, datasets or environments, or more generally benchmarks or environment suites,describe a hierarchy of aggregations of experience. Henceforth, we utilize the term unit of experience for referring toany collection of discrete units that a teacher algorithm can use to control the dynamics of the learner algorithm. Example 3.1. For an analysis, we may define a unit of experience as the set of instances of class in a SL classificationproblem. For example, in the MNIST dataset (LeCun & Cortes, 2010), there may be ten units of experience, namely,classes ZERO, ONE, TWO, . . . , NINE. The units of experience abstraction indistinctly applies to supervised or reinforcement learning problems. On eitherparadigm, any iterative learning algorithm is a controllable system whose control inputs are units of experience.",
  "i=1(k, xi)": "A TSCL-style algorithm, as presented in Alg. 1, solves a data-centric control problem. The learner algorithmL(k1, uk) is a black-box system (line 6) controlled by a teacher algorithm through units drawn with probabilityuk k(u). The learner output, at each iteration k, is policy or model k whose performance is measured by a metricfunction J that quantifies the models performance on a set of evaluation units U. The teacher aims to maximize thecumulative learning progression reward (line 7). For the teachers UPDATERULE, we focus on multi-armed banditlearning (see Sec. 2.2). We adopt a data-centric perspective to perform a systematic investigation of its components.",
  "The Cooperative Mechanics of Experience": "The ideal teacher-student interaction mechanics assume that the learner monotonically increases its performance on thetarget task. We conjecture that a prerequisite for this idealistic curriculum learning dynamics (Matiisen et al., 2020) tooccur within TSCL-style algorithms is that experience (or data) presented to the learner should not interfere with eachother. In other words, units of experience should interact cooperatively. From a game-theoretic perspective, we explainhow these cooperative mechanics may emerge among units by examining the history of teacher-student interactions, thereward function, and the bandit selection policy.",
  "The Mechanics of Coalition Formation": "We establish a cooperative game where each unit of experience u U is a player. Next, we interpret the history ofk K teacher-student interactions Hk = {u1, . . . , uk} through their empirical frequencies pk(u) U which formunit vectors that lie in the |U|-probability simplex (U). The effective support (i.e., non-zero probabilities) determinesan unordered coalition (i.e., a set) Ck U (see Faigle (2022), Chapter 8), formed by the units presented to the learnerup to interaction k K. We study this interpretation through a cooperative game in characteristic function form(Sec. 2.1). Example 4.1. (Example 3.1 contd) In the class-as-unit equivalence on MNIST, an unordered training coalition,e.g., the two-unit coalition C = {ZERO, NINE}, describes teacher-student interactions limited to instances from thoseclasses. Next, we note that the outcome of a coalitions work is the policy or model k. Thus, estimating the performance ofthe policy k through the metric function J is akin to approximating the characteristic function v(Ck) (Alg. 1, line7). Moreover, these approximations are conditioned on an evaluation (or target) unit u U. We model the target-taskand multiple-task settings (Graves et al., 2017) where units of experience should increase learner performance on anevaluation unit (e.g., a task, or an environment) or on multiple evaluation units (e.g., a set of tasks or environments).",
  "Consequently, every notion of coalitional worth is conditional on the evaluation units, thus generating a space ofcooperative games": "Definition 4.1. (TSCL Cooperative Games) Let U denote a set of units of experience u U and U a set of evaluationunits u U. Every evaluation coalition C 2U induces a parameterized characteristic function vC(Ck) R whosevalue measures the worth of a coalition Ck when the members of C are the evaluation units. Therefore, the TSCL-familyof algorithms operate over a parameterized space of cooperative games:",
  "Marginal Contributions to Learning": "The notions of coalitions and coalitional worth above induce a game-theoretic interpretation of the learning progressionreward. At any iteration k K, this reward r(uk) R (Alg. 1, line 7) measures the improvement in policy performanceafter the teacher presents a unit uk to the learner algorithm that produces a new policy k L(k1, uk). Thus, wecan restate this reward in terms of a game in characteristic function form:",
  "A Fair Allocation Mechanism": "A principle of fair attribution in cooperative games is that players get assigned values proportional to their expectedmarginal contribution. We note that under the learning progression objective, a bandit action-value estimate qk(u)(Eq. 4) approximates every units (or arms) average marginal contribution after k interactions. Moreover, as discussedin Sec. 2.2, multi-arm bandit algorithms may transform action-values through a Boltzmann projection (Eq. 5) thatconverts the value estimates into units probabilities of interaction with the learner (i.e., the (stochastic) policy k(u)).Consequently, the units that, up to interaction k K, have produced more significant increases on learner performanceand would be allocated larger fractions of the remaining K k interactions. Thus, a multi-armed bandit teacher implements a fair allocation mechanism that computes units values by approximatingtheir average marginal contributions and converts these approximations into the currency-like utility of the TSCL games,namely, interactions with the learner.",
  "An Experiment on The Prospect of Cooperation": "We design an experimental setting to empirically verify the equivalences we draw between TSCL components andcooperative game-theoretic concepts (e.g., do cooperative solution concepts capture some notion of curriculum?) and tohighlight the utility of this data-centric approach to understanding TSCL failure modes. To this end, we incrementallybuild different parts of the equivalence in supervised learning, reinforcement learning, and classical game settings asfollows:",
  ". For any set of units of experience given to the teacher algorithm, we simulate their interactions and computeeach a priori Shapley or Nowak & Radzik value (Sec. 5.1)": "2. We build two value-proportional mechanisms (i.e., pre-computed teacher policies) leveraging the prior unitsvalues to validate whether a curriculum exists and to show that cooperative solution concepts retrieve such anotion. 3. We leverage units estimated pairwise interactions to understand the value-proportional mechanism successand TSCL failure from a data-centric perspective (i.e., the composition of the set of units).",
  "Coalitional Mechanics and Worth": "Cooperative (Non-Ordered) Game. To simulate a traditional cooperative game (Sec. 2), we design a coalitionformation process that draws at each interaction k K a unit uk C with uniform probability C(uk) |C|1, froma coalition C, and present it to a learner algorithm. We compute this procedure for every coalition of units C 2U. Theuniform distribution reflects an a priori ignorance of units importance before measuring their effect on the learner. Generalized (Ordered) Game. To test whether our formulation captures order for curriculum, we build a coalitionformation process that simulates a generalized cooperative game. In this setting, a unit u C is continually presented tothe learner, for K/|C| interactions, in its permutation order on an ordered coalition C. We repeat this procedure forevery C P(2U). As before, the ordered equipartition of interactions reflects our ignorance about units a priori effect. Coalitional Worth & Characteristic Function. For every coalition, we obtain a model KC after K interactions withthe units in C through either the traditional or generalized mechanics described above. Therefore, by the equivalencewe established between policy performance and (conditional) coalitional worth (Sec. 4.2), the evaluation of each policydetermines the characteristic function v(C). More importantly, because the resulting policy KC is unbiased for theevaluation units or coalitions (e.g., the uninformed priors in the mechanics), we estimate, using the same policy KC ,the coalitions worth for every characteristic function vC(C) parameterized by every evaluation or target coalition C. Example 5.1. (Example 4.1 contd) Assume a budget of K = 100 interactions and a subset (coalition) of classesfrom MNIST, for instance, units (classes) ZERO and NINE. In the simulation of a traditional game, for a coalitionC = {ZERO, NINE}, we uniformly draw instances from each unit with probability (u) =12. For a coalitionC = [ZERO, NINE] in a generalized game, instances from unit ZERO are presented for the first k = 50 iterationsfollowed by k = 50 instances from NINE. The resulting policy KC performance is equivalent to the value of thecharacteristic function v(C), evaluated at coalition C. Marginal Contributions & Solution Concepts. The players marginal contributions are the central quantity ofcooperative solution concepts. What do marginal contributions capture in our experiments? First, in both coalitionformation processes, adding a unit u to a coalition C while keeping the number of interactions K constant reduces thelearner algorithms interactions with the existing units. For instance, in the traditional cooperative game simulation,adding a unit u reduces the probability of drawing any unit already in u C from pC(u) = |C|1 to pC+u(u) =(|C| + 1)1, while in the the generalized game reduces units interactions from K/|C| to K/(|C| + 1). Therefore,",
  "A Sanity Check Through Supervised Classification": "Our running examples on MNIST inspire the first setting we examine to empirically validate the connections we haveestablished between TSCL and cooperative games. In this experiment, we considered instances aggregated in classesas units of experience and benefited from a trained classifiers confusion matrix to extract ground-truth informationfrom unit interactions. For both MNIST and CIFAR10 (Krizhevsky, 2009), we trained a model on the completedataset (e.g., for 200 epochs), extracted the confusion matrix on validation, and identify the top-k most confused pairsof classes. On MNIST, we selected the five classes TWO, THREE, FOUR, FIVE and SEVEN belonging to the top-threemost confused pairs (see Appendix A, a), grouped their instances into five units of experience, and conduct theapproximations described in Sec. 5.1 for a traditional cooperative game (i.e., not considering order). Next, we followedthe same approach for CIFAR10 six classes with more significant pairwise confusion errors on validation, namely,CAR, CAT, DEER, DOG, FROG and HORSE (see Appendix A, b). If the equivalences we drew are correct, one may expect a unit Shapley value to be higher when the evaluation target isthe same unit. Moreover, the pairwise interactions between units should approximately reflect the negative interactionswe extracted from the confusion matrices of each trained classifier. Units Values. In effect, for either MNIST or CIFAR10, each units Shapley value estimated from the traditionalcooperative game simulations correctly matches the ground truth information. In the target-unit setting, where eachunit of experience is also used as evaluation unit, each unit of experience (or class) matching the evaluation unit (orclass) has the largest Shapley value, as depicted in a (first five targets) for MNIST and c (first six targets)for CIFAR10. For instance, on MNIST, unit u = TWO has the largest Shapley value (TWO) = 0.995 when theevaluation unit is u = TWO. We observed a similar effect on CIFAR10. Furthermore, for the all-units setting, everyunits Shapley value is approximately equal on both MNIST and CIFAR10 (a and 1c, all column), matching theintuition that, conditional on an all units evaluation, every unit of experience should be equally valuable. Measures of Interactions. We also computed the vPoP measure (see Sec. 2.1, Eq. 3) to verify whether its decom-position of Shapley values into pairwise interaction values correctly identifies the most confused pairs of classes.For both MNIST and CIFAR10, their respective vPoP matrices, displayed in b and 1d, provide a reasonableapproximation to the ground-truth pairwise interactions extracted from the confusion matrices. For instance, on MNISTthe units TWO and SEVEN have the lowest interaction value (TWO, SEVEN) = (SEVEN, TWO) = 0.007 whichcorresponds to largest entry M(2, 7) = 20 of the confusion matrix (see Appendix A, a). Also, in CIFAR10 theunits DOG and CAT have the lowest interaction value (DOG, CAT) = 0.0164 coinciding with the most confusedclasses M(DOG, CAT) = 66 on validation (see Appendix A, b). However, we note that, for instance, on MNISTconfusion matrix M(2, 7) = M(7, 2) and, similarly, on CIFAR10s M(DOG, CAT) = M(CAT, DOG). Nevertheless,we interpret these values as reasonable proxies for units negative, positive, or neutral pairwise interactions. A valuable aspect of our cooperative game-theoretic analysis of TSCL is that the equivalences we introduced are notlimited to the supervised classification setting where ground truth information is available. Next, we demonstrate thebroad applicability of these ideas through problems in RL and classical games, where finding ground-truth notions ofcurriculum is non-trivial. Details to reproduce these experiments are provided in Appendix A.",
  "(d)": ": We validated the prospect prior using the class-as-a-unit analogy on MNIST and CIFAR10. In Figures (a) and (c), eachcolumn represents units Shapley values (u) in each cooperative game parameterized by a target-unit u and the target coalitionof all units. In Figures (b) and (d), we present the vPoP decomposition matrix (Eq. 3) measuring the pairwise interaction values(ui, uj) among units in the all-units target.",
  "Reinforcement Learning": "We investigate the MINIGRID-ROOMS (Chevalier-Boisvert et al., 2018) set of three environments, namely, TWOROOMS,FOURROOMS, and SIXROOMS for which it is folk knowledge that an optimal curriculum exists.1 We apply the prospectprior simulation of a generalized game where we consider each environment a unit of experience. As a learneralgorithm, we used PPO (Schulman et al., 2017) with an interaction budget of K = 500, 000 steps, and estimated,from the outcome of these simulations, the Nowak & Radzik values (Sec. 2, Eq. 2), with every environment and auniform distribution over all, as evaluation targets. In a, we show that the Nowak & Radzik values we estimatematch folk knowledge. First, there is no requirement for environments other than TWOROOMS as the only positivevalue u(TWOROOMS) = 0.423 correctly measures. Then, for FOURROOMS, the values of u(TWOROOMS) = 0.041and u(FOURROOMS) = 0.107 indicate that both environments are required. And finally, environments values ofu(FOURROOMS) = 0.03 and u(SIXROOMS) = 0.03 indicate that both are needed for solving SIXROOMS.",
  "Classical Games": "We introduce an experimental setting, the Adversarial Sparse Iterated Prisoners Dilemma (A-SIPD), that utilizesthe Prisioners Dilemma (Flood, 1952; Axelrod & Hamilton, 1981) classical two-player game as a base but in a morechallenging sparse and iterated version where at the end of a finite number of interactions (e.g., 200 steps), a win-draw-loss reward is given to the learner if it accrues more cumulative payoff than its opponent. Opponents are drawnfrom a population of five well-known strategies: ALWAYSCOOPERATE, ALWAYSDEFECT, WINSTAYLOSESWITCH,TITFORTAT (Axelrod, 1981) and a ZERODETERMINANT strategy (Hilbe et al., 2013). We apply the prospect priorsimulation of a generalized game where we consider each opponent a unit of experience. As a learner algorithm, weused PPO (Schulman et al., 2017) with a budget of K = 100, 000 interactions. We estimated the Nowak & Radzik values (Sec. 2, Eq. 2), conditioned on each opponent and a uniform mixture over all,as evaluation targets. The results we present in c show that playing uniquely against TITFORTAT is sufficientacross all evaluation targets, including the most challenging opponents, ALWAYSDEFECT and ZERODETERMINANT.This result contrasts with folk knowledge in population-based training (e.g., playing against the populations Nashstrategy (Nash, 1950)). We defer to Sec. 5.3 a more in-depth discussion of this finding.",
  "Curriculum from A Priori Values": "For both the RL and classical games settings, the Nowak & Radzik value correctly ordered tasks and opponents,respectively, by their contribution to learning. Nevertheless, we also investigated whether these values magnitudeindicated the proportion of interactions (i.e., a fraction of the K interactions budget) that should be allocated to eachunit and whether the combination of order and magnitude retrieved an approximate curriculum. Therefore, inspired by value-proportional allocations (Bachrach et al., 2020), we developed two mechanisms that turnunits values into interactions with the learner by projecting any value vector (u) R|U| onto vectors (u) U in a|U|-simplex. While Bachrach et al. (2020) use a linear projection unable to handle negative marginal contributions, we",
  ": The vPoP decomposition of Shapleys and Nowak & Radzik values, conditioned on the all-units evaluation target, for theMINIGRID-ROOMS (a, b) and A-SIPD (c, d) problem settings": "investigate the Boltzmann or softmax projection, commonly used in multi-arm bandit algorithms (see Sec. 2.2, Eq. 5),and an Euclidean projection (Blondel et al., 2014) that projects to zero any unit with negative value. When values (u) are Shapley values, the projected vectors are used as pre-computed teacher policies (i.e., probabilitydistributions), mimicking TSCLs interactions but fixed a-priori knowledge of units value. However, when values(u) are Nowak & Radzik, vectors U are used as ordered compositional vectors (Aitchison, 1982) that representordered fractions of K interactions. Thus, we construct a pre-computed teacher policy that, first orders units by theirNowak & Radzik values, projects the ordered values onto KU , and presents the i-th ranked unit ui U to thelearner for the number of interactions indicated by i N. This mechanism preserves the ordered values captured byNowak & Radziks solution concept. shows that for both MINIGRID-ROOMS and A-SIPD the teacher policies obtained from these value-proportionalmechanisms, in particular the Euclidean projection of Nowak & Radzik values (nowak-all-simplex), consistently producelearner-induced policies that solve the target tasks, and empirically validate our hypothesis that a priori values, inparticular the generalized Nowak & Radzik value we introduced, retrieve both the order of and the proportions ofinteractions that should be allocated to each unit to produce curriculum. TSCL Data-Centric Failures. Moreover, also highlights the multi-arm bandit approach (EXP3 (Auer et al.,2003; Graves et al., 2017)) to TSCL (i.e., tscl-all-exp3s) failure to produce an effective curriculum. We found that in thepresence of units with non-cooperative interactions, bandit-driven TSCL fails. presents the vPoP decompositionof Shapley and Nowak & Radzik values conditioned on the all-units evaluation. In both settings, the interactionsmeasured from the Shapley-based decomposition produce lower (negative) values than those obtained with Nowak &Radzik. As we show through Sec. 4 and Sec. 5, we take the Shapley-based interaction values as fair approximationsof TSCL interactions, and thus they provide a data-centric explanation to TSCL failures (see Appendix B). On theother hand, the Nowak & Radzik-based values explain the success of nowak-all-simplex mechanism, along with theelimination, through the Euclidean projection, of negatively-valued units. These pruning strategies are employedeffectively by data valuation techniques (Yan & Procaccia, 2021).",
  "Limitations": "The simulations we computed in our experiments are computationally expensive. It is well-established that cooperativesolution concepts are NP-hard (Deng & Papadimitriou, 1994; Elkind et al., 2009). However, better approximations arepossible (Yan & Procaccia, 2021; Mitchell et al., 2022) although we do not explore them in this work. Consequently,we do not consider the prospect prior experiments and the value-proportional curriculum mechanism as algorithmicinnovations to replace TSCL. They represent a data-centric approach to study the limits imposed on TSCL-stylealgorithms by the (non)cooperative mechanics among units of experience. However, we acknowledge that the mechanicsof units interactions also affect other aspects of TSCL. These aspects may include, for instance, the teachers credit-assignment problem (Gittins, 1979) or neural networks learning and forgetting dynamics (e.g., Lee et al. (2021)). Wecontrol for these factors by keeping them constant in our experiments (see Appendix A) but do not undertake theiranalysis here. Our work is a starting point for more thorough explorations of TSCL and curriculum learning, theirunderlying mechanisms and broader applicability in machine learning.",
  "Related Work": "Curriculum Learning.Since the seminal works of Elman (1993); Krueger & Dayan (2009) and Bengio et al. (2009),a large body of literature has been produced on curricula for machine learning algorithms. Excellent surveys presentedby Narvekar et al. (2022); Wang et al. (2022), and Soviany et al. (2022) offer a comprehensive state of recent advances inthe field. Our work closely inspects the TSCL framework concurrently introduced by Graves et al. (2017) and Matiisenet al. (2020). While follow-up works have generally focused on either algorithmic innovations (Weinshall et al., 2018;Portelas et al., 2020; Feng et al., 2021; Liu et al., 2020; Portelas et al., 2019b; Racaniere et al., 2020; Florensa et al.,2017; Campero et al., 2021; Du et al., 2022; Florensa et al., 2018) or evaluation benchmarks (Chevalier-Boisvertet al., 2019; Romac et al., 2021), the data-centric perspective (Ng, 2021; Karpathy & Abbeel, 2021) we proposed hereis less explored. The work by Wu et al. (2020) empirically verifies the same fundamental questions on why, when,and how curriculum learning works. However, while our work is more specifically focused on TSCL, we provide agame-theoretic grounding that could be further extended to analyze Wu et al. (2020) setting. On the other hand, Leeet al. (2021) specifically explore TSCL but with a focus on the deep neural network dynamics leading to catastrophicforgetting McCloskey & Cohen (1989). The evaluation mechanisms that we propose in the prospect prior simulationand the game-theoretic characterization of experience interference we introduce here provide a more general groundingfor the problems studied there. Machine Learning & Game Theory.Our work draws much of its inspiration from the tradition of cross-pollinationbetween machine learning and game theory research (Cesa-Bianchi & Lugosi, 2006). In recent years, game theoryresearch has fuelled work in optimization (Daskalakis & Panageas, 2018; Jin et al., 2019), generative modelling (Good-fellow et al., 2014; Farnia & Ozdaglar, 2020; Mohebbi Moghaddam et al., 2023), or robustness (Madry et al., 2018;Huang et al., 2022a). Also, game-theoretic arguments have been used to revitalize long-standing problems in ma-chine learning. For instance, Gemp et al. (2021) reframed the century-old problem of Principal Component Analysis(PCA) (F.R.S., 1901; Hotelling, 1933) as the Nash equilibrium (Nash, 1950) of a multiplayer game. Recently, Chang",
  "Conclusions & Future Work": "We reexamined TSCL through the lens of cooperative game theory. By drawing inspiration from work on data valuation,feature attribution and explainability, we provide a novel data-centric perspective that re-frames several of its componentsthrough alternative cooperative game-theoretic interpretations. Our experiments confirmed the appropriateness ofstudying TSCL-style under this framework and highlighted the impact of units cooperative mechanics on this problem.However, we only began to unveil the potential of allocation mechanisms, solution concepts, and measures of interactionsto explain some fundamental aspects of TSCL and hope our work inspires an influx of novel game-theoretic approachesto the problem. Future work would explore more theoretically-grounded analysis of this problem through the connectionbetween convex games (Shapley, 1971) and super(sub)modularity in discrete combinatorial optimization (Dughmi,2009; Bach, 2011; Krause & Guestrin, 2011) and the extension to continuous set of units through values of non-atomicgames (Aumann & Shapley, 1974).",
  "J Aitchison. The statistical analysis of compositional data. Journal of the Royal Statistical Society, 44(2):139160,January 1982. 9": "Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob McGrew, JoshTobin, Openai Pieter Abbeel, and Wojciech Zaremba. Hindsight experience replay. In I Guyon, U Von Luxburg,S Bengio, H Wallach, R Fergus, S Vishwanathan, and R Garnett (eds.), Advances in Neural Information ProcessingSystems, volume 30. Curran Associates, Inc., 2017. 11 Peter Auer, Nicol Cesa-Bianchi, Yoav Freund, and Robert E. Schapire. The nonstochastic multiarmed banditproblem. SIAM J. Comput., 32(1):4877, jan 2003. ISSN 0097-5397. doi: 10.1137/S0097539701398375. URL 3, 9, 20, 21",
  "Francis Bach. Learning with submodular functions: A convex optimization perspective. November 2011. 11": "Yoram Bachrach, Richard Everett, Edward Hughes, Angeliki Lazaridou, Joel Z Leibo, Marc Lanctot, Michael Johanson,Wojciech M Czarnecki, and Thore Graepel. Negotiating team formation using deep reinforcement learning. Artificialintelligence, 288(103356):103356, November 2020. 2, 8 David Balduzzi, Marta Garnelo, Yoram Bachrach, Wojciech Czarnecki, Julien Perolat, Max Jaderberg, and ThoreGraepel. Open-ended learning in symmetric zero-sum games. In Kamalika Chaudhuri and Ruslan Salakhutdinov(eds.), Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of MachineLearning Research, pp. 434443. PMLR, 2019. 9 Yoshua Bengio, Jrme Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. In Proceedings of the26th Annual International Conference on Machine Learning, ICML 09, pp. 4148, New York, NY, USA, 2009.Association for Computing Machinery. ISBN 9781605585161. URL 1, 2, 10 Lilian Besson. SMPyBandits: an Open-Source Research Framework for Single and Multi-Players Multi-Arms Bandits(MAB) Algorithms in Python. Online at: github.com/SMPyBandits/SMPyBandits, 2018. URL Code at at 20, 21 Mathieu Blondel, Akinori Fujino, and Naonori Ueda. Large-Scale multiclass support vector machine training viaeuclidean projection onto the simplex. In 2014 22nd International Conference on Pattern Recognition, pp. 12891294,August 2014. 9 Andres Campero, Roberta Raileanu, Heinrich Kuttler, Joshua B. Tenenbaum, Tim Rocktschel, and Edward Grefen-stette. Learning with {amig}o: Adversarially motivated intrinsic goals. In International Conference on LearningRepresentations, 2021. URL 10",
  "Maxime Chevalier-Boisvert, Lucas Willems, and Suman Pal. Minimalistic gridworld environment for gymnasium,2018. URL 8, 20": "Maxime Chevalier-Boisvert, Dzmitry Bahdanau, Salem Lahlou, Lucas Willems, Chitwan Saharia, Thien Huu Nguyen,and Yoshua Bengio. BabyAI: First steps towards grounded language learning with a human in the loop. InInternational Conference on Learning Representations, 2019. URL 10 Constantinos Daskalakis and Ioannis Panageas.The limit points of (optimistic) gradient descent inmin-max optimization.In S. Bengio,H. Wallach,H. Larochelle,K. Grauman,N. Cesa-Bianchi,and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 31. Curran Asso-ciates, Inc., 2018. URL 10 Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, Ales Leonardis, Gregory Slabaugh, and TinneTuytelaars. A continual learning survey: Defying forgetting in classification tasks. IEEE transactions on patternanalysis and machine intelligence, 44(7):33663385, July 2022. 11",
  "Merrill M. Flood. Some Experimental Games. RAND Corporation, Santa Monica, CA, 1952. 8": "Carlos Florensa, David Held, Markus Wulfmeier, Michael Zhang, and Pieter Abbeel. Reverse curriculum generationfor reinforcement learning. In Sergey Levine, Vincent Vanhoucke, and Ken Goldberg (eds.), Proceedings of the 1stAnnual Conference on Robot Learning, volume 78 of Proceedings of Machine Learning Research, pp. 482495.PMLR, 2017. 10 Carlos Florensa, David Held, Xinyang Geng, and Pieter Abbeel. Automatic goal generation for reinforcement learningagents. In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th International Conference on MachineLearning, volume 80 of Proceedings of Machine Learning Research, pp. 15151528. PMLR, 2018. 10",
  "Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep learning. MIT press, 2016. 4": "Michel Grabisch and Marc Roubens. An axiomatic approach to the concept of interaction among players in cooperativegames. International Journal of Game Theory, 28(4):547565, Nov 1999. ISSN 0020-7276. 2, 3 Alex Graves, Marc G Bellemare, Jacob Menick, Rmi Munos, and Koray Kavukcuoglu. Automated curriculum learningfor neural networks. In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th International Conference onMachine Learning, volume 70 of Proceedings of Machine Learning Research, pp. 13111320. PMLR, 2017. 1, 2, 3,4, 9, 10, 11, 20, 21",
  "Siqi Liu, Luke Marris, Daniel Hennes, Josh Merel, Nicolas Heess, and Thore Graepel. NeuPL: Neural populationlearning. In International Conference on Learning Representations, 2022. URL 9": "Vincenzo Lomonaco and Davide Maltoni. Core50: a new dataset and benchmark for continuous object recognition. InSergey Levine, Vincent Vanhoucke, and Ken Goldberg (eds.), Proceedings of the 1st Annual Conference on RobotLearning, volume 78 of Proceedings of Machine Learning Research, pp. 1726. PMLR, 1315 Nov 2017. URL 11 Scott M. Lundberg and Su-In Lee. A unified approach to interpreting model predictions. In Proceedings of the 31stInternational Conference on Neural Information Processing Systems, NIPS17, pp. 47684777, Red Hook, NY, USA,2017. Curran Associates Inc. ISBN 9781510860964. 1, 11 Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learningmodels resistant to adversarial attacks. In International Conference on Learning Representations, 2018. URL 10",
  "Rory Mitchell, Joshua Cooper, Eibe Frank, and Geoffrey Holmes. Sampling permutations for shapley value estimation.Journal of machine learning research: JMLR, 23(43):146, 2022. 10": "Monireh Mohebbi Moghaddam, Bahar Boroomand, Mohammad Jalali, Arman Zareian, Alireza Daeijavad, Moham-mad Hossein Manshaei, and Marwan Krunz. Games of GANs: game-theoretical models for generative adversarialnetworks. Artificial Intelligence Review, February 2023. 10 Martin Mundt, Yongwon Hong, Iuliia Pliushch, and Visvanathan Ramesh. A wholistic view of continual learning withdeep neural networks: Forgotten lessons and the bridge to active and open world learning. Neural networks: theofficial journal of the International Neural Network Society, 160:306336, March 2023. 11 Sanmit Narvekar, Bei Peng, Matteo Leonetti, Jivko Sinapov, Matthew E Taylor, and Peter Stone. Curriculum learningfor reinforcement learning domains: a framework and survey. Journal of machine learning research: JMLR, 21(1):73827431, June 2022. 10",
  "Pierre-Yves Oudeyer, Frdric Kaplan, and Verena V Hafner. Intrinsic motivation systems for autonomous mentaldevelopment. IEEE transactions on evolutionary computation, 11(2):265286, 2007. 2": "German I Parisi, Ronald Kemker, Jose L Part, Christopher Kanan, and Stefan Wermter. Continual lifelong learning withneural networks: A review. Neural networks: the official journal of the International Neural Network Society, 113:5471, May 2019. 11 Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, ZemingLin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, MartinRaison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch:An imperative style, high-performance deep learning library. In Advances in Neural Information ProcessingSystems 32, pp. 80248035. Curran Associates, Inc., 2019. URL 20, 21 Roma Patel, Marta Garnelo, Ian Gemp, Chris Dyer, and Yoram Bachrach. Game-theoretic vocabulary selection viathe shapley value and banzhaf index. In Proceedings of the 2021 Conference of the North American Chapter ofthe Association for Computational Linguistics: Human Language Technologies, pp. 27892798, Online, June 2021.Association for Computational Linguistics. 1",
  "AProspect Prior Experiments Details": "We provide for all problems, models or policies architectures, algorithm hyperparameters, and other reproducibilitydetails.2All models and architectures are implemented with PYTORCH (Paszke et al., 2019), are configured usingHYDRA (Yadan, 2019), and fit on a workstation equipped with a 16 GB NVIDIA RTX A4000 GPU, 32 GB of RAM, and32 CPU cores.",
  "CONV2D(32, 3, 1)RELU()CONV2D (64, 3, 1)RELU()MAXPOOL2D (2, 2)DROPOUT(0.25)FLATTEN()LINEAR(9216, 128)RELU()DROPOUT(0.5)LINEAR(128, 10)": ": Details on the learning algorithm hyperparameters (left) and model architecture (right) used in the MNIST (LeCun &Cortes, 2010) experiments. Model components and the optimizer are provided by PYTORCH (Paszke et al., 2019). These detailsremained constant throughout the rest of the experiments with MNIST. CIFAR10. The experiments on CIFAR10 (Krizhevsky et al., 2009) follow the same setting as those on MNIST. Wesimilarly trained a model on the supervised 10-classes task. Specification of the model architecture and hyperparameterselection are provided in .",
  "(b) CIFAR10": ": The class-as-a-unit analogy applied to MNIST (a) and CIFAR10 (b) served as our ground truth. For each problem, wederived the Shapleys value from the precomputed priors (left) [Eq. 1] on each cooperative game (Sec. 5). Our results verify that unitsvalues on the target-unit settings approximately ordered the most confused pairs of classes. For instance, digits 2 & 7 in MNIST, ordog & cat in CIFAR10. When the target is all classes, the vPoP decomposition (right) also (Sec. 2.1) identifies interfering pairs.",
  "A.2Reinforcement Learning": "MINIGRID ROOMS. We utilized a sequence of TWOROOMS, FOURROOMS, and SIXROOMS gridworlds provided onMINIGRID (Chevalier-Boisvert et al., 2018) as units of experience. As the learning algorithm, we trained for 500, 000steps a PPO Schulman et al. (2017) agent, whose implementation we derived from CLEANRL Huang et al. (2022b).Policy and actor-critic architecture, with shared backbone, as well as other PPO hyperparameters details are presentedin . For the TSCL experiments, we leveraged Exp3S (Auer et al., 2003) implementation from Besson (2018)with default hyperparameters = 105 and = 0.05, as defined in Graves et al. (2017).",
  "LINEAR(64, 64)LINEAR(64, 64)TANH()TANH()LINEAR(64, 7)LINEAR(64, 1)": ": Details on the PPO hyperparameters (left) and actor-critic architecture (right) used in the MINIGRIDROOMS (Chevalier-Boisvert et al., 2018) experiments. Policy and critic components, and the optimizer, are provided by PYTORCH (Paszke et al., 2019).Implementation and default hyperparameters are derived from CLEANRL (Huang et al., 2022b). These details remained constantthroughout the rest of the experiments with MINIGRIDROOMS.",
  "A.3Populations & Games": "Adversarial SIPD. In our more challenging sparse and iterated version of Prisoners Dilemma, at the end of 200interactions, inspired by Axelrods competition (Axelrod, 1981). A win-draw-loss reward r = {1, 0, 1} is given toa learning player if it beats a fixed opponent. Opponents are drawn from a population of five well-known strategies:always cooperate, always defect, win-stay-lose-switch, tit-for-tat, and a zero-determinant strategy (Axelrod, 1981; Hilbeet al., 2013; Knight et al., 2021). We trained for 500 episodes (or 100, 000 steps) a PPO (Schulman et al., 2017) agentadapted from CLEANRL Huang et al. (2022b) default implementation. Policy and actor-critic architecture, withoutshared backbone, as well as other PPO hyperparameters details are presented in . For the TSCL experiments,we leveraged Exp3S (Auer et al., 2003) implementation from Besson (2018) with default hyperparameters = 105",
  "LINEAR(2, 64)LINEAR(2, 64)ORTHOINIT()ORTHOINIT()TANH()TANH()LINEAR(64, 64)LINEAR(64, 64)ORTHOINIT()ORTHOINIT()TANH()TANH()LINEAR(64, 2)LINEAR(64, 1)": ": Details on the PPO hyperparameters (left) and actor-critic architecture (right) used in the ADVERARIAL-SIPD experiments.Policy and critic components, and the optimizer, are provided by PYTORCH (Paszke et al., 2019). Implementation and defaulthyperparameters are derived from CLEANRL (Huang et al., 2022b). These details remained constant throughout the rest of theexperiments.",
  "(b) ADVERSARIAL-SIPD": ": We also investigated the prior-proportional curriculum in the target-unit setting. For each target unit, we allocateto each training unit interactions proportional to their pre-computed values for each target. For the ADVERSARIAL-SIPD andMINIGRID-ROOMS controlled their learning dynamics by presenting the units according to unordered and ordered mechanismsin Sec. 5. On each task, the value-proportional curriculum derived from the prospect priot outperforms TSCL (tscl-*-exp3s). Wefurther investigate the reason for TSCL failures on this scenario.",
  "(b) VPOP METRIC": ": To understand the failure modes of TSCL on ADVERSARIAL-SIPD, we represented the individual runs (i.e., each ofthe five seeds) on every target unit. TSCL (tscl-*-exp3s) (top row) is extremely brittle, unstable, and generally not robust to unitsinterference. We surmise that these failures are related to the exploration-exploitation dilemma. Exploratory steps presenting anegatively-valued unit are hard to overcome (forgetting dynamics). This issue requires further investigation, and we defer it to futurework.",
  "Step": "0.04 0.02 0.00 0.02 0.04 returns seed SixRooms Two-Rooms FourRooms SixRooms u(ui, uj) Two-Rooms FourRooms SixRooms units [grid] 0.6089-0.0121-0.1817 -0.05010.00090.0217 -0.13640.0011-0.1817 units [grid] Two-Rooms FourRooms SixRooms u(ui, uj) Two-Rooms FourRooms SixRooms units [grid] 0.06290.0891-0.0535 -0.01100.0891-0.0289 -0.0110-0.0713-0.0535 units [grid] Two-Rooms FourRooms SixRooms u(ui, uj) Two-Rooms FourRooms SixRooms units [grid] 0.00000.0277-0.0111 0.00000.0277-0.0111 0.0000-0.0221-0.0111 units [grid]"
}