{
  "Abstract": "Web automation holds the potential to revolutionize how users interact with the digitalworld, oering unparalleled assistance and simplifying tasks via sophisticated computationalmethods.Central to this evolution is the web element nomination task, which entails identifying unique elements on webpages. Unfortunately, the development of algorithmicdesigns for web automation is hampered by the scarcity of comprehensive and realisticdatasets that reect the complexity faced by real-world applications on the Web. To addressthis, we introduce the Klarna Product Page Dataset, a comprehensive and diverse collection ofwebpages that surpasses existing datasets in richness and variety. The dataset features 51,701manually labeled product pages from 8,175 e-commerce websites across eight geographicregions, accompanied by a dataset of rendered page screenshots. To initiate research on theKlarna Product Page Dataset, we empirically benchmark a range of Graph Neural Networks(GNNs) on the web element nomination task. We make three important contributions. First,we found that a simple Convolutional GNN (GCN) outperforms complex state-of-the-artnomination methods, and further enhance its performance using a Reversible GNN (RevGNN)architecture. Second, we introduce a training renement procedure that involves identifyinga small number of relevant elements from each page using the aforementioned GNN. Theseelements are then passed to a Large Language Model for the nal nomination. This proceduresignicantly improves the nomination accuracy by 10.9 percentage points on our challengingdataset, without any need for ne-tuning. Finally, in response to another prevalent challengein this eld the abundance of training methodologies suitable for element nomination we introduce the Challenge Nomination Training Procedure, a training method that furtherboosts nomination accuracy.",
  "Published in Transactions on Machine Learning Research (11/2024)": "Raymond, Francis Real, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick Ryder, MarioSaltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David Schnurr, John Schulman,Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah Shoker, Pranav Shyam, SzymonSidor, Eric Sigler, Maddie Simens, Jordan Sitkin, Katarina Slama, Ian Sohl, Benjamin Sokolowsky, YangSong, Natalie Staudacher, Felipe Petroski Such, Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas Tezak,Madeleine Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston Tuggle, Nick Turley, JerryTworek, Juan Felipe Cern Uribe, Andrea Vallone, Arun Vijayvergiya, Chelsea Voss, Carroll Wainwright,Justin Jay Wang, Alvin Wang, Ben Wang, Jonathan Ward, Jason Wei, CJ Weinmann, Akila Welihinda,Peter Welinder, Jiayi Weng, Lilian Weng, Matt Wietho, Dave Willner, Clemens Winter, Samuel Wolrich,Hannah Wong, Lauren Workman, Sherwin Wu, Je Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah Yoo,Kevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang, Marvin Zhang, Shengjia Zhao,Tianhao Zheng, Juntang Zhuang, William Zhuk, and Barret Zoph. Gpt-4 technical report, 2023.",
  ": Four pages from the Finnish, Dutch and US markets in our dataset": "thousands of nodes on a page, only one element belongs to each class. For this purpose, we train severalneural networks on the Document Object Model (DOM) tree representations of webpages, using three popularfamilies of graph neural networks (GNNs): recurrent GNNs (Tree Long Short-Term Memory networks),convolutional GNNs (GCNs), and attention-based GNNs. Our aim is to compare their performance in webelement nomination and identify the most promising GNN family for future research on the Klarna ProductPage Dataset. We also compare their performance against state-of-the-art (SOTA) methods FreeDOM (Linet al., 2020) and DOM-Q-NET (Jia et al., 2019), both of which are fairly intricate algorithms. FreeDOM,for instance, requires a two-stage training process, and both algorithms utilize information from the entireDOM tree to represent a single element. We discovered that a surprisingly simple GCN, which solely usesinformation from local neighbourhoods around each node in the DOM tree, outperforms these SOTA methods.We further enhance the GCN by incorporating it into a Reversible GNN (RevGNN) architecture (Li et al.,2021). RevGNN improves both memory and parameter eciency through reversible connections, groupconvolutions, and weight tying, enabling us to scale up to 20 layers and further boost the nal nominationaccuracy. These nding highlights the potential of employing GCNs as building blocks in more advanced webelement nomination architectures in future research. We subsequently investigate three methodologies aimed at further enhancing the web element nominationperformance. Firstly, we explore whether the text present on the page is informative for nominating webelements. Specically, we use a pre-trained language model to create embeddings of the text on the page,which we then add to the set of features. Our ndings indicate that models incorporating text in generalachieve higher average accuracy. Surprisingly, for certain tasks, the included text proves to be non-informativeand, in fact, reduces performance. Secondly, applying LLMs to the entire HTML of realistic pages is unfeasible,partly due to limited context windows but mainly due to the high costs associated with processing a largenumber of tokens. To address this, we explore an approach where we signicantly enhance the performance ofthe highest-performing GNN model with a post-training renement step where an LLM is applied to a smallsubset of the highest ranked elements of each page. Finally, we address the absence of a standardized trainingmethodology for element nomination2. To this end, we propose and evaluate a novel training approach thatconsiderably improves nomination accuracy.",
  ". We adapt and apply six algorithms from three of the most popular families of GNNs to a web": "element nomination task, namely recurrent GNNs (Tree Long Short-Term Memory networks),convolutional GNNs (GCNs) and attention based GNNs. We also investigate the performance ofSOTA baselines for element nomination. In particular, we identify that GCN-Mean, a simple 2-layerGCN, outperforms the second-best model by an impressive margin of 9.2 percentage points measuredin average nomination accuracy. We then enhance the scalability of this algorithm by employingRevGNN, which uses the GCN as a base model.",
  ". We explore a post-training nomination renement step using an LLM. Specically, we use a trained": "RevGNN algorithm to lter out all elements except the top ten for each class on every webpage in thetest set. We then use GPT-4 to perform the nal nomination based on the local HTML content ofthese ltered elements. Our results show that this renement step substantially enhances nominationaccuracy across all explored tasks, resulting in an average increase of 10.9 percentage points.",
  "The Klarna Product Page Dataset": "We collected the Klarna Product Page Dataset dataset over several months between 2018 and 2019. Thedataset comprises 51, 701 product pages from 8, 175 distinct e-commerce merchants. It is conveniently dividedinto 80% training and 20% test sets, which ensures that pages from the same website do not appear in boththe training and test sets. presents an overview of the dataset statistics. Each page in our dataset issaved as an MHTML le, which includes all images and assets required to render the page. Furthermore,every page in the dataset has 5 labeled elements: 2 corresponding to action elements (buy button and cartbutton) and 3 to information elements (product price, product name, and product image). These labels havebeen manually annotated by human analysts. In our experiments, we also consider a 6th, more abstract label:the subject node, dened as the lowest common ancestor of all other labels. An example page screenshot",
  "Nominate Cart": ": Dierence between nomination and classication: A DOM-tree representation of a webpageis depicted to the left. During the classication process, the GNN embedder takes a node and its context, herebeing its directly neighboring nodes, as input. The output from the embedder is then fed into a classicationlayer that produces classication scores. In element nomination, the model is applied to every node in thetree. For each label type, the resulting classication scores are ranked across all nodes, and the node receivingthe highest rank is nominated for that specic label type. For example, here we see how the node receivingthe highest ranking for the Cart Button label is nominated as the Cart Button element of the page.",
  "Element nomination": ": A sample webpage inour dataset with 748 nodes. Thelabeled elements are surroundedby red boxes. (1) is the buy but-ton, (2) is the cart button, (3) isthe image, (4) is the price and(5) is the product name. Precisely identifying the correct element before executing any action inweb automation is vital. In automation features, element nominationplays a crucial role as it solves the common subtask of identifying a singleelement on a webpage for a specic action. Since we often have only oneopportunity to perform a task, incorrect actions, such as clicking the wrongbutton or lling in the wrong form, can degrade the user experience. Unfortunately, there is no established training objective specically for webelement nomination (For further details on the complexities of formulatingan objective for element nomination, see Appendix A.6) instead, thecommon practice is to train a model to classify elements and then assessits nomination performance. The distinction between classication andnomination is illustrated in . During training, a GNN embedsa small subset of nodes from each page, and these embeddings are thenpassed to a classier. After training, the models performance is assessedbased on its ability to nominate elements. In nomination, the task is toidentify a specic element from a given class on each page. For instance,the cart button is nominated by applying the trained model to every nodeon the page to obtain classication scores. These scores are then ranked,and the node with the highest cart score is nominated as the cart element.",
  "To excel at the cart button nomination task, the classier must consequently assign the highest cart score tothe true cart element relative to all other elements on the page": "Element nomination is particularly challenging on realistic pages. A perfect classier, which always assignsthe correct class to every element, would also always nominate the correct element on every page. Hence,the classication objective serves as a proxy for training on the nomination task. However, the pages in ourdataset consist of thousands of elements, and among these, there is a single labeled element for each class. Forinstance, a classier with 99% accuracy would be expected to incorrectly classify 10 elements as add-to-cartbuttons on a page with 1, 000 elements, even though only one true add-to-cart button exists. Consequently, in.5, we propose an eective, novel training method that improves the element nomination performanceby steering the classication training objective closer towards the nomination objective.",
  "In the literature, there is an unfortunate scarcity of datasets suitable for web element nomination. In thissection, we walk through and compare existing datasets to the Klarna Product Page Dataset": "The minimalistic World-of-bits environment (Shi et al., 2017b), known as Mini-WoB, is useful for task-solvingon the Web. However, the pages in Mini-WoB are simple compared to sites found on the Internet. Apage in Mini-WoB might, for instance, consist of just a single form and a submit button. In contrast, theKlarna Product Page Dataset comprises manually annotated clones of real webpages, with the median pagefeaturing 1, 308 elements and thousands of lines of code. The recently proposed WebShop (Yao et al., 2022) isanother simulated interactive environment where agents can learn to navigate an e-commerce store and makepurchases based on text instructions. WebShop includes 1.18 million products and 12, 087 text instructions.While it eectively emulates an e-commerce store, its primary limitation is that it comprises pages from asingle website. There are datasets related to ours that are useful for web content extraction, perhaps the most well-knownbeing the SWDE dataset (Hao et al., 2011). This dataset comprises 124, 291 real webpages, each with 3 5labeled attributes, from 80 dierent websites across 8 verticals, such as online bookstores and camera retailers.Previous studies on SWDE (Lin et al., 2020; Zhou et al., 2021) have observed that test accuracy improveswith the number of sites included in the training set for each vertical. However, the SWDE dataset is limitedby having only 10 sites per vertical, thereby capping the potential gains. In comparison, our dataset consistsof 8, 175 diverse e-commerce websites, and thus provides a much more comprehensive representation of avertical. Furthermore, Klarna Product Page Dataset encompasses labels that present a wider range of learningchallenges. Whereas the SWDE dataset solely contains labeled leaf nodes, which are often identiable basedon local information, our dataset includes labels where local information alone is insucient for accurateidentication. Elements like the price and name, for instance, are primarily dened by local information,making their identication similar to standard web data extraction tasks. Conversely, correctly identifyingthe buy button often requires both local and contextual information. Lastly, the subject node has no meaningon its own and can only be represented based on its context. Lastly, unlike the SWDE dataset, which containsonly English pages, our dataset includes pages with text written in multiple languages. A recent large-scale webpage dataset that signies a substantial advancement over previous works is Mind2Web(Deng et al., 2023). Mind2Web encompasses 137 websites across 31 diverse domains, such as Music, Airlines,Housing, and Social Media. Consequently, it oers a valuable opportunity to assess generalizability acrossvarious tasks and domains. Our dataset complements Mind2Web in this aspect, as Klarna Product PageDataset enables the assessment of generalizability for the same task across a large number of websites. Overall, we rmly believe that the diverse range of webpage structures and labels in our dataset marks asignicant advancement over current SOTA datasets for web element nomination. However, we wish toemphasize that having access to a variety of datasets, each with dierent challenges and contexts, is invaluable.Such diversity is crucial for advancing toward truly autonomous and generalizable web agents.",
  "Embedders for DOM Elements": "We now describe the GNN embedders that we train together with a classication layer (see Fig 3) on ourdataset. The embedders represent three GNN families, specically recurrent, convolutional and attention-basednetworks. We now detail and then evaluate these embedders in the simulation section. GCN-Mean. we rst consider a multi-layer GCN model inspired by GraphSage (Hamilton et al., 2017) andPinSage (Ying et al., 2018). Each convolutional layer , computes the representation of the current node, v,as z()",
  "of layers are tunable hyperparameters. By stacking K convolution layers, information can be propagatedfrom K-hop neighborhoods": "TransformerEncoder. This model consists of a single-layer multi-headed attention encoder stack (Vaswaniet al., 2017) fed with a sequence consisting of the features xv of the local node, v, stacked with the featuresxu of its neighbors u N(v). The embedding zv of node v is then extracted as the rst element (here, atindex 0) of the sequence Hv computed by the transformer encoder stack:",
  "In our implementation, the number of heads in the encoder and the embedding size are tunable hyperparame-ters": "We excluded positional encodings from this model, as they negatively impacted the transformers performance.We explored several types of positional encodings, initially formulating them by traversing the DOM treeusing both depth-rst and breadth-rst search orders. Additionally, we attempted, albeit unsuccessfully,to train a general, powerful, and scalable graph transformer (GraphGPS) (Rampek et al., 2023) usingLaplacian eigenvector positional encodings. The ineectiveness of positional encodings in element nomination might be due to the considerable variation inDOM tree structures across dierent websites. In our dataset, the number of nodes varies from a few hundredto approximately 20,000. However, a more consistent representation of page structure could be explored infuture work by considering the positions of node elements in their corresponding rendered screenshots. Forexample, a screenshot of a page on an iPhone X device will have a consistent width across websites, and theposition of elements in the screenshot tends to remain stable (e.g., the \"go to cart\" button is typically locatedin the upper right corner). This stability would make the position in the screenshot a more consistent andmeaningful positional representation compared to the position in the DOM tree. Tree-LSTMs. We consider four tree-based LSTMs. They all revolve around a standard LSTM cell (Hochreiter& Schmidhuber, 1997) and are characterized by how the input sequence is constructed from the DOM tree.",
  "v ). Also, in this case, the weights in both components are trained simultaneously": "GCN-GRU. This model is inspired by the local embedding module in the DOM-Q-Net algorithm (Jiaet al., 2019). Here, the encoding of node v is computed by feeding a Gated Recurrent Unit (GRU) with thelocal features of the node and an average encoding of the neighborhood of the node; then, the embedding iscomputed with a transformation of the local encoding hv and the input features:",
  "where V, b, W, and w, are trained together with the parameters of the GRU": "FreeDOM. We adapt the FreeDOM architecture (Lin et al., 2020). This is a two-stage method where aneural network rst computes node representations based on local and contextual information (using bothtext and HTML), and then uses representation distances and semantic relatedness between node pairs inthe second stage. In the original implementation, (Lin et al., 2020) perform a ltering step where templatescommon to multiple pages are identied and used to reduce the number of nodes that the model considers.On our dataset, this is not possible since the pages generally do not come from the same set of websites. Thatbeing said, FreeDOM may be at a disadvantage compared to the other models because it does not perfectlysuit our application or dataset. However, the authors set out to learn site-invariant feature representationsand found that FreeDOM could generalize well to unseen websites.",
  "Element Nomination Evaluation": "As a starting point for research on the Klarna Product Page Dataset, we now compare various GNNs onthe nomination task. To that eect, we rst describe the common experimental setup in .1 andthen present the results of this empirical evaluation in the following sections. These results rst include thenomination accuracies from the Basic Training Procedure in .2. Next, we use the best-performingmodel from the initial evaluation to lter out most elements from the webpages and further rene ournomination results by applying GPT-4 to the HTML content of the ltered elements in .3. In .4, we conduct a sensitivity analysis on the best-performing GNN to investigate whether it suers fromover-smoothing and over-squashing. Lastly, we demonstrate that the CNTP training scheme signicantlyboosts nomination performance in .5.",
  "REVGNN - 20 layer.81-.65-.62-.79-.66-.89-.74-": "its output is then classied by a single-layer fully connected neural network (see Fig 3 and ). Theclassication gives rise to a cross-entropy training objective that is minimized using Adam. Furthermore, dueto the large class imbalance between the six labeled nodes and the potentially thousands of unlabeled nodeson a page, we cannot train on all nodes of a page. In the Basic Nomination Training Procedure, presented in.2, every model instead classies ve labeled and ten randomly sampled unlabeled DOM elementsfrom each page. In the Challenge Nomination Training Procedure, presented in .5, we conduct anadditional experiment where we train on labeled nodes and unlabeled negative examples that the modelscondently misclassied. Information regarding hyper-parameters can be found in Section A.1. Embedders. We use embedders from three important GNN families, specically recurrent, convolutional,and attention-based networks. Specically, we consider LSTM-based embedders (LSTM-TD, LSTM-BU,LSTM-Bi, LSTM-BiE), the mean-pooled GCN model (GCN-Mean), the GRU-based GCN inspired by DOM-Q-Net (GCN-GRU), a 20 layer and 2 group RevGNN and the transformer-encoder (TE).4 We also includea 20-layer, 2-group RevGNN model, which uses the GCN-Mean model as its base GNN. For comparison,we also present the results of the SOTA FreeDOM architecture, which we train on two dierent sets offeatures. First, we use the original features dened in (Lin et al., 2020), and then we specify an extendedversion (FreeDOM-ext) which, in addition, uses the style features employed by our other models (see the nextparagraph). We also implement a 2-layer Fully Connected Network (FCN) that only looks at the features of alocal node without considering any neighbors, thus acting as a context-oblivious baseline. After training, weevaluate how accurately the GNNs can nominate the six labels described in . Features. For each nodes local features, we dene a set of style features which consists of the boundingbox of the elements as rendered on the page, font weight, font size, number of images contained within thesubtree, visibility, and HTML tag. For the FreeDOM-specic, pre-trained, NLP features described in (Linet al., 2020), we use the Spacy5(Honnibal et al., 2020) library. To also gauge the impact of textual featureson the other models, we perform an experiment where we create an additional feature by embedding the textof a web element using the Universal Sentence Encoder6 (Cer et al., 2018). FreeDOMs implementation staysthe same in this experiment as it already uses text.",
  "Basic Training Procedure": "Setup. In this comparison we trained the models on a subset of the Klarna Product Page Dataset containing10, 000 pages from English markets. We did not train the models on the entire dataset because the LSTMmodels are very computationally expensive. It also oers a more level playing eld for the SOTA FreeDOMarchitecture, since it relies on page text and was originally applied to English websites.",
  "Results": "Nomination Accuracy. contains the nomination results. Note that here precision and recall bothequal accuracy as the methods make exactly one prediction on each page, and there is one correct element perclass. From the results with only style features (no text features), we rst notice that GCN-Mean achieves thehighest average accuracy by a wide margin (6.8 p.p.). Despite its relative simplicity, GCN-Mean consistentlyscores among the best for all tasks explored here. Since GCN-Mean uses k-hop neighborhoods (see ), this suggests that a large proportion of the context relevant for element nomination is concentrated inan elements vicinity. The RevGNN algorithm successfully builds upon the GCN-Mean algorithm, leadingto further improvements in the nal nomination accuracy. Among the runner-ups, LSTM-BiE stands out,as it shows similar performance to FreeDOM-ext (using text), while GCN-GRU, LSTM-TD, LSTM-Bi andLSTM-BU all show similar average performance. It is worth noting that FreeDOM requires two separatetraining stages and that the LSTMs require substantially more compute than GCN-Mean, GCN-GRU andFreeDOM. Classication Accuracy. The classication accuracy of the models considered here are presented in SectionA.5. Despite all models achieving very high and similar classication scores, their nomination performancediers substantially. This illustrates the discrepancy between the classication training objective and thenomination evaluation task. We address this discrepancy using CNTP, which we present further below. The Impact of Contextual Information on Nomination. We found that more context helps performanceon certain tasks. As an indicator of the importance of context, we consider the performance of the FCN.Specically, since the FCN only looks at a local nodes features without including information from neighbors,the gap between the best model and the FCN gives us a lower bound on the gain we can achieve by includingcontextual information. With this contextual importance indicator, it is possible to identify tasks wherecontextual information is more important. For example, the subject node on its own does not contain muchinformation but is instead dened by its relationship to other nodes in the tree; therefore, we need tolook at its context to identify it. For other tasks, context is less important. For example, the buy buttonseemingly contains sucient information to be correctly identied based on its local features. Finally, anotherinteresting observation related to context is that LSTM-BiE outperforms LSTM-Bi on all tasks, indicatingthat processing all context from the page improves accuracy. The Impact of Text on Nomination. When we add text features, GCN-Mean still has the highestaverage accuracy, this time by an even wider margin. Though LSTM-BiE was not trained with text due tocomputational constraints, its performance remains competitive with all other algorithms except GCN-Meanwhen trained on solely style features. This further solidies our intuition that straightforward graphicalalgorithms such as GCN-Mean and LSTM-BiE should be explored further as building blocks in web elementnomination algorithms. Furthermore, one important benet of LSTM-BiE and GCN-Mean over FreeDOM,SimpDOM, and WebFormer is that they remain competitive without text features. This allows us to deploya single model that can be used across webpages, regardless of the language used on the page. Another important nding is that algorithms that try to exploit text perform worse when this informationis irrelevant to the task. We can observe this degradation when GCN-Mean, GCN-GRU, and even FCNattempt to use text to nominate the image node. LSTM-TD performance drops when text is added to itsfeature set since it looks at the text in nodes above the subject node. The subject nodes are usually closeto the root of the DOM tree; hence, it is unlikely that nodes in the considered sequence even contain text.This variety in performance suggests that we need varying degrees of contextual information to nominate",
  "RevGNNRevGNN & GPT-4": ": Substantially Enhanced NominationAccuracy by Combining RevGNN with GPT-4: The performance of the RevGNN algorithm aloneversus its performance when combined with a GPT-4-based renement step. In the enhanced scenario, theRevGNN algorithm initially lters the dataset to retainonly the top 10 elements with the highest classicationscores on each page. Subsequently, GPT-4 undertakesthe nal nomination step, based on the local HTMLcontent of these selected elements. This renementstep signicantly boosts nomination accuracy across alltasks, resulting in an average increase of 10.9 percentagepoints. Setup. The renement step was initiated by lteringout all non-informative elements from the test setusing the trained 20-layer RevGNN algorithm from.2. This ltering was performed by rank-ing the elements by their classication scores andretaining the ten highest-ranked elements. The rawHTML of these elements, combined with their bound-ing boxes, was then used to formulate a query wherethe task was to select the single labeled element fromthe set of elements. We then presented this queryto GPT-4, which performed the nal nomination byproviding an answer to the query. This experimentwas constrained to 500 randomly sampled Englishpages from the test set. Results. From the results shown in , it isevident that the LLM renement step substantiallyenhances the nal nominations, despite relying solelyon local element content. It signicantly boosts thenomination performance across all tasks, resultingin an average nomination accuracy increase of 10.9percentage points. The improvement is particularlynoticeable for the product image element, indicatingthat the raw HTML contains substantial informationfor this specic task. However, there is still room forimprovement, which could potentially be achievedby incorporating screenshots of the rendered pages.",
  "Sensitivity Analysis": "Simple message-passing GNNs may suer from over-smoothing and over-squashing, where information owbetween distant nodes becomes distorted (Topping et al., 2021). This distortion can negatively impactthe models performance in tasks that rely on long-range interactions, such as element nomination, wherecritical information may be located far from the root in large trees. To assess whether the GCN-Meanalgorithm is prone to these issues, we conducted a sensitivity analysis on a subset of 8,000 pages, graduallyincreasing the number of layers, as shown in . The results indicate that while the GCNs performanceinitially improves with more layers, it begins to degrade after adding a third layer, suggesting the presence ofover-smoothing and over-squashing eects. One potential approach to mitigating these eects is graph rewiring, where the DOM-tree is modied toenhance information ow. A particularly promising method that could be explored in future work is ExpanderGraph Propagation (Deac et al., 2022), which augments the DOM-tree with expander graphs. These graphs,despite their sparsity, ensure ecient message passing throughout the entire structure, enabling informationto propagate quickly and eectively. This approach could be well-suited for managing the complexities oflarge DOM-trees, where critical information may be dispersed across the tree.",
  ": Sensitivity analysis results from varying the number of layers in the GCN-Mean Algorithm": "train on the classication objective, which means that we only need to train on a small subset of elementsfrom each page in the training set. We then periodically rank the elements on the page based on theirclassication scores and add the confusing elements to the training set. We dene a confusing element as anunlabeled element that inaccurately receives a very high score for a particular label. This procedure increasesthe likelihood that the true labeled elements appear at the top of the ranking during the nomination phase. Notation. The parameters of CNTP are: M, the number of unlabeled elements to be randomly included inthe training set from each page; T, the number of training epochs; and K, the number of additional hardelements identied by the model in the nomination task after T epochs. CNTP. The pseudo-code for the training data augmentation step is presented in Algorithm 1. A descriptionof the procedure goes as follows: At the start of every epoch, we begin with an empty training set of elementsStrain = ?. For each webpage P , (V, E), where V is the set of elements in the training set P, we uniformlysample a set SP = {vi Vunlabelled : i = 1, . . . , M} of M unlabeled elements. We add SP to the classierstraining set, along with all L labeled elements: Strain = t",
  "P P(SP Vlabelled). We then train the model for": "one epoch. At epoch T, we evaluate the model on the nomination task across the entire set P and initializethe sets of hard training examples, HP = ? for all P P. Then, for all P P, we add the top K unlabeledelements in the proposed ranking for each of the L labels to HP . It is sucient to perform this step once.For all future epochs, these elements are always considered additional training examples in the classicationtask: Strain = t",
  "labeled ones. Note that, as opposed to Ben-Baruch et al. (2022), we attach the negative label to these nodes": "CNTP oers two main benets: 1) it uses only a small fraction of the total elements in the training set, and2) it bridges the gap between the nomination and classication tasks. The rst benet allows us to avoid theimpractical computational cost of training to rank all elements, focusing only on the top candidates insteadof those condently identied as uninteresting. The second benet is achieved by training on more confusingunlabeled elementsthose likely to appear high in the nomination ranking. This approach aids the model indistinguishing between the ground truth elements and the uninteresting elements that receive high scores.",
  "namepricesubject-node": ": Eect on the average condence gap from using hard examples for GCN-Mean. The left plot showsthe average condence gaps during training without augmentation. On the right, we show the correspondingcondence gaps when the hard examples are added to the training set. Eects on Nomination Accuracy. highlights the eect of the timing of the hard exampleaugmentation step on the average nomination accuracy on the validation set while training the FCN andGCN-Mean. We observe that the timing of the augmentation makes little dierence on the nal nominationaccuracy. This suggests that there are a few consistently confusing elements that can be relatively easilyidentied using models trained on randomly selected unlabeled elements as negatives. Once these elementsare in the training set, the models nomination performance improves signicantly. Indeed, both FCN andGCN-Mean achieve lower errors after the augmentation, by about 2 and 5 percentage points, respectively,with GCN-Mean reaching beyond 80 Eects on the Quality of Rankings. We evaluate whether CNTP brings us closer to training directly onthe nomination objective. As a proxy fort his, we measure the condence gap: the dierence between thecondence assigned to the labeled element and the highest condence assigned to any unlabeled element onthe same page. We assume that the larger this dierence is, the better the resulting ranking will be. Sinceeither unlabeled elements become less likely to be mistaken for the true element (when the true element isranked rst) or the dierence by which an unlabeled element wins in the ranking (when the true element isnot ranked rst) is lower, and hence the ground-truth labeled element appears higher in the ranking. In , we ask the models to perform the nomination task on the entire training set every 50 epochs andmeasure the average condence gap per class. From the basic classication objective, we observe that thecondence gap increases on average, yet this behaviour is far from monotonic. In contrast, when we add hardexamples during training, the rankings that the nomination objective is based on gets steadily better when weadd hard examples during training according to our metric. This performance more closely resembles whatwe would expect when optimizing directly for the nomination objective. In this experiment, augmentation isperformed every 50 epochs, though the same behavior is observed when this step is performed only once.",
  "Limitations": "One shortcoming of the Klarna Product Page Dataset is that far from all elements have a ground truthlabel; we only have one label per class per page, which makes our results slightly pessimistic. In reality, aclick event on several elements within the buy button (e.g., the button itself and the node containing thebutton text) could generate the same result. Thus, there are several acceptable nominations in practice.The single label per element type issue reduces the number of positive training examples in the training setand increases the diculty of the nomination task. While this results in a lower overall accuracy than whatwould be achieved in practice, it should not aect the comparison between methods. Furthermore, perfectlyannotating all nodes of each element in a dataset of this size is very challenging and would require substantialweb development knowledge. One approach to reduce the stringency of the evaluation would be to check the",
  "Conclusion": "We introduce Klarna Product Page Dataset, a large-scale realistic dataset containing 51, 701 product pagesfrom 8, 175 merchants across 8 markets, with labels that present varied challenges. To initiate research on ourdataset, we explored the potential of GNNs for nominating webpage elements and how these methods couldbe combined with LLMs. Our experiments demonstrate the untapped potential of GNNs for web elementnomination and illustrate how to eectively combine GNNs with LLMs for this purpose.",
  "OpenAI, :, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni": "Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin,Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mo Bavarian, Je Belgum, Irwan Bello, JakeBerdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdono, Oleg Boiko, Madelaine Boyd,Anna-Luisa Brakman, Greg Brockman, Tim Brooks, Miles Brundage, Kevin Button, Trevor Cai, RosieCampbell, Andrew Cann, Brittany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang,Fotis Chantzis, Derek Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho,Casey Chu, Hyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux, ThomasDegry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning, AdrienEcoet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix, Simn Posada Fishman, JustonForte, Isabella Fulford, Leo Gao, Elie Georges, Christian Gibson, Vik Goel, Tarun Gogineni, GabrielGoh, Rapha Gontijo-Lopes, Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross,Shixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han, Je Harris, Yuchen He, Mike Heaton, JohannesHeidecke, Chris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele, Brandon Houghton, Kenny Hsu,Shengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain, Shawn Jain, Joanne Jang, Angela Jiang, Roger Jiang,Haozhun Jin, Denny Jin, Shino Jomoto, Billie Jonn, Heewoo Jun, Tomer Kaftan, ukasz Kaiser, Ali Kamali,Ingmar Kanitscheider, Nitish Shirish Keskar, Tabarak Khan, Logan Kilpatrick, Jong Wook Kim, ChristinaKim, Yongjik Kim, Hendrik Kirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo, ukasz Kondraciuk,Andrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen Krueger, Vishal Kuo, Michael Lampe, IkaiLan, Teddy Lee, Jan Leike, Jade Leung, Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin, StephanieLin, Mateusz Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Makanju, Kim Malfacini, SamManning, Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew Mayne, Bob McGrew,Scott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil, David Medina, Aalok Mehta,Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin, Vinnie Monaco, Evan Morikawa, DanielMossing, Tong Mu, Mira Murati, Oleg Murk, David Mly, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak,Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh, Long Ouyang, Cullen OKeefe, Jakub Pachocki, AlexPaino, Joe Palermo, Ashley Pantuliano, Giambattista Parascandolo, Joel Parish, Emy Parparita, AlexPassos, Mikhail Pavlov, Andrew Peng, Adam Perelman, Filipe de Avila Belbute Peres, Michael Petrov,Henrique Ponde de Oliveira Pinto, Michael, Pokorny, Michelle Pokrass, Vitchyr Pong, Tolly Powell, AletheaPower, Boris Power, Elizabeth Proehl, Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron"
}