{
  "Abstract": "While LLMs excel in processing text in these human conversations, they struggle with thenuances of verbal instructions in scenarios like social navigation, where ambiguity and un-certainty can erode trust in robotic and other AI systems. We can address this shortcomingby moving beyond text and additionally focusing on the paralinguistic features of theseaudio responses. These features are the aspects of spoken communication that do not in-volve the literal wording (lexical content) but convey meaning and nuance through howsomething is said.We present Beyond Text; an approach that improves LLM decision-making by integrating audio transcription along with a subsection of these features, whichfocus on the affect and more relevant in human-robot conversations. This approach not onlyachieves a 70.26% winning rate, outperforming existing LLMs by 22.16% to 48.30% (gemini-1.5-pro and gpt-3.5 respectively), but also enhances robustness against token manipulationadversarial attacks, highlighted by a 22.44% less decrease ratio than the text-only languagemodel in winning rate. We also present the first dataset on disfluent human audio-guided in-structions ( for future research in thisfield. Beyond Text marks an advancement in social robot navigation and broader human-robot interactions, seamlessly integrating text-based guidance with human-audio-informedlanguage models.",
  "Introduction": "The rapid advancements in text-based Large Language Models (LLMs) like GPT-4 and the Gemini series(Achiam et al., 2023; Reid et al., 2024) have significantly enhanced capabilities in collaborative human-robot interactions (HRI) especially in diverse settings, including task planning, or social navigation (Shahet al., 2023; Liu et al., 2023a; Li et al., 2023; Ren et al., 2023; Liang et al., 2023).Broadly speaking,for HRI tasks, we want to design the robot to think like a human and best carry out its tasks. We as",
  "Published in Transactions on Machine Learning Research (10/2024)": "John Morris, Eli Lifland, Jin Yong Yoo, Jake Grigsby, Di Jin, and Yanjun Qi. Textattack: A frameworkfor adversarial attacks, data augmentation, and adversarial training in nlp. In Proceedings of the 2020Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pp. 119126,2020. Jun Ogata, Masataka Goto, and Katunobu Itou. The use of acoustically detected filled and silent pauses inspontaneous speech recognition. In 2009 IEEE International Conference on Acoustics, Speech and SignalProcessing, pp. 43054308. IEEE, 2009. Jillian L Prestopnik and Beverly Roskos-Ewoldsen. The relations among wayfinding strategy use, sense ofdirection, sex, familiarity, and wayfinding ability. Journal of environmental psychology, 20(2):177191,2000. Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. Robustspeech recognition via large-scale weak supervision. In International Conference on Machine Learning,pp. 2849228518. PMLR, 2023. Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste Alayrac,Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et al. Gemini 1.5: Unlocking multi-modal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. Allen Z Ren, Anushri Dixit, Alexandra Bodrova, Sumeet Singh, Stephen Tu, Noah Brown, Peng Xu, LeilaTakayama, Fei Xia, Jake Varley, et al. Robots that ask for help: Uncertainty alignment for large languagemodel planners. In Proceeding of 2023 Conference on Robot Learning (CoRL 2023), 2023.",
  "Our main contributions are summarized as follows:": "1. Provided Evidence for Advancing LLM Navigation with Vocal Features. The currentfoundation models are unable to capture the nuanced human ambiguity via only textual input.We propose the integration of affective vocal cue analysis beyond text to let intelligent systemslike robots synthesize these streams into actionable intelligence, and thus not only reason what thehuman said, but also how the human said. Our results show low bias and low variance: a 70.26%+winning rate in detecting the uncertainty and generating appropriate next-step action and a lowconfidence score compared to only using LLMs to read textual transcription, indicating increasedhigh confidence in analyzing human uncertainty. Notably, all three different large language modelswe tested, including GPT3.5, GPT3, and Gemini Pro, exhibited a significant increase in winningrates. Beyond Text augments LLMs ability to model human uncertainty in navigational scenarios.",
  "Proposed": ": Beyond Text Overview: With a human instruction audio clip, we simultaneously do transcriptionand vocal affective cue analysis. We prompt the language model to reason and generate five next-step choices.Then, we pick the action based on the highest next token logit probability. Note that with only the transcriptionmodel, the framework chooses the red choice. With affective analysis model, the green choice is picked. navigational instructions with perceptual ground truth captured human labeling. This dataset( is characterized by a diverse range of seman-tic and vocal ambiguities, offering an extensive experimental foundation for our proposed methodand subsequent investigative endeavors in this space. 3. Show Robustness of Vocal Cues on Improving the LLM Decision-Making: We also showthe robustness of our framework against adversarial attacks by conducting token manipulation ex-periments on the prompts fed to LLMs. Under attack, our framework has 22.44% less decrease ratiothan the text-only language model in winning rate. We present a comprehensive ablation studyevaluating the effectiveness of three specific vocal cues we utilized, 15% to 20%+ increase in winningrate clearly indicating the significant role these cues play in augmenting the reasoning capabilitiesof LLMs. Lastly, we discuss the potential of audio augmentation in LLMs, highlighting this as apromising avenue for future research.",
  "Related Works": "Large Language Models for Robotics. Recent studies have demonstrated the versatility of LLMs invarious robotic contexts. These include employing few-shot summarizing skills for task planning in robots(Wu et al., 2023; Liu et al., 2023b), interpreting contextual information for robotic locomotion decision-making (Shek et al., 2023), integrating visual feedback for open-world robotic manipulation (Lin et al.,2023a; Ahn et al., 2022), and enhancing motion planning in autonomous driving Mao et al. (2023). Forhuman-robot interaction, Shah et al. (2023) suggests using LLMs for semantic prediction of object locations,thereby optimizing navigation.Ahn et al. (2022) introduces multimodal input to generate the optimallong-horizon instructions and effectively score the affordance probability of the instruction for robotic taskplanning. Meanwhile, Ren et al. (2023) develops a framework to assess and synchronize the uncertaintylevels in LLM-based planning systems, enabling them to seek assistance when necessary. However, unlikeour framework, Ren et al. (2023) does not account for potentially unreliable human input but rather relieson human selection among generated options.Beyond Text uniquely concentrates on assessing humanuncertainty through affective computing for robotic navigation, particularly when guided by human audiothat might be ambiguous or incorrect. This approach allows robots to gauge the reliability of human-audionavigation instructions by analyzing vocal emotions, enabling more informed reasoning. Foundation Models with Audio. The ability to perceive and interpret auditory information is essentialfor AI agents in the real world. GPT-4s recent update(Achiam et al., 2023), incorporating Whispers audio-",
  "Our Approach": "In this section, we first mathematically characterize our novel framework, Beyond Text, which uses LLMs tomodel human trust and uncertainty based on audio-based navigational guidance. As shown in , weconsider a setting where a robot asks the human for instructions, and then the human provides instructionsin the form of audio instruction. This setting is adapted based on prior work on social navigation (Hu et al.,2019; Lakomkin et al., 2018b; Eppe et al., 2016; Francis et al., 2023). In order to answer the question raisedby the robot, the human replies with a voice prompt v V, which belongs to set of audio responses V, whichis then converted to a textual description S using the best available transcription model Tm : V S (hereS is the set of all possible sentences). Simultaneously, v is mapped to an affective cue set K via an affectivecue model AC : V K. The combination of textual and affective cues is denoted as:",
  "(i) Textual Uncertainty: Phrases like probably, may, might, and I think within the transcribedtext suggest a lack of conviction (Dorbala et al., 2021)": "(ii) Speech Repair: In instances where individuals correct themselves (Lou et al., 2019), such as saying,Take a rightuhh, I mean take a left and go straight, it is crucial to parse and interpret these correctionsto avoid misdirection. (iii) Hesitation Signals: Pauses or hesitations, as in Go straight and, err, take the second left, maysignify uncertainty regarding subsequent directions (Ogata et al., 2009), despite apparent confidence inpreceding instructions. These textual patterns are typical in human communication when conveying uncertainty. We prompt theLLMs to recognize these cues, subsequently adjusting the confidence level in the reliability of the instructionsthat follow these indicators.",
  "Vocal Affective Cue Analysis": "In addition to textual content, the prosody of spoken instructions can be indicative of human uncertainty(Mittal et al., 2020; Goupil et al., 2021; Jiang & Pell, 2017; Guyer et al., 2019). Our analysis focuses on threeprimary auditory features captured within the Mel-frequency cepstral coefficients of the voice waveform todetect potential uncertainty: Duration: When an individual is unsure about the instruction, the speed of their speech either slows downas they are trying to think through what to say or speeds up as becoming more nervous (Jiang & Pell,2017; Szkely et al., 2017). The speech rate is a vocal characteristic indicating certainty tone change (Guyeret al., 2019). We measure the duration of each instruction segment inside the audio and evaluate whetherthe duration has drastically changed, changing it to too long or too short for specific segments. For instance,for a recording that each instruction phrase is as short as 1 second, an elongated phrase like or the secondright with a duration of 5.68 seconds can indicate hesitation and diminished certainty. Pitch: A common feature of uncertainty is a rising intonation at the end of phrases or sentences, which cansound as if the individual is asking a question rather than making a statement (Goupil et al., 2021; Guyeret al., 2019). We identify the most unusual pitch change pattern in the waveform and align it with theinstruction piece by timestamp. Loudness: Alongside pitch, the uncertainty can also be embedded in loudness, since unpredictably risingand falling volume can indicate hesitation or self-doubt (Goupil et al., 2021; Szkely et al., 2017). Similarto pitch, we only detect the most drastic loudness change in the audio clip and align it with the instructionpiece by timestamp to infer possible uncertainty of instruction pieces. To quantitatively measure pitch and loudness features, we design a lightweight yet effective algorithm (seeAlgorithm 1 in AppendixF) to detect the max change of these two features in the audio clip.For theduration, we split the transcription sentence into sub-instructions and measured the duration of each sub-instructions. We only mark the sub-instructions that are significantly longer than other sub-instructions inthe same audio clips as uncertainty signals. Force alignment, a process that synchronizes text fragments totheir corresponding segments in audio narration, is used to align the vocal features timestamps with thesub-instructions. Examples are shown in and detailed in Appendix C. In this work, we focus on significant changes in these audio features that exceeded a predetermined thresholdwhile excluding variations in the initial and final three seconds of recordings to avoid contamination byrecording noise. This audio augmentation prompt serves as input for decision-making processes that leveragelanguage models. In a way, we give the LLMs the ear to listen to human affective cues embedded inside thevocal clip, thus improving their reasoning skills.",
  "In-Context Prompting": "Beyond Text enables the language model to exploit its inherent reasoning capacities for strategic planningand decision-making tasks through In-Context Learning (ICL) (Yousefi et al., 2023; Liu et al., 2023a; Liet al., 2023), by providing only few-shot examples with no explicit training. We illustrate ICL (Yousefi et al.,2023) and Chain-of-Thought (Wei et al., 2022) by including three comprehensive examples that delineate allpotential scenarios a robot might encounter while navigating human interactions for directional assistance.Specifically, we provide examples that: 1. only has language uncertainty; 2. only has vocal uncertainty;3. has both semantic and vocal uncertainty signals. In the response phase, the model reason to suggesta spectrum of five potential actions. Notably, the first option (A) is always a paraphrased version of thetranscription sentence without uncertainty, mimicking the scenario where the robot chooses to blindly followthe human audio-guided instruction; and the final option (E) is consistently presented as ask another personnearby for direction, ensuring a fail-safe in the decision-making process. Detailed prompts and LLM outputexamples are shown in the Appendix C.",
  "Scoring Choices by Polling LLMs": "Each step in an LLM generated plan denoted as y, comprises a sequence of symbols (1, 2, ..., n). TraditionalLLMs calculate the joint probability of these sequences as p(y) = ni=1 p(i|1, . . . , i1). However, thisapproach is flawed due to its high sensitivity to the sequence length n, rendering p(y) an unreliable scoringfunction. To mitigate the length bias, we introduce a multiple-choice Q&A framework for planning. Thismethod refines the planning process to select the most probable next token from a predefined set of labelsY = {A, B, C, D, E}. This results in a probability set p(Y ) = p(A), p(B), p(C), p(D), p(E), from which weselect the label with the highest probability, max p(Y ), as the optimal action. By comparing the confidencechange C = C(vocal) C(text), we assess the impact of vocal prompts on the decision-making process ofLLMs.",
  "New Dataset: Disfluent Navigational Instruction Audio Dataset (DNIA)": "DNIA is a dataset comprising a diverse collection of human audio recordings. All the audio clips are groundedin the robot navigation settings, as we ask the volunteers to give instructions to a location inside the buildingsthey have been to. These recordings capture a spectrum of navigational disfluencies, including 500 audioclips contributed by twenty individuals, including ten males and ten females, and encompassing a wide arrayof native and non-native speaker vocal styles. Each clip, ranging from 10 to 30 seconds, includes a series",
  "Count": "Language Uncertainty (LU) Transcription OnlyWith ReasonWith Reason and Affective Cue (Ours) : Distribution of top log-probability choices by context and audio type. Our work (green) shows a dy-namic change from Transcription Only (Blue). Instead of blindly choosing A and following the instructionwith uncertainty, Beyond Text chooses B, C, D, or E, which successfully identifies the uncertaintycomponent within the audio.",
  "of English directives necessary for navigating to a specified location. The audio files are in WAV format,normalized with a headroom of -2dB, facilitating ease of use and consistent audio quality": "We identified and categorized two types of disfluency within our dataset: language uncertainty (LU): 285data clips and text correct by vocal tone uncertainty (VU): 215 data clips. The LU category is the audiothat contains semantic disfluency signals, such as hesitation and language uncertainty. The VU category isparticularly noteworthy, as it encompasses instances where the textual transcription alone does not manifestany indicators of uncertainty, but the vocal tone shows significant uncertainty clues. Regarding ethics andprivacy concerns, our dataset is collected by a group of fellow students with written consent to anonymouslypublish the recordings of their voices for research purposes. We reviewed all the audio files to ensure nopersonal information is present, and the annotations (including audio filenames) can not be matched to aspecific participant. Authors have human subjects training certificates from the university. Therefore, wefollow the existing guidelines to ensure the DNIA dataset does not have any ethics issues, and we will publishthe dataset soon after the review process for research purposes. shows examples for the audio promptin DNIA, more details can be found in Appendix E. To mimic real-world Human-Robot Interaction (HRI) scenarios as closely as possible, we align with themethodologies suggested in LLM evaluation literature (Bang et al., 2023; Chang et al., 2023). We employhuman annotation as one of the primary assessment techniques. This involves manual evaluation, whichis particularly relevant in HRI contexts due to its ability to capture the nuanced complexities of humaninteraction. We use human evaluation to calculate the winning rate of analyzing human navigational guidanceuncertainty in .4. Our approach entails labeling human audio-guided instructions through a selectionprocess from a set of five multiple-choice options designed to represent the uncertainty inherent in the contentaccurately. We recruit a different set of 20 participants with diverse genders and dialect backgrounds toensure a broad range of perspectives. We developed a user study website (see Appendix H) where theseparticipants listened to navigational audio clips. The instructions emphasized careful listening and unbiasedselection, with a constraint of a maximum of one minute per clip to simulate real-world decision-makingtime pressures.",
  "Experimental Results": "To validate the efficacy of our approach, we utilize confidence score and winning rate to show that BeyondText can achieve low variance and low bias results compared to current LLM-based human instructionswithout vocal cues. We will first introduce the evaluation metric, and the comparison experiments, ablationstudies on various vocal cues and LLM types.",
  "In this paper, we specifically focus on improving the uncertainty or the confidence score of the LM p on theoptimal (ground-truth) candidate responses yj": "We note that as defined in equation 1, our model is given by p(|Q(v)) where Q(v) is combination ofvocal and text cue. For a given Q(v), let us consider {yj}Jj=1 as the set of possible candidates, and theprobability of each candidate is given by p(yj|Q(v)), and we define the probability distribution as :=[p(y1|Q(v)), p(y2|Q(v)), , p(yJ|Q(v))]. Hence, this is clearly interpratable in the sense that it is theprobability distribution across possible candidates.",
  "KL(, ),(3)": "where we note that the confidence measure will reduce as the KL divergence between and increases.The the range of a confidence measure, which is inversely proportional to entropy, is bounded between. Ideally, we would like to have the confidence measure as high as possible, which requires us to reduceKL(, ). We first expand and note that",
  "j (yj) log((yj))": "Notably, a higher confidence scorewhich indicates a lower KL divergence between and is desirable,as it signifies a closer alignment with the ground truth. In this work, we empirically show that adding thevocal cue in the input helps to reduce the KL(, ) by plotting the confidence measure equation 3 for roboticnavigation tasks in .",
  "Confidence Score Improvement": "As discussed in .1, we show that our method achieves a low variance with an increased confidencescore. To better visualize the low variance of our approach, we first selected a subset of 100 data instanceswherein the choices made by Large Language Models (LLMs) were in alignment with user selections, shownin .Then, we analyze the average confidence score across all 500 data clips in the dataset todemonstrate the efficacy and generalizability of our framework, shown in . It reveals a markedincrease in confidence score: Beyond Text significantly surpasses the text-only language model, both thetranscription-only and the LLM using Chain-of-Thought (CoT). This trend was also observed across twodistinct categories. The introduction of affective cues significantly lowers variance, enhancing existing LLMsability to interpret human vocal uncertainty with confidence.",
  "in .3), such as looking for more information or asking another person at the location point wherehuman voice sounds unsure": "The blue bars mostly accumulated on choice (A) (220 out of 500), meaning that with only transcription, theLLM tends to blindly follow the navigational instruction without identifying uncertainty in the instruction,which is undesirable. With reasoning in in-context learning, the distribution shifts, and with the affectivecue audio augmentation, the data that result in top log-prob on choice (A) becomes even lower (15 out of500). The shift in choice distribution qualitatively shows the effectiveness of our method in choosing thenext-step action that handles human vocal uncertainty.",
  "Winning Rate": "Besides indicating low variances and multiple choice distribution change, we also calculate the winningrate. We compare the human annotations for each DNIA data clip with the highest log-probability choicesgenerated by the model to ascertain whether the choices and reasoning processes generated by the LLMs arein harmony with human conceptual understanding.",
  "Ntotal(5)": "where Nsucc represents the number of instances where the LLM selects an option that aligns with the DNIAdatasets human annotations, and Ntotal denotes the total number of audio clips that prompt the LLM.Essentially, the winning rate is analogous to a success rate, ranging from 0% to 100%, with a higher winningrate indicating better performance.",
  "The winning rate reported in our paper is calculated over the entire DNIA dataset, which consists of 500audio clips: 285 for LU and 215 for VU": "The results, as detailed in , show the limitation of current LLMs in explaining vocal uncertainty fromonly textual input and the effectiveness of our audio augmentation approach. Given that the audio clipsoriginated from human speakers, the most reliable measure of their trustworthiness was human perception,hence the incorporation of a user study to gauge the winning rate with which the language model identifieduncertainties in the audio clips and selected the appropriate next-step action. demonstrates that our method exhibits reduced bias, outperforming both the single-modal transcrip-tion method and the reasoning-augmented approach. Integrating vocal affective cues, which allow LLMs to",
  "Ablation Study": "Vocal Cue: To elucidate the impact of individual components within the vocal cue framework, we conducta comprehensive qualitative winning rate study by isolating each category of vocal cues and presenting themexclusively to the language model.We report the vocal cue ablative study in .In the absenceof reasoning, the highest winning rate was observed with the exclusive use of loudness features.Thissuggests that incorporating multiple vocal features without reasoning may overwhelm the model, hinderingits ability to correlate vocal characteristics with uncertainty. Conversely, with reasoning integrated, vocalcues significantly strengthened the LLMs proficiency in interpreting vocal uncertainty and facilitating roboticnavigation, as reflected in the increased winning rates. LLM Type: To show the generalized ability of Beyond Text, we test it on three different popular LLMs,including GPT3.5, GPT 3, and Gemini. Based on the result shown in , we conclude that BeyondText improves the winning rate on all three LLMs significantly. Details about parameters for each type ofLLM is shown in Appendix A.",
  "Robustness to Adversarial LLM Attacks": "With the advancement of language models, safety concerns such as adversarial attacks and token manip-ulation prompts have become increasingly prominent. Adversarial attacks or token manipulation prompts(Ribeiro et al., 2018; Shayegani et al., 2023) could potentially trigger the model to output something unde-sired (Zou et al., 2023; Greshake et al., 2023). Given a piece of text input containing a sequence of tokens,",
  "Ours70.46%72.56%68.77%Ours (Attack)46.90%50.23%44.21%Decrease (in Percentage)33.43%30.77%35.71%": "We demonstrate that Beyond Text exhibits robustness against token manipulation adversarial attacks tar-geting LLMs. Our attack pipeline is designed as follows: Post-transcription, a paraphraser alters the originaltranscription (T1) into a new version (T2), systematically replacing uncertainty semantics with deterministiclanguage. We then adjust the prompt to align the vocal and textual information with T2. Subsequently, theLLM is instructed to replicate T2 as an option (A) instead of generating a freeform paraphrase (T1) devoidof uncertainty cues. This approach highlights the limitations of current LLMs, which tend to over-rely onsemantic textual instructions with no clue about the nuances of vocal delivery. illustrates the attackpipeline A detail example is shown in Appendix D. illustrates the comparative robustness of Beyond Text. While the transcription-only approach ex-hibits a significant performance decrease of 55.87% under attack, Beyond Text, with its audio augmentationpreserving vocal cues, exhibited a notably lower reduction of 33.43%. This suggests that audio augmenta-tion in Beyond Text enables LLMs to maintain reasoning capabilities and resist being misled by text-basedadversarial attacks.",
  "Conclusions and Open Questions": "In our work, we highlight the shortcomings of current LLMs in robotic navigation, specifically their inabilityto compute uncertainties in human audio instructions. This issue largely stems from the reliance on audiotranscriptions in state-of-the-art methods, leading to a loss of critical audio features such as duration, pitch,and loudness. Therefore, we state our position that we cannot just reply to the transcriptions of the humanaudio descriptions but should instead also focus on the audio features. To this end, we introduced BeyondText, a novel approach that integrates the affective cues from the human voice and provides empirical evidencethat it significantly enhances the LLMs ability to interpret human vocal uncertainty. We developed the DNIAaudio dataset to push forward the research in this direction. Notably, Beyond Text outperforms existingtext-based language model techniques in terms of win rate and entropy confidence, as demonstrated in ouruser study. This represents a progression in the development of sophisticated audio-augmented LLMs. Itunderscores the importance of incorporating affective audio computing capabilities in LLMs, particularly forenhancing social robotics navigation tasks. Questions Regarding Setup. While explicit disambiguation, such as asking an additional question, couldindeed clarify instructions, it is not always feasible in real-time interactions where immediacy is crucial. Ourapproach is designed to allow robots to infer potential uncertainties based on vocal cues, enabling them to actmore autonomously and adaptively in dynamic environments. This is particularly important in real-worldscenarios, such as theme parks or shopping malls, where visitors are often unfamiliar with the environment.As first-time visitors, they might provide uncertain or incomplete answers due to their lack of knowledge. Insuch settings, a robot cannot rely on repeatedly asking for clarification to find a confident response. Instead,our method allows the robot to navigate uncertainty and make informed decisions without the need forcontinuous human interaction. While it is true that some existing studies focus on having robots directly re-ask humans to align uncertainty, such as in Ren et al. (2023), our work serves as a complementary approach.We specifically target the robots ability to infer uncertainties based on vocal cues alone, without the need for",
  "Broader Impact Statement": "This research has the potential to advance our understanding of vocal cue interpretation, particularly in fieldssuch as large language models, audio uncertainty quantification, and human-robot interaction. However, itis crucial to acknowledge and address potential biases that may arise in interpreting vocal cues. These biasescould stem from factors such as gender, accent, or cultural background, which may influence how vocalsignals are perceived and understood. While our approach is effective, our approach is restricted by thecapability of current state-of-the-art Large-Language Models. Therefore, it should be applied thoughtfullyand is more appropriate for less sensitive situations to reduce the risk of biases impacting downstream tasks. We are deeply grateful to Bufan Xiao, Xuanting Fina Zhou, Yutong Chloe Wu, Tongjia Zhang, MuqingTang, Linqing Liu, Xindi Tang, Phu Pham, and Hrishikesh Viswanath for their invaluable contributions tothe DNIA dataset collection and their assistance in revising this paper.",
  "Reginald G Golledge. Human wayfinding and cognitive maps. Colonization of unfamiliar landscapes: thearchaeology of adaptation, 25, 2003": "Louise Goupil, Emmanuel Ponsot, Daniel Richardson, Gabriel Reyes, and Jean-Julien Aucouturier. Listenersperceptions of the certainty and honesty of a speaker are associated with a common prosodic signature.nature communications, 2021. Kai Greshake, Sahar Abdelnabi, Shailesh Mishra, Christoph Endres, Thorsten Holz, and Mario Fritz. Morethan youve asked for: A comprehensive analysis of novel prompt injection threats to application-integratedlarge language models. arXiv e-prints, pp. arXiv2302, 2023. Joshua J Guyer, Leandre R Fabrigar, and Thomas I Vaughan-Johnston. Speech rate, intonation, and pitch:Investigating the bias and cue effects of vocal confidence on persuasion. Personality and Social PsychologyBulletin, 45(3):389405, 2019.",
  "Xiaoming Jiang and Marc D Pell. The sound of confidence and doubt. Speech Communication, 88:106126,2017": "Alexander Ku, Peter Anderson, Roma Patel, Eugene Ie, and Jason Baldridge.Room-across-room:Multilingual vision-and-language navigation with dense spatiotemporal grounding.arXiv preprintarXiv:2010.07954, 2020. Lorenz Kuhn, Yarin Gal, and Sebastian Farquhar. Semantic uncertainty: Linguistic invariances for uncer-tainty estimation in natural language generation. International Conference on Learning Representations(ICLR), 2023. Egor Lakomkin, Mohammad Ali Zamani, Cornelius Weber, Sven Magg, and Stefan Wermter. On the ro-bustness of speech emotion recognition for human-robot interaction with deep neural networks. In 2018IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 854860. IEEE, 2018a. Egor Lakomkin, Mohammad Ali Zamani, Cornelius Weber, Sven Magg, and Stefan Wermter. On the ro-bustness of speech emotion recognition for human-robot interaction with deep neural networks. In 2018IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 854860, 2018b. doi:10.1109/IROS.2018.8593571.",
  "Boyi Li, Philipp Wu, Pieter Abbeel, and Jitendra Malik. Interactive task planning with language models,2023": "Linyang Li, Ruotian Ma, Qipeng Guo, Xiangyang Xue, and Xipeng Qiu. Bert-attack: Adversarial attackagainst bert using bert. In Proceedings of the 2020 Conference on Empirical Methods in Natural LanguageProcessing (EMNLP), pp. 61936202, 2020. Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence, and AndyZeng. Code as policies: Language model programs for embodied control. In 2023 IEEE InternationalConference on Robotics and Automation (ICRA), pp. 94939500. IEEE, 2023. Ji Lin, Hongxu Yin, Wei Ping, Yao Lu, Pavlo Molchanov, Andrew Tao, Huizi Mao, Jan Kautz, Mo-hammad Shoeybi, and Song Han.Vila: On pre-training for visual language models.arXiv preprintarXiv:2312.07533, 2023a.",
  "Eva Szkely, Joseph Mendelson, and Joakim Gustafson. Synthesising uncertainty: The interplay of vocaleffort and hesitation disfluencies. In INTERSPEECH, pp. 804808, 2017": "Changli Tang, Wenyi Yu, Guangzhi Sun, Xianzhao Chen, Tian Tan, Wei Li, Lu Lu, Zejun Ma, andChao Zhang.Salmonn: Towards generic hearing abilities for large language models.arXiv preprintarXiv:2310.13289, 2023. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al.Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural InformationProcessing Systems, 35:2482424837, 2022. Jimmy Wu, Rika Antonova, Adam Kan, Marion Lepert, Andy Zeng, Shuran Song, Jeannette Bohg, Szy-mon Rusinkiewicz, and Thomas Funkhouser. Tidybot: Personalized robot assistance with large languagemodels. arXiv preprint arXiv:2305.05658, 2023.",
  "ALLM Implementation Design": "We present the pipeline of querying LLM to analyze human audio inputs in . GPT-4 is used forchoice generation in the first stage, with in-context examples provided. The response contains the chain-of-thought reasoning process for the final action choice.The LLM output is cascaded to GPT-3.5 forscoring the confidence of each choice. In each stage, we enforce LLM to return output in JSON format,ensuring consistency in both our implementation and the parsing process. We also encapsulate the promptsin an API request parallel processor to speed up our workflow.For clarity and transparency, we used",
  "BTranscription Model Prompting": "We employ the state-of-the-art Whisper model (Radford et al., 2023) to transcribe the human audio-guidedinstruction.We use Whisper-Small with 244M parameters that require 2GB VRAM to maintain theseamless transcription speed (within 1 second).We add a custom prompting \"Umm, let me think like,hmm... Okay, heres what Im, like, thinking.\" to enhance its proficiency in capturing subtle nuances ofspeech, such as word mumbling, hesitations, and speech repairs that may sound like a slip of the tongue, asthese are crucial uncertainty-related semantic signals.",
  "LLM Output:": "{\"Reasoning \":\" The phrase \"And then you willprobablyneed to make a leftturn\" has a longduration of 7.40 seconds , whichindicateshesitationor uncertainty in thedirectiongiven. Additionally , the largepitchchange at 23.000 second , whichfallswithin theduration of \"and youwill see lab B\", suggeststhat thespeaker is unsureabout the finalpart of theinstruction. The use of the word probably also adds totheuncertainty of thedirection to make a left turn.\",\"Answer \":{A: Startfrom lab A, go straight , pass lab A and the locker room , make aleft turn , go straight , and you will see lab BB: Startfrom lab A, go straight , pass lab A and the locker room , and askotherpeople forfurtherinstructionC: Startfrom lab A, go straight , pass lab A and the locker room , make aright turn , go straight , and you will see lab BD: Startfrom lab A, go straight , pass lab A and the locker room , make aleft turn , and then ask otherpeople forfurtherinstructionE: Askanotherpersonnear you fordirection}}",
  "Agent Response": "{\"Reasoning \":\"Err is a hesitationsignalsthatshowshumanresponse isnottrustworthy. The longduration on and err , take the second left 3.12 seconds , shows a lack of certainty on whether to takesecondleft to arrive at toilet. The largepitch is a sign of intonationincreasethatsoundslikequestionmark. It is at 02.200 second , inthe range of [00.360 ,03.480] (lastinstructionphrase), confirmthatthe last part of response , and err , take the second left , is nottrustworthy .\",\"Answer \":{\"A\": \"Go straight , take the secondleft\",\"B\": \"Go backward , take the secondleft\",\"C\": \"Go straight , take the firstleft\",\"D\": \"Go straight , at the second left , askanotherperson fordetailedguidance\",\"E\": \"Askanotherpersonnear you fordirection\",}}",
  "Human Audio and Affective Cue:": "\"So first , when you startfrom lab A, you wanna go straightand youwill pass lab A and then the lockerroom. And then you willprobablyneed to make a left turn. And then you go straightandyou will see lab B.\" Largeloudnessdecrease: No ChangeLargepitchchange: at time = 23.000secondDuration: \"So first ,\" => [0.000 ,2.160](2.16seconds);Duration: \"when you startfrom lab A,\" => [2.160 ,4.200](2.04seconds);Duration: \"you wanna go straight\" => [4.200 ,6.840](2.64seconds);Duration: \"and you will pass lab A\" => [6.840 ,9.280](2.44seconds);Duration: \"and then the lockerroom .\" => [9.280 ,12.440](3.16seconds);Duration: \"And then you willprobablyneed to make a left turn .\" =>[12.440 ,19.840](7.40seconds);Duration: \"And then you go straight\" => [19.840 ,22.720](2.88seconds);Duration: \"and you will see lab B.\" => [22.720 ,28.080](5.36seconds););",
  "Choices:": "A: Startfrom the mail office , go straight , at theauditoriummake a leftturn , go straight , take the firstright turn , then go straightagainuntil you see thecafeteriaB: Startfrom the mail office , go straight , at theauditoriummake a leftturn , go straight , take the firstright turn , then ask otherpeople forfurtherinstructionC: Startfrom the mail office , go straight , at theauditoriummake a rightturn , go straight , take the firstright turn , then go straightagainuntil you see thecafeteriaD: Startfrom the mail office , go straight , at theauditoriummake a leftturn , go straight , take the secondright turn , then go straightagainuntil you see thecafeteriaE: Askanotherpersonnear you fordirection",
  "Human Affective Cue:": "Largeloudnessdecrease: at time = 17.000secondLargepitchchange: at time = 23.000secondDuration: \"So startfrom the mail office ,\" => [0.000 ,4.880](4.88seconds);Duration: \"you wanna go straight\" => [4.880 ,8.680](3.80seconds);Duration: \"and then when you are at the auditorium ,\" => [8.680 ,11.040](2.36seconds);Duration: \"you make a left turn .\" => [11.040 ,12.480](1.44seconds);Duration: \"And then you go straightagain\" => [12.480 ,15.680](3.20seconds);Duration: \"and here you wannatake the firstrightturn\" => [15.680 ,22.560](6.88seconds);Duration: \"and then if you just go straight again ,\" => [22.560 ,28.600](6.04seconds);",
  "Reasoning:": "The humanresponsecontains a largeloudnessdecrease at 17.000second anda largepitchchange at 23.000 second , whichcorrespond to thephrases\"and here you wannatake the firstrightturn\" and \"and then if youjust go straight again ,\" respectively . Thesevocalaffectivecuessuggestuncertainty in theseparts of theinstructions . Theduration ofthephrases \"and here you wannatake the firstrightturn\" (6.88seconds) and \"you will see thecafeteria .\" (6.04seconds) are alsolongerthan the other parts , whichcouldindicate a lack of confidenceor certainty in thesedirections.",
  "Before Attack:": "\"So first , when you startfrom lab A, you wanna go straightand youwill pass lab A and then the lockerroom. And then you willprobablyneed to make a left turn. And then you go straightandyou will see lab B.\" Largeloudnessdecrease: No ChangeLargepitchchange: at time = 23.000secondDuration: \"So first ,\" => [0.000 ,2.160](2.16seconds);Duration: \"when you startfrom lab A,\" => [2.160 ,4.200](2.04seconds);Duration: \"you wanna go straight\" => [4.200 ,6.840](2.64seconds);Duration: \"and you will pass lab A\" => [6.840 ,9.280](2.44seconds);Duration: \"and then the lockerroom .\" => [9.280 ,12.440](3.16seconds);Duration: \"And then you willprobablyneed to make a left turn .\" =>[12.440 ,19.840](7.40seconds);Duration: \"And then you go straight\" => [19.840 ,22.720](2.88seconds);Duration: \"and you will see lab B.\" => [22.720 ,28.080](5.36seconds););",
  "\"First , startfrom lab A, you go straightand pass lab A and then thelockerroom. And then you will make a left turn. And then you gostraightand you will see lab B.\"": "Largeloudnessdecrease: No ChangeLargepitchchange: at time = 23.000secondDuration: \"First ,\" => [0.000 ,2.160](2.16seconds);Duration: \"startfrom lab A,\" => [2.160 ,4.200](2.04seconds);Duration: \"you go straight\" => [4.200 ,6.840](2.64seconds);Duration: \"and pass lab A\" => [6.840 ,9.280](2.44seconds);Duration: \"and then the lockerroom .\" => [9.280 ,12.440](3.16seconds);Duration: \"And then you will make a left turn .\" => [12.440 ,19.840](7.40seconds);Duration: \"And then you go straight\" => [19.840 ,22.720](2.88seconds);Duration: \"and you will see lab B.\" => [22.720 ,28.080](5.36seconds););",
  "EDNIA Dataset Details and Samples": "For VU clips, we ensure no semantic uncertainty signals, such as uncertain-related words (i.e., maybe, Iguess), speech repairs, and hesitations. We believe those are common affective characteristics associatedwith human speech. However, those affective cues are not prominent in LU clips. As our position states thataffective cues (duration, pitch, loudness) are associated with speech uncertainty levels in robot navigationtasks, we believe the setup of our dataset is comprehensive enough to show that vocal cues are good predictors.",
  "Total66154959689LU4090525350VU2664434339": "7. The instructions emphasized careful listening and unbiased selection, with a constraint of one minute perclip to simulate real-world decision-making time pressures. We recruited a diverse group of 20 participants,varying in gender and dialect backgrounds, to ensure a broad range of perspectives. From the datasets ground truth scores, its evident that the majority of correct choices fall within theB,C,D,E options. This aligns with the results in , further demonstrating that our methodachieves higher accuracy than the baselines.",
  "FPitch and Loudness Shifts Detection": "Algorithm 1 detects the largest shifts in pitch and loudness in a given audio clip. We use the timestamp toalign with the sequence of instructions for vocal cue prompts.Algorithm 1 Loudness and Pitch DetectionRequire: audio V, l (loudness threshold), p (pitch threshold)Ensure: Timestamps of maximum loudness change (tl) and pitch shift (tp)",
  "GSensitivity Study to Loudness and Pitch Threshold": "We conducted a threshold experiment on loudness and pitch to demonstrate the effectiveness of our method.With loudness levels set at 0.03 and 0.05, and pitch set at 300 Hz and 500 Hz respectively, the winning rate ofour method consistently outperformed the baseline methods by a significant margin. Specifically, compared tothe \"Transcription Only LLM,\" which had a winning rate of 22.16% (total), 21.75% (Language Uncertainty,LU), and 22.79% (Vocal Uncertainty, VU), and the \"With Chain-of-Thought Reasoning\" method, whichachieved 49.30% (total), 58.60% (Language Uncertainty, LU), and 36.74% (Vocal Uncertainty, VU), our",
  "HUser Interface for Human Evaluation on DNIA Dataset": "This section details the user interface that we used for human assessment. As shown in , the websiteallows users to log in and participate in the human evaluation session. The user is asked to listen to arandomly selected audio clip from the audio dataset and answer the question by choosing the most accurateand efficient answer from a list of five options. Once the user select the answer, they need to hit Confirmbutton to submit the answer the proceed to the next question."
}