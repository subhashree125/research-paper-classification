{
  "Abstract": "Operator learning has become a powerful tool in machine learning for modeling com-plex physical systems governed by partial differential equations (PDEs). Although DeepOperator Networks (DeepONet) show promise, they require extensive data acquisition.Physics-informed DeepONets (PI-DeepONet) mitigate data scarcity but suffer from ineffi-cient training processes. We introduce Separable Operator Networks (SepONet), a novelframework that significantly enhances the efficiency of physics-informed operator learn-ing.SepONet uses independent trunk networks to learn basis functions separately fordifferent coordinate axes, enabling faster and more memory-efficient training via forward-mode automatic differentiation.We provide a universal approximation theorem for Se-pONet proving the existence of a separable approximation to any nonlinear continuousoperator.Then, we comprehensively benchmark its representational capacity and com-putational performance against PI-DeepONet. Our results demonstrate SepONets supe-rior performance across various nonlinear and inseparable PDEs, with SepONets advan-tages increasing with problem complexity, dimension, and scale. For 1D time-dependentPDEs, SepONet achieves up to 112 faster training and 82 reduction in GPU mem-ory usage compared to PI-DeepONet, while maintaining comparable accuracy.For the2D time-dependent nonlinear diffusion equation, SepONet efficiently handles the com-plexity, achieving a 6.44% mean relative 2 test error, while PI-DeepONet fails due tomemory constraints.This work paves the way for extreme-scale learning of continuousmappings between infinite-dimensional function spaces. Open source code is available at",
  "Published in Transactions on Machine Learning Research (12/2024)": ": Performance comparison of PI-DeepONet and SepONet with TanH trunk network activationfunctions, varying number of training points (Nc) and fixed number of input functions (Nf = 100). Resultsshow test accuracy, GPU memory usage, and training time for four PDEs. : Performance comparison of PI-DeepONet and SepONet with TanH trunk network activationfunctions, increasing number of input functions (Nf) and fixed number of training points (Nc = 128d, whered is the problem dimension). Note: PI-DeepONet results for the (2+1)-dimensional diffusion equation areunavailable due to memory constraints.",
  "Introduction": "Operator learning, which aims to learn mappings between infinite-dimensional function spaces, has gainedsignificant attention in scientific machine learning thanks to its ability to model complex dynamics in physicssystems. This approach has been successfully applied to a wide range of applications, including climatemodeling (Kashinath et al., 2021; Pathak et al., 2022), multiphysics simulation (Liu et al., 2023; Cai et al.,2021; Mao et al., 2021; Lin et al., 2021; Kontolati et al., 2024), inverse design (Lu et al., 2022b; Gu et al.,2022) and more (Shukla et al., 2023; Gupta & Brandstetter, 2022). Various operator learning algorithms(Lu et al., 2021; Li et al., 2020b;a; Ovadia et al., 2023; Wen et al., 2022; Ashiqur Rahman et al., 2022) havebeen developed to address these applications, with Deep Operator Networks (DeepONets) (Lu et al., 2021)being particularly notable due to their universal approximation guarantee for operators (Chen & Chen, 1995;Lanthaler et al., 2022; Gopalani et al.) and robustness (Lu et al., 2022a). To approximate the function operator G : U S, DeepONets are trained in a supervised manner using adataset of Nf pair functionsu(i), s(i)Nfi=1, where in the context of parametric partial differential equations(PDEs), each u(i) represents a PDE configuration function and each s(i) represents the corresponding solu-tion. Unlike traditional numerical methods which require repeated simulations for each different PDE config-uration, once well trained, a DeepONet allows for efficient parallel inference in abstract infinite-dimensionalfunction spaces. Given new PDE configurations, it can immediately provide the corresponding solutions.However, this advantage comes with a significant challenge: to achieve satisfactory generalization error, thenumber of required input-output function pairs grows quadratically (Lu et al., 2021; Lanthaler et al., 2022;Liu et al., 2022a; Gopalani et al.). Generating enough function pairs can be computationally expensive oreven impractical in some applications, creating a bottleneck in the effective deployment of DeepONets. Physics-informed deep operator networks (PI-DeepONet) (Wang et al., 2021b) have been introduced as asolution to the costly data acquisition problem.Inspired by physics-informed neural networks (PINNs)(Raissi et al., 2019), PI-DeepONet learns operators by constraining the DeepONet outputs to approximatelysatisfy the underlying governing PDE system parameterized by the input function u. This is achieved bypenalizing the physics loss (including PDE residual loss, initial loss and boundary loss), thus eliminating",
  ". We provide a theoretical foundation for SepONet through the universal approximation theorem,proving its capability to approximate any nonlinear continuous operator with arbitrary accuracy": "3. We provide extensive benchmarks validating SepONets representational capacity and computationalperformance relative to PI-DeepONet on a range of 1D and 2D time-dependent nonlinear PDEs.Our findings reveal that scaling up SepONet in both number of functions and collocation points con-sistently improves its accuracy, while typically outperforming PI-DeepONet. On 1D time-dependentPDEs, we achieve up to 112 training speed-up with minimal memory increase. Notably, at mod-erately large scales where training PI-DeepONet exhausts 80GB of GPU memory, SepONet trainsand operates efficiently with less than 1GB. Furthermore, we observe efficient scaling of SepONetwith problem dimension, enabling accurate prediction of 2D time-dependent PDEs at scales wherePI-DeepONet fails.",
  "N(u, s) = 0, I(u, s) = 0, B(u, s) = 0,(1)": "where N is a nonlinear differential operator, I and B represent the initial and boundary conditions, u Udenotes the PDE configurations (source terms, coefficients, initial conditions, and etc.), and s S denotesthe corresponding PDE solution. Assuming that for any u U there exists a unique solution s S, we candefine the solution operator G : U S as s = G(u).",
  "A widely used framework for approximating such an operator G involves constructing G through threemaps (Lanthaler et al., 2022):G G := D A E.(2)": "First, the encoder E : U Rm maps an input function u U to a finite-dimensional feature representation.Next, the approximator A : Rm Rr transforms this encoded data within the finite-dimensional spaceRm to another finite-dimensional space Rr. Finally, the decoder D : Rr S produces the output functions(y) = G(u)(y) for y K1.",
  "Deep Operator Networks (DeepONet)": "The original DeepONet formulation (Lu et al., 2021) can be analyzed through the 3-step approximationframework (2). The encoder E : U Rm maps the input function u to its point-wise evaluations at m fixedsensors x1, x2, . . . , xm K, e.g., (u(x1), ..., u(xm)) = E(u). Two separate neural networks (usually multilayerperceptrons), the branch net and the trunk net, serve as the approximator and decoder, respectively. Thebranch net b : Rm Rr parameterized by processes (u(x1), . . . , u(xm)) to produce a feature embedding(1, 2, . . . , r). The trunk net t : Rd Rr with parameters , takes a continuous coordinate y = (y1, ..., yd)as input and outputs a feature embedding (1, 2, . . . , r). The final DeepONet prediction of a function u fora query y is:",
  "where is the vector dot product and = (, ) represents all the trainable parameters in the branch andtrunk nets": "Despite DeepONets remarkable success across a range of applications in multiphysics simulation (Cai et al.,2021; Mao et al., 2021; Lin et al., 2021), inverse design (Lu et al., 2022b), and carbon storage (Jiang et al.,2023), its supervised training process is highly dependent on the availability of training data, which can becostly. Indeed, the generalization error of DeepONets scales quadratically with the number of training input-output function pairs (Lu et al., 2021; Lanthaler et al., 2022; Liu et al., 2022a; Gopalani et al.). Generating alarge number of high-quality training data is expensive or even impractical in some applications. For example,in simulating high Reynolds number (Re) turbulent flow (Pope, 2001), accurate numerical simulations requirea fine mesh, leading to a computational cost scaling with Re3 (Kochkov et al., 2021), making the generationof sufficiently large and diverse training datasets prohibitively expensive. To address the need for costly data acquisition, physics-informed deep operator networks (PI-DeepONet)(Wang et al., 2021b), inspired by physics-informed neural networks (PINNs) (Raissi et al., 2019), havebeen proposed to learn operators without relying on observed input-output function pairs.Given adataset of Nf input training functions, Nr residual points, NI initial points, and Nb boundary points:",
  "(5)": "Here, I and b denote the weight coefficients for different loss terms. However, as noted in the originalPI-DeepONet paper (Wang et al., 2021b), the training process can be both memory-intensive and time-consuming. Similar to PINNs (Raissi et al., 2019), this inefficiency arises because optimizing the physics lossrequires calculating high-order derivatives of the PDE solution with respect to numerous collocation points,typically achieved via reverse-mode automatic differentiation (Baydin et al., 2018). This process involvesbackpropagating the physics loss through the unrolled computational graph to update the model parameters.For PI-DeepONet, the inefficiency is even more pronounced, as the physics loss terms (equation (5)) mustbe evaluated across multiple PDE configurations. Although various works (Chiu et al., 2022; He et al., 2023;Cho et al., 2024) have proposed different methods to improve the training efficiency of PINNs, little researchhas focused on enhancing the training efficiency of PI-DeepONet. We propose to address this inefficiencythrough a separation of input variables.",
  "M[t]s(y) = L1[y1]s(y) + + Ld[yd]s(y),(6)": "where M[t] =ddt + h(t) is a first order differential operator of t, and L1[y1], ..., Ld[yd] are linear secondorder ordinary differential operators of their respective variables y1, ..., yd only. Furthermore, assume we areprovided Robin boundary conditions in each variable and separable initial condition s(t = 0, y1, . . . , yd) =dn=1 n(yn) for functions n(yn) that satisfy the boundary conditions. Then, leveraging Sturm-Liouvilletheory and some massaging, the solution to this problem can be written",
  "n=1Y kn (yn),(7)": "where k is a lumped index that counts over infinite eigenfunctions of each Li operator (potentially withrepeats). For example, given n {1, ..., d}, LnY kn (yn) = knY kn for eigenvalue kn R. T k(t) depends on allthe eigenvalues kn corresponding to index k. Ak R is a coefficient determined by the initial condition.More details can be found in Appendix C. The method of separation of variables applied to a linear heatequation example can be found in Appendix D.2. One may notice the resemblance between the form of the DeepONet prediction in (3) with (7), providedk = Ak and k = T k(t) di=1 Y ki (yi), with appropriately ordered k. We leverage this similarity explicitly inthe construction of SepONet below.",
  "(8)": "where is the Hadamard (element-wise) vector product and is the vector dot product. Here, k = b(E(u))kis the k-th output of the branch net, as in DeepONet. However, unlike DeepONet, which employs a singletrunk net that processes each collocation point y individually, SepONet uses d independent trunk nets,tnn : R Rr for n = 1, . . . , d. In particular, n,k = tnn(yn)k denotes the k-th output of the n-th trunk net.Importantly, the parameters of the n-th trunk net n are independent of all other trunk net parameter sets.Viewed through the 3-step approximation framework (2), SepONet and DeepONet have identical encoderand approximator but different decoders. Equation (8) can be understood as a low-rank approximation of the solution operator by truncating the basisfunction series (represented by the output shape of the trunk nets) at a maximal number of ranks r. SepONetnot only enjoys the advantage that basis functions for different variables with potentially different scales canbe learned more easily and efficiently, but also allows for fast and efficient training by leveraging forward-mode automatic differentiation, which we will discuss in .1 and .2. Moreover, despitethe resemblance between (8) and the separation of variables method for linear PDEs (7) (discussed belowin .1.3), we find that SepONet can effectively approximate the solutions to nonlinear parametricPDEs. Indeed, we provide a universal approximation theorem for separable operator networks in .3and extensive accuracy and performance scaling numerical experiments for nonlinear and inseparable PDEsin . Finally, it is worth noting that if one is only interested in solving deterministic PDEs under a certainconfiguration (i.e., u is fixed), then the coefficients k are also fixed and can be absorbed by the basisfunctions. In this case, SepONet will reduce to separable PINN Cho et al. (2024), which has been proven tobe efficient and accurate in solving single-scenario PDE systems (Es kin et al., 2024; Oh et al., 2024).",
  "SepONet Architecture and Implementation Details": "Suppose we are provided a computation domain K1 = d of dimension d and an input function u. Toevaluate the residual loss term in equation (5) on Nr random collocation points, PI-DeepONet samples allNr points directly from K1. However, if Nr can be approximately factorized as N1 N2 Nd (e.g.,1024 = 1688 for d = 3), and we relax the Monte Carlo sampling requirement for d-dimensional collocationpoints, SepONet only needs to randomly sample Nn points along the n-th coordinate axis, resulting in atotal of N1 + N2 + + Nd samples. It is important to note that SepONets mapping from N1+N2+ +Nd inputs to N1N2 Nd outputsis most efficient when using regular grid sampling. However, we have empirically demonstrated (in ) that this per-axis grid-based sampling strategy does not degrade SepONets accuracy compared to PI-DeepONets Monte Carlo random sampling over the entire domain K1. Irregular domains may be sampledby (a) dividing the irregular domain into subdomains each approximated by a regular grid, or (b) applyinga coordinate transformation to map the irregular domain onto a regular one (e.g., converting Cartesian topolar coordinates for a circular domain). For shorthand and generality, we will denote the dataset of input points for SepONet as D = {y(:)1 , . . . , y(:)d }.Each y(:)n= {y(i)n }Nni=1 represents an array of Nn samples along the n-th coordinate axis for a total ofN1 + N2 + + Nd samples. The initial and boundary points may be separately sampled from K1; thenumber of samples (NI and Nb) and sampling strategy are equivalent for SepONet and PI-DeepONet.",
  "where is the (outer) tensor product, which produces an output predictive array along a meshgrid ofN1 N2 Nd collocation points. Notably, (:)n,k = tnn(y(:)n )k represents a vector of Nn values produced": "by the n-th trunk net along the k-th output mode after feeding all y(:)n points. After taking the outer productalong each of n = 1, . . . , d dimensions for all r modes, the modes are sum-reduced with the predictions ofthe branch net k = b(E(u))k. While not shown here, our implementation also batches over input functions{u(i)}Nfi=1 for Nf functions. Thus, for only Nf + N1 + + Nd inputs, SepONet produces a predictive arraywith shape Nf N1 Nd.",
  "Model Update": "In evaluation of the physics loss (4), SepONet enables more efficient computation of high-order derivativesin terms of both time and memory use compared to PI-DeepONet by leveraging forward-mode automaticdifferentiation (AD) (Khan & Barton, 2015). This is fairly evident by the form of (9). For example, tocompute derivatives of all SepONet outputs with respect to the m-th variable ym:",
  "ym:= e(k)J[tmm(y(:)m )]1,(11)": "where e(k) selects the k-th output mode from the resulting r Nm JVP output.This is equivalent toforward-mode AD. Consequently, the derivatives along the m-th coordinate axis across the entire grid ofpredictions can be obtained by pushing forward derivatives of the m-th trunk net, and then reusing theoutputs of all other n = m trunk nets via outer product. By contrast, PI-DeepOnet must compute derivativesG(y(i)1 , ..., y(i)d )/ym for each input-output pair individually, y(i) = (y(i)1 , ..., y(i)d ) for i = 1, ..., M, wherethere is no such computational advantage and it is more prudent to use reverse-mode AD. Fundamentally, theadvantage of SepONet for using forward-mode AD can be attributed to the significantly smaller input-outputrelationship when evaluating along coordinate grids RN1++Nd RN1Nd compared to PI-DeepONetRMd RM1 when we choose M = N1N2 Nd.The time and space complexity analysis below in.2 provides a more descriptive breakdown of computational scaling behavior. For a more detailedexplanation of forward- and reverse-mode AD, we refer readers to Cho et al. (2024); Margossian (2019). Oncethe physics loss is computed, often involving multiple evaluations of (10), reverse-mode AD is employed toupdate the model parameters = (, 1, . . . , d).",
  "SepONet (Forward AD)O(N d Lr2 + N d rd)O(N rd + N d)PI-DeepONet (Reverse AD)O(N d Lr2)O(N d Lr)": "branch net with the basis functions learned by the trunk nets.Indeed, intriguing comparisons can bemade between the results of the method of separation of variables (7) and trained SepONet predictions(8).For an initial value problem with linear, separable PDE operator treated in .3, we mightexpect SepONet to learn initial value function dependent coefficients b(E(u))k = Ak and spatiotemporalbasis functions tnn(yn)k = Y kn (yn) (provided we supply one additional trunk net t00(t)k = T k(t) for thetemporal dimension) for appropriately ordered modes k and sufficiently large r. Examples of the learnedbasis functions for a separable 1D time-dependent heat equation initial value problem example are providedin Appendix D.2 as a function of the number of the trunk net output shape r. For a small number of modesr = 1 or r = 2, SepONet learns nearly the exact spatiotemporal basis functions obtained by separationof variables. For larger r, the SepONet basis functions do not converge to the analytically expected basisfunctions. Nevertheless, approximation error is observed to improve with r, and near perfect accuracy isobtained at large r by comparison to numerical estimates of the analytic solution. While the form of SepONet predictions (8) resemble separation of variables, we note that the method ofseparation of variables typically only applies to linear PDEs with restricted properties. In spite of this,SepONet is capable of accurately approximating arbitrary operator learning problems (including nonlinearPDEs) as guaranteed by a universal approximation property, provided below in .3.",
  "Complexity Analysis": "Suppose we are provided a computational domain K1 = d of dimension d. For PI-DeepONet, collocationpoints are sampled randomly from the entire d-dimensional domain, with a total of M points. For SepONet,as described previously in .1, we randomly sample N1 + N2 + + Nd inputs for Nn pointsalong the n-th axis, and construct a Cartesian product grid in K1 via (9). The resulting output of PI-DeepONet has shape M 1, and the output of SepONet has shape N1 N2 Nd. For simplicity, weassume all trunk nets are L-layer fully connected networks with hidden and output dimensions r, and thatN1 = N2 = = Nd = N, and M = N d. The resulting time and space complexity to compute first-orderderivatives of all SepONet and PI-DeepONet outputs is provided in . The first term in SepONets time and space complexity is due to the forward-mode AD computation of each ofthe d trunk networks derivatives with respect to N inputs per axis. The second term, containing N d, is fromcomputing and storing the tensor product. On the other hand, PI-DeepONet individually backpropagatesall M = N d outputs, resulting in complexity scaling with N d. From this analysis, in the limiting case of N d1 Lr, we observe that both SepONet and PI-DeepONethave time and space complexities that include N d due to evaluations over all points in a d-dimensional space.However, since typically d Lr, SepONet is more efficient in practice due to smaller coefficients in the N d term. Moreover, the tensor product in SepONet can be greatly accelerated by GPU parallelization, whichis not taken into account in this analysis. In the limiting case Lr N d1, the first term in SepONets timecomplexity dominates O(N d Lr2), or in other words, it scales linearly with dimension and sub-linearlywith the total number of collocation points N d. This situation is not uncommon in many practical 2D and3D operator learning problems. Note that only considered first-order derivatives. Higher-order derivatives are typically needed toevaluate physics loss functions (5). Fortunately, higher-order derivatives for SepONet are computed withsimilar complexity to , since they amount to sequentially repeating the Jacobian-vector products(JVP) from (10) and (11). Lastly, we did not consider the parameter update for training with a physics loss",
  "Universal Approximation Property of SepONet": "The universal approximation property of DeepONet has been discussed in Chen & Chen (1995); Lu et al.(2021). Here we present the universal approximation theorem to show that proposed separable operatornetworks can also approximate any nonlinear continuous operators that map infinite-dimensional functionspaces to others. Theorem 1 (Universal Approximation Theorem for Separable Operator Networks). Suppose that is aTauber-Wiener function, g is a sinusoidal function, X is a Banach space, K X, K1 Rd1 and K2 Rd2are three compact sets in X, Rd1 and Rd2, respectively, U is a compact set in C (K), G is a nonlinearcontinuous operator, which maps U into a compact set S C (K1 K2), then for any > 0, there arepositive integers n, r, m, constants cki , 1k, 2k, kij, ki R, points 1k Rd1, 2k Rd2, xj K, i = 1, . . . , n,k = 1, . . . , r, j = 1, . . . , m, such that",
  "Remark 1. The definition of the Tauber-Wiener function is given in Appendix A.1. It is worth noting thatmany common-used activations, such as ReLU, GELU and Tanh, are Tauber-Wiener functions": "Remark 2. Here we show the approximation property of a separable operator network with two trunk nets,by repeatedly applying trigonometric angle addition formula, it is trivial to separate y as (y1, y2, . . . , yd) K1 K2 . . . Kd and extend (12) to d trunk nets. Remark 3. In our assumptions, we restrict the activation function for the trunk nets to be sinusoidal. Thischoice is motivated by the natural suitability of sinusoidal functions for constructing basis functions (Stein& Shakarchi, 2011) and their empirical effectiveness in solving PDEs (Sitzmann et al., 2020). However, itwould be interesting to explore whether Theorem 1 still holds when g is a more general activation function,such as a Tauber-Wiener function. We will leave this investigation for future work. Remark 4. Here we assume a two-layer branch network and d one-layer trunk networks with sinusoidal ac-tivations. For practical implementation, our theoretical results can be extended to multi-layer trunk networksby leveraging the Universal Approximation Theorem (UAT) to approximate sinusoidal functions. Remark 5. Theorem 1 suggests the existence of a separable operator network approximation for any non-linear continuous operator. This does not imply error bounds nor tractable scaling laws with respect to anyspecific error metric, nor does it provide a prescription for how to define and update model parameters.Below we will provide experimental evidence that error scaling is comparable to PI-DeepONet when usingphysics-informed operator learning. Please note that error bounds for the supervised training of DeepONethave been previously derived by Lanthaler et al. (2022); similar error bounds for SepONet are not providedin this work.",
  "physics loss (equation (4)) on a dataset D consisting of input functions, residual points, initial points, andboundary points. PDE definitions are summarized in": "We set the number of residual points to Nr = N d = Nc, where d is the problem dimension and N isan integer. Here, Nc refers to the total number of training points. For SepONet, the residual points aregenerated by randomly sampling N points along each axis and constructing a Cartesian product grid. Incontrast, for PI-DeepONet, the N d residual points are randomly sampled from the entire d-dimensionaldomain. The number of initial and boundary points per axis is set to NI = Nb = N =dNc, and thesepoints are also randomly sampled from the solution domain. For each PDE, the model size remains fixed asNc or Nf varies. Specifically, both PI-DeepONet and SepONet have branch and trunk networks of the samesize; the main difference is that SepONet uses d independent trunk networks, one for each axis. We evaluate both models by varying the number of input functions (Nf) and training points (Nc) across fourkey perspectives: test accuracy, GPU memory usage, training time, and extreme-scale learning capabilities.The main results are illustrated in and , with complete test results reported in Appendix B.3.Loss functions, training details, and problem-specific parameters are provided in Appendix B.1 and AppendixB.2. We provide additional ablation studies for our test results using trunk networks with hyperbolic tangentactivation functions in Appendix B.4.1, and varied number of input sensors for the branch net in AppendixB.4.2.",
  "While both models show improved accuracy with increasing Nc or Nf, their memory usage patterns differsignificantly. This divergence is particularly evident in the case of the advection equation": "When fixing Nf = 100 and varying Nc, PI-DeepONet exhibits a steep increase in GPU memory consumptionduring training, rising from 0.967 GB at Nc = 82 to 59.806 GB at Nc = 1282. In contrast, SepONet maintainsa relatively constant and low memory footprint during training, ranging between 0.713 GB and 0.719 GBacross the same range of Nc. Similarly, when fixing Nc = 1282 and varying Nf from 5 to 100, PI-DeepONetsmemory usage escalates from 3.021 GB to 59.806 GB. SepONet, however, maintains a stable memory usagethroughout this range.",
  "Training time": "The training time scaling exhibits a pattern similar to memory usage, as demonstrated by the advection equa-tion example. As Nc increases from 82 to 1282 with fixed Nf = 100, PI-DeepONets training time increasessignificantly from 0.0787 to 8.231 hours (2.361 to 246.93 ms per iteration). In contrast, SepONet maintainsrelatively stable training times, ranging from 0.0730 to 0.0843 hours (2.19 to 2.529 ms per iteration) over thesame Nc range. Similarly, when varying Nf from 5 to 100 with fixed Nc = 1282, PI-DeepONets training timeincreases from 0.3997 to 8.231 hours (11.991 to 246.93 ms per iteration). SepONet, however, keeps trainingtimes between 0.0730 and 0.0754 hours. These results demonstrate SepONets superior scalability in termsof training time. The ability to maintain near-constant training times across a wide range of problem sizesis a significant advantage, particularly for large-scale applications where computational efficiency is crucial.",
  "The Burgers and nonlinear diffusion equations highlight SepONets capabilities in extreme-scale learningscenarios": "For the Burgers equation, PI-DeepONet encounters memory limitations at larger scales. As seen in (c), PI-DeepONet can only compute up to Nc = 642, achieving a relative 2 error of 13.72%. In contrast,SepONet continues to improve, reaching a 7.51% error at Nc = 1282. The nonlinear diffusion equationfurther emphasizes this difference. In (d), PI-DeepONet results are entirely unavailable due to out-of-memory issues. SepONet, however, efficiently handles this complex problem, achieving a relative 2 errorof 6.44% with Nc = 1283 and Nf = 100. demonstrates SepONets ability to tackle even larger scalesfor the Burgers equation. It achieves a relative 2 error as low as 4.12% with Nc = 5122 and Nf = 800, whilemaintaining reasonable memory usage (10.485 GB) and training time (0.478 hours). These results underscoreSepONets capability to handle extreme-scale learning problems beyond the reach of PI-DeepONet due tocomputational constraints.",
  "Relative 2 error (%)6.606.215.684.465.384.12Memory (GB)0.9661.4662.4664.4665.59310.485Training time (hours)0.07710.09570.12380.17170.27510.478": "complex systems. Physics-informed DeepONets (PI-DeepONets) relax the data requirement but at the costof resource-intensive training processes. This creates a challenging trade-off: balancing resource allocationeither before training (data generation) or during training (computational resources). Our proposed Separable Operator Networks (SepONet) effectively address both of these concerns. Inspiredby the separation of variables technique typically used for linear PDEs, SepONet constructs its own basisfunctions to approximate a nonlinear operator. SepONets expressive power is guaranteed by the universalapproximation property (Theorem 1), ensuring it can approximate any nonlinear continuous operator witharbitrary accuracy. Our numerical results corroborate this theoretical guarantee, demonstrating SepONetsability to handle complex, nonlinear systems efficiently. By leveraging independent trunk networks for different variables, SepONet enables an efficient implementa-tion via forward-mode automatic differentiation (AD). This approach achieves remarkable efficiency in bothdata utilization and computational resources. SepONet is trained solely by optimizing the physics loss, elimi-nating the need for expensive simulations to generate ground truth PDE solutions. In terms of computationalresources, SepONet maintains stable GPU memory usage and training time, even with increasing trainingdata and network size, in contrast to PI-DeepONets dramatic resource consumption increases under similarscaling. We anticipate that SepONets advantages will allow it to tackle more challenging physics-informedoperator learning problems, such as the Navier-Stokes equations (Jin et al., 2021), where both input andoutput functions are vector-valued. These are problems that PI-DeepONet may struggle to train on dueto resource constraints. As an example, we have considered a (2+1)-dimensional Navier-Stokes equation,previously investigated in the context of PINN (Cho et al., 2024; Wang et al., 2024). Some early, preliminaryresults can be found in Appendix D.1.",
  "Md Ashiqur Rahman, Zachary E Ross, and Kamyar Azizzadenesheli. U-no: U-shaped neural operators.arXiv e-prints, pp. arXiv2204, 2022": "Atilim Gunes Baydin, Barak A Pearlmutter, Alexey Andreyevich Radul, and Jeffrey Mark Siskind. Automaticdifferentiation in machine learning: a survey. Journal of machine learning research, 18(153):143, 2018. James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin,George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao Zhang. JAX: compos-able transformations of Python+NumPy programs, 2018. URL Shengze Cai, Zhicheng Wang, Lu Lu, Tamer A Zaki, and George Em Karniadakis. Deepm&mnet: Inferringthe electroconvection multiphysics fields based on operator approximation by neural networks. Journal ofComputational Physics, 436:110296, 2021. Tianping Chen and Hong Chen. Universal approximation to nonlinear operators by neural networks witharbitrary activation functions and its application to dynamical systems. IEEE transactions on neuralnetworks, 6(4):911917, 1995. Pao-Hsiung Chiu, Jian Cheng Wong, Chinchun Ooi, My Ha Dao, and Yew-Soon Ong. Can-pinn: A fastphysics-informed neural network based on coupled-automaticnumerical differentiation method. ComputerMethods in Applied Mechanics and Engineering, 395:114909, 2022.",
  "Tobin A Driscoll, Nicholas Hale, and Lloyd N Trefethen. Chebfun guide, 2014": "Vasiliy A Es kin, Danil V Davydov, Julia V Gureva, Alexey O Malkhanov, and Mikhail E Smorkalov.Separable physics-informed neural networks for the solution of elasticity problems.arXiv preprintarXiv:2401.13486, 2024. Zhiwei Fang, Sifan Wang, and Paris Perdikaris. Learning only on boundaries: a physics-informed neuraloperator for solving parametric partial differential equations in complex geometries. Neural Computation,36(3):475498, 2024.",
  "Arieh Iserles. A first course in the numerical analysis of differential equations. Number 44. Cambridgeuniversity press, 2009": "Zhongyi Jiang, Min Zhu, Dongzhuo Li, Qiuzi Li, Yanhua O Yuan, and Lu Lu. Fourier-mionet: Fourier-enhanced multiple-input neural operators for multiphase modeling of geological carbon sequestration.arXiv preprint arXiv:2303.04778, 2023. Xiaowei Jin, Shengze Cai, Hui Li, and George Em Karniadakis. Nsfnets (navier-stokes flow nets): Physics-informed neural networks for the incompressible navier-stokes equations.Journal of ComputationalPhysics, 426:109951, 2021. Karthik Kashinath, M Mustafa, Adrian Albert, JL Wu, C Jiang, Soheil Esmaeilzadeh, Kamyar Azizzade-nesheli, R Wang, Ashesh Chattopadhyay, A Singh, et al. Physics-informed machine learning: case studiesfor weather and climate modelling. Philosophical Transactions of the Royal Society A, 379(2194):20200093,2021.",
  "Diederik P Kingma and Jimmy Ba.Adam:A method for stochastic optimization.arXiv preprintarXiv:1412.6980, 2014": "Dmitrii Kochkov, Jamie A Smith, Ayya Alieva, Qing Wang, Michael P Brenner, and Stephan Hoyer. Machinelearningaccelerated computational fluid dynamics. Proceedings of the National Academy of Sciences, 118(21):e2101784118, 2021. Katiana Kontolati, Somdatta Goswami, George Em Karniadakis, and Michael D Shields. Learning nonlinearoperators in latent spaces for real-time predictions of complex dynamics in physical systems.NatureCommunications, 15(1):5101, 2024.",
  "Ziyue Liu, Xinling Yu, and Zheng Zhang. Tt-pinn: a tensor-compressed neural pde solver for edge computing.arXiv preprint arXiv:2207.01751, 2022b": "Ziyue Liu, Yixing Li, Jing Hu, Xinling Yu, Shinyu Shiau, Xin Ai, Zhiyu Zeng, and Zheng Zhang. Deepoheat:operator learning-based ultra-fast thermal simulation in 3d-ic design. In 2023 60th ACM/IEEE DesignAutomation Conference (DAC), pp. 16. IEEE, 2023. Lu Lu, Pengzhan Jin, Guofei Pang, Zhongqiang Zhang, and George Em Karniadakis. Learning nonlinearoperators via deeponet based on the universal approximation theorem of operators.Nature machineintelligence, 3(3):218229, 2021. Lu Lu, Xuhui Meng, Shengze Cai, Zhiping Mao, Somdatta Goswami, Zhongqiang Zhang, and George EmKarniadakis. A comprehensive and fair comparison of two neural operators (with practical extensions)based on fair data. Computer Methods in Applied Mechanics and Engineering, 393:114778, 2022a. Lu Lu, Raphal Pestourie, Steven G Johnson, and Giuseppe Romano. Multifidelity deep neural operatorsfor efficient learning of partial differential equations with application to fast inverse design of nanoscaleheat transport. Physical Review Research, 4(2):023210, 2022b. Luis Mandl, Somdatta Goswami, Lena Lambers, and Tim Ricken. Separable physics-informed deeponet:Breaking the curse of dimensionality in physics-informed machine learning. Computer Methods in AppliedMechanics and Engineering, 434:117586, 2025. Zhiping Mao, Lu Lu, Olaf Marxen, Tamer A Zaki, and George Em Karniadakis. Deepm&mnet for hyper-sonics: Predicting the coupled flow and finite-rate chemistry behind a normal shock using neural-networkapproximation of operators. Journal of computational physics, 447:110698, 2021.",
  "Jacob Seidman, Georgios Kissas, Paris Perdikaris, and George J Pappas.Nomad: Nonlinear manifolddecoders for operator learning. Advances in Neural Information Processing Systems, 35:56015613, 2022": "Louis Serrano, Lise Le Boudec, Armand Kassa Koupa, Thomas X Wang, Yuan Yin, Jean-Nol Vittaut, andPatrick Gallinari. Operator learning with neural fields: Tackling pdes on general geometries. Advances inNeural Information Processing Systems, 36, 2024. Khemraj Shukla, Vivek Oommen, Ahmad Peyvan, Michael Penwarden, Luis Bravo, Anindya Ghoshal,Robert M Kirby, and George Em Karniadakis. Deep neural operators can serve as accurate surrogates forshape optimization: a case study for airfoils. arXiv preprint arXiv:2302.00807, 2023. Vincent Sitzmann, Julien Martel, Alexander Bergman, David Lindell, and Gordon Wetzstein. Implicit neuralrepresentations with periodic activation functions. Advances in neural information processing systems, 33:74627473, 2020.",
  "Sifan Wang, Shyam Sankaran, and Paris Perdikaris.Respecting causality for training physics-informedneural networks. Computer Methods in Applied Mechanics and Engineering, 421:116813, 2024": "Karl Weierstrass.ber die analytische darstellbarkeit sogenannter willkrlicher functionen einer reellenvernderlichen. Sitzungsberichte der Kniglich Preuischen Akademie der Wissenschaften zu Berlin, 2(633-639):364, 1885. Gege Wen, Zongyi Li, Kamyar Azizzadenesheli, Anima Anandkumar, and Sally M Benson.U-fnoanenhanced fourier neural operator-based deep-learning model for multiphase flow.Advances in WaterResources, 163:104180, 2022. Yequan Zhao, Xinling Yu, Zhixiong Chen, Ziyue Liu, Sijia Liu, and Zheng Zhang. Tensor-compressed back-propagation-free training for (physics-informed) neural networks. arXiv preprint arXiv:2308.09858, 2023.",
  "AUniversal Approximation Theorem for Separable Operator Networks": "Here we present the universal approximation theorem for the proposed separable operator networks, originallywritten in Theorem 1 and repeated below in Theorem 5. We begin by reviewing established theoretical resultson approximating continuous functions and functionals. Following this review, we introduce the preliminarylemmas and proofs necessary for understanding Theorem 5. We refer our readers to Chen & Chen (1995);Weierstrass (1885) for detailed proofs of Theorems 2, 3, 4. Main notations are listed in .",
  "A.1Preliminaries and Auxiliary Results": "Definition 1 (Tauber-Wiener (TW)). If a function g : R R (continuous or discontinuous) satisfies thatall the linear combinations Ni=1 cig (ix + i), i R, i R, ci R, i = 1, 2, . . . , N, are dense in everyC[a, b], then g is called a Tauber-Wiener (TW) function. Remark 1 (Density in C[a, b]). A set of functions is said to be dense in C[a, b] if every function in thespace of continuous functions on the interval [a, b] can be approximated arbitrarily closely by functions fromthe set. Definition 2 (Compact Set). Suppose that X is a Banach space, V X is called a compact set in X, iffor every sequence {xn}n=1 with all xn V , there is a subsequence {xnk}, which converges to some elementx V . Theorem 2 (Chen & Chen (1995)). Suppose that K is a compact set in Rn, S is a compact set in C(K),g (TW), then for any > 0, there exist a positive integer N, real numbers i, vectors i Rn, i = 1, . . . , N,which are independent of f C(K) and constants ci(f), i = 1, . . . , N depending on f, such thatf(x)",
  "holds for all x K and f S. Moreover, each ci(f) is a linear continuous functional defined on S": "Theorem 3 (Chen & Chen (1995)). Suppose that (TW), X is a Banach Space, K X is a compactset, U is a compact set in C(K), f is a continuous functional defined on U, then for any > 0, there arepositive integers N, m points x1, . . . , xm K, and real constants ci, i, ij, i = 1, . . . , N, j = 1, . . . , m, suchthatf(u)",
  ": Notations and Symbols": "Xsome Banach space with norm XRdEuclidean space of dimension dKsome compact set in a Banach spaceC(K)Banach space of all continuous functions defined on K, with norm fC(K) = maxxK |f(x)|C[a, b]the space of functions in C[a, b] satisfying f(a) = f(b)Vsome compact set in C(K)u(x)some input functionUthe space of input functionsGsome continuous operatorG(u)(y) or s(y)some output function that is mapped from the corresponding input function u by the operator GSthe space of output functions(TW)all the Tauber-Wiener functions and gactivation function for branch net and trunk nets in Theorem 5{x1, x2, . . . , xm}m sensor points for identifying input function urrank of some deep operator network or separable operator networkn, moperator network size hyperparameters in Theorem 5",
  "< (29)": "holds for all x [a, b]. Therefore, it follows that for any continuous function g on [a, b] and any > 0, wecan approximate g within by choosing N sufficiently large and adjusting ci, i, i accordingly. Hence, theset of all such linear combinations of sin(x) is dense in C[a, b], confirming that sin(x) is a Tauber-Wienerfunction.",
  "Lemma 2. Suppose that V1 X1, V2 X2 are two compact sets in Banach spaces X1 and X2, respectively,then their Cartesian product V1 V2 is also compact": "Proof. For every sequencex1n, x2nin V1 V2, since V1 is compact,x1nhas a subsequencex1nkthatconverges to some element x1 V1. As well, since V2 is compact, there exists a subsequencex2nkthatconverges to x2 V2. It follows thatx1n, x2nconverges tox1, x2 V1 V2, thus V1 V2 is compact. Lemma 3. Suppose that X is a Banach space, K1 X1, K2 X2 are two compact sets in X1 and X2,respectively. U is a compact set in C(K1), then the range G(U) of the continuous operator G from U toC(K2) is compact in C(K2).",
  "A.2Universal Approximation Theorem for SepONet": "Theorem 5 (Universal Approximation Theorem for Separable Operator Networks). Suppose that (TW),g is a sinusoidal function, X is a Banach Space, K X, K1 Rd1 and K2 Rd2 are three compact sets inX, Rd1 and Rd2, respectively, U is a compact set in C (K), G is a nonlinear continuous operator, which mapsU into a compact set S C (K1 K2), then for any > 0, there are positive integers n, r, m, constants cki ,1k, 2k, kij, ki R, points 1k Rd1, 2k Rd2, xj K1, i = 1, . . . , n, k = 1, . . . , r, j = 1, . . . , m, such that",
  "holds for all u U, y = (y1, y2) K1 K2": "Proof. Without loss of generality, we can assume that g is sine function, by Lemma 1, we have g (TW);From the assumption that K1 and K2 are compact, by Lemma 2, K1 K2 is compact; Since G is acontinuous operator that maps U into C(K1 K2), it follows that the range G(U) = {G(u) : u U} iscompact in C(K1 K2) due to Lemma 3; Thus by Theorem 2, for any > 0, there exists a positive integerN, real numbers ck(G(u)) and k, vectors k Rd1+d2, k = 1, . . . , N, such that",
  ".(34)": "Since G is a continuous operator, according to the last proposition of Theorem 2, we conclude that foreach k = 1, . . . , 2N, ck(G(u)) is a continuous functional defined on U. Repeatedly applying Theorem 3, foreach k = 1, . . . , 2N, ck(G(u)), we can find positive integers nk,mk, constants cki , kij, ki R and xj K1,i = 1, . . . , nk, j = 1, . . . , mk, such thatck(G(u))",
  "B.1.1Diffusion-Reaction Systems": "We set the diffusion coefficient D = 0.01 and the reaction rate k = 0.01. The input training source terms aresampled from a mean-zero Gaussian random field (GRF) (Seeger, 2004) with a length scale 0.2. To generatethe test dataset, we sample 100 different source terms from the same GRF and apply a second-order implicitfinite difference method (Iserles, 2009) to obtain the reference solutions on a uniform 128 128 grid. Thespecific physics loss terms in equation (5) are defined as follows:",
  "B.1.2Advection Equation": "The input training variable coefficients are strictly positive by defining u(x) = v(x) minx v(x) + 1, wherev is sampled from a GRF with length scale 0.2. To create the test dataset, we generate 100 new coefficientsin the same manner that are not used in training and apply the LaxWendroff scheme (Iserles, 2009) tosolve the advection equation on a uniform 128 128 grid. The specific physics loss terms in equation (5) aredefined as follows:",
  "The input training initial conditions are sampled from a GRF N0, 252 + 52I4using the Chebfun": "package (Driscoll et al., 2014), satisfying the periodic boundary conditions. Synthetic test dataset consistsof 100 unseen initial functions and their corresponding solutions, which are generated from the same GRFand are solved by spectral method on a 101 101 uniform grid using the spinOp library (Palani, 2024),respectively. The corresponding physics loss terms in equation (5) are defined as:",
  "B.2Training Details and Hyperparameters": "Both PI-DeepONet and SepONet were trained by minimizing the physics loss (equation(4)) using gradientdescent with the Adam optimizer (Kingma & Ba, 2014). The initial learning rate is 1 103 and decays bya factor of 0.9 every 1,000 iterations. Additionally, we resample input training functions and training points(including residual, initial, and boundary points) every 100 iterations. Across all benchmarks and on both models (SepONet and PI-DeepONet), we apply Tanh activation for thebranch net and Sine activation for the trunk net. We note that no extensive hyperparameter tuning wasperformed for either PI-DeepONet or SepONet. The code in this study is implemented using JAX andEquinox libraries (Bradbury et al., 2018; Kidger & Garcia, 2021), and all training was performed on a singleNVIDIA A100 GPU with 80 GB of memory. Training hyperparameters are provided in .",
  "B.3Complete Test Results": "We report the relative 2 error, root mean squared error (RMSE), GPU memory usage and total trainingtime as metrics to assess the performance of PI-DeepONet and SepONet. Specifically, the mean and standarddeviation of the relative 2 error and RMSE are calculated over all functions in the test dataset. The completetest results are shown in and . : Performance comparison of PI-DeepONet and SepONet with varying number of training points(Nc) and fixed number of input training functions (Nf = 100). The - symbol indicates that results are notavailable due to out-of-memory issues.",
  "B.4.1Trunk Networks with Hyperbolic Tangent Activations": "In and , we provide complete testing results repeating our experiments from and for all PDE examples, varying Nc and Nf, except we use hyperbolic tangenet (TanH) activationfunctions for all hidden and output layers of the trunk networks in both PI-DeepONet and SepONet. Theresults are very similar, indicating that alternative activation functions may be chosen for multi-layer trunknetworks to maintain the universal approximation property in accordance with Theorem 1.",
  "B.4.2Varying the Input Function Discretization to the Branch Network": "Given an input function PDE configuration u, recall that the branch network predicts coefficients k =b(E(u)) for k = 1, ..., r. Our studies in use a simple encoder that measures the input functionE(u) = (u(x1), ..., u(xm)) at points x1, x2, ..., xm K in the input function domain. High-dimensional orhighly oscillatory input functions may lead to unwieldy discretizations with large branch input dimensionthat affect training performance. Here, in , we study the sensitivity of the PDE examples from with respect to the number of input function sensors (branch input dimension). Note that we fixthe number of input functions Nf = 20 for all experiments, while Nc = 322 for diffusion-reaction, advection,and Burgers equations, and Nc = 163 for nonlinear diffusion. We find that the error curves converge tothe minimum value using only a fraction of the number of input sensors that we used in the main text in and . This indicates that the input functions we considered may be identified with a smallnumber of points. Nevertheless, we find that training performance in terms of both memory consumptionand training time is constant with the number of sensors. This is because training complexity is mainlydata-dominated by the need to compute high-order derivatives with respect to a large number of collocationpoints for evaluation of the physics loss, as discussed in .2.",
  "M[t]s(y) = L1[y1]s(y) + + Ld[yd]s(y),(44)": "where M[t] =ddt + h(t) is a first order differential operator of t, and L1[y1], ..., Ld[yd] are linear secondorder ordinary differential operators of their respective variables y1, ..., yd only. Furthermore, assume we areprovided Robin boundary conditions in each variable and separable initial condition s(t = 0, y1, . . . , yd) =dn=1 n(yn) for functions n(yn) that satisfy the boundary conditions.",
  "(46)": "Here, k = (k1, . . . , kd), where kn {1, 2, ..., }, n {1, ..., d}, is a lumped index that counts over allpossible products of eigenfunctions Y knnwith associated eigenvalues knn . n is an appropriate inner productassociated with the separated Hilbert space of the Ln-th operator. To obtain equation (7) in the mainmanuscript, one only need to break up the sum over all kn indices into a single ordered index.",
  "(47)": "where s = (sx, sy) R2 is the velocity field, x = (x, y) denotes 2D spatial variables, = [0, T] is the timewindow, and = s = xsy ysx is the vorticity. We aim to learn the solution operator that maps theinitial velocity and vorticity field u(x) = (s0(x), 0(x)) R3 to the solution s(x, t) R2 using SepONet,parameterized by = (, 1, 2, 3), which represents all the trainable parameters in the branch and trunknets. The vector-valued velocity is approximated as:",
  "(x) = [1, sin(x), sin(2x), sin(3x), sin(4x), sin(5x), cos(x), cos(2x), cos(3x), cos(4x), cos(5x)].(51)": "SepONet architectureThe encoder E maps the initial condition u(x) to its point-wise evaluations at128 128 3 sensors on a uniform 128 128 grid over 2. The branch network b is a CNN, startingwith a 1 1 convolution that increases the channels from 3 to 16, followed by four residual blocks. Eachresidual block consists of two 3 3 convolutions with GeLU activations; the first convolution in each blockuses a stride of 2 to halve the spatial dimensions while doubling the number of channels. Skip connectionsemploy 1 1 convolutions with a stride of 2 whenever there is a change in dimension. After flattening, afully connected layer produces a vector of dimension 2r. The trunk networks tn are 7-layer modified MLPs (Wang et al., 2021a), each with 128 neurons per hiddenlayer, an input size of 11, and a rank/output size of 256/512, using TanH activations. The initial velocitiesare sampled from a Gaussian random field with a maximum velocity of 5. Training settingsWe consider learning the solution operator within two time windows: T = 0.1 andT = 1. Two separate SepONet models were trained, each for 100,000 iterations using the Adam optimizer,to minimize the physics loss (49) within their respective time windows. The number of residual points wasset to Nr = 25625632 (NxNy Nt), obtained by randomly sampling 256, 256, and 32 points along x, y,and t axes, respectively, and constructing a mesh grid via the tensor product. The number of initial points,NI = 128128, corresponds to a uniform mesh grid. Initial velocities were sampled from a Gaussian randomfield with a maximum velocity of 5. Both the initial conditions and collocation points were resampled every100 iterations. We varied the number of input functions Nf to see the scaling as data is increased. EvaluationThe model was evaluated on 100 unseen initial conditions, sampled from the same Gaussianrandom field. Reference solutions for both time windows were obtained using the JAX-CFD solver (Kochkovet al., 2021) on a uniform 128 128 10 (Nx Ny Nt) grid. ResultsThe results for two time windows are shown in . We find that for T = 0.1 we can achievevery low relative 2 error of 2.5%. For T = 1 the error only scales to 40%. We think that improving the errorfor the longer time scale represents an interesting application direction for future work. Our architectureand implementation choices were not optimized for this example.",
  "where x1, x2, . . . , x128 are 128 equi-spaced sensors in , k is the k-th output of the branch net, and thebasis functions k(t) and k(x) are the k-th outputs of two independent trunk nets": "Training settingsThe branch and trunk networks each have a width of 5 and a depth of 50. To determinethe parameters , we trained SepONet for 80,000 iterations, minimizing the physics loss. Specifically, weset I = 20, b = 1, Nf = 100, and Nc = 1282 in the physics loss.The training functions (initial",
  "EvaluationWe evaluated the model on 100 unseen initial conditions sampled from the same GRF, usingthe forward Euler method to obtain reference solutions on a 128 128 uniform spatio-temporal grid": "Impact of the rank rSince k(t) and k(x) are independent of the initial condition u, learning anexpressive and rich set of basis functions is crucial for SepONet to generalize to unseen initial conditions.To investigate the impact of the rank r on the generalization error, we trained SepONet with ranks rangingfrom 1 to 50. The mean RMSE between SepONets predictions and the reference solutions over 100 unseentest initial conditions was reported. For comparison, we also computed the mean RMSE of the truncatedanalytical solution at rank r for r from 1 to 15. The results are presented in . As r increases, the truncated analytical solution quickly converges to the reference solution. The nonzeroRMSE arises due to numerical errors in computing the coefficients Ak and the inherent inaccuracies of theforward Euler method used to generate the reference solution. For SepONet, we observed that when r = 1, 2,the mean RMSE aligns closely with that of the truncated solution. However, as r increases beyond thatpoint, the error decreases more gradually, stabilizing around r = 50. This indicates that SepONet may notnecessarily learn the exact same basis functions as those from the truncated analytical solution. Instead, ahigher rank r allows SepONet to develop its own set of basis functions, achieving similar accuracy to thetruncated solution.",
  "SepONet basis functionsThe learned basis functions for different ranks r are visualized in to": "At r = 1, SepONet learns basis functions that closely resemble the first term of the truncated solution. Forr = 2, the learned functions are quite similar to the first two terms of the truncated series. However, whenr = 5, the basis functions diverge from the truncated solution series, although some spatial componentsstill resemble sinusoidal functions and the temporal components remain monotonic. As r increases further,SepONet continues to improve in accuracy, though the learned basis functions increasingly differ from thetruncated series, confirming SepONets ability to accurately approximate the solution using its own learnedbasis functions."
}