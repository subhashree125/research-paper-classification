{
  "Abstract": "Network embedding, which maps graphs to distributed representations, is a unified frame-work for various graph inference tasks. According to the topology properties (e.g., structuralroles and community memberships of nodes) to be preserved, it can be categorized into theidentity and position embedding. Most existing methods can only capture one type of prop-erty. Some approaches can support the inductive inference that generalizes the embeddingmodel to new nodes or graphs but relies on the availability of attributes. Due to the compli-cated correlations between topology and attributes, it is unclear for some inductive methodswhich type of property they can capture. In this study, we explore a unified framework forthe joint inductive inference of identity and position embeddings without attributes. Aninductive random walk embedding (IRWE) method is proposed, which combines multipleattention units to handle the random walk (RW) on graph topology and simultaneouslyderives identity and position embeddings that are jointly optimized. We demonstrate thatsome RW statistics can characterize node identities and positions while supporting the in-ductive inference. Experiments validate the superior performance of IRWE over variousbaselines for the transductive and inductive inference of identity and position embeddings.",
  "Introduction": "For various graph inference techniques, network embedding (a.k.a. graph representation learning) is a com-monly used framework. It maps each node of a graph to a low-dimensional vector representation (a.k.a.embedding) with some key properties preserved. The derived representations are used to support severaldownstream inference tasks, e.g., node classification (Kipf & Welling, 2017; Velikovi et al., 2018), nodeclustering (Ye et al., 2022; Qin et al., 2023a; Gao et al., 2023), and link prediction (Lei et al., 2018; 2019;Qin et al., 2023b; Qin & Yeung, 2023). According to the topology properties to be preserved, existing network embedding techniques can be catego-rized into the identity and position embedding (Zhu et al., 2021). The identity embedding (a.k.a. structuralembedding) preserves the structural role of each node characterized by its rooted subgraph, which is alsodefined as node identity. The position embedding (a.k.a. proximity-preserving embedding) captures the link-age similarity between nodes in terms of the overlap of local neighbors (i.e., community structures (Newman,2006)), which is also defined as node position or proximity. In (a), each color denotes a structural role.For instance, red and yellow may indicate the opinion leader and hole spanner in a social network (Yanget al., 2015). Moreover, there are two communities denoted by the two dotted circles in , where nodesin the same community have dense linkages and thus are more likely to have similar positions. The identity and position embedding should respectively force nodes with similar identities (e.g., {v1, v8})and positions (e.g., {v1, v2, v6}) to have close embeddings. As a demonstration, we applied struc2vec (Ribeiroet al., 2017) and node2vec (Grover & Leskovec, 2016) (with embedding dimensionality d = 2), which are",
  ": An example of identity and position embedding in terms of (b) struc2vec and (c) node2vec, whereeach color denotes a unique identity while nodes in the same community have similar positions": "typical identity and position embedding methods, to the example in (a) and visualize the derivedembeddings. Note that two nodes may have the same identity even though they are far away from eachother. In contrast, nodes with similar positions must be close to each other with dense linkage and shortdistances. Due to the contradiction, it is challenging to simultaneously capture the two types of properties ina common embedding space. For instance, v1 and v8 with the same identity have close identity embeddingsin (b). However, their position embeddings are far away from each other in (c). Since the twotypes of embeddings may be appropriate for different downstream tasks (e.g., structural role classificationand community detection), we expect a unified embedding model. Most conventional embedding methods (Wu et al., 2020; Grover & Leskovec, 2016; Ribeiro et al., 2017;Donnat et al., 2018) follow the embedding lookup scheme and can only support transductive embeddinginference. In this scheme, node embeddings are model parameters optimized only for the currently observedgraph topology. When applying the model to new unseen nodes or graphs, one needs to re-train the modelfrom scratch.Compared with transductive methods, some state-of-the-art techniques (Hamilton et al.,2017; Velickovic et al., 2019) can support the advanced inductive inference, which directly generalizes theembedding model trained on observed topology to new unseen nodes or graphs without re-training. Most existing inductive approaches (e.g., those based on graph neural networks (GNNs) (Wu et al., 2020))rely on the availability of node attributes and an attribute aggregation mechanism. However, prior studies(Qin et al., 2018; Li et al., 2019; Wang et al., 2020; Qin & Lei, 2021) have demonstrated some complicatedcorrelations between graph topology and attributes. For instance, attributes may provide (i) complemen-tary characteristics orthogonal to topology for better quality of downstream tasks or (ii) inconsistent noisecausing unexpected quality degradation. It is unclear for most inductive methods that their performanceimprovement is brought about by the incorporation of attributes or better exploration of topology. Whenattributes are unavailable, most inductive approaches require additional procedures to extract auxiliary at-tribute inputs from topology (e.g., one-hot node degree encodings). Our experiments demonstrate that someinductive baselines with these naive attribute extraction strategies may even fail to outperform conventionaltransductive methods on the inference of identity and position embeddings. It is also hard to determinewhich type of properties (i.e., node identities or positions) that some inductive approaches can capture. In this study, we consider the unsupervised network embedding and explore a unified framework for the jointinductive inference of identity and position embeddings. To clearly distinguish between the two types ofembeddings, we consider the case where topology is the only available information source. This eliminatesthe unclear influence from graph attributes due to the complicated correlations between the two sources.Different from most existing inductive approaches relying on the availability of node attributes, we proposean inductive random walk embedding (IRWE) method. It combines multiple attention units with differentchoices of key, query, and value to handle the random walk (RW) and induced statistics on graph topology. RW is an effective technique to explore topology properties for network embedding. However, most RW-based methods (Grover & Leskovec, 2016; Ribeiro et al., 2017) follow the transductive embedding lookupscheme, failing to support the advanced inductive inference. We demonstrate that anonymous walk (AW)(Ivanov & Burnaev, 2018), the anonymization of RW, and its induced statistics can be informative featuresshared by all possible nodes and graphs and thus have the potential to support inductive inference.",
  "Published in Transactions on Machine Learning Research (10/2024)": "Petar Velikovi, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio.Graph attention networks. In Proceedings of the 6th International Conference on Learning Representations,2018. Petar Velickovic, William Fedus, William L Hamilton, Pietro Li, Yoshua Bengio, and R Devon Hjelm. Deepgraph infomax. In Proceedings of the 7th International Conference on Learning Representations, 2019.",
  "Identity & Position Embedding": "In the past several years, a series of network embedding techniques have been proposed. Rossi et al. (2020)gave an overview of existing methods covering the identity and position embedding. Most existing embeddingapproaches can only capture one type of topology properties (i.e., node identities or positions). Perozzi et al. (2014) proposed DeepWalk that applies skip-gram to learn node embeddings from RWs ongraph topology. The ability of DeepWalk to capture node positions is further validated in (Pei et al., 2020;Rossi et al., 2020). Grover & Leskovec (2016) modified the RW in DeepWalk to a biased form and introducednode2vec that can derive richer position embeddings by adjusting the trade-off between breadth- and depth-first sampling. Cao et al. (2015) reformulated the RW in DeepWalk to matrix factorization objectives. Wanget al. (2017), Ye et al. (2022), and Chen et al. (2023) introduced community-preserving embedding methodsbased on nonnegative matrix factorization, hyperbolic embedding, and graph contrastive learning. Ribeiro et al. (2017) proposed struc2vec, an identity embedding method, by applying RW to a multi-layer graph constructed via hierarchical similarities w.r.t. node degrees. Donnat et al. (2018) used graphwavelets to develop GraphWave and proved its ability to capture node identities. Pei et al. (2020) introducedstruc2gauss, which encodes node identities in a space formulated by Gaussian distributions, and analyzed theeffectiveness of different energy functions and similarity measures. Guo et al. (2020) enhanced the ability ofGNNs to preserve node identities by reconstructing several manually-designed statistics. Chen et al. (2022)enabled the graph transformer to capture node identities by incorporating the rooted subgraph of each node. Hoff (2007) demonstrated that the latent class and distance models can respectively capture node positionsand identities but real networks may exhibit combinations of both properties. An eigen-model was proposed,which can generalize either the latent class model or distance model. However, the proposed eigen-model is aconventional probabilistic model and cannot simultaneously capture both properties in a unified framework.Zhu et al. (2021) proposed a PhUSION framework with three steps and showed which components canbe used for the identity or position embedding. Although PhUSION reveals the similarity and differencebetween the two types of embeddings, it can only derive one type of embedding under each unique setting.Rossi et al. (2020) validated that some techniques (e.g., RW and attribute aggregation) of existing methodscan only derive either identity or position embeddings. Srinivasan & Ribeiro (2020) proved that the relationbetween identity and position embeddings can be analogous to that of a probability distribution and itssamples. Similarly, PaCEr (Yan et al., 2024) is a concurrent transductive method that considers the relationbetween the two types of embeddings based on RW with restart. Although these methods (Srinivasan &Ribeiro, 2020; Yan et al., 2024) can derive both identity and position embeddings, they only involve theoptimization of one type of embedding and a simple transform to another type. In contrast, we focus on thejoint learning and inductive inference of the two types of embeddings.",
  "Inductive Network Embedding": "Some recent studies explore the inductive inference that directly derives embeddings for new unseen nodes orgraphs by generalizing the model parameters optimized on known topology. Hamilton et al. (2017) introducedGraphSAGE, an inductive GNN framework, including the neighbor sampling and feature aggregation withdifferent choices of aggregation functions. GAT (Velikovi et al., 2018) leverages self-attention into theattribute aggregation of GNN, which automatically determines the aggregation weights for the neighbors ofeach node. Velickovic et al. (2019) proposed DGI that maximizes the mutual information between patchembeddings and high-level graph summaries. Without using the feature aggregation of GNN, Nguyen et al.(2021) developed SANNE that applies self-attention to handle RWs sampled from graph topology. However,the inductiveness of the these methods relies on the availability of node attributes. Some recent research analyzed the ability of several new GNN structures to capture node identities orpositions in specific cases about node attributes (e.g., all the nodes have the same scalar attribute input (Xuet al., 2019)). Wu et al. (2019) and You et al. (2021) proposed DEMO-Net and ID-GNN that can capturenode identities using the degree-specific multi-task graph convolution and heterogeneous message passing onthe rooted subgraph of each node, respectively. Jin et al. (2020) leveraged AW statistics into the featureaggregation to enhance the ability of GNN to preserve node identities. P-GNN (You et al., 2019) can deriveposition-aware embeddings based on a distance-weighted aggregation scheme over the sets of sampled anchornodes. However, these GNN structures can only capture either node identities or positions. In contrast to the aforementioned methods, we explore a unified inductive framework for the joint inferenceof identity and position embeddings without relying on the availability and aggregation of attributes.",
  "Problem Statements & Preliminaries": "We consider the unsupervised network embedding on undirected unweighted graphs. A graph can be repre-sented as G = (V, E), with V = {v1, v2, . . . , vN} and E = {(vi, vj)|vi, vj V} as the sets of nodes and edges.We also assume that graph topology is the only available information source and attributes are unavailable. Definition 1 (Node Identity). Node identity describes the structural role that a node v plays in graphtopology (e.g., opinion leader and hole spanner w.r.t. red and yellow nodes in (a)), which can becharacterized by its l-hop rooted subgraph Gs(v, l). Given a pre-set l, nodes (v, u) with similar subgraphs(Gs(v, l), Gs(u, l)) (e.g., measured by the WL graph isomorphism test) are expected to play similar structuralroles and have similar identities. Definition 2 (Node Position).Positions of nodes in graph topology can be encoded by their relativedistances and can be further characterized by the linkage similarity in terms of the overlap of l-hop neighbors(i.e., community structures). Nodes with a high overlap of l-hop neighbors are more likely to (i) have shortdistance, (ii) belong to the same community, and thus (iii) have similar positions. Definition 3 (Network Embedding). Given a graph G, we consider the network embedding (a.k.a. graphrepresentation learning) f : V Rd that maps each node v to a vector f(v) (a.k.a. embedding), witheither node identities or positions preserved. We define f(v) := (v) (or f(v) := (v)) as the identity (orposition) embedding if {(v)} (or {(v)}) preserve node identities (or positions). Namely, nodes (v, u) withsimilar identities (or positions) should have close representations ((v), (u)) (or ((v), (u))). The learnedembeddings are adopted as the inputs of some downstream modules to support concrete inference tasks. The embedding inference includes the transductive and inductive settings. A transductive method focuseson the optimization of f on the currently observed topology G = (V, E) and can only support inferencetasks on V. In contrast, an inductive approach can directly generalize its model parameters, which are firstoptimized on (V, E), to new unseen nodes V or even a new graph G = (V, E) and support tasks on V",
  ": Model architecture of IRWE including modules of (b) identity and (c) position embeddings": "Definition 4 (Random Walk & Anonymous Walk).An RW with length l is a node sequence w =(w(0), w(1), . . . , w(l)), where w(j) V is the j-th node and (w(j), w(j+1)) E. Assume that the index jstarts from 0. For an RW w, one can map it to an AW = (Iw(w(0)), . . . , Iw(w(l))), where Iw(w(j)) mapsw(j) to its first occurrence index in w. In (a), (v1, v4, v5, v1, v6) is a valid RW with (0, 1, 2, 0, 3) as its AW. In particular, two RWs (e.g.,(v1, v4, v5, v1) and (v8, v10, v9, v8)) can be mapped to a common AW (i.e., (0, 1, 2, 0)).In , wefurther demonstrate that AW and its induced statistics can be features shared by all possible topology andthus can support the inductive embedding inference without attributes.",
  "Identity Embedding Module": "(b) highlights details of the identity embedding module. It derives identity embeddings {(v)} basedon auxiliary AW embeddings {()}, AW statistics {s(v)}, and high-order degree features {(v)}. gives running examples about the extraction of {()}, {s(v)}, and {(v)} based on the local topology ofnode v1 in , where we set RW length l = 3 and the number of sampled RWs nS = 5 as a demonstration.The optimization and inference of this module includes the (1) AW embedding auto-encoder, (2) identityembedding encoder, and (3) identity embedding decoder.",
  "AW Embedding Auto-Encoder": "As discussed in , it is possible to map RWs with different sets of nodes to a common AW. Forinstance, (0, 1, 2, 0) is the common AW of RWs (v1, v2, v3, v1) and (v1, v4, v5, v1) in . Given a fixedlength l, RWs on all possible topology structures can only be mapped to a finite set of AWs l. Namely, land its induced statistics are shared by all possible nodes and graphs, thus having the potential to support theinductive embedding inference. Based on this intuition, IRWE maintains an AW embedding () Rd foreach AW l. In this setting, {()} can be used as a special embedding lookup table for the derivationof inductive features regarding graph topology. We also consider an additional constraint on {()}, where two AWs with more common elements in corre-sponding positions should have closer representations. For instance, (0, 1, 2, 1, 2) and (0, 1, 0, 1, 2) should becloser in the AW embedding space than (0, 1, 2, 1, 2) and (0, 1, 0, 2, 3). To apply this constraint, we transformeach AW with length l to a one-hot encoding () {0, 1}(l+1)2, where ()jl:(j+1)l (i.e., subsequencefrom the jl-th to the (j + 1)l-th positions) is the one-hot encoding of the j-th element in . For instance,we have () = for = (0, 1, 2, 3) in . An auto-encoder is then introduced toderive and regularize {()}, including an encoder and a decoder. Given an AW , the encoder Enc() anddecoder Dec() are defined as",
  "IRWE derives identity embeddings {(v)} via the combination of AW embeddings {()} inspired by thefollowing Theorem 1 (Micali & Zhu, 2016)": "Theorem 1. Let Gs(v, r) be the rooted subgraph induced by nodes with a distance less than r from v. Letq(v, l) be the distribution of AWs w.r.t. RWs starting from v with length l. One can reconstruct Gs(v, r) intime O(n2) with O(n2) access to [q(v, 1), , q(v, l)], where l = O(m); n and m are the numbers of nodesand edges in Gs(v, r). For a given length l, let l be the number of AWs. q(v, l) can be represented as an l-dimensional vector, withthe j-th element as the occurrence probability of the j-th AW. Since AWs with length l include sequencesof those with length less than l (e.g., (0, 1, 2, 3) provides information about (0, 1, 2)), one can derive q(v, k)(k < l) based on q(v, l). Therefore, q(v, l) can be used to characterize Gs(v, r) according to Theorem 1.",
  "5s(v)j s(v)j + 1 //Update s(v)": "As defined in , nodes with similar rooted subgraphs are expected to play similar structural roles andthus have similar identities. For instance, in , Gs(v1, 1) and Gs(v8, 1) have the same topology structure,which is consistent with the same identity they have. Hence, q(v, l) can characterize the identity of node v. To estimate q(v, l), we extract AW statistic s(v) for each node v using Algorithm 1. We first sample RWswith length l starting from v via the standard unbiased strategy (Perozzi et al., 2014) (see Algorithm 6 inAppendix A). Let W(v) be the set of sampled RWs starting from v. Each RW w W(v) is then mappedto its AW. Let l be an AW lookup table including all the l AWs with length l, which is fixed and sharedby all possible topology. We define the AW statistic as s(v) := [c(1), , c(l)] Zl+, where c(j) is thefrequency of the j-th AW in l as illustrated in .",
  "l522038774,14021,147115,975l15521956101,5403,173": "Although l grows exponentially with the increase of length l, {s(v)} are usually sparse. visualizes theexample AW statistics {s(v)} derived from RWs on the Brazil dataset (see .1 for details) with l = 4and |W(v)| = 1, 000. The i-th row in is the AW statistic s(vi) of node vi. Dark blue indicates that thecorresponding element is 0. There exist many AWs {j} not observed during the RW sampling (i.e., v Vs.t. s(v)j = 0). We then remove terms w.r.t. these unobserved AWs in l and {s(v)}. Let l, s(v), and lbe the reduced l, s(v), and l. shows the variation of l and l on Brazil as l increases from 4 to9, where l is significantly reduced.",
  "In addition to {s(v)}, one can also characterize node identities from the view of node degrees (Ribeiro et al.,2017; Wu et al., 2019) based on the following Hypothesis 1": "Hypothesis 1. Nodes with the same degree are expected to play the same structural role. This concept canbe extended to high-order neighbors of nodes. Namely, nodes are expected to have similar identities if theyhave similar node degree statistics (e.g., distribution over all degree values) w.r.t. their high-order neighbors. Based on this motivation, we extract high-order degree feature (v) for each node v using Algo-rithm 2.Given a node u, one can construct a bucket one-hot encoding d(u) {0, 1}e w.r.t.its de-gree deg(u), where only the j-th element d(u)j is set to 1 with the remaining elements set to 0 andj = (deg(u) degmin)e/(degmax degmin); degmin and degmax are the minimum and maximum degrees.Since high-order neighbors of a node v can be explored by RWs W(v) starting from v, we define (v) Z(l+1)e as an (l + 1)e-dimensional vector, where the subsequence (v)ie:(i+1)e is the sum of bucket one-hot degreeencodings w.r.t. nodes occurred at the i-th position of RWs in W(v). gives a running example toderive (v1) (with e = 5) for node v1 in .",
  "(v)ie:(i+1)e (v)ie:(i+1)e + d(u)//Update (v)": "Following the aforementioned discussions regarding Theorem 1 and Hypothesis 1, IRWE derives identityembeddings {(v)} via the adaptive combination of AW embeddings {()} w.r.t. AW statistics {s(v)} anddegree features {(v)}. The multi-head attention is applied to automatically determine the contribution ofeach AW embedding () in the combination, where we treat {()} as the key and value; the concatenatedfeature [s(v)||(v)] is used as the query. Before feeding [s(v)||(v)] R(l+le) to the multi-head attention,we introduce a feature reduction unit Reds(), an MLP, to reduce its dimensionality to d:",
  "Position Embedding Module": "(c) gives an overview of the position embedding module.It derives position embeddings {(v)}based on (i) identity embeddings {(v)} given by the previous module and (ii) auxiliary position encodings{g(v), l(j)} extracted from the sampled RWs {W(v)}. Instead of using the attribute aggregation mechanismof GNNs, we convert the graph topology into a set of RWs and use the transformer encoder (Vaswani et al.,2017), a sophisticated structure that can process sequential data, to handle RWs. Besides the sequentialinput, transformer also requires a position encoding to describe the position of each element in the sequence.As highlighted in Definition 3, the physical meaning of node position in non-Euclidean graphs is differentfrom that in Euclidean sequences (e.g., sentences and RWs). To describe the (i) Euclidean position in RWsand (ii) node position in a graph, we introduce the local and global position encodings (denoted as l(j) andg(v)) for a sequence position with index j and each node v. The optimization and inference of this moduleincludes the (1) input fusion unit, (2) position embedding encoder, and (3) position embedding decoder.",
  "Input Fusion Unit": "The input fusion unit extracts {l(j), g(v)} and derives inputs of the transformer encoder combined with{(v)}. Since the RW length l is usually not very large (e.g., 10 in our experiments), we define the localposition encoding l(j) {0, 1}l+1 as the standard one-hot encoding of index j. Inspired by previous studies(Perozzi et al., 2014; Grover & Leskovec, 2016; Zhu et al., 2021) that validated the potential of RW forexploring local community structures, we extract the global position encoding {g(v)} for each node v w.r.t.RW statistic r(v) using Algorithm 3. Given a node v, we maintain r(v) Z|V|, with the j-th element r(v)j as the frequency that node vj occursin RWs W(v) starting from v. For instance, we have r(v1) = for the runningexample in with 13 nodes. Since nodes in a community are densely connected, nodes within the samecommunity are more likely to be reached via RWs. Therefore, nodes (v, u) with similar positions are expectedto have similar statistics (r(v), r(u)). We then derive g(v) by mapping r(v) to a d-dimensional vector viathe following Gaussian random projection, an efficient dimension reduction technique that can preserve therelative distance between input features with a rigorous guarantee (Arriaga & Vempala, 2006):",
  "Hypothesis 2. In a community (i.e., node cluster with dense linkages), nodes with different structural rolesmay have different contributions in forming the community structure": "For instance, in a social network, an opinion leader (e.g., v1 and v8 in ) is expected to have morecontributions in forming the community it belongs to than an ordinary audience (e.g., v2 and v9 in ).Based on this intuition, we use identity embeddings {(v)} to reweight global position encodings {g(v)},with the reweighting contributions determined by a modified attention operation. Concretely, we set identityembeddings {(v)} as the query and let global position encodings {g(v)} as the key and value (i.e., Qi,: =(vi) and Ki,: = Vi,: = g(vi)). The modified attention operation is defined as",
  "Z = ReAtt(Q, K, V) := (MLP(Q) + MLP( K)) V,(6)": "where Q := BN(Q), K := BN(K), and V := BN(V); BN() and are the batch normalization and element-wise multiplication. In (6), we apply two MLPs to derive nonlinear mappings of the normalized {Q, K} anduse their sum to support the element-wise reweighting of the normalized V. For convenience, we denotethe reweighted vector w.r.t. a node vi as g(vi) = Zi,:. Given an RW w = (w(0), w(1), , w(l)), IRWEconcatenates the reweighted vector g(w(j)) and local position encoding l(j) for the j-th node and feeds itslinear mapping to the transformer encoder:",
  "(t(w(0)), ) = TransEnc(t(w(0)), , t(w(l))).(8)": "It takes the corresponding sequence of vectors (t(w(0)), , t(w(l))) as input and derives another sequenceof vectors (t(w(0)), , t(w(l))) with the same dimensionality. TransEnc() follows a multi-layer structure,with each layer including the self-attention, skip connections, layer normalization, and feedforward mapping.Due to space limit, we omit details of TransEnc() that can be found in (Vaswani et al., 2017). For an RW w starting from a node v, the first output vector t(w(0)) = t(v) can be a representation of v. Aswe sample multiple RWs W(v) starting from each node v, one can obtain multiple such representations basedon W(v). However, we only need one unique positon embedding (v) for v. Let t(v) := {t(w(0))|w W(v)}. Anaive strategy to derive (v) is to average representations in t(v). Instead, we develop the following attentivereadout function to compute the weighted mean of t(v), with the weights determined by attention:",
  "z = ROut(t(v), g(v)) := Att(g(v), t(v), t(v)), (v) := zW + b, and (v) := zW + b.(9)": "In (9), Att(, , ) is the standard multi-head attention unit (see Appendix D for details), where we let theglobal position encoding g(v) be the query (i.e., Q = g(v) R1d) and t(v) be the key and value (i.e.,Kj,: = Vj,: = t(v)j ). (v) and (v) are the (i) position embedding and (ii) auxiliary context embedding of nodev, with {W, b, W, b} as trainable parameters.",
  "(vi,vj)D [pij ln ((vi)T (vj)/) + Qnj ln ((vi)T (vj)/)],(10)": "where D denotes the training set including positive and negative samples in terms of node pairs {(vi, vj)};pij is defined as the statistic of a positive node pair (vi, vj) (e.g., the frequency that (vi, vj) occurs in theRW sampling); Q is the number of negative samples; nj is usually set to be the probability that (vi, vj) isselected as a negative sample; () is the sigmoid function; is a temperature parameter to be specified. Wefollow prior work (Tang et al., 2015) to let pij := Aij/deg(vi) (i.e., the probability that there is an edge fromvi to vj with A {0, 1}|V||V| as the adjacency matrix) and nj ( i:(vi,vj)E pij)0.75. In the next section,we demonstrate that the contrastive loss (10) can be converted to a reconstruction loss such that the jointoptimization of IRWE only includes several reconstruction objectives.",
  "vV |[s(v)||(v)]/|W(v)| g(v)|22,(13)": "where Lreg regularizes auxiliary AW embeddings {()} by reconstructing the one-hot AW encodings{()} via the auto-encoder defined in (1); Lreg regularizes the derived identity embeddings {(v)} byminimizing the error between (i) features {[s(v)||(v)]} normalized by the number of sampled RWs |W(v)|and (ii) reconstructed values {g(v)} given by (4); is a tunable parameter.",
  "23save model parameters {, }": "As described in .2.3, one can optimize position embeddings {(v)} via a contrastive loss (10). Itcan be converted to another reconstruction loss based on the following Proposition 1. In this setting, theoptimization of {(v)} and {(v)} only includes three simple reconstruction losses. Proposition 1. Let R|V|d and R|V|d be the matrix forms of {(vi)} and {(vi)} with the i-throws denoting the corresponding embeddings of node vi. We introduce the auxiliary contrastive statisticsC R|V||V| in terms of a sparse matrix where Cij = ln pij ln(Qnj) if (vi, vj) E and Cij = 0 otherwise.The contrastive loss (10) is equivalent to the following reconstruction loss:",
  "The key idea to prove Proposition 1 is to let the partial derivative Lcnr/[(vi)T (vj)/] w.r.t. each edge(vi, vj) to 0. We leave the proof of Proposition 1 in Appendix C": "Algorithm 4 summarizes the joint optimization procedure of IRWE. Before formally optimizing the model, wesampled nS RWs W(v) starting from each node v and derive statistics {s(v), (v), g(v)} induced by {W(v)}.In particular, we randomly select nI RWs W(v)Ifrom W(v) (nI < nS) for each node, which are handled bythe transformer encoder in the position embedding module. Namely, we use a ratio of the sampled RWs toderive {(v)} due to the high complexity of transformer. We only sample RWs and derive induced statisticsonce, which are shared by the following optimization iterations. To jointly optimize {(v)} and {(v)}, one can combine (11) and (14) to derive a single hybrid optimizationobjective. Our pre-experiments show that better embedding quality can be achieved if we separately optimizethe two types of embeddings.One possible reason is that the two modules have unbalanced scales ofparameters. Let and be the sets of model parameters of the identity and position modules. The scale",
  "get {(v)} based on {(v), g(v), l(j), W(v)I} w.r.t. V": "of is larger than due to the application of transformer. As described in lines 14-17 and lines 19-22, werespectively update {(v)} and {(v)} m 1 and m 1 times based on (11) and (14) in each iteration,where we can balance the optimization of {(v)} and {(v)} by adjusting m and m. Note that {(v)} are inputs of the position embedding module, providing node identity information for theinference of {(v)}. The optimization of {(v)} also includes the update of via gradient descent, whichalso affect the inference of {(v)}. Therefore, the two types of embeddings are jointly optimized although weadopt a separate updating strategy. The Adam optimizer is used to update {, }, with and as thelearning rates for {(v)} and {(v)}. Finally, we save model parameters after m iterations. During the model optimization, we save the sampled RWs {W(v), W(v)I}, reduced AW lookup table l, andinduced statistics {s(v), (v), g(v)} (i.e., lines 4-11 in Algorithm 4) and use them as inputs of the transductiveinference of {(v)} and {(v)}. Then, the transductive inference only includes one feedforward propagationthrough the model. We summarize this simple inference procedure in Algorithm 7 (see Appendix A). To support the inductive inference for new nodes within a graph, we adopt an incremental strategy toget the inductive statistics {s(v), (v), g(v)} via modified versions of Algorithms 1, 2, and 3 that utilizesome intermediate results derived during the training on old topology (V, E). Algorithm 5 summarizes theinductive inference within a graph. Let V and E be the set of new nodes and edge set induced by V V.We sample RWs W(v) for each new node v V and get the AW statistic s(v) w.r.t. AWs in the lookuptable l reduced on old topology (V, E) rather than all AWs. (v) is derived based on the one-hot degreeencoding truncated by the minimum and maximum degrees of (V, E). In the derivation of g(v), we computetruncated RW statistic r(v) only w.r.t. previously observed nodes V. We detail procedures to derive inductive{s(v), (v), g(v)} in Algorithms 8, 9, and 10 (see Appendix A). Similar to the transductive inference, giventhe derived {s(v), (v), g(v)}, we obtain the inductive {(v)} and {(v)} via one feedforward propagation. For the inductive inference across graphs, we sample RWs {W(v), W(v)I} on each new graph (V, E). Sincethere are no shared nodes between the training and inference topology, we only incrementally compute thereduced/truncated statistics {s(v), (v)} using the procedures of lines 3-4 in Algorithm 5. We derive globalposition encodings {g(v)} from scratch via Algorithm 3. We summarize this inductive inference procedurein Algorithm 11 (see Appendix A). We also leave detailed complexity analysis of IRWE in Appendix B.",
  "Experiments": "In this section, we elaborate on our experiments. .1 introduces experiment setups. Evaluationresults for the transductive and inductive embedding inference are described and analyzed in Sections 5.2and 5.3. Ablation study and parameter analysis are introduced in Sections 5.4 and 5.5. Due to space limit,we leave detailed experiment settings and further experiment results in Appendix D and E.",
  "Datasets. We used seven datasets commonly used by related research to validate the effectiveness of IRWE,with statistics shown in , where N, E, and K are the numbers of nodes, edges, and classes": "PPI, Wiki, and BlogCatalog are the first type of datasets (Grover & Leskovec, 2016; Zhu et al., 2021)providing the ground-truth of node positions for multi-label classification. USA, Europe, and Brazil are thesecond type of datasets (Ribeiro et al., 2017; Zhu et al., 2021) with node identity ground-truth for multi-classclassification. In summary, PPI, Wiki, and BlogCatalog are widely used to evaluate the quality of positionembedding while USA, Europe, and Brazil are well-known datasets for the evaluation of identity embedding. PPIs is a widely used dataset for the inductive inference across graphs (Hamilton et al., 2017; Velikoviet al., 2018), which includes a set of protein-protein interaction graphs (in terms of connected components).In addition to graph topology, PPIs also provides node features and ground-truth for node classification.As stated in , we do not consider graph attributes due to the complicated correlations betweentopology and attributes. It is also unclear whether the classification ground-truth is dominated by topologyor attributes. Therefore, we only used the graph topology of PPIs. Downstream Tasks.We adopted multi-label and multi-class node classification for the evaluation ofposition and identity embeddings on the first and second types of datasets, respectively. In particular, eachnode may belong to multiple classes in multi-label classification while each node only belongs to one classin multi-class classification. We used Micro F1-score as the quality metric for the two classification tasks.To avoid the exception that some labels are not presented in all training examples, we removed classes withvery few numbers of members (i.e., less than 8) when conducting node classification. We also adopted unsupervised node clustering to evaluate the quality of identity and position embeddings.Inspired by spectral clustering (Von Luxburg, 2007) and Hypothesis 1, we constructed an auxiliary (top-10)similarity graph GD based on the high-order degree features {(v) R(l+1)e} derived via a procedure similarto Algorithm 2. The only difference between {(v)} (used for evaluation) and {(v)} (used in IRWE) is that(v) is derived from the rooted subgraph Gs(v, l) but not the sampled RWs W(v). To obtain {(v)}, we setl = 5 (i.e., the order of neighbors) and e = 500 (i.e., the dimensionality of the one-hot degree encoding) forthe first type of datasets while we let l = 3 and e = 200 for PPIs. Namely, we applied a clustering algorithmto embeddings learned on the original graph G but evaluated the clustering result on GD. We define this taskas the node identity clustering and expect that it can measure the quality of identity embeddings becuasehigh-order degree features {(v)} can capture node identities. In addition, we treated the node clustering evaluated on the original graph G as community detection (New-man, 2006), a task commonly used for the evaluation of position embeddings.Normalized cut (NCut)(Von Luxburg, 2007) w.r.t. GD and modularity (Newman, 2006) w.r.t. G were used as quality metrics fornode identity clustering and community detection. We leave details of NCut and modularity in Appendix D.",
  "Evaluation of Transductive Embedding Inference": "We first evaluated the transductive embedding inference of all the methods on the first and second typesof datasets. For the two classification tasks, we randomly sampled T {20%, 40%, 60%, 80%} and 10% ofthe nodes to form the training and validation sets with the remaining nodes as the test set on each dataset.Similar to 10-fold cross-validation, we repeated the data splitting 10 times, where we split the node set into10 subsets with each one as the validation set in a round and used the average quality w.r.t. the validationset to tune parameters of all the methods. Evaluation results of the transductive embedding inference areshown in Tables 4 and 5, where metrics are in bold or underlined if they perform the best or within top-3. For transductive baselines, identity embedding approaches (i.e., struc2vec, struc2gauss, and PhN-HK) andposition embedding methods (i.e., node2vec, GraRep, PhN-PPMI) are in groups with top clustering perfor-mance (in terms of NCut and modularity) on the first and second types of datasets, respectively. Since priorstudies have demonstrated the ability of these transductive baselines to capture node identities or positions,the evaluation results validate our motivation of using node identity clustering and community detection to",
  "IRWE()58.0263.58 66.19 65.46 1.7852.06 54.88 58.10 60.24 -0.5270.22 74.09 72.2575.00 1.17IRWE()55.2558.6960.6461.6831.2443.6747.4150.2549.2717.7436.8540.1544.2541.4321.26": "evaluate the quality of identity and position embeddings. Our node identity clustering results also validateHypothesis 1 that the high-order degree features {(v)} can encode node identity information. On each dataset, most baselines can only achieve relatively high performance for one task w.r.t. identityor position embedding.It indicates that most existing embedding methods can only capture either nodeidentities or positions.In most cases, [n2v||s2v] outperforms neither (i) node2vec for tasks w.r.t.nodepositions nor (ii) struc2vec for those w.r.t. node identities. It implies that the simple integration of the twotypes of embeddings may even damage the quality of capturing node identities or positions. Therefore, it ischallenging to preserve both properties in a common embedding space. For tasks w.r.t. each type of embedding, conventional transductive baselines can achieve much better per-formance than most of the advanced inductive baselines.One possible reason is that existing inductiveembedding approaches rely on the availability of node attributes. However, there are complicated correla-tions between graph topology and attributes as discussed in . Our results imply that the embeddingquality of some inductive baselines is largely affected by their attribute inputs. Some standard settings for the",
  "Evaluation of Inductive Embedding Inference": "We further consider the inductive inference (i) for new unseen nodes within a graph and (ii) across graphs,which were evaluated on the (i) first two types of datasets (i.e., PPI, Wiki, BlogCatalog, USA, Europe,and Brazil) and (ii) PPIs, respectively. We could only evaluate the quality of inductive methods becausetransductive baselines cannot support the inductive inference. For the inductive inference within a graph, we randomly selected 80%, 10%, and 10% of nodes on eachsingle graph to form the training, validation, and test sets (denoted as Vtrn, Vval, and Vtst), where Vvaland Vtst represent sets of new nodes not observed in Vtrn. The embedding model of each inductive methodwas optimized only on the topology induced by Vtrn. When validating and testing a method using thenode classification task, embeddings w.r.t. Vtrn and Vtrn Vval were used to train the downstream logisticregression. We repeated the data splitting 10 times following a strategy similar to 10-fold cross validationand used the average quality w.r.t. the validation set to tune parameters of all the methods. For the inductive inference across graphs, we sampled 3 graphs from PPIs denoted as Gtrn, Gval, and Gtst,which were used for training, validation, and testing. We first optimized the embedding model on Gtrn.To validate or test the model, we derived inductive embeddings w.r.t. Gval or Gtst and obtained clusteringresults for evaluation by applying KMeans. This procedure was repeated 5 times, where 15 graphs weresampled. Finally, the average quality over the 5 data splits was reported. Evaluation results of the inductive embedding inference are depicted in , where metrics are in boldor underlined if they perform the best or within top-3.IRWE achieves the best quality in most cases.In particular, the quality metrics of IRWE are significantly better than other inductive baselines, whoseinductiveness relies on the availability of node attributes. Our results further demonstrate that IRWE cansupport the inductive inference of identity and position embeddings, simulataneously generating two sets ofinformative embeddings, without relying on the availability and aggregation of any graph attributes.",
  "Ablation Study": "In ablation study, we respectively removed some components from the IRWE model to explore their effec-tiveness for ensuring the high embedding quality of our method. For the identity embedding module, weconsidered the (i) AW embedding regularization loss Lreg (12), (ii) AW statistic inputs {s(v)}, (iii) high-order degree feature inputs {(v)}, and (iv) identity embedding regularization loss Lreg (13). In cases (i)and (iv), identity embeddings were only optimized via one loss (i.e., Lreg or Lreg).",
  ": Parameter analysis w.r.t. l, , and on PPI in terms of F1-score (node position classification)and NCut (node identity clustering)": "For the position embedding module, we checked the effectiveness of the (v) identity embedding inputs {(v)},(vi) global position encoding inputs {g(v)}, (vii) attentive readout function ROut() described in (9), and(viii) reconstruction loss L (14). In case (v), the two modules of IRWE were independently optimized. Forcase (vii), we simply averaged the representations in t(v) to replace ROut(). For case (viii), we replaced thecontrastive statistics C in (14) with adjacency matrix A. We also used some induced statistics as baselines. Concretely, we evaluated the quality of (ix) AW statistics{s(v)} and (x) degree features {(v)} to capture node identities. In contrast, we checked the quality of(xi) global position encodings {g(v)} and (xii) contrastive statistics C for node positions. In case (xii), wederived representations with the same dimensionality as other embedding methods by applying SVD to C. As a demonstration, we report results of transductive embedding inference on PPI and USA (with 80% ofnodes sampled as the training set for classification) in . According to our results, Lreg is essentialfor identity embedding learning, since there are significant quality declines for node identity clustering andclassification in case (iv). ROut() and L are key components to capture node positions due to the significantquality declines for node position classification and community detection in cases (vii) and (viii). All theremaining components can further enhance the ability to capture node identities and positions. The jointoptimization of identity and position embeddings can also improve the quality of one another.",
  "Parameter Analysis": "We tested the effects of (i) RW length l, (ii) in loss (11), and (iii) temperature parameter in loss (14).Concretely, we set l {4, 5, , 9}, {0.1, 0.5, 1, 5, 10, 50, 100}, and {1, 5, 10, 50, 100, 500, 1000}.Example parameter analysis results of the transductive embedding inference on PPI and USA (with 80%of nodes sampled as the training set for classification) are illustrated in and 6. The quality of bothtypes of embeddings is not sensitive to the settings of l. Compared with position embeddings, the quality",
  "Conclusion": "In this paper, we considered unsupervised network embedding and explored a unified framework for the jointoptimization and inductive inference of identity and position embeddings without relying on the availabilityand aggregation of graph attributes. An IRWE method was proposed, which combines multiple attentionunits with different designs to handle RWs on graph topology. We demonstrated that AW derived fromRW and induced statistics can (i) be features shared by all possible nodes and graphs to support inductiveinference and (ii) characterize node identities to derive identity embeddings. We also showed the intrinsicrelation between the two types of embeddings.Based on this relation, the derived identity embeddingscan be used for the inductive inference of position embeddings. Experiments on public datasets validatedthat IRWE can achieve superior quality compared with various baselines for the transductive and inductiveinference of identity and position embeddings. We leave discussions of future directions in Appendix F.",
  "Rosa I Arriaga and Santosh Vempala. An algorithmic theory of learning: Robust concepts and randomprojection. Machine learning, 63:161182, 2006": "Shaosheng Cao, Wei Lu, and Qiongkai Xu. Grarep: Learning graph representations with global structuralinformation. In Proceedings of the 24th ACM International on Conference on Information & KnowledgeManagement, pp. 891900, 2015. Dexiong Chen, Leslie OBray, and Karsten Borgwardt. Structure-aware transformer for graph representationlearning. In Proceedings of the 2022 International Conference on Machine Learning, pp. 34693489, 2022. Han Chen, Ziwen Zhao, Yuhua Li, Yixiong Zou, Ruixuan Li, and Rui Zhang. Csgcl: Community-strength-enhanced graph contrastive learning. In Proceedings of the 32nd International Joint Conference on Arti-ficial Intelligence, pp. 20592067, 2023. Claire Donnat, Marinka Zitnik, David Hallac, and Jure Leskovec. Learning structural node embeddingsvia diffusion wavelets. In Proceedings of the 24th ACM SIGKDD International Conference on KnowledgeDiscovery & Data Mining, pp. 13201329, 2018.",
  "Aditya Grover and Jure Leskovec. node2vec: Scalable feature learning for networks. In Proceedings of the22nd ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 855864,2016": "Junliang Guo, Linli Xu, and Jingchang Liu. Spine: Structural identity preserved inductive network embed-ding. In Proceedings of the 28th International Joint Conference on Artificial Intelligence, pp. 23992405,2019. Xuan Guo, Wang Zhang, Wenjun Wang, Yang Yu, Yinghui Wang, and Pengfei Jiao. Role-oriented graphauto-encoder guided by structural information. In Proceedings of the 25th International Conference onDatabase Systems for Advanced Applications, pp. 466481, 2020.",
  "Peter Hoff. Modeling homophily and stochastic equivalence in symmetric relational data. Advances in NeuralInformation Processing Systems, 20, 2007": "Zhenyu Hou, Xiao Liu, Yukuo Cen, Yuxiao Dong, Hongxia Yang, Chunjie Wang, and Jie Tang. Graphmae:Self-supervised masked graph autoencoders. In Proceedings of the 28th ACM SIGKDD Conference onKnowledge Discovery & Data Mining, pp. 594604, 2022. Zhenyu Hou, Yufei He, Yukuo Cen, Xiao Liu, Yuxiao Dong, Evgeny Kharlamov, and Jie Tang. Graphmae2: Adecoding-enhanced masked self-supervised graph learner. In Proceedings of the 2023 ACM Web Conference,pp. 737746, 2023.",
  "Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. InProceedings of the 5th International Conference on Learning Representations, 2017": "Kai Lei, Meng Qin, Bo Bai, and Gong Zhang.Adaptive multiple non-negative matrix factorization fortemporal link prediction in dynamic networks. In Proceedings of the 2018 ACM SIGCOMM Workshop onNetwork Meets AI & ML, pp. 2834, 2018. Kai Lei, Meng Qin, Bo Bai, Gong Zhang, and Min Yang. Gcn-gan: A non-linear temporal link predic-tion model for weighted dynamic networks. In Proceedings of the 2019 IEEE Conference on ComputerCommunications, pp. 388396, 2019. Pan Li, Yanbang Wang, Hongwei Wang, and Jure Leskovec.Distance encoding: Design provably morepowerful neural networks for graph representation learning. Proceedings of the 2020 Advances in NeuralInformation Processing Systems, 33:44654478, 2020. Wei Li, Meng Qin, and Kai Lei.Identifying interpretable link communities with user interactions andmessages in social networks. In Proceedings of the 2019 IEEE International Conference on Parallel &Distributed Processing with Applications, pp. 271278, 2019. Zirui Liu, Chen Shengyuan, Kaixiong Zhou, Daochen Zha, Xiao Huang, and Xia Hu. Rsc: accelerate graphneural networks training via randomized sparse computations. In International Conference on MachineLearning, pp. 2195121968, 2023.",
  "Dai Quoc Nguyen, Tu Dinh Nguyen, and Dinh Phung. Universal graph transformer self-attention networks.In Companion Proceedings of the Web Conference 2022, pp. 193196, 2022": "Yulong Pei, Xin Du, Jianpeng Zhang, George Fletcher, and Mykola Pechenizkiy. struc2gauss: Structuralrole preserving network embedding via gaussian embedding. Data Mining & Knowledge Discovery, 34(4):10721103, 2020. Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. Deepwalk: Online learning of social representations. InProceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery & data Mining,pp. 701710, 2014.",
  "Meng Qin, Di Jin, Kai Lei, Bogdan Gabrys, and Katarzyna Musial. Adaptive community detection incor-porating topology and content in social networks. Knowledge-Based Systems, 161:342356, 2018": "Meng Qin, Chaorui Zhang, Bo Bai, Gong Zhang, and Dit-Yan Yeung. Towards a better trade-off betweenquality and efficiency of community detection: An inductive embedding method across graphs. ACMTransactions on Knowledge Discovery from Data, 2023a. Meng Qin, Chaorui Zhang, Bo Bai, Gong Zhang, and Dit-Yan Yeung. High-quality temporal link predictionfor weighted dynamic graphs via inductive embedding aggregation. IEEE Transactions on Knowledge andData Engineering, 2023b. Leonardo FR Ribeiro, Pedro HP Saverese, and Daniel R Figueiredo. struc2vec: Learning node representationsfrom structural identity. In Proceedings of the 23rd ACM SIGKDD International Conference on KnowledgeDiscovery & Data Mining, pp. 385394, 2017. Ryan A Rossi, Di Jin, Sungchul Kim, Nesreen K Ahmed, Danai Koutra, and John Boaz Lee. On proximityand structural role-based embeddings in networks: Misconceptions, techniques, and applications. ACMTransactions on Knowledge Discovery from Data, 14(5):137, 2020. Balasubramaniam Srinivasan and Bruno Ribeiro. On the equivalence between positional node embeddingsand structural graph representations.In Proceedings of the 8th International Conference on LearningRepresentations, 2020. Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei. Line: Large-scale informationnetwork embedding. In Proceedings of the 24th International Conference on World Wide Web, pp. 10671077, 2015. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ukasz Kaiser,and Illia Polosukhin. Attention is all you need. In Proceedings of the 2017 Advances in Neural InformationProcessing Systems, pp. 59986008, 2017.",
  "Ulrike Von Luxburg. A tutorial on spectral clustering. Statistics & Computing, 17(4):395416, 2007": "Xiao Wang, Peng Cui, Jing Wang, Jian Pei, Wenwu Zhu, and Shiqiang Yang. Community preserving networkembedding. In Proceedings of the 2017 AAAI Conference on Artificial Iintelligence, pp. 203209, 2017. Xiao Wang, Meiqi Zhu, Deyu Bo, Peng Cui, Chuan Shi, and Jian Pei. Am-gcn: Adaptive multi-channelgraph convolutional networks. In Proceedings of the 26th ACM SIGKDD International Conference onKnowledge Discovery & Data Mining, pp. 12431253, 2020. Jun Wu, Jingrui He, and Jiejun Xu. Demo-net: Degree-specific graph neural networks for node and graphclassification. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery& Data Mining, pp. 406415, 2019. Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and S Yu Philip. A comprehensivesurvey on graph neural networks. IEEE Transactions on Neural Networks & Learning Systems, 32(1):424,2020.",
  "Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? InProceedings of the 7th International Conference on Learning Representations, 2019": "Yuchen Yan, Yongyi Hu, Qinghai Zhou, Lihui Liu, Zhichen Zeng, Yuzhong Chen, Menghai Pan, HuiyuanChen, Mahashweta Das, and Hanghang Tong. Pacer: Network embedding from positional to structural.In Proceedings of the ACM on Web Conference 2024, pp. 24852496, 2024. Yang Yang, Jie Tang, Cane Wing-ki Leung, Yizhou Sun, Qicong Chen, Juanzi Li, and Qiang Yang. RAIN:social role-aware information diffusion. In Proceedings of the 29th AAAI Conference on Artificial Intelli-gence, pp. 367373, 2015.",
  "Jiaxuan You, Jonathan M Gomes-Selman, Rex Ying, and Jure Leskovec. Identity-aware graph neural net-works. In Proceedings of the 2012 AAAI Conference on Artificial Intelligence, pp. 1073710745, 2021": "Wentao Zhang, Yu Shen, Zheyu Lin, Yang Li, Xiaosen Li, Wen Ouyang, Yangyu Tao, Zhi Yang, and BinCui. Pasca: A graph neural architecture search system under the scalable paradigm. In Proceedings of the2022 ACM Web Conference, pp. 18171828, 2022. Jing Zhu, Xingyu Lu, Mark Heimann, and Danai Koutra. Node proximity is all you need: Unified structuraland positional node and graph embedding. In Proceedings of the 2021 SIAM International Conference onData Mining, pp. 163171, 2021.",
  "Therefore, the transudcitve inference of identity embeddings {(v)} and position embeddings {(v)} onlyincludes one feedforward propagation through the model": "Procedures to get inductive AW statistics {s(v)}, high-order degree features {(v)}, and global positionencodings {g(v)}, which support the inductive inference for new nodes within a graph (i.e., Algorithm 5),are described in Algorithms 8, 9, and 10, respectively. When deriving {s(v)}, we only compute the frequencyof AWs in the lookup table l reduced on (V, E) rather than all AWs. Moreover, we get {(v)} based on theone-hot degree encoding truncated by the minimum and maximum degrees of the training topology (V, E)but not those of the inference topology (V V, E). For g(v), we compute truncated RW statistic r(v) onlyw.r.t. previously observed nodes V rather than V V. The inductive inference across graphs is summarized in Algorithm 11. We sample RWs {W(v), W(v)I} oneach new graph (V, E). Since there are no shared nodes between the training topology (V, E) and infer-ence topology (V, E), we only incrementally compute statistics {s(v), (v)} based on {l, degmin, degmax}derived from (V, E) but compute global position encodings {g(v)} from scratch.",
  "BComplexity Analysis": "The complexity of the RW sampling starting from each node (i.e., Algorithm 6) is no more than O(nSl).The complexities to derive AW statistics s(v) (i.e., Algorithm 1), high-order degree features (v) (i.e.,Algorithm 2), and global position encoding g(v) (i.e., Algorithm 3) w.r.t. a node v are O(nS), O(nSl), andO(nSl + k(v)d), with k(v) as the number of nodes observed in W(v). The overall complexity to derive theRW-induced statistics (i.e., the feature inputs of IRWE) from a graph (V, E) is no more than O(|V|nSl +|V|nS + |V|nSl + (|V|nSl + kd)) = O(|V|nSl + kd), with k :=",
  "vV k(v)": "As described in Algorithm 7, the transductive inference of IRWE only includes one feedforward propagationthrough the model. Its complexity is no more than O(ll2d+|V|(el + l)d+|V|ldh+(|V|d2 +|V|d)+|V|(d+l)d + |V|nIl2dh + nId) = O(|V|(l + nIl2)dh), where we assume that el d, l2 |V|, and d l; h is thenumber of attention heads. According to Algorithm 5, the complexity of inductive inference for new nodeswithin a graph is O(|V|nSl+kd+|V V|(l +nIl2)dh), with k :=",
  "(v)ie:(i+1)e (v)ie:(i+1)e + d(u)": "l is the RW/AW length; l and l denote the (i) number of AWs w.r.t. length l and (ii) reduced value of l;L is the number of layers of GNN or transformer encoder; h is the number of attention heads. Since mosttransductive embedding methods (e.g., node2vec and struc2vec) follow the embedding lookup scheme, theirmodel parameters are with a complexity of at least O(Nd). Most inductive approaches rely on the attributeaggregation mechanism of GNNs. Their model parameters have a complexity of at least O(Ld2). Methodsbased on the multi-head attention or transformer encoder (e.g., SANNE, UGFormer, and IRWE) shouldhave at least O(Lhd2) learnable model parameters. In addition, IRWE also includes the AW auto-encoderand MLPs in the identity embedding encoder and decoder. Therefore, the model parameters of IRWE havea complexity of O(l2d + (l + le)d + Lhd2) = O(l2d + ld + Lhd2), where we assume that el d.",
  "(d, e, nS, nI)(, )(m, m, m)(l, , )": "PPI(256, 100, 1e3, 10)(5e-4,1e-4)(1e3, 20, 1)(7, 10, 5e2)Wiki(256, 100, 1e3, 10)(1e-3,5e-4)(1e3, 1, 1)(7, 10, 5e2)Blog(256, 100, 1e3, 10)(5e-4,5e-4)(1e3, 20, 5)(5, 10, 5)USA(128, 100, 1e3, 10)(5e-4,5e-4)(500, 10, 1)(9, 10, 10)Europe(64, 100, 1e3, 10)(5e-4,5e-4)(200, 1, 1)(9, 10, 10)Brazil(64, 32, 1e3, 10)(5e-4,5e-4)(200, 1, 1)(9, 0.1, 1e2)PPIs(256, 100, 1e3, 10)(5e-4,5e-4)(1000, 5, 1)(9, 10, 50)",
  "i deg(vi)/2 is the number of edges": "The parameter settings of IRWE for the transductive and inductive inference are depicted in Tables 9 and10, where d is the embedding dimensionality; e is the dimensionality of one-hot degree encoding for thedegree features {(v)}; nS := |W(v)| and nI := |W(v)I| are the number of sampled RWs and number ofRWs used to infer position embeddings for each node v; and are learning rates to optimize identityand position embeddings; m is the number of training iterations; in each iteration, we update identity andposition embeddings m and m times; l is the RW length; and are hyper-parameters in the traininglosses. Tables 11 and 12 give layer configurations for the transductive and inductive embedding inference, whereEnc() and Dec() denote the AW encoder and decoder described in (1); Reds() is the feature reduction",
  "[n2v||att]50.803.3835.263.1244.053.5054.023.13[s2v||att]-5.761.81-12.771.33-2.811.00-9.581.35": "unit (2); Dec() represents the identity embedding decoder (4); ReAtt() is the attentive reweighting function(6); l := |l| is the reduced number of AWs; h, htran, and hrout represent the numbers of attention headsin identity embedding encoder (3), transformer encoder (8), and attentive readout function (9); Ltran isthe number of transformer encoder layers; t, s, and r denote the activation functions of Tanh, Sigmoid,and ReLU. For our IRWE method, we recommend setting l {4, 5, , 9}, {0.1, 0.5, 1, 5, 10}, {1, 5, 10, 50, 100, 500, 1000}, and m, m {1, 5, 10, 20}. We adopted the standard multi-head attention (Vaswani et al., 2017) to build the identity embedding encoder(3) and attentive readout unit (9) of IRWE. An attention unit includes the inputs of key, query, and valuedescribed by K Rmd, Q Rnd, and V Rmd. Assume that there are h attention heads. Let d = d/h.For the j-th head, we first derive linear mappings K(j) = KW(j)k , Q(j) = QW(j)q , and V(j) = VW(j)v , with{W(j)k Rd d, W(j)q Rd d, W(j)v Rd d} as trainable parameters. The attention head is defined as",
  "We further concatenate the outputs of all the heads via Z = Att(Q, K, V) := [Z(1)|| ||Z(h)]": "All the experiments were conducted on a server with AMD EPYC 7742 64-Core CPU, 512GB main memory,and one NVIDIA A100 GPU (80GB memory). We used the official code or public implementations of allthe baselines and tuned parameters to report their best performance. On each dataset, we set the sameembedding dimensionality for all the methods.",
  "EFurther Experiment Results": "To demonstrate the possible inconsistency of graph attributes for identity and position embedding as dis-cussed in , we conducted additional experiments on four attributed graphs (i.e., Cornell, Texas,Washington, and Wisconsin) from WebKB1. For each graph, we extracted the largest connected componentfrom its topology. After the pre-processing, we have (N, E, M, K) = (183, 227, 1703, 5), (183, 279, 1703, 5),(215, 365, 1703, 5), and (251, 450, 1703, 5) for Cornell, Texas, Washington, and Wisconsin, where N, E, andK are numbers of nodes, edges, and clusters; M is the dimensionality of node attributes. We then applied node2vec and struc2vec, which are typical position and identity embedding baselines asdescribed in , to the extracted topology of each graph, where we set embedding dimensionality d = 64.Furthermore, we derived special attribute embeddings (denoted as att-emb) with the same dimensionalityby applying SVD to node attributes. Namely, we have three baseline methods (e.g., node2vec, struc2vec,and att-emb). To simulate the incorporation of attributes, we concatenated att-emb with node2vec andstruc2vec, forming another two additional baselines denoted as [n2v||att] and [s2v||att]. The unsupervisedcommunity detection and node identity clustering were adopted as the downstream tasks for position andidentity embedding, respectively. The evaluation results are depicted in , where att-emb outperformsneither (i) node2vec for community detection nor (ii) struc2vec for node identity clustering; the concatenationof att-emb cannot further improve the embedding quality of node2vec and struc2vec. The results imply that(i) attributes may fail to capture both node positions and identities; (ii) the simple integration of attributesmay even damage the quality of position and identity embeddings.",
  "Some possible future research directions of this study are summarized as follows": "In this study, we focused on network embedding where topology is the only available information sourcewithout any attributes, due to the complicated correlations between the two sources (Qin et al., 2018; Liet al., 2019; Wang et al., 2020; Qin & Lei, 2021).We intend to explore the adaptive incorporation ofattributes. Concretely, when attributes carry characteristics consistent with topology, one can fully utilizeattribute information to enhance the embedding quality. In contrast, when there is inconsistent noise inattributes, we need to adaptively control the effect of attributes to avoid unexpected quality degradation. In addition to mapping each node to a low-dimensional vector (a.k.a.node-level embedding), networkembedding also includes the representation of a graph (a.k.a. graph-level embedding). We plan to extendIRWE to the graph-level embedding and evaluate the embedding quality for some graph-level downstreamtasks (e.g., graph classification). To analyze the relations of graph-level embeddings to identity and positionembeddings is also our next focus. The optimization of IRWE adopts the standard full-batch setting, where we derive statistics or embeddingsw.r.t. all the nodes when computing the training losses. This setting may not be scalable to graphs withlarge numbers of nodes. Inspired by recent studies of scalable GNNs (Zhang et al., 2022; Liu et al., 2023),we intend to explore a scalable optimization strategy based on mini-batch settings."
}