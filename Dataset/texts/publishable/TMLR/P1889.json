{
  "Abstract": "Artificial neural networks are powerful machine learning methods used in many modernapplications. A common issue is that they have millions or billions of parameters and tendto overfit. Bayesian neural networks (BNN) can improve on this since they incorporateparameter uncertainty. Latent binary Bayesian neural networks (LBBNN) further take intoaccount structural uncertainty by allowing the weights to be turned on or off, enabling infer-ence in the joint space of weights and structures. Mean-field variational inference is typicallyused for computation within such models. In this paper, we will consider two extensionsof variational inference for the LBBNN: Firstly, by using the local reparametrization trick(LCRT), we improve computational efficiency. Secondly, and more importantly, by usingnormalizing flows on the variational posterior distribution of the LBBNN parameters, welearn a more flexible variational posterior than the mean field Gaussian. Experimental re-sults on real data show that this improves predictive power compared to using mean fieldvariational inference on the LBBNN method, while also obtaining sparser networks. Wealso perform a simulation study, where we consider variable selection in a logistic regressionsetting with highly correlated data, where the more flexible variational distribution improvesresults.",
  "Introduction": "Modern deep learning architectures can have billions of trainable parameters (Khan et al., 2020). Due tothe large number of parameters in the model, the network can overfit, and therefore may not generalize wellto unseen data. Further, the large number of parameters gives computational challenges both concerningtraining the network and for prediction. Various regularization methods are used to try to deal with this,such as early stopping (Prechelt, 1998), dropout (Srivastava et al., 2014) or data augmentation (Shorten &Khoshgoftaar, 2019). These techniques are heuristic and therefore it is not always clear how to use themand how well they work in practice. Another issue with deep learning models is that they often make overconfident predictions. In Szegedy et al.(2013), it was shown that adding a small amount of noise to an image can trick a classifier into making acompletely wrong prediction (with high confidence), even though the image looks the same to the human eye.The opposite is also possible, images that are white noise can be classified with almost complete certaintyto belong to a specific class (Nguyen et al., 2015).",
  ": A dense network on the left, one possible sparse structure on the right": "Bayesian neural networks (BNN, Neal, 1992; MacKay, 1995; Bishop, 1997) use a rigorous Bayesian method-ology to handle parameter and prediction uncertainty. In principle, prior knowledge can be incorporatedthrough appropriate prior distributions, but most approaches within BNN so far only apply very simpleconvenience priors (Fortuin, 2022). However, approaches where knowledge-based priors are incorporated arestarting to appear (Tran et al., 2022; Sam et al., 2024). Incorporating parameter uncertainty can lead toless overfitting and better-calibrated predictions, however, this comes at the expense of extremely high com-putational costs. Until recently, inference on Bayesian neural networks could not scale to large multivariatedata due to limitations of standard Markov chain Monte Carlo (MCMC) approaches, the main quantitativeprocedure used for complex Bayesian inference. Recent developments of variational Bayesian approaches(Gal, 2016) allow us to approximate the posterior of interest and lead to more scalable methods. In the frequentist setting, the lottery ticket hypothesis (Frankle & Carbin, 2018; Pensia et al., 2020) statesthat dense networks contain sparse subnetworks that can achieve a similar performance of the full network.These sparse subnetworks are typically obtained by threshold pruning of the weights.Similarly, in theBayesian framework, Deng et al. (2019) obtain sparse networks with magnitude-based weight pruning. In amore recent work, Li et al. (2024) introduce a methodology where sparsity is inherent in the posterior fromthe very beginning of training, significantly saving on computational costs. In that work, the (non-Bayesian)inclusion parameters are also pruned, (and reintroduced back) using some pruning criteria. Another way toobtain sparse networks in the Bayesian framework is through sparsity-inducing priors. Variational dropout(Kingma et al., 2015; Molchanov et al., 2017) uses the independent log uniform prior on the weights. However,this is an improper prior, which in combination with commonly used likelihood functions leads to an improperposterior, therefore (Hron et al., 2017) argues that the obtained results can not be interpreted as a Bayesianmethodology to sparsification, but rather falls into the same category as frequentist approaches. Anothertype of sparsity-inducing prior is the independent scale mixture prior, where Blundell et al. (2015) proposeda mixture of two Gaussian densities, where using a small variance for the second mixture component leadsto many of the weights having a prior around zero. In this work, we consider the spike-and-slab prior, used in latent binary Bayesian neural networks (LBBNN)introduced by Hubin & Storvik (2019; 2024) and concurrently in Bai et al. (2020), resulting in a formalBayesian approach for obtaining sparse subnetworks. The spike-and-slab prior for a special case of LBBNNwith the ReLu activation function was studied from a theoretical perspective in Polson & Rokov (2018).In Hubin & Storvik (2019) it was empirically shown that using this prior will induce a very sparse network(around 90 % of the weights were removed) while maintaining good predictive power. Using this approachthus takes into account uncertainty around whether each weight is included or not (structural uncertainty)and uncertainty in the included weights (parameter uncertainty) given a structure, allowing for a Bayesianapproach to network sparsification (see ), thus weights are removed based on (learned) inclusionprobabilities rather than ad-hoc pruning. In Hubin & Storvik (2019; 2024), a simple mean-field variational approximation was applied for performingcomputation. In this paper, we show that transforming the variational posterior distribution with normalizingflows can result in even sparser networks while improving predictive power compared to the earlier work.Additionally, we demonstrate that the flow network handles predictive uncertainty well, and performs better",
  "Published in Transactions on Machine Learning Research (10/2024)": "over these 10 repetitions, in addition to the mean sparsity. We also report the expected calibration error(ECE), in addition to pWAIC1 and pWAIC2 for the classification datasets, while for the regression datasets wereport RMSE, the pinball loss, pWAIC1 and pWAIC2. For the six classification datasets, most are taken from the UCI machine learning repository. The CreditApproval dataset (Quinlan) consists of 690 samples with 15 variables, with the response variable beingwhether someone gets approved for a credit card or not. The Bank Marketing dataset (Moro et al., 2012)consists of data (45211 samples and 17 variables) related to a marketing campaign of a Portuguese bankinginstitution, where the goal is to classify whether the persons subscribed to the service or not. In addition tothis, we use the Census Income dataset (Kohavi, 1996) with 48842 samples and 14 variables, where we tryto classify whether someones income exceeds 50000 dollars per year. Additionally, we have three datasetsrelated to classifying food items. The first, the Raisins dataset (inar et al., 2020), consists of 900 samplesand 7 variables, where the goal is to classify into two different types of raisins grown in Turkey. Secondly,we use the Dry Beans dataset (UCI, 2020), consisting of 13611 samples, 17 variables, and 7 different typesof beans. Lastly, the Pistachio dataset (Ozkan et al., 2021) consists of 2148 samples and 28 variables, withtwo different types of Pistachios. For the regression datasets, we use the abalone shell data (Nash et al., 1995), the wine quality dataset (Cortezet al., 2009), and the Boston housing data (Harrison Jr & Rubinfeld, 1978). To avoid model misspecification,responses were standardized for the regression datasets. The variance of the responses was then assumedfixed and equal to 1 in all of the models, while the mean parameter was modeled. The predictive accuracy and density for the classification datasets can be found in and ,and the expected calibration error in . Finally, we have the pWAIC1 and pWAIC2 metrics in and 13, respectively. For the regression datasets, we have the RMSE and density results in , inaddition to pinball loss in and pWAIC1 and pWAIC2 in . For the classification datasets, we see that our LBBNN-FLOW method typically performs well compared tothe LBBNN baseline, mostly having higher predictive power, also with the median probability model. Italso performs well compared to our other baseline methods. On calibration, we see that our method againperforms well compared to baselines. Finally, for the pWAIC1 and pWAIC2, we see that using the medianprobability model typically reduces this metric.We note that our method often has lower values thanBNN-FLOW and BNN, even though these methods have fewer parameters. As mentioned before, however,it is unclear how good of an estimate this is for the effective number of parameters in highly non-linearmethods such as neural networks. For the regression examples, our method performs slightly worse thansome baselines in terms of RMSE and pinball loss, however, LBBNN-LCRT performs best (on average) onone dataset.",
  "where j(x) = u(L)j, Additionally, u(l1) denotes the inputs from the previous layer (with u0 = x corre-": "sponding to the explanatory variables), the w(l)ij s are the weights, the b(l)j s are the bias terms, and n(l) (andn(0) = n) the number of inputs at layer l of a total L layers. Further, we have the elementwise non-linearactivation functions (l). The additional parameters (l)ij {0, 1} denote binary inclusion variables for thecorresponding weights. Following Polson & Rokov (2018); Hubin & Storvik (2019); Bai et al. (2020), we consider a structure tobe defined by the configuration of the binary vector , and the weights of each structure conditional onthis configuration. To consider uncertainty in both structures and weights, we use the spike-and-slab prior,where for each (independent) layer l of the network, we also consider the weights to be independent:",
  "p((l)ij ) = Bernoulli((l)ij ; (l))": "We will use the nomenclature from Hubin & Storvik (2019) and refer to this as the LBBNN model. Here,() is the Dirac delta function, which is considered to be zero everywhere except for a spike at zero. Inaddition, 2 and denote the prior variance and the prior inclusion probability of the weights, respectively.In practice, we use the same variance and inclusion probability across all the layers and weights, but thisis not strictly necessary. In principle, one can incorporate knowledge about the importance of individualcovariates or their co-inclusion patterns by adjusting the prior inclusion probabilities for the input layer orspecifying hyper-priors. This is common in Bayesian model selection literature (Fletcher & Fletcher, 2018),but not yet within BNNs.",
  "Bayesian inference": "The main motivation behind using LBBNNs is that we are able to take into account both structural andparameter uncertainty, whereas standard BNNs are only concerned with parameter uncertainty. By doinginference through the posterior predictive distribution, we average over all possible structural configurations,and parameters. For a new observation y given training data, D, we have:",
  "wp(y|w, , D)p(w, |D) dw": "This expression is intractable due to the ultra-high dimensionality of w and , and using Monte Carlosampling as an approximation is also challenging due to the difficulty of obtaining samples from the posteriordistribution, p(w, |D). Instead of trying to sample from the true posterior, we turn it into an optimizationproblem, using variational inference (VI, Blei et al., 2017). The key idea is that we replace the true posteriordistribution with an approximation, q(w, ), with denoting some variational parameters.We learnthe variational parameters that make the approximate posterior as close as possible to the true posterior.Closeness is measured through the Kullback-Leibler (KL) divergence,",
  "Choices of variational families": "A common choice (Blundell et al., 2015) for the approximate posterior in (dense) Bayesian neural networksis the mean-field Gaussian distribution. For simplicity of notation, denote now by W the set of weightscorresponding to a specific layer. Note that from here on, we drop the layer notation for readability, sincethe parameters at different layers will always be considered independent in both the variational distributionand the prior. Then",
  "(4)": "Here, is the set of inclusion indicators corresponding to a specific layer. However, the mean-field Gaussiandistribution (Blundell et al., 2015) is typically too simple to be able to capture the complexity of the trueposterior distribution. We follow Ranganath et al. (2016), and introduce a set of auxiliary latent variablesz to model dependencies between the weights in q, and use the following variational posterior distribution:",
  ": On the left, the mean-field variational posterior where the weights are assumed independent. Onthe right, the latent variational distribution z allows for modeling dependencies between the weights": "where z = (z1, ..., znin) follows a distribution q(z). For an illustration of the difference between the two vari-ational distributions in Equation (4) and Equation (5), see . The novelty in our suggested variationaldistribution is to combine both weight and structural uncertainty, in addition to modeling dependencies be-tween the weights. As for W, also z is a set of variables related to a specific layer and independence betweenlayers is assumed also for zs. To increase the flexibility of the variational posterior, we apply normalizingflows (Rezende & Mohamed, 2015) to q(z). In general, a normalizing flow is a composition of invertibletransformations of some initial (simple) random variable z0,",
  ".(6)": "We are typically interested in transformations that have a Jacobian determinant that is tractable, and fastto compute, in addition to being highly flexible. Transforming the variational posterior distribution in aBNN with normalizing flows was first done in Louizos & Welling (2017), who coined the term multiplicativenormalizing flows (BNN-FLOW), where the transformations were applied in the activation space instead ofthe weight space. As the weights are of much higher dimensions, the number of flow parameters and thus thenumber of parameters of variational distribution would explode quickly. We will follow Louizos & Welling(2017) here. The main difference in our work is that by using the variational posterior in Equation (5), wealso get sparse networks. For the normalizing flows, we will use the inverse autoregressive flow (IAF), with numerically stable updates,introduced by Kingma et al. (2016). It works by transforming the input in the following way:",
  "Computing the variational bounds": "Minimization of the KL in Equation (2) is difficult due to the introduction of the auxiliary variable zin the variational distribution.In principle, z could be integrated out, but in practice this is difficult.Following Ranganath et al. (2016), we instead introduce z as an auxiliary variable also in the posteriordistribution by definingp(w, , z|D) = p(w, |D)r(z|w, )",
  "i=1o2i ij(2ij + (1 ij)z2i 2ij)": "It should be noted that affects both the mean and the variance of our Gaussian approximation, whereas inLouizos & Welling (2017) it only influences the mean. Louizos & Welling (2017) also sample one z for eachobservation within the mini-batch. We found that empirically it made no difference in performance to onlysample one vector and multiply the same z with each input vector. We do this, as it is more computationallyefficient.",
  "A simplified version only applying the LCRT method assumes zi = 1 for all i. We will denote this method byLBBNN-LCRT in section 5. The method combining both LCRT and MNF will be denoted LBBNN-FLOW": "In addition to a huge reduction in the number of variables to be sampled, we get a reduction in the varianceof the gradient estimates, as shown in Kingma et al. (2015). Note also that the approximations inducedby the sampling procedure for h can be considered as an alternative variational approximation directly forp(h|D).",
  "Experiments": "In this section, we demonstrate the robustness of our approach and show improvements with respect to theclosest baseline methods of Hubin & Storvik (2024), (denoted LBBNN-SSP-MF in their paper), denotedLBBNN here, with the two approaches proposed in this paper. We consider both Bayesian model averag-ing (Hoeting et al., 1999) over the variational posterior distribution for the inclusion variables and weights,as well as the use of the median probability model (Barbieri et al., 2018), only sampling from weights withposterior probabilities for inclusion variables above 0.5. Median probability models have the potential to givesignificant sparsity gains. We also compare it to other reasonable baseline methods that allow us to evaluatethe direct treatment effects. Our aim is to demonstrate the potential for improvements by the use of theproposed method, focusing on comparison under similar settings. For all methods, further improvementsare possible through extensive tuning of hyper-parameters and different ad-hoc tricks commonly found inthe Bayesian deep learning literature that are utilized to improve on predictive performance (e.g. temperingthe posterior (Wenzel et al., 2020) or clipping the variance of the variational posterior distribution as donein Louizos & Welling (2017)). Using these tricks (although tempting) would not allow us to evaluate thepure contribution of the methodology. For the classification experiments, we provide comparisons to a stan-dard frequentist neural network (ANN) without regularization, corresponding to the maximum likelihoodestimation of the network weights, in addition to one with L2 regularization (ANN + L2), correspondingto the maximum a posteriori estimator (MAP) with independent Gaussian priors from a standard Bayesianneural network (BNN). We also have a standard BNN, taking weight uncertainty into account. From there,we get to the LBBNN method by having an extra inclusion parameter per weight, allowing for a sparseBNN. For LBBNN, exactly the same parameter priors (slab components) as in BNN were used, allowingus to evaluate the effects of adding the structural uncertainty. The multiplicative normalizing flow method(BNN-FLOW) is also closely related to a standard BNN, but here instead of sparsifying the network, weallow the variational posterior distribution to be more flexible than a standard mean-field Gaussian, usedin the BNN. Further, using the local reparametrization trick (LBBNN-LCRT) is mainly a computationaladvantage compared to the LBBNN method. Finally, LBBNN-FLOW (proposed in this paper) is related toboth BNN-FLOW and LBBNN-LCRT, in the sense that it can learn a sparse BNN, and in addition, havea more flexible posterior distribution than the mean-field Gaussian used in LBBNN-LCRT. For a graphicalillustration of how these methods are related, we refer to .",
  "mean TPR0.6810.8380.972mean FPR0.1250.0840.074": "In addition to the classification experiments in the main text, we also provide experiments (both classificationand regression) on various tabular datasets in Appendix D. Here, in addition to the methods detailed above,we also include Monte Carlo (MC) dropout (Gal & Ghahramani, 2016), and (exact) Gaussian processes(more details about how this is implemented in Appendix D) for the regression datasets. Finally, for thetabular datasets, we check the internal parsimony of all of the Bayesian baselines through the pWAIC1 andpWAIC2 penalties from Gelman et al. (2014), metrics that also have been used as estimates of the effectivenumber of parameters. Additionally, we perform two simulation studies. In the first one, we consider variable selection in a logisticregression setting, with highly correlated explanatory variables.We further vary correlation levels andnoise levels of linear predictor. In additional experiments reported in Appendix B, we address 3 differentcorrelation structures and relations to the response, where we also change the level of correlations betweenthe covariates and repeat that for several noise levels.",
  "Logistic regression simulation study": "In this section, we do a variable selection experiment within a logistic regression setting. As logistic regressionis just a special case of a neural network with one neuron (and hence one layer), modifying the algorithmsis straightforward. We are limiting ourselves to the logistic regression context to be able to compare tothe original baseline method from Carbonetto & Stephens (2012), who demonstrated that the mean-fieldvariational approximation starts to fail the variable selection task when the covariates are correlated. Aswe are only interested in comparing the mean-field variational approach against the variational distributionwith normalizing flows, we do not include comparisons with more traditional variable selection methods suchas Lasso (Tibshirani, 1996) or Elastic Net (Zou & Hastie, 2005). We use the same data as in Hubin & Storvik (2018), consisting of a mix of 20 binary and continuousvariables, with a binary outcome, and 2 000 observations. The covariates, x, are generated with a strong andcomplicated correlation structure between many of the variables (see ). For more details on exactlyhow the covariates are generated, see appendix B of Hubin & Storvik (2018). The response variable, y, isgenerated according to the following data-generating process:",
  "T = (4, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1.2, 0, 37.1, 0, 0, 50, 0.00005, 10, 3, 0)": "The goal is to train the different methods to select the non-zero elements of . We consider the parameterj to be included if the corresponding posterior inclusion probability j > 0.5, i.e. the median probabilitymodel of Barbieri & Berger (2004). We fit the different methods 100 times (to the same data), each timecomputing the true positive rate (TPR), and the false positive rate (FPR).",
  "Extension to different levels of correlation": "To extend the experiment to other correlation levels, we use the same data as in the simulation study from Hu-bin & Storvik (2018), but with varying noise levels, using {1, 0.1, 0.01} that we use to generate and with10 different correlation structures for xi,j per signal level. For a correlation parameter {1, 0.9, . . . , 0.1},the covariates were updated by the following rule: xi,j = xi,j4 + (1 ) xi,j, i = 1, ..., 2000, j = 5, ..., 20.Then, the resulting predictors are standardized resulting in the final correlated covariates used to generatethe responses just like described in equation 12 above. Finally, CS, LBBNN-LCRT, and LBBNN-FLOW wererun 20 times per setting using exactly the same hyper and tuning parameters as in the previous simulationstudy.",
  ": On the left, true positive rates, with false positive rates to the right. The maximum correlationranged between 0.954 and 0.997": "For this experiment, in , we show plots of average true positive and false positive rates, wherethe dashed lines indicate a 90% empirical confidence interval obtained from 20 runs of the correspondingalgorithms per settings on different seeds. Along the x-axis, we show (every other one to save space) themean absolute correlation between all covariates in the corresponding setting. As mentioned previously, forthis experiment we consider 30 different settings, divided into 3 parts where we simulated three differentnoise levels in the linear predictor, and for each of these, we have 10 different correlation levels. The resultsin are consistent with those obtained on the original data from Hubin & Storvik (2018). Acrossall settings here, the LBBNN-FLOW is on average the most favorable method with respect to a trade-offbetween TPR and FPR, although in many cases the confidence intervals are overlapping. We added three more studies with other complicated correlation structures, varying signal strength (3 dif-ferent levels in every study), and correlation levels (10 levels per signal level) between the covariates. Theseadditional simulation studies are provided in the Appendix B to this paper.",
  "Experimental setup": "We perform two classification experiments, one with the same fully connected architecture as in Hubin &Storvik (2019), and the other with a convolutional architecture (see appendix C for details on how this isimplemented). On the fully connected architecture, we use two hidden layers with 400 and 600 neuronsrespectively, whereas, for the other experiment, we use the LeNet-5 (LeCun et al., 1998) convolutionalarchitecture, but with 32 and 48 filters for the convolutional layers. We emphasize that it is possible touse deeper and more complicated architectures (for example Resnet-18, He et al., 2016), which would likelyimprove the results reported in this paper. As the goal here is not to try to approach (or hack throughtuning and engineering) state-of-the-art results, we do not experiment any further with this. In both cases,we classify on MNIST (Deng, 2012), FMNIST (Fashion MNIST) (Xiao et al., 2017) and KMNIST (KuzushijiMNIST) (Clanuwat et al., 2018). All of these datasets contain 28x28 grayscale images, divided into a trainingand validation set with 60 000 and 10 000 images respectively. MNIST and FMNIST are well-known andoften utilized datasets, so it is easy to compare performance when testing novel algorithms, while KMNISTis a more recent addition. For both the fully connected and convolutional architectures, we use the Adam (Kingma & Ba, 2014)optimizer, batch size of 100, and train for 250 epochs. All the experiments are run 10 times, and we report theminimum, median, and maximum predictive accuracy over these 10 runs as well as expected calibration errorsand negative test log-likelihood. We measure predictive performance and predictive uncertainty handlingwithin two approaches: First, the Bayesian model averaging approach, where we average over 100 samplesfrom the variational posterior distribution, where for our proposed method we are taking into accountuncertainty in both weights and structures following Hubin & Storvik (2019). For the standard BNN andBNN-FLOW, this model averaging is only over the weights, whereas for the frequentist approaches, we donot have any model averaging.Secondly, for the methods with both weight and structural uncertainty,we consider the median probability model (Barbieri & Berger, 2004), where we only do averaging over theweights that have a posterior inclusion probability greater than 0.5, whilst others are excluded from themodel. This allows for significant sparsification of the network. We emphasize that this is possible becausewe can go back to sampling the weights when doing inference, i.e. we sample only from the weights thathave a corresponding inclusion probability greater than 0.5. In addition (where applicable), we also report the density of the network, defined as the proportion of non-zero weights. The reported density (1-sparsity) is an average over these 10 runs. For the full variationalmodel averaging approach, we consider the density to be equal to one since we do not explicitly exclude anyweights when computing the predictions (even though a large proportion of the weights may have a smallinclusion probability and in practice within any 10 samples over which we are marginalizing less than 100%of the weights will be used, yet ideally one wants to average over more than 10 samples).",
  "Methods": "For these experiments, we are interested in comparing our method LBBNN-FLOW against the closest baselinemethods (see ). For ANN + L2, we use weight decay of 0.5, inducing a penalized likelihood, whichcorresponds to MAP (maximum aposteriori probability) solutions of BNN and BNN-FLOW under standardnormal priors. For BNN and BNN-FLOW, we use standard normal priors for the weights. For the LBBNN-LCRT and LBBNN-FLOW methods, we also use the standard normal prior for the slab components of allthe weights and biases in the network, and a prior inclusion probability of 0.10. For both q(z) and r(z|W, ),we use flows of length two, where the neural networks consist of two hidden layers with 250 neurons each.For the Bayesian model averaging approaches, we average over 100 samples from the variational posterior",
  "Results": "The results with the fully connected architecture can be found in Tables 2, 3, and 4 and for the convolutionalarchitecture in Tables 5, 6, and 7.Firstly, we see that using the LBBNN-LCRT gives results that arecomparable to the baseline LBBNN method, except for FMNIST where it performs a bit worse both withthe fully connected and with the convolutional architecture. It is no surprise that these results are similar,as using the LCRT is mainly a computational advantage.Secondly, we note that our LBBNN-FLOWmethod performs better than the two aforementioned methods in terms of prediction accuracy, on bothconvolutional and fully connected architectures, while having the most sparse networks. We also see thatin terms of accuracy LBBNN-FLOW performs mostly on par with the other two Bayesian architectures,BNN and BNN-FLOW, especially on the fully connected architecture where it gets comparable accuracyeven with very sparse networks. In terms of predictive uncertainty handling, in almost all settings the bestperformance (across all addressed in our design methods) is achieved by one of the LBBNN methods and in",
  "FMNISTMedian probability modelDense model": "MethodminmedianmaxminmedianmaxLBBNN0.0220.0250.0260.0280.0300.034LBBNN-LCRT0.0130.0180.0210.0180.0200.021LBBNN-FLOW0.0220.0260.0300.0200.0220.028BNN-FLOW---0.0230.0270.031BNN---0.0210.0220.025ANN---0.0590.0640.068ANN + L2---0.0200.0230.029 In our experimental design, we check the effect of direct treatments from , especially we were inter-ested in edges from LBBNN to LBBNN-LCRT and to LBBNN-FLOW and from BNN-FLOW to LBNNN-FLOW as the closest neighbors to the proposed in this paper methodology. None of the results is completelyuniform across all cases, however we demonstrated that as compared to BNN-FLOW, our proposed approachtypically improves on sparsity without losing predictive performance and yields robustly well-calibrated un-certainty handling.Further, as compared to LBBNN, we improve the predictions and sparsity slightly.Improvements are observed in most addressed variable selection tasks when using the LBBNN-FLOW ap-proach as compared to LBBNN-LRCT, while in all variable selection tasks, LBBNN-FLOW with respectto CS approach and in many cases outperforms it, which is especially the case for block diagonal correla-tion structures in Sections 5.1 and experiment 3 in Appendix B. Methods beyond including othersparsity inducing Bayesian approaches to BNN (Molchanov et al., 2017; Deng et al., 2019; Li et al., 2024),that are less related to our approach and could not be incorporated with direct edges in , wereomitted in this paper. Yet in the future, following the guidance from Herrmann et al. (2024), it would beinteresting to perform a neutral comparison study (Boulesteix et al., 2013). This study should not be doneby the authors of any of these papers to avoid biases and tuning ones own methods to beat (in some sense)all competitors and hack the state-of-the-art, which recently became a common practice that often resultsin poor empirical evidence (Herrmann et al., 2024). This should also be ideally done on new datasets not",
  "Discussion": "We have demonstrated that increasing the flexibility in the variational posterior distribution with normalizingflows improves the predictive power compared to the baseline method (with mean-field posterior) whileobtaining more sparse networks, despite having a looser variational bound than the mean-field approach.Also, the flow method performed best on a variable selection problem demonstrating better structure learningperformance, while the mean-field approaches struggle with highly correlated variables. Also, the calibrationof uncertainties in predictive applications is similar with a slight advantage of the proposed approach inthis paper.Unlike dense BNNs, our methods have the additional advantage of being able to performvariable selection. The downside is that LBBNNs have an extra parameter per weight, making them lesscomputationally efficient than dense BNNs. Using normalizing flows is a further computational burden as wemust also optimize over all the extra flow parameters. If uncertainty handling is not desirable, one could gainthe minimal number of predictive parameters using the model trained with flows by relying on the posteriormeans of the median probability models parameters. This approach is studied for simpler approximationsin more detail in Hubin & Storvik (2024) but it is omitted in this paper. Also, in the future, it would beof interest to develop reversible jump MCMC (Green & Hastie, 2009) or other relevant MCMC methods forLBBNN and compare the variational approximations with the exact sample at least in the low-dimensionalcases where the latter could be feasible.",
  "Karl W Broman, Hao Wu, aunak Sen, and Gary A Churchill. R/qtl: Qtl mapping in experimental crosses.bioinformatics, 19(7):889890, 2003": "Steven L Brunton, Joshua L Proctor, and J Nathan Kutz. Discovering governing equations from data bysparse identification of nonlinear dynamical systems. Proceedings of the national academy of sciences, 113(15):39323937, 2016. Peter Carbonetto and Matthew Stephens. Scalable variational inference for Bayesian variable selection inregression, and its accuracy in genetic association studies. Bayesian analysis, 7(1):73108, 2012.",
  "Ilker Ali Ozkan, Murat Koklu, and Rdvan Saraolu. Classification of pistachio species using improved k-nnclassifier. Health, 23:e2021044, 2021": "AdamPaszke,SamGross,FranciscoMassa,AdamLerer,JamesBradbury,GregoryChanan,Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf,Edward Yang,Zachary DeVito,Martin Raison,Alykhan Tejani,Sasank Chilamkurthy,BenoitSteiner,Lu Fang,Junjie Bai,and Soumith Chintala.Pytorch:An imperative style,high-performancedeeplearninglibrary.InAdvancesinNeuralInformationProcessingSystems32,pp.80248035.CurranAssociates,Inc.,2019.URL Ankit Pensia, Shashank Rajput, Alliot Nagle, Harit Vishwakarma, and Dimitris Papailiopoulos. Optimallottery tickets via subset sum: Logarithmic over-parameterization is sufficient. Advances in neural infor-mation processing systems, 33:25992610, 2020.",
  "AComputational and memory efficiency": ": Training Cost (per iteration of first order optimizer) and Memory Cost (for saving the trainedmodels parameters) for Different Methods. Assume (only) for this table the following notation: n - numberof data points in the minibatch; p - number of trainable parameters in the architecture; s - number ofsamples from parameters for gradient estimation; f - additional flow parameters; w - speed-up factor dueto sampling directly from neurons (number of weights per neuron); - ratio of non-pruned weights. Thetraining iteration cost reflects the computational complexity for each method. The memory cost includesconsiderations for additional parameters and storage efficiency, especially for pruned weights. A non-prunedweight is assumed to need a double-precision number to be stored (64 bits). In the pruned model, we furtherassume to need 1 bit per position of whether the weight there is pruned.",
  "MethodTraining CostMemory Cost (bits)Sparse Memory Cost (bits)": "ANNO(n p)64 pNot applicableBNNO(n 2p s)64 2pNot applicableBNN-FLOWO(n (2p + f) s/w)64 (2p + f)Not applicableLBBNNO(n 3p s)64 3p 2p + 65 (1 ) 2pLBBNN-LRCTO(n 3p s/w)64 3p 2p + 65 (1 ) 2pLBBNN-FLOWO(n (3p + f) s/w)64 (2p + f) 2p + 65 ((1 ) 2p) + 64 f)",
  "BAdditional simulation studies": "In this section, we describe three additional simulation studies, conducted to evaluate the performance ofBayesian variable selection using the same approaches as in .1 of the paper. Just like in the extensionfrom .1.1., the studies involved generating simulated datasets and assessing the True Positive Rate(TPR) and False Positive Rate (FPR) of selected by different methods variables across 3 different signal-to-noise ratios and 10 correlation structures per signal level. 20 simulations on different seeds were run persetting per method, resulting in 600 simulations per method in each of these three studies. Bayesian variableselection was performed using the median probability model. We do not tune any methods but rather useexactly the same hyperparameters for each experiment as we did in .1. However, we use differentdata-generative processes in each study. Below, we explain these processes.",
  "B.1.1Experiment 1:": "For the first experiment, we use the Simrel package (Sb et al., 2015), which allows for simulating correlateddata where the level of correlation can be controlled through the parameter, which is the declining (decay-ing) factor of eigenvalues of predictors. The parameter was ranging between 0.1 and 1. For this experiment,we have 50 predictor variables, and sample 1000 data points, with the true effect vector is defined as:",
  "B.1.2Experiment 2:": "Here, we simulate i.i.d. xi,j N(0, 1), then induce the following correlation structure: For a correlationparameter {1, 0.9, . . . , 0.1}, update the covariates as: xi,j = xi,j10 + (1 ) xi,j, i = 1, ..., 1000, j =11, ..., 50. The resulting predictors are then standardized, followed by binarizing all even indexed covariates",
  "B.1.3Experiment 3:": "Finally, we take inspiration from the Rejoinder to Hubin et al. (2020) and simulate Quantitative trait locus(QTL) mapping using R/QTL package (Broman et al., 2003). Here, just as in .1, we have a block-diagonal correlation structure between the covariates. Further, in this experiment, we randomly select eachi to be a true parameter with probability of 0.25. The true i are then sampled as i N(1, 1). Here, we",
  ": On the left, true positive rates, with false positive rates to the right. The maximum absolutepairwise correlations between the covariates ranged between 0.909 and 0.999": "are closer on the FPRs, with the LCRT being slightly worse. In the second experiment, the results are quiteclose, with flows having a slightly higher TPR for high levels, and slightly lower with smaller . In the lastexperiment, we see that FLOW generally has the lowest FPR and highest TPR. As indicated in experiment3 and also in Sections 5.1 and 5.1.1, we can see the advantage of using flows when the covariance structurebetween the covariates is block-diagonal.",
  "DExperiments on tabular datasets": "In this experiment, we consider six classification datasets and three regression datasets. We compare ourapproach LBBNN-FLOW against LBBNN-LCRT, LBBNN, a dense BNN, BNN-FLOW, ANN+L2, ANN,and Monte Carlo (MC) dropout (with dropout rates corresponding to our prior inclusion probabilities), inaddition to Gaussian processes for the regression datasets. Here, we use an out-of-the-box version of thepackage from Varvia et al. (2023), using the Matrn 3/2 covariance function (see chapter 4 of Rasmussen(2003) for how this is defined). For the mean function, we use the training data average. For the covariancefunction, the implementation defines three hyperparameters: the kernel variance and length scale, controllingthe smoothness properties of the regression, and the error variance parameter, regulating how closely to fitthe training data. We define these parameters to be 1, 5, and 0.1 respectively. For the neural networks, we use a single hidden layer with 500 neurons and again train for 250 epochs withthe Adam optimizer. We use 10-fold cross-validation and report the minimum, mean and maximum accuracy",
  "ECE (min, mean, max)": "MethodCredit ApprovalBank MarketingCensus IncomeDry BeansPistachioRaisinsLBBNN(0.056, 0.079, 0.127)(0.023, 0.031, 0.039)(0.005, 0.010, 0.015)(0.011, 0.016, 0.021)(0.012, 0.028, 0.047)(0.043, 0.073, 0.097)LBBNN-MPM(0.035, 0.080, 0.122)(0.013, 0.025, 0.033)(0.006, 0.011, 0.016)(0.002, 0.015, 0.031 )(0.014, 0.028, 0.043)(0.035, 0.069, 0.103)LBBNN-LCRT(0.035, 0.071,0.097)(0.001, 0.016, 0.020)(0.006, 0.010, 0.012)(0.008, 0.014, 0.019)(0.014, 0.022, 0.034)(0.034, 0.070, 0.094)LBBNN-LCRT-MPM(0.015, 0.073, 0.119)(0.011, 0.015, 0.018)(0.008, 0.012, 0.016)(0.007, 0.013, 0.025)(0.012, 0.025, 0.035)(0.057, 0.076, 0.115)LBBNN-FLOW(0.030, 0.069, 0.103)(0.002, 0.007, 0.015)(0.007, 0.011, 0.016)(0.010, 0.017, 0.040)(0.011, 0.024, 0.045)(0.018, 0.068, 0.103)LBBNN-FLOW-MPM(0.025, 0.071, 0.126)(0.003, 0.008, 0.012)(0.009, 0.012, 0.015 )(0.008, 0.014, 0.026)(0.012, 0.030, 0.047)(0.038, 0.072, 0.096)BNN-FLOW(0.025, 0.074, 0.123)(0.006, 0.012, 0.022)(0.020, 0.027, 0.047)(0.007, 0.014, 0.029)(0.010, 0.029, 0.055)(0.038, 0.069, 0.104)BNN(0.046, 0.097, 0.195)(0.010, 0.015, 0.024)(0.020, 0.025, 0.031)(0.005, 0.013, 0.030)(0.010, 0.039, 0.057)(0.034, 0.070, 0.103)ANN(0.072, 0.131, 0.188)(0.012, 0.017, 0.023)(0.020, 0.025, 0.031)(0.007, 0.013, 0.021)(0.027, 0.042, 0.056)(0.040, 0.074, 0.109)ANN + L2(0.020, 0.114, 0.178)(0.013, 0.018, 0.024)(0.015, 0.020, 0.026)(0.007, 0.013, 0.020)(0.014, 0.040, 0.053)(0.025, 0.066, 0.111)ANN + MC dropout(0.028, 0.079, 0.115)(0.011, 0.014, 0.017)(0.008, 0.011, 0.018)(0.033, 0.039, 0.048)(0.016, 0.029, 0.051)(0.027, 0.069, 0.113)",
  "pWAIC1 (min, mean, max)": "MethodCredit ApprovalBank MarketingCensus IncomeDry BeansPistachioRaisinsLBBNN(0.620, 1.323, 2.595)(28.18, 31.15, 34,74)(22.98, 29.33, 37.14)(15.82, 23.18, 30.44)(2.594, 3.277, 4.762)(0.149, 0.271, 0.668)LBBNN-MPM(0.000, 0.000, 0.000)(0.001, 0.002, 0.003)(0.002, 0.002, 0.003)(0.001, 0.002, 0.003)(0.000, 0.000, 0.000)(0.000, 0.000, 0.000)LBBNN-LCRT(0.013, 0.026, 0.041)(0.575, 0.643, 0.725)(0.997, 1.144, 1.357)(9.937, 13.32, 20.12)(0.015, 0.034, 0.052)(0.011, 0.023, 0.043)LBBNN-LCRT-MPM(0.011, 0.025, 0.043)(0.566, 0.645, 0.718)(1.013, 1.140, 1.180)(0.002, 0.002, 0.003)(0.016, 0.032, 0.054)(0.011, 0.024, 0.045)LBBNN-FLOW(0.009, 0.021, 0.033)(0.486, 0.542, 0.608)(1.045, 1.216, 1.488)(12.83, 17.66, 26.65)(0.016, 0.034, 0.058)(0.011, 0.025, 0.052)LBBNN-FLOW-MPM(0.009, 0.022, 0.037)(0.476, 0.544, 0.621)(1.065, 1.245, 1.501)(0.331, 0.420, 0.567)(0.013, 0.033, 0.055)(0.011, 0.026, 0.055)BNN-FLOW(0.012, 0.029, 0.054)(0.523, 0.593, 0.655)(1.235, 1.398, 1.745)(0.402, 0.599, 1.089)(0.017, 0.039, 0.062)(0.012, 0.023, 0.045)BNN(0.015, 0.055, 0.255)(0.580, 0.686, 0.760)(1.165, 1.352, 1.442)(0.005, 0.007, 0.009)(0.019, 0.048, 0.073)(0.011, 0.022, 0.034)MC dropout(0.016, 0.058, 0.249)(0.536, 0.600, 0.687)(1.057, 1.201, 1.353)(75.99, 105.3, 133.2)(0.027, 0.040, 0.059)(0.011, 0.027, 0.042)",
  "pWAIC2 (min, mean, max)": "MethodCredit ApprovalBank MarketingCensus IncomeDry BeansPistachioRaisinsLBBNN(0.649, 1.367, 2.711)(28.18, 31.73, 35,74)(23.20, 65.55, 396.7)(16.69, 24.55, 32.75)(2.724, 3.485, 5.058)(0.150, 0.269, 0.647)LBBNN-MPM(0.000, 0.000, 0.000)(0.001, 0.002, 0.003)(0.002, 0.002, 0.003)(0.001 0.002, 0.003)(0.000, 0.000, 0.000)(0.000, 0.000, 0.000)LBBNN-LCRT(0.021, 0.058, 0.111)(1.016, 1.176, 1.437)(1.588, 2.981, 12.04)(10.38, 13.86, 20.86)(0.023, 0.078, 0.138)(0.015, 0.044, 0.101)LBBNN-LCRT-MPM(0.017, 0.056, 0.114)(0.985, 1.188, 1.374)(1.1639, 2.041, 2.247)(0.002, 0.002, 0.003)(0.026, 0.073, 0.151)(0.016, 0.047, 0.112)LBBNN-FLOW(0.014, 0.043, 0.072)(0.780, 0.926, 1.154)(1.718, 7.906, 21.91)(13.30, 18.88, 29.37)(0.025, 0.071, 0.137)(0.016, 0.058, 0.197)LBBNN-FLOW-MPM(0.014, 0.047, 0.083)(0.755, 0.934, 1.193)(1.781, 8.940, 21.97)(0.331, 0.421, 0.569)(0.018, 0.070, 0.126)(0.016, 0.059, 0.188)BNN-FLOW(0.020, 0.074, 0.199)(0.856, 1.049, 1.303)(2.230, 4.745, 12.83)(0.403, 0.601, 1.093)(0.030, 0.097, 0.178)(0.016, 0.046, 0.123)BNN(0.024, 1.087, 10.14)(1.036, 1.307, 1.517)(2.052, 3.591, 12.34)(0.005, 0.007, 0.009)(0.034, 0.137, 0.244)(0.015, 0.039, 0.064)MC dropout(0.027, 1.112, 10.13)(0.905, 1.059, 1.351)(1.767, 4.148, 12.03)(101.0, 148.4, 265.8)(0.054, 0.102, 0.216)(0.015, 0.064, 0.181)",
  "Mean pinball (min, mean, max)": "MethodAbaloneWine QualityBoston HousingLBBNN(0.210, 0.236, 0.260)(0.291, 0.310, 0.325)(0.100, 0.133, 0.164)LBBNN-MPM(0.220, 0.242, 0.260)(0.296, 0.313, 0.328)(0.097, 0.137, 0.167)LBBNN-LCRT(0.203, 0.227, 0.251)(0.289, 0.304, 0.316)(0.087, 0.121, 0.150)LBBNN-LCRT-MPM(0.204, 0.228, 0.259)(0.290, 0.304, 0.318)(0.089, 0.123, 0.151)LBBNN-FLOW(0.208, 0.232, 0.253)(0.289 0.306 0.321)(0.091, 0.132, 0.158)LBBNN-FLOW-MPM(0.209, 0.231, 0.261)(0.290, 0.306 0.319)(0.092, 0.133, 0.154)BNN-FLOW(0.211, 0.232, 0.250)(0.293, 0.303, 0.317)(0.085, 0.120, 0.142)BNN(0.210, 0.230, 0.255)(0.286, 0.295, 0.307)(0.084, 0.117, 0.145)ANN(0.210, 0.231, 0.255)(0.279, 0.292, 0.301)(0.087, 0.119, 0.149)ANN + L2(0.208, 0.230, 0.255)(0.283, 0.293, 0.301)(0.083, 0.114, 0.142)ANN + MC dropout(0.211, 0.231, 0.252)(0.295, 0.312, 0.328)(0.089, 0.125, 0.156)Gaussian process(0.210, 0.231, 0.252)(0.267, 0.283, 0.295)(0.079, 0.117, 0.145)",
  "pWAIC1 (min, mean, max)pWAIC2 (min, mean, max)": "MethodAbaloneWine QualityBoston HousingAbaloneWine QualityBoston HousingLBBNN(1.937, 4.486, 14.09)(6.304, 7.496, 8.617)(0.037, 0.202, 0.890)(1.987, 5.206, 20.01)(6.660, 7.832, 9.642)(0.038, 0.210, 0.933)LBBNN-MPM(0.000, 0.001, 0.002)(0.001, 0.001, 0.003)(0.000, 0.000, 0.000)(0.000, 0.001, 0.002)(0.001, 0.001, 0.003)(0.000, 0.000, 0.000)LBBNN-LCRT(0.766, 1.469, 2.228)(4.978, 5.620, 6.318)(0.013, 0.128, 0.483)(0.774, 1.528, 2.595)(5.112, 5.888, 6.989)(0.013, 0.133, 0.514)LBBNN-LCRT-MPM(0.001, 0.001, 0.003)(0.003, 0.004, 0.006)(0.000, 0.000, 0.000)(0.001, 0.001, 0.003)(0.003, 0.004, 0.006)(0.000, 0.000, 0.000)LBBNN-FLOW(0.563, 1.923, 7.515)(3.220, 4.202, 6.069)(0.025, 0.223, 1.025)(0.568, 2.191, 9.925)(3.273, 4.407, 7.352)(0.025, 0.238, 1.157)LBBNN-FLOW-MPM(0.039, 0.147, 0.430)(0.171, 0.226, 0.268)(0.001, 0.012, 0.077)(0.039, 0.147, 0.432)(0.172, 0.226, 0.269)(0.001, 0.012, 0.078)BNN-FLOW(0.074, 0.237, 0.468)(0.362, 0.609, 2.210)(0.003, 0.021, 0.062)(0.075, 0.238, 0.473)(0.363, 0.632, 2.415)(0.003, 0.021, 0.062)BNN(0.002, 0.007, 0.045)(0.014, 0.019, 0.039)(0.000, 0.000, 0.000)(0.002, 0.007, 0.045)(0.014, 0.019, 0.039)(0.000, 0.000, 0.000)MC dropout(7.295, 15.42, 44.78)(18.70, 22.81, 27.87)(0.344, 1.721, 6.331)(7.972, 56.82, 430.4)(20.35, 29.71, 71.86)(0.397, 2.658, 12.43)"
}