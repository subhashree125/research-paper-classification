{
  "Abstract": "Tabular data is arguably one of the most ubiquitous data structures in application domainssuch as science, healthcare, finance and manufacturing. Given the recent success of deeplearning (DL), there has been a surge of new DL models for tabular learning. However,despite the efforts, tabular DL models still clearly trail behind tree-based approaches. In thiswork, we propose DisTab, a novel framework for tabular learning based on the transformerarchitecture.Our method leverages model distillation to mimic the favorable inductivebiases of tree-based models, and incorporates language guidance for more expressive featureembeddings. Empirically, DisTab outperforms existing tabular DL models and is highlycompetitive against tree-based models across diverse datasets, effectively closing the gapwith these methods.",
  "Introduction": "Deep learning (DL) has achieved remarkable progress in learning from visual (He et al., 2016; Dosovitskiyet al., 2020), textual (Vaswani et al., 2017; Brown et al., 2020) or audio data (Arik et al., 2018; Qin et al.,2023), emerging as the preferred approach for various tasks such as image classification Deng et al. (2009)and language translation Poibeau (2017). Inspired by these successes, there is a surge of interest in extendingDL capabilities to tabular learning. Tabular data stands out as one of the most ubiquitous data structuresfor diverse domains, from patient records in healthcare to experimental results in scientific research. Theability to effectively glean insights from tabular data holds immense significance with many applications. Among the existing works on tabular DL, a major research direction adopts model pre-training by initiallybootstrapping DL models through pre-text (or pre-training) tasks before training (or fine-tuning) them on theactual labeled data of interest. Pre-text tasks tailored for tabular learning includes contrastive learning, inputreconstruction and/or data synthesis via random perturbations of the labeled data (Ucar et al., 2021; Bahriet al., 2022; Majmundar et al., 2022). Orthogonally, Wang & Sun (2022); Ye et al. (2024); Yan et al. (2024)proposed cross-table pre-training approaches to learn features that can generalize across different tabulardatasets. Empirical evidence supports the efficacy of pre-training, demonstrating superior generalizationperformance compared to direct training on the labeled data from scratch. On the other hand, following the successes of transformer architectures (Vaswani et al., 2017) in challengingdomains including natural language processing (NLP) (Ouyang et al., 2022), computer vision (CV) (Doso-vitskiy et al., 2020), and reinforcement learning (RL) (Chen et al., 2021), recent works have investigatedtheir impact on tabular applications (Gorishniy et al., 2021; Somepalli et al., 2021; Wang & Sun, 2022).The core idea behind these tabular transformers is to represent table columns (or features) as a sequence",
  "Published in Transactions on Machine Learning Research (11/2024)": "Talip Ucar, Ehsan Hajiramezanali, and Lindsay Edwards. Subtab: Subsetting features of tabular data forself-supervised representation learning. Advances in Neural Information Processing Systems, 34:1885318865, 2021. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ukasz Kaiser,and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30,2017.",
  "Related Works": "Tabular Pre-training. Inspired by the success of pre-training in CV and NLP, recent studies have exploredapplying these strategies to tabular data settings, where training data scarcity is a significant concern. (e.g.,Bahri et al., 2022; Yoon et al., 2020; Majmundar et al., 2022; Rubachev et al., 2022; Zhu et al., 2023). Amongthese approaches, Ucar et al. (2021) introduced an auto-encoder model equipped with an objective functionto reconstruct randomly masked columns of a table. Bahri et al. (2022) adapted contrastive learning asthe pre-training objective for tabular tasks, extending the SimCLR framework (Chen et al., 2020) originallydesigned for visual representation learning.Furthermore, Rubachev et al. (2022); Wang & Sun (2022)integrated \"target-aware\" pre-training objectives by incorporating target labels, resulting in performanceenhancements. The majority of existing pre-training approaches are domain-specific: the labeled training data also serveas pre-training data (Bahri et al., 2022; Ucar et al., 2021), or they are closely related Wang & Sun (2022).In contrast, Zhu et al. (2023) showcased the viability of pre-training on a large collection of tables span-ning diverse domains through multi-task learning Sener & Koltun (2018). In this paradigm, each tabularlearning task possesses its independent feature embeddings and objective functions, while sharing a tabulartransformer model trained to generalize across different tabular datasets. Our proposed DisTab also adopts domain-specific pre-training but opts for knowledge distillation (Hintonet al., 2015) as the pre-training objective. Like existing pre-training methodologies, DisTab leverages syn-thetic training samples generated during pre-training to mitigate data scarcity. However, it possesses theadded potential to learn the inductive biases of tree-based models favorable for tabular tasks, as hypothesizedin Grinsztajn et al. (2022). Tabular transformers. Transformer models (Vaswani et al., 2017) have recently gained significant pop-ularity in tabular learning scenarios. For instance, Gorishniy et al. (2021) introduced FT-Transformers,demonstrating superior performance in tabular classification/regression tasks compared to classical DL ar-chitectures like MLPs and ResNets. Additionally, Somepalli et al. (2021) proposed column-wise attentionto capture inter-sample interactions, while Fastformer utilizes additive attention on tabular tasks, offering alightweight attention mechanism with linear complexity relative to the length of input sequences (Wu et al.,2021). A crucial aspect in designing tabular transformers is how to embed tabular features into token sequences,aligning with the input format required by transformer models. Most existing approaches (Zhu et al., 2023;Somepalli et al., 2021; Gorishniy et al., 2021) use a single token to represent each column and learn linearmappings from raw tabular features to token embeddings. Conversely, Wang & Sun (2022) map each columnas a variable number of tokens, with column headers and text for categorical features represented by multipletokens based on their word counts. Moreover, (Gorishniy et al., 2022) investigated piece-wise linear encodingand periodic encoding for numerical features, demonstrating improved generalization over linear mappings.Our DisTab adopts a transformer architecture, however it differs from the existing approaches by introducinglanguage-guided embeddings, which encode textual information for columns as context tokens to augmentpreviously proposed feature embeddings (e.g., learned linear mappings). GBDTs for Tabular Learning. Despite the advancements in tabular DL methods, recent large-scalebenchmarks have demonstrated that gradient boosting decision tree (GBDT) models remain the state-of-the-art for tabular learning (Grinsztajn et al., 2022; Chen et al., 2023; Zhu et al., 2023). Commonly usedGBDT models include XGBoost (Chen & Guestrin, 2016), CatBoost (Prokhorenkova et al., 2018), andLightGBM (Ke et al., 2017). They also offer several advantages, such as interpretability, the capabilityto handle heterogeneous features including null values, and robustness without hyper parameter tuning.However, a critical challenge of these models is to properly integrate large-scale vectorial data (such astext or image representation from a vision or language model respectively). For instance, while Ye et al.(2024); Wang & Sun (2022); Yan et al. (2024) have demonstrated the effectiveness of incorporating textual",
  "Method": "In this section, we detail the key components for DisTab, including the proposed feature embeddings withlanguage guidance in Sec. 3.1, the pre-training process in Sec. 3.2 and the overall algorithm in Sec. 3.3. : Left: Language-guided categorical (Top) and numerical (Bottom) embeddings for DisTab.Right: DisTab architecture. A categorical or numerical embedding is extracted from individual columnentry, processed by a transformer and an MLP before returning a prediction. Notation. For a given supervised learning problem on tabular data, we denote the dataset as D = {xi, yi}Ni=1where xi X is a row in the table and yi Y the corresponding label. Let Xj represent the j-th columnof D, hj the text header (if available) for the j-th column, and xij the j-th column of xi. Lastly, we denotefenc : T Rd as the text embedding function (e.g., BehnamGhader et al., 2024) that maps a text stringt T to a continuous embedding fenc(t) Rd.",
  "Input Embeddings for Tabular Features": "Similar to previous works (Gorishniy et al., 2021; 2022; Zhu et al., 2023; Somepalli et al., 2021). we chooseto represent each column with a single token. We observe that this design, when coupled with a suitabletransformer architecture (Sec. 3.3), satisfies output invariance for all permutations of a input sequence. Weargue that this is a potentially desirable inductive bias for tabular data, since permutations of a tablescolumns should not affect the underlying learning task. Language-Guided Embeddings. Most existing approaches for tabular learning overlook valuable textualinformation embedded within tabular datasets, such as column headers or text descriptions associated withcategorical values. These textual elements provide rich semantic context, which could lead to better inputrepresentation and in turn improved performance. Tabular data is a human construct and textual columnheaders are often necessary even for human interpretation. It is thus reasonable to assume the presence of",
  "Ej(xij) = mj(xij) + fenc(hj xij)(1)": "where mj : T Rd denotes the learnable embedding function that embeds xij via look-up, commonlyadopted in tabular transformer models (Zhu et al., 2023; Wu et al., 2024; Gorishniy et al., 2021). The termfenc(hjxij) provides semantic context for the feature by concatenating the column header hj and the textualdescription for xj as a text string to be embedded by fenc. We combine the two embeddings additively toderive the final embedding for xij. The embedding process is depicted in : the lookup embeddings (topstream, denoted in green) is linearly combined with the language-based embedding fenc (bottom stream). Wedefine fenc = fproj flm, where flm : Rnd Rnd outputs the sequence representation flm(t) = v Rnd from a language model. Then, fproj = MLP(AvgPool(v)) summarizes the sequence representation into asingle vector, followed by a small learnable projection network.Please see App. B.2 for further modeldetails.",
  "p(x) = sin(v) cos(v) Rkwherev = [c1x, . . . , ck/2x],withci N(0, 2)(2)": "where sin(v) and cos(v) apply the corresponding function entry-wise to v. Notably, Rahimi & Recht (2008;2007) showed that p(x) is a feature map that approximates the Gaussian RBF kernel, namely p(x)p(y) exp( 2(xy)2 2) (with the approximation improving as the latent dimension k in (2) increases). p(x) is alsoknown as random features in the kernel literature Rahimi & Recht (2007). Evidently, the bandwidth iscrucial to the quality of encoding, as it determines how similar two values x, y should be regarded withina table column.",
  "where = {1, . . . , m} is a set of bandwidths. To account for different length scales for similarity, we use": "p(xij) to concatenate multiple p(xij) with different into a single embedding of dimension d withk =d|| in (2). Our formulation differs from Gorishniy et al. (2022) that uses only a single p() for encoding,but requires expensive hyper-parameter tuning for . In our experiments, we set = {0.1, 1, 10}. The embedding process modeled in (3) is depicted in : the random-feature embedding (top stream,depicted in red) is linearly combined with the language model embedding fenc of the header (bottom stream),scaled proportionally to the numerical feature that is being embedded. This latter quantity provides semanticcontext to encode the associated numerical value suitably. Relation to Previous Works.Our embedding functions generalize previous works by incorporatinglanguage guidance. Specifically, we recover the embedding functions in Gorishniy et al. (2021; 2022); Zhuet al. (2023); Somepalli et al. (2021) if we remove fenc() from (1) and (3). Another key difference is thatour embedding functions combine different representations of a given feature additively to derive a unifiedembedding, which we find to work well in practice.",
  ": The distillation-based pre-training pipeline for DisTab": "improved performance. Most directly, pre-training synthesizes new training data from real one, significantlyincreasing the size of the training data and in turn improves generalization performance. This is particularlycrucial for small tabular datasets, for which DL models tend to overfit. On the other hand, pre-training provides auxiliary learning signals to improve generalization performance.As discussed in Sec. 2, commonly used self-supervised objectives include reconstruction loss and contrastiveloss using random perturbation of real data. For reconstruction learning, we observe that some table columnscannot be learned from others. For instance, tables involving transaction data (e.g., house sales) often includesales date as a column, which cannot be reliably predicted from other columns describing product features(e.g., house size or room count). This may limit the effectiveness of reconstruction learning. For contrastivelearning, heuristics are required to generate similar views of the same data, which may not align with thedesired data similarity with respect to the actual learning task. For instance, random perturbation of acritical column (e.g., lab results in medical diagnosis) could drastically change the target label, but is viewedas a similar view since only a single column is perturbed. Distillation for Pre-training. We therefore propose to utilize knowledge distillation of tree-based modelsfor pre-training. Knowledge distillation aims to directly mimic the superior performance of tree-based modelsin tabular settings, without needing to heuristically constructing a pre-text task with potential limitationsdiscussed above. Formally, given a tabular dataset D = (xi, yi)Ni=1 and access to a teacher model gT , wedenote the teacher-labeled dataset as gT (D) (xi, gT (xi))Ni=1. We also modifies the mix-up technique (Zhanget al., 2017) for data augmentation Aug(D),",
  "where is the suitable loss function for the tabular task, such as least-square errors for the regression setting": "By combining model distillation and mix-up augmentation, we are able to sidestep heuristics needed todefine target labels for synthetic samples. For instance, the original mix-up approach determines labels forsynthetic samples using yi, yj and m. Instead, our approach labels all real and synthetic samples with theteacher model and pre-train on this dataset to emulate the teachers behavior. The pre-training pipeline described above is summarized in : first, the teacher is trained on theground-truth data. In all experiments we choose CatBoost as the teacher model, which has been observed to",
  "Model Architecture. Our model f = fout ftrans consists of a transformer model ftrans : Rnd Rnd": "that processes table rows and fout : Rnd Y that maps the transformer output to target labels. Weuse a simplified Llama transformer architecture (Touvron et al., 2023) for ftrans, which removes all positionencoding. fout(x) = MLP(AvgPool(x)) apply average pooling to the transformer output to obtain samplerepresentations, followed by a linear layer for final output. We note that f is invariant to all permutationsof a given input, a desirable inductive bias for tabular data as discussed earlier. For model training, we first transform the training data using the embedding functions (Sec. 3.1). followedby distillation pre-training (Sec. 3.2). Since pre-training only uses teacher labels, we further fine-tune thepre-trained model using on only the real data with original labels.",
  "Experiments": "In this section, we evaluate our proposed method against different tabular learning methods on test perfor-mance, over a diverse set of benchmark datasets. We first describe the experimental settings below, followedby the detailed results and discussion1. Datasets. We use 25 datasets from OpenML for all evaluations (see Appendix A for details). We follow thedatasets used in Zhu et al. (2023), but focus on those with meaningful textual column headers, since theyallow us to apply and evaluate the proposed language-guided embeddings. For each OpenML dataset, weuse the default train/test splits defined by the OpenML library to ensure better reproducibility (10% datais reserved for testing for each split). For each training split, we randomly partition 90% of data for trainingand the rest for validation. All methods are trained and evaluated using the same splits. Data Preprocessing.For DL approaches designed with MLP architectures, we follow the previousworks (Bahri et al., 2022; Yoon et al., 2020) to represent categorical features by one-hot encoding. Fortransformer-based architectures, categorical features are represented using a ordinal encoding. For all DL-based approaches, numerical features are scaled by z-score. For tree-based approaches, we adopt the defaultprepossessing associated with each method in AutoGluon (Erickson et al., 2020), a tabular learning librarythat provides strong performance for tree-based methods. Model Training. For DisTab, we use a batch size of 1024 for pre-training and 128 during fine-tuning.For the existing DL tabular methods, we use the batch size 128 for both pre-training and fine-tuning, asrecommended in Bahri et al. (2022); Zhu et al. (2023). All DL-based methods use Adam optimizer with alearning rate of 1e-4, with a weight decay of 1e-5, following Gorishniy et al. (2021); Rubachev et al. (2022).Number of pre-training and fine-tuning epochs are empirically determined for each method, but remainconsistent across different tasks. For DisTab, we use 30 epochs for pre-training and 20 for fine-tuning. Evaluation methods. We divide the datasets into regression, binary classification and multi-class clas-sification tasks. For model performance, we use root mean least square (RMS) for regression tasks, areaunder the receiver operating characteristic curve (AUC) for binary classification, and accuracy for multi-classclassification. For each task, every model is trained and evaluated using the same 5 splits, and we use theaverage performance over the 5 splits as a models task performance.",
  "Given the diversity of tabular datasets evaluated, we use the following metrics to effectively compare differentmethods over all datasets:": "Win matrix. Following Bahri et al. (2022), we report our findings in the form of a M M matrix W wherethe (i, j)-th entry Wi,j denotes the ratio of datasets for which method i outperformed method j. We presentthis information in fractional form to include the total number of tasks evaluated and as a heat map tohighlight scale. Average ranks. While win matrix effectively conveys pair-wise comparison, it does not directly provide anoverall ranking of all methods. We follow Zhu et al. (2023) to report the more traditional average rankingof all methods across each task category.",
  "Task performance. We also include raw task performances for each method that underlies the win matrixand average ranking. For brevity, these results are deferred to the Appendix C": "Baselines.We consider a wide range of existing tabular methods for comparison, including CatBoost(CAT) (Prokhorenkova et al., 2018), random forests (RF) (Breiman, 2001), LightGBM (GBM) (Ke et al.,2017), and XGBoost (XGB) (Chen & Guestrin, 2016) for tree-based methods.For DL approaches, weinclude FastAI tabular (FastAI) (Howard & Gugger, 2020), FT-Transformer (FTT) (Gorishniy et al., 2021),XTab (Zhu et al., 2023), Saint (Somepalli et al., 2021), VIME (Yoon et al., 2020), SCRAF (Bahri et al., 2022),SwitchTab (Switch) (Wu et al., 2024) and TP-BERTa (TBERT) (Yan et al., 2024). The DL approachesinclude both transformer and MLP architectures, along with different pre-training strategies and modelslearned from scratch.",
  "Comparison with DL Methods": "We compare DisTab against a diverse set of existing DL tabular methods in Tab. 1 and . The evaluatedmethods include both MLP and transformer architectures, various pre-training strategies, as well as baselinemodels trained from scratch (FastAI and FTT). For average rankings in Tab. 1, DisTab clearly outperforms the existing methods, with over 0.5 rank higherthan the 2nd best performing method. Similarly in , our proposed method outperforms the DL baselinesin all settings, winning in over 75% tasks in all pairwise comparison.",
  "Overall6.2 2.65.6 2.25.6 2.23.5 1.94.0 2.15.0 2.26.0 1.86.3 2.82.6 2.3": ": Comparison of tabular prediction performance between DisTab and other tabular DL methods.Average rank and its standard deviation reported for each method. DisTab outperforms the existing methodsfor all task categories and is overall best performing. While existing pre-training strategies perform generally well for classification settings: they beat the trained-from-scratch baseline in majority of tasks in . Their performance on regression tasks are mixed. thisresult indicates that general pre-training strategies adapted primarily from visual learning tasks may not besuitable for tabular learning, as we argued in Sec. 3.2. In contrast, distillation pre-training shows its efficacyby consistently outperforming the trained-from-scratch baseline and other pre-training strategies. Lastly, we note that transformer-based models noticeable outperform MLP-based ones, validating similarobservations in Grinsztajn et al. (2022); Gorishniy et al. (2021).The results empirically supports ourhypothesis that transformers provide good prior for tabular learning with its permutation invariance withrespect to table columns.",
  ": Comparison of tabular prediction performance between DisTab and tree-based methods. Averagerank and its standard deviation reported for each method": "Consistent with previous benchmark results from Zhu et al. (2023); Chen et al. (2023), we re-validatedCatBoost as the overall best performing tree-based method, which we used as the teacher model. For bothregression and multiclass settings, DisTab outperforms the teacher model, suggesting that it is not merelyparroting predictions from the teacher. In particular, CatBoost is in fact the worst performing model formulticlass setting. Despite distilling from a clearly sub-optimal model during pre-training, DisTab eventuallyemerged as the best performing model is this setting, indicating the robustness of the proposed approach. expands Tab. 2 to focus on pairwise comparison via the win matrices. For regression and binarysetting, DisTab performs comparably to CatBoost and dominates the other tree-based approaches.Formulti-class setting, DisTab clearly outperform random forests and CatBoost and is marginally better thanLightGBM and XGBoost. We highlight that the evaluated datasets not only include different task settings, but with diverse datasetsizes (from only 1k to over 580k samples), column counts (from 7 to 80) and presence of missing values.Across these datasets, Tab. 2 and indicate that DisTab either surpasses or performs comparably totree-based methods, effectively bridging the performance gap between tabular DL and tree-based approaches. In Appendix C, we compare DisTab with tree-based methods using average task performance, to furtherquantify the scale of performance difference among them. The results are consistent with those in Tab. 2,showing that DisTab matches or surpass tree-based methods on the evaluated datasets.",
  ": Win matrices between DisTab and tree-based methods": "above components and we compare their test performance in Tab. 3 and . As described in Sec. 3.1,model variants without +LM use standard input embeddings for tabular datasets, including learnablelookup embeddings and random features.The remaining models additionally include language-guidedembeddings to provide additional context to input data. We also include CatBoost, the teacher model, in as a comparison reference.",
  "ModelDistillationLangEmbedFinetuneRegressionBinaryMulticlassAll": "Base4.38 2.003.90 1.144.29 1.484.16 1.57Base+LM4.62 1.584.60 1.284.21 1.894.50 1.57Distil3.62 1.493.95 1.564.57 1.294.02 1.51Distil+LM3.38 1.493.40 1.694.14 0.833.60 1.47FT2.50 1.322.65 1.522.29 0.882.50 1.31DisTab2.50 0.872.50 1.911.50 0.602.22 1.41 : Ablation comparison on DisTab. Each variant has one or more components turned off with respectto DisTab.Base model denotes standard supervised learning on the datasets.DisTab outperforms allvariants, suggesting that pre-training, language embedding and fine-tuning all contributed meaningfully tomodel performance.",
  "Discussion and Conclusion": "In this work, we introduced DisTab as a framework to bridge the performance gap between DL and tree-basedmethods for tabular prediction tasks. We demonstrated that a straightforward yet previously overlookedstrategy is to leverage distillation for model pre-training, employing an appropriate tree-based model asteacher. We also introduced a simple yet effective data augmentation strategy compatible with distillationto tackle training data scarcity, a common scenario in tabular domains. Empirically, our results suggestthat DisTab compares favorably to other DL methods, including a variety of alternative pre-training strate-gies, model architectures customized for tabular learning, and learning from scratch. More importantly,our approach either surpasses or matches the performance of tree-based methods, effectively closing theperformance gap between the two classes of methods. Beyond these practical contributions, our work demonstrates also the potential structural advantages oftabular DL over tree-based methods, namely the flexibility and ease of integrating different informationsources during learning, such as language. In particular, we showed how to incorporate semantic informationduring learning, including column headers and textual descriptions for categorical data. Our results show thatlanguage guidance is effective for tabular learning, improving the model performance for the vast majorityof datasets. We believe that exploring such structural potentials of DL models is crucial for their furtherimprovements in tabular learning, as the performance gap closes between DL and tree-based methods. Limitation and Future work. An under-explored aspect of this work is the impact of extensive hyper-parameters tuning on tabular learning.In our experiments, we focused our hyper-parameter search toidentify, for each method, a set of default values that work well across all datasets. This is because task-levelhyper-parameter tuning can be computationally prohibitive (especially for tabular DL) and produces mixedresults (Yan et al., 2024; Chen et al., 2023). However, tree-based methods are generally more advantageousthan DL ones in terms of computational efficiency, and the former could afford more extensive hyper-parameter search to yield better models. We leave the investigation on the trade-off between computationalbudget and model performance to future works. Our proposed framework is a general strategy compatible with recent advancements in tabular DL andbeyond. For instance, distillation could be directly combined with transformer architectures tailored fortabular learning (e.g., Somepalli et al., 2021; Chen et al., 2023), or with cross-tabular pre-training (e.g.,Zhu et al., 2023; Yan et al., 2024; Ye et al., 2024). In addition, our framework could leverage different dataaugmentation techniques or language models. We leave these exploration to future works.",
  "Tianqi Chen and Carlos Guestrin. Xgboost: A scalable tree boosting system. In Proceedings of the 22ndacm sigkdd international conference on knowledge discovery and data mining, pp. 785794, 2016": "Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastivelearning of visual representations. In International conference on machine learning, pp. 15971607. PMLR,2020. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchicalimage database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248255. Ieee,2009. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Un-terthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. Nick Erickson, Jonas Mueller, Alexander Shirkov, Hang Zhang, Pedro Larroy, Mu Li, and Alexander Smola.Autogluon-tabular: Robust and accurate automl for structured data. arXiv preprint arXiv:2003.06505,2020.",
  "Ravid Shwartz-Ziv and Amitai Armon. Tabular data: Deep learning is not all you need. Information Fusion,81:8490, 2022": "Gowthami Somepalli, Micah Goldblum, Avi Schwarzschild, C Bayan Bruss, and Tom Goldstein.Saint:Improved neural networks for tabular data via row attention and contrastive pre-training. arXiv preprintarXiv:2106.01342, 2021. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothe Lacroix,Baptiste Rozire, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundationlanguage models. arXiv preprint arXiv:2302.13971, 2023.",
  "Jiahuan Yan, Bo Zheng, Hongxia Xu, Yiheng Zhu, Danny Chen, Jimeng Sun, Jian Wu, and Jintai Chen.Making pre-trained language models great on tabular prediction. arXiv preprint arXiv:2403.01841, 2024": "Chao Ye, Guoshan Lu, Haobo Wang, Liyao Li, Sai Wu, Gang Chen, and Junbo Zhao.Towards cross-table masked pretraining for web data mining. In Proceedings of the ACM on Web Conference 2024, pp.44494459, 2024. Jinsung Yoon, Yao Zhang, James Jordon, and Mihaela Van der Schaar. Vime: Extending the success ofself-and semi-supervised learning to tabular domain. Advances in Neural Information Processing Systems,33:1103311043, 2020."
}