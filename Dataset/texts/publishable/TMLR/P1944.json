{
  "Abstract": "Contrastive instance discrimination methods outperform supervised learning in downstreamtasks such as image classification and object detection. However, these methods rely heavilyon data augmentation during representation learning, which can lead to suboptimal resultsif not implemented carefully. A common augmentation technique in contrastive learningis random cropping followed by resizing. This can degrade the quality of representationlearning when the two random crops contain distinct semantic content. To tackle this issue,we introduce LeOCLR (Leveraging Original Images for Contrastive Learning of Visual Rep-resentations), a framework that employs a novel instance discrimination approach and anadapted loss function. This method prevents the loss of important semantic features causedby mapping different object parts during representation learning. Our experiments demon-strate that LeOCLR consistently improves representation learning across various datasets,outperforming baseline models.For instance, LeOCLR surpasses MoCo-v2 by 5.1% onImageNet-1K in linear evaluation and outperforms several other methods on transfer learn-ing and object detection tasks.",
  "Introduction": "Self-supervised learning (SSL) approaches based on instance discrimination (Chen et al., 2020b; Chen & He,2021; Chen et al., 2020a; Misra & Maaten, 2020; Grill et al., 2020) heavily rely on data augmentations, suchas random cropping, rotation, and colour Jitter, to build invariant representation for all the instances in thedataset. To do so, the two augmented views (positive pairs) for the same instance are attracted in the latentspace while avoiding collapse to the trivial solution (representation collapse). These approaches have provenefficient in learning useful representations by using different downstream tasks, e.g., image classificationand object detection, as proxy evaluations for representation learning. However, these strategies ignore theimportant fact that the augmented views may have different semantic content due to random cropping,which can lead to a degradation in visual representation learning (Song et al., 2023a; Zhang et al., 2022; Liuet al., 2020; Mishra et al., 2021). On the one hand, creating positive pairs through random cropping andencouraging the model to make them similar based on the shared information in the two views makes theSSL task harder, ultimately improving representation quality (Mishra et al., 2021; Chen et al., 2020a). Inaddition, random cropping followed by resizing leads model representation to capture information for the",
  "Published in Transactions on Machine Learning Research (10/2024)": "Spyros Gidaris, Andrei Bursuc, Gilles Puy, Nikos Komodakis, Matthieu Cord, and Patrick Perez. Obow:Online bag-of-visual-words generation for self-supervised learning. In Proceedings of the IEEE/CVF Con-ference on Computer Vision and Pattern Recognition, pp. 68306840, 2021. Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik.Rich feature hierarchies for accurateobject detection and semantic segmentation. In Proceedings of the IEEE conference on computer visionand pattern recognition, pp. 580587, 2014. Jean-Bastien Grill, Florian Strub, Florent Altch, Corentin Tallec, Pierre Richemond, Elena Buchatskaya,Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap yourown latent-a new approach to self-supervised learning. Advances in neural information processing systems,33:2127121284, 2020.",
  ": Examples of positive pairs that might be created by random cropping and resizing": "(a and b) show examples of incorrect semantic positive pairs (i.e., positive pairs that containmismatched semantic information for the same object) that may result from random cropping. In example(a), when the model is forced to align the representations of a dogs head and leg closer in the latent space,it may discard important semantic features. This happens because the model bases its representations onthe shared region between the two views. If this shared region does not contain semantically consistentinformation, the representations become trivial. For random cropping to be effective and achieve occlusioninvariance, the shared region must convey the same semantic information in both views. In contrast, (c and d) shows positive pairs where the shared region contains similar semantic content. For example,in (c), both views contain the dogs head, which helps the model capture features of the dogs head acrossdifferent scales and angles. It is worth noting, though, that there is value in contrasting pairs that mightinclude different semantic information about the same object (e.g., a dog in our case), as it can aid in learningglobal features. As the examples show, creating random crops for one-centric object does not guarantee obtaining correctsemantic pairs. This consideration is important for improving representation learning. Instance discrimina-tion SSL approaches, such as MoCo-v2 (Chen et al., 2020b) and SimCLR (Chen et al., 2020a), encouragethe model to bring positive pairs, i.e., two views for the same instance, closer in the latent space regardlessof their semantic content (Xiao et al., 2021; Zhang et al., 2022). This could limit the models ability to learnrepresentations of different object parts and may impair its capability to learn representations of semanticfeatures (Song et al., 2023a; Zhang et al., 2022) (see (left)). It has been shown that undesirable views containing different semantic content may be unavoidable whenemploying random cropping (Song et al., 2023a). Therefore, we need a method to train the model on differentobject parts to develop robust representations against natural transformations, such as scale and occlusion,rather than simply pulling the augmented views together indiscriminately (Mishra et al., 2021). This issueshould be addressed, as the performance of downstream tasks depends on high-quality visual representationslearned through self-supervised learning (Alkhalefi et al., 2024; Donahue et al., 2014; Manov et al., 2023;Girshick et al., 2014; Zeiler & Fergus, 2014; Kim & Walter, 2017; Zhang & Ma, 2022; Xiao et al., 2020).",
  "We demonstrate that our approach consistently enhances visual representation learning for con-trastive instance discrimination across different datasets and contrastive mechanisms": ": The figure on the left shows the embedding space of established approaches (Chen et al., 2020a;b)where the two views are attracted to each other regardless of their content. In contrast, the figure on theright illustrates our approach, which clusters the two random views together with the original image in theembedding space.",
  "Related Work": "SSL approaches are divided into two broad categories: contrastive and non-contrastive learning.Whileall these methods aim to bring positive pairs closer in latent space, each uses a different strategy to avoidrepresentation collapse. This section provides a brief overview of some of these approaches, and we encouragereaders to refer to the respective papers for more details. Contrastive Learning: Instance discrimination methods, such as SimCLR, MoCo, and PIRL (Chen et al.,2020a; He et al., 2020; Chen et al., 2020b; Misra & Maaten, 2020) employ a similar idea. They attract thepositive pairs together and push the negative pairs apart in the embedding space, albeit through a differentmechanism. SimCLR (Chen et al., 2020a) uses an end-to-end approach where a large batch size is usedfor the negative examples and both encoders parameters in the Siamese network are updated together.",
  "Methodology": "Mapping incorrect semantic positive pairs (i.e., positive pairs containing different semantic views) leads to theloss of semantic features, which degrades model representation learning (Mishra et al., 2021; Purushwalkam& Gupta, 2020; Song et al., 2023b). To address this, we introduce a new contrastive instance discriminationSSL strategy called LeOCLR. Our approach aims to capture meaningful features from two random positivepairs, even if they contain different semantic content, to enhance representation learning. To achieve this,it is crucial to ensure that the information in the shared region between the attracted views is semanticallycorrect. This is because the choice of views controls the information captured by the representations learnedin contrastive learning (Tian et al., 2020). Since we cannot guarantee that the shared region between the twoviews includes correct semantic parts of the object, we propose to involve the original image in the training",
  "and X2) are randomly cropped (RC1 and RC2) and resized to 224 224, followed by the application oftransformations. The embedding space of our approach is depicted on the right side of the Figure": "As shown in (left), our methodology generates three views (X, X1, and X2). The original image(X) is resized without cropping, while the other views (X1 and X2) are randomly cropped and resized.All views are then randomly augmented to prevent the model from learning trivial features. We use dataaugmentations similar to those employed in MoCo-v2 (Chen et al., 2020b). The original image (X) is encodedby the encoder fq, while the two views (X1, X2) are encoded by a momentum encoder fk, whose parametersare updated using the following formula:",
  "k mk + (1 m)q(1)": "where m is a coefficient set to 0.999, q are encoder parameters of fq updated through backpropagation, andk are momentum encoder parameters of fk updated by q. Finally, the objective function forces the modelto pull both views (X1, X2) closer to the original image (X) in the embedding space, while pushing apartall other instances, as illustrated in (right).",
  "(u, v+) = logexp(u v+/)Nn=0 exp(u vn/),(2)": "where similarity is measured by the dot product. The objective function increases the similarity betweenthe positive pairs (u v+) by bringing them closer in the embedding space, while pushing apart all thenegative samples (vn) in the dictionary to avoid representation collapse. is the temperature hyperparameterof softmax. In our approach, we increase the similarity between the original image (i.e., querys featurerepresentation) u = fq(x) with the positive pair (i.e., keys feature representation) v+ = fk(xi) (i = 1, 2)and push apart all the negative examples (vn). Therefore the total loss for the mini-batch is:",
  ":return x": "Equation 3 and Algorithm 1, illustrate the key differences between our approach and previous multi-cropapproaches, such as CLSA (Wang & Qi, 2022), SCFC (Song et al., 2023a) , and DINO (Caron et al., 2021).The key differences are as follows: Previous approaches assume that two global views contain the same semantic information, encourag-ing the model to focus on similarities and create similar representations for both views. In contrast,our approach uses the original images instead of global views, as we argue that global views may con-tain incorrect semantic information for the same object. While they may help capture some globalfeatures, this could limit the models ability to learn more universally useful semantic features,ultimately affecting performance.",
  "Previous approaches use several local random crops, which might be time- and memory-intensive,while our approach uses only two random crops (Caron et al., 2020; Wang & Qi, 2022)": "Our objective function employs different methods to enhance the models visual representation learn-ing. We encourage the model to make the two random crops similar to the original image, whichcontains the semantic information for all random crops while avoiding forcing the two crops to havesimilar representations if they do not share similar semantic information. This approach differs fromprevious methods, which encourage all crops (global and local) to have similar representations re-gardless of their semantic information. As a result, although useful for learning some global features,those methods may discard relevant semantic information, potentially hindering the transferabilityof the resulting representations to downstream tasks.",
  "Experiments and Results": "Datasets: We conducted multiple experiments on three datasets: STL-10 \"unlabeled\" with 100K trainingimages (Coates & Ng, 2011), CIFAR-10 with 50K training images (Krizhevsky, 2009), and ImageNet-1Kwith 1.28M training images (Russakovsky et al., 2015).Training Setup: We used ResNet50 as the backbone, and the model was trained with the SGD optimizer,with a weight decay 0.0001, momentum of 0.9, and an initial learning rate of 0.03. The mini-batch size was256, and the model was trained for up to 800 epochs on ImageNet-1K.Evaluation: We used different downstream tasks to evaluate the LeOCLRs representation learning againstleading SOTA approaches on ImageNet-1K: linear evaluation, semi-supervised learning, transfer learning,and object detection. For linear evaluation, we followed the standard evaluation protocol (Chen et al., 2020a;He et al., 2020; Huynh et al., 2022; Dwibedi et al., 2021), where a linear classifier was trained for 100 epochson top of a frozen backbone pre-trained with LeOCLR. The ImageNet-1K training set was used to train thelinear classifier from scratch, with random cropping and left-to-right flipping augmentations. Results arereported on the ImageNet-1K validation set using a center crop (224 224). In the semi-supervised setting,we fine-tuned the network for 60 epochs using 1% labeled data and 30 epochs using 10% labeled data. Also,we evaluated the learned features on smaller datasets, such as CIFAR (Krizhevsky, 2009), and fine-graineddatasets (Krause et al., 2013; Parkhi et al., 2012; Berg et al., 2014), using transfer learning. Finally, we usethe PASCAL VOC (Everingham et al., 2010) dataset for object detection.Comparing with SOTA Approaches: We used vanilla MoCo-v2 (Chen et al., 2020b) as a baseline tocompare with our approach across different benchmark datasets, given our use of a momentum contrastivelearning framework. Additionally, we compared our approach with other SOTA methods on the ImageNet-1Kdataset.",
  "ApproachEpochsBatchAccuracy": "MoCo-v2 (Chen et al., 2020b)80025671.1%BYOL (Grill et al., 2020)1000409674.4%SWAV (Caron et al., 2020)800409675.3%SimCLR (Chen et al., 2020a)1000409669.3%HEXA (Li et al., 2020)80025671.7%SimSiam (Chen & He, 2021)80051271.3%VICReg (Bardes et al., 2021)1000204873.2%MixSiam (Guo et al., 2021)80012872.3%OBoW (Gidaris et al., 2021)20025673.8%DINO (Caron et al., 2021)800102475.3%Barlow Twins (Zbontar et al., 2021)1000204873.2%CLSA (Wang & Qi, 2022)80025676.2%RegionCL-M (Xu et al., 2022)80025673.9%UnMix (Shen et al., 2022)80025671.8%HCSC (Guo et al., 2022)20025673.3%UniVIP (Li et al., 2022)300409674.2%HAIEV (Zhang & Ma, 2022)20025670.1%SCFS (Song et al., 2023a)800102475.7%",
  "LeOCLR (ours)80025676.2%": "presents the linear evaluation of our approach compared to other SOTA methods. As shown, ourapproach outperforms all others, surpassing the baseline (i.e., vanilla MoCo-v2) by 5.1%. This supports ourhypothesis that while two global views can capture some global features, they may also contain differentsemantic information for the same object (e.g., a dogs head versus its leg), which should be considered toimprove representation learning. The observed performance gap (i.e., the difference between vanilla MoCo-v2 and LeOCLR) demonstrates that mapping pairs with divergent semantic content hinders representationlearning and affects the models performance in downstream tasks.",
  "LeOCLR (ours)62.8%71.5%": "Transfer Learning on Downstream Tasks: We evaluate our self-supervised pretrained model usingtransfer learning by fine-tuning it on small datasets such as CIFAR (Krizhevsky, 2009), Stanford Cars (Krauseet al., 2013), Oxford-IIIT Pets (Parkhi et al., 2012), and Birdsnap (Berg et al., 2014). We follow the transferlearning procedures outlined in (Chen et al., 2020a; Grill et al., 2020) to find optimal hyperparameters foreach downstream task. As shown in , our approach, LeOCLR, outperforms all compared approacheson various downstream tasks. This demonstrates that our model learns useful semantic features, enablingit to generalize better to unseen data in different downstream tasks compared to other approaches. Ourmethod preserves the semantic features of the given objects, thereby improving the models representationlearning capabilities. As a result, it is more effective at extracting important features and predicting correctclasses on transferred tasks.",
  "LeOCLR (ours)98.1%86.9%91.6%76.8%92.1%": "Object Detection Task: To further evaluate the transferability of the learned representation, we compareour approach with other SOTA approaches using object detection on the PASCAL VOC. We follow thesame settings as MoCo-v2 (Chen et al., 2020b), fine-tuning on the VOC07+12 trainval dataset using FasterR-CNN with an R50-C4 backbone, and evaluating on VOC07 test dataset. The model is fine-tuned for 24kiterations ( 23 epochs). As shown in , our method outperforms all compared approaches. Thissuperior performance can be attributed to our models ability to capture richer semantic features comparedto the baseline (MoCo-v2) and other approaches, leading to better results in object detection and relatedtasks.",
  "Ablation Studies": "In the following subsections, we further analyze our approach using another contrastive instance discrimi-nation approach (i.e., end-to-end mechanism) to explore how our method performs within this framework.Additionally, we perform studies on the benchmark datasets STL-10 and CIFAR-10 using a different backbone(ResNet-18) to assess the consistency of our approach across various datasets and backbones. Furthermore,we employ a random crop test to simulate natural transformations, such as variations in scale or occlusionof objects in the image, to analyze the robustness of the features learned by our approach, LeOCLR. Wealso compare our approach with vanilla MoCo-v2 by manipulating their data augmentation techniques todetermine which models performance is more significantly affected by the removal of certain augmentations.In addition, we experiment with different fine-tuning settings to evaluate which model learns better andfaster. Furthermore, we adapt the attraction strategy and cropping method of the original image, as wellas compute the running time of our approach. Finally, we examine our approach on a non-centric objectdataset where the probability of mapping two views containing distinct information is higher.",
  "Different Contrastive Instance Discrimination Framework": "We utilize an end-to-end framework in which the two encoders fq and fk are updated via backpropagationto train a model with our approach for 200 epochs and 256 batch size. Following this, we conduct a linearevaluation of our model against SimCLR, which also employs an end-to-end mechanism. As shown in , our approach outperforms vanilla SimCLR by a significant margin of 3.5%, demonstrating its suitabilityfor integration with various contrastive learning frameworks.",
  "Scalability": "In , we evaluate our approach on different datasets (STL-10 and CIFAR-10) using a ResNet-18backbone to ensure its consistency across various backbones and datasets (i.e., scalability). We pre-trainedall the approaches for 800 epochs with batch size 256 on both datasets and then conducted a linear evaluation.Our approach demonstrates superior performance on both datasets compared to all approaches. For instance,",
  "Center and Random Crop Test": "In , we reported the top-1 accuracy for vanilla MoCo-v2 and our approach after 200 epochs onImageNet-1K, focusing on two tasks: a) center crop test, similar to (Chen et al., 2020a;b) where imagesare resized to 256 pixels along the shorter side using bicubic resampling, followed by a 224 224 centercrop; and b) random crop, where images are resized to 256 256 and then randomly cropped and resizedto 224 224. We obtained the MoCo-v2 center crop result directly from (Chen et al., 2020b), but therandom crop result was not reported. To ensure a fair comparison when reporting the random crop results,we replicated MoCo-v2 using the same hyperparameters from the original paper that were used to reportthe center crop.According to the results, the performance of MoCo-v2 dropped by 4.3% with randomcropping, whereas our approach experienced a smaller drop of 2.8%. This suggests that our approach learnsbetter semantic features, demonstrating greater invariance to natural transformations such as occlusion andvariations in object scales. Additionally, we compare the performance of CLSA (Wang & Qi, 2022) with ourapproach, given that both perform similarly after 800 epochs (see . Note that the CLSA approachuses multi-crop (i.e., five strong and two weak augmentations), while our approach employs only two randomcrops and the original image. As shown in , LeOCLR outperforms the CLSA approach by 2.3% after200 epochs on ImageNet-1K. To address concerns about the increased computational cost associated withtraining LeOCLR compared to MoCo V2, we include the training time for both approaches in . Wetrained both models on three A100 GPUs with 80GB for 200 epochs. Our approach took an additional 13hours to train over the same number of epochs, but it delivers significantly better performance than thebaseline.",
  "Augmentation and Fine-tuning": "Contrastive instance discrimination approaches are sensitive to the choice of image augmentations (Grillet al., 2020). This sensitivity necessitates further analysis comparing our approach to Moco-v2 (Chen et al.,2020b). These experiments aim to explore which model learns better semantic features and produces morerobust representations under different data augmentations. As shown in , both models are affectedby the removal of certain data augmentations. However, our approach shows a more invariant representationand exhibits less performance degradation due to transformation manipulation compared to vanilla MoCo-v2. For instance, when we apply only random cropping augmentation, the performance of vanilla MoCo-v2drops by 28 percentage points (from a baseline of 67.5% to 39.5% with only random cropping). In contrast,our approach experiences a decrease of only 25 percentage points (from a baseline of 71.7% to 46.6% withonly random cropping). This indicates that our approach learns better semantic features and produces moreeffective representations for the given objects than vanilla MoCo-v2. In , presented in , we fine-tune the representations over the 1% and 10% ImageNet-1K splitsfrom (Chen et al., 2020a) using the ResNet-50 architecture. In the ablation study, we compare the fine-tunedrepresentations of our approach with the reproduced vanilla MoCo-v2(Chen et al., 2020b) across 1%, 2%,5%, 10%, 20%, 50%, and 100% of the ImageNet-1K dataset, following the methodology in (Henaff, 2020;Grill et al., 2020). In this setting, we observe that tuning a LeOCLR representation consistently outperformsvanilla MoCo-v2. For instance, (a) demonstrates that LeOCLR fine-tuned with 10% of ImageNet-1K labeled data outperforms vanilla Moco-v2 (Chen et al., 2020b) fine-tuned with 20% of labeled data. Thisindicates that our approach is advantageous when the labeled data for downstream tasks is limited.",
  "Augmentation": "Decrease of accuracy from baseline LeOCLRMoCov2 : Decrease in top-1 accuracy (in % points) of LeOCLR and our reproduction of vanilla MoCo-v2after 200 epochs, under linear evaluation on ImageNet-1K. R_Grayscale refers to results without grayscaleaugmentations, while R_color refers to results without color jitter but with grayscale augmentations. 125 102050100 Percentage of training data ImageNet Top-1 Accuracy (%) LeOCLRMoCov2 125 102050100 Percentage of training data 75.0 77.5 80.0 82.5 85.0 87.5 90.0 92.5 ImageNet Top-5 Accuracy (%) LeOCLRMoCov2",
  "Attraction Strategy": "In this subsection, we apply a random crop to the original image (x) and attract the two views (x1,x2)toward it to evaluate its impact on our approachs performance. We also conducted an experiment whereall views were attracted to each other.However, in our method, we avoid attracting the two views toeach other, enforcing the model to draw the two views toward the original image only (i.e., the uncroppedimage containing semantic features for all crops).For these experiments, we pre-trained the model onImageNet-1K for 200 epochs using the same hyperparameters employed in the main experiment.The experiments in underscore the significance of the information shared between the two views.They also highlight the importance of leveraging the original image and avoiding the attraction of viewscontaining varied semantic information to preserve the semantic features of the objects. When we createa random crop for the original image (x) and enforce the model to make the two views similar to the",
  "LeOCLR (ours)71.7%": "original image (i.e., LeOCLR(Random original image)), the model performance decreases by 2.4%. Thisdrop occurs because cropping the original image and enforcing the model to attract the two viewstoward it increases the likelihood of having two views with varied semantic information, leading to aloss of semantic features of the objects.The situation worsens when we attract all views (x,x1,x2) toeach other in LeOCLR (attract all crops), causing performance to drop closer to that of vanilla MoCo-v2(67.5%). This is due to the high probability of attracting two views containing distinct semantic information.",
  "Non-Object-Centric Tasks": "Non-object-centric datasets, such as COCO (Lin et al., 2014), portray real scenes where the objects ofinterest are not centered or prominently situated, as opposed to object-centric datasets like ImageNet-1K.In this case, the probability of creating two views containing distinct semantic information for the object ishigher, exacerbating the problem of losing semantic features. Therefore, we train both our approach and theMoCo-v2 baseline from scratch on the COCO dataset to evaluate how our method handles the discardingof semantic features in such datasets. We used the same hyperparameters as for ImageNet-1K, training themodels with a batch size of 256 over 500 epochs. Subsequently, we fine-tuned these pre-trained models onthe COCO dataset for object detection.",
  "Conclusion": "This paper introduces a novel contrastive instance discrimination approach for SSL to enhance repre-sentation learning.Our method mitigates the loss of semantic features by incorporating the originalimage during training, even when the two views contain distinct semantic content. We demonstrate thatour approach consistently improves the representation learning of contrastive instance discrimination invarious benchmark datasets, backbones, and mechanisms, such as momentum contrast and end-to-endmethods.In linear evaluation, we achieved an accuracy of 76.2% on ImageNet-1K after 800 epochs,outperforming several SOTA instance discrimination SSL approaches. Additionally, we demonstrated theinvariance and robustness of our approach across different downstream tasks, including transfer learningand semi-supervised fine-tuning.",
  "Adrien Bardes, Jean Ponce, and Yann LeCun.Vicreg: Variance-invariance-covariance regularization forself-supervised learning. arXiv preprint arXiv:2105.04906, 2021": "Thomas Berg, Jiongxin Liu, Seung Woo Lee, Michelle L Alexander, David W Jacobs, and Peter N Belhumeur.Birdsnap: Large-scale fine-grained visual categorization of birds. In Proceedings of the IEEE Conferenceon Computer Vision and Pattern Recognition, pp. 20112018, 2014. Mathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze. Deep clustering for unsupervisedlearning of visual features. In Proceedings of the European conference on computer vision (ECCV), pp.132149, 2018. Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsu-pervised learning of visual features by contrasting cluster assignments. Advances in neural informationprocessing systems, 33:99129924, 2020. Mathilde Caron, Hugo Touvron, Ishan Misra, Herv Jgou, Julien Mairal, Piotr Bojanowski, and ArmandJoulin.Emerging properties in self-supervised vision transformers.In Proceedings of the IEEE/CVFinternational conference on computer vision, pp. 96509660, 2021. Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastivelearning of visual representations. In International conference on machine learning, pp. 15971607. PMLR,2020a.",
  "Adam Coates and Andrew Y Ng. Analysis of large-scale visual recognition. In Advances in neural informationprocessing systems, pp. 284292, 2011": "Jeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoffman, Ning Zhang, Eric Tzeng, and Trevor Darrell.Decaf: A deep convolutional activation feature for generic visual recognition. In International conferenceon machine learning, pp. 647655. PMLR, 2014. Debidatta Dwibedi, Yusuf Aytar, Jonathan Tompson, Pierre Sermanet, and Andrew Zisserman. With a littlehelp from my friends: Nearest-neighbor contrastive learning of visual representations. In Proceedings ofthe IEEE/CVF International Conference on Computer Vision, pp. 95889597, 2021.",
  "Xiaoyang Guo, Tianhao Zhao, Yutian Lin, and Bo Du. Mixsiam: a mixture-based approach to self-supervisedrepresentation learning. arXiv preprint arXiv:2111.02679, 2021": "Yuanfan Guo, Minghao Xu, Jiawen Li, Bingbing Ni, Xuanyu Zhu, Zhenbang Sun, and Yi Xu. Hcsc: Hierar-chical contrastive selective coding. In Proceedings of the IEEE/CVF Conference on Computer Vision andPattern Recognition (CVPR), pp. 97069715, June 2022. Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervisedvisual representation learning. In Proceedings of the IEEE/CVF conference on computer vision and patternrecognition, pp. 97299738, 2020.",
  "Chunyuan Li, Xiujun Li, Lei Zhang, Baolin Peng, Mingyuan Zhou, and Jianfeng Gao.Self-supervisedpre-training with hard examples improves visual representations. arXiv preprint arXiv:2012.13493, 2020": "Zhaowen Li, Yousong Zhu, Fan Yang, Wei Li, Chaoyang Zhao, Yingying Chen, Zhiyang Chen, JiahaoXie, Liwei Wu, Rui Zhao, et al. Univip: A unified framework for self-supervised visual pre-training. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1462714636,2022. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollr, andC Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer VisionECCV 2014: 13thEuropean Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pp. 740755.Springer, 2014.",
  "Omkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, and CV Jawahar. Cats and dogs. In 2012 IEEEconference on computer vision and pattern recognition, pp. 34983505. IEEE, 2012": "Senthil Purushwalkam and Abhinav Gupta. Demystifying contrastive self-supervised learning: Invariances,augmentations and dataset biases. Advances in Neural Information Processing Systems, 33:34073418,2020. Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, AndrejKarpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet Large ScaleVisual Recognition Challenge. International Journal of Computer Vision (IJCV), 115(3):211252, 2015.doi: 10.1007/s11263-015-0816-y. Zhiqiang Shen, Zechun Liu, Zhuang Liu, Marios Savvides, Trevor Darrell, and Eric Xing. Un-mix: Rethinkingimage mixtures for unsupervised visual representation learning. In Proceedings of the AAAI Conferenceon Artificial Intelligence, volume 36, pp. 22162224, 2022. Kaiyou Song, Shan Zhang, Zimeng Luo, Tong Wang, and Jin Xie. Semantics-consistent feature search forself-supervised visual representation learning. In Proceedings of the IEEE/CVF International Conferenceon Computer Vision (ICCV), pp. 1609916108, October 2023a. Kaiyou Song, Shan Zhang, Zimeng Luo, Tong Wang, and Jin Xie. Semantics-consistent feature search forself-supervised visual representation learning. In Proceedings of the IEEE/CVF International Conferenceon Computer Vision, pp. 1609916108, 2023b. Yonglong Tian, Chen Sun, Ben Poole, Dilip Krishnan, Cordelia Schmid, and Phillip Isola. What makes forgood views for contrastive learning? Advances in Neural Information Processing Systems, 33:68276839,2020.",
  "Tete Xiao, Xiaolong Wang, Alexei A Efros, and Trevor Darrell. What should not be contrastive in contrastivelearning. arXiv preprint arXiv:2008.05659, 2020": "Tete Xiao, Colorado J Reed, Xiaolong Wang, Kurt Keutzer, and Trevor Darrell. Region similarity repre-sentation learning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp.1053910548, 2021. Yufei Xu, Qiming Zhang, Jing Zhang, and Dacheng Tao. Regioncl: exploring contrastive region pairs for self-supervised representation learning. In European Conference on Computer Vision, pp. 477494. Springer,2022. Jure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and Stphane Deny. Barlow twins: Self-supervised learningvia redundancy reduction. In International Conference on Machine Learning, pp. 1231012320. PMLR,2021. Matthew D Zeiler and Rob Fergus. Visualizing and understanding convolutional networks. In ComputerVisionECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings,Part I 13, pp. 818833. Springer, 2014. Junbo Zhang and Kaisheng Ma. Rethinking the augmentation module in contrastive learning: Learninghierarchical augmentation invariance with expanded views. In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pp. 1665016659, 2022."
}