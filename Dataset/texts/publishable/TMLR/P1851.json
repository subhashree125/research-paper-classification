{
  "Abstract": "Large multimodal models (LMMs) have shown great results in single-image vision languagetasks. However, their abilities to solve multi-image visual language tasks is yet to be improved.The existing LMMs like OpenFlamingo, Emu2, and Idefics gain their multi-image abilitythrough pre-training on hundreds of millions of noisy interleaved image-text data from theweb, which is neither efficient nor effective. In this paper, we aim to build strong multi-imageLMMs via instruction tuning with academic-level resources. Therefore, we meticulouslyconstruct Mantis-Instruct containing 721K multi-image instruction data to train a familyof Mantis models. The instruction tuning empowers Mantis with different multi-image skillslike co-reference, comparison, reasoning, and temporal understanding. We evaluate Mantison 8 multi-image benchmarks and 6 single-image benchmarks. Mantis-Idefics2 can achieveSoTA results on all the multi-image benchmarks and beat the strongest multi-image baseline,Idefics2-8B by an average of 13 absolute points. Notably, Idefics2-8B was pre-trained on 140Minterleaved multi-image data, which is 200x larger than Mantis-Instruct. We observethat Mantis performs equivalently well on the held-in and held-out benchmarks, whichshows its generalization ability. We further evaluate Mantis on single-image benchmarksand demonstrate that Mantis also maintains a strong single-image performance on par withCogVLM and Emu2. Our results show that multi-image abilities are not necessarily gainedthrough massive pre-training, instead, they can be gained by low-cost instruction tuning.The training and evaluation of Mantis has paved the road for future work to improve LMMsmulti-image abilities.",
  "Introduction": "Large Multimodal Models (LMMs) have advanced significantly in recent years. Both closed-source models likeGPT-4V (Achiam et al., 2023), Gemini (Team et al., 2023), Reka (Ormazabal et al., 2024), MM1 (McKinzieet al., 2024) and open-source models like LLaVA (Liu et al., 2023c;b), LLaVA-NeXT (Liu et al., 2024),BLIP (Li et al., 2023c), CogVLM (Wang et al., 2023), Idefics (Laurenon et al., 2023) have shown strongvisual-language understanding and generation capabilities in single-image tasks like VQA (Antol et al., 2015),TextVQA (Singh et al., 2019b), GQA (Hudson & Manning, 2019), etc. Despite the significant interest inimproving LMMs performance on single-image vision-language tasks, relatively less work attempts to improveLMMs capability to solve multi-image vision-language tasks. To the best of our knowledge, only a fewopen-source models (e.g. Idefics (Laurenon et al., 2023), OpenFlamingo (Awadalla et al., 2023), Emu2 (Sunet al., 2023)) and VILA (Lin et al., 2023c) support multi-image inputs. We argue that multi-image visual ability is also a crucial in real-world applications. Specifically, we categorizethe ability into four major skills we want the LMM to encompass: (1) Co-reference: understanding thereferences like second image in the natural language expression and grounding it on the referred image togenerate a response. (2) Comparison: capturing the nuances and commonalities between several images.(3) Reasoning: capturing the information across multiple images and reasoning over these multiple pieces",
  "to derive the response. (4) Temporal understanding: observing multiple frames of a video or a scene tounderstand the temporal information like actions, behaviors, interactions, etc": "The existing multi-image LMMs like Idefics (Laurenon et al., 2023), OpenFlamingo (Awadalla et al., 2023),Emu2 (Sun et al., 2023), MM1 (McKinzie et al., 2024), VILA (Lin et al., 2023c) heavily rely on pre-trainingon a massive interleaved image-text web documents like MMC4 (Zhu et al., 2024), Obelics (Laurenon et al.,2023). For example, MMC4 contains 500 million examples, while Obelics contains 140 million examples. Sucha pre-training procedure would require a massive amount of computation, which hampered its adoption inmost open-source LMM training pipelines. In this paper, we attempt to build LMMs with academic-levelresources to support interleaved multi-image inputs. Specifically, we made a few efforts along this line: Dataset: We create the first multi-image instruction-tuning dataset Mantis-Instruct. It has a total of721K instances, consisting of 14 subsets to cover all the multi-image skills. Among the 14 subsets, 10 subsetsare from the existing datasets. For example, NLVR2 (Suhr et al., 2018), IconQA (Jhamtani & Berg-Kirkpatrick,2018), etc are used to cover reasoning skill; DreamSim (Fu et al., 2023), Birds-to-Words (Forbes et al., 2019),etc are used to cover comparison skill; NExT-QA (Xiao et al., 2021), STAR (Wu & Yu, 2021), etc are used tocover temporal understanding skill. Additionally, 4 new subsets are newly curated, where LLaVA-665k-multi,and LRV-multi are used to to cover the coref skill, Contrast-Caption, and Multi-VQA are used tobroaden the reasoning skill. The interleaving image placeholders are inserted into the text under variousheuristics. Architecture: Similar to LLaVA (Liu et al., 2024), we start from a pre-trained language model like LLaMA-3 (AI@Meta, 2024) or Fuyu (Adept AI, 2023) and a vision transformer encoder from CLIP (Radford et al.,2021) or SigLIP (Zhai et al., 2023). We use the same multimodal projector as LLaVA to map the visionembeddings to the text embeddings space to simply concatenate them with the text embeddings. We alsodefine a text-image interleaving format: ...(image {i}:<BOI>image embeddings</EOI>)..., where <BOI>and</EOI> are the image delimiters. We explored multiple instantiations and found this optimal design.",
  "Evaluation: We manually curate a new challenging dataset Mantis-Eval to cover several multi-image skills.This evaluation can help us better analyze LMMs multi-image abilities": "We train Mantis by combining Mantis-Instruct with another 268K single-image vision-language data tobalance the multi-image and single-image abilities. To evaluate the multi-image skills, we adopt 8 benchmarksincluding NLVR2 (Suhr et al., 2018), BLINK (Fu et al., 2024), MVBench (Li et al., 2023d), Mantis-Eval,etc. to cover all the mentioned skills. We include 15 competitive baseline models including single-imageLMMs like LLaVA-Next (Liu et al., 2024), Qwen-VL (Bai et al., 2023) and multi-image LMMs like GPT-4V,",
  "Published in Transactions on Machine Learning Research (11/2024)": "Haoning Wu, Zicheng Zhang, Erli Zhang, Chaofeng Chen, Liang Liao, Annan Wang, Chunyi Li, Wenxiu Sun,Qiong Yan, Guangtao Zhai, and Weisi Lin. Q-bench: A benchmark for general-purpose foundation modelson low-level vision. ArXiv, abs/2309.14181, 2023a. URL Haoning Wu, Hanwei Zhu, Zicheng Zhang, Erli Zhang, Chaofeng Chen, Liang Liao, Chunyi Li, Annan Wang,Wenxiu Sun, Qiong Yan, Xiaohong Liu, Guangtao Zhai, Shiqi Wang, and Weisi Lin. Towards open-endedvisual quality comparison. ArXiv, abs/2402.16641, 2024. URL",
  "Method": "Model Architecture:Most existing LMMs do not support multi-image inputs, either due to the architecturedesign (Huang et al., 2016; Li et al., 2023c), or the lack of associated code support (Peng et al., 2023; AdeptAI, 2023). To enable multi-image training and inference, we have modified the LLaVA architecture and addedcode support for it. Idefics2 (Laurenccon et al., 2024) already supports multi-image as inputs naturally, wethus inherit their structure. Image Context Length:How many images can a multi-image supported model accept? This is limitedby both the LLM backbones max token length and the number of image tokens the vision encoder generated.For the LLaVA architecture, each image consumes a fixed (336/14)2 = 576 image tokens. For Llama3 with amaximum window size of 8K, we can feed at most 14 images. For Idefics2, the number of tokens per image isresampled to be 64 tokens only due to the existence of perceiver, which is pretty efficient. Given 8K contextlength, it can accept 128 images at most, even more than some video models. Interleaving Text-Image:A proper text-image interleaving format can help acquire multi-image under-standing and reasoning ability. We contend that a good text-image interleaving format should: (1) markboundaries between images clearly, and (2) denote the serial number of images. Following this principle,we designed our interleaving format as follows: \"(image {i}:<BOI><image><EOI>)\", where <BOI> is thebegin of image token and <EOI> is the end of image token. <image> is the placeholder for image patches.This format adds clear separators between images, and gives serialized information of the image through\"image {i}\". In practice, we set <BOI> and <EOI> to be <Image> and </Image> respectively. Previous workalso demonstrates its effectiveness through a comprehensive ablation study (Wu et al., 2024).",
  "Mantis-Instruct: A large scale multiple image question answering dataset": "We construct Mantis-Instruct from multiple publicly available datasets. Detailed statistics are shownin with several newly curated subset by us. We use a similar dataset format with LLaVAs, whereeach data item contains multiple images and multiple turns of QA pairs are gathered. We demonstrate someexamples of our newly curated subset in . We report the number of examples, the number of imagesper item, the average conversation turns, and the average length. Mantis-Instruct is collected based onthe 4 multi-image skills. We here describe the gathered subsets for each skill briefly. Construction details canbe found in subsection A.1. Co-referencerequires the model to create a mapping from natural language references, such as \"secondimage,\" to the actual images in the input. It does not require the model to infer across multiple images.",
  "Total-268K1.0114.7144720": ": The statistics of Mantis-Instruct. We list the number of examples, average images per example,maximum image per example, average turns, average text token lengths, and average text+image tokenlengths. means the new subsets we constructed in this paper. Here we use Llavas tokenizer and assume eachimage occupies 576 image tokens, which is the number of tokens for each image. Here \"Avg LengthT \" denotesthe average number of tokens for the text part, while \"Avg LengthT &I\" means the average number of tokensof text and images in total. Heuristics about how each subset is curated can be found in subsection A.1. Please generate 10 independent QA pairs. Each question shall involve at least 2 images to answer.Try to cover different ability like reasoning, planning, common sense understanding, etc. Becreative with your questions and **make sure the answers require integration of informationfrom multiple images**. Use \"image i\" to refer to the i-th image in your questions.",
  ": Propmt template used to curate the Multi-VQA subset": "To achieve this, we constructed LLaVA-665k-multi and LRV-multi by concatenating multiple single-imageconversations into a multi-image sequence. Deliberately, we included natural language references like \"For thesecond image\" for each question. Comparisonrequires the model to not only have good co-reference ability but also be able to compareand understand differences across multiple images. We gather Co-Instruct, Dreamsim, Spot-the-Diff, and",
  "Birds-to-Words together to enhance the models comparison ability. They cover a wide range of topics suchas image quality, visual similarity, and difference description": "Reasoningrequires the model to further make inferences by combining its world knowledge with theinformation collected using the co-reference and comparison ability. It distinguishes from the comparisonskill by introducing more complex human instruction associated with the images. We gather NLVR2, IconQA,Contrast-Caption by reformatting captioning datasets, ImageCoDe, and our self-collected Multi-VQA tofurther equip the model with reasoning ability. The topics include logical reasoning, counting, image matching,image retrieval, and free-form multi-image QA. Multi-VQA is synthesized by prompting GPT-4V, where theprompt template is shown in . Temporal Understandingrequires the model to understand temporal image sequences such as videos,comics, etc. We include a small number of video understanding datasets (14K), VIST, NExT-QA, and STAR, toactivate the models temporal understanding ability. We contend that a model with the above 3 multi-imageskills shall be easily tuned to understand image sequences.",
  "Heuristics of data curation": "Similar to visual instruction tuning (Liu et al., 2023c), data curation is an important step to ensure goodperformance. During the curation, we found some heuristics that could improve performance. We list thembelow. Note that the first heuristics is a simple way to diversify QA format to avoid overfitting, and the resttwo heuristics have already been proven effective by previous work Co-Instruct (Wu et al., 2024). We thusadopt them directly to avoid duplicate efforts to confirm their effectiveness. 1. Conflicts between short answers and long answers. Many datasets we collected are domain-specific, and some of them are classification tasks like NLVR2 (Suhr et al., 2018) and DreamSim (Fuet al., 2023). For these datasets, we convert them into multiple-choice QA format to avoid conflictsbetween repeated short-answers and some long answers from other datasets. 2. Addition of image denotation in the text for each question. We manually add imagedenotations like \"the first image\", \"image 2\", \"the right image\", etc. to each question in our datasetto make sure the question involved with multiple images is clear to answer. 3. Positions of the \"<image>\" placeholder. For each item with multiple images, we write some rulesto randomly put the image placeholders at the beginning of the first question or the end of the firstquestion. This technique is also used by the curation LLaVA-665k (Liu et al., 2023b).",
  "Experiments": "In this section, we aim to evaluate Mantiss ability on both multi-image and single-image vision-languagetasks.For multi-image tasks, depending on whether the LMM can handle multiple images, we eitherconcatenate multiple images (merge) horizontally as a single image or feed multiple images as a sequenceto the model. We have trained four model variants listed in , where we list the architecture anddataset details. Mantis-CLIP and Mantis-SigLIP only adopt the CC3M 560K subset for feature alignment.Mantis-Flamingo and Mantis-Idefics2 are initialized from OpenFlamingo and Idefics2, respectively, whichhave been intensively pre-trained on the multi-image corpus. We also include Mantis-Fuyu to investigate thesignificance of vision encoder in the LMM architecture. After this pre-training and preparation, we fine-tunethem on Mantis-Instruct to get Mantis.",
  "Training Details": "For Mantis-CLIP and Mantis-SigLIP, we first pre-train the multimodal projector on LLaVA pre-train data,where the learning rate is set to 1e-3, the batch size is set to 256. After pre-training, we fine-tune the 2models on the Mantis-Instruct. For Mantis-Fuyu, Mantis-Flamingo and Mantis-Idefics2, we directlyfine-tune them on Mantis-Instruct since they have already been pre-trained on millions of data. During the fine-tuning, we train each model on the data for 1 epoch, with a batch size of 128. The maximumcontext length is set to 8192. The learning rate is all set to 1e-5, except for Idefics2, where the learning rateis set to 5e-6 to better preserve its original knowledge. We set the warmup ratio to be 0.03 and use a cosinelearning rate scheduler. We speed up our training and inference with Flash-attention2 (Dao, 2023). We useDeepSpeed Zero-3 (Aminabadi et al., 2022) for full fine-tuning. We apply QLoRA (Dettmers et al., 2023)along with DoRA (yang Liu et al., 2024) to more efficiently do comprehensive ablation studies under limitedresources. All full fine-tuning ran on 16 A100 GPUs while the ablation study ran on 8 A100 GPUs.",
  "Baselines": "For merge evaluation, we include BLIP-2-13B (Li et al., 2023c), InstructBLIP-13B (Dai et al., 2023), Qwen-VL-Chat (Bai et al., 2023), LLaVA-1.5-7B (Liu et al., 2023c), LLaVA-1.6-7B (Liu et al., 2024), Kosmos2 (Penget al., 2023), and Fuyu-8B (Adept AI, 2023). For sequence evaluation, we include OpenFlamingo (Awadallaet al., 2023), Otter (Li et al., 2023b), VideoLLaVA (Lin et al., 2023a), Emu2-Chat-34B (Sun et al., 2023),VILA (Lin et al., 2023b), Idefics1 (Laurenon et al., 2023), Idefics2 (Laurenon et al., 2023), GPT-4V (Achiamet al., 2023). Additionally, for single-image tasks, we include InstructBLIP-7B-Vicuna (Dai et al., 2023),Yi-VL-6B (AI et al., 2024).",
  "Evaluation Benchmarks": "For multi-image tasks, we use 2 held-in benchmarks: NLVR2 (Suhr et al., 2018) and Qbench (Wu et al.,2023a); and 3 held-out benchmarks: Mantis-Eval, BLINK (Fu et al., 2024), and MVBench (Li et al., 2023d).We report detailed statistics in .NLVR2 (Suhr et al., 2018) evaluates the models ability to conduct logical reasoning across the contentsof images. It asks the model to compare the contents of the two given images and judge whether a givenstatement is correct or not. All questions are multiple-choice. We use the test-public split for evaluation.",
  ": Statistics of the evaluation dataset. Held-in benchmarks, NLVR2 and Q-Bench, are marked with notation. The rest of them are held-out benchmarks. Mixed means a": "Qbench (Wu et al., 2023a) is a benchmark evaluating whether LMMs can properly judge and comparethe quality of a benchmark. Q-bench aims to evaluate the low-level visual abilities of LMMs, such as judgingthe image quality. All questions are multiple-choice. In our experiments, we evaluate the Qbench2-A2-pairdev set, where a low-level visual question is asked based on multiple image contents.Mantis-Eval consists of 217 multiple-image reasoning examples that cover different topics, including sizeperceptions, weight comparisons, etc. This dataset is carefully curated by our annotators, where imagesare acquired via Google Search, and then come up with a proper question which requires understandingthe contents in the two images well to answer. Mantis-Eval contains both multiple-choice and short-answerquestions. We report results in the test split.BLINK (Fu et al., 2024) is a benchmark on core visual perception abilities, where humans can solvemost tasks within a blink (e.g., relative depth estimation, visual correspondence, forensics detection, andmulti-view reasoning). Some questions in the benchmark involved multiple images, such as image similaritycomparison. We report results in the validation set of the benchmark.MVBench (Li et al., 2023d) is a comprehensive multimodal video understanding benchmark covering 20challenging video tasks. The questions cannot be solved with a single frame, necessitating understanding animage sequence to solve the questions in the benchmark. We extract 8 frames per video for the evaluation.We report results in test split.MileBench (Song et al., 2024) is a novel benchmark designed to test the multimodal long-contextcapabilities of MLLMs. It comprises realistic evaluation and diagnostic evaluation. The former involvestasks like temporal understanding and semantic multi-image comprehension. The later diagnostic evaluationinvolves TextNeedle and ImageNeedle retrieval tasks. We only reported the performance of the realisticevaluation. The results of the diagnostic evaluation can be found in MileBenchs paper.MuirBench (Wang et al., 2024a) is a comprehensive benchmark consisting of 12 diverse multi-imagetasks, such as scene understanding, ordering, etc. It contains 2,600 multiple-choice questions with 11,264images involved in total. We report the overall average performance across the 12 tasks.MMIU (Meng et al., 2024) is a multi-image evaluation benchmark encompassing 7 types of multi-imagerelationships, 52 tasks, 77K images, and 11K multiple-choice questions. We report the overall averageperformance across all the tasks.",
  "Results on Multi-Image Tasks": "We report our results on the multi-image task in . We put the results of GPT-4V/o at the top ofthe table as its a closed-source LMM. We can see that our best version Mantis-Idefics2 almost matches theoverall performance of GPT-4V. We put comparison results between Mantis and other open-source LMMs,along with the insights derived in the following paragraphs. Mantis performs well on held-in evaluation: We found that Mantis-SigLIP without any multi-imagepre-training can already achieve the best-known performance on these two benchmarks. Mantis-Idefics2 hasbetter performance of 89.71 and 75.20 on NLVR2 and Q-Bench respectively, marking it the SoTA for thesetwo tasks. Notably, Mantis-Idefics2 also beats the Idefics2 by 2.84 points, which was already trained onNLVR2. It shows the cross-task benefits of our instruction tuning. The remarkable performance on these 2",
  "over SOTA-2.8418.208.303.871.983.109.0717.80": ": Evaluation results on multi-image benchmarks. The highest score of open-source models for eachtask is bolded. The highest score of the open-source baseline model is underscored. is the performancegains of Mantis compared to the best baseline model for each task. We report the GPT-4os results onMuirBench and MMIU, and GPT-4Vs on the rest of them.",
  "benchmarks demonstrates that Mantis has gained strong ability in multi-image logical reasoning and low-levelvision understanding": "Mantis generalizes well on held-out evaluation: We show that our models can attain strong performanceon the 6 held-out benchmarks. Mantis-SigLIP achieves 59.45 on Mantis-Eval, and Mantis-Idefics2 achieves49.05, and 51.38 on BLINK and MVBench respectively, surpassing all other baselines on these held-outtasks. Its also worth mentioning the reported performance of Mantis on the 3 recently released multi-imagebenchmarks with comprehensive evaluation tasks. Mantis models surpassed the VILA and Idefics2, which arepre-trained with millions of multi-image data, by a large margin, making them the best open-source modelson the leaderboard. This is strong evidence of Mantiss good generalization ability in the unseen scenario. Multi-Image Pre-training is not necessary: In these experiments, Mantis-CLIP and Mantis-SigLIPare attaining much better performance than Mantis-Flamingo, and similar performance as Mantis-Idefics,which are pre-trained on millions of text-image interleaved data. These results reveal that the multi-imagepre-training (on massive web corpus) is not a necessary path toward strong multi-image performance. Merge vs Sequence input: The merge evaluation is an important baseline to evaluate the multi-imageunderstanding ability of those LMMs that can only accept one image at a time. LLaVA-v1.6s dynamic imageresolution feature can divide a single image into multiple tiles, processing the merge image like multipleimages (Liu et al., 2024). But it still performs poorly on multi-image tasks, only getting 45.62 and 39.55 onthe Mantis-Eval and BLINK benchmark, which are at least 10 absolute points lower than Mantis. This resultindicates the advantages of building models to accept sequence image inputs. Vision encoder matters: Unlike many other LMMs like Flamingo, LLaVA, and BLIP, Fuyu is a decoder-only LMM without a vision encoder. To investigate the importance of vision encoder, we also train anadditional Mantis-Fuyu as a comparison. Results in show that the performance of Mantis-Fuyu isworse than Mantis-CLIP and Mantis-SigLIP where a vision encoder exists. While we acknowledge a possiblefactor brought by the different LLM backbones, we think whether vision encoder exists is a bigger differenceand conclude that a good vision encoder can enhances the performance.",
  "Results on Single-Image Tasks": "We also evaluate Mantis-CLIP and Mantis-SigLIP on various single-image tasks, including TextVQA (Singhet al., 2019a), VQA-v2 (Goyal et al., 2016), MMBench (Liu et al., 2023d), MMMU (Yue et al., 2023), etc.Results are shown in . All the evaluations are conducted with the help of LMMs-Eval (Li* et al.,2024) tool. Mantis-Flamingos and Mantis-Fuyus performance is omitted due to its bad performance. Compared to a set of popular LMMs, Mantis-SigLIP gets significant improvements on MMBench-English,MMMU, and ScienceQA benchmarks though we did not specifically optimize the single-image abilities. Solvingproblems in these tasks requires not only the visual perception ability but also the good reasoning abilityof the LLM backbone. Considering that we are using the powerful LLaMA-3 as the backbone LLM, theseimprovements are expected. We also compare the results of Mantis-Idefics2 against Idefics2 on single-imagetasks. On several tasks like MMB, MMMU, and OKVQA, Mantis-Idefics maintains a similarly strongperformance as Idefics2. However, we do observe some drops in ScienceQA, which are mainly due to a lowermix ratio of OCR and Math data in our instruction tuning. Overall, the average drop of 4% is tolerable.",
  "Ablation Studies": "Is multi-image instruction tuning necessary? To investigate, we further train a Mantis-Flamingo basedon OpenFlamingo (Awadalla et al., 2023), which has been trained on MMC4 (Zhu et al., 2023), a text-imageinterleaved pre-training dataset. As shown in , the average performance of Mantis-Flamingo increasedby 19.2 points after further fine-tuning on Mantis-Instruct. The results show that there is still a hugeimprovement space, even if a model is fully pre-trained on a multi-image dataset, which demonstrates thenecessity of learning multi-image skills in the fine-tuning phase. Is multi-image pre-training necessary? To investigate, we design an ablation study, where one modelis pre-trained on the CC3M 560K subset, which contains only image-text pairs, while the other one ispre-trained additionally on OBELICS-100K subset, which contains interleaved multi-image-text pairs. Due",
  "(with OBELICS-100K)+0.10+3.50+6.00+1.41-1.11+1.98": ": Ablation on whether multi-image pre-training is necessary. One model is pre-trained on the CC3Msubset, while the other is additionally pre-trained on the a OBELICS-100K multi-image dataset. They areboth fine-tuned on a subset Mantis-Instruct for comparison. Results show minor improvement withadditional multi-image pre-training. Each ablation model is trained with QLoRA with DoRA.",
  "+Coref84.4863.2057.1448.5849.4060.56+7.04+Compare85.9269.0053.9248.0048.3361.03+0.47+Reason89.6268.5062.2148.3147.9363.31+2.28+Temporal89.5467.0061.2949.3450.0263.44+0.12": ": Data ablation study results of different subsets of Mantis-Instruct, where each skill correspondingto a specific subsets defined in . column of each row represents the performance improvements afteradding new subsets to the last column results. Each ablation model is trained with QLoRA with DoRA. to limited computation resources, we fine-tune both models on a downsampled Mantis-Instruct dataset tocompare their performance. Results in show that there are little improvements by adding additionalpre-training on OBELICS-100K. We thus conclude that multi-image pre-training could be useful, but it isnot a necessary path for models to gain multi-image abilities. The poor performance on Mantis-Flamingoalso confirm our hypothesis. Ablation study of Mantis-Instruct components: We conduct a data ablation study to analyze theeffects of different subsets in Mantis-Instruct to enhance a models multi-image ability. As shown in, As new subsets of Mantis-Instruct are added continually, the overall performance is continuallyimproving, proving that every subset is contributing positively.",
  "Qualitative Results": "To conduct a qualitative analysis of Mantis models on specific multi-image scenarios, we include somecase studies in , where we include Emu2, which can accept a sequence of images as inputs, andLLaVA-1.5, which can only accept one image at a time, and the powerful GPT-4V, which perform well onboth single-image and multi-image scenario. The first case requires the model to count the number of dice respectively in the two images. While previousLMMs might have learned good counting ability in the single-image scenario, most of them failed in themulti-image counting problem, including GPT-4V. The second case requires the model to involve their worldknowledge to infer the mood of the character in three images. However, Emu2, and LLaVA-1.5 output similarcontents, and both fall back into the simple captioning behavior. It demonstrates that these models struggleto distinguish two images as separate information. In contrast, Mantis can correctly do the task due to theexisting of reasoning subset in Mantis-Instruct.",
  "Large Multimodal Models": "Large Multimodal Models (LMMs) are mostly denoted as large language models that can understand multiplemodalities besides the human language. Some works aim to fuse image, audio, video, or more modalities withlanguage (Wu et al., 2023b; Zhan et al., 2024), while more works mainly focus on better-combining visionknowledge with the language (Alayrac et al., 2022; Awadalla et al., 2023; Dai et al., 2023). BLIP-2 has curateda large-scale image captioning dataset, bootstrapping a language model along with a vision encoder to be apowerful multimodal model (Li et al., 2023c). Following this, LLaVA further proposes a cheaper approach totrain a powerful LMM through visual instruction tuning (Huang et al., 2016). LLaVA-Next further optimizessingle-image performance, with a cost of an increasing number of image tokens per token (Liu et al., 2024),consuming more than 2k for each image, which is about 4 times of the original LLaVA model. Later LMMslike QwenVL (Bai et al., 2023), CogVLM (Wang et al., 2023), Yi-VL (AI et al., 2024), etc. all follow a similararchitecture of LLaVA. However, real-world visual problems are way more complex and usually require asequence of images to describe the situation. Although better performance is achieved in the single-imagescenario, its doubtful whether its worth the cost.",
  "Multi-Image Supported Large Multimodal Models": "Previous works have also noticed the importance of multi-image ability.Deepminds close-sourcedFlamingo (Alayrac et al., 2022) focuses on optimizing in-context learning for a sequence of image/text pairsbut lacking free-from image-text interleaved training examples. Kosmos2 (Peng et al., 2023), Fuyu (AdeptAI, 2023) both support text-image interleaved format given the model structure, but they did not optimize inmulti-image reasoning, and the released inference codes also do not support multi-image as input. Emu2 (Sunet al., 2023) is a generative multimodal model that supports interleaved text-image inputs, as well asgenerating both images and texts. However, its mainly focused on in-context examples like Flamingo andauto-regressive image generation, instead of the 4 skills mentioned in section 1 Another common variant ofLMMs that can accept multiple images as input is video understanding models, such as Video-LLaMA (Zhanget al., 2023a). However, as shown in the MVBench benchmark, it is also shown that Video-LLaMA behavesworse than LMMs that can only accept one image as input (Li et al., 2023d), which raises doubts about itsmulti-image understanding ability. Different from these works, Mantis aims to optimize the multi-image abilityof LMMs guided by the 4 defined skills, co-reference, reasoning, comparison, and temporal understanding,thus equipping the model with better multi-image ability. Our contemporary works on optimizing interleavedmulti-image ability include LLaVA-Next-Interleave (Li et al., 2024b) and LLaVA-OneVision (Li et al., 2024a),which also collects large scale of single-image, multi-image, and video understanding data to train a more",
  "Multi-Image Evaluation": "While many benchmarks have been developed for single-image tasks, such as TextVQA (Singh et al., 2019a),VQA-V2 (Goyal et al., 2016), MMBench (Liu et al., 2023d), and so on, the multi-image evaluation did notcapture much attention until recently. Difference description is the first task which large-scale data and goodevaluation benchmarks are developed for, including works like Spot-the-Diff (Jhamtani & Berg-Kirkpatrick,2018), Birds-to-Words (Forbes et al., 2019), etc. They are usually evaluated by n-gram metrics like BLEU,and ROUGE, and only involve two images. The later NLVR2 (Suhr et al., 2018) further includes logicalreasoning across 2 images into evaluation, focusing on a higher level of cognition ability. Q-bench (Wu et al.,2023a) and BLINK (Fu et al., 2024) are both proposed for low-level multi-image vision tasks, where humanseasily do but model struggles. Another set of multi-image evaluations comes from video or image sequenceevaluation, including Mementos (Wang et al., 2024b), MVBench (Li et al., 2023d), etc. In this case, themodel usually needs to take more than 2 images to answer. Recently, there are also attempts to promptLMMs to evaluate vision-language task (Zhang et al., 2023b), or image-generation tasks (Ku et al., 2023),which also demands good multi-image reasoning ability. The accuracies of the these tasks of different LMMscan be seen as another form of benchmark for multi-image ability.",
  "Conclusion": "We propose Mantis, a new large multimodal model family designed to be able to accept interleaved text-imageinputs. Mantis is fine-tuned on our Mantis-Instruct, a text-image interleaved dataset consisting of 721Kexamples, covering 4 crucial skills, including co-reference, reasoning, comparison, and temporal understanding,for multi-image tasks. We also propose Mantis-Eval, a high-quality multi-image evaluation benchmarkconsisting of 217 examples with an average of 2.5 images per example. Comprehensive experiments show thatMantis models effectively acquire the 4 multi-image skills and achieves state-of-the-art performance on 5multi-image evaluation benchmarks, and preserve decent single-image reasoning performance. We sharedinsights about the model architecture and inference approaches based on the experiment results. Extendingthe image sequence context length and compressing the number of tokens per tokens efficiently are regardedas our future work.",
  "Limitations": "Our work mainly focuses on improving the multi-image ability of LMMs. Our collected dataset Mantis-Instructreformat some existing multi-image datasets, which might still contain some noise due to the quality of theoriginal collection. Besides, we found that most of the examples in the Mantis-Instrut tend to be multiple-choice QA questions, where the response tends to be short, resulting in the current version of Mantis modelstending to output shorter responses instead of long reasoning texts and falling short in instruction-followingability in the multi-image scenario. Although we have introduced Multi-VQA subset to alleviate this issue,this problem still exists, and we will leave it to future work by introducing more long-form multi-image data.While enhancing the multi-image ability well, we find out that there is a trend of performance degenerationin the single-image performance compared to the original model . Some single-image QA datasetshave been introduced to alleviate this issue, but the conflicts between multi-image and single-image abilityseem to exist continually. How to balance these two abilities will also be our future direction.",
  "Societal Impacts": "Our proposed model, Mantis, and released dataset, Mantis-Instruct, can have substantial impacts on societythrough the application of Large Multimodal Models. LMMs with multi-image ability can analyze, andreason in real-world scenarios, helping machines, and robots, to make decisions automatically, assisting peoplein website browsing, travel planning based on multiple maps and pictures, etc. We believe Mantis can serve",
  "as one of the first useful open-source LMMs that make progress in these applications to make these scenariospossible and efficient": "However, there is a potential that our Mantis can generate hallucinations, make the wrong decisions, andfail to reason across complex real-world scenarios. While we have tried to improve Mantiss accuracy andperformance in instruction-following and multi-image understanding by carefully training on high-qualityMantis-Instruct, Mantis can still sometimes make mistakes, thus harming its faithfulness as a useful LMMtool. There is also the potential for misuse of our model and dataset after we make them open source. Thoughwe have added proper licenses and terms of usage to our assets, misuse cannot be safely eliminated.",
  "Adept AI. Fuyu-8B: A Multimodal Architecture for AI Agents": "01. AI, :, Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Heng Li, JiangchengZhu, Jianqun Chen, Jing Chang, Kaidong Yu, Peng Liu, Qiang Liu, Shawn Yue, Senbin Yang, ShimingYang, Tao Yu, Wen Xie, Wenhao Huang, Xiaohui Hu, Xiaoyi Ren, Xinyao Niu, Pengcheng Nie, Yuchi Xu,Yudong Liu, Yue Wang, Yuxuan Cai, Zhenyu Gu, Zhiyuan Liu, and Zonghong Dai. Yi: Open foundationmodels by 01.ai, 2024.",
  "AI@Meta. Llama 3 model card, 2024. URL": "Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, ArthurMensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han,Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andy Brock, AidaNematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman,and Karen Simonyan. Flamingo: a visual language model for few-shot learning. ArXiv, abs/2204.14198,2022. URL Reza Yazdani Aminabadi, Samyam Rajbhandari, Minjia Zhang, Ammar Ahmad Awan, Cheng Li, Du Li,Elton Zheng, Jeff Rasley, Shaden Smith, Olatunji Ruwase, and Yuxiong He.Deepspeed- inference:Enabling efficient inference of transformer models at unprecedented scale. SC22: International Conferencefor High Performance Computing, Networking, Storage and Analysis, pp. 115, 2022.URL Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence Zitnick, andDevi Parikh. Vqa: Visual question answering. In Proceedings of the IEEE international conference oncomputer vision, pp. 24252433, 2015. Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe,Yonatan Bitton, Samir Yitzhak Gadre, Shiori Sagawa, Jenia Jitsev, Simon Kornblith, Pang Wei Koh,Gabriel Ilharco, Mitchell Wortsman, and Ludwig Schmidt. Openflamingo: An open-source frameworkfor training large autoregressive vision-language models. ArXiv, abs/2308.01390, 2023. URL Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou,and Jingren Zhou.Qwen-vl: A frontier large vision-language model with versatile abilities.ArXiv,abs/2308.12966, 2023. URL",
  "Lin Chen, Jinsong Li, Xiao wen Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin.Sharegpt4v: Improving large multi-modal models with better captions. ArXiv, abs/2311.12793, 2023. URL": "Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Dollr, and C. LawrenceZitnick. Microsoft coco captions: Data collection and evaluation server. ArXiv, abs/1504.00325, 2015. URL Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang AlbertLi, Pascale Fung, and Steven C. H. Hoi. Instructblip: Towards general-purpose vision-language models withinstruction tuning. ArXiv, abs/2305.06500, 2023. URL",
  "Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning of quantizedllms. ArXiv, abs/2305.14314, 2023. URL": "Maxwell Forbes, Christine Kaeser-Chen, Piyush Sharma, and Serge J. Belongie. Neural naturalist: Generatingfine-grained image comparisons. In Conference on Empirical Methods in Natural Language Processing,2019. URL Stephanie Fu, Netanel Tamir, Shobhita Sundaram, Lucy Chai, Richard Zhang, Tali Dekel, and PhillipIsola. Dreamsim: Learning new dimensions of human visual similarity using synthetic data. ArXiv,abs/2306.09344, 2023. URL",
  "Bin Lin, Bin Zhu, Yang Ye, Munan Ning, Peng Jin, and Li Yuan.Video-llava: Learning united vi-sual representation by alignment before projection.ArXiv, abs/2311.10122, 2023a.URL": "Ji Lin, Hongxu Yin, Wei Ping, Yao Lu, Pavlo Molchanov, Andrew Tao, Huizi Mao, Jan Kautz, MohammadShoeybi, and Song Han. Vila: On pre-training for visual language models. ArXiv, abs/2312.07533, 2023b.URL Ji Lin, Hongxu Yin, Wei Ping, Yao Lu, Pavlo Molchanov, Andrew Tao, Huizi Mao, Jan Kautz, MohammadShoeybi, and Song Han. Vila: On pre-training for visual language models. arXiv preprint arXiv:2312.07533,2023c.",
  "Ahmed Masry, Xuan Long Do, Jia Qing Tan, Shafiq R. Joty, and Enamul Hoque. Chartqa: A benchmark forquestion answering about charts with visual and logical reasoning. ArXiv, abs/2203.10244, 2022. URL": "Minesh Mathew, Dimosthenis Karatzas, R. Manmatha, and C. V. Jawahar. Docvqa: A dataset for vqaon document images. 2021 IEEE Winter Conference on Applications of Computer Vision (WACV), pp.21992208, 2020. URL Brandon McKinzie, Zhe Gan, Jean-Philippe Fauconnier, Sam Dodge, Bowen Zhang, Philipp Dufter, DhrutiShah, Xianzhi Du, Futang Peng, Floris Weers, et al. Mm1: Methods, analysis & insights from multimodalllm pre-training. arXiv preprint arXiv:2403.09611, 2024. Fanqing Meng, Jin Wang, Chuanhao Li, Quanfeng Lu, Hao Tian, Jiaqi Liao, Xizhou Zhu, Jifeng Dai, Yu Qiao,Ping Luo, Kaipeng Zhang, and Wenqi Shao. Mmiu: Multimodal multi-image understanding for evaluatinglarge vision-language models. 2024. URL Aitor Ormazabal, Che Zheng, Cyprien de Masson dAutume, Dani Yogatama, Deyu Fu, Donovan Ong, EricChen, Eugenie Lamprecht, Hai Pham, Isaac Ong, et al. Reka core, flash, and edge: A series of powerfulmultimodal language models. arXiv preprint arXiv:2404.12387, 2024.",
  "Christoph Schuhmann and Peter Bevan. 220k-gpt4vision-captions-from-lvis. 2023": "Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and MarcusRohrbach. Towards vqa models that can read. 2019 IEEE/CVF Conference on Computer Vision andPattern Recognition (CVPR), pp. 83098318, 2019a. URL Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, andMarcus Rohrbach. Towards vqa models that can read. In Proceedings of the IEEE/CVF conference oncomputer vision and pattern recognition, pp. 83178326, 2019b.",
  "Alane Suhr, Stephanie Zhou, Iris Zhang, Huajun Bai, and Yoav Artzi. A corpus for reasoning about naturallanguage grounded in photographs. ArXiv, abs/1811.00491, 2018. URL": "Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Zhengxiong Luo, Yueze Wang, YongmingRao, Jingjing Liu, Tiejun Huang, and Xinlong Wang. Generative multimodal models are in-context learners.ArXiv, abs/2312.13286, 2023. URL Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut,Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly capable multimodalmodels. arXiv preprint arXiv:2312.11805, 2023. Fei Wang, Xingyu Fu, James Y. Huang, Zekun Li, Qin Liu, Xiaogeng Liu, Mingyu Derek Ma, Nan Xu,Wenxuan Zhou, Kai Zhang, Tianyi Yan, Wenjie Jacky Mo, Hsiang-Hui Liu, Pan Lu, Chunyuan Li,Chaowei Xiao, Kai-Wei Chang, Dan Roth, Sheng Zhang, Hoifung Poon, and Muhao Chen. Muirbench: Acomprehensive benchmark for robust multi-image understanding. ArXiv, abs/2406.09411, 2024a. URL Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang,Lei Zhao, Xixuan Song, Jiazheng Xu, Bin Xu, Juanzi Li, Yuxiao Dong, Ming Ding, and Jie Tang.Cogvlm: Visual expert for pretrained language models. ArXiv, abs/2311.03079, 2023.URL Xin Eric Wang, Jiawei Wu, Junkun Chen, Lei Li, Yuan fang Wang, and William Yang Wang. Vatex: A large-scale, high-quality multilingual dataset for video-and-language research. 2019 IEEE/CVF InternationalConference on Computer Vision (ICCV), pp. 45804590, 2019. URL Xiyao Wang, Yuhang Zhou, Xiaoyu Liu, Hongjin Lu, Yuancheng Xu, Feihong He, Jaehong Yoon, TaixiLu, Gedas Bertasius, Mohit Bansal, et al. Mementos: A comprehensive benchmark for multimodal largelanguage model reasoning over image sequences. arXiv preprint arXiv:2401.10529, 2024b.",
  "Shih yang Liu, Chien-Yi Wang, Hongxu Yin, Pavlo Molchanov, Yu-Chiang Frank Wang, Kwang-Ting Cheng,and Min-Hung Chen. Dora: Weight-decomposed low-rank adaptation. ArXiv, abs/2402.09353, 2024. URL": "Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang,Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng,Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. Mmmu: A massivemulti-discipline multimodal understanding and reasoning benchmark for expert agi. ArXiv, abs/2311.16502,2023. URL Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language imagepre-training. 2023 IEEE/CVF International Conference on Computer Vision (ICCV), pp. 1194111952,2023. URL Jun Zhan, Junqi Dai, Jiasheng Ye, Yunhua Zhou, Dong Zhang, Zhigeng Liu, Xin Zhang, Ruibin Yuan,Ge Zhang, Linyang Li, Hang Yan, Jie Fu, Tao Gui, Tianxiang Sun, Yugang Jiang, and Xipeng Qiu.Anygpt: Unified multimodal llm with discrete sequence modeling. ArXiv, abs/2402.12226, 2024. URL",
  "Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned audio-visual language model forvideo understanding. ArXiv, abs/2306.02858, 2023a. URL": "Xinlu Zhang, Yujie Lu, Weizhi Wang, An Yan, Jun Yan, Lianke Qin, Heng Wang, Xifeng Yan, William YangWang, and Linda Ruth Petzold. Gpt-4v(ision) as a generalist evaluator for vision-language tasks. ArXiv,abs/2311.01361, 2023b. URL Wanrong Zhu, Jack Hessel, Anas Awadalla, Samir Yitzhak Gadre, Jesse Dodge, Alex Fang, Youngjae Yu,Ludwig Schmidt, William Yang Wang, and Yejin Choi. Multimodal c4: An open, billion-scale corpus ofimages interleaved with text. ArXiv, abs/2304.06939, 2023. URL Wanrong Zhu, Jack Hessel, Anas Awadalla, Samir Yitzhak Gadre, Jesse Dodge, Alex Fang, Youngjae Yu,Ludwig Schmidt, William Yang Wang, and Yejin Choi. Multimodal c4: An open, billion-scale corpus ofimages interleaved with text. Advances in Neural Information Processing Systems, 36, 2024.",
  "A.1Details of Mantis-Instruct subsets": "We construct Mantis-Instruct from multiple publicly available datasets. Detailed statistics are shown in. We use a similar dataset format with LLaVAs, where each data item contains multiple images andmultiple turns of QA pairs are gathered. We report the number of examples, the number of images per item,the average conversation turns, and the average length. We describe the task type of each dataset and theprocessing techniques used in the following: LLaVA-665k-multi: Besides the above-mentioned multi-image reasoning tasks, we also manually reformatthe LLaVA-655k dataset from LLaVA-1.5 into a \"synthetic\" multi-image dataset. Since each original data itemconsists of multiple QA pairs about a single image, we randomly merge 2 to 4 data items into a multi-imagedata item. Then we add proper image denotation like \"For the second image, ...\" for each question, and shuffleall the QA pairs to form the final multi-image data item. Answering each question in this \"synthetic\" datasetdoes not require reasoning across images, but demands high image co-reference ability. Besides, it also helpsavoid forgetting single-image ability in multi-image scenarios and increases instruction-following ability.Co-Instruct (Wu et al., 2024): Co-Instruct is a dataset curated to compare the quality of 2-4 images inopen-ended answer or multi-choice QA format. Its similar to the tasks in the Qbench (Wu et al., 2023a).NLVR2 (Suhr et al., 2018): NLVR2 is a natural language reasoning dataset grounded in images. Thetask is to determine whether a statement is true or false given the visual contents of 2 images. NLVR2 coversmany linguistic phenomena, such as cardinality, existential quantifiers, universal quantifiers, etc.Dreamsim (Fu et al., 2023): DreamSim proposes a dataset to train a model to judge image similarity.We reuse their datasets, and the task is to judge which candidate image is more similar to a reference image.ImageCoDe (Krojer et al., 2022): ImageCoDe is a multimodal challenge called image retrieval fromcontextual descriptions, where a multimodal model is required to retrieve/select the correct image from aset of 10 minimally contrastive images. For each image, 9 similar images are retrieved from Open ImagesDataset V6 (Kuznetsova et al., 2018) using CLIP encodings.Contrast-Caption: Inspired by the paradigm of contrastive learning, we reformat existing image captioningtasks into a multi-image version, where 2 tasks are defined. Given multiple images, the first task we definedis to judge which image matches the provided caption, and the second task is to generate a caption for adenoted image. We randomly select 2 to 8 images along with their captions to form a single item, then applythe template of the 2 tasks defined. We used the images and captions provided by ShareGPT-4V (Chen et al.,2023) and LAION GPT-4V captions from LVIS (Schuhmann & Bevan, 2023).Spot-the-diff (Jhamtani & Berg-Kirkpatrick, 2018): This dataset requires the model to generate thedifference description given 2 images. We use the processed version from MIMIC-IT (Li et al., 2023a).LRV-multi (Liu et al., 2023a): This is a single-image dataset used to mitigate hallucinations of LMMs.We process it similarly to LLaVA-665k-multi to get a \"synthetic\" multi-image dataset.Birds-to-Words (Forbes et al., 2019): The task of this dataset is to generate different descriptionsand give 2 images of different birds. We directly take the training split for training. Besides, we promptGPT-3.5-turbo to generate a possible question given the reference answer, so we can get a more diversequestion pool instead of always asking the model to generate the difference description. Multi-VQA: Thisdataset contains multi-image reasoning QA pairs, where each question is generated by GPT-4 based on thecaptions of the images. Each question is required to involve at least two images. Images and captions aresampled from ShareGPT-4V (Chen et al., 2023). The prompt template is in .Video QA: We also include some video question answering datasets, including NExTQA (Xiao et al.,2021), STAR (Wu & Yu, 2021), and visual-story-telling (Huang et al., 2016). nextqa and star are theprocessed version from Flipped-VQA (Ko et al., 2023). The visual-story-telling are the processed versionfrom MIMIC-IT (Li et al., 2023a).Single-image-VQA: Instead of focusing only on multi-image tasks, we claim that its necessary to alsoinclude some single-image VQA dataset to avoid catastrophic forgetting on single-image tasks. We includedsome of the single image data from LLaVA-665k as well as DocVQA (Mathew et al., 2020), DVQA (Kafleet al., 2018), and ChartQA (Masry et al., 2022) to enhance its ability on diagrams and OCR. In practice,since the size of DVQA is too large, we only use 30k out of 200k examples for the training.",
  "A.2Investigation of Mantiss capability on open-ended tasks": "To assess Mantiss performance on open-ended generation tasks, we evaluated it on image captioning (usingthe CoCo-2017-Lite dataset) (Chen et al., 2015) and video captioning (using the Vatex dataset) (Wang et al.,2019). For evaluation, we utilized lmms-eval Li* et al. (2024). The results are presented in and. As shown in these tables, Mantis-Idefics2s captioning performance improved after training onMantis-Instruct compared to its previous checkpoint Idefics2, though still fall behind of the expert captioningmodels in these areas. We attribute this improvement to the design of our dataset, which encourages the model to differentiatebetween images and better understand semantic information within visual content. Notably, contrastivelearning of this kind is often limited in vision-language pre-training datasets, so fine-tuning on Mantis-Instructappears to enhance the models comprehension of visual content further.",
  "While claiming Mantis to be a model that can accept text-image interleaved inputs, its interesting toinvestigate how Mantis will perform in long-context scenarios": "We first conduct experiments to see how Mantiss performance will change as the number of images/framesincreases. We run Mantis-Idefics2 on MVBench using 2/4/8/16 frames and report the performance in. It turns out that Mantiss performance is stable across various numbers of sampled frames. Whatsworth noting is that Mantis can use only 2 frames to achieve 48.38 accuracy, which means that most of thequestions in the MVBench can be answered by keyframes. As the number of frames goes up from 2 to 8,the accuracy also goes up to 51.38 accuracy. The performance under 16 samples is 1 point lower than theperformance under 8 frames, we hypothesize that the redundant information might distract Mantes fromgiving the current answer.",
  ": Mantis-Idefics2s performance on MVBench with different number of uniformly sampled frames": "As shown in , MileBench consists of examples with an average of 15 images per example and asequence length ranging from 1,000 to 9,000 tokens, making it well-suited for evaluating Mantiss long-contextability. As shown in , Mantis-SIGLIP achieves 47.5 on MileBench, only 5.5 points behind GPT-4V.We do believe this is good evidence that Mantis maintains competitive multimodal long-context ability.",
  "A.4Comparison with MIMIC-IT": "MIMIC-IT is also a multi-modal dataset that aims to boost the models ability for in-context learning, whichalso contains multiple images for most of the QA pair (Li et al., 2023a). To investigate how well Mantis-Instruct outperforms MIMIC-IT, we conduct a fair comparison between the Otter and Mantis-Flamingo.As shown in , Otter is tuned from the Open-Flamingo using MIMIC-IT as the instruction-tuningdataset, and one of our Mantiss variants, Mantis-Flamingo, is also tuned from the Open-Flamingo but insteadusing our Mantis-Instruct as the instruction-tuning dataset. Therefore, Mantis-Flamingo and Otter are afair comparison where they share the same pre-training dataset, MMC4+LAION, and trained on differentinstruction-following datasets, MIMIC-IT and Mantis-Instruct, respectively. We report the performance in . It turns out that Mantis-Flamingo is better than Otter in all thesebenchmarks, based on the same pre-trained model but different instruction-tuning datasets. This proves thesuperior quality of Mantis-Instruct compared to the MIMIC-IT. This is mainly because Mantis-Instruct isdesigned to equip models with the 4 essential abilities for multi-image reasoning, while MIMIC-IT is mainlydesigned for multimodal in-context reasoning."
}