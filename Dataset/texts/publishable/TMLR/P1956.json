{
  "Abstract": "Current machine learning methods struggle to solve Bongard problems, which are a type ofIQ test that requires deriving an abstract concept from a set of positive and negative sup-port images, and then classifying whether or not a new query image depicts the key concept.On Bongard-HOI, a benchmark for natural-image Bongard problems, most existing methodshave reached at best 69% accuracy (where chance is 50%). Low accuracy is often attributedto neural nets lack of ability to find human-like symbolic rules. In this work, we point outthat many existing methods are forfeiting accuracy due to a much simpler problem: they donot adapt image features given information contained in the support set as a whole, and relyinstead on information extracted from individual supports. This is a critical issue, becausethe key concept in a typical Bongard problem can often only be distinguished using multi-ple positives and multiple negatives. We explore simple methods to incorporate this contextand show substantial gains over prior works, leading to new state-of-the-art accuracy onBongard-LOGO (75.3%) and Bongard-HOI (76.4%) compared to methods with equivalentvision backbone architectures and strong performance on the original Bongard problem set(60.8%). Code is available at",
  "Introduction": "In the 1960s, Mikhail Bongard published a set of puzzles designed to highlight visual reasoning capabilitiesthat computers lack (Bongard, 1968). The idea is to present two sets of images, where in the first set thereis some abstract concept shared across all images (e.g., each image shows a triangle), and in the second set,that key concept is missing from all images (e.g., each image shows a non-triangle polygon). The goal is toname the concept in play. These puzzles can be made arbitrarily diverse and complex, making them usefulintelligence tests for machines and humans alike (Hofstadter, 1979). A sample problem is shown in . Now 70 years later computer vision systems still lack the capability to reliably solve such puzzles (Mitchell,2021). Recent work deals with simplified versions of the task, such as Bongard-LOGO (Nie et al., 2021)and Bongard-HOI (Jiang et al., 2023), where the goal is to classify whether or not a query image satisfiesthe concept in play (i.e., few-shot classification, as opposed to naming the abstract concept). Even in thissimplified task, average accuracy has often only reached the 55-65% range (Nie et al., 2021; Jiang et al.,2023). This evaluation includes popular techniques such as contrastive learning (He et al., 2020) and meta-learning (Lee et al., 2019; Chen et al., 2021), which have seen success in other few-shot learning problems.",
  "(d) Query 2": ": Sample problem from Bongard-HOI. The positives share a concept, and the negatives lackthat concept. Considering all supports and one query at a time, is the query a positive or negative exampleof the key concept? Our models outputs, and ground truth, are in the footnote1. Several recent works have pointed to low accuracy in Bongard problems as a reflection of shortcomingsin the deep learning paradigm overall (Greff et al., 2020; Mitchell, 2021; Nie et al., 2021; Jiang et al.,2023). While this may still be an accurate assessment, we propose that neural nets ability to solve Bongardproblems has been under-estimated, due to subtle errors in setup. In our experiments, we revisit multiplebaseline approaches and demonstrate that they can be improved up to 5-10 points with a very simple change:standardize the feature vectors computed from the images, using a mean and variance computed within eachproblem. This change has a big effect because it incorporates context gathered across the other examplesin the problem. Incorporating set-level knowledge is a known technique in few-shot learning literature (Yeet al., 2020), though not currently used in state-of-the-art methods (Hu et al., 2022). In Bongard problems,however, using set-level knowledge is critical, because the tasks are often only solvable by using multiplesupports, including both positives and negatives. We name this set-level knowledge support-set context. This paper has two main contributions. First, we demonstrate the effectiveness of a simple parameter-freetechnique to incorporate this set-level knowledge, which we call support-set standardization. Using support-set standardization, we re-evaluate the effectiveness of standard few-shot learning methods on Bongard prob-lems, yielding a slate of new baselines with accuracies substantially higher than previously reported. Second,we explore a Transformer-based approach to learn set-level knowledge, and use explicit set-level trainingobjectives, which force the Transformer to extract rules or prototypes from the available supports. Our Transformer-based approach sets a new state-of-the-art on Bongard-LOGO (75.3%) and Bongard-HOI(76.4%) compared to approaches with equivalent backbone architectures, and also performs fairly well onthe original Bongard problem set (60.8%). 1The concept in play is read laptop. Our best model (SVM-Mimic, introduced in a later section) predicts a positivescore (8.7) for the first query, and a negative score (-8.3) for the second query, which is correct.",
  "Related Work": "Puzzle solvingVisual puzzles are a useful tool for measuring progress on high-level desiderata for machine-learned models, such as few-shot concept learning, logical reasoning, common sense, and compositional-ity (Greff et al., 2020).A popular area of work revolves around Ravens Progressive Matrices (Raven,1956), which are a multiple-choice IQ test where the correct choice is indicated by a subtle pattern incontext images, and sometimes clarified by an elimination procedure on the choices. Researchers havedevised large-scale auto-generated datasets in this style (Barrett et al., 2018; Zhang et al., 2019; Spratleyet al., 2020), opening the door to large-scale learning-based models, and revealing shortcomings of existingend-to-end methods. Bongard problems (Maksimov & Bongard, 1968) were designed from the outset tomeasure deficiencies in computer vision models (as opposed to humans), but similarly measure abstractreasoning capability (Linhares, 2000). Powerful specialized algorithms have been proposed for solving theoriginal collection of Bongard problems (Grinberg et al., 2023; Foundalis, 2006; Depeweg et al., 2018; Son-wane et al., 2021; Youssef et al., 2022). Our own interest is not in these particular puzzles, but rather tomake progress on the core abilities required to solve such puzzles in general. In line with this perspective,researchers have produced auto-generated large-scale Bongard datasets, such as Bongard-LOGO (Nie et al.,2021), where the images consist of compositions of arbitrary geometric shapes and strokes, and Bongard-HOI (Jiang et al., 2023), where the images consist of real-world photos of humans interacting with variousobjects. The dataset authors have benchmarked a variety of approaches, with these baselines top accuraciesreaching approximately 66% in Bongard-LOGO and Bongard-HOI, attesting to the difficulty of the task. Few-shot learningSolving a Bongard puzzle essentially requires few-shot learning. In many few-shotlearning settings, the target concept is indicated only by positive samples, but the concept can also bedistinguished from previously-learned concepts (Miller et al., 2000; Fei-Fei et al., 2006; Lake et al., 2015). Ina Bongard problem, the concept is indicated by examples and counter-examples, but any given concept mayoverlap with previous concepts in arbitrary ways. Promising methods here include meta-learning (Thrun &Pratt, 2012; Finn et al., 2017) and prototype learning (Vinyals et al., 2016; Snell et al., 2017). One approachwith good results on both Bongard-LOGO (Nie et al., 2021) and Bongard-HOI (Jiang et al., 2023) is Meta-Baseline (Chen et al., 2021), which first learns a classifier for a set of base concepts, and then meta-learnsan objective which seeks similarity between a query and its corresponding support set. Similar to this paper,Ye et al. (2020) propose a set-to-set approach, where support vectors are adapted using set-level contextwith a Transformer, and these are sent to a downstream classification algorithm. Our work has similarmotivations, but we use standardization as a feature adaptation technique and use a student-teacher setupto force the Transformer to learn a robust classifier. Vision-language modelsContrastive Language-Image Pre-training (CLIP) (Radford et al., 2021) jointlylearns an image encoder and text encoder, and trains for contrastive matching of these encodings, using anenormous private dataset of image-text pairs. An exciting result from that work is prompt engineering,whereby a new visual concept can be acquired zero shot (in the sense that no visual examples are provided),by simply providing it with a short text description (or prompt). Numerous methods have applied CLIPto Bongard problems. Shu et al. (2022) proposed Test-time Prompt-Tuning (TPT), which performs prompttuning on CLIP at test time, so as to maximally retain generalization capability to unseen data. While TPToptimizes inputs to CLIPs text encoder, BDC-Adapter (Zhang et al., 2023) trains additional small adapternetworks on top of frozen features to improve visual reasoning performance. The Augmented Queriesapproach proposed by Lei et al. (2023) makes use of CLIP image and text features for data augmentationbut trains a separate encoder for query classification. At the time of writing, TPT, BDC-Adapter, andAugmented Queries perform the best on Bongard-HOI, each attaining between 66% and 69% accuracy. Ourproposed methods build upon CLIP similarly to methods such as BDC-Adapter, but attain much betterperformance as we will later show. Large autoregressive vision-language models also can be applied toward Bongard problems. For example,GPT-4V (Achiam et al., 2023) can be adapted to the task; Zhao et al. (2023) also provide a vision-languagemodel designed to have strong performance on multimodal in-context learning tasks, including Bongard-HOI.",
  "Problem Definition": "A Bongard problem is a type of puzzle which tests the ability to learn concepts from a limited number ofvisual examples. A problem consists of a set of positive sample images PI = {IP1 , . . . , IPK}, a set of negativesample images NI = {IN1 , . . . , INK }, and a query image IQ. Typically, the size of each set is small, for exampleK = 6. Images in PI all include a specific visual concept (e.g., read laptop in ), and images in NIdo not include that concept. The positive and negative samples are also called supports, or the supportset. The task is to classify the query image as positive or negative, using the support set as a guide.",
  "Vision Backbone": "Our method begins with a vision backbone, which is a model that maps the set of input images into aset of feature vectors. These vectors will be used in the subsequent sections to attempt to solve Bongardproblems. We define an image encoder , which maps an image I RHW 3 to a vector f RC. We pass everyimage (positives, negatives, and the query) through the encoder, yielding a set of positive features Pf ={f P1 , . . . , f PK}, a set of negative features Nf = {f N1 , . . . , f NK }, and a query feature f Q. We would like for dot products in feature space to indicate some conceptual similarity. Metric learningmethods, such as contrastive learning (Chen et al., 2020a) and triplet loss functions (Schroff et al., 2015),can provide this property. For Bongard problems with natural images, we find that the visual encoder fromCLIP (Radford et al., 2021), which is trained via a contrastive loss on a large dataset of Internet images,performs well. For Bongard problems with more abstract shapes (such as the line drawings in Bongard-LOGO (Nie et al., 2021)), we find that CLIP performs poorly due to the distribution shift from its trainingdataset, and we therefore train a ResNet (He et al., 2016) from scratch on the training set with a contrastiveloss. For fairest comparison with prior works, we use the modified ResNet architecture from Nie et al. (2021)with 15 convolutional layers, which they name ResNet-15. For a pair of vectors fi, fj where both are in Pf,we encourage the vectors to be similar. For any pair of vectors fi, fk where fi Pf and fk Nf, we ask thevectors to be dissimilar. More formally, taking a positive pair of indices (i, j), where fi, fj Pf and i = j,we define a loss i,j as:",
  "Support-Set Context via Standardization": "The simple baselines in the previous subsection actually miss important contextual information, due tothe fact that the features produced by the vision backbone are all independent. helps illustratethe importance of considering context from multiple supports: observing that a laptop is involved in allsupports (positive and negative), a human may effectively ignore this visual attribute and pay attention toother details to solve the problem. This motivates in-context feature adaptation, which we define as theadaptation of Pf, Nf, and f Q within a single problem, using context contained in the support set Pf Nf. A simple way to perform in-context feature adaptation is to standardize the mean and variance across thesupport set, for each feature dimension independently. This would accentuate what is unique in the positivesvs. the negatives if there are any similar feature dimensions prior to standardization. Taking the full set ofpositive and negative support features (within a problem) as Sf = Pf Nf, we compute the element-wisemean RC and standard deviation RC of this set, and standardize all vectors using those statistics:fi = (fi)/. The query f Q is standardized using and as well but does not contribute to the statisticalcalculation. In principle this standardization step could be applied anywhere in the pipeline, but we find it is convenientto apply it immediately after the vision backbone. That is, after we convert the set of images to a setof feature vectors, we standardize the set, and proceed with a classifier. Although standardization usingsupport-set statistics has been applied in the past to few-shot problems (Bronskill et al., 2020; Nichol et al.,2018; Wang et al., 2019), it is to the best of our knowledge frequently missed in state-of-the-art methods inthe few-shot (e.g., Hu et al. (2022)) and Bongard literature. As we will demonstrate, this simple learning-freestep improves results substantially.",
  "Support-Set Context via Transformer": "We next turn to the problem of attending to support-set context using a learned algorithm. The statisticalstandardization from the previous section can be seen as a learning-free method, which operates one problemat a time, with the aim of improving the success of a downstream classifier. With access to a whole datasetof problems (as is the case in Bongard-LOGO and Bongard-HOI), it should be possible to learn priors, andalso absorb the classifier into the learner. We use a Transformer encoder (Vaswani et al., 2017) to incorporate support-set context through self-attention. The main input to our Transformer encoder is the set of support vectors fi Pf Nf (afterstatistical standardization); each support vector acts as a token in the Transformer. To inform the Trans-former on the labels of the supports, we learn indicator vectors for positive and negative, and add thecorresponding indicator to each support. We additionally input one or two learnable task tokens (i.e., freevariables in the overall optimization), to represent the classifier at the output layer. We would like the Transformer to learn a process that converts the supports into a classifier, or rule, takinginto account not only the specific supports given, but also knowledge acquired across a training dataset. Toachieve this, we propose a kind of student-teacher approach, where the student model must perform thesame task as the teacher but using only partial (or noisy) information, forcing it to learn a prior. Wepropose to use our simple baseline classifiers as teachers. Specifically, we explore two variations: 1. Prototype-Mimic: We ask the Transformer to regress class prototypes. Specifically, we computetwo ground-truth prototypes p and n by taking the element-wise mean of the standardized positiveand negative supports, respectively. We train the Transformer with two additional learnable tasktokens and regress the output tokens at these positions, which we name p and n, to match theground-truth p and n prototypes. Our loss is formulated using a cosine similarity:",
  "Input: labelled visual embeddings for supports Output: hyperplane for classifying the concept": ": Vision Backbone and SVM-Mimic. Given support images labelled positive and negative,our goal is to obtain a hyperplane which can accurately classify new query images. We use a vision backboneto obtain an embedding for each support image, and then feed these in parallel to a Transformer encoderand to an SVM. Both modules output a hyperplane, and we penalize the cosine distance between them. Fornatural images, we use the encoder from CLIP; for geometric drawings, we train an encoder from scratch. 2. SVM-Mimic: We regress an SVM hyperplane computed over the standardized supports witha Transformer. Specifically, we compute a ground-truth hyperplane h by fitting an SVM to theBongard problem, which produces the coefficients and intercept of the decision boundary. We useone learnable task token to estimate the hyperplane, and train its output h with cosine similarity:",
  "where again h and h are vectors. This approach is illustrated in": "Crucially, we use an asymmetric student-teacher setup: the teacher (i.e., averaging or SVM) uses all availablesupports in the Bongard problem, yielding clean training targets. In fact, we allow the teachers to use theproblems queries f Q as additional supports to produce the strongest possible targets. The student (i.e., theTransformer) receives only a random subset of supports, forcing it to learn to complete missing information(i.e., the remaining supports and the queries in the problem). We refer to this step as support dropout.To make the learners task even more challenging, we use label noise: we randomly flip the positive/negativeindicators for some supports (but leave the majority intact, to keep the problem well-posed). These trainingdetails encourage the Transformer to not only match the simple baseline teachers, but exceed them interms of robustness to number of supports and errors in labels. It is unconventional to use a student (in this case the Transformer) that contains many more learnableparameters than the teacher (Prototypes or SVM). However, by making the students task harder thanthe teachers task and training the Transformer across a dataset of Bongard problems, we intend for theTransformer to learn priors that generalize across Bongard problems and make it more robust than an SVMor Prototype baseline. We later show that the Transformer outperforms the SVM and Prototype baselines,which cannot be trained across the entire dataset of Bongard problems. A unique aspect of our approach is that we directly supervise the learning of (ideal) prototypes, or (max-margin) rules, with a regression loss. This is instead of supervising for the correct classification of queriesas Ye et al. (2020) do, with for example a cross entropy loss. When training a ResNet backbone from scratch, we train both the backbone and the Transformer jointly bysumming both loss terms. Note that it is possible to allow gradients to flow through the Transformer intothe vision backbone, but for simplicity we omit this.",
  "MethodEnc.Free-FormBasicCombinatorialNovelAverage": "SNAIL (Mishra et al., 2018)RN1256.3 3.560.2 3.660.1 3.161.3 0.859.5ProtoNet (Snell et al., 2017)RN1264.8 0.972.4 0.862.4 1.365.4 1.266.3MetaOptNet (Lee et al., 2019)RN1260.3 0.671.7 2.561.7 1.163.3 1.964.3ANIL (Raghu et al., 2020)RN1256.6 1.059.0 2.059.6 1.361.0 1.559.1Meta-Baseline-SC (Chen et al., 2020b)RN1266.3 0.673.3 1.363.5 0.363.9 0.866.8Meta-Baseline-MoCo (Nie et al., 2021)RN1265.9 1.472.2 0.863.9 0.864.7 0.366.7WReN-Bongard (Barrett et al., 2018)RN1250.1 0.150.9 0.553.8 1.054.3 0.652.3CNN-Baseline (Nie et al., 2021)RN1251.9 0.556.6 2.953.6 2.057.6 0.754.9Meta-Baseline-PS (Nie et al., 2021)RN1268.2 0.375.7 1.567.4 0.371.5 0.570.7TPT (Shu et al., 2022)CLIP52.565.958.656.358.3PredRNet (Yang et al., 2023)Other74.6 0.375.2 0.671.1 1.568.4 0.772.3 PrototypeRN1268.9 0.568.8 1.664.7 0.866.2 0.867.1 0.7Prototype + standardizeRN1272.9 0.580.8 0.566.8 1.171.3 1.473.0 0.8SVMRN1265.7 0.475.3 1.365.4 2.070.4 1.669.2 1.1SVM + standardizeRN1272.2 0.383.8 0.968.3 1.671.8 1.274.0 0.5k-NNRN1262.1 0.769.3 0.962.8 0.663.3 0.964.4 0.4k-NN + standardizeRN1273.0 0.684.9 0.867.8 1.871.0 1.374.2 0.5Prototype-MimicRN1273.1 0.480.9 0.668.4 1.172.0 0.973.6 0.3SVM-MimicRN1273.3 0.384.3 0.869.4 0.874.2 0.375.3 0.1",
  "Experiments": "To train our models, we used the AdamW optimizer (Loshchilov & Hutter, 2017), a maximum learning rateof 5e5, and a 1-cycle learning rate policy (Smith & Topin, 2019), increasing the learning rate linearly forthe first 5% of the training steps. We ran all experiments on NVIDIA Titan RTX, Titan XP, GeForce, orA5000 GPUs. Each experiment used at most 1 GPU and 30GB of GPU memory.",
  "Bongard-LOGO": "DatasetWe evaluate on the Bongard-LOGO dataset (Nie et al., 2021), which consists of synthetic imagescontaining geometric shapes similar to those in the original Bongard problems. We follow previous works (Nieet al., 2021) and use a ResNet-15 (He et al., 2016) trained from scratch. Each Bongard-LOGO problemconsists of seven positive and seven negative examples. The first six positives and first six negatives arechosen to serve as supports, and the remaining two are used as queries. We used the dataset splits defined by the dataset authors. The train, validation, and test sets consist of9300, 900, and 1800 problems, respectively. The test set is further subdivided into four splits, each of whichassesses various forms of out-of-domain generalization. In the basic shape split, each image contains twoshapes that were seen separately in training but were not seen together. In the free-form shape split,each image contains a free-form shape that was not seen in training. The combinatorial abstract shapesplit uses seen shape attributes (e.g., has four straight lines) in unseen combinations. The novel abstractshape split is similar, except one of the concepts was never seen in training. We used the validation set toselect hyperparameters and report results on the test set splits.",
  "Human (Jiang et al., 2023)87.2190.0193.6194.8591.42": "Training DetailsWe train models for 500,000 iterations with a batch size of 2. We resize images to512 512 pixels and apply random cropping and horizontal flipping to augment.We train all modelsusing support dropout. Label noise did not improve validation results (likely because the ground truth iserror-free), so we omit it. Results shows results on Bongard-LOGOs four test splits. We observe that our simple base-lines (Prototype, SVM, and k-NN) coupled with our contrastive encoder match or exceed most previousapproaches. Incorporating standardization leads to a 5-10 point boost for each of them. The support-setTransformer models, Prototype-Mimic and SVM-Mimic, perform better than the non-Transformer variants.SVM-Mimic performs the best, with an accuracy of 75.3%. This is 3 points better than the best prior methodPredRNet (Yang et al., 2023) and fewer than 8 points behind human amateurs. While PredRNet performswell on Bongard-LOGO, it is not straightforward to extend it to natural image tasks like Bongard-HOI,since it cannot leverage arbitrary vision backbones, and relies on training end-to-end from scratch. Resultsvary substantially between the four test splits as each split consists of a different distribution shift, requiringa different type of problem-solving of varying difficulty (humans find the combinatorial and novel abstractshape tests the hardest). Concurrently to the preparation of our work, Song and Yuan introduced threemethods that achieve average accuracies in the 80% range on Bongard-LOGO (Song & Yuan, 2024a;b;c).We do not compare to these methods as they were evaluated using larger vision backbones (ResNet-18 andabove). Our methods can be added to diverse baselines, as demonstrated in , so it is possible thatSong and Yuans methods would be improved by incorporating support-set standardization or Transformers.In contrast to us, Song and Yuan do not evaluate on natural image tasks such as Bongard-HOI.",
  "Bongard-HOI": "DatasetBongard-HOI (Jiang et al., 2023) structures Bongard problems around natural images of human-object interaction. The positive support set of each problem involves a human interaction with an object, andthe negative support set contains different interactions with the same object. Following prior works (Shuet al., 2022; Zhang et al., 2023), we use CLIP with a ResNet-50 backbone as our image encoder.TheBongard-HOI dataset is structured similarly to the Bongard-LOGO dataset, with seven positives and sevennegatives per problem. Since the dataset does not specify the support/query split for each problem, wearbitrarily select the final positive and negative images as queries.",
  "Published in Transactions on Machine Learning Research (10/2024)": "One limitation of our approach is sensitivity to the choice of vision backbone. The Transformer learns tobe robust to errors or ambiguities in the individual representations, but with stronger representations thismay not be needed. This explains why Prototype-Mimic fails to improve upon Prototype + standardize forthe DINO and PMF encoders (Tables 4 and 5), which both attain stronger baseline performance than CLIPdoes. Another limitation is that the Transformer is learned and may not generalize to new distributionsupon which no training set exists. This explains why the Transformer does not improve upon baselines inBongard-Classic, where we did no training. Finally, we note that our Bongard-LOGO results appear tohave benefited greatly from our contrastive learning stage, and perhaps other methods could be improvedby incorporating this feature-learning technique.",
  "Bongard-Classic": "DatasetMikhail Bongards original dataset (Foundalis, 2006; Yun et al., 2020) is a challenging dataset ofroughly 200 problems with no training or validation set. Since there is no training set, we directly evaluatebackbones and support-set Transformers pre-trained on Bongard-LOGO. We perform the same data pre-processing steps as done for Bongard-LOGO. Since the dataset has six positive and six negative examplesfor each problem, we select one positive and one negative example as queries. For each Bongard problem, weaverage results over every possible choice of positive and negative query and report accuracy, i.e., the percentof correct predictions. To our knowledge, prior works do not report results using accuracy as we define it.Grinberg et al. (2023) do not actually solve a query classification task as we do (and as done in Bongard-HOIand -LOGO) and instead search for a discrete rule which satisfies all supports, and they report the successrate of this search. Yun et al. (2020) solve a query classification task but use new manually created queriesthat they did not publish along with their work. We believe our accuracy metric is more reproducible. Results shows the results of our approaches on Bongard-Classic. Due to the evaluation differencesmentioned, we are unable to compare with other works but hope our results can serve as baselines forfuture work. Standardization leads to improvements in every case. Support-set Transformers do not lead toconsistent improvements, which is expected as the Transformer has not been fine-tuned on Bongard-Classicand encounters a distribution shift. SVM-Mimic is our second-best method, attaining 60.84% accuracy.",
  "Prototype72.51Prototype + standardize73.88SVM73.29SVM + standardize73.54k-NN68.34k-NN + standardize71.76Prototype-Mimic72.66 0.30SVM-Mimic74.08 0.10": "to our approach. shows results from combining our methods with PMF, evaluated on Bongard-HOI.PMF has not previously been applied to Bongard problems, and it cannot be fairly compared with the meth-ods in due to the fine-tuned rather than frozen encoder. We combine support-set standardization,support-set Transformers, and our baselines with PMF by first fine-tuning the CLIP encoder with the PMFobjective, freezing the weights, and then plugging the fine-tuned encoder directly into each method. PMFperforms very well, exceeding the results obtained with frozen CLIP. Support-set standardization and Trans-formers lead to further improvements. One failure case is Prototype-Mimic; note that the PMF trainingobjective already optimizes the prototypes, so it is possible that the forward pass by the Transformer is notnecessary. SVM-Mimic combined with PMF performs best. PMF training details are in the Appendix.",
  "DINO Backbone": "We next use a DINO ViT-base backbone (Caron et al., 2021) instead of a CLIP-based one. containsresults with a DINO backbone on Bongard-HOI. Support-set standardization leads to consistent performanceimprovements for each of the three baselines. SVM-Mimic achieves a further performance boost of 0.54 pointscompared to SVM + standardize. One exception to the performance improvements is Prototype-Mimic,which fails to improve upon Prototype + standardize.",
  "Robustness": "To further demonstrate the success of our methods, we measure the robustness of SVM-Mimic.First,we report robustness to the number of support samples provided as input. shows the results ofSVM-Mimic, a version of SVM-Mimic trained without support dropout or label noise (SVM-Mimic w/oReg), and SVM baseline + standardization (SVM + Std) on the test splits of Bongard-HOI. SVM-Mimic",
  "(d) Seen Act, Seen Obj": ": Robustness to number of supports in Bongard-HOI. The x-axis measures the number ofsupports of each class seen, with 6 being the default setting, and the y-axis measures accuracy. Means andstandard deviations are across three runs, where randomness is with respect to the subset of supports chosen. 57.5 60.0 62.5 65.0 67.5 70.0 SVM-MimicSVM-Mimic w/o RegSVM + Std",
  ": Robustness to label noise in Bongard-HOI. The x-axis measures the number of supports ofeach class that have not been noised. Other details are the same as in": "attains consistently higher accuracy than the other approaches for different numbers of supports. demonstrates the robustness of SVM-Mimic to noise in support labels, where noise is created by flipping thelabels for random choices of positive and negative supports. In almost all cases, SVM-Mimic is clearly morerobust than SVM-Mimic w/o Reg and SVM + Std. These results demonstrate the benefits of learning aTransformer to produce rules rather than non-parametrically fitting an SVM to each Bongard problem.",
  "Comparison to Autoregressive Vision-Language Models": "Large autoregressive vision-language foundation models such as GPT-4V (Achiam et al., 2023) and GPT-4o (OpenAI, 2024) have demonstrated strong reasoning capabilities, which raises the question: how well doour simple methods compare to these state-of-the-art foundation models on Bongard problems? GPT-4o isexpensive to evaluate, so we evaluated it on the dataset where we have the most baselines, Bongard-LOGO.GPT-4o performs poorly compared to SVM-Mimic, as shown in . This may be partly due to thesignificant distribution shift between Bongard-LOGO (synthetic abstract images) and GPT-4os trainingdataset (likely Internet images), so it is possible that GPT-4o would perform better on Bongard-HOI. Inconcurrent work by Zhao et al. (2023), a new vision-language foundation model MMICL was proposed, andevaluated on a subset of Bongard-HOI. We compare this value to SVM-Mimic + PMF in , and findthat SVM-Mimic performs more than 2 points better than MMICL. MMICL, which uses InstructBLIP (Daiet al., 2023) as its backbone, likely does not suffer the same distribution shift on Bongard-HOI as GPT-4odoes on Bongard-LOGO, so it is notable that our simple methods still outperform it. Implementation detailsWe used the gpt-4o model in the OpenAI API. To have the model classifya query image, we included a prompt with all support images and labels, the unlabeled query, and anexplanation of the classification objective. We experimented with multiple variations of the prompt on asubset of the Bongard-LOGO validation set, including (i) instructing the model to think step-by-step priorto guessing (Kojima et al., 2022) and (ii) few-shot prompting (Brown et al., 2020), where the shots arethe support examples. The results in use few-shot prompting, which gave the best performance onthe validation subset. The final prompt is in the Appendix.",
  "Ablation Study: Forms of Normalization": "Can support-set standardization be replaced with other forms of centering and scaling? comparesthe performance of different forms of normalization on Bongard-HOI. Support-set standardize is our mainapproach; train-set standardize computes mean and variance statistics using the entire training set ofCLIP embeddings; l2 normalization normalizes each embedding independently. Importantly, neither train-setstandardization nor l2 normalization adapts features using task-level context (instead focusing on the dataset-level and image-level). Train-set standardization leads to a 0.1 point gain over support-set standardizationwhen using SVMs, but it performs 1 point worse with Prototypes and over 3 points worse with k-NN,suggesting that support-set standardization is a safe choice.We also find that l2 normalization harmsperformance on the SVM, despite the fact that this is a design step included by default in Prototype andk-NN methods for computing cosine similarity.",
  "Discussion and Conclusion": "In this work, we point out that simple inductive biases to incorporate support-set context can lead to signif-icant performance improvements on Bongard problems. We demonstrate that support-set standardization,coupled with simple baselines, in many cases achieves state-of-the-art performance on Bongard-LOGO andBongard-HOI. Second, we explore learning to attend to support-set context with a Transformer and showthat this leads to further improvements in accuracy. After controlling for vision backbone architecture, ourbest model, SVM-Mimic, sets a new state-of-the-art on Bongard-LOGO and Bongard-HOI.",
  "Overall, our experiments attest to the importance of support-set context for Bongard problems. We hopethat our results, our models, and our code, will facilitate future work in this domain": "Broader impactOur work describes improvements to make neural networks better at a certain visual IQtask. As AI systems approach human accuracy on this and other tasks, AI researchers will need to carefullyconsider the risks associated with having machines take over work traditionally performed by humans, such asthe economic and emotional impact on the individuals that would normally do the work. As the limitationsof AI systems are lifted, particularly in symbolic reasoning tasks such as those considered here, there iseventual risk of superhuman general-purpose task-solving, for which safeguards will need to be set in place.These ethical concerns are not limited to Bongard problems in particular. Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, DiogoAlmeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al.Gpt-4 technical report.arXivpreprint arXiv:2303.08774, 2023.",
  "Mikhail Moiseevich Bongard. The recognition problem. Foreign Technology Div Wright-Patterson AFBOhio, 1968": "John Bronskill, Jonathan Gordon, James Requeima, Sebastian Nowozin, and Richard Turner. Tasknorm:Rethinking batch normalization for meta-learning. In International Conference on Machine Learning, pp.11531164. PMLR, 2020. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, ArvindNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.Advances in neural information processing systems, 33:18771901, 2020. Mathilde Caron, Hugo Touvron, Ishan Misra, Herv Jgou, Julien Mairal, Piotr Bojanowski, and ArmandJoulin.Emerging properties in self-supervised vision transformers.In Proceedings of the IEEE/CVFinternational conference on computer vision, pp. 96509660, 2021. Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastivelearning of visual representations. In International conference on machine learning, pp. 15971607. PMLR,2020a.",
  "VV Maksimov and MM Bongard. A program that learns to classify geometrical figures. IFAC ProceedingsVolumes, 2(4):771775, 1968": "Thomas Mensink, Jakob Verbeek, Florent Perronnin, and Gabriela Csurka. Distance-based image classifica-tion: Generalizing to new classes at near-zero cost. IEEE transactions on pattern analysis and machineintelligence, 35(11):26242637, 2013. Erik G Miller, Nicholas E Matsakis, and Paul A Viola. Learning from one example through shared densitieson transforms. In Proceedings IEEE Conference on Computer Vision and Pattern Recognition. CVPR2000 (Cat. No. PR00662), volume 1, pp. 464471. IEEE, 2000.",
  "Yan Wang, Wei-Lun Chao, Kilian Q Weinberger, and Laurens Van Der Maaten. Simpleshot: Revisitingnearest-neighbor classification for few-shot learning. arXiv preprint arXiv:1911.04623, 2019": "Lingxiao Yang, Hongzhi You, Zonglei Zhen, Dahui Wang, Xiaohong Wan, Xiaohua Xie, and Ru-Yuan Zhang.Neural prediction errors enable analogical visual reasoning in human standard intelligence tests. In ICML,2023. Han-Jia Ye, Hexiang Hu, De-Chuan Zhan, and Fei Sha. Few-shot learning via embedding adaptation with set-to-set functions. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,pp. 88088817, 2020.",
  "Salahedine Youssef, Matej Zeevi, Devendra Singh Dhami, and Kristian Kersting. Towards a Solution toBongard Problems: A Causal Approach, 2022": "Xinyu Yun, Tanner Bohn, and Charles Ling. A deeper look at bongard problems. In Advances in Artifi-cial Intelligence: 33rd Canadian Conference on Artificial Intelligence, Canadian AI 2020, Ottawa, ON,Canada, May 1315, 2020, Proceedings 33, pp. 528539. Springer, 2020. Chi Zhang, Feng Gao, Baoxiong Jia, Yixin Zhu, and Song-Chun Zhu. Raven: A dataset for relational andanalogical visual reasoning. In Proceedings of the IEEE/CVF conference on computer vision and patternrecognition, pp. 53175327, 2019.",
  "BExperimental Details": "Transformer ArchitectureFor both Bongard-LOGO and Bongard-HOI, we use a Transformer encoderwith a depth of six, eight heads per layer, and 64 dimensions per head. We set the Transformer token andMLP dimensions to match the dimensionality of vision backbone outputs. For Bongard-HOI with a CLIPResNet-50 backbone, this is 1024. For Bongard-LOGO with a ResNet-15 backbone, this is 128. For taskoutputs (prototypes and rules), we perform layer normalization on the Transformers output for the clstoken(s) and transform them with a linear layer. PMF Training DetailsTo train PMF, we use a maximum learning rate of 5e7 and a batch size of 4and train for 40,000 iterations. We use neither support dropout nor label noise. In order to train Prototype-Mimic using the PMF backbone, we observed that it was necessary to train for 40,000 iterations instead ofthe usual 10,000 iterations. We did not need to make any changes to the SVM-Mimic training procedurewhen using a PMF backbone. DINO Training DetailsTo obtain results using a DINO backbone, we use the same training procedureas for CLIP. We set the token and MLP dimensions at all layers of the Transformer to 768 to match DINOsoutput dimensionality. We found it necessary to train SVM-Mimic for 20,000 iterations and Prototype-Mimicfor 30,000 iterations.",
  "Support Dropout DetailsWhen using train-time support dropout, we always retain at least two sup-ports from each class and at most all supports. We sample the same number of supports for each class": "Label Noise DetailsWhen using train-time label noise, we only flip the labels of one positive and onenegative support. We found it beneficial to apply label noise to only 25% of the training batches at random. GPT-4o Prompting DetailsOur best-performing GPT-4o prompting tactic is few-shot prompting. Thefollowing is an example prompt on a Bongard-LOGO problem; note that the last shown image is the queryand that we randomized the order of the supports: System: You are about to solve a type of visual reasoning problem called a Bongard problem. Youwill be shown 6 positive images followed by 6 negative images. The positive images each displaysome visual concept, and the negative images dont display the visual concept. After, you will beshown a single query image. Your goal is to deduce the concept in play and classify the queryimage as positive or negative, depending on whether it demonstrates the concept. As the examplesdemonstrate, output only a single word as your answer: positive or negative.",
  "CAdditional Dataset Details": "Dataset LicensesThe datasets used in our experiments are all publicly available. The Bongard-LOGOdataset uses an MIT License. The Bongard-HOI dataset is open-sourced for research purposes. The datasetof problems from Mikhail Bongard (called Bongard-Classic in the paper) does not have a known license. Cleaned Bongard-HOI DatasetWe observed that the Bongard-HOI dataset contains many incorrectlabels and suffers from class imbalance. To mitigate this, we created a cleaned version of the train set andall validation sets. All SVM-Mimic results in our work were trained and validated on these cleaned setsbut evaluated on the original Bongard-HOI test sets for fairest comparison with prior works. We manuallycurated these cleaned datasets. To do so, we first shuffled each Bongard problems positive and negative sets,and randomly selected one image in each set to serve as the positive and negative queries. For easy manualinspection, we cropped every image to the human-object-interaction of interest using annotations providedin the dataset, and resized the resulting image to a square. We then discarded all Bongard problems thatdid not meet all of the following criteria: (i) the object should be very clearly present in both query images,(ii) the positive query should demonstrate the interaction indicated in the label, and (iii) the negative queryshould not demonstrate the interaction. For faster curation, we did not inspect the support images. To ensurea balanced dataset, we sampled at most 100 Bongard problems for each unique human-object interactionin the train set, and 20 for each interaction in the validation sets. Our cleaned train set contained 2,196problems, while the original train set contained 23,041 problems. contains results for SVM-Mimic models trained on the original and the cleaned train sets. Theseresults demonstrate a very small but nearly consistent improvement from using the cleaned train set (despitethe smaller size).The improvement justifies our use of the cleaned train set, but the small magnitudedifference suggests that this cleaning is not critical. Both models outperform SVM + standardize.",
  "DQualitative Examples": "In this section, we provide several qualitative examples on each dataset. Each example is a single Bongardproblem with a positive and a negative query. We additionally report a score for each query using the marginbetween the querys embedding and SVM-Mimics predicted hyperplane. Higher magnitude scores indicategreater distance between the query embedding and the hyperplane, which reflects greater confidence. To beclassified correctly, positive queries should have a positive score and negative queries should have a negativescore."
}