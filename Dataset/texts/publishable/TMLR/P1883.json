{
  "Abstract": "The utilisation of Plug-and-Play (PnP) priors in inverse problems has become increasinglyprominent in recent years. This preference is based on the mathematical equivalence betweenthe general proximal operator and the regularised denoiser, facilitating the adaptation ofvarious off-the-shelf denoiser priors to a wide range of inverse problems. However, existingPnP models predominantly rely on pre-trained denoisers using large datasets. In this work, weintroduce Single-Shot PnP methods (SS-PnP), shifting the focus to solving inverse problemswith minimal data. First, we integrate Single-Shot proximal denoisers into iterative methods,enabling training with single instances. Second, we propose implicit neural priors based ona novel function that preserves relevant frequencies to capture fine details while avoidingthe issue of vanishing gradients. We demonstrate, through extensive numerical and visualexperiments, that our method leads to better approximations.",
  "Introduction": "Inverse problems have long been a fundamental challenge in the field of mathematics and applied sciences,encompassing a wide range of applications from image reconstruction to signal processing (Devaney, 2012;Bertero et al., 2021). Traditionally, these problems have been approached through various analytical andnumerical methods (Vogel, 2002; Jordan, 1881; Metropolis & Ulam, 1949) using either single or multipleimages (without a deep net or learning process). However, the advent of deep learning has revolutionisedthis domain. Deep inverse problems present a modern approach, offering new insights and solutions whereconventional methods have limitations (McCann et al., 2017). This shift towards leveraging machine learningtechniques marks a significant evolution in tackling inverse problems, opening doors to more sophisticatedand efficient problem-solving techniques. A popular framework in this deep inverse problem era is Plug-and-Play (PnP) methods (Venkatakrishnanet al., 2013; Zhang et al., 2021; Chan et al., 2016; Ono, 2017).At the core of this approach lies themathematical equivalence of the proximal operator to denoising (Venkatakrishnan et al., 2013), a concept thatintertwines optimisation theory with modern denoisers. This equivalence paves the way for the integrationof advanced deep learning-based denoisers into the inverse problem-solving process. The Plug-and-Playframework essentially allows for the seamless insertion of these denoisers into iterative algorithms, enhancingtheir ability to recover high-quality signals or images from corrupted observations (Zhang et al., 2021; 2019;Ahmad et al., 2020).",
  "Published in Transactions on Machine Learning Research (10/2024)": "Kaixuan Wei, Angelica Aviles-Rivero, Jingwei Liang, Ying Fu, Carola-Bibiane Schnlieb, and Hua Huang.Tuning-free plug-and-play proximal algorithm for inverse imaging problems. In International Conferenceon Machine Learning, pp. 1015810169. PMLR, 2020. 3, 13 Kaixuan Wei, Angelica Aviles-Rivero, Jingwei Liang, Ying Fu, Hua Huang, and Carola-Bibiane Schnlieb.Tfpnp: Tuning-free plug-and-play proximal algorithms with applications to inverse imaging problems. TheJournal of Machine Learning Research, 23(1):699746, 2022. 6, 13",
  "We introduce Single-Shot proximal denoisers into iterative methods for solving any inverseproblem. Our scheme eliminates the need for a pre-trained model, enabling training with asingle instance": "We introduce implicit neural priors for Plug-and-Play methods that enables the network topreserve more details during training. Additionally, we provide a theoretical justification for ourprior, emphasising how its continuity and differentiability play a crucial role in mitigating theissue of vanishing gradients during the training process and preserve fine details. We demonstrate, through extensive experiments on several inverse problems, that our techniqueleads to a better approximation on capturing finer details, smoother edge features and better colourrepresentation. The method is evaluated on inverse problems with both single operator task andmulti-operator task like joint demosaicing and deconvolution task. With only one image input in thewhole reconstruction process, it outperforms the classical methods and pre-trained models among allthe tasks.",
  "In this section, we review the existing literature and the concepts closely related to our work": "Plug-and-Play (PnP) Methods. They have revolutionised the field of inverse problems by integratingadvanced denoisers into iterative algorithms. This innovative approach, initiated by Venkatakrishnan et al.(2013), has undergone significant evolution. Meinhardt et al. (2017) showcased its effectiveness in diverseimaging tasks, marking a turning point in PnPs development. The subsequent works (Ryu et al., 2019;Sun et al., 2019; Hurault et al., 2022) further refined PnP, enhancing its stability and convergence, thusbroadening its applicability. In addition, the works of that (Teodoro et al., 2018; Yuan et al., 2020; Zhang et al., 2017b; Ono, 2017; Sunet al., 2019) further expanded the scope of PnP, demonstrating its adaptability in complex imaging scenarios.A notable advancement in the optimisation landscape came with the introduction of TFPnP (Tuning-Free",
  "Plug-and-Play) (Wei et al., 2020), which innovatively eliminated the need for parameter tuning in PnPalgorithms": "Previous methods have relied on data-driven pre-training, which becomes impractical in situations withlimited data or on smaller devices due to the extensive size of the required models. Consequently, developinga method for Single-Shot image prior denoising emerges as a compelling solution for such resource-constrainedenvironments. A wide range of denoisers has been used within the PnP framework. Classical denoisers such as BM3D (Dabovet al., 2007) have been the most prevalent.Other notable approaches include Teodoro et al. (2016)and Venkatakrishnan et al. (2013).These traditional denoisers are well-established and often requirelittle or no data pre-training. On the other hand, another emerging family of denoisers leverages deep learningtechniques (Meinhardt et al., 2017; Zhang et al., 2017b; Laumont et al., 2022). These deep learning-baseddenoisers have gained prominence for their ability to capture complex features and patterns in data, makingthem highly effective in PnP frameworks. The aim of this paper is to further explore and advance the use ofdeep learning-based denoisers, particularly in scenarios with minimal data, through the proposed Single-Shotmethodology. Single-Shot Image Denoising. A crucial element in PnP methods is the denoiser model. During thelast years, the denoiser technique in PnP has evolved remarkably, transitioning from traditional methodsto advanced deep learning techniques. Pioneering works such as the BM3D algorithm (Dabov et al., 2007)and the Non-Local Means (NLM) algorithm (Buades et al., 2005) laid the groundwork, setting significantbenchmarks. The introduction of deep learning marked a paradigm shift, exemplified by the DnCNN modelproposed by Zhang et al. (2017a) and its variant DnCNN-S (Zhang et al., 2018), which demonstrated theefficacy of convolutional networks in denoising. The Deep Image Prior (DIP) by Lempitsky et al. (2018)furthered this progression, utilising the structure of convolutional networks as a prior for denoising. A notable advancement is the rising of self-supervised methods, which revolutionised the Single-Shot denoisingfield by eliminating the need for clean training data. The Noise2Void framework by Krull et al. (2019) andthe Noise2Self algorithm (Batson & Royer, 2019) are pioneering examples, utilising concepts like blind-spotnetworks for effective denoising. Building on these, Self2Self (Quan et al., 2020), Noise2Same (Xie et al.,2020), and Noise2Info (Wang et al., 2023) further explored self-supervision, offering unique strategies forleveraging the inherent properties of noisy images. Additional approaches like CycleISP (Zamir et al., 2020),IRCNN (Zhang et al., 2017b), GCDN (Anwar & Barnes, 2020), and bayesian denoising with blind-spotnetworks (Laine et al., 2019) further enrich the landscape, each contributing novel perspectives and solutionsto the challenge of Single-Shot image denoising. These advancements are not confined to noise reduction alone; many of the developed deep denoisers areinherently adaptable and can be generalised to tackle various inverse problems in imaging. Inverse problems,such as demosaicing, denoising, and deconvolution, share common traits with denoising. The underlyingprinciples and network architectures developed for denoising can often be extended or fine-tuned to addressthese challenges (Romano et al., 2017; Akyz et al., 2020). Furthermore, the concept of Plug-and-Playmethods opens up new avenues. However, this progress highlights a gap: despite the evolution of Single-Shotdenoisers, there is currently no work on Single-Shot denoisers into iterative algorithms. Therefore, this workintroduces the concept of Single-Shot Plug-and-Play methods. Despite the versatility of Single-Shot image denoising, CNN-based estimators frequently fail to capturecontinuously high-frequency details crucial for image reconstruction. Implicit neural representation (INR)emerges as a solution, adept at addressing these high-frequency challenges in inverse problems. Implicit Neural Representation, characterised as a fully connected network-based method, has seen arise in popularity for solving inverse problems as highlighted by Sun et al. (2021a). Traditional activationfunctions like ReLU have exhibited limitations in representing high-frequency features, as discussed by Dabovet al. (2007). This shortcoming has led to the exploration of nonlinear activation functions, such as thesinusoidal function (Sitzmann et al., 2020), enhancing representational capabilities. The adaptability of INRis evident in its diverse applications across medical imaging (Wang et al., 2022), image processing (Chenet al., 2021; Attal et al., 2021), and super-resolution (Saragadam et al., 2023).",
  "zk+1": ": The pipeline of Single-Shot Plug-and-Play methods (SS-PnP). The 2 blocks indicate the 2 steps asin Algorithm 1, and the indicates the fix in denoiser weight over the ADMM iterations, where there areK iterations in total (i.e. k {0,1,...,K 1}). A distinct advantage of INR lies in its independence from pre-training, attributed to its rapid trainingcapability, as indicated bySaragadam et al. (2023). This feature makes INR particularly suitable forSingle-Shot image denoising tasks, where a single image suffices for effective learning, bypassing the need forextensive data-driven pre-training. The ability of INR to learn from minimal data points underscores itspotential in resource-limited scenarios.",
  "D(x) refers to the data fidelity term, usually taking the form D(x) = 1": "2 A(x) y2. R(x) denotes theregularisation term, which encodes prior knowledge about x. The parameter serves as the regularisationweight, determining the trade-off between data fidelity and regularisation. A widely used approach tosolve equation 2 is the family of first-order optimisation methods (Beck & Teboulle, 2009b; Boyd et al., 2011;Chambolle & Pock, 2011).",
  "Single-Shot Proximal Denoiser": "We introduce Single-Shot Plug-and-Play methods (SS-PnP) as demonstrated in . The optimisation ofequation 2 typically exhibits non-smooth characteristics due to R. A widely adopted strategy for addressingthis problem is to employ first-order methods such as the alternating direction method of multipliers (ADMM).Given a function F(), we define the proximal operator of F at v with step size as:",
  "where Prox2kR() is the proximal operator of the regularisation with noise strength k and Prox 1": "k D()is to enforce the data consistency (Ryu et al., 2019) with penalty parameter in the k-th iteration, fork {0,1,2,...,K 1}. From equation 4 -, we can observe that Plug-and-Play (PnP) methods leverage theequivalence between the proximal operator Prox2kR() and a denoiser Hk with the denoising parameterk 0. Single-Shot Denoiser is All You Need for PnP. PnP techniques primarily rely on denoisers, oftentrained on extensive datasets and using off-the-shelf deep denoisers (Ryu et al., 2019; Sun et al., 2019; Huraultet al., 2022). However, an unexplored question is whether Single-Shot denoisers can be used in iterativemethods and what their properties are. To our knowledge, there are no existing works on this. In our Single-Shot denoising stage, we utilise the observed image with noise y + to train the denoiser,enabling it to distinguish and mitigate complex noise and distortions specific to the example. The neuralnetwork f aims to transform the noisy and corrupted image into its less corrupted counterpart. This isachieved through an optimisation process given by:",
  "where L is a loss function that evaluates the difference between the networks output and the correspondingobserved image with noise. Refer to Step 1 in Algorithm 1": "This pre-trained model is subsequently integrated into a Plug-and-Play (PnP) framework as a prior forregularisation. Let H = f, the trained denoiser serves as a guiding force in the iterative reconstructionprocess, enhancing the ability to recover high-quality images from corrupted observations. By embeddingthis Single-Shot learning model into the PnP framework, we establish a potent approach for tackling a range",
  "Implicit Neural Prior for Plug-and-Play": "Implicit neural representations enable the learning of continuous functions from a signal. State-of-the-artperformance relies on deep denoisers like UNet (Ronneberger et al., 2015) and FFDNet (Zhang et al., 2018).While convolutional-based denoisers have shown impressive results, there is currently no research exploringthe use of implicit neural representation (INR) as a prior for PnP methods. INR can be used as a Single-Shotapproach in equation 4, we then open the door to a new research direction for PnP. INR boasts remarkable expressive power and inductive bias, paving the way for innovative denoising approaches.Another significant contribution of this work is the introduction of a new Single-Shot framework for PnP. Inparticular, we present a novel INR prior. Unlike the majority of existing works, we provide a solid theoreticaljustification for its properties and behavior.",
  "exp (y)+1 converges to 1 when x . Consequently, the function is convergent to 1when x": "We underline that previous proof that the convergence results we present are specifically in terms of thebehaviour of the prior within our proposed framework. By leveraging the convergence and bounded natureof the activation function, we can significantly mitigate the issue of exploding gradients. Moreover, as themodel converges to 0, it also effectively reduces the impact of outliers, enhancing the overall robustness ofour model.",
  "Experimental Setting": "In our image preprocessing, we utilised dual resizing strategies. We sourced the images with Creative CommonsLicenses and resized to 512 384, and in the meantime tested on the selected data in Bevilacqua et al.(2012) and Zeyde et al. (2012), without resizing. We remind to the reader, the experiments on Single-ShotPlug-and-Play methods (SS-PnP) considered only a single image input in the whole pipeline. Training Scheme. During the initial implicit neural representation (INR) pre-training phase, Gaussian noisewith a standard deviation in the range of [0.001,0.5] was explored with 0.1 was set for all the experiments.The training was conducted over 100 iterations, with a network configuration comprising 2 hidden layers and64 features per layer. The learning rate was set to 0.001. We then reconstruct the image for 5 ADMM iterationsteps with dynamic noise strength and penalty parameter chosen by logarithmic descent that graduallydecreases value between 35 and 30 over steps. Evaluation Protocol. For comparative analysis, the Noise2Self pre-training scheme (Krull et al., 2019),was applied to three networks, each undergoes 100 training iterations with a learning rate of 0.01. TheDnCNN (Zhang et al., 2017a) and FFDNet (Zhang et al., 2018) architectures, were configured with 8 hidden",
  "DnCNNFFDNetUNet": ": Visual comparison of the 4 deep denoising priors (include our proposed as ) on deconvolution taskin Single-Shot Plug-and-Play (SS-PnP) strategy for \"Giraffe\" example, with a zoomed-in region exhibitingintricate details. DnCNN, FFDNet, and UNet, which are all trained with the Noise2Self pre-training scheme. The quantitativeresults, as detailed in , reveal our methods outstanding performance, outshining the benchmarkswith significant margins. Notably, our approach demonstrates a marked improvement in PSNR values, withparticularly pronounced enhancements in the 4x upscaling scenario. These advancements indicate our modelsrobustness and substantial improvement forward in Single-Shot super resolution. Our method demonstrates exceptional preservation of texture and detail complexity, as shown in Figures 2, 3.Our technique achieves high-resolution enhancement while intricately reconstructing fine details, evident fromthe minimised artifacts such as grid patterns and spurious spots in . In , DnCNN introducesnoticeable colour distortions in both the background and the foreground. The UNet generates numerousundesirable grids. Although our approach presents a comparable visual quality to FFDNet, it achieves asuperior PSNR, suggesting a quantitatively and qualitatively improved performance. Collectively, our resultsindicate that our method not only holds promise for practical application but also sets a new standard forsuper-resolution tasks.",
  "This section shows all the numerical and visual results that support our method": "Super-resolution (SR). It refers to the process of reconstructing a high-resolution image (HR) from oneor more low-resolution observations (LR). The forward measurement operator for super-resolution is givenby: A(x) = (k x) s, where k represents the kernel for convoluting the image, and s is the downscalingoperation with scale s. Here, kernel operator was set as size 5 with standard deviation of the Gaussiandistribution as 3. In evaluating the Single-Shot super-resolution efficacy of our method, we utilise six varied categories of images,with upscaling factors of 2 and 4 to ensure a fair comparison against well-established techniques such as",
  "PretrainedUNet": ": The comparative visualisation of joint deconvolution and demosaicing tasks on the \"Fractal\" example.The comparison is made across different denoising priors in Play-and-Play framworks, inclusing Single-Shotdeep denoising priors (our proposed prior, Noise2Self-UNET, and Noise2Self-FFDNet), pre-trained deepdenoising priors (Pretrained-UNet and Pretrained-FFDNet), and classical denoising priors (TV). Image Deconvolution.This is a computational technique aimed at reversing the effects of blur onphotographs. Mathematically, the observed image, y, is the result of convoluing the true image, x, with aforward measurement operator: A(x) = k x. The goal of deconvolution is to estimate the original image xby deconvoluting the observed image y with the gaussian kernel k. Here, the kernel size was set as 15 withstandard deviation 5 of Gaussian distribution. The performance comparison of various Single-Shot deep denoiser algorithms on a deconvolution task, shownin , evaluated using both PSNR and SSIM metrics as well. The comparison includes our methodalongside established techniques like DnCNN, FFDNet, and UNet across other six image categories. Ourapproach consistently achieves competitive PSNR scores, surpassing others in the Turtle and Monarchcategories, and still shows parity improvements in SSIM values. This indicates not only enhanced accuracy inimage reconstruction but also improved perceptual quality. These results underscore our methods effectivenessin noise reduction and sharpness, demonstrating its potential for practical deconvolution applications. In , we illustrate a qualitative comparison of our deconvolution algorithm on a Giraffe image againstfour leading Single-Shot deep denoising priors. The visual fidelity of our method is apparent, particularly inits capacity to reconstruct intricate details, such as the giraffes fur that reflected in the zoomed views. Thisattention to detail extends to the preservation of edge sharpness and the subtle gradations, which contributeto a more natural and cohesive image composition. Contrastingly, other methods exhibit varying degrees ofblurring and artifact introduction. This qualitative advancement underscores our algorithms potential to seta remarkable benchmark for image deconvolution.",
  ": The difference of the error maps between the proposed Single-Shot deep denoising prior and theother compared methods in on the joint deconvolution and demosaicing task on \"Squirrel\" example": "Joint Demosaicing and Deconvolution. Demosaicing is an algorithmic process that reconstructs afull-colour image from the incomplete colour samples output by an image sensor overlaid with a colour filterarray (CFA). The joint demosaicing and deconvolutional process can be represented as: A(x) = k (M x), where x is thefull-colour image, M is the colour filter array (CFA) , denotes element-wise multiplication. The setting ofk was same with kernel in deconvolution task. Based on performances from previous tasks, we note that FFDNet and UNet produce good results. In themore challenging tasks as it is joint demosaicing and deconvolution, these methods, along with classical TotalVariation (TV) (Jordan, 1881), are further compared with the data-driven pre-trained denoising priors, andSingle-Shot denoising priors training under Noise2Self strategy against our proposed method. illustrates that our method outperforms all methods across all categories, achieving the highest PSNRand SSIM scores in most cases. Notably, in the Wolf and Monarch categories, our method significantlyleads, reflecting a substantial improvement in both reconstruction accuracy and image quality, as indicatedby the PSNR and SSIM metrics respectively. This demonstrates the effectiveness of our approach in handlingcomplex image restoration tasks. In and 6, our method outperforms other established algorithms in joint deconvolution and demosaicing,particularly in mitigating chromatic aberrations such as red grids or spots. These artifacts, which degradeimage quality, are significantly reduced in our approach, leading to a visually coherent result that aligns moreclosely with the ground truth. Competing methods, including pre-trained networks and classical denoisingtechniques, often introduce or inadequately suppress such distortions, resulting in inferior colour and contrastsoutcomes. Our method advances the visual quality of image restoration, effectively preserving the naturalcolour and detail fidelity.",
  "We provide a further empirical study forcomparing our proposed implicit neuralnetwork (INR) with the existing classicalINR, SIREN (Strmpler et al., 2022)": "We follow the same experimental settingsas described in 4.1, and measure the per-formance based on Peak Signal-to-NoiseRatio (PSNR). In , our proposedINR outperforms SIREN in all the 4 tasks:deconvolution, super resolution 2 and4, and joint deconvolution and demo-saicing. This performance is significantin the super resolution with an averageimprovement of 0.8dB. The spatial com-pactness brought by our INR guaranteesthe representativeness of different levelsof feature that pushes the performance ofsuper resolution tasks.",
  "PSNR (dB)": ": The visualisation comparison of the implicitneural priors (Ours and SIREN) on deconvolution(Deconv), super resolution(SR) with 2 and 4 upscal-ings, and joint deconvolution and demosaicing (Joint)tasks on \"Bird\" example. The difference of the errormaps of and SIREN are provided in the third row.The plot indicates the performance (PSNR) with bothimplicit neural priors changing over the ADMM itera-tion on the joint deconvolution and demosaicing task. We can observe from that our INR excels inreconstructing the image with smoother edge featuresand enhanced colour representation. The differenceis more clearly reflected in the error maps, with anoticeable variation around the edge features acrossall four tasks. The performance over iteration curvesdemonstrate our leading advances over the optimi-sation iterations, meanwhile show the early iterationsteps is effective for achieving the best performance.This is commonly observed in iterative optimisationtechniques (Wei et al., 2020; 2022), where initialiterations tend to improve the quality of the recon-struction, while beyond a certain point, the modelbegin to introduce artifacts or oversmooth the image,which leads to a degradation in performance andconsequently a drop in PSNR. We also conducted a comparative analysis of ourproposed Single-Shot prior against other state-of-the-art priors within Plug-and-Play frameworks, asshown in . The BM3D (Dabov et al., 2007)and DIP (Sun et al., 2021b) priors were each run for240 iterationsten times the iterations used for ourSingle-Shot approach. Despite this, the classic BM3Dmethods performance was markedly lower comparedto both DIP and our method. While the DIP priorwithin the Plug-and-Play framework demonstratedresults closer to ours, a significant performance gapremains, underscoring the superior efficacy of ourapproach in both 2 and 4 super-resolution tasks.",
  "Conclusion": "Our work pioneers the Single-Shot Plug-and-Play (SS-PnP) method, transforming the use of PnP priorsin inverse problem-solving by reducing reliance on large-scale pre-training of denoisers, and using a singleinstance. We also propose Single-Shot proximal denoisers via implicit neural priors allowing for superiorapproximation quality for solving inverse problems using only a single instance. We propose a novel functionfor implicit neural priors that has desirable theoretical guarantees. We also showed that our work generaliseswell across different tasks, showing empirical stability for single and multiple operators. We then open the doorto a new research line for Single-Shot PnP. Whilst this work uses mainly PnP-ADMM due to the well-knowproperties, and performance in comparison with other algorithms. Future work will include to evaluate ournew research line on Single-Shot PnP on different algorithms including but not limited to FISTA (Beck &Teboulle, 2009a), HQS (Geman & Yang, 1995), and Primal-dual (Dantzig et al., 1956) algorithms. Anotheradditional side insight could be to explore the result over randomising z0 in step 2 of Algorithm 1. Thoughthe primary focus of this work is not the convergence of the proposed implicit neural prior, the convergenceof the broader PnP framework is a significant and intricate topic that is valuable to analysis in the futureresearch work.",
  "Acknowledgements": "This project was supported with funding from the Cambridge Centre for Data-Driven Discovery and AccelerateProgramme for Scientific Discovery, made possible by a donation from Schmidt Futures. YC is funded by anAstraZeneca studentship and a Google studentship. The work of RHC was partially supported by HKRGCGRF grants CityU11309922, CRF grant C1013-21GF and HKITF MHKJFS Grant MHP/054/22. CBSacknowledges support from the Philip Leverhulme Prize, the Royal Society Wolfson Fellowship, the EPSRCadvanced career fellowship EP/V029428/1, EPSRC grants EP/S026045/1 and EP/T003553/1, EP/N014588/1,EP/T017961/1, the Wellcome Innovator Awards 215733/Z/19/Z and 221633/Z/20/Z, the European UnionHorizon 2020 research and innovation programme under the Marie Skodowska-Curie grant agreement No.777826 NoMADS, the Cantab Capital Institute for the Mathematics of Information and the Alan TuringInstitute. AIAR acknowledges support from CMIH (EP/T017961/1) and CCIMI, University of Cambridge.This work was supported in part by Oracle Cloud credits and related resources provided by Oracle forResearch. Also, EPSRC Digital Core Capability. Rizwan Ahmad, Charles A Bouman, Gregery T Buzzard, Stanley Chan, Sizhuo Liu, Edward T Reehorst,and Philip Schniter. Plug-and-play methods for magnetic resonance imaging: Using denoisers for imagerecovery. IEEE signal processing magazine, 37(1):105116, 2020. 1",
  "Marco Bevilacqua, Aline Roumy, Christine Guillemot, and Marie Line Alberi-Morel.Low-complexitysingle-image super-resolution based on nonnegative neighbor embedding. 2012. 7": "Stephen Boyd, Neal Parikh, Eric Chu, Borja Peleato, Jonathan Eckstein, et al. Distributed optimizationand statistical learning via the alternating direction method of multipliers. Foundations and Trends inMachine learning, 3(1):1122, 2011. 4 Antoni Buades, Bartomeu Coll, and J-M Morel. A non-local algorithm for image denoising. In 2005 IEEEcomputer society conference on computer vision and pattern recognition (CVPR05), volume 2, pp. 6065.Ieee, 2005. 3",
  "Stanley H Chan, Xiran Wang, and Omar A Elgendy. Plug-and-play admm for image restoration: Fixed-pointconvergence and applications. IEEE Transactions on Computational Imaging, 3(1):8498, 2016. 1": "Yinbo Chen, Sifei Liu, and Xiaolong Wang. Learning continuous image representation with local implicitimage function. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,pp. 86288638, 2021. 3 Kostadin Dabov, Alessandro Foi, Vladimir Katkovnik, and Karen Egiazarian. Image denoising by sparse 3-dtransform-domain collaborative filtering. IEEE Transactions on image processing, 16(8):20802095, 2007.3, 12, 13",
  "Camille Jordan. Sur la series de fourier. CR Acad. Sci., Paris, 92:228230, 1881. 1, 11": "Alexander Krull, Tim-Oliver Buchholz, and Florian Jug. Noise2void-learning denoising from single noisyimages. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp.21292137, 2019. 3, 7 Zeqiang Lai, Kaixuan Wei, Ying Fu, Philipp Hrtel, and Felix Heide. -prox: Differentiable proximal algorithmmodeling for large-scale optimization. ACM Transactions on Graphics (TOG), 42(4):119, 2023. 8, 12",
  "Yaniv Romano, Michael Elad, and Peyman Milanfar. The little engine that could: Regularization by denoising(red). SIAM Journal on Imaging Sciences, 10(4):18041844, 2017. 3": "Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical imagesegmentation. In Medical Image Computing and Computer-Assisted InterventionMICCAI 2015: 18thInternational Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18, pp. 234241.Springer, 2015. 6, 8 Ernest Ryu, Jialin Liu, Sicheng Wang, Xiaohan Chen, Zhangyang Wang, and Wotao Yin. Plug-and-playmethods provably converge with properly trained denoisers. In International Conference on MachineLearning, pp. 55465557. PMLR, 2019. 2, 5, 6 Vishwanath Saragadam, Daniel LeJeune, Jasper Tan, Guha Balakrishnan, Ashok Veeraraghavan, andRichard G Baraniuk. Wire: Wavelet implicit neural representations. In Proceedings of the IEEE/CVFConference on Computer Vision and Pattern Recognition, pp. 1850718516, 2023. 2, 3, 4, 12 Vincent Sitzmann, Julien Martel, Alexander Bergman, David Lindell, and Gordon Wetzstein. Implicit neuralrepresentations with periodic activation functions. Advances in neural information processing systems, 33:74627473, 2020. 3 Yannick Strmpler, Janis Postels, Ren Yang, Luc Van Gool, and Federico Tombari. Implicit neural represen-tations for image compression. In European Conference on Computer Vision, pp. 7491. Springer, 2022. 2,12",
  "Yu Sun, Jiaming Liu, Mingyang Xie, Brendt Wohlberg, and Ulugbek S Kamilov. Coil: Coordinate-basedinternal learning for imaging inverse problems. arXiv preprint arXiv:2102.05181, 2021a. 3": "Zhaodong Sun, Fabian Latorre, Thomas Sanchez, and Volkan Cevher. A plug-and-play deep image prior. InICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),pp. 81038107. IEEE, 2021b. 12, 13 Matthew Tancik, Pratul Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Raghavan, Utkarsh Singhal,Ravi Ramamoorthi, Jonathan Barron, and Ren Ng. Fourier features let networks learn high frequencyfunctions in low dimensional domains. Advances in Neural Information Processing Systems, 33:75377547,2020. 2 Afonso M Teodoro, Jos M Bioucas-Dias, and Mrio AT Figueiredo. Image restoration and reconstructionusing variable splitting and class-adapted image priors. In 2016 IEEE International Conference on ImageProcessing (ICIP), pp. 35183522. IEEE, 2016. 3 Afonso M Teodoro, Jos M Bioucas-Dias, and Mrio AT Figueiredo. A convergent image fusion algorithmusing scene-adapted gaussian-mixture-based denoising. IEEE Transactions on Image Processing, 28(1):451463, 2018. 2 Singanallur V Venkatakrishnan, Charles A Bouman, and Brendt Wohlberg. Plug-and-play priors for modelbased reconstruction. In 2013 IEEE global conference on signal and information processing, pp. 945948.IEEE, 2013. 1, 2, 3, 6",
  "Curtis R Vogel. Computational methods for inverse problems. SIAM, 2002. 1": "Jiachuan Wang, Shimin Di, Lei Chen, and Charles Wang Wai Ng. Noise2info: Noisy image to information ofnoise for self-supervised image denoising. In Proceedings of the IEEE/CVF International Conference onComputer Vision, pp. 1603416043, 2023. 3 Yuehao Wang, Yonghao Long, Siu Hin Fan, and Qi Dou. Neural rendering for stereo 3d reconstructionof deformable tissues in robotic surgery. In International Conference on Medical Image Computing andComputer-Assisted Intervention, pp. 431441. Springer, 2022. 3",
  "Yaochen Xie, Zhengyang Wang, and Shuiwang Ji. Noise2same: Optimizing a self-supervised bound for imagedenoising. Advances in neural information processing systems, 33:2032020330, 2020. 3": "Xin Yuan, Yang Liu, Jinli Suo, and Qionghai Dai.Plug-and-play algorithms for large-scale snapshotcompressive imaging. In Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition, pp. 14471457, 2020. 2 Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, Ming-Hsuan Yang,and Ling Shao. Cycleisp: Real image restoration via improved data synthesis. In IEEE/CVF conferenceon computer vision and pattern recognition, pp. 26962705, 2020. 3 Roman Zeyde, Michael Elad, and Matan Protter. On single image scale-up using sparse-representations. InCurves and Surfaces: 7th International Conference, Avignon, France, June 24-30, 2010, Revised SelectedPapers 7, pp. 711730. Springer, 2012. 7 Kai Zhang, Wangmeng Zuo, Yunjin Chen, Deyu Meng, and Lei Zhang. Beyond a gaussian denoiser: Residuallearning of deep cnn for image denoising. IEEE transactions on image processing, 26(7):31423155, 2017a.3, 7 Kai Zhang, Wangmeng Zuo, Shuhang Gu, and Lei Zhang. Learning deep cnn denoiser prior for imagerestoration.In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.39293938, 2017b. 2, 3",
  "Kai Zhang, Wangmeng Zuo, and Lei Zhang. Ffdnet: Toward a fast and flexible solution for cnn-based imagedenoising. IEEE Transactions on Image Processing, 27(9):46084622, 2018. 3, 6, 7": "Kai Zhang, Wangmeng Zuo, and Lei Zhang. Deep plug-and-play super-resolution for arbitrary blur kernels.In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 16711681,2019. 1 Kai Zhang, Yawei Li, Wangmeng Zuo, Lei Zhang, Luc Van Gool, and Radu Timofte. Plug-and-play imagerestoration with deep denoiser prior. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(10):63606376, 2021. 1"
}