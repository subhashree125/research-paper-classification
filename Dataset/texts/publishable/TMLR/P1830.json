{
  "Abstract": "Current image manipulation primarily centers on static manipulation, such as replacingspecific regions within an image or altering its overall style. In this paper, we introduce aninnovative dynamic manipulation task, subject repositioning. This task involves relocating auser-specified subject to a desired position while preserving the images fidelity. Our researchreveals that the fundamental sub-tasks of subject repositioning, which include filling thevoid left by the repositioned subject, reconstructing obscured portions of the subject andblending the subject to be consistent with surrounding areas, can be effectively reformulatedas a unified, prompt-guided inpainting task. Consequently, we can employ a single diffusiongenerative model to address these sub-tasks using various task prompts learned throughour proposed task inversion technique. Additionally, we integrate pre-processing and post-processing techniques to further enhance the quality of subject repositioning. These elementstogether form our SEgment-gEnerate-and-bLEnd (SEELE) framework. To assess SEELEseffectiveness in subject repositioning, we assemble a real-world subject repositioning datasetcalled ReS. Results of SEELE on ReS demonstrate its efficacy. Code and ReS dataset areavailable at",
  "Introduction": "In 2023, Google Photos introduced an AI editing feature allowing users to reposition subjects within theirimages (Google, 2023). However, a lack of technical documentation limits understanding of this feature.Some researches have touched on aspects of it. Iizuka et al. (2014) explored object repositioning before thedeep learning era, using user inputs like ground regions and bounding boxes. In the deep learning era, fieldslike scene decomposition (Zheng et al., 2021) and de-occlusion (Zhan et al., 2020) enable manipulation ofobject positions after the explicit understanding of scene and object relationships. This paper addressesgeneral Subject Repositioning (SubRep) task without explicit scene understanding. Our aim is to addressSubRep via a meticulously crafted solution, driven by a single diffusion model. From an academic perspective, this task falls under image manipulation (Gatys et al., 2016; Isola et al.,2017; Zhu et al., 2017; Wang et al., 2018; El-Nouby et al., 2019; Fu et al., 2020; Zhang et al., 2021). Recentadvancements in large-scale generative models have fueled interest in this field. These models, includinggenerative adversarial models (Goodfellow et al., 2014), variational autoencoders (Kingma & Welling, 2014),auto-regressive models (Vaswani et al., 2017), and notably, diffusion models (Sohl-Dickstein et al., 2015),demonstrate impressive image manipulation capabilities with expanding model architectures and trainingdatasets (Rombach et al., 2022; Kawar et al., 2022; Chang et al., 2023). However, current image manipula-tion methods primarily target \"static\" alterations, modifying specific image regions using cues like naturallanguage, sketches, or layouts (El-Nouby et al., 2019; Zhang et al., 2021; Fu et al., 2020). Another aspectinvolves style-transfer tasks, transforming overall image styles such as converting photos into anime picturesor paintings (Chen et al., 2018; Wang et al., 2018; Jiang et al., 2021). Some extend to video manipulation,altering style or subjects over time (Kim et al., 2019; Xu et al., 2019; Fu et al., 2022). In contrast, subjectrepositioning dynamically relocates selected subjects within a single image while leaving the rest unchanged.",
  "(b) Subject Completion": ":We compare subject repositioning using our SEELE with Googles Magic Editor. SEELE effec-tively addresses tasks like subject removal, completion, and harmonization through a unified prompt-guidedinpainting process, powered by a single diffusion model. Comprehensive results are depicted in . The SubRep task involves multiple stages, including non-generative and generative tasks.Existing pre-trained models are effective for non-generative tasks like segmenting subjects (Kirillov et al., 2023) andestimating occlusion relationships (Ranftl et al., 2020). Our focus lies on the generative tasks of SubRep,including: i) Subject removal: The generative model must fill voids left after repositioning without introduc-ing new elements. ii) Subject completion: If the repositioned subject is partially obscured, the model mustcomplete it to maintain integrity. iii) Subject harmonization: The repositioned subject should blend withsurrounding areas. All these sub-tasks demand unique generative capabilities. The most powerful text-to-image diffusion models (Nichol et al., 2022; Ho et al., 2022; Saharia et al., 2022;Ramesh et al., 2022; Rombach et al., 2022) show potential promise for SubRep. However, a key challengeis finding suitable text prompts, as these models are usually trained with image captions rather than task-specific instructions. The best prompts are often image-dependent and hard to generalize, limiting practicaluse in real-world applications. Translating these task instructions into caption-style prompts for fixed text-to-image diffusion models is particularly challenging. On the other hand, specialized models exist for specificaspects () of SubRep, like local inpainting (Zeng et al., 2020; Zhao et al., 2021; Li et al., 2022;Suvorov et al., 2022; Dong et al., 2022), subject completion (Zhan et al., 2020), and local harmonization (Xuet al., 2017; Zhang et al., 2020; Tsai et al., 2017). However, combining components from these models canmake the SubRep system bulky and less elegant. Given the shared generative nature of these sub-tasks, ourstudy raises an intriguing question: \"Can we achieve all these sub-tasks using a single model?\" To answer this question, we introduce \"task inversion\", a novel concept that learns latent embeddings asalternative of text conditions to guide diffusion models with specific task instructions. The embedding spaceof text prompts in diffusion models offers versatility beyond just captions. Employing prompt tuning at thetask level allows us to learn latent embeddings to guide diffusion models based on task instructions. Taskinversion enables diffusion models to adapt to various tasks by adjusting task-level \"text\" prompts. Unliketextual inversion (Gal et al., 2022) which learns image-dependent caption prompts and prompt tuning (Lesteret al., 2021; Liu et al., 2021a) which learns domain adaptation, our method employs task-level instructionalprompts to approximate optimal text prompts for each image in a specific task, transforming text-to-image",
  ": Subject Harmonization": ":SEELE for SubRep includes i) pre-processing: identifying the subject via user-provided conditions, andpreserving occlusion relationships between subjects; ii) manipulation: filling gaps left in the image and correctsobscured subjects with user-specified incomplete masks; iii) post-processing: addressing disparities between the repo-sitioned subject and its new surroundings. SEELE addresses all generative sub-tasks in SubRep via a single diffusionmodel. In this example, only local harmonization is used in postprocessing. See shadow generation results in . diffusion model into task-to-image model. Our approach pioneers the systematic use of learned embeddingsacross generative sub-tasks within a single SD, effectively addressing the complex challenge of SubRep. To formally address the SubRep task, we propose the SEgment-gEnerate-and-bLEnd (SEELE) framework.As in , SEELE manages the subject repositioning with a pre-processing, manipulation, post-processing pipeline. i) In the pre-processing stage, SEELE segments the subject based on user-specifiedpoints, bounding boxes, or text prompts. With the provided moving direction, SEELE relocates the subjectwhile considering occlusion relationships between subjects. ii) In the manipulation stage, SEELE uses asingle diffusion model guided by learned task prompts to handle subject removal and completion. iii) In thepost-processing stage, SEELE harmonizes the repositioned subject to blend with adjacent regions. Weve curated a dataset named ReS to test subject repositioning algorithms in real-world scenarios. Wemade efforts in covering various scenes and times to give a wide range of examples. Particularly, the real-world images for this task demand very exhaustively ground-truth annotation, including the mask of therepositioned subject and the moving direction. We annotate the mask using SAM (Kirillov et al., 2023) andmanual refinement, and estimating the moving direction based on the center point of masks in the pairedimage. Additionally, we also provide amodal masks for subjects that are partly hidden. This results 100 2paired real image, actually diverse enough to support the evaluation of our task, as illustrated in (b).As far as we know, this is the first dataset designed specifically for subject repositioning. Its diverse andwell-organized, making it a great benchmark for validating methods for this task.",
  "ContributionsOur contributions are as follows:": "We delineate the Subject Repositioning (SubRep) task as a specialized interactive image manipulationchallenge, decomposed into several distinct sub-tasks, each of which presents unique challenges and ne-cessitates specific capacities. We present the SEgment-gEnerate-and-bLEnd (SEELE) framework, which tackles various generativetasks using a single diffusion model. SEELE offers an application akin to Googles magic editor. Ad-ditionally, SEELE goes beyond the Magic Editor by offering advanced features like preserving occlusionand perspective, as well as local harmonization. We present task inversion, demonstrating that we can re-formulate the text-conditions to represent taskinstructions. This exploration opens up new possibilities for adapting diffusion models to specific tasks.",
  "(b) Examples of ReS dataset": ": (a) User inputs in each stage of SubRep. (b) Examples of Res dataset. We provide paired imageswith subject full and visible mask annotations as well as moving direction information. The moving directionis marked as blue. The mask of visible part and completed subject specified by user are marked as orange.",
  "Subject Repositioning": "Subject repositioning (SubRep) relocates the user-specified subject within an image. The \"subject\" can beanything the user focuses on, such as a part of an object, an entire object, or multiple objects. Althoughit sounds straightforward, this task is quite complex. It requires coordination of multiple sub-tasks andinteraction between user and learning models. User inputsAn illustration of the user inputs is shown in (a). SubRep follows user intention toidentify the subject, move it to the desired location, complete it, and address disparities. Particularly, theuser identifies the interested subject via pointing, bounding box, or text prompts. Then, the user providesthe desired repositioning location via dragging or direction. The user also needs to specify the occluded partof the subject for completion, and decide whether to apply post-processing to reduce visible differences. ReS datasetTo evaluate the effectiveness of subject repositioning algorithms, we curated a benchmarkdataset called ReS. It includes 1002 paired images: one image features a repositioned subject while the otherelements remain constant. These images were collected from over 20 indoor and outdoor scenes, featuringsubjects from over 50 categories. This diversity enables effective simulation of real-world applications, makingour dataset suitable for evaluating our SEELE model. We also contribute very detailed annotations to this dataset. Particularly, The masks for the repositionedsubjects were initially generated using SAM and refined by multiple experts. Occluded masks were providedfor subject completion. The direction of repositioning was estimated by measuring the distance between thecenter points of the masks in each image pair. For each paired image in the dataset, we can assess subjectrepositioning performance from one image to the other and in reverse, resulting in double testing examples.(b) illustrates the ReS dataset. We release the ReS dataset at to encourage research in subject repositioning.",
  "SEELE Framework for Subject Repositioning": "Task decompositionTo tackle this task, we introduce the SEgment-gEnerate-and-bLEnd (SEELE)framework, shown in . Specifically, SEELE breaks down the task into three stages: pre-processing,manipulation, and post-processing. Pre-processing handles non-generative tasks, while manipulation andpost-processing require generative capabilities. We use a unified diffusion model for all generative sub-tasksand pre-trained models for non-generative tasks in SEELE. i) The pre-processing addresses how to precisely locate the specified subject with minimal user input, consid-ering that the subject may be a single object, part of an object, or a group of objects identified by the usersintention; reposition the identified subject to the desired location; and also identify occlusion relationshipsto maintain geometric consistency. Additionally, adjusting the subjects size might be necessary to maintainthe perspective relationship.",
  "Published in Transactions on Machine Learning Research (11/2024)": "Yu Zeng, Zhe Lin, and Vishal M Patel. Sketchedit: Mask-free local image manipulation with partial sketches.In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 59515961,2022. Xiaohang Zhan, Xingang Pan, Bo Dai, Ziwei Liu, Dahua Lin, and Chen Change Loy. Self-supervised scenede-occlusion. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp.37843792, 2020.",
  "After identifying the subject, SEELE follows user intention to reposition the subject to the desired location,and masks the original area": "SEELE handles the potential occlusion between the moved subject and other elements in the image. Ifthere are other subjects present at the desired location, SEELE employs the monocular depth estimationalgorithm MiDaS (Ranftl et al., 2020) to discern occlusion relationships between subjects.SEELE willthen appropriately mask the occluded portions of the subject if the user wants to preserve these occlusionrelationships. MiDaS is also used to estimate the perspective relationships among subjects and resize thesubject accordingly to maintain geometric consistency. For subjects with ambiguous boundaries, SEELEincorporates the ViTMatte matting algorithm (Yao et al., 2023) for better compositing with surroundingareas. An illustrated comparison of incorporated modules can be found in . ManipulationIn this stage, SEELE deals with the primary tasks of manipulating subjects, includingsubject removal and subject completion, as illustrated in . Critically, such two steps can be effec-tively solved by a single generative model, as the masked region of both steps should be filled in to matchthe surrounding areas. However, these two sub-tasks require different information and types of masks. Par-ticularly, for subject removal, a non-semantic inpainting is applied uniformly from the unmasked regions,using a typical object-shaped mask. This often falsely results in the creation of new, random subjects withinthe holes. On the other hand, subject completion involves semantic-rich inpainting and aims to incorporatethe majority of the masked region as part of the subject. Critically, to adapt the same diffusion model tothe different generation directions needed for the above sub-tasks, we propose the task inversion techniquein SEELE. This technique guides the diffusion model according to specific task instructions. Thus, with thelearned remove-prompt and complete-prompt, SEELE tackles these sub-tasks via a single generative model.An illustrated comparison between different task-prompts can be found in (a).",
  "Post-processingIn the final stage, SEELE blends the repositioned subject with its surroundings bytackling two challenges below. The illustrated comparison of post-processing can be found in": "i) Local harmonization ensures natural appearance in boundary and lighting statistics. SEELE confinesthis process to the relocated subject to avoid affecting other image parts. It takes the image and a maskindicating the subjects repositioning as inputs. However, the stable diffusion model is initially trained togenerate new contents within the masked region, conflicting with our goal of only ensuring consistency inthe masked region and its surroundings. To address this, SEELE adapts the model by learning a harmonize-prompt with LoRA adapter (Hu et al., 2021) to guide masked regions. This can also be integrated into thesame diffusion model used in the manipulation stage with our newly proposed design. ii) Shadow generation aims to create realistic shadows for repositioned subjects, enhancing the realism.Generating high-fidelity shadows in high-resolution images of diverse subjects remains challenging. SEELEuses the diffusion model for shadow generation, addressing two scenarios: 1) If the subject already hasshadows, we use complete-prompt for shadow completion. 2) For subjects without shadows, we follow user-intention to locate the desired shadow area. This task then transforms into a local harmonization process.",
  "Task promptsTo address these challenges, we introduce task inversion, a method that trains prompts toguide the diffusion model for specific generation tasks while keeping its backbone fixed": "In standard text-to-image diffusion models, text prompts like \"a cute cat\" are processed through a textencoder, which generates token sequences to guide image generation. Task inversion eliminates the need fortext inputs and the text encoder. Instead, we train learnable prompts, called task prompts, to serve as inputsequences. These task prompts directly guide the model to perform specific tasks. Conceptually, they actas instructions such as \"complete the subject\". See (a) for an illustration. A key challenge is the domain gap: text-to-image diffusion models are not originally trained to respond toinstruction-based prompts. However, our experiments demonstrate that learned task prompts significantlyenhance performance.Compared to unconditional generation or simple semantic and instructional textprompts, task prompts deliver substantial improvements in standard inpainting and outpainting tasks (see) and sub-tasks like subject repositioning (see ). Our approach also reduces user effort by serving as an alternative to image-dependent text prompts forsubject repositioning. Moreover, task inversion seamlessly integrates various generative sub-tasks for sub-ject repositioning using stable diffusion. This eliminates the need for new generative models or extensiveadditional modules, emphasizing its plug-and-play simplicity. Inverse to learn task promptWe aim to learn an optimal task prompt that conditions diffusion modelsfor specific inpainting tasks. This task prompt is trained on input-output pairs, enabling it to translate taskinstructions from the training dataset into learned representations. In text-to-image diffusion inpainting models, input conditions are typically text strings embedded into asequence of vectors. For instance, in SD 2.0, a text encoder processes the text into a sequence of size [L, D],which is then passed through the U-Nets cross-attention layers. Instead of using a text-based approach, ourmethod directly learns a sequence of size [L, D] to represent the task prompts. Unlike user-driven prompts for specifying object location or direction, these task prompts are pre-learnedduring training. Each prompt is associated with specific input-output pairs tailored for the task, as explainedin Sec. 3.2. During inference, task prompts are integrated into the pipeline shown in . Users canthen select between subject completion or harmonization via a simple button. Formally, task inversion adheres to the original training objectives of diffusion models. Specifically, denotethe training image as x, the local mask as m, the learnable task prompt as z. Our objective is",
  "L(z) := EN(0,1),tU(0,1) ([xt, m, x (1 m)], t, z2F,(1)": "where is the random noise; is the diffusion model, t is the normalized noise-level; xt is the noised image, is element-wise multiplication; and F is the Frobenius norm. When training with Eq. (1), the isfrozen, making the embedding z the only learnable parameters. Our task inversion is a distinctive approach, influenced by various existing works but with clear differences.The instruction prompt mentioned for our task inversion goes beyond the training datas scope, where thetext describes the content of image, potentially affecting the desired generation results in practice. Recentadvancements in textual inversion (Gal et al., 2022) emphasize the potential to comprehend user-specifiedconcepts within the embedding space. In contrast, prompt tuning (Lester et al., 2021; Liu et al., 2021a)",
  "(b) Training masks": ": (a) Comparison between task inversion and other techniques. Task inversion does not require textinputs, addresses different objectives, and serves different tasks, thus differing from other approaches. Theembeddings v and vi are learnable and represented as z in Eq. (1). (b) We generate masks to representparticular tasks to train task inversion, addressing different tasks with a single diffusion model. enhances adaptation to specific domains by introducing learnable tokens to the inputs.Unlike textualinversion, which trains a few tokens for visual understanding, our task inversion trains the whole latent toprovide task instruction. Our task inversion differs prompt-tuning in that: prompt-tuning adds new tokens,while our approach replaces text condition inputs. We dont depend on text inputs to guide the diffusionmodel. See (a) for the distinction.",
  "Learning task inversion": "Existing inpainting model is trained with randomly generated masks to generalize in diverse scenarios. Incontrast, task inversion involves creating task-specific masks during training, allowing the model to learnspecialized task prompts. i) Generating masks for subject removal: In subject repositioning, the mask for the left void mirrors thesubjects shape, but our goal isnt to generate the subject within the mask. To create training data for thisscenario, for each image, we randomly choose a subject and its mask. Next, we move the mask, as shownby the girls mask in the center of (b). This results in an image where the masked region includesrandom portions unrelated to the masks shape. This serves as the target for subject removal, with the maskindicating the original subject location and the ground-truth is background areas. ii) Generating masks for subject completion: In this phase, SEELE addresses scenarios where the subject ispartially obscured, with the goal of effectively completing the subject. To integrate this prior informationinto the task prompt, we generate training data as follows: for each image, we randomly select a subjectand extract its mask. Then, we randomly choose a continuous portion of the mask as the input mask. Sinceuser-specified masks are typically imprecise, we introduce random dilation to include adjacent regions withinthe mask. As illustrated by the umbrella mask on the right side of (b), such a mask serves as anestimate for the mask used in subject completion. iii) Learning subject harmonization. In SEELE, we achieve subject harmonization by altering the target ofdiffusion model. To this end, we take as input the inharmonious image and take as output the harmoniousimage. Additionally, we replace the unmasked region condition with original inharmonious image. Taskprompt mainly influences the cross-attention layers. To adapt the self-attention in the diffusion model topreserve the content of masked region while harmonizing appearance, we introduce LoRA adapters (Huet al., 2021). Our training objective is:",
  ": Subject repositioning on 10242 images. SEELE works well on diverse scenarios, enabling flexiblerepositioning, and achieves high-fidelity repositioned images. Larger version in": "where x represents the target harmonized image, and x is the input inharmonious image. This allows thediffusion model to gradually harmonize the image during denoising. While we modify the training objective,the generation process remains unchanged. This allows us to still utilize the pre-trained stable diffusion modelwith the learned harmonize-prompt and LoRA parameters, and seamlessly integrate with other modules.",
  "Experimental Results and Analysis": "Examples of subject repositioningWe present subject repositioning results on real-world 10242 imagesusing SEELE in .SEELE works well on diverse scenarios, enabling flexible repositioning, andachieves high-fidelity repositioned images. Competitors and setup on ReSGoogle Photos Magic Editor isnt publicly accessible, so we cantcompare it with our method. We mainly compare with original Stable Diffusion inpainting model (SD)v2.0. We test SD with different prompts, including i) SDno performs unconditional generation; ii) SDsimpleuses \"inpaint\" and \"complete the subject\"; iii) SDcomplex uses \"Incorporate visually cohesive and high-fidelitybackground and texture into the provided image through inpainting\" and \"Complete the subject by filling inthe missing region with visually cohesive and high-fidelity background and texture\" for subject removal andcompletion tasks, respectively. iv) SDLoRA uses the LoRA fine-tuning strategy to fine-tune the SD at thesame training setup of SEELE. Furthermore, we can incorporate alternative inpainting algorithms in SEELE.Specifically, we incorporate LaMa (Suvorov et al., 2021), MAT (Li et al., 2022), MAE-FAR (Cao et al., 2022),and ZITS++ (Cao et al., 2023) into SEELE. We resize images to 512 pixels minimum for compatibility withstandard inpainting algorithms. Note that in this experiment, SEELE does not utilize any pre-processingor post-processing techniques. Standard inpainting algorithms cannot tackle subject repositioning without theincorporation of SEELE. Qualitative comparisonWe present qualitative comparison results in where a larger versionand more results are in the appendix. We add orange subject removal mask and blue subject completionmask in the input image. The SD column is SD guided by simple prompt as this variant performs best.Our qualitative analysis indicates that SEELE exhibits better subject removal capabilities without addingrandom parts and excels in subject completion. When the moved subject overlaps with the left void, SDfills the void by extending the subject. In contrast, SEELE avoids the influence of the subject, as in the toprow of . If the mask isnt precise, SEELE works better than other methods by reducing the impactof unclear edges and smoothing the area, as in the fourth row. SEELE excels in subject completion thantypical inpainting algorithms, as in the second-to-last row. Note that SEELE can be enhanced through thepost-processing stage.",
  "InputTargetSDSEELESEELE(ZITS++)SEELE(MAEFAR)SEELE(LAMA)SEELE(MAT)": ": Qualitative comparison of subject repositioning on ReS. We add orange subject removal mask andblue subject completion mask in the input image. SEELE works better in the diverse real-world scenarios,even if the mask is not precise. Note that SEELE can be enhanced through the post-processing stage. : Quantitative comparison and user-study on ReS. (): SD; (*): SEELE; Quality: the fidelity of theresults; Consist.: the consistency with surrounding area. SEELE consistently works better than SD variants.",
  "LPIPS()0.1570.1570.1570.1620.1560.1760.1720.1630.163Quality()0.0570.0900.0730.2070.2900.0800.0530.0730.076Consist.()0.0540.0570.0500.0360.3290.0890.1140.1680.104": "Quantitative comparison and user-studyWe use Learned Perceptual Image Patch Similarity (LPIPS)as quantitative metric and conduct user-study to evaluate user preference from i) quality: the fidelity of theresults; ii) visual-consistency (Consist.): the consistency with surrounding area.Our user study on allReS dataset involves 100 anonymous surveys, reporting the ratio of top-1 preferred option. Results are in. Compared with other methods, SEELE demonstrates significant enhancements in the quality ofmanipulated images across all metrics. Particularly for the SDLoRA, i) our construction of training maskrequires object-level ground-truth segmentation in the training dataset, while public dataset do not havelarge scale annotated dataset (compared with the LAION dataset (Schuhmann et al., 2022) used by SDwhich contains 5B training data.) ii) when the training dataset is limited, the task inversion enjoys superiorperformance while fine-tuning technique leads to over-fitting and cause worse performance. Effectiveness of the proposed task-inversionTo further validate the proposed task-inversion, weconduct experiments on standard inpainting task on Places2 (Zhou et al., 2017) and outpainting task onFlickr-Scenery (Cheng et al., 2022), following the standard training and evaluation principles. Quantitativeresults is in , showcasing the superiority of the proposed task-inversion on both inpainting andoutpainting tasks. We provide details and qualitative results in the appendix. Influence of different task promptsWe train different task prompts to guide different generationdirection. Using wrong prompts for tasks can make the model give bad results. We tested this by comparingresults from different learned task prompts. As in (a), using a wrong prompt can change the outcome.",
  "(b) Ablation of local harmonization": ": (a) Opposite task prompts cause bad results. Zoom in to find the fly in 2rd-row. Different taskprompt will lead to different generation direction. Use these prompt in the opposite way will cause badresults. (b) The local harmonization can be properly addressed with both the harmony-prompt along withthe LoRA parameters. For subject removal, remove-prompt can correctly generate with background flowers, while complete-promptwrongly try to add a fly instead of flowers. For subject completion example of trying to add a birds head,remove-prompt only added water, but the complete-prompt added the birds head properly. This validatethe different generation direction learned by our task prompt. One might also want to use the LoRA technique to adapt the diffusion model for subject removal andcompletion. We run the experiments and compare this variant with only using the task inversion prompts.As shown in Tab. 3 (b), the LoRA-fine-tuned variant performs poorly. This shows that: (1) The subjectremoval and completion tasks are similar to generalized inpainting tasks, meaning that simply training task-specific prompts can effectively guide the diffusion model for these sub-tasks. (2) With the same trainingsetup (using COCO), LoRA fine-tuning may reduce the U-Nets generalization ability.In contrast, ourSEELE method keeps the U-Net frozen, maintaining its generalization ability. Ablation of Local HarmonizationTo tackle the local harmonization sub-task, we learn the harmony-prompt along with the LoRA parameters. To show the efficacy of each module, we conduct an qualitativeablation study in (b). Naturally, if we disable the LoRA parameters, as we use the inharmoniousimage as unmasked image condition for the stable diffusion model, the model tends to copy the image withoutsignificant modification. If we only use LoRA parameter, it works like the unconditional diffusion model toperform local harmonization, but usually performs over- or under- harmonization. Such a manner works tosome extent, but can be enhanced with the learned harmony-prompt. Another choice for local harmonization is using specific local harmonization models. However, the benchmarkdataset iHarmony4 (Cong et al., 2020) is usually used to train and test on a image size of 256 256,which is smaller than the standarad working resolution in SD 2.0 of size 512 512. Furthermore, the local",
  "Localharmonization": ":Ablation of using components X in SEELE. Applying specific component will lead to betterconsistency of generated images in corresponding perspective, and thus generating higher-fidelity images.See detailed analysis in the appendix. harmonization models trained on smaller image sizes cannot generalize well on larger images. For instance,when we tested one of the SOTA models, DucoNet Tan et al. (2023), trained on iHarmony4, it didnt workas well as our model in our setup, as shown in (a). Hence we train image harmonization as part ofour own framework. Since our framework is flexible, we can easily switch to a better harmonization modelin the future if needed. This is also a benefit of our framework. SEELE w/ XWe assess the effectiveness of various components within SEELE during both pre-processingand post-processing phases. We conduct a qualitative comparison of SEELEs results with and without theutilization of these components, as in , while a detailed analysis of is provided in the appendix. Failure analysisAs a sophisticated system, the success of SEELE relies on the success of each includedmodule. Particularly, the core challenges of subject repositioning include appearance, geometry, and semanticinconsistency issues, as shown in . i) SEELE addresses the appearance issue, which encompasses theabsence of subjects and shadows, as well as unnatural shadows and boundaries. This is achieved throughthe innovative methods of subject completion, shadow generation, and local harmonization. ii) To tackle",
  ": Failure case visualization. The failure of modules in SEELE can lead to several inconsistencies": "the geometry issue, SEELE employs a depth estimation approach that maintains occlusion relationshipsand perspective accuracy. iii) For resolving semantic inconsistency, SEELE employs techniques for subjectremoval and completion. The failure of each specific module may lead to the corresponding inconsistency,and resulting in a less-fidelity image. LimitationsOne significant limitation of SEELE is that when the system performs sub-optimally, manualuser intervention becomes necessary to enhance the results. For instance, in cases where segmentation fails,users are required to manually correct the segment mask. Similarly, when the subject is occluded, usersmust provide a mask of potential regions to complete the subject. The former issue could potentially bemitigated through improvements in the segmentation model. However, the latter challenge necessitates thedevelopment of a novel model to address the problem of open-vocabulary amodal mask generation (Zhanet al., 2020). Currently, there lack available foundation models to support open-vocabulary amodal maskgeneration. These are potential avenues for future research.",
  "Related Works": "Image and video manipulation aims to manipulate images and videos in accordance with user-specifiedguidance. Among these guidance, natural language guidance, as presented in previous studies (Dong et al.,2017; Nam et al., 2018; Li et al., 2020a;b; Xia et al., 2021; Karras et al., 2019; El-Nouby et al., 2019; Zhanget al., 2021; Fu et al., 2020; Chen et al., 2018; Wang et al., 2018; Jiang et al., 2021), stands out as particularlyappealing due to its adaptability and user-friendliness. Some research efforts have also explored the useof visual conditions, which can be conceptualized as image-to-image translation tasks. These conditionsencompass sketch-based (Yu et al., 2019; Jo & Park, 2019; Chen et al., 2020; Kim et al., 2020; Chen et al.,2021; Richardson et al., 2021; Zeng et al., 2022), label-based (Park et al., 2019; Zhu et al., 2020; Richardsonet al., 2021; Lee et al., 2020), line-based (Li et al., 2019), and layout-based (Liu et al., 2019) conditions.In contrast to image manipulation, video manipulation (Kim et al., 2019; Xu et al., 2019; Fu et al., 2022)introduces the additional challenge of ensuring temporal consistency across different frames, necessitating thedevelopment of novel temporal architectures (Bar-Tal et al., 2022) . Image manipulation primarily revolvesaround modifying static images, whereas video manipulation deals with dynamic scenes in which multiplesubjects are in motion. In contrast, our paper focuses on subject repositioning, relocating one subject whilethe rest of the image remains unchanged. Textual inversion (Gal et al., 2022) is designed to personalize text-to-image diffusion models accordingto user-specified concepts.It learns new concepts within the embedding space of text conditions whilefreezing other modules. Null-text inversion (Mokady et al., 2022) learns distinct embeddings at differentnoise levels to enhance capacity. Some fine-tuning (Ruiz et al., 2022) or adaptation (Zhang & Agrawala,2023; Mou et al., 2023b) techniques inject visual conditions into text-to-image diffusion models. While theseapproaches concentrate on image patterns, SEELE focuses on the task instruction to guide diffusion models. Prompt tuning (Lester et al., 2021; Liu et al., 2021b;a) entails training a model to learn specific tokens asadditional inputs to transformer models, thereby enabling model adaptation to a specific domain withoutfine-tuning the model. This technique been widely used in vision-language models (Radford et al., 2021; Yaoet al., 2021; Ge et al., 2022). This inspired us to adapt the text-to-image into task-to-image diffusion modelby replacing the text conditions. Image composition (Niu et al., 2021) is the process of combining a foreground and background to create ahigh-quality image. Due to differences in the characteristics of foreground and background elements, incon-",
  "Conclusion": "In this paper, we introduce an innovative task known as subject repositioning, which involves manipulatingan input image to reposition one of its subjects to a desired location while preserving the images fidelity.To tackle subject repositioning, we present SEELE, a framework that leverages a single diffusion model toaddress the generative sub-tasks through our proposed task inversion technique. This includes tasks suchas subject removal, subject completion, and subject harmonization. To evaluate the effectiveness of subjectrepositioning, we have curated a real-world dataset called ReS. Our experiments on ReS demonstrate theproficiency of SEELE.",
  "Broader Impact Statement": "Our proposed SEELE system aims to address the issue of subject repositioning within single images andwill be responsive to user intentions. However, there is a risk that it could be misused to create prankimages with malicious intent towards individuals, entities, or objects. To mitigate this, we add watermarksto images generated by our SEELE system to indicate their artificial nature.",
  "Chenjie Cao, Qiaole Dong, and Yanwei Fu. Zits++: Image inpainting by improving the incremental trans-former on structural priors. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023": "Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot, Jose Lezama, Lu Jiang, Ming-Hsuan Yang, KevinMurphy, William T Freeman, Michael Rubinstein, et al. Muse: Text-to-image generation via maskedgenerative transformers. arXiv preprint arXiv:2301.00704, 2023. Jianbo Chen, Yelong Shen, Jianfeng Gao, Jingjing Liu, and Xiaodong Liu. Language-based image editingwith recurrent attentive models. In Proceedings of the IEEE Conference on Computer Vision and PatternRecognition, pp. 87218729, 2018. Shu-Yu Chen, Wanchao Su, Lin Gao, Shihong Xia, and Hongbo Fu. DeepFaceDrawing: Deep generation offace images from sketches. ACM Transactions on Graphics (Proceedings of ACM SIGGRAPH 2020), 39(4):72:172:16, 2020. Shu-Yu Chen, Feng-Lin Liu, Yu-Kun Lai, Paul L Rosin, Chunpeng Li, Hongbo Fu, and Lin Gao. Deep-faceediting: Deep face generation and editing with disentangled geometry and appearance control. arXivpreprint arXiv:2105.08935, 2021. Yen-Chi Cheng, Chieh Hubert Lin, Hsin-Ying Lee, Jian Ren, Sergey Tulyakov, and Ming-Hsuan Yang. Inout:diverse image outpainting via gan inversion. In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition, pp. 1143111440, 2022.",
  "Hao Dong, Simiao Yu, Chao Wu, and Yike Guo. Semantic image synthesis via adversarial learning. InProceedings of the IEEE International Conference on Computer Vision, pp. 57065714, 2017": "Qiaole Dong, Chenjie Cao, and Yanwei Fu. Incremental transformer structure enhanced image inpaintingwith masking positional encoding. In Proceedings of the IEEE/CVF Conference on Computer Vision andPattern Recognition, pp. 1135811368, 2022. Alaaeldin El-Nouby, Shikhar Sharma, Hannes Schulz, Devon Hjelm, Layla El Asri, Samira Ebrahimi Kahou,Yoshua Bengio, and Graham W Taylor. Tell, draw, and repeat: Generating and modifying images based oncontinual linguistic instruction. In Proceedings of the IEEE/CVF International Conference on ComputerVision, pp. 1030410312, 2019. Tsu-Jui Fu, Xin Wang, Scott Grafton, Miguel Eckstein, and William Yang Wang. Iterative language-basedimage editing via self-supervised counterfactual reasoning.In Proceedings of the 2020 Conference onEmpirical Methods in Natural Language Processing (EMNLP), pp. 44134422, 2020. Tsu-Jui Fu, Xin Eric Wang, Scott T Grafton, Miguel P Eckstein, and William Yang Wang. M3l: Language-based video editing via multi-modal multi-level transformers. In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pp. 1051310522, 2022. Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H Bermano, Gal Chechik, and Daniel Cohen-Or. An image is worth one word: Personalizing text-to-image generation using textual inversion. arXivpreprint arXiv:2208.01618, 2022. Leon A Gatys, Alexander S Ecker, and Matthias Bethge. Image style transfer using convolutional neuralnetworks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 24142423, 2016.",
  "Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, andWeizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685,2021": "Satoshi Iizuka, Yuki Endo, Masaki Hirose, Yoshihiro Kanamori, Jun Mitani, and Yukio Fukui.Objectrepositioning based on the perspective in a single image. In Computer Graphics Forum, volume 33, pp.157166. Wiley Online Library, 2014. Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image translation with conditionaladversarial networks. In Proceedings of the IEEE conference on computer vision and pattern recognition,pp. 11251134, 2017. Wentao Jiang, Ning Xu, Jiayun Wang, Chen Gao, Jing Shi, Zhe Lin, and Si Liu. Language-guided global im-age editing via cross-modal cyclic mechanism. In Proceedings of the IEEE/CVF International Conferenceon Computer Vision, pp. 21152124, 2021.",
  "Youngjoo Jo and Jongyoul Park. Sc-fegan: Face editing generative adversarial network with users sketchand color. In The IEEE International Conference on Computer Vision (ICCV), October 2019": "Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarialnetworks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.44014410, 2019. Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar Mosseri, and MichalIrani. Imagic: Text-based real image editing with diffusion models. arXiv preprint arXiv:2210.09276,2022.",
  "Chong Mou, Xintao Wang, Jiechong Song, Ying Shan, and Jian Zhang. Dragondiffusion: Enabling drag-stylemanipulation on diffusion models. arXiv preprint arXiv:2307.02421, 2023a": "Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhongang Qi, Ying Shan, and Xiaohu Qie. T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. arXivpreprint arXiv:2302.08453, 2023b. Chong Mou, Xintao Wang, Jiechong Song, Ying Shan, and Jian Zhang. Diffeditor: Boosting accuracy andflexibility on diffusion-based image editing. In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition, pp. 84888497, 2024. Seonghyeon Nam, Yunji Kim, and Seon Joo Kim. Text-adaptive generative adversarial networks: manip-ulating images with natural language. In Proceedings of the 32nd International Conference on NeuralInformation Processing Systems, pp. 4251, 2018.",
  "Li Niu, Wenyan Cong, Liu Liu, Yan Hong, Bo Zhang, Jing Liang, and Liqing Zhang. Making images realagain: A comprehensive survey on deep image composition. arXiv preprint arXiv:2106.14490, 2021": "Xingang Pan, Ayush Tewari, Thomas Leimkhler, Lingjie Liu, Abhimitra Meka, and Christian Theobalt.Drag your gan: Interactive point-based manipulation on the generative image manifold. In ACM SIG-GRAPH 2023 Conference Proceedings, pp. 111, 2023. Taesung Park, Ming-Yu Liu, Ting-Chun Wang, and Jun-Yan Zhu. Semantic image synthesis with spatially-adaptive normalization. In Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition, pp. 23372346, 2019. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, GirishSastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models fromnatural language supervision. In International conference on machine learning, pp. 87488763. PMLR,2021.",
  "Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditionalimage generation with clip latents. arXiv preprint arXiv:2204.06125, 2022": "Ren Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, and Vladlen Koltun. Towards robust monoc-ular depth estimation: Mixing datasets for zero-shot cross-dataset transfer. IEEE transactions on patternanalysis and machine intelligence, 44(3):16231637, 2020. Elad Richardson, Yuval Alaluf, Or Patashnik, Yotam Nitzan, Yaniv Azar, Stav Shapiro, and Daniel Cohen-Or. Encoding in style: a stylegan encoder for image-to-image translation. In IEEE/CVF Conference onComputer Vision and Pattern Recognition (CVPR), June 2021. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjrn Ommer. High-resolutionimage synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on ComputerVision and Pattern Recognition, pp. 1068410695, 2022. Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman.Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. arXiv preprintarXiv:2208.12242, 2022. Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, KamyarGhasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in Neural Information ProcessingSystems, 35:3647936494, 2022. Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti,Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa Kun-durthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. Laion-5b: An openlarge-scale dataset for training next generation image-text models. In S. Koyejo, S. Mohamed, A. Agarwal,D. Belgrave, K. Cho, and A. Oh (eds.), Advances in Neural Information Processing Systems, volume 35,pp. 2527825294. Curran Associates, Inc., 2022. URL Yujun Shi, Chuhui Xue, Jun Hao Liew, Jiachun Pan, Hanshu Yan, Wenqing Zhang, Vincent YF Tan,and Song Bai. Dragdiffusion: Harnessing diffusion models for interactive point-based image editing. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 88398849,2024.",
  "Linfeng Tan, Jiangtong Li, Li Niu, and Liqing Zhang. Deep image harmonization in dual color spaces. InProceedings of the 31st ACM International Conference on Multimedia, pp. 21592167, 2023": "Shashank Tripathi, Siddhartha Chandra, Amit Agrawal, Ambrish Tyagi, James M Rehg, and Visesh Chari.Learning to generate synthetic data via compositing. In Proceedings of the IEEE/CVF Conference onComputer Vision and Pattern Recognition, pp. 461470, 2019. Yi-Hsuan Tsai, Xiaohui Shen, Zhe Lin, Kalyan Sunkavalli, Xin Lu, and Ming-Hsuan Yang. Deep imageharmonization. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp.37893797, 2017. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ukasz Kaiser,and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30,2017.",
  "Yuan Yao, Ao Zhang, Zhengyan Zhang, Zhiyuan Liu, Tat-Seng Chua, and Maosong Sun. Cpt: Colorfulprompt tuning for pre-trained vision-language models. arXiv preprint arXiv:2109.11797, 2021": "Jiahui Yu, Zhe Lin, Jimei Yang, Xiaohui Shen, Xin Lu, and Thomas S Huang. Free-form image inpaintingwith gated convolution. In Proceedings of the IEEE/CVF international conference on computer vision,pp. 44714480, 2019. Yu Zeng, Zhe Lin, Jimei Yang, Jianming Zhang, Eli Shechtman, and Huchuan Lu. High-resolution im-age inpainting with iterative confidence feedback and guided upsampling. In Computer VisionECCV2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part XIX 16, pp. 117.Springer, 2020.",
  "Lvmin Zhang and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. arXivpreprint arXiv:2302.05543, 2023": "Tianhao Zhang, Hung-Yu Tseng, Lu Jiang, Weilong Yang, Honglak Lee, and Irfan Essa. Text as neural op-erator: Image manipulation by text instruction. In Proceedings of the 29th ACM International Conferenceon Multimedia, pp. 18931902, 2021. Shengyu Zhao, Jonathan Cui, Yilun Sheng, Yue Dong, Xiao Liang, Eric I Chang, and Yan Xu. Large scaleimage completion via co-modulated generative adversarial networks. arXiv preprint arXiv:2103.10428,2021. Chuanxia Zheng, Duy-Son Dao, Guoxian Song, Tat-Jen Cham, and Jianfei Cai. Visiting the invisible: Layer-by-layer completed scene decomposition. International Journal of Computer Vision, 129:31953215, 2021. Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva, and Antonio Torralba. Places: A 10 million imagedatabase for scene recognition. IEEE transactions on pattern analysis and machine intelligence, 40(6):14521464, 2017. Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image translation usingcycle-consistent adversarial networks. In Proceedings of the IEEE international conference on computervision, pp. 22232232, 2017. Peihao Zhu, Rameen Abdal, Yipeng Qin, and Peter Wonka. Sean: Image synthesis with semantic region-adaptive normalization. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),June 2020.",
  "A.2Experimental Setting": "SEELE is built upon the text-guided inpainting model fine-tuned from SD 2.0, employing the task inversiontechnique to learn each task prompt with 50 learnable tokens, initialized with text descriptions from the taskinstructions. For each task, we utilize the AdamW optimizer (Loshchilov & Hutter, 2017) with a learningrate of 8.0e 5, weight decay of 0.01, and a batch size of 32. Training is conducted on two A6000 GPUsover 9,000 steps, selecting the best checkpoints based on the held-out validation set. When addressing subject moving and completion, we employ the MSCOCO dataset (Lin et al., 2014), whichprovides object masks. For image harmonization, the iHarmony4 dataset (Cong et al., 2020) is utilized,offering unharmonized-harmonized image pairs along with subject-to-harmonize masks. MSCOCO comprises80k training images and 40k testing images, while iHarmony4 includes 65k training images and 7k testing",
  "images. This diversity ensures robustness in training task prompts, guarding against overfitting on specificimages": "Cost analysisThe core component of SEELE is the pre-trained stable diffusion inpainting model, boast-ing 865.93 million parameters within its UNet backbone. To tailor this stable diffusion model for subjectrepositioning, we incorporate three distinct task prompts, each sized at 501024 and has 0.5 million train-able parameters. For the local harmonization task, we introduce the LoRA adapter, which encompasses5.12 million trainable parameters. Its worth noting that these newly added parameters are lightweight andintroduce no additional inference latency when compared to the stable diffusion backbone.",
  "Here we provide the analysis of each component used in SEELE": "i) Depth estimation for occlusion becomes crucial when users wish to move a subject from the foregroundto the background. It helps estimate and correct the occluded parts, ensuring that the repositioned subjectblends seamlessly into the scene. As illustrated in the first row of , this depth estimation playsa pivotal role in repositioning objects like the tower behind leaves or people behind a car. Neglecting theocclusion relationship can result in unnatural-looking repositioned subjects and a significant loss of imagefidelity. ii) Depth estimation for perspective comes into play when users want to resize the subject proportionallyduring repositioning. If this aspect is overlooked, the subjects size remains fixed, which may contradict userexpectations. iii) Matting primarily addresses issues arising from imprecise masks provided by SAM, particularly whendealing with subjects with ambiguous boundaries. Precise masking is crucial because inaccuracies can leadto information leaking in the final output. For example, in , imprecise masking might encourage thegaps to generate unnatural dog fur. iv) Shadow generation is handled by reusing the generative model within SEELE. In cases where a subjectincludes shadows, such as the left part in , we approach it as a subject completion task.Theshadow itself becomes the subject, and we employ a learned complete-prompt to guide the diffusion model.Conversely, when a subject lacks shadows, we can transform it into a local harmonization task by utilizingSEELEs harmonization model to generate shadows. v) Local harmonization addresses the challenge of appearance inconsistency. When the illumination statisticschange after subject repositioning, its essential to adjust the subjects appearance while preserving its tex-ture. As depicted in , SEELE excels at this local harmonization task, ensuring seamless integrationinto the new environment.",
  "A.4Standard Image Inpainting and Outpainting": "Image inpaintingThe proposed task-inversion approach not only specializes the inpainting model forspecific tasks but also enhances its standard inpainting capabilities. We substantiate this claim throughexperiments conducted on the Places2 dataset (Zhou et al., 2017), where we train SEELE using standardinpainting prompts and compare its performance with other inpainting algorithms. The results are presentedin Tab.2(a) in our paper.Additionally, we provide visual representations of the results in ,demonstrating SEELEs advantage in reducing hallucinatory artifacts. Image outpaintingAnother commonly used manipulation task involves extending the image beyond itsoriginal content. This approach shares a similar concept with subject completion, but it takes a more holisticperspective by enhancing the entire image. We have also conducted experiments on the outpainting task anddemonstrated the effectiveness of task inversion. Our experiments were carried out using the Flickr-Scenerydataset (Cheng et al., 2022), and the results are compared with stable diffusion in Tab. 2(b) in our paper.The results indicate the superiority of task inversion employed in SEELE. Furthermore, we provide visualexamples for qualitative assessment in .",
  "A.5Necessity of Using Different Datasets to Train SEELE": "Our training of the SEELE model utilized only two datasets: COCO, which provides ground-truth objectsegmentation masks, and iHarmony4, which offers paired images for local harmonization tasks.Thesedatasets, chosen for their public availability, aptly fulfill the varying requirements of different generativesub-tasks. Our training approach, which encompasses both subject movement and completion, employs aunified task inversion technique. Given that local harmonization focuses on not introducing new details inmasked areas, we have modified the diffusion model to integrate the characteristics of the masked region,ensuring it aligns with the tasks specific needs.",
  "A.6Integrating LoRA": "When the LoRA adapter is trained, we load them along with the frozen stable diffusion model. As LoRA isimplemented as additive layers with the original layers. For example, suppose for a particular layer f withinput xi and output xi+1. The original stable diffusion performs xi+1 = f(xi), while LoRA is trained toperform xi+1 = f(xi) + LoRA(xi) and only learn LoRA() while freezing f(). Then we could introduce ascale hyper-parameter for a trained model xi+1 = f(xi) + cLoRA(xi) When SEELE performs the sub-tasksin manipulation process, we set the lora scale as c = 0 to preserve the original outputs of stable diffusion.While in the local harmonization process, we set the lora scale as c = 1 to perform local harmonization. Inthis regard, we could use the same stable diffusion backbone and perform different sub-tasks using differentsub-task prompts (and LoRA parameters)."
}