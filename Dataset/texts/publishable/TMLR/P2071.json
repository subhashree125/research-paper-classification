{
  "Abstract": "Pre-trained language models (PLM), for example BERT or RoBERTa, mark the state-of-the-art for natural language understanding task when fine-tuned on labeled data. However, theirlarge size poses challenges in deploying them for inference in real-world applications, dueto significant GPU memory requirements and high inference latency. This paper exploresneural architecture search (NAS) for structural pruning to find sub-parts of the fine-tunednetwork that optimally trade-off efficiency, for example in terms of model size or latency, andgeneralization performance. We also show how we can utilize more recently developed two-stage weight-sharing NAS approaches in this setting to accelerate the search process. Unliketraditional pruning methods with fixed thresholds, we propose to adopt a multi-objectiveapproach that identifies the Pareto optimal set of sub-networks, allowing for a more flexibleand automated compression process.",
  "Introduction": "Pre-trained language models (PLMs) such as BERT (Devlin et al., 2019) or RoBERTa (Liu et al., 2019b) arewidely used for natural language understanding (NLU) tasks when large amount of labelled data is availablefor fine-tuning. However, deploying PLMs for inference can be challenging due to their large parametercount. They demand significant GPU memory and exhibit high inference latency, making them impracticalfor many real-world applications, for example when used in an end-point for a web service or deployed on anembedded systems. Recent work (Blalock et al., 2020; Kwon et al., 2022; Michel et al., 2019; Sajjad et al.,2022) demonstrated that in many cases only a subset of the pre-trained models parameters significantlycontributes to the downstream task performance. This allows for compressing the model by pruning partsof the network while minimizing performance deterioration. Unstructured pruning (Blalock et al., 2020) computes a score for each weight in the network, such as theweights magnitude, and removes weights with scores below a predetermined threshold. This approach of-ten achieves high pruning rates with minimal performance degradation, but it also leads to sparse weightmatrices, which are not well-supported by commonly used machine learning frameworks. Structured prun-ing (Michel et al., 2019; Sajjad et al., 2022) removes larger components of the network, such as layers or",
  "(b) Pareto front of sub-networks": ": Illustration of our approach. a) We fine-tune the pre-trained architecture by updating only sub-networks, which we select by placing a binary mask over heads and units in each MHA and FFN layer. b)Afterwards, we run a multi-objective search to select the optimal set of sub-networks that balance parametercount and validation error. heads. Although it typically does not achieve the same pruning rates as unstructured pruning, it only prunesentire columns/rows of the weight matrix, making it compatible with popular deep learning frameworks andhardware. Neural architecture search (Zoph & Le, 2017; Real et al., 2017; Bergstra et al., 2013) (NAS) finds moreresource efficient neural network architectures in a data-driven way by casting it as an optimization problem.To reduce the computational burden of vanilla NAS, which needs to train and validate multiple architectures,weight-sharing-based neural architecture search (Pham et al., 2018; Liu et al., 2019b; Elsken et al., 2018)first trains a single large network, called the super-network, and then searches for sub-networks within thesuper-network. We propose to use NAS for structural pruning of pre-trained networks, to find sub-networks that sustainperformance of the pre-trained network after fine-tuning (see for an illustration). Most structuralpruning approaches prune the networks based on a predefined threshold on the pruning ratio. In scenarioswhere there is no strict constraint on model size, it can be challenging to define such a fixed threshold inadvance. NAS offers a distinct advantage over other pruning strategies by enabling a multi-objective approachto identify the Pareto optimal set of sub-networks, which captures the nonlinear relationship between modelsize and performance instead of just obtaining a single solution. This allows us to automate the compressionprocess and to select the best model that meets our requirements post-hoc after observing the non-linearPareto front, instead of running the pruning process multiple rounds to find the right threshold parameter. While there is a considerable literature on improving the efficiency of PLMs, to the best of our knowledgethere is no work yet that explored the potential of NAS for pruning fine-tuned PLMs. Our contributions arethe following: We discuss the intricate relationship between NAS and structural pruning and present a NAS ap-proach that compresses PLMs for inference after fine-tuning on downstream tasks, while minimizingperformance deterioration. Our focus lies not in proposing a novel NAS method per se, but ratherin offering a practical use-case for NAS in the context of PLM that works competitively to structuralpruning methods from the literature. We propose four different search spaces with varying complexity to prune components of transformerbased PLM and discuss how their definition affect the structure of sub-networks. Two of these searchspaces are typically used by existing structural pruning approaches (see .2). While one ofthese commonly used search spaces exhibits the highest degree of freedom, we show in .1.1that a search space with lower complexity can be more efficient to explore and eventually lead tobetter performance within a reasonable budget.",
  "Published in Transactions on Machine Learning Research (09/2024)": "runtime 0.450 0.475 0.500 0.525 0.550 0.575 0.600 0.625 0.650 regret hypervolume RTE EHVILSMO-ASHAMO-REARS runtime 0.600 0.625 0.650 0.675 0.700 0.725 0.750 0.775 0.800 regret hypervolume MRPC EHVILSMO-ASHAMO-REARS runtime 0.15 0.20 0.25 0.30 0.35 0.40 regret hypervolume STSB EHVILSMO-ASHAMO-REARS runtime 0.45 0.50 0.55 0.60 0.65 0.70 regret hypervolume COLA EHVILSMO-ASHAMO-REARS runtime 0.10 0.15 0.20 0.25 0.30 0.35 0.40 0.45 0.50 regret hypervolume IMDB EHVILSMO-ASHAMO-REARS runtime 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40 regret hypervolume SST2 EHVILSMO-ASHAMO-REARS runtime 0.40 0.45 0.50 0.55 0.60 0.65 0.70 regret hypervolume SWAG EHVILSMO-ASHAMO-REARS runtime 0.20 0.25 0.30 0.35 0.40 0.45 regret hypervolume QNLI EHVILSMO-ASHAMO-REARS runtime 0.6 0.8 1.0 1.2 1.4 1.6 1.8 regret hypervolume RTE EHVILSMO-ASHAMO-REARS runtime 0.4 0.5 0.6 0.7 0.8 0.9 regret hypervolume MRPC EHVILSMO-ASHAMO-REARS runtime 0.15 0.20 0.25 0.30 0.35 0.40 regret hypervolume STSB EHVILSMO-ASHAMO-REARS runtime 0.5 0.6 0.7 0.8 0.9 1.0 1.1 1.2 regret hypervolume COLA EHVILSMO-ASHAMO-REARS runtime 0.10 0.15 0.20 0.25 0.30 0.35 0.40 regret hypervolume IMDB EHVILSMO-ASHAMO-REARS runtime 0.05 0.10 0.15 0.20 0.25 0.30 regret hypervolume SST2 EHVILSMO-ASHAMO-REARS runtime 0.40 0.45 0.50 0.55 0.60 0.65 0.70 0.75 0.80 regret hypervolume SWAG EHVILSMO-ASHAMO-REARS runtime 0.15 0.20 0.25 0.30 0.35 0.40 0.45 0.50 regret hypervolume QNLI EHVILSMO-ASHAMO-REARS",
  "Related Work": "Neural Architecture Search (NAS) (see Elsken et al. (2018) for an overview) automates the designof neural network architectures to maximize generalization performance and efficiency (e.g., in terms oflatency, model size or memory consumption). The limiting factor of conventional NAS is the computationalburden of the search, which requires multiple rounds of training and validating neural network architectures(Zoph & Le, 2017; Real et al., 2017).To mitigate this cost, various approaches have been proposed toaccelerate the search process. For example, some of these methods early terminate the training process forpoorly performing configurations (Li et al., 2018) or extrapolating learning curves (White et al., 2021b).Weight-sharing NAS (Pham et al., 2018; Liu et al., 2019a) addresses the cost issue by training a singlesuper-network consisting of all architectures in the search space, such that each path represent a uniquearchitecture. Initially, Liu et al. (2019a) framed this as a bi-level optimization problem, where the innerobjective involves the optimization of the network weights, and the outer objective the selection of thearchitecture. After training the super-network, the best architecture is selected based on the shared weightsand then re-trained from scratch. However, several papers (Li & Talwalkar, 2020; Yang et al., 2020) reportedthat this formulation heavily relies on the search space and does not yield better results than just randomlysampling architectures. To address this limitation, Yu et al. (2020) proposed a two-stage NAS process. Inthe first stage, the super-network is trained by updating individual sub-networks in each iteration, insteadof updating the entire super-network. After training, the final model is selected by performing gradient-freeoptimization based on the shared weights of the super-network, without any further training. Concurrently,Cai et al. (2020) applies a similar approach for convolutional neural networks in the multi-objective settingby first training a single super-network and then searching for sub-networks to minimize latency on sometarget devices. Related to our work is also the work by Xu et al. (2021), which searches for more efficientBERT architectures during the pre-training phase. Structural Pruning involves removing parts of a trained neural network, such as heads (Michel et al., 2019),or entire layers (Sajjad et al., 2022), to reduce the overall number of parameters while preserving performance.Individual components are pruned based on a specific scoring function, using a manually defined threshold.For transformer-based architectures, Michel et al. (2019) observed that a significant number of heads, up toa single head in a multi-head attention layer, can be deleted after fine-tuning without causing a significantloss in performance. Voita et al. (2019) proposed L0 regularization as a means to prune individual heads ina multi-head attention layer. Kwon et al. (2022) prunes individual heads and units in the fully-connectedlayers after fine-tuning according to the Fisher information matrix. Sajjad et al. (2022) demonstrated thatit is even possible to remove entire layers of a pre-trained network prior to fine-tuning, with minimal impacton performance. In comparison to our data-driven approach, Sajjad et al. (2022) suggested using predefinedheuristics (e.g., deleting top / odd / even layers) to determine layers to prune. However, as shown in ourexperiments, the appropriate architecture depends on the specific task, and more data-driven methods arenecessary to accurately identify the best layers to prune. Distillation (Hinton et al., 2015) trains a smaller student model to mimic the predictions of a pre-trainedteacher model. For instance, Sanh et al. (2020) used this approach to distill a pre-trained BERT model (De-vlin et al., 2019) into a smaller model for fine-tuning. Jiao et al. (2019) proposed a knowledge distillationapproach specifically for transformer-based models, which first distills from a pre-trained teacher into asmaller model and then performs task-specific distillation in a second step based on a task augmenteddataset. Related to our method is also AdaBERT (Chen et al., 2020) which trains task-specific convolu-",
  "tional neural networks based on differentiable NAS (Liu et al., 2019a) by distilling the knowledge of a PMLsuch as BERT": "Unlike pruning-based methods, distillation allows for complete architectural changes beyond merely drop-ping individual components. However, from a practical standpoint, determining the optimal structure andcapacity of the student network needed to match the performance of the teacher network also amounts to ahyperparameter and neural architecture search problem. Additionally, training a student network requiresa significant amount of computational resources. For example, the model by Sanh et al. (2020) was trainedfor around 90 hours on 8 16GB V100 GPUs. This cost can be amortized by fine-tuning the student modelto solve many different tasks, but, depending on the downstream tasks, it potentially requires a substantialamount of iterations which is not always desirable for practitioners who aim to solve a single specific task.This is especially important in the multi-objective setting where many networks need to be distilled to coverthe full size/accuracy Pareto front. Quantization (Dettmers et al., 2022; Dettmers & Zettlemoyer, 2023) reduces the precision of model pa-rameters from floating-point numbers to lower bit representations (e.g., 8-bit integers). The main advantageof quantization is the reduction in memory footprint. However, as we show in the Appendix F, this does notnecessarily lead to faster latency. Quantization is independent of our NAS approach and can be employedon the pruned network to further decrease memory usage.",
  "Multi-Objective Sub-Network Selection": "We consider a pre-trained transformer model based on an encoder-only architecture, such as for exampleBERT (Vaswani et al., 2017), with L non-embedding layers, each composed of a multi-head attention (MHA)layer followed by a fully connected feed forward (FFN) layer. However, all methods presented here can also beapplied to decoder or encoder-decoder based architectures. Given an input sequence X Rndmodel, wheren represents the sequence length and dmodel the size of the token embedding, the MHA layer is definedby: MHA(X) = Hi Att(W (i)Q , W (i)K , W (i)V , W (i)O , X) where W (i)Q , W (i)K , W (i)V Rdmodeld and W (i)ORHddmodel are weight matrices. Att() is a dot product attention head (Bahdanau et al., 2015) and H is thenumber of heads. The output is then computed by XMHA = LN(X + MHA(X)), where LN denotes layernormalization (Ba et al., 2016). The FFN layer is defined by FFN(X) = W1(W0X), with W0 RUdmodeland W1 RdmodelU, where U denotes the intermediate size and () is a non-linear activation function.Also here we use a residual connection to compute the final output: xF F N = LN(XMHA + FFN(XMHA)). We define a binary mask Mhead {0, 1}LH for each head in the multi-head attention layer and a binarymask Mneuron {0, 1}LU for each neuron in the fully-connected layers. The output of the l-th MHAlayer and FFN layer is computed by MHAl(X) = Hi Mhead[i, l]Att() and FFNl(X) = W1 Mneuron[l, :](W0X), respectively, where denotes element-wise multiplication. Now, lets define a search space that contains a finite set of configurations to define possible sub-networks sliced from the pre-trained network. We define a function CREATEMASK that maps from a configu-ration Mhead, Mneuron to binary masks. Lets denote the function f0 : R as the validation errorof the sub-network defined by configuration after fine-tuning on some downstream task. To compute thevalidation score induced by we place corresponding masks Mhead, Mneuron over the network. Additionally,we define the total number of trainable parameter f1 : N of the sub-network. Our goal is to solve thefollowing multi-objective optimisation problem:",
  "Weight-sharing based Neural Architecture Search": "Following previous work(Yu et al., 2020; Wang et al., 2021), our weight-sharing based NAS approachconsists of two stages: the first stage is to treat the pre-trained model as super-network and fine-tune it onthe downstream task. We explore different super-network training strategies from the literature that updateonly parts of the network in each step, to avoid co-adaption of sub-networks. The second stage, utilizesmulti-objective search strategies to approximate the Pareto-optimal set of sub-networks.",
  "Super-Network Training": "In the standard NAS setting, we would evaluate f0() by first fine-tuning the sub-network defined by onthe training data before evaluating on the validation data. The weights of the sub-network are initializedbased on the pre-trained weights. While more recent NAS approaches (Li & Talwalkar, 2020; Klein et al.,2020) accelerate the search process by early stopping poorly performing sub-networks, this still amounts toan optimization process that requires the compute of multiple independent fine-tuning runs. The idea of two-stage weight-sharing-based NAS (Yu et al., 2020) is to train a single-set of shared weights,dubbed super-network, that contains all possible networks in the search space. After training the super-networks, evaluating f0() only requires a single pass over the validation data. We consider the pre-trained network as super-network with shared weights that contains all possible sub-networks . To avoid that sub-networks co-adapt and still work outside the super-network, previouswork (Yu et al., 2020; Wang et al., 2021) suggested to update only a subset of sub-networks in each stochasticgradient descent step, instead of updating all weights jointly. We adapt this strategy and sample sub-networksaccording to the sandwich rule (Yu et al., 2020; Wang et al., 2021) in each update step, which always updatesthe smallest, the largest and k random sub-networks. The smallest and largest sub-network correspond tothe lower and upper bound of , respectively. For all search spaces define below, the upper bound isequal to full network architecture, i.e, the super-network and the lower bound removes all layer except theembedding and classification layer. Additionally, we use in-place knowledge distillation (Yu et al., 2019) which accelerate the training process ofsub-networks. Given the logits supernet(x) of the super-network, which we obtain for free with the sandwichrule, and the logits of a sub-network (x), the loss function to obtain gradients for the sub-networks followsthe idea of knowledge distillation:",
  "Search Space": "First, we compare the search spaces definitions from .3 using weight-sharing based NAS. We fine-tune the super-network as described in .2 and sample 100 sub-networks uniformly at random tocompute the hypervolume. Conclusions: Within this budget (see ), the SMALL search space achieves the best performanceacross all datasets, except for COLA. Interestingly, even though the MEDIUM search space allows for a morefine-grained per layer pruning, it leads to worse results. We attribute this to the non-uniform distributionof parameter count as described in .3. The LAYER search space often out-performs the MEDIUMand LARGE search space, but, except for COLA, leads to Pareto sets that under-perform compared tothe SMALL search space. The LARGE search space, which is a superset of the other search spaces, seemsinfeasible to explore with random sampling over so few observations. We use the SMALL search space forthe remaining experiments.",
  "Experiments": "We evaluate different types of NAS for structural pruning on eight text classification tasks, including textualentailment, sentiment analysis and multiple-choice question / answering. We provide a detailed descriptionof each task in Appendix C. All tasks come with a predefined training and evaluation set with labels anda hold-out test set without labels. We split the training set into a training and validation set (70%/30%split) and use the evaluation set as test set. We fine-tune every network, sub-network or super-network, for",
  ": Distribution of the parameter count f1() for uniformly sampled": "5 epochs on a single GPU. For all multi-objective search methods, we use Syne Tune (Salinas et al., 2022)on a single GPU instance. We use BERT-base (Devlin et al., 2019) (cased) and RoBERTa-base (Liu et al.,2019b) as pre-trained network, which consists of L = 12 layers, I = 3072 units and H = 12 heads (otherhyperparameters are described in Appendix A). While arguably rather small for todays standards, they stillachieve competitive performance on these benchmarks and allow for a more thorough evaluation. We alsopresent a comparison to quantization in Appendix F.",
  ": Example to compute the Hypervol-ume HV (Pf|r), corresponding to the sum ofthe rectangles, across a reference point r anda set of points Pf = {y0, y1, y2, y3}": "We now present an evaluation of different multi-objectiveNAS approaches on our benchmarking suite. To quantifythe performance of a Pareto set Pf, we compute for eachPareto set the Hypervolume (Zitzler et al., 2003) and reportthe regret, i e. the difference to the best possible Hypervol-ume averaged across all repetitions. Given a reference pointr RM, the Hypervolume HV (Pf|r) = (yPf [y, r]) is de-fined as the M-th dimensional Lebesgue measure betweenthe Pareto set Pf and the reference point r, where [y, r] rep-resents the hyper-rectangle between y and r (see for an example). To compute the Hypervolume, we first normalize each ob-jective based on all observed values across all methods andrepetitions via Quantile normalization. This results in a uni-form distribution between , and we use r = (2, 2) asreference point, which means the highest possible Hypervol-ume would be 4. We evaluate each method with 10 differentseeds for the random number generation.",
  "Standard Neural Architecture Search": "We compare the following multi-objective search methods to tackle the NAS problem described in where each sub-network is fine-tuned in isolation. We provide a more detailed description of each methodin Appendix D. A simple multi-objective local search (LS) described in Appendix D. This is inspired bythe work of White et al. (2021a), which showed that local search often performs competitively on NASproblems. Random search (RS) (Bergstra & Bengio, 2012) samples architectures uniformly at random fromthe search space. A multi-objective version of the regularized evolution (REA) algorithm (Real et al., 2019),frequently used in the NAS literature. Compared to the original singe-objective algorithm, we sort elementsin the population via non-dominated sorting. Expected Hypervolume Improvement (EHVI) (Daulton et al.,2020) is a multi-objective Bayesian optimization strategy that samples candidate points using a Gaussianprocess model of the objective function. Lastly, we include MO-ASHA (Schmucker et al., 2021), a multi-objective version of asynchronous successive halving (Li & Talwalkar, 2020; Jamieson & Talwalkar, 2016) thatterminates the training process of poorly performing candidates early to accelerate the overall optimizationprocess. While MO-ASHA could potentially be combined with a model-based approach, as commonly donefor single-objective optimization (Falkner et al., 2018; Klein et al., 2020), here we followed the originalalgorithm and sample candidate uniformly at random from the search space. Following common experimental practice from the HPO literature, we aggregate results by computing theaverage ranks of each methods across repetitions, datasets and time steps. Following Feurer et al. (2015),we sample 1000 bootstrap samples across all repetitions and tasks, to compute the rank of each method andaverage across all samples. Results are shown in a. We shows results for each individual task inAppendix E. Conclusions: Somewhat surprisingly RS is a strong baseline on these benchmarks, outperforming moresophisticated approaches such as EHVI or MO-REA. Fine-tuning these models is often unstable (Mosbachet al., 2021) especially on smaller datasets, resulting in high observation noise. For the RoBERTa-base model,LS often performs competitively to RS given a sufficient large budget. MO-ASHA quickly stops the evaluationof poorly performing sub-networks and hence outperforms RS on average. However, on small datasets suchas RTE, fine-tuning is faster than the non-dominated sorting of MO-ASHA, such that it converges slowerthan RS (see Appendix E).",
  "(b) RoBERTa-base": ": Comparison of the four different search spaces using weight-sharing based NAS. We sample 100random sub-networks uniformly at random using the fine-tuned weights of the super-network. The SMALLsearch space dominates the other search spaces except for the COLA dataset. While SMALL is a subsetof MEDIUM and LARGE, these spaces are too high-dimensional to be explored with a sensible computebudget. First two rows show results for BERT-base-cased and last two rows for RoBERTa-base. Hypervolume. We repeat this process 10 times with a different seed for the random number generation. Foreach repetition we use the exact same set of random sub-networks for all super-network training strategies.",
  "(b) Average ranks weight-sharing based NAS": ": Average ranks of the different multi-objective search methods across all dataset for a) standard-NASand b) weights-sharing based NAS for both BERT-based cased and RoBERTa-base models. Random searchbased approaches (RS, MO-ASHA)perform competitively in the standard NAS setting. EHVI outperformsother methods at earlier time steps which we attribute to its probabilistic model. Given a sufficient amountof time often finds well-performing Pareto fronts.often finds a well-perofforming Pareto fronts.",
  "full: Implements the training protocol described in .2, i.e it combines the sandwich rulewith in-place knowledge distillation to update sub-networks": "Conclusions: shows the Hypervolume across all task for BERT-base and RoBERTa-base, re-spectively. Standard fine-tuning and just randomly sampling a sub-network leads to poorly performing Paretosets compared to other methods. The only exception is the COLA dataset, where standard fine-tuning some-times works best. However, we also observe high variation across runs on this datasets. Linearly increasingthe probability of sampling a random sub-networks improves to just random sampling. Better results areachieved by using the sandwich rule or knowledge distillation. Thereby, combining both slightly improvesresults further. Multi-Objective SearchLastly, we compare in b average ranks of the same multi-objectivesearch methods as for standard-NAS. We follow the same process as described in .1.2 to computeaverange ranks. We do not include MO-ASHA in this setting, since each function evaluation only consists ofvalidating a sub-network based on the shared weights of the super-network after fine-tuning and hence doesnot allow for a multi-fidelity approach. Optimization trajectories for all datasets are in Appendix E. Conclusions: As for standard NAS, RS is a surprisingly strong baseline. EHVI performs better at earlytime-steps. We found using the shared weights of the super-networks for evaluation results in a much smallerobservation noise than fine-tuning each sub-network in isolation, which is less deceiving for the probabilisticmodel of EHVI. Given enough time, LS starts outperforming RS and EHVI on RoBERTa-base model andperforms competitively to EHVI on the BERT-base model.",
  ": Comparison of different strategies to fine-tune the super-network. First two rows show BERT-baseand last two rows show RoBERTa-base": "The first phase computes a binary mask for heads and units by computing the diagonal Fisherinformation matrix. The matrix is then rearranged by a block-approximated Fisher informationmatrix. In the last step, the masked is further tuned by minimizing the layer-wise reconstructionerror. This method operates in the LARGE search space described in .3. We run RFPwith different values for {0.1, 0.2, ..., 0.9} to obtain a Pareto set of architectures. Layer Dropping (LD): Following Sajjad et al. (2022) we first remove the top n 1, ..., L 1 layersand fine-tune the remaining layers directly on the downstream task. To obtain a Pareto set of Npoints, we fine-tune N models with different amount of layers removed. This method serves as asimple heuristic to explore the LAYER search space.",
  "Conclusions": "We propose NAS for structural pruning of fine-tuned PLMs. By utilising a multi-objective approach, we canfind the Pareto optimal set of sub-networks that balance between model size and validation error. Returninga Pareto set of sub-networks allows practitioners to select the optimal network without running the pruningprocess multiple times with different thresholds. We also provide an in-depth analysis of recently developedtwo-stage weight-sharing approaches in this setting, which require only a single fine-tuning run of the PLM. Future work could explore the instruction tuning (Wei et al., 2022) setting, where the final model is evaluatedin a few-shot setting. Our approach samples sub-networks uniformly at random, which allocates the sameamount of update steps to all sub-networks on average. Future work could explore more complex samplingdistribution biased towards sub-networks closer to the Pareto set.",
  "H. Cai, C. Gan, T. Wang, Z. Zhang, and S. Han. Once-for-all: Train one network and specialize it forefficient deployment. In International Conference on Learning Representations (ICLR20), 2020": "D. Chen, Y. Li, M. Qiu, Z. Wang, B. Li, B. Ding, H. Deng, J. Huang, W. Lin, and J. Zhou. Adabert:Task-adaptive bert compression with differentiable neural architecture search. In Proceedings of the 29thInternational Joint Conference on Artificial Intelligence (IJCAI20), 2020. S. Daulton, M. Balandat, and E. Bakshy. Differentiable expected hypervolume improvement for parallelmulti-objective bayesian optimization. In Proceedings of the 34th International Conference on Advancesin Neural Information Processing Systems (NeuRIPS20), 2020.",
  "T. Dettmers and L. Zettlemoyer. The case for 4-bit precision: k-bit inference scaling laws. arXiv:2212.09720[cs.LG], 2023": "T. Dettmers, M. Lewis, Y. Belkada, and L. Zettlemoyer. Llm.int8(): 8-bit matrix multiplication for trans-formers at scale. In Proceedings of the 36th International Conference on Advances in Neural InformationProcessing Systems (NeuRIPS22), 2022. J. Devlin, M. Chang, K. Lee, and K. Toutanova. BERT: Pre-training of deep bidirectional transformers forlanguage understanding. In Proceedings of the 2019 Conference of the North American Chapter of theAssociation for Computational Linguistics: Human Language Technologies, 2019.",
  "C. White, S. Nolen, and Y. Savani. Exploring the loss landscape in neural architecture search. In Proceedingsof the 37th conference on Uncertainty in Artificial Intelligence (UAI21), 2021a": "C. White, A. Zela, R. Ru, Y. Liu, and F. Hutter. How powerful are performance predictors in neural archi-tecture search? In Proceedings of the 35th International Conference on Advances in Neural InformationProcessing Systems (NeuRIPS21), 2021b. J. Xu, X. Tan, R. Luo, K. Song, J. Li, T. Qin, and T. Liu. NAS-BERT: task-agnostic and adaptive-sizebert compression with neural architecture search. In Proceedings of the 27th ACM SIGKDD InternationalConference on Knowledge Discovery & Data Mining (KDD21), 2021.",
  "BMasking": "Algorithm 1, 2, 3 and 4 show pseudo code for the LAYER, SMALL, MEDIUM and LARGE search space,respectively. Note that, 1 indicates a vector of ones. For a matrix M, we write M[:, : N] to denote the firstN columns for all rows and, vice versa, M[: N, :] for the first N rows.",
  "DAdditional Details NAS Search": "In this section, we present more details about all multi-objective search strategies that we used for boththe standard NAS scenario, where each sub-network is fine-tuned from the pre-trained weights, and theweight-sharing scenario, where sub-networks are evaluated using the shared weights of the super-network. Random Search: We follow the standard random search approach (Bergstra & Bengio, 2012),where for a fixed number of iterations T, we sample in each iteration t {0, ..., T} a randomsub-networks uniformly from the search space t U(). MO-Regularized Evolution: We adapt the original algorithm proposed by (Real et al., 2019)which maintains a population of architectures and, in each iteration, removes the oldest architecturefrom the population. To sample a new configuration, we follow Real et al. (2019) and first sample aset of random configurations and mutate the configuration from the set with the lowest rank. Insteadof just using the validation performance, we rank each element in the population via non-dominatedsorting. We mutate an architecture by first sampling a random dimension of the search spaced U(0, ||) and than sample a new value for this dimension [d] = U(d). This follows the sameprocess as for our multi-objective local search described below. Expected Hypervolume Improvement:Bayesian optimization optimizes a single functionf : R, where in each iteration t we select the most promising candidate in the input space arg max ap(f|Dt)() according to some acquisition function a : R. The idea of this ac-quisition function is to trade-off exploration and exploitation, based on a probabilistic model of theobjective function p(f|Dt), trained on some observed data points Dt = {(0, y0), ...(t, yt)}, wherey N(f, ). To adapt Bayesian optimization to the multi-objective setting, we follow Daulton et al.(2020) and use a single Gaussian process p(f|D) for all objective functions f() = {f0(), ..., fk()}.Here, the acquisition function a() = Ep(f|D)[HV I(f()] computes the expected improvement ofthe hypervolume HV I(y) = HV (Pf y, r) HV (Pf, r) based on the current Pareto front Pf andsome reference point r. Multi-Objective ASHA: Given a halving constant , a minimum rmin and maximum rmax numberof epochs for fine-tuning, successive halving defines a set of rungs R = {rkmin|k = 0, ..., K} wherefor simplicity we assume that rmax rmin = K. Starting from a set of randomly sampled configurationsC = {0, ..., n}, successive halving evaluates all configuration on the first rung level r0 and promotesthe top 1 configuration for the next rung while discarding the others. This process is iterateduntil we reach the maximum rung level rk = rmax. We follow common practice (Li et al., 2017;Falkner et al., 2018), and run multiple rounds of successive halving until we hit a maximum budget(defined in wall-clock time).Asynchronous successive halving(Li et al., 2018)adapts standardsuccessive halving to distributed asynchronous case, which requires some changes in the decisionmaking routing. To cater for the multi-objective setting, we use multi-objective ASHA (Schmuckeret al., 2021) which uses non-dominated sorting instead of just the validation accuracy to rankconfigurations on each rung-level. Multi-objective Local Search: Previous work (White et al., 2021a) has demonstrated that simplelocal search often performs competitively compared to more advanced NAS methods. We proposea straightforward multi-objective local search approach. Starting from the current Pareto front Pf,which is initialized by some starting point, we randomly sample an element Pf and thengenerate a random neighbor point by permuting a single random entry of .",
  "FQuantization": "Quantization (Dettmers et al., 2022; Dettmers & Zettlemoyer, 2023) is a powerful technique that significantlyreduces the memory footprint of neural networks. However, its impact on latency is not immediate, especiallywhen dealing with batch sizes that can not fit into the cache of the device Dettmers & Zettlemoyer (2023).With our flexible NAS framework we can simply replace objectives and directly optimize latency on thetarget device instead of parameter count. left shows the Pareto set obtained with our NAS approach, where we optimize latency instead ofparameter count on the COLA dataset across 3 different GPU types. Additionally, we evaluate the perfor-mance of the unpruned super-network with 8-bit (Dettmers et al., 2022) and 4-bit (Dettmers & Zettlemoyer,2023) quantization. While quantization substantially reduces the memory footprint ( right), it ac-tually leads to worse latency. While quantization introduces a small overhead due to the additional roundingsteps, the latency could potentially be reduced by optimizing the low-level CUDA implementation. Some-what surprisingly using a int-8bit quantization leads to high performance drop on some hardware. NASeffectively reduces the sizes of weight matrices, leading to reduced GPU computation and, thus, is lesshardware depend. We can also apply quantization to sub-networks, making it orthogonal to our NAS methodology and offeringfurther improvements to the memory footprint. Overall, these findings shed light on the trade-offs between"
}