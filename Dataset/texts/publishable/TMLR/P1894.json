{
  "Abstract": "With the advent of score-matching techniques for model training and Langevin dynamics forsample generation, energy-based models (EBMs) have gained renewed interest as generativemodels. Recent EBMs usually use neural networks to define their energy functions. In thiswork, we introduce a novel hybrid approach that combines an EBM with an exponentialfamily model to incorporate inductive bias into data modeling. Specifically, we augmentthe energy term with a parameter-free statistic function to help the model capture key datastatistics. Like an exponential family model, the hybrid model aims to align the distributionstatistics with data statistics during model training, even when it only approximately max-imizes the data likelihood. This property enables us to impose constraints on the hybridmodel. Our empirical study validates the hybrid models ability to match statistics. Fur-thermore, experimental results show that data fitting and generation improve when suitableinformative statistics are incorporated into the hybrid model.",
  "Introduction": "Energy-based models (EBMs) (Murphy, 2012; LeCun et al., 2006; Ngiam et al., 2011), which provide aflexible way of parameterization, are widely used in data modeling such as sensitive estimation (Nguyen& Reiter, 2015; Wenliang et al., 2019; Jiang & Xiao, 2021), structured prediction (Belanger & McCallum,2016; He & Jiang, 2020; Pan et al., 2020), and anomaly detection (Zhai et al., 2016; Liu et al., 2020). Theyrecently have achieved successes as generative models for data of different modalities, such as images (Du &Mordatch, 2019; Vahdat & Kautz, 2020), texts (Deng et al., 2020; Yu et al., 2022), graph (Liu et al., 2021;Chen et al., 2022; Xu et al., 2022) and point cloud (Xie et al., 2020; 2021; Luo & Hu, 2021), thanks to theadvent of new training methods of score matching (Song & Ermon, 2019; Song et al., 2020b; Ho et al., 2020). Previous EBMs often have linear energy functions in model parameters, so they fall into the category ofexponential-family models (Wainwright et al., 2008). An exponential family model has a few nice properties.It matches data statics when it maximizes the likelihood of the training data. Among all models, it alsohas the largest entropy with the constraint of matching statistics. When constructing an EBM, statisticfunctions are often used to capture key statistics of the data. However, in recent years, EBMs have usedneural networks as their energy functions. Since their energy functions are not linear in trainable parameters,they are not exponential-family models anymore. In our construction of EBMs, we propose to include linear terms defined with special statistic functions inthe energy function to retrieve some good properties of exponential-family models. Even with complex datatypes such as point clouds and graphs, we still often have domain knowledge to specify statistic functionsso that the model can capture desired properties. Therefore, the linear term with special statistic functions",
  "allows us to inject inductive biases in model fitting. This framework applies to the domains where priorinformation can be written in statistic functions": "The proposed method is very useful for data fitting in multiple iterations. Note that data fitting is oftenan iterative procedure (Gelman et al., 1995), and the goodness of fit is often monitored by the discrepancybetween statistics computed respectively from model samples and real data.With our method, we canimprove the model by adding corresponding terms to narrow the discrepancy. Taking molecule generationas an example, when we observe molecule samples violate the valency constraint, we can incorporate theknowledge of valency rules to improve the model. We evaluate the effectiveness of this new approach on three tasks: modeling molecular data, fitting handwrit-ten digits, and modeling point clouds. The results demonstrate the effectiveness of the proposed approachin a wide range of data-fitting domains.",
  "Related Work": "The need to incorporate inductive bias into a generative model has drawn researchers attention for a longtime. Zhu et al. (1997) proposes the general principle for incorporating prior knowledge into a generativemodel. Khalifa et al. (2020) introduces a novel method for text generation using EBMs to enforce desiredstatistical constraints. Their framework employs the EBM model to approximate a pre-trained languagemodel instead of directly fitting the data. Korbak et al. (2021) extends this approach to code generation,imposing a constraint that generated sequences are compilable. Qin et al. (2022) applies energy constraintsto a transformer decoder to control text semantics and style. Lafon et al. (2023) learns the hybrid modelcombined with the Gaussian Mixture Model (GMM) and EBMs for out-of-distribution detection. In contrast,our work focuses on improving data fitting with inductive bias. Another thread of research changes a generative models behavior during the sampling stage (Dhariwal &Nichol, 2021). For example, Chung et al. (2022) proposes to use manifold constraint to guide a pre-traineddiffusion models sampling path. Kim et al. (2022); Zhao et al. (2022); Song et al. (2022); Yu et al. (2023); Liuet al. (2023); Bansal et al. (2023); Li & Liu (2024) propose different guidance for different specific problemsto boost the performance of each task. In contrast, our method aims to fit the original data distribution moreaccurately by adding the statistics term. Notably, the classifier-guidance method operates during inferencewith manually tuned weights, whereas our model learns the inductive bias strength corresponding to datadistribution statistics.",
  "Z.(1)": "Here, x Rd, d is the feature dimension. Z is the partition function: Z =exp(E(x))dx, which isusually intractable. Various methods exist to train an energy-based model. Maximum likelihood trainingwith Markov Chain Monte Carlo (MCMC) sampling (Hinton, 2002) is one standard method to train theEnergy-based model, but MCMC is typically computationally expensive. A modern EBM often devises the energy function E(x) with a neural network, and represents learnableparameters of the neural network (Song & Kingma, 2021). These models are usually fit by score matching(Hyvrinen & Dayan, 2005; Kingma & Cun, 2010), which avoids the explicit calculation of the Z. Scorematching minimizes the mean square error between the model score function s(x) = x log p(x) and thedata score function x log pdata(x):",
  "(vx log pdata(x) vx log p(x))2.(4)": "The parallelizable variant of sliced Score Matching is Finite difference sliced score matching (FDSSM) (Panget al., 2020). Another score-matching variant is Denoising Score Matching (Vincent, 2011)(DSM). DSMavoids the computation of the second-order derivatives by adding some small noise perturbation to the data.Then DSM learns the noisy data distributions q(x) pdata(x):",
  "Z().(6)": "In this form, we assume the base measure is 1. The statistic function T(x) decides the distribution type,while the parameter decides the actual distribution of that type. The partition function Z(), which isoften hard to compute, ensures that the PDF integrates to 1 over the domain of the distribution. The statistic function T(x) is also meaningful in capturing data statistics. The statistics1NNi=1 T(xi)computed from a dataset (xi : i = 1, . . . , N) is called the datas sufficient statistics, which provide sufficientinformation about the distribution parameter. Data fitting with p(x; ) is centering at sufficient statistics:if the data likelihood is maximized with a particular under the model p(x; ), then Ep(x;) [T(x)] =1NNi=1 T(xi).",
  "Here Z(, ) =": "x expF(x) + T(x)dx is the partition function. F(x) R is a real-valued functionconstructed with a neural network parameterized by . The statistic function T(x) itself has no learnableparameters, but it is multiplied to the learnable parameter in the linear term. The hybrid model still possesses a nice property of the exponential family distribution. Model fitting stillaims to match the distribution mean of sufficient statistics to the sample mean (Wainwright et al., 2008).Let l(, ) denote the log-likelihood of the data, A(, ) = log Z(, ):",
  "If the weight is at the local minimum, then l = 0, which leads to the conclusion": "With this property, we will construct a hybrid model with specific statistics in its linear energy terms. Modelfitting will pay attention to the specified statistics in the data. It can be viewed as an approach to injectinginductive bias into the model. Before we discuss such models in real applications, we first consider theirtraining procedure.",
  "i=1log p,(xi) L(, ; )(13)": "Model training with score matching approximately maximizes the data likelihood. With a similar spirit,the learned model approximately matches the data statistics.Furthermore, the difference between thedistribution mean and the data mean: T(x) = Expdata[Tx] Expmodel[Tx] approaches to zero as themodel parameter approaches a local maximum of the log-likelihood. This can be proved based on theconvexity of the log-likelihood function w.r.t. parameter . For any fixed, exp (F(x)) can be viewedas a base measure, then the hybrid distribution in (7) falls in the exponential family of distributions, andits log-likelihood is convex w.r.t. by Wainwright et al. (2008). Therefore, parameters (, ) at a localmaximum of the likelihood indicates that is at the maximum of the likelihood function with fixed.From convex analysis (Boyd & Vandenberghe, 2004), the norm of the gradient approaches zero as (, )approaches the local maximum.",
  "Inject inductive bias through the function T(x)": "As in an exponential-family model, the statistic function points the hybrid models attention to specialstatistics of data distribution. Therefore, it is a convenient approach to inject inductive bias into the model.Without any restrictions over the function form, the statistic function is a convenient approach to expresssuch inductive bias. In this section, we leverage this property to develop models for three applications toshow its practical value.",
  "Statistic function for fitting molecule data": "A molecular generative model is an important tool for chemical applications, and it is often defined as anEBM (Liu et al., 2021; Niu et al., 2020; Jo et al., 2022). Here, we consider two molecular generative models,EDP-GNN Niu et al. (2020) and GDSS Jo et al. (2022), which both can be viewed as energy-based modelsand specify distributions of molecules through their molecular graphs and atom types. Assume the atomhas k types. In these two models, the random variable x = (b, A) represents a molecule graph containingn nodes (atoms), with b {1, . . . , k}n representing atom types in a molecule, and A being the adjacencymatrix representing the connectivity of corresponding atoms. Without the statistic function T, these twomodels use a neural network to define xF(x). An energy-based model based on a neural function often assigns non-zero probabilities to invalid moleculeseven though the data contains zero such examples. In particular, all molecules in the data satisfy valencyconstraints. For example, a carbon atom has four bonds, an oxygen atom has two bonds, and a nitrogen atomhas three or five bonds. However, samples from an EBM defined by a neural energy function often violatethe valency constraint. While there are methods employing other techniques to enforce valency constraintsin the sampling stage (Zang & Wang, 2020), we consider such constraints in a canonical approach to datafitting. Specifically, we express the valency constraint as a statistic function: a valid molecule has zero valuefrom the function, while an invalid molecule has a positive value. Let the constant vector v {1, . . . } storevalences for k atom types, with being the maximum valence possible. Then, the valency constraint is:",
  "Statistic function for fitting handwritten digits": "In the handwritten digits dataset LeCun (1998), we observe that the mar-gin area surrounding the digit has pixels all taking value 0. showssome examples of handwritten digit images, and it can be observed thatthe pixels in the margin are all zeros. A model with this knowledge willbetter fit the data. We introduce a statistic function defined as the sum ofpixels located at the boundaries of the image. Let x Rhh represent oneimage, where h and h are the height and width of the image, respectively.",
  "Statistic function for fitting point clouds": "Point clouds have been widely used in computer graphics, computer vision, and robotics (Achlioptas et al.,2018; Xiao et al., 2023; He et al., 2023; Huang et al., 2024). Several algorithms have been proposed forpoint cloud generation. In this work, we consider the DPM (Luo & Hu, 2021) and latent diffusion model.These two methods can be taken as directly modeling the joint distribution of all the points, meaning thatxF(x) is the gradient of all the points joint distribution in our framework.",
  "T(x) = tr(xLx).(16)": "Here x RN3 is the 3D coordinates of N points. L is the sparse Laplacian matrix of the point cloudsk-nearest neighbor (k-nn) graph. The k-nn graph is constructed using a kd-tree, with a time complexity ofO(n log n) (Preparata & Shamos, 2012), where n represents the number of data points. This is computa-tionally more efficient than the naive k-nn method and can scale better. The function T(x) computes thedifferences between a node and its neighbors in the graph. It is a measure of the uniformness of points in thepoint cloud. With this statistic function, the model fitting procedure will explicitly adjust the uniformnessof generated point clouds to the same level as training samples.",
  "Experiments": "We evaluate the effect of our special statistics on data fitting through three generative tasks: molecule graphgeneration, image generation, and point cloud generation. Experiments related to molecule generation aredetailed in .1. .2 discussed the image generation experiments. .4 discussed thepoint cloud generation task.Each section briefly discusses each tasks experiment setup and evaluationmetrics we used. More details are in the Appendix.",
  "In the first experiment, we incorporate our statistic function derived from valency constraints into an EBMmodel for molecule generation": "Experiment Settings: Our model is evaluated on the QM9 dataset (Pinheiro et al., 2020), which comprisesa diverse collection of 133,885 molecules, each containing up to a maximum of 9 atoms. Following previousstudies, the molecules are kekulizated using the RDKit library (Bento et al., 2020), and hydrogen atoms areremoved. Two essential metrics are employed to evaluate the quality of the generated molecules: validity andvalency ratio. A valid molecule must satisfy certain chemical constraints and rules, indicating a chemicallyreasonable structure. Validity is determined by calculating the proportion of valid molecules without valencycorrection or edge resampling. We evaluated our strategy on two methods: EDP-GNN(Niu et al., 2020) andGDSS(Jo et al., 2022). We trained GDSS for 300 epochs, the batch size is 1024, and the learning rate is5e-3. We trained EDP-GNN for 1000 epochs, the batch size is 1024, and the learning rate is 8e-3. Results:We first evaluate the validity ratio of generated molecules. The results are shown in the leftcolumn of Tab. 1. Ten thousand molecules are sampled for evaluation. The results show that the modelimproves the validity ratio of the samples with our new term expressing the validity information.Wefurther check expectation E [T(x)] in two distributions with and without the statistic function. Let T (x) =Epmodel [T(x)] Eptrain [T(x)] denote the difference between the distribution mean and the sample mean ofthe statistics term. The results clearly show that our model pays attention to the new term and tries tomatch its mean to the sample mean of the data. It demonstrates that the extra valency term does help themodel learn the valency property.",
  "Fitting data of hand-written digits": "We evaluated the effectiveness of our method on the MNIST dataset, a widely used benchmark for vari-ous image downstream tasks. Each image in the MNIST dataset is a 28x28 grayscale representation of ahandwritten digit from 0 to 9. Experiment Settings: We experimented on the two forward SDEs: VP-SDE and VE-SDE. The boardingpixels are extracted using a mask. The size of the mask is 22 20. The model is trained for 1000 epochsand then compared to the likelihood. The batch size is 4096, and the learning rate is 1e-2. The learning rateis kept constant for the first 300 epochs and decreases linearly from 300 to 1000 epochs. The parameter forthe statistics term is initialized to be zero. We assess the performances of competing models by comparingtheir likelihoods on the test set. Results: We evaluated our model via the test set negative log-likelihood (NLL) in terms of bits/dim (bpd).Tab. 2 reports the averaged NLLs of probability flow ODE Song et al. (2020b) over two repeated runs oflikelihood computations. The results suggest incorporating the statistics term into the model improves thelikelihood. This improvement implies that the models distribution aligns better with the observed datawhen considering the inductive bias. The results also show that the difference between the distributionmean and the sample mean of the new statistic is smaller with our method, which implies that our modelbetter captures the data statistic.",
  "Fitting data of small grayscale images": "We tested our approach on the FashionMNIST dataset Kayed et al. (2020). Similar to the MNIST dataset,we added the constraint on the boarding pixels. Let x Rhh represent one image, where h and h arethe height and width of the image, respectively. Let h and w represent the height and width of imagesin the dataset, and be the width of the margin, then we use the same statistic function T(x) in (15)that computes the gray level of the border area of an image. In real data, the border area contains mostlywhite pixels. Without any specification, a typical generative model often overlooks such patterns. Here, weexplicitly emphasize this pattern in the model that fits with this statistic function. Experiment Settings: We evaluated performance using the VE-SDE method with a batch size of 64 andan initial learning rate 1e-2. The learning rate decays step-wise by multiplying it by 0.95 every ten epochs.The model was trained for 100 epochs. Boarding pixels were extracted using a 21 19 mask. We repeatedthe experiments with different random seeds three times and averaged the results. Results:We assess the performance by comparing the negative log-likelihood (NLL) on the test set andthe deviation between the distribution mean and the sample mean of the statistics term (T (x)) againstbaseline methods. The results, presented in , indicate our methods performance improvements inNLL and the statistical term difference compared to the VE-SDE without T(x).",
  "Fitting data of point clouds": "Point cloud datasets are typically collections of individual 3D points, each with its position in space andpotentially associated attributes, forming a sparse and irregular representation of the underlying scene. Uni-formity is a critical factor for generating point clouds. Clumping often occurs when points are concentratedin specific areas, potentially losing detail in other regions. On the other hand, sparsity results in areaswith few points, leading to information loss and inaccuracies. Therefore, a generative model needs to payattention to uniformness among data points to achieve premium results. Experiment Settings:We evaluated our model on the ShapeNet dataset (Chang et al., 2015), a widelyused benchmark in 3D shape analysis and understanding. The ShapeNet dataset comprises 51,127 uniqueobjects distributed across 55 categories. Each category represents a distinct class of objects, covering variousshapes, such as airplanes, cars, chairs, and animals. The datasets richness and diversity make it ideal forevaluating generation performance. We evaluated our strategy on the two models. One is the DPM model(Luo & Hu, 2021), and the other is conducted on a latent diffusion model with the decoder is also the score-based model. The latent code is the shape code for each input point cloud. We use PointNet(Qi et al., 2017)to map the input points into a 512-dimension latent feature code for the encoder model. For the decoderand the latent prior score model, we use OccNet (Mescheder et al., 2019), which stacked 6 ResNet blockswith 256 dimensions for every hidden layer. We evaluated our model in two categories: airplane and car. Wereport Minimum Matching Distance(MMD), Coverage score(COV), and 1-NN classifier accuracy(1-NNA) toevaluate the quality of the samples. Results: The results are shown in Tab. 4. The results demonstrated that integrating smoothness constraintsinto generating and processing point cloud data improves generation quality.Encouraging a more evendistribution of points makes the resulting point cloud representation more accurate, robust, and suitablefor various applications. We further evaluate the difference between the sample mean and the distributionmean of the new statistic. We see that our method reduces the gap, which indicates that our model bettercaptures a key statistic in the data.",
  "The quality of the statistic function": "One question is whether an arbitrary function can act as a statistic term and improve the data fittingperformance. The question is important because if an arbitrary statistic function can improve the modelperformance, it is easy for a neural model F(x) to pick up such statistics in the energy function. In thissection, we set up two types of statistic functions: the first type captures data statistics meaningfully, andthe second type has no obvious relationship with the data.For the first type, we still use the statistic",
  "Published in Transactions on Machine Learning Research (10/2024)": "Tianyu Pang, Kun Xu, Chongxuan Li, Yang Song, Stefano Ermon, and Jun Zhu.Efficient learning ofgenerative models via finite-difference score matching. Advances in Neural Information Processing Systems,33:1917519188, 2020. Gabriel A Pinheiro, Johnatan Mucelini, Marinalva D Soares, Ronaldo C Prati, Juarez LF Da Silva, andMarcos G Quiles. Machine learning prediction of nine molecular properties based on the smiles represen-tation of the qm9 quantum-chemistry dataset. The Journal of Physical Chemistry A, 124(47):98549866,2020.",
  "Conclusion": "In this work, we propose an energy-based model whose energy function includes a neural energy functionand a specially designed statistic function. We show that this hybrid model inherits the nice property ofexponential-family models, that is, model training approximately matches the statistics distribution mean toits sample mean. This property allows us to express special statistics in the energy function as an inductivebias. We have shown that this technique can be extensively applied to multiple applications. The experimentsshow that the proposed strategy improves the modeling performance on three data types, molecular graphs,hand-written digits, and point clouds.",
  "We sincerely thank all reviewers and the editor for their insightful comments. The work was supported byNSF Award 2239869": "Panos Achlioptas, Olga Diamanti, Ioannis Mitliagkas, and Leonidas Guibas. Learning representations andgenerative models for 3d point clouds. In International conference on machine learning, pp. 4049. PMLR,2018. Arpit Bansal, Hong-Min Chu, Avi Schwarzschild, Soumyadip Sengupta, Micah Goldblum, Jonas Geiping,and Tom Goldstein. Universal guidance for diffusion models. In Proceedings of the IEEE/CVF Conferenceon Computer Vision and Pattern Recognition, pp. 843852, 2023.",
  "Andrew Gelman, John B Carlin, Hal S Stern, and Donald B Rubin. Bayesian data analysis. Chapman andHall/CRC, 1995": "Wenchong He and Zhe Jiang. Semi-supervised learning with the em algorithm: A comparative study betweenunstructured and structured prediction. IEEE Transactions on Knowledge and Data Engineering, 34(6):29122920, 2020. Wenchong He, Zhe Jiang, Tingsong Xiao, Zelin Xu, Shigang Chen, Ronald Fick, Miles Medina, and ChristineAngelini. A hierarchical spatial transformer for massive point samples in continuous space. Advances inneural information processing systems, 36:3336533378, 2023.",
  "Aapo Hyvrinen and Peter Dayan.Estimation of non-normalized statistical models by score matching.Journal of Machine Learning Research, 6(4), 2005": "Renyan Jiang and Shihai Xiao. Performance comparison of three parameter estimation methods on heavilycensored data.In 2021 8th International Conference on Dependable Systems and Their Applications(DSA), pp. 295301. IEEE, 2021. Jaehyeong Jo, Seul Lee, and Sung Ju Hwang. Score-based generative modeling of graphs via the systemof stochastic differential equations. In International Conference on Machine Learning, pp. 1036210383.PMLR, 2022.",
  "Anh-Tuan Nguyen and Sigrid Reiter. A performance comparison of sensitivity analysis methods for buildingenergy models. In Building simulation, volume 8, pp. 651664. Springer, 2015": "Chenhao Niu, Yang Song, Jiaming Song, Shengjia Zhao, Aditya Grover, and Stefano Ermon. Permutationinvariant graph generation via score-based generative modeling. In International Conference on ArtificialIntelligence and Statistics, pp. 44744484. PMLR, 2020. Pingbo Pan, Ping Liu, Yan Yan, Tianbao Yang, and Yi Yang. Adversarial localized energy network forstructured prediction. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pp.53475354, 2020.",
  "Franco P Preparata and Michael I Shamos. Computational geometry: an introduction. Springer Science &Business Media, 2012": "Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas. Pointnet: Deep learning on point sets for 3dclassification and segmentation. In Proceedings of the IEEE conference on computer vision and patternrecognition, pp. 652660, 2017. Lianhui Qin, Sean Welleck, Daniel Khashabi, and Yejin Choi. Cold decoding: Energy-based constrained textgeneration with langevin dynamics. Advances in Neural Information Processing Systems, 35:95389551,2022.",
  "Li Wenliang, Danica J Sutherland, Heiko Strathmann, and Arthur Gretton.Learning deep kernels forexponential family densities. In International Conference on Machine Learning, pp. 67376746. PMLR,2019": "Aoran Xiao, Jiaxing Huang, Dayan Guan, Xiaoqin Zhang, Shijian Lu, and Ling Shao. Unsupervised pointcloud representation learning with deep neural networks: A survey. IEEE Transactions on Pattern Analysisand Machine Intelligence, 45(9):1132111339, 2023. doi: 10.1109/TPAMI.2023.3262786. Jianwen Xie, Zilong Zheng, Ruiqi Gao, Wenguan Wang, Song-Chun Zhu, and Ying Nian Wu. Generativevoxelnet: learning energy-based models for 3d shape synthesis and analysis. IEEE Transactions on PatternAnalysis and Machine Intelligence, 44(5):24682484, 2020.",
  "Jiwen Yu, Yinhuai Wang, Chen Zhao, Bernard Ghanem, and Jian Zhang. Freedom: Training-free energy-guided conditional diffusion model. arXiv preprint arXiv:2303.09833, 2023": "Peiyu Yu, Sirui Xie, Xiaojian Ma, Baoxiong Jia, Bo Pang, Ruiqi Gao, Yixin Zhu, Song-Chun Zhu, andYing Nian Wu.Latent diffusion energy-based model for interpretable text modeling.arXiv preprintarXiv:2206.05895, 2022. Chengxi Zang and Fei Wang. Moflow: an invertible flow model for generating molecular graphs. In Proceed-ings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp.617626, 2020.",
  "We examined the impact of various parameters on the model, with results shown in and": "We examined how the number of neural network layers affects the fitting point cloud data task. In Tab. 5,T (x) denotes the difference between the distribution mean and the sample mean of the smoothness term.Our model is less sensitive to the hyper-parameter and has a smaller variance than the model without T(x)and has a smaller difference. Similar trends are observed in the task of fitting molecular data. In Tab. 6,T (x) refers to the difference between the distribution mean and the sample mean of the valency term. Theresults show that as the number of GNN layers increases, EDPGNN exhibits a higher variance in the validityratio and T (x), whereas EDPGNN with T(x) shows minor changes."
}