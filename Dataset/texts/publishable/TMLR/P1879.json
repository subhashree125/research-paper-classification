{
  "Abstract": "Federated Reinforcement Learning (FRL) allows multiple agents to collaboratively build adecision making policy without sharing raw trajectories. However, if a small fraction of theseagents are adversarial, it can lead to catastrophic results. We propose a policy gradient basedapproach that is robust to adversarial agents which can send arbitrary values to the server.Under this setting, our results form the first global convergence guarantees with generalparametrization.These results demonstrate resilience with adversaries, while achievingoptimal sample complexity of order O1",
  "Introduction": "Reinforcement Learning (RL) encompasses a category of challenges in which an agent iteratively selects ac-tions and acquires rewards within an unfamiliar environment, all while striving to maximize the cumulativerewards. The Policy Gradient (PG) based approaches serve as an effective means of addressing RL prob-lems, demonstrating its successful application in a diverse range of complex domains, such as game playing(Schrittwieser et al., 2020; Bonjour et al., 2022), transportation (Kiran et al., 2021; Al-Abbasi et al., 2019),robotics (Abeyruwan et al., 2023; Chen et al., 2023b), telesurgery (Gonzalez et al., 2023), network scheduling(Geng et al., 2020; Chen et al., 2023a), and healthcare (Yu et al., 2021). RL applications often demand extensive training data to attain the desired accuracy level. Parallelizing train-ing can significantly expedite this process, with one approach being Federated Reinforcement Learning (FRL)(Jin et al., 2022). In FRL, workers exchange locally trained models instead of raw data, ensuring efficientcommunication and data privacy. While Federated Learning (FL) is typically associated with supervisedlearning (Hosseinalipour et al., 2020), recent developments have extended its application to FRL, enablingmultiple agents to collaboratively construct decision-making policies without sharing raw trajectories. Distributed systems, including FRL, face vulnerabilities such as random failures or adversarial attacks.These issues may arise from agents arbitrary behavior due to hardware glitches, inaccurate training data,or deliberate malicious attacks. In the case of attacks, the possibility exists that attackers possess extensiveknowledge and can collaborate with others to maximize disruption. These scenarios fall under the Byzantinefailure model, the most rigorous fault framework in distributed computing, where a minority of agents can",
  "Related Works:": "1. Global Convergence of Policy Gradient Approaches:Recently, there has been an increasing re-search emphasis on exploring the global convergence of PG-based methods, going beyond the well-recognizedconvergence to first-order stationary policies. In (Agarwal et al., 2021a), a fairly comprehensive characteri-zation of global convergence for PG approaches is provided. Additionally, other significant studies on samplecomplexity for global convergence include (Wang et al., 2019; Xu et al., 2019; Liu et al., 2020; Masiha et al.,2022; Fatkhullin et al., 2023; Mondal & Aggarwal, 2024). We note that both (Fatkhullin et al., 2023) and(Mondal & Aggarwal, 2024) achieve near-optimal sample complexity of O(1/2). In the absence of adversarialelements, we use the work in (Fatkhullin et al., 2023) as a foundational reference for this paper. 2. Federated Reinforcement Learning:Federated reinforcement learning has been explored in varioussetups, including tabular RL (Agarwal et al., 2021b; Jin et al., 2022; Khodadadian et al., 2022), controltasks (Wang et al., 2023b), and value-function based algorithms (Wang et al., 2023a; Xie & Song, 2023),showcasing linear speedup. In the case of policy-gradient based algorithms, its evident that linear speedup isachievable, as each agents collected trajectories can be parallelized (Lan et al., 2023). Nevertheless, achievingspeedup becomes challenging with increasing nodes when adversaries are introduced, and this paper focuseson addressing this key issue. 3. Byzantine Fault Tolerance in Federated Learning:A significant body of research has focused ondistributed Stochastic Gradient Descent (SGD) with adversaries, with various works such as (Chen et al.,2017; El Mhamdi et al., 2018; Yin et al., 2018; Xie et al., 2018; Alistarh et al., 2018; Diakonikolas et al., 2019;Allen-Zhu et al., 2020; Prasad et al., 2020). Most studies typically employ an aggregator to merge gradientestimates from workers while filtering out unreliable ones (Xie et al., 2018; Chen et al., 2017; El Mhamdiet al., 2018; Yin et al., 2018). However, many of these approaches make stringent assumptions regardinggradient noise, making it challenging to compare different aggregators. Notably, (Farhadkhani et al., 2022)introduces the concept of (f, )-resilient averaging, demonstrating that several well-known aggregators canbe seen as special instances of this idea.In our work, we explore (f, )-averaging for combining policygradient estimates from workers.",
  "N2/35/3 + 4/3": "5/3,where N represents the number of workers and denotes the fraction of adversarial workers. However, theydo not provide global convergence guarantees and require that the variance of importance sampling weightsis upper bounded, which is unverifiable in practice. Moreover, their result is sub-optimal in N, which fails toprovide linear speedup even when no adversaries are present. They additionally perform variance reductionwith samples drawn from the central server, assuming its reliability. This is not generally allowed underfederated learning since this could lead to data leakage and privacy issues (Kairouz et al., 2021). In contrast,our study centers on the global convergence of federated RL while keeping the server process minimal,aggregating different gradients using (f, )-resilient aggregator (Farhadkhani et al., 2022), without requiringadditional samples at server. Distributed RL algorithms with adversaries have been studied for episodic tabular MDPs in (Chen et al.,2023c).Another line of works focus on empirical evaluations of Federated RL with adversaries (Lin &Ling, 2022; Rjoub et al., 2022; Zhang et al., 2022; Xu et al., 2022) and do not provide sample complexityguarantees.",
  ". This paper provides the first global convergence sample complexity findings for federated policygradient-based approaches with general parametrization in the presence of adversaries": "2. We derive the sample complexity for Res-NHARPG for a broad class of aggregators, called (f, )-aggregators (Farhadkhani et al., 2022). This includes several popular methods such as Krum (Blan-chard et al., 2017), Co-ordinate Wise Median (CWMed) and Co-ordinate Wise Trimmed Mean(CWTM) (Yin et al., 2018), Minimum Diameter Averaging (MDA) (El Mhamdi et al., 2018), Geo-metric Median (Chen et al., 2017) and Mean around Medians (MeaMed) (Xie et al., 2018).",
  "(see Remark 4.7). In particular, this implies linear speedup when f = O(N ), where 0.5": "We also provide experimental results showing the effectiveness of our algorithm under popular adversarialattacks (random noise, random action and sign flipping) under different environments (Cartpole-v1 fromOpenAI Gym and InvertedPendulum-v2, HalfCheetah, Hopper, Inverted Double Pendulum and Walkerfrom MuJoCo). We obtain a significant improvement over the original bounds for (f, )-resilient averaging in the contextof stochastic optimization (Farhadkhani et al., 2022). We achieve this by showing that our policy gradientupdate direction has certain desirable properties that allows us to utilize sharper concentration inequalities,instead of a simple union-bound (see ).",
  "Problem Setup": "We consider a discounted Markov decision process defined by the tuple (S, A, P, r, ), where S denotes thestate space, A denotes the action space, P(s|s, a) is the probability of transitioning from state s to states after taking action a, (0, 1) is the discount factor, and r : S A [R, R] is the reward functionof s and a. At each time t, the agent is at the current state st S and takes action at A based on apossibly stochastic policy : S P(A), i.e., at (|st). The agent then obtains a reward rt = r(st, at).The sequence of state-action pairs = {s0, a0, s1, a1, s2, a2, } is called a trajectory. We consider a parameter-server architecture with a trusted central server and N distributed workers oragents, labelled 1, 2, , N.We assume each agent is given an independent and identical copy of theMDP. Among these N agents, we assume that f are adversarial, with these agents providing any arbitraryinformation (such adversaries are also called Byzantine adversaries). The aim is for the agents to collaborateusing a federated policy gradient based approach to come up with a policy that maximizes the valuefunction, i.e.,",
  ",(1)": "where the initial state s0 is drawn from some distribution . In practice, the state and action spaces aretypically very large and thus the policy is parameterized by some Rd. The problem in equation 1 nowbecomes finding using a collaborative approach among agents that maximizes J(). However since thismaximization problem is usually nonconvex, it is difficult to find the globally optimal policy. Let J() denote J() henceforth and let J = max J(). The optimization problem equation 1 can besolved using the Policy Gradient (PG) approach at each agent, where the gradient J() is estimated throughsampling trajectories at that agent and observing rewards collected and then used in gradient ascent.",
  "Published in Transactions on Machine Learning Research (11/2024)": "Guangchen Lan, Han Wang, James Anderson, Christopher Brinton, and Vaneet Aggarwal. Improved commu-nication efficiency in federated natural policy gradient via admm-based gradient updates. In Proceedingsof the 37th International Conference on Neural Information Processing Systems, pp. 5987359885, 2023. Qifeng Lin and Qing Ling.Byzantine-robust federated deep deterministic policy gradient.In ICASSP2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp.40134017. IEEE, 2022. Yanli Liu, Kaiqing Zhang, Tamer Basar, and Wotao Yin. An improved analysis of (variance-reduced) policygradient and natural policy gradient methods. Advances in Neural Information Processing Systems, 33:76247636, 2020. Saeed Masiha, Saber Salehkaleybar, Niao He, Negar Kiyavash, and Patrick Thiran. Stochastic second-ordermethods improve best-known sample complexity of SGD for gradient-dominated functions. In Alice H. Oh,Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information ProcessingSystems, 2022. Washim U Mondal and Vaneet Aggarwal. Improved sample complexity analysis of natural policy gradientalgorithm with general parameterization for infinite horizon discounted reward markov decision processes.In International Conference on Artificial Intelligence and Statistics, pp. 30973105. PMLR, 2024. Matteo Papini, Damiano Binaghi, Giuseppe Canonaco, Matteo Pirotta, and Marcello Restelli.Stochas-tic variance-reduced policy gradient. In Proceedings of the 35th International Conference on MachineLearning, pp. 40264035, 2018. Adarsh Prasad, Arun Sai Suggala, Sivaraman Balakrishnan, and Pradeep Ravikumar. Robust estimation viarobust gradient estimation. Journal of the Royal Statistical Society: Series B (Statistical Methodology),82(3):601627, 2020. Gaith Rjoub, Jamal Bentahar, and Omar Abdel Wahab. Explainable ai-based federated deep reinforcementlearning for trusted autonomous driving. In 2022 International Wireless Communications and MobileComputing (IWCMC), pp. 318323. IEEE, 2022.",
  "The update in equation 3 is inspired by the N-HARPG algorithm in (Fatkhullin et al., 2023) sinceits use of Hessian information was shown to helpachieve sample complexity of order O 1": "2, whichis currently the state of the art.The algorithmdraws inspiration from the STORM variance reduc-tion technique (Cutkosky & Orabona, 2019), but in-stead of relying on the difference between successivestochastic gradients, it incorporates second-order in-formation (Tran & Cutkosky, 2022). This approachremoves the dependency on Importance Sampling(IS) and circumvents the need for unverifiable as-sumptions to bound the IS weights (Fatkhullin et al.,2023). In Step 10 of Algorithm 1, the update di-rection d(n)tis computed by adding a second-ordercorrection, (1 t)v(n)t(as defined in Step 9), tothe momentum stochastic gradient (1 t)d(n)t1 +tg( (n)t, t).The uniform sampling procedure inSteps 6-9 ensures that v(n)tis an unbiased estimatorof JH(t)JH(t1), closely resembling the termused in the original STORM method (Cutkosky &Orabona, 2019).",
  "k+1 = k + tdtdt,(5)": "where F is an aggregator and t > 0 is the stepsize. For the robust aggregator F, we consider (f, )-averagingintroduced in (Farhadkhani et al., 2022) as this family of aggregators encompasses several popularly usedmethods in Byzantine literature. The aggregator function aims to make our algorithm resilient to adversaries.The last step of the algorithm (Step 14) uses the direction dt with normalization to update the parametert+1. We now define (f, )-resilient averaging as in (Farhadkhani et al., 2022) below for ease of reference, wheref < N/2. We note that f < N/2 is the optimal breakdown point as no aggregator can possibly toleratef N/2 adversaries (Karimireddy et al., 2020). Definition 3.1 ((f, )-Resilient averaging). For f < N/2 and real value 0, an aggregation rule F iscalled (f, )-resilient averaging if for any collection of N vectors x1, . . . , xN, and any set G {1, . . . , N} ofsize N f,",
  "| G |iG xi, and | G | is the cardinality of G": "Several well-known aggregators such as Krum, CWMed, CWTM, MDA, GM and MeaMed are known to be(f, )-resilient. We note that for these aggregators, as a function of f is summarized in , wherethese values are from (Farhadkhani et al., 2022).As a result, our unified analysis provides guarantees forall the above mentioned methods. We define and discuss these aggregators in Appendix B.",
  ": In the above table, = min{": "d, 2N f}. The computational complexity of the aggregator, and theorder optimality of sample complexity (in terms of f, N, and ) are also mentioned in the Table. Remark 4.8 providesa discussion on the sample complexity bounds. (1) We note that while GM is convex optimization with no closed-formsolution, an (1 + )-approximate solution can be found in O(dN log3(N/)) time (Cohen et al., 2016). where A(s, a) is the advantage function of policy at (s, a), (s, a) = d (s) (a|s) is the state-actiondistribution induced by an optimal policy that maximizes J(), and w is the exact Natural PolicyGradient update direction at . bias captures the parametrization capacity of .For using the softmax parametrization, we havebias = 0 (Agarwal et al., 2021a).When is a restricted parametrization, which may not contain allstochastic policies, we have bias > 0. It is known that bias is very small when rich neural parametrizationsare used (Wang et al., 2019).Assumption 4.2. For all Rd, the Fisher information matrix induced by policy and initial statedistribution satisfies",
  "Comments on Assumptions 4.2-4.4: We would like to highlight that all the assumptions used in thiswork are commonly found in PG literature. We elaborate more on these assumptions below": "Assumption 4.2 requires that the eigenvalues of the Fisher information matrix can be bounded from below.This assumption is also known as the Fisher non-degenerate policy assumption and is commonly used inobtaining global complexity bounds for PG based methods (Liu et al., 2020; Zhang et al., 2021; Bai et al.,2022; Fatkhullin et al., 2023). Assumption 4.3 requires that the variance of the PG estimator must be bounded and 4.4 requires that thescore function is bounded and Lipschitz continuous. Both assumptions are widely used in the analysis of PGbased methods (Liu et al., 2020; Agarwal et al., 2021a; Papini et al., 2018; Xu et al., 2020; 2019; Fatkhullinet al., 2023).",
  "N + 2 log(N)": "For ease of exposition, we keep only dependence on , N and while providing a detailed expression inequation 38.We also note that the expression implicitly reflects the dependence on f, as the value of depends on both f and the choice of the aggregator. The sample complexity for different aggregatorfunctions are summarized in . The value of in the table for each aggregator is from (Farhadkhaniet al., 2022).",
  "We now remark on the lower bound for the sample complexity. In order to do that, we will first provide thelower bound for Stochastic Gradient Descent with Adversaries in (Alistarh et al., 2018)": "Lemma 4.6 (Alistarh et al. (2018)). For any D, V , and > 0, let there exists a linear function g :[D, D] R (of Lipschitz continuity G = /D) with a stochastic estimator gs such that E[gs] = g and||gs(x) g(x)|| V for all x in the domain. Then, given N machines, of which f are adversaries, andT samples from the stochastic estimator per machine, no algorithm can output x so that g(x) g(x) <",
  "Remark 4.7. Lemma 4.6 provides the lower bound for the sample complexity of SGD with adversaries suchas: T = 121N + f 2": "N 2. We note that the lower bound function class in Lemma 4.6 may not satisfy thefunction class in this paper explicitly. However, we believe that since the lower bound holds for any class offunctions that includes linear functions, the result should still hold with the assumptions in this paper. Wealso note that since 1/2 is a lower bound for the centralized case (Mondal & Aggarwal, 2024), it followsthat the lower bound for the distributed setup is 1",
  "N": "Remark 4.8. From Remark 4.7, it follows that MDA, CWTM and MeaMed achieve optimal sample com-plexity in terms of , N and f (upto logarithmic factors). As a result, these methods exhibit linear speedupwhen the number of adversarial agents is on the order of O( N). When the number of adversarial agentsis O(N ), where > 0.5, they achieve speedup of order O(N 2(1)). Its worth noting that while MDAposes computational challenges, CWTM and MeaMed are computationally inexpensive, on par with simpleaveraging used in vanilla federated policy gradient.Remark 4.9. Works in Byzantine literature generally use strong assumptions on the noise. For example, thereare a line of works assuming vanishing variance (Blanchard et al., 2017; El Mhamdi et al., 2018; Xie et al.,2018), which does not hold even for the simplest policy parametrizations. Intuitively, when the assumptionson the noise is weakened, it becomes harder to distinguish adversarial and honest workers. In contrast, weprovide these near-optimal bounds only using standard assumptions in Policy Gradient literature.",
  "specifically Of": "Nfand f = O(N ). Even with optimal and with 0 < 0.5, their sample complexitybounds do not achieve linear speedup. Furthermore, if the number of faulty workers > 0.5, the convergencerate deteriorates with an increase in the number of workers N. More specifically, if f = (N +1/2), their",
  "showing speedup of order O(N 2(1))": "We do this by showing that the update direction calculated by each worker i in Algorithm 1, d(i)t , is a sumof bounded martingale differences. As a result, we can then invoke Azuma-Hoeffding inequality to obtainsharper bounds. This ensures that our algorithm guarantees speedup as long as f = O(N ), where < 1.",
  "Evaluation": "To show the effectiveness of our algorithm design (i.e., Res-NHARPG), we provide evaluation results ontwo commonly-used continuous control tasks: CartPole-v1 from OpenAI Gym (Brockman et al., 2016) andInvertedPendulum-v2 from MuJoCo (Todorov et al., 2012). Additional experiments on more demandingMuJoCo tasks, including HalfCheetah, Hopper, Inverted Double Pendulum, and Walker are provided inAppendix A. For each task on CartPole-v1 and InvertedPendulum-v2, there are ten workers to individuallysample trajectories and compute gradients, and three of them are adversaries who would apply attacks tothe learning process. Note that we do not know which worker is an adversary, so we cannot simply ignorecertain gradient estimates to avoid the attacks. We simulate three types of attacks to the learning process:random noise, random action, and sign flipping. In , we present the learning process of the eight algorithms in two environments under three types ofattacks. In each subfigure, the x-axis represents the number of sampled trajectories; the y-axis records the",
  "(f) InvertedPendulum (Sign Flipping)": ": Evaluation results of Res-NHARPG on CartPole and InvertedPendulum. We test Res-NHARPG with thesix aggregators as shown in . For baselines, we select Res-NHARPG with a simple mean (SM) function asthe aggregator, which is equivalent to the original N-HARPG algorithm, and a vanilla policy gradient method withthe simple mean aggregator (PG-SM). For each environment, there are ten workers, of which three are adversaries,and we simulate three types of attacks: random noise, random action, and sign flipping. It can be observed thatN-HARPG outperforms PG and Res-NHARPG with those (f, ) aggregators can effectively handle multiple types ofattacks during the learning process.",
  "number of faulty workers. Notably, when certain aggregators are used (MDA, CWTM and MeaMed), weshow our approach achieves optimal sample complexity": "This work opens up multiple possible future directions in RL with adversaries, including delay in agentfeedback and heterogeneous agents. We further note that a parameter free version of the proposed algorithmis also an important future direction. Swetha Ganeshs research is supported by the Overseas Visiting Doctoral Fellowship (OVDF) and PrimeMinisters Research Fellowship (PMRF). Gugan Thoppes research is supported by the Indo-French Centrefor the Promotion of Advanced ResearchCEFIPRA (7102-1), the Walmart Centre for Tech Excellenceat IISc (CSR Grant WMGT-23-0001), DST-SERBs Core Research Grant (CRG/2021/008330), and thePratiksha Trust Young Investigator Award. Saminda Wishwajith Abeyruwan, Laura Graesser, David B DAmbrosio, Avi Singh, Anish Shankar, AlexBewley, Deepali Jain, Krzysztof Marcin Choromanski, and Pannag R Sanketi. i-sim2real: Reinforcementlearning of robotic policies in tight human-robot interaction loops. In Conference on Robot Learning, pp.212224. PMLR, 2023. Alekh Agarwal, Sham M Kakade, Jason D Lee, and Gaurav Mahajan. On the theory of policy gradientmethods: Optimality, approximation, and distribution shift. The Journal of Machine Learning Research,22(1):44314506, 2021a.",
  "Jonathan Baxter and Peter L Bartlett. Infinite-horizon policy-gradient estimation. Journal of ArtificialIntelligence Research, 15:319350, 2001": "Peva Blanchard, El Mahdi El Mhamdi, Rachid Guerraoui, and Julien Stainer.Machine learning withadversaries: Byzantine tolerant gradient descent. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach,R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems30, pp. 119129. Curran Associates, Inc., 2017. Trevor Bonjour, Marina Haliem, Aala Alsalem, Shilpa Thomas, Hongyu Li, Vaneet Aggarwal, MayankKejriwal, and Bharat Bhargava. Decision making in monopoly using a hybrid deep reinforcement learningapproach. IEEE Transactions on Emerging Topics in Computational Intelligence, 6(6):13351344, 2022.",
  "Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and WojciechZaremba. Openai gym. CoRR, abs/1606.01540, 2016": "Chang-Lin Chen, Bharat Bhargava, Vaneet Aggarwal, Basavaraj Tonshal, and Amrit Gopal. A hybrid deepreinforcement learning approach for jointly optimizing offloading and resource management in vehicularnetworks. IEEE Transactions on Vehicular Technology, 2023a. Jiayu Chen, Tian Lan, and Vaneet Aggarwal. Option-aware adversarial inverse reinforcement learning forrobotic control. In 2023 IEEE International Conference on Robotics and Automation (ICRA), pp. 59025908. IEEE, 2023b. Yiding Chen, Xuezhou Zhang, Kaiqing Zhang, Mengdi Wang, and Xiaojin Zhu. Byzantine-robust online andoffline distributed reinforcement learning. In Francisco Ruiz, Jennifer Dy, and Jan-Willem van de Meent(eds.), Proceedings of The 26th International Conference on Artificial Intelligence and Statistics, volume206 of Proceedings of Machine Learning Research, pp. 32303269. PMLR, 2023c. Yudong Chen, Lili Su, and Jiaming Xu. Distributed statistical machine learning in adversarial settings:Byzantine gradient descent. Proceedings of the ACM on Measurement and Analysis of Computing Systems,1(2):125, 2017. Michael B. Cohen, Yin Tat Lee, Gary Miller, Jakub Pachocki, and Aaron Sidford. Geometric median innearly linear time. In Proceedings of the Forty-Eighth Annual ACM Symposium on Theory of Computing,STOC 16, pp. 921. Association for Computing Machinery, 2016.",
  "Ashok Cutkosky and Francesco Orabona. Momentum-based variance reduction in non-convex sgd. Advancesin neural information processing systems, 32, 2019": "Ilias Diakonikolas, Gautam Kamath, Daniel Kane, Jerry Li, Jacob Steinhardt, and Alistair Stewart. Sever:A robust meta-algorithm for stochastic optimization. In International Conference on Machine Learning,pp. 15961606. PMLR, 2019. El Mahdi El Mhamdi, Rachid Guerraoui, and Sbastien Rouault. The hidden vulnerability of distributedlearning in Byzantium. In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th InternationalConference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp. 35213530.PMLR, 1015 Jul 2018.",
  "Thomas P Hayes. A large-deviation inequality for vector-valued martingales. Combinatorics, Probabilityand Computing, 2005": "Seyyedali Hosseinalipour, Christopher G. Brinton, Vaneet Aggarwal, Huaiyu Dai, and Mung Chiang. Fromfederated to fog learning: Distributed machine learning over heterogeneous wireless networks.IEEECommunications Magazine, 58(12):4147, 2020. doi: 10.1109/MCOM.001.2000410. Hao Jin, Yang Peng, Wenhao Yang, Shusen Wang, and Zhihua Zhang. Federated reinforcement learning withenvironment heterogeneity. In International Conference on Artificial Intelligence and Statistics, 2022. Peter Kairouz, H. Brendan McMahan, Brendan Avent, Aurlien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji,K. A. Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, Rafael G.L. DOliveira, Salim ElRouayheb, David Evans, Josh Gardner, Zachary Garrett, Adri Gascn, Badih Ghazi, Phillip B. Gibbons,Marco Gruteser, Zaid Harchaoui, Chaoyang He, Lie He, Zhouyuan Huo, Ben Hutchinson, Justin Hsu,Martin Jaggi, Tara Javidi, Gauri Joshi, Mikhail Khodak, Jakub Konen, Aleksandra Korolova, Fari-naz Koushanfar, Sanmi Koyejo, Tancrde Lepoint, Yang Liu, Prateek Mittal, Mehryar Mohri, RichardNock, Ayfer zgr, Rasmus Pagh, Mariana Raykova, Hang Qi, Daniel Ramage, Ramesh Raskar, DawnSong, Weikang Song, Sebastian U. Stich, Ziteng Sun, Ananda Theertha Suresh, Florian Tramr, PraneethVepakomma, Jianyu Wang, Li Xiong, Zheng Xu, Qiang Yang, Felix X. Yu, Han Yu, and Sen Zhao. Ad-vances and open problems in federated learning. Foundations and trends in machine learning, 14(12):1210, 2021.",
  "Sai Praneeth Karimireddy, Lie He, and Martin Jaggi. Byzantine-robust learning on heterogeneous datasetsvia bucketing. arXiv preprint arXiv:2006.09365, 2020": "Sajad Khodadadian, Pranay Sharma, Gauri Joshi, and Siva Theja Maguluri. Federated reinforcement learn-ing: Linear speedup under Markovian sampling. In International Conference on Machine Learning, 2022. B Ravi Kiran, Ibrahim Sobh, Victor Talpaert, Patrick Mannion, Ahmad A Al Sallab, Senthil Yogamani,and Patrick Prez. Deep reinforcement learning for autonomous driving: A survey. IEEE Transactionson Intelligent Transportation Systems, 23(6):49094926, 2021.",
  "Peter J Rousseeuw. Multivariate estimation with high breakdown point. Mathematical statistics and appli-cations, 8(37):283297, 1985": "Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt,Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, et al. Mastering atari, go, chess andshogi by planning with a learned model. Nature, 588(7839):604609, 2020. Zebang Shen, Alejandro Ribeiro, Hamed Hassani, Hui Qian, and Chao Mi. Hessian aided policy gradient.In Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings ofMachine Learning Research, pp. 57295738. PMLR, 0915 Jun 2019.",
  "Cong Xie, Oluwasanmi Koyejo, and Indranil Gupta. Generalized byzantine-tolerant sgd. arXiv preprintarXiv:1802.10116, 2018": "Zhijie Xie and Shenghui Song. FedKL: Tackling data heterogeneity in federated reinforcement learning bypenalizing KL divergence. IEEE Journal on Selected Areas in Communications, 41(4):12271242, 2023. Jian Xu, Shao-Lun Huang, Linqi Song, and Tian Lan. Byzantine-robust federated learning through collabo-rative malicious gradient filtering. In 2022 IEEE 42nd International Conference on Distributed ComputingSystems (ICDCS), pp. 12231235. IEEE, 2022.",
  "Chao Yu, Jiming Liu, Shamim Nemati, and Guosheng Yin. Reinforcement learning in healthcare: A survey.ACM Computing Surveys (CSUR), 55(1):136, 2021": "Rui Yuan, Robert M. Gower, and Alessandro Lazaric. A general sample complexity analysis of vanilla policygradient. In Proceedings of The 25th International Conference on Artificial Intelligence and Statistics,volume 151 of Proceedings of Machine Learning Research, pp. 33323380. PMLR, 2830 Mar 2022. Junyu Zhang, Chengzhuo Ni, Zheng Yu, Csaba Szepesvari, and Mengdi Wang. On the convergence andsample efficiency of variance-reduced policy gradient method. In A. Beygelzimer, Y. Dauphin, P. Liang,and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems, 2021. Mingyue Zhang, Zhi Jin, Jian Hou, and Renwei Luo. Resilient mechanism against byzantine failure for dis-tributed deep reinforcement learning. In 2022 IEEE 33rd International Symposium on Software ReliabilityEngineering (ISSRE), pp. 378389. IEEE, 2022.",
  ": Res-NHARPG with the MDA aggregator consistently outperform the baselines: N-HARPG (i.e.,SM) and Vanilla PG (i.e., PG-SM), on a series of MuJoCo tasks": "Previous evaluation results have shown the superiority of Res-NHARPG with (f, )- aggregators. To furtherdemonstrate its applicability, we consider Res-NHARPG with MDA and compare it with the baselines: N-HARPG (i.e., SM) and Vanilla PG (i.e., PG-SM), on a series of more challenging MuJoCo tasks: HalfCheetah,Hopper, Inverted Double Pendulum, Walker, of which the result is shown as .Our algorithmconsistently outperforms the baselines when adversaries (specifically, random noise) exist, and relatively,N-HARPG performs better than Vanilla PG. Note that our purpose is not to reach SOTA performancebut to testify the effectiveness of aggregators, so the three algorithms in each subfigure share the same setof hyperparameters (without heavy fine-tuning). In , we illustrate the training progress, up to amaximum number of sampled trajectories (6000 for the Inverted Double Pendulum and 1800 for other tasks),for each algorithm by plotting their episodic returns. The advantage of Res-NHARPG is more significantwhen considering the peak model performance. For instance, the highest evaluation score achieved by Res-NHARPG on Inverted Double Pendulum can exceed 9000, i.e., the SOTA performance as noted in (Wenget al., 2022), while the baselines scores are under 500. We also evaluate the Fed-ADMM algorithm proposedin (Lan et al., 2023) in . As expected, even with a higher number of samples, Fed-ADMM yieldssignificantly lower returns compared to Res-NHARPG with the MDA aggregator (see, for instance, Fed-ADMM performs poorly in HalfCheetah, achieves a return of 350 in Hopper compared to consistently above400 for our algorithm; 90 in Inverted Double Pendulum to above 4000 for our algorithm; less than 300 inWalker to around 1000 for our algorithm). Details regarding attacks considered: By random noise or sign flipping, the real estimated policygradients are altered by adding random noises or multiplying by a negative factor, respectively. While, forrandom action, adversarial workers would select random actions at each step, regardless of the state, whensampling trajectories for gradient estimations. Unlike the other attacks, random action does not directlychange the gradients, making it more challenging to detect. Also, random action is different from thewidely-adopted -greedy exploration method, since the action choice is fully random and the randomnessdoes not decay with the learning process.",
  "iCk[xi]k": "Krum (Blanchard et al., 2017): Multi-Krum outputs an average of the vectors that are the closestto their neighbors upon discarding f farthest vectors. Specifically, for each i [n] and k [n 1],let ik [n]\\{i} be the index of the k-th closest input vector from xi, i.e., we have xi xi1 . . . xi xin1 with ties broken arbitrarily. Let Ci be the set of N f 1 closest vectors to xi, i.e.,",
  "Comments:": "We observe that MDA, CWTM and MeaMed achieve optimal bounds, while CWMed, Krum andGM do not. This is explained in (Farhadkhani et al., 2022) by noting that Krum, CWMed, and GMoperate solely on median-based aggregation, while MDA, CWTM, and MeaMed perform an averagingstep after filtering out suspicious estimates. This averaging step results in variance reduction, similarto simple averaging in vanilla distributed SGD. Its worth mentioning that CWMed and GM do not require any knowledge of f to be implemented.On the other hand, while MDA, CWTM, MeaMed, and Krum do rely on some knowledge of f, theycan still be implemented by substituting an upper bound for f instead. In this case, our guaranteeswould scale based on this upper bound, instead of f.",
  "EBt(, ) 2JH()2 2H.(18)": "The first two statements are given in Lemma 4.2 and 4.4, (Yuan et al., 2022) and the third and fourthstatements are given in Lemma 4.1 in (Shen et al., 2019). Since the algorithm makes use of the truncatedgradient and hessian estimates JH() and 2JH() instead of J() and 2J(), we must bound the errorterms J() JH() and 2J() 2JH(). It is known that these error terms vanish geometrically fastwith H and is stated below:",
  "F (L+GH) and C2 = 22 +12(2L2 +2H +D2h2H) 6G1": "F +24D2g. The Lipschitz constantL, variance bounds 2 and 2H and the remaining terms Gg, GH, Dh and Dg in turn, can be bounded interms of , F , the bound on the reward function R and the Lipschitz and smoothness constants of the scorefunction G1, G2 (see Lemma C.1 and C.2). Substituting these bounds, we obtain",
  "DCompute Resources": "Experiments were conducted using the Oracle Cloud infrastructure, where each computation instance wasequipped with 8 Intel Xeon Platinum CPU cores and 128 GB of memory. In , each subfigure containscomparisons among eight algorithms each of which is repeated five times with different random seeds. Thesingle running time for an algorithm is approximately two hours. Thus, to reproduce , it requiresapproximately 480 CPU hours. Similarly, for , the time would be about 120 CPU hours."
}